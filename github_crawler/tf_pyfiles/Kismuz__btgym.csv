file_path,api_count,code
setup.py,0,"b""###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom setuptools import setup, find_packages\n\npackages = find_packages()\npackages = list(filter(lambda p: not p.startswith('btgym.research'), packages)) # removing due to syntax errors\n\nsetup(\n    name='btgym',\n    description='OpenAI Gym Environment API for Backtrader portfolio backtesting engine',\n    keywords='openai gym reinforcement learning backtrader portfolio trading ai finance',\n    author='Andrew Muzikin',\n    author_email='muzikinae@gmail.com',\n    url='https://github.com/Kismuz/btgym',\n    project_urls={\n        'Documentation': 'https://kismuz.github.io/btgym/',\n        'Source': 'https://github.com/Kismuz/btgym',\n        'Tracker': 'https://github.com/Kismuz/btgym/issues',\n    },\n    license='GPLv3+',\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'Intended Audience :: Financial and Insurance Industry',\n        'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Scientific/Engineering :: Information Analysis',\n        'Topic :: Office/Business :: Financial :: Investment',\n        'Topic :: Software Development :: Libraries :: Application Frameworks',\n\n\n    ],\n    version='0.0.8',\n    install_requires=[\n        'tensorflow>=1.5',\n        'opencv-python',\n        'gym[atari]',\n        'backtrader',\n        'pyzmq',\n        'matplotlib<=2.0.2',\n        'pillow',\n        'numpy',\n        'scipy',\n        'pandas',\n        'ipython',\n        'psutil',\n        'logbook'\n    ],\n    python_requires='>=3',\n    include_package_data=True,\n    packages=packages\n)\n"""
btgym/__init__.py,0,"b""###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom gym.envs.registration import register\n\nfrom .spaces import DictSpace, ActionDictSpace\nfrom .strategy import BTgymBaseStrategy\nfrom .server import BTgymServer\nfrom .datafeed import BTgymDataset, BTgymRandomDataDomain, BTgymSequentialDataDomain\nfrom .datafeed import DataSampleConfig, EnvResetConfig\nfrom .dataserver import BTgymDataFeedServer\nfrom .rendering import BTgymRendering\nfrom .envs.base import BTgymEnv\nfrom btgym.envs.multidiscrete import MultiDiscreteEnv\nfrom btgym.envs.portfolio import PortfolioEnv\n\nregister(\n    id='backtrader-v0000',\n    entry_point='btgym.envs:BTgymEnv',\n)\n"""
btgym/dataserver.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nimport multiprocessing\nimport copy\nimport zmq\nimport datetime\n\nfrom .datafeed import DataSampleConfig\n\n\nclass BTgymDataFeedServer(multiprocessing.Process):\n    """"""\n    Data provider server class.\n    Enables efficient data sampling for asynchronous multiply BTgym environments execution.\n    Manages global back-testing time and broadcast messages.\n    """"""\n    process = None\n    dataset_stat = None\n\n    def __init__(self, dataset=None, network_address=None, log_level=None, task=0):\n        """"""\n        Configures data server instance.\n\n        Args:\n            dataset:            data domain instance;\n            network_address:    ...to bind to.\n            log_level:          int, logbook.level\n            task:               id\n        """"""\n        super(BTgymDataFeedServer, self).__init__()\n\n        self.log_level = log_level\n        self.task = task\n        self.log = None\n        self.local_step = 0\n        self.dataset = dataset\n        self.network_address = network_address\n        self.default_sample_config = copy.deepcopy(DataSampleConfig)\n        self.broadcast_message = None\n\n        self.debug_pre_sample_fails = 0\n        self.debug_pre_sample_attempts = 0\n\n        # self.global_timestamp = 0\n\n    def get_data(self, sample_config=None):\n        """"""\n        Get Trial sample according to parameters received.\n        If no parameters being passed - makes sample with default parameters.\n\n        Args:\n            sample_config:   sampling parameters configuration dictionary\n\n        Returns:\n            sample:     if `sample_params` arg has been passed and dataset is ready\n            None:       otherwise\n        """"""\n        if self.dataset.is_ready:\n            if sample_config is not None:\n                # We do not allow configuration timestamps which point earlier than current global_timestamp;\n                # if config timestamp points later - it is ok because global time will be shifted accordingly after\n                # [traget test] sample will get into work.\n                if sample_config[\'timestamp\'] is None:\n                    sample_config[\'timestamp\'] = 0\n\n                # If config timestamp is outdated - refresh with latest:\n                if sample_config[\'timestamp\'] < self.dataset.global_timestamp:\n                    sample_config[\'timestamp\'] = copy.deepcopy(self.dataset.global_timestamp)\n\n                self.log.debug(\'Sampling with params: {}\'.format(sample_config))\n                sample = self.dataset.sample(**sample_config)\n\n            else:\n                self.default_sample_config[\'timestamp\'] = copy.deepcopy(self.dataset.global_timestamp)\n                self.log.debug(\'Sampling with default params: {}\'.format(self.default_sample_config))\n                sample = self.dataset.sample(**self.default_sample_config)\n\n            self.local_step += 1\n\n        else:\n            # Dataset not ready, make dummy:\n            sample = None\n\n        return sample\n\n    def run(self):\n        """"""\n        Server process runtime body.\n        """"""\n        # Logging:\n        from logbook import Logger, StreamHandler, WARNING\n        import sys\n        StreamHandler(sys.stdout).push_application()\n        if self.log_level is None:\n            self.log_level = WARNING\n        self.log = Logger(\'BTgymDataServer_{}\'.format(self.task), level=self.log_level)\n\n        self.process = multiprocessing.current_process()\n        self.log.info(\'PID: {}\'.format(self.process.pid))\n\n        # Set up a comm. channel for server as ZMQ socket:\n        context = zmq.Context()\n        socket = context.socket(zmq.REP)\n        socket.bind(self.network_address)\n\n        # Actually load data to BTgymDataset instance, will reset it later on:\n        try:\n            assert not self.dataset.data.empty\n\n        except (AssertionError, AttributeError) as e:\n            self.dataset.read_csv()\n\n        # Describe dataset:\n        self.dataset_stat = self.dataset.describe()\n\n        # Main loop:\n        while True:\n            # Stick here until receive any request:\n            service_input = socket.recv_pyobj()\n            self.log.debug(\'Received <{}>\'.format(service_input))\n\n            if \'ctrl\' in service_input:\n                # It\'s time to exit:\n                if service_input[\'ctrl\'] == \'_stop\':\n                    # Server shutdown logic:\n                    # send last run statistic, release comm channel and exit:\n                    message = {\'ctrl\': \'Exiting.\'}\n                    self.log.info(str(message))\n                    socket.send_pyobj(message)\n                    socket.close()\n                    context.destroy()\n                    return None\n\n                # Reset datafeed:\n                elif service_input[\'ctrl\'] == \'_reset_data\':\n                    try:\n                        kwargs = service_input[\'kwargs\']\n\n                    except KeyError:\n                        kwargs = {}\n\n                    self.dataset.reset(**kwargs)\n                    # self.global_timestamp = self.dataset.global_timestamp\n                    self.log.notice(\n                        \'Initial global_time set to: {} / stamp: {}\'.\n                        format(\n                            datetime.datetime.fromtimestamp(self.dataset.global_timestamp),\n                            self.dataset.global_timestamp\n                        )\n                    )\n                    message = {\'ctrl\': \'Reset with kwargs: {}\'.format(kwargs)}\n                    self.log.debug(\'Data_is_ready: {}\'.format(self.dataset.is_ready))\n                    socket.send_pyobj(message)\n                    self.local_step = 0\n\n                # Send dataset sample:\n                elif service_input[\'ctrl\'] == \'_get_data\':\n                    if self.dataset.is_ready:\n                        sample = self.get_data(sample_config=service_input[\'kwargs\'])\n                        message = \'Sending sample_#{}.\'.format(self.local_step)\n                        self.log.debug(message)\n                        socket.send_pyobj(\n                            {\n                                \'sample\': sample,\n                                \'stat\': self.dataset_stat,\n                                \'origin\': \'data_server\',\n                                \'timestamp\': self.dataset.global_timestamp,\n                            }\n                        )\n\n                    else:\n                        message = {\'ctrl\': \'Dataset not ready, waiting for control key <_reset_data>\'}\n                        self.log.debug(\'Sent: \' + str(message))\n                        socket.send_pyobj(message)  # pairs any other input\n\n                # Send dataset statisitc:\n                elif service_input[\'ctrl\'] == \'_get_info\':\n                    message = \'Sending info for #{}.\'.format(self.local_step)\n                    self.log.debug(message)\n                    # Compose response:\n                    info_dict = dict(\n                        dataset_stat=self.dataset_stat,\n                        dataset_columns=list(self.dataset.names),\n                        pid=self.process.pid,\n                        dataset_is_ready=self.dataset.is_ready,\n                        data_names=self.dataset.data_names\n                    )\n                    socket.send_pyobj(info_dict)\n\n                # Set global time:\n                elif service_input[\'ctrl\'] == \'_set_broadcast_message\':\n                    if self.dataset.global_timestamp != 0 and self.dataset.global_timestamp > service_input[\'timestamp\']:\n                        message = \'Moving back in time not supported! \' +\\\n                                  \'Current global_time: {}, \'.\\\n                                      format(datetime.datetime.fromtimestamp(self.dataset.global_timestamp)) +\\\n                                  \'attempt to set: {}; global_time and broadcast message not set.\'.\\\n                                      format(datetime.datetime.fromtimestamp(service_input[\'timestamp\'])) +\\\n                                  \'Hint: check sampling logic consistency.\'\n\n                        self.log.info(message)\n\n                    else:\n                        self.dataset.global_timestamp = service_input[\'timestamp\']\n                        self.broadcast_message = service_input[\'broadcast_message\']\n                        message = \'global_time set to: {} / stamp: {}\'.\\\n                            format(\n                                datetime.datetime.fromtimestamp(self.dataset.global_timestamp),\n                                self.dataset.global_timestamp\n                            )\n                    socket.send_pyobj(message)\n                    self.log.debug(message)\n\n                elif service_input[\'ctrl\'] == \'_get_global_time\':\n                    # Tell time:\n                    message = {\'timestamp\': self.dataset.global_timestamp}\n                    socket.send_pyobj(message)\n\n                elif service_input[\'ctrl\'] == \'_get_broadcast_message\':\n                    # Tell:\n                    message = {\n                        \'timestamp\': self.dataset.global_timestamp,\n                        \'broadcast_message\': self.broadcast_message,\n                    }\n                    socket.send_pyobj(message)\n\n                else:  # ignore any other input\n                    # NOTE: response dictionary must include \'ctrl\' key\n                    message = {\n                        \'ctrl\':\n                            \'waiting for control keys:  <_reset_data>, <_get_data>, \' +\n                            \'<_get_info>, <_stop>, <_get_global_time>, <_get_broadcast_message>\'\n                    }\n                    self.log.debug(\'Sent: \' + str(message))\n                    socket.send_pyobj(message)  # pairs any other input\n\n            else:\n                message = {\'ctrl\': \'No <ctrl> key received, got:\\n{}\'.format(service_input)}\n                self.log.debug(str(message))\n                socket.send_pyobj(message) # pairs input\n'"
btgym/server.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017-19 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nimport multiprocessing\nimport gc\n\nimport itertools\nimport zmq\nimport copy\n\nimport time, datetime\nimport random\nfrom datetime import timedelta\n\nimport backtrader as bt\nfrom .datafeed import DataSampleConfig, EnvResetConfig\nfrom .strategy.observers import NormPnL, Position, Reward\n\n###################### BT Server in-episode communocation method ##############\n\n\nclass _BTgymAnalyzer(bt.Analyzer):\n    """"""\n    This [kind of] misused analyzer handles strategy/environment communication logic\n    while in episode mode.\n    As part of core server operational logic, it should not be explicitly called/edited.\n    Yes, it actually analyzes nothing.\n    """"""\n    log = None\n    socket = None\n\n    def __init__(self):\n        # Inherit logger and ZMQ socket from parent:\n        self.log = self.strategy.env._log\n        self.socket = self.strategy.env._socket\n        self.data_socket = self.strategy.env._data_socket\n        self.render = self.strategy.env._render\n\n        # Pass data serving methods:\n        self.get_current_trial = self.strategy.env._get_data\n        self.can_broadcast = self.strategy.can_broadcast\n        self.get_timestamp = self.strategy._get_timestamp\n        self.get_dataset_info = self.strategy.env._get_info\n        self.get_broadcast_info = self.strategy._get_broadcast_info\n\n        self.message = None\n        self.step_to_render = None  # Due to reset(), this will get populated before first render() call.\n        self.respond_pending = False\n\n        # At the end of the episode - render everything but episode:\n        self.render_at_stop = self.render.render_modes.copy()\n        try:\n            self.render_at_stop.remove(\'episode\')\n\n        except:\n            pass\n\n        self.info_list = []\n\n    def prenext(self):\n        pass\n\n    def stop(self):\n        pass\n\n    def early_stop(self):\n        """"""\n        Stop, take picture and get out.\n        """"""\n        self.log.debug(\'RunStop() invoked with {}\'.format(self.strategy.broker_message))\n\n        # Do final renderings, it will be kept by renderer class, not sending anywhere:\n        self.render.render(self.render_at_stop, step_to_render=self.step_to_render, send_img=False)\n\n        self.strategy.close()\n        self.strategy.env.runstop()\n\n    def send_env_response(self, is_done):\n        """"""\n        Sends environment response as <o, r, d, i> tuple.\n        See issue #84.\n        """"""\n        # Gather response:\n        raw_state = self.strategy.get_raw_state()\n        state = self.strategy.get_state()\n        reward = self.strategy.get_reward()\n        # Send response as <o, r, d, i> tuple (Gym convention),\n        # opt to send entire info_list or just latest part:\n        info = [self.info_list[-1]]\n        self.socket.send_pyobj((state, reward, is_done, info))\n\n        # Increment global time by sending timestamp to data_server, if authorized;\n        if self.can_broadcast:\n            global_timestamp = self.get_timestamp()\n            broadcast_info = self.get_broadcast_info()\n            self.log.debug(\'broadcasting timestamp: {}\'.format(global_timestamp))\n\n            self.data_socket.send_pyobj(\n                {\n                    \'ctrl\': \'_set_broadcast_message\',\n                    \'timestamp\': global_timestamp,\n                    \'broadcast_message\': broadcast_info,\n                }\n            )\n            broadcast_set_response = self.data_socket.recv_pyobj()\n            self.log.debug(\'DATA_COMM/broadcast received: {}\'.format(broadcast_set_response))\n\n        # Back up step information for rendering.\n        # It pays when using skip-frames: will\'ll get future state otherwise.\n\n        self.step_to_render = ({\'human\': raw_state}, state, reward, is_done, self.info_list)\n\n        # Reset info:\n        self.info_list = []\n        self.strategy.env_iteration += 1\n        self.respond_pending = False\n\n    def next(self):\n        """"""\n        Actual env.step() communication and episode termination is here.\n        """"""\n        # We\'ll do it every step:\n        # If it\'s time to leave:\n        is_done = self.strategy._get_done()\n        # Collect step info:\n        self.info_list.append(self.strategy.get_info())\n        # Put agent on hold:\n        self.strategy.action = self.strategy.p.initial_portfolio_action\n        # Trick to avoid excessive orders emitting during skip_frame loop:\n        self.strategy.action[\'_skip_this\'] = True\n\n        # Only if it\'s time to communicate or episode has come to end:\n        if self.strategy.iteration % self.strategy.p.skip_frame == 0 or is_done:\n            if self.respond_pending:\n                # Other side is waiting for response:\n                self.send_env_response(is_done)\n\n                # If done, initiate fallback to Control Mode:\n                if is_done:\n                    self.early_stop()\n                    return\n\n            #print(\'Analyzer_strat_iteration:\', self.strategy.iteration)\n            #print(\'Analyzer_env_iteration:\', self.strategy.env_iteration)\n\n            # Halt and wait to receive message from outer world:\n            self.message = self.socket.recv_pyobj()\n            msg = \'COMM received: {}\'.format(self.message)\n            self.log.debug(msg)\n\n            # Control actions loop, ignoring \'action\' key:\n            while \'ctrl\' in self.message:\n                # Rendering requested:\n                if self.message[\'ctrl\'] == \'_render\':\n                    self.socket.send_pyobj(\n                        self.render.render(\n                            self.message[\'mode\'],\n                            step_to_render=self.step_to_render,\n                        )\n                    )\n                # Episode termination requested:\n                elif self.message[\'ctrl\'] == \'_done\':\n                    is_done = True  # redundant\n                    self.socket.send_pyobj(\'_DONE SIGNAL RECEIVED\')\n                    self.early_stop()\n                    return None\n\n                elif self.message[\'ctrl\'] == \'_get_data\':\n                    self.socket.send_pyobj(self.get_current_trial())\n\n                elif self.message[\'ctrl\'] == \'_get_info\':\n                    self.socket.send_pyobj(self.get_dataset_info())\n\n                # Unknown key:\n                else:\n                    message = {\'ctrl\': \'send control keys: <_reset>, <_getstat>, \' +\n                                       \'<_render>, <_stop>, or valid agent action\'}\n                    self.log.warning(\n                        \'Analyzer received unexpected key: {}; Sent: {}\'.format(self.message, str(message))\n                    )\n                    self.socket.send_pyobj(message)\n\n                # Halt again:\n                self.message = self.socket.recv_pyobj()\n                msg = \'COMM recieved: {}\'.format(self.message)\n                self.log.debug(msg)\n\n            # Store agent action an rise respond_pending flag:\n            if \'action\' in self.message:  # now it should!\n                self.strategy.action = self.message[\'action\']\n                self.strategy.last_action = self.message[\'action\']\n                self.respond_pending = True\n\n            else:\n                msg = \'No <action> key recieved:\\n\' + msg\n                raise AssertionError(msg)\n\n        # If done, initiate fallback to Control Mode:\n        if is_done:\n            self.early_stop()\n\n        # Strategy housekeeping:\n        self.strategy.iteration += 1\n        self.strategy.broker_message = \'-\'\n\n    ##############################  BTgym Server Main  ##############################\n\n\nclass BTgymServer(multiprocessing.Process):\n    """"""Backtrader server class.\n\n    Expects to receive dictionary, containing at least \'action\' field.\n\n    Control mode IN::\n\n        dict(action=<control action, type=str>,), where control action is:\n        \'_reset\' - rewinds backtrader engine and runs new episode;\n        \'_getstat\' - retrieve episode results and statistics;\n        \'_stop\' - server shut-down.\n\n    Control mode OUT::\n\n        <string message> - reports current server status;\n        <statisic dict> - last run episode statisics.  NotImplemented.\n\n        Within-episode signals:\n        Episode mode IN:\n        dict(action=<agent_action, type=str>,), where agent_action is:\n        {\'buy\', \'sell\', \'hold\', \'close\', \'_done\'} - agent or service actions; \'_done\' - stops current episode;\n\n    Episode mode OUT::\n\n        response  <tuple>: observation, <array> - observation of the current environment state,\n                                                 could be any tensor; default is:\n                                                 [4,m] array of <fl32>, where:\n                                                 m - num. of last datafeed values,\n                                                 4 - num. of data features (Lines);\n                           reward, <any> - current portfolio statistics for environment reward estimation;\n                           done, <bool> - episode termination flag;\n                           info, <list> - auxiliary information.\n    """"""\n    data_server_response = None\n\n    def __init__(\n        self,\n        cerebro=None,\n        render=None,\n        network_address=None,\n        data_network_address=None,\n        connect_timeout=90,\n        log_level=None,\n        task=0,\n    ):\n        """"""\n\n        Args:\n            cerebro:                backtrader.cerebro engine class.\n            render:                 render class\n            network_address:        environmnet communication, str\n            data_network_address:   data communication, str\n            connect_timeout:        seconds, int\n            log_level:              int, logbook.level\n        """"""\n\n        super(BTgymServer, self).__init__()\n        self.task = task\n        self.log_level = log_level\n        self.log = None\n        self.process = None\n        self.cerebro = cerebro\n        self.network_address = network_address\n        self.render = render\n        self.data_network_address = data_network_address\n        self.connect_timeout = connect_timeout # server connection timeout in seconds.\n        self.connect_timeout_step = 0.01\n\n        self.trial_sample = None\n        self.trial_stat = None\n        self.dataset_stat = None\n\n    @staticmethod\n    def _comm_with_timeout(socket, message):\n        """"""\n        Exchanges messages via socket with timeout.\n\n        Note:\n            socket zmq.RCVTIMEO and zmq.SNDTIMEO should be set to some finite number of milliseconds\n\n        Returns:\n            dictionary:\n                status: communication result;\n                message: received message, if any.\n        """"""\n        response=dict(\n            status=\'ok\',\n            message=None,\n        )\n        try:\n            socket.send_pyobj(message)\n\n        except zmq.ZMQError as e:\n            if e.errno == zmq.EAGAIN:\n                response[\'status\'] = \'send_failed_due_to_connect_timeout\'\n\n            else:\n                response[\'status\'] = \'send_failed_for_unknown_reason\'\n            return response\n\n        start = time.time()\n        try:\n            response[\'message\'] = socket.recv_pyobj()\n            response[\'time\'] =  time.time() - start\n\n        except zmq.ZMQError as e:\n            if e.errno == zmq.EAGAIN:\n                response[\'status\'] = \'receive_failed_due_to_connect_timeout\'\n\n            else:\n                response[\'status\'] = \'receive_failed_for_unknown_reason\'\n            return response\n\n        return response\n\n    def get_dataset_stat(self):\n        data_server_response = self._comm_with_timeout(\n            socket=self.data_socket,\n            message={\'ctrl\': \'_get_info\'}\n        )\n        if data_server_response[\'status\'] in \'ok\':\n            self.log.debug(\'Data_server @{} responded with dataset statistic in about {} seconds.\'.\n                           format(self.data_network_address, data_server_response[\'time\']))\n\n            return data_server_response[\'message\']\n\n        else:\n            msg = \'BtgymServer_sampling_attempt: data_server @{} unreachable with status: <{}>.\'. \\\n                format(self.data_network_address, data_server_response[\'status\'])\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n    def get_trial(self, **reset_kwargs):\n        """"""\n\n        Args:\n            reset_kwargs:   dictionary of args to pass to parent data iterator\n\n        Returns:\n            trial_sample, trial_stat, dataset_stat\n        """"""\n        wait = 0\n        while True:\n            # Get new data subset:\n            data_server_response = self._comm_with_timeout(\n                socket=self.data_socket,\n                message={\'ctrl\': \'_get_data\', \'kwargs\': reset_kwargs}\n            )\n            if data_server_response[\'status\'] in \'ok\':\n                self.log.debug(\'Data_server @{} responded in ~{:1.6f} seconds.\'.\n                               format(self.data_network_address, data_server_response[\'time\']))\n\n            else:\n                msg = \'BtgymServer_sampling_attempt: data_server @{} unreachable with status: <{}>.\'. \\\n                    format(self.data_network_address, data_server_response[\'status\'])\n                self.log.error(msg)\n                raise ConnectionError(msg)\n\n            # Ready or not?\n            try:\n                assert \'Dataset not ready\' in data_server_response[\'message\'][\'ctrl\']\n                if wait <= self.wait_for_data_reset:\n                    pause = random.random() * 2\n                    time.sleep(pause)\n                    wait += pause\n                    self.log.info(\n                        \'Domain dataset not ready, wait time left: {:4.2f}s.\'.format(self.wait_for_data_reset - wait)\n                    )\n                else:\n                    data_server_response = self._comm_with_timeout(\n                        socket=self.data_socket,\n                        message={\'ctrl\': \'_stop\'}\n                    )\n                    self.socket.close()\n                    self.context.destroy()\n                    raise RuntimeError(\'Failed to assert Domain dataset is ready. Exiting.\')\n\n            except (AssertionError, KeyError) as e:\n                break\n        # Get trial instance:\n        trial_sample = data_server_response[\'message\'][\'sample\']\n        trial_stat = trial_sample.describe()\n        trial_sample.reset()\n        dataset_stat = data_server_response[\'message\'][\'stat\']\n        origin = data_server_response[\'message\'][\'origin\']\n        timestamp = data_server_response[\'message\'][\'timestamp\']\n\n        return trial_sample, trial_stat, dataset_stat, origin, timestamp\n\n    def get_trial_message(self):\n        """"""\n        Prepares  message containing current trial instance, mimicking data_server message protocol.\n        Intended for serving requests from data_slave environment.\n\n        Returns:\n            dict containing trial instance, d_set statistic and origin key; dict containing \'ctrl\' response if master\n            d_set is not ready;\n        """"""\n        if self.trial_sample is not None:\n            message = {\n                \'sample\': self.trial_sample,\n                \'stat\': self.dataset_stat,\n                \'origin\': \'master_environment\',\n                \'timestamp\': self.get_global_time()\n            }\n\n        else:\n            message = {\'ctrl\': \'Dataset not ready, hold on...\'}\n            self.log.debug(\'Sent to slave: \' + str(message))\n\n        return message\n\n    def get_global_time(self):\n        """"""\n        Asks dataserver for current dataset global_time.\n\n        Returns:\n            POSIX timestamp\n        """"""\n        data_server_response = self._comm_with_timeout(\n            socket=self.data_socket,\n            message={\'ctrl\': \'_get_global_time\'}\n        )\n        if data_server_response[\'status\'] in \'ok\':\n            pass\n\n        else:\n            msg = \'BtgymServer_sampling_attempt: data_server @{} unreachable with status: <{}>.\'. \\\n                format(self.data_network_address, data_server_response[\'status\'])\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n        return data_server_response[\'message\'][\'timestamp\']\n\n    def get_broadcast_message(self):\n        """"""\n        Asks dataserver for current dataset global_time and broadcast message.\n\n        Returns:\n            POSIX timestamp\n        """"""\n        data_server_response = self._comm_with_timeout(\n            socket=self.data_socket,\n            message={\'ctrl\': \'_get_broadcast_message\'}\n        )\n        if data_server_response[\'status\'] in \'ok\':\n            pass\n\n        else:\n            msg = \'BtgymServer_sampling_attempt: data_server @{} unreachable with status: <{}>.\'. \\\n                format(self.data_network_address, data_server_response[\'status\'])\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n        return data_server_response[\'message\'][\'timestamp\'], data_server_response[\'message\'][\'broadcast_message\']\n\n    def run(self):\n        """"""\n        Server process runtime body. This method is invoked by env._start_server().\n        """"""\n        # Logging:\n        from logbook import Logger, StreamHandler, WARNING\n        import sys\n        StreamHandler(sys.stdout).push_application()\n        if self.log_level is None:\n            self.log_level = WARNING\n        self.log = Logger(\'BTgymServer_{}\'.format(self.task), level=self.log_level)\n\n        self.process = multiprocessing.current_process()\n        self.log.info(\'PID: {}\'.format(self.process.pid))\n\n        # Runtime Housekeeping:\n        cerebro = None\n        episode_result = dict()\n        episode_sample = None\n\n        # How long to wait for data_master to reset data:\n        self.wait_for_data_reset = 300  # seconds\n\n        connect_timeout = 60  # in seconds\n\n        # Set up a comm. channel for server as ZMQ socket\n        # to carry both service and data signal\n        # !! Reminder: Since we use REQ/REP - messages do go in pairs !!\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.REP)\n        self.socket.setsockopt(zmq.RCVTIMEO, -1)\n        self.socket.setsockopt(zmq.SNDTIMEO, connect_timeout * 1000)\n        self.socket.bind(self.network_address)\n\n        self.data_context = zmq.Context()\n        self.data_socket = self.data_context.socket(zmq.REQ)\n        self.data_socket.setsockopt(zmq.RCVTIMEO, connect_timeout * 1000)\n        self.data_socket.setsockopt(zmq.SNDTIMEO, connect_timeout * 1000)\n        self.data_socket.connect(self.data_network_address)\n\n        # Check connection:\n        self.log.debug(\'Pinging data_server at: {} ...\'.format(self.data_network_address))\n\n        data_server_response = self._comm_with_timeout(\n            socket=self.data_socket,\n            message={\'ctrl\': \'ping!\'}\n        )\n        if data_server_response[\'status\'] in \'ok\':\n            self.log.debug(\'Data_server seems ready with response: <{}>\'.\n                          format(data_server_response[\'message\']))\n\n        else:\n            msg = \'Data_server unreachable with status: <{}>.\'.\\\n                format(data_server_response[\'status\'])\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n        # Init renderer:\n        self.render.initialize_pyplot()\n\n        # Mandatory DrawDown and auxillary plotting observers to add to data-master strategy instance:\n        # TODO: make plotters optional args\n        if self.render.enabled:\n            aux_obsrevers = [bt.observers.DrawDown, Reward, Position, NormPnL]\n\n        else:\n            aux_obsrevers = [bt.observers.DrawDown]\n\n        # Server \'Control Mode\' loop:\n        for episode_number in itertools.count(0):\n            while True:\n                # Stuck here until \'_reset\' or \'_stop\':\n                service_input = self.socket.recv_pyobj()\n                msg = \'Control mode: received <{}>\'.format(service_input)\n                self.log.debug(msg)\n\n                if \'ctrl\' in service_input:\n                    # It\'s time to exit:\n                    if service_input[\'ctrl\'] == \'_stop\':\n                        # Server shutdown logic:\n                        # send last run statistic, release comm channel and exit:\n                        message = \'Exiting.\'\n                        self.log.info(message)\n                        self.socket.send_pyobj(message)\n                        self.socket.close()\n                        self.context.destroy()\n                        return None\n\n                    # Start episode:\n                    elif service_input[\'ctrl\'] == \'_reset\':\n                        message = \'Preparing new episode with kwargs: {}\'.format(service_input[\'kwargs\'])\n                        self.log.debug(message)\n                        self.socket.send_pyobj(message)  # pairs \'_reset\'\n                        break\n\n                    # Retrieve statistic:\n                    elif service_input[\'ctrl\'] == \'_getstat\':\n                        self.socket.send_pyobj(episode_result)\n                        self.log.debug(\'Episode statistic sent.\')\n\n                    # Send episode rendering:\n                    elif service_input[\'ctrl\'] == \'_render\' and \'mode\' in service_input.keys():\n                        # Just send what we got:\n                        self.socket.send_pyobj(self.render.render(service_input[\'mode\']))\n                        self.log.debug(\'Episode rendering for [{}] sent.\'.format(service_input[\'mode\']))\n\n                    # Serve data-dependent environment with trial instance:\n                    elif service_input[\'ctrl\'] == \'_get_data\':\n                        message = \'Sending trial data to slave\'\n                        self.log.debug(message)\n                        self.socket.send_pyobj(self.get_trial_message())\n\n                    # Serve data-dependent environment with dataset statisitc:\n                    elif service_input[\'ctrl\'] == \'_get_info\':\n                        message = \'Sending dataset statistic to slave\'\n                        self.log.debug(message)\n                        self.socket.send_pyobj(self.get_dataset_stat())\n\n                    else:  # ignore any other input\n                        # NOTE: response string must include \'ctrl\' key\n                        # for env.reset(), env.get_stat(), env.close() correct operation.\n                        message = {\'ctrl\': \'send control keys: <_reset>, <_getstat>, <_render>, <_stop>.\'}\n                        self.log.debug(\'Control mode: sent: \' + str(message))\n                        self.socket.send_pyobj(message)  # pairs any other input\n\n                else:\n                    message = \'No <ctrl> key received:{}\\nHint: forgot to call reset()?\'.format(msg)\n                    self.log.debug(message)\n                    self.socket.send_pyobj(message)\n\n            # Got \'_reset\' signal -> prepare Cerebro subclass and run episode:\n            start_time = time.time()\n            cerebro = copy.deepcopy(self.cerebro)\n            cerebro._socket = self.socket\n            cerebro._data_socket = self.data_socket\n            cerebro._log = self.log\n            cerebro._render = self.render\n\n            # Pass methods for serving capabilities:\n            cerebro._get_data = self.get_trial_message\n            cerebro._get_info = self.get_dataset_stat\n\n            # Add auxillary observers, if not already:\n            for aux in aux_obsrevers:\n                is_added = False\n                for observer in cerebro.observers:\n                    if aux in observer:\n                        is_added = True\n                if not is_added:\n                    cerebro.addobserver(aux)\n\n            # Add communication utility:\n            cerebro.addanalyzer(_BTgymAnalyzer, _name=\'_env_analyzer\',)\n\n            # Data preparation:\n\n            # Renew system state:\n            current_timestamp, current_broadcast_message = self.get_broadcast_message()\n\n            # Parse args we got with _reset call:\n            sample_config = dict(\n                episode_config=copy.deepcopy(DataSampleConfig),\n                trial_config=copy.deepcopy(DataSampleConfig)\n            )\n            for key, config in sample_config.items():\n                try:\n                    config.update(service_input[\'kwargs\'][key])\n\n                except KeyError:\n                    self.log.debug(\n                        \'_reset <{}> kwarg not found, using default values: {}\'.format(key, config)\n                    )\n            sample_config[\'trial_config\'][\'broadcast_message\'] = current_broadcast_message\n            sample_config[\'episode_config\'][\'broadcast_message\'] = current_broadcast_message\n\n            # Get new Trial from data_server if requested,\n            # despite bult-in new/reuse data object sampling option, perform checks here to avoid\n            # redundant traffic:\n            if sample_config[\'trial_config\'][\'get_new\'] or self.trial_sample is None:\n                self.log.info(\n                    \'Requesting new Trial sample with args: {}\'.format(sample_config[\'trial_config\'])\n                )\n                self.trial_sample, self.trial_stat, self.dataset_stat, origin, current_timestamp =\\\n                    self.get_trial(**sample_config[\'trial_config\'])\n\n                if origin in \'data_server\':\n                    self.trial_sample.set_logger(self.log_level, self.task)\n\n                self.log.debug(\'Got new Trial: <{}>\'.format(self.trial_sample.filename))\n\n            else:\n                self.log.info(\'Reusing Trial <{}>\'.format(self.trial_sample.filename))\n                # current_timestamp = self.get_global_time()\n\n            self.log.debug(\n                \'current global_time: {}\'.format(datetime.datetime.fromtimestamp(current_timestamp))\n            )\n            # Get episode:\n            if sample_config[\'episode_config\'][\'timestamp\'] is None or\\\n                    sample_config[\'episode_config\'][\'timestamp\'] < current_timestamp:\n                sample_config[\'episode_config\'][\'timestamp\'] = current_timestamp\n\n            self.log.info(\n                \'Requesting episode from <{}> with args: {}\'.\n                format(self.trial_sample.filename, sample_config[\'episode_config\'])\n            )\n\n            episode_sample = self.trial_sample.sample(**sample_config[\'episode_config\'])\n            self.log.debug(\'Got new Episode: <{}>\'.format(episode_sample.filename))\n\n            # Get episode data statistic and pass it to strategy params:\n            cerebro.strats[0][0][2][\'trial_stat\'] = self.trial_stat\n            cerebro.strats[0][0][2][\'trial_metadata\'] = self.trial_sample.metadata\n            cerebro.strats[0][0][2][\'dataset_stat\'] = self.dataset_stat\n            cerebro.strats[0][0][2][\'episode_stat\'] = episode_sample.describe()\n            cerebro.strats[0][0][2][\'metadata\'] = episode_sample.metadata\n\n            cerebro.strats[0][0][2][\'broadcast_message\'] = current_broadcast_message\n\n            # Set nice broker cash plotting:\n            cerebro.broker.set_shortcash(False)\n\n            # Convert and add data to engine:\n            feed = episode_sample.to_btfeed()\n            if isinstance(feed, dict):\n                for key, stream in feed.items():\n                    cerebro.adddata(stream, name=key)\n\n            else:\n                cerebro.adddata(feed, name=\'base_asset\')\n\n            # Finally:\n            episode = cerebro.run(stdstats=True, preload=False, oldbuysell=True, tradehistory=True)[0]\n\n            self.log.debug(\'Episode run finished.\')\n\n            # Update episode rendering:\n            _ = self.render.render(\'just_render\', cerebro=cerebro)\n            _ = None\n\n            # Recover that bloody analytics:\n            analyzers_list = episode.analyzers.getnames()\n            analyzers_list.remove(\'_env_analyzer\')\n\n            elapsed_time = timedelta(seconds=time.time() - start_time)\n            self.log.debug(\'Episode elapsed time: {}.\'.format(elapsed_time))\n\n            episode_result[\'episode\'] = episode_number\n            episode_result[\'runtime\'] = elapsed_time\n            episode_result[\'length\'] = len(episode.data.close)\n\n            for name in analyzers_list:\n                episode_result[name] = episode.analyzers.getbyname(name).get_analysis()\n\n            gc.collect()\n\n        # Just in case -- we actually shouldn\'t get there except by some error:\n        return None\n'"
btgym/spaces.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin, muzikinae@gmail.com\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom gym import Space\nfrom gym import spaces\n\nfrom collections import OrderedDict\nfrom itertools import product\nfrom math import log2, ceil\n\nfrom numpy import asarray, squeeze, zeros\n\n\nclass DictSpace(spaces.Dict):\n    """"""\n    Wrapper for gym.spaces.Dict class. Adds support for .shape attribute.\n    Defines space as nested dictionary of simpler gym spaces.\n    """"""\n\n    def __init__(self, spaces_dict):\n        """"""\n\n        Args:\n            spaces_dict:    [nested] dictionary of core Gym spaces.\n        """"""\n        super(DictSpace, self).__init__(spaces_dict)\n        self.shape = self._get_shape()\n\n    def _get_shape(self):\n        return OrderedDict([(k, space.shape) for k, space in self.spaces.items()])\n\n\nclass ActionDictSpace(DictSpace):\n    """"""\n    Extension of OpenAI Gym DictSpace providing additional domain-specific functionality.\n    Action space for btgym environments as shallow dictionary of discrete or continuous spaces.\n    Defines several handy attributes and encoding conversion methods.\n    """"""\n\n    def __init__(self, assets, base_actions=None,):\n        """"""\n\n        Args:\n            base_actions:   None or iterable of base asset discrete actions;\n                            if no actions provided - continuous 1D base action space is set in [0,1] interval.\n            assets:         iterable of assets names\n        """"""\n        assert not isinstance(assets, str),\\\n            \'ActionDictSpace: expected `assets` be iterable, got <{}> type <{}>\'.format(assets, type(assets))\n        self.assets = tuple(sorted(assets))\n        if base_actions is not None:\n            # Discrete base actions provided, will use binary encoding for encode/decode methods\n            self.base_actions = tuple(base_actions)\n            self.base_actions_lookup_table = dict(list(enumerate(self.base_actions)))\n            self.base_space = spaces.Discrete\n            self.is_discrete = True\n            self.tensor_shape = (len(self.assets), len(self.base_actions))\n            self.lookup_table = self._make_lookup_table(\n                base_actions=list(self.base_actions_lookup_table.keys()),\n                num_assets=len(self.assets)\n            )\n            # Infer binary code length (depth):\n            self.cardinality = len(list(self.lookup_table.keys()))\n            self.encoded_depth = ceil(log2(self.cardinality))\n            self.one_hot_depth = self.cardinality\n            spaces_dict = {key: spaces.Discrete(self.tensor_shape[-1]) for key in self.assets}\n\n            self.encode_method = self._action_to_binary\n            self.decode_method = self._binary_to_action\n            self.one_hot_encode_method = self._to_one_hot\n            self.one_hot_decode_method = None\n\n        else:\n            # Using continuous base actions,\n            # encoding will be simply making 1D array out of shallow dictionary and back:\n            self.base_actions = None\n            self.base_actions_lookup_table = None\n            self.base_space = spaces.Box\n            self.is_discrete = False\n            self.tensor_shape = (len(self.assets), 1)\n            self.lookup_table = None\n            self.cardinality = None  # ~inf.\n            self.encoded_depth = self.tensor_shape[0]\n            self.one_hot_depth = self.tensor_shape[0]\n            spaces_dict = {\n                key: spaces.Box(low=0, high=1, shape=(self.tensor_shape[-1],), dtype=\'float32\') for key in self.assets\n            }\n\n            # For continuous space encoding is simple:\n            self.encode_method = self._action_to_vec\n            self.decode_method = self._vec_to_action\n            self.one_hot_encode_method = self._action_to_vec\n            self.one_hot_decode_method = None\n\n        super(ActionDictSpace, self).__init__(spaces_dict)\n\n    def get_initial_action(self):\n        """"""\n\n        Returns:\n            \'do nothing\' action as OrderedDict (for discrete spaces)\n            \'put all in cash\' action as OrderedDict (for continuous actions)\n        """"""\n        raise NotImplementedError\n\n    def encode(self, action):\n        """"""\n        Given action returns it\'s encoding.\n        Encoding method depends on type of base actions:\n        - if base actions defined are discrete (gym.spaces.Discrete), binary encoding is used;\n        - if base actions defined are continuous(gym.spaces.Box),\n          encoding is translating shallow dictionary to vector of same values and back\n\n        Args:\n            action:     action from this space (shallow dictionary)\n\n        Returns:\n                1D array of floats in [0, 1]\n        """"""\n        return self.encode_method(action)\n\n    def decode(self, code):\n        """"""\n        Given code returns action.\n        Encoding method depends on type of base actions:\n        - if base actions defined are discrete (gym.spaces.Discrete), binary encoding is used;\n        - if base actions defined are continuous(gym.spaces.Box),\n          encoding is translating shallow dictionary to vector of same values and back\n\n        Args:\n            code:     1D array of floats in [0, 1]\n\n        Returns:\n                action from this space (shallow dictionary)\n        """"""\n        return self.decode_method(code)\n\n    def one_hot_encode(self, action):\n        """"""\n        Given action returns it\'s encoding.\n        Encoding method depends on type of base actions:\n        - if base actions defined are discrete (gym.spaces.Discrete), one_hot encoding is used;\n        - if base actions defined are continuous(gym.spaces.Box),\n          encoding is translating shallow dictionary to vector of same values and back\n\n        Args:\n            action:     action from this space (shallow dictionary)\n\n        Returns:\n                1D array of floats in [0, 1]\n        """"""\n        return self.one_hot_encode_method(action)\n\n    def one_hot_decode(self, code):\n        raise NotImplementedError\n\n    def _to_one_hot(self, action):\n        cat = self._vec_to_cat(self._action_to_vec(action))\n        one_hot = zeros(self.one_hot_depth)\n        one_hot[cat] = 1\n        return squeeze(one_hot)\n\n    def _vec_to_one_hot(self, vec):\n        if self.cardinality is None:\n            return vec\n\n        else:\n            one_hot = zeros(self.one_hot_depth)\n            one_hot[self._vec_to_cat(vec)] = 1\n            return squeeze(one_hot)\n\n    @staticmethod\n    def _make_lookup_table(base_actions, num_assets):\n        """"""\n        Creates lookup table for set of environment actions for K assets\n        and N base actions as a cartesian product of K sets of N elements each.\n\n        Args:\n            base_actions:   iterable of base asset actions\n            num_assets:     int, number of assets\n\n        Returns:\n            lookup table as dictionary form {num_0: env_action_0, ...}\n        """"""\n        return dict(list(enumerate(product(list(base_actions), repeat=num_assets))))\n\n    def _action_to_binary(self, action):\n        """"""\n        Given action returns it binary encoding\n\n        Args:\n            action:     action from this space (shallow dictionary)\n\n        Returns:\n            1D numpy array of floats in [0, 1]\n        """"""\n        cat = self._vec_to_cat(self._action_to_vec(action))\n        bit_string = format(cat, \'b\').zfill(self.encoded_depth)\n        bit_array = asarray(list(bit_string), dtype=\'float\')\n        return bit_array\n\n    def _binary_to_action(self, binary_code):\n        """"""\n        Given binary action encoding, returns action\n\n        Args:\n            binary_code: 1D array of ints or floats in [0, 1]\n\n        Returns:\n            action from action space\n        """"""\n        assert len(binary_code.shape) <= 1, \\\n            \'Only 1D code vectors are supported, got array of shape: {}\'.format(binary_code.shape)\n        bit_string = \'\'\n        for bit in list(binary_code.astype(int)):\n            bit_string += str(bit)\n        cat = int(bit_string, 2)\n        return self._vec_to_action(self._cat_to_vec(cat))\n\n    def _action_to_vec(self, action):\n        """"""\n        Given action returns its vector encoding.\n\n        Args:\n            action:     action from this space (shallow dictionary)\n\n        Returns:\n            numpy array\n        """"""\n        assert self.contains(action), \'Action {} does not belongs to this space\'.format(action)\n\n        if self.is_discrete:\n            return asarray([action[key] for key in self.assets])\n\n        else:\n\n            return asarray([action[key] for key in self.assets])[..., 0]\n\n    def _vec_to_action(self, vector):\n        """"""\n        Given vector encoding of an action returns action from this space.\n\n        Args:\n            vector:     iterable of scalars\n\n        Returns:\n            action as shallow dictionary of scalars\n        """"""\n        assert len(vector) == len(self.assets), \\\n            \'Length of encoding and number of assets should match, got: {} / {}\'.format(len(vector), len(self.assets))\n        if self.cardinality is None:\n            action = OrderedDict([(asset, asarray([value])) for asset, value in zip(self.assets, vector)])\n\n        else:\n            action = OrderedDict([(asset, value) for asset, value in zip(self.assets, vector)])\n\n        assert self.contains(action), \'Vector {} can not be converted to action of this space\'.format(vector)\n        return action\n\n    def _vec_to_cat(self, action):\n        """"""\n        Given action vector returns it\'s position (categorical encoding).\n        Valid for dictionary of discrete base spaces only.\n\n        Args:\n            action:     environment action as tuple, list or array of base asset cations\n\n        Returns:\n            int, position in lookup table\n\n        Raises:\n            ValueError, if no matches found\n        """"""\n        assert self.lookup_table is not None, \'Lookup table not defined for base {}\'.format(self.base_space)\n\n        for key, value in self.lookup_table.items():\n            if list(value) == list(action):\n                return key\n        raise ValueError(\'Action vector {} is not in lookup table of this space.\'.format(action))\n\n    def _cat_to_vec(self, category):\n        """"""\n        Given integer as categorical encoding returns corresponding env. action vector.\n        Valid for dictionary of discrete base spaces only.\n\n        Args:\n            category:   int, encoding\n            table:      lookup table\n\n        Returns:\n            environment action as numpy array of base asset actions\n\n        Raises:\n            ValueError, if no matches found\n\n        """"""\n        assert self.lookup_table is not None, \'Lookup table not defined for base {}\'.format(self.base_space)\n        try:\n            return asarray(self.lookup_table[category])\n\n        except KeyError:\n            raise ValueError(\'Category {} does not match action space.\'.format(category))\n\n\nclass __DictSpace(Space):\n    """"""\n    DEPRECATED\n    Defines space as nested dictionary of simpler gym spaces.\n\n    """"""\n    def __init__(self, spaces_dict):\n        """"""\n\n        Args:\n            spaces_dict:    [nested] dictionary of core Gym spaces.\n        """"""\n        self._nested_map(self._make_assert_gym_space(), spaces_dict)\n        self.spaces = spaces_dict\n        self.shape = self._nested_shape()\n\n    @staticmethod\n    def _gym_spaces():\n        attr_names = [attr for attr in dir(spaces) if attr[0].isupper()]\n        return tuple([getattr(spaces, name) for name in attr_names])\n\n    @staticmethod\n    def _contains(space, sample):\n        return space.contains(sample)\n\n    @staticmethod\n    def _shape(space, *args):\n        return space.shape\n\n    @staticmethod\n    def _sample(space, *args):\n        return space.sample()\n\n    def _make_assert_gym_space(self):\n        gym_spaces = self._gym_spaces()\n\n        def assert_gym_space(space, *args):\n            try:\n                assert isinstance(space, gym_spaces)\n\n            except:\n                raise AssertionError(\'Space {} is not valid Gym space\'.format(type(space)))\n\n        return assert_gym_space\n\n    def _nested_contains(self, x):\n        try:\n            self._assert_structure(self.spaces, x)\n            return self._nested_map(self._contains, self.spaces, x)\n\n        except:\n            return False\n\n    def _nested_shape(self):\n        return self._nested_map(self._shape, self.spaces)\n\n    def _nested_sample(self):\n        return self._nested_map(self._sample, self.spaces)\n\n    def _assert_structure(self, s1, s2):\n        if isinstance(s1, dict) or isinstance(s2, dict):\n            try:\n                assert isinstance(s1, dict) and isinstance(s2, dict)\n\n            except:\n                raise AssertionError(\'Args are not of the same structure. Got arg1: {}, arg2: {}\'.\n                                     format(type(s1), type(s2)))\n            keys1 = set(s1.keys())\n            keys2 = set(s2.keys())\n            for key in keys1 | keys2:\n                try:\n                    assert key in keys1\n\n                except:\n                    raise AssertionError(\'Key <{}> not present in arg1\'.format(key))\n\n                try:\n                    assert key in keys2\n\n                except:\n                    raise AssertionError(\'Key <{}> not present in arg2\'.format(key))\n\n                self._assert_structure(s1[key], s2[key])\n\n    def _nested_map(self, func, struct, *arg):\n        if not callable(func):\n            raise TypeError(\'`func` arg. must be callable.\')\n\n        if len(arg) == 0:\n            struct2 = struct\n\n        else:\n            struct2 = arg[0]\n\n        if isinstance(struct, dict):\n            mapped = {key: self._nested_map(func, struct[key], struct2[key]) for key in struct.keys()}\n\n        else:\n            mapped = func(struct, struct2)\n\n        return mapped\n\n    def sample(self):\n        """"""\n        Uniformly randomly sample a random element of this space.\n\n        Returns:\n            dictionary of samples\n        """"""\n        return self._nested_sample()\n\n    def contains(self, x):\n        """"""\n        Return boolean specifying if x is a valid\n        member of this space\n        """"""\n        return self._nested_contains(x)\n\n    def to_jsonable(self, sample_n):\n        """"""Convert a batch of samples from this space to a JSONable data type.""""""\n        # By default, assume identity is JSONable\n        #return sample_n\n        raise NotImplementedError\n\n    def from_jsonable(self, sample_n):\n        """"""Convert a JSONable data type to a batch of samples from this space.""""""\n        # By default, assume identity is JSONable\n        #return sample_n\n        raise NotImplementedError\n\n\n\n\n'"
btgym/algorithms/__init__.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom btgym.algorithms.runner.threadrunner import RunnerThread\nfrom .aac import BaseAAC, Unreal, A3C, PPO\nfrom .envs import AtariRescale42x42\nfrom btgym.algorithms.launcher.base import Launcher\nfrom .policy import BaseAacPolicy, Aac1dPolicy, StackedLstmPolicy, AacStackedRL2Policy\nfrom .worker import Worker\n\n'"
btgym/algorithms/aac.py,74,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom __future__ import print_function\n\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom logbook import Logger, StreamHandler\n\nfrom btgym.algorithms.memory import Memory\nfrom btgym.algorithms.rollout import make_data_getter\nfrom btgym.algorithms.runner import BaseEnvRunnerFn, RunnerThread\nfrom btgym.algorithms.math_utils import log_uniform\nfrom btgym.algorithms.nn.losses import value_fn_loss_def, rp_loss_def, pc_loss_def, aac_loss_def, ppo_loss_def\nfrom btgym.algorithms.utils import feed_dict_rnn_context, feed_dict_from_nested, batch_stack\nfrom btgym.spaces import DictSpace as BaseObSpace\nfrom btgym.spaces import ActionDictSpace as BaseAcSpace\n\n\nclass BaseAAC(object):\n    """"""\n    Base Asynchronous Advantage Actor Critic algorithm framework class with auxiliary control tasks and\n    option to run several instances of environment for every worker in vectorized fashion, PAAC-like.\n    Can be configured to run with different losses and policies.\n\n    Auxiliary tasks implementation borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:\n    https://miyosuda.github.io/\n    https://github.com/miyosuda/unreal\n\n    Original A3C code comes from OpenAI repository under MIT licence:\n    https://github.com/openai/universe-starter-agent\n\n    Papers:\n    https://arxiv.org/abs/1602.01783\n    https://arxiv.org/abs/1611.05397\n    """"""\n    def __init__(self,\n                 env,\n                 task,\n                 policy_config,\n                 log_level,\n                 name=\'AAC\',\n                 on_policy_loss=aac_loss_def,\n                 off_policy_loss=aac_loss_def,\n                 vr_loss=value_fn_loss_def,\n                 rp_loss=rp_loss_def,\n                 pc_loss=pc_loss_def,\n                 runner_config=None,\n                 runner_fn_ref=BaseEnvRunnerFn,\n                 cluster_spec=None,\n                 random_seed=None,\n                 model_gamma=0.99,  # decay\n                 model_gae_lambda=1.00,  # GAE lambda\n                 model_beta=0.01,  # entropy regularizer\n                 opt_max_env_steps=10 ** 7,\n                 opt_decay_steps=None,\n                 opt_end_learn_rate=None,\n                 opt_learn_rate=1e-4,\n                 opt_decay=0.99,\n                 opt_momentum=0.0,\n                 opt_epsilon=1e-8,\n                 rollout_length=20,\n                 time_flat=False,\n                 episode_train_test_cycle=(1,0),\n                 episode_summary_freq=2,  # every i`th environment episode\n                 env_render_freq=10,  # every i`th environment episode\n                 model_summary_freq=100,  # every i`th algorithm iteration\n                 test_mode=False,  # gym_atari test mode\n                 replay_memory_size=2000,\n                 replay_batch_size=None,\n                 replay_rollout_length=None,\n                 use_off_policy_aac=False,\n                 use_reward_prediction=False,\n                 use_pixel_control=False,\n                 use_value_replay=False,\n                 rp_lambda=1.0,  # aux tasks loss weights\n                 pc_lambda=1.0,\n                 vr_lambda=1.0,\n                 off_aac_lambda=1,\n                 gamma_pc=0.9,  # pixel change gamma-decay - not used\n                 rp_reward_threshold=0.1,  # r.prediction: abs.rewards values bigger than this are considered non-zero\n                 rp_sequence_size=3,  # r.prediction sampling\n                 clip_epsilon=0.1,\n                 num_epochs=1,\n                 pi_prime_update_period=1,\n                 global_step_op=None,\n                 global_episode_op=None,\n                 inc_episode_op=None,\n                 _use_global_network=True,\n                 _use_target_policy=False,  # target policy tracking behavioral one with delay\n                 _use_local_memory=False,  # in-place memory\n                 aux_render_modes=None,\n                 **kwargs):\n        """"""\n\n        Args:\n            env:                    environment instance or list of instances\n            task:                   int, parent worker id\n            policy_config:          policy estimator class and configuration dictionary\n            log_level:              int, logbook.level\n            name:                   str, class-wide name-scope\n            on_policy_loss:         callable returning tensor holding on_policy training loss graph and summaries\n            off_policy_loss:        callable returning tensor holding off_policy training loss graph and summaries\n            vr_loss:                callable returning tensor holding value replay loss graph and summaries\n            rp_loss:                callable returning tensor holding reward prediction loss graph and summaries\n            pc_loss:                callable returning tensor holding pixel_control loss graph and summaries\n            runner_config:          runner class and configuration dictionary,\n            runner_fn_ref:          callable defining environment runner execution logic,\n                                    valid only if no \'runner_config\' arg is provided\n            cluster_spec:           dict, full training cluster spec (may be used by meta-trainer)\n            random_seed:            int or None\n            model_gamma:            scalar, gamma discount factor\n            model_gae_lambda:       scalar, GAE lambda\n            model_beta:             entropy regularization beta, scalar or [high_bound, low_bound] for log_uniform.\n            opt_max_env_steps:      int, total number of environment steps to run training on.\n            opt_decay_steps:        int, learn ratio decay steps, in number of environment steps.\n            opt_end_learn_rate:     scalar, final learn rate\n            opt_learn_rate:         start learn rate, scalar or [high_bound, low_bound] for log_uniform distr.\n            opt_decay:              scalar, optimizer decay, if apll.\n            opt_momentum:           scalar, optimizer momentum, if apll.\n            opt_epsilon:            scalar, optimizer epsilon\n            rollout_length:         int, on-policy rollout length\n            time_flat:              bool, flatten rnn time-steps in rollouts of size 1 - see `Notes` below\n            episode_train_test_cycle:   tuple or list as (train_number, test_number), def=(1,0): enables infinite\n                                        loop such as: run `train_number` of train data episodes,\n                                        than `test_number` of test data episodes, repeat. Should be consistent\n                                        with provided dataset parameters (test data should exist if `test_number > 0`)\n            episode_summary_freq:   int, write episode summary for every i\'th episode\n            env_render_freq:        int, write environment rendering summary for every i\'th train step\n            model_summary_freq:     int, write model summary for every i\'th train step\n            test_mode:              bool, True: Atari, False: BTGym\n            replay_memory_size:     int, in number of experiences\n            replay_batch_size:      int, mini-batch size for off-policy training, def = 1\n            replay_rollout_length:  int off-policy rollout length by def. equals on_policy_rollout_length\n            use_off_policy_aac:     bool, use full AAC off-policy loss instead of Value-replay\n            use_reward_prediction:  bool, use aux. off-policy reward prediction task\n            use_pixel_control:      bool, use aux. off-policy pixel control task\n            use_value_replay:       bool, use aux. off-policy value replay task (not used if use_off_policy_aac=True)\n            rp_lambda:              reward prediction loss weight, scalar or [high, low] for log_uniform distr.\n            pc_lambda:              pixel control loss weight, scalar or [high, low] for log_uniform distr.\n            vr_lambda:              value replay loss weight, scalar or [high, low] for log_uniform distr.\n            off_aac_lambda:         off-policy AAC loss weight, scalar or [high, low] for log_uniform distr.\n            gamma_pc:               NOT USED\n            rp_reward_threshold:    scalar, reward prediction classification threshold, above which reward is \'non-zero\'\n            rp_sequence_size:       int, reward prediction sample size, in number of experiences\n            clip_epsilon:           scalar, PPO: surrogate L^clip epsilon\n            num_epochs:             int, num. of SGD runs for every train step, val. > 1 should be used with caution.\n            pi_prime_update_period: int, PPO: pi to pi_old update period in number of train steps, def: 1\n            global_step_op:         external tf.variable holding global step counter\n            global_episode_op:      external tf.variable holding global episode counter\n            inc_episode_op:         external tf.op incrementing global step counter\n            _use_global_network:    bool, either to use parameter server policy instance\n            _use_target_policy:     bool, PPO: use target policy (aka pi_old), delayed by `pi_prime_update_period` delay\n            _use_local_memory:      bool: use in-process replay memory instead of runner-based one\n            aux_render_modes:      additional visualisations to include in per-episode rendering summary\n\n        Note:\n            - On `time_flat` arg:\n\n                Note that previous explanation of this arg was erroneous;\n                Time_flat=False:\n                    Implements Truncated BPTT with backpropagation depth equal to rollout length.\n                    In this case we need to feed initial rnn_states for rollouts only.\n                    Thus, when time_flat=False, we unroll RNN in specified number of time-steps for every rollout.\n\n                Time_flat=True:\n                Basicaly forces TBPTT with rollout depth = 1.\n                Not recommended to use as it prevents policy from learning long-range dependencies.\n        """"""\n        # Logging:\n        self.log_level = log_level\n        self.name = name\n        self.task = task\n        self.cluster_spec = cluster_spec\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'{}_{}\'.format(self.name, self.task), level=self.log_level)\n\n        # Get direct traceback:\n        try:\n            self.random_seed = random_seed\n            if self.random_seed is not None:\n                np.random.seed(self.random_seed)\n                tf.set_random_seed(self.random_seed)\n            self.log.debug(\'rnd_seed:{}, log_u_sample_(0,1]x5: {}\'.\n                           format(random_seed, log_uniform([1e-10,1], 5)))\n\n            if kwargs != {}:\n                self.log.warning(\'Unexpected kwargs found: {}, ignored.\'.format(kwargs))\n\n            self.env_list = env\n            try:\n                assert isinstance(self.env_list, list)\n\n            except AssertionError:\n                self.env_list = [env]\n\n            self.ref_env = self.env_list[0]  # reference instance to get obs shapes etc.\n\n            try:\n                assert isinstance(self.ref_env.observation_space, BaseObSpace)\n\n            except AssertionError:\n                self.log.exception(\n                    \'expected environment observation space of type {}, got: {}\'.\\\n                    format(BaseObSpace, type(self.ref_env.observation_space))\n                )\n                raise AssertionError\n\n            try:\n                assert isinstance(self.ref_env.action_space, BaseAcSpace)\n\n            except AssertionError:\n                self.log.exception(\n                    \'expected environment observation space of type {}, got: {}\'.\\\n                    format(BaseAcSpace, type(self.ref_env.action_space))\n                )\n                raise AssertionError\n\n            self.policy_class = policy_config[\'class_ref\']\n            self.policy_kwargs = policy_config[\'kwargs\']\n\n            # Losses:\n            self.on_policy_loss = on_policy_loss\n            self.off_policy_loss = off_policy_loss\n            self.vr_loss = vr_loss\n            self.rp_loss = rp_loss\n            self.pc_loss = pc_loss\n\n            if runner_config is None:\n                # Runner will be async. ThreadRunner class with runner_fn logic:\n                self.runner_config = {\n                    \'class_ref\': RunnerThread,\n                    \'kwargs\': {\n                        \'runner_fn_ref\': runner_fn_ref,\n                    }\n                }\n            else:\n                self.runner_config = runner_config\n\n            # AAC specific:\n            self.model_gamma = model_gamma  # decay\n            self.model_gae_lambda = model_gae_lambda  # general advantage estimator lambda\n            self.model_beta = log_uniform(model_beta, 1)  # entropy reg.\n\n            self.time_flat = time_flat\n\n            # Optimizer\n            self.opt_max_env_steps = opt_max_env_steps\n            self.opt_learn_rate = log_uniform(opt_learn_rate, 1)\n\n            if opt_end_learn_rate is None:\n                self.opt_end_learn_rate = self.opt_learn_rate\n            else:\n                self.opt_end_learn_rate = opt_end_learn_rate\n\n            if opt_decay_steps is None:\n                self.opt_decay_steps = self.opt_max_env_steps\n            else:\n                self.opt_decay_steps = opt_decay_steps\n\n            self.opt_decay = opt_decay\n            self.opt_epsilon = opt_epsilon\n            self.opt_momentum = opt_momentum\n            self.rollout_length = rollout_length\n\n            # Data sampling control:\n            self.num_train_episodes = episode_train_test_cycle[0]\n            self.num_test_episodes = episode_train_test_cycle[-1]\n            try:\n                assert self.num_train_episodes + self.num_test_episodes > 0 and \\\n                    self.num_train_episodes >= 0 and \\\n                    self.num_test_episodes >= 0\n\n            except AssertionError:\n                self.log.exception(\n                    \'Train/test episode cycle values could not be both zeroes or negative, got: train={}, test={}\'.\\\n                    format(self.num_train_episodes, self.num_test_episodes)\n                )\n                raise AssertionError\n\n            self.current_train_episode = 0\n            self.current_test_episode = 0\n\n            # Summaries :\n            self.episode_summary_freq = episode_summary_freq\n            self.env_render_freq = env_render_freq\n            self.model_summary_freq = model_summary_freq\n\n            # If True - use ATARI gym env.:\n            self.test_mode = test_mode\n\n            # UNREAL/AUX and Off-policy specific:\n            self.off_aac_lambda = log_uniform(off_aac_lambda, 1)\n            self.rp_lambda = log_uniform(rp_lambda, 1)\n            self.pc_lambda = log_uniform(pc_lambda, 1)\n            self.vr_lambda = log_uniform(vr_lambda, 1)\n            self.gamma_pc = gamma_pc\n            self.replay_memory_size = replay_memory_size\n\n            if replay_rollout_length is not None:\n                self.replay_rollout_length = replay_rollout_length\n\n            else:\n                self.replay_rollout_length = rollout_length # by default off-rollout equals on-policy one\n\n            self.rp_sequence_size = rp_sequence_size\n            self.rp_reward_threshold = rp_reward_threshold\n\n            if replay_batch_size is not None:\n                self.replay_batch_size = replay_batch_size\n\n            else:\n                self.replay_batch_size = len(self.env_list)  # by default off-batch equals on-policy one\n\n            # PPO related:\n            self.clip_epsilon = clip_epsilon\n            self.num_epochs = num_epochs\n            self.pi_prime_update_period = pi_prime_update_period\n\n            # On/off switchers for off-policy training and auxiliary tasks:\n            self.use_off_policy_aac = use_off_policy_aac\n            self.use_reward_prediction = use_reward_prediction\n            self.use_pixel_control = use_pixel_control\n            if use_off_policy_aac:\n                self.use_value_replay = False  # v-replay is redundant in this case\n            else:\n                self.use_value_replay = use_value_replay\n\n            self.use_any_aux_tasks = use_value_replay or use_pixel_control or use_reward_prediction\n            self.use_local_memory = _use_local_memory\n            self.use_memory = (self.use_any_aux_tasks or self.use_off_policy_aac) and not self.use_local_memory\n\n            self.use_target_policy = _use_target_policy\n            self.use_global_network = _use_global_network\n\n            self.log.notice(\'learn_rate: {:1.6f}, entropy_beta: {:1.6f}\'.format(self.opt_learn_rate, self.model_beta))\n\n            if self.use_off_policy_aac:\n                self.log.notice(\'off_aac_lambda: {:1.6f}\'.format(self.off_aac_lambda,))\n\n            if self.use_any_aux_tasks:\n                self.log.notice(\'vr_lambda: {:1.6f}, pc_lambda: {:1.6f}, rp_lambda: {:1.6f}\'.\n                              format(self.vr_lambda, self.pc_lambda, self.rp_lambda))\n\n            if aux_render_modes is not None:\n                self.aux_render_modes = list(aux_render_modes)\n            else:\n                self.aux_render_modes = []\n\n            #self.log.notice(\n            #    \'AAC_{}: max_steps: {}, decay_steps: {}, end_rate: {:1.6f},\'.\n            #        format(self.task, self.opt_max_env_steps, self.opt_decay_steps, self.opt_end_learn_rate))\n\n            self.worker_device = ""/job:worker/task:{}/cpu:0"".format(task)\n\n            # Update policy configuration\n            self.policy_kwargs.update(\n                {\n                    \'ob_space\': self.ref_env.observation_space,\n                    \'ac_space\': self.ref_env.action_space,\n                    \'rp_sequence_size\': self.rp_sequence_size,\n                    \'aux_estimate\': self.use_any_aux_tasks,\n                    \'static_rnn\': self.time_flat,\n                    \'task\': self.task,\n                    \'cluster_spec\': self.cluster_spec\n                }\n            )\n\n            if global_step_op is not None:\n                self.global_step = global_step_op\n\n            if global_episode_op is not None:\n                self.global_episode = global_episode_op\n\n            if inc_episode_op is not None:\n                self.inc_episode = inc_episode_op\n\n            # Should be defined later:\n            self.sync = None\n            self.sync_pi = None\n            self.sync_pi_prime = None\n            self.grads = None\n            self.summary_writer = None\n            self.local_steps = 0\n\n            # Start building graphs:\n            self.log.debug(\'started building graphs...\')\n            if self.use_global_network:\n                # PS:\n                with tf.device(tf.train.replica_device_setter(1, worker_device=self.worker_device)):\n                    self.network = pi_global = self._make_policy(\'global\')\n                    if self.use_target_policy:\n                        self.network_prime = self._make_policy(\'global_prime\')\n                    else:\n                        self.network_prime = self._make_dummy_policy()\n            else:\n                self.network = pi_global = self._make_dummy_policy()\n                self.network_prime = self._make_dummy_policy()\n\n            # Worker:\n            with tf.device(self.worker_device):\n                with tf.variable_scope(self.name):\n                    self.local_network = pi = self._make_policy(\'local\')\n\n                    if self.use_target_policy:\n                        self.local_network_prime = pi_prime = self._make_policy(\'local_prime\')\n\n                    else:\n                        self.local_network_prime = pi_prime = self._make_dummy_policy()\n\n                    self.worker_device_callback_0()  # if need more networks etc.\n\n                    # Meant for Batch-norm layers:\n                    pi.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\'.*local.*\')\n\n                    # Just in case:\n                    self.dummy_pi = self._make_dummy_policy()\n\n                    self.log.debug(\'local_network_upd_ops_collection:\\n{}\'.format(pi.update_ops))\n                    self.log.debug(\'\\nlocal_network_var_list_to_save:\')\n                    for v in pi.var_list:\n                        self.log.debug(\'{}: {}\'.format(v.name, v.get_shape()))\n\n                    #  Learning rate annealing:\n                    self.learn_rate_decayed = tf.train.polynomial_decay(\n                        self.opt_learn_rate,\n                        self.global_step + 1,\n                        self.opt_decay_steps,\n                        self.opt_end_learn_rate,\n                        power=1,\n                        cycle=False,\n                    )\n                    # Freeze training if train_phase is False:\n                    self.train_learn_rate = self.learn_rate_decayed * tf.cast(pi.train_phase, tf.float64)\n                    self.log.debug(\'learn rate ok\')\n\n                    # Define loss and related summaries\n                    self.loss, self.loss_summaries = self._make_loss(pi=pi, pi_prime=pi_prime)\n\n                    if self.use_global_network:\n                        # Define train, sync ops:\n                        self.train_op = self._make_train_op(pi=pi, pi_prime=pi_prime, pi_global=pi_global)\n\n                    else:\n                        self.train_op = []\n\n                    # Model stat. summary, episode summary:\n                    self.model_summary_op, self.ep_summary = self._combine_summaries(\n                        policy=pi,\n                        model_summaries=self.loss_summaries\n                    )\n\n                    # Make thread-runner processes:\n                    self.runners = self._make_runners(policy=pi)\n\n                    # Make rollouts provider[s] for async runners:\n                    if self.runner_config[\'class_ref\'] == RunnerThread:\n                        # Make rollouts provider[s] for async threaded runners:\n                        self.data_getter = [make_data_getter(runner.queue) for runner in self.runners]\n                    else:\n                        # Else assume runner is in-thread synchro type and  supports .get data() method:\n                        self.data_getter = [runner.get_data for runner in self.runners]\n\n                    self.log.debug(\'trainer.__init__() ok\')\n\n        except:\n            msg = \'Base class __init__() exception occurred.\' +\\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def worker_device_callback_0(self):\n        pass\n\n    def _make_loss(self, **kwargs):\n        return self._make_base_loss(name=self.name, verbose=True, **kwargs)\n\n    def _make_base_loss(self, pi, pi_prime, name=\'base\', verbose=True):\n        """"""\n        Defines base AAC on- and off-policy loss, auxiliary VR, RP and PC losses, placeholders and summaries.\n\n        Args:\n            pi:                 policy network obj.\n            pi_prime:           optional policy network obj.\n            name:               str, name scope\n            verbose:            summary level\n\n        Returns:\n            tensor holding estimated loss graph\n            list of related summaries\n        """"""\n        with tf.name_scope(name):\n            # On-policy AAC loss definition:\n            pi.on_pi_act_target = tf.placeholder(\n                tf.float32, [None, self.ref_env.action_space.one_hot_depth], name=""on_policy_action_pl""\n            )\n            pi.on_pi_adv_target = tf.placeholder(tf.float32, [None], name=""on_policy_advantage_pl"")\n            pi.on_pi_r_target = tf.placeholder(tf.float32, [None], name=""on_policy_return_pl"")\n\n            clip_epsilon = tf.cast(self.clip_epsilon * self.learn_rate_decayed / self.opt_learn_rate, tf.float32)\n\n            on_pi_loss, on_pi_summaries = self.on_policy_loss(\n                act_target=pi.on_pi_act_target,\n                adv_target=pi.on_pi_adv_target,\n                r_target=pi.on_pi_r_target,\n                pi_logits=pi.on_logits,\n                pi_vf=pi.on_vf,\n                pi_prime_logits=pi_prime.on_logits,\n                entropy_beta=self.model_beta,\n                epsilon=clip_epsilon,\n                name=\'on_policy\',\n                verbose=verbose\n            )\n            # Start accumulating total loss:\n            loss = on_pi_loss\n            model_summaries = on_pi_summaries\n\n            # Off-policy losses:\n            pi.off_pi_act_target = tf.placeholder(\n                tf.float32, [None, self.ref_env.action_space.one_hot_depth], name=""off_policy_action_pl"")\n            pi.off_pi_adv_target = tf.placeholder(tf.float32, [None], name=""off_policy_advantage_pl"")\n            pi.off_pi_r_target = tf.placeholder(tf.float32, [None], name=""off_policy_return_pl"")\n\n            if self.use_off_policy_aac:\n                # Off-policy AAC loss graph mirrors on-policy:\n                off_pi_loss, off_pi_summaries = self.off_policy_loss(\n                    act_target=pi.off_pi_act_target,\n                    adv_target=pi.off_pi_adv_target,\n                    r_target=pi.off_pi_r_target,\n                    pi_logits=pi.off_logits,\n                    pi_vf=pi.off_vf,\n                    pi_prime_logits=pi_prime.off_logits,\n                    entropy_beta=self.model_beta,\n                    epsilon=clip_epsilon,\n                    name=\'off_policy\',\n                    verbose=False\n                )\n                loss = loss + self.off_aac_lambda * off_pi_loss\n                model_summaries += off_pi_summaries\n\n            if self.use_pixel_control:\n                # Pixel control loss:\n                pi.pc_action = tf.placeholder(tf.float32, [None, self.ref_env.action_space.tensor_shape[0]], name=""pc_action"")\n                pi.pc_target = tf.placeholder(tf.float32, [None, None, None], name=""pc_target"")\n\n                pc_loss, pc_summaries = self.pc_loss(\n                    actions=pi.pc_action,\n                    targets=pi.pc_target,\n                    pi_pc_q=pi.pc_q,\n                    name=\'off_policy\',\n                    verbose=verbose\n                )\n                loss = loss + self.pc_lambda * pc_loss\n                # Add specific summary:\n                model_summaries += pc_summaries\n\n            if self.use_value_replay:\n                # Value function replay loss:\n                pi.vr_target = tf.placeholder(tf.float32, [None], name=""vr_target"")\n                vr_loss, vr_summaries = self.vr_loss(\n                    r_target=pi.vr_target,\n                    pi_vf=pi.vr_value,\n                    name=\'off_policy\',\n                    verbose=verbose\n                )\n                loss = loss + self.vr_lambda * vr_loss\n                model_summaries += vr_summaries\n\n            if self.use_reward_prediction:\n                # Reward prediction loss:\n                pi.rp_target = tf.placeholder(tf.float32, [None, 3], name=""rp_target"")\n\n                rp_loss, rp_summaries = self.rp_loss(\n                    rp_targets=pi.rp_target,\n                    pi_rp_logits=pi.rp_logits,\n                    name=\'off_policy\',\n                    verbose=verbose\n                )\n                loss = loss + self.rp_lambda * rp_loss\n                model_summaries += rp_summaries\n\n        return loss, model_summaries\n\n    def _make_train_op(self, pi, pi_prime, pi_global):\n        """"""\n        Defines training op graph and supplementary sync operations.\n\n        Args:\n            pi:                 policy network obj.\n            pi_prime:           optional policy network obj.\n            pi_global:          shared policy network obj. hosted by parameter server\n\n        Returns:\n            tensor holding training op graph;\n        """"""\n\n        # Each worker gets a different set of adam optimizer parameters:\n        self.optimizer = tf.train.AdamOptimizer(self.train_learn_rate, epsilon=1e-5)\n\n        # self.optimizer = tf.train.RMSPropOptimizer(\n        #    learning_rate=train_learn_rate,\n        #    decay=self.opt_decay,\n        #    momentum=self.opt_momentum,\n        #    epsilon=self.opt_epsilon,\n        # )\n\n        # Clipped gradients:\n        self.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.loss, pi.var_list),\n            40.0\n        )\n        self.grads_global_norm = tf.global_norm(self.grads)\n        # Copy weights from the parameter server to the local model\n        self.sync = self.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_global.var_list)]\n        )\n        if self.use_target_policy:\n            # Copy weights from new policy model to target one:\n            self.sync_pi_prime = tf.group(\n                *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi.var_list)]\n            )\n        grads_and_vars = list(zip(self.grads, pi_global.var_list))\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(pi.on_state_in.keys())\n\n        # Handles case when \'external\' is nested or flat dict:\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        if isinstance(pi.on_state_in[\'external\'], dict):\n            stream = pi.on_state_in[\'external\'][list(pi.on_state_in[\'external\'].keys())[0]]\n        else:\n            stream = pi.on_state_in[\'external\']\n        self.inc_step = self.global_step.assign_add(tf.shape(stream)[0])\n\n        train_op = self.optimizer.apply_gradients(grads_and_vars)\n        self.log.debug(\'train_op defined\')\n        return train_op\n\n    def _combine_summaries(self, policy=None, model_summaries=None):\n        """"""\n        Defines model-wide and episode-related summaries\n\n        Returns:\n            model_summary op\n            episode_summary op\n        """"""\n        if model_summaries is not None:\n            if self.use_global_network:\n                # Model-wide statistics:\n                with tf.name_scope(\'model\'):\n                    model_summaries += [\n                        tf.summary.scalar(""grad_global_norm"", self.grads_global_norm),\n                        # TODO: add gradient variance summary\n                        #tf.summary.scalar(""learn_rate"", self.train_learn_rate),\n                        tf.summary.scalar(""learn_rate"", self.learn_rate_decayed),  # cause actual rate is a jaggy due to test freezes\n                        tf.summary.scalar(""total_loss"", self.loss),\n                        # tf.summary.scalar(\'roll_reward\', tf.reduce_mean(self.local_network.on_last_reward_in)),\n                        # tf.summary.scalar(\'roll_advantage\', tf.reduce_mean(self.local_network.on_pi_adv_target)),\n                    ]\n                    if policy is not None:\n                        model_summaries += [tf.summary.scalar(""var_global_norm"", tf.global_norm(policy.var_list))]\n        else:\n            model_summaries = []\n        # Model stat. summary:\n        model_summary = tf.summary.merge(model_summaries, name=\'model_summary\')\n\n        # Episode-related summaries:\n        ep_summary = dict(\n            # Summary placeholders\n            render_atari=tf.placeholder(tf.uint8, [None, None, None, 1]),\n            total_r=tf.placeholder(tf.float32, ),\n            cpu_time=tf.placeholder(tf.float32, ),\n            final_value=tf.placeholder(tf.float32, ),\n            steps=tf.placeholder(tf.int32, ),\n        )\n        if self.test_mode:\n            # For Atari:\n            ep_summary[\'render_op\'] = tf.summary.image(""model/state"", ep_summary[\'render_atari\'])\n\n        else:\n            # BTGym rendering:\n            ep_summary.update(\n                {\n                    mode: tf.placeholder(tf.uint8, [None, None, None, None], name=mode + \'_pl\')\n                    for mode in self.env_list[0].render_modes + self.aux_render_modes\n                }\n            )\n            ep_summary[\'render_op\'] = tf.summary.merge(\n                [tf.summary.image(mode, ep_summary[mode])\n                 for mode in self.env_list[0].render_modes + self.aux_render_modes]\n            )\n        # Episode stat. summary:\n        ep_summary[\'btgym_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode_train/total_reward\', ep_summary[\'total_r\']),\n                tf.summary.scalar(\'episode_train/cpu_time_sec\', ep_summary[\'cpu_time\']),\n                tf.summary.scalar(\'episode_train/final_value\', ep_summary[\'final_value\']),\n                tf.summary.scalar(\'episode_train/env_steps\', ep_summary[\'steps\'])\n            ],\n            name=\'episode_train_btgym\'\n        )\n        # Test episode stat. summary:\n        ep_summary[\'test_btgym_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode_test/total_reward\', ep_summary[\'total_r\']),\n                tf.summary.scalar(\'episode_test/final_value\', ep_summary[\'final_value\']),\n                tf.summary.scalar(\'episode_test/env_steps\', ep_summary[\'steps\'])\n            ],\n            name=\'episode_test_btgym\'\n        )\n        ep_summary[\'atari_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode/total_reward\', ep_summary[\'total_r\']),\n                tf.summary.scalar(\'episode/steps\', ep_summary[\'steps\'])\n            ],\n            name=\'episode_atari\'\n        )\n        self.log.debug(\'model-wide and episode summaries ok.\')\n        return model_summary, ep_summary\n\n    def _make_runners(self, policy):\n        """"""\n        Defines thread-runners processes instances.\n\n        Args:\n            policy:     policy for runner to execute\n\n        Returns:\n            list of runners\n        """"""\n        # Replay memory_config:\n        if self.use_memory:\n            memory_config = dict(\n                class_ref=Memory,\n                kwargs=dict(\n                    history_size=self.replay_memory_size,\n                    max_sample_size=self.replay_rollout_length,\n                    priority_sample_size=self.rp_sequence_size,\n                    reward_threshold=self.rp_reward_threshold,\n                    use_priority_sampling=self.use_reward_prediction,\n                    task=self.task,\n                    log_level=self.log_level,\n                )\n            )\n        else:\n            memory_config = None\n\n        # Make runners:\n        # `rollout_length` represents the number of ""local steps"":  the number of time steps\n        # we run the policy before we get full rollout, run train step and update the parameters.\n        runners = []\n        task = 0  # Runners will have [worker_task][env_count] id\'s\n        for env in self.env_list:\n            kwargs=dict(\n                env=env,\n                policy=policy,\n                task=self.task + task,\n                rollout_length=self.rollout_length,  # ~20\n                episode_summary_freq=self.episode_summary_freq,\n                env_render_freq=self.env_render_freq,\n                test=self.test_mode,\n                ep_summary=self.ep_summary,\n                memory_config=memory_config,\n                log_level=self.log_level,\n                global_step_op=self.global_step,\n                aux_render_modes=self.aux_render_modes\n            )\n            kwargs.update(self.runner_config[\'kwargs\'])\n            # New runner instance:\n            runners.append(self.runner_config[\'class_ref\'](**kwargs))\n            task += 0.01\n        self.log.debug(\'runners ok.\')\n        return runners\n\n    def _make_step_counters(self):\n        """"""\n        Defines operations for global step and global episode;\n\n        Returns:\n            None, sets attrs.\n        """"""\n        self.global_step = tf.get_variable(\n            ""global_step"",\n            [],\n            tf.int32,\n            initializer=tf.constant_initializer(\n                0,\n                dtype=tf.int32\n            ),\n            trainable=False\n        )\n        tf.add_to_collection(tf.GraphKeys.GLOBAL_STEP, self.global_step)\n        self.reset_global_step = self.global_step.assign(0)\n\n        self.global_episode = tf.get_variable(\n            ""global_episode"",\n            [],\n            tf.int32,\n            initializer=tf.constant_initializer(\n                0,\n                dtype=tf.int32\n            ),\n            trainable=False\n        )\n        # Increment episode count:\n        self.inc_episode = self.global_episode.assign_add(1)\n\n    def _make_policy(self, scope):\n        """"""\n        Configures and instantiates policy network and ops.\n\n        Note:\n            `global` name_scope networks should be defined first.\n\n        Args:\n            scope:  name scope\n\n        Returns:\n            policy instance\n        """"""\n        with tf.variable_scope(scope):\n            # Make policy instance:\n            network = self.policy_class(**self.policy_kwargs)\n            if \'global\' not in scope:\n                try:\n                    # For locals those should be already defined:\n                    assert hasattr(self, \'global_step\') and \\\n                           hasattr(self, \'global_episode\') and \\\n                           hasattr(self, \'inc_episode\')\n                    # Add attrs to local:\n                    network.global_step = self.global_step\n                    network.global_episode = self.global_episode\n                    network.inc_episode= self.inc_episode\n                    # Override with aac method:\n                    network.get_sample_config = self.get_sample_config\n\n                except AssertionError:\n                    self.log.exception(\n                        \'`global` name_scope network[s] should be defined before any `local` one[s].\'.\n                        format(self.task)\n                    )\n                    raise RuntimeError\n            else:\n                # Set counters:\n                self._make_step_counters()\n\n        return network\n\n    def _make_dummy_policy(self):\n        class _Dummy(object):\n            """"""\n            Policy plug when target network is not used.\n            """"""\n            def __init__(self):\n                self.on_state_in = None\n                self.off_state_in = None\n                self.on_lstm_state_pl_flatten = None\n                self.off_lstm_state_pl_flatten = None\n                self.on_a_r_in = None\n                self.off_a_r_in = None\n                self.on_logits = None\n                self.off_logits = None\n                self.on_vf = None\n                self.off_vf = None\n                self.on_batch_size = None\n                self.on_time_length = None\n                self.off_batch_size = None\n                self.off_time_length = None\n        return _Dummy()\n\n    def get_data(self, **kwargs):\n        """"""\n        Collect rollouts from every environment.\n\n        Returns:\n            dictionary of lists of data streams collected from every runner\n        """"""\n        data_streams = [get_it(**kwargs) for get_it in self.data_getter]\n\n        return {key: [stream[key] for stream in data_streams] for key in data_streams[0].keys()}\n\n    def get_sample_config(self, _new_trial=True, **kwargs):\n        """"""\n        WARNING: _new_trial=True is quick fix, TODO: fix it properly!\n        Returns environment configuration parameters for next episode to sample.\n        By default is simple stateful iterator,\n        works correctly with `DTGymDataset` data class, repeating cycle:\n            - sample `num_train_episodes` from train data,\n            - sample `num_test_episodes` from test data.\n\n        Convention: supposed to override dummy method of local policy instance, see inside ._make_policy() method\n\n        Returns:\n            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n        """"""\n        # sess = tf.get_default_session()\n        if self.current_train_episode < self.num_train_episodes:\n            episode_type = 0  # train\n            self.current_train_episode += 1\n            self.log.debug(\n                \'c_1, c_train={}, c_test={}, type={}\'.\n                format(self.current_train_episode, self.current_test_episode, episode_type)\n            )\n        else:\n            if self.current_test_episode < self.num_test_episodes:\n                episode_type = 1  # test\n                self.current_test_episode += 1\n                self.log.debug(\n                    \'c_2, c_train={}, c_test={}, type={}\'.\n                    format(self.current_train_episode, self.current_test_episode, episode_type)\n                )\n            else:\n                # cycle end, reset and start new (rec. depth 1)\n                self.current_train_episode = 0\n                self.current_test_episode = 0\n                self.log.debug(\n                    \'c_3, c_train={}, c_test={}\'.\n                    format(self.current_train_episode, self.current_test_episode)\n                )\n                return self.get_sample_config(_new_trial=True)\n\n        # Compose btgym.datafeed.base.EnvResetConfig-consistent dict:\n        sample_config = dict(\n            episode_config=dict(\n                get_new=True,\n                sample_type=episode_type,\n                b_alpha=1.0,\n                b_beta=1.0\n            ),\n            trial_config=dict(\n                get_new=_new_trial,\n                sample_type=episode_type,\n                b_alpha=1.0,\n                b_beta=1.0\n            )\n        )\n        return sample_config\n\n    def start(self, sess, summary_writer, **kwargs):\n        """"""\n        Executes all initializing operations,\n        starts environment runner[s].\n        Supposed to be called by parent worker just before training loop starts.\n\n        Args:\n            sess:           tf session object.\n            kwargs:         not used by default.\n        """"""\n        try:\n            # Copy weights from global to local:\n            sess.run(self.sync)\n\n            # Start thread_runners:\n            self._start_runners(sess, summary_writer, **kwargs)\n\n        except Exception as e:\n            msg = \'start() exception occurred\' + \\\n                \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise e\n\n    def _start_runners(self, sess, summary_writer, **kwargs):\n        """"""\n\n        Args:\n            sess:\n            summary_writer:\n\n        Returns:\n\n        """"""\n        for runner in self.runners:\n            runner.start_runner(sess, summary_writer, **kwargs)  # starting runner threads\n\n        self.summary_writer = summary_writer\n\n    def _get_rp_feeder(self, pi, batch):\n        """"""\n        Returns feed dictionary for `reward prediction` loss estimation subgraph.\n\n        Args:\n            pi:     policy to feed\n        """"""\n        feeder = feed_dict_from_nested(pi.rp_state_in, batch[\'state\'])\n        feeder.update(\n            {\n                pi.rp_target: batch[\'rp_target\'],\n                pi.rp_batch_size: batch[\'batch_size\'],\n            }\n        )\n        return feeder\n\n    def _get_vr_feeder(self, pi, batch):\n        """"""\n        Returns feed dictionary for `value replay` loss estimation subgraph.\n\n        Args:\n            pi:     policy to feed\n        """"""\n        if not self.use_off_policy_aac:  # use single pass of network on same off-policy batch\n            feeder = feed_dict_from_nested(pi.vr_state_in, batch[\'state\'])\n            feeder.update(feed_dict_rnn_context(pi.vr_lstm_state_pl_flatten, batch[\'context\']))\n            feeder.update(\n                {\n                    pi.vr_batch_size: batch[\'batch_size\'],\n                    pi.vr_time_length: batch[\'time_steps\'],\n                    pi.vr_last_a_in: batch[\'last_action\'],\n                    pi.vr_last_reward_in: batch[\'last_reward\'],\n                    pi.vr_target: batch[\'r\']\n                }\n            )\n        else:\n            feeder = {pi.vr_target: batch[\'r\']}  # redundant actually :)\n        return feeder\n\n    def _get_pc_feeder(self, pi, batch):\n        """"""\n        Returns feed dictionary for `pixel control` loss estimation subgraph.\n\n        Args:\n            pi:     policy to feed\n        """"""\n        if not self.use_off_policy_aac:  # use single pass of network on same off-policy batch\n            feeder = feed_dict_from_nested(pi.pc_state_in, batch[\'state\'])\n            feeder.update(\n                feed_dict_rnn_context(pi.pc_lstm_state_pl_flatten, batch[\'context\']))\n            feeder.update(\n                {\n                    pi.pc_last_a_in: batch[\'last_action\'],\n                    pi.pc_last_reward_in: batch[\'last_reward\'],\n                    pi.pc_action: batch[\'action\'],\n                    pi.pc_target: batch[\'pixel_change\']\n                }\n            )\n        else:\n            feeder = {pi.pc_action: batch[\'action\'], pi.pc_target: batch[\'pixel_change\']}\n        return feeder\n\n    def _process_rollouts(self, rollouts):\n        """"""\n        rollout.process wrapper: makes single batch from list of rollouts\n\n        Args:\n            rollouts:   list of btgym.algorithms.Rollout class instances\n\n        Returns:\n            single batch data\n\n        """"""\n        batch = batch_stack(\n            [\n                r.process(\n                    gamma=self.model_gamma,\n                    gae_lambda=self.model_gae_lambda,\n                    size=self.rollout_length,\n                    time_flat=self.time_flat,\n                ) for r in rollouts\n            ]\n        )\n        return batch\n\n    def _get_main_feeder(\n            self,\n            sess,\n            on_policy_batch=None,\n            off_policy_batch=None,\n            rp_batch=None,\n            is_train=True,\n            pi=None,\n            pi_prime=None):\n        """"""\n        Composes entire train step feed dictionary.\n        Args:\n            sess:                   tf session obj.\n            pi:                     policy to feed\n            pi_prime:               optional policy to feed\n            on_policy_batch:        on-policy data batch\n            off_policy_batch:       off-policy (replay memory) data batch\n            rp_batch:               off-policy reward prediction data batch\n            is_train (bool):        is data provided are train or test\n\n        Returns:\n            feed_dict (dict):   train step feed dictionary\n        """"""\n        feed_dict = {}\n        # Feeder for on-policy AAC loss estimation graph:\n        if on_policy_batch is not None:\n            feed_dict = feed_dict_from_nested(pi.on_state_in, on_policy_batch[\'state\'])\n            feed_dict.update(\n                feed_dict_rnn_context(pi.on_lstm_state_pl_flatten, on_policy_batch[\'context\'])\n            )\n            feed_dict.update(\n                {\n                    pi.on_last_a_in: on_policy_batch[\'last_action\'],\n                    pi.on_last_reward_in: on_policy_batch[\'last_reward\'],\n                    pi.on_batch_size: on_policy_batch[\'batch_size\'],\n                    pi.on_time_length: on_policy_batch[\'time_steps\'],\n                    pi.on_pi_act_target: on_policy_batch[\'action\'],\n                    pi.on_pi_adv_target: on_policy_batch[\'advantage\'],\n                    pi.on_pi_r_target: on_policy_batch[\'r\'],\n                    pi.train_phase: is_train,  # Zeroes learn rate, [+ batch_norm + dropout]\n                }\n            )\n            if self.use_target_policy and pi_prime is not None:\n                feed_dict.update(\n                    feed_dict_from_nested(pi_prime.on_state_in, on_policy_batch[\'state\'])\n                )\n                feed_dict.update(\n                    feed_dict_rnn_context(pi_prime.on_lstm_state_pl_flatten, on_policy_batch[\'context\'])\n                )\n                feed_dict.update(\n                    {\n                        pi_prime.on_batch_size: on_policy_batch[\'batch_size\'],\n                        pi_prime.on_time_length: on_policy_batch[\'time_steps\'],\n                        pi_prime.on_last_a_in: on_policy_batch[\'last_action\'],\n                        pi_prime.on_last_reward_in: on_policy_batch[\'last_reward\'],\n                        # TODO: pi prime train phase?\n                    }\n                )\n        if (self.use_any_aux_tasks or self.use_off_policy_aac) and off_policy_batch is not None:\n            # Feeder for off-policy AAC loss estimation graph:\n            off_policy_feed_dict = feed_dict_from_nested(pi.off_state_in, off_policy_batch[\'state\'])\n            off_policy_feed_dict.update(\n                feed_dict_rnn_context(pi.off_lstm_state_pl_flatten, off_policy_batch[\'context\']))\n            off_policy_feed_dict.update(\n                {\n                    pi.off_last_a_in: off_policy_batch[\'last_action\'],\n                    pi.off_last_reward_in: off_policy_batch[\'last_reward\'],\n                    pi.off_batch_size: off_policy_batch[\'batch_size\'],\n                    pi.off_time_length: off_policy_batch[\'time_steps\'],\n                    pi.off_pi_act_target: off_policy_batch[\'action\'],\n                    pi.off_pi_adv_target: off_policy_batch[\'advantage\'],\n                    pi.off_pi_r_target: off_policy_batch[\'r\'],\n                }\n            )\n            if self.use_target_policy and pi_prime is not None:\n                off_policy_feed_dict.update(\n                    feed_dict_from_nested(pi_prime.off_state_in, off_policy_batch[\'state\'])\n                )\n                off_policy_feed_dict.update(\n                    {\n                        pi_prime.off_batch_size: off_policy_batch[\'batch_size\'],\n                        pi_prime.off_time_length: off_policy_batch[\'time_steps\'],\n                        pi_prime.off_last_a_in: off_policy_batch[\'last_action\'],\n                        pi_prime.off_last_reward_in: off_policy_batch[\'last_reward\'],\n                    }\n                )\n                off_policy_feed_dict.update(\n                    feed_dict_rnn_context(\n                        pi_prime.off_lstm_state_pl_flatten,\n                        off_policy_batch[\'context\']\n                    )\n                )\n            feed_dict.update(off_policy_feed_dict)\n\n            # Update with reward prediction subgraph:\n            if self.use_reward_prediction and rp_batch is not None:\n                # Rebalanced 50/50 sample for RP:\n                feed_dict.update(self._get_rp_feeder(pi, rp_batch))\n\n            # Pixel control ...\n            if self.use_pixel_control and off_policy_batch is not None:\n                feed_dict.update(self._get_pc_feeder(pi, off_policy_batch))\n\n            # VR...\n            if self.use_value_replay and off_policy_batch is not None:\n                feed_dict.update(self._get_vr_feeder(pi, off_policy_batch))\n\n        return feed_dict\n\n    def process_data(self, sess, data, is_train, pi, pi_prime=None):\n        """"""\n        Processes data, composes train step feed dictionary.\n        Args:\n            sess:               tf session obj.\n            pi:                 policy to feed\n            pi_prime:           optional policy to feed\n            data (dict):        data dictionary\n            is_train (bool):    is data provided are train or test\n\n        Returns:\n            feed_dict (dict):   train step feed dictionary\n        """"""\n        # Process minibatch for on-policy train step:\n        on_policy_batch = self._process_rollouts(data[\'on_policy\'])\n\n        if self.use_memory:\n            # Process rollouts from replay memory:\n            off_policy_batch = self._process_rollouts(data[\'off_policy\'])\n\n            if self.use_reward_prediction:\n                # Rebalanced 50/50 sample for RP:\n                rp_rollouts = data[\'off_policy_rp\']\n                rp_batch = batch_stack([rp.process_rp(self.rp_reward_threshold) for rp in rp_rollouts])\n\n            else:\n                rp_batch = None\n\n        else:\n            off_policy_batch = None\n            rp_batch = None\n\n        return self._get_main_feeder(sess, on_policy_batch, off_policy_batch, rp_batch, is_train, pi, pi_prime)\n\n    def process_summary(self, sess, data, model_data=None, step=None, episode=None):\n        """"""\n        Fetches and writes summary data from `data` and `model_data`.\n        Args:\n            sess:               tf summary obj.\n            data(dict):         thread_runner rollouts and metadata\n            model_data(dict):   model summary data\n            step:               int, global step or None\n            episode:            int, global episode number or None\n        """"""\n        if step is None:\n            step = sess.run(self.global_step)\n\n        if episode is None:\n            episode = sess.run(self.global_episode)\n        # Every worker writes train episode summaries:\n        ep_summary_feeder = {}\n\n        # Look for train episode summaries from all env runners:\n\n        # self.log.warning(\'data+ep_summary: {}\'.format( data[\'ep_summary\']))\n\n        for stat in data[\'ep_summary\']:\n            if stat is not None:\n                for key in stat.keys():\n                    if key in ep_summary_feeder.keys():\n                        ep_summary_feeder[key] += [stat[key]]\n                    else:\n                        ep_summary_feeder[key] = [stat[key]]\n\n        # Average values among thread_runners, if any, and write episode summary:\n\n        # self.log.warning(\'ep_summary_feeder: {}\'.format(ep_summary_feeder))\n\n        if ep_summary_feeder != {}:\n            ep_summary_feed_dict = {\n                self.ep_summary[key]: np.average(list) for key, list in ep_summary_feeder.items()\n            }\n\n            if self.test_mode:\n                # Atari:\n                fetched_episode_stat = sess.run(self.ep_summary[\'atari_stat_op\'], ep_summary_feed_dict)\n\n            else:\n                # BTGym\n                fetched_episode_stat = sess.run(self.ep_summary[\'btgym_stat_op\'], ep_summary_feed_dict)\n\n            self.summary_writer.add_summary(fetched_episode_stat, episode)\n            # self.summary_writer.flush()\n\n        # Every worker writes test episode  summaries:\n        test_ep_summary_feeder = {}\n\n        # Look for test episode summaries:\n\n        # self.log.warning(\'data+test_ep_summary: {}\'.format(data[\'test_ep_summary\']))\n\n        for stat in data[\'test_ep_summary\']:\n            if stat is not None:\n                for key in stat.keys():\n                    if key in test_ep_summary_feeder.keys():\n                        test_ep_summary_feeder[key] += [stat[key]]\n                    else:\n                        test_ep_summary_feeder[key] = [stat[key]]\n\n        # Average values among thread_runners, if any, and write episode summary:\n\n        # self.log.warning(\'test_ep_summary_feeder: {}\'.format(test_ep_summary_feeder))\n\n        if test_ep_summary_feeder != {}:\n            test_ep_summary_feed_dict = {\n                self.ep_summary[key]: np.average(list) for key, list in test_ep_summary_feeder.items()\n            }\n            fetched_test_episode_stat = sess.run(self.ep_summary[\'test_btgym_stat_op\'], test_ep_summary_feed_dict)\n            self.summary_writer.add_summary(fetched_test_episode_stat, episode)\n\n        # Look for renderings (chief worker only, always 0-numbered environment in a list):\n        if self.task == 0:\n            if data[\'render_summary\'][0] is not None:\n\n                #self.log.warning(\'data[render_summary]: {}\'.format(data[\'render_summary\']))\n                #self.log.warning(\'self.ep_summary: {}\'.format(self.ep_summary))\n\n                render_feed_dict = {\n                    self.ep_summary[key]: pic for key, pic in data[\'render_summary\'][0].items()\n                }\n                renderings = sess.run(self.ep_summary[\'render_op\'], render_feed_dict)\n                self.summary_writer.add_summary(renderings, episode)\n                self.summary_writer.flush()\n\n        # Every worker writes train episode summaries:\n        if model_data is not None:\n            self.summary_writer.add_summary(tf.Summary.FromString(model_data), step)\n        self.summary_writer.flush()\n\n    def process(self, sess, **kwargs):\n        """"""\n        Main train step method wrapper. Override if needed.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n            kwargs:                      any\n\n\n        """"""\n        # return self._process(sess)\n        self._process(sess)\n\n    def _process(self, sess):\n        """"""\n        Grabs an on_policy_rollout [and off_policy rollout[s] from replay memory] that\'s been produced\n        by the thread runner. If data identified as \'train data\' - computes gradients and updates the parameters;\n        writes summaries if any. The update is then sent to the parameter server.\n        If on_policy_rollout identified as \'test data\' -  no policy update is performed (learn rate is set to zero);\n        Note that test data does not get stored in replay memory (thread runner area).\n        Writes all available summaries.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n        """"""\n        # Quick wrap to get direct traceback from this trainer if something goes wrong:\n        try:\n            # Collect data from child thread runners:\n            data = self.get_data()\n\n            # Copy weights from local policy to local target policy:\n            if self.use_target_policy and self.local_steps % self.pi_prime_update_period == 0:\n                sess.run(self.sync_pi_prime)\n\n            # Test or train: if at least one on-policy rollout from parallel runners is test one -\n            # set learn rate to zero for entire minibatch. Doh.\n            try:\n                is_train = not np.asarray([env[\'state\'][\'metadata\'][\'type\'] for env in data[\'on_policy\']]).any()\n\n            except KeyError:\n                is_train = True\n\n            self.log.debug(\n                \'Got rollout episode. type: {}, trial_type: {}, is_train: {}\'.format(\n                    np.asarray([env[\'state\'][\'metadata\'][\'type\'] for env in data[\'on_policy\']]).any(),\n                    np.asarray([env[\'state\'][\'metadata\'][\'trial_type\'] for env in data[\'on_policy\']]).any(),\n                    is_train\n                )\n            )\n\n            if is_train:\n                # If there is no any test rollouts  - do a train step:\n                sess.run(self.sync_pi)  # only sync at train time\n\n                feed_dict = self.process_data(sess, data, is_train, self.local_network, self.local_network_prime)\n\n                # Say `No` to redundant summaries:\n                wirte_model_summary =\\\n                    self.local_steps % self.model_summary_freq == 0\n\n                #fetches = [self.train_op, self.local_network.debug]  # include policy debug shapes\n                fetches = [self.train_op]\n\n                if wirte_model_summary:\n                    fetches_last = fetches + [self.model_summary_op, self.inc_step]\n                else:\n                    fetches_last = fetches + [self.inc_step]\n\n                # Do a number of SGD train epochs:\n                # When doing more than one epoch, we actually use only last summary:\n                for i in range(self.num_epochs - 1):\n                    fetched = sess.run(fetches, feed_dict=feed_dict)\n\n                fetched = sess.run(fetches_last, feed_dict=feed_dict)\n\n                if wirte_model_summary:\n                    model_summary = fetched[-2]\n\n                else:\n                    model_summary = None\n\n                self.local_steps += 1  # only update on train steps\n\n            else:\n                model_summary = None\n\n            # Write down summaries:\n            self.process_summary(sess, data, model_summary)\n\n            # print debug info:\n            #for k, v in fetched[1].items():\n            #    print(\'{}: {}\'.format(k,v))\n            #print(\'\\n\')\n\n            #for k, v in feed_dict.items():\n            #    try:\n            #        print(k, v.shape)\n            #    except:\n            #        print(k, type(v))\n\n            # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n\n        except:\n            msg = \'process() exception occurred\' + \\\n                \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n\nclass Unreal(BaseAAC):\n    """"""\n    Unreal: Asynchronous Advantage Actor Critic with auxiliary control tasks.\n\n    Auxiliary tasks implementation borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:\n    https://miyosuda.github.io/\n    https://github.com/miyosuda/unreal\n\n    Original A3C code comes from OpenAI repository under MIT licence:\n    https://github.com/openai/universe-starter-agent\n\n    Papers:\n    https://arxiv.org/abs/1602.01783\n    https://arxiv.org/abs/1611.05397\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n        See BaseAAC class args for details:\n\n        Args:\n            env:                    environment instance or list of instances\n            task:                   int, parent worker id\n            policy_config:          policy estimator class and configuration dictionary\n            log_level:              int, logbook.level\n            on_policy_loss:         callable returning tensor holding on_policy training loss graph and summaries\n            off_policy_loss:        callable returning tensor holding off_policy training loss graph and summaries\n            vr_loss:                callable returning tensor holding value replay loss graph and summaries\n            rp_loss:                callable returning tensor holding reward prediction loss graph and summaries\n            pc_loss:                callable returning tensor holding pixel_control loss graph and summaries\n            random_seed:            int or None\n            model_gamma:            scalar, gamma discount factor\n            model_gae_lambda:       scalar, GAE lambda\n            model_beta:             entropy regularization beta, scalar or [high_bound, low_bound] for log_uniform.\n            opt_max_env_steps:      int, total number of environment steps to run training on.\n            opt_decay_steps:        int, learn ratio decay steps, in number of environment steps.\n            opt_end_learn_rate:     scalar, final learn rate\n            opt_learn_rate:         start learn rate, scalar or [high_bound, low_bound] for log_uniform distr.\n            opt_decay:              scalar, optimizer decay, if apll.\n            opt_momentum:           scalar, optimizer momentum, if apll.\n            opt_epsilon:            scalar, optimizer epsilon\n            rollout_length:         int, on-policy rollout length\n            time_flat:              bool, flatten rnn time-steps in rollouts while training - see `Notes` below\n            episode_train_test_cycle:   tuple or list as (train_number, test_number), def=(1,0): enables infinite\n                                        loop such as: run `train_number` of train data episodes,\n                                        than `test_number` of test data episodes, repeat. Should be consistent\n                                        with provided dataset parameters (test data should exist if `test_number > 0`)\n            episode_summary_freq:   int, write episode summary for every i\'th episode\n            env_render_freq:        int, write environment rendering summary for every i\'th train step\n            model_summary_freq:     int, write model summary for every i\'th train step\n            test_mode:              bool, True: Atari, False: BTGym\n            replay_memory_size:     int, in number of experiences\n            replay_batch_size:      int, mini-batch size for off-policy training, def = 1\n            replay_rollout_length:  int off-policy rollout length by def. equals on_policy_rollout_length\n            use_off_policy_aac:     bool, use full AAC off-policy loss instead of Value-replay\n            use_reward_prediction:  bool, use aux. off-policy reward prediction task\n            use_pixel_control:      bool, use aux. off-policy pixel control task\n            use_value_replay:       bool, use aux. off-policy value replay task (not used if use_off_policy_aac=True)\n            rp_lambda:              reward prediction loss weight, scalar or [high, low] for log_uniform distr.\n            pc_lambda:              pixel control loss weight, scalar or [high, low] for log_uniform distr.\n            vr_lambda:              value replay loss weight, scalar or [high, low] for log_uniform distr.\n            off_aac_lambda:         off-policy AAC loss weight, scalar or [high, low] for log_uniform distr.\n            gamma_pc:               NOT USED\n            rp_reward_threshold:    scalar, reward prediction classification threshold, above which reward is \'non-zero\'\n            rp_sequence_size:       int, reward prediction sample size, in number of experiences\n            clip_epsilon:           scalar, PPO: surrogate L^clip epsilon\n            num_epochs:             int, num. of SGD runs for every train step, val. > 1 should be used with caution.\n            pi_prime_update_period: int, PPO: pi to pi_old update period in number of train steps, def: 1\n            _use_target_policy:     bool, PPO: use target policy (aka pi_old), delayed by `pi_prime_update_period` delay\n\n        Note:\n            - On `time_flat` arg:\n\n                There are two alternatives to run RNN part of policy estimator:\n\n                a. Feed initial RNN state for every experience frame in rollout\n                        (those are stored anyway if we want random memory repaly sampling) and do single time-step RNN\n                        advance for all experiences in a batch; this is when time_flat=True;\n\n                b. Reshape incoming batch after convolution part of network in time-wise fashion\n                        for every rollout in a batch i.e. batch_size=number_of_rollouts and\n                        rnn_timesteps=max_rollout_length. In this case we need to feed initial rnn_states\n                        for rollouts only. There is some little extra work to pad rollouts to max_time_size\n                        and feed true rollout lengths to rnn. Thus, when time_flat=False, we unroll RNN in\n                        specified number of time-steps for every rollout.\n\n                Both options has pros and cons:\n\n                Unrolling dynamic RNN is computationally more expensive but gives clearly faster convergence,\n                    [possibly] due to the fact that RNN states for 2nd, 3rd, ... frames\n                    of rollouts are computed using updated policy estimator, which is supposed to be\n                    closer to optimal one. When time_flattened, every time-step uses RNN states computed\n                    when rollout was collected (i.e. by behavioral policy estimator with older\n                    parameters).\n\n                Nevertheless, time_flatting can be interesting\n                    because one can safely shuffle training batch or mix on-policy and off-policy data in single mini-batch,\n                    ensuring iid property and allowing, say, proper batch normalisation (this has yet to be tested).\n        """"""\n        try:\n            super(Unreal, self).__init__(name=\'UNREAL\', **kwargs)\n        except:\n            msg = \'Child class Unreal __init()__ exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n\nclass A3C(BaseAAC):\n    """"""\n    Vanilla Asynchronous Advantage Actor Critic algorithm.\n\n    Based on original code taken from OpenAI repository under MIT licence:\n    https://github.com/openai/universe-starter-agent\n\n    Paper: https://arxiv.org/abs/1602.01783\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        A3C args. is a subset of BaseAAC arguments, see `BaseAAC` class for descriptions.\n\n        Args:\n            env:\n            task:\n            policy_config:\n            log:\n            random_seed:\n            model_gamma:\n            model_gae_lambda:\n            model_beta:\n            opt_max_env_steps:\n            opt_decay_steps:\n            opt_end_learn_rate:\n            opt_learn_rate:\n            opt_decay:\n            opt_momentum:\n            opt_epsilon:\n            rollout_length:\n            episode_summary_freq:\n            env_render_freq:\n            model_summary_freq:\n            test_mode:\n        """"""\n        super(A3C, self).__init__(\n            on_policy_loss=aac_loss_def,\n            use_off_policy_aac=False,\n            use_reward_prediction=False,\n            use_pixel_control=False,\n            use_value_replay=False,\n            _use_target_policy=False,\n            name=\'A3C\',\n            **kwargs\n        )\n\n\nclass PPO(BaseAAC):\n    """"""\n    AAC with Proximal Policy Optimization surrogate L^Clip loss,\n    optionally augmented with auxiliary control tasks.\n\n    paper:\n    https://arxiv.org/pdf/1707.06347.pdf\n\n    Based on PPO-SGD code from OpenAI `Baselines` repository under MIT licence:\n    https://github.com/openai/baselines\n\n    Async. framework code comes from OpenAI repository under MIT licence:\n    https://github.com/openai/universe-starter-agent\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n         PPO args. is a subset of BaseAAC arguments, see `BaseAAC` class for descriptions.\n\n        Args:\n            env:\n            task:\n            policy_config:\n            log_level:\n            vr_loss:\n            rp_loss:\n            pc_loss:\n            random_seed:\n            model_gamma:\n            model_gae_lambda:\n            model_beta:\n            opt_max_env_steps:\n            opt_decay_steps:\n            opt_end_learn_rate:\n            opt_learn_rate:\n            opt_decay:\n            opt_momentum:\n            opt_epsilon:\n            rollout_length:\n            episode_summary_freq:\n            env_render_freq:\n            model_summary_freq:\n            test_mode:\n            replay_memory_size:\n            replay_rollout_length:\n            use_off_policy_aac:\n            use_reward_prediction:\n            use_pixel_control:\n            use_value_replay:\n            rp_lambda:\n            pc_lambda:\n            vr_lambda:\n            off_aac_lambda:\n            rp_reward_threshold:\n            rp_sequence_size:\n            clip_epsilon:\n            num_epochs:\n            pi_prime_update_period:\n        """"""\n        super(PPO, self).__init__(\n            on_policy_loss=ppo_loss_def,\n            off_policy_loss=ppo_loss_def,\n            _use_target_policy=True,\n            name=\'PPO\',\n            **kwargs\n        )\n\n\n'"
btgym/algorithms/envs.py,0,"b'# Original code is taken from OpenAI repository under MIT licence:\n# https://github.com/openai/universe-starter-agent\n\nimport numpy as np\nimport cv2\nimport gym\nfrom gym import spaces\nfrom btgym import DictSpace, ActionDictSpace\n\n\ndef _process_frame42(frame):\n    frame = frame[34:34+160, :160]\n    # Resize by half, then down to 42x42 (essentially mipmapping). If\n    # we resize directly we lose pixels that, when mapped to 42x42,\n    # aren\'t close enough to the pixel boundary.\n    frame = cv2.resize(frame, (80, 80))\n    frame = cv2.resize(frame, (42, 42))\n    frame = frame.mean(2)\n    frame = frame.astype(np.float32)\n    frame *= (1.0 / 255.0)\n    frame = np.reshape(frame, [42, 42, 1])\n    return frame\n\n\nclass AtariRescale42x42(gym.ObservationWrapper):\n    """"""\n    Gym wrapper, pipes Atari into BTgym algorithms, as later expect observations to be DictSpace.\n    Makes Atari environment return state as dictionary with single key \'external\' holding\n    normalized in [0,1] grayscale 42x42 visual output.\n    """"""\n    # TODO: INPRoGRESS: dict observation space, include metadata etc.\n    def __init__(self, env_id=None):\n        """"""\n\n        Args:\n            env_id:     conventional Gym id.\n        """"""\n        assert ""."" not in env_id  # universe environments have dots in names.\n        env = gym.make(env_id)\n        super(AtariRescale42x42, self).__init__(env)\n        self.observation_space = DictSpace(\n            {\'external\': spaces.Box(0.0, 1.0, [42, 42, 1], dtype=np.float32)}\n        )\n        self.asset_names = [\'atari_player\']\n        num_actions = self.action_space.n\n        self.action_space = ActionDictSpace(\n            base_actions=list(np.arange(num_actions)),\n            assets=self.asset_names\n        )\n\n    def observation(self, observation):\n        return {\'external\': _process_frame42(observation)}\n\n    def get_initial_action(self):\n        return {asset: 0 for asset in self.asset_names}\n\n    def step(self, action):\n        # TODO: fix it\n        action = action[self.asset_names[0]]\n        observation, reward, done, info = self.env.step(action)\n        reward = np.asarray(reward)\n        return self.observation(observation), reward, done, info\n'"
btgym/algorithms/math_utils.py,11,"b'import numpy as np\nimport scipy.signal\n\nfrom scipy.special import logsumexp\n\nimport tensorflow as tf\n\n\ndef discount(x, gamma):\n    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n\n\ndef log_uniform(lo_hi, size):\n    """"""\n    Samples from log-uniform distribution in range specified by `lo_hi`.\n    Takes:\n        lo_hi: either scalar or [low_value, high_value]\n        size: sample size\n    Returns:\n         np.array or np.float (if size=1).\n    """"""\n    r = np.asarray(lo_hi)\n    try:\n        lo = r[0]\n        hi = r[-1]\n    except:\n        lo = hi = r\n    x = np.random.random(size)\n    log_lo = np.log(lo + 1e-12)\n    log_hi = np.log(hi + 1e-12)\n    v = log_lo * (1 - x) + log_hi * x\n    if size > 1:\n        return np.exp(v)\n    else:\n        return np.exp(v)[0]\n\n\ndef cat_entropy(logits):\n    a0 = logits - tf.reduce_max(logits, 1, keepdims=True)\n    ea0 = tf.exp(a0)\n    z0 = tf.reduce_sum(ea0, 1, keepdims=True)\n    p0 = ea0 / z0\n    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)\n\n\ndef kl_divergence(logits_1, logits_2):\n    a0 = logits_1 - tf.reduce_max(logits_1, axis=-1, keepdims=True)\n    a1 = logits_2 - tf.reduce_max(logits_2, axis=-1, keepdims=True)\n    ea0 = tf.exp(a0)\n    ea1 = tf.exp(a1)\n    z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n    z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)\n    p0 = ea0 / z0\n    return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)\n\n\n# def softmax(x):\n#     if len(x.shape) > 1:\n#         tmp = np.max(x, axis = 1)\n#         x -= tmp.reshape((x.shape[0], 1))\n#         x = np.exp(x)\n#         tmp = np.sum(x, axis = 1)\n#         x /= tmp.reshape((x.shape[0], 1))\n#     else:\n#         tmp = np.max(x)\n#         x -= tmp\n#         x = np.exp(x)\n#         tmp = np.sum(x)\n#         x /= tmp\n#\n#     return x\n\n\ndef softmax(a, axis=None):\n    """"""\n    Computes exp(a)/sumexp(a); relies on scipy logsumexp implementation.\n    Credit goes to https://stackoverflow.com/users/4115369/yibo-yang\n\n    Args:\n        a: ndarray/tensor\n        axis: axis to sum over; default (None) sums over everything\n    """"""\n    lse = logsumexp(a, axis=axis)  # this reduces along axis\n    if axis is not None:\n        lse = np.expand_dims(lse, axis)  # restore that axis for subtraction\n    return np.exp(a - lse)\n\n\ndef sample_dp(logits, alpha=200.0):\n    """"""\n    Given vector of unnormalised log probabilities,\n    returns sample of probability distribution taken from induced Dirichlet Process,\n    where `logits` define DP mean and `alpha` is inverse variance.\n\n    Args:\n        logits:     vector of unnormalised probabilities\n        alpha:      scalar, concentration parameter\n\n    Returns:\n        vector of probabilities\n    """"""\n    return softmax(np.random.multivariate_normal(mean=logits, cov=np.eye(logits.shape[-1]) * alpha ** -1))\n\n'"
btgym/algorithms/memory.py,0,"b'# This implementation is based on Kosuke Miyoshi code, under Apache License 2.0:\n# https://miyosuda.github.io/\n# https://github.com/miyosuda/unreal\n\nfrom logbook import Logger, StreamHandler, WARNING\nimport sys\n\nimport numpy as np\nfrom collections import deque\nfrom btgym.algorithms.rollout import Rollout\n\n\nclass Memory(object):\n    """"""\n    Replay memory with rebalanced replay based on reward value.\n\n    Note:\n        must be filled up before calling sampling methods.\n    """"""\n    def __init__(self, history_size, max_sample_size, priority_sample_size, log_level=WARNING,\n                 rollout_provider=None, task=-1, reward_threshold=0.1, use_priority_sampling=False):\n        """"""\n\n        Args:\n            history_size:           number of experiences stored;\n            max_sample_size:        maximum allowed sample size (e.g. off-policy rollout length);\n            priority_sample_size:   sample size of priority_sample() method\n            log_level:              int, logbook.level;\n            rollout_provider:       callable returning list of Rollouts NOT USED\n            task:                   parent worker id;\n            reward_threshold:       if |experience.reward| > reward_threshold: experience is saved as \'prioritized\';\n        """"""\n        self._history_size = history_size\n        self._frames = deque(maxlen=history_size)\n        self.reward_threshold = reward_threshold\n        self.max_sample_size = int(max_sample_size)\n        self.priority_sample_size = int(priority_sample_size)\n        self.rollout_provider = rollout_provider\n        self.task = task\n        self.log_level = log_level\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'ReplayMemory_{}\'.format(self.task), level=self.log_level)\n        self.use_priority_sampling = use_priority_sampling\n        # Indices for non-priority frames:\n        self._zero_reward_indices = deque()\n        # Indices for priority frames:\n        self._non_zero_reward_indices = deque()\n        self._top_frame_index = 0\n\n        if use_priority_sampling:\n            self.sample_priority = self._sample_priority\n\n        else:\n            self.sample_priority = self._sample_dummy\n\n    def add(self, frame):\n        """"""\n        Appends single experience frame to memory.\n\n        Args:\n            frame:  dictionary of values.\n        """"""\n        if frame[\'terminal\'] and len(self._frames) > 0 and self._frames[-1][\'terminal\']:\n            # Discard if terminal frame continues\n            self.log.warning(""Memory_{}: Sequential terminal frame encountered. Discarded."".format(self.task))\n            self.log.warning(\'{} -- {}\'.format(self._frames[-1][\'position\'], frame[\'position\']))\n            return\n\n        frame_index = self._top_frame_index + len(self._frames)\n        was_full = self.is_full()\n\n        # Append frame:\n        self._frames.append(frame)\n\n        # Decide and append index:\n        if frame_index >= self.max_sample_size - 1:\n            if abs(frame[\'reward\']) <= self.reward_threshold:\n                self._zero_reward_indices.append(frame_index)\n\n            else:\n                self._non_zero_reward_indices.append(frame_index)\n\n        if was_full:\n            # Decide from which index to discard:\n            self._top_frame_index += 1\n\n            cut_frame_index = self._top_frame_index + self.max_sample_size - 1\n            # Cut frame if its index is lower than cut_frame_index:\n            if len(self._zero_reward_indices) > 0 and \\\n                            self._zero_reward_indices[0] < cut_frame_index:\n                self._zero_reward_indices.popleft()\n\n            if len(self._non_zero_reward_indices) > 0 and \\\n                            self._non_zero_reward_indices[0] < cut_frame_index:\n                self._non_zero_reward_indices.popleft()\n\n    def add_rollout(self, rollout):\n        """"""\n        Adds frames from given rollout to memory with respect to episode continuation.\n\n        Args:\n            rollout:    `Rollout` instance.\n        """"""\n        # Check if current rollout is direct extension of last stored frame sequence:\n        if len(self._frames) > 0 and not self._frames[-1][\'terminal\']:\n            # E.g. check if it is same local episode and successive frame order:\n            if self._frames[-1][\'position\'][\'episode\'] == rollout[\'position\'][\'episode\'][0] and \\\n                    self._frames[-1][\'position\'][\'step\'] + 1 == rollout[\'position\'][\'step\'][0]:\n                # Means it is ok to just extend previously stored episode\n                pass\n            else:\n                # Means part or tail of previously recorded episode is somehow lost,\n                # so we need to mark stored episode as \'ended\':\n                self._frames[-1][\'terminal\'] = True\n                self.log.warning(\'{} changed to terminal\'.format(self._frames[-1][\'position\']))\n                # If we get a lot of such messages it is an indication something is going wrong.\n        # Add experiences one by one:\n        # TODO: pain-slow.\n        for i in range(len(rollout[\'terminal\'])):\n            frame = rollout.get_frame(i)\n            self.add(frame)\n\n    def is_full(self):\n        return len(self._frames) >= self._history_size\n\n    def fill(self):\n        """"""\n        Fills replay memory with initial experiences. NOT USED.\n        Supposed to be called by parent worker() just before training begins.\n\n        Args:\n            rollout_getter:     callable, returning list of Rollouts.\n\n        """"""\n        if self.rollout_provider is not None:\n            while not self.is_full():\n                for rollout in self.rollout_provider():\n                    self.add_rollout(rollout)\n            self.log.info(\'Memory_{}: filled.\'.format(self.task))\n\n        else:\n            raise AttributeError(\'Rollout_provider is None, can not fill memory.\')\n\n    def sample_uniform(self, sequence_size):\n        """"""\n        Uniformly samples sequence of successive frames of size `sequence_size` or less (~off-policy rollout).\n\n        Args:\n            sequence_size:  maximum sample size.\n        Returns:\n            instance of Rollout of size <= sequence_size.\n        """"""\n        start_pos = np.random.randint(0, self._history_size - sequence_size - 1)\n        # Shift by one if hit terminal frame:\n        if self._frames[start_pos][\'terminal\']:\n            start_pos += 1  # assuming that there are no successive terminal frames.\n\n        sampled_rollout = Rollout()\n\n        for i in range(sequence_size):\n            frame = self._frames[start_pos + i]\n            sampled_rollout.add(frame)\n            if frame[\'terminal\']:\n                break  # it\'s ok to return less than `sequence_size` frames if `terminal` frame encountered.\n\n        return sampled_rollout\n\n    def _sample_priority(self, size=None, exact_size=False, skewness=2, sample_attempts=100):\n        """"""\n        Implements rebalanced replay.\n        Samples sequence of successive frames from distribution skewed by means of reward of last sample frame.\n\n        Args:\n            size:               sample size, must be <= self.max_sample_size;\n            exact_size:         whether accept sample with size less than \'size\'\n                                or re-sample to get sample of exact size (used for reward prediction task);\n            skewness:           int>=1, sampling probability denominator, such as probability of sampling sequence with\n                                last frame having non-zero reward is: p[non_zero]=1/skewness;\n            sample_attempts:    if exact_size=True, sets number of re-sampling attempts\n                                to get sample of continuous experiences (no `Terminal` frames inside except last one);\n                                if number is reached - sample returned \'as is\'.\n        Returns:\n            instance of Rollout().\n        """"""\n        if size is None:\n            size = self.priority_sample_size\n\n        if size > self.max_sample_size:\n            size = self.max_sample_size\n\n        # Toss skewed coin:\n        if np.random.randint(int(skewness)) == 0:\n            from_zero = False\n        else:\n            from_zero = True\n\n        if len(self._zero_reward_indices) == 0:\n            # zero rewards container was empty\n            from_zero = False\n        elif len(self._non_zero_reward_indices) == 0:\n            # non zero rewards container was empty\n            from_zero = True\n\n        # Try to sample sequence of given length from one episode.\n        # Take maximum of \'sample_attempts\', if no luck\n        # (e.g too short episodes and/or too big sampling size) ->\n        # return inconsistent sample and issue warning.\n        check_sequence = True\n        for attempt in range(sample_attempts):\n            if from_zero:\n                index = np.random.randint(len(self._zero_reward_indices))\n                end_frame_index = self._zero_reward_indices[index]\n\n            else:\n                index = np.random.randint(len(self._non_zero_reward_indices))\n                end_frame_index = self._non_zero_reward_indices[index]\n\n            start_frame_index = end_frame_index - size + 1\n            raw_start_frame_index = start_frame_index - self._top_frame_index\n\n            sampled_rollout = Rollout()\n            is_full = True\n            if attempt == sample_attempts - 1:\n                check_sequence = False\n                self.log.warning(\n                    \'Memory_{}: failed to sample {} successive frames, sampled as is.\'.format(self.task, size)\n                )\n\n            for i in range(size - 1):\n                frame = self._frames[raw_start_frame_index + i]\n                sampled_rollout.add(frame)\n                if check_sequence:\n                    if frame[\'terminal\']:\n                        if exact_size:\n                            is_full = False\n                        #print(\'attempt:\', attempt)\n                        #print(\'frame.terminal:\', frame[\'terminal\'])\n                        break\n            # Last frame can be terminal anyway:\n            frame = self._frames[raw_start_frame_index + size - 1]\n            sampled_rollout.add(frame)\n\n            if is_full:\n                break\n\n        return sampled_rollout\n\n    @staticmethod\n    def _sample_dummy(**kwargs):\n        return None\n\n\nclass _DummyMemory:\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def add(frame):\n        return None\n\n    @staticmethod\n    def sample_uniform(**kwargs):\n        return None\n\n    @staticmethod\n    def sample_priority(**kwargs):\n        return  None\n\n    @staticmethod\n    def is_full():\n        return True'"
btgym/algorithms/rollout.py,0,"b'# Original A3C code comes from OpenAI repository under MIT licence:\n# https://github.com/openai/universe-starter-agent\n#\n# Papers:\n# https://arxiv.org/abs/1602.01783\n# https://arxiv.org/abs/1611.05397\n\n\nimport numpy as np\n\nfrom tensorflow.contrib.rnn import LSTMStateTuple\nfrom btgym.algorithms.math_utils import discount\nfrom btgym.algorithms.utils import batch_pad\n\n\n# Info:\nExperienceConfig = [\'position\', \'state\', \'action\', \'reward\', \'value\', \'terminal\', \'r\', \'context\',\n                    \'last_action_reward\', \'pixel_change\']\n\n\ndef make_data_getter(queue):\n    """"""\n    Data stream getter constructor.\n\n    Args:\n        queue:     instance of `Queue` class to get rollouts from.\n\n    Returns:\n        callable, returning dictionary of data.\n\n    """"""\n    def pull_rollout_from_queue(**kwargs):\n        return queue.get(timeout=600.0)\n\n    return pull_rollout_from_queue\n\n\nclass Rollout(dict):\n    """"""\n    Experience rollout as [nested] dictionary of lists of ndarrays, tuples and rnn states.\n    """"""\n\n    def __init__(self):\n        super(Rollout, self).__init__()\n        self.size = 0\n\n    def add(self, values, _struct=None):\n        """"""\n        Adds single experience frame to rollout.\n\n        Args:\n            values:    [nested] dictionary of values.\n        """"""\n        if _struct is None:\n            # Top level:\n            _struct = self\n            self.size += 1\n            top = True\n\n        else:\n            top = False\n\n        try:\n            if isinstance(values, dict):\n                for key, value in values.items():\n                    if key not in _struct.keys():\n                        _struct[key] = {}\n                    _struct[key] = self.add(value, _struct[key])\n\n            elif isinstance(values, tuple):\n                if not isinstance(_struct, tuple):\n                    _struct = [\'empty\' for entry in values]\n                _struct = tuple([self.add(*pair) for pair in zip(values, _struct)])\n\n            elif isinstance(values, LSTMStateTuple):\n                if not isinstance(_struct, LSTMStateTuple):\n                    _struct = LSTMStateTuple(0, 0)\n                c = self.add(values[0], _struct[0])\n                h = self.add(values[1], _struct[1])\n                _struct = LSTMStateTuple(c, h)\n\n            else:\n                if isinstance(_struct, list):\n                    _struct += [values]\n\n                else:\n                    _struct = [values]\n\n        except:\n            print(\'values:\\n\', values)\n            print(\'_struct:\\n\', _struct)\n            raise RuntimeError\n\n        if not top:\n            return _struct\n\n    def add_memory_sample(self, sample):\n        """"""\n        Given replay memory sample as list of experience-dictionaries of `length`,\n        converts it to rollout of same `length`.\n        """"""\n        for frame in sample:\n            self.add(frame)\n\n    def process(self, gamma, gae_lambda=1.0, size=None, time_flat=False):\n        """"""\n        Converts single-trajectory rollout of experiences to dictionary of ready-to-feed arrays.\n        Computes rollout returns and the advantages.\n        Pads with zeroes to desired length, if size arg is given.\n\n        Args:\n            gamma:          discount factor\n            gae_lambda:     GAE lambda\n            size:           if given and time_flat=False, pads outputs with zeroes along `time\' dim. to exact \'size\'.\n            time_flat:      reduce time dimension to 1 step by stacking all experiences along batch dimension.\n\n        Returns:\n            batch as [nested] dictionary of np.arrays, tuples and LSTMStateTuples. of size:\n\n                [1, time_size, depth] or [1, size, depth] if not time_flatten and `size` is not/given, with single\n                `context` entry for entire trajectory, i.e. of size [1, context_depth];\n\n                [batch_size, 1, depth], if time_flatten, with batch_size = time_size and `context` entry for\n                every experience frame, i.e. of size [batch_size, context_depth].\n        """"""\n        # self._check_it()\n        batch = dict()\n        for key in self.keys() - {\'context\', \'reward\', \'r\', \'value\', \'position\'}:\n            batch[key] = self.as_array(self[key])\n\n        if time_flat:\n            batch[\'context\'] = self.as_array(self[\'context\'], squeeze_axis=1)  # LSTM state for every frame\n\n        else:\n            batch[\'context\'] = self.get_frame(0)[\'context\'] # just get rollout initial LSTM state\n\n        #print(\'batch_context:\')\n        #self._check_it(batch[\'context\'])\n\n        # Total accumulated empirical return:\n        rewards = np.asarray(self[\'reward\'])\n        rollout_r = self[\'r\'][-1][0]  # bootstrapped V_next or 0 if terminal\n        vpred_t = np.asarray(self[\'value\'] + [rollout_r])\n        rewards_plus_v = np.asarray(self[\'reward\'] + [rollout_r])\n        batch[\'r\'] = discount(rewards_plus_v, gamma)[:-1]\n\n        # This formula for the advantage is (16) from ""Generalized Advantage Estimation"" paper:\n        # https://arxiv.org/abs/1506.02438\n        delta_t = rewards + gamma * vpred_t[1:] - vpred_t[:-1]\n        batch[\'advantage\'] = discount(delta_t, gamma * gae_lambda)\n\n        # Shape it out:\n        if time_flat:\n            batch[\'batch_size\'] = batch[\'advantage\'].shape[0]  # time length turned batch size\n            batch[\'time_steps\'] = np.ones(batch[\'batch_size\'])\n\n        else:\n            batch[\'time_steps\'] = batch[\'advantage\'].shape[0]  # real non-padded time length\n            batch[\'batch_size\'] = 1  # want rollout as a trajectory\n\n        if size is not None and not time_flat and batch[\'advantage\'].shape[0] != size:\n            # Want all batches to be exact size for further batch stacking:\n            batch = batch_pad(batch, to_size=size)\n\n        return batch\n\n    def process_rp(self, reward_threshold=0.1):\n        """"""\n        Processes rollout process()-alike and estimates reward prediction target for first n-1 frames.\n\n        Args:\n            reward_threshold:   reward values such as |r|> reward_threshold are classified as neg. or pos.\n\n        Returns:\n            Processed batch with size reduced by one and with extra `rp_target` key\n            holding one hot encodings for classes {zero, positive, negative}.\n        """"""\n\n        # Remove last frame:\n        last_frame = self.pop_frame(-1)\n\n        batch = self.process(gamma=1)\n\n        # Make one hot vector for target rewards (i.e. reward taken from last of sampled frames):\n        r = last_frame[\'reward\']\n        rp_t = np.zeros(3)\n        if r > reward_threshold:\n            rp_t[1] = 1.0  # positive [010]\n\n        elif r < - reward_threshold:\n            rp_t[2] = 1.0  # negative [001]\n\n        else:\n            rp_t[0] = 1.0  # zero [100]\n\n        batch[\'rp_target\'] = rp_t[None,...]\n        batch[\'time_steps\'] = batch[\'advantage\'].shape[0]  # e.g -1 of original\n\n        return batch\n\n    def get_frame(self, idx, _struct=None):\n        """"""\n        Extracts single experience from rollout.\n\n        Args:\n            idx:    experience position\n\n        Returns:\n            frame as [nested] dictionary\n        """"""\n        # No idx range checks here!\n        if _struct is None:\n            _struct = self\n\n        if isinstance(_struct, dict) or type(_struct) == type(self):\n            frame = {}\n            for key, value in _struct.items():\n                frame[key] = self.get_frame(idx, value)\n            return frame\n\n        elif isinstance(_struct, tuple):\n            return tuple([self.get_frame(idx, value) for value in _struct])\n\n        elif isinstance(_struct, LSTMStateTuple):\n            return LSTMStateTuple(self.get_frame(idx, _struct[0]), self.get_frame(idx, _struct[1]))\n\n        else:\n            return _struct[idx]\n\n    def pop_frame(self, idx, _struct=None):\n        """"""\n        Pops single experience from rollout.\n\n        Args:\n            idx:    experience position\n\n        Returns:\n            frame as [nested] dictionary\n        """"""\n        # No idx range checks here!\n        if _struct is None:\n            _struct = self\n\n        if isinstance(_struct, dict) or type(_struct) == type(self):\n            frame = {}\n            for key, value in _struct.items():\n                frame[key] = self.pop_frame(idx, value)\n            return frame\n\n        elif isinstance(_struct, tuple):\n            return tuple([self.pop_frame(idx, value) for value in _struct])\n\n        elif isinstance(_struct, LSTMStateTuple):\n            return LSTMStateTuple(self.pop_frame(idx, _struct[0]), self.pop_frame(idx, _struct[1]))\n\n        else:\n            return _struct.pop(idx)\n\n    def as_array(self, struct, squeeze_axis=None):\n        if isinstance(struct, dict):\n            out = {}\n            for key, value in struct.items():\n                out[key] = self.as_array(value, squeeze_axis)\n            return out\n\n        elif isinstance(struct, tuple):\n            return tuple([self.as_array(value, squeeze_axis) for value in struct])\n\n        elif isinstance(struct, LSTMStateTuple):\n            return LSTMStateTuple(self.as_array(struct[0], squeeze_axis), self.as_array(struct[1], squeeze_axis))\n\n        else:\n            if squeeze_axis is not None:\n                return np.squeeze(np.asarray(struct), axis=squeeze_axis)\n\n            else:\n                return np.asarray(struct)\n\n    def _check_it(self, _struct=None):\n        if _struct is None:\n            _struct = self\n        if type(_struct) == dict or type(_struct) == type(self):\n            for key, value in _struct.items():\n                print(key, \':\')\n                self._check_it(_struct=value)\n\n        elif type(_struct) == tuple or type(_struct) == list:\n            print(\'tuple/list:\')\n            for value in _struct:\n                self._check_it(_struct=value)\n\n        else:\n            try:\n                print(\'length: {}, type: {}, shape of element: {}\\n\'.format(len(_struct), type(_struct[0]), _struct[0].shape))\n            except:\n                print(\'length: {}, type: {}\\n\'.format(len(_struct), type(_struct[0])))\n'"
btgym/algorithms/test.py,0,"b'import gym\nfrom gym import error, spaces\nimport numpy as np\n\n\nclass test_env(gym.Env):\n    """"""\n    Simple atari-like tester environment for checking a3c output consistency.\n    Use with a3c launcher configurator:\n\n        cluster_config = dict(\n        host=\'127.0.0.1\',\n        port=42222,\n        num_workers=8,\n        num_ps=1,\n        log_dir=\'./tmp/a3c_testing_\',\n        )\n\n        env_config = dict(gym_id=\'test-v01\')\n        launcher = Launcher(\n            cluster_config=cluster_config,\n            env_config=env_config,\n            train_steps=500,\n            opt_learn_rate=1e-4,\n            rollout_length=20,\n            test_mode=True,\n            model_summary_freq=50,\n            episode_summary_freq=2,\n            env_render_freq=10,\n            verbose=1\n        )\n\n    """"""\n    def __init__(self):\n        # self.metadata = {}\n        self.observation_space = spaces.Box(shape=(100, 100, 3), low=0, high=10, )\n        self.action_space = spaces.Discrete(6)\n        self.ep_step = 0\n\n    def _reset(self):\n        self.ep_step = 0\n        return np.zeros(self.observation_space.shape)\n\n    def _step(self, action):\n        if self.ep_step >= 10: # max episode length: 10\n            done = True\n\n        else:\n            done = False\n\n        o = np.ones(self.observation_space.shape) * self.ep_step / 10\n        r = self.ep_step / 100\n        self.ep_step += 1\n\n        return (o, r, done, \'==test_env==\')\n\n'"
btgym/algorithms/utils.py,10,"b'\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.util.nest import flatten as flatten_nested\nfrom tensorflow.python.util.nest import assert_same_structure\nfrom tensorflow.contrib.rnn import LSTMStateTuple\n\nfrom gym.spaces import Discrete, Dict\n\nfrom itertools import product\n\n\ndef rnn_placeholders(state):\n    """"""\n    Given nested [multilayer] RNN state tensor, infers and returns state placeholders.\n\n    Args:\n        state:  tf.nn.lstm zero-state tuple.\n\n    Returns:    tuple of placeholders\n    """"""\n    if isinstance(state, tf.contrib.rnn.LSTMStateTuple):\n        c, h = state\n        c = tf.placeholder(tf.float32, tf.TensorShape([None]).concatenate(c.get_shape()[1:]), c.op.name + \'_c_pl\')\n        h = tf.placeholder(tf.float32, tf.TensorShape([None]).concatenate(h.get_shape()[1:]), h.op.name + \'_h_pl\')\n        return tf.contrib.rnn.LSTMStateTuple(c, h)\n    elif isinstance(state, tf.Tensor):\n        h = state\n        h = tf.placeholder(tf.float32, tf.TensorShape([None]).concatenate(h.get_shape()[1:]), h.op.name + \'_h_pl\')\n        return h\n    else:\n        structure = [rnn_placeholders(x) for x in state]\n        return tuple(structure)\n\n\ndef nested_placeholders(ob_space, batch_dim=None, name=\'nested\'):\n    """"""\n    Given nested observation space as dictionary of shape tuples,\n    returns nested state batch-wise placeholders.\n\n    Args:\n        ob_space:   [nested] dict of shapes\n        name:       name scope\n        batch_dim:  batch dimension\n    Returns:\n        nested dictionary of placeholders\n    """"""\n    if isinstance(ob_space, dict):\n        out = {key: nested_placeholders(value, batch_dim, name + \'_\' + key) for key, value in ob_space.items()}\n        return out\n    else:\n        out = tf.placeholder(tf.float32, [batch_dim] + list(ob_space), name + \'_pl\')\n        return out\n\n\ndef nested_discrete_gym_shape(ac_space):\n    """"""\n    Given instance of gym.spaces.Dict holding base  gym.spaces.Discrete,\n    returns nested dictionary of  spaces depths ( =dict of gym.spaces.Discrete.n)\n    This util is here due to fact in practice we need .n attr of discrete space [as cat. encoding depth]\n     rather than .shape, which is always ()\n\n    Args:\n        ac_space: instance of gym.spaces.Dict\n\n    Returns:\n        nested dictionary of lengths\n    """"""\n    if isinstance(ac_space, Dict):\n        return {key: nested_discrete_gym_shape(space) for key, space in ac_space.spaces.items()}\n\n    elif isinstance(ac_space, Discrete):\n        return (ac_space.n,)\n\n    else:\n        raise TypeError(\'Expected gym.spaces.Dict or gym.spaces.Discrete, got: {}\'.format(ac_space))\n\n\ndef flat_placeholders(ob_space, batch_dim=None, name=\'flt\'):\n    """"""\n    Given nested observation space as dictionary of shape tuples,\n    returns flattened dictionary of batch-wise placeholders.\n\n    Args:\n        ob_space:   [nested dict] of tuples\n        name:       name_scope\n        batch_dim:  batch dimension\n    Returns:\n        flat dictionary of tf.placeholders\n    """"""\n    return flatten_nested(nested_placeholders(ob_space, batch_dim=batch_dim, name=name))\n\n\ndef feed_dict_from_nested(placeholder, value, expand_batch=False):\n    """"""\n    Zips flat feed dictionary form nested dictionaries of placeholders and values.\n\n    Args:\n        placeholder:    nested dictionary of placeholders\n        value:          nested dictionary of values\n        expand_batch:   if true - add fake batch dimension to values\n\n    Returns:\n        flat feed_dict\n    """"""\n    assert_same_structure(placeholder, value, check_types=True)\n    return _flat_from_nested(placeholder, value, expand_batch)\n\n\ndef _flat_from_nested(placeholder, value, expand_batch):\n    feed_dict = {}\n    if isinstance(placeholder, dict):\n        for key in placeholder.keys():\n            feed_dict.update(_flat_from_nested(placeholder[key], value[key], expand_batch))\n\n    else:\n        if expand_batch:\n            feed_dict.update({placeholder: [value]})\n\n        else:\n            feed_dict.update({placeholder: value})\n\n    return feed_dict\n\n\ndef feed_dict_rnn_context(placeholders, values):\n    """"""\n    Creates tf.feed_dict for flat placeholders and nested values.\n\n    Args:\n        placeholders:       flat structure of placeholders\n        values:             nested structure of values\n\n    Returns:\n        flat feed dictionary\n    """"""\n    return {key: value for key, value in zip(placeholders, flatten_nested(values))}\n\n\ndef as_array(struct):\n    """"""\n    Given a dictionary of lists or tuples returns dictionary of np.arrays of same structure.\n\n    Args:\n        struct: dictionary of list, tuples etc.\n\n    Returns:\n        dict of np.arrays\n    """"""\n    if isinstance(struct,dict):\n        out = {}\n        for key, value in struct.items():\n            out[key] = as_array(value)\n        return out\n\n    else:\n        return np.asarray(struct)\n\n\ndef batch_stack(dict_list, _top=True):\n    """"""\n    Stacks values of given processed rollouts along batch dimension.\n    Cumulative batch dimension is saved as key \'batch_size\' for further shape inference.\n\n    Example:\n        dict_list sizes: [[20,10,10,1], [20,10,10,1]] --> result size: [40,10,10,1],\n        result[\'rnn_batch_size\'] = 20\n\n    Args:\n        dict_list:   list of processed rollouts of the same size.\n\n    Returns:\n        dictionary of stacked arrays.\n    """"""\n    master = dict_list[0]\n    batch = {}\n\n    if isinstance(master, dict):\n        for key in master.keys():\n            value_list = [value[key] for value in dict_list]\n            batch[key] = batch_stack(value_list, False)\n\n    elif isinstance(master, LSTMStateTuple):\n        c = batch_stack([state[0] for state in dict_list], False)\n        h = batch_stack([state[1] for state in dict_list], False)\n        batch = LSTMStateTuple(c=c, h=h)\n\n    elif isinstance(master, tuple):\n        batch = tuple([batch_stack([struct[i] for struct in dict_list], False) for i in range(len(master))])\n\n    else:\n        try:\n            batch = np.concatenate(dict_list, axis=0)\n\n        except ValueError:\n            batch = np.stack(dict_list, axis=0)\n    if _top:\n        # Mind shape inference:\n        batch[\'batch_size\'] = batch[\'batch_size\'].sum()\n        \n    return batch\n\n\ndef batch_gather(batch_dict, indices, _top=True):\n    """"""\n    Gathers experiences from processed batch according to specified indices.\n\n    Args:\n        batch_dict:     batched data dictionary\n        indices:        array-like, indices to gather\n        _top:           internal\n\n    Returns:\n        batched data of same structure as dict\n\n    """"""\n    batch = {}\n\n    if isinstance(batch_dict, dict):\n        for key, value in batch_dict.items():\n            batch[key] = batch_gather(value, indices, False)\n\n    elif isinstance(batch_dict, LSTMStateTuple):\n        c = batch_gather(batch_dict[0], indices, False)\n        h = batch_gather(batch_dict[1], indices, False)\n        batch = LSTMStateTuple(c=c, h=h)\n\n    elif isinstance(batch_dict, tuple):\n        batch = tuple([batch_gather(struct, indices, False) for struct in batch_dict])\n\n    else:\n        batch = np.take(batch_dict, indices=indices, axis=0, mode=\'wrap\')\n\n    if _top:\n        # Mind shape inference:\n        batch[\'batch_size\'] = indices.shape[0]\n\n    return batch\n\n\ndef batch_pad(batch, to_size, _one_hot=False):\n    """"""\n    Pads given `batch` with zeros along zero dimension\n\n    Args:\n        batch:      processed rollout as dictionary of np.arrays\n        to_size:    desired batch size\n\n    Returns:\n        dictionary with all included np.arrays being zero-padded to size [to_size, own_depth].\n    """"""\n    if isinstance(batch, dict):\n        padded_batch = {}\n        for key, struct in batch.items():\n            # Mind one-hot action encoding:\n            if key in [\'action\', \'last_action_reward\']:\n                one_hot = True\n\n            else:\n                one_hot = False\n\n            padded_batch[key] = batch_pad(struct, to_size, one_hot)\n\n    elif isinstance(batch, np.ndarray):\n        shape = batch.shape\n        assert shape[0] < to_size, \\\n            \'Padded batch size must be greater than initial, got: {}, {}\'.format(to_size, shape[0])\n\n        pad = np.zeros((to_size - shape[0],) + shape[1:])\n        if _one_hot:\n            pad[:, 0, ...] = 1\n        padded_batch = np.concatenate([batch, pad], axis=0)\n\n    else:\n        # Hit tuple, scalar or something else:\n        padded_batch = batch\n\n    return padded_batch\n\n\ndef is_subdict(sub_dict, big_dict):\n    """"""\n    Checks if first arg is sub_dictionary of second arg by means of structure and values.\n\n    Args:\n        sub_dict:       dictionary\n        big_dict:       dictionary\n\n    Returns:\n        bool\n    """"""\n    conditions = []\n    if isinstance(sub_dict, dict):\n        for key, value in sub_dict.items():\n            try:\n                conditions.append(is_subdict(value, big_dict[key]))\n            except KeyError:\n                conditions.append(False)\n    else:\n        try:\n            conditions.append(sub_dict == big_dict)\n        except KeyError:\n            conditions.append(False)\n\n    return np.asarray(conditions).all()\n\n\ndef _show_struct(struct):\n    # Debug utility\n    if isinstance(struct, dict):\n        for key, value in struct.items():\n            print(key)\n            _show_struct(value)\n\n    elif type(struct) in [LSTMStateTuple, tuple, list]:\n        print(\'LSTM/tuple/list:\', type(struct), len(struct))\n        for i in struct:\n            _show_struct(i)\n\n    else:\n        try:\n            print(\'shape: {}, type: {}\'.format(np.asarray(struct).shape, type(struct)))\n\n        except AttributeError:\n            print(\'value:\', struct)\n'"
btgym/algorithms/worker.py,24,"b'#\n# Original A3C code comes from OpenAI repository under MIT licence:\n# https://github.com/openai/universe-starter-agent\n#\n# Papers:\n# https://arxiv.org/abs/1602.01783\n# https://arxiv.org/abs/1611.05397\n\nfrom logbook import Logger, StreamHandler\nimport sys\nimport os\nimport random\nimport multiprocessing\nimport datetime\n\nimport tensorflow as tf\n\nsys.path.insert(0, \'..\')\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nclass FastSaver(tf.train.Saver):\n    """"""\n    Disables write_meta_graph argument,\n    which freezes entire process and is mostly useless.\n    """"""\n    def save(\n        self,\n        sess,\n        save_path,\n        global_step=None,\n        latest_filename=None,\n        meta_graph_suffix=""meta"",\n        write_meta_graph=True,\n        write_state=True,\n        strip_default_attrs=False\n    ):\n        super(FastSaver, self).save(\n            sess,\n            save_path,\n            global_step,\n            latest_filename,\n            meta_graph_suffix,\n            write_meta_graph=False,\n        )\n\n\nclass Worker(multiprocessing.Process):\n    """"""\n    Distributed tf worker class.\n\n    Sets up environment, trainer and starts training process in supervised session.\n    """"""\n    env_list = None\n\n    def __init__(self,\n                 env_config,\n                 policy_config,\n                 trainer_config,\n                 cluster_spec,\n                 job_name,\n                 task,\n                 log_dir,\n                 log_ckpt_subdir,\n                 initial_ckpt_dir,\n                 save_secs,\n                 log_level,\n                 max_env_steps,\n                 random_seed=None,\n                 render_last_env=True,\n                 test_mode=False):\n        """"""\n\n        Args:\n            env_config:             environment class_config_dict.\n            policy_config:          model policy estimator class_config_dict.\n            trainer_config:         algorithm class_config_dict.\n            cluster_spec:           tf.cluster specification.\n            job_name:               worker or parameter server.\n            task:                   integer number, 0 is chief worker.\n            log_dir:                path for tb summaries and current checkpoints.\n            log_ckpt_subdir:        log_dir subdirectory to store current checkpoints\n            initial_ckpt_dir:       path for checkpoint to load as pre-trained model.\n            save_secs:              int, save model checkpoint every N secs.\n            log_level:              int, logbook.level\n            max_env_steps:          number of environment steps to run training on\n            random_seed:            int or None\n            render_last_env:        bool, if True and there is more than one environment specified for each worker,\n                                    only allows rendering for last environment in a list;\n                                    allows rendering for all environments of a chief worker otherwise;\n            test_mode:              if True - use Atari mode, BTGym otherwise.\n\n            Note:\n                - Conventional `self.global_step` refers to number of environment steps,\n                    summarized over all environment instances, not to number of policy optimizer train steps.\n\n                - Every worker can run several environments in parralell, as specified by `cluster_config\'[\'num_envs\'].\n                    If use 4 forkers and num_envs=4 => total number of environments is 16. Every env instance has\n                    it\'s own ThreadRunner process.\n\n                - When using replay memory, keep in mind that every ThreadRunner is keeping it\'s own replay memory,\n                    If memory_size = 2000, num_workers=4, num_envs=4 => total replay memory size equals 32 000 frames.\n        """"""\n        super(Worker, self).__init__()\n        self.env_class = env_config[\'class_ref\']\n        self.env_kwargs = env_config[\'kwargs\']\n        self.policy_config = policy_config\n        self.trainer_class = trainer_config[\'class_ref\']\n        self.trainer_kwargs = trainer_config[\'kwargs\']\n        self.cluster_spec = cluster_spec\n        self.job_name = job_name\n        self.task = task\n        self.is_chief = (self.task == 0)\n        self.log_dir = log_dir\n        self.save_secs = save_secs\n        self.max_env_steps = max_env_steps\n        self.log_level = log_level\n        self.log = None\n        self.test_mode = test_mode\n        self.random_seed = random_seed\n        self.render_last_env = render_last_env\n\n        # Saver and summaries path:\n        self.current_ckpt_dir = self.log_dir + log_ckpt_subdir\n        self.initial_ckpt_dir = initial_ckpt_dir\n        self.summary_dir = self.log_dir + \'/worker_{}\'.format(self.task)\n\n        # print(log_ckpt_subdir)\n        # print(self.log_dir)\n        # print(self.current_ckpt_dir)\n        # print(self.initial_ckpt_dir)\n        # print(self.summary_dir)\n\n        self.summary_writer = None\n        self.config = None\n        self.saver = None\n\n    def _restore_model_params(self, sess, save_path):\n        """"""\n        Restores model parameters from specified location.\n\n        Args:\n            sess:       tf.Session obj.\n            save_path:  path where parameters were previously saved.\n\n        Returns: True if model has been successfully loaded, False otherwise.\n        """"""\n        if save_path is None:\n            return False\n\n        assert self.saver is not None, \'FastSaver has not been configured.\'\n\n        try:\n            # Look for valid checkpoint:\n            ckpt_state = tf.train.get_checkpoint_state(save_path)\n            if ckpt_state is not None and ckpt_state.model_checkpoint_path:\n                self.saver.restore(sess, ckpt_state.model_checkpoint_path)\n\n            else:\n                self.log.notice(\'no saved model parameters found in:\\n{}\'.format(save_path))\n                return False\n\n        except (ValueError, tf.errors.NotFoundError, tf.errors.InvalidArgumentError) as e:\n            self.log.notice(\'failed to restore model parameters from:\\n{}\'.format(save_path))\n            return False\n\n        return True\n\n    def _save_model_params(self, sess, global_step):\n        """"""\n        Saves model checkpoint to predefined location.\n\n        Args:\n            sess:           tf.Session obj.\n            global_step:    global step number is appended to save_path to create the checkpoint filenames\n        """"""\n        assert self.saver is not None, \'FastSaver has not been configured.\'\n        self.saver.save(\n            sess,\n            save_path=self.current_ckpt_dir + \'/model_parameters\',\n            global_step=global_step\n        )\n\n    def run(self):\n        """"""Worker runtime body.\n        """"""\n        # Logging:\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'Worker_{}\'.format(self.task), level=self.log_level)\n        try:\n            tf.reset_default_graph()\n\n            if self.test_mode:\n                import gym\n\n            # Define cluster:\n            cluster = tf.train.ClusterSpec(self.cluster_spec).as_cluster_def()\n\n            # Start tf.server:\n            if self.job_name in \'ps\':\n                server = tf.train.Server(\n                    cluster,\n                    job_name=self.job_name,\n                    task_index=self.task,\n                    config=tf.ConfigProto(device_filters=[""/job:ps""])\n                )\n                self.log.debug(\'parameters_server started.\')\n                # Just block here:\n                server.join()\n\n            else:\n                server = tf.train.Server(\n                    cluster,\n                    job_name=\'worker\',\n                    task_index=self.task,\n                    config=tf.ConfigProto(\n                        intra_op_parallelism_threads=4,  # original was: 1\n                        inter_op_parallelism_threads=4,  # original was: 2\n                    )\n                )\n                self.log.debug(\'tf.server started.\')\n\n                self.log.debug(\'making environments:\')\n                # Making as many environments as many entries in env_config `port` list:\n                # TODO: Hacky-II: only one example over all parallel environments can be data-master [and renderer]\n                # TODO: measure data_server lags, maybe launch several instances\n                self.env_list = []\n                env_kwargs = self.env_kwargs.copy()\n                env_kwargs[\'log_level\'] = self.log_level\n                port_list = env_kwargs.pop(\'port\')\n                data_port_list = env_kwargs.pop(\'data_port\')\n                data_master = env_kwargs.pop(\'data_master\')\n                render_enabled = env_kwargs.pop(\'render_enabled\')\n\n                render_list = [False for entry in port_list]\n                if render_enabled:\n                    if self.render_last_env:\n                        render_list[-1] = True\n                    else:\n                        render_list = [True for entry in port_list]\n                        # render_list[0] = True\n\n                data_master_list = [False for entry in port_list]\n                if data_master:\n                    data_master_list[0] = True\n\n                # Parallel envs. numbering:\n                if len(port_list) > 1:\n                    task_id = 0.0\n                else:\n                    task_id = 0\n\n                for port, data_port, is_render, is_master in zip(port_list, data_port_list, render_list, data_master_list):\n                    # Get random seed for environments:\n                    env_kwargs[\'random_seed\'] = random.randint(0, 2 ** 30)\n\n                    if not self.test_mode:\n                        # Assume BTgym env. class:\n                        self.log.debug(\'setting env at port_{} is data_master: {}\'.format(port, data_master))\n                        self.log.debug(\'env_kwargs:\')\n                        for k, v in env_kwargs.items():\n                            self.log.debug(\'{}: {}\'.format(k, v))\n                        try:\n                            self.env_list.append(\n                                self.env_class(\n                                    port=port,\n                                    data_port=data_port,\n                                    data_master=is_master,\n                                    render_enabled=is_render,\n                                    task=self.task + task_id,\n                                    **env_kwargs\n                                )\n                            )\n                            data_master = False\n                            self.log.info(\'set BTGym environment {} @ port:{}, data_port:{}\'.\n                                          format(self.task + task_id, port, data_port))\n                            task_id += 0.01\n\n                        except Exception as e:\n                            self.log.exception(\n                                \'failed to make BTGym environment at port_{}.\'.format(port)\n                            )\n                            raise e\n\n                    else:\n                        # Assume atari testing:\n                        try:\n                            self.env_list.append(self.env_class(env_kwargs[\'gym_id\']))\n                            self.log.debug(\'set Gyn/Atari environment.\')\n\n                        except Exception as e:\n                            self.log.exception(\'failed to make Gym/Atari environment\')\n                            raise e\n\n                self.log.debug(\'Defining trainer...\')\n\n                # Define trainer:\n                trainer = self.trainer_class(\n                    env=self.env_list,\n                    task=self.task,\n                    policy_config=self.policy_config,\n                    log_level=self.log_level,\n                    cluster_spec=self.cluster_spec,\n                    random_seed=self.random_seed,\n                    **self.trainer_kwargs,\n                )\n\n                self.log.debug(\'trainer ok.\')\n\n                # Saver-related:\n                variables_to_save = [v for v in tf.global_variables() if not \'local\' in v.name]\n                local_variables = [v for v in tf.global_variables() if \'local\' in v.name] + tf.local_variables()\n                init_op = tf.initializers.variables(variables_to_save)\n                local_init_op = tf.initializers.variables(local_variables)\n                init_all_op = tf.global_variables_initializer()\n\n                def init_fn(_sess):\n                    self.log.notice(""initializing all parameters..."")\n                    _sess.run(init_all_op)\n\n                # def init_fn_scaff(scaffold, _sess):\n                #     self.log.notice(""initializing all parameters..."")\n                #     _sess.run(init_all_op)\n\n                # self.log.warning(\'VARIABLES TO SAVE:\')\n                # for v in variables_to_save:\n                #     self.log.warning(v)\n                #\n                # self.log.warning(\'LOCAL VARS:\')\n                # for v in local_variables:\n                #     self.log.warning(v)\n\n                self.saver = FastSaver(var_list=variables_to_save, max_to_keep=1, save_relative_paths=True)\n\n                self.config = tf.ConfigProto(device_filters=[""/job:ps"", ""/job:worker/task:{}/cpu:0"".format(self.task)])\n\n                sess_manager = tf.train.SessionManager(\n                    local_init_op=local_init_op,\n                    ready_op=None,\n                    ready_for_local_init_op=tf.report_uninitialized_variables(variables_to_save),\n                    graph=None,\n                    recovery_wait_secs=90,\n                )\n                with sess_manager.prepare_session(\n                    master=server.target,\n                    init_op=init_op,\n                    config=self.config,\n                    init_fn=init_fn,\n                ) as sess:\n\n                    # Try to restore pre-trained model\n                    pre_trained_restored = self._restore_model_params(sess, self.initial_ckpt_dir)\n                    _ = sess.run(trainer.reset_global_step)\n\n                    if not pre_trained_restored:\n                        # If not - try to recover current checkpoint:\n                        current_restored = self._restore_model_params(sess, self.current_ckpt_dir)\n\n                    else:\n                        current_restored = False\n\n                    if not pre_trained_restored and not current_restored:\n                        self.log.notice(\'training from scratch...\')\n\n                    self.log.info(""connecting to the parameter server... "")\n\n                    self.summary_writer = tf.summary.FileWriter(self.summary_dir, sess.graph)\n                    trainer.start(sess, self.summary_writer)\n\n                    # Note: `self.global_step` refers to number of environment steps\n                    # summarized over all environment instances, not to number of policy optimizer train steps.\n                    global_step = sess.run(trainer.global_step)\n                    self.log.notice(""started training at step: {}"".format(global_step))\n\n                    last_saved_time = datetime.datetime.now()\n                    last_saved_step = global_step\n\n                    while global_step < self.max_env_steps:\n                        trainer.process(sess)\n                        global_step = sess.run(trainer.global_step)\n\n                        time_delta = datetime.datetime.now() - last_saved_time\n                        if self.is_chief and time_delta.total_seconds() > self.save_secs:\n                            self._save_model_params(sess, global_step)\n                            train_speed = (global_step - last_saved_step) / (time_delta.total_seconds() + 1)\n                            self.log.notice(\n                                \'env. step: {}; cluster speed: {:.0f} step/sec; checkpoint saved.\'.format(\n                                    global_step,\n                                    train_speed\n                                )\n                            )\n                            last_saved_time = datetime.datetime.now()\n                            last_saved_step = global_step\n\n                # Ask for all the services to stop:\n                for env in self.env_list:\n                    env.close()\n\n                self.log.notice(\'reached {} steps, exiting.\'.format(global_step))\n\n        except Exception as e:\n            self.log.exception(e)\n            raise e\n\n\n\n'"
btgym/datafeed/__init__.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\n\nfrom .base import BTgymBaseData, DataSampleConfig, EnvResetConfig\nfrom .derivative import BTgymEpisode, BTgymDataTrial, BTgymRandomDataDomain, BTgymDataset\nfrom .stateful import BTgymSequentialDataDomain\n'"
btgym/datafeed/base.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom logbook import Logger, StreamHandler, WARNING\n\nimport datetime\nimport random\nfrom numpy.random import beta as random_beta\nimport copy\nimport os\nimport sys\n\nfrom backtrader import TimeFrame\nimport backtrader.feeds as btfeeds\nimport pandas as pd\n\nDataSampleConfig = dict(\n    get_new=True,\n    sample_type=0,\n    timestamp=None,\n    b_alpha=1,\n    b_beta=1\n)\n""""""\ndict: Conventional sampling configuration template to pass to data class `sample()` method:\n\n```sample = my_data.sample(**DataSampleConfig)```\n""""""\n\n\nEnvResetConfig = dict(\n    episode_config=copy.deepcopy(DataSampleConfig),\n    trial_config=copy.deepcopy(DataSampleConfig),\n)\n""""""\ndict: Conventional reset configuration template to pass to environment `reset()` method:\n\n```observation = env.reset(**EnvResetConfig)```\n""""""\n\n\nclass BTgymBaseData:\n    """"""\n    Base BTgym data provider class.\n    Provides core data loading, sampling, splitting  and converting functionality.\n    Do not use directly.\n\n    Enables Pipe::\n\n        CSV[source data]-->pandas[for efficient sampling]-->bt.feeds\n\n    """"""\n\n    def __init__(\n            self,\n            filename=None,\n            dataframe=None,\n            parsing_params=None,\n            sampling_params=None,\n            name=\'base_data\',\n            data_names=(\'default_asset\',),\n            task=0,\n            frozen_time_split=None,\n            log_level=WARNING,\n            _config_stack=None,\n            **kwargs\n    ):\n        """"""\n        Args:\n\n            filename:                       Str or iterable of of str, filenames holding csv data;\n                                            should be given either here or when calling read_csv(), see `Notes`\n            dataframe:                      pd.dataframe holding data, if this arg is given - overrides ``filename` arg.\n\n            specific_params CSV to Pandas parsing\n\n            sep:                            \';\'\n            header:                         0\n            index_col:                      0\n            parse_dates:                    True\n            names:                          [\'open\', \'high\', \'low\', \'close\', \'volume\']\n\n            specific_params Pandas to BT.feeds conversion\n\n            timeframe=1:                    1 minute.\n            datetime:                       0\n            open:                           1\n            high:                           2\n            low:                            3\n            close:                          4\n            volume:                         -1\n            openinterest:                   -1\n\n            specific_params Sampling\n\n            sample_class_ref:               None - if not None, than sample() method will return instance of specified\n                                            class, which itself must be subclass of BaseBTgymDataset,\n                                            else returns instance of the base data class.\n\n            start_weekdays:                 [0, 1, 2, 3, ] - Only weekdays from the list will be used for sample start.\n            start_00:                       True - sample start time will be set to first record of the day\n                                            (usually 00:00).\n            sample_duration:                {\'days\': 1, \'hours\': 23, \'minutes\': 55} - Maximum sample time duration\n                                            in days, hours, minutes\n            time_gap:                       {\'\'days\': 0, hours\': 5, \'minutes\': 0} - Data omittance threshold:\n                                            maximum no-data time gap allowed within sample in days, hours.\n                                            Thereby, if set to be < 1 day, samples containing weekends and holidays gaps\n                                            will be rejected.\n            test_period:                    {\'days\': 0, \'hours\': 0, \'minutes\': 0} - setting this param to non-zero\n                                            duration forces instance.data split to train / test subsets with test\n                                            subset duration equal to `test_period` with `time_gap` tolerance. Train data\n                                            always precedes test one:\n                                            [0_record<-train_data->split_point_record<-test_data->last_record].\n            sample_expanding:               None, reserved for child classes.\n\n        Note:\n            - CSV file can contain duplicate records, checks will be performed and all duplicates will be removed;\n\n            - CSV file should be properly sorted by date_time in ascending order, no sorting checks performed.\n\n            - When supplying list of file_names, all files should be also listed ascending by their time period,\n              no correct sampling will be possible otherwise.\n\n            - Default parameters are source-specific and made to correctly parse 1 minute Forex generic ASCII\n              data files from www.HistData.com. Tune according to your data source.\n        """"""\n        self.filename = filename\n\n        if parsing_params is None:\n            self.parsing_params = dict(\n                # Default parameters for source-specific CSV datafeed class,\n                # correctly parses 1 minute Forex generic ASCII\n                # data files from www.HistData.com:\n\n                # CSV to Pandas params.\n                sep=\';\',\n                header=0,\n                index_col=0,\n                parse_dates=True,\n                names=[\'open\', \'high\', \'low\', \'close\', \'volume\'],\n\n                # Pandas to BT.feeds params:\n                timeframe=1,  # 1 minute.\n                datetime=0,\n                open=1,\n                high=2,\n                low=3,\n                close=4,\n                volume=-1,\n                openinterest=-1,\n            )\n        else:\n            self.parsing_params = parsing_params\n\n        if sampling_params is None:\n            self.sampling_params = dict(\n                # Sampling params:\n                start_weekdays=[],  # Only weekdays from the list will be used for episode start.\n                start_00=False,  # Sample start time will be set to first record of the day (usually 00:00).\n                sample_duration=dict(  # Maximum sample time duration in days, hours, minutes:\n                    days=0,\n                    hours=0,\n                    minutes=0\n                ),\n                time_gap=dict(  # Maximum data time gap allowed within sample in days, hours. Thereby,\n                    days=0,  # if set to be < 1 day, samples containing weekends and holidays gaps will be rejected.\n                    hours=0,\n                ),\n                test_period=dict(  # Time period to take test samples from, in days, hours, minutes:\n                    days=0,\n                    hours=0,\n                    minutes=0\n                ),\n                expanding=False,\n            )\n        else:\n            self.sampling_params = sampling_params\n\n        self.name = name\n        # String will be used as key name for bt_feed data-line:\n\n        self.task = task\n        self.log_level = log_level\n        self.data_names = data_names\n        self.data_name = self.data_names[0]\n\n        if dataframe is not None:\n            self.data = dataframe\n\n        else:\n            self.data = None  # will hold actual data as pandas dataframe\n\n        self.is_ready = False\n\n        self.global_timestamp = 0\n        self.start_timestamp = 0\n        self.final_timestamp = 0\n\n        self.data_stat = None  # Dataset descriptive statistic as pandas dataframe\n        self.data_range_delta = None  # Dataset total duration timedelta\n        self.max_time_gap = None\n        self.time_gap = None\n        self.max_sample_len_delta = None\n        self.sample_duration = None\n        self.sample_num_records = 0\n        self.start_weekdays = {0, 1, 2, 3, 4, 5, 6}\n        self.start_00 = False\n        self.expanding = False\n\n        self.sample_instance = None\n\n        self.test_range_delta = None\n        self.train_range_delta = None\n        self.test_num_records = 0\n        self.train_num_records = 0\n        self.total_num_records = 0\n        self.train_interval = [0, 0]\n        self.test_interval = [0, 0]\n        self.test_period = {\'days\': 0, \'hours\': 0, \'minutes\': 0}\n        self.train_period = {\'days\': 0, \'hours\': 0, \'minutes\': 0}\n        self._test_period_backshift_delta = datetime.timedelta(**{\'days\': 0, \'hours\': 0, \'minutes\': 0})\n        self.sample_num = 0\n        self.task = 0\n        self.metadata = {\'sample_num\': 0, \'type\': None}\n\n        self.set_params(self.parsing_params)\n        self.set_params(self.sampling_params)\n\n        self._config_stack = copy.deepcopy(_config_stack)\n        try:\n            nested_config = self._config_stack.pop()\n\n        except (IndexError, AttributeError) as e:\n            # IF stack is empty, sample of this instance itself is not supposed to be sampled.\n            nested_config = dict(\n                class_ref=None,\n                kwargs=dict(\n                    parsing_params=self.parsing_params,\n                    sample_params=None,\n                    name=\'data_stream\',\n                    task=self.task,\n                    log_level=self.log_level,\n                    _config_stack=None,\n                )\n            )\n        # Configure sample instance parameters:\n        self.nested_class_ref = nested_config[\'class_ref\']\n        self.nested_params = nested_config[\'kwargs\']\n        self.sample_name = \'{}_w_{}_\'.format(self.nested_params[\'name\'], self.task)\n        self.nested_params[\'_config_stack\'] = self._config_stack\n\n        # Logging:\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'{}_{}\'.format(self.name, self.task), level=self.log_level)\n\n        # Legacy parameter dictionary, left here for BTgym API_shell:\n        self.params = {}\n        self.params.update(self.parsing_params)\n        self.params.update(self.sampling_params)\n\n        if frozen_time_split is not None:\n            self.frozen_time_split = datetime.datetime(**frozen_time_split)\n\n        else:\n            self.frozen_time_split = None\n\n        self.frozen_split_timestamp = None\n\n    def set_params(self, params_dict):\n        """"""\n        Batch attribute setter.\n\n        Args:\n            params_dict: dictionary of parameters to be set as instance attributes.\n        """"""\n        for key, value in params_dict.items():\n            setattr(self, key, value)\n\n    def set_logger(self, level=None, task=None):\n        """"""\n        Sets logbook logger.\n\n        Args:\n            level:  logbook.level, int\n            task:   task id, int\n\n        """"""\n        if task is not None:\n            self.task = task\n\n        if level is not None:\n            self.log = Logger(\'{}_{}\'.format(self.name, self.task), level=level)\n\n    def set_global_timestamp(self, timestamp):\n        if self.data is not None:\n            self.global_timestamp = self.data.index[0].timestamp()\n\n    def reset(self, data_filename=None, **kwargs):\n        """"""\n        Gets instance ready.\n\n        Args:\n            data_filename:  [opt] string or list of strings.\n            kwargs:         not used.\n\n        """"""\n        self._reset(data_filename=data_filename, **kwargs)\n\n    def _reset(self, data_filename=None, timestamp=None, **kwargs):\n\n        self.read_csv(data_filename)\n\n        # Add global timepoints:\n        self.start_timestamp = self.data.index[0].timestamp()\n        self.final_timestamp = self.data.index[-1].timestamp()\n\n        if self.frozen_time_split is not None:\n            frozen_index = self.data.index.get_loc(self.frozen_time_split, method=\'ffill\')\n            self.frozen_split_timestamp = self.data.index[frozen_index].timestamp()\n            self.set_global_timestamp(self.frozen_split_timestamp)\n\n        else:\n            self.frozen_split_timestamp = None\n            self.set_global_timestamp(timestamp)\n\n        self.log.debug(\n            \'time stamps start: {}, current: {} final: {}\'.format(\n                self.start_timestamp,\n                self.global_timestamp,\n                self.final_timestamp\n            )\n        )\n\n        # Maximum data time gap allowed within sample as pydatetimedelta obj:\n        self.max_time_gap = datetime.timedelta(**self.time_gap)\n\n        # Max. gap number of records:\n        self.max_gap_num_records = int(self.max_time_gap.total_seconds() / (60 * self.timeframe))\n\n        # ... maximum episode time duration:\n        self.max_sample_len_delta = datetime.timedelta(**self.sample_duration)\n\n        # Maximum possible number of data records (rows) within episode:\n        self.sample_num_records = int(self.max_sample_len_delta.total_seconds() / (60 * self.timeframe))\n\n        self.backshift_num_records = round(self._test_period_backshift_delta.total_seconds() / (60 * self.timeframe))\n\n        # Train/test timedeltas:\n        if self.train_period is None or self.test_period == -1:\n            # No train data assumed, test only:\n            self.train_num_records = 0\n            self.test_num_records = self.data.shape[0] - self.backshift_num_records\n            break_point = self.backshift_num_records\n            self.train_interval = [0, 0]\n            self.test_interval = [self.backshift_num_records, self.data.shape[0]]\n\n        else:\n            # Train and maybe test data assumed:\n            if self.test_period is not None:\n                self.test_range_delta = datetime.timedelta(**self.test_period)\n                self.test_num_records = round(self.test_range_delta.total_seconds() / (60 * self.timeframe))\n                self.train_num_records = self.data.shape[0] - self.test_num_records\n                break_point = self.train_num_records\n                self.train_interval = [0, break_point]\n                self.test_interval = [break_point - self.backshift_num_records, self.data.shape[0]]\n            else:\n                self.test_num_records = 0\n                self.train_num_records = self.data.shape[0]\n                break_point = self.train_num_records\n                self.train_interval = [0, break_point]\n                self.test_interval = [0, 0]\n\n        if self.train_num_records > 0:\n            try:\n                assert self.train_num_records + self.max_gap_num_records >= self.sample_num_records\n\n            except AssertionError:\n                self.log.exception(\n                    \'Train subset should contain at least one sample, \' +\n                    \'got: train_set size: {} rows, sample_size: {} rows, tolerance: {} rows\'.\n                    format(self.train_num_records, self.sample_num_records, self.max_gap_num_records)\n                )\n                raise AssertionError\n\n        if self.test_num_records > 0:\n            try:\n                assert self.test_num_records + self.max_gap_num_records >= self.sample_num_records\n\n            except AssertionError:\n                self.log.exception(\n                    \'Test subset should contain at least one sample, \' +\n                    \'got: test_set size: {} rows, sample_size: {} rows, tolerance: {} rows\'.\n                    format(self.test_num_records, self.sample_num_records, self.max_gap_num_records)\n                )\n                raise AssertionError\n\n        self.sample_num = 0\n        self.is_ready = True\n\n    def read_csv(self, data_filename=None, force_reload=False):\n        """"""\n        Populates instance by loading data: CSV file --> pandas dataframe.\n\n        Args:\n            data_filename: [opt] csv data filename as string or list of such strings.\n            force_reload:  ignore loaded data.\n        """"""\n        if self.data is not None and not force_reload:\n            data_range = pd.to_datetime(self.data.index)\n            self.total_num_records = self.data.shape[0]\n            self.data_range_delta = (data_range[-1] - data_range[0]).to_pytimedelta()\n            self.log.debug(\'data has been already loaded. Use `force_reload=True` to reload\')\n            return\n        if data_filename:\n            self.filename = data_filename  # override data source if one is given\n        if type(self.filename) == str:\n            self.filename = [self.filename]\n\n        dataframes = []\n        for filename in self.filename:\n            try:\n                assert filename and os.path.isfile(filename)\n                current_dataframe = pd.read_csv(\n                    filename,\n                    sep=self.sep,\n                    header=self.header,\n                    index_col=self.index_col,\n                    parse_dates=self.parse_dates,\n                    names=self.names,\n                )\n\n                # Check and remove duplicate datetime indexes:\n                duplicates = current_dataframe.index.duplicated(keep=\'first\')\n                how_bad = duplicates.sum()\n                if how_bad > 0:\n                    current_dataframe = current_dataframe[~duplicates]\n                    self.log.warning(\'Found {} duplicated date_time records in <{}>.\\\n                     Removed all but first occurrences.\'.format(how_bad, filename))\n\n                dataframes += [current_dataframe]\n                self.log.info(\'Loaded {} records from <{}>.\'.format(dataframes[-1].shape[0], filename))\n\n            except:\n                msg = \'Data file <{}> not specified / not found / parser error.\'.format(str(filename))\n                self.log.error(msg)\n                raise FileNotFoundError(msg)\n\n        self.data = pd.concat(dataframes)\n        data_range = pd.to_datetime(self.data.index)\n        self.total_num_records = self.data.shape[0]\n        self.data_range_delta = (data_range[-1] - data_range[0]).to_pytimedelta()\n\n    def describe(self):\n        """"""\n        Returns summary dataset statistic as pandas dataframe:\n\n            - records count,\n            - data mean,\n            - data std dev,\n            - min value,\n            - 25% percentile,\n            - 50% percentile,\n            - 75% percentile,\n            - max value\n\n        for every data column.\n        """"""\n        # Pretty straightforward, using standard pandas utility.\n        # The only caveat here is that if actual data has not been loaded yet, need to load, describe and unload again,\n        # thus avoiding passing big files to BT server:\n        flush_data = False\n        try:\n            assert not self.data.empty\n            pass\n\n        except (AssertionError, AttributeError) as e:\n            self.read_csv()\n            flush_data = True\n\n        self.data_stat = self.data.describe()\n        self.log.info(\'Data summary:\\n{}\'.format(self.data_stat.to_string()))\n\n        if flush_data:\n            self.data = None\n            self.log.info(\'Flushed data.\')\n\n        return self.data_stat\n\n    def to_btfeed(self):\n        """"""\n        Performs BTgymData-->bt.feed conversion.\n\n        Returns:\n             dict of type: {data_line_name: bt.datafeed instance}.\n        """"""\n        def bt_timeframe(minutes):\n            timeframe = TimeFrame.Minutes\n            if minutes / 1440 == 1:\n                timeframe = TimeFrame.Days\n            return timeframe\n        try:\n            assert not self.data.empty\n            btfeed = btfeeds.PandasDirectData(\n                dataname=self.data,\n                timeframe=bt_timeframe(self.timeframe),\n                datetime=self.datetime,\n                open=self.open,\n                high=self.high,\n                low=self.low,\n                close=self.close,\n                volume=self.volume,\n                openinterest=self.openinterest\n            )\n            btfeed.numrecords = self.data.shape[0]\n            return {self.data_name: btfeed}\n\n        except (AssertionError, AttributeError) as e:\n            msg = \'Instance holds no data. Hint: forgot to call .read_csv()?\'\n            self.log.error(msg)\n            raise AssertionError(msg)\n\n    def sample(self, **kwargs):\n        return self._sample(**kwargs)\n\n    def _sample(\n            self,\n            get_new=True,\n            sample_type=0,\n            b_alpha=1.0,\n            b_beta=1.0,\n            force_interval=False,\n            interval=None,\n            **kwargs\n    ):\n        """"""\n        Samples continuous subset of data.\n\n        Args:\n            get_new (bool):                     sample new (True) or reuse (False) last made sample;\n            sample_type (int or bool):          0 (train) or 1 (test) - get sample from train or test data subsets\n                                                respectively.\n            b_alpha (float):                    beta-distribution sampling alpha > 0, valid for train episodes.\n            b_beta (float):                     beta-distribution sampling beta > 0, valid for train episodes.\n            force_interval(bool):               use exact sampling interval (should be given)\n            interval(iterable of int, len2):    exact interval to sample from when force_interval=True\n\n        Returns:\n        if no sample_class_ref param been set:\n            BTgymDataset instance with number of records ~ max_episode_len,\n            where `~` tolerance is set by `time_gap` param;\n        else:\n            `sample_class_ref` instance with same as above number of records.\n\n        Note:\n                Train sample start position within interval is drawn from beta-distribution\n                with default parameters b_alpha=1, b_beta=1, i.e. uniform one.\n                Beta-distribution makes skewed sampling possible , e.g.\n                to give recent episodes higher probability of being sampled, e.g.:  b_alpha=10, b_beta=0.8.\n                Test samples are always uniform one.\n\n        """"""\n        try:\n            assert self.is_ready\n\n        except AssertionError:\n            msg = \'sampling attempt: data not ready. Hint: forgot to call data.reset()?\'\n            self.log.error(msg)\n            raise RuntimeError(msg)\n\n        try:\n            assert sample_type in [0, 1]\n\n        except AssertionError:\n            msg = \'sampling attempt: expected sample type be in {}, got: {}\'.format([0, 1], sample_type)\n            self.log.error(msg)\n            raise ValueError(msg)\n\n        if force_interval:\n            try:\n                assert interval is not None and len(list(interval)) == 2\n\n            except AssertionError:\n                msg = \'sampling attempt: got force_interval=True, expected interval=[a,b], got: <{}>\'.format(interval)\n                self.log.error(msg)\n                raise ValueError(msg)\n\n        if self.sample_instance is None or get_new:\n            if sample_type == 0:\n                # Get beta_distributed sample in train interval:\n                if force_interval:\n                    sample_interval = interval\n                else:\n                    sample_interval = self.train_interval\n\n                self.sample_instance = self._sample_interval(\n                    sample_interval,\n                    force_interval=force_interval,\n                    b_alpha=b_alpha,\n                    b_beta=b_beta,\n                    name=\'train_\' + self.sample_name,\n                    **kwargs\n                )\n\n            else:\n                # Get uniform sample in test interval:\n                if force_interval:\n                    sample_interval = interval\n                else:\n                    sample_interval = self.test_interval\n\n                self.sample_instance = self._sample_interval(\n                    sample_interval,\n                    force_interval=force_interval,\n                    b_alpha=1,\n                    b_beta=1,\n                    name=\'test_\' + self.sample_name,\n                    **kwargs\n                )\n            self.sample_instance.metadata[\'type\'] = sample_type  # TODO: can move inside sample()\n            self.sample_instance.metadata[\'sample_num\'] = self.sample_num\n            self.sample_instance.metadata[\'parent_sample_num\'] = copy.deepcopy(self.metadata[\'sample_num\'])\n            self.sample_instance.metadata[\'parent_sample_type\'] = copy.deepcopy(self.metadata[\'type\'])\n            self.sample_num += 1\n\n        else:\n            # Do nothing:\n            self.log.debug(\'Reusing sample, id: {}\'.format(self.sample_instance.filename))\n\n        return self.sample_instance\n\n    def _sample_random(\n            self,\n            sample_type=0,\n            timestamp=None,\n            name=\'random_sample_\',\n            interval=None,\n            force_interval=False,\n            **kwargs\n    ):\n        """"""\n        Randomly samples continuous subset of data.\n\n        Args:\n            name:        str, sample filename id\n\n        Returns:\n             BTgymDataset instance with number of records ~ max_episode_len,\n             where `~` tolerance is set by `time_gap` param.\n        """"""\n        try:\n            assert not self.data.empty\n\n        except (AssertionError, AttributeError) as e:\n            self.log.exception(\'Instance holds no data. Hint: forgot to call .read_csv()?\')\n            raise AssertionError\n\n        if force_interval:\n            raise NotImplementedError(\'Force_interval for random sampling not implemented.\')\n\n        self.log.debug(\'Maximum sample time duration set to: {}.\'.format(self.max_sample_len_delta))\n        self.log.debug(\'Respective number of steps: {}.\'.format(self.sample_num_records))\n        self.log.debug(\'Maximum allowed data time gap set to: {}.\\n\'.format(self.max_time_gap))\n\n        sampled_data = None\n        sample_len = 0\n\n        # Sanity check param:\n        max_attempts = 100\n        attempts = 0\n\n        # # Keep sampling random enter points until all conditions are met:\n        while attempts <= max_attempts:\n\n            # Randomly sample record (row) from entire datafeed:\n            first_row = int((self.data.shape[0] - self.sample_num_records - 1) * random.random())\n            sample_first_day = self.data[first_row:first_row + 1].index[0]\n            self.log.debug(\'Sample start: {}, weekday: {}.\'.format(sample_first_day, sample_first_day.weekday()))\n\n            # Keep sampling until good day:\n            while not sample_first_day.weekday() in self.start_weekdays and attempts <= max_attempts:\n                self.log.debug(\'Not a good day to start, resampling...\')\n                first_row = int((self.data.shape[0] - self.sample_num_records - 1) * random.random())\n                sample_first_day = self.data[first_row:first_row + 1].index[0]\n                self.log.debug(\'Sample start: {}, weekday: {}.\'.format(sample_first_day, sample_first_day.weekday()))\n                attempts +=1\n\n            # Check if managed to get proper weekday:\n            assert attempts <= max_attempts, \\\n                \'Quitting after {} sampling attempts. Hint: check sampling params / dataset consistency.\'. \\\n                format(attempts)\n\n            # If 00 option set, get index of first record of that day:\n            if self.start_00:\n                adj_timedate = sample_first_day.date()\n                self.log.debug(\'Start time adjusted to <00:00>\')\n\n            else:\n                adj_timedate = sample_first_day\n\n            first_row = self.data.index.get_loc(adj_timedate, method=\'nearest\')\n\n            # Easy part:\n            last_row = first_row + self.sample_num_records  # + 1\n            sampled_data = self.data[first_row: last_row]\n            sample_len = (sampled_data.index[-1] - sampled_data.index[0]).to_pytimedelta()\n            self.log.debug(\'Actual sample duration: {}.\'.format(sample_len, ))\n            self.log.debug(\'Total sample time gap: {}.\'.format(self.max_sample_len_delta - sample_len))\n\n            # Perform data gap check:\n            if self.max_sample_len_delta - sample_len < self.max_time_gap:\n                self.log.debug(\'Sample accepted.\')\n                # If sample OK - compose and return sample:\n                new_instance = self.nested_class_ref(**self.nested_params)\n                new_instance.filename = name + \'n{}_at_{}\'.format(self.sample_num, adj_timedate)\n                self.log.info(\'Sample id: <{}>.\'.format(new_instance.filename))\n                new_instance.data = sampled_data\n                new_instance.metadata[\'type\'] = \'random_sample\'\n                new_instance.metadata[\'first_row\'] = first_row\n                new_instance.metadata[\'last_row\'] = last_row\n\n                return new_instance\n\n            else:\n                self.log.debug(\'Duration too big, resampling...\\n\')\n                attempts += 1\n\n        # Got here -> sanity check failed:\n        msg = (\n            \'\\nQuitting after {} sampling attempts.\\n\' +\n            \'Full sample duration: {}\\n\' +\n            \'Total sample time gap: {}\\n\' +\n            \'Sample start time: {}\\n\' +\n            \'Sample finish time: {}\\n\' +\n            \'Hint: check sampling params / dataset consistency.\'\n        ).format(\n            attempts,\n            sample_len,\n            sample_len - self.max_sample_len_delta,\n            sampled_data.index[0],\n            sampled_data.index[-1]\n\n        )\n        self.log.error(msg)\n        raise RuntimeError(msg)\n\n    def _sample_interval(\n            self,\n            interval,\n            b_alpha=1.0,\n            b_beta=1.0,\n            name=\'interval_sample_\',\n            force_interval=False,\n            **kwargs\n    ):\n        """"""\n        Samples continuous subset of data,\n        such as entire episode records lie within positions specified by interval.\n        Episode start position within interval is drawn from beta-distribution parametrised by `b_alpha, b_beta`.\n        By default distribution is uniform one.\n\n        Args:\n            interval:       tuple, list or 1d-array of integers of length 2: [lower_row_number, upper_row_number];\n            b_alpha:        float > 0, sampling B-distribution alpha param, def=1;\n            b_beta:         float > 0, sampling B-distribution beta param, def=1;\n            name:           str, sample filename id\n            force_interval: bool,  if true: force exact interval sampling\n\n\n        Returns:\n             - BTgymDataset instance such as:\n                1. number of records ~ max_episode_len, subj. to `time_gap` param;\n                2. actual episode start position is sampled from `interval`;\n             - `False` if it is not possible to sample instance with set args.\n        """"""\n        try:\n            assert not self.data.empty\n\n        except (AssertionError, AttributeError) as e:\n            self.log.exception(\'Instance holds no data. Hint: forgot to call .read_csv()?\')\n            raise AssertionError\n\n        try:\n            assert len(interval) == 2\n\n        except AssertionError:\n            self.log.exception(\n                \'Invalid interval arg: expected list or tuple of size 2, got: {}\'.format(interval)\n            )\n            raise AssertionError\n\n        if force_interval:\n            return self._sample_exact_interval(interval, name)\n\n        try:\n            assert b_alpha > 0 and b_beta > 0\n\n        except AssertionError:\n            self.log.exception(\n                \'Expected positive B-distribution [alpha, beta] params, got: {}\'.format([b_alpha, b_beta])\n            )\n            raise AssertionError\n\n        if interval[-1] - interval[0] + self.max_gap_num_records > self.sample_num_records:\n            sample_num_records = self.sample_num_records\n        else:\n            sample_num_records = interval[-1] - interval[0]\n\n        self.log.debug(\'Sample interval: {}\'.format(interval))\n        self.log.debug(\'Maximum sample time duration set to: {}.\'.format(self.max_sample_len_delta))\n        self.log.debug(\'Sample number of steps (adjusted to interval): {}.\'.format(sample_num_records))\n        self.log.debug(\'Maximum allowed data time gap set to: {}.\\n\'.format(self.max_time_gap))\n\n        sampled_data = None\n        sample_len = 0\n\n        # Sanity check param:\n        max_attempts = 100\n        attempts = 0\n\n        # # Keep sampling random enter points until all conditions are met:\n        while attempts <= max_attempts:\n\n            first_row = interval[0] + int(\n                (interval[-1] - interval[0] - sample_num_records) * random_beta(a=b_alpha, b=b_beta)\n            )\n\n            #print(\'_sample_interval_sample_num_records: \', sample_num_records)\n            #print(\'_sample_interval_first_row: \', first_row)\n\n            sample_first_day = self.data[first_row:first_row + 1].index[0]\n            self.log.debug(\n                \'Sample start row: {}, day: {}, weekday: {}.\'.\n                format(first_row, sample_first_day, sample_first_day.weekday())\n            )\n\n            # Keep sampling until good day:\n            while not sample_first_day.weekday() in self.start_weekdays and attempts <= max_attempts:\n                self.log.debug(\'Not a good day to start, resampling...\')\n                first_row = interval[0] + round(\n                    (interval[-1] - interval[0] - sample_num_records) * random_beta(a=b_alpha, b=b_beta)\n                )\n                #print(\'r_sample_interval_sample_num_records: \', sample_num_records)\n                #print(\'r_sample_interval_first_row: \', first_row)\n                sample_first_day = self.data[first_row:first_row + 1].index[0]\n                self.log.debug(\n                    \'Sample start row: {}, day: {}, weekday: {}.\'.\n                    format(first_row, sample_first_day, sample_first_day.weekday())\n                )\n                attempts += 1\n\n            # Check if managed to get proper weekday:\n            try:\n                assert attempts <= max_attempts\n\n            except AssertionError:\n                self.log.exception(\n                    \'Quitting after {} sampling attempts. Hint: check sampling params / dataset consistency.\'.\n                    format(attempts)\n                )\n                raise RuntimeError\n\n            # If 00 option set, get index of first record of that day:\n            if self.start_00:\n                adj_timedate = sample_first_day.date()\n                self.log.debug(\'Start time adjusted to <00:00>\')\n                first_row = self.data.index.get_loc(adj_timedate, method=\'nearest\')\n\n            else:\n                adj_timedate = sample_first_day\n\n            # first_row = self.data.index.get_loc(adj_timedate, method=\'nearest\')\n\n            # Easy part:\n            last_row = first_row + sample_num_records  # + 1\n            sampled_data = self.data[first_row: last_row]\n\n            self.log.debug(\n                \'first_row: {}, last_row: {}, data_shape: {}\'.format(\n                    first_row,\n                    last_row,\n                    sampled_data.shape\n                )\n            )\n            sample_len = (sampled_data.index[-1] - sampled_data.index[0]).to_pytimedelta()\n            self.log.debug(\'Actual sample duration: {}.\'.format(sample_len))\n            self.log.debug(\'Total sample time gap: {}.\'.format(self.max_sample_len_delta - sample_len))\n\n            # Perform data gap check:\n            if self.max_sample_len_delta - sample_len < self.max_time_gap:\n                self.log.debug(\'Sample accepted.\')\n                # If sample OK - return new dataset:\n                new_instance = self.nested_class_ref(**self.nested_params)\n                new_instance.filename = name + \'num_{}_at_{}\'.format(self.sample_num, adj_timedate)\n                self.log.info(\'New sample id: <{}>.\'.format(new_instance.filename))\n                new_instance.data = sampled_data\n                new_instance.metadata[\'type\'] = \'interval_sample\'\n                new_instance.metadata[\'first_row\'] = first_row\n                new_instance.metadata[\'last_row\'] = last_row\n\n                return new_instance\n\n            else:\n                self.log.debug(\'Attempt {}: gap is too big, resampling, ...\\n\'.format(attempts))\n                attempts += 1\n\n        # Got here -> sanity check failed:\n        msg = (\n                \'\\nQuitting after {} sampling attempts.\\n\' +\n                \'Full sample duration: {}\\n\' +\n                \'Total sample time gap: {}\\n\' +\n                \'Sample start time: {}\\n\' +\n                \'Sample finish time: {}\\n\' +\n                \'Hint: check sampling params / dataset consistency.\'\n        ).format(\n            attempts,\n            sample_len,\n            sample_len - self.max_sample_len_delta,\n            sampled_data.index[0],\n            sampled_data.index[-1]\n\n        )\n        self.log.error(msg)\n        raise RuntimeError(msg)\n\n    def _sample_aligned_interval(\n            self,\n            interval,\n            align_left=False,\n            b_alpha=1.0,\n            b_beta=1.0,\n            name=\'interval_sample_\',\n            force_interval=False,\n            **kwargs\n    ):\n        """"""\n        Samples continuous subset of data,\n        such as entire episode records lie within positions specified by interval\n        Episode start position within interval is drawn from beta-distribution parametrised by `b_alpha, b_beta`.\n        By default distribution is uniform one.\n\n        Args:\n            interval:       tuple, list or 1d-array of integers of length 2: [lower_row_number, upper_row_number];\n            align:          if True - try to align sample to beginning of interval;\n            b_alpha:        float > 0, sampling B-distribution alpha param, def=1;\n            b_beta:         float > 0, sampling B-distribution beta param, def=1;\n            name:           str, sample filename id\n            force_interval: bool,  if true: force exact interval sampling\n\n        Returns:\n             - BTgymDataset instance such as:\n                1. number of records ~ max_episode_len, subj. to `time_gap` param;\n                2. actual episode start position is sampled from `interval`;\n             - `False` if it is not possible to sample instance with set args.\n        """"""\n        try:\n            assert not self.data.empty\n\n        except (AssertionError, AttributeError) as e:\n            self.log.exception(\'Instance holds no data. Hint: forgot to call .read_csv()?\')\n            raise AssertionError\n\n        try:\n            assert len(interval) == 2\n\n        except AssertionError:\n            self.log.exception(\n                \'Invalid interval arg: expected list or tuple of size 2, got: {}\'.format(interval)\n            )\n            raise AssertionError\n\n        if force_interval:\n            return self._sample_exact_interval(interval, name)\n\n        try:\n            assert b_alpha > 0 and b_beta > 0\n\n        except AssertionError:\n            self.log.exception(\n                \'Expected positive B-distribution [alpha, beta] params, got: {}\'.format([b_alpha, b_beta])\n            )\n            raise AssertionError\n\n        sample_num_records = self.sample_num_records\n\n        self.log.debug(\'Maximum sample time duration set to: {}.\'.format(self.max_sample_len_delta))\n        self.log.debug(\'Respective number of steps: {}.\'.format(sample_num_records))\n        self.log.debug(\'Maximum allowed data time gap set to: {}.\\n\'.format(self.max_time_gap))\n\n        # Sanity check param:\n        if align_left:\n            max_attempts = interval[-1] - interval[0]\n        else:\n            # Sanity check:\n            max_attempts = 100\n\n        attempts = 0\n        align_shift = 0\n\n        # Sample enter point as close to beginning  until all conditions are met:\n        while attempts <= max_attempts:\n            if align_left:\n                first_row = interval[0] + align_shift\n\n            else:\n                first_row = interval[0] + int(\n                    (interval[-1] - interval[0] - sample_num_records) * random_beta(a=b_alpha, b=b_beta)\n                )\n\n            #print(\'_sample_interval_sample_num_records: \', sample_num_records)\n            self.log.debug(\'_sample_interval_first_row: {}\'.format(first_row))\n\n            sample_first_day = self.data[first_row:first_row + 1].index[0]\n            self.log.debug(\'Sample start: {}, weekday: {}.\'.format(sample_first_day, sample_first_day.weekday()))\n\n            # Keep sampling until good day:\n            while not sample_first_day.weekday() in self.start_weekdays and attempts <= max_attempts:\n                align_shift += 1\n\n                self.log.debug(\'Not a good day to start, resampling...\')\n\n                if align_left:\n                    first_row = interval[0] + align_shift\n                else:\n\n                    first_row = interval[0] + int(\n                        (interval[-1] - interval[0] - sample_num_records) * random_beta(a=b_alpha, b=b_beta)\n                    )\n                #print(\'r_sample_interval_sample_num_records: \', sample_num_records)\n                self.log.debug(\'_sample_interval_first_row: {}\'.format(first_row))\n\n                sample_first_day = self.data[first_row:first_row + 1].index[0]\n\n                self.log.debug(\'Sample start: {}, weekday: {}.\'.format(sample_first_day, sample_first_day.weekday()))\n\n                attempts += 1\n\n            # Check if managed to get proper weekday:\n            try:\n                assert attempts <= max_attempts\n\n            except AssertionError:\n                self.log.exception(\n                    \'Quitting after {} sampling attempts. Hint: check sampling params / dataset consistency.\'.\n                    format(attempts)\n                )\n                raise RuntimeError\n\n            # If 00 option set, get index of first record of that day:\n            if self.start_00:\n                adj_timedate = sample_first_day.date()\n                self.log.debug(\'Start time adjusted to <00:00>\')\n                first_row = self.data.index.get_loc(adj_timedate, method=\'nearest\')\n\n            else:\n                adj_timedate = sample_first_day\n\n            # first_row = self.data.index.get_loc(adj_timedate, method=\'nearest\')\n\n            # Easy part:\n            last_row = first_row + sample_num_records  # + 1\n            sampled_data = self.data[first_row: last_row]\n            sample_len = (sampled_data.index[-1] - sampled_data.index[0]).to_pytimedelta()\n            self.log.debug(\'Actual sample duration: {}.\'.format(sample_len))\n            self.log.debug(\'Total sample time gap: {}.\'.format(sample_len - self.max_sample_len_delta))\n\n            # Perform data gap check:\n            if sample_len - self.max_sample_len_delta < self.max_time_gap:\n                self.log.debug(\'Sample accepted.\')\n                # If sample OK - return new dataset:\n                new_instance = self.nested_class_ref(**self.nested_params)\n                new_instance.filename = name + \'num_{}_at_{}\'.format(self.sample_num, adj_timedate)\n                self.log.info(\'New sample id: <{}>.\'.format(new_instance.filename))\n                new_instance.data = sampled_data\n                new_instance.metadata[\'type\'] = \'interval_sample\'\n                new_instance.metadata[\'first_row\'] = first_row\n                new_instance.metadata[\'last_row\'] = last_row\n\n                return new_instance\n\n            else:\n                self.log.debug(\'Attempt {}: duration too big, resampling, ...\\n\'.format(attempts))\n                attempts += 1\n                align_shift += 1\n\n        # Got here -> sanity check failed:\n        msg = (\'Quitting after {} sampling attempts.\' +\n               \'Hint: check sampling params / dataset consistency.\').format(attempts)\n        self.log.error(msg)\n        raise RuntimeError(msg)\n\n    def _sample_exact_interval(self, interval, name=\'interval_sample_\', **kwargs):\n        """"""\n        Samples exactly defined interval.\n\n        Args:\n            interval:   tuple, list or 1d-array of integers of length 2: [lower_row_number, upper_row_number];\n            name:       str, sample filename id\n\n        Returns:\n             BTgymDataset instance.\n\n        """"""\n        try:\n            assert not self.data.empty\n\n        except (AssertionError, AttributeError) as e:\n            self.log.exception(\'Instance holds no data. Hint: forgot to call .read_csv()?\')\n            raise AssertionError\n\n        try:\n            assert len(interval) == 2\n\n        except AssertionError:\n            self.log.exception(\n                \'Invalid interval arg: expected list or tuple of size 2, got: {}\'.format(interval)\n            )\n            raise AssertionError\n\n        first_row = interval[0]\n        last_row = interval[-1]\n        sampled_data = self.data[first_row: last_row]\n\n        sample_first_day = self.data[first_row:first_row + 1].index[0]\n\n        new_instance = self.nested_class_ref(**self.nested_params)\n        new_instance.filename = name + \'num_{}_at_{}\'.format(self.sample_num, sample_first_day)\n        self.log.info(\'New sample id: <{}>.\'.format(new_instance.filename))\n        new_instance.data = sampled_data\n        new_instance.metadata[\'type\'] = \'interval_sample\'\n        new_instance.metadata[\'first_row\'] = first_row\n        new_instance.metadata[\'last_row\'] = last_row\n\n        return new_instance\n\n\n\n\n\n\n'"
btgym/datafeed/casual.py,0,"b'import copy\nimport random\nimport math\nimport datetime\nfrom logbook import WARNING\n\nfrom .base import BTgymBaseData\nfrom .derivative import BTgymEpisode, BTgymDataTrial,  BTgymRandomDataDomain\n\n\nclass BTgymCasualTrial(BTgymDataTrial):\n    """"""\n    Intermediate-level data class.\n    Implements conception of `Trial` object.\n    Supports exact data train/test separation by means of `global_time`\n    Do not use directly.\n    """"""\n    trial_params = dict(\n        nested_class_ref=BTgymEpisode,\n    )\n\n    def __init__(self, name=\'TimeTrial\', **kwargs):\n        """"""\n        Args:\n            filename:           not used;\n            sampling_params:    dict, sample retrieving options, see base class description for details;\n            task:               int, optional;\n            parsing_params:     csv parsing options, see base class description for details;\n            log_level:          int, optional, logbook.level;\n            _config_stack:      dict, holding configuration for nested child samples;\n        """"""\n\n        super(BTgymCasualTrial, self).__init__(name=name, **kwargs)\n        # self.log.warning(\'self.frozen_time_split: {}\'.format(self.frozen_time_split))\n\n    def set_global_timestamp(self, timestamp):\n        """"""\n        Performs validity checks and sets current global_time.\n        Args:\n            timestamp:  POSIX timestamp\n\n        Returns:\n\n        """"""\n        if self.data is not None:\n            if self.frozen_split_timestamp is not None:\n                self.global_timestamp = self.frozen_split_timestamp\n\n            else:\n                if self.metadata[\'type\']:\n                    if timestamp is not None:\n                        assert timestamp < self.final_timestamp, \\\n                            \'global time passed <{}> is out of upper bound <{}> for provided data.\'. \\\n                            format(\n                                datetime.datetime.fromtimestamp(timestamp),\n                                datetime.datetime.fromtimestamp(self.final_timestamp)\n                            )\n                        if timestamp < self.start_timestamp:\n                            if self.global_timestamp == 0:\n                                self.global_timestamp = self.start_timestamp\n\n                        else:\n                            if timestamp > self.global_timestamp:\n                                self.global_timestamp = timestamp\n\n                    else:\n                        if self.global_timestamp == 0:\n                            self.global_timestamp = self.start_timestamp\n                else:\n                    self.global_timestamp = self.start_timestamp\n\n    def get_global_index(self):\n        """"""\n        Returns:\n            data row corresponded to current global_time\n        """"""\n        if self.is_ready:\n            return self.data.index.get_loc(\n                datetime.datetime.fromtimestamp(self.global_timestamp),\n                method=\'backfill\'\n            )\n\n        else:\n            return 0\n\n    def get_intervals(self):\n        """"""\n        Estimates exact sampling intervals such as test episode starts as close to current global time point as\n        data consistency allows but no earlier;\n\n        Returns:\n            dict of train and test sampling intervals for current global_time point\n        """"""\n        if self.is_ready:\n            if self.metadata[\'type\']:\n                # Intervals for target trial:\n                current_index = self.get_global_index()\n\n                self.log.debug(\n                    \'current_index: {}, total_num_records: {}, sample_num_records: {}\'.format(\n                        current_index,\n                        self.total_num_records,\n                        self.sample_num_records\n                    )\n                )\n                assert 0 <= current_index <= self.total_num_records - self.sample_num_records,\\\n                    \'global_time: {} outside data interval: {} - {}, considering sample duration: {}\'.format(\n                        self.data.index[current_index],\n                        self.data.index[0],\n                        self.data.index[-1],\n                        self.max_sample_len_delta\n                    )\n                train_interval = [0, current_index]\n                test_interval = [current_index + 1, self.total_num_records - 1]\n\n            else:\n                # Intervals for source trial:\n                train_interval = [0, self.train_num_records - 1]\n                test_interval = [self.train_num_records, self.total_num_records - 1]  # TODO: ?!\n\n            self.log.debug(\n                \'train_interval: {}, datetimes: {} - {}\'.\n                format(\n                    train_interval,\n                    self.data.index[train_interval[0]],\n                    self.data.index[train_interval[-1]],\n                )\n            )\n            self.log.debug(\n                \'test_interval: {}, datetimes: {} - {}\'.\n                format(\n                    test_interval,\n                    self.data.index[test_interval[0]],\n                    self.data.index[test_interval[-1]],\n                )\n            )\n        else:\n            train_interval = None\n            test_interval = None\n\n        return train_interval, test_interval\n\n    def sample(\n            self,\n            get_new=True,\n            sample_type=0,\n            timestamp=None,\n            align_left=True,\n            b_alpha=1.0,\n            b_beta=1.0,\n            **kwargs\n    ):\n        """"""\n        Samples continuous subset of data.\n\n        Args:\n            get_new (bool):                 sample new (True) or reuse (False) last made sample;\n            sample_type (int or bool):      0 (train) or 1 (test) - get sample from train or test data subsets\n                                            respectively.\n            timestamp:                      POSIX timestamp.\n            align_left:                     bool, if True: set test interval as close to current timepoint as possible.\n            b_alpha (float):                beta-distribution sampling alpha > 0, valid for train episodes.\n            b_beta (float):                 beta-distribution sampling beta > 0, valid for train episodes.\n        """"""\n        try:\n            assert self.is_ready\n\n        except AssertionError:\n            self.log.exception(\n                \'Sampling attempt: data not ready. Hint: forgot to call data.reset()?\'\n            )\n            raise AssertionError\n\n        try:\n            assert sample_type in [0, 1]\n\n        except AssertionError:\n            self.log.exception(\n                \'Sampling attempt: expected sample type be in {}, got: {}\'.\\\n                format([0, 1], sample_type)\n            )\n            raise AssertionError\n\n        # Set actual time:\n        if timestamp is not None:\n            self.set_global_timestamp(timestamp)\n\n        if \'interval\' not in kwargs.keys():\n            train_interval, test_interval = self.get_intervals()\n        else:\n            train_interval = test_interval = kwargs.pop(\'interval\')\n\n        if self.sample_instance is None or get_new:\n            if sample_type == 0:\n                # Get beta_distributed sample in train interval:\n                self.sample_instance = self._sample_interval(\n                    train_interval,\n                    b_alpha=b_alpha,\n                    b_beta=b_beta,\n                    name=\'train_\' + self.sample_name,\n                    **kwargs\n                )\n\n            else:\n                # If parent is target - get left-aligned (i.e. as close as possible to current global_time)\n                # sample in test interval; else (parenet is source) - uniformly sample from test interval:\n                if self.metadata[\'parent_sample_type\']:\n                    align = align_left\n\n                else:\n                    align = False\n\n                self.sample_instance = self._sample_aligned_interval(\n                    test_interval,\n                    align_left=align,\n                    b_alpha=1,\n                    b_beta=1,\n                    name=\'test_\' + self.sample_name,\n                    **kwargs\n                )\n            self.sample_instance.metadata[\'type\'] = sample_type\n            self.sample_instance.metadata[\'sample_num\'] = self.sample_num\n            self.sample_instance.metadata[\'parent_sample_num\'] = copy.deepcopy(self.metadata[\'sample_num\'])\n            self.sample_instance.metadata[\'parent_sample_type\'] = copy.deepcopy(self.metadata[\'type\'])\n            self.sample_num += 1\n\n        else:\n            # Do nothing:\n            self.log.debug(\'Reusing sample, id: {}\'.format(self.sample_instance.filename))\n\n        return self.sample_instance\n\n\nclass BTgymCasualDataDomain(BTgymRandomDataDomain):\n    """"""\n    Imitates online data stream by implementing conception of sliding `current time point`\n    and enabling sampling control according to it.\n\n    Objective is to enable proper train/evaluation/test data split and prevent data leakage by\n     allowing training on known, past data only and testing on unknown, future data, providing realistic training cycle.\n\n    Source trials set is defined as all trials starting somewhere in past and ending no later than current time point,\n    and target trials set as set of trials such as: trial test period starts somewhere in the past and ends at\n    current time point and trial test period starts from now on for all time points in available dataset range.\n\n    Sampling control is defined by:\n    - `current time point` is set arbitrary and is stateful in sense it can be only increased (no backward time);\n    - source trials can be sampled from past (known) data multiply times;\n    - target trial can only be sampled once according to current time point or later (unknown data);\n    - as any sampled target trial is being evaluated by outer algorithm, current time should be incremented either by\n      providing \'timestamp\' arg. to sample() method or calling set_global_timestamp() method,\n      to match last evaluated record (marking all evaluated data as already known\n      and making it available for training);\n    """"""\n    trial_class_ref = BTgymCasualTrial\n    episode_class_ref = BTgymEpisode\n\n    def __init__(\n            self,\n            filename,\n            trial_params,\n            episode_params,\n            frozen_time_split=None,\n            name=\'TimeDataDomain\',\n            data_names=(\'default_asset\',),\n            **kwargs):\n        """"""\n        Args:\n            filename:           Str or list of str, file_names containing CSV historic data;\n            parsing_params:     csv parsing options, see base class description for details;\n            trial_params:       dict, describes trial parameters, should contain keys:\n                                {sample_duration, time_gap, start_00, start_weekdays, test_period, expanding};\n            episode_params:     dict, describes episode parameters, should contain keys:\n                                {sample_duration, time_gap, start_00, start_weekdays};\n            name:               str, optional\n            task:               int, optional\n            log_level:          int, logbook.level\n\n        """"""\n        self.train_range_row = 0\n        self.test_range_row = 0\n\n        self.test_range_row = 0\n        self.test_mean_row = 0\n\n        self.global_step = 0\n        self.total_samples = -1\n        self.sample_num = -1\n        self.sample_stride = -1\n\n        # if frozen_time_split is not None:\n        #     self.frozen_time_split = datetime.datetime(**frozen_time_split)\n        #\n        # else:\n        #     self.frozen_time_split = None\n        #\n        # self.frozen_split_timestamp = None\n\n        kwargs.update({\'target_period\': episode_params[\'sample_duration\']})\n\n        trial_params[\'start_00\'] = False\n        trial_params[\'frozen_time_split\'] = frozen_time_split\n\n        super(BTgymCasualDataDomain, self).__init__(\n            filename=filename,\n            trial_params=trial_params,\n            episode_params=episode_params,\n            use_target_backshift=False,\n            name=name,\n            data_names=data_names,\n            frozen_time_split=frozen_time_split,\n            **kwargs\n        )\n\n        # self.log.warning(\'2: self.frozen_time_split: {}\'.format(self.frozen_time_split))\n\n        self.log.debug(\'trial_class_ref: {}\'.format(self.trial_class_ref))\n        self.log.debug(\'episode_class_ref: {}\'.format(self.episode_class_ref))\n\n        self.log.debug(\'sampling_params: {}\'.format(self.sampling_params))\n        self.log.debug(\'nested_params: {}\'.format(self.nested_params))\n\n    def set_global_timestamp(self, timestamp):\n        """"""\n        Performs validity checks and sets current global_time.\n        Args:\n            timestamp:  POSIX timestamp\n\n        Returns:\n\n        """"""\n        if self.data is not None:\n            if self.frozen_split_timestamp is not None:\n                self.global_timestamp = self.frozen_split_timestamp\n\n            else:\n                if timestamp is not None:\n                    assert timestamp < self.final_timestamp, \\\n                        \'global time passed <{}> is out of upper bound <{}> for provided data.\'. \\\n                        format(\n                            datetime.datetime.fromtimestamp(timestamp),\n                            datetime.datetime.fromtimestamp(self.final_timestamp)\n                        )\n                    if timestamp < self.start_timestamp:\n                        if self.global_timestamp == 0:\n                            self.global_timestamp = self.start_timestamp\n\n                    else:\n                        if timestamp > self.global_timestamp:\n                            self.global_timestamp = timestamp\n\n                else:\n                    if self.global_timestamp == 0:\n                        self.global_timestamp = self.start_timestamp\n\n    def get_global_index(self):\n        """"""\n        Returns:\n            data row corresponded to current global_time\n        """"""\n        if self.is_ready:\n            return self.data.index.get_loc(\n                datetime.datetime.fromtimestamp(self.global_timestamp),\n                method=\'backfill\'\n            )\n\n        else:\n            return 0\n\n    def get_intervals(self):\n        """"""\n        Estimates exact sampling intervals such as train period of target trial overlaps by known up to date data\n\n        Returns:\n            dict of train and test sampling intervals for current global_time point\n        """"""\n        if self.is_ready:\n            current_index = self.get_global_index()\n            assert current_index >= self.train_num_records\n            assert current_index + self.test_num_records <= self.total_num_records, \'End of data!\'\n\n            self.log.debug(\n                \'current_index: {}, total_num_records: {}, sample_num_records: {}\'.format(\n                    current_index,\n                    self.total_num_records,\n                    self.sample_num_records\n                )\n            )\n\n            if self.expanding:\n                train_interval = [0, current_index]\n\n            else:\n                train_interval = [current_index - self.sample_num_records, current_index]\n\n            test_interval = [current_index - self.train_num_records, current_index + self.test_num_records]\n\n            self.log.debug(\n                \'train_interval: {}, datetimes: {} - {}\'.\n                format(\n                    train_interval,\n                    self.data.index[train_interval[0]],\n                    self.data.index[train_interval[-1]],\n                )\n            )\n            self.log.debug(\n                \'test_interval: {}, datetimes: {} - {}\'.\n                    format(\n                        test_interval,\n                        self.data.index[test_interval[0]],\n                        self.data.index[test_interval[-1]],\n                )\n            )\n        else:\n            train_interval = None\n            test_interval = None\n\n        return train_interval, test_interval\n\n    def _reset(self, data_filename=None, timestamp=None, **kwargs):\n\n        self.read_csv(data_filename)\n\n        # Maximum data time gap allowed within sample as pydatetimedelta obj:\n        self.max_time_gap = datetime.timedelta(**self.time_gap)\n\n        # Max. gap number of records:\n        self.max_gap_num_records = int(self.max_time_gap.total_seconds() / (60 * self.timeframe))\n\n        # ... maximum episode time duration:\n        self.max_sample_len_delta = datetime.timedelta(**self.sample_duration)\n\n        # Maximum possible number of data records (rows) within episode:\n        self.sample_num_records = int(self.max_sample_len_delta.total_seconds() / (60 * self.timeframe))\n\n        self.log.debug(\'sample_num_records: {}\'.format(self.sample_num_records))\n        self.log.debug(\'sliding_test_period: {}\'.format(self.test_period))\n\n        # Train/test timedeltas:\n        self.test_range_delta = datetime.timedelta(**self.test_period)\n        self.train_range_delta = datetime.timedelta(**self.sample_duration) - datetime.timedelta(**self.test_period)\n\n        self.test_num_records = round(self.test_range_delta.total_seconds() / (60 * self.timeframe))\n        self.train_num_records = self.sample_num_records - self.test_num_records\n\n        self.log.debug(\'test_num_records: {}\'.format(self.test_num_records))\n        self.log.debug(\'train_num_records: {}\'.format(self.train_num_records))\n\n        self.start_timestamp = self.data.index[self.sample_num_records].timestamp()\n        self.final_timestamp = self.data.index[-self.test_num_records].timestamp()\n\n        if self.frozen_time_split is not None:\n            frozen_index = self.data.index.get_loc(self.frozen_time_split, method=\'ffill\')\n            self.frozen_split_timestamp = self.data.index[frozen_index].timestamp()\n            self.set_global_timestamp(self.frozen_split_timestamp)\n\n        else:\n            self.frozen_split_timestamp = None\n            self.set_global_timestamp(timestamp)\n        current_index = self.get_global_index()\n\n        try:\n            assert self.train_num_records >= self.test_num_records\n\n        except AssertionError:\n            self.log.exception(\n                \'Train subset should contain at least one episode, got: train_set size: {} rows, episode_size: {} rows\'.\n                    format(self.train_num_records, self.test_num_records)\n            )\n            raise AssertionError\n\n        self.sample_num = 0\n\n        self.is_ready = True\n\n    def sample(self, get_new=True, sample_type=0, timestamp=None, b_alpha=1.0, b_beta=1.0, **kwargs):\n        """"""\n        Samples from sequence of `Trials`.\n\n        Args:\n            get_new (bool):                 sample new (True) or reuse (False) last made sample; n/a for target trials\n            sample_type (int or bool):      0 (train) or 1 (test) - get sample from source or target data subsets\n                                            respectively;\n            timestamp:                      POSIX timestamp indicating current global time of training loop\n            b_alpha (float):                beta-distribution sampling alpha > 0, valid for train episodes.\n            b_beta (float):                 beta-distribution sampling beta > 0, valid for train episodes.\n\n\n        Returns:\n            Trial as `BTgymBaseDataTrial` instance;\n            None, if trial\'s sequence is exhausted (global time is up).\n        """"""\n        self.set_global_timestamp(timestamp)\n\n        if \'interval\' not in kwargs.keys():\n            train_interval, test_interval = self.get_intervals()\n        else:\n            train_interval = test_interval = kwargs.pop(\'interval\')\n\n        if get_new or self.sample_instance is None:\n            if sample_type:\n                self.sample_instance = self._sample_interval(\n                    interval=test_interval,\n                    b_alpha=b_alpha,\n                    b_beta=b_beta,\n                    name=\'target_trial_\',\n                    **kwargs\n                )\n                if self.sample_instance is None:\n                    # Exhausted:\n                    return False\n\n            else:\n                self.sample_instance = self._sample_interval(\n                    interval=train_interval,\n                    b_alpha=b_alpha,\n                    b_beta=b_beta,\n                    name=\'source_trial_\',\n                    **kwargs\n                )\n                if self.sample_instance is None:\n                    # Exhausted:\n                    return False\n\n            self.log.debug(\n                \'sampled new trial <{}> with metadata: {}\'.\n                format(self.sample_instance.filename, self.sample_instance.metadata)\n            )\n\n        else:\n            self.log.debug(\n                \'reused trial <{}> with metadata: {}\'.\n                    format(self.sample_instance.filename, self.sample_instance.metadata)\n            )\n        self.sample_instance.metadata[\'type\'] = sample_type\n        self.sample_instance.metadata[\'sample_num\'] = self.sample_num\n        self.sample_instance.metadata[\'parent_sample_num\'] = copy.deepcopy(self.metadata[\'sample_num\'])\n        self.sample_instance.metadata[\'parent_sample_type\'] = copy.deepcopy(self.metadata[\'type\'])\n\n        return self.sample_instance\n'"
btgym/datafeed/derivative.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017-2018 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom logbook import WARNING\nfrom .base import BTgymBaseData\nimport datetime\n\n\nclass BTgymEpisode(BTgymBaseData):\n    """"""\n    Low-level data class.\n    Implements `Episode` object containing single episode data sequence.\n    Doesnt allows further sampling and data loading.\n    Supposed to be converted to bt.datafeed object via .to_btfeed() method.\n    Do not use directly.\n    """"""\n    def __init__(\n            self,\n            filename=None,\n            parsing_params=None,\n            sampling_params=None,\n            name=None,\n            data_names=(\'default_asset\',),\n            task=0,\n            log_level=WARNING,\n            _config_stack=None,\n    ):\n\n        super(BTgymEpisode, self).__init__(\n            filename=filename,\n            parsing_params=parsing_params,\n            sampling_params=None,\n            name=\'episode\',\n            task=task,\n            data_names=data_names,\n            log_level=log_level,\n            _config_stack=_config_stack\n        )\n\n    def reset(self, **kwargs):\n        raise RuntimeError(\'Episode object doesnt support .reset() method.\')\n\n    def sample(self, **kwargs):\n        raise RuntimeError(\'Episode object doesnt support .sample() method.\')\n\n\nclass BTgymDataTrial(BTgymBaseData):\n    """"""\n    Intermediate-level data class.\n    Implements conception of `Trial` object.\n    Supports data train/test separation.\n    Do not use directly.\n    """"""\n    trial_params = dict(\n        nested_class_ref=BTgymEpisode,\n    )\n\n    def __init__(\n            self,\n            filename=None,\n            parsing_params=None,\n            sampling_params=None,\n            name=None,\n            data_names=(\'default_asset\',),\n            frozen_time_split=None,\n            task=0,\n            log_level=WARNING,\n            _config_stack=None,\n\n\n    ):\n        """"""\n        Args:\n            filename:           not used;\n            sampling_params:    dict, sample retrieving options, see base class description for details;\n            task:               int, optional;\n            parsing_params:     csv parsing options, see base class description for details;\n            log_level:          int, optional, logbook.level;\n            _config_stack:      dict, holding configuration for nested child samples;\n        """"""\n\n        super(BTgymDataTrial, self).__init__(\n            filename=filename,\n            parsing_params=parsing_params,\n            sampling_params=sampling_params,\n            name=\'Trial\',\n            data_names=data_names,\n            frozen_time_split=frozen_time_split,\n            task=task,\n            log_level=log_level,\n            _config_stack=_config_stack\n        )\n\n\nclass BTgymRandomDataDomain(BTgymBaseData):\n    """"""\n    Top-level data class. Implements one way data domains can be defined,\n    namely when source domain precedes and target one. Implements pipe::\n\n        Domain.sample() --> Trial.sample() --> Episode.to_btfeed() --> bt.Startegy\n\n    This particular class randomly samples Trials from provided dataset.\n\n    """"""\n    # Classes to use for sample objects:\n    trial_class_ref = BTgymDataTrial\n    episode_class_ref = BTgymEpisode\n\n    def __init__(\n            self,\n            trial_params,\n            episode_params,\n            filename=None,\n            dataframe=None,\n            parsing_params=None,\n            target_period=None,\n            use_target_backshift=False,\n            frozen_time_split=None,\n            name=\'RndDataDomain\',\n            task=0,\n            data_names=(\'default_asset\',),\n            log_level=WARNING,\n    ):\n        """"""\n        Args:\n            filename:               Str or list of str, file_names containing CSV historic data;\n            dataframe:          pd.dataframe or iterable of pd.dataframes containing historic data;\n            parsing_params:         csv parsing options, see base class description for details;\n            trial_params:           dict, describes trial parameters, should contain keys:\n                                    {sample_duration, time_gap, start_00, start_weekdays, test_period, expanding};\n            episode_params:         dict, describes episode parameters, should contain keys:\n                                    {sample_duration, time_gap, start_00, start_weekdays};\n\n            target_period:          dict, None or Int, domain target period, def={\'days\': 0, \'hours\': 0, \'minutes\': 0};\n                                    setting this param to non-zero duration forces separation to source/target\n                                    domains (which can be thought of as creating  top-level train/test subsets) with\n                                    target data duration equal to `target_period`;\n                                    if set to None - no target period assumed;\n                                    if set to -1 - no source period assumed;\n                                    Source data always precedes target one.\n            use_target_backshift:   bool, if true - target domain is shifted back by the duration of trial train period,\n                                    thus allowing training on part of target domain data,\n                                    namely train part of the trial closest to source/target break point.\n            name:                   str, optional\n            task:                   int, optional\n            log_level:              int, logbook.level\n        """"""\n        sample_params_keys = {\'sample_duration\', \'time_gap\'}\n\n        assert isinstance(trial_params, dict) and sample_params_keys <= set(trial_params.keys()),\\\n            \'Expected dict. <trial_params> contain keys: {}, got: {}\'.format(sample_params_keys, trial_params)\n\n        assert isinstance(episode_params, dict) and sample_params_keys <= set(episode_params.keys()), \\\n            \'Expected dict. <episode_params> contain keys: {}, got: {}\'.format(sample_params_keys, episode_params)\n\n        if parsing_params is None:\n            parsing_params = dict(\n                # Default parameters for source-specific CSV datafeed class,\n                # correctly parses 1 minute Forex generic ASCII\n                # data files from www.HistData.com:\n\n                # CSV to Pandas params.\n                sep=\';\',\n                header=0,\n                index_col=0,\n                parse_dates=True,\n                names=[\'open\', \'high\', \'low\', \'close\', \'volume\'],\n\n                # Pandas to BT.feeds params:\n                timeframe=1,  # 1 minute.\n                datetime=0,\n                open=1,\n                high=2,\n                low=3,\n                close=4,\n                volume=-1,\n                openinterest=-1,\n            )\n\n        # Hacky cause we want trial test period to be attr of Trial instance\n        # and top-level test (target) period to be attribute of Domain instance:\n        try:\n            trial_test_period = trial_params.pop(\'test_period\')\n\n        except(AttributeError, KeyError):\n            trial_test_period = {\'days\': 0, \'hours\': 0, \'minutes\': 0}\n\n        episode_params.update({\'test_period\': trial_test_period})\n\n        # if target_period is None:\n        #     target_period = {\'days\': 0, \'hours\': 0, \'minutes\': 0}\n\n        trial_params[\'test_period\'] = target_period\n\n        # Setting target backshift:\n        if use_target_backshift:\n            trial_params[\'_test_period_backshift_delta\'] =\\\n                datetime.timedelta(**trial_params[\'sample_duration\']) - datetime.timedelta(**trial_test_period)\n\n        episode_config = dict(\n            class_ref=self.episode_class_ref,\n            kwargs=dict(\n                parsing_params=parsing_params,\n                sampling_params=None,\n                name=\'episode\',\n                task=task,\n                log_level=log_level,\n                _config_stack=None,\n            ),\n        )\n        trial_config = dict(\n            class_ref=self.trial_class_ref,\n            kwargs=dict(\n                parsing_params=parsing_params,\n                sampling_params=episode_params,\n                name=\'trial\',\n                task=task,\n                frozen_time_split=frozen_time_split,\n                log_level=log_level,\n                _config_stack=[episode_config],\n            ),\n        )\n\n        super(BTgymRandomDataDomain, self).__init__(\n            filename=filename,\n            dataframe=dataframe,\n            parsing_params=parsing_params,\n            sampling_params=trial_params,\n            name=name,\n            task=task,\n            frozen_time_split=frozen_time_split,\n            data_names=data_names,\n            log_level=log_level,\n            _config_stack=[episode_config, trial_config]\n        )\n\n\nclass BTgymDataset(BTgymRandomDataDomain):\n    """"""\n    DEPRECATED CLASS, use BTgymDataset2 instead.\n    Does not support dataframe input.\n    Simple top-level data class, implements direct random episode sampling from data set induced by csv file,\n    i.e it is a special case for `Trial=def=Episode`.\n    Supports source and target data domains separation with some caveat - see Note.\n\n    Note:\n        Due to current implementation sampling test episode actually requires sampling test TRIAL.\n        To be improved.\n\n    """"""\n\n    class BTgymSimpleTrial(BTgymDataTrial):\n        """"""\n        Truncated Trial without test period: always samples from train,\n        sampled episode inherits tarin/test metadata of parent trail.\n        """"""\n\n        def sample(self, sample_type=0, **kwargs):\n            episode = self._sample(sample_type=0, **kwargs)\n            episode.metadata[\'type\'] = sample_type\n            return episode\n\n    # Override trial sample class:\n    trial_class_ref = BTgymSimpleTrial\n\n    params_deprecated=dict(\n        episode_len_days=(\'episode_duration\', \'days\'),\n        episode_len_hours=(\'episode_duration\',\'hours\'),\n        episode_len_minutes=(\'episode_duration\', \'minutes\'),\n        time_gap_days=(\'time_gap\', \'days\'),\n        time_gap_hours=(\'time_gap\', \'hours\')\n    )\n\n    def __init__(\n            self,\n            filename=None,\n            episode_duration=None,\n            time_gap=None,\n            start_00=False,\n            start_weekdays=None,\n            parsing_params=None,\n            target_period=None,\n            name=\'SimpleDataSet\',\n            data_names=(\'default_asset\',),\n            log_level=WARNING,\n            **kwargs\n    ):\n        """"""\n        Args:\n            filename:           Str or list of str, file_names containing CSV historic data;\n            episode_duration:   dict, maximum episode duration in d:h:m, def={\'days\': 0, \'hours\': 23, \'minutes\': 55},\n                                alias for `sample_duration`;\n            time_gap:           dict, data time gap allowed within sample in d:h:m, def={\'days\': 0, \'hours\': 6};\n            start_00:           bool, episode start point will be shifted back to first record;\n                                of the day (usually 00:00), def=False;\n            start_weekdays:     list, only weekdays from the list will be used for sample start,\n                                def=[0, 1, 2, 3, 4, 5, 6];\n            target_period:      domain test(aka target) period. def={\'days\': 0, \'hours\': 0, \'minutes\': 0};\n                                setting this param to non-zero duration forces data separation to train/test\n                                subsets. Train data always precedes test one.\n            parsing_params:     csv parsing options, see base class description for details;\n            name:               str, instance name;\n            log_level:          int, logbook.level;\n            **kwargs:           deprecated kwargs;\n        """"""\n        print(\'BTgymDataset class is DEPRECATED, use btgym.datafeed.derivative.BTgymDataset2 instead.\')\n        # Default sample time duration:\n        if episode_duration is None:\n            self._episode_duration = dict(\n                    days=0,\n                    hours=23,\n                    minutes=55,\n                )\n        else:\n            self._episode_duration = episode_duration\n\n        # Default data time gap allowed within sample:\n        if time_gap is None:\n            self._time_gap = dict(\n                days=0,\n                hours=6,\n            )\n        else:\n            self._time_gap = time_gap\n\n        # Default weekdays:\n        if start_weekdays is None:\n            start_weekdays = [0, 1, 2, 3, 4, 5, 6]\n\n        # Insert deprecated params, if any:\n        for key, value in kwargs.items():\n            if key in self.params_deprecated.keys():\n                self.log.warning(\n                    \'Key: <{}> is deprecated, use: <{}> instead\'.format(key, self.params_deprecated[key])\n                )\n                key1, key2 = self.params_deprecated[key]\n                attr = getattr(self, key1)\n                attr[key2] = value\n\n        trial_params = dict(\n            sample_duration=self._episode_duration,\n            start_weekdays=start_weekdays,\n            start_00=start_00,\n            time_gap=self._time_gap,\n            # test_period={\'days\': 0, \'hours\': 0, \'minutes\': 0},\n            test_period=target_period,\n            expanding=False\n        )\n        episode_params = trial_params.copy()\n        super(BTgymDataset, self).__init__(\n            filename=filename,\n            parsing_params=parsing_params,\n            trial_params=trial_params,\n            episode_params=episode_params,\n            target_period=target_period,\n            name=name,\n            data_names=data_names,\n            log_level=log_level,\n        )\n\n\nclass BTgymDataset2(BTgymRandomDataDomain):\n    """"""\n    Simple top-level data class, implements direct random episode sampling from data set induced by csv file,\n    i.e it is a special case for `Trial=def=Episode`.\n    """"""\n    def __init__(\n            self,\n            filename=None,\n            dataframe=None,\n            episode_duration=None,\n            time_gap=None,\n            start_00=False,\n            start_weekdays=None,\n            parsing_params=None,\n            target_period=None,\n            name=\'SimpleDataSet2\',\n            data_names=(\'default_asset\',),\n            log_level=WARNING,\n            **kwargs\n    ):\n        """"""\n        Args:\n            filename:           Str or list of str, file_names containing CSV historic data;\n            dataframe:          pd.dataframe or iterable of pd.dataframes containing historic data;\n            episode_duration:   dict, maximum episode duration in d:h:m, def={\'days\': 0, \'hours\': 23, \'minutes\': 55},\n                                alias for `sample_duration`;\n            time_gap:           dict, data time gap allowed within sample in d:h:m, def={\'days\': 0, \'hours\': 6};\n            start_00:           bool, episode start point will be shifted back to first record;\n                                of the day (usually 00:00), def=False;\n            start_weekdays:     list, only weekdays from the list will be used for sample start,\n                                def=[0, 1, 2, 3, 4, 5, 6];\n            target_period:      domain test(aka target) period. def={\'days\': 0, \'hours\': 0, \'minutes\': 0};\n                                setting this param to non-zero duration forces data separation to train/test\n                                subsets. Train data always precedes test one.\n            parsing_params:     csv parsing options, see base class description for details;\n            name:               str, instance name;\n            log_level:          int, logbook.level;\n            **kwargs:\n        """"""\n        # Default sample time duration:\n        if episode_duration is None:\n            self._episode_duration = dict(\n                    days=0,\n                    hours=23,\n                    minutes=55,\n                )\n        else:\n            self._episode_duration = episode_duration\n\n        # Default data time gap allowed within sample:\n        if time_gap is None:\n            self._time_gap = dict(\n                days=0,\n                hours=6,\n            )\n        else:\n            self._time_gap = time_gap\n\n        # Default weekdays:\n        if start_weekdays is None:\n            start_weekdays = [0, 1, 2, 3, 4, 5, 6]\n\n        trial_params = dict(\n            sample_duration=self._episode_duration,\n            start_weekdays=start_weekdays,\n            start_00=start_00,\n            time_gap=self._time_gap,\n            # test_period={\'days\': 0, \'hours\': 0, \'minutes\': 0},\n            test_period=target_period,\n            expanding=False\n        )\n        episode_params = trial_params.copy()\n        super(BTgymDataset2, self).__init__(\n            filename=filename,\n            dataframe=dataframe,\n            parsing_params=parsing_params,\n            trial_params=trial_params,\n            episode_params=episode_params,\n            target_period=target_period,\n            name=name,\n            data_names=data_names,\n            log_level=log_level,\n        )\n\n'"
btgym/datafeed/multi.py,0,"b'from logbook import Logger, StreamHandler, WARNING\n\nimport datetime\nimport random\nfrom numpy.random import beta as random_beta\nimport copy\nimport os\nimport sys\n\nimport backtrader.feeds as btfeeds\nimport pandas as pd\n\nfrom collections import OrderedDict\n\n\nclass BTgymMultiData:\n    """"""\n    Multiply data streams wrapper.\n    """"""\n\n    def __init__(\n            self,\n            data_class_ref=None,\n            data_config=None,\n            name=\'multi_data\',\n            data_names=None,\n            task=0,\n            log_level=WARNING,\n            **kwargs\n    ):\n        """"""\n        Args:\n            data_class_ref:         one of BTgym single-stream datafeed classes\n            data_config:            nested dictionary of individual data streams sources, see notes below.\n\n            kwargs:                 shared parameters for all data streams, see base dataclass\n\n        Notes:\n            `Data_config` specifies all data sources consumed by strategy::\n\n                data_config = {\n                    data_line_name_0: {\n                        filename: [source csv filename string or list of strings],\n                        [config: {optional dict of individual stream config. params},]\n                    },\n                    ...,\n                    data_line_name_n : {...}\n                }\n\n        Example::\n\n            data_config = {\n                \'usd\': {\'filename\': \'.../DAT_ASCII_EURUSD_M1_2017.csv\'},\n                \'gbp\': {\'filename\': \'.../DAT_ASCII_EURGBP_M1_2017.csv\'},\n                \'jpy\': {\'filename\': \'.../DAT_ASCII_EURJPY_M1_2017.csv\'},\n                \'chf\': {\'filename\': \'.../DAT_ASCII_EURCHF_M1_2017.csv\'},\n            }\n            It is user responsibility to correctly choose historic data conversion rates wrt cash currency (here - EUR).\n        """"""\n        self.data_class_ref = data_class_ref\n        if data_config is None:\n            self.data_config = {}\n\n        else:\n            self.data_config = data_config\n\n        self.master_data = None\n        self.name = name\n        self.task = task\n        self.metadata = {\'sample_num\': 0, \'type\': None}\n        self.filename = None\n        self.is_ready = False\n        self.global_timestamp = 0\n        self.log_level = log_level\n        self.params = {}\n        self.names = []\n        self.sample_num = 0\n\n        # Logging:\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'{}_{}\'.format(self.name, self.task), level=self.log_level)\n\n        if data_names is None:\n            # Infer from data configuration (at top-level):\n            self.data_names = list(self.data_config.keys())\n\n        else:\n            self.data_names = data_names\n        try:\n            assert len(self.data_names) > 0, \'At least one data_line should be provided\'\n\n        except AssertionError:\n            self.log.error(\'At least one data_line should be provided\')\n            raise ValueError\n\n        # Make dictionary of single-stream datasets:\n        self.data = OrderedDict()\n        for key, stream in self.data_config.items():\n            try:\n                stream[\'config\'].update(kwargs)\n\n            except KeyError:\n                stream[\'config\'] = kwargs\n\n            try:\n                if stream[\'dataframe\'] is None:\n                    pass\n\n            except KeyError:\n                stream[\'dataframe\'] = None\n\n            self.data[key] = self.data_class_ref(\n                filename=stream[\'filename\'],\n                dataframe=stream[\'dataframe\'],\n                data_names=(key,),\n                task=task,\n                name=\'{}_{}\'.format(name, key),\n                log_level=log_level,\n                **stream[\'config\']\n            )\n            try:\n                # If master-data has been pointed explicitly by \'base\' kwarg:\n                if stream[\'base\']:\n                    self.master_data = self.data[key]\n\n            except KeyError:\n                pass\n\n    def set_logger(self, level=None, task=None):\n        """"""\n        Sets logbook logger.\n\n        Args:\n            level:  logbook.level, int\n            task:   task id, int\n\n        """"""\n        if task is not None:\n            self.task = task\n\n        if level is not None:\n            for stream in self.data.values():\n                stream.log = Logger(\'{}_{}\'.format(stream.name, stream.task), level=level)\n\n            self.log = Logger(\'{}_{}\'.format(self.name, self.task), level=level)\n\n    def set_params(self, params_dict):\n        """"""\n        Batch attribute setter.\n\n        Args:\n            params_dict: dictionary of parameters to be set as instance attributes.\n        """"""\n        for key, value in params_dict.items():\n            for stream in self.data.values():\n                setattr(stream, key, value)\n\n    def read_csv(self, data_filename=None, force_reload=False):\n        # Load:\n        indexes = []\n        for stream in self.data.values():\n            stream.read_csv(force_reload=force_reload)\n            indexes.append(stream.data.index)\n\n        # Get indexes intersection:\n        if len(indexes) > 1:\n            idx_intersected = indexes[0]\n            for i in range(1, len(indexes)):\n                idx_intersected = idx_intersected.intersection(indexes[i])\n\n            # Truncate data to common index:\n            for stream in self.data.values():\n                stream.data = stream.data.loc[idx_intersected]\n\n    def reset(self, **kwargs):\n        indexes = []\n        for stream in self.data.values():\n            stream.reset(**kwargs)\n            indexes.append(stream.data.index)\n\n        # Get indexes intersection:\n        if len(indexes) > 1:\n            idx_intersected = indexes[0]\n            for i in range(1, len(indexes)):\n                idx_intersected = idx_intersected.intersection(indexes[i])\n\n            idx_intersected.drop_duplicates()\n            self.log.info(\'shared num. records: {}\'.format(len(idx_intersected)))\n\n            # Truncate data to common index:\n            for stream in self.data.values():\n                stream.data = stream.data.loc[idx_intersected]\n\n            # Choose master_data\n            if self.master_data is None:\n                # Just choose first key:\n                all_keys = list(self.data.keys())\n                if len(all_keys) > 0:\n                    self.master_data = self.data[all_keys[0]]\n\n            self.global_timestamp = self.master_data.global_timestamp\n            self.names = self.master_data.names\n        self.sample_num = 0\n        self.is_ready = True\n\n    def set_global_timestamp(self, timestamp):\n        for stream in self.data.values():\n            stream.set_global_timestamp(timestamp)\n\n        self.global_timestamp = self.master_data.global_timestamp\n\n    def describe(self):\n        return {key: stream.describe() for key, stream in self.data.items()}\n\n    def sample(self, **kwargs):\n\n        # Get sample to infer exact interval:\n        self.log.debug(\'Making master sample...\')\n        master_sample = self.master_data.sample(**kwargs)\n        self.log.debug(\'Making master ok.\')\n\n        # Prepare empty instance of multistream data:\n        sample = BTgymMultiData(\n            data_names=self.data_names,\n            task=self.task,\n            log_level=self.log_level,\n            name=\'sub_\' + self.name,\n        )\n        sample.metadata = copy.deepcopy(master_sample.metadata)\n\n        interval = [master_sample.metadata[\'first_row\'], master_sample.metadata[\'last_row\']]\n\n        # Populate sample with data:\n        for key, stream in self.data.items():\n            self.log.debug(\'Sampling <{}> with interval: {}, kwargs: {}\'.format(key, interval, kwargs))\n            sample.data[key] = stream.sample(interval=interval, force_interval=True, **kwargs)\n\n        sample.filename = {key: stream.filename for key, stream in self.data.items()}\n        self.sample_num += 1\n        return sample\n\n    def to_btfeed(self):\n        feed = OrderedDict()\n        for key, stream in self.data.items():\n            # Get single-dataline btfeed dict:\n            feed_dict = stream.to_btfeed()\n            assert len(list(feed_dict.keys())) == 1, \\\n                \'Expected base datafeed dictionary contain single data_line, got: {}\'.format(feed_dict)\n            # Rename every base btfeed according to data_config keys:\n            feed[key] = feed_dict[list(feed_dict.keys())[0]]\n        return feed\n\n\n\n\n'"
btgym/datafeed/stateful.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nimport random\nimport math\nimport datetime\n\nfrom .derivative import BTgymRandomDataDomain\n\n\nclass BTgymSequentialDataDomain(BTgymRandomDataDomain):\n    """"""\n    Top-level sequential data iterator.\n    Implements conception of sliding [or expanding] train/test time-window.\n    Due to sequential nature doesnt support firm source/target domain separation.\n\n    Note:\n\n        Single Trial is defined by support interval (overall duration)  and test interval::\n\n            [trial_start_time=train_start_time <-> train_end_time=test_start_time <-> test_end_time=trial_end_time],\n\n        Sliding time-window data iterating:\n\n        If training is started from the beginningg of the dataset, `train_start_time` is set to that of first record,\n        for example, for the start of the year::\n\n            Trial duration: 10 days; test interval: 2 days, iterating from 0-th, than:\n\n            train interval: 8 days, 0:00:00; test interval: 2 days, 0:00:00.\n\n        Then first trial intervals will be (note that omitted data periods like holidays will not be counted,\n        so overall trial duration is dilated to get proper number of records)::\n\n            Trial #0 @: 2016-01-03 17:01:00 <--> 2016-01-17 17:05:00,\n            and last two days will be reserved for test data\n\n        Since `reset_data()` method call, every next call to `BTgymSequentialDataDomain.sample()` method will return\n        Trial object, such as::\n\n            train_start_time_next_trial = `test_end_time_previous_trial + 1_time_unit\n\n        i.e. next trial will be shifted by the duration of test period.\n\n        Repeats until entire dataset is exhausted.\n\n        Note that while train periods are overlapping, test periods form a partition.\n\n        Here, next trial will be shifted by two days::\n\n            Trial #1 @: 2016-01-05 00:00:00 <--> 2016-01-19 00:10:00\n\n\n        Expanding time-window data iterating:\n\n        Differs from above in a way that trial interval start position is fixed at the earliest time of dataset. Thus,\n        trial support interval is expanding to the right and every subsequent trial is `longer` than previous one\n        by amount of test interval.\n\n        Episodes sampling:\n\n        Episodes sampling is performed in such a way that entire episode duration lies within `Trial` interval.\n\n    """"""\n\n    @staticmethod\n    def _lin_decay(step, param_0, max_steps):\n        # TODO: move to utils\n        """"""\n        Linear decay from param_0 to 1 in `max_steps`.\n        """"""\n        if max_steps > 0:\n            if step <= max_steps:\n                return ((1 - param_0) / max_steps) * step + param_0\n\n            else:\n                return 1.0\n\n        else:\n            return param_0\n\n    @staticmethod\n    def _exp_decay(step, param_0, max_steps, gamma=3.5):\n        # TODO: not used here anymore, move to utils\n        """"""\n        For given step <= max_steps returns exp-decayed value in [param_0, 1]; returns 1 if step > max_steps;\n        gamma - steepness control.\n        """"""\n        if max_steps > 0:\n            if step <= max_steps:\n                step = 2 - step / max_steps\n                return math.exp(step ** gamma - 2 ** gamma) * (param_0 - 1) + 1\n\n            else:\n                return 1.0\n\n        else:\n            return param_0\n\n    def __init__(self, name=\'SeqDataDomain\', **kwargs):\n        """"""\n        Args:\n            filename:           Str or list of str, file_names containing CSV historic data;\n            parsing_params:     csv parsing options, see base class description for details;\n            trial_params:       dict, describes trial parameters, should contain keys:\n                                {sample_duration, time_gap, start_00, start_weekdays, test_period, expanding};\n            episode_params:     dict, describes episode parameters, should contain keys:\n                                {sample_duration, time_gap, start_00, start_weekdays};\n            name:               str, optional\n            task:               int, optional\n            log_level:          int, logbook.level\n\n        Note:\n            - Total number of `Trials` (cardinality) is inferred upon args given and overall dataset size.\n        """"""\n        self.train_range_row = 0\n        self.test_range_row = 0\n\n        self.test_range_row = 0\n        self.test_mean_row = 0\n\n        self.global_step = 0\n        self.total_samples = -1\n        self.sample_num = -1\n        self.sample_stride = -1\n\n        super(BTgymSequentialDataDomain, self).__init__(name=name, **kwargs)\n\n    def sample(self, **kwargs):\n        """"""\n        Iteratively samples from sequence of `Trials`.\n\n        Sampling loop::\n\n            - until Trial_sequence is exhausted or .reset():\n                - sample next Trial in Trial_sequence;\n\n        Args:\n            kwargs:             not used.\n\n        Returns:\n            Trial as `BTgymBaseDataTrial` instance;\n            None, if trial\'s sequence is exhausted.\n        """"""\n        self.sample_instance = self._sample_sequential()\n        if self.sample_instance is None:\n            # Exhausted:\n            return False\n\n        else:\n            self.sample_instance.metadata[\'type\'] = 0  # 0 - always train\n            self.sample_instance.metadata[\'sample_num\'] = self.sample_num\n            self.log.debug(\n                \'got new trial <{}> with metadata: {}\'.\n                format(self.sample_instance.filename, self.sample_instance.metadata)\n            )\n            return self.sample_instance\n\n    def _get_interval(self, sample_num):\n        """"""\n        Defines exact interval and corresponding datetime stamps for Trial\n        Args:\n            sample_num: Trial position in iteration sequence\n\n        Returns:\n            two lists: [first_row, last_row], [start_time, end_time]\n        """"""\n        # First current trial interval:\n        first_row = sample_num * self.sample_stride\n\n        if self.start_00:\n            first_day = self.data[first_row:first_row + 1].index[0]\n            first_row = self.data.index.get_loc(first_day.date(), method=\'nearest\')\n            self.log.debug(\'Trial train start time adjusted to <00:00>\')\n\n        last_row = first_row + self.sample_num_records\n\n        if self.expanding:\n            start_time = self.data.index[0]\n            first_row = 0\n\n        else:\n            start_time = self.data.index[first_row]\n\n        end_time = self.data.index[last_row]\n        return [first_row, last_row], [start_time, end_time]\n\n    def reset(self, global_step=0, total_steps=None, skip_frame=10, data_filename=None, **kwargs):\n        """"""\n        [Re]starts sampling iterator from specified position.\n\n        Args:\n            data_filename:  Str or list of str, file_names containing CSV historic data;\n            global_step:    position in [0, total_steps] interval to start sampling from;\n            total_steps:    max gym environmnet steps allowed for full sweep over `Trials`;\n            skip_frame:     BTGym specific, such as: `total_btgym_dataset_steps = total_steps * skip_frame`;\n        """"""\n        self._reset(data_filename=data_filename, **kwargs)\n\n        # Total gym-environment steps and step training starts with:\n        if total_steps is not None:\n            self.total_steps = total_steps\n            self.global_step = global_step\n            try:\n                assert self.global_step < self.total_steps\n\n            except AssertionError:\n                self.log.exception(\n                    \'Outer space jumps not supported. Got: global_step={} of {}.\'.\n                    format(self.global_step, self.total_steps)\n                )\n                raise AssertionError\n\n        else:\n            self.global_step = 0\n            self.total_steps = -1\n\n        # Infer Trial test support interval in number of records:\n        self.trial_test_range_delta = datetime.timedelta(**self.nested_params[\'sampling_params\'][\'test_period\'])\n        self.trial_test_range_row = int(self.trial_test_range_delta.total_seconds() / (self.timeframe * 60))\n\n        #print(\'self.trial_test_range_delta.total_seconds(): \', self.trial_test_range_delta.total_seconds())\n        #print(\'self.trial_test_range_row: \', self.trial_test_range_row)\n\n        # Infer Trial train support interval in number of records:\n        self.trial_train_range_delta = self.max_sample_len_delta - self.trial_test_range_delta\n\n        try:\n            assert self.trial_train_range_delta.total_seconds() > 0\n\n        except AssertionError:\n            self.log.exception(\n                \'Trial train period should not be negative, got: {}\'.format(self.trial_train_range_delta)\n            )\n            raise AssertionError\n\n        self.trial_train_range_row = int(self.trial_train_range_delta.total_seconds() / (self.timeframe * 60))\n\n        # Infer cardinality of Trials:\n        self.total_samples = int(\n            (self.data.shape[0] - self.trial_train_range_row) / self.trial_test_range_row\n        )\n\n        # Set domain sample stride as duration of Trial test period:\n        self.sample_stride = self.trial_test_range_row\n\n        try:\n            assert self.total_samples > 0\n\n        except AssertionError:\n            self.log.exception(\n                \'Trial`s cardinality below 1. Hint: check data parameters consistency.\'\n            )\n            raise AssertionError\n\n        # Current trial to start with:\n        self.sample_num = int(self.total_samples * self.global_step / self.total_steps)\n\n        if self.expanding:\n            t_type = \'EXPANDING\'\n\n        else:\n            t_type = \'SLIDING\'\n\n        self.log.notice(\n            (\n                \'\\nTrial type: {}; [initial] train interval: {}; test interval: {}.\' +\n                \'\\nCardinality: {}; iterating from: {}.\'\n            ).format(\n                t_type,\n                self.trial_train_range_delta,\n                self.trial_test_range_delta,\n                self.total_samples,\n                self.sample_num,\n            )\n        )\n\n        self.is_ready = True\n\n    def _sample_sequential(self):\n        """"""\n        Iteratively samples Trials.\n        Returns:\n                (Trial instance, trial number)\n                (None, trials_cardinality),  it trials sequence exhausted\n        """"""\n\n        if self.sample_num > self.total_samples:\n            self.is_ready = False\n            self.log.warning(\'Sampling sequence exhausted at {}-th Trial\'.format(self.sample_num))\n            return None\n\n        else:\n            # Get Trial:\n            interval, time = self._get_interval(self.sample_num)\n\n            self.log.notice(\n                \'Trial #{} @: {} <--> {};\'.format(self.sample_num, time[0], time[-1])\n            )\n            self.log.debug(\n                \'Trial #{} rows: {} <--> {}\'.\n                    format(\n                    self.sample_num,\n                    interval[0],\n                    interval[-1]\n                )\n            )\n            trial = self._sample_interval(interval, name=\'sequential_trial_\')\n            self.sample_num += 1\n            return trial\n\n\n'"
btgym/datafeed/test_casual_data.py,0,"b'\nimport unittest\nimport datetime\nfrom .casual import BTgymCasualDataDomain\n\nfrom logbook import WARNING, INFO, DEBUG\n\n\nfilename=\'../examples/data/DAT_ASCII_EURUSD_M1_2016.csv\'\n\ntrial_params=dict(\n    start_weekdays={0, 1, 2, 3, 4, 5, 6},\n    sample_duration={\'days\': 10, \'hours\': 0, \'minutes\': 0},\n    start_00=False,\n    time_gap={\'days\': 5, \'hours\': 0},\n    test_period={\'days\': 3, \'hours\': 0, \'minutes\': 0},\n)\nepisode_params=dict(\n    start_weekdays={0, 1, 2, 3, 4, 5, 6},\n    sample_duration={\'days\': 2, \'hours\': 23, \'minutes\': 55},\n    start_00=False,\n    time_gap={\'days\': 2, \'hours\': 10},\n)\n\n\nlog_level = INFO\n\n\nclass TimeDomainTest(unittest.TestCase):\n    """"""Testing time domain class""""""\n\n    def test_BTgymDataset_sampling_bounds_consistency(self):\n        """"""\n        Any train trial mast precede any test period.\n        Same true for train/test episodes.\n        """"""\n        domain = BTgymCasualDataDomain(\n            filename=filename,\n            trial_params=trial_params,\n            episode_params=episode_params,\n            log_level=log_level,\n        )\n        domain.reset()\n        timestamp = domain.global_timestamp\n\n        for i in range(100):\n            global_time = datetime.datetime.fromtimestamp(timestamp)\n            print(\'\\nGLOBAL_TIME: {}\'.format(global_time))\n            source_trial = domain.sample(get_new=True, sample_type=0, timestamp=timestamp, align_left=0)\n            # print(source_trial.filename)\n            target_trial = domain.sample(get_new=True, sample_type=1, timestamp=timestamp)\n            # print(target_trial.filename)\n\n            source_trial.reset()\n            target_trial.reset()\n            print(\'\\nSource episodes:\')\n            ep_s_0 = source_trial.sample(get_new=True, sample_type=0, timestamp=timestamp)\n            ep_s_1 = source_trial.sample(get_new=True, sample_type=1, timestamp=timestamp)\n            print(\'\\nTarget episodes:\')\n            ep_t_0 = target_trial.sample(get_new=True, sample_type=0, timestamp=timestamp)\n            ep_t_1 = target_trial.sample(get_new=True, sample_type=1, timestamp=timestamp)\n\n            with self.subTest(\n                msg=\'Source train episode final time should be less than source test start time\',\n                source_train_finish=ep_s_0.data.index[-1],\n                source_test_start=ep_s_1.data.index[0]\n            ):\n                self.assertLess(ep_s_0.data.index[-1], ep_s_1.data.index[0])\n\n            with self.subTest(\n                msg=\'Source test episode finish time should be less than global time\',\n                source_test_finish=ep_s_1.data.index[-1],\n                global_time=global_time\n            ):\n                self.assertLessEqual(ep_s_1.data.index[-1], global_time)\n\n            with self.subTest(\n                msg=\'Target train episode finish time should be less than target test episode start time\',\n                target_train_finish=ep_t_0.data.index[-1],\n                target_test_start=ep_t_1.data.index[0],\n            ):\n                self.assertLess(ep_t_0.data.index[-1], ep_t_1.data.index[0])\n\n            with self.subTest(\n                msg=\'Target test episode start time should be no less than global time\',\n                global_time=global_time,\n                target_test_start=ep_t_1.data.index[0]\n            ):\n                self.assertLessEqual(global_time, ep_t_1.data.index[0])\n\n            timestamp += 100000\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
btgym/datafeed/test_data.py,0,"b'\nimport unittest\nfrom .derivative import BTgymDataset, BTgymRandomDataDomain\nfrom .stateful import BTgymSequentialDataDomain\n\n\nfilename=\'../examples/data/DAT_ASCII_EURUSD_M1_2016.csv\'\n\ntrial_params=dict(\n    start_weekdays={0, 1, 2, 3, 4, 5, 6},\n    sample_duration={\'days\': 8, \'hours\': 0, \'minutes\': 0},\n    start_00=False,\n    time_gap={\'days\': 4, \'hours\': 0},\n    test_period={\'days\': 2, \'hours\': 0, \'minutes\': 0},\n)\n\nepisode_params=dict(\n    start_weekdays={0, 1, 2, 3, 4, 5, 6},\n    sample_duration={\'days\': 0, \'hours\': 23, \'minutes\': 55},\n    start_00=False,\n    time_gap={\'days\': 0, \'hours\': 10},\n)\n\ntarget_period={\'days\': 29, \'hours\': 0, \'minutes\': 0}\n\nlog_level=12\n\n\nclass DomainTest(unittest.TestCase):\n    """"""Testing domain class""""""\n\n    def test_domain_target_period_bounds_fail(self):\n        """"""\n        Tests non-zero train size check\n        """"""\n        with self.assertRaises(AssertionError) as cm:\n            domain = BTgymDataset(\n                filename=filename,\n                episode_duration={\'days\': 0, \'hours\': 23, \'minutes\': 55},\n                start_00=False,\n                time_gap={\'days\': 0, \'hours\': 10},\n                target_period={\'days\': 360, \'hours\': 0, \'minutes\': 0}\n            )\n            domain.reset()\n\n    def test_BTgymDataset_sampling_bounds_consistency(self):\n        """"""\n        Any train trial mast precede any test period.\n        Same true for train/test episodes.\n        """"""\n        domain = BTgymDataset(\n          filename=filename,\n          episode_duration={\'days\': 0, \'hours\': 23, \'minutes\': 55},\n          start_00=False,\n          time_gap={\'days\': 0, \'hours\': 10},\n          target_period={\'days\': 40, \'hours\': 0, \'minutes\': 0}\n        )\n        domain.reset()\n        sup_train_time = 0\n        inf_test_time = domain.data[-2:-1].index[0].value\n        for i in range(100):\n            train_trial = domain.sample(get_new=True, sample_type=0)\n            last_train_time = train_trial.data[-2:-1].index[0].value\n            if last_train_time > sup_train_time:\n                sup_train_time = last_train_time\n\n            test_trial = domain.sample(get_new=True, sample_type=1)\n            first_test_time = test_trial.data[0:1].index[0].value\n            if first_test_time < inf_test_time:\n                inf_test_time = first_test_time\n\n            with self.subTest(msg=\'sub_{}\'.format(i), train_trial=train_trial.filename, test_trial=test_trial.filename):\n                self.assertLess(sup_train_time, inf_test_time)\n                with self.subTest(\'Train/test should be irrelevant dor Dataset episodes\'):\n                    train_trial.reset()\n                    test_trial.reset()\n                    episode_1 = test_trial.sample(get_new=True, sample_type=1)\n                    episode_2 = train_trial.sample(get_new=True, sample_type=1)\n\n    def test_BTgymRandomDataDomain_sampling_bounds_consistency(self):\n        """"""\n        Any train trial mast precede any test period.\n        Same true for train/test episodes.\n        """"""\n        rnd_domain = BTgymRandomDataDomain(\n            filename=filename,\n            trial_params=trial_params,\n            episode_params=episode_params,\n            target_period={\'days\': 40, \'hours\': 0, \'minutes\': 0},\n            log_level=log_level,\n        )\n\n        domains = [rnd_domain]\n\n        for domain in domains:\n            with self.subTest(domain=type(domain)):\n                domain.reset()\n                sup_train_time = 0\n                inf_test_time = domain.data[-2:-1].index[0].value\n                for i in range(50):\n                    train_trial = domain.sample(get_new=True, sample_type=0)\n                    last_train_time = train_trial.data[-2:-1].index[0].value\n                    if last_train_time > sup_train_time:\n                        sup_train_time = last_train_time\n\n                    test_trial = domain.sample(get_new=True, sample_type=1)\n                    first_test_time = test_trial.data[0:1].index[0].value\n                    if first_test_time < inf_test_time:\n                        inf_test_time = first_test_time\n\n                    self.assertLess(sup_train_time, inf_test_time)\n\n                    with self.subTest(train_trial=train_trial.filename, test_trial=test_trial.filename):\n                        trials = [train_trial, test_trial]\n\n                        for trial in trials:\n                            with self.subTest(actual_trial=trial.filename):\n                                trial.reset()\n                                e_sup_time = 0\n                                e_inf_time = trial.data[-2:-1].index[0].value\n                                for i in range(20):\n                                    train_episode = trial.sample(get_new=True, sample_type=0)\n                                    e_last_time = train_episode.data[-2:-1].index[0].value\n                                    if e_last_time > e_sup_time:\n                                        e_sup_time = e_last_time\n\n                                    test_episode = trial.sample(get_new=True, sample_type=1)\n                                    e_first_time = test_episode.data[0:1].index[0].value\n                                    if e_first_time < e_inf_time:\n                                        e_inf_time = e_first_time\n\n                                    with self.subTest(\n                                            train_episode=train_episode.filename,\n                                            test_episode=test_episode.filename\n                                    ):\n                                        self.assertLess(e_sup_time, e_inf_time)\n\n    def _BTgymSequentialDataDomain_sampling_bounds_consistency(self):\n        """"""\n        Any train trial mast precede any test period.\n        Same true for train/test episodes.\n        """"""\n        seq_domain = BTgymSequentialDataDomain(\n            filename=filename,\n            trial_params=trial_params,\n            episode_params=episode_params,\n            log_level=log_level,\n        )\n        domains = [seq_domain]\n\n        for domain in domains:\n            with self.subTest(domain=type(domain)):\n                domain.reset()\n                cardinality = domain.total_samples\n                print(\'BTgymSequentialDataDomain cardinality:\', cardinality)\n\n                last_trial_sup = 0\n                for i in range(cardinality - 1):\n                    trial = domain.sample(get_new=True)\n                    trial_sup = trial.data[-2:-1].index[0].value\n\n                    self.assertLess(last_trial_sup, trial_sup)\n\n                    with self.subTest(trial=trial.filename):\n                        trial.reset()\n                        e_train_sup_time = 0\n                        e_test_inf_time = trial.data[-2:-1].index[0].value\n                        for i in range(10):\n                            train_episode = trial.sample(get_new=True, sample_type=0)\n                            e_last_time = train_episode.data[-2:-1].index[0].value\n                            if e_last_time > e_train_sup_time:\n                                e_train_sup_time = e_last_time\n\n                            test_episode = trial.sample(get_new=True, sample_type=1)\n                            e_first_time = test_episode.data[0:1].index[0].value\n                            if e_first_time < e_test_inf_time:\n                                e_test_inf_time = e_first_time\n\n                            with self.subTest(\n                                    msg=\'Train episode precedes Test\',\n                                    train_episode=train_episode.filename,\n                                    test_episode=test_episode.filename\n                            ):\n                                self.assertLess(e_train_sup_time, e_test_inf_time)\n\n                            with self.subTest(\n                                    msg=\'Test episodes form partition\',\n                                    test_episode=test_episode.filename\n                            ):\n                                self.assertLess(last_trial_sup, e_test_inf_time)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n\n\n\n\n'"
btgym/envs/__init__.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom btgym.envs.base import BTgymEnv\nfrom btgym.envs.multidiscrete import MultiDiscreteEnv\n'"
btgym/envs/base.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin, muzikinae@gmail.com\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom logbook import Logger, StreamHandler, WARNING, NOTICE, INFO, DEBUG\nimport sys\nimport time\nimport zmq\nimport os\nimport copy\nimport numpy as np\nimport gym\nfrom gym import spaces\n\nfrom collections import OrderedDict\n\nimport backtrader as bt\n\nfrom btgym import BTgymServer, BTgymBaseStrategy, BTgymDataset, BTgymRendering, BTgymDataFeedServer\nfrom btgym import DictSpace, ActionDictSpace\nfrom btgym.datafeed.multi import BTgymMultiData\n\nfrom btgym.rendering import BTgymNullRendering\n\n############################## OpenAI Gym Environment  ##############################\n\n\nclass BTgymEnv(gym.Env):\n    """"""\n    Base OpenAI Gym API shell for Backtrader backtesting/trading library.\n    """"""\n    # Datafeed Server management:\n    data_master = True\n    data_network_address = \'tcp://127.0.0.1:\'  # using localhost.\n    data_port = 4999\n    data_server = None\n    data_server_pid = None\n    data_context = None\n    data_socket = None\n    data_server_response = None\n\n    # Dataset:\n    dataset = None  # BTgymDataset instance.\n    dataset_stat = None\n\n    # Backtrader engine:\n    engine = None  # bt.Cerbro subclass for server to execute.\n\n    # Strategy:\n    strategy = None  # strategy to use if no <engine> class been passed.\n\n    # Server and network:\n    server = None  # Server process.\n    context = None  # ZMQ context.\n    socket = None  # ZMQ socket, client side.\n    port = 5500  # network port to use.\n    network_address = \'tcp://127.0.0.1:\'  # using localhost.\n    ctrl_actions = (\'_done\', \'_reset\', \'_stop\', \'_getstat\', \'_render\')  # server control messages.\n    server_response = None\n\n    # Connection timeout:\n    connect_timeout = 60  # server connection timeout in seconds.\n    #connect_timeout_step = 0.01  # time between retries in seconds.\n\n    # Rendering:\n    render_enabled = True\n    render_modes = [\'human\', \'episode\',]\n    # `episode` - plotted episode results.\n    # `human` - raw_state observation in conventional human-readable format.\n    #  <obs_space_key> - rendering of arbitrary state presented in observation_space with same key.\n\n    renderer = None  # Rendering support.\n    rendered_rgb = dict()  # Keep last rendered images for each mode.\n\n    # Logging and id:\n    log = None\n    log_level = None  # logbook level: NOTICE, WARNING, INFO, DEBUG etc. or its integer equivalent;\n    verbose = 0  # verbosity mode, valid only if no `log_level` arg has been provided:\n    # 0 - WARNING, 1 - INFO, 2 - DEBUG.\n    task = 0\n    asset_names = (\'default_asset\',)\n    data_lines_names = (\'default_asset\',)\n    cash_name = \'default_cash\'\n\n    random_seed = None\n\n    closed = True\n\n    def __init__(self, **kwargs):\n        """"""\n\n        Keyword Args:\n            filename=None (str, list):                      csv data file.\n            **datafeed_args (any):                          any datafeed-related args, passed through to\n                                                            default btgym.datafeed class.\n            dataset=None (btgym.datafeed):                  BTgymDataDomain instance,\n                                                            overrides `filename` or any other datafeed-related args.\n            strategy=None (btgym.startegy):                 strategy to be used by `engine`, any subclass of\n                                                            btgym.strategy.base.BTgymBaseStrateg\n            engine=None (bt.Cerebro):                       environment simulation engine, any bt.Cerebro subclass,\n                                                            overrides `strategy` arg.\n            network_address=`tcp://127.0.0.1:` (str):       BTGym_server address.\n            port=5500 (int):                                network port to use for server - API_shell communication.\n            data_master=True (bool):                        let this environment control over data_server;\n            data_network_address=`tcp://127.0.0.1:` (str):  data_server address.\n            data_port=4999 (int):                           network port to use for server -- data_server communication.\n            connect_timeout=60 (int):                       server connection timeout in seconds.\n            render_enabled=True (bool):                     enable rendering for this environment;\n            render_modes=[\'human\', \'episode\'] (list):       `episode` - plotted episode results;\n                                                            `human` - raw_state observation.\n            **render_args (any):                            any render-related args, passed through to renderer class.\n            verbose=0 (int):                                verbosity mode, {0 - WARNING, 1 - INFO, 2 - DEBUG}\n            log_level=None (int):                           logbook level {DEBUG=10, INFO=11, NOTICE=12, WARNING=13},\n                                                            overrides `verbose` arg;\n            log=None (logbook.Logger):                      external logbook logger,\n                                                            overrides `log_level` and `verbose` args.\n            task=0 (int):                                   environment id\n            random_seed(int):                               numpy random seed, def: None\n\n        Environment kwargs applying logic::\n\n            if <engine> kwarg is given:\n                do not use default engine and strategy parameters;\n                ignore <strategy> kwarg and all strategy and engine-related kwargs.\n\n            else (no <engine>):\n                use default engine parameters;\n                if any engine-related kwarg is given:\n                    override corresponding default parameter;\n\n                if <strategy> is given:\n                    do not use default strategy parameters;\n                    if any strategy related kwarg is given:\n                        override corresponding strategy parameter;\n\n                else (no <strategy>):\n                    use default strategy parameters;\n                    if any strategy related kwarg is given:\n                        override corresponding strategy parameter;\n\n            if <dataset> kwarg is given:\n                do not use default dataset parameters;\n                ignore dataset related kwargs;\n\n            else (no <dataset>):\n                use default dataset parameters;\n                    if  any dataset related kwarg is given:\n                        override corresponding dataset parameter;\n\n            If any <other> kwarg is given:\n                override corresponding default parameter.\n        """"""\n        # Parameters and default values:\n        self.params = dict(\n\n            # Backtrader engine mandatory parameters:\n            engine=dict(\n                start_cash=100.0,  # initial trading capital.\n                broker_commission=0.001,  # trade execution commission, default is 0.1% of operation value.\n                fixed_stake=10,  # single trade stake is fixed type by def.\n            ),\n            # Dataset mandatory parameters:\n            dataset=dict(\n                filename=None,\n            ),\n            strategy=dict(\n                state_shape=dict(),\n            ),\n            render=dict(),\n        )\n        p2 = dict(  # IS HERE FOR REFERENCE ONLY\n            # Strategy related parameters:\n            # Observation state shape is dictionary of Gym spaces,\n            # at least should contain `raw_state` field.\n            # By convention first dimension of every Gym Box space is time embedding one;\n            # one can define any shape; should match env.observation_space.shape.\n            # observation space state min/max values,\n            # For `raw_state\' - absolute min/max values from BTgymDataset will be used.\n            state_shape=dict(\n                raw=spaces.Box(\n                    shape=(10, 4),\n                    low=-100,\n                    high=100,\n                    dtype=np.float32\n                )\n            ),\n            drawdown_call=None,  # episode maximum drawdown threshold, default is 90% of initial value.\n            portfolio_actions=None,\n                # agent actions,\n                # should consist with BTgymStrategy order execution logic;\n                # defaults are: 0 - \'do nothing\', 1 - \'buy\', 2 - \'sell\', 3 - \'close position\'.\n            skip_frame=None,\n                # Number of environment steps to skip before returning next response,\n                # e.g. if set to 10 -- agent will interact with environment every 10th episode step;\n                # Every other step agent\'s action is assumed to be \'hold\'.\n                # Note: INFO part of environment response is a list of all skipped frame\'s info\'s,\n                #       i.e. [info[-9], info[-8], ..., info[0].\n        )\n        # Update self attributes, remove used kwargs:\n        for key in dir(self):\n            if key in kwargs.keys():\n                setattr(self, key, kwargs.pop(key))\n\n        self.metadata = {\'render.modes\': self.render_modes}\n\n        # Logging and verbosity control:\n        if self.log is None:\n            StreamHandler(sys.stdout).push_application()\n            if self.log_level is None:\n                log_levels = [(0, NOTICE), (1, INFO), (2, DEBUG)]\n                self.log_level = WARNING\n                for key, value in log_levels:\n                    if key == self.verbose:\n                        self.log_level = value\n            self.log = Logger(\'BTgymAPIshell_{}\'.format(self.task), level=self.log_level)\n\n        # Random seeding:\n        np.random.seed(self.random_seed)\n\n        # Network parameters:\n        self.network_address += str(self.port)\n        self.data_network_address += str(self.data_port)\n\n        # Set server rendering:\n        if self.render_enabled:\n            self.renderer = BTgymRendering(self.metadata[\'render.modes\'], log_level=self.log_level, **kwargs)\n\n        else:\n            self.renderer = BTgymNullRendering()\n            self.log.info(\'Rendering disabled. Call to render() will return null-plug image.\')\n\n        # Append logging:\n        self.renderer.log = self.log\n\n        # Update params -1: pull from renderer, remove used kwargs:\n        self.params[\'render\'].update(self.renderer.params)\n        for key in self.params[\'render\'].keys():\n            if key in kwargs.keys():\n                _ = kwargs.pop(key)\n\n        # Disable multiply data streams (multi-assets) [for data-master]:\n        try:\n            assert not isinstance(self.dataset, BTgymMultiData)\n\n        except AssertionError:\n            self.log.error(\n                \'Using multiply data streams with base BTgymEnv class not supported. Use designated class.\'\n            )\n            raise ValueError\n\n        if self.data_master:\n            # DATASET preparation, only data_master executes this:\n            #\n            if self.dataset is not None:\n                # If BTgymDataset instance has been passed:\n                # do nothing.\n                msg = \'Custom Dataset class used.\'\n\n            else:\n                # If no BTgymDataset has been passed,\n                # Make default dataset with given CSV file:\n                try:\n                    os.path.isfile(str(self.params[\'dataset\'][\'filename\']))\n\n                except:\n                    raise FileNotFoundError(\'Dataset source data file not specified/not found\')\n\n                # Use kwargs to instantiate dataset:\n                self.dataset = BTgymDataset(**kwargs)\n                msg = \'Base Dataset class used.\'\n\n            # Append logging:\n            self.dataset.set_logger(self.log_level, self.task)\n\n            # Update params -2: pull from dataset, remove used kwargs:\n            self.params[\'dataset\'].update(self.dataset.params)\n            for key in self.params[\'dataset\'].keys():\n                if key in kwargs.keys():\n                    _ = kwargs.pop(key)\n\n            self.log.info(msg)\n\n        # Connect/Start data server (and get dataset configuration and statistic):\n        self.log.info(\'Connecting data_server...\')\n        self._start_data_server()\n        self.log.info(\'...done.\')\n\n        # After starting data-server we have self.data_names attribute filled.\n\n        # ENGINE preparation:\n        # Update params -3: pull engine-related kwargs, remove used:\n        for key in self.params[\'engine\'].keys():\n            if key in kwargs.keys():\n                self.params[\'engine\'][key] = kwargs.pop(key)\n\n        if self.engine is not None:\n            # If full-blown bt.Cerebro() subclass has been passed:\n            # Update info:\n            msg = \'Custom Cerebro class used.\'\n            self.strategy = msg\n            for key in self.params[\'engine\'].keys():\n                self.params[\'engine\'][key] = msg\n\n        # Note: either way, bt.observers.DrawDown observer [and logger] will be added to any BTgymStrategy instance\n        # by BTgymServer process at runtime.\n\n        else:\n            # Default configuration for Backtrader computational engine (Cerebro),\n            # if no bt.Cerebro() custom subclass has been passed,\n            # get base class Cerebro(), using kwargs on top of defaults:\n            self.engine = bt.Cerebro()\n            msg = \'Base Cerebro class used.\'\n\n            # First, set STRATEGY configuration:\n            if self.strategy is not None:\n                # If custom strategy has been passed:\n                msg2 = \'Custom Strategy class used.\'\n\n            else:\n                # Base class strategy :\n                self.strategy = BTgymBaseStrategy\n                msg2 = \'Base Strategy class used.\'\n\n            # Add, using kwargs on top of defaults:\n            #self.log.debug(\'kwargs for strategy: {}\'.format(kwargs))\n            strat_idx = self.engine.addstrategy(self.strategy, **kwargs)\n\n            msg += \' \' + msg2\n\n            # Second, set Cerebro-level configuration:\n            self.engine.broker.setcash(self.params[\'engine\'][\'start_cash\'])\n            self.engine.broker.setcommission(self.params[\'engine\'][\'broker_commission\'])\n            self.engine.addsizer(bt.sizers.SizerFix, stake=self.params[\'engine\'][\'fixed_stake\'])\n\n        self.log.info(msg)\n\n        # Define observation space shape, minimum / maximum values and agent action space.\n        # Retrieve values from configured engine or...\n\n        # ...Update params -4:\n        # Pull strategy defaults to environment params dict :\n        for t_key, t_value in self.engine.strats[0][0][0].params._gettuple():\n            self.params[\'strategy\'][t_key] = t_value\n\n        # Update it with values from strategy \'passed-to params\':\n        for key, value in self.engine.strats[0][0][2].items():\n            self.params[\'strategy\'][key] = value\n\n        self.asset_names = self.params[\'strategy\'][\'asset_names\']\n        self.server_actions = {name: self.params[\'strategy\'][\'portfolio_actions\'] for name in self.asset_names}\n        self.cash_name = self.params[\'strategy\'][\'cash_name\']\n\n        self.params[\'strategy\'][\'initial_action\'] = self.get_initial_action()\n        self.params[\'strategy\'][\'initial_portfolio_action\'] = self.get_initial_portfolio_action()\n\n        # Only single  asset is supported by base class:\n        try:\n            assert len(list(self.asset_names)) == 1\n\n        except AssertionError:\n            self.log.error(\n                \'Using multiply assets with base BTgymEnv class not supported. Use designated class.\'\n            )\n            raise ValueError\n\n        try:\n            assert set(self.asset_names).issubset(set(self.data_lines_names))\n\n        except AssertionError:\n            msg = \'Assets names should be subset of data_lines names, but got: assets: {}, data_lines: {}\'.format(\n                set(self.asset_names), set(self.data_lines_names)\n            )\n            self.log.error(msg)\n            raise ValueError(msg)\n\n\n        # ... Push it all back (don\'t ask):\n        for key, value in  self.params[\'strategy\'].items():\n            self.engine.strats[0][0][2][key] = value\n\n        # For \'raw_state\' min/max values,\n        # the only way is to infer from raw Dataset price values (we already got those from data_server):\n        if \'raw\' in self.params[\'strategy\'][\'state_shape\'].keys():\n            # Exclude \'volume\' from columns we count:\n            self.dataset_columns.remove(\'volume\')\n\n            #print(self.params[\'strategy\'])\n            #print(\'self.engine.strats[0][0][2]:\', self.engine.strats[0][0][2])\n            #print(\'self.engine.strats[0][0][0].params:\', self.engine.strats[0][0][0].params._gettuple())\n\n            # Override with absolute price min and max values:\n            self.params[\'strategy\'][\'state_shape\'][\'raw\'].low =\\\n                self.engine.strats[0][0][2][\'state_shape\'][\'raw\'].low =\\\n                np.zeros(self.params[\'strategy\'][\'state_shape\'][\'raw\'].shape) +\\\n                self.dataset_stat.loc[\'min\', self.dataset_columns].min()\n\n            self.params[\'strategy\'][\'state_shape\'][\'raw\'].high = \\\n                self.engine.strats[0][0][2][\'state_shape\'][\'raw\'].high = \\\n                np.zeros(self.params[\'strategy\'][\'state_shape\'][\'raw\'].shape) + \\\n                self.dataset_stat.loc[\'max\', self.dataset_columns].max()\n\n            self.log.info(\'Inferring `state[raw]` high/low values form dataset: {:.6f} / {:.6f}.\'.\n                          format(self.dataset_stat.loc[\'min\', self.dataset_columns].min(),\n                                 self.dataset_stat.loc[\'max\', self.dataset_columns].max()))\n\n        # Set observation space shape from engine/strategy parameters:\n        self.observation_space = DictSpace(self.params[\'strategy\'][\'state_shape\'])\n\n        self.log.debug(\'Obs. shape: {}\'.format(self.observation_space.spaces))\n        #self.log.debug(\'Obs. min:\\n{}\\nmax:\\n{}\'.format(self.observation_space.low, self.observation_space.high))\n\n        # Set action space (one-key dict for this class) and corresponding server messages:\n        self.action_space = ActionDictSpace(\n            base_actions=self.params[\'strategy\'][\'portfolio_actions\'],\n            assets=self.asset_names\n        )\n\n        # Finally:\n        self.server_response = None\n        self.env_response = None\n\n        self._start_server()\n        self.closed = False\n\n        self.log.info(\'Environment is ready.\')\n\n    def _seed(self, seed=None):\n        """"""\n        Sets env. random seed.\n\n        Args:\n            seed:   int or None\n        """"""\n        self.random_seed = seed\n        np.random.seed(self.random_seed)\n\n    @staticmethod\n    def _comm_with_timeout( socket, message,):\n        """"""\n        Exchanges messages via socket, timeout sensitive.\n\n        Args:\n            socket: zmq connected socket to communicate via;\n            message: message to send;\n\n        Note:\n            socket zmq.RCVTIMEO and zmq.SNDTIMEO should be set to some finite number of milliseconds.\n\n        Returns:\n            dictionary:\n                `status`: communication result;\n                `message`: received message if status == `ok` or None;\n                `time`: remote side response time.\n        """"""\n        response = dict(\n            status=\'ok\',\n            message=None,\n        )\n        try:\n            socket.send_pyobj(message)\n\n        except zmq.ZMQError as e:\n            if e.errno == zmq.EAGAIN:\n                response[\'status\'] = \'send_failed_due_to_connect_timeout\'\n\n            else:\n                response[\'status\'] = \'send_failed_for_unknown_reason\'\n            return response\n\n        start = time.time()\n        try:\n            response[\'message\'] = socket.recv_pyobj()\n            response[\'time\'] = time.time() - start\n\n        except zmq.ZMQError as e:\n            if e.errno == zmq.EAGAIN:\n                response[\'status\'] = \'receive_failed_due_to_connect_timeout\'\n\n            else:\n                response[\'status\'] = \'receive_failed_for_unknown_reason\'\n            return response\n\n        return response\n\n    def _start_server(self):\n        """"""\n        Configures backtrader REQ/REP server instance and starts server process.\n        """"""\n\n        # Ensure network resources:\n        # 1. Release client-side, if any:\n        if self.context:\n            self.context.destroy()\n            self.socket = None\n\n        # 2. Kill any process using server port:\n        cmd = ""kill $( lsof -i:{} -t ) > /dev/null 2>&1"".format(self.port)\n        os.system(cmd)\n\n        # Set up client channel:\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.REQ)\n        self.socket.setsockopt(zmq.RCVTIMEO, self.connect_timeout * 1000)\n        self.socket.setsockopt(zmq.SNDTIMEO, self.connect_timeout * 1000)\n        self.socket.connect(self.network_address)\n\n        # Configure and start server:\n        self.server = BTgymServer(\n            cerebro=self.engine,\n            render=self.renderer,\n            network_address=self.network_address,\n            data_network_address=self.data_network_address,\n            connect_timeout=self.connect_timeout,\n            log_level=self.log_level,\n            task=self.task,\n        )\n        self.server.daemon = False\n        self.server.start()\n        # Wait for server to startup:\n        time.sleep(1)\n\n        # Check connection:\n        self.log.info(\'Server started, pinging {} ...\'.format(self.network_address))\n\n        self.server_response = self._comm_with_timeout(\n            socket=self.socket,\n            message={\'ctrl\': \'ping!\'}\n        )\n        if self.server_response[\'status\'] in \'ok\':\n            self.log.info(\'Server seems ready with response: <{}>\'.\n                           format(self.server_response[\'message\']))\n\n        else:\n            msg = \'Server unreachable with status: <{}>.\'.format(self.server_response[\'status\'])\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n        self._closed = False\n\n    def _stop_server(self):\n        """"""\n        Stops BT server process, releases network resources.\n        """"""\n        if self.server:\n\n            if self._force_control_mode():\n                # In case server is running and client side is ok:\n                self.socket.send_pyobj({\'ctrl\': \'_stop\'})\n                self.server_response = self.socket.recv_pyobj()\n\n            else:\n                self.server.terminate()\n                self.server.join()\n                self.server_response = \'Server process terminated.\'\n\n            self.log.info(\'{} Exit code: {}\'.format(self.server_response,\n                                                    self.server.exitcode))\n\n        # Release client-side, if any:\n        if self.context:\n            self.context.destroy()\n            self.socket = None\n\n    def _force_control_mode(self):\n        """"""Puts BT server to control mode.\n        """"""\n        # Check is there any faults with server process and connection?\n        network_error = [\n            (not self.server or not self.server.is_alive(), \'No running server found. Hint: forgot to call reset()?\'),\n            (not self.context or self.context.closed, \'No network connection found.\'),\n        ]\n        for (err, msg) in network_error:\n            if err:\n                self.log.info(msg)\n                self.server_response = msg\n                return False\n\n            # If everything works, insist to go \'control\':\n            self.server_response = {}\n            attempt = 0\n\n            while \'ctrl\' not in self.server_response:\n                self.socket.send_pyobj({\'ctrl\': \'_done\'})\n                self.server_response = self.socket.recv_pyobj()\n                attempt += 1\n                self.log.debug(\'FORCE CONTROL MODE attempt: {}.\\nResponse: {}\'.format(attempt, self.server_response))\n\n            return True\n\n    def _assert_response(self, response):\n        """"""\n        Simple watcher:\n        roughly checks if we really talking to environment (== episode is running).\n        Rises exception if response given is not as expected.\n        """"""\n        try:\n            assert type(response) == tuple and len(response) == 4\n\n        except AssertionError:\n            msg = \'Unexpected environment response: {}\\nHint: Forgot to call reset() or reset_data()?\'.format(response)\n            self.log.exception(msg)\n            raise AssertionError(msg)\n\n        self.log.debug(\'Response checker received:\\n{}\\nas type: {}\'.\n                       format(response, type(response)))\n\n    def _print_space(self, space, _tab=\'\'):\n        """"""\n        Parses observation space shape or response.\n\n        Args:\n            space: gym observation space or state.\n\n        Returns:\n            description as string.\n        """"""\n        response = \'\'\n        if type(space) in [dict, OrderedDict]:\n            for key, value in space.items():\n                response += \'\\n{}{}:{}\\n\'.format(_tab, key, self._print_space(value, \'   \'))\n\n        elif type(space) in [spaces.Dict, DictSpace]:\n            for s in space.spaces:\n                response += self._print_space(s, \'   \')\n\n        elif type(space) in [tuple, list]:\n            for i in space:\n                response += self._print_space(i, \'   \')\n\n        elif type(space) == np.ndarray:\n            response += \'\\n{}array of shape: {}, low: {}, high: {}\'.format(_tab, space.shape, space.min(), space.max())\n\n        else:\n            response += \'\\n{}{}, \'.format(_tab, space)\n            try:\n                response += \'low: {}, high: {}\'.format(space.low.min(), space.high.max())\n\n            except (KeyError, AttributeError, ArithmeticError, ValueError) as e:\n                pass\n                #response += \'\\n{}\'.format(e)\n\n        return response\n\n    def get_initial_action(self):\n        return {asset: 0 for asset in self.asset_names}\n\n    def get_initial_portfolio_action(self):\n        return {asset: actions[0] for asset, actions in self.server_actions.items()}\n\n    def reset(self, **kwargs):\n        """"""\n        Implementation of OpenAI Gym env.reset method. Starts new episode. Episode data are sampled\n        according to data provider class logic, controlled via kwargs. Refer `BTgym_Server` and data provider\n        classes for details.\n\n        Args:\n            kwargs:         any kwargs; this dictionary is passed through to BTgym_server side without any checks and\n                            modifications; currently used for data sampling control;\n\n        Returns:\n            observation space state\n\n        Notes:\n            Current kwargs accepted is::\n\n\n                episode_config=dict(\n                    get_new=True,\n                    sample_type=0,\n                    b_alpha=1,\n                    b_beta=1\n                ),\n                trial_config=dict(\n                    get_new=True,\n                    sample_type=0,\n                    b_alpha=1,\n                    b_beta=1\n                )\n\n        """"""\n        # Data Server check:\n        if self.data_master:\n            if not self.data_server or not self.data_server.is_alive():\n                self.log.info(\'No running data_server found, starting...\')\n                self._start_data_server()\n\n            # Domain dataset status check:\n            self.data_server_response = self._comm_with_timeout(\n                socket=self.data_socket,\n                message={\'ctrl\': \'_get_info\'}\n            )\n            if not self.data_server_response[\'message\'][\'dataset_is_ready\']:\n                self.log.info(\n                    \'Data domain `reset()` called prior to `reset_data()` with [possibly inconsistent] defaults.\'\n                )\n                self.reset_data()\n\n        # Server process check:\n        if not self.server or not self.server.is_alive():\n            self.log.info(\'No running server found, starting...\')\n            self._start_server()\n\n        if self._force_control_mode():\n            self.server_response = self._comm_with_timeout(\n                socket=self.socket,\n                message={\'ctrl\': \'_reset\', \'kwargs\': kwargs}\n            )\n            # Get initial environment response:\n            self.env_response = self.step(self.get_initial_action())\n\n            # Check (once) if it is really (o,r,d,i) tuple:\n            self._assert_response(self.env_response)\n\n            # Check (once) if state_space is as expected:\n            try:\n                assert self.observation_space.contains(self.env_response[0])\n\n            except (AssertionError, AttributeError) as e:\n                msg1 = self._print_space(self.observation_space.spaces)\n                msg2 = self._print_space(self.env_response[0])\n                msg3 = \'\'\n                for step_info in self.env_response[-1]:\n                    msg3 += \'{}\\n\'.format(step_info)\n                msg = (\n                    \'\\nState observation shape/range mismatch!\\n\' +\n                    \'Space set by env: \\n{}\\n\' +\n                    \'Space returned by server: \\n{}\\n\' +\n                    \'Full response:\\n{}\\n\' +\n                    \'Reward: {}\\n\' +\n                    \'Done: {}\\n\' +\n                    \'Info:\\n{}\\n\' +\n                    \'Hint: Wrong Strategy.get_state() parameters?\'\n                ).format(\n                    msg1,\n                    msg2,\n                    self.env_response[0],\n                    self.env_response[1],\n                    self.env_response[2],\n                    msg3,\n                )\n                self.log.exception(msg)\n                self._stop_server()\n                raise AssertionError(msg)\n\n            return self.env_response[0]\n\n\n        else:\n            msg = \'Something went wrong. env.reset() can not get response from server.\'\n            self.log.exception(msg)\n            raise ChildProcessError(msg)\n\n    def step(self, action):\n        """"""\n        Implementation of OpenAI Gym env.step() method.\n        Makes a step in the environment.\n\n        Args:\n            action:     int or dict, action compatible to env.action_space\n\n        Returns:\n            tuple (Observation, Reward, Info, Done)\n\n        """"""\n        # If we got int as action - try to treat it as an action for single-valued action space dict:\n        self.log.debug(\'got action: {} as {}\'.format(action, type(action)))\n\n        # Are you in the list, ready to go and all that?\n        if not self.action_space.contains(action):\n            # If action received as scalar - try to convert it to action space:\n            try:\n                a = copy.deepcopy(int(action))\n\n            except Exception as e:\n                self.log.error(\'Received value {} can not be converted to member of action space.\'.format(action))\n                raise e\n\n            action = {key: a for key in self.action_space.spaces.keys()}\n\n            try:\n                # Check again:\n                assert self.action_space.contains(action)\n\n            except AssertionError as e:\n                self.log.error(\'Action from scalar {} --> {} is out of action space.\'.format(a, action))\n                raise e\n\n            self.log.debug(\'got action as scalar: {}, converted to: {}\'.format(a, action))\n\n        if not self._closed\\\n            and (self.socket is not None)\\\n            and not self.socket.closed:\n            pass\n\n        else:\n            msg = (\n                \'\\nAt least one of these is true:\\n\' +\n                \'Environment closed: {}\\n\' +\n                \'Network error [socket doesnt exists or closed]: {}\\n\' +\n                \'Hint: forgot to call reset() or out of action space?\'\n            ).format(\n                self._closed,\n                not self.socket or self.socket.closed,\n            )\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n        # Send action (as dict of strings) to backtrader engine, receive environment response:\n        action_as_dict = {key: self.server_actions[key][value] for key, value in action.items()}\n        #print(\'step: \', action, action_as_dict)\n        env_response = self._comm_with_timeout(\n            socket=self.socket,\n            message={\'action\': action_as_dict}\n        )\n        if not env_response[\'status\'] in \'ok\':\n            msg = \'.step(): server unreachable with status: <{}>.\'.format(env_response[\'status\'])\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n        self.env_response = env_response [\'message\']\n\n        return self.env_response\n\n    def close(self):\n        """"""\n        Implementation of OpenAI Gym env.close method.\n        Puts BTgym server in Control Mode.\n        """"""\n        self.log.debug(\'close.call()\')\n        self._stop_server()\n        self._stop_data_server()\n        self.log.info(\'Environment closed.\')\n\n    def get_stat(self):\n        """"""\n        Returns last run episode statistics.\n\n        Note:\n            when invoked, forces running episode to terminate.\n        """"""\n        if self._force_control_mode():\n            self.socket.send_pyobj({\'ctrl\': \'_getstat\'})\n            return self.socket.recv_pyobj()\n\n        else:\n            return self.server_response\n\n    def render(self, mode=\'other_mode\', close=False):\n        """"""\n        Implementation of OpenAI Gym env.render method.\n        Visualises current environment state.\n\n        Args:\n            `mode`:     str, any of these::\n\n                            `human` - current state observation as price lines;\n                            `episode` - plotted results of last completed episode.\n                            [other_key] - corresponding to any custom observation space key\n        """"""\n        if close:\n            return None\n\n        if not self._closed\\\n            and self.socket\\\n            and not self.socket.closed:\n            pass\n\n        else:\n            msg = (\n                \'\\nCan\'\'t get renderings.\'\n                \'\\nAt least one of these is true:\\n\' +\n                \'Environment closed: {}\\n\' +\n                \'Network error [socket doesnt exists or closed]: {}\\n\' +\n                \'Hint: forgot to call reset()?\'\n            ).format(\n                self._closed,\n                not self.socket or self.socket.closed,\n            )\n            self.log.warning(msg)\n            return None\n        if mode not in self.render_modes:\n            raise ValueError(\'Unexpected render mode {}\'.format(mode))\n        self.socket.send_pyobj({\'ctrl\': \'_render\', \'mode\': mode})\n\n        rgb_array_dict = self.socket.recv_pyobj()\n\n        self.rendered_rgb.update(rgb_array_dict)\n\n        return self.rendered_rgb[mode]\n\n    def _stop(self):\n        """"""\n        Finishes current episode if any, does nothing otherwise. Leaves server running.\n        """"""\n        if self._force_control_mode():\n            self.log.info(\'Episode stop forced.\')\n\n    def _restart_server(self):\n        """"""Restarts server.\n        """"""\n        self._stop_server()\n        self._start_server()\n        self.log.info(\'Server restarted.\')\n\n    def _start_data_server(self):\n        """"""\n        For data_master environment:\n            - configures backtrader REQ/REP server instance and starts server process.\n\n        For others:\n            - establishes network connection to existing data_server.\n        """"""\n        self.data_server = None\n\n        # Ensure network resources:\n        # 1. Release client-side, if any:\n        if self.data_context:\n            self.data_context.destroy()\n            self.data_socket = None\n\n        # Only data_master launches/stops data_server process:\n        if self.data_master:\n            # 2. Kill any process using server port:\n            cmd = ""kill $( lsof -i:{} -t ) > /dev/null 2>&1"".format(self.data_port)\n            os.system(cmd)\n\n            # Configure and start server:\n            self.data_server = BTgymDataFeedServer(\n                dataset=self.dataset,\n                network_address=self.data_network_address,\n                log_level=self.log_level,\n                task=self.task\n            )\n            self.data_server.daemon = False\n            self.data_server.start()\n            # Wait for server to startup\n            time.sleep(1)\n\n        # Set up client channel:\n        self.data_context = zmq.Context()\n        self.data_socket = self.data_context.socket(zmq.REQ)\n        self.data_socket.setsockopt(zmq.RCVTIMEO, self.connect_timeout * 1000)\n        self.data_socket.setsockopt(zmq.SNDTIMEO, self.connect_timeout * 1000)\n        self.data_socket.connect(self.data_network_address)\n\n        # Check connection:\n        self.log.debug(\'Pinging data_server at: {} ...\'.format(self.data_network_address))\n\n        self.data_server_response = self._comm_with_timeout(\n            socket=self.data_socket,\n            message={\'ctrl\': \'ping!\'}\n        )\n        if self.data_server_response[\'status\'] in \'ok\':\n            self.log.debug(\'Data_server seems ready with response: <{}>\'.\n                          format(self.data_server_response[\'message\']))\n\n        else:\n            msg = \'Data_server unreachable with status: <{}>.\'.\\\n                format(self.data_server_response[\'status\'])\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n        # Get info and statistic:\n        self.dataset_stat, self.dataset_columns, self.data_server_pid,  self.data_lines_names = self._get_dataset_info()\n\n    def _stop_data_server(self):\n        """"""\n        For data_master:\n            - stops BT server process, releases network resources.\n        """"""\n        if self.data_master:\n            if self.data_server is not None and self.data_server.is_alive():\n                # In case server is running and is ok:\n                self.data_socket.send_pyobj({\'ctrl\': \'_stop\'})\n                self.data_server_response = self.data_socket.recv_pyobj()\n\n            else:\n                self.data_server.terminate()\n                self.data_server.join()\n                self.data_server_response = \'Data_server process terminated.\'\n\n            self.log.info(\'{} Exit code: {}\'.format(self.data_server_response, self.data_server.exitcode))\n\n        if self.data_context:\n            self.data_context.destroy()\n            self.data_socket = None\n\n    def _restart_data_server(self):\n        """"""\n        Restarts data_server.\n        """"""\n        if self.data_master:\n            self._stop_data_server()\n            self._start_data_server()\n\n    def _get_dataset_info(self):\n        """"""\n        Retrieves dataset configuration and descriptive statistic.\n        """"""\n        self.data_socket.send_pyobj({\'ctrl\': \'_get_info\'})\n        self.data_server_response = self.data_socket.recv_pyobj()\n\n        return self.data_server_response[\'dataset_stat\'],\\\n            self.data_server_response[\'dataset_columns\'],\\\n            self.data_server_response[\'pid\'], \\\n            self.data_server_response[\'data_names\']\n\n    def reset_data(self, **kwargs):\n        """"""\n        Resets data provider class used, whatever it means for that class. Gets data_server ready to provide data.\n        Supposed to be called before first env.reset().\n\n        Note:\n            when invoked, forces running episode to terminate.\n\n        Args:\n            **kwargs:   data provider class .reset() method specific.\n        """"""\n        if self.closed:\n            self._start_server()\n            if self.data_master:\n                self._start_data_server()\n            self.closed = False\n\n        else:\n            _ = self._force_control_mode()\n\n        if self.data_master:\n            if self.data_server is None or not self.data_server.is_alive():\n                self._restart_data_server()\n\n            self.data_server_response = self._comm_with_timeout(\n                socket=self.data_socket,\n                message={\'ctrl\': \'_reset_data\', \'kwargs\': kwargs}\n            )\n            if self.data_server_response[\'status\'] in \'ok\':\n                self.log.debug(\'Dataset seems ready with response: <{}>\'.\n                               format(self.data_server_response[\'message\']))\n\n            else:\n                msg = \'Data_server unreachable with status: <{}>.\'. \\\n                    format(self.data_server_response[\'status\'])\n                self.log.error(msg)\n                raise SystemExit(msg)\n\n        else:\n            pass\n\n\n'"
btgym/envs/multidiscrete.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin, muzikinae@gmail.com\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom logbook import Logger, StreamHandler, WARNING, NOTICE, INFO, DEBUG\nimport sys\nimport numpy as np\n\nimport backtrader as bt\n\nfrom btgym import BTgymRendering, DictSpace, ActionDictSpace\n\nfrom btgym.rendering import BTgymNullRendering\n\nfrom btgym.envs.base import BTgymEnv\n\n\nclass MultiDiscreteEnv(BTgymEnv):\n    """"""\n    OpenAI Gym API shell for Backtrader backtesting/trading library with multiply data streams (assets) support.\n    Action space is dictionary of discrete actions for every asset.\n\n    Multi-asset setup explanation:\n\n        1. This environment expects Dataset to be instance of `btgym.datafeed.multi.BTgymMultiData`, which sets\n        number,  specifications and sampling synchronisation for historic data for all assets\n        one want to trade jointly.\n\n        2. Internally every episodic asset data is converted to single bt.feed and added to environment strategy\n        as separate named data-line (see backtrader docs for extensive explanation of data-lines concept). Strategy is\n        expected to properly handle all received data-lines.\n\n        3. btgym.spaces.ActionDictSpace and order execution. Strategy expects to receive separate action\n        for every asset in form of dictionary: `{asset_name_1: action, ..., asset_name_K: action}`\n        for K assets added, and issues orders for all assets within a single strategy step.\n        It is supposed that actions are discrete [for this environment] and same for every asset.\n        Base actions are set by strategy.params.portfolio_actions, defaults are: (\'hold\', \'buy\', \'sell\', \'close\') which\n        equals to `gym.spaces.Discrete` with depth `N=4 (~number of actions: 0, 1, 2, 3)`.\n        That is, for `K` assets environment action space will be a shallow dictionary `(DictSpace)` of discrete spaces:\n        `{asset_name_1: gym.spaces.Discrete(N), ..., asset_name_K: gym.spaces.Discrete(N)}`\n\n            Example::\n\n                if datalines added via BTgymMultiData are: [\'eurchf\', \'eurgbp\', \'eurgpy\', \'eurusd\'],\n                and base asset actions are [\'hold\', \'buy\', \'sell\', \'close\'], than:\n\n                env.action.space will be:\n                    DictSpace(\n                        {\n                            \'eurchf\': gym.spaces.Discrete(4),\n                            \'eurgbp\': gym.spaces.Discrete(4),\n                            \'eurgpy\': gym.spaces.Discrete(4),\n                            \'eurusd\': gym.spaces.Discrete(4),\n                        }\n                    )\n                single environment action instance (as seen inside strategy):\n                    {\n                        \'eurchf\': \'hold\',\n                        \'eurgbp\': \'buy\',\n                        \'eurgpy\': \'hold\',\n                        \'eurusd\': \'close\',\n                    }\n                corresponding action integer encoding as passed to environment via .step():\n                    {\n                        \'eurchf\': 0,\n                        \'eurgbp\': 1,\n                        \'eurgpy\': 0,\n                        \'eurusd\': 3,\n                    }\n                vector of integers (categorical):\n                    (0, 1, 0, 3)\n\n        4. Environment actions cardinality and encoding. Note that total set of environment actions for `K` assets\n        and `N` base actions is a `cartesian product of K sets of N elements each`. It can be encoded as `vector of integers,\n        single scalar, binary or one_hot`. As cardinality skyrockets with `K`, `multi-discrete` action setup is only suited\n        for small number of assets.\n\n            Example::\n\n                Setup with 4 assets and 4 base actions [hold, buy, sell, close] spawns total of 256 possible\n                environment actions expressed by single integer in [0, 255] or binary encoding:\n                    vector str :                            vector:         int:   binary:\n                    (\'hold\', \'hold\', \'hold\', \'hold\')     -> (0, 0, 0, 0) -> 0   -> 00000000\n                    (\'hold\', \'hold\', \'hold\', \'buy\')      -> (0, 0, 0, 1) -> 1   -> 00000001\n                    ...         ...         ...\n                    (\'close\', \'close\', \'close\', \'sell\')  -> (3, 3, 3, 2) -> 254 -> 11111110\n                    (\'close\', \'close\', \'close\', \'close\') -> (3, 3, 3, 3) -> 255 -> 11111111\n\n        Internally there is some weirdness with encodings as we jump forth and back between\n        dictionary of names or categorical encodings and binary encoding or one-hot encoding.\n        As a rule: strategy operates with dictionary of string names of actions, environment sees action as dictionary\n        of integer numbers while policy estimator operates with either binary or one-hot encoding.\n\n        5. Observation space: is nested DictSpace, where \'external\' part part of space should hold specifications for\n        every asset added.\n\n            Example::\n\n                if datalines added via BTgymMultiData are:\n                    \'eurchf\', \'eurgbp\', \'eurgpy\', \'eurusd\';\n\n                environment observation space should be DictSpace:\n                 {\n                    \'raw\': spaces.Box(low=-1000, high=1000, shape=(128, 4), dtype=np.float32),\n                    \'external\': DictSpace(\n                        {\n                            \'eurusd\': spaces.Box(low=-1000, high=1000, shape=(128, 1, num_features), dtype=np.float32),\n                            \'eurgbp\': spaces.Box(low=-1000, high=1000, shape=(128, 1, num_features), dtype=np.float32),\n                            \'eurchf\': spaces.Box(low=-1000, high=1000, shape=(128, 1, num_features), dtype=np.float32),\n                            \'eurgpy\': spaces.Box(low=-1000, high=1000, shape=(128, 1, num_features), dtype=np.float32),\n                        }\n                    ),\n                    \'internal\': spaces.Box(...),\n                    \'datetime\': spaces.Box(...),\n                    \'metadata\': DictSpace(...)\n                }\n\n                refer to strategies declarations for full code.\n    """"""\n\n    # Datafeed Server management:\n    data_master = True\n    data_network_address = \'tcp://127.0.0.1:\'  # using localhost.\n    data_port = 4999\n    data_server = None\n    data_server_pid = None\n    data_context = None\n    data_socket = None\n    data_server_response = None\n\n    # Dataset:\n    dataset = None  # BTgymDataset instance.\n    dataset_stat = None\n\n    # Backtrader engine:\n    engine = None  # bt.Cerbro subclass for server to execute.\n\n    # Strategy:\n    strategy = None  # strategy to use if no <engine> class been passed.\n\n    # Server and network:\n    server = None  # Server process.\n    context = None  # ZMQ context.\n    socket = None  # ZMQ socket, client side.\n    port = 5500  # network port to use.\n    network_address = \'tcp://127.0.0.1:\'  # using localhost.\n    ctrl_actions = (\'_done\', \'_reset\', \'_stop\', \'_getstat\', \'_render\')  # server control messages.\n    server_response = None\n\n    # Connection timeout:\n    connect_timeout = 60  # server connection timeout in seconds.\n    # connect_timeout_step = 0.01  # time between retries in seconds.\n\n    # Rendering:\n    render_enabled = True\n    render_modes = [\'human\', \'episode\', ]\n    # `episode` - plotted episode results.\n    # `human` - raw_state observation in conventional human-readable format.\n    #  <obs_space_key> - rendering of arbitrary state presented in observation_space with same key.\n\n    renderer = None  # Rendering support.\n    rendered_rgb = dict()  # Keep last rendered images for each mode.\n\n    # Logging and id:\n    log = None\n    log_level = None  # logbook level: NOTICE, WARNING, INFO, DEBUG etc. or its integer equivalent;\n    verbose = 0  # verbosity mode, valid only if no `log_level` arg has been provided:\n    # 0 - WARNING, 1 - INFO, 2 - DEBUG.\n    task = 0\n    asset_names = (\'default_asset\',)\n    data_lines_names = (\'default_asset\',)\n    cash_name = \'default_cash\'\n\n    random_seed = None\n\n    closed = True\n\n    def __init__(self, engine, dataset=None, **kwargs):\n        """"""\n        This class requires dataset, strategy, engine instances to be passed explicitly.\n\n        Args:\n            dataset(btgym.datafeed):                        BTgymDataDomain instance;\n            engine(bt.Cerebro):                             environment simulation engine, any bt.Cerebro subclass,\n\n        Keyword Args:\n            network_address=`tcp://127.0.0.1:` (str):       BTGym_server address.\n            port=5500 (int):                                network port to use for server - API_shell communication.\n            data_master=True (bool):                        let this environment control over data_server;\n            data_network_address=`tcp://127.0.0.1:` (str):  data_server address.\n            data_port=4999 (int):                           network port to use for server -- data_server communication.\n            connect_timeout=60 (int):                       server connection timeout in seconds.\n            render_enabled=True (bool):                     enable rendering for this environment;\n            render_modes=[\'human\', \'episode\'] (list):       `episode` - plotted episode results;\n                                                            `human` - raw_state observation.\n            **render_args (any):                            any render-related args, passed through to renderer class.\n            verbose=0 (int):                                verbosity mode, {0 - WARNING, 1 - INFO, 2 - DEBUG}\n            log_level=None (int):                           logbook level {DEBUG=10, INFO=11, NOTICE=12, WARNING=13},\n                                                            overrides `verbose` arg;\n            log=None (logbook.Logger):                      external logbook logger,\n                                                            overrides `log_level` and `verbose` args.\n            task=0 (int):                                   environment id\n\n\n        """"""\n        self.dataset = dataset\n        self.engine = engine\n        # Parameters and default values:\n        self.params = dict(\n            engine={},\n            dataset={},\n            strategy={},\n            render={},\n        )\n        # Update self attributes, remove used kwargs:\n        for key in dir(self):\n            if key in kwargs.keys():\n                setattr(self, key, kwargs.pop(key))\n\n        self.metadata = {\'render.modes\': self.render_modes}\n\n        # Logging and verbosity control:\n        if self.log is None:\n            StreamHandler(sys.stdout).push_application()\n            if self.log_level is None:\n                log_levels = [(0, NOTICE), (1, INFO), (2, DEBUG)]\n                self.log_level = WARNING\n                for key, value in log_levels:\n                    if key == self.verbose:\n                        self.log_level = value\n            self.log = Logger(\'BTgymMultiDataShell_{}\'.format(self.task), level=self.log_level)\n\n        # Random seeding:\n        np.random.seed(self.random_seed)\n\n        # Network parameters:\n        self.network_address += str(self.port)\n        self.data_network_address += str(self.data_port)\n\n        # Set server rendering:\n        if self.render_enabled:\n            self.renderer = BTgymRendering(self.metadata[\'render.modes\'], log_level=self.log_level, **kwargs)\n\n        else:\n            self.renderer = BTgymNullRendering()\n            self.log.info(\'Rendering disabled. Call to render() will return null-plug image.\')\n\n        # Append logging:\n        self.renderer.log = self.log\n\n        # Update params -1: pull from renderer, remove used kwargs:\n        self.params[\'render\'].update(self.renderer.params)\n        for key in self.params[\'render\'].keys():\n            if key in kwargs.keys():\n                _ = kwargs.pop(key)\n\n        if self.data_master:\n            try:\n                assert self.dataset is not None\n\n            except AssertionError:\n                msg = \'Dataset instance shoud be provided for data_master environment.\'\n                self.log.error(msg)\n                raise ValueError(msg)\n\n            # Append logging:\n            self.dataset.set_logger(self.log_level, self.task)\n\n            # Update params -2: pull from dataset, remove used kwargs:\n            self.params[\'dataset\'].update(self.dataset.params)\n            for key in self.params[\'dataset\'].keys():\n                if key in kwargs.keys():\n                    _ = kwargs.pop(key)\n\n        # Connect/Start data server (and get dataset statistic):\n        self.log.info(\'Connecting data_server...\')\n        self._start_data_server()\n        self.log.info(\'...done.\')\n        # After starting data-server we have self.assets attribute, dataset statisitc etc. filled.\n\n        # Define observation space shape, minimum / maximum values and agent action space.\n        # Retrieve values from configured engine or...\n\n        # ...Update params -4:\n        # Pull strategy defaults to environment params dict :\n        for t_key, t_value in self.engine.strats[0][0][0].params._gettuple():\n            self.params[\'strategy\'][t_key] = t_value\n\n        # Update it with values from strategy \'passed-to params\':\n        for key, value in self.engine.strats[0][0][2].items():\n            self.params[\'strategy\'][key] = value\n\n        self.asset_names = self.params[\'strategy\'][\'asset_names\']\n        self.server_actions = {name: self.params[\'strategy\'][\'portfolio_actions\'] for name in self.asset_names}\n        self.cash_name = self.params[\'strategy\'][\'cash_name\']\n\n        self.params[\'strategy\'][\'initial_action\'] = self.get_initial_action()\n        self.params[\'strategy\'][\'initial_portfolio_action\'] = self.get_initial_portfolio_action()\n\n        # Disabling this check allows derivative assets:\n\n        # try:\n        #     assert set(self.asset_names).issubset(set(self.data_lines_names))\n        #\n        # except AssertionError:\n        #     msg = \'Assets names should be subset of data_lines names, but got: assets: {}, data_lines: {}\'.format(\n        #         set(self.asset_names), set(self.data_lines_names)\n        #     )\n        #     self.log.error(msg)\n        #     raise ValueError(msg)\n\n        # ... Push it all back (don\'t ask):\n        for key, value in self.params[\'strategy\'].items():\n            self.engine.strats[0][0][2][key] = value\n\n        # For \'raw_state\' min/max values,\n        # the only way is to infer from raw Dataset price values (we already got those from data_server):\n        if \'raw_state\' in self.params[\'strategy\'][\'state_shape\'].keys():\n            # Exclude \'volume\' from columns we count:\n            self.dataset_columns.remove(\'volume\')\n\n            # print(self.params[\'strategy\'])\n            # print(\'self.engine.strats[0][0][2]:\', self.engine.strats[0][0][2])\n            # print(\'self.engine.strats[0][0][0].params:\', self.engine.strats[0][0][0].params._gettuple())\n\n            # Override with absolute price min and max values:\n            self.params[\'strategy\'][\'state_shape\'][\'raw_state\'].low = \\\n                self.engine.strats[0][0][2][\'state_shape\'][\'raw_state\'].low = \\\n                np.zeros(self.params[\'strategy\'][\'state_shape\'][\'raw_state\'].shape) + \\\n                self.dataset_stat.loc[\'min\', self.dataset_columns].min()\n\n            self.params[\'strategy\'][\'state_shape\'][\'raw_state\'].high = \\\n                self.engine.strats[0][0][2][\'state_shape\'][\'raw_state\'].high = \\\n                np.zeros(self.params[\'strategy\'][\'state_shape\'][\'raw_state\'].shape) + \\\n                self.dataset_stat.loc[\'max\', self.dataset_columns].max()\n\n            self.log.info(\'Inferring `state_raw` high/low values form dataset: {:.6f} / {:.6f}.\'.\n                          format(self.dataset_stat.loc[\'min\', self.dataset_columns].min(),\n                                 self.dataset_stat.loc[\'max\', self.dataset_columns].max()))\n\n        # Set observation space shape from engine/strategy parameters:\n        self.observation_space = DictSpace(self.params[\'strategy\'][\'state_shape\'])\n\n        self.log.debug(\'Obs. shape: {}\'.format(self.observation_space.spaces))\n\n        # Set action space and corresponding server messages:\n        self.action_space = ActionDictSpace(\n            base_actions=self.params[\'strategy\'][\'portfolio_actions\'],\n            assets=self.asset_names\n        )\n\n        self.log.debug(\'Act. space shape: {}\'.format(self.action_space.spaces))\n\n        # Finally:\n        self.server_response = None\n        self.env_response = None\n\n        # if not self.data_master:\n        self._start_server()\n        self.closed = False\n\n        self.log.info(\'Environment is ready.\')\n\n'"
btgym/envs/portfolio.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin, muzikinae@gmail.com\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom logbook import Logger, StreamHandler, WARNING, NOTICE, INFO, DEBUG\nimport sys\nimport numpy as np\nimport copy\n\nimport backtrader as bt\n\nfrom btgym import BTgymRendering, DictSpace, ActionDictSpace\n\nfrom btgym.rendering import BTgymNullRendering\n\nfrom btgym.envs.base import BTgymEnv\n\n\nclass PortfolioEnv(BTgymEnv):\n    """"""\n        OpenAI Gym API shell for Backtrader backtesting/trading library with multiply assets support.\n        Action space is dictionary of contionious  actions for every asset.\n        This setup closely relates to continuous portfolio optimisation problem definition.\n\n        Setup explanation:\n\n            0. Problem definition.\n            Consider setup with one riskless asset acting as broker account cash and K (by default - one) risky assets.\n            For every risky asset there exists track of historic price records referred as `data-line`.\n            Apart from assets data lines there possibly exists number of exogenous data lines holding some\n            information and statistics, e.g. economic indexes, encoded news, macroeconomic indicators, weather forecasts\n            etc. which are considered relevant and valuable for decision-making.\n            It is supposed for this setup that:\n            i. there is no interest rate for base (riskless) asset;\n            ii. short selling is not permitted;\n            iii. transaction costs are modelled via broker commission;\n            iv. \'market liquidity\' and \'capital impact\' assumptions are met;\n            v. time indexes match for all data lines provided;\n\n            1. Assets and datalines.\n            This environment expects Dataset to be instance of `btgym.datafeed.multi.BTgymMultiData`, which sets\n            number,  specifications and sampling synchronisation for historic data for all assets and data lines.\n\n            Namely, one should define data_config dictionary of `data lines` and list of `assets`.\n            `data_config` specifies all data sources used by strategy, while `assets` defines subset of `data lines`\n            which is supposed to hold historic data for risky portfolio assets.\n\n            Internally every episodic asset data is converted to single bt.feed and added to environment strategy\n            as separate named data_line (see backtrader docs for extensive explanation of data_lines concept).\n            Every non-asset data line as also added as bt.feed with difference that it is not \'tradable\' i.e. it is\n            impossible to issue trade orders on such line.\n            Strategy is expected to properly handle all received data-lines.\n\n                Example::\n\n                    1. Four data streams added via Dataset.data_config,\n                       portfolio consists of four assets, added via strategy_params, cash is EUR:\n\n                        data_config = {\n                            \'usd\': {\'filename\': \'.../DAT_ASCII_EURUSD_M1_2017.csv\'},\n                            \'gbp\': {\'filename\': \'.../DAT_ASCII_EURGBP_M1_2017.csv\'},\n                            \'jpy\': {\'filename\': \'.../DAT_ASCII_EURJPY_M1_2017.csv\'},\n                            \'chf\': {\'filename\': \'.../DAT_ASCII_EURCHF_M1_2017.csv\'},\n                        }\n                        cash_name = \'eur\'\n                        assets_names = [\'usd\', \'gbp\', \'jpy\', \'chf\']\n\n                    2. Three streams added, only two of them form portfolio; DXY stream is `decision-making` only:\n                        data_config = {\n                            \'usd\': {\'filename\': \'.../DAT_ASCII_EURUSD_M1_2017.csv\'},\n                            \'gbp\': {\'filename\': \'.../DAT_ASCII_EURGBP_M1_2017.csv\'},\n                            \'\xe2\x80\x8bDXY\': {\'filename\': \'.../DAT_ASCII_DXY_M1_2017.csv\'},\n                        }\n                        cash_name = \'eur\'\n                        assets_names = [\'usd\', \'gbp\']\n\n\n            2. btgym.spaces.ActionDictSpace and order execution.\n            ActionDictSpace is an extension of OpenAI Gym DictSpace providing domain-specific functionality.\n            Strategy expects to receive separate action for every K+1 asset in form of dictionary:\n            `{cash_name: a[0], asset_name_1: a[1], ..., asset_name_K: a[K]}` for K risky assets added,\n            where base actions are real numbers: `a[i] in [0,1], 0<=i<=K, SUM{a[i]} = 1`. Whole action should be\n            interpreted as order to adjust portfolio to have share `a[i] * 100% for i-th  asset`.\n\n            Therefore, base actions are gym.spaces.Box and for K assets environment action space will be a shallow\n            DictSpace of K+1 continuous spaces: `{cash_name: gym.spaces.Box(low=0, high=1),\n            asset_name_1: gym.spaces.Box(low=0, high=1), ..., asset_name_K: gym.spaces.Box(low=0, high=1)}`\n\n            3. TODO: refine order execution control, see: https://community.backtrader.com/topic/152/multi-asset-ranking-and-rebalancing/2?page=1\n\n                Example::\n\n                    if cash asset is \'eur\',\n                    risky assets added are: [\'chf\', \'gbp\', \'gpy\', \'usd\'],\n                    and data lines added via BTgymMultiData are:\n                    {\n                        \'chf\': eurchf_hist_data_source,\n                        \'gbp\', eurgbp_hist_data_source,\n                        \'jpy\', eurgpy_hist_data_source,\n                        \'usd\', eurusd_hist_data_source,\n                    },\n                    than:\n\n                    env.action.space will be:\n                        DictSpace(\n                            {\n                                \'eur\': gym.spaces.Box(low=0, high=1, dtype=np.float32),\n                                \'chf\': gym.spaces.Box(low=0, high=1, dtype=np.float32),\n                                \'gbp\': gym.spaces.Box(low=0, high=1, dtype=np.float32),\n                                \'jpy\': gym.spaces.Box(low=0, high=1, dtype=np.float32),\n                                \'usd\': gym.spaces.Box(low=0, high=1, dtype=np.float32),\n                            }\n                        )\n\n                    single environment action instance (as seen inside strategy or passed to environment via .step()):\n                        {\n                            \'eur\': 0.3\n                            \'chf\': 0.1,\n                            \'gbp\': 0.1,\n                            \'jpy\': 0.2,\n                            \'usd\': 0.3,\n                        }\n\n                    or vector (unlike multi-asset discrete setup, there is no binary/one hot encoding):\n                        (0.3, 0.1, 0.1, 0.2, 0.3)\n\n                    which says to broker: ""... adjust positions to get 30% in base EUR asset (cash), and amounts of\n                    10%, 10%, 20% and 30% off current portfolio value in CHF, GBP, JPY respectively"".\n\n                    Note that under the hood broker uses `order_target_percent` for every risky asset and can issue\n                    \'sell\', \'buy\' or \'close\' orders depending on positive/negative difference of current to desired\n                    share of asset.\n\n            3. Observation space: is nested DictSpace, where \'external\' part part of space should hold specifications\n            for every data line added (note that cash asset does not have it\'s own data line).\n\n                Example::\n\n                    if data lines added via BTgymMultiData are:\n                        \'chf\', \'gbp\', \'jpy\', \'usd\';\n\n                    environment observation space can be DictSpace:\n                     {\n                        \'external\': DictSpace(\n                            {\n                                \'usd\': spaces.Box(low=-1000, high=1000, shape=(128, 1, num_features), dtype=np.float32),\n                                \'gbp\': spaces.Box(low=-1000, high=1000, shape=(128, 1, num_features), dtype=np.float32),\n                                \'chf\': spaces.Box(low=-1000, high=1000, shape=(128, 1, num_features), dtype=np.float32),\n                                \'jpy\': spaces.Box(low=-1000, high=1000, shape=(128, 1, num_features), dtype=np.float32),\n                            }\n                        ),\n                        \'raw\': spaces.Box(...),\n                        \'internal\': spaces.Box(...),\n                        \'datetime\': spaces.Box(...),\n                        \'metadata\': DictSpace(...)\n                    }\n\n                    refer to strategies declarations for full code.\n\n        """"""\n\n    # Datafeed Server management:\n    data_master = True\n    data_network_address = \'tcp://127.0.0.1:\'  # using localhost.\n    data_port = 4999\n    data_server = None\n    data_server_pid = None\n    data_context = None\n    data_socket = None\n    data_server_response = None\n\n    # Dataset:\n    dataset = None  # BTgymDataset instance.\n    dataset_stat = None\n\n    # Backtrader engine:\n    engine = None  # bt.Cerbro subclass for server to execute.\n\n    # Strategy:\n    strategy = None  # strategy to use if no <engine> class been passed.\n\n    # Server and network:\n    server = None  # Server process.\n    context = None  # ZMQ context.\n    socket = None  # ZMQ socket, client side.\n    port = 5500  # network port to use.\n    network_address = \'tcp://127.0.0.1:\'  # using localhost.\n    ctrl_actions = (\'_done\', \'_reset\', \'_stop\', \'_getstat\', \'_render\')  # server control messages.\n    server_response = None\n\n    # Connection timeout:\n    connect_timeout = 60  # server connection timeout in seconds.\n    # connect_timeout_step = 0.01  # time between retries in seconds.\n\n    # Rendering:\n    render_enabled = True\n    render_modes = [\'human\', \'episode\', ]\n    # `episode` - plotted episode results.\n    # `human` - raw_state observation in conventional human-readable format.\n    #  <obs_space_key> - rendering of arbitrary state presented in observation_space with same key.\n\n    renderer = None  # Rendering support.\n    rendered_rgb = dict()  # Keep last rendered images for each mode.\n\n    # Logging and id:\n    log = None\n    log_level = None  # logbook level: NOTICE, WARNING, INFO, DEBUG etc. or its integer equivalent;\n    verbose = 0  # verbosity mode, valid only if no `log_level` arg has been provided:\n    # 0 - WARNING, 1 - INFO, 2 - DEBUG.\n    task = 0\n    asset_names = (\'default_asset\',)\n    data_lines_names = (\'default_asset\',)\n    cash_name = \'default_cash\'\n\n    random_seed = None\n\n    closed = True\n\n    def __init__(self, engine, dataset=None, **kwargs):\n        """"""\n        This class requires dataset, strategy, engine instances to be passed explicitly.\n\n        Args:\n            dataset(btgym.datafeed):                        BTgymDataDomain instance;\n            engine(bt.Cerebro):                             environment simulation engine, any bt.Cerebro subclass,\n\n        Keyword Args:\n            network_address=`tcp://127.0.0.1:` (str):       BTGym_server address.\n            port=5500 (int):                                network port to use for server - API_shell communication.\n            data_master=True (bool):                        let this environment control over data_server;\n            data_network_address=`tcp://127.0.0.1:` (str):  data_server address.\n            data_port=4999 (int):                           network port to use for server -- data_server communication.\n            connect_timeout=60 (int):                       server connection timeout in seconds.\n            render_enabled=True (bool):                     enable rendering for this environment;\n            render_modes=[\'human\', \'episode\'] (list):       `episode` - plotted episode results;\n                                                            `human` - raw_state observation.\n            **render_args (any):                            any render-related args, passed through to renderer class.\n            verbose=0 (int):                                verbosity mode, {0 - WARNING, 1 - INFO, 2 - DEBUG}\n            log_level=None (int):                           logbook level {DEBUG=10, INFO=11, NOTICE=12, WARNING=13},\n                                                            overrides `verbose` arg;\n            log=None (logbook.Logger):                      external logbook logger,\n                                                            overrides `log_level` and `verbose` args.\n            task=0 (int):                                   environment id\n\n\n        """"""\n        self.dataset = dataset\n        self.engine = engine\n        # Parameters and default values:\n        self.params = dict(\n            engine={},\n            dataset={},\n            strategy={},\n            render={},\n        )\n        # Update self attributes, remove used kwargs:\n        for key in dir(self):\n            if key in kwargs.keys():\n                setattr(self, key, kwargs.pop(key))\n\n        self.metadata = {\'render.modes\': self.render_modes}\n\n        # Logging and verbosity control:\n        if self.log is None:\n            StreamHandler(sys.stdout).push_application()\n            if self.log_level is None:\n                log_levels = [(0, NOTICE), (1, INFO), (2, DEBUG)]\n                self.log_level = WARNING\n                for key, value in log_levels:\n                    if key == self.verbose:\n                        self.log_level = value\n            self.log = Logger(\'BTgymPortfolioShell_{}\'.format(self.task), level=self.log_level)\n\n        # Random seeding:\n        np.random.seed(self.random_seed)\n\n        # Network parameters:\n        self.network_address += str(self.port)\n        self.data_network_address += str(self.data_port)\n\n        # Set server rendering:\n        if self.render_enabled:\n            self.renderer = BTgymRendering(self.metadata[\'render.modes\'], log_level=self.log_level, **kwargs)\n\n        else:\n            self.renderer = BTgymNullRendering()\n            self.log.info(\'Rendering disabled. Call to render() will return null-plug image.\')\n\n        # Append logging:\n        self.renderer.log = self.log\n\n        # Update params -1: pull from renderer, remove used kwargs:\n        self.params[\'render\'].update(self.renderer.params)\n        for key in self.params[\'render\'].keys():\n            if key in kwargs.keys():\n                _ = kwargs.pop(key)\n\n        # self.assets = list(self.dataset.assets)\n\n        if self.data_master:\n            try:\n                assert self.dataset is not None\n\n            except AssertionError:\n                msg = \'Dataset instance shoud be provided for data_master environment.\'\n                self.log.error(msg)\n                raise ValueError(msg)\n\n            # Append logging:\n            self.dataset.set_logger(self.log_level, self.task)\n\n            # Update params -2: pull from dataset, remove used kwargs:\n            self.params[\'dataset\'].update(self.dataset.params)\n            for key in self.params[\'dataset\'].keys():\n                if key in kwargs.keys():\n                    _ = kwargs.pop(key)\n\n        # Connect/Start data server (and get dataset statistic):\n        self.log.info(\'Connecting data_server...\')\n        self._start_data_server()\n        self.log.info(\'...done.\')\n        # After starting data-server we have self.assets attribute, dataset statisitc etc. filled.\n\n        # Define observation space shape, minimum / maximum values and agent action space.\n        # Retrieve values from configured engine or...\n\n        # ...Update params -4:\n        # Pull strategy defaults to environment params dict :\n        for t_key, t_value in self.engine.strats[0][0][0].params._gettuple():\n            self.params[\'strategy\'][t_key] = t_value\n\n        # Update it with values from strategy \'passed-to params\':\n        for key, value in self.engine.strats[0][0][2].items():\n            self.params[\'strategy\'][key] = value\n\n        self.asset_names = self.params[\'strategy\'][\'asset_names\']\n        self.cash_name = self.params[\'strategy\'][\'cash_name\']\n\n        self.params[\'strategy\'][\'initial_action\'] = self.get_initial_action()\n        self.params[\'strategy\'][\'initial_portfolio_action\'] = self.get_initial_action()\n\n        self.server_actions = {name: self.params[\'strategy\'][\'portfolio_actions\'] for name in self.asset_names}\n\n        try:\n            assert set(self.asset_names).issubset(set(self.data_lines_names))\n\n        except AssertionError:\n            msg = \'Assets names should be subset of data_lines names, but got: assets: {}, data_lines: {}\'.format(\n                set(self.asset_names), set(self.data_lines_names)\n            )\n            self.log.error(msg)\n            raise ValueError(msg)\n\n        try:\n            assert self.params[\'strategy\'][\'portfolio_actions\'] is None\n\n        except AssertionError:\n            self.log.debug(\n                \'For continious action space strategy.params[`portfolio_actions`] should be `None`, corrected.\'\n            )\n            self.params[\'strategy\'][\'portfolio_actions\'] = None\n\n        # ... Push it all back (don\'t ask):\n        for key, value in self.params[\'strategy\'].items():\n            self.engine.strats[0][0][2][key] = value\n\n        # For \'raw_state\' min/max values,\n        # the only way is to infer from raw Dataset price values (we already got those from data_server):\n        if \'raw_state\' in self.params[\'strategy\'][\'state_shape\'].keys():\n            # Exclude \'volume\' from columns we count:\n            self.dataset_columns.remove(\'volume\')\n\n            # print(self.params[\'strategy\'])\n            # print(\'self.engine.strats[0][0][2]:\', self.engine.strats[0][0][2])\n            # print(\'self.engine.strats[0][0][0].params:\', self.engine.strats[0][0][0].params._gettuple())\n\n            # Override with absolute price min and max values:\n            self.params[\'strategy\'][\'state_shape\'][\'raw_state\'].low = \\\n                self.engine.strats[0][0][2][\'state_shape\'][\'raw_state\'].low = \\\n                np.zeros(self.params[\'strategy\'][\'state_shape\'][\'raw_state\'].shape) + \\\n                self.dataset_stat.loc[\'min\', self.dataset_columns].min()\n\n            self.params[\'strategy\'][\'state_shape\'][\'raw_state\'].high = \\\n                self.engine.strats[0][0][2][\'state_shape\'][\'raw_state\'].high = \\\n                np.zeros(self.params[\'strategy\'][\'state_shape\'][\'raw_state\'].shape) + \\\n                self.dataset_stat.loc[\'max\', self.dataset_columns].max()\n\n            self.log.info(\'Inferring `state_raw` high/low values form dataset: {:.6f} / {:.6f}.\'.\n                          format(self.dataset_stat.loc[\'min\', self.dataset_columns].min(),\n                                 self.dataset_stat.loc[\'max\', self.dataset_columns].max()))\n\n        # Set observation space shape from engine/strategy parameters:\n        self.observation_space = DictSpace(self.params[\'strategy\'][\'state_shape\'])\n\n        self.log.debug(\'Obs. shape: {}\'.format(self.observation_space.spaces))\n\n        # Set action space and corresponding server messages:\n        self.action_space = ActionDictSpace(\n            base_actions=self.params[\'strategy\'][\'portfolio_actions\'],  # None\n            assets=list(self.asset_names) + [self.cash_name]\n        )\n\n        self.log.debug(\'Act. space shape: {}\'.format(self.action_space.spaces))\n\n        # Finally:\n        self.server_response = None\n        self.env_response = None\n\n        # if not self.data_master:\n        self._start_server()\n        self.closed = False\n\n        self.log.info(\'Environment is ready.\')\n\n    def get_initial_action(self):\n        action = {asset: np.asarray([0.0]) for asset in self.asset_names}\n        action[self.cash_name] = np.asarray([1.0])\n        return action\n\n    def step(self, action):\n        """"""\n        Implementation of OpenAI Gym env.step() method.\n        Makes a step in the environment.\n\n        Args:\n            action:     int or dict, action compatible to env.action_space\n\n        Returns:\n            tuple (Observation, Reward, Info, Done)\n\n        """"""\n        # Are you in the list, ready to go and all that?\n        if self.action_space.contains(action) \\\n                and not self._closed \\\n                and (self.socket is not None) \\\n                and not self.socket.closed:\n            pass\n\n        else:\n            msg = (\n                    \'\\nAt least one of these is true:\\n\' +\n                    \'Action error: (space is {}, action sent is {}): {}\\n\' +\n                    \'Environment closed: {}\\n\' +\n                    \'Network error [socket doesnt exists or closed]: {}\\n\' +\n                    \'Hint: forgot to call reset()?\'\n            ).format(\n                self.action_space, action, not self.action_space.contains(action),\n                self._closed,\n                not self.socket or self.socket.closed,\n            )\n            self.log.exception(msg)\n            raise AssertionError(msg)\n\n        # print(\'step: \', action, action_as_dict)\n        env_response = self._comm_with_timeout(\n            socket=self.socket,\n            message={\'action\': action}\n        )\n        if not env_response[\'status\'] in \'ok\':\n            msg = \'.step(): server unreachable with status: <{}>.\'.format(env_response[\'status\'])\n            self.log.error(msg)\n            raise ConnectionError(msg)\n\n        self.env_response = env_response[\'message\']\n\n        return self.env_response\n'"
btgym/monitor/__init__.py,0,"b""###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom .tensorboard import BTgymMonitor  # 'cause we dont want excessive warnings about Tensorflow requrement\n\nfrom .tensorboard2 import BTgymMonitor2\n"""
btgym/monitor/tensorboard.py,12,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\nfrom subprocess import Popen, PIPE\nimport psutil\nimport glob\n\nimport warnings\n\ntry:\n    import tensorflow as tf\n\nexcept:\n    warnings.warn(\'BTgymMonitor requires Tensorflow\')\n\n    class BTgymMonitor():\n        pass\n\n    quit(1)\n\n\nclass BTgymMonitor():\n    """"""Light tensorflow \'summaries\' wrapper for convenient tensorboard logging.\n    """"""\n    def __init__(self,\n                 scalars={},\n                 images={},\n                 histograms={},\n                 text={},\n                 logdir=\'./tb_log\',\n                 subdir=\'/\',\n                 purge_previous=True,\n                 **kwargs):\n        """"""\n        Monitor parameters:\n        Sets of names for every value category: scalars, images, histograms ant text.\n        logdir - tensorboard log directory;\n        subdir - this monitor log subdirectory;\n        port - localhost webpage addr to look at;\n        reload - web page refresh rate.\n        purge_previous - delete previous logs in logdir/subdir if found.\n        """"""\n        self.tf = tf\n        self.tensorboard = Tensorboard(logdir=logdir, **kwargs)\n        self.logdir = logdir+subdir\n        self.purge_previous = purge_previous\n        self.feed_holder = dict()\n        self.summary = None\n\n        # Remove previous log files if opted:\n        if self.purge_previous:\n            files = glob.glob(self.logdir + \'/*\')\n            p = psutil.Popen([\'rm\', \'-R\', ] + files, stdout=PIPE, stderr=PIPE)\n\n        # Prepare writer:\n        self.tf.reset_default_graph()\n        self.sess = self.tf.Session()\n        self.writer = self.tf.summary.FileWriter(self.logdir, graph=self.tf.get_default_graph())\n\n\n        # Create summary:\n        summaries = []\n\n        for entry in scalars:\n            assert type(entry) == str\n            self.feed_holder[entry] = self.tf.placeholder(self.tf.float32)\n            summaries += [self.tf.summary.scalar(entry, self.feed_holder[entry],)]\n\n        for entry in images:\n            assert type(entry) == str\n            self.feed_holder[entry] = self.tf.placeholder(self.tf.uint8, [None, None, None, 3])\n            summaries += [self.tf.summary.image(entry, self.feed_holder[entry], )]\n\n        for entry in histograms:\n            assert type(entry) == str\n            self.feed_holder[entry] = self.tf.placeholder(self.tf.float32,[None, None],)\n            summaries += [self.tf.summary.histogram(entry, self.feed_holder[entry], )]\n\n        for entry in text:\n            assert type(entry) == str\n            self.feed_holder[entry] = self.tf.placeholder(self.tf.string)\n            summaries += [self.tf.summary.histogram(entry, self.feed_holder[entry], )]\n\n        self.summary = self.tf.summary.merge(summaries)\n\n    def write(self, feed_dict, global_step):\n        """"""\n        Updates monitor with provided data.\n        """"""\n        feeder = dict()\n\n        # Assert feed_dict is ok:\n        try:\n            for key in self.feed_holder.keys():\n                assert key in feed_dict\n                feeder.update({self.feed_holder[key]: feed_dict[key]})\n\n\n        except:\n            raise AssertionError(\'Inconsistent monitor feed:\\nGot: {}\\nExpected: {}\\n\'.\n                                 format(feed_dict.keys(),self.feed_holder.keys())\n                                )\n        # Write down:\n        evaluated = self.sess.run(self.summary, feed_dict=feeder)\n        self.writer.add_summary(summary=evaluated, global_step=global_step)\n        self.writer.flush()\n\n    def close(self):\n        self.writer.close()\n\nclass Tensorboard():\n    """"""\n    Utility class to start/stop tensorboard server.\n    """"""\n    def __init__(self, logdir=\'./btgym_log\', port=6006, reload=30,):\n        """"""____""""""\n        self.port = port\n        self.logdir = logdir\n        self.process = None\n        self.pid = \'\'\n\n        # Compose start command:\n        self.start_string = [\'tensorboard\']\n\n        assert type(logdir) == str\n        self.start_string += [\'--logdir={}\'.format(logdir)]\n\n        assert type(port) == int\n        self.start_string += [\'--port={}\'.format(port)]\n\n        assert type(reload) == int\n        self.start_string += [\'--reload_interval={}\'.format(reload)]\n\n        self.start_string += [\'--purge_orphaned_data\']\n\n    def start(self):\n        """"""Launches Tensorboard app.""""""\n\n        # Kill everything on port-to-use:\n        p = psutil.Popen([\'lsof\', \'-i:{}\'.format(self.port), \'-t\'], stdout=PIPE, stderr=PIPE)\n        self.pid = p.communicate()[0].decode()[:-1]  # retrieving PID\n\n        if self.pid is not \'\':\n            p = psutil.Popen([\'kill\', self.pid])  # , stdout=PIPE, stderr=PIPE)\n\n        # Start:\n        self.process = psutil.Popen(self.start_string)  # , stdout=PIPE, stderr=PIPE)\n\n    def stop(self):\n        """"""Closes tensorboard server.""""""\n        if self.process is not None:\n            self.process.terminate()\n'"
btgym/monitor/tensorboard2.py,10,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\nfrom subprocess import PIPE\nimport psutil\nimport glob\n\nimport warnings\n\ntry:\n    import tensorflow as tf\n\nexcept:\n    warnings.warn(\'BTgymMonitor2 requires Tensorflow\')\n\n    class BTgymMonitor2():\n        pass\n\n    quit(1)\n\n\nclass BTgymMonitor2():\n    """"""\n    Light tensorflow \'summaries\' wrapper for convenient tensorboard logging.\n    """"""\n    def __init__(self,\n                 scalars={},\n                 images={},\n                 histograms={},\n                 text={},\n                 logdir=\'./tb_log\',\n                 subdir=\'/\',\n                 purge_previous=True,\n                 **kwargs):\n        """"""\n        Monitor parameters:\n        Sets of names for every value category: scalars, images, histograms ant text.\n        logdir - tensorboard log directory;\n        subdir - this monitor log subdirectory;\n        port - localhost webpage addr to look at;\n        reload - web page refresh rate.\n        purge_previous - delete previous logs in logdir/subdir if found.\n        """"""\n\n        self.tensorboard = Tensorboard(logdir=logdir, **kwargs)\n        self.logdir = logdir+subdir\n        self.purge_previous = purge_previous\n        self.feed_holder = dict()\n        self.summary = None\n\n        # Remove previous log files if opted:\n        if self.purge_previous:\n            files = glob.glob(self.logdir + \'/*\')\n            p = psutil.Popen([\'rm\', \'-R\', ] + files, stdout=PIPE, stderr=PIPE)\n\n        # Prepare writer:\n        self.writer = tf.summary.FileWriter(self.logdir, graph=tf.get_default_graph())\n\n\n        # Create summary:\n        summaries = []\n\n        for entry in scalars:\n            assert type(entry) == str\n            self.feed_holder[entry] = tf.placeholder(tf.float32)\n            summaries += [tf.summary.scalar(entry, self.feed_holder[entry],)]\n\n        for entry in images:\n            assert type(entry) == str\n            self.feed_holder[entry] = tf.placeholder(tf.uint8, [None, None, None, 3])\n            summaries += [tf.summary.image(entry, self.feed_holder[entry], )]\n\n        for entry in histograms:\n            assert type(entry) == str\n            self.feed_holder[entry] = tf.placeholder(tf.float32,[None, None],)\n            summaries += [tf.summary.histogram(entry, self.feed_holder[entry], )]\n\n        for entry in text:\n            assert type(entry) == str\n            self.feed_holder[entry] = tf.placeholder(tf.string)\n            summaries += [tf.summary.histogram(entry, self.feed_holder[entry], )]\n\n        self.summary = tf.summary.merge(summaries)\n\n    def write(self, sess, feed_dict, global_step):\n        """"""\n        Updates monitor with provided data.\n        """"""\n        feeder = dict()\n\n        # Assert feed_dict is ok:\n        try:\n            for key in self.feed_holder.keys():\n                assert key in feed_dict\n                feeder.update({self.feed_holder[key]: feed_dict[key]})\n\n\n        except:\n            raise AssertionError(\'Inconsistent monitor feed:\\nGot: {}\\nExpected: {}\\n\'.\n                                 format(feed_dict.keys(),self.feed_holder.keys())\n                                )\n        # Write down:\n        evaluated = sess.run(self.summary, feed_dict=feeder)\n        self.writer.add_summary(summary=evaluated, global_step=global_step)\n        self.writer.flush()\n\n    def close(self):\n        self.writer.close()\n\nclass Tensorboard():\n    """"""\n    Utility class to start/stop tensorboard server.\n    """"""\n    def __init__(self, logdir=\'./btgym_log\', port=6006, reload=30,):\n        """"""____""""""\n        self.port = port\n        self.logdir = logdir\n        self.process = None\n        self.pid = \'\'\n\n        # Compose start command:\n        self.start_string = [\'tensorboard\']\n\n        assert type(logdir) == str\n        self.start_string += [\'--logdir={}\'.format(logdir)]\n\n        assert type(port) == int\n        self.start_string += [\'--port={}\'.format(port)]\n\n        assert type(reload) == int\n        self.start_string += [\'--reload_interval={}\'.format(reload)]\n\n        self.start_string += [\'--purge_orphaned_data\']\n\n    def start(self):\n        """"""Launches Tensorboard app.""""""\n\n        # Kill everything on port-to-use:\n        p = psutil.Popen([\'lsof\', \'-i:{}\'.format(self.port), \'-t\'], stdout=PIPE, stderr=PIPE)\n        self.pid = p.communicate()[0].decode()[:-1]  # retrieving PID\n\n        if self.pid is not \'\':\n            p = psutil.Popen([\'kill\', self.pid])  # , stdout=PIPE, stderr=PIPE)\n\n        # Start:\n        self.process = psutil.Popen(self.start_string)  # , stdout=PIPE, stderr=PIPE)\n\n    def stop(self):\n        """"""Closes tensorboard server.""""""\n        if self.process is not None:\n            self.process.terminate()\n'"
btgym/rendering/__init__.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom .plotter import DrawCerebro\n\nfrom .renderer import BTgymRendering, BTgymNullRendering\n\n'"
btgym/rendering/plotter.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\nimport bisect\nimport datetime\nimport multiprocessing\nimport numpy as np\nfrom backtrader.plot import Plot_OldSync\n\n\nclass BTgymPlotter(Plot_OldSync):\n    """"""Hacky way to get cerebro.plot() renderings.\n    Overrides default backtrader plotter behaviour.\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        pass\n        """"""\n        super(BTgymPlotter, self).__init__(**kwargs)\n\n    def savefig(self, fig, filename, width=16, height=9, dpi=300, tight=True,):\n        """"""\n        We neither need picture to appear in <stdout> nor file to be written to disk (slow).\n        Just set params and return `fig` to be converted to rgb array.\n        """"""\n        fig.set_size_inches(width, height)\n        fig.set_dpi(dpi)\n        fig.set_tight_layout(tight)\n        fig.canvas.draw()\n\n\nclass DrawCerebro(multiprocessing.Process):\n    """"""That\'s the way we plot it...\n    """"""\n    def __init__(self, cerebro, width, height, dpi, result_pipe, use=None, rowsmajor=1):\n        super(DrawCerebro, self).__init__()\n        self.result_pipe = result_pipe\n        self.cerebro = cerebro\n        self.plotter = BTgymPlotter()\n        self.width = width\n        self.height = height\n        self.dpi = dpi\n        self.use = use\n        self.rowsmajor=rowsmajor\n\n    def run(self):\n        """"""\n\n        Returns:\n             rgb_array.\n        """"""\n        fig = self.cerebro.plot(plotter=self.plotter,  # Modified above plotter class, doesnt actually saves anything.\n                                savefig=True,\n                                width=self.width,\n                                height=self.height,\n                                dpi=self.dpi,\n                                use=self.use,\n                                iplot=False,\n                                rowsmajor=self.rowsmajor,\n                                figfilename=\'_tmp_btgym_render.png\',\n                               )[0][0]\n        fig.canvas.draw()\n        rgb_string = fig.canvas.tostring_rgb()\n        rgb_shape = fig.canvas.get_width_height()[::-1] + (3,)\n        rgb_array = np.fromstring(rgb_string, dtype=np.uint8, sep=\'\')\n        rgb_array = rgb_array.reshape(rgb_shape)\n\n        try:\n            self.result_pipe.send(rgb_array)\n            self.result_pipe.close()\n\n        except:\n            raise RuntimeError(\'Can not perform episode rendering.\\n\' +\n                               \'Hint: check storage consumption or use: render_enabled=False\')\n        return None\n\n'"
btgym/rendering/renderer.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\nfrom logbook import Logger, StreamHandler, WARNING\nimport sys\nimport numpy as np\n\n#from .plotter import BTgymPlotter\nfrom .plotter import DrawCerebro\n\nclass BTgymRendering():\n    """"""\n    Handles BTgym Environment rendering.\n\n    Note:\n        Call `initialize_pyplot()` method before first render() call!\n    """"""\n    # Here we\'ll keep last rendered image for each rendering mode:\n    rgb_dict = dict()\n    render_modes = [\'episode\', \'human\']\n    params = dict(\n        # Plotting controls, can be passed as kwargs:\n        render_state_as_image=True,\n        render_state_channel=0,\n        render_size_human=(6, 3.5),\n        render_size_state=(7, 3.5),\n        render_size_episode=(12,8),\n        render_rowsmajor_episode=1,\n        render_dpi=75,\n        render_plotstyle=\'seaborn\',\n        render_cmap=\'PRGn\',\n        render_xlabel=\'Relative timesteps\',\n        render_ylabel=\'Value\',\n        render_title=\'local step: {}, state observation min: {:.4f}, max: {:.4f}\',\n        render_boxtext=dict(fontsize=12,\n                            fontweight=\'bold\',\n                            color=\'w\',\n                            bbox={\'facecolor\': \'k\', \'alpha\': 0.3, \'pad\': 3},\n                            ),\n        plt_backend=\'Agg\',  # Not used.\n    )\n    enabled = True\n    ready = False\n\n    def __init__(self, render_modes, **kwargs):\n        """"""\n        Plotting controls, can be passed as kwargs.\n\n        Args:\n            render_state_as_image=True,\n            render_state_channel=0,\n            render_size_human=(6, 3.5),\n            render_size_state=(7, 3.5),\n            render_size_episode=(12,8),\n            render_dpi=75,\n            render_plotstyle=\'seaborn\',\n            render_cmap=\'PRGn\',\n            render_xlabel=\'Relative timesteps\',\n            render_ylabel=\'Value\',\n            render_title=\'local step: {}, state observation min: {:.4f}, max: {:.4f}\',\n            render_boxtext=dict(fontsize=12,\n                                fontweight=\'bold\',\n                                color=\'w\',\n                                bbox={\'facecolor\': \'k\', \'alpha\': 0.3, \'pad\': 3},\n                                )\n        """"""\n        # Update parameters with relevant kwargs:\n        for key, value in kwargs.items():\n            if key in self.params.keys():\n                self.params[key] = value\n\n        # Unpack it as attributes:\n        for key, value in self.params.items():\n                setattr(self, key, value)\n\n        # Logging:\n        if \'log_level\' not in dir(self):\n            self.log_level = WARNING\n\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'BTgymRenderer\', level=self.log_level)\n\n        #from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n        #self.FigureCanvas = FigureCanvas\n\n        self.plt = None  # Will set it inside server process when calling initialize_pyplot().\n\n        #self.plotter = BTgymPlotter() # Modified bt.Cerebro() plotter, to get episode renderings.\n\n        # Set empty plugs for each render mode:\n        self.render_modes = render_modes\n        for mode in self.render_modes:\n            self.rgb_dict[mode] = self.rgb_empty()\n\n    def initialize_pyplot(self):\n        """"""\n        Call me before use!\n        [Supposed to be done inside already running server process]\n        """"""\n        if not self.ready:\n            from multiprocessing import Pipe\n            self.out_pipe, self.in_pipe = Pipe()\n\n            if self.plt is None:\n                import matplotlib\n                matplotlib.use(self.plt_backend, force=True)\n                import matplotlib.pyplot as plt\n\n            self.plt = plt\n            self.ready = True\n\n    def to_string(self, dictionary, excluded=[]):\n        """"""\n        Converts given dictionary to more-or-less good looking `text block` string.\n        """"""\n        text = \'\'\n        for k, v in dictionary.items():\n            if k not in excluded:\n                if type(v) in [float]:\n                    v = \'{:.4f}\'.format(v)\n                text += \'{}: {}\\n\'.format(k, v)\n        return text[:-1]\n\n    def rgb_empty(self):\n        """"""\n        Returns empty \'plug\' image.\n        """"""\n        return (np.random.rand(100, 200, 3) * 255).astype(dtype=np.uint8)\n\n    def parse_response(self, state, mode, reward, info, done,):\n        """"""\n        Converts environment response to plotting attributes:\n        state, title, text.\n        """"""\n        if len(state[mode].shape) <= 2:\n            state = np.asarray(state[mode])\n\n        elif len(state[mode].shape) == 3:\n            if state[mode].shape[1] == 1:\n                # Assume 2nd dim (H) is fake expansion for 1D input, so can render all channels:\n                state = np.asarray(state[mode][:, 0, :])\n\n            else:\n                # Assume it is HWC 2D input, only can render single channel:\n                state = np.asarray(state[mode][:, :, self.render_state_channel])\n\n        else:\n            raise NotImplementedError(\n                \'2D rendering can be done for obs. state tensor with rank <= 3; \' +\\\n                \'got state shape: {}\'.format(np.asarray(state[mode]).shape))\n\n        # Figure out how to deal with info output:\n        try:\n            assert type(info[-1]) == dict\n            info_dict = info[-1]\n\n        except AssertionError:\n            try:\n                assert type(info) == dict\n                info_dict = info\n\n            except AssertionError:\n                try:\n                    info_dict = {\'info\': str(dict)}\n\n                except:\n                    info_dict = {}\n\n        # Add records:\n        info_dict.update(reward=reward, is_done=done,)\n\n        # Try to get step information:\n        try:\n            current_step = info_dict[\'step\']\n\n        except:\n            current_step = \'--\'\n\n        # Set box text, excluding redundant fields:\n        box_text = self.to_string(info_dict, excluded=[\'step\'])\n\n        # Set title output:\n        title = self.render_title.format(current_step, state.min(), state.max())\n\n        return state, title, box_text\n\n    def render(self, mode_list, cerebro=None, step_to_render=None, send_img=True):\n        """"""\n        Renders given mode if possible, else\n        just passes last already rendered image.\n        Returns rgb image as numpy array.\n\n        Logic:\n            - If `cerebro` arg is received:\n                render entire episode, using built-in backtrader plotting feature,\n                update stored `episode` image.\n\n            - If `step_to_render\' arg is received:\n                - if mode = \'raw_state\':\n                    render current state observation in conventional \'price\' format,\n                    update stored `raw_state` image;\n                - if mode = something_else\':\n                    visualise observation as \'seen\' by agent,\n                    update stored \'agent\' image.\n\n        Returns:\n             `mode` image.\n\n        Note:\n            It can actually return several modes in a single dict.\n            It prevented by Gym modes convention, but done internally at the end of the episode.\n        """"""\n        if type(mode_list) == str:\n            mode_list = [mode_list]\n\n        if cerebro is not None:\n            self.rgb_dict[\'episode\'] = self.draw_episode(cerebro)\n            self.log.debug(\'Episode rendering done.\')\n            # Try to render given episode:\n            #try:\n                # Get picture of entire episode:\n            #fig = cerebro.plot(plotter=self.plotter,  # Modified plotter class, doesnt actually save anything.\n            #                   savefig=True,\n            #                   width=self.render_size_episode[0],\n            #                   height=self.render_size_episode[1],\n            #                   dpi=self.render_dpi,\n            #                   use=None, #self.plt_backend,\n            #                   iplot=False,\n            #                   figfilename=\'_tmp_btgym_render.png\',\n            #                   )[0][0]\n\n            #fig.canvas.draw()\n            #rgb_array = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n            #self.rgb_dict[\'episode\'] = rgb_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            # Clean up:\n            #self.plt.gcf().clear()\n            #self.plt.close(fig)\n\n            #except:\n                # Just keep previous rendering\n             #   pass\n\n        if step_to_render is not None:\n            # Perform step rendering:\n\n            # Unpack:\n            raw_state, state, reward, done, info = step_to_render\n\n            for mode in mode_list:\n                if mode in self.render_modes and mode not in [\'episode\', \'human\']:\n                    # Render user-defined (former agent) mode state:\n                    agent_state, title, box_text = self.parse_response(state, mode, reward, info, done)\n                    if self.render_state_as_image:\n                        self.rgb_dict[mode] = self.draw_image(agent_state,\n                                                                 figsize=self.render_size_state,\n                                                                 title=\'{} / {}\'.format(mode, title),\n                                                                 box_text=box_text,\n                                                                 ylabel=self.render_ylabel,\n                                                                 xlabel=self.render_xlabel,\n                                                                 )\n                    else:\n                        self.rgb_dict[mode] = self.draw_plot(agent_state,\n                                                                figsize=self.render_size_state,\n                                                                title=\'{} / {}\'.format(mode, title),\n                                                                box_text=box_text,\n                                                                ylabel=self.render_ylabel,\n                                                                xlabel=self.render_xlabel,\n                                                                )\n\n                if \'human\' in mode:\n                    # Render `human` state:\n                    human_state, title, box_text = self.parse_response(raw_state, mode, reward, info, done)\n                    self.rgb_dict[\'human\'] = self.draw_plot(human_state,\n                                                            figsize=self.render_size_human,\n                                                            title=title,\n                                                            box_text=box_text,\n                                                            ylabel=\'Price\',\n                                                            xlabel=self.render_xlabel,\n                                                            line_labels=[\'Open\', \'High\', \'Low\', \'Close\'],\n                                                            )\n            if send_img:\n                return self.rgb_dict\n\n        else:\n            # this case is for internal use only;\n            # now `mode` supposed to contain several modes, let\'s return dictionary of arrays:\n            return_dict = dict()\n            for entry in mode_list:\n                if entry in self.rgb_dict.keys():\n                    # ...and it is legal:\n                    return_dict[entry] = self.rgb_dict[entry]\n\n                else:\n                    return_dict[entry] = self.rgb_empty()\n\n            return return_dict\n\n    def draw_plot(self, data, figsize=(10,6), title=\'\', box_text=\'\', xlabel=\'X\', ylabel=\'Y\', line_labels=None):\n        """"""\n        Visualises environment state as 2d line plot.\n        Retrurns image as rgb_array.\n\n        Args:\n            data:           np.array of shape [num_values, num_lines]\n            figsize:        figure size (in.)\n            title:\n            box_text:\n            xlabel:\n            ylabel:\n            line_labels:    iterable holding line legends as str\n\n        Returns:\n                rgb image as np.array of size [with, height, 3]\n        """"""\n        if line_labels is None:\n            # If got no labels - make it numbers:\n            if len(data.shape) > 1:\n                line_labels = [\'line_{}\'.format(i) for i in range(data.shape[-1])]\n            else:\n                line_labels = [\'line_0\']\n                data = data[:, None]\n        else:\n            assert len(line_labels) == data.shape[-1], \\\n                \'Expected `line_labels` kwarg consist of {} names, got: {}\'. format(data.shape[-1], line_labels)\n\n        fig = self.plt.figure(figsize=figsize, dpi=self.render_dpi, )\n        #ax = fig.add_subplot(111)\n\n        self.plt.style.use(self.render_plotstyle)\n        self.plt.title(title)\n\n        # Plot x axis as reversed time-step embedding:\n        xticks = np.linspace(data.shape[0] - 1, 0, int(data.shape[0]), dtype=int)\n        self.plt.xticks(xticks.tolist(), (- xticks[::-1]).tolist(), visible=False)\n\n        # Set every 5th tick label visible:\n        for tick in self.plt.xticks()[1][::5]:\n            tick.set_visible(True)\n\n        self.plt.xlabel(xlabel)\n        self.plt.ylabel(ylabel)\n        self.plt.grid(True)\n\n        # Switch off antialiasing:\n        #self.plt.setp([ax.get_xticklines() + ax.get_yticklines() + ax.get_xgridlines() + ax.get_ygridlines()],antialiased=False)\n        #self.plt.rcParams[\'text.antialiased\']=False\n\n        # Add Info box:\n        self.plt.text(0, data.min(), box_text, **self.render_boxtext)\n\n        for line, label in enumerate(line_labels):\n            self.plt.plot(data[:, line], label=label)\n        self.plt.legend()\n        self.plt.tight_layout()\n\n        fig.canvas.draw()\n\n        # Save it to a numpy array:\n        rgb_array = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n\n        # Clean up:\n        self.plt.close(fig)\n        #self.plt.gcf().clear()\n\n        return rgb_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n\n    def draw_image(self, data, figsize=(12,6), title=\'\', box_text=\'\', xlabel=\'X\', ylabel=\'Y\', line_labels=None):\n        """"""\n        Visualises environment state as image.\n        Returns rgb_array.\n        """"""\n        fig = self.plt.figure(figsize=figsize, dpi=self.render_dpi, )\n        #ax = fig.add_subplot(111)\n\n        self.plt.style.use(self.render_plotstyle)\n        self.plt.title(title)\n\n        # Plot x axis as reversed time-step embedding:\n        xticks = np.linspace(data.shape[0] - 1, 0, int(data.shape[0]), dtype=int)\n        self.plt.xticks(xticks.tolist(), (- xticks[::-1]).tolist(), visible=False)\n\n        # Set every 5th tick label visible:\n        for tick in self.plt.xticks()[1][::5]:\n            tick.set_visible(True)\n\n        #self.plt.yticks(visible=False)\n\n        self.plt.xlabel(xlabel)\n        self.plt.ylabel(ylabel)\n        self.plt.grid(False)\n\n        # Switch off antialiasing:\n        # self.plt.setp([ax.get_xticklines() + ax.get_yticklines() + ax.get_xgridlines() + ax.get_ygridlines()],antialiased=False)\n        # self.plt.rcParams[\'text.antialiased\']=False\n        #self.log.warning(\'render_data_shape:{}\'.format(data.shape))\n\n        # Add Info box:\n        self.plt.text(0, data.shape[1] - 1, box_text, **self.render_boxtext)\n\n        im = self.plt.imshow(data.T, aspect=\'auto\', cmap=self.render_cmap)\n        self.plt.colorbar(im, use_gridspec=True)\n\n        self.plt.tight_layout()\n\n        fig.canvas.draw()\n\n        # Save it to a numpy array:\n        rgb_array = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n\n        # Clean up:\n        self.plt.close(fig)\n        #self.plt.gcf().clear()\n\n        #ax.cla()\n        return rgb_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n\n    def draw_episode(self, cerebro):\n        """"""\n        Hacky way to render episode.\n        Due to backtrader/matplotlib memory leaks have to encapsulate it in separate process.\n        Strange but reliable. PID\'s are driving crazy.\n\n        Args:\n            cerebro instance\n\n        Returns:\n            rgb array.\n        """"""\n        draw_process = DrawCerebro(cerebro=cerebro,\n                                   width=self.render_size_episode[0],\n                                   height=self.render_size_episode[1],\n                                   dpi=self.render_dpi,\n                                   result_pipe=self.in_pipe,\n                                   rowsmajor=self.render_rowsmajor_episode,\n                                   )\n\n        draw_process.start()\n        #print(\'Plotter PID: {}\'.format(draw_process.pid))\n        try:\n            rgb_array = self.out_pipe.recv()\n\n            draw_process.terminate()\n            draw_process.join()\n\n            return rgb_array\n\n        except:\n            return self.rgb_empty()\n\n\nclass BTgymNullRendering():\n    """"""\n    Empty renderer to use when resources are concern.\n    """"""\n    enabled = False\n    def __init__(self, *args, **kwargs):\n        self.plug = (np.random.rand(100, 200, 3) * 255).astype(dtype=np.uint8)\n        self.params = {\'rendering\': \'disabled\'}\n        self.render_modes = []\n        # self.log_level = WARNING\n        # StreamHandler(sys.stdout).push_application()\n        # self.log = Logger(\'BTgymRenderer\', level=self.log_level)\n\n    def initialize_pyplot(self):\n        pass\n\n    def render(self, mode_list, **kwargs):\n        # self.log.debug(\'render() call to environment with disabled rendering. Returning dict of null-images.\')\n        if type(mode_list) == str:\n            mode_list = [mode_list]\n        rgb_dict = {}\n        for mode in mode_list:\n            rgb_dict[mode] = self.plug\n\n        return rgb_dict\n\n    def draw_plot(self, *args, **kwargs):\n        # self.log.debug(\'draw_plot() call to environment with disabled rendering. Returning null-image.\')\n        return self.plug\n\n    def draw_image(self, *args, **kwargs):\n        # self.log.debug(\'draw_image() call to environment with disabled rendering. Returning null-image.\')\n        return self.plug\n\n    def draw_episode(self, *args, **kwargs):\n        # self.log.debug(\'draw_episode() call to environment with disabled rendering. Returning null-image.\')\n        return self.plug\n'"
btgym/research/__init__.py,0,"b'#\n# This package contains RnD-grade code.\n# Can be unstable, buggy, poor performing and generally is subject to change.\n#\n\n'"
btgym/research/b_vae_a3c.py,58,"b'from tensorflow.contrib.layers import flatten as batch_flatten\n\nfrom btgym.algorithms.policy.base import BaseAacPolicy\nfrom btgym.algorithms.policy.stacked_lstm import AacStackedRL2Policy\nfrom btgym.algorithms.nn.networks import *\nfrom btgym.algorithms.utils import *\nfrom btgym.algorithms.nn.layers import noisy_linear\nfrom btgym.algorithms.nn.ae import beta_var_conv2d_autoencoder, conv2d_autoencoder\n\nfrom btgym.algorithms import BaseAAC\nfrom btgym.algorithms.nn.losses import beta_vae_loss_def, ae_loss_def\n\n\nclass bVAENPolicy(AacStackedRL2Policy):\n    """"""\n    Stacked LSTM with auxillary b-Variational AutoEncoder loss support and Noisy-net linear layers policy,\n    based on `NAV A3C agent` architecture from\n\n    `LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS` by Mirowski et all. and\n\n    `LEARNING TO REINFORCEMENT LEARN` by JX Wang et all.\n\n    This policy class in conjunction with DataDomain classes from btgym.datafeed\n    mimics RL^2 algorithm from\n\n    `FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING` by Duan et al.\n\n    The difference is `get_initial_features()` method, which has been changed\n    either to reset RNN context to zero-state or return context from the end of previous episode,\n    depending on episode metadata received and `lstm_2_init_period\' parameter.\n\n    Papers:\n\n    https://arxiv.org/pdf/1611.03673.pdf\n\n    https://arxiv.org/pdf/1611.05763.pdf\n\n    https://arxiv.org/abs/1706.10295.pdf\n\n    https://arxiv.org/pdf/1611.02779.pdf\n\n    """"""\n\n    def __init__(self,\n                 ob_space,\n                 ac_space,\n                 rp_sequence_size,\n                 conv_2d_layer_config=(\n                     (32, (3, 1), (2, 1)),\n                     (32, (3, 1), (2, 1)),\n                     (32, (3, 1), (2, 1)),\n                     (32, (3, 1), (2, 1))\n                 ),\n                 lstm_class_ref=tf.contrib.rnn.LayerNormBasicLSTMCell,\n                 lstm_layers=(256, 256),\n                 lstm_2_init_period=50,\n                 linear_layer_ref=noisy_linear,\n                 encoder_class_ref=beta_var_conv2d_autoencoder,\n                 aux_estimate=False,\n                 encode_internal_state=False,\n                 **kwargs):\n        """"""\n        Defines [partially shared] on/off-policy networks for estimating  action-logits, value function,\n        reward and state \'pixel_change\' predictions.\n        Expects multi-modal observation as array of shape `ob_space`.\n\n        Args:\n            ob_space:           dictionary of observation state shapes\n            ac_space:           discrete action space shape (length)\n            rp_sequence_size:   reward prediction sample length\n            lstm_class_ref:     tf.nn.lstm class to use\n            lstm_layers:        tuple of LSTM layers sizes\n            lstm_2_init_period: number of `get_initial_context()` method calls before force LSTM_2 context reset.\n            linear_layer_ref:   linear layer class to use\n            aux_estimate:       (bool), if True - add auxiliary tasks estimations to self.callbacks dictionary.\n            **kwargs            not used\n        """"""\n        # 1D parameters override:\n        # TODO: move to init kwargs\n        kwargs.update(\n            dict(\n                pc_estimator_stride=[2, 1],\n                duell_pc_x_inner_shape=(6, 1, 32),  # [6,3,32] if swapping W-C dims\n                duell_pc_filter_size=(4, 1),\n                duell_pc_stride=(2, 1),\n            )\n        )\n\n        self.ob_space = ob_space\n        self.ac_space = ac_space\n        self.rp_sequence_size = rp_sequence_size\n        self.lstm_class = lstm_class_ref\n        self.lstm_layers = lstm_layers\n        self.aux_estimate = aux_estimate\n        self.callback = {}\n        self.encode_internal_state = encode_internal_state\n        self.debug = {}\n\n        # RL^2 related:\n        self.current_trial_num = -1  # always give initial context at first call\n        self.lstm_2_init_period = lstm_2_init_period\n        self.current_ep_num = 0\n\n        # Placeholders for obs. state input:\n        self.on_state_in = nested_placeholders(ob_space, batch_dim=None, name=\'on_policy_state_in\')\n        self.off_state_in = nested_placeholders(ob_space, batch_dim=None, name=\'off_policy_state_in_pl\')\n        self.rp_state_in = nested_placeholders(ob_space, batch_dim=None, name=\'rp_state_in\')\n\n        # Placeholders for concatenated action [one-hot] and reward [scalar]:\n        self.on_a_r_in = tf.placeholder(tf.float32, [None, ac_space + 1], name=\'on_policy_action_reward_in_pl\')\n        self.off_a_r_in = tf.placeholder(tf.float32, [None, ac_space + 1], name=\'off_policy_action_reward_in_pl\')\n\n        # Placeholders for rnn batch and time-step dimensions:\n        self.on_batch_size = tf.placeholder(tf.int32, name=\'on_policy_batch_size\')\n        self.on_time_length = tf.placeholder(tf.int32, name=\'on_policy_sequence_size\')\n\n        self.off_batch_size = tf.placeholder(tf.int32, name=\'off_policy_batch_size\')\n        self.off_time_length = tf.placeholder(tf.int32, name=\'off_policy_sequence_size\')\n\n        # ============= Base on-policy AAC network ===========\n\n        # Conv. autoencoder:\n        _, on_aac_x_ext, on_decoded_layers_ext, self.on_state_decoded_ext, on_d_kl_ext = encoder_class_ref(\n            inputs=self.on_state_in[\'external\'],\n            layer_config=conv_2d_layer_config,\n            linear_layer_ref=linear_layer_ref,\n            max_batch_size=64,\n            name=\'encoder_external\',\n            reuse=False\n        )\n\n        # VAE KL-divergence output:\n        self.on_vae_d_kl_ext = on_d_kl_ext\n\n        # Reshape rnn inputs for  batch training as [rnn_batch_dim, rnn_time_dim, flattened_depth]:\n        x_shape_dynamic = tf.shape(on_aac_x_ext)\n        max_seq_len = tf.cast(x_shape_dynamic[0] / self.on_batch_size, tf.int32)\n        x_shape_static = on_aac_x_ext.get_shape().as_list()\n\n        on_a_r_in = tf.reshape(self.on_a_r_in, [self.on_batch_size, max_seq_len, ac_space + 1])\n        on_aac_x_ext = tf.reshape( on_aac_x_ext, [self.on_batch_size, max_seq_len, np.prod(x_shape_static[1:])])\n\n        # Prepare `internal` state, if any:\n        if \'internal\' in list(self.on_state_in.keys()):\n            if self.encode_internal_state:\n                # Use convolution encoder:\n                _, on_x_int, on_decoded_layers_int, self.on_state_decoded_int, on_d_kl_int = encoder_class_ref(\n                    inputs=self.on_state_in[\'internal\'],\n                    layer_config=conv_2d_layer_config,\n                    linear_layer_ref=linear_layer_ref,\n                    max_batch_size=64,\n                    name=\'encoder_internal\',\n                    reuse=False\n                )\n                # VAE KL-divergence output:\n                self.on_vae_d_kl_int = on_d_kl_int\n\n                x_int_shape_static = on_x_int.get_shape().as_list()\n                on_x_int = [\n                    tf.reshape(on_x_int, [self.on_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])])]\n                self.debug[\'state_internal_enc\'] = tf.shape(on_x_int)\n\n            else:\n                # Feed as is:\n                x_int_shape_static = self.on_state_in[\'internal\'].get_shape().as_list()\n                on_x_int = tf.reshape(\n                    self.on_state_in[\'internal\'],\n                    [self.on_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n                )\n                self.debug[\'state_internal\'] = tf.shape(self.on_state_in[\'internal\'])\n                on_x_int = [on_x_int]\n                self.on_state_decoded_int = None\n                self.on_vae_d_kl_int = None\n\n        else:\n            on_x_int = []\n            self.on_state_decoded_int = None\n            self.on_vae_d_kl_int = None\n\n        self.debug[\'conv_input_to_lstm1\'] = tf.shape(on_aac_x_ext)\n\n        # Feed last last_reward into LSTM_1 layer along with encoded `external` state features:\n        on_stage2_1_input = [on_aac_x_ext, on_a_r_in[..., -1][..., None]] #+ on_x_internal\n\n        # Feed last_action, encoded `external` state,  `internal` state into LSTM_2:\n        on_stage2_2_input = [on_aac_x_ext, on_a_r_in] + on_x_int\n\n        # LSTM_1 full input:\n        on_aac_x_ext = tf.concat(on_stage2_1_input, axis=-1)\n\n        self.debug[\'concat_input_to_lstm1\'] = tf.shape(on_aac_x_ext)\n\n        # First LSTM layer takes encoded `external` state:\n        [on_x_lstm_1_out, self.on_lstm_1_init_state, self.on_lstm_1_state_out, self.on_lstm_1_state_pl_flatten] =\\\n            lstm_network(on_aac_x_ext, self.on_time_length, lstm_class_ref, (lstm_layers[0],), name=\'lstm_1\')\n\n        self.debug[\'on_x_lstm_1_out\'] = tf.shape(on_x_lstm_1_out)\n        self.debug[\'self.on_lstm_1_state_out\'] = tf.shape(self.on_lstm_1_state_out)\n        self.debug[\'self.on_lstm_1_state_pl_flatten\'] = tf.shape(self.on_lstm_1_state_pl_flatten)\n\n        # For time_flat only: Reshape on_lstm_1_state_out from [1,2,20,size] -->[20,1,2,size] --> [20,1, 2xsize]:\n        reshape_lstm_1_state_out = tf.transpose(self.on_lstm_1_state_out, [2, 0, 1, 3])\n        reshape_lstm_1_state_out_shape_static = reshape_lstm_1_state_out.get_shape().as_list()\n        reshape_lstm_1_state_out = tf.reshape(\n            reshape_lstm_1_state_out,\n            [self.on_batch_size, max_seq_len, np.prod(reshape_lstm_1_state_out_shape_static[-2:])],\n        )\n        #self.debug[\'reshape_lstm_1_state_out\'] = tf.shape(reshape_lstm_1_state_out)\n\n        # Take policy logits off first LSTM-dense layer:\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = on_x_lstm_1_out.get_shape().as_list()\n        rsh_on_x_lstm_1_out = tf.reshape(on_x_lstm_1_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        self.debug[\'reshaped_on_x_lstm_1_out\'] = tf.shape(rsh_on_x_lstm_1_out)\n\n        # Aac policy output and action-sampling function:\n        [self.on_logits, _, self.on_sample] = dense_aac_network(\n            rsh_on_x_lstm_1_out,\n            ac_space,\n            linear_layer_ref=linear_layer_ref,\n            name=\'aac_dense_pi\'\n        )\n        # Second LSTM layer takes concatenated encoded \'external\' state, LSTM_1 output,\n        # last_action and `internal_state` (if present) tensors:\n        on_stage2_2_input += [on_x_lstm_1_out]\n\n        # Try: feed context instead of output\n        #on_stage2_2_input = [reshape_lstm_1_state_out] + on_stage2_1_input\n\n        # LSTM_2 full input:\n        on_aac_x_ext = tf.concat(on_stage2_2_input, axis=-1)\n\n        self.debug[\'on_stage2_2_input\'] = tf.shape(on_aac_x_ext)\n\n        [on_x_lstm_2_out, self.on_lstm_2_init_state, self.on_lstm_2_state_out, self.on_lstm_2_state_pl_flatten] = \\\n            lstm_network(on_aac_x_ext, self.on_time_length, lstm_class_ref, (lstm_layers[-1],), name=\'lstm_2\')\n\n        self.debug[\'on_x_lstm_2_out\'] = tf.shape(on_x_lstm_2_out)\n        self.debug[\'self.on_lstm_2_state_out\'] = tf.shape(self.on_lstm_2_state_out)\n        self.debug[\'self.on_lstm_2_state_pl_flatten\'] = tf.shape(self.on_lstm_2_state_pl_flatten)\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = on_x_lstm_2_out.get_shape().as_list()\n        on_x_lstm_out = tf.reshape(on_x_lstm_2_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        self.debug[\'reshaped_on_x_lstm_out\'] = tf.shape(on_x_lstm_out)\n\n        # Aac value function:\n        [_, self.on_vf, _] = dense_aac_network(\n            on_x_lstm_out,\n            ac_space,\n            linear_layer_ref=linear_layer_ref,\n            name=\'aac_dense_vfn\'\n        )\n\n        # Concatenate LSTM placeholders, init. states and context:\n        self.on_lstm_init_state = (self.on_lstm_1_init_state, self.on_lstm_2_init_state)\n        self.on_lstm_state_out = (self.on_lstm_1_state_out, self.on_lstm_2_state_out)\n        self.on_lstm_state_pl_flatten = self.on_lstm_1_state_pl_flatten + self.on_lstm_2_state_pl_flatten\n\n        # ========= Off-policy AAC network (shared) ==========\n\n        # Conv. autoencoder:\n        _, off_aac_x, off_decoded_layers_ext, self.off_state_decoded_ext, off_d_kl_ext = encoder_class_ref(\n            inputs=self.off_state_in[\'external\'],\n            layer_config=conv_2d_layer_config,\n            linear_layer_ref=linear_layer_ref,\n            max_batch_size=64,\n            name=\'encoder_external\',\n            reuse=True\n        )\n        # VAE KL-divergence output:\n        self.off_vae_d_kl_ext = off_d_kl_ext\n\n        # Reshape rnn inputs for  batch training as [rnn_batch_dim, rnn_time_dim, flattened_depth]:\n        x_shape_dynamic = tf.shape(off_aac_x)\n        max_seq_len = tf.cast(x_shape_dynamic[0] / self.off_batch_size, tf.int32)\n        x_shape_static = off_aac_x.get_shape().as_list()\n\n        off_a_r_in = tf.reshape(self.off_a_r_in, [self.off_batch_size, max_seq_len, ac_space + 1])\n        off_aac_x = tf.reshape( off_aac_x, [self.off_batch_size, max_seq_len, np.prod(x_shape_static[1:])])\n\n        # Prepare `internal` state, if any:\n        if \'internal\' in list(self.off_state_in.keys()):\n            if self.encode_internal_state:\n                # Use convolution encoder:\n                _, off_x_int, off_decoded_layers_int, self.off_state_decoded_int, off_d_kl_int = encoder_class_ref(\n                    inputs=self.off_state_in[\'internal\'],\n                    layer_config=conv_2d_layer_config,\n                    linear_layer_ref=linear_layer_ref,\n                    max_batch_size=64,\n                    name=\'encoder_internal\',\n                    reuse=True\n                )\n                self.off_vae_d_kl_int = off_d_kl_int\n\n                x_int_shape_static = off_x_int.get_shape().as_list()\n                off_x_int = [\n                    tf.reshape(off_x_int, [self.off_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])])\n                ]\n            else:\n                x_int_shape_static = self.off_state_in[\'internal\'].get_shape().as_list()\n                off_x_int = tf.reshape(\n                    self.off_state_in[\'internal\'],\n                    [self.off_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n                )\n                off_x_int = [off_x_int]\n                self.off_state_decoded_int = None\n                self.off_vae_d_kl_int = None\n\n        else:\n            off_x_int = []\n            self.off_state_decoded_int = None\n            self.off_vae_d_kl_int = None\n\n        off_stage2_1_input = [off_aac_x, off_a_r_in[..., -1][..., None]] #+ off_x_internal\n\n        off_stage2_2_input = [off_aac_x, off_a_r_in] + off_x_int\n\n        off_aac_x = tf.concat(off_stage2_1_input, axis=-1)\n\n        [off_x_lstm_1_out, _, _, self.off_lstm_1_state_pl_flatten] =\\\n            lstm_network(off_aac_x, self.off_time_length, lstm_class_ref, (lstm_layers[0],), name=\'lstm_1\', reuse=True)\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = off_x_lstm_1_out.get_shape().as_list()\n        rsh_off_x_lstm_1_out = tf.reshape(off_x_lstm_1_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        [self.off_logits, _, _] =\\\n            dense_aac_network(\n                rsh_off_x_lstm_1_out,\n                ac_space,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_pi\',\n                reuse=True\n            )\n\n        off_stage2_2_input += [off_x_lstm_1_out]\n\n        # LSTM_2 full input:\n        off_aac_x = tf.concat(off_stage2_2_input, axis=-1)\n\n        [off_x_lstm_2_out, _, _, self.off_lstm_2_state_pl_flatten] = \\\n            lstm_network(off_aac_x, self.off_time_length, lstm_class_ref, (lstm_layers[-1],), name=\'lstm_2\', reuse=True)\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = off_x_lstm_2_out.get_shape().as_list()\n        off_x_lstm_out = tf.reshape(off_x_lstm_2_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        # Aac value function:\n        [_, self.off_vf, _] = dense_aac_network(\n            off_x_lstm_out,\n            ac_space,\n            linear_layer_ref=linear_layer_ref,\n            name=\'aac_dense_vfn\',\n            reuse=True\n        )\n\n        # Concatenate LSTM states:\n        self.off_lstm_state_pl_flatten = self.off_lstm_1_state_pl_flatten + self.off_lstm_2_state_pl_flatten\n\n        # Aux1:\n        # `Pixel control` network.\n        #\n        # Define pixels-change estimation function:\n        # Yes, it rather env-specific but for atari case it is handy to do it here, see self.get_pc_target():\n        [self.pc_change_state_in, self.pc_change_last_state_in, self.pc_target] =\\\n            pixel_change_2d_estimator(ob_space[\'external\'], **kwargs)\n\n        self.pc_batch_size = self.off_batch_size\n        self.pc_time_length = self.off_time_length\n\n        self.pc_state_in = self.off_state_in\n        self.pc_a_r_in = self.off_a_r_in\n        self.pc_lstm_state_pl_flatten = self.off_lstm_state_pl_flatten\n\n        # Shared conv and lstm nets, same off-policy batch:\n        pc_x = off_x_lstm_out\n\n        # PC duelling Q-network, outputs [None, 20, 20, ac_size] Q-features tensor:\n        self.pc_q = duelling_pc_network(pc_x, self.ac_space, linear_layer_ref=linear_layer_ref, **kwargs)\n\n        # Aux2:\n        # `Value function replay` network.\n        #\n        # VR network is fully shared with ppo network but with `value` only output:\n        # and has same off-policy batch pass with off_ppo network:\n        self.vr_batch_size = self.off_batch_size\n        self.vr_time_length = self.off_time_length\n\n        self.vr_state_in = self.off_state_in\n        self.vr_a_r_in = self.off_a_r_in\n\n        self.vr_lstm_state_pl_flatten = self.off_lstm_state_pl_flatten\n        self.vr_value = self.off_vf\n\n        # Aux3:\n        # `Reward prediction` network.\n        self.rp_batch_size = tf.placeholder(tf.int32, name=\'rp_batch_size\')\n\n        # Shared conv. output:\n        rp_encoded_layers, rp_x, rp_decoded_layers, _, rp_d_kl = encoder_class_ref(\n            self.rp_state_in[\'external\'],\n            layer_config=conv_2d_layer_config,\n            linear_layer_ref=linear_layer_ref,\n            max_batch_size=64,\n            name=\'encoder_external\',\n            reuse=True\n        )\n        # Flatten batch-wise:\n        rp_x_shape_static = rp_x.get_shape().as_list()\n        rp_x = tf.reshape(rp_x, [self.rp_batch_size, np.prod(rp_x_shape_static[1:]) * (self.rp_sequence_size-1)])\n\n        # RP output:\n        self.rp_logits = dense_rp_network(rp_x, linear_layer_ref=linear_layer_ref)\n\n        # Batch-norm related (useless, ignore):\n        try:\n            if self.train_phase is not None:\n                pass\n\n        except AttributeError:\n            self.train_phase = tf.placeholder_with_default(\n                tf.constant(False, dtype=tf.bool),\n                shape=(),\n                name=\'train_phase_flag_pl\'\n            )\n        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        # Add moving averages to save list:\n        moving_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name + \'.*moving.*\')\n        renorm_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name + \'.*renorm.*\')\n\n        # What to save:\n        self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n        self.var_list += moving_var_list + renorm_var_list\n\n        # Callbacks:\n        if self.aux_estimate:\n            self.callback[\'pixel_change\'] = self.get_pc_target\n\n    def get_initial_features(self, state, context=None):\n        """"""\n        Returns RNN initial context.\n        RNN_1 (lower) context is reset at every call.\n\n        RNN_2 (upper) context is reset:\n            - every `lstm_2_init_period\' episodes;\n            - episode  initial `state` `trial_num` metadata has been changed form last call (new train trial started);\n            - episode metatdata `type` is non-zero (test episode);\n            - no context arg is provided (initial episode of training);\n            - ... else carries context on to new episode;\n\n        Episode metadata are provided by DataTrialIterator, which is shaping Trial data distribution in this case,\n        and delivered through env.strategy as separate key in observation dictionary.\n\n        Args:\n            state:      initial episode state (result of env.reset())\n            context:    last previous episode RNN state (last_context of runner)\n\n        Returns:\n            2_RNN zero-state tuple.\n\n        Raises:\n            KeyError if [`metadata`]:[`trial_num`,`type`] keys not found\n        """"""\n        try:\n            sess = tf.get_default_session()\n            new_context = list(sess.run(self.on_lstm_init_state))\n            if state[\'metadata\'][\'trial_num\'] != self.current_trial_num\\\n                    or context is None\\\n                    or state[\'metadata\'][\'type\']\\\n                    or self.current_ep_num % self.lstm_2_init_period == 0:\n                # Assume new/initial trial or test sample, reset_1, 2 context:\n                pass #print(\'RL^2 policy context 1, 2 reset\')\n\n            else:\n                # Asssume same training trial, keep context_2 same as received:\n\n                new_context[-1] = context[-1]\n\n                # UPD: keep both contexts:\n                #new_context = context\n                #print(\'RL^2 policy context 1, reset\')\n            # Back to tuple:\n            new_context = tuple(new_context)\n            # Keep trial number:\n            self.current_trial_num = state[\'metadata\'][\'trial_num\']\n\n\n        except KeyError:\n            raise KeyError(\n                \'RL^2 policy: expected observation state dict. to have keys [`metadata`]:[`trial_num`,`type`]; got: {}\'.\n                format(state.keys())\n            )\n        self.current_ep_num +=1\n        return new_context\n\n\nclass bVAENA3C(BaseAAC):\n\n    def __init__(self, ae_loss=beta_vae_loss_def, ae_alpha=1.0, ae_beta=1.0, _log_name=\'bVAEN_A3C\', **kwargs):\n        try:\n            super(bVAENA3C, self).__init__(name=_log_name, **kwargs)\n            with tf.device(self.worker_device):\n                with tf.variable_scope(\'local\'):\n                    on_vae_loss_ext, on_ae_summary_ext = ae_loss(\n                        targets=self.local_network.on_state_in[\'external\'],\n                        logits=self.local_network.on_state_decoded_ext,\n                        d_kl=self.local_network.on_vae_d_kl_ext,\n                        alpha=ae_alpha,\n                        beta=ae_beta,\n                        name=\'external_state\',\n                        verbose=True\n                    )\n                    self.loss = self.loss + on_vae_loss_ext\n                    extended_summary =[on_ae_summary_ext]\n\n                    if self.local_network.encode_internal_state:\n                        on_vae_loss_int, on_ae_summary_int = ae_loss(\n                            targets=self.local_network.on_state_in[\'internal\'],\n                            logits=self.local_network.on_state_decoded_int,\n                            d_kl=self.local_network.on_vae_d_kl_int,\n                            alpha=ae_alpha,\n                            beta=ae_beta,\n                            name=\'internal_state\',\n                            verbose=True\n                        )\n                        self.loss = self.loss + on_vae_loss_int\n                        extended_summary.append(on_ae_summary_int)\n\n                    # Override train op def:\n                    self.grads, _ = tf.clip_by_global_norm(\n                        tf.gradients(self.loss, self.local_network.var_list),\n                        40.0\n                    )\n                    grads_and_vars = list(zip(self.grads, self.network.var_list))\n                    self.train_op = self.optimizer.apply_gradients(grads_and_vars)\n\n                    # Merge summary:\n                    extended_summary.append(self.model_summary_op)\n                    self.model_summary_op = tf.summary.merge(extended_summary, name=\'extended_summary\')\n\n        except:\n            msg = \'Child 0.0 class __init()__ exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n'"
btgym/research/misc_utils.py,0,"b'\nclass EnvRunner:\n    """"""\n    Handy data provider. Runs specified environments, gets per episode data and statistic.\n    """"""\n    def __init__(self, env):\n        """"""\n\n        Args:\n            env: btgym environment instance\n        """"""\n        self.env = env\n        self.done = True\n        self.sample_config = dict(\n            episode_config=dict(\n                get_new=True,\n                sample_type=0,\n                b_alpha=1.0,\n                b_beta=1.0\n            ),\n            trial_config=dict(\n                get_new=True,\n                sample_type=0,\n                b_alpha=1.0,\n                b_beta=1.0\n            )\n        )\n\n    def get_batch(self, batch_size):\n        obs_list = []\n        # batch_int = []\n        batch_r = []\n        batch_i = []\n        while len(batch_r) < batch_size:\n            if not self.done:\n                o, r, self.done, i = self.env.step(self.env.get_initial_action())\n            else:\n                o = self.env.reset()\n                r = 0\n                i = None\n                self.done = False\n            # obs_list.append(o[\'raw_state\'])\n            # obs_list.append(o[\'external\'])\n            # batch_int.append(o[\'internal\'])\n            # batch_r.append(r)\n            # batch_i.append(i)\n        return obs_list\n\n    def get_episode(self, sample_type=0):\n        self.sample_config[\'episode_config\'][\'sample_type\'] = sample_type\n        self.sample_config[\'trial_config\'][\'sample_type\'] = sample_type\n        self.done = False\n        _ = self.env.reset(**self.sample_config)\n        obs_list = []\n        batch_r = []\n        batch_i = []\n        while not self.done:\n            o, r, self.done, i = self.env.step(self.env.action_space.sample())\n            obs_list.append(o)\n            batch_r.append(r)\n\n        batch_obs = {key: [] for key in o.keys()}\n\n        for obs in obs_list:\n            for k, v in obs.items():\n                batch_obs[k].append(v)\n\n        batch_obs[\'reward\'] = batch_r\n\n        print(\'env_stat:\\n{}\'.format(self.env.get_stat()))\n\n        return batch_obs\n\n    def close(self):\n        self.env.close()\n        self.done = True'"
btgym/research/policy_rl2.py,1,"b'import tensorflow as tf\nfrom btgym.algorithms.policy.base import Aac1dPolicy\n\n\nclass AacRL2Policy(Aac1dPolicy):\n    """"""\n    This policy class in conjunction with DataTrialIterator classes from btgym.datafeed\n    is aimed to implement RL^2 algorithm by Duan et al.\n\n    Paper:\n    `FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING`,\n    https://arxiv.org/pdf/1611.02779.pdf\n\n    The only difference from Base policy is `get_initial_features()` method, which has been changed\n    either to reset RNN context to zero-state or return context from the end of previous episode,\n    depending on episode metadata received.\n    """"""\n    def __init__(self, **kwargs):\n        super(AacRL2Policy, self).__init__(**kwargs)\n        self.current_trial_num = -1  # always give initial context at first call\n\n    def get_initial_features(self, state, context=None):\n        """"""\n        Returns RNN initial context.\n        Basically, RNN context is reset if:\n            - episode  initial `state` `trial_num` metadata has been changed form last call (new train trial started);\n            - episode metatdata `type` is non-zero (test episode);\n            - no context arg is provided (initial episode of training);\n        Else, it assumes this is new episode of same train trial has started and carries context on to new episode;\n\n        Episode metadata are provided by DataTrialIterator, which is shaping Trial data distribution in this case,\n        and delivered through env.strategy as separate key in observation dictionary.\n\n        Args:\n            state:      initial episode state (result of env.reset())\n            context:    last previous episode RNN state (last_context of runner)\n\n        Returns:\n            RNN zero-state tuple if new trial is detected or same `context`.\n        """"""\n        try:\n            if state[\'metadata\'][\'trial_num\'] != self.current_trial_num or context is None or state[\'metadata\'][\'type\']:\n                # Assume new/initial trial or test, reset context:\n                sess = tf.get_default_session()\n                new_context = sess.run(self.on_lstm_init_state)\n                print(\'RL^2 policy context reset\')\n\n            else:\n                # Asssume same training trial, keep context chain:\n                new_context = context\n            # Keep trial number:\n            self.current_trial_num = state[\'metadata\'][\'trial_num\']\n\n        except KeyError:\n            raise KeyError(\n                \'RL^2 policy: expected observation state dict. to have keys [`metadata`]:[`trial_num`,`type`]; got: {}\'.\n                format(state.keys())\n            )\n\n        return new_context\n'"
btgym/research/strategy_gen_2.py,0,"b'import numpy as np\nimport scipy.signal as signal\nfrom scipy.stats import zscore\n\nimport backtrader as bt\nimport backtrader.indicators as btind\n\nfrom btgym.strategy.base import BTgymBaseStrategy\nfrom btgym.strategy.utils import tanh, abs_norm_ratio, exp_scale, discounted_average, log_transform\n\nfrom gym import spaces\nfrom btgym import DictSpace\n\nfrom btgym.research.strategy_gen_4 import DevStrat_4_10\n\nclass DevStrat_2_0(DevStrat_4_10):\n    """"""\n    Get back to CWT:\n    As 4_10, but computes observation state\n    by applying continious wavelet transform to time-embedded vector\n    to hi/low median price gradient.\n    """"""\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 0.99  # fi_gamma, should be equal MDP gamma decay\n\n    reward_scale = 1  # reward scaling scalar\n\n    cwt_signal_scale = 2e3  # first gradient scaling [scalar]\n    cwt_lower_bound = 3.0   # CWT scales\n    cwt_upper_bound = 12.0\n\n    state_ext_scale = np.linspace(3, 1, num=5)  # second gradient scaling [vector]\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, 5), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    )\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        skip_frame=skip_frame,\n        gamma=gamma,\n        reward_scale=1.0,\n        state_ext_scale=state_ext_scale,  # EURUSD\n        state_int_scale=1.0,\n        cwt_lower_bound=cwt_lower_bound,\n        cwt_upper_bound=cwt_upper_bound,\n        cwt_signal_scale=cwt_signal_scale,\n        metadata={},\n    )\n\n    def __init__(self, **kwargs):\n        super(DevStrat_2_0, self).__init__(**kwargs)\n        self.num_channels = self.p.state_shape[\'external\'].shape[-1]\n        # Define CWT scales:\n        self.cwt_width = np.linspace(self.p.cwt_lower_bound, self.p.cwt_upper_bound, self.num_channels)\n\n    def get_external_state(self):\n        # Use Hi-Low median as signal:\n        x = (\n            np.frombuffer(self.data.high.get(size=self.time_dim)) +\n            np.frombuffer(self.data.low.get(size=self.time_dim))\n        ) / 2\n\n        # Differences along time dimension:\n        d_x = np.gradient(x, axis=0) * self.p.cwt_signal_scale\n\n        # Compute continuous wavelet transform using Ricker wavelet:\n        cwt_x = signal.cwt(d_x, signal.ricker, self.cwt_width).T\n\n        # Note: differences taken once again along channels axis,\n        # apply weighted scaling to normalize channels\n        norm_x = np.gradient(cwt_x, axis=-1)\n        norm_x = zscore(norm_x, axis=0) * self.p.state_ext_scale\n        #out_x = tanh(norm_x)\n        out_x = np.clip(norm_x, -10, 10)\n\n        return out_x[:, None, :]\n\n    def get_internal_state(self):\n        x_broker = np.concatenate(\n            [\n                np.asarray(self.broker_stat[\'broker_value\'])[..., None],\n                np.asarray(self.broker_stat[\'unrealized_pnl\'])[..., None],\n                np.asarray(self.broker_stat[\'realized_pnl\'])[..., None],\n                np.asarray(self.broker_stat[\'broker_cash\'])[..., None],\n                np.asarray(self.broker_stat[\'exposure\'])[..., None],\n            ],\n            axis=-1\n        )\n        x_broker = tanh(np.gradient(x_broker, axis=-1) * self.p.state_int_scale)\n        return x_broker[:, None, :]\n'"
btgym/research/strategy_gen_4.py,0,"b'import numpy as np\nfrom scipy.stats import zscore\n\nimport backtrader as bt\nimport backtrader.indicators as btind\n\nfrom btgym.strategy.base import BTgymBaseStrategy\nfrom btgym.strategy.utils import tanh, abs_norm_ratio, exp_scale, discounted_average, log_transform\n\nfrom gym import spaces\nfrom btgym import DictSpace\n\n""""""\nResearch grade code. Can be unstable, buggy, poor performing and generally is subject to change.\n""""""\n\n\nclass DevStrat_4_6(BTgymBaseStrategy):\n    """"""\n    Objectives:\n        external state data feature search:\n            time_embedded three-channeled vector:\n                - `Open` channel is one time-step difference of Open price;\n                - `High` and `Low` channels are differences\n                  between current Open price and current High or Low prices respectively\n\n        internal state data feature search:\n            time_embedded concatenated vector of broker and portfolio statistics\n            time_embedded vector of last actions recieved (one-hot)\n            time_embedded vector of rewards\n\n        reward shaping search:\n           potential-based shaping functions\n\n    Data:\n        synthetic/real\n    """"""\n\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = time_dim\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n            {\n                \'external\': spaces.Box(low=-1, high=1, shape=(time_dim, 1, 3), dtype=np.float32),\n                \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n                \'metadata\': DictSpace(\n                    {\n                        \'type\': spaces.Box(\n                            shape=(),\n                            low=0,\n                            high=1,\n                            dtype=np.uint32\n                        ),\n                        \'trial_num\': spaces.Box(\n                            shape=(),\n                            low=0,\n                            high=10**10,\n                            dtype=np.uint32\n                        ),\n                        \'trial_type\': spaces.Box(\n                            shape=(),\n                            low=0,\n                            high=1,\n                            dtype=np.uint32\n                        ),\n                        \'sample_num\': spaces.Box(\n                            shape=(),\n                            low=0,\n                            high=10**10,\n                            dtype=np.uint32\n                        ),\n                        \'first_row\': spaces.Box(\n                            shape=(),\n                            low=0,\n                            high=10**10,\n                            dtype=np.uint32\n                        ),\n                        \'timestamp\': spaces.Box(\n                            shape=(),\n                            low=0,\n                            high=np.finfo(np.float64).max,\n                            dtype=np.float64\n                        ),\n                    }\n                )\n            },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        state_ext_scale=2e3,  # EURUSD\n        state_int_scale=1.0,  # not used\n        metadata={}\n    )\n\n    def __init__(self, **kwargs):\n        """"""\n\n        Args:\n            **kwargs:   see BTgymBaseStrategy args.\n        """"""\n        super(DevStrat_4_6, self).__init__(**kwargs)\n        self.state[\'metadata\'] = self.metadata\n\n        self.log.debug(\'DEV_state_shape: {}\'.format(self.p.state_shape))\n        self.log.debug(\'DEV_skip_frame: {}\'.format(self.p.skip_frame))\n        self.log.debug(\'DEV_portfolio_actions: {}\'.format(self.p.portfolio_actions))\n        self.log.debug(\'DEV_drawdown_call: {}\'.format(self.p.drawdown_call))\n        self.log.debug(\'DEV_target_call: {}\'.format(self.p.target_call))\n        self.log.debug(\'DEV_dataset_stat:\\n{}\'.format(self.p.dataset_stat))\n        self.log.debug(\'DEV_episode_stat:\\n{}\'.format(self.p.episode_stat))\n\n    def set_datalines(self):\n\n        # Define data channels:\n        self.channel_O = bt.Sum(self.data.open, - self.data.open(-1))\n        self.channel_H = bt.Sum(self.data.high, - self.data.open)\n        self.channel_L = bt.Sum(self.data.low, - self.data.open)\n\n    def get_external_state(self):\n\n        x = np.stack(\n            [\n                np.frombuffer(self.channel_O.get(size=self.time_dim)),\n                np.frombuffer(self.channel_H.get(size=self.time_dim)),\n                np.frombuffer(self.channel_L.get(size=self.time_dim)),\n            ],\n            axis=-1\n        )\n        # Amplify and squash in [-1,1], seems to be best option as of 4.10.17:\n        # `self.p.state_ext_scale` param is supposed to keep most of the signal\n        # in \'linear\' part of tanh while squashing spikes.\n        x_market = tanh(x * self.p.state_ext_scale)\n\n        return x_market[:, None, :]\n\n\nclass DevStrat_4_7(DevStrat_4_6):\n    """"""\n    4_6 + Sliding statistics avg_period disentangled from time embedding dim;\n    Only one last step sliding stats are used for internal state;\n    Reward weights: 1, 2, 10 , reward scale factor aded;\n    """"""\n\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 1.0  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1.0  # reward scaler\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-1, high=1, shape=(time_dim, 1, 3), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(1, 1, 5), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        gamma=gamma,\n        reward_scale=1.0,\n        state_ext_scale=2e3,  # EURUSD\n        state_int_scale=1.0,  # not used\n        metadata={}\n    )\n\n    def __init__(self, **kwargs):\n        super(DevStrat_4_7, self).__init__(**kwargs)\n\n    def get_internal_state(self):\n        x_broker = np.stack(\n            [\n                self.broker_stat[\'value\'][-1],\n                self.broker_stat[\'unrealized_pnl\'][-1],\n                self.broker_stat[\'realized_pnl\'][-1],\n                self.broker_stat[\'cash\'][-1],\n                self.broker_stat[\'exposure\'][-1],\n            ]\n        )\n        return x_broker[None, None, :]\n\n\nclass DevStrat_4_8(DevStrat_4_7):\n    """"""\n    4_7 + Uses full average_period of inner stats for use with inner_conv_encoder.\n    """"""\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    # !..-> here it is also `broker state` time-embedding period\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 1.0  # fi_gamma, should be MDP gamma decay, but somehow undiscounted works better <- wtf?\n\n    reward_scale = 1  # reward multiplicator\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-1, high=1, shape=(time_dim, 1, 3), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        gamma=gamma,\n        reward_scale=1.0,\n        state_ext_scale=2e3,  # EURUSD\n        state_int_scale=1.0,  # not used\n        metadata={},\n    )\n\n    def get_internal_state(self):\n        x_broker = np.concatenate(\n            [\n                np.asarray(self.broker_stat[\'value\'])[..., None],\n                np.asarray(self.broker_stat[\'unrealized_pnl\'])[..., None],\n                np.asarray(self.broker_stat[\'realized_pnl\'])[..., None],\n                np.asarray(self.broker_stat[\'cash\'])[..., None],\n                np.asarray(self.broker_stat[\'exposure\'])[..., None],\n                # np.asarray(self.sliding_stat[\'episode_step\'])[..., None],\n                # np.asarray(self.sliding_stat[\'reward\'])[..., None],\n                # np.asarray(self.sliding_stat[\'action\'])[..., None],\n                # norm_position_duration[...,None],\n                # max_unrealized_pnl[..., None],\n                # min_unrealized_pnl[..., None],\n            ],\n            axis=-1\n        )\n        return x_broker[:, None, :]\n\n\nclass DevStrat_4_9(DevStrat_4_7):\n    """"""\n    4_7 + Uses simple SMA market state features.\n    """"""\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 1.0  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1  # reward multiplicator, touchy!\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, 8), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(1, 1, 5), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        gamma=gamma,\n        reward_scale=1.0,\n        state_ext_scale=1e4,  # EURUSD\n        state_int_scale=1.0,  # not used\n        metadata={},\n    )\n\n    def set_datalines(self):\n        self.data.sma_4 = btind.SimpleMovingAverage(self.datas[0], period=4)\n        self.data.sma_8 = btind.SimpleMovingAverage(self.datas[0], period=8)\n        self.data.sma_16 = btind.SimpleMovingAverage(self.datas[0], period=16)\n        self.data.sma_32 = btind.SimpleMovingAverage(self.datas[0], period=32)\n        self.data.sma_64 = btind.SimpleMovingAverage(self.datas[0], period=64)\n        self.data.sma_128 = btind.SimpleMovingAverage(self.datas[0], period=128)\n        self.data.sma_256 = btind.SimpleMovingAverage(self.datas[0], period=256)\n\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=(256 + self.time_dim)\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n    def get_external_state(self):\n\n        x = np.stack(\n            [\n                np.frombuffer(self.data.open.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_4.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_8.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_16.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_32.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_64.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_128.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_256.get(size=self.time_dim)),\n            ],\n            axis=-1\n        )\n        # Gradient along features axis:\n        x = np.gradient(x, axis=1) * self.p.state_ext_scale\n\n        # Log-scale:\n        x = log_transform(x)\n        return x[:, None, :]\n\n\nclass DevStrat_4_10(DevStrat_4_7):\n    """"""\n    4_7 + Reward search: log-normalised potential functions. Nope.\n    """"""\n\n    # def get_reward(self):\n    #     """"""\n    #     Shapes reward function as normalized single trade realized profit/loss,\n    #     augmented with potential-based reward shaping functions in form of:\n    #     F(s, a, s`) = gamma * FI(s`) - FI(s);\n    #\n    #     - potential FI_1 is current normalized unrealized profit/loss;\n    #     - potential FI_2 is current normalized broker value.\n    #     - FI_3: penalizing exposure toward the end of episode\n    #\n    #     Paper:\n    #         ""Policy invariance under reward transformations:\n    #          Theory and application to reward shaping"" by A. Ng et al., 1999;\n    #          http://www.robotics.stanford.edu/~ang/papers/shaping-icml99.pdf\n    #     """"""\n    #\n    #     # All sliding statistics for this step are already updated by get_state().\n    #     debug = {}\n    #     scale = 10.0\n    #     # Potential-based shaping function 1:\n    #     # based on log potential of averaged profit/loss for current opened trade (unrealized p/l):\n    #     unrealised_pnl = np.asarray(self.broker_stat[\'unrealized_pnl\']) / 2 + 1 # shift [-1,1] -> [0,1]\n    #     # TODO: make normalizing util func to return in [0,1] by default\n    #     f1 = self.p.gamma * np.log(np.average(unrealised_pnl[1:])) - np.log(np.average(unrealised_pnl[:-1]))\n    #\n    #     debug[\'f1\'] = f1\n    #\n    #     # Potential-based shaping function 2:\n    #     # based on potential of averaged broker value, log-normalized wrt to max drawdown and target bounds.\n    #     norm_broker_value = np.asarray(self.broker_stat[\'value\']) / 2 + 1 # shift [-1,1] -> [0,1]\n    #     f2 = self.p.gamma * np.log(np.average(norm_broker_value[1:])) - np.log(np.average(norm_broker_value[:-1]))\n    #\n    #     debug[\'f2\'] = f2\n    #\n    #     # Potential-based shaping function 3: NOT USED\n    #     # negative potential of abs. size of position, exponentially weighted wrt. episode steps\n    #     # abs_exposure = np.abs(np.asarray(self.broker_stat[\'exposure\']))\n    #     # time = np.asarray(self.broker_stat[\'episode_step\'])\n    #     # #time_w = exp_scale(np.average(time[:-1]), gamma=5)\n    #     # #time_w_prime = exp_scale(np.average(time[1:]), gamma=5)\n    #     # #f3 = - 1.0 * time_w_prime * np.average(abs_exposure[1:]) #+ time_w * np.average(abs_exposure[:-1])\n    #     # f3 = - self.p.gamma * exp_scale(time[-1], gamma=3) * abs_exposure[-1] + \\\n    #     #      exp_scale(time[-2], gamma=3) * abs_exposure[-2]\n    #     # debug[\'f3\'] = f3\n    #     f3 = 1\n    #\n    #     # `Spike` reward function: normalized realized profit/loss:\n    #     realized_pnl = self.broker_stat[\'realized_pnl\'][-1]\n    #     debug[\'f_real_pnl\'] = 10 * realized_pnl\n    #\n    #     # Weights are subject to tune:\n    #     self.reward = (1.0 * f1 + 2.0 * f2 + 0.0 * f3 + 10.0 * realized_pnl) * self.p.reward_scale\n    #\n    #     debug[\'r\'] = self.reward\n    #     debug[\'b_v\'] = self.broker_stat[\'value\'][-1]\n    #     debug[\'unreal_pnl\'] = self.broker_stat[\'unrealized_pnl\'][-1]\n    #     debug[\'iteration\'] = self.iteration\n    #\n    #     #for k, v in debug.items():\n    #     #    print(\'{}: {}\'.format(k, v))\n    #     #print(\'\\n\')\n    #\n    #     # ------ignore-----:\n    #     # \'Do-not-expose-for-too-long\' shaping term:\n    #     # - 1.0 * self.exp_scale(avg_norm_position_duration, gamma=3)\n    #\n    #     self.reward = np.clip(self.reward, -self.p.reward_scale, self.p.reward_scale)\n    #\n    #     return self.reward\n\n\nclass DevStrat_4_11(DevStrat_4_10):\n    """"""\n    4_10 + Another set of sma-features, grads for broker state\n    """"""\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 0.99  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1  # reward multiplicator\n\n    state_ext_scale = np.linspace(3e3, 1e3, num=5)\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, 5), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 6), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        gamma=gamma,\n        reward_scale=1.0,\n        state_ext_scale=state_ext_scale,  # EURUSD\n        state_int_scale=1.0,\n        metadata={},\n    )\n\n    def set_datalines(self):\n        self.data.sma_16 = btind.SimpleMovingAverage(self.datas[0], period=16)\n        self.data.sma_32 = btind.SimpleMovingAverage(self.datas[0], period=32)\n        self.data.sma_64 = btind.SimpleMovingAverage(self.datas[0], period=64)\n        self.data.sma_128 = btind.SimpleMovingAverage(self.datas[0], period=128)\n        self.data.sma_256 = btind.SimpleMovingAverage(self.datas[0], period=256)\n\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=(256 + self.time_dim)\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n    def get_external_state(self):\n\n        x_sma = np.stack(\n            [\n                np.frombuffer(self.data.sma_16.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_32.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_64.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_128.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_256.get(size=self.time_dim)),\n            ],\n            axis=-1\n        )\n        # Gradient along features axis:\n        dx = np.gradient(x_sma, axis=-1) * self.p.state_ext_scale\n\n        x = tanh(dx)\n\n        return x[:, None, :]\n\n    def get_internal_state(self):\n\n        x_broker = np.concatenate(\n            [\n                np.asarray(self.broker_stat[\'value\'])[..., None],\n                np.asarray(self.broker_stat[\'unrealized_pnl\'])[..., None],\n                np.asarray(self.broker_stat[\'realized_pnl\'])[..., None],\n                np.asarray(self.broker_stat[\'cash\'])[..., None],\n                np.asarray(self.broker_stat[\'exposure\'])[..., None],\n                np.asarray(self.broker_stat[\'pos_direction\'])[..., None],\n\n                # np.asarray(self.broker_stat[\'value\'])[-self.p.skip_frame:, None],\n                # np.asarray(self.broker_stat[\'unrealized_pnl\'])[-self.p.skip_frame:, None],\n                # np.asarray(self.broker_stat[\'realized_pnl\'])[-self.p.skip_frame:, None],\n                # np.asarray(self.broker_stat[\'cash\'])[-self.p.skip_frame:, None],\n                # np.asarray(self.broker_stat[\'exposure\'])[-self.p.skip_frame:, None],\n                # np.asarray(self.broker_stat[\'pos_direction\'])[-self.p.skip_frame:, None],\n            ],\n            axis=-1\n        )\n        x_broker = tanh(np.gradient(x_broker, axis=-1) * self.p.state_int_scale)\n        # return x_broker[:, None, :]\n        return np.clip(x_broker[:, None, :], -2, 2)\n\n\nclass DevStrat_4_11_1(DevStrat_4_11):\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n    gamma = 0.99  # fi_gamma, should be MDP gamma decay\n    reward_scale = 1  # reward multiplicator\n    state_ext_scale = np.linspace(3e3, 1e3, num=5)\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': DictSpace(\n                {\n                    \'diff\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, 5), dtype=np.float32),\n                    \'avg\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, 5), dtype=np.float32),\n                }\n            ),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 6), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        gamma=gamma,\n        reward_scale=1.0,\n        state_ext_scale=state_ext_scale,  # EURUSD\n        state_int_scale=1.0,\n        metadata={},\n    )\n\n    def get_external_state(self):\n        x_sma = np.stack(\n            [\n                np.frombuffer(self.data.sma_16.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_32.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_64.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_128.get(size=self.time_dim)),\n                np.frombuffer(self.data.sma_256.get(size=self.time_dim)),\n            ],\n            axis=-1\n        )\n        # Gradient along features axis:\n        diff = np.gradient(x_sma, axis=-1) * self.p.state_ext_scale\n        diff = tanh(diff)\n        avg = np.gradient(x_sma, axis=0) * self.p.state_ext_scale\n        avg = tanh(avg)\n\n        return {\'avg\': avg[:, None, :], \'diff\': diff[:, None, :]}\n\nclass DevStrat_4_12(DevStrat_4_11):\n    """"""\n    4_11 + sma-features 8, 512;\n    """"""\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Hyperparameters for estimating signal features:\n    features_parameters = [8, 16, 32, 64, 128, 256]\n    num_features = len(features_parameters)\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 0.99  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1  # reward multiplicator\n\n    state_ext_scale = np.linspace(3e3, 1e3, num=num_features)\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, num_features), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'datetime\': spaces.Box(low=0, high=1, shape=(1, 5), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        state_ext_scale=state_ext_scale,  # EURUSD\n        state_int_scale=1.0,\n        gamma=gamma,\n        reward_scale=1.0,\n        metadata={},\n    )\n\n    def set_datalines(self):\n        self.data.features = [\n            btind.SimpleMovingAverage(self.datas[0], period=period) for period in self.features_parameters\n        ]\n\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=(np.asarray(self.features_parameters).max() + self.time_dim)\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n    def get_external_state(self):\n\n        x_sma = np.stack(\n            [\n                feature.get(size=self.time_dim) for feature in self.data.features\n            ],\n            axis=-1\n        )\n        # Gradient along features axis:\n        dx = np.gradient(x_sma, axis=-1) * self.p.state_ext_scale\n\n        # In [-1,1]:\n        x = tanh(dx)\n        return x[:, None, :]\n\n    def get_internal_state(self):\n\n        x_broker = np.concatenate(\n            [\n                np.asarray(self.broker_stat[\'value\'])[..., None],\n                np.asarray(self.broker_stat[\'unrealized_pnl\'])[..., None],\n                np.asarray(self.broker_stat[\'realized_pnl\'])[..., None],\n                np.asarray(self.broker_stat[\'cash\'])[..., None],\n                np.asarray(self.broker_stat[\'exposure\'])[..., None],\n            ],\n            axis=-1\n        )\n        x_broker = tanh(np.gradient(x_broker, axis=-1) * self.p.state_int_scale)\n\n        return x_broker[:, None, :]\n\n    def get_datetime_state(self):\n        time = self.data.datetime.time()\n        date = self.data.datetime.date()\n\n        # Encode in [0, 1]:\n        mn = date.month / 12\n        wd = date.weekday() / 6\n        d = date.day / 31\n        h = time.hour / 24\n        mm = time.minute / 60\n\n        encoded_stamp = [mn, d, wd, h, mm]\n        return np.asarray(encoded_stamp)[None, :]\n\n\n'"
btgym/strategy/__init__.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom .base import BTgymBaseStrategy\n'"
btgym/strategy/base.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nimport backtrader as bt\nimport backtrader.indicators as btind\n\nfrom gym import spaces\nfrom btgym import DictSpace\n\nimport numpy as np\nfrom collections import deque\n\nfrom btgym.strategy.utils import norm_value, decayed_result, exp_scale\n\n\n############################## Base BTgymStrategy Class ###################\n\n\nclass BTgymBaseStrategy(bt.Strategy):\n    """"""\n    Controls Environment inner dynamics and backtesting logic. Provides gym\'my (State, Action, Reward, Done, Info) data.\n    Any State, Reward and Info computation logic can be implemented by subclassing BTgymStrategy and overriding\n    get_[mode]_state(), get_reward(), get_info(), is_done() and set_datalines() methods.\n    One can always go deeper and override __init__ () and next() methods for desired\n    server cerebro engine behaviour, including order execution logic etc.\n\n    Note:\n        - base class supports single asset iteration via default data_line named \'base_asset\', see derived classes\n          multi-asset support\n        - bt.observers.DrawDown observer will be automatically added to BTgymStrategy instance at runtime.\n        - Since it is bt.Strategy subclass, refer to https://www.backtrader.com/docu/strategy.html for more information.\n    """"""\n\n    # Time embedding period:\n    time_dim = 4  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 1\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = time_dim\n\n    gamma = 0.99  # fi_gamma, should match MDP gamma decay\n\n    reward_scale = 1.0  # reward multiplicator\n\n    # Possible agent actions;  Note: place \'hold\' first! :\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    params = dict(\n        # Observation state shape is dictionary of Gym spaces,\n        # at least should contain `raw_state` field.\n        # By convention first dimension of every Gym Box space is time embedding one;\n        # one can define any shape; should match env.observation_space.shape.\n        # observation space state min/max values,\n        # For `raw_state\' (default) - absolute min/max values from BTgymDataset will be used.\n        state_shape={\n            \'raw\': spaces.Box(\n                shape=(time_dim, 4),\n                low=0, # will get overridden.\n                high=0,\n                dtype=np.float32,\n            ),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        gamma=gamma,\n        reward_scale=reward_scale,\n        drawdown_call=10,  # finish episode when hitting drawdown treshghold , in percent.\n        target_call=10,  # finish episode when reaching profit target, in percent.\n        dataset_stat=None,  # Summary descriptive statistics for entire dataset and\n        episode_stat=None,  # current episode. Got updated by server.\n        metadata={},\n        trial_stat=None,\n        trial_metadata=None,\n        portfolio_actions=portfolio_actions,\n        skip_frame=skip_frame,\n        order_size=None,\n        initial_action=None,\n        initial_portfolio_action=None,\n    )\n\n    def __init__(self, **kwargs):\n        """"""\n        Keyword Args:\n            params (dict):          parameters dictionary, see Note below.\n\n\n            Notes:\n                Due to backtrader convention, any strategy arguments should be defined inside `params` dictionary\n                or passed as kwargs to bt.Cerebro() class via .addstrategy() method. Parameter dictionary\n                should contain at least these keys::\n\n                    state_shape:        Observation state shape is dictionary of Gym spaces, by convention\n                                        first dimension of every Gym Box space is time embedding one;\n                    cash_name:          str, name for cash asset\n                    asset_names:        iterable of str, names for assets\n                    start_cash:         float, broker starting cash\n                    commission:         float, broker commission value, .01 stands for 1%\n                    leverage:           broker leverage, default is 1.0\n                    order_size:         dict of fixed order stakes (floats); keys should match assets names.\n                    drawdown_call:      finish episode when hitting this drawdown treshghold , in percent.\n                    target_call:        finish episode when reaching this profit target, in percent.\n                    portfolio_actions:  possible agent actions.\n                    skip_frame:         number of environment steps to skip before returning next response,\n                                        e.g. if set to 10 -- agent will interact with environment every 10th step;\n                                        every other step agent action is assumed to be \'hold\'.\n\n                Default values are::\n\n                    state_shape=dict(raw_state=spaces.Box(shape=(4, 4), low=0, high=0,))\n                    cash_name=\'default_cash\'\n                    asset_names=[\'default_asset\']\n                    start_cash=None\n                    commission=None\n                    leverage=1.0\n                    drawdown_call=10\n                    target_call=10\n                    dataset_stat=None\n                    episode_stat=None\n                    portfolio_actions=(\'hold\', \'buy\', \'sell\', \'close\')\n                    skip_frame=1\n                    order_size=None\n        """"""\n        try:\n            self.time_dim = self.p.state_shape[\'raw\'].shape[0]\n        except KeyError:\n            pass\n\n        try:\n            self.skip_frame = self.p.skip_frame\n        except KeyError:\n            pass\n        \n        self.iteration = 0\n        self.env_iteration = 0\n        self.inner_embedding = 1\n        self.is_done = False\n        self.is_done_enabled = False\n        self.steps_till_is_done = 2  # extra steps to make when episode terminal conditions are met\n        self.action = self.p.initial_portfolio_action\n        self.action_to_repeat = self.p.initial_portfolio_action\n        self.action_repeated = 0\n        self.num_action_repeats = None\n        self.reward = 0\n        self.order = None\n        self.order_failed = 0\n        self.broker_message = \'_\'\n        self.final_message = \'_\'\n        self.raw_state = None\n        self.time_stamp = 0\n\n        # Inherit logger from cerebro:\n        self.log = self.env._log\n\n        # Prepare broker:\n        if self.p.start_cash is not None:\n            self.env.broker.setcash(self.p.start_cash)\n\n        if self.p.commission is not None:\n            self.env.broker.setcommission(commission=self.p.commission, leverage=self.p.leverage)\n\n        # Normalisation constant for statistics derived from account value:\n        self.broker_value_normalizer = 1 / \\\n            self.env.broker.startingcash / (self.p.drawdown_call + self.p.target_call) * 100\n\n        self.target_value = self.env.broker.startingcash * (1 + self.p.target_call / 100)\n\n        # Try to define stake, if no self.p.order_size dict has been set:\n        if self.p.order_size is None:\n            # If no order size has been set for every data_line,\n            # try to infer stake size from sizer set by bt.Cerebro.addsizer() method:\n            try:\n                assert len(list(self.env.sizers.values())) == 1\n                env_sizer_params = list(self.env.sizers.values())[0][-1]  # pull dict of outer set sizer params\n                assert \'stake\' in env_sizer_params.keys()\n\n            except (AssertionError, KeyError) as e:\n                msg = \'Order stake is not set neither via strategy.param.order_size nor via bt.Cerebro.addsizer method.\'\n                self.log.error(msg)\n                raise ValueError(msg)\n\n            self.p.order_size = {name: env_sizer_params[\'stake\'] for name in self.p.asset_names}\n\n        elif isinstance(self.p.order_size, int) or isinstance(self.p.order_size, float):\n            unimodal_stake = {name: self.p.order_size for name in self.p.asset_names}\n            self.p.order_size = unimodal_stake\n\n        self.trade_just_closed = False\n        self.trade_result = 0\n\n        self.unrealized_pnl = None\n        self.norm_broker_value = None\n        self.realized_pnl = None\n\n        self.current_pos_duration = 0\n        self.current_pos_min_value = 0\n        self.current_pos_max_value = 0\n\n        self.realized_broker_value = self.env.broker.startingcash\n        self.episode_result = 0  # not used\n\n        # Service sma to get correct first features values:\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=self.time_dim\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n        # self.log.warning(\'self.p.dir: {}\'.format(dir(self.params)))\n\n        # Episode-wide metadata:\n        self.metadata = {\n            \'type\': np.asarray(self.p.metadata[\'type\']),\n            \'trial_num\': np.asarray(self.p.metadata[\'parent_sample_num\']),\n            \'trial_type\': np.asarray(self.p.metadata[\'parent_sample_type\']),\n            \'sample_num\': np.asarray(self.p.metadata[\'sample_num\']),\n            \'first_row\': np.asarray(self.p.metadata[\'first_row\']),\n            \'timestamp\': np.asarray(self.time_stamp, dtype=np.float64)\n        }\n        self.state = {\n            \'raw\': None,\n            \'metadata\': None\n        }\n\n        # This flag shows to the outer world if this episode can broadcast world-state information, e.g. move global\n        # time forward (see: btgym.server._BTgymAnalyzer.next() method);\n        # default logic: true iff. it is test episode from target domain:\n        self.can_broadcast = self.metadata[\'type\'] and self.metadata[\'trial_type\']\n\n        self.log.debug(\'strategy.metadata: {}\'.format(self.metadata))\n        self.log.debug(\'can_broadcast: {}\'.format(self.can_broadcast))\n\n        # Broker data lines of interest (used for estimation inner state of agent:\n        self.broker_datalines = [\n            \'cash\',\n            \'value\',\n            \'exposure\',\n            \'drawdown\',\n            \'pos_direction\',\n            \'pos_duration\',\n            \'realized_pnl\',\n            \'unrealized_pnl\',\n            \'min_unrealized_pnl\',\n            \'max_unrealized_pnl\',\n        ]\n        # Define collection dictionary looking for methods for estimating broker statistics, one method for one mode,\n        # should be named .get_broker_[mode_name]():\n        self.collection_get_broker_stat_methods = {}\n        for line in self.broker_datalines:\n            try:\n                self.collection_get_broker_stat_methods[line] = getattr(self, \'get_broker_{}\'.format(line))\n\n            except AttributeError:\n                raise NotImplementedError(\'Callable get_broker_{}.() not found\'.format(line))\n\n        # Broker and account related sliding statistics accumulators, globally normalized last `avg_perod` values,\n        # so it\'s a bit more comp. efficient than use of bt.Observers:\n        self.broker_stat = {key: deque(maxlen=self.avg_period) for key in self.broker_datalines}\n\n        # Add custom data Lines if any (convenience wrapper):\n        self.set_datalines()\n        self.log.debug(\'Kwargs:\\n{}\\n\'.format(str(kwargs)))\n\n        # Define collection dictionary looking for methods for estimating observation state, one method for one mode,\n        # should be named .get_[mode_name]_state():\n        self.collection_get_state_methods = {}\n        for key in self.p.state_shape.keys():\n            try:\n                self.collection_get_state_methods[key] = getattr(self, \'get_{}_state\'.format(key))\n\n            except AttributeError:\n                raise NotImplementedError(\'Callable get_{}_state.() not found\'.format(key))\n\n        for data in self.datas:\n            self.log.debug(\'data_name: {}\'.format(data._name))\n\n        self.log.debug(\'stake size: {}\'.format(self.p.order_size))\n\n        # Define how this strategy should handle actions: either as discrete or continuous:\n        if self.p.portfolio_actions is None or set(self.p.portfolio_actions) == {}:\n            # No discrete actions provided, assume continuous:\n            try:\n                assert self.p.skip_frame > 1\n\n            except AssertionError:\n                msg = \'For continuous actions it is essential to set `skip_frame` parameter > 1, got: {}\'.format(\n                    self.p.skip_frame\n                )\n                self.log.error(msg)\n                raise ValueError(msg)\n            # Disable broker checking margin,\n            # see: https://community.backtrader.com/topic/152/multi-asset-ranking-and-rebalancing/2?page=1\n            self.env.broker.set_checksubmit(False)\n            self.next_process_fn = self._next_target_percent\n            # Repeat action 2 times:\n            self.num_action_repeats = 2\n\n        else:\n            # Use discrete handling method otherwise:\n            self.env.broker.set_checksubmit(True)\n            self.next_process_fn = self._next_discrete\n\n            # Do not repeat action for discrete:\n            self.num_action_repeats = 0 \n\n    def prenext(self):\n        self.update_broker_stat()\n\n    def nextstart(self):\n        self.inner_embedding = self.data.close.buflen()\n        self.log.debug(\'Inner time embedding: {}\'.format(self.inner_embedding))\n\n    def next(self):\n        """"""\n        Default implementation for built-in backtrader method.\n        Defines one step environment routine;\n        Handles order execution logic according to action received.\n        Note that orders can only be submitted for data_lines in action_space (assets).\n        `self.action` attr. is updated by btgym.server._BTgymAnalyzer, and `None` actions\n        are emitted while doing `skip_frame` loop.\n        """"""\n        self.update_broker_stat()\n\n        if \'_skip_this\' in self.action.keys():\n                # print(\'a_skip, b_message: \', self.broker_message)\n                if self.action_repeated < self.num_action_repeats:\n                    self.next_process_fn(self.action_to_repeat)\n                    self.action_repeated += 1\n\n        else:\n            self.next_process_fn(self.action)\n            self.action_repeated = 0\n            self.action_to_repeat = self.action\n            # print(\'a_process, b_message: \', self.broker_message)\n\n    def notify_trade(self, trade):\n        if trade.isclosed:\n            # Set trade flags: True if trade have been closed just now and within last frame-skip period,\n            # and store trade result:\n            self.trade_just_closed = True\n            # Note: `trade_just_closed` flag has to be reset manually after evaluating.\n            self.trade_result = trade.pnlcomm\n\n            # Store realized prtfolio value:\n            self.realized_broker_value = self.broker.get_value()\n\n    def update_broker_stat(self):\n        """"""\n        Updates all sliding broker statistics deques with latest-step values such as:\n            - normalized broker value\n            - normalized broker cash\n            - normalized exposure (position size)\n            - exp. scaled episode duration in steps, normalized wrt. max possible episode steps\n            - normalized realized profit/loss for last closed trade (is zero if no pos. closures within last env. step)\n            - normalized profit/loss for current opened trade (unrealized p/l);\n        """"""\n        current_value = self.env.broker.get_value()\n\n        for key, method in self.collection_get_broker_stat_methods.items():\n            self.broker_stat[key].append(method(current_value=current_value))\n\n        # Reset one-time flags:\n        self.trade_just_closed = False\n\n    def get_broker_value(self, current_value, **kwargs):\n        """"""\n\n        Args:\n            current_value:  current portfolio value\n\n        Returns:\n            normalized broker value.\n        """"""\n        return norm_value(\n            current_value,\n            self.env.broker.startingcash,\n            self.p.drawdown_call,\n            self.p.target_call,\n        )\n\n    def get_broker_cash(self, **kwargs):\n        """"""\n\n        Returns:\n            normalized broker cash\n        """"""\n        return norm_value(\n            self.env.broker.get_cash(),\n            self.env.broker.startingcash,\n            99.0,\n            self.p.target_call,\n        )\n\n    def get_broker_exposure(self, **kwargs):\n        """"""\n\n        Returns:\n            normalized exposure (position size)\n        """"""\n        return self.position.size / (self.env.broker.startingcash * self.env.broker.get_leverage() + 1e-2)\n\n    def get_broker_pos_direction(self, **kwargs):\n        """"""\n\n        Returns:\n            short/long/out position indicator\n        """"""\n        if self.position.size > 0:\n            return 1.0\n\n        elif self.position.size < 0:\n            return - 1.0\n\n        else:\n            return 0.0\n\n    def get_broker_realized_pnl(self, current_value, **kwargs):\n        """"""\n\n        Args:\n            current_value: current portfolio value\n\n        Returns:\n            normalized realized profit/loss for last closed trade (is zero if no pos. closures within last env. step)\n        """"""\n        if self.trade_just_closed:\n            pnl = decayed_result(\n                self.trade_result,\n                current_value,\n                self.env.broker.startingcash,\n                self.p.drawdown_call,\n                self.p.target_call,\n                gamma=1\n            )\n            # Reset flag:\n            # self.trade_just_closed = False\n            #print(\'broker_realized_pnl: step {}, just closed.\'.format(self.iteration))\n\n        else:\n            pnl = 0.0\n        return pnl\n\n    def get_broker_unrealized_pnl(self, current_value, **kwargs):\n        """"""\n\n        Args:\n            current_value: current portfolio value\n\n        Returns:\n            normalized profit/loss for current opened trade\n        """"""\n        return (current_value - self.realized_broker_value) * self.broker_value_normalizer\n\n    def get_broker_episode_step(self, **kwargs):\n        """"""\n\n        Returns:\n            exp. scaled episode duration in steps, normalized wrt. max possible episode steps\n        """"""\n        return exp_scale(\n            self.iteration / (self.data.numrecords - self.inner_embedding),\n            gamma=3\n        )\n\n    def get_broker_drawdown(self, **kwargs):\n        """"""\n\n        Returns:\n            current drawdown value\n        """"""\n        try:\n            dd = self.stats.drawdown.drawdown[-1] #/ self.p.drawdown_call\n        except IndexError:\n            dd = 0.0\n        return dd\n\n    def get_broker_pos_duration(self, current_value):\n        if self.position.size == 0:\n            self.current_pos_duration = 0\n            # self.current_pos_min_value = current_value\n            # self.current_pos_max_value = current_value\n            # print(\'ZERO_POSITION\\n\')\n\n        else:\n            self.current_pos_duration += 1\n            # if self.current_pos_max_value < current_value:\n            #     self.current_pos_max_value = current_value\n            #\n            # elif self.current_pos_min_value > current_value:\n            #     self.current_pos_min_value = current_value\n\n        return self.current_pos_duration #/ (self.data.numrecords - self.inner_embedding)\n\n    def get_broker_max_unrealized_pnl(self, current_value, **kwargs):\n        if self.position.size == 0:\n            self.current_pos_max_value = current_value\n\n        else:\n            if self.current_pos_max_value < current_value:\n                self.current_pos_max_value = current_value\n        return (self.current_pos_max_value - self.realized_broker_value) * self.broker_value_normalizer\n\n    def get_broker_min_unrealized_pnl(self, current_value, **kwargs):\n        if self.position.size == 0:\n            self.current_pos_min_value = current_value\n\n        else:\n            if self.current_pos_min_value > current_value:\n                self.current_pos_min_value = current_value\n        return (self.current_pos_min_value - self.realized_broker_value) * self.broker_value_normalizer\n\n    def set_datalines(self):\n        """"""\n        Default datalines are: Open, Low, High, Close, Volume.\n        Any other custom data lines, indicators, etc. should be explicitly defined by overriding this method.\n        Invoked once by Strategy.__init__().\n        """"""\n        pass\n\n    def get_raw_state(self):\n        """"""\n        Default state observation composer.\n\n        Returns:\n             and updates time-embedded environment state observation as [n,4] numpy matrix, where:\n                4 - number of signal features  == state_shape[1],\n                n - time-embedding length  == state_shape[0] == <set by user>.\n\n        Note:\n            `self.raw_state` is used to render environment `human` mode and should not be modified.\n\n        """"""\n        self.raw_state = np.row_stack(\n            (\n                np.frombuffer(self.data.open.get(size=self.time_dim)),\n                np.frombuffer(self.data.high.get(size=self.time_dim)),\n                np.frombuffer(self.data.low.get(size=self.time_dim)),\n                np.frombuffer(self.data.close.get(size=self.time_dim)),\n            )\n        ).T\n\n        return self.raw_state\n\n    def get_internal_state(self):\n        """"""\n        Composes internal state tensor by calling all statistics from broker_stat dictionary.\n        Generally, this method should not be modified, implement corresponding get_broker_[mode]() methods.\n\n        """"""\n        x_broker = np.concatenate(\n            [np.asarray(stat)[..., None] for stat in self.broker_stat.values()],\n            axis=-1\n        )\n        return x_broker[:, None, :]\n\n    def get_metadata_state(self):\n        self.metadata[\'timestamp\'] = np.asarray(self._get_timestamp())\n\n        return self.metadata\n\n    def _get_time(self):\n        """"""\n        Retrieves current time point of the episode data.\n\n        Returns:\n            datetime object\n        """"""\n        return self.data.datetime.datetime()\n\n    def _get_timestamp(self):\n        """"""\n        Sets attr. and returns current data timestamp.\n\n        Returns:\n            POSIX timestamp\n        """"""\n        self.time_stamp = self._get_time().timestamp()\n\n        return self.time_stamp\n\n    def _get_broadcast_info(self):\n        """"""\n        Transmits broadcasting message.\n\n        Returns:\n            dictionary of global settings\n        """"""\n        return dict()\n\n    def get_state(self):\n        """"""\n        Collects estimated values for every mode of observation space by calling methods from\n        `collection_get_state_methods` dictionary.\n        As a rule, this method should not be modified, override or implement corresponding get_[mode]_state() methods,\n        defining necessary calculations and return arbitrary shaped tensors for every space mode.\n\n        Note:\n            - \'data\' referes to bt.startegy datafeeds and should be treated as such.\n                Datafeed Lines that are not default to BTgymStrategy should be explicitly defined by\n                 __init__() or define_datalines().\n        """"""\n        # Update inner state statistic and compose state: <- moved to .next()\n        # self.update_broker_stat()\n\n        self.state = {key: method() for key, method in self.collection_get_state_methods.items()}\n        # Above line is generalisation of, say:\n        # self.state = {\n        #     \'external\': self.get_external_state(),\n        #     \'internal\': self.get_internal_state(),\n        #     \'datetime\': self.get_datetime_state(),\n        #     \'metadata\': self.get_metadata_state(),\n        # }\n        return self.state\n\n    def get_reward(self):\n        """"""\n        Shapes reward function as normalized single trade realized profit/loss,\n        augmented with potential-based reward shaping functions in form of:\n        F(s, a, s`) = gamma * FI(s`) - FI(s);\n\n        - potential FI_1 is current normalized unrealized profit/loss;\n        EXCLUDED/ - potential FI_2 is current normalized broker value.\n        EXCLUDED/ - FI_3: penalizing exposure toward the end of episode\n\n        Paper:\n            ""Policy invariance under reward transformations:\n             Theory and application to reward shaping"" by A. Ng et al., 1999;\n             http://www.robotics.stanford.edu/~ang/papers/shaping-icml99.pdf\n        """"""\n\n        # All sliding statistics for this step are already updated by get_state().\n\n        # Potential-based shaping function 1:\n        # based on potential of averaged profit/loss for current opened trade (unrealized p/l):\n        unrealised_pnl = np.asarray(self.broker_stat[\'unrealized_pnl\'])\n        current_pos_duration = self.broker_stat[\'pos_duration\'][-1]\n\n        # We want to estimate potential `fi = gamma*fi_prime - fi` of current opened position,\n        # thus need to consider different cases given skip_fame parameter:\n        if current_pos_duration == 0:\n            # Set potential term to zero if there is no opened positions:\n            f1 = 0\n\n        else:\n            if current_pos_duration < self.p.skip_frame:\n                fi_1 = 0\n                fi_1_prime = np.average(unrealised_pnl[-current_pos_duration:])\n\n            elif current_pos_duration < 2 * self.p.skip_frame:\n                fi_1 = np.average(\n                    unrealised_pnl[-(self.p.skip_frame + current_pos_duration):-self.p.skip_frame]\n                )\n                fi_1_prime = np.average(unrealised_pnl[-self.p.skip_frame:])\n\n            else:\n                fi_1 = np.average(\n                    unrealised_pnl[-2 * self.p.skip_frame:-self.p.skip_frame]\n                )\n                fi_1_prime = np.average(unrealised_pnl[-self.p.skip_frame:])\n\n            # Potential term:\n            f1 = self.p.gamma * fi_1_prime - fi_1\n\n        # Main reward function: normalized realized profit/loss:\n        realized_pnl = np.asarray(self.broker_stat[\'realized_pnl\'])[-self.p.skip_frame:].sum()\n\n        # Weights are subject to tune:\n        self.reward = (10.0 * f1 + 10.0 * realized_pnl) * self.p.reward_scale\n\n        self.reward = np.clip(self.reward, -self.p.reward_scale, self.p.reward_scale)\n\n        return self.reward\n\n    def get_info(self):\n        """"""\n        Composes information part of environment response,\n        can be any object. Override to own taste.\n\n        Note:\n            Due to \'skip_frame\' feature, INFO part of environment response transmitted by server can be  a list\n            containing either all skipped frame\'s info objects, i.e. [info[-9], info[-8], ..., info[0]] or\n            just latest one, [info[0]]. This behaviour is set inside btgym.server._BTgymAnalyzer().next() method.\n        """"""\n        return dict(\n            step=self.iteration,\n            time=self.data.datetime.datetime(),\n            action=self.action,\n            broker_message=self.broker_message,\n            broker_cash=self.stats.broker.cash[0],\n            broker_value=self.stats.broker.value[0],\n            drawdown=self.stats.drawdown.drawdown[0],\n            max_drawdown=self.stats.drawdown.maxdrawdown[0],\n        )\n\n    def get_done(self):\n        """"""\n        Episode termination estimator,\n        defines any trading logic conditions episode stop is called upon, e.g. <OMG! Stop it, we became too rich!>.\n        It is just a structural a convention method. Default method is empty.\n\n        Expected to return:\n            tuple (<is_done, type=bool>, <message, type=str>).\n        """"""\n        return False, \'-\'\n\n    def _get_done(self):\n        """"""\n        Default episode termination method,\n        checks base conditions episode stop is called upon:\n            1. Reached maximum episode duration. Need to check it explicitly, because <self.is_done> flag\n               is sent as part of environment response.\n            2. Got \'_done\' signal from outside. E.g. via env.reset() method invoked by outer RL algorithm.\n            3. Hit drawdown threshold.\n            4. Hit target profit threshold.\n\n        This method shouldn\'t be overridden or called explicitly.\n\n        Runtime execution logic is:\n            terminate episode if:\n                get_done() returned (True, \'something\')\n                OR\n                ANY _get_done() default condition is met.\n        """"""\n        if not self.is_done_enabled:\n            # Episode is on its way,\n            # apply base episode termination rules:\n            is_done_rules = [\n                # Do we approaching the end of the episode?:\n                (self.iteration >= \\\n                 self.data.numrecords - self.inner_embedding - self.p.skip_frame - self.steps_till_is_done,\n                 \'END OF DATA\'),\n                # Any money left?:\n                (self.stats.drawdown.maxdrawdown[0] >= self.p.drawdown_call, \'DRAWDOWN CALL\'),\n                # Party time?\n                (self.env.broker.get_value() > self.target_value, \'TARGET REACHED\'),\n            ]\n            # Append custom get_done() results, if any:\n            is_done_rules += [self.get_done()]\n\n            # Sweep through rules:\n            for (condition, message) in is_done_rules:\n                if condition:\n                    # Start episode termination countdown for clean exit:\n                    # to forcefully execute final `close` order and compute proper reward\n                    # we need to make `steps_till_is_done` number of steps until `is_done` flag can be safely risen:\n                    self.is_done_enabled = True\n                    self.broker_message += message\n                    self.final_message = message\n                    self.order = self.close()\n                    self.log.debug(\n                        \'Episode countdown started at: {}, {}, r:{}\'.format(self.iteration, message, self.reward)\n                    )\n\n        else:\n            # Now in episode termination phase,\n            # just keep hitting `Close` button:\n            self.steps_till_is_done -= 1\n            self.broker_message = \'CLOSE, {}\'.format(self.final_message)\n            self.order = self.close()\n            self.log.debug(\n                \'Episode countdown contd. at: {}, {}, r:{}\'.format(self.iteration, self.broker_message, self.reward)\n            )\n\n        if self.steps_till_is_done <= 0:\n            # Now we\'ve done, terminate:\n            self.is_done = True\n\n        return self.is_done\n\n    def notify_order(self, order):\n        """"""\n        Shamelessly taken from backtrader tutorial.\n        TODO: better multi data support\n        """"""\n        if order.status in [order.Submitted, order.Accepted]:\n            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n            return\n        # Check if an order has been completed\n        # Attention: broker could reject order if not enough cash\n        if order.status in [order.Completed]:\n            if order.isbuy():\n                self.broker_message = \'BUY executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n                self.buyprice = order.executed.price\n                self.buycomm = order.executed.comm\n\n            else:  # Sell\n                self.broker_message = \'SELL executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n            self.bar_executed = len(self)\n\n        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n            self.broker_message = \'ORDER FAILED with status: \' + str(order.getstatusname())\n            # Rise order_failed flag until get_reward() will [hopefully] use and reset it:\n            self.order_failed += 1\n\n        self.order = None\n\n    def _next_discrete(self, action):\n        """"""\n        Default implementation for discrete actions.\n        Note that orders can be submitted only for data_lines in action_space (assets).\n\n        Args:\n            action:     dict, string encoding of btgym.spaces.ActionDictSpace\n\n        """"""\n        for key, single_action in action.items():\n            # Simple action-to-order logic:\n            if single_action == \'hold\' or self.is_done_enabled:\n                pass\n            elif single_action == \'buy\':\n                self.order = self.buy(data=key, size=self.p.order_size[key])\n                self.broker_message = \'new {}_BUY created; \'.format(key) + self.broker_message\n            elif single_action == \'sell\':\n                self.order = self.sell(data=key, size=self.p.order_size[key])\n                self.broker_message = \'new {}_SELL created; \'.format(key) + self.broker_message\n            elif single_action == \'close\':\n                self.order = self.close(data=key)\n                self.broker_message = \'new {}_CLOSE created; \'.format(key) + self.broker_message\n\n        # Somewhere after this point, server-side _BTgymAnalyzer() is exchanging information with environment wrapper,\n        # obtaining <self.action> , composing and sending <state,reward,done,info> etc... never mind.\n\n    def _next_target_percent(self, action):\n        """"""\n        Uses `order_target_percent` method to rebalance assets to given ratios. Expects action for every asset to be\n        a float scalar in [0,1], with actions sum to 1 over all assets (including base one).\n        Note that action for base asset (cash) is ignored.\n        For details refer to: https://www.backtrader.com/docu/order_target/order_target.html\n        """"""\n        # TODO 1: filter similar actions to prevent excessive orders issue e.g by DKL on two consecutive ones\n        # TODO 2: actions discretesation on level of execution\n        for asset in self.p.asset_names:\n                # Reducing assets positions subj to 5% margin reserve:\n                single_action = round(float(action[asset]) * 0.9, 2)\n                self.order = self.order_target_percent(data=asset, target=single_action )\n                self.broker_message += \' new {}->{:1.0f}% created; \'.format(asset, single_action * 100)\n\n'"
btgym/strategy/observers.py,0,"b'import backtrader as bt\n\n\nclass Reward(bt.observer.Observer):\n    """"""\n    Keeps track of reward values.\n    """"""\n    lines = (\'reward\',)\n    plotinfo = dict(plot=True, subplot=True, plotname=\'Reward\')\n    plotlines = dict(reward=dict(markersize=4.0, color=\'darkviolet\', fillstyle=\'full\'))\n\n    def next(self):\n        self.lines.reward[0] = self._owner.reward\n\n\nclass Position(bt.observer.Observer):\n    """"""\n    Keeps track of position size.\n    """"""\n    lines = (\'abs_sum_exposure\',)\n    plotinfo = dict(plot=True, subplot=True, plotname=\'Position\')\n    plotlines = dict(abs_sum_exposure=dict(marker=\'.\', markersize=1.0, color=\'blue\', fillstyle=\'full\'))\n\n    def next(self):\n        # self.lines.exposure[0] = self._owner.position.size\n        # for d in self._owner.datas:\n        #     print(d._name, self._owner.getposition(data=d._name))\n        self.lines.abs_sum_exposure[0] = sum([abs(pos.size) for pos in self._owner.positions.values()])\n\n\n# class MultiPosition(bt.observer.Observer):\n#     """"""\n#     Keeps track of position size.\n#     """"""\n#     lines = ()\n#     plotinfo = dict(plot=True, subplot=True, plotname=\'Position\')\n#     plotlines = {}\n#\n#     def __init__(self, **kwargs):\n#         self.lines = [\'{}_exposure\'.format(name) for name in self._owner.getdatanames()]\n#         self.plotlines = {\n#             line: dict(marker=\'.\', markersize=1.0, color=\'blue\', fillstyle=\'full\') for line in self.lines\n#         }\n#         super().__init__(**kwargs)\n#         print(dir(self.lines))\n#\n#     def next(self):\n#         self.lines.exposure[0] = self._owner.position.size\n\n\nclass NormPnL(bt.observer.Observer):\n    """"""\n    Keeps track of PnL stats.\n    """"""\n    lines = (\'realized_pnl\', \'unrealized_pnl\', \'max_unrealized_pnl\', \'min_unrealized_pnl\')\n    plotinfo = dict(plot=True, subplot=True, plotname=\'Normalized PnL\', plotymargin=.05)\n    plotlines = dict(\n        realized_pnl=dict(marker=\'o\', markersize=4.0, color=\'blue\', fillstyle=\'full\'),\n        unrealized_pnl=dict(marker=\'.\', markersize=1.0, color=\'grey\', fillstyle=\'full\'),\n        max_unrealized_pnl=dict(marker=\'.\', markersize=1.0, color=\'c\', fillstyle=\'full\'),\n        min_unrealized_pnl=dict(marker=\'.\', markersize=1.0, color=\'m\', fillstyle=\'full\'),\n    )\n\n    def next(self):\n        try:\n            if self._owner.broker_stat[\'realized_pnl\'][-1] != 0:\n                self.lines.realized_pnl[0] = self._owner.broker_stat[\'realized_pnl\'][-1]\n        except IndexError:\n            self.lines.realized_pnl[0] = 0.0\n\n        try:\n            self.lines.unrealized_pnl[0] = self._owner.broker_stat[\'unrealized_pnl\'][-1]\n        except IndexError:\n            self.lines.unrealized_pnl[0] = 0.0\n\n        try:\n            self.lines.max_unrealized_pnl[0] = self._owner.broker_stat[\'max_unrealized_pnl\'][-1]\n            self.lines.min_unrealized_pnl[0] = self._owner.broker_stat[\'min_unrealized_pnl\'][-1]\n\n        except (IndexError, KeyError):\n            self.lines.max_unrealized_pnl[0] = 0.0\n            self.lines.min_unrealized_pnl[0] = 0.0\n'"
btgym/strategy/utils.py,0,"b'import  numpy as np\n\n\ndef log_transform(x):\n    return np.sign(x) * np.log(np.fabs(x) + 1)\n\n\ndef tanh(x):\n    return 2 / (1 + np.exp(-2 * x)) - 1\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef abs_norm_ratio(x, a, b):\n    """"""\n    Norm. V-shaped realtive position of x in [a,b], a<=x<=b.\n    """"""\n    return abs((2 * x - a - b) / (abs(a) + abs(b)))\n\n\ndef norm_log_value(current_value, start_value, drawdown_call, target_call, epsilon=1e-4):\n    """"""\n    Current value log-normalized in [-1,1] wrt p/l limits.\n    """"""\n    x = np.asarray(current_value)\n    x = (x / start_value - 1) * 100\n    x = (x - target_call) / (drawdown_call + target_call) + 1\n    x = np.clip(x, epsilon, 1 - epsilon)\n    x = 1 - 2 * np.log(x) / np.log(epsilon)\n    return x\n\n\ndef norm_value(current_value, init_value, lower_bound, upper_bound, epsilon=1e-8):\n    """"""\n    Current value normalized in [-1,1] wrt upper and lower bounds.\n    """"""\n    x = np.asarray(current_value)\n    x = (x / init_value - 1) * 100\n    x = (x - upper_bound) / (lower_bound + upper_bound) + 1\n    x = 2 * np.clip(x, epsilon, 1 - epsilon) - 1\n    return x\n\n\ndef __norm_value(current_value, init_value, lower_bound, upper_bound, epsilon=1e-8):\n    """"""\n    Current value, piece-wise linear normalized in [-1,1] and zero-centered  at `start_value`\n    """"""\n    lower_bound /= 100\n    upper_bound /= 100\n    x = np.asarray(current_value)\n    x1 = (1 / (init_value * lower_bound)) * x[x < init_value] - 1 / lower_bound\n    x2 = (1 / (init_value * upper_bound)) * x[x >= init_value] - 1 / upper_bound\n    x = np.concatenate([x1, x2], axis=-1)\n    x = np.squeeze(np.clip(x, -1, 1))\n    return x\n\n\ndef decayed_result(trade_result, current_value, base_value, lower_bound, upper_bound, gamma=1.0):\n    """"""\n    Normalized in [-1,1] trade result, lineary decayed wrt current_value.\n    """"""\n    target_value = base_value * (1 + upper_bound / 100)\n    value_range = base_value * (lower_bound + upper_bound) / 100\n    decay = (gamma - 1) * (current_value - target_value) / value_range + gamma\n    x = trade_result * decay / value_range\n    return x\n\n\n# def decayed_result(trade_result, current_value, base_value, lower_bound, upper_bound, gamma=1.0):\n#     """"""\n#     Normalized in [-1,1] trade result, lineary decayed wrt current_value.\n#     """"""\n#     return (trade_result - base_value) / (upper_bound - lower_bound)\n\n\ndef exp_scale(x, gamma=4, epsilon=1e-10):\n    """"""\n    Returns exp. scaled value in [epsilon, 1] for x in [0, 1]; gamma controls steepness.\n    """"""\n    x = np.asarray(x) + 1\n    return np.clip(np.exp(x ** gamma - 2 ** gamma), epsilon, 1)\n\n\ndef discounted_average(x, gamma=1):\n    """"""\n    Returns gamma_power weighted average of 2D input array along 0-axis.\n\n    Args:\n        x:      scalar or array of rank <= 2.\n        gamma: discount, <=1.\n\n    Returns:\n        Array of rank 1 of averages along zero axis. For x of shape[n,m] AVG computed as:\n        AVG[j] = (x[0,j]*gamma^(n) + x[1,j]*gamma^(n-1) +...+  x[n,j]*gamma^(0)) / n\n\n    """"""\n    x = np.asarray(x)\n    while len(x.shape) < 2:\n        x = x[..., None]\n    gamma = gamma * np.ones(x.shape)\n    return np.squeeze(np.average(x, weights=(gamma ** np.arange(x.shape[0])[..., None])[::-1], axis=0))'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# BTgym documentation build configuration file, created by\n# sphinx-quickstart on Fri Oct 27 13:55:40 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinxcontrib.napoleon\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'.templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'BTGym\'\ncopyright = \'2017, 2018, Andrew Muzikin\'\nauthor = \'Andrew Muzikin\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.0.7\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.0.7\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = \'nature\'\nhtml_theme =  ""sphinx_rtd_theme""\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'.static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n    ]\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'BTgymDoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'BTgym.tex\', \'BTgym Documentation\',\n     \'Andrew Muzikin\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'btgym\', \'BTgym Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'BTgym\', \'BTgym Documentation\',\n     author, \'BTgym\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None}\n\n\n# Autodoc:\nautoclass_content = \'both\'\nautodoc_member_order =\'bysource\''"
btgym/algorithms/launcher/__init__.py,0,b''
btgym/algorithms/launcher/base.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n#\n# Original asynchronous framework code comes from OpenAI repository under MIT licence:\n# https://github.com/openai/universe-starter-agent\n#\n\n\nimport os\nfrom logbook import Logger, StreamHandler, WARNING, NOTICE, INFO, DEBUG\nimport time\nimport psutil\nimport glob\nfrom subprocess import PIPE\nimport signal\nimport numpy as np\nimport copy\n\nfrom btgym.algorithms.worker import Worker\nfrom btgym.algorithms.aac import A3C\nfrom btgym.algorithms.policy import BaseAacPolicy\n\nimport sys\nsys.path.insert(0,\'..\')\n\n\nclass Launcher():\n    """"""\n    Configures and starts distributed TF training session with workers\n    running sets of separate instances of BTgym/Atari environment.\n\n    """"""\n\n    def __init__(self,\n                 env_config=None,\n                 cluster_config=None,\n                 policy_config=None,\n                 trainer_config=None,\n                 max_env_steps=None,\n                 save_secs=600,\n                 root_random_seed=None,\n                 test_mode=False,\n                 purge_previous=1,\n                 render_last_env=True,\n                 log_level=None,\n\n                 verbose=0):\n        """"""\n\n\n        Args:\n            env_config (dict):          environment class_config_dict, see \'Note\' below.\n            cluster_config (dict):      tf cluster configuration, see \'Note\' below.\n            policy_config (dict):       policy class_config_dict holding corr. policy class args.\n            trainer_config (dict):      trainer class_config_dict holding corr. trainer class args.\n            max_env_steps (int):        total number of environment steps to run training on.\n            save_secs(int):             save model checkpoint every N secs.\n            root_random_seed (int):     int or None\n            test_mode (bool):           if True - use Atari gym env., BTGym otherwise.\n            purge_previous (int):       keep or remove previous log files and saved checkpoints from log_dir:\n                                        {0 - keep, 1 - ask, 2 - remove}.\n            render_last_env:            bool, if True and there is more than one environment specified for each worker,\n                                        only allows rendering for last environment in a list;\n                                        allows rendering for all environments of a chief worker otherwise;\n            verbose (int):              verbosity mode, {0 - WARNING, 1 - INFO, 2 - DEBUG}.\n            log_level (int):            logbook level {DEBUG=10, INFO=11, NOTICE=12, WARNING=13},\n                                        overrides `verbose` arg.\n\n        Note:\n            class_config_dict:  dictionary containing at least two keys:\n                                - `class_ref`:    reference to class constructor or function;\n                                - `kwargs`:       dictionary of keyword arguments, see corr. environment class args.\n\n            cluster_config:     dictionary containing at least these keys:\n                                - \'host\':         cluster host, def: \'127.0.0.1\'\n                                - \'port\':         cluster port, def: 12222\n                                - \'num_workers\':  number of workers to run, def: 1\n                                - \'num_ps\':       number of parameter servers, def: 1\n                                - \'num_envs\':     number of environments to run in parallel for each worker, def: 1\n                                - \'log_dir\':      directory to save model and summaries, def: \'./tmp/btgym_aac_log\'\n\n        """"""\n\n        self.env_config = dict(\n            class_ref=None,\n            kwargs=dict(\n                port=5000,\n                data_port=4999,\n                gym_id=None,\n            )\n        )\n        self.cluster_config = dict(\n            host=\'127.0.0.1\',\n            port=12222,\n            num_workers=1,\n            num_ps=1,\n            log_dir=\'./tmp/btgym_aac_log\',\n            initial_ckpt_dir=None,\n            log_ckpt_subdir=\'/current_train_checkpoint\',\n            num_envs=1,\n        )\n        self.policy_config = dict(\n            class_ref=BaseAacPolicy,\n            kwargs=dict(\n                lstm_layers=(256,)\n            )\n        )\n        self.trainer_config = dict(\n            class_ref=A3C,\n            kwargs={}\n        )\n        self.ports_to_use = []\n        self.root_random_seed = root_random_seed\n        self.purge_previous = purge_previous\n        self.test_mode = test_mode\n        self.log_level = log_level\n        self.verbose = verbose\n        self.save_secs = save_secs\n        self.render_last_env = render_last_env\n\n        if max_env_steps is not None:\n            self.max_env_steps = max_env_steps\n\n        else:\n            self.max_env_steps = 100 * 10 ** 6\n\n        self.env_config = self._update_config_dict(self.env_config, env_config)\n\n        self.cluster_config = self._update_config_dict(self.cluster_config, cluster_config)\n\n        self.policy_config = self._update_config_dict(self.policy_config, policy_config)\n\n        self.trainer_config = self._update_config_dict(self.trainer_config, trainer_config)\n\n        self.trainer_config[\'kwargs\'][\'test_mode\'] = self.test_mode\n\n        # Logging config:\n        StreamHandler(sys.stdout).push_application()\n        if self.log_level is None:\n            log_levels = [(0, NOTICE), (1, INFO), (2, DEBUG)]\n            self.log_level = WARNING\n            for key, value in log_levels:\n                if key == self.verbose:\n                    self.log_level = value\n        self.log = Logger(\'LauncherShell\', level=self.log_level)\n\n        # Seeding:\n        if self.root_random_seed is not None:\n            np.random.seed(self.root_random_seed)\n            self.log.info(\'Random seed: {}\'.format(self.root_random_seed))\n\n        # Seeding for workers:\n        self.workers_rnd_seeds = list(\n            np.random.randint(0, 2 ** 30, self.cluster_config[\'num_workers\'] + self.cluster_config[\'num_ps\'])\n        )\n\n        # Log_dir housekeeping:\n        if os.path.exists(self.cluster_config[\'log_dir\']):\n            # Remove previous log files and saved model checkpoints if opted:\n            if self.purge_previous > 0:\n                confirm = \'y\'\n                if self.purge_previous < 2:\n                    confirm = input(\'<{}> already exists. Override[y/n]? \'.format(self.cluster_config[\'log_dir\']))\n                if confirm in \'y\':\n                    files = glob.glob(self.cluster_config[\'log_dir\'] + \'/*\')\n                    p = psutil.Popen([\'rm\', \'-R\', ] + files, stdout=PIPE, stderr=PIPE)\n                    self.log.notice(\'files in: {} purged.\'.format(self.cluster_config[\'log_dir\']))\n\n            else:\n                self.log.notice(\'writing to: {}.\'.format(self.cluster_config[\'log_dir\']))\n\n        else:\n            os.makedirs(self.cluster_config[\'log_dir\'])\n            self.log.notice(\'<{}> created.\'.format(self.cluster_config[\'log_dir\']))\n\n        for kwarg in [\'port\', \'data_port\']:\n            assert kwarg in self.env_config[\'kwargs\'].keys()\n\n        assert self.env_config[\'class_ref\'] is not None\n\n        # Make cluster specification dict:\n        self.cluster_spec = self._make_cluster_spec(self.cluster_config)\n\n        # Configure workers:\n        self.workers_config_list = self._make_workers_spec()\n\n        # Ensure data_server port is clear:\n        self.clear_port(self.env_config[\'kwargs\'][\'data_port\'])\n\n        self.log.debug(\'Launcher ready.\')\n\n    def _make_workers_spec(self):\n        """"""\n        Creates list of workers specifications.\n        Returns:\n            list of dict\n        """"""\n        workers_config_list = []\n        env_ports = np.arange(self.cluster_config[\'num_envs\'], dtype=np.int32)\n        env_data_ports = np.zeros(self.cluster_config[\'num_envs\'], dtype=np.int32)\n        worker_port = self.env_config[\'kwargs\'][\'port\']  # start value for BTGym comm. port\n\n        # TODO: Hacky, cause dataset is threadlocked; do: pass dataset as class_ref + kwargs_dict:\n        if self.test_mode:\n            dataset_instance = None\n\n        else:\n            dataset_instance = self.env_config[\'kwargs\'].pop(\'dataset\')\n\n        for key, spec_list in self.cluster_spec.items():\n            task_index = 0  # referenced farther as worker id\n            for _id in spec_list:\n                env_config = copy.deepcopy(self.env_config)\n                worker_config = {}\n                if key in \'worker\':\n                    # Configure worker BTgym environment:\n                    if task_index == 0:\n                        env_config[\'kwargs\'][\'data_master\'] = True  # set worker_0 as chief and data_master\n                        env_config[\'kwargs\'][\'dataset\'] = dataset_instance\n                        env_config[\'kwargs\'][\'render_enabled\'] = True\n                    else:\n                        env_config[\'kwargs\'][\'data_master\'] = False\n                        # env_config[\'kwargs\'][\'dataset\'] = dataset_instance\n                        env_config[\'kwargs\'][\'render_enabled\'] = False  # disable rendering for all but chief\n\n                    # Add list of connection ports for every parallel env for each worker:\n                    env_config[\'kwargs\'][\'port\'] = list(worker_port + env_ports)\n                    env_config[\'kwargs\'][\'data_port\'] = list(env_config[\'kwargs\'][\'data_port\'] + env_data_ports)\n                    worker_port += self.cluster_config[\'num_envs\']\n                worker_config.update(\n                    {\n                        \'env_config\': env_config,\n                        \'policy_config\': self.policy_config,\n                        \'trainer_config\': self.trainer_config,\n                        \'cluster_spec\': self.cluster_spec,\n                        \'job_name\': key,\n                        \'task\': task_index,\n                        \'test_mode\': self.test_mode,\n                        \'log_dir\': self.cluster_config[\'log_dir\'],\n                        \'initial_ckpt_dir\': self.cluster_config[\'initial_ckpt_dir\'],\n                        \'log_ckpt_subdir\': self.cluster_config[\'log_ckpt_subdir\'],\n                        \'max_env_steps\': self.max_env_steps,\n                        \'save_secs\': self.save_secs,\n                        \'log_level\': self.log_level,\n                        \'random_seed\': self.workers_rnd_seeds.pop(),\n                        \'render_last_env\': self.render_last_env\n                    }\n                )\n                self.clear_port(env_config[\'kwargs\'][\'port\'])\n                workers_config_list.append(worker_config)\n                task_index += 1\n\n        return workers_config_list\n\n    def _make_cluster_spec(self, config):\n        """"""\n        Composes cluster specification dictionary.\n        """"""\n        cluster = {}\n        all_ps = []\n        port = config[\'port\']\n\n        for _ in range(config[\'num_ps\']):\n            self.clear_port(port)\n            self.ports_to_use.append(port)\n            all_ps.append(\'{}:{}\'.format(config[\'host\'], port))\n            port += 1\n        cluster[\'ps\'] = all_ps\n\n        all_workers = []\n        for _ in range(config[\'num_workers\']):\n            self.clear_port(port)\n            self.ports_to_use.append(port)\n            all_workers.append(\'{}:{}\'.format(config[\'host\'], port))\n            port += 1\n        cluster[\'worker\'] = all_workers\n        return cluster\n\n    def clear_port(self, port_list):\n        """"""\n        Kills process on specified ports list, if any.\n        """"""\n        if not isinstance(port_list, list):\n            port_list = [port_list]\n\n        for port in port_list:\n            p = psutil.Popen([\'lsof\', \'-i:{}\'.format(port), \'-t\'], stdout=PIPE, stderr=PIPE)\n            pid = p.communicate()[0].decode()[:-1]  # retrieving PID\n            if pid is not \'\':\n                p = psutil.Popen([\'kill\', pid])\n                self.log.info(\'port {} cleared\'.format(port))\n\n    def _update_config_dict(self, old_dict, new_dict=None):\n        """"""\n        Service, updates nested dictionary with values from other one of same structure.\n\n        Args:\n            old_dict:   dict to update to\n            new_dict:   dict to update from\n\n        Returns:\n            new updated dict\n        """"""\n        if type(new_dict) is not dict:\n            new_dict = old_dict  # ~identity op\n\n        for key, value in new_dict.items():\n            if type(value) == dict:\n                if key not in old_dict.keys():\n                    old_dict[key] = {}\n                old_dict[key] = self._update_config_dict(old_dict[key], value)\n\n            else:\n                old_dict[key] = value\n\n        return old_dict\n\n    def run(self):\n        """"""\n        Launches processes:\n\n            distributed workers;\n            parameter_server.\n        """"""\n        workers_list = []\n        p_servers_list = []\n        chief_worker = None\n\n        def signal_handler(signal, frame):\n            nonlocal workers_list\n            nonlocal chief_worker\n            nonlocal p_servers_list\n\n            def stop_worker(worker_list):\n                for worker in worker_list:\n                    worker.terminate()\n\n            stop_worker(workers_list)\n            stop_worker([chief_worker])\n            stop_worker(p_servers_list)\n\n        # Start workers:\n        for worker_config in self.workers_config_list:\n            # Make:\n            worker = Worker(**worker_config)\n            # Launch:\n            worker.daemon = False\n            worker.start()\n\n            if worker.job_name in \'worker\':\n                # Allow data-master to launch datafeed_server:\n                if worker_config[\'env_config\'][\'kwargs\'][\'data_master\']:\n                    time.sleep(5)\n                    chief_worker = worker\n\n                else:\n                    workers_list.append(worker)\n\n            else:\n                p_servers_list.append(worker)\n\n        # TODO: auto-launch tensorboard?\n\n        signal.signal(signal.SIGINT, signal_handler)\n\n        # Halt here:\n        msg = \'\\n********************************************************************************************\\n\' +\\\n                \'**  Press `Ctrl-C` or jupyter:[Kernel]->[Interrupt] to stop training and close launcher.  **\\n\' + \\\n                \'********************************************************************************************\\n\'\n        print(msg)\n        signal.pause()\n\n        # Wait every worker to finish:\n        for worker in workers_list:\n            worker.join()\n            self.log.notice(\'worker_{} has joined.\'.format(worker.task))\n\n        chief_worker.join()\n        self.log.notice(\'chief_worker_{} has joined.\'.format(chief_worker.task))\n\n        for ps in p_servers_list:\n            ps.terminate()\n            ps.join()\n            self.log.notice(\'parameter_server_{} has joined.\'.format(ps.task))\n\n        # TODO: close tensorboard\n        # TODO: maybe export TB summaries accumulators links\n\n        self.log.notice(\'Launcher closed.\')\n\n    def export_checkpoint(self, save_path):\n        """"""\n        Helper function: copies last saved checkpoint files to specified location;\n        usually to serve as pre-trained model.\n\n        Args:\n            save_path:  path to copy checkpoint files to;\n\n        """"""\n        source = self.cluster_config[\'log_dir\'] + self.cluster_config[\'log_ckpt_subdir\']\n        target = save_path\n\n        assert os.path.exists(source), \'Source dir not found: {}\'.format(source)\n\n        if not os.path.exists(target):\n            os.makedirs(target)\n            self.log.notice(\'target dir created: {}\'.format(target))\n\n        else:\n            old_files = glob.glob(target + \'/*\')\n            p = psutil.Popen([\'rm\', \'-R\', ] + old_files, stdout=PIPE, stderr=PIPE)\n            self.log.notice(\'target dir purged.\')\n\n        p = os.popen(\'cp -R \' + source + \'/* \' + target)\n\n        self.log.notice(\'copied to: {}\'.format(target))\n\n\n\n\n\n\n\n\n'"
btgym/algorithms/launcher/meta.py,0,"b'\nimport numpy as np\nimport copy\n\nfrom btgym.algorithms.launcher.base import Launcher\n\n\nclass MetaLauncher(Launcher):\n    """"""\n    Launcher class with extended functionality to support gradient-based meta-learning algorithms.\n    For every distributed worker it configures two master/slave environments such that that slave environment\n    always runs same data trial as master one.\n    Typically master environment is configured to run episodes from train data of the trial and salve one - from test\n    data. With AAC framework properly set up it enables single worker to estimate meta-loss by collecting relevant\n    test and train trajectories in parallel.\n    """"""\n\n    def __init__(self, cluster_config=None, render_slave_env=True, **kwargs):\n        """"""\n\n        Args:\n            cluster_config:     environment class_config_dict\n            render_slave_env:   bool, if True - rendering enabled for slave environment; master otherwise.\n            **kwargs:           same as base class args: btgym.algorithms.launcher.Launcher\n        """"""\n        meta_cluster_config = dict(\n            host=\'127.0.0.1\',\n            port=12222,\n            num_workers=1,\n            num_ps=1,\n            log_dir=\'./tmp/meta_aac_log\',\n            num_envs=2,  # just a declaration\n        )\n        meta_cluster_config = self._update_config_dict(meta_cluster_config, cluster_config)\n\n        # Force number of parallel envs anyway:\n        meta_cluster_config[\'num_envs\'] = 2\n\n        self.render_slave_env = render_slave_env\n\n        # Update:\n        kwargs[\'cluster_config\'] = meta_cluster_config\n        kwargs[\'test_mode\'] = False\n\n        super(MetaLauncher, self).__init__(**kwargs)\n\n    def _make_workers_spec(self):\n        """"""\n        Creates list of workers specifications.\n        Overrides base class method. Sets master/slave pair of environments for every worker.\n\n        Returns:\n            list of dict\n        """"""\n        workers_config_list = []\n        env_ports = np.arange(self.cluster_config[\'num_envs\'], dtype=np.int32)\n        env_data_ports = np.zeros(self.cluster_config[\'num_envs\'], dtype=np.int32)\n        worker_port = self.env_config[\'kwargs\'][\'port\']  # start value for BTGym comm. port\n\n        # TODO: Hacky, cause dataset is threadlocked; do: pass dataset as class_ref + kwargs_dict:\n        dataset_instance = self.env_config[\'kwargs\'].pop(\'dataset\')\n\n        for key, spec_list in self.cluster_spec.items():\n            task_index = 0  # referenced farther as worker id\n            for _id in spec_list:\n                env_config = copy.deepcopy(self.env_config)\n                worker_config = {}\n                if key in \'worker\':\n                    # Configure worker BTgym environment:\n                    if task_index == 0:\n                        env_config[\'kwargs\'][\'data_master\'] = True  # set worker_0 as chief and data_master\n                        env_config[\'kwargs\'][\'dataset\'] = dataset_instance\n                        env_config[\'kwargs\'][\'render_enabled\'] = True\n                    else:\n                        env_config[\'kwargs\'][\'data_master\'] = False\n                        env_config[\'kwargs\'][\'render_enabled\'] = False  # disable rendering for all but chief\n\n                    # Add list of connection ports for every parallel env for each worker:\n                    env_config[\'kwargs\'][\'port\'] = list(worker_port + env_ports)\n                    # Here master/slave pair is defined:\n                    env_config[\'kwargs\'][\'data_port\'] = [\n                        env_config[\'kwargs\'][\'data_port\'],  # data_server_port\n                        env_config[\'kwargs\'][\'port\'][0]     # comm_port of master env. as data_port for slave\n                    ]\n\n                    self.log.info(\'env_config: {}\'.format(env_config))\n\n                    worker_port += self.cluster_config[\'num_envs\']\n                worker_config.update(\n                    {\n                        \'env_config\': env_config,\n                        \'policy_config\': self.policy_config,\n                        \'trainer_config\': self.trainer_config,\n                        \'cluster_spec\': self.cluster_spec,\n                        \'job_name\': key,\n                        \'task\': task_index,\n                        \'test_mode\': self.test_mode,\n                        \'log_dir\': self.cluster_config[\'log_dir\'],\n                        \'max_env_steps\': self.max_env_steps,\n                        \'log_level\': self.log_level,\n                        \'random_seed\': self.workers_rnd_seeds.pop(),\n                        \'render_last_env\': self.render_slave_env  # last env in a pair is slave\n                    }\n                )\n                self.clear_port(env_config[\'kwargs\'][\'port\'])\n                workers_config_list.append(worker_config)\n                task_index += 1\n\n        return workers_config_list\n'"
btgym/algorithms/nn/__init__.py,0,b''
btgym/algorithms/nn/ae.py,38,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import flatten as batch_flatten\nfrom tensorflow.contrib.layers import layer_norm as norm_layer\n\nfrom btgym.algorithms.nn.layers import normalized_columns_initializer, linear, conv2d\n\n\ndef conv2d_encoder(x,\n                   layer_config=(\n                        (32, (3, 1), (2, 1)),\n                        (32, (3, 1), (2, 1)),\n                        (32, (3, 1), (2, 1)),\n                   ),\n                   pad=\'SAME\',\n                   name=\'encoder\',\n                   reuse=False):\n    """"""\n    Defines convolutional encoder.\n\n    Args:\n        x:              input tensor\n        layer_config:   first to last nested layers configuration list: [layer_1_config, layer_2_config,...], where:\n                        layer_i_config = [num_filters(int), filter_size(list), stride(list)]\n        pad:            str, padding scheme: \'SAME\' or \'VALID\'\n        name:           str, mame scope\n        reuse:          bool\n\n    Returns:\n        list of tensors holding encoded features for every layer outer to inner,\n        level-wise list of encoding layers shapes, first ro last.\n\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        layer_shapes = [x.get_shape()]\n        layer_outputs = []\n        for i, layer_spec in enumerate(layer_config, 1):\n            x = tf.nn.elu(\n                norm_layer(\n                    conv2d(\n                        x=x,\n                        num_filters=layer_spec[0],\n                        name=\'/conv_kernels_{}\'.format(i ),\n                        filter_size=layer_spec[1],\n                        stride=layer_spec[2],\n                        pad=pad,\n                        reuse=reuse\n                    )\n                ),\n                name=\'encoder_layer_{}\'.format(i),\n            )\n            layer_shapes.append(x.get_shape())\n            layer_outputs.append(x)\n\n        return layer_outputs, layer_shapes\n\n\ndef conv2d_decoder(z,\n                   layer_shapes,\n                   layer_config=(\n                        (32, (3, 1), (2, 1)),\n                        (32, (3, 1), (2, 1)),\n                        (32, (3, 1), (2, 1)),\n                   ),\n                   pad=\'SAME\',\n                   resize_method=tf.image.ResizeMethod.BILINEAR,\n                   name=\'decoder\',\n                   reuse=False):\n    """"""\n    Defines convolutional decoder.\n\n    Args:\n        z:                  tensor holding encoded state\n        layer_shapes:       level-wise list of matching encoding layers shapes, last to first.\n        layer_config:       layers configuration list: [layer_1_config, layer_2_config,...], where:\n                            layer_i_config = [num_filters(int), filter_size(list), stride(list)]\n        pad:                str, padding scheme: \'SAME\' or \'VALID\'\n        resize_method:      up-sampling method, one of supported tf.image.ResizeMethod\'s\n        name:               str, mame scope\n        reuse:              bool\n\n    Returns:\n        list of tensors holding decoded features for every layer inner to outer\n\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        x = z\n        layer_shapes = list(layer_shapes)\n        layer_shapes.reverse()\n        layer_config = list(layer_config)\n        layer_config.reverse()\n        layer_output = []\n        for i, (layer_spec, layer_shape) in enumerate(zip(layer_config,layer_shapes[1:]), 1):\n            x = tf.image.resize_images(\n                images=x,\n                size=[int(layer_shape[1]), int(layer_shape[2])],\n                method=resize_method,\n            )\n            x = tf.nn.elu(\n                conv2d(\n                    x=x,\n                    num_filters=layer_spec[0],\n                    name=\'conv_kernels_{}\'.format(i),\n                    filter_size=layer_spec[1],\n                    stride=[1, 1],\n                    pad=pad,\n                    reuse=reuse\n                ),\n                name=\'decoder_layer_{}\'.format(i),\n            )\n            layer_output.append(x)\n        y_hat = conv2d(\n            x=x,\n            num_filters=layer_shapes[-1][-1],\n            name=\'decoded_y_hat\',\n            filter_size=layer_config[-1][1],\n            stride=[1, 1],\n            pad=\'SAME\',\n            reuse=reuse\n        )\n        layer_output.append(y_hat)\n        return layer_output\n\n\ndef conv2d_autoencoder(\n        inputs,\n        layer_config,\n        resize_method=tf.image.ResizeMethod.BILINEAR,\n        pad=\'SAME\',\n        linear_layer_ref=linear,\n        name=\'base_conv2d_autoencoder\',\n        reuse=False,\n        **kwargs\n    ):\n    """"""\n    Basic convolutional autoencoder.\n    Hidden state is passed through dense linear layer.\n\n    Args:\n        inputs:             input tensor\n        layer_config:       layers configuration list: [layer_1_config, layer_2_config,...], where:\n                            layer_i_config = [num_filters(int), filter_size(list), stride(list)];\n                            this list represent decoder part of autoencoder bottleneck,\n                            decoder part is inferred symmetrically\n        resize_method:      up-sampling method, one of supported tf.image.ResizeMethod\'s\n        pad:                str, padding scheme: \'SAME\' or \'VALID\'\n        linear_layer_ref:   linear layer class to use\n        name:               str, mame scope\n        reuse:              bool\n\n    Returns:\n        list of tensors holding encoded features, layer_wise from outer to inner\n        tensor holding batch-wise flattened hidden state vector\n        list of tensors holding decoded features, layer-wise from inner to outer\n        tensor holding reconstructed output\n        None value\n\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        # Encode:\n        encoder_layers, shapes = conv2d_encoder(\n            x=inputs,\n            layer_config=layer_config,\n            pad=pad,\n            reuse=reuse\n        )\n        # Flatten hidden state, pass through dense :\n        z = batch_flatten(encoder_layers[-1])\n        h, w, c = encoder_layers[-1].get_shape().as_list()[1:]\n\n        z = linear_layer_ref(\n            x=z,\n            size=h * w * c,\n            name=\'hidden_dense\',\n            initializer=normalized_columns_initializer(1.0),\n            reuse=reuse\n        )\n        # Reshape back and feed to decoder:\n        decoder_layers = conv2d_decoder(\n            z=tf.reshape(z, [-1, h, w, c]),\n            layer_config=layer_config,\n            layer_shapes=shapes,\n            pad=pad,\n            resize_method=resize_method,\n            reuse=reuse\n        )\n        y_hat = decoder_layers[-1]\n        return encoder_layers, z, decoder_layers, y_hat, None\n\n\ndef cw_conv2d_autoencoder(\n        inputs,\n        layer_config,\n        resize_method=tf.image.ResizeMethod.BILINEAR,\n        pad=\'SAME\',\n        linear_layer_ref=linear,\n        name=\'cw_conv2d_autoencoder\',\n        reuse=False,\n        **kwargs\n    ):\n    """"""\n    Channel-wise convolutional autoencoder.\n    Hidden state is passed through dense linear layer.\n    Pain-slow, do not use.\n\n    Args:\n        inputs:             input tensor\n        layer_config:       layers configuration list: [layer_1_config, layer_2_config,...], where:\n                            layer_i_config = [num_filters(int), filter_size(list), stride(list)];\n                            this list represent decoder part of autoencoder bottleneck,\n                            decoder part is inferred symmetrically\n        resize_method:      up-sampling method, one of supported tf.image.ResizeMethod\'s\n        pad:                str, padding scheme: \'SAME\' or \'VALID\'\n        linear_layer_ref:   linear layer class to use\n        name:               str, mame scope\n        reuse:              bool\n\n    Returns:\n        per-channel list of lists of tensors holding encoded features, layer_wise from outer to inner\n        tensor holding batch-wise flattened hidden state vector\n        per-channel list of lists of tensors holding decoded features, layer-wise from inner to outer\n        tensor holding reconstructed output\n        None value\n\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        ae_bank = []\n        for i in range(inputs.get_shape().as_list()[-1]):\n            # Making list of list of AE\'s:\n            encoder_layers, z, decoder_layers, y_hat, _ = conv2d_autoencoder(\n                inputs=inputs[..., i][..., None],\n                layer_config=layer_config,\n                resize_method=resize_method,\n                linear_layer_ref=linear_layer_ref,\n                name=\'ae_channel_{}\'.format(i),\n                pad=pad\n            )\n            ae = dict(\n                inputs=inputs[..., i][..., None],\n                encoder_layers=encoder_layers,\n                z=z,\n                decoder_layers=decoder_layers,\n                y_hat=y_hat,\n            )\n\n            ae_bank.append(ae)\n\n        y_hat = []\n        z = []\n        cw_encoder_layers = []\n        cw_decoder_layers = []\n\n        for ae in ae_bank:\n            y_hat.append(ae[\'y_hat\'])\n            z.append(ae[\'z\'])\n            cw_encoder_layers.append(ae[\'encoder_layers\'])\n            cw_decoder_layers.append(ae[\'decoder_layers\'])\n\n        # Flatten hidden state:\n        z = tf.concat(z, axis=-1, name=\'hidden_state\')\n\n        # encoder_layers = []\n        # for layer in zip(*cw_encoder_layers):\n        #     encoder_layers.append(tf.concat(layer, axis=-2))\n        #\n        # decoder_layers = []\n        # for layer in zip(*cw_decoder_layers):\n        #     decoder_layers.append(tf.concat(layer, axis=-2))\n\n        # Reshape back reconstruction:\n        y_hat = tf.concat(y_hat, axis=-1, name=\'decoded_y_hat\')\n\n        return cw_encoder_layers, z, cw_decoder_layers, y_hat, None\n\n\ndef beta_var_conv2d_autoencoder(\n        inputs,\n        layer_config,\n        resize_method=tf.image.ResizeMethod.BILINEAR,\n        pad=\'SAME\',\n        linear_layer_ref=linear,\n        name=\'vae_conv2d\',\n        max_batch_size=256,\n        reuse=False\n    ):\n    """"""\n    Variational autoencoder.\n\n    Papers:\n        https://arxiv.org/pdf/1312.6114.pdf\n        https://arxiv.org/pdf/1606.05908.pdf\n        http://www.matthey.me/pdf/betavae_iclr_2017.pdf\n\n\n    Args:\n        inputs:             input tensor\n        layer_config:       layers configuration list: [layer_1_config, layer_2_config,...], where:\n                            layer_i_config = [num_filters(int), filter_size(list), stride(list)];\n                            this list represent decoder part of autoencoder bottleneck,\n                            decoder part is inferred symmetrically\n        resize_method:      up-sampling method, one of supported tf.image.ResizeMethod\'s\n        pad:                str, padding scheme: \'SAME\' or \'VALID\'\n        linear_layer_ref:   linear layer class - not used\n        name:               str, mame scope\n        max_batch_size:     int, dynamic batch size should be no greater than this value\n        reuse:              bool\n\n    Returns:\n        list of tensors holding encoded features, layer_wise from outer to inner\n        tensor holding batch-wise flattened hidden state vector\n        list of tensors holding decoded features, layer-wise from inner to outer\n        tensor holding reconstructed output\n        tensor holding estimated KL divergence\n\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n\n        # Encode:\n        encoder_layers, shapes = conv2d_encoder(\n            x=inputs,\n            layer_config=layer_config,\n            pad=pad,\n            reuse=reuse\n        )\n        # Flatten hidden state, pass through dense:\n        z_flat = batch_flatten(encoder_layers[-1])\n\n        h, w, c = encoder_layers[-1].get_shape().as_list()[1:]\n\n        z = tf.nn.elu(\n            linear(\n                x=z_flat,\n                size=h * w * c,\n                name=\'enc_dense\',\n                initializer=normalized_columns_initializer(1.0),\n                reuse=reuse\n            )\n        )\n        # TODO: revert back to dubled Z-size\n        # half_size_z = h * w * c\n        # size_z = 2 * half_size_z\n\n        size_z = int(h * w * c/2)\n        z = tf.nn.elu(\n            linear(\n                #x=z_flat,\n                x=z,\n                #size=size_z,\n                size=size_z * 2,\n                name=\'hidden_dense\',\n                initializer=normalized_columns_initializer(1.0),\n                reuse=reuse\n            )\n        )\n        # Get sample parameters:\n        #mu, log_sigma = tf.split(z, [half_size_z, half_size_z], axis=-1)\n        mu, log_sigma = tf.split(z, [size_z, size_z], axis=-1)\n\n        # Oversized noise generator:\n        #eps = tf.random_normal(shape=[max_batch_size, half_size_z], mean=0., stddev=1.)\n        eps = tf.random_normal(shape=[max_batch_size, size_z], mean=0., stddev=1.)\n        eps = eps[:tf.shape(z)[0],:]\n\n        # Get sample z ~ Q(z|X):\n        z_sampled = mu + tf.exp(log_sigma / 2) * eps\n\n        # D_KL(Q(z|X) || P(z|X)):\n        # TODO: where is sum?!\n        d_kl = 0.5 * (tf.exp(log_sigma) + tf.square(mu) - 1. - log_sigma)\n\n        # Reshape back and feed to decoder:\n\n        z_sampled_dec = tf.nn.elu(\n            linear(\n                x=z_sampled,\n                size=h * w * c,\n                name=\'dec_dense\',\n                initializer=normalized_columns_initializer(1.0),\n                reuse=reuse\n            )\n        )\n\n        decoder_layers = conv2d_decoder(\n            z=tf.reshape(z_sampled_dec, [-1, h, w, c]),\n            layer_config=layer_config,\n            layer_shapes=shapes,\n            pad=pad,\n            resize_method=resize_method,\n            reuse=reuse\n        )\n        y_hat = decoder_layers[-1]\n        return encoder_layers, z_sampled, decoder_layers, y_hat, d_kl\n\n\nclass KernelMonitor():\n    """"""\n    Visualises convolution filters learnt for specific layer.\n    Source: https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n    """"""\n\n    def __init__(self, conv_input, layer_output):\n        """"""\n\n        Args:\n            conv_input:         convolution stack input tensor\n            layer_output:       tensor holding output of layer of interest from stack\n        """"""\n        self.idx = tf.placeholder(tf.int32, name=\'kernel_index\')\n        self.conv_input = conv_input\n        self.layer_output = layer_output\n        # Build a loss function that maximizes the activation\n        # of the n-th filter of the layer considered:\n        self.vis_loss = tf.reduce_mean(self.layer_output[:, :, :, self.idx])\n\n        # Gradient of the input picture wrt this loss:\n        self.vis_grads = tf.gradients(self.vis_loss, self.conv_input)[0]\n\n        # Normalization trick:\n        self.vis_grads /= (tf.sqrt(tf.reduce_mean(tf.square(self.vis_grads))) + 1e-5)\n\n    def _iterate(self, sess, signal, kernel_index):\n        """"""\n        Returns the loss and grads for specified kernel given the input signal\n\n        Args:\n            sess:           tf.Session object\n            signal:         input signal to convolution stack\n            kernel_index:   filter number in layer considered\n\n        Returns:\n            loss and gradients values\n        """"""\n        return sess.run([self.vis_loss, self.vis_grads], {self.conv_input: signal, self.idx: kernel_index})\n\n    def fit(self, sess, kernel_index, step=1e-3, num_steps=40):\n        """"""\n        Learns input signal that maximizes the activation of given kernel.\n\n        Args:\n            sess:               tf.Session object\n            kernel_index:       filter number of interest\n            step:               gradient ascent step size\n            num_steps:          number of steps to fit\n\n        Returns:\n            learnt signal as np.array\n\n        """"""\n        # Start from some noise:\n        signal = np.random.random([1] + self.conv_input.get_shape().as_list()[1:])\n\n        # Run gradient ascent:\n        for i in range(num_steps):\n            loss_value, grads_value = self._iterate(sess, signal, kernel_index)\n            signal += grads_value * step\n\n        return signal\n'"
btgym/algorithms/nn/layers.py,48,"b'# Original code comes from OpenAI repository under MIT licence:\n#\n# https://github.com/openai/universe-starter-agent\n# https://github.com/openai/baselines\n#\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef normalized_columns_initializer(std=1.0):\n    def _initializer(shape, dtype=None, partition_info=None):\n        out = np.random.randn(*shape).astype(np.float32)\n        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n        return tf.constant(out)\n    return _initializer\n\n\n# def categorical_sample(logits, d):\n#     value = tf.squeeze(tf.multinomial(logits - tf.reduce_max(logits, [1], keepdims=True), 1), [1])\n#     return tf.one_hot(value, d)\n\ndef categorical_sample(logits, depth):\n    """"""\n    Given logits returns one-hot encoded categorical sample.\n    Args:\n        logits:\n        depth:\n\n    Returns:\n            tensor of shape [batch_dim, logits_depth]\n    """"""\n    # print(\'categorical_sample_logits: \', logits)\n    value = tf.squeeze(tf.multinomial(logits, 1), [1])\n    one_hot = tf.one_hot(value, depth, name=\'sample_one_hot\')\n    return one_hot\n\n\ndef linear(x, size, name, initializer=None, bias_init=0, reuse=False):\n    """"""\n    Linear network layer.\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        w = tf.get_variable(""/w"", [x.get_shape()[1], size], initializer=initializer)\n        b = tf.get_variable(""/b"", [size], initializer=tf.constant_initializer(bias_init))\n        return tf.matmul(x, w) + b\n\n\ndef noisy_linear(x, size, name, bias=True, activation_fn=tf.identity, reuse=False, **kwargs):\n    """"""\n    Noisy Net linear network layer using Factorised Gaussian noise;\n    Code by Andrew Liao, https://github.com/andrewliao11/NoisyNet-DQN\n\n    Papers:\n        https://arxiv.org/abs/1706.10295\n        https://arxiv.org/abs/1706.01905\n\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        # the function used in eq.7,8\n        def f(x):\n            return tf.multiply(tf.sign(x), tf.pow(tf.abs(x), 0.5))\n        # Initializer of \\mu and \\sigma\n        mu_init = tf.random_uniform_initializer(minval=-1*1/np.power(x.get_shape().as_list()[1], 0.5),\n                                                    maxval=1*1/np.power(x.get_shape().as_list()[1], 0.5))\n        sigma_init = tf.constant_initializer(0.4/np.power(x.get_shape().as_list()[1], 0.5))\n        # Sample noise from gaussian\n        p = tf.random_normal([x.get_shape().as_list()[1], 1])\n        q = tf.random_normal([1, size])\n        f_p = f(p); f_q = f(q)\n        w_epsilon = f_p*f_q; b_epsilon = tf.squeeze(f_q)\n\n        # w = w_mu + w_sigma*w_epsilon\n        w_mu = tf.get_variable(""/w_mu"", [x.get_shape()[1], size], initializer=mu_init)\n        w_sigma = tf.get_variable(""/w_sigma"", [x.get_shape()[1], size], initializer=sigma_init)\n        w = w_mu + tf.multiply(w_sigma, w_epsilon)\n        ret = tf.matmul(x, w)\n        if bias:\n            # b = b_mu + b_sigma*b_epsilon\n            b_mu = tf.get_variable(""/b_mu"", [size], initializer=mu_init)\n            b_sigma = tf.get_variable(""/b_sigma"", [size], initializer=sigma_init)\n            b = b_mu + tf.multiply(b_sigma, b_epsilon)\n            return activation_fn(ret + b)\n        else:\n            return activation_fn(ret)\n\n\ndef conv2d(x, num_filters, name, filter_size=(3, 3), stride=(1, 1), pad=""SAME"", dtype=tf.float32,\n           collections=None, reuse=False):\n    """"""\n    2D convolution layer.\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        stride_shape = [1, stride[0], stride[1], 1]\n        filter_shape = [filter_size[0], filter_size[1], int(x.get_shape()[3]), num_filters]\n\n        w = tf.get_variable(""W"", filter_shape, dtype, initializer=tf.contrib.layers.xavier_initializer(),\n                            collections=collections)\n        b = tf.get_variable(""b"", [1, 1, 1, num_filters], initializer=tf.constant_initializer(0.0),\n                            collections=collections)\n        return tf.nn.conv2d(x, w, stride_shape, pad) + b\n\n\ndef deconv2d(x, output_channels, name, filter_size=(4, 4), stride=(2, 2),\n             dtype=tf.float32, collections=None, reuse=False):\n    """"""\n    Deconvolution layer, paper:\n    http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        stride_shape = [1, stride[0], stride[1], 1]\n\n        batch_size = tf.shape(x)[0]\n        input_height = int(x.get_shape()[1])\n        input_width = int(x.get_shape()[2])\n        input_channels = int(x.get_shape()[3])\n\n        out_height = (input_height - 1) * stride[0] + filter_size[0]\n        out_width = (input_width - 1) * stride[1] + filter_size[1]\n\n        filter_shape = [filter_size[0], filter_size[1], output_channels, input_channels]\n        output_shape = tf.stack([batch_size, out_height, out_width, output_channels])\n\n        fan_in = np.prod(filter_shape[:2]) * input_channels\n        fan_out = np.prod(filter_shape[:2]) * output_channels\n        # initialize weights with random weights\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n\n        w = tf.get_variable(""d_W"", filter_shape, dtype, initializer=tf.contrib.layers.xavier_initializer(),\n                            collections=collections)\n        b = tf.get_variable(""d_b"", [1, 1, 1, output_channels], initializer=tf.constant_initializer(0.0),\n                            collections=collections)\n\n        return tf.nn.conv2d_transpose(x, w, output_shape,\n                                      strides=stride_shape,\n                                      padding=\'VALID\') + b\n\n\ndef conv1d(x, num_filters, name, filter_size=3, stride=2, pad=""SAME"", dtype=tf.float32,\n           collections=None, reuse=False):\n    """"""\n    1D convolution layer.\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        stride_shape = stride\n\n        # print(\'stride_shape:\',stride_shape)\n\n        filter_shape = [filter_size, int(x.get_shape()[-1]), num_filters]\n\n        # print(\'filter_shape:\', filter_shape)\n\n        w = tf.get_variable(""W"", filter_shape, dtype, initializer=tf.contrib.layers.xavier_initializer(),\n                            collections=collections)\n        b = tf.get_variable(""b"", [1, 1, num_filters], initializer=tf.constant_initializer(0.0),\n                            collections=collections)\n        return tf.nn.conv1d(x, w, stride_shape, pad) + b\n\n\ndef conv2d_dw(x, num_filters, name=\'conv2d_dw\', filter_size=(3, 3), stride=(1, 1), pad=""SAME"", dtype=tf.float32,\n              collections=None, reuse=False):\n    """"""\n    Depthwise 2D convolution layer. Slow, do not use.\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        stride_shape = [1, stride[0], stride[1], 1]\n        filter_shape = [filter_size[0], filter_size[1], int(x.get_shape()[-1]), num_filters]\n        fan_in = np.prod(filter_shape[:3])\n        fan_out = np.prod(filter_shape[:2]) * num_filters\n        # initialize weights with random weights\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n\n        w = tf.get_variable(""W"", filter_shape, dtype,\n                            tf.contrib.layers.xavier_initializer(), collections=collections)\n        b = tf.get_variable(""b"", [1, 1, 1, num_filters * int(x.get_shape()[-1])],\n                            initializer=tf.constant_initializer(0.0), collections=collections)\n        return tf.nn.depthwise_conv2d(x, w, stride_shape, pad, [1, 1]) + b\n'"
btgym/algorithms/nn/losses.py,61,"b'import tensorflow as tf\nimport  numpy as np\nfrom btgym.algorithms.math_utils import cat_entropy, kl_divergence\n\n\ndef aac_loss_def(act_target, adv_target, r_target, pi_logits, pi_vf, pi_prime_logits,\n                 entropy_beta, epsilon=None, name=\'_aac_\', verbose=False):\n    """"""\n    Advantage Actor Critic loss definition.\n    Paper: https://arxiv.org/abs/1602.01783\n\n    Args:\n        act_target:      tensor holding policy actions targets;\n        adv_target:      tensor holding policy estimated advantages targets;\n        r_target:        tensor holding policy empirical returns targets;\n        pi_logits:       policy logits output tensor;\n        pi_prime_logits: not used;\n        pi_vf:           policy value function output tensor;\n        entropy_beta:    entropy regularization constant;\n        epsilon:         not used;\n        name:            scope;\n        verbose:         summary level.\n\n    Returns:\n        tensor holding estimated AAC loss;\n        list of related tensorboard summaries.\n    """"""\n    with tf.name_scope(name + \'/aac\'):\n        neg_pi_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=pi_logits,\n            labels=act_target\n        )\n        pi_loss = tf.reduce_mean(neg_pi_log_prob * adv_target)\n        vf_loss = 0.5 * tf.losses.mean_squared_error(r_target, pi_vf)\n        entropy = tf.reduce_mean(cat_entropy(pi_logits))\n\n        loss = pi_loss + vf_loss - entropy * entropy_beta\n\n        mean_vf = tf.reduce_mean(pi_vf)\n        mean_t_target = tf.reduce_mean(r_target)\n\n        summaries = [\n            tf.summary.scalar(\'policy_loss\', pi_loss),\n            tf.summary.scalar(\'value_loss\', vf_loss),\n        ]\n        if verbose:\n            summaries += [\n                tf.summary.scalar(\'entropy\', entropy),\n                tf.summary.scalar(\'value_fn\', mean_vf),\n                # tf.summary.scalar(\'empirical_return\',mean_t_target),\n                # tf.summary.histogram(\'value_fn\', pi_vf),\n                # tf.summary.histogram(\'empirical_return\', r_target),\n            ]\n\n    return loss, summaries\n\n\ndef ppo_loss_def(act_target, adv_target, r_target, pi_logits, pi_vf, pi_prime_logits, entropy_beta, epsilon,\n                 name=\'_ppo_\', verbose=False):\n    """"""\n    PPO clipped surrogate loss definition, as (7) in https://arxiv.org/pdf/1707.06347.pdf\n\n    Args:\n        act_target:      tensor holding policy actions targets;\n        adv_target:      tensor holding policy estimated advantages targets;\n        r_target:        tensor holding policy empirical returns targets;\n        pi_logits:       policy logits output tensor;\n        pi_vf:           policy value function output tensor;\n        pi_prime_logits: old_policy logits output tensor;\n        entropy_beta:    entropy regularization constant\n        epsilon:         L^Clip epsilon tensor;\n        name:            scope;\n        verbose:         summary level.\n\n    Returns:\n        tensor holding estimated PPO L^Clip loss;\n        list of related tensorboard summaries.\n    """"""\n    #act_target = tf.placeholder(tf.float32, [None, env.action_space.n], name=""on_policy_action_pl"")\n    #adv_target = tf.placeholder(tf.float32, [None], name=""on_policy_advantage_pl"")\n    #r_target = tf.placeholder(tf.float32, [None], name=""on_policy_return_pl"")\n    with tf.name_scope(name + \'/ppo\'):\n        pi_log_prob = - tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=pi_logits,\n            labels=act_target\n        )\n        pi_old_log_prob = tf.stop_gradient(\n            - tf.nn.softmax_cross_entropy_with_logits_v2(\n                logits=pi_prime_logits,\n                labels=act_target\n            )\n        )\n        pi_ratio = tf.exp(pi_log_prob - pi_old_log_prob)\n\n        surr1 = pi_ratio * adv_target  # surrogate from conservative policy iteration\n        surr2 = tf.clip_by_value(pi_ratio, 1.0 - epsilon, 1.0 + epsilon) * adv_target\n\n        pi_surr_loss = - tf.reduce_mean(tf.minimum(surr1, surr2))  # PPO\'s pessimistic surrogate (L^CLIP)\n        vf_loss = tf.losses.mean_squared_error(r_target, pi_vf)  # V.fn. loss\n        entropy = tf.reduce_mean(cat_entropy(pi_logits))\n\n        loss = pi_surr_loss + vf_loss - entropy * entropy_beta\n\n        # Info:\n        mean_pi_ratio = tf.reduce_mean(pi_ratio)\n        mean_vf = tf.reduce_mean(pi_vf)\n        mean_kl_old_new = tf.reduce_mean(kl_divergence(pi_prime_logits, pi_logits))\n\n        summaries = [\n            tf.summary.scalar(\'l_clip_loss\', pi_surr_loss),\n            tf.summary.scalar(\'value_loss\', vf_loss),\n        ]\n        if verbose:\n            summaries += [\n                tf.summary.scalar(\'entropy\', entropy),\n                tf.summary.scalar(\'Dkl_old_new\', mean_kl_old_new),\n                tf.summary.scalar(\'pi_ratio\', mean_pi_ratio),\n                tf.summary.scalar(\'value_fn\', mean_vf),\n            ]\n\n    return loss, summaries\n\n\ndef value_fn_loss_def(r_target, pi_vf, name=\'_vr_\', verbose=False):\n    """"""\n    Value function loss.\n\n    Args:\n        r_target:        tensor holding policy empirical returns targets;\n        pi_vf:           policy value function output tensor;\n        name:            scope;\n        verbose:         summary level.\n\n    Returns:\n        tensor holding estimated value fn. loss;\n        list of related tensorboard summaries.\n    """"""\n    # r_target = tf.placeholder(tf.float32, [None], name=""vr_target"")\n    with tf.name_scope(name + \'/value_replay\'):\n        loss = tf.losses.mean_squared_error(r_target, pi_vf)\n\n        if verbose:\n            summaries = [tf.summary.scalar(\'v_loss\', loss)]\n        else:\n            summaries = []\n\n    return loss, summaries\n\n\ndef pc_loss_def(actions, targets, pi_pc_q, name=\'_pc_\', verbose=False):\n    """"""\n    Pixel control auxiliary task loss definition.\n\n    Paper: https://arxiv.org/abs/1611.05397\n\n    Borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:\n\n    https://miyosuda.github.io/\n\n    https://github.com/miyosuda/unreal\n\n    Args:\n        actions:     tensor holding policy actions;\n        targets:     tensor holding estimated pixel-change targets;\n        pi_pc_q:     policy Q-value features output tensor;\n        name:        scope;\n        verbose:     summary level.\n\n    Returns:\n        tensor holding estimated pc loss;\n        list of related tensorboard summaries.\n    """"""\n    #actions = tf.placeholder(tf.float32, [None, env.action_space.n], name=""pc_action"")\n    #targets = tf.placeholder(tf.float32, [None, None, None], name=""pc_target"")\n    with tf.name_scope(name + \'/pixel_control\'):\n        # Get Q-value features for actions been taken and define loss:\n        pc_action_reshaped = tf.reshape(actions, [-1, 1, 1, tf.shape(actions)[-1]])\n        pc_q_action = tf.multiply(pi_pc_q, pc_action_reshaped)\n        pc_q_action = tf.reduce_sum(pc_q_action, axis=-1, keepdims=False)\n\n        batch_size = tf.shape(targets)[0]\n        loss = tf.reduce_sum(tf.square(targets - pc_q_action)) / tf.cast(batch_size, tf.float32)\n        #loss = tf.losses.absolute_difference(targets, pc_q_action)\n        if verbose:\n            summaries = [tf.summary.scalar(\'q_loss\', loss)]\n        else:\n            summaries = []\n\n    return loss, summaries\n\n\ndef rp_loss_def(rp_targets, pi_rp_logits, name=\'_rp_\', verbose=False):\n    """"""\n    Reward prediction auxillary task loss definition.\n\n    Paper: https://arxiv.org/abs/1611.05397\n\n    Borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:\n\n    https://miyosuda.github.io/\n\n    https://github.com/miyosuda/unreal\n\n\n    Args:\n        targets:         tensor holding reward prediction target;\n        pi_rp_logits:    policy reward predictions tensor;\n        name:             scope;\n        verbose:          summary level.\n\n    Returns:\n        tensor holding estimated rp loss;\n        list of related tensorboard summaries.\n    """"""\n    #rp_targets = tf.placeholder(tf.float32, [1, 3], name=""rp_target"")\n    with tf.name_scope(name + \'/reward_prediction\'):\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n            labels=rp_targets,\n            logits=pi_rp_logits\n        )[0]\n        if verbose:\n            summaries = [tf.summary.scalar(\'class_loss\', loss), ]\n        else:\n            summaries = []\n\n    return loss, summaries\n\n\ndef ae_loss_def(targets, logits, alpha=1.0, name=\'ae_loss\', verbose=False, **kwargs):\n    """"""\n    Mean quadratic autoencoder reconstruction loss definition\n\n    Args:\n        targets:        tensor holding reconstruction target\n        logits:         t ensor holding decoded aa decoder output\n        alpha:          loss weight constant\n        name:           scope\n        verbose:        summary level.\n\n    Returns:\n        tensor holding estimated reconstruction loss\n        list of summarues\n    """"""\n    with tf.name_scope(name + \'/ae\'):\n        loss = tf.losses.mean_squared_error(targets, logits)\n\n        if verbose:\n            summaries = [tf.summary.scalar(\'reconstruct_loss\', loss)]\n        else:\n            summaries = []\n\n        return alpha * loss, summaries\n\n\ndef beta_vae_loss_def(targets, logits, d_kl, alpha=1.0, beta=1.0, name=\'beta_vae_loss\', verbose=False):\n    """"""\n    Beta-variational autoencoder loss definition\n\n    Papers:\n        http://www.matthey.me/pdf/betavae_iclr_2017.pdf\n        https://drive.google.com/file/d/0Bwy4Nlx78QCCNktVTFFMTUs4N2oxY295VU9qV25MWTBQS2Uw/view\n\n    Args:\n        targets:\n        logits:\n        d_kl:\n        alpha:\n        beta:\n        name:\n        verbose:\n\n    Returns:\n        tensor holding estimated loss\n        list of summarues\n\n    """"""\n    with tf.name_scope(name + \'/b_vae\'):\n        r_loss = tf.losses.mean_squared_error(targets, logits)\n        vae_loss = tf.reduce_mean(d_kl)\n        loss = alpha * r_loss + beta * vae_loss\n        if verbose:\n            summaries = [\n                tf.summary.scalar(\'reconstruct_loss\', r_loss),\n                tf.summary.scalar(\'d_kl_loss\', vae_loss),\n            ]\n        else:\n            summaries = []\n\n        return loss, summaries\n\n\n\n'"
btgym/algorithms/nn/networks.py,32,"b'# Original code comes from OpenAI repository under MIT licence:\n#\n# https://github.com/openai/universe-starter-agent\n# https://github.com/openai/baselines\n#\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.rnn as rnn\nfrom tensorflow.contrib.layers import layer_norm as norm_layer\nfrom tensorflow.python.util.nest import flatten as flatten_nested\n\nfrom btgym.algorithms.nn.layers import normalized_columns_initializer, categorical_sample\nfrom btgym.algorithms.nn.layers import linear, noisy_linear, conv2d, deconv2d, conv1d\nfrom btgym.algorithms.utils import rnn_placeholders\n\n\ndef conv_2d_network(x,\n                    ob_space,\n                    ac_space,\n                    conv_2d_layer_ref=conv2d,\n                    conv_2d_num_filters=(32, 32, 64, 64),\n                    conv_2d_filter_size=(3, 3),\n                    conv_2d_stride=(2, 2),\n                    pad=""SAME"",\n                    dtype=tf.float32,\n                    name=\'conv2d\',\n                    collections=None,\n                    reuse=False,\n                    keep_prob=None,\n                    **kwargs):\n    """"""\n    Stage1 network: from preprocessed 2D input to estimated features.\n    Encapsulates convolutions + layer normalisation + nonlinearity. Can be shared.\n\n    Returns:\n        tensor holding state features;\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        for i, num_filters in enumerate(conv_2d_num_filters):\n            x = tf.nn.elu(\n                norm_layer(\n                    conv_2d_layer_ref(\n                        x,\n                        num_filters,\n                        ""_layer_{}"".format(i + 1),\n                        conv_2d_filter_size,\n                        conv_2d_stride,\n                        pad,\n                        dtype,\n                        collections,\n                        reuse\n                    ),\n                    scope=name + ""_norm_layer_{}"".format(i + 1)\n                )\n            )\n            if keep_prob is not None:\n                x = tf.nn.dropout(x, keep_prob=keep_prob, name=""_layer_{}_with_dropout"".format(i + 1))\n\n        # A3c/BaseAAC original paper design:\n        # x = tf.nn.elu(conv2d(x, 16, \'conv2d_1\', [8, 8], [4, 4], pad, dtype, collections, reuse))\n        # x = tf.nn.elu(conv2d(x, 32, \'conv2d_2\', [4, 4], [2, 2], pad, dtype, collections, reuse))\n        # x = tf.nn.elu(\n        #   linear(batch_flatten(x), 256, \'conv_2d_dense\', normalized_columns_initializer(0.01), reuse=reuse)\n        # )\n        return x\n\n\ndef conv_1d_network(x,\n                    ob_space,\n                    ac_space,\n                    conv_1d_num_layers=4,\n                    conv_1d_num_filters=32,\n                    conv_1d_filter_size=3,\n                    conv_1d_stride=2,\n                    pad=""SAME"",\n                    dtype=tf.float32,\n                    collections=None,\n                    reuse=False,\n                    **kwargs):\n    """"""\n    Stage1 network: from preprocessed 1D input to estimated features.\n    Encapsulates convolutions, [possibly] skip-connections etc. Can be shared.\n\n    Returns:\n        tensor holding state features;\n    """"""\n    for i in range(conv_1d_num_layers):\n        x = tf.nn.elu(\n            conv1d(\n                x,\n                conv_1d_num_filters,\n                ""conv1d_{}"".format(i + 1),\n                conv_1d_filter_size,\n                conv_1d_stride,\n                pad,\n                dtype,\n                collections,\n                reuse\n            )\n        )\n    return x\n\n\ndef lstm_network(\n        x,\n        lstm_sequence_length,\n        lstm_class=rnn.BasicLSTMCell,\n        lstm_layers=(256,),\n        static=False,\n        keep_prob=None,\n        name=\'lstm\',\n        reuse=False,\n        **kwargs\n    ):\n    """"""\n    Stage2 network: from features to flattened LSTM output.\n    Defines [multi-layered] dynamic [possibly shared] LSTM network.\n\n    Returns:\n         batch-wise flattened output tensor;\n         lstm initial state tensor;\n         lstm state output tensor;\n         lstm flattened feed placeholders as tuple.\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        # Prepare rnn type:\n        if static:\n            rnn_net = tf.nn.static_rnn\n            # Remove time dimension (suppose always get one) and wrap to list:\n            x = [x[:, 0, :]]\n\n        else:\n            rnn_net = tf.nn.dynamic_rnn\n        # Define LSTM layers:\n        lstm = []\n        for size in lstm_layers:\n            layer = lstm_class(size)\n            if keep_prob is not None:\n                layer = tf.nn.rnn_cell.DropoutWrapper(layer, output_keep_prob=keep_prob)\n\n            lstm.append(layer)\n\n        lstm = rnn.MultiRNNCell(lstm, state_is_tuple=True)\n        # Get time_dimension as [1]-shaped tensor:\n        step_size = tf.expand_dims(tf.shape(x)[1], [0])\n\n        lstm_init_state = lstm.zero_state(1, dtype=tf.float32)\n\n        lstm_state_pl = rnn_placeholders(lstm.zero_state(1, dtype=tf.float32))\n        lstm_state_pl_flatten = flatten_nested(lstm_state_pl)\n\n        # print(\'rnn_net: \', rnn_net)\n\n        lstm_outputs, lstm_state_out = rnn_net(\n            cell=lstm,\n            inputs=x,\n            initial_state=lstm_state_pl,\n            sequence_length=lstm_sequence_length,\n        )\n\n        # print(\'\\nlstm_outputs: \', lstm_outputs)\n        # print(\'\\nlstm_state_out:\', lstm_state_out)\n\n        # Unwrap and expand:\n        if static:\n            x_out = lstm_outputs[0][:, None, :]\n        else:\n            x_out = lstm_outputs\n        state_out = lstm_state_out\n    return x_out, lstm_init_state, state_out, lstm_state_pl_flatten\n\n\ndef dense_aac_network(x, ac_space_depth, name=\'dense_aac\', linear_layer_ref=noisy_linear, reuse=False):\n    """"""\n    Stage3 network: from LSTM flattened output to advantage actor-critic.\n\n    Returns:\n        dictionary containg tuples:\n            logits tensor\n            value function tensor\n            action sampling function.\n        for every space in ac_space_shape dictionary\n    """"""\n\n    with tf.variable_scope(name, reuse=reuse):\n        # Center-logits:\n        logits = norm_layer(\n            linear_layer_ref(\n                x=x,\n                size=ac_space_depth,\n                name=\'action\',\n                initializer=normalized_columns_initializer(0.01),\n                reuse=reuse\n            ),\n            center=True,\n            scale=False,\n        )\n\n        vf = tf.reshape(\n            linear_layer_ref(\n                x=x,\n                size=1,\n                name=""value"",\n                initializer=normalized_columns_initializer(1.0),\n                reuse=reuse\n            ),\n            [-1]\n        )\n        sample = categorical_sample(logits=logits, depth=ac_space_depth)[0, :]\n\n\n\n    return logits, vf, sample\n\n\ndef dense_rp_network(x, linear_layer_ref=noisy_linear):\n    """"""\n    Stage3 network: From shared convolutions to reward-prediction task output tensor.\n    """"""\n    # print(\'x_shape:\', x.get_shape())\n    #x = tf.reshape(x, [1, -1]) # flatten to pretend we got batch of size 1\n\n    # Fully connected x128 followed by 3-way classifier [with softmax], as in paper:\n    x = tf.nn.elu(\n        linear_layer_ref(\n            x=x,\n            size=128,\n            name=\'rp_dense\',\n            initializer=normalized_columns_initializer(0.01)\n        )\n    )\n    logits = linear_layer_ref(\n        x=x,\n        size=3,\n        name=\'rp_classifier\',\n        initializer=normalized_columns_initializer(0.01)\n    )\n    # Note:  softmax is actually not here but inside loss operation (see losses.py)\n    return logits\n\n\ndef pixel_change_2d_estimator(ob_space, pc_estimator_stride=(2, 2), **kwargs):\n    """"""\n    Defines tf operation for estimating `pixel change` as subsampled absolute difference of two states.\n\n    Note:\n        crops input array by one pix from either side; --> 1D signal to be shaped as [signal_length, 3]\n    """"""\n    input_state = tf.placeholder(tf.float32, list(ob_space), name=\'pc_change_est_state_in\')\n    input_last_state = tf.placeholder(tf.float32, list(ob_space), name=\'pc_change_est_last_state_in\')\n\n    x = tf.abs(tf.subtract(input_state, input_last_state)) # TODO: tf.square?\n\n    if x.shape[-2] <= 3:\n        x = tf.expand_dims(x, 0)[:, 1:-1, :, :]  # Assume 1D signal, fake batch dim and crop H dim only\n        #x = tf.transpose(x, perm=[0, 1, 3, 2])  # Swap channels and height for\n    else:\n        x = tf.expand_dims(x, 0)[:, 1:-1, 1:-1, :]  # True 2D,  fake batch dim and crop H, W dims\n\n    x = tf.reduce_mean(x, axis=-1, keepdims=True)\n\n    x_out = tf.nn.max_pool(\n        x,\n        [1, pc_estimator_stride[0], pc_estimator_stride[1], 1],\n        [1, pc_estimator_stride[0], pc_estimator_stride[1], 1],\n        \'SAME\'\n    )\n    return input_state, input_last_state, x_out\n\n\ndef duelling_pc_network(x,\n                        ac_space,\n                        duell_pc_x_inner_shape=(9, 9, 32),\n                        duell_pc_filter_size=(4, 4),\n                        duell_pc_stride=(2, 2),\n                        linear_layer_ref=noisy_linear,\n                        reuse=False,\n                        **kwargs):\n    """"""\n    Stage3 network for `pixel control\' task: from LSTM output to Q-aux. features tensor.\n    """"""\n    x = tf.nn.elu(\n        linear_layer_ref(\n            x=x,\n            size=np.prod(duell_pc_x_inner_shape),\n            name=\'pc_dense\',\n            initializer=tf.contrib.layers.xavier_initializer(),\n            reuse=reuse\n        )\n    )\n    x = tf.reshape(x, [-1] + list(duell_pc_x_inner_shape))\n    pc_a = deconv2d(x, ac_space, \'pc_advantage\', duell_pc_filter_size, duell_pc_stride, reuse=reuse)  # [None, 20, 20, ac_size]\n    pc_v = deconv2d(x, 1, \'pc_value_fn\', duell_pc_filter_size, duell_pc_stride, reuse=reuse)  # [None, 20, 20, 1]\n\n    # Q-value estimate using advantage mean,\n    # as (9) in ""Dueling Network Architectures..."" paper:\n    # https://arxiv.org/pdf/1511.06581.pdf\n    pc_a_mean = tf.reduce_mean(pc_a, axis=-1, keepdims=True)\n    pc_q = pc_v + pc_a - pc_a_mean  # [None, 20, 20, ac_size]\n\n    return pc_q\n\n\n'"
btgym/algorithms/policy/__init__.py,0,"b'from .base import BaseAacPolicy, Aac1dPolicy\nfrom .stacked_lstm import StackedLstmPolicy, AacStackedRL2Policy'"
btgym/algorithms/policy/base.py,40,"b'# Asynchronous implementation of a3c/ppo algorithm.\n# paper:\n# https://arxiv.org/pdf/1707.06347.pdf\n#\n# Based on PPO-SGD code from OpenAI `Baselines` repository under MIT licence:\n# https://github.com/openai/baselines\n#\n# Async. framework code comes from OpenAI repository under MIT licence:\n# https://github.com/openai/universe-starter-agent\n#\n\nfrom gym.spaces import Discrete, Dict\n\nfrom btgym.algorithms.nn.networks import *\nfrom btgym.algorithms.utils import *\nfrom btgym.algorithms.math_utils import sample_dp, softmax\nfrom btgym.datafeed.base import EnvResetConfig\nfrom btgym.spaces import DictSpace, ActionDictSpace\nfrom gym.spaces import Discrete\n\n\nclass BaseAacPolicy(object):\n    """"""\n    Base advantage actor-critic Convolution-LSTM policy estimator with auxiliary control tasks for\n    discrete or nested discrete action spaces.\n\n    Papers:\n\n        https://arxiv.org/abs/1602.01783\n        https://arxiv.org/abs/1611.05397\n    """"""\n\n    def __init__(self,\n                 ob_space,\n                 ac_space,\n                 rp_sequence_size,\n                 lstm_class=rnn.BasicLSTMCell,\n                 lstm_layers=(256,),\n                 action_dp_alpha=200.0,\n                 aux_estimate=False,\n                 **kwargs):\n        """"""\n        Defines [partially shared] on/off-policy networks for estimating  action-logits, value function,\n        reward and state \'pixel_change\' predictions.\n        Expects multi-modal observation as array of shape `ob_space`.\n\n        Args:\n            ob_space:           instance of btgym.spaces.DictSpace\n            ac_space:           instance of btgym.spaces.ActionDictSpace\n            rp_sequence_size:   reward prediction sample length\n            lstm_class:         tf.nn.lstm class\n            lstm_layers:        tuple of LSTM layers sizes\n            aux_estimate:       bool, if True - add auxiliary tasks estimations to self.callbacks dictionary\n            time_flat:          bool, if True - use static rnn, dynamic otherwise\n            **kwargs            not used\n        """"""\n        assert isinstance(ob_space, DictSpace), \\\n            \'Expected observation space be instance of btgym.spaces.DictSpace, got: {}\'.format(ob_space)\n        self.ob_space = ob_space\n\n        assert isinstance(ac_space, ActionDictSpace), \\\n            \'Expected action space be instance of btgym.spaces.ActionDictSpace, got: {}\'.format(ac_space)\n\n        assert ac_space.base_space == Discrete, \\\n            \'Base policy restricted to gym.spaces.Discrete base action spaces, got: {}\'.format(ac_space.base_space)\n\n        self.ac_space = ac_space\n        self.rp_sequence_size = rp_sequence_size\n        self.lstm_class = lstm_class\n        self.lstm_layers = lstm_layers\n        self.action_dp_alpha = action_dp_alpha\n        self.aux_estimate = aux_estimate\n        self.callback = {}\n\n        # Placeholders for obs. state input:\n        self.on_state_in = nested_placeholders(self.ob_space.shape, batch_dim=None, name=\'on_policy_state_in\')\n        self.off_state_in = nested_placeholders(self.ob_space.shape, batch_dim=None, name=\'off_policy_state_in_pl\')\n        self.rp_state_in = nested_placeholders(self.ob_space.shape, batch_dim=None, name=\'rp_state_in\')\n\n        # Placeholders for previous step action[multi-categorical vector encoding]  and reward [scalar]:\n        self.on_last_a_in = tf.placeholder(\n            tf.float32,\n            [None, self.ac_space.encoded_depth],\n            name=\'on_policy_last_action_in_pl\'\n        )\n        self.on_last_reward_in = tf.placeholder(tf.float32, [None], name=\'on_policy_last_reward_in_pl\')\n\n        self.off_last_a_in = tf.placeholder(\n            tf.float32,\n            [None, self.ac_space.encoded_depth],\n            name=\'off_policy_last_action_in_pl\'\n        )\n        self.off_last_reward_in = tf.placeholder(tf.float32, [None], name=\'off_policy_last_reward_in_pl\')\n\n        # Placeholders for rnn batch and time-step dimensions:\n        self.on_batch_size = tf.placeholder(tf.int32, name=\'on_policy_batch_size\')\n        self.on_time_length = tf.placeholder(tf.int32, name=\'on_policy_sequence_size\')\n\n        self.off_batch_size = tf.placeholder(tf.int32, name=\'off_policy_batch_size\')\n        self.off_time_length = tf.placeholder(tf.int32, name=\'off_policy_sequence_size\')\n\n        try:\n            if self.train_phase is not None:\n                pass\n\n        except AttributeError:\n            self.train_phase = tf.placeholder_with_default(\n                tf.constant(False, dtype=tf.bool),\n                shape=(),\n                name=\'train_phase_flag_pl\'\n            )\n        # Base on-policy AAC network:\n        # Conv. layers:\n        on_aac_x = conv_2d_network(self.on_state_in[\'external\'], self.ob_space.shape[\'external\'], ac_space, **kwargs)\n\n        # Reshape rnn inputs for  batch training as [rnn_batch_dim, rnn_time_dim, flattened_depth]:\n        x_shape_dynamic = tf.shape(on_aac_x)\n        max_seq_len = tf.cast(x_shape_dynamic[0] / self.on_batch_size, tf.int32)\n        x_shape_static = on_aac_x.get_shape().as_list()\n\n        on_last_action_in = tf.reshape(\n            self.on_last_a_in,\n            [self.on_batch_size, max_seq_len, self.ac_space.encoded_depth]\n        )\n        on_r_in = tf.reshape(self.on_last_reward_in, [self.on_batch_size, max_seq_len, 1])\n\n        on_aac_x = tf.reshape( on_aac_x, [self.on_batch_size, max_seq_len, np.prod(x_shape_static[1:])])\n\n        # print(\'*** POLICY DEBUG ***\')\n        # print(\'self.on_last_a_in :\', self.on_last_a_in)\n        # print(\'on_last_action_in: \', on_last_action_in)\n        # print(\'on_r_in: \', on_r_in)\n        # print(\'on_aac_x: \', on_aac_x)\n\n        # Feed last action, reward [, internal obs. state] into LSTM along with external state features:\n        on_stage2_input = [on_aac_x, on_last_action_in, on_r_in]\n\n        if \'internal\' in list(self.on_state_in.keys()):\n            x_int_shape_static = self.on_state_in[\'internal\'].get_shape().as_list()\n            x_int = tf.reshape(\n                self.on_state_in[\'internal\'],\n                [self.on_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n            )\n            on_stage2_input.append(x_int)\n\n        on_aac_x = tf.concat(on_stage2_input, axis=-1)\n\n        # print(\'on_stage2_input->on_aac_x: \', on_aac_x)\n\n        # LSTM layer takes conv. features and concatenated last action_reward tensor:\n        [on_x_lstm_out, self.on_lstm_init_state, self.on_lstm_state_out, self.on_lstm_state_pl_flatten] =\\\n            lstm_network(\n                x=on_aac_x,\n                lstm_sequence_length=self.on_time_length,\n                lstm_class=lstm_class,\n                lstm_layers=lstm_layers,\n            )\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = on_x_lstm_out.get_shape().as_list()\n        on_x_lstm_out = tf.reshape(on_x_lstm_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        # Aac policy and value outputs and action-sampling function:\n        [self.on_logits, self.on_vf, self.on_sample] = dense_aac_network(on_x_lstm_out, self.ac_space.one_hot_depth)\n\n        # Off-policy AAC network (shared):\n        off_aac_x = conv_2d_network(self.off_state_in[\'external\'], self.ob_space.shape[\'external\'], ac_space, reuse=True, **kwargs)\n\n        # Reshape rnn inputs for  batch training as [rnn_batch_dim, rnn_time_dim, flattened_depth]:\n        x_shape_dynamic = tf.shape(off_aac_x)\n        max_seq_len = tf.cast(x_shape_dynamic[0] / self.off_batch_size, tf.int32)\n        x_shape_static = off_aac_x.get_shape().as_list()\n\n        off_action_in = tf.reshape(\n            self.off_last_a_in,\n            [self.off_batch_size, max_seq_len, self.ac_space.encoded_depth]\n        )\n        off_r_in = tf.reshape(self.off_last_reward_in, [self.off_batch_size, max_seq_len, 1])  # reward is scalar\n\n        off_aac_x = tf.reshape( off_aac_x, [self.off_batch_size, max_seq_len, np.prod(x_shape_static[1:])])\n\n        off_stage2_input = [off_aac_x, off_action_in, off_r_in]\n\n        if \'internal\' in list(self.off_state_in.keys()):\n            x_int_shape_static = self.off_state_in[\'internal\'].get_shape().as_list()\n            off_x_int = tf.reshape(\n                self.off_state_in[\'internal\'],\n                [self.off_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n            )\n            off_stage2_input.append(off_x_int)\n\n        off_aac_x = tf.concat(off_stage2_input, axis=-1)\n\n        [off_x_lstm_out, _, _, self.off_lstm_state_pl_flatten] =\\\n            lstm_network(off_aac_x, self.off_time_length, lstm_class, lstm_layers, reuse=True)\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = off_x_lstm_out.get_shape().as_list()\n        off_x_lstm_out = tf.reshape(off_x_lstm_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        # Off policy dense:\n        [self.off_logits, self.off_vf, _] = dense_aac_network(off_x_lstm_out, self.ac_space.one_hot_depth, reuse=True)\n\n        # Aux1: `Pixel control` network:\n        # Define pixels-change estimation function:\n        # Yes, it rather env-specific but for atari case it is handy to do it here, see self.get_pc_target():\n        [self.pc_change_state_in, self.pc_change_last_state_in, self.pc_target] =\\\n            pixel_change_2d_estimator(self.ob_space.shape[\'external\'], **kwargs)\n\n        self.pc_batch_size = self.off_batch_size\n        self.pc_time_length = self.off_time_length\n\n        self.pc_state_in = self.off_state_in\n        #self.pc_a_r_in = self.off_last_action_in\n        self.pc_last_a_in = self.off_last_a_in\n        self.pc_last_reward_in = self.off_last_reward_in\n        self.pc_lstm_state_pl_flatten = self.off_lstm_state_pl_flatten\n\n        # Shared conv and lstm nets, same off-policy batch:\n        pc_x = off_x_lstm_out\n\n        # PC duelling Q-network, outputs [None, 20, 20, ac_size] Q-features tensor:\n        # Restricted to single action space:\n        act_space = self.ac_space.one_hot_depth\n        self.pc_q = duelling_pc_network(pc_x, act_space, **kwargs)\n\n        # Aux2: `Value function replay` network:\n        # VR network is fully shared with ppo network but with `value` only output:\n        # and has same off-policy batch pass with off_ppo network:\n        self.vr_batch_size = self.off_batch_size\n        self.vr_time_length = self.off_time_length\n\n        self.vr_state_in = self.off_state_in\n        #self.vr_a_r_in = self.off_last_action_in\n        self.vr_last_a_in = self.off_last_a_in\n        self.vr_last_reward_in = self.off_last_reward_in\n\n        self.vr_lstm_state_pl_flatten = self.off_lstm_state_pl_flatten\n        self.vr_value = self.off_vf\n\n        # Aux3: `Reward prediction` network:\n        self.rp_batch_size = tf.placeholder(tf.int32, name=\'rp_batch_size\')\n\n        # Shared conv. output:\n        rp_x = conv_2d_network(self.rp_state_in[\'external\'], self.ob_space.shape[\'external\'], ac_space, reuse=True, **kwargs)\n\n        # Flatten batch-wise:\n        rp_x_shape_static = rp_x.get_shape().as_list()\n        rp_x = tf.reshape(rp_x, [self.rp_batch_size, np.prod(rp_x_shape_static[1:]) * (self.rp_sequence_size-1)])\n\n        # RP output:\n        self.rp_logits = dense_rp_network(rp_x)\n\n        # Batch-norm related :\n        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        # Add moving averages to save list:\n        moving_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name + \'.*moving.*\')\n        renorm_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name + \'.*renorm.*\')\n\n        # What to save:\n        self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n        self.var_list += moving_var_list + renorm_var_list\n\n        # Callbacks:\n        if self.aux_estimate:\n            self.callback[\'pixel_change\'] = self.get_pc_target\n\n    def get_initial_features(self, **kwargs):\n        """"""\n        Returns initial context.\n\n        Returns:\n            LSTM zero-state tuple.\n        """"""\n        # TODO: rework as in: AacStackedMetaPolicy --> base runner, verbose runner; synchro_runner ok\n        sess = tf.get_default_session()\n        return sess.run(self.on_lstm_init_state)\n\n    def act(self, observation, lstm_state, last_action, last_reward, deterministic=False):\n        """"""\n        Emits action.\n\n        Args:\n            observation:    dictionary containing single observation\n            lstm_state:     lstm context value\n            last_action:    action value from previous step\n            last_reward:    reward value previous step\n            deterministic:  bool, it True - act deterministically,\n                            use random sampling otherwise (default);\n                            effective for discrete action sapce only (TODO: continious)\n\n        Returns:\n            Action as dictionary of several action encodings, actions logits, V-fn value, output RNN state\n        """"""\n        try:\n            sess = tf.get_default_session()\n            feeder = {pl: value for pl, value in zip(self.on_lstm_state_pl_flatten, flatten_nested(lstm_state))}\n            feeder.update(feed_dict_from_nested(self.on_state_in, observation, expand_batch=True))\n            feeder.update(\n                {\n                    self.on_last_a_in: last_action,\n                    self.on_last_reward_in: last_reward,\n                    self.on_batch_size: 1,\n                    self.on_time_length: 1,\n                    self.train_phase: False\n                }\n            )\n            logits, value, context = sess.run([self.on_logits, self.on_vf, self.on_lstm_state_out], feeder)\n            logits = logits[0, ...]\n            if self.ac_space.is_discrete:\n                if deterministic:\n                    sample = softmax(logits)\n\n                else:\n                    # Use multinomial to get sample (discrete):\n                    sample = np.random.multinomial(1, softmax(logits))\n\n                # print(\'ploicy_determ: {}, logits: {}, sample: {}\'.format(deterministic, logits, sample))\n\n                sample = self.ac_space._cat_to_vec(np.argmax(sample))\n\n                # print(\'policy_sample_vector: \', sample)\n\n            else:\n                # Use DP to get sample (continuous):\n                sample = sample_dp(logits, alpha=self.action_dp_alpha)\n\n            # Get all needed action encodings:\n            action = self.ac_space._vec_to_action(sample)\n            one_hot = self.ac_space._vec_to_one_hot(sample)\n            action_pack = {\n                \'environment\': action,\n                \'encoded\': self.ac_space.encode(action),\n                \'one_hot\': one_hot,\n            }\n            # print(\'action_pack: \', action_pack)\n        except Exception as e:\n            print(e)\n            raise e\n\n        return action_pack, logits, value, context\n\n    def get_value(self, observation, lstm_state, last_action, last_reward):\n        """"""\n        Estimates policy V-function.\n\n        Args:\n            observation:    single observation value\n            lstm_state:     lstm context value\n            last_action:    action value from previous step\n            last_reward:    reward value from previous step\n\n        Returns:\n            V-function value\n        """"""\n        sess = tf.get_default_session()\n        feeder = feed_dict_rnn_context(self.on_lstm_state_pl_flatten, lstm_state)\n        feeder.update(feed_dict_from_nested(self.on_state_in, observation, expand_batch=True))\n        feeder.update(\n            {\n                self.on_last_a_in: last_action,\n                self.on_last_reward_in: last_reward,\n                self.on_batch_size: 1,\n                self.on_time_length: 1,\n                self.train_phase: False\n            }\n        )\n\n        return sess.run(self.on_vf, feeder)[0]\n\n    def get_pc_target(self, state, last_state, **kwargs):\n        """"""\n        Estimates pixel-control task target.\n\n        Args:\n            state:      single observation value\n            last_state: single observation value\n            **kwargs:   not used\n\n        Returns:\n            Estimated absolute difference between two subsampled states.\n        """"""\n        sess = tf.get_default_session()\n        feeder = {self.pc_change_state_in: state[\'external\'], self.pc_change_last_state_in: last_state[\'external\']}\n\n        return sess.run(self.pc_target, feeder)[0,...,0]\n\n    @staticmethod\n    def get_sample_config(*args, **kwargs):\n        """"""\n        Dummy implementation.\n\n        Returns:\n                default data sample configuration dictionary `btgym.datafeed.base.EnvResetConfig`\n        """"""\n        return EnvResetConfig\n\n\nclass Aac1dPolicy(BaseAacPolicy):\n    """"""\n    AAC policy for one-dimensional signal obs. state.\n    """"""\n\n    def __init__(self,\n                 ob_space,\n                 ac_space,\n                 rp_sequence_size,\n                 lstm_class=rnn.BasicLSTMCell,\n                 lstm_layers=(256,),\n                 action_dp_alpha=200.0,\n                 aux_estimate=True,\n                 **kwargs):\n        """"""\n        Defines [partially shared] on/off-policy networks for estimating  action-logits, value function,\n        reward and state \'pixel_change\' predictions.\n        Expects bi-modal observation as dict: `external`, `internal`.\n\n        Args:\n            ob_space:           dictionary of observation state shapes\n            ac_space:           discrete action space shape (length)\n            rp_sequence_size:   reward prediction sample length\n            lstm_class:         tf.nn.lstm class\n            lstm_layers:        tuple of LSTM layers sizes\n            aux_estimate:       (bool), if True - add auxiliary tasks estimations to self.callbacks dictionary.\n            **kwargs            not used\n        """"""\n        kwargs.update(\n            dict(\n                conv_2d_filter_size=[3, 1],\n                conv_2d_stride=[2, 1],\n                pc_estimator_stride=[2, 1],\n                duell_pc_x_inner_shape=(6, 1, 32),  # [6,3,32] if swapping W-C dims\n                duell_pc_filter_size=(4, 1),\n                duell_pc_stride=(2, 1),\n            )\n        )\n        super(Aac1dPolicy, self).__init__(\n            ob_space,\n            ac_space,\n            rp_sequence_size,\n            lstm_class,\n            lstm_layers,\n            action_dp_alpha,\n            aux_estimate,\n            **kwargs\n        )\n\n'"
btgym/algorithms/policy/meta.py,34,"b'\nfrom btgym.algorithms.utils import *\n\nfrom btgym.research.casual_conv.policy import CasualConvPolicy_0_0\n\n\nclass MetaSubPolicy:\n    """"""\n    Stateful meta-optimisation policy to be hosted by of behavioral policy.\n    Performs hyper-parameter and other type of meta-optimisation across different instances of hosting policy.\n    """"""\n\n    def __init__(self, task, num_host_policies, learn_rate, name=\'SubMetaPolicy\'):\n        """"""\n\n        Args:\n            task:                   int, host policy task, in [0, num_host_policies].\n            num_host_policies:      total number of behavioral host policies instances to optimise across.\n            learn_rate:             meta-policy learning rate\n            name:                   name scope\n        """"""\n        with tf.variable_scope(name_or_scope=name):\n            self.task = task\n            self.learn_rate = learn_rate\n            self.num_host_policies = num_host_policies\n\n            self.input_stat_pl = tf.placeholder(dtype=tf.float32, name=\'in_stat_pl\')\n\n            self.input_stat = tf.reduce_mean(self.input_stat_pl)\n\n            self.initial_cluster_value = tf.concat(\n                [\n                    tf.zeros(shape=[1, self.num_host_policies]),\n                    tf.zeros(shape=[1, self.num_host_policies]),\n                ],\n                axis=0,\n                name=\'initial_cluster_value\'\n            )\n\n            self.cluster_averages_slot = tf.Variable(\n                initial_value=self.initial_cluster_value,\n                trainable=False,\n                name=\'cluster_wide_averages_slot\'\n            )\n\n            update_task_iteration = tf.scatter_nd_add(self.cluster_averages_slot, [[0, task]], [1])\n\n            with tf.control_dependencies([update_task_iteration]):\n                avg_prev = self.cluster_averages_slot[1, task]\n                k = self.cluster_averages_slot[0, task]\n                avg = avg_prev + (self.input_stat - avg_prev) / k\n                self.update_op = tf.scatter_nd_update(self.cluster_averages_slot, [[1, task]], [avg])\n\n            self.reset_op = tf.assign(\n                self.cluster_averages_slot,\n                self.initial_cluster_value\n            )\n\n            # Toy network:\n            prob = tf.layers.dense(\n                tf.expand_dims(self.cluster_averages_slot[1, :], axis=-1),\n                units=10,\n                activation=tf.nn.sigmoid,\n                use_bias=False,\n            )\n            self.next_step_prob = tf.layers.dense(\n                prob,\n                units=1,\n                activation=tf.nn.sigmoid,\n                use_bias=False,\n            )\n            self.distribution = tf.distributions.Bernoulli(\n                probs=tf.reduce_max(self.next_step_prob)\n            )\n            self.sample = self.distribution.sample()\n\n            self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n\n            self.cluster_stat = tf.clip_by_value(\n                # tf.reduce_mean(\n                #     self.cluster_averages_slot\n                # ),\n                tf.expand_dims(self.cluster_averages_slot[1, :], axis=-1),\n                -40,\n                40\n            )\n            bound_avg = tf.sigmoid(- self.cluster_stat)\n            self.loss = tf.reduce_mean(\n                bound_avg * (1 - self.next_step_prob) + (1 - bound_avg) * self.next_step_prob\n            )\n            self.grads = tf.gradients(self.loss, self.var_list)\n\n            self.summaries = [\n                tf.summary.scalar(\'worker_avg_stat\', self.cluster_averages_slot[1, task]),\n                tf.summary.scalar(\'worker_iterations\', self.cluster_averages_slot[0, task]),\n                #tf.summary.histogram(\'clipped_cluster_stat\', self.cluster_stat),\n                tf.summary.scalar(\'loss\', self.loss),\n                tf.summary.histogram(\'next_step_prob\', self.next_step_prob),\n                tf.summary.scalar(\'grads_norm\', tf.global_norm(self.grads))\n            ]\n\n    def update(self, input_stat):\n        sess = tf.get_default_session()\n        feed_dict = {self.input_stat_pl: input_stat}\n        sess.run(self.update_op, feed_dict)\n\n    def reset(self):\n        sess = tf.get_default_session()\n        sess.run(self.reset_op)\n\n    def global_reset(self):\n        raise NotImplementedError\n\n    def act(self):\n        """"""\n        Do another step?\n\n        Args:\n            sess:\n            iteration:\n\n        Returns:\n\n        """"""\n        sess = tf.get_default_session()\n        fetched = sess.run([self.sample])\n\n        return fetched[-1]\n\n\nclass CasualMetaPolicy(CasualConvPolicy_0_0):\n\n    def __init__(self, task=None,  cluster_spec=None, **kwargs):\n        super(CasualMetaPolicy, self).__init__(**kwargs)\n        if task is not None and cluster_spec is not None:\n            self.meta = MetaSubPolicy(\n                task=task,\n                num_host_policies=len(cluster_spec[\'worker\']),\n                learn_rate=1e-3\n            )\n\n            self.var_list += self.meta.var_list\n\n\n\n        else:\n            print(\'Task ID or cluster_spec not provided, no meta-policy enabled.\')\n\n\n\n\n\n\n'"
btgym/algorithms/policy/stacked_lstm.py,64,"b'from tensorflow.contrib.layers import flatten as batch_flatten\n\nfrom btgym.algorithms.policy.base import BaseAacPolicy\nfrom btgym.algorithms.nn.networks import *\nfrom btgym.algorithms.utils import *\n\nfrom btgym.spaces import DictSpace, ActionDictSpace\n\n\nclass StackedLstmPolicy(BaseAacPolicy):\n    """"""\n    Conv.-Stacked_LSTM policy, based on `NAV A3C agent` architecture from\n\n    `LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS` by Mirowski et all. and\n\n    `LEARNING TO REINFORCEMENT LEARN` by JX Wang et all.\n\n    Papers:\n\n    https://arxiv.org/pdf/1611.03673.pdf\n\n    https://arxiv.org/pdf/1611.05763.pdf\n    """"""\n\n    def __init__(self,\n                 ob_space,\n                 ac_space,\n                 rp_sequence_size,\n                 state_encoder_class_ref=conv_2d_network,\n                 lstm_class_ref=tf.contrib.rnn.LayerNormBasicLSTMCell,\n                 lstm_layers=(256, 256),\n                 linear_layer_ref=noisy_linear,\n                 share_encoder_params=False,\n                 dropout_keep_prob=1.0,\n                 action_dp_alpha=200.0,\n                 aux_estimate=False,\n                 encode_internal_state=None,\n                 static_rnn=True,\n                 shared_p_v=False,\n                 **kwargs):\n        """"""\n        Defines [partially shared] on/off-policy networks for estimating  action-logits, value function,\n        reward and state \'pixel_change\' predictions.\n        Expects multi-modal observation as array of shape `ob_space`.\n\n        Args:\n            ob_space:               instance of btgym.spaces.DictSpace\n            ac_space:               instance of btgym.spaces.ActionDictSpace\n            rp_sequence_size:       reward prediction sample length\n            lstm_class_ref:         tf.nn.lstm class to use\n            lstm_layers:            tuple of LSTM layers sizes\n            linear_layer_ref:       linear layer class to use\n            share_encoder_params:   bool, whether to share encoder parameters for every \'external\' data stream\n            dropout_keep_prob:      in (0, 1] dropout regularisation parameter\n            action_dp_alpha:\n            aux_estimate:           (bool), if True - add auxiliary tasks estimations to self.callbacks dictionary\n            encode_internal_state:  legacy, not used\n            static_rnn:             (bool), it True - use static rnn graph, dynamic otherwise\n            **kwargs                not used\n        """"""\n\n        assert isinstance(ob_space, DictSpace), \\\n            \'Expected observation space be instance of btgym.spaces.DictSpace, got: {}\'.format(ob_space)\n        self.ob_space = ob_space\n\n        assert isinstance(ac_space, ActionDictSpace), \\\n            \'Expected action space be instance of btgym.spaces.ActionDictSpace, got: {}\'.format(ac_space)\n\n        self.ac_space = ac_space\n        self.rp_sequence_size = rp_sequence_size\n        self.state_encoder_class_ref = state_encoder_class_ref\n        self.lstm_class = lstm_class_ref\n        self.lstm_layers = lstm_layers\n        self.action_dp_alpha = action_dp_alpha\n        self.aux_estimate = aux_estimate\n        self.callback = {}\n        # self.encode_internal_state = encode_internal_state\n        self.share_encoder_params = share_encoder_params\n        if self.share_encoder_params:\n            self.reuse_encoder_params = tf.AUTO_REUSE\n\n        else:\n            self.reuse_encoder_params = False\n        self.static_rnn = static_rnn\n        self.dropout_keep_prob = dropout_keep_prob\n        assert 0 < self.dropout_keep_prob <= 1, \'Dropout keep_prob value should be in (0, 1]\'\n\n        self.debug = {}\n\n        # Placeholders for obs. state input:\n        self.on_state_in = nested_placeholders(self.ob_space.shape, batch_dim=None, name=\'on_policy_state_in\')\n        self.off_state_in = nested_placeholders(self.ob_space.shape, batch_dim=None, name=\'off_policy_state_in_pl\')\n        self.rp_state_in = nested_placeholders(self.ob_space.shape, batch_dim=None, name=\'rp_state_in\')\n\n        # Placeholders for previous step action[multi-categorical vector encoding]  and reward [scalar]:\n        self.on_last_a_in = tf.placeholder(\n            tf.float32,\n            [None, self.ac_space.encoded_depth],\n            name=\'on_policy_last_action_in_pl\'\n        )\n        self.on_last_reward_in = tf.placeholder(tf.float32, [None], name=\'on_policy_last_reward_in_pl\')\n\n        self.off_last_a_in = tf.placeholder(\n            tf.float32,\n            [None, self.ac_space.encoded_depth],\n            name=\'off_policy_last_action_in_pl\'\n        )\n        self.off_last_reward_in = tf.placeholder(tf.float32, [None], name=\'off_policy_last_reward_in_pl\')\n\n        # Placeholders for rnn batch and time-step dimensions:\n        self.on_batch_size = tf.placeholder(tf.int32, name=\'on_policy_batch_size\')\n        self.on_time_length = tf.placeholder(tf.int32, name=\'on_policy_sequence_size\')\n\n        self.off_batch_size = tf.placeholder(tf.int32, name=\'off_policy_batch_size\')\n        self.off_time_length = tf.placeholder(tf.int32, name=\'off_policy_sequence_size\')\n\n        self.debug[\'on_state_in_keys\'] = list(self.on_state_in.keys())\n\n        # Dropout related:\n        try:\n            if self.train_phase is not None:\n                pass\n\n        except AttributeError:\n            self.train_phase = tf.placeholder_with_default(\n                tf.constant(False, dtype=tf.bool),\n                shape=(),\n                name=\'train_phase_flag_pl\'\n            )\n        self.keep_prob = 1.0 - (1.0 - self.dropout_keep_prob) * tf.cast(self.train_phase, tf.float32)\n\n        # Default parameters:\n        default_kwargs = dict(\n            conv_2d_filter_size=[3, 1],\n            conv_2d_stride=[2, 1],\n            conv_2d_num_filters=[32, 32, 64, 64],\n            pc_estimator_stride=[2, 1],\n            duell_pc_x_inner_shape=(6, 1, 32),  # [6,3,32] if swapping W-C dims\n            duell_pc_filter_size=(4, 1),\n            duell_pc_stride=(2, 1),\n            keep_prob=self.keep_prob,\n        )\n        # Insert if not already:\n        for key, default_value in default_kwargs.items():\n            if key not in kwargs.keys():\n                kwargs[key] = default_value\n\n        # Base on-policy AAC network:\n        self.modes_to_encode = [\'external\', \'internal\']\n        for mode in self.modes_to_encode:\n            assert mode in self.on_state_in.keys(), \\\n                \'Required top-level mode `{}` not found in state shape specification\'.format(mode)\n\n        # Separately encode than concatenate all `external` and \'internal\' states modes,\n        # [jointly] encode every stream within mode:\n        self.on_aac_x_encoded = {}\n        for key in self.modes_to_encode:\n            if isinstance(self.on_state_in[key], dict):  # got dictionary of data streams\n                if self.share_encoder_params:\n                    layer_name_template = \'encoded_{}_shared\'\n                else:\n                    layer_name_template = \'encoded_{}_{}\'\n                encoded_streams = {\n                    name: tf.layers.flatten(\n                        self.state_encoder_class_ref(\n                            x=stream,\n                            ob_space=self.ob_space.shape[key][name],\n                            ac_space=self.ac_space,\n                            name=layer_name_template.format(key, name),\n                            reuse=self.reuse_encoder_params,  # shared params for all streams in mode\n                            **kwargs\n                        )\n                    )\n                    for name, stream in self.on_state_in[key].items()\n                }\n                encoded_mode = tf.concat(\n                    list(encoded_streams.values()),\n                    axis=-1,\n                    name=\'multi_encoded_{}\'.format(key)\n                )\n            else:\n                # Got single data stream:\n                encoded_mode = tf.layers.flatten(\n                    self.state_encoder_class_ref(\n                        x=self.on_state_in[key],\n                        ob_space=self.ob_space.shape[key],\n                        ac_space=self.ac_space,\n                        name=\'encoded_{}\'.format(key),\n                        **kwargs\n                    )\n                )\n            self.on_aac_x_encoded[key] = encoded_mode\n\n        self.debug[\'on_state_external_encoded_dict\'] = self.on_aac_x_encoded\n\n        # on_aac_x = tf.concat(list(self.on_aac_x_encoded.values()), axis=-1, name=\'on_state_external_encoded\')\n        on_aac_x = self.on_aac_x_encoded[\'external\']\n\n        self.debug[\'on_state_external_encoded\'] = on_aac_x\n\n        # TODO: for encoder prediction test, output `naive` estimates for logits and value directly from encoder:\n        [self.on_simple_logits, self.on_simple_value, _] = dense_aac_network(\n            tf.layers.flatten(on_aac_x),\n            ac_space_depth=self.ac_space.one_hot_depth,\n            linear_layer_ref=linear_layer_ref,\n            name=\'aac_dense_simple_pi_v\'\n        )\n\n        # Reshape rnn inputs for batch training as: [rnn_batch_dim, rnn_time_dim, flattened_depth]:\n        x_shape_dynamic = tf.shape(on_aac_x)\n        max_seq_len = tf.cast(x_shape_dynamic[0] / self.on_batch_size, tf.int32)\n        x_shape_static = on_aac_x.get_shape().as_list()\n\n        on_last_action_in = tf.reshape(\n            self.on_last_a_in,\n            [self.on_batch_size, max_seq_len, self.ac_space.encoded_depth]\n        )\n        on_last_r_in = tf.reshape(self.on_last_reward_in, [self.on_batch_size, max_seq_len, 1])\n\n        on_aac_x = tf.reshape(on_aac_x, [self.on_batch_size, max_seq_len, np.prod(x_shape_static[1:])])\n\n        # # Prepare `internal` state, if any:\n        # if \'internal\' in list(self.on_state_in.keys()):\n        #     if self.encode_internal_state:\n        #         # Use convolution encoder:\n        #         on_x_internal = self.state_encoder_class_ref(\n        #             x=self.on_state_in[\'internal\'],\n        #             ob_space=self.ob_space.shape[\'internal\'],\n        #             ac_space=self.ac_space,\n        #             name=\'encoded_internal\',\n        #             **kwargs\n        #         )\n        #         x_int_shape_static = on_x_internal.get_shape().as_list()\n        #         on_x_internal = [\n        #             tf.reshape(on_x_internal, [self.on_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])])]\n        #         self.debug[\'on_state_internal_encoded\'] = on_x_internal\n        #\n        #     else:\n        #         # Feed as is:\n        #         x_int_shape_static = self.on_state_in[\'internal\'].get_shape().as_list()\n        #         on_x_internal = tf.reshape(\n        #             self.on_state_in[\'internal\'],\n        #             [self.on_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n        #         )\n        #         self.debug[\'on_state_internal_encoded\'] = on_x_internal\n        #         on_x_internal = [on_x_internal]\n        #\n        # else:\n        #     on_x_internal = []\n\n        on_x_internal = self.on_aac_x_encoded[\'internal\']\n\n        # Reshape to batch-feed rnn:\n        x_int_shape_static = on_x_internal.get_shape().as_list()\n        on_x_internal = tf.reshape(\n            on_x_internal,\n            [self.on_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n        )\n        self.debug[\'on_state_internal_encoded\'] = on_x_internal\n\n        on_x_internal = [on_x_internal]\n\n        # Prepare datetime index if any:\n        if \'datetime\' in list(self.on_state_in.keys()):\n            x_dt_shape_static = self.on_state_in[\'datetime\'].get_shape().as_list()\n            on_x_dt = tf.reshape(\n                self.on_state_in[\'datetime\'],\n                [self.on_batch_size, max_seq_len, np.prod(x_dt_shape_static[1:])]\n            )\n            on_x_dt = [on_x_dt]\n\n        else:\n            on_x_dt = []\n\n        self.debug[\'on_state_dt_encoded\'] = on_x_dt\n        self.debug[\'conv_input_to_lstm1\'] = on_aac_x\n\n        # Feed last last_reward into LSTM_1 layer along with encoded `external` state features and datetime stamp:\n        # on_stage2_1_input = [on_aac_x, on_last_action_in, on_last_reward_in] + on_x_dt\n        on_stage2_1_input = [on_aac_x, on_last_r_in] #+ on_x_dt\n\n        # Feed last_action, encoded `external` state,  `internal` state, datetime stamp into LSTM_2:\n        # on_stage2_2_input = [on_aac_x, on_last_action_in, on_last_reward_in] + on_x_internal + on_x_dt\n        on_stage2_2_input = [on_aac_x, on_last_action_in] + on_x_internal #+ on_x_dt\n\n        # LSTM_1 full input:\n        on_aac_x = tf.concat(on_stage2_1_input, axis=-1)\n\n        self.debug[\'concat_input_to_lstm1\'] = on_aac_x\n\n        # First LSTM layer takes encoded `external` state:\n        [on_x_lstm_1_out, self.on_lstm_1_init_state, self.on_lstm_1_state_out, self.on_lstm_1_state_pl_flatten] =\\\n            lstm_network(\n                x=on_aac_x,\n                lstm_sequence_length=self.on_time_length,\n                lstm_class=lstm_class_ref,\n                lstm_layers=(lstm_layers[0],),\n                static=static_rnn,\n                name=\'lstm_1\',\n                **kwargs,\n            )\n\n        # var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n        # print(\'var_list: \', var_list)\n\n        self.debug[\'on_x_lstm_1_out\'] = on_x_lstm_1_out\n        self.debug[\'self.on_lstm_1_state_out\'] = self.on_lstm_1_state_out\n        self.debug[\'self.on_lstm_1_state_pl_flatten\'] = self.on_lstm_1_state_pl_flatten\n\n        # For time_flat only: Reshape on_lstm_1_state_out from [1,2,20,size] -->[20,1,2,size] --> [20,1, 2xsize]:\n        reshape_lstm_1_state_out = tf.transpose(self.on_lstm_1_state_out, [2, 0, 1, 3])\n        reshape_lstm_1_state_out_shape_static = reshape_lstm_1_state_out.get_shape().as_list()\n\n        # Take policy logits off first LSTM-dense layer:\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = on_x_lstm_1_out.get_shape().as_list()\n        rsh_on_x_lstm_1_out = tf.reshape(on_x_lstm_1_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        self.debug[\'reshaped_on_x_lstm_1_out\'] = rsh_on_x_lstm_1_out\n\n        if not shared_p_v:\n            # Aac policy output and action-sampling function:\n            [self.on_logits, _, self.on_sample] = dense_aac_network(\n                rsh_on_x_lstm_1_out,\n                ac_space_depth=self.ac_space.one_hot_depth,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_pi\'\n            )\n\n        # Second LSTM layer takes concatenated encoded \'external\' state, LSTM_1 output,\n        # last_action and `internal_state` (if present) tensors:\n        on_stage2_2_input += [on_x_lstm_1_out]\n\n        # Try: feed context instead of output\n        #on_stage2_2_input = [reshape_lstm_1_state_out] + on_stage2_1_input\n\n        # LSTM_2 full input:\n        on_aac_x = tf.concat(on_stage2_2_input, axis=-1)\n\n        self.debug[\'on_stage2_2_input\'] = on_aac_x\n\n        [on_x_lstm_2_out, self.on_lstm_2_init_state, self.on_lstm_2_state_out, self.on_lstm_2_state_pl_flatten] = \\\n            lstm_network(\n                x=on_aac_x,\n                lstm_sequence_length=self.on_time_length,\n                lstm_class=lstm_class_ref,\n                lstm_layers=(lstm_layers[-1],),\n                static=static_rnn,\n                name=\'lstm_2\',\n                **kwargs,\n            )\n\n        self.debug[\'on_x_lstm_2_out\'] = on_x_lstm_2_out\n        self.debug[\'self.on_lstm_2_state_out\'] = self.on_lstm_2_state_out\n        self.debug[\'self.on_lstm_2_state_pl_flatten\'] = self.on_lstm_2_state_pl_flatten\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = on_x_lstm_2_out.get_shape().as_list()\n        rsh_on_x_lstm_2_out = tf.reshape(on_x_lstm_2_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        self.debug[\'reshaped_on_x_lstm_2_out\'] = rsh_on_x_lstm_2_out\n\n        if shared_p_v:\n            [self.on_logits, self.on_vf, self.on_sample] = dense_aac_network(\n                rsh_on_x_lstm_2_out,\n                ac_space_depth=self.ac_space.one_hot_depth,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_pi_vfn\'\n            )\n\n        else:\n            # Aac value function:\n            [_, self.on_vf, _] = dense_aac_network(\n                rsh_on_x_lstm_2_out,\n                ac_space_depth=self.ac_space.one_hot_depth,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_vfn\'\n            )\n\n        # Concatenate LSTM placeholders, init. states and context:\n        self.on_lstm_init_state = (self.on_lstm_1_init_state, self.on_lstm_2_init_state)\n        self.on_lstm_state_out = (self.on_lstm_1_state_out, self.on_lstm_2_state_out)\n        self.on_lstm_state_pl_flatten = self.on_lstm_1_state_pl_flatten + self.on_lstm_2_state_pl_flatten\n\n        self.off_aac_x_encoded = {}\n        for key in self.modes_to_encode:\n            if isinstance(self.off_state_in[key], dict):  # got dictionary of data streams\n                if self.share_encoder_params:\n                    layer_name_template = \'encoded_{}_shared\'\n                else:\n                    layer_name_template = \'encoded_{}_{}\'\n                encoded_streams = {\n                    name: tf.layers.flatten(\n                        self.state_encoder_class_ref(\n                            x=stream,\n                            ob_space=self.ob_space.shape[key][name],\n                            ac_space=self.ac_space,\n                            name=layer_name_template.format(key, name),\n                            reuse=True,  # shared params for all streams in mode\n                            **kwargs\n                        )\n                    )\n                    for name, stream in self.off_state_in[key].items()\n                }\n                encoded_mode = tf.concat(\n                    list(encoded_streams.values()),\n                    axis=-1,\n                    name=\'multi_encoded_{}\'.format(key)\n                )\n            else:\n                # Got single data stream:\n                encoded_mode = tf.layers.flatten(\n                    self.state_encoder_class_ref(\n                        x=self.off_state_in[key],\n                        ob_space=self.ob_space.shape[key],\n                        ac_space=self.ac_space,\n                        name=\'encoded_{}\'.format(key),\n                        reuse=True,\n                        **kwargs\n                    )\n                )\n            self.off_aac_x_encoded[key] = encoded_mode\n\n        # off_aac_x = tf.concat(list(self.off_aac_x_encoded.values()), axis=-1, name=\'off_state_external_encoded\')\n\n        off_aac_x = self.off_aac_x_encoded[\'external\']\n\n        # Reshape rnn inputs for  batch training as [rnn_batch_dim, rnn_time_dim, flattened_depth]:\n        x_shape_dynamic = tf.shape(off_aac_x)\n        max_seq_len = tf.cast(x_shape_dynamic[0] / self.off_batch_size, tf.int32)\n        x_shape_static = off_aac_x.get_shape().as_list()\n\n        off_last_action_in = tf.reshape(\n            self.off_last_a_in,\n            [self.off_batch_size, max_seq_len, self.ac_space.encoded_depth]\n        )\n        off_last_r_in = tf.reshape(self.off_last_reward_in, [self.off_batch_size, max_seq_len, 1])\n\n        off_aac_x = tf.reshape( off_aac_x, [self.off_batch_size, max_seq_len, np.prod(x_shape_static[1:])])\n\n        # # Prepare `internal` state, if any:\n        # if \'internal\' in list(self.off_state_in.keys()):\n        #     if self.encode_internal_state:\n        #         # Use convolution encoder:\n        #         off_x_internal = self.state_encoder_class_ref(\n        #             x=self.off_state_in[\'internal\'],\n        #             ob_space=self.ob_space.shape[\'internal\'],\n        #             ac_space=self.ac_space,\n        #             name=\'encoded_internal\',\n        #             reuse=True,\n        #             **kwargs\n        #         )\n        #         x_int_shape_static = off_x_internal.get_shape().as_list()\n        #         off_x_internal = [\n        #             tf.reshape(off_x_internal, [self.off_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])])\n        #         ]\n        #     else:\n        #         x_int_shape_static = self.off_state_in[\'internal\'].get_shape().as_list()\n        #         off_x_internal = tf.reshape(\n        #             self.off_state_in[\'internal\'],\n        #             [self.off_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n        #         )\n        #         off_x_internal = [off_x_internal]\n        #\n        # else:\n        #     off_x_internal = []\n\n        off_x_internal = self.off_aac_x_encoded[\'internal\']\n\n        x_int_shape_static = off_x_internal.get_shape().as_list()\n\n        # Properly feed LSTM2:\n        off_x_internal = tf.reshape(\n            off_x_internal,\n            [self.off_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n        )\n        off_x_internal = [off_x_internal]\n\n        if \'datetime\' in list(self.off_state_in.keys()):\n            x_dt_shape_static = self.off_state_in[\'datetime\'].get_shape().as_list()\n            off_x_dt = tf.reshape(\n                self.off_state_in[\'datetime\'],\n                [self.off_batch_size, max_seq_len, np.prod(x_dt_shape_static[1:])]\n            )\n            off_x_dt = [off_x_dt]\n\n        else:\n            off_x_dt = []\n\n        # off_stage2_1_input = [off_aac_x,  off_last_action_in, off_last_reward_in] + off_x_dt\n        off_stage2_1_input = [off_aac_x, off_last_r_in]  # + off_x_dt\n\n        # off_stage2_2_input = [off_aac_x,  off_last_action_in, off_last_reward_in] + off_x_internal + off_x_dt\n        off_stage2_2_input = [off_aac_x,  off_last_action_in] + off_x_internal  # + off_x_dt\n\n        off_aac_x = tf.concat(off_stage2_1_input, axis=-1)\n\n        [off_x_lstm_1_out, _, _, self.off_lstm_1_state_pl_flatten] =\\\n            lstm_network(\n                off_aac_x,\n                self.off_time_length,\n                lstm_class_ref,\n                (lstm_layers[0],),\n                name=\'lstm_1\',\n                static=static_rnn,\n                reuse=True,\n                **kwargs,\n            )\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = off_x_lstm_1_out.get_shape().as_list()\n        rsh_off_x_lstm_1_out = tf.reshape(off_x_lstm_1_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        if not shared_p_v:\n            [self.off_logits, _, _] =\\\n                dense_aac_network(\n                    rsh_off_x_lstm_1_out,\n                    ac_space_depth=self.ac_space.one_hot_depth,\n                    linear_layer_ref=linear_layer_ref,\n                    name=\'aac_dense_pi\',\n                    reuse=True\n                )\n\n        off_stage2_2_input += [off_x_lstm_1_out]\n\n        # LSTM_2 full input:\n        off_aac_x = tf.concat(off_stage2_2_input, axis=-1)\n\n        [off_x_lstm_2_out, _, _, self.off_lstm_2_state_pl_flatten] = \\\n            lstm_network(\n                off_aac_x,\n                self.off_time_length,\n                lstm_class_ref,\n                (lstm_layers[-1],),\n                name=\'lstm_2\',\n                static=static_rnn,\n                reuse=True,\n                **kwargs,\n            )\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = off_x_lstm_2_out.get_shape().as_list()\n        rsh_off_x_lstm_2_out = tf.reshape(off_x_lstm_2_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        if shared_p_v:\n            [self.off_logits, self.off_vf, _] = dense_aac_network(\n                rsh_off_x_lstm_2_out,\n                ac_space_depth=self.ac_space.one_hot_depth,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_pi_vfn\',\n                reuse=True\n            )\n        else:\n            # Aac value function:\n            [_, self.off_vf, _] = dense_aac_network(\n                rsh_off_x_lstm_2_out,\n                ac_space_depth=self.ac_space.one_hot_depth,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_vfn\',\n                reuse=True\n            )\n\n        # Concatenate LSTM states:\n        self.off_lstm_state_pl_flatten = self.off_lstm_1_state_pl_flatten + self.off_lstm_2_state_pl_flatten\n\n        if False:  # TEMP DISABLE\n            # Aux1:\n            # `Pixel control` network.\n            #\n            # Define pixels-change estimation function:\n            # Yes, it rather env-specific but for atari case it is handy to do it here, see self.get_pc_target():\n            [self.pc_change_state_in, self.pc_change_last_state_in, self.pc_target] =\\\n                pixel_change_2d_estimator(ob_space[\'external\'], **kwargs)\n\n            self.pc_batch_size = self.off_batch_size\n            self.pc_time_length = self.off_time_length\n\n            self.pc_state_in = self.off_state_in\n            self.pc_a_r_in = self.off_a_r_in\n            self.pc_lstm_state_pl_flatten = self.off_lstm_state_pl_flatten\n\n            # Shared conv and lstm nets, same off-policy batch:\n            pc_x = rsh_off_x_lstm_2_out\n\n            # PC duelling Q-network, outputs [None, 20, 20, ac_size] Q-features tensor:\n            self.pc_q = duelling_pc_network(pc_x, self.ac_space, linear_layer_ref=linear_layer_ref, **kwargs)\n\n        # Aux2:\n        # `Value function replay` network.\n        #\n        # VR network is fully shared with ppo network but with `value` only output:\n        # and has same off-policy batch pass with off_ppo network:\n        self.vr_batch_size = self.off_batch_size\n        self.vr_time_length = self.off_time_length\n\n        self.vr_state_in = self.off_state_in\n        self.vr_last_a_in = self.off_last_a_in\n        self.vr_last_reward_in = self.off_last_reward_in\n\n        self.vr_lstm_state_pl_flatten = self.off_lstm_state_pl_flatten\n        self.vr_value = self.off_vf\n\n        # Aux3:\n        # `Reward prediction` network.\n        self.rp_batch_size = tf.placeholder(tf.int32, name=\'rp_batch_size\')\n\n        # Shared encoded output:\n        rp_x = {}\n        for key in self.rp_state_in.keys():\n            if \'external\' in key:\n                if isinstance(self.rp_state_in[key], dict):  # got dictionary of data streams\n                    if self.share_encoder_params:\n                        layer_name_template = \'encoded_{}_shared\'\n                    else:\n                        layer_name_template = \'encoded_{}_{}\'\n                    encoded_streams = {\n                        name: tf.layers.flatten(\n                            self.state_encoder_class_ref(\n                                x=stream,\n                                ob_space=self.ob_space.shape[key][name],\n                                ac_space=self.ac_space,\n                                name=layer_name_template.format(key, name),\n                                reuse=True,  # shared params for all streams in mode\n                                **kwargs\n                            )\n                        )\n                        for name, stream in self.rp_state_in[key].items()\n                    }\n                    encoded_mode = tf.concat(\n                        list(encoded_streams.values()),\n                        axis=-1,\n                        name=\'multi_encoded_{}\'.format(key)\n                    )\n                else:\n                    # Got single data stream:\n                    encoded_mode = tf.layers.flatten(\n                        self.state_encoder_class_ref(\n                            x=self.rp_state_in[key],\n                            ob_space=self.ob_space.shape,\n                            ac_space=self.ac_space,\n                            name=\'encoded_{}\'.format(key),\n                            reuse=True,\n                            **kwargs\n                        )\n                    )\n                rp_x[key] = encoded_mode\n\n        rp_x = tf.concat(list(rp_x.values()), axis=-1, name=\'rp_state_external_encoded\')\n\n        # Flatten batch-wise:\n        rp_x_shape_static = rp_x.get_shape().as_list()\n        rp_x = tf.reshape(rp_x, [self.rp_batch_size, np.prod(rp_x_shape_static[1:]) * (self.rp_sequence_size-1)])\n\n        # RP output:\n        self.rp_logits = dense_rp_network(rp_x, linear_layer_ref=linear_layer_ref)\n\n        # Batch-norm related:\n        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        # Add moving averages to save list:\n        moving_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name + \'.*moving.*\')\n        renorm_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name + \'.*renorm.*\')\n\n        # What to save:\n        self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n        self.var_list += moving_var_list + renorm_var_list\n\n        # Callbacks:\n        if self.aux_estimate:\n            pass\n            # TEMP DISABLE: due to computation costs\n            # do not use pixel change aux. task; otherwise enable lines 533 - 553 & 640:\n            if False:\n                self.callback[\'pixel_change\'] = self.get_pc_target\n\n        # print(\'policy_debug_dict:\\n\', self.debug)\n\n\nclass AacStackedRL2Policy(StackedLstmPolicy):\n    """"""\n    Attempt to implement two-level RL^2\n    This policy class in conjunction with DataDomain classes from btgym.datafeed\n    is aimed to implement RL^2 algorithm by Duan et al.\n\n    Paper:\n    `FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING`,\n    https://arxiv.org/pdf/1611.02779.pdf\n\n    The only difference from Base policy is `get_initial_features()` method, which has been changed\n    either to reset RNN context to zero-state or return context from the end of previous episode,\n    depending on episode metadata received or `lstm_2_init_period\' parameter.\n    """"""\n    def __init__(self, lstm_2_init_period=50, **kwargs):\n        super(AacStackedRL2Policy, self).__init__(**kwargs)\n        self.current_trial_num = -1  # always give initial context at first call\n        self.lstm_2_init_period = lstm_2_init_period\n        self.current_ep_num = 0\n\n    def get_initial_features(self, state, context=None):\n        """"""\n        Returns RNN initial context.\n        RNN_1 (lower) context is reset at every call.\n\n        RNN_2 (upper) context is reset:\n            - every `lstm_2_init_period\' episodes;\n            - episode  initial `state` `trial_num` metadata has been changed form last call (new train trial started);\n            - episode metatdata `type` is non-zero (test episode);\n            - no context arg is provided (initial episode of training);\n            - ... else carries context on to new episode;\n\n        Episode metadata are provided by DataTrialIterator, which is shaping Trial data distribution in this case,\n        and delivered through env.strategy as separate key in observation dictionary.\n\n        Args:\n            state:      initial episode state (result of env.reset())\n            context:    last previous episode RNN state (last_context of runner)\n\n        Returns:\n            2_RNN zero-state tuple.\n\n        Raises:\n            KeyError if [`metadata`]:[`trial_num`,`type`] keys not found\n        """"""\n        try:\n            sess = tf.get_default_session()\n            new_context = list(sess.run(self.on_lstm_init_state))\n            if state[\'metadata\'][\'trial_num\'] != self.current_trial_num\\\n                    or context is None\\\n                    or state[\'metadata\'][\'type\']\\\n                    or self.current_ep_num % self.lstm_2_init_period == 0:\n                # Assume new/initial trial or test sample, reset_1, 2 context:\n                pass #print(\'RL^2 policy context 1, 2 reset\')\n\n            else:\n                # Asssume same training trial, keep context_2 same as received:\n                new_context[-1] = context[-1]\n                #print(\'RL^2 policy context 1, reset\')\n            # Back to tuple:\n            new_context = tuple(new_context)\n            # Keep trial number:\n            self.current_trial_num = state[\'metadata\'][\'trial_num\']\n\n        except KeyError:\n            raise KeyError(\n                \'RL^2 policy: expected observation state dict. to have keys [`metadata`]:[`trial_num`,`type`]; got: {}\'.\n                format(state.keys())\n            )\n        self.current_ep_num +=1\n        return new_context\n\n'"
btgym/algorithms/runner/__init__.py,0,b'from .base import BaseEnvRunnerFn\nfrom .threadrunner import RunnerThread'
btgym/algorithms/runner/base.py,1,"b'import numpy as np\n\nfrom btgym.algorithms.rollout import Rollout\nfrom btgym.algorithms.memory import _DummyMemory\n\n\ndef BaseEnvRunnerFn(\n    sess,\n    env,\n    policy,\n    task,\n    rollout_length,\n    summary_writer,\n    episode_summary_freq,\n    env_render_freq,\n    atari_test,\n    ep_summary,\n    memory_config,\n    log,\n    **kwargs\n):\n    """"""\n    Default function defining runtime logic of the thread runner.\n    In brief, it constantly keeps on running\n    the policy, and as long as the rollout exceeds a certain length, the thread\n    runner appends all the collected data to the queue.\n\n    Args:\n        env:                    environment instance\n        policy:                 policy instance\n        task:                   int\n        rollout_length:         int\n        episode_summary_freq:   int\n        env_render_freq:        int\n        atari_test:             bool, Atari or BTGyn\n        ep_summary:             dict of tf.summary op and placeholders\n        memory_config:          replay memory configuration dictionary\n        log:                    logbook logger\n\n    Yelds:\n        collected data as dictionary of on_policy, off_policy rollouts and episode statistics.\n    """"""\n    try:\n        if memory_config is not None:\n            memory = memory_config[\'class_ref\'](**memory_config[\'kwargs\'])\n\n        else:\n            memory = _DummyMemory()\n\n        if not atari_test:\n            # Pass sample config to environment:\n            last_state = env.reset(**policy.get_sample_config())\n\n        else:\n            last_state = env.reset()\n\n        last_context = policy.get_initial_features(state=last_state)\n        length = 0\n        local_episode = 0\n        reward_sum = 0\n        last_action = env.action_space.encode(env.get_initial_action())\n        last_reward = np.asarray(0.0)\n\n        # Summary averages accumulators:\n        total_r = []\n        cpu_time = []\n        final_value = []\n        total_steps = []\n        total_steps_atari = []\n\n        ep_stat = None\n        test_ep_stat = None\n        render_stat = None\n\n        while True:\n            terminal_end = False\n            rollout = Rollout()\n\n            action, _, value_, context = policy.act(\n                last_state,\n                last_context,\n                last_action[None, ...],\n                last_reward[None, ...]\n            )\n            # Make a step:\n            state, reward, terminal, info = env.step(action[\'environment\'])\n\n            # Partially collect first experience of rollout:\n            last_experience = {\n                \'position\': {\'episode\': local_episode, \'step\': length},\n                \'state\': last_state,\n                \'action\': action[\'one_hot\'],\n                \'reward\': reward,\n                \'value\': value_,\n                \'terminal\': terminal,\n                \'context\': last_context,\n                \'last_action\': last_action,\n                \'last_reward\': last_reward,\n            }\n            # Execute user-defined callbacks to policy, if any:\n            for key, callback in policy.callback.items():\n                last_experience[key] = callback(**locals())\n\n            length += 1\n            reward_sum += reward\n            last_state = state\n            last_context = context\n            last_action = action[\'encoded\']\n            last_reward = reward\n\n            for roll_step in range(1, rollout_length):\n                if not terminal:\n                    # Continue adding experiences to rollout:\n                    action, _, value_, context = policy.act(\n                        last_state,\n                        last_context,\n                        last_action[None, ...],\n                        last_reward[None, ...]\n                    )\n\n                    state, reward, terminal, info = env.step(action[\'environment\'])\n\n                    # print(\n                    #     \'RUNNER: one_hot: {}, vec: {}, dict: {}\'.format(\n                    #         action_one_hot,\n                    #         action,\n                    #         env.action_space._vec_to_action(action)\n                    #     )\n                    # )\n\n                    # Partially collect next experience:\n                    experience = {\n                        \'position\': {\'episode\': local_episode, \'step\': length},\n                        \'state\': last_state,\n                        \'action\': action[\'one_hot\'],\n                        \'reward\': reward,\n                        \'value\': value_,\n                        \'terminal\': terminal,\n                        \'context\': last_context,\n                        \'last_action\': last_action,\n                        \'last_reward\': last_reward,\n                        #\'pixel_change\': 0 #policy.get_pc_target(state, last_state),\n                    }\n                    for key, callback in policy.callback.items():\n                        experience[key] = callback(**locals())\n\n                    # Bootstrap to complete and push previous experience:\n                    last_experience[\'r\'] = value_\n                    rollout.add(last_experience)\n                    memory.add(last_experience)\n\n                    # Housekeeping:\n                    length += 1\n                    reward_sum += reward\n                    last_state = state\n                    last_context = context\n                    last_action = action[\'encoded\']\n                    last_reward = reward\n                    last_experience = experience\n\n                if terminal:\n                    # Finished episode within last taken step:\n                    terminal_end = True\n                    # All environment-specific summaries are here due to fact\n                    # only runner allowed to interact with environment:\n                    # Accumulate values for averaging:\n                    total_r += [reward_sum]\n                    total_steps_atari += [length]\n                    if not atari_test:\n                        episode_stat = env.get_stat()  # get episode statistic\n                        last_i = info[-1]  # pull most recent info\n                        cpu_time += [episode_stat[\'runtime\'].total_seconds()]\n                        final_value += [last_i[\'broker_value\']]\n                        total_steps += [episode_stat[\'length\']]\n\n                    # Episode statistics:\n                    try:\n                        # Was it test episode ( `type` in metadata is not zero)?\n                        if not atari_test and state[\'metadata\'][\'type\']:\n                            is_test_episode = True\n\n                        else:\n                            is_test_episode = False\n\n                    except KeyError:\n                        is_test_episode = False\n\n                    if is_test_episode:\n                        test_ep_stat = dict(\n                            total_r=total_r[-1],\n                            final_value=final_value[-1],\n                            steps=total_steps[-1]\n                        )\n                    else:\n                        if local_episode % episode_summary_freq == 0:\n                            if not atari_test:\n                                # BTgym:\n                                ep_stat = dict(\n                                    total_r=np.average(total_r),\n                                    cpu_time=np.average(cpu_time),\n                                    final_value=np.average(final_value),\n                                    steps=np.average(total_steps)\n                                )\n                            else:\n                                # Atari:\n                                ep_stat = dict(\n                                    total_r=np.average(total_r),\n                                    steps=np.average(total_steps_atari)\n                                )\n                            total_r = []\n                            cpu_time = []\n                            final_value = []\n                            total_steps = []\n                            total_steps_atari = []\n\n                    if task == 0 and local_episode % env_render_freq == 0 :\n                        if not atari_test:\n                            # Render environment (chief worker only, and not in atari atari_test mode):\n                            render_stat = {\n                                mode: env.render(mode)[None,:] for mode in env.render_modes\n                            }\n                        else:\n                            # Atari:\n                            render_stat = dict(render_atari=state[\'external\'][None,:] * 255)\n\n                    # New episode:\n                    if not atari_test:\n                        # Pass sample config to environment:\n                        last_state = env.reset(**policy.get_sample_config())\n\n                    else:\n                        last_state = env.reset()\n\n                    last_context = policy.get_initial_features(state=last_state, context=last_context)\n                    length = 0\n                    reward_sum = 0\n                    last_action = env.action_space.encode(env.get_initial_action())\n                    last_reward = np.asarray(0.0)\n\n                    # Increment global and local episode counts:\n                    sess.run(policy.inc_episode)\n                    local_episode += 1\n                    break\n\n            # After rolling `rollout_length` or less (if got `terminal`)\n            # complete final experience of the rollout:\n            if not terminal_end:\n                # Bootstrap:\n                last_experience[\'r\'] = np.asarray(\n                    [policy.get_value(last_state, last_context, last_action[None, ...], last_reward[None, ...])]\n                )\n\n            else:\n                last_experience[\'r\'] = np.asarray([0.0])\n\n            rollout.add(last_experience)\n\n            # Only training rollouts are added to replay memory:\n            try:\n                # Was it test (`type` in metadata is not zero)?\n                if not atari_test and last_experience[\'state\'][\'metadata\'][\'type\']:\n                    is_test = True\n\n                else:\n                    is_test = False\n\n            except KeyError:\n                is_test = False\n\n            if not is_test:\n                memory.add(last_experience)\n\n            #print(\'last_experience {}\'.format(last_experience[\'position\']))\n            #for k, v in last_experience.items():\n            #    try:\n            #        print(k, \'shape: \', v.shape)\n            #    except:\n            #        try:\n            #            print(k, \'type: \', type(v), \'len: \', len(v))\n            #        except:\n            #            print(k, \'type: \', type(v), \'value: \', v)\n\n            #print(\'rollout_step: {}, last_exp/frame_pos: {}\\nr: {}, v: {}, v_next: {}, t: {}\'.\n            #    format(\n            #        length,\n            #        last_experience[\'position\'],\n            #        last_experience[\'reward\'],\n            #        last_experience[\'value\'],\n            #        last_experience[\'value_next\'],\n            #        last_experience[\'terminal\']\n            #    )\n            #)\n            #print(\'rollout size: {}, last r: {}\'.format(len(rollout.position), rollout.r[-1]))\n            #print(\'last value_next: \', last_experience[\'value_next\'], \', rollout flushed.\')\n\n            # Once we have enough experience and memory can be sampled, yield it,\n            # and have the ThreadRunner place it on a queue:\n            if memory.is_full():\n                data = dict(\n                    on_policy=rollout,\n                    off_policy=memory.sample_uniform(sequence_size=rollout_length),\n                    off_policy_rp=memory.sample_priority(exact_size=True),\n                    ep_summary=ep_stat,\n                    test_ep_summary=test_ep_stat,\n                    render_summary=render_stat,\n                )\n                yield data\n\n                ep_stat = None\n                test_ep_stat = None\n                render_stat = None\n\n    except Exception as e:\n        log.exception(e)\n        raise e'"
btgym/algorithms/runner/synchro.py,0,"b'import numpy as np\nfrom logbook import Logger, StreamHandler, WARNING\nimport sys\nimport time\n\nfrom btgym.algorithms.rollout import Rollout\nfrom btgym.algorithms.memory import _DummyMemory\nfrom btgym.algorithms.math_utils import softmax\nfrom btgym.algorithms.utils import is_subdict\n\n\nclass BaseSynchroRunner():\n    """"""\n    Experience provider class. Interacts with environment and outputs data in form of rollouts augmented with\n    relevant summaries and metadata. This runner is `synchronous` in sense that data collection is `in-process\'\n    and every rollout is collected by explicit call to respective `get_data()` method [this is unlike \'async-`\n    thread-runner version found earlier in this this package which, once being started,\n    runs on its own and can not be moderated].\n    Makes precise control on policy being executed possible.\n    Does not support \'atari\' mode.\n    """"""\n\n    def __init__(\n            self,\n            env,\n            task,\n            rollout_length,\n            episode_summary_freq,\n            env_render_freq,\n            ep_summary,\n            test=False,\n            policy=None,\n            data_sample_config=None,\n            memory_config=None,\n            test_conditions=None,\n            test_deterministic=True,\n            slowdown_steps=0,\n            global_step_op=None,\n            aux_render_modes=None,\n            _implemented_aux_render_modes=None,\n            name=\'synchro\',\n            log_level=WARNING,\n            **kwargs\n    ):\n        """"""\n\n        Args:\n            env:                            BTgym environment instance\n            task:                           int, runner task id\n            rollout_length:                 int\n            episode_summary_freq:           int\n            env_render_freq:                int\n            test:                           legacy, not used\n            ep_summary:                     legacy, not used\n            policy:                         policy instance to execute\n            data_sample_config:             dict, data sampling configuration dictionary\n            memory_config:                  dict, replay memory configuration\n            test_conditions:                dict or None,\n                                            dictionary of single experience conditions to check to mark it as test one.\n            test_deterministic:             bool, if True - act deterministically for test episodes\n            slowdown_time:                  time to sleep between steps\n            aux_render_modes:               iterable of str, additional summaries to compute\n            _implemented_aux_render_modes   iterable of str, implemented additional summaries\n            name:                           str, name scope\n            log_level:                      int, logbook.level\n        """"""\n        self.env = env\n        self.task = task\n        self.name = name\n        self.rollout_length = rollout_length\n        self.episode_summary_freq = episode_summary_freq\n        self.env_render_freq = env_render_freq\n\n        self.memory_config = memory_config\n        self.policy = policy\n        self.data_sample_config = data_sample_config\n\n        self.log_level = log_level\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'{}_Runner_{}\'.format(self.name, self.task), level=self.log_level)\n\n        # Aux rendering setup:\n        if _implemented_aux_render_modes is None:\n            self.implemented_aux_render_modes = []\n\n        else:\n            self.implemented_aux_render_modes = _implemented_aux_render_modes\n\n        self.aux_render_modes = []\n        if aux_render_modes is not None:\n            for mode in aux_render_modes:\n                if mode in self.implemented_aux_render_modes:\n                    self.aux_render_modes.append(mode)\n\n                else:\n                    msg = \'Render mode `{}` is not implemented.\'.format(mode)\n                    self.log.error(msg)\n                    raise NotImplementedError(msg)\n\n        self.log.debug(\'self.render modes: {}\'.format(self.aux_render_modes))\n\n        self.sess = None\n        self.summary_writer = None\n\n        self.global_step_op = global_step_op\n\n        if self.task == 0 and slowdown_steps > 0 and self.global_step_op is not None:\n            self.log.notice(\'is slowed down by {} global_iterations/step\'.format(slowdown_steps))\n            self.slowdown_steps = slowdown_steps\n\n        else:\n            self.slowdown_steps = 0\n\n        if test_conditions is None:\n            # Default test conditions are: experience comes from test episode, from target domain:\n            self.test_conditions = {\n                \'state\': {\n                    \'metadata\': {\n                        \'type\': 1,\n                        \'trial_type\': 1\n                    }\n                }\n            }\n        else:\n            self.test_conditions = test_conditions\n\n        # Actions handling for test runs:\n        self.test_deterministic = test_deterministic\n        # self.log.warning(\'test_deterministic: {}\'.format(self.test_deterministic))\n\n        # Make replay memory:\n        if self.memory_config is not None:\n            self.memory = self.memory_config[\'class_ref\'](**self.memory_config[\'kwargs\'])\n\n        else:\n            self.memory = _DummyMemory()\n\n        self.length = 0\n        self.local_episode = 0\n        self.reward_sum = 0\n\n        self.terminal_end = True\n\n        # Summary averages accumulators:\n        self.total_r = []\n        self.cpu_time = []\n        self.final_value = []\n        self.total_steps = []\n        self.total_steps_atari = []\n        self.info = [None]\n        self.pre_experience = None\n        self.state = None\n        self.context = None\n\n        self.last_action = None\n        self.last_reward = None\n\n        # Episode accumulators:\n        self.ep_accum = None\n\n        self.log.debug(\'__init__() done.\')\n\n    def sleep(self):\n        if self.slowdown_steps > 0:\n            start_global_step = self.sess.run(self.global_step_op)\n            while start_global_step + self.slowdown_steps > self.sess.run(self.global_step_op):\n                time.sleep(0.05)\n\n    def start_runner(self, sess, summary_writer, **kwargs):\n        """"""\n        Legacy wrapper.\n        """"""\n        self.start(sess, summary_writer, **kwargs)\n\n    def start(self, sess, summary_writer, init_context=None, data_sample_config=None):\n        """"""\n        Executes initial sequence; fills initial replay memory if any.\n        """"""\n        assert self.policy is not None, \'Initial policy not specified\'\n        self.sess = sess\n        self.summary_writer = summary_writer\n\n        # # Hacky but we need env.renderer methods ready: NOT HERE, went to VerboseRunner\n        # self.env.renderer.initialize_pyplot()\n\n        self.pre_experience, self.state, self.context, self.last_action, self.last_reward = self.get_init_experience(\n            policy=self.policy,\n            init_context=init_context,\n            data_sample_config=data_sample_config\n        )\n\n        if self.memory_config is not None:\n            while not self.memory.is_full():\n                # collect some rollouts to fill memory:\n                _ = self.get_data()\n            self.log.notice(\'Memory filled\')\n        self.log.notice(\'started collecting data.\')\n\n    def get_init_experience(self, policy, policy_sync_op=None, init_context=None, data_sample_config=None):\n        """"""\n        Starts new environment episode.\n\n        Args:\n            policy:                 policy to execute.\n            policy_sync_op:         operation copying local behavioural policy params from global one\n            init_context:           initial policy context for new episode.\n            data_sample_config:     configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n\n        Returns:\n            incomplete initial experience of episode as dictionary (misses bootstrapped R value),\n            next_state,\n            next, policy RNN context\n            action_reward\n        """"""\n        self.length = 0\n        self.reward_sum = 0\n        # Increment global and local episode counters:\n        self.sess.run(self.policy.inc_episode)\n        self.local_episode += 1\n\n        # self.log.warning(\'get_init_exp() data_sample_config: {}\'.format(data_sample_config))\n\n        if data_sample_config is None:\n            data_sample_config = policy.get_sample_config()\n\n        # Pass sample config to environment (.get_sample_config() is actually aac framework method):\n        init_state = self.env.reset(**data_sample_config)\n\n        # Infer train/test episode type from initial state:\n        self.is_test = is_subdict(self.test_conditions, {\'state\': init_state})\n\n        # self.log.warning(\'init_experience.is_test: {}\'.format(self.is_test))\n\n        # Master worker always resets context at the episode beginning:\n        # TODO: !\n        # if not self.data_sample_config[\'mode\']:\n        init_context = None\n\n        # self.log.warning(\'init_context_passed: {}\'.format(init_context))\n        # self.log.warning(\'state_metadata: {}\'.format(state[\'metadata\']))\n\n        init_action = self.env.action_space.encode(self.env.get_initial_action())\n        init_reward = np.asarray(0.0)\n\n        # Update policy:\n        if policy_sync_op is not None:\n            self.sess.run(policy_sync_op)\n\n        init_context = policy.get_initial_features(state=init_state, context=init_context)\n        action, logits, value, next_context = policy.act(\n            init_state,\n            init_context,\n            init_action[None, ...],\n            init_reward[None, ...],\n            self.is_test and self.test_deterministic,  # deterministic actions for test episode\n        )\n        next_state, reward, terminal, self.info = self.env.step(action[\'environment\'])\n\n        experience = {\n            \'position\': {\'episode\': self.local_episode, \'step\': self.length},\n            \'state\': init_state,\n            \'action\': action[\'one_hot\'],\n            \'reward\': reward,\n            \'value\': value,\n            \'terminal\': terminal,\n            \'context\': init_context,\n            \'last_action\': init_action,\n            \'last_reward\': init_reward,\n            \'r\': None,  # to be updated\n            \'info\': self.info[-1],\n        }\n        # Execute user-defined callbacks to policy, if any:\n        for key, callback in policy.callback.items():\n            experience[key] = callback(**locals())\n\n        # reset per-episode  counters and accumulators:\n        self.ep_accum = {\n            \'logits\': [logits],\n            \'value\': [value],\n            \'context\': [init_context]\n        }\n        self.terminal_end = terminal\n        #self.log.warning(\'init_experience_context: {}\'.format(context))\n\n        # Take a nap:\n        # self.sleep()\n\n        return experience, next_state, next_context, action[\'encoded\'], reward\n\n    def get_experience(self, policy, state, context, action, reward, policy_sync_op=None):\n        """"""\n        Get single experience (possibly terminal).\n\n        Returns:\n            incomplete experience as dictionary (misses bootstrapped R value),\n            next_state,\n            next, policy RNN context\n            action_reward\n        """"""\n        # Update policy if operation has been provided:\n        if policy_sync_op is not None:\n            self.sess.run(policy_sync_op)\n            # self.log.debug(\'Policy sync. ok!\')\n\n        # Continue adding experiences to rollout:\n        next_action, logits, value, next_context = policy.act(\n            state,\n            context,\n            action[None, ...],\n            reward[None, ...],\n            self.is_test and self.test_deterministic,  # deterministic actions for test episode\n        )\n        self.ep_accum[\'logits\'].append(logits)\n        self.ep_accum[\'value\'].append(value)\n        self.ep_accum[\'context\'].append(next_context)\n\n        # self.log.notice(\'context: {}\'.format(context))\n        next_state, next_reward, terminal, self.info = self.env.step(next_action[\'environment\'])\n\n        # Partially compose experience:\n        experience = {\n            \'position\': {\'episode\': self.local_episode, \'step\': self.length},\n            \'state\': state,\n            \'action\': next_action[\'one_hot\'],\n            \'reward\': next_reward,\n            \'value\': value,\n            \'terminal\': terminal,\n            \'context\': context,\n            \'last_action\': action,\n            \'last_reward\': reward,\n            \'r\': None,\n            \'info\': self.info[-1],\n        }\n        for key, callback in policy.callback.items():\n            experience[key] = callback(**locals())\n\n        # Housekeeping:\n        self.length += 1\n\n        # Take a nap:\n        # self.sleep()\n\n        return experience, next_state, next_context, next_action[\'encoded\'], next_reward\n\n    def get_train_stat(self, is_test=False):\n        """"""\n        Updates and computes average statistics for train episodes.\n        Args:\n            is_test: bool, current episode type\n\n        Returns:\n            dict of stats\n        """"""\n\n        ep_stat = {}\n        if not is_test:\n            self.total_r += [self.reward_sum]\n            episode_stat = self.env.get_stat()  # get episode statistic\n            last_i = self.info[-1]  # pull most recent info\n            self.cpu_time += [episode_stat[\'runtime\'].total_seconds()]\n            self.final_value += [last_i[\'broker_value\']]\n            self.total_steps += [episode_stat[\'length\']]\n\n            if self.local_episode % self.episode_summary_freq == 0:\n                ep_stat = dict(\n                    total_r=np.average(self.total_r),\n                    cpu_time=np.average(self.cpu_time),\n                    final_value=np.average(self.final_value),\n                    steps=np.average(self.total_steps)\n                )\n                self.total_r = []\n                self.cpu_time = []\n                self.final_value = []\n                self.total_steps = []\n                self.total_steps_atari = []\n        return ep_stat\n\n    def get_test_stat(self, is_test=False):\n        """"""\n        Updates and computes  statistics for single test episode.\n\n        Args:\n            is_test: bool, current episode type\n\n        Returns:\n            dict of stats\n\n        """"""\n        ep_stat = {}\n        if is_test:\n            episode_stat = self.env.get_stat()  # get episode statistic\n            last_i = self.info[-1]  # pull most recent info\n            ep_stat = dict(\n                total_r=self.reward_sum,\n                final_value=last_i[\'broker_value\'],\n                steps=episode_stat[\'length\']\n            )\n        return ep_stat\n\n    def get_ep_render(self, is_test=False):\n        """"""\n        Collects environment renderings. Relies on environment renderer class methods,\n        so it is only valid when environment rendering is enabled (typically it is true for master runner).\n\n        Returns:\n            dictionary of images as rgb arrays\n\n        """"""\n        # Only render chief worker and test (slave) environment:\n        # if self.task < 1 and (\n        #     is_test or(\n        #         self.local_episode % self.env_render_freq == 0 and not self.data_sample_config[\'mode\']\n        #     )\n        # ):\n        if self.task < 1 and self.local_episode % self.env_render_freq == 0:\n\n            # Render environment (chief worker only):\n            render_stat = {\n                mode: self.env.render(mode)[None, :] for mode in self.env.render_modes\n            }\n\n        else:\n            render_stat = None\n\n        return render_stat\n\n    def get_data(\n            self,\n            policy=None,\n            policy_sync_op=None,\n            init_context=None,\n            data_sample_config=None,\n            rollout_length=None,\n            force_new_episode=False\n    ):\n        """"""\n        Collects single trajectory rollout and bunch of summaries using specified policy.\n        Updates episode statistics and replay memory.\n\n        Args:\n            policy:                 policy to execute\n            policy_sync_op:         operation copying local behavioural policy params from global one\n            init_context:           if specified, overrides initial episode context provided bu self.context\n                                    (valid only if new episode is started within this rollout).\n            data_sample_config:     environment configuration parameters for next episode to sample:\n                                    configuration dictionary of type `btgym.datafeed.base.EnvResetConfig\n            rollout_length:         length of rollout to collect, if specified  - overrides self.rollout_length attr\n            force_new_episode:      bool, if True - resets the environment\n\n\n        Returns:\n                data dictionary\n        """"""\n        if policy is None:\n            policy = self.policy\n\n        if init_context is None:\n            init_context = self.context\n\n        if rollout_length is None:\n            rollout_length = self.rollout_length\n\n        rollout = Rollout()\n        train_ep_summary = None\n        test_ep_summary = None\n        render_ep_summary = None\n\n        if self.terminal_end or force_new_episode:\n            # Start new episode:\n            self.pre_experience, self.state, self.context, self.last_action, self.last_reward = self.get_init_experience(\n                policy=policy,\n                policy_sync_op=policy_sync_op,\n                init_context=init_context,\n                data_sample_config=data_sample_config\n            )\n            # self.log.warning(\n            #     \'started new episode with:\\ndata_sample_config: {}\\nforce_new_episode: {}\'.\n            #         format(data_sample_config, force_new_episode)\n            # )\n            # self.log.warning(\'pre_experience_metadata: {}\'.format(self.pre_experience[\'state\'][\'metadata\']))\n\n        # NOTE: self.terminal_end is set actual via get_init_experience() method\n\n        # Collect single rollout:\n        while rollout.size < rollout_length - 1 and not self.terminal_end:\n            if self.pre_experience[\'terminal\']:\n                # Episode has been just finished,\n                # need to complete and push last experience and update all episode summaries\n                self.pre_experience[\'r\'] = np.asarray([0.0])\n                experience = None\n                self.state = None\n                self.context = None\n                self.last_action = None\n                self.last_reward = None\n\n                self.terminal_end = True\n                train_ep_summary = self.get_train_stat(self.is_test)\n                test_ep_summary = self.get_test_stat(self.is_test)\n                render_ep_summary = self.get_ep_render(self.is_test)\n\n                # self.log.debug(\n                #     \'terminal, train_summary: {}, test_summary: {}\'.format(train_ep_summary, test_ep_summary)\n                # )\n\n            else:\n                experience, self.state, self.context, self.last_action, self.last_reward = self.get_experience(\n                    policy=policy,\n                    policy_sync_op=policy_sync_op,\n                    state=self.state,\n                    context=self.context,\n                    action=self.last_action,\n                    reward=self.last_reward\n                )\n                # Complete previous experience by bootstrapping V from next one:\n                self.pre_experience[\'r\'] = experience[\'value\']\n\n            # Push:\n            rollout.add(self.pre_experience)\n\n            # Where are you coming from?\n            # self.is_test is updated by self.get_init_experience()\n\n            # Only training rollouts are added to replay memory:\n            if not self.is_test:\n                self.memory.add(self.pre_experience)\n\n            self.reward_sum += self.pre_experience[\'reward\']\n\n            # Move one step froward:\n            self.pre_experience = experience\n\n        # Done collecting rollout, either got termination of episode or not:\n        if not self.terminal_end:\n            # Bootstrap:\n            self.pre_experience[\'r\'] = np.asarray(\n                [\n                    policy.get_value(\n                        self.pre_experience[\'state\'],\n                        self.pre_experience[\'context\'],\n                        self.pre_experience[\'last_action\'][None, ...],\n                        self.pre_experience[\'last_reward\'][None, ...],\n                    )\n                ]\n            )\n            rollout.add(self.pre_experience)\n            if not self.is_test:\n                self.memory.add(self.pre_experience)\n\n        # self.log.warning(\'rollout.terminal: {}\'.format(self.terminal_end))\n        # self.log.warning(\'rollout.size: {}\'.format(rollout.size))\n        # self.log.warning(\'rollout.is_test: {}\'.format(self.is_test))\n\n        data = dict(\n            on_policy=rollout,\n            terminal=self.terminal_end,\n            off_policy=self.memory.sample_uniform(sequence_size=rollout_length),\n            off_policy_rp=self.memory.sample_priority(exact_size=True),\n            ep_summary=train_ep_summary,\n            test_ep_summary=test_ep_summary,\n            render_summary=render_ep_summary,\n            is_test=self.is_test,\n        )\n        return data\n\n    def get_batch(\n            self,\n            size,\n            policy=None,\n            policy_sync_op=None,\n            require_terminal=True,\n            same_trial=True,\n            init_context=None,\n            data_sample_config=None\n    ):\n        """"""\n        Returns batch as list of \'size\' or more rollouts collected under specified policy.\n        Rollouts can be collected from several episodes consequently; there is may be more rollouts than set \'size\' if\n        it is necessary to collect at least one terminal rollout.\n\n        Args:\n            size:                   int, number of rollouts to collect\n            policy:                 policy to use\n            policy_sync_op:         operation copying local behavioural policy params from global one\n            require_terminal:       bool, if True - require at least one terminal rollout to be present.\n            same_trial:             bool, if True - all episodes are sampled from same trial\n            init_context:           if specified, overrides initial episode context provided bu self.context\n            data_sample_config:     environment configuration parameters for all episodes in batch:\n                                    configuration dictionary of type `btgym.datafeed.base.EnvResetConfig\n\n        Returns:\n            dict containing:\n            \'data\'key holding list of data dictionaries;\n            \'terminal_context\' key holding list of terminal output contexts.\n            If \'require_terminal = True, this list is guarantied to hold at least one element.\n        """"""\n\n        batch = []\n        terminal_context = []\n\n        if require_terminal:\n            got_terminal = False\n        else:\n            got_terminal = True\n\n        if same_trial:\n            assert isinstance(data_sample_config, dict),\\\n                \'get_batch(same_trial=True) expected `data_sample_config` dict., got: {}\'.format(data_sample_config)\n\n        # Collect first rollout:\n        batch = [\n            self.get_data(\n                policy=policy,\n                policy_sync_op=policy_sync_op,\n                init_context=init_context,\n                data_sample_config=data_sample_config,\n                force_new_episode=True\n            )\n        ]\n        if same_trial:\n            # sample new episodes from same trial only:\n            data_sample_config[\'trial_config\'][\'get_new\'] = False\n\n        collected_size = 1\n\n        if batch[0][\'terminal\']:\n            terminal_context.append(self.context)\n            got_terminal = True\n\n        # Collect others:\n        while not (collected_size >= size and got_terminal):\n            rollout_data = self.get_data(\n                policy=policy,\n                policy_sync_op=policy_sync_op,\n                init_context=init_context,\n                data_sample_config=data_sample_config\n            )\n            batch.append(rollout_data)\n\n            if rollout_data[\'terminal\']:\n                terminal_context.append(self.context)\n                got_terminal = True\n\n            collected_size += 1\n\n        data = dict(\n            data=batch,\n            terminal_context=terminal_context,\n        )\n        return data\n\n\nclass VerboseSynchroRunner(BaseSynchroRunner):\n    """"""\n    Extends `BaseSynchroRunner` class with additional visualisation summaries in some expense of running speed.\n    """"""\n\n    def __init__(\n            self,\n            name=\'verbose_synchro\',\n            aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            **kwargs\n    ):\n\n        super(VerboseSynchroRunner, self).__init__(\n            name=name,\n            aux_render_modes=aux_render_modes,\n            _implemented_aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            **kwargs\n        )\n        self.norm_image = lambda x: np.round((x - x.min()) / np.ptp(x) * 255)\n\n    def get_ep_render(self, is_test=False):\n        """"""\n        Collects episode, environment and policy visualisations. Relies on environment renderer class methods,\n        so it is only valid when environment rendering is enabled (typically it is true for master runner).\n\n        Returns:\n            dictionary of images as rgb arrays\n        """"""\n        # Only render chief worker and test (slave) environment:\n        # if self.task < 1 and (\n        #     is_test or(\n        #         self.local_episode % self.env_render_freq == 0 and not self.data_sample_config[\'mode\']\n        #     )\n        # ):\n        if self.task < 1 and self.local_episode % self.env_render_freq == 0:\n\n            # Render environment (chief worker only):\n            render_stat = {\n                mode: self.env.render(mode)[None, :] for mode in self.env.render_modes\n            }\n            # Update renderings with aux:\n\n            # ep_a_logits = self.ep_accum[\'logits\']\n            # ep_value = self.ep_accum[\'value\']\n            # self.log.notice(\'ep_logits shape: {}\'.format(np.asarray(ep_a_logits).shape))\n            # self.log.notice(\'ep_value shape: {}\'.format(np.asarray(ep_value).shape))\n\n            # Unpack LSTM states:\n            rnn_1, rnn_2 = zip(*self.ep_accum[\'context\'])\n            rnn_1 = [state[0] for state in rnn_1]\n            rnn_2 = [state[0] for state in rnn_2]\n            c1, h1 = zip(*rnn_1)\n            c2, h2 = zip(*rnn_2)\n\n            # Render everything implemented (doh!):\n            implemented_aux_images = {\n                \'action_prob\': self.env.renderer.draw_plot(\n                    # data=softmax(np.asarray(ep_a_logits)[:, 0, :] - np.asarray(ep_a_logits).max()),\n                    data=softmax(np.asarray(self.ep_accum[\'logits\'])), #[:, 0, :]),\n                    title=\'Episode actions probabilities\',\n                    figsize=(12, 4),\n                    box_text=\'\',\n                    xlabel=\'Backward env. steps\',\n                    ylabel=\'R+\',\n                    line_labels=[\'Hold\', \'Buy\', \'Sell\', \'Close\']\n                )[None, ...],\n                \'value_fn\': self.env.renderer.draw_plot(\n                    data=np.asarray(self.ep_accum[\'value\']),\n                    title=\'Episode Value function\',\n                    figsize=(12, 4),\n                    xlabel=\'Backward env. steps\',\n                    ylabel=\'R\',\n                    line_labels=[\'Value\']\n                )[None, ...],\n                # \'lstm_1_c\': norm_image(np.asarray(c1).T[None, :, 0, :, None]),\n                \'lstm_1_h\': self.norm_image(np.asarray(h1).T[None, :, 0, :, None]),\n                # \'lstm_2_c\': norm_image(np.asarray(c2).T[None, :, 0, :, None]),\n                \'lstm_2_h\': self.norm_image(np.asarray(h2).T[None, :, 0, :, None])\n            }\n\n            # Pick what has been set:\n            aux_images = {summary: implemented_aux_images[summary] for summary in self.aux_render_modes}\n            render_stat.update(aux_images)\n\n        else:\n            render_stat = None\n\n        return render_stat\n\n    def start(self, sess, summary_writer, init_context=None, data_sample_config=None):\n        """"""\n        Executes initial sequence; fills initial replay memory if any.\n        Extra: initialises environment renderer to get aux. images.\n        """"""\n        assert self.policy is not None, \'Initial policy not specified\'\n        self.sess = sess\n        self.summary_writer = summary_writer\n\n        # Hacky but we need env.renderer methods ready:\n        self.env.renderer.initialize_pyplot()\n\n        self.pre_experience, self.state, self.context, self.last_action, self.last_reward = self.get_init_experience(\n            policy=self.policy,\n            init_context=init_context,\n            data_sample_config=data_sample_config\n        )\n\n        if self.memory_config is not None:\n            while not self.memory.is_full():\n                # collect some rollouts to fill memory:\n                _ = self.get_data()\n            self.log.notice(\'Memory filled\')\n        self.log.notice(\'started collecting data.\')\n\n\n\n\n\n\n\n\n\n'"
btgym/algorithms/runner/threadrunner.py,1,"b'# Async. framework code comes from OpenAI repository under MIT licence:\n# https://github.com/openai/universe-starter-agent\n#\n\nfrom logbook import Logger, StreamHandler, WARNING\nimport sys\n\nimport six.moves.queue as queue\nimport threading\n\nfrom btgym.algorithms.runner import BaseEnvRunnerFn\n\n\nclass RunnerThread(threading.Thread):\n    """"""\n    Async. framework code comes from OpenAI repository under MIT licence:\n    https://github.com/openai/universe-starter-agent\n\n    Despite the fact BTgym is not real-time environment [yet], thread-runner approach is still here. From\n    original `universe-starter-agent`:\n    `...One of the key distinctions between a normal environment and a universe environment\n    is that a universe environment is _real time_.  This means that there should be a thread\n    that would constantly interact with the environment and tell it what to do.  This thread is here.`\n\n    Another idea is to see ThreadRunner as all-in-one data provider, thus shaping data distribution\n    fed to estimator from single place.\n    So, replay memory is also here, as well as some service functions (collecting summary data).\n    """"""\n    def __init__(self,\n                 env,\n                 policy,\n                 task,\n                 rollout_length,\n                 episode_summary_freq,\n                 env_render_freq,\n                 test,\n                 ep_summary,\n                 runner_fn_ref=BaseEnvRunnerFn,\n                 memory_config=None,\n                 log_level=WARNING,\n                 **kwargs):\n        """"""\n\n        Args:\n            env:                    environment instance\n            policy:                 policy instance\n            task:                   int\n            rollout_length:         int\n            episode_summary_freq:   int\n            env_render_freq:        int\n            test:                   Atari or BTGyn\n            ep_summary:             tf.summary\n            runner_fn_ref:          callable defining runner execution logic\n            memory_config:          replay memory configuration dictionary\n            log_level:              int, logbook.level\n        """"""\n        threading.Thread.__init__(self)\n        self.queue = queue.Queue(5)\n        self.rollout_length = rollout_length\n        self.env = env\n        self.last_features = None\n        self.policy = policy\n        self.runner_fn_ref = runner_fn_ref\n        self.daemon = True\n        self.sess = None\n        self.summary_writer = None\n        self.episode_summary_freq = episode_summary_freq\n        self.env_render_freq = env_render_freq\n        self.task = task\n        self.test = test\n        self.ep_summary = ep_summary\n        self.memory_config = memory_config\n        self.log_level = log_level\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'ThreadRunner_{}\'.format(self.task), level=self.log_level)\n\n    def start_runner(self, sess, summary_writer, **kwargs):\n        try:\n            self.sess = sess\n            self.summary_writer = summary_writer\n            self.start()\n\n        except:\n            msg = \'start() exception occurred.\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError\n\n    def run(self):\n        """"""Just keep running.""""""\n        try:\n            with self.sess.as_default():\n                self._run()\n\n        except:\n            msg = \'RunTime exception occurred.\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError\n\n    def _run(self):\n        rollout_provider = self.runner_fn_ref(\n            self.sess,\n            self.env,\n            self.policy,\n            self.task,\n            self.rollout_length,\n            self.summary_writer,\n            self.episode_summary_freq,\n            self.env_render_freq,\n            self.test,\n            self.ep_summary,\n            self.memory_config,\n            self.log\n        )\n        while True:\n            # the timeout variable exists because apparently, if one worker dies, the other workers\n            # won\'t die with it, unless the timeout is set to some large number.  This is an empirical\n            # observation.\n\n            self.queue.put(next(rollout_provider), timeout=600.0)\n\n'"
btgym/research/casual/__init__.py,0,b''
btgym/research/casual/aac.py,13,"b'import tensorflow as tf\nimport numpy as np\nimport time\nimport datetime\n\nfrom btgym.research.gps.aac import GuidedAAC\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\n\n\nclass CA3C(GuidedAAC):\n    """"""\n    Temporally dependant vanilla A3C. This is mot a meta-learning class.\n    Requires stateful temporal data stream provider class such as btgym.datafeed.time.BTgymCasualDataDomain\n    """"""\n\n    def __init__(\n            self,\n            runner_config=None,\n            trial_source_target_cycle=(1, 0),\n            num_episodes_per_trial=1,  # one-shot adaptation\n            test_slowdown_steps=1,\n            episode_sample_params=(1.0, 1.0),\n            trial_sample_params=(1.0, 1.0),\n            _aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            _use_target_policy=False,\n            name=\'CasualA3C\',\n            **kwargs\n    ):\n        try:\n            if runner_config is None:\n                self.runner_config = {\n                    \'class_ref\': BaseSynchroRunner,\n                    \'kwargs\': {\n                        \'data_sample_config\': {\'mode\': 0},\n                        \'test_conditions\': {\n                            \'state\': {\n                                \'metadata\': {\n                                    \'trial_type\': 1,  # only test episode from target dom. considered test one\n                                    \'type\': 1\n                                }\n                            }\n                        },\n                        \'slowdown_steps\': test_slowdown_steps,\n                        \'name\': \'\',\n                    },\n                }\n            else:\n                self.runner_config = runner_config\n\n            # Trials sampling control:\n            self.num_source_trials = trial_source_target_cycle[0]\n            self.num_target_trials = trial_source_target_cycle[-1]\n            self.num_episodes_per_trial = num_episodes_per_trial\n\n            self.test_slowdown_steps = test_slowdown_steps\n\n            self.episode_sample_params = episode_sample_params\n            self.trial_sample_params = trial_sample_params\n\n            self.global_timestamp = 0\n\n            self.current_source_trial = 0\n            self.current_target_trial = 0\n            self.current_trial_mode = 0  # source\n            self.current_episode = 1\n\n            super(CA3C, self).__init__(\n                runner_config=self.runner_config,\n                _aux_render_modes=_aux_render_modes,\n                name=name,\n                **kwargs\n            )\n        except:\n            msg = \'{}.__init()__ exception occurred\'.format(name) + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def get_sample_config(self, **kwargs):\n        """"""\n        Returns environment configuration parameters for next episode to sample.\n\n        Here we always prescribe to sample test episode from source or target domain.\n\n        Args:\n              kwargs:     not used\n\n        Returns:\n            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n        """"""\n\n        new_trial = 0\n        if self.current_episode >= self.num_episodes_per_trial:\n            # Reset episode counter:\n            self.current_episode = 0\n\n            # Request new trial:\n            new_trial = 1\n            # Decide on trial type (source/target):\n            if self.current_source_trial >= self.num_source_trials:\n                # Time to switch to target mode:\n                self.current_trial_mode = 1\n                # Reset counters:\n                self.current_source_trial = 0\n                self.current_target_trial = 0\n\n            if self.current_target_trial >= self.num_target_trials:\n                # Vise versa:\n                self.current_trial_mode = 0\n                self.current_source_trial = 0\n                self.current_target_trial = 0\n\n            # Update counter:\n            if self.current_trial_mode:\n                self.current_target_trial += 1\n            else:\n                self.current_source_trial += 1\n\n        self.current_episode += 1\n\n        if self.task == 0:\n            trial_sample_type = 1\n\n        else:\n            trial_sample_type = self.current_trial_mode\n\n        # Compose btgym.datafeed.base.EnvResetConfig-consistent dict:\n        sample_config = dict(\n            episode_config=dict(\n                get_new=True,\n                sample_type=1,\n                timestamp= self.global_timestamp,\n                b_alpha=self.episode_sample_params[0],\n                b_beta=self.episode_sample_params[-1]\n            ),\n            trial_config=dict(\n                get_new=new_trial,\n                sample_type=trial_sample_type,\n                timestamp=self.global_timestamp,\n                b_alpha=self.trial_sample_params[0],\n                b_beta=self.trial_sample_params[-1]\n            )\n        )\n        return sample_config\n\n    def process(self, sess, **kwargs):\n        try:\n            sess.run(self.sync_pi)\n            # Get data configuration:\n            data_config = self.get_sample_config()\n\n            # self.log.warning(\'data_config: {}\'.format(data_config))\n\n            # If this step data comes from source or target domain\n            is_test = data_config[\'trial_config\'][\'sample_type\'] and data_config[\'episode_config\'][\'sample_type\']\n\n            # self.log.warning(\'is_test: {}\'.format(is_test))\n\n            if is_test:\n                if self.task == 0:\n                    self.process_test(sess, data_config)\n\n                else:\n                    pass\n\n            else:\n                self.process_train(sess, data_config)\n\n        except:\n            msg = \'process() exception occurred\' + \\\n                \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def process_test(self, sess, data_config):\n        data = {}\n        done = False\n        # Set target episode beginning to be at current timepoint:\n        data_config[\'trial_config\'][\'align_left\'] = 1\n        self.log.info(\'test episode started...\')\n\n        while not done:\n            #sess.run(self.sync_pi)\n\n            data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config,\n                policy_sync_op=self.sync_pi,  # update policy after each single step instead of single rollout\n            )\n            done = np.asarray(data[\'terminal\']).any()\n\n            # self.log.warning(\'test episode done: {}\'.format(done))\n\n            self.global_timestamp = data[\'on_policy\'][0][\'state\'][\'metadata\'][\'timestamp\'][-1]\n\n            # # Wait for other workers to catch up with training:\n            # start_global_step = sess.run(self.global_step)\n            # while self.test_skeep_steps >= sess.run(self.global_step) - start_global_step:\n            #     time.sleep(self.test_sleep_time)\n\n            self.log.info(\n                \'test episode rollout done, global_time: {}, global_step: {}\'.format(\n                    datetime.datetime.fromtimestamp(self.global_timestamp),\n                    sess.run(self.global_step)\n                )\n            )\n\n        self.log.notice(\n            \'test episode finished, global_time set: {}\'.format(\n                datetime.datetime.fromtimestamp(self.global_timestamp)\n            )\n        )\n        self.log.notice(\n            \'final value: {:8.2f} after {} steps @ {}\'.format(\n                data[\'on_policy\'][0][\'info\'][\'broker_value\'][-1],\n                data[\'on_policy\'][0][\'info\'][\'step\'][-1],\n                data[\'on_policy\'][0][\'info\'][\'time\'][-1],\n            )\n        )\n        data[\'ep_summary\'] = [None]  # We only test here, want no train NAN\'s\n        self.process_summary(sess, data)\n\n    def process_train(self, sess, data_config):\n        data = {}\n        done = False\n        # Set source episode to be sampled uniformly from test interval:\n        data_config[\'trial_config\'][\'align_left\'] = 0\n        # self.log.warning(\'train episode started...\')\n\n        while not done:\n            sess.run(self.sync_pi)\n\n            wirte_model_summary = \\\n                self.local_steps % self.model_summary_freq == 0\n\n            data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config\n            )\n            done = np.asarray(data[\'terminal\']).any()\n            feed_dict = self.process_data(sess, data, is_train=True, pi=self.local_network)\n\n            if wirte_model_summary:\n                fetches = [self.train_op, self.model_summary_op, self.inc_step]\n            else:\n                fetches = [self.train_op, self.inc_step]\n\n            fetched = sess.run(fetches, feed_dict=feed_dict)\n\n            if wirte_model_summary:\n                model_summary = fetched[-2]\n\n            else:\n                model_summary = None\n\n            self.process_summary(sess, data, model_summary)\n\n            self.local_steps += 1\n\n        # self.log.warning(\n        #     \'train episode finished at {} vs was_global_time: {}\'.format(\n        #         data[\'on_policy\'][0][\'info\'][\'time\'][-1],\n        #         datetime.datetime.fromtimestamp(data[\'on_policy\'][0][\'state\'][\'metadata\'][\'timestamp\'][-1])\n        #\n        #     )\n        # )\n\n\nclass CA3Ca(CA3C):\n    """"""\n    + Adaptive iteratations.\n    """"""\n\n    def __init__(self, name=\'CasualAdaA3C\', **kwargs):\n        super(CA3Ca, self).__init__(name=name, **kwargs)\n\n    def _make_loss(self, **kwargs):\n        aac_loss, summaries = self._make_base_loss(**kwargs)\n\n        # Guidance annealing:\n        if self.guided_decay_steps is not None:\n            self.guided_lambda_decayed = tf.train.polynomial_decay(\n                self.guided_lambda,\n                self.global_step + 1,\n                self.guided_decay_steps,\n                0,\n                power=1,\n                cycle=False,\n            )\n        else:\n            self.guided_lambda_decayed = self.guided_lambda\n        # Switch to zero when testing - prevents information leakage:\n        self.train_guided_lambda = self.guided_lambda_decayed * tf.cast(self.local_network.train_phase, tf.float32)\n\n        self.guided_loss, guided_summary = self.expert_loss(\n            pi_actions=self.local_network.on_logits,\n            expert_actions=self.local_network.expert_actions,\n            name=\'on_policy\',\n            verbose=True,\n            guided_lambda=self.train_guided_lambda\n        )\n        loss = self.aac_lambda * aac_loss + self.guided_loss\n\n        summaries += guided_summary\n\n        self.log.notice(\n            \'guided_lambda: {:1.6f}, guided_decay_steps: {}\'.format(self.guided_lambda, self.guided_decay_steps)\n        )\n\n        if hasattr(self.local_network, \'meta\'):\n            self.log.notice(\'meta_policy enabled\')\n            summaries += self.local_network.meta.summaries\n\n        return loss, summaries\n\n    def _make_train_op(self, pi, pi_prime, pi_global):\n        """"""\n        Defines training op graph and supplementary sync operations.\n\n        Args:\n            pi:                 policy network obj.\n            pi_prime:           optional policy network obj.\n            pi_global:          shared policy network obj. hosted by parameter server\n\n        Returns:\n            tensor holding training op graph;\n        """"""\n\n        # Each worker gets a different set of adam optimizer parameters:\n        self.optimizer = tf.train.AdamOptimizer(self.train_learn_rate, epsilon=1e-5)\n\n        # Clipped gradients:\n        self.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.loss, pi.var_list),\n            40.0\n        )\n        self.grads_global_norm = tf.global_norm(self.grads)\n        # Copy weights from the parameter server to the local model\n        self.sync = self.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_global.var_list)]\n        )\n        if self.use_target_policy:\n            # Copy weights from new policy model to target one:\n            self.sync_pi_prime = tf.group(\n                *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi.var_list)]\n            )\n        grads_and_vars = list(zip(self.grads, pi_global.var_list))\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(pi.on_state_in.keys())\n\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        self.inc_step = self.global_step.assign_add(tf.shape(pi.on_state_in[\'external\'])[0])\n\n        self.local_network.meta.grads_and_vars = list(\n            zip(self.local_network.meta.grads, self.network.meta.var_list)\n        )\n        self.meta_opt = tf.train.GradientDescentOptimizer(self.local_network.meta.learn_rate)\n\n        self.meta_train_op = self.meta_opt.apply_gradients(self.local_network.meta.grads_and_vars)\n\n        self.local_network.meta.sync_slot_op = tf.assign(\n            self.local_network.meta.cluster_averages_slot,\n            self.network.meta.cluster_averages_slot,\n        )\n\n        self.local_network.meta.send_stat_op = tf.scatter_nd_update(\n            self.network.meta.cluster_averages_slot,\n            [[0, self.task], [1, self.task]],\n            [\n                self.local_network.meta.cluster_averages_slot[0, self.task],\n                self.local_network.meta.cluster_averages_slot[1, self.task]\n            ]\n        )\n        self.local_network.meta.global_reset = self.network.meta.reset\n\n        train_op = self.optimizer.apply_gradients(grads_and_vars)\n\n        self.local_network.meta.ppp = self.network.meta\n\n        self.log.debug(\'train_op defined\')\n        return tf.group([train_op, self.meta_train_op])\n\n    def process_test(self, sess, data_config):\n        data = {}\n        done = False\n        # Set target episode beginning to be at current timepoint:\n        data_config[\'trial_config\'][\'align_left\'] = 1\n\n        self.log.info(\'test episode started...\')\n\n        while not done:\n\n            # TODO: temporal, refract\n            # self.local_network.meta.global_reset()\n            sess.run(self.sync_pi)\n\n            data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config,\n                rollout_length=2,\n                policy_sync_op=self.sync_pi,  # update policy after each single step instead of single rollout\n            )\n            done = np.asarray(data[\'terminal\']).any()\n\n            # self.log.warning(\'test episode done: {}\'.format(done))\n\n            self.global_timestamp = data[\'on_policy\'][0][\'state\'][\'metadata\'][\'timestamp\'][-1]\n\n            # # Wait for other workers to catch up with training:\n            # start_global_step = sess.run(self.global_step)\n            # while self.test_skeep_steps >= sess.run(self.global_step) - start_global_step:\n            #     time.sleep(self.test_sleep_time)\n\n            self.log.info(\n                \'test episode rollout done, global_time: {}, global_step: {}\'.format(\n                    datetime.datetime.fromtimestamp(self.global_timestamp),\n                    sess.run(self.global_step)\n                )\n            )\n            # Now can train on already past data:\n            feed_dict = self.process_data(sess, data, is_train=True, pi=self.local_network)\n\n            fetches = [self.train_op, self.model_summary_op, self.inc_step]\n\n            fetched = sess.run(fetches, feed_dict=feed_dict)\n\n            model_summary = fetched[-2]\n\n            data[\'ep_summary\'] = [None]  # We only test here, want no train NAN\'s\n            self.process_summary(sess, data, model_summary)\n\n        self.log.notice(\n            \'test episode finished, global_time set: {}\'.format(\n                datetime.datetime.fromtimestamp(self.global_timestamp)\n            )\n        )\n        self.log.notice(\n            \'final value: {:8.2f} after {} steps @ {}\'.format(\n                data[\'on_policy\'][0][\'info\'][\'broker_value\'][-1],\n                data[\'on_policy\'][0][\'info\'][\'step\'][-1],\n                data[\'on_policy\'][0][\'info\'][\'time\'][-1],\n            )\n        )\n\n    def process_train(self, sess, data_config):\n        data = {}\n        done = False\n        # Set source episode to be sampled uniformly from test interval:\n        data_config[\'trial_config\'][\'align_left\'] = 0\n        # self.log.warning(\'train episode started...\')\n\n        while not done:\n            sess.run(self.sync_pi)\n\n            wirte_model_summary = \\\n                self.local_steps % self.model_summary_freq == 0\n\n            data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config,\n                policy_sync_op=None\n            )\n            done = np.asarray(data[\'terminal\']).any()\n            feed_dict = self.process_data(sess, data, is_train=True, pi=self.local_network)\n\n            if wirte_model_summary:\n                fetches = [self.local_network.on_vf, self.train_op, self.model_summary_op, self.inc_step]\n            else:\n                fetches = [self.local_network.on_vf, self.train_op, self.inc_step]\n\n            sess.run(self.local_network.meta.sync_slot_op)\n\n            fetched = sess.run(fetches, feed_dict=feed_dict)\n\n            if wirte_model_summary:\n                model_summary = fetched[-2]\n\n            else:\n                model_summary = None\n\n            self.process_summary(sess, data, model_summary)\n\n            # Meta:\n            train_stat = fetched[0] - 10\n\n            if sess.run(self.network.meta.cluster_averages_slot)[0, self.task] == 0:\n                self.local_network.meta.reset()\n\n            self.local_network.meta.update(train_stat)\n\n            sess.run(self.local_network.meta.send_stat_op)\n\n            self.local_steps += 1\n\n\n'"
btgym/research/casual_conv/__init__.py,0,b''
btgym/research/casual_conv/layers.py,15,"b""import tensorflow as tf\n\nfrom btgym.algorithms.nn.layers import conv1d\n\n\ndef time_to_batch(value, dilation, name=None):\n    with tf.name_scope('time_to_batch'):\n        shape = tf.shape(value)\n        pad_elements = dilation - 1 - (shape[1] + dilation - 1) % dilation\n        padded = tf.pad(value, [[0, 0], [0, pad_elements], [0, 0]])\n        reshaped = tf.reshape(padded, [-1, dilation, shape[2]])\n        transposed = tf.transpose(reshaped, perm=[1, 0, 2])\n        return tf.reshape(transposed, [shape[0] * dilation, -1, shape[2]])\n\n\ndef batch_to_time(value, dilation, name=None):\n    with tf.name_scope('batch_to_time'):\n        shape = tf.shape(value)\n        prepared = tf.reshape(value, [dilation, -1, shape[2]])\n        transposed = tf.transpose(prepared, perm=[1, 0, 2])\n        return tf.reshape(transposed,\n                          [tf.div(shape[0], dilation), -1, shape[2]])\n\n\ndef dilated_conv1d(\n        inputs,\n        filters,\n        filter_width=2,\n        dilation_rate=1,\n        pad='VALID',\n        activation=None,\n        name='dialted_conv_1d',\n        reuse=False\n):\n    with tf.name_scope(name):\n        if dilation_rate > 1:\n            transformed = time_to_batch(inputs, dilation_rate)\n            conv = conv1d(\n                x=transformed,\n                num_filters=filters,\n                filter_size=filter_width,\n                stride=1,\n                pad=pad,\n                name=name,\n                reuse=reuse\n            )\n            restored = batch_to_time(conv, dilation_rate)\n        else:\n            restored = conv1d(\n                x=inputs,\n                num_filters=filters,\n                filter_size=filter_width,\n                stride=1,\n                pad=pad,\n                name=name,\n                reuse=reuse\n            )\n        # Remove excess elements at the end.\n        out_width = tf.shape(inputs)[1] - (filter_width - 1) * dilation_rate\n        result = tf.slice(restored,\n                          [0, 0, 0],\n                          [-1, out_width, -1])\n\n        if activation is not None:\n            result = activation(result)\n\n        return result\n\n"""
btgym/research/casual_conv/networks.py,32,"b'\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import layer_norm as norm_layer\n\nimport numpy as np\nimport math\n\nfrom btgym.algorithms.nn.layers import conv1d\n\n\ndef conv_1d_casual_encoder(\n        x,\n        ob_space,\n        ac_space,\n        conv_1d_num_filters=32,\n        conv_1d_filter_size=2,\n        conv_1d_activation=tf.nn.elu,\n        conv_1d_overlap=1,\n        name=\'casual_encoder\',\n        keep_prob=None,\n        conv_1d_gated=False,\n        reuse=False,\n        collections=None,\n        **kwargs\n    ):\n    """"""\n    Tree-shaped convolution stack encoder as more comp. efficient alternative to dilated one.\n\n    Stage1 casual convolutions network: from 1D input to estimated features.\n\n    Returns:\n        tensor holding state features;\n    """"""\n\n    with tf.variable_scope(name_or_scope=name, reuse=reuse):\n        shape = x.get_shape().as_list()\n        if len(shape) > 3:  # remove pseudo 2d dimension\n            x = x[:, :, 0, :]\n        num_layers = int(math.log(shape[1], conv_1d_filter_size))\n\n        # print(\'num_layers: \', num_layers)\n\n        layers = []\n        slice_depth = []\n        y = x\n\n        for i in range(num_layers):\n\n            _, length, channels = y.get_shape().as_list()\n\n            # t2b:\n            tail = length % conv_1d_filter_size\n            if tail != 0:\n                pad = conv_1d_filter_size - tail\n                paddings = [[0, 0], [pad, 0], [0, 0]]\n                y = tf.pad(y, paddings)\n                length += pad\n\n            # print(\'padded_length: \', length)\n\n            num_time_batches = int(length / conv_1d_filter_size)\n\n            stride = conv_1d_filter_size - conv_1d_overlap\n            assert stride > 0, \'conv_1d_filter_size should be greater then conv_1d_overlap\'\n        \n            # print(\'num_time_batches: \', num_time_batches)\n\n            y = tf.reshape(y, [-1, conv_1d_filter_size, channels], name=\'layer_{}_t2b\'.format(i))\n\n            y = conv1d(\n                x=y,\n                num_filters=conv_1d_num_filters,\n                filter_size=conv_1d_filter_size,\n                stride=stride,\n                pad=\'VALID\',\n                name=\'conv1d_layer_{}\'.format(i)\n            )\n            # b2t:\n            y = tf.reshape(y, [-1, num_time_batches, conv_1d_num_filters], name=\'layer_{}_output\'.format(i))\n\n            y = norm_layer(y)\n            if conv_1d_activation is not None:\n                y = conv_1d_activation(y)\n\n            if keep_prob is not None:\n                y = tf.nn.dropout(y, keep_prob=keep_prob, name=""_layer_{}_with_dropout"".format(i))\n\n            layers.append(y)\n\n            depth = conv_1d_overlap // conv_1d_filter_size ** i\n\n            if depth < 1:\n                depth = 1\n\n            slice_depth.append(depth)\n\n        # encoded = tf.stack([h[:, -1, :] for h in layers], axis=1, name=\'encoded_state\')\n\n        sliced_layers = [\n            tf.slice(\n                h,\n                begin=[0, h.get_shape().as_list()[1] - d, 0],\n                size=[-1, d, -1]\n            ) for h, d in zip(layers, slice_depth)\n        ]\n        output_stack = sliced_layers\n        # But:\n        if conv_1d_gated:\n            split_size = int(conv_1d_num_filters / 2)\n            output_stack = []\n            for l in sliced_layers:\n                x1 = l[..., :split_size]\n                x2 = l[..., split_size:]\n\n                y = tf.multiply(\n                    x1,\n                    tf.nn.sigmoid(x2),\n                    name=\'gated_conv_output\'\n                )\n                output_stack.append(y)\n\n        encoded = tf.concat(output_stack, axis=1, name=\'encoded_state\')\n\n        # encoded = tf.concat(\n        #     [\n        #         tf.slice(\n        #             h,\n        #             begin=[0, h.get_shape().as_list()[1] - d, 0],\n        #             size=[-1, d, -1]\n        #         ) for h, d in zip(layers, slice_depth)\n        #     ],\n        #     axis=1,\n        #     name=\'encoded_state\'\n        # )\n        # print(\'encoder :\', encoded)\n\n    return encoded\n\n\ndef attention_layer(inputs, attention_ref=tf.contrib.seq2seq.LuongAttention, name=\'attention_layer\', **kwargs):\n    """"""\n    Temporal attention layer.\n    Computes attention context based on last(left) value in time dim.\n\n    Paper:\n    Minh-Thang Luong, Hieu Pham, Christopher D. Manning.,\n    ""Effective Approaches to Attention-based Neural Machine Translation."" https://arxiv.org/abs/1508.04025\n\n    Args:\n        inputs:\n        attention_ref:      attention mechanism class\n        name:\n\n    Returns:\n        attention output tensor\n    """"""\n    shape = inputs.get_shape().as_list()\n    source_states = inputs[:, :-1, :]  # all but last\n    query_state = inputs[:, -1, :]\n\n    attention_mechanism = attention_ref(\n        num_units=shape[-1],\n        memory=source_states,\n        #scale=True,\n        name=name,\n        **kwargs\n    )\n\n    # alignments = attention_mechanism(query_state, None)  # normalized attention weights\n\n    # Suppose there is no previous context for attention (uhm?):\n    alignments = attention_mechanism(\n        query_state,\n        attention_mechanism.initial_alignments(tf.shape(inputs)[0], dtype=tf.float32)\n    )\n    # Somehow attention call returns tuple of twin tensors (wtf?):\n    if isinstance(alignments, tuple):\n        alignments = alignments[0]\n\n    # Compute context vector:\n    expanded_alignments = tf.expand_dims(alignments, axis=-2)\n\n    # print(\'attention_mechanism.values:\', attention_mechanism.values)\n    # print(\'alignments: \', alignments)\n    # print(\'expanded_alignments:\', expanded_alignments)\n\n    context = tf.matmul(expanded_alignments, attention_mechanism.values)  # values == source_states\n\n    # context = tf.squeeze(context, [1])\n    # attention = tf.layers.Dense(shape-1, name=\'attention_layer\')(tf.concat([query_state, context], 1))\n\n    attention = context\n\n    return attention\n\n\ndef conv_1d_casual_attention_encoder(\n        x,\n        ob_space,\n        ac_space,\n        conv_1d_num_filters=32,\n        conv_1d_filter_size=2,\n        conv_1d_activation=tf.nn.elu,\n        conv_1d_attention_ref=tf.contrib.seq2seq.LuongAttention,\n        name=\'casual_encoder\',\n        keep_prob=None,\n        conv_1d_gated=False,\n        conv_1d_full_hidden=False,\n        reuse=False,\n        collections=None,\n        **kwargs\n    ):\n    """"""\n    Tree-shaped convolution stack encoder with self-attention.\n\n    Stage1 casual convolutions network: from 1D input to estimated features.\n\n    Returns:\n        tensor holding state features;\n    """"""\n\n    with tf.variable_scope(name_or_scope=name, reuse=reuse):\n        shape = x.get_shape().as_list()\n        if len(shape) > 3:  # remove pseudo 2d dimension\n            x = x[:, :, 0, :]\n        num_layers = int(math.log(shape[1], conv_1d_filter_size))\n\n        # print(\'num_layers: \', num_layers)\n\n        layers = []\n        attention_layers = []\n        y = x\n\n        for i in range(num_layers):\n\n            _, length, channels = y.get_shape().as_list()\n\n            # t2b:\n            tail = length % conv_1d_filter_size\n            if tail != 0:\n                pad = conv_1d_filter_size - tail\n                paddings = [[0, 0], [pad, 0], [0, 0]]\n                y = tf.pad(y, paddings)\n                length += pad\n\n            # print(\'padded_length: \', length)\n\n            num_time_batches = int(length / conv_1d_filter_size)\n\n            # print(\'num_time_batches: \', num_time_batches)\n\n            y = tf.reshape(y, [-1, conv_1d_filter_size, channels], name=\'layer_{}_t2b\'.format(i))\n\n            y = conv1d(\n                x=y,\n                num_filters=conv_1d_num_filters,\n                filter_size=conv_1d_filter_size,\n                stride=1,\n                pad=\'VALID\',\n                name=\'conv1d_layer_{}\'.format(i)\n            )\n            # b2t:\n            y = tf.reshape(y, [-1, num_time_batches, conv_1d_num_filters], name=\'layer_{}_output\'.format(i))\n\n            y = norm_layer(y)\n\n            if conv_1d_activation is not None:\n                y = conv_1d_activation(y)\n\n            if keep_prob is not None:\n                y = tf.nn.dropout(y, keep_prob=keep_prob, name=""_layer_{}_with_dropout"".format(i))\n\n            if conv_1d_gated:\n                split_size = int(conv_1d_num_filters / 2)\n\n                y1 = y[..., :split_size]\n                y2 = y[..., split_size:]\n\n                y = tf.multiply(\n                    y1,\n                    tf.nn.sigmoid(y2),\n                    name=""_layer_{}_gated"".format(i)\n                )\n            layers.append(y)\n\n            # Insert attention for all but top layer:\n            if num_time_batches > 1:\n                attention = attention_layer(\n                    y,\n                    attention_ref=conv_1d_attention_ref,\n                    name=\'attention_layer_{}\'.format(i)\n                )\n                attention_layers.append(attention)\n\n        if conv_1d_full_hidden:\n            convolved = tf.concat(layers, axis=-2, name=\'convolved_stack_full\')\n\n        else:\n            convolved = tf.stack([h[:, -1, :] for h in layers], axis=1, name=\'convolved_stack\')\n\n        attended = tf.concat(attention_layers, axis=-2, name=\'attention_stack\')\n\n        encoded = tf.concat([convolved, attended], axis=-2, name=\'encoded_state\')\n        # print(\'layers\', layers)\n        # print(\'convolved: \', convolved)\n        # print(\'attention_layers:\', attention_layers)\n        # print(\'attention_stack: \', attended)\n        # print(\'encoded :\', encoded)\n\n    return encoded\n'"
btgym/research/casual_conv/policy.py,1,"b'import tensorflow as tf\n\nfrom btgym.research.gps.policy import GuidedPolicy_0_0\nfrom btgym.research.casual_conv.networks import conv_1d_casual_encoder\n\n\nclass CasualConvPolicy_0_0(GuidedPolicy_0_0):\n    """"""\n    Casual.0.\n    """"""\n    def __init__(\n        self,\n        state_encoder_class_ref=conv_1d_casual_encoder,\n        conv_1d_num_filters=32,\n        conv_1d_filter_size=2,\n        conv_1d_slice_size=1,  # future use, do not modify yet\n        conv_1d_activation=tf.nn.elu,\n        conv_1d_use_bias=False,\n        **kwargs\n    ):\n        assert conv_1d_slice_size == 1\n\n        super().__init__(\n            state_encoder_class_ref=state_encoder_class_ref,\n            conv_1d_num_filters=conv_1d_num_filters,\n            conv_1d_filter_size=conv_1d_filter_size,\n            conv_1d_slice_size=conv_1d_slice_size,\n            conv_1d_activation=conv_1d_activation,\n            conv_1d_use_bias=conv_1d_use_bias,\n            **kwargs\n        )\n\n'"
btgym/research/casual_conv/strategy.py,0,"b'import numpy as np\n\nfrom gym import spaces\nfrom btgym import DictSpace\nimport backtrader.indicators as btind\nfrom backtrader import Indicator\n\nfrom btgym.strategy.utils import tanh, exp_scale\n\nfrom btgym.research.gps.strategy import GuidedStrategy_0_0\nfrom btgym.research.strategy_gen_4 import DevStrat_4_12\n\n\nclass CasualConvStrategy(DevStrat_4_12):\n# class CasualConvStrategy(GuidedStrategy_0_0):\n    """"""\n    Provides stream of data for casual convolutional encoder\n    """"""\n    # Time embedding period:\n    time_dim = 128  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Hyperparameters for estimating signal features:\n    # features_parameters = [8, 32, 64]\n    features_parameters = [8, 32, 128, 512]\n    num_features = len(features_parameters)\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 0.99  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1  # reward multiplicator\n\n    state_ext_scale = np.linspace(4e3, 1e3, num=num_features)\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, num_features), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'datetime\': spaces.Box(low=0, high=1, shape=(1, 5), dtype=np.float32),\n            # \'expert\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),  # TODO: change inheritance!\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        skip_frame=skip_frame,\n        state_ext_scale=state_ext_scale,  # EURUSD\n        state_int_scale=1.0,\n        gamma=gamma,\n        reward_scale=1.0,\n        metadata={},\n    )\n\n    def set_datalines(self):\n        self.data.features = [\n            btind.SimpleMovingAverage(self.datas[0], period=period) for period in self.features_parameters\n        ]\n\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=(np.asarray(self.features_parameters).max() + self.time_dim)\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n\nclass MaxPool(Indicator):\n    """"""\n    Custom period `sliding candle` upper bound.\n    """"""\n    lines = (\'max\',)\n    params = ((\'period\', 1),)\n    plotinfo = dict(\n        subplot=False,\n        plotlinevalues=False,\n    )\n\n    def __init__(self):\n        self.addminperiod(self.params.period)\n\n    def next(self):\n        self.lines.max[0] = np.frombuffer(self.data.high.get(size=self.p.period)).max()\n\n\nclass MinPool(Indicator):\n    """"""\n    Custom period `sliding candle` lower bound.\n    """"""\n    lines = (\'min\',)\n    params = ((\'period\', 1),)\n    plotinfo = dict(\n        subplot=False,\n        plotlinevalues=False,\n    )\n\n    def __init__(self):\n        self.addminperiod(self.params.period)\n\n    def next(self):\n        self.lines.min[0] = np.frombuffer(self.data.low.get(size=self.p.period)).min()\n\n\nclass CasualConvStrategy_0(CasualConvStrategy):\n    """"""\n    Casual convolutional encoder + `sliding candle` price data features instead of SMA.\n    """"""\n    # Time embedding period:\n    # time_dim = 512  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n    time_dim = 128\n    # time_dim = 32\n\n    # Periods for estimating signal features,\n    # note: here number of feature channels is doubled due to fact Hi/Low values computed for each period specified:\n\n    # features_parameters = [8, 32, 128, 512]\n    # features_parameters = [2, 8, 32, 64, 128]\n    features_parameters = [8, 16, 32, 64, 128, 256]\n\n    num_features = len(features_parameters)\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are collected over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 0.99  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1  # reward multiplicator\n\n    state_ext_scale = np.linspace(2e3, 1e3, num=num_features)\n\n    params = dict(\n        # Note: fake `Width` dimension to stay in convention with 2d conv. dims:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, num_features * 2), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'datetime\': spaces.Box(low=0, high=1, shape=(1, 5), dtype=np.float32),\n            # \'expert\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n        # TODO: change inheritance!\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        state_ext_scale=state_ext_scale,  # EURUSD\n        state_int_scale=1.0,\n        gamma=gamma,\n        reward_scale=1.0,\n        metadata={},\n    )\n\n    def set_datalines(self):\n        features_low = [MinPool(self.data, period=period) for period in self.features_parameters]\n        features_high = [MaxPool(self.data, period=period) for period in self.features_parameters]\n\n        # If `scale` was scalar - make it vector:\n        if len(np.asarray(self.p.state_ext_scale).shape) < 1:\n            self.p.state_ext_scale = np.repeat(np.asarray(self.p.state_ext_scale), self.num_features)\n\n        # Sort features by `period` for .get_external_state() to estimate\n        # more or less sensible gradient; double-stretch scale vector accordingly:\n        # TODO: maybe 2 separate conv. encoders for hi/low?\n        self.data.features = []\n        for f1, f2 in zip(features_low, features_high):\n            self.data.features += [f1, f2]\n\n        self.p.state_ext_scale = np.repeat(self.p.state_ext_scale, 2)\n\n        # print(\'p.state_ext_scale: \', self.p.state_ext_scale, self.p.state_ext_scale.shape)\n\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=(np.asarray(self.features_parameters).max() + self.time_dim)\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n\nimport scipy.signal as signal\nfrom scipy.stats import zscore\n\n\nclass CasualConvStrategy_1(CasualConvStrategy_0):\n    """"""\n    CWT. again.\n    """"""\n    # Time embedding period:\n    # time_dim = 512\n    # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n    # NOTE_2: should be power of 2 if using casual conv. state encoder\n    time_dim = 128\n    # time_dim = 32\n\n    # Periods for estimating signal features,\n    # note: here number of feature channels is doubled due to fact Hi/Low values computed for each period specified:\n\n    # features_parameters = [8, 32, 128, 512]\n    # features_parameters = [2, 8, 32, 64, 128]\n    # features_parameters = [8, 16, 32, 64, 128, 256]\n    #\n    # num_features = len(features_parameters)\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are collected over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period\n    # NOTE_: should be power of 2 if using casual conv. state encoder:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 0.99  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1  # reward multiplicator\n\n    num_features = 16\n\n    cwt_signal_scale = 3e3  # first gradient scaling [scalar]\n    cwt_lower_bound = 3.0   # CWT scales\n    cwt_upper_bound = 90.0\n\n    state_ext_scale = np.linspace(1, 3, num=num_features)\n\n    params = dict(\n        # Note: fake `Width` dimension to stay in convention with 2d conv. dims:\n        state_shape=\n        {\n            \'raw\': spaces.Box(low=-100, high=100, shape=(time_dim, 4), dtype=np.float32),\n            # \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, num_features), dtype=np.float32),\n            \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, num_features, 1), dtype=np.float32),\n            # \'external_2\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, 4), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'datetime\': spaces.Box(low=0, high=1, shape=(1, 5), dtype=np.float32),\n            # \'expert\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n            # TODO: change inheritance!\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        skip_frame=skip_frame,\n        state_ext_scale=state_ext_scale,  # EURUSD\n        state_int_scale=1.0,\n        gamma=gamma,\n        reward_scale=1.0,\n        metadata={},\n        cwt_lower_bound=cwt_lower_bound,\n        cwt_upper_bound=cwt_upper_bound,\n        cwt_signal_scale=cwt_signal_scale,\n    )\n\n    def __init__(self, **kwargs):\n        super(CasualConvStrategy_1, self).__init__(**kwargs)\n        # self.num_channels = self.p.state_shape[\'external\'].shape[-1]\n        self.num_channels = self.num_features\n        # Define CWT scales:\n        self.cwt_width = np.linspace(self.p.cwt_lower_bound, self.p.cwt_upper_bound, self.num_channels)\n\n    def set_datalines(self):\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=(np.asarray(self.features_parameters).max() + self.time_dim)\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n    def get_external_state(self):\n        # Use Hi-Low median as signal:\n        x = (\n            np.frombuffer(self.data.high.get(size=self.time_dim)) +\n            np.frombuffer(self.data.low.get(size=self.time_dim))\n        ) / 2\n\n        # Differences along time dimension:\n        d_x = np.gradient(x, axis=0) * self.p.cwt_signal_scale\n\n        # Compute continuous wavelet transform using Ricker wavelet:\n        cwt_x = signal.cwt(d_x, signal.ricker, self.cwt_width).T\n\n        norm_x = cwt_x\n\n        # Note: differences taken once again along channels axis,\n        # apply weighted scaling to normalize channels\n        # norm_x = np.gradient(cwt_x, axis=-1)\n        # norm_x = zscore(norm_x, axis=0) * self.p.state_ext_scale\n        # norm_x *= self.p.state_ext_scale\n\n        out_x = tanh(norm_x)\n\n        # out_x = np.clip(norm_x, -10, 10)\n\n        # return out_x[:, None, :]\n        return out_x[..., None]\n\n    def get_external_2_state(self):\n        x = np.stack(\n            [\n                np.frombuffer(self.data.high.get(size=self.time_dim)),\n                np.frombuffer(self.data.open.get(size=self.time_dim)),\n                np.frombuffer(self.data.low.get(size=self.time_dim)),\n                np.frombuffer(self.data.close.get(size=self.time_dim)),\n            ],\n            axis=-1\n        )\n        # # Differences along features dimension:\n        d_x = np.gradient(x, axis=-1) * self.p.cwt_signal_scale\n\n        # Compute continuous wavelet transform using Ricker wavelet:\n        # cwt_x = signal.cwt(d_x, signal.ricker, self.cwt_width).T\n\n        norm_x = d_x\n\n        # Note: differences taken once again along channels axis,\n        # apply weighted scaling to normalize channels\n        # norm_x = np.gradient(cwt_x, axis=-1)\n        # norm_x = zscore(norm_x, axis=0) * self.p.state_ext_scale\n        # norm_x *= self.p.state_ext_scale\n\n        out_x = tanh(norm_x)\n\n        # out_x = np.clip(norm_x, -10, 10)\n\n        return out_x[:, None, :]\n\n\nclass CasualConvStrategyMulti(CasualConvStrategy_0):\n    """"""\n    CWT + multiply data streams.\n    Beta - data names are class hard-coded.\n    TODO: pass data streams names as params\n    """"""\n    # Time embedding period:\n    # NOTE_2: should be power of 2 if using casual conv. state encoder\n    time_dim = 128\n\n    # Periods for estimating signal features,\n    # note: here number of feature channels is doubled due to fact Hi/Low values computed for each period specified:\n\n    # features_parameters = [8, 32, 128, 512]\n    # features_parameters = [2, 8, 32, 64, 128]\n    # features_parameters = [8, 16, 32, 64, 128, 256]\n    #\n    # num_features = len(features_parameters)\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are collected over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period\n    # NOTE_: should be power of 2 if using casual conv. state encoder:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 0.99  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1  # reward multiplicator\n\n    num_features = 16  # TODO: 8? (was: 16)\n\n    cwt_signal_scale = 3e3  # first gradient scaling [scalar]\n    cwt_lower_bound = 4.0   # CWT scales  TODO: 8.? (was : 3.)\n    cwt_upper_bound = 100.0\n\n    state_ext_scale = {\n        \'USD\': np.linspace(1, 2, num=num_features),\n        \'GBP\': np.linspace(1, 2, num=num_features),\n        \'CHF\': np.linspace(1, 2, num=num_features),\n        \'JPY\': np.linspace(5e-3, 1e-2, num=num_features),\n    }\n    order_size = {\n        \'USD\': 1000,\n        \'GBP\': 1000,\n        \'CHF\': 1000,\n        \'JPY\': 1000,\n    }\n\n    params = dict(\n        # Note: fake `Width` dimension to stay in convention with 2d conv. dims:\n        state_shape=\n        {\n            \'raw\': spaces.Box(low=-1000, high=1000, shape=(time_dim, 4), dtype=np.float32),\n            \'external\': DictSpace(\n                {\n                    \'USD\': spaces.Box(low=-1000, high=1000, shape=(time_dim, 1, num_features), dtype=np.float32),\n                    \'GBP\': spaces.Box(low=-1000, high=1000, shape=(time_dim, 1, num_features), dtype=np.float32),\n                    \'CHF\': spaces.Box(low=-1000, high=1000, shape=(time_dim, 1, num_features), dtype=np.float32),\n                    \'JPY\': spaces.Box(low=-1000, high=1000, shape=(time_dim, 1, num_features), dtype=np.float32),\n                }\n            ),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'datetime\': spaces.Box(low=0, high=1, shape=(1, 5), dtype=np.float32),\n            # \'expert\': DictSpace(\n            #     {\n            #         \'USD\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n            #         \'GBP\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n            #         \'CHF\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n            #         \'JPY\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n            #     }\n            # ),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'EUR\',\n        asset_names={\'USD\', \'GBP\', \'CHF\', \'JPY\'},\n        start_cash=None,\n        commission=None,\n        leverage=1.0,\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        initial_action=None,\n        initial_portfolio_action=None,\n        order_size=order_size,\n        skip_frame=skip_frame,\n        state_ext_scale=state_ext_scale,\n        state_int_scale=1.0,\n        gamma=gamma,\n        # base_dataline=\'USD\',\n        reward_scale=1.0,\n        metadata={},\n        cwt_lower_bound=cwt_lower_bound,\n        cwt_upper_bound=cwt_upper_bound,\n        cwt_signal_scale=cwt_signal_scale,\n    )\n\n    def __init__(self, **kwargs):\n        self.data_streams = {}\n        super(CasualConvStrategyMulti, self).__init__(**kwargs)\n        # self.num_channels = self.p.state_shape[\'external\'].shape[-1]\n        self.num_channels = self.num_features\n        # Define CWT scales:\n        self.cwt_width = np.linspace(self.p.cwt_lower_bound, self.p.cwt_upper_bound, self.num_channels)\n\n        # print(\'p: \', dir(self.p))\n\n    def nextstart(self):\n        """"""\n        Overrides base method augmenting it with estimating expert actions before actual episode starts.\n        """"""\n        # This value shows how much episode records we need to spend\n        # to estimate first environment observation:\n        self.inner_embedding = self.data.close.buflen()\n        self.log.info(\'Inner time embedding: {}\'.format(self.inner_embedding))\n\n        # Now when we know exact maximum possible episode length -\n        #  can extract relevant episode data and make expert predictions:\n        # data = self.datas[0].p.dataname.as_matrix()[self.inner_embedding:, :]\n        data = {d._name : d.p.dataname.as_matrix()[self.inner_embedding:, :] for d in self.datas}\n\n        # Note: need to form sort of environment \'custom candels\' by taking min and max price values over every\n        # skip_frame period; this is done inside Oracle class;\n        # TODO: shift actions forward to eliminate one-point prediction lag?\n        # expert_actions is a matrix representing discrete distribution over actions probabilities\n        # of size [max_env_steps, action_space_size]:\n\n\n        # self.expert_actions = {\n        #     key: self.expert.fit(episode_data=line, resampling_factor=self.p.skip_frame)\n        #     for key, line in data.items()\n        # }\n\n    # def get_expert_state(self):\n    #     # self.current_expert_action = self.expert_actions[self.env_iteration]\n    #     self.current_expert_action = {\n    #         key: line[self.env_iteration] for key, line in self.expert_actions.items()\n    #     }\n    #\n    #     return self.current_expert_action\n\n    def set_datalines(self):\n        self.data_streams = {\n            stream._name: stream for stream in self.datas\n        }\n        # self.data = self.data_streams[self.p.base_dataline] # TODO: ??!!\n\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.data,\n            period=(np.asarray(self.features_parameters).max() + self.time_dim)\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n    def get_external_state(self):\n        return {key: self.get_single_external_state(key) for key in self.data_streams.keys()}\n\n    def get_single_external_state(self, key):\n        # Use Hi-Low median as signal:\n        x = (\n                    np.frombuffer(self.data_streams[key].high.get(size=self.time_dim)) +\n                    np.frombuffer(self.data_streams[key].low.get(size=self.time_dim))\n            ) / 2\n\n        # Differences along time dimension:\n        d_x = np.gradient(x, axis=0) * self.p.cwt_signal_scale\n\n        # Compute continuous wavelet transform using Ricker wavelet:\n        cwt_x = signal.cwt(d_x, signal.ricker, self.cwt_width).T\n\n        norm_x = cwt_x\n\n        # Note: differences taken once again along channels axis,\n        # apply weighted scaling to normalize channels\n        # norm_x = np.gradient(cwt_x, axis=-1)\n        # norm_x = zscore(norm_x, axis=0) * self.p.state_ext_scale\n        norm_x *= self.p.state_ext_scale[key]\n\n        out_x = tanh(norm_x)\n\n        # out_x = np.clip(norm_x, -10, 10)\n\n        # return out_x[:, None, :]\n        return out_x[:, None, :]\n\n\n\n'"
btgym/research/encoder_test/__init__.py,0,b''
btgym/research/encoder_test/aac.py,91,"b'import tensorflow as tf\nimport numpy as np\nimport time\nimport datetime\n\nfrom btgym.algorithms import BaseAAC\nfrom btgym.algorithms.math_utils import cat_entropy\n# from btgym.algorithms.runner.synchro import BaseSynchroRunner\nfrom btgym.research.encoder_test.runner import RegressionRunner\n\n\n# class EncoderClassifier(BaseAAC):\n#     """"""\n#     `Fake AAC` class meant to test policy state encoder ability to predict price movement\n#     as an isolated classification/regression problem.\n#     """"""\n#\n#     def __init__(\n#             self,\n#             runner_config=None,\n#             trial_source_target_cycle=(1, 0),\n#             num_episodes_per_trial=1,  # one-shot adaptation\n#             test_slowdown_steps=0,\n#             episode_sample_params=(1.0, 1.0),\n#             trial_sample_params=(1.0, 1.0),\n#             aac_lambda=0,\n#             class_lambda=1.0,\n#             class_use_rnn=True,\n#             _aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n#             _use_target_policy=False,\n#             name=\'EncoderClassifier\',\n#             **kwargs\n#     ):\n#         try:\n#             if runner_config is None:\n#                 self.runner_config = {\n#                     \'class_ref\': BaseSynchroRunner,\n#                     \'kwargs\': {\n#                         \'data_sample_config\': {\'mode\': 0},\n#                         \'test_conditions\': {\n#                             \'state\': {\n#                                 \'metadata\': {\n#                                     \'trial_type\': 1,  # only test episode from target dom. considered test one\n#                                     \'type\': 1\n#                                 }\n#                             }\n#                         },\n#                         \'slowdown_steps\': test_slowdown_steps,\n#                         \'name\': \'\',\n#                     },\n#                 }\n#             else:\n#                 self.runner_config = runner_config\n#\n#             # Trials sampling control:\n#             self.num_source_trials = trial_source_target_cycle[0]\n#             self.num_target_trials = trial_source_target_cycle[-1]\n#             self.num_episodes_per_trial = num_episodes_per_trial\n#\n#             self.aac_lambda = aac_lambda\n#             self.class_lambda = class_lambda\n#             self.class_use_rnn = class_use_rnn\n#\n#             self.test_slowdown_steps = test_slowdown_steps\n#\n#             self.episode_sample_params = episode_sample_params\n#             self.trial_sample_params = trial_sample_params\n#\n#             self.global_timestamp = 0\n#\n#             self.current_source_trial = 0\n#             self.current_target_trial = 0\n#             self.current_trial_mode = 0  # source\n#             self.current_episode = 1\n#\n#             super(EncoderClassifier, self).__init__(\n#                 runner_config=self.runner_config,\n#                 aux_render_modes=_aux_render_modes,\n#                 name=name,\n#                 **kwargs\n#             )\n#         except:\n#             msg = \'{}.__init()__ exception occurred\'.format(name) + \\\n#                   \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n#             self.log.exception(msg)\n#             raise RuntimeError(msg)\n#\n#     def _make_loss(self, pi, pi_prime, name=\'base\', verbose=True, **kwargs):\n#         """"""\n#         Defines policy state encoder classification loss, placeholders and summaries.\n#\n#         Args:\n#             pi:                 policy network obj.\n#             pi_prime:           optional policy network obj.\n#             name:               str, name scope\n#             verbose:            summary level\n#\n#         Returns:\n#             tensor holding estimated loss graph\n#             list of related summaries\n#         """"""\n#         with tf.name_scope(name):\n#             # On-policy AAC loss definition:\n#             pi.on_pi_act_target = tf.placeholder(\n#                 tf.float32, [None, self.ref_env.action_space.n], name=""on_policy_action_pl""\n#             )\n#             pi.on_pi_adv_target = tf.placeholder(tf.float32, [None], name=""on_policy_advantage_pl"")\n#             pi.on_pi_r_target = tf.placeholder(tf.float32, [None], name=""on_policy_return_pl"")\n#\n#             clip_epsilon = tf.cast(self.clip_epsilon * self.learn_rate_decayed / self.opt_learn_rate, tf.float32)\n#\n#             on_pi_loss, on_pi_summaries = self.on_policy_loss(\n#                 act_target=pi.on_pi_act_target,\n#                 adv_target=pi.on_pi_adv_target,\n#                 r_target=pi.on_pi_r_target,\n#                 pi_logits=pi.on_logits,\n#                 pi_vf=pi.on_vf,\n#                 pi_prime_logits=pi_prime.on_logits,\n#                 entropy_beta=self.model_beta,\n#                 epsilon=clip_epsilon,\n#                 name=\'on_policy\',\n#                 verbose=verbose\n#             )\n#\n#             # Classification loss for price movements prediction:\n#\n#             # oracle_labels = tf.one_hot(tf.argmax(pi.expert_actions, axis=-1), depth=4)\n#\n#             if self.class_use_rnn:\n#                 class_logits = pi.on_logits\n#\n#             else:\n#                 class_logits = pi.on_simple_logits\n#\n#\n#             # class_loss = tf.reduce_mean(\n#             #     tf.nn.softmax_cross_entropy_with_logits_v2(\n#             #         labels=pi.expert_actions,#oracle_labels,\n#             #         logits=class_logits,\n#             #     )\n#             # )\n#\n#             class_loss = tf.losses.mean_squared_error(\n#                 labels=pi.expert_actions[..., 1:3],\n#                 predictions=tf.nn.softmax(class_logits)[..., 1:3],\n#             )\n#             entropy = tf.reduce_mean(cat_entropy(class_logits))\n#\n#             # self.accuracy = tf.metrics.accuracy(\n#             #     labels=tf.argmax(pi.expert_actions, axis=-1),\n#             #     predictions=tf.argmax(class_logits, axis=-1)\n#             # )\n#\n#             self.accuracy = tf.metrics.accuracy(\n#                 labels=tf.argmax(pi.expert_actions[..., 1:3], axis=-1),\n#                 predictions=tf.argmax(class_logits[..., 1:3], axis=-1)\n#             )\n#\n#             model_summaries = [\n#                 tf.summary.scalar(\'class_loss\', class_loss),\n#                 tf.summary.scalar(\'class_accuracy\', self.accuracy[0])\n#             ]\n#             # Accumulate total loss:\n#             loss = float(self.class_lambda) * class_loss + float(self.aac_lambda) * on_pi_loss\\\n#                 - float(self.model_beta) * entropy\n#\n#             model_summaries += on_pi_summaries\n#\n#         return loss, model_summaries\n#\n#     def _make_train_op(self, pi, pi_prime, pi_global):\n#         """"""\n#         Defines training op graph and supplementary sync operations.\n#\n#         Args:\n#             pi:                 policy network obj.\n#             pi_prime:           optional policy network obj.\n#             pi_global:          shared policy network obj. hosted by parameter server\n#\n#         Returns:\n#             tensor holding training op graph;\n#         """"""\n#\n#         # Each worker gets a different set of adam optimizer parameters:\n#         self.optimizer = tf.train.AdamOptimizer(self.train_learn_rate, epsilon=1e-5)\n#\n#         # Clipped gradients:\n#         self.grads, _ = tf.clip_by_global_norm(\n#             tf.gradients(self.loss, pi.var_list),\n#             40.0\n#         )\n#         self.grads_global_norm = tf.global_norm(self.grads)\n#         # Copy weights from the parameter server to the local model\n#         self.sync = self.sync_pi = tf.group(\n#             *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_global.var_list)]\n#         )\n#         if self.use_target_policy:\n#             # Copy weights from new policy model to target one:\n#             self.sync_pi_prime = tf.group(\n#                 *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi.var_list)]\n#             )\n#         grads_and_vars = list(zip(self.grads, pi_global.var_list))\n#\n#         # Set global_step increment equal to observation space batch size:\n#         obs_space_keys = list(pi.on_state_in.keys())\n#\n#         assert \'external\' in obs_space_keys, \\\n#             \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n#         if isinstance(pi.on_state_in[\'external\'], dict):\n#             stream = pi.on_state_in[\'external\'][list(pi.on_state_in[\'external\'].keys())[0]]\n#         else:\n#             stream = pi.on_state_in[\'external\']\n#         self.inc_step = self.global_step.assign_add(tf.shape(stream)[0])\n#\n#         train_op = [self.optimizer.apply_gradients(grads_and_vars),  self.accuracy]\n#\n#         self.evaluate_op = [self.loss, self.accuracy]\n#\n#         self.log.debug(\'train_op defined\')\n#         return train_op\n#\n#     def _combine_summaries(self, policy=None, model_summaries=None):\n#         """"""\n#         Defines model-wide and episode-related summaries\n#\n#         Returns:\n#             model_summary op\n#             episode_summary op\n#         """"""\n#         if model_summaries is not None:\n#             if self.use_global_network:\n#                 # Model-wide statistics:\n#                 with tf.name_scope(\'model\'):\n#                     model_summaries += [\n#                         tf.summary.scalar(""grad_global_norm"", self.grads_global_norm),\n#                         tf.summary.scalar(""learn_rate"", self.learn_rate_decayed),  # cause actual rate is a jaggy due to test freezes\n#                         tf.summary.scalar(""total_loss"", self.loss),\n#                     ]\n#                     if policy is not None:\n#                         model_summaries += [ tf.summary.scalar(""var_global_norm"", tf.global_norm(policy.var_list))]\n#         else:\n#             model_summaries = []\n#         # Model stat. summary:\n#         model_summary = tf.summary.merge(model_summaries, name=\'model_summary\')\n#\n#         # Episode-related summaries:\n#         ep_summary = dict(\n#             # Summary placeholders\n#             render_atari=tf.placeholder(tf.uint8, [None, None, None, 1]),\n#             total_r=tf.placeholder(tf.float32, ),\n#             cpu_time=tf.placeholder(tf.float32, ),\n#             final_value=tf.placeholder(tf.float32, ),\n#             steps=tf.placeholder(tf.int32, ),\n#         )\n#         if self.test_mode:\n#             # For Atari:\n#             ep_summary[\'render_op\'] = tf.summary.image(""model/state"", ep_summary[\'render_atari\'])\n#\n#         else:\n#             # BTGym rendering:\n#             ep_summary.update(\n#                 {\n#                     mode: tf.placeholder(tf.uint8, [None, None, None, None], name=mode + \'_pl\')\n#                     for mode in self.env_list[0].render_modes + self.aux_render_modes\n#                 }\n#             )\n#             ep_summary[\'render_op\'] = tf.summary.merge(\n#                 [tf.summary.image(mode, ep_summary[mode])\n#                  for mode in self.env_list[0].render_modes + self.aux_render_modes]\n#             )\n#         # Episode stat. summary:\n#         ep_summary[\'btgym_stat_op\'] = tf.summary.merge(\n#             [\n#                 tf.summary.scalar(\'episode_train/cpu_time_sec\', ep_summary[\'cpu_time\']),\n#                 tf.summary.scalar(\'episode_train/total_reward\', ep_summary[\'total_r\']),\n#             ],\n#             name=\'episode_train_btgym\'\n#         )\n#         # Test episode stat. summary:\n#         ep_summary[\'test_btgym_stat_op\'] = tf.summary.merge(\n#             [\n#                 tf.summary.scalar(\'episode_test/total_reward\', ep_summary[\'total_r\']),\n#             ],\n#             name=\'episode_test_btgym\'\n#         )\n#         ep_summary[\'atari_stat_op\'] = tf.summary.merge(\n#             [\n#                 tf.summary.scalar(\'episode/total_reward\', ep_summary[\'total_r\']),\n#                 tf.summary.scalar(\'episode/steps\', ep_summary[\'steps\'])\n#             ],\n#             name=\'episode_atari\'\n#         )\n#         self.log.debug(\'model-wide and episode summaries ok.\')\n#         return model_summary, ep_summary\n#\n#     def get_sample_config(self, **kwargs):\n#         """"""\n#         Returns environment configuration parameters for next episode to sample.\n#\n#         Here we always prescribe to sample test episode from source or target domain.\n#\n#         Args:\n#               kwargs:     not used\n#\n#         Returns:\n#             configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n#         """"""\n#\n#         new_trial = 0\n#         if self.current_episode >= self.num_episodes_per_trial:\n#             # Reset episode counter:\n#             self.current_episode = 0\n#\n#             # Request new trial:\n#             new_trial = 1\n#             # Decide on trial type (source/target):\n#             if self.current_source_trial >= self.num_source_trials:\n#                 # Time to switch to target mode:\n#                 self.current_trial_mode = 1\n#                 # Reset counters:\n#                 self.current_source_trial = 0\n#                 self.current_target_trial = 0\n#\n#             if self.current_target_trial >= self.num_target_trials:\n#                 # Vise versa:\n#                 self.current_trial_mode = 0\n#                 self.current_source_trial = 0\n#                 self.current_target_trial = 0\n#\n#             # Update counter:\n#             if self.current_trial_mode:\n#                 self.current_target_trial += 1\n#             else:\n#                 self.current_source_trial += 1\n#\n#         self.current_episode += 1\n#\n#         if self.task == 0:\n#             trial_sample_type = 1\n#\n#         else:\n#             trial_sample_type = self.current_trial_mode\n#\n#         # Compose btgym.datafeed.base.EnvResetConfig-consistent dict:\n#         sample_config = dict(\n#             episode_config=dict(\n#                 get_new=True,\n#                 sample_type=1,\n#                 timestamp= self.global_timestamp,\n#                 b_alpha=self.episode_sample_params[0],\n#                 b_beta=self.episode_sample_params[-1]\n#             ),\n#             trial_config=dict(\n#                 get_new=new_trial,\n#                 sample_type=trial_sample_type,\n#                 timestamp=self.global_timestamp,\n#                 b_alpha=self.trial_sample_params[0],\n#                 b_beta=self.trial_sample_params[-1]\n#             )\n#         )\n#         return sample_config\n#\n#     def process(self, sess, **kwargs):\n#         try:\n#             sess.run(self.sync_pi)\n#             # Get data configuration:\n#             data_config = self.get_sample_config()\n#\n#             # self.log.warning(\'data_config: {}\'.format(data_config))\n#\n#             # If this step data comes from source or target domain\n#             is_test = data_config[\'trial_config\'][\'sample_type\'] and data_config[\'episode_config\'][\'sample_type\']\n#\n#             # self.log.warning(\'is_test: {}\'.format(is_test))\n#\n#             if is_test:\n#                 if self.task == 0:\n#                     self.process_eval(sess, data_config)\n#\n#                 else:\n#                     pass\n#\n#             else:\n#                 self.process_train(sess, data_config)\n#\n#         except:\n#             msg = \'process() exception occurred\' + \\\n#                 \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n#             self.log.exception(msg)\n#             raise RuntimeError(msg)\n#\n#     def process_eval(self, sess, data_config):\n#         data = {}\n#         done = False\n#         # # Set target episode beginning to be at current timepoint:\n#         data_config[\'trial_config\'][\'align_left\'] = 0\n#         self.log.info(\'test episode started...\')\n#\n#         while not done:\n#             sess.run(self.sync_pi)\n#\n#             data = self.get_data(\n#                 policy=self.local_network,\n#                 data_sample_config=data_config,\n#             )\n#             done = np.asarray(data[\'terminal\']).any()\n#             feed_dict = self.process_data(sess, data, is_train=False, pi=self.local_network)\n#\n#             fetches = [self.evaluate_op, self.model_summary_op, self.inc_step]\n#             fetched = sess.run(fetches, feed_dict=feed_dict)\n#\n#             model_summary = fetched[-2]\n#\n#             self.process_summary(sess, data, model_summary)\n#\n#             # self.global_timestamp = data[\'on_policy\'][0][\'state\'][\'metadata\'][\'timestamp\'][-1]\n#\n#         self.log.info(\n#             \'test episode finished, global_time set: {}\'.format(\n#                 datetime.datetime.fromtimestamp(self.global_timestamp)\n#             )\n#         )\n#\n#     def process_train(self, sess, data_config):\n#         data = {}\n#         done = False\n#         # Set source episode to be sampled uniformly from test interval:\n#         data_config[\'trial_config\'][\'align_left\'] = 0\n#         # self.log.warning(\'train episode started...\')\n#\n#         while not done:\n#             sess.run(self.sync_pi)\n#\n#             wirte_model_summary = \\\n#                 self.local_steps % self.model_summary_freq == 0\n#\n#             data = self.get_data(\n#                 policy=self.local_network,\n#                 data_sample_config=data_config\n#             )\n#             done = np.asarray(data[\'terminal\']).any()\n#             feed_dict = self.process_data(sess, data, is_train=True, pi=self.local_network)\n#\n#             if wirte_model_summary:\n#                 fetches = [self.train_op, self.model_summary_op, self.inc_step]\n#             else:\n#                 fetches = [self.train_op, self.inc_step]\n#\n#             fetched = sess.run(fetches, feed_dict=feed_dict)\n#\n#             if wirte_model_summary:\n#                 model_summary = fetched[-2]\n#\n#             else:\n#                 model_summary = None\n#\n#             self.process_summary(sess, data, model_summary)\n#\n#             self.local_steps += 1\n#\n#         self.log.info(\n#             \'train episode finished at {} vs was_global_time: {}\'.format(\n#                 data[\'on_policy\'][0][\'info\'][\'time\'][-1],\n#                 datetime.datetime.fromtimestamp(data[\'on_policy\'][0][\'state\'][\'metadata\'][\'timestamp\'][-1])\n#\n#             )\n#         )\n\n\nclass RegressionTestAAC(BaseAAC):\n    """"""\n    Simplified AAC class meant to test state encoder ability to solve an isolated classification/regression problem.\n    """"""\n    def __init__(\n            self,\n            runner_config=None,\n            trial_source_target_cycle=(1, 0),\n            num_episodes_per_trial=1,\n            test_slowdown_steps=0,\n            episode_sample_params=(1.0, 1.0),\n            trial_sample_params=(1.0, 1.0),\n            aac_lambda=0,\n            regress_lambda=1.0,\n            aux_render_modes=(),\n            _use_target_policy=False,\n            name=\'TestAAC\',\n            **kwargs\n    ):\n        try:\n            if runner_config is None:\n                self.runner_config = {\n                    \'class_ref\': RegressionRunner,\n                    \'kwargs\': {\n                        \'data_sample_config\': {\'mode\': 0},\n                        \'test_conditions\': {\n                            \'state\': {\n                                \'metadata\': {\n                                    \'trial_type\': 1,  # only test episode from target dom. considered test one\n                                    \'type\': 1\n                                }\n                            }\n                        },\n                        \'slowdown_steps\': test_slowdown_steps,\n                        \'name\': \'\',\n                    },\n                }\n            else:\n                self.runner_config = runner_config\n\n            # Trials sampling control:\n            self.num_source_trials = trial_source_target_cycle[0]\n            self.num_target_trials = trial_source_target_cycle[-1]\n            self.num_episodes_per_trial = num_episodes_per_trial\n\n            self.aac_lambda = aac_lambda\n            self.regress_lambda = regress_lambda\n\n            self.test_slowdown_steps = test_slowdown_steps\n\n            self.episode_sample_params = episode_sample_params\n            self.trial_sample_params = trial_sample_params\n\n            self.global_timestamp = 0\n\n            self.current_source_trial = 0\n            self.current_target_trial = 0\n            self.current_trial_mode = 0  # source\n            self.current_episode = 1\n\n            super(RegressionTestAAC, self).__init__(\n                runner_config=self.runner_config,\n                aux_render_modes=aux_render_modes,\n                name=name,\n                **kwargs\n            )\n        except:\n            msg = \'{}.__init()__ exception occurred\'.format(name) + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def _make_loss(self, pi, pi_prime, name=\'base\', verbose=True, **kwargs):\n        """"""\n        Defines policy state encoder regression loss, placeholders and summaries.\n\n        Args:\n            pi:                 policy network obj.\n            pi_prime:           optional policy network obj.\n            name:               str, name scope\n            verbose:            summary level\n\n        Returns:\n            tensor holding estimated loss graph\n            list of related summaries\n        """"""\n        with tf.name_scope(name):\n            # On-policy AAC loss definition:\n            pi.on_pi_act_target = tf.placeholder(\n                tf.float32, [None, self.ref_env.action_space.one_hot_depth], name=""on_policy_action_pl""\n            )\n            pi.on_pi_adv_target = tf.placeholder(tf.float32, [None], name=""on_policy_advantage_pl"")\n            pi.on_pi_r_target = tf.placeholder(tf.float32, [None], name=""on_policy_return_pl"")\n\n            # clip_epsilon = tf.cast(self.clip_epsilon * self.learn_rate_decayed / self.opt_learn_rate, tf.float32)\n            #\n            # on_pi_loss, on_pi_summaries = self.on_policy_loss(\n            #     act_target=pi.on_pi_act_target,\n            #     adv_target=pi.on_pi_adv_target,\n            #     r_target=pi.on_pi_r_target,\n            #     pi_logits=pi.on_logits,\n            #     pi_vf=pi.on_vf,\n            #     pi_prime_logits=pi_prime.on_logits,\n            #     entropy_beta=self.model_beta,\n            #     epsilon=clip_epsilon,\n            #     name=\'on_policy\',\n            #     verbose=verbose\n            # )\n            pi_regression = tf.exp(pi.regression)\n            regress_loss = tf.losses.mean_squared_error(\n                labels=pi.regression_targets,\n                predictions=pi_regression,\n                weights=self.regress_lambda,\n            )\n\n            self.mse = tf.metrics.mean_squared_error(\n                labels=pi.regression_targets,\n                predictions=pi_regression\n            )\n\n            model_summaries = [\n                tf.summary.scalar(\'regress_loss\', regress_loss),\n                tf.summary.scalar(\'mse_metric\', self.mse[0])\n            ]\n            # Accumulate total loss:\n            # loss = float(self.class_lambda) * regress_loss + float(self.aac_lambda) * on_pi_loss\\\n            #     - float(self.model_beta) * entropy\n\n            #model_summaries += on_pi_summaries\n\n            loss = regress_loss\n\n            return loss, model_summaries\n\n    def _make_train_op(self, pi, pi_prime, pi_global):\n        """"""\n        Defines training op graph and supplementary sync operations.\n\n        Args:\n            pi:                 policy network obj.\n            pi_prime:           optional policy network obj.\n            pi_global:          shared policy network obj. hosted by parameter server\n\n        Returns:\n            tensor holding training op graph;\n        """"""\n\n        # Each worker gets a different set of adam optimizer parameters:\n        self.optimizer = tf.train.AdamOptimizer(self.train_learn_rate, epsilon=1e-5)\n\n        # Clipped gradients:\n        self.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.loss, pi.var_list),\n            40.0\n        )\n        self.grads_global_norm = tf.global_norm(self.grads)\n\n        # Copy weights from the parameter server to the local model:\n        self.sync = self.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_global.var_list)]\n        )\n        if self.use_target_policy:\n            # Copy weights from new policy model to target one:\n            self.sync_pi_prime = tf.group(\n                *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi.var_list)]\n            )\n        grads_and_vars = list(zip(self.grads, pi_global.var_list))\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(pi.on_state_in.keys())\n\n        required_modes = [\'external\', \'regression_targets\', \'metadata\']\n        for mode in required_modes:\n            assert mode in obs_space_keys, \\\n                \'Expected observation space to contain `{}` mode, got: {}\'.format(mode, obs_space_keys)\n\n        if isinstance(pi.on_state_in[\'external\'], dict):\n            stream = pi.on_state_in[\'external\'][list(pi.on_state_in[\'external\'].keys())[0]]\n        else:\n            stream = pi.on_state_in[\'external\']\n        self.inc_step = self.global_step.assign_add(tf.shape(stream)[0])\n\n        train_op = [self.optimizer.apply_gradients(grads_and_vars), self.mse]\n\n        self.log.debug(\'train_op defined\')\n        return train_op\n\n    def _combine_summaries(self, policy=None, model_summaries=None):\n        """"""\n        Defines model-wide and episode-related summaries\n\n        Returns:\n            model_summary op\n            episode_summary op\n        """"""\n        if model_summaries is not None:\n            if self.use_global_network:\n                # Model-wide statistics:\n                with tf.name_scope(\'model\'):\n                    model_summaries += [\n                        tf.summary.scalar(""grad_global_norm"", self.grads_global_norm),\n                        tf.summary.scalar(""learn_rate"", self.learn_rate_decayed),\n                        # cause actual rate is a jaggy due to test freezes\n                        tf.summary.scalar(""total_loss"", self.loss),\n                    ]\n                    if policy is not None:\n                        model_summaries += [tf.summary.scalar(""var_global_norm"", tf.global_norm(policy.var_list))]\n        else:\n            model_summaries = []\n        # Model stat. summary:\n        model_summary = tf.summary.merge(model_summaries, name=\'model_summary\')\n\n        # Episode-related summaries:\n        ep_summary = dict(\n            # Summary placeholders\n            render_atari=tf.placeholder(tf.uint8, [None, None, None, 1]),\n            total_r=tf.placeholder(tf.float32, ),\n            cpu_time=tf.placeholder(tf.float32, ),\n            final_value=tf.placeholder(tf.float32, ),\n            steps=tf.placeholder(tf.int32, ),\n        )\n        if self.test_mode:\n            # For Atari:\n            ep_summary[\'render_op\'] = tf.summary.image(""model/state"", ep_summary[\'render_atari\'])\n\n        else:\n            # BTGym rendering:\n            ep_summary.update(\n                {\n                    mode: tf.placeholder(tf.uint8, [None, None, None, None], name=mode + \'_pl\')\n                    for mode in self.env_list[0].render_modes + self.aux_render_modes\n                }\n            )\n            ep_summary[\'render_op\'] = tf.summary.merge(\n                [tf.summary.image(mode, ep_summary[mode])\n                 for mode in self.env_list[0].render_modes + self.aux_render_modes]\n            )\n        # Episode stat. summary:\n        ep_summary[\'btgym_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode_train/cpu_time_sec\', ep_summary[\'cpu_time\']),\n                tf.summary.scalar(\'episode_train/total_reward\', ep_summary[\'total_r\']),\n            ],\n            name=\'episode_train_btgym\'\n        )\n        # Test episode stat. summary:\n        ep_summary[\'test_btgym_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode_test/total_reward\', ep_summary[\'total_r\']),\n            ],\n            name=\'episode_test_btgym\'\n        )\n        ep_summary[\'atari_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode/total_reward\', ep_summary[\'total_r\']),\n                tf.summary.scalar(\'episode/steps\', ep_summary[\'steps\'])\n            ],\n            name=\'episode_atari\'\n        )\n        self.log.debug(\'model-wide and episode summaries ok.\')\n        return model_summary, ep_summary\n\n    def process(self, sess, **kwargs):\n        try:\n            sess.run(self.sync_pi)\n            # Get data configuration:\n            data_config = self.get_sample_config()\n\n            self.process_train(sess, data_config)\n\n        except:\n            msg = \'process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def process_train(self, sess, data_config):\n        data = {}\n        done = False\n        # Set source episode to be sampled uniformly from test interval:\n        data_config[\'trial_config\'][\'align_left\'] = 0\n        # self.log.warning(\'train episode started...\')\n\n        while not done:\n            wirte_model_summary = \\\n                self.local_steps % self.model_summary_freq == 0\n\n            data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config\n            )\n            done = np.asarray(data[\'terminal\']).any()\n            feed_dict = self.process_data(sess, data, is_train=True, pi=self.local_network)\n\n            if wirte_model_summary:\n                fetches = [self.train_op, self.model_summary_op, self.inc_step]\n            else:\n                fetches = [self.train_op, self.inc_step]\n\n            fetched = sess.run(fetches, feed_dict=feed_dict)\n\n            if wirte_model_summary:\n                model_summary = fetched[-2]\n\n            else:\n                model_summary = None\n\n            self.process_summary(sess, data, model_summary)\n\n            self.local_steps += 1\n\n        self.log.info(\n            \'train episode finished at {} vs was_global_time: {}\'.format(\n                data[\'on_policy\'][0][\'info\'][\'time\'][-1],\n                datetime.datetime.fromtimestamp(data[\'on_policy\'][0][\'state\'][\'metadata\'][\'timestamp\'][-1])\n\n            )\n        )\n'"
btgym/research/encoder_test/networks.py,9,"b'import numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.rnn as rnn\nfrom tensorflow.contrib.layers import layer_norm as norm_layer\nfrom tensorflow.python.util.nest import flatten as flatten_nested\n\nfrom btgym.algorithms.nn.layers import normalized_columns_initializer, categorical_sample\nfrom btgym.algorithms.nn.layers import linear, noisy_linear, conv2d, deconv2d, conv1d\nfrom btgym.algorithms.utils import rnn_placeholders\n\n\ndef conv_2d_network_skip(x,\n                    ob_space,\n                    ac_space,\n                    conv_2d_layer_ref=conv2d,\n                    conv_2d_num_filters=(32, 32, 64, 64),\n                    conv_2d_filter_size=(3, 3),\n                    conv_2d_stride=(2, 2),\n                    pad=""SAME"",\n                    dtype=tf.float32,\n                    name=\'conv2d\',\n                    collections=None,\n                    reuse=False,\n                    keep_prob=None,\n                    conv_2d_gated=True,\n                    conv_2d_enable_skip=False,\n                    **kwargs):\n    """"""\n    Convolution encoder with gated output\n\n    Returns:\n        tensor holding state features;\n    """"""\n    assert conv_2d_num_filters[-1] % 2 == 0\n    layers = []\n    with tf.variable_scope(name, reuse=reuse):\n        for i, num_filters in enumerate(conv_2d_num_filters):\n            x = tf.nn.elu(\n                norm_layer(\n                    conv_2d_layer_ref(\n                        x,\n                        num_filters,\n                        ""_layer_{}"".format(i + 1),\n                        conv_2d_filter_size,\n                        conv_2d_stride,\n                        pad,\n                        dtype,\n                        collections,\n                        reuse\n                    ),\n                    scope=name + ""_norm_layer_{}"".format(i + 1)\n                )\n            )\n            if keep_prob is not None:\n                x = tf.nn.dropout(x, keep_prob=keep_prob, name=""_layer_{}_with_dropout"".format(i + 1))\n\n            layers.append(x)\n\n        if conv_2d_gated:\n            x = layers.pop(-1)\n            split_size = int(conv_2d_num_filters[-1] / 2)\n            x1 = x[..., :split_size]\n            x2 = x[..., split_size:]\n\n            x = tf.multiply(\n                x1,\n                tf.nn.sigmoid(x2),\n                name=\'gated_conv_output\'\n            )\n            layers.append(x)\n\n            # print(\'{}.shape = {}\'.format(x.name, x.get_shape().as_list()))\n\n        if conv_2d_enable_skip:\n            x = tf.concat([tf.layers.flatten(l) for l in layers], axis=-1, name=\'flattened_encoded_state\')\n\n        # print(\'{}.shape = {}\'.format(x.name, x.get_shape().as_list()))\n        return x\n\n\ndef identity_encoder(x, name=\'identity_encoder\', **kwargs):\n    """"""\n    Identity plug\n\n    Returns:\n        tensor holding state features;\n    """"""\n    with tf.variable_scope(name,):\n        x = tf.layers.flatten(x)\n\n        return x'"
btgym/research/encoder_test/policy.py,42,"b'from btgym.algorithms.policy.base import BaseAacPolicy\nfrom btgym.algorithms.nn.networks import *\nfrom btgym.algorithms.utils import *\nfrom btgym.spaces import DictSpace, ActionDictSpace\n\nfrom btgym.algorithms.math_utils import sample_dp, softmax\n\n\nclass RegressionTestPolicy(BaseAacPolicy):\n    """"""\n    Simplified LSTM policy (off-policy training excluded, regression heads added)\n    TODO: remove last_action, last_reward terms\n    """"""\n\n    def __init__(self,\n                 ob_space,\n                 ac_space,\n                 rp_sequence_size=4,\n                 state_encoder_class_ref=conv_2d_network,\n                 lstm_class_ref=tf.contrib.rnn.LayerNormBasicLSTMCell,\n                 lstm_layers=(256, 256),\n                 linear_layer_ref=noisy_linear,\n                 share_encoder_params=False,\n                 dropout_keep_prob=1.0,\n                 action_dp_alpha=200.0,\n                 aux_estimate=False,\n                 encode_internal_state=False,\n                 static_rnn=False,\n                 shared_p_v=False,\n                 lstm_2_init_period=50,\n                 regression_type=\'simple\',\n                 **kwargs):\n        """"""\n        Defines [partially shared] on/policy networks for estimating  action-logits, value function,\n        reward and state \'pixel_change\' predictions.\n        Expects multi-modal observation as array of shape `ob_space`.\n\n        Args:\n            ob_space:               instance of btgym.spaces.DictSpace\n            ac_space:               instance of btgym.spaces.ActionDictSpace\n            rp_sequence_size:       reward prediction sample length\n            lstm_class_ref:         tf.nn.lstm class to use\n            lstm_layers:            tuple of LSTM layers sizes\n            linear_layer_ref:       linear layer class to use\n            share_encoder_params:   bool, whether to share encoder parameters for every \'external\' data stream\n            dropout_keep_prob:      in (0, 1] dropout regularisation parameter\n            action_dp_alpha:\n            aux_estimate:           (bool), if True - add auxiliary tasks estimations to self.callbacks dictionary\n            encode_internal_state:  use encoder over \'internal\' part of observation space\n            static_rnn:             (bool), it True - use static rnn graph, dynamic otherwise\n            lstm_2_init_period:     int, RL2 single trial size\n            regression_type:        str, regression type (currently: \'simple\', \'rnn\')\n            **kwargs                not used\n        """"""\n\n        assert isinstance(ob_space, DictSpace), \\\n            \'Expected observation space be instance of btgym.spaces.DictSpace, got: {}\'.format(ob_space)\n        self.ob_space = ob_space\n\n        assert isinstance(ac_space, ActionDictSpace), \\\n            \'Expected action space be instance of btgym.spaces.ActionDictSpace, got: {}\'.format(ac_space)\n\n        self.ac_space = ac_space\n\n        # self.rp_sequence_size = rp_sequence_size\n        self.state_encoder_class_ref = state_encoder_class_ref\n        self.lstm_class = lstm_class_ref\n        self.lstm_layers = lstm_layers\n        self.action_dp_alpha = action_dp_alpha\n        self.aux_estimate = aux_estimate\n        self.callback = {}\n        self.encode_internal_state = encode_internal_state\n        self.share_encoder_params = share_encoder_params\n        if self.share_encoder_params:\n            self.reuse_encoder_params = tf.AUTO_REUSE\n\n        else:\n            self.reuse_encoder_params = False\n        self.static_rnn = static_rnn\n        self.dropout_keep_prob = dropout_keep_prob\n        assert 0 < self.dropout_keep_prob <= 1, \'Dropout keep_prob value should be in (0, 1]\'\n\n        self.regression_type = regression_type\n\n        self.debug = {}\n\n        # Placeholders for obs. state input:\n        self.on_state_in = nested_placeholders(self.ob_space.shape, batch_dim=None, name=\'on_policy_state_in\')\n\n        # Placeholders for previous step action[multi-categorical vector encoding]  and reward [scalar]:\n        self.on_last_a_in = tf.placeholder(\n            tf.float32,\n            [None, self.ac_space.encoded_depth],\n            name=\'on_policy_last_action_in_pl\'\n        )\n        self.on_last_reward_in = tf.placeholder(tf.float32, [None], name=\'on_policy_last_reward_in_pl\')\n\n        # Placeholders for rnn batch and time-step dimensions:\n        self.on_batch_size = tf.placeholder(tf.int32, name=\'on_policy_batch_size\')\n        self.on_time_length = tf.placeholder(tf.int32, name=\'on_policy_sequence_size\')\n\n        self.debug[\'on_state_in_keys\'] = list(self.on_state_in.keys())\n\n        assert \'regression_targets\' in self.on_state_in.keys(), \'Obs. space should provide regression targets.\'\n        self.regression_targets = self.on_state_in[\'regression_targets\']\n        self.debug[\'regression_targets\'] = self.regression_targets\n\n        # Dropout related:\n        try:\n            if self.train_phase is not None:\n                pass\n\n        except AttributeError:\n            self.train_phase = tf.placeholder_with_default(\n                tf.constant(False, dtype=tf.bool),\n                shape=(),\n                name=\'train_phase_flag_pl\'\n            )\n        self.keep_prob = 1.0 - (1.0 - self.dropout_keep_prob) * tf.cast(self.train_phase, tf.float32)\n\n        # Default parameters:\n        default_kwargs = dict(\n            conv_2d_filter_size=[3, 1],\n            conv_2d_stride=[2, 1],\n            conv_2d_num_filters=[32, 32, 64, 64],\n            pc_estimator_stride=[2, 1],\n            duell_pc_x_inner_shape=(6, 1, 32),  # [6,3,32] if swapping W-C dims\n            duell_pc_filter_size=(4, 1),\n            duell_pc_stride=(2, 1),\n            keep_prob=self.keep_prob,\n        )\n        # Insert if not already:\n        for key, default_value in default_kwargs.items():\n            if key not in kwargs.keys():\n                kwargs[key] = default_value\n\n        # Base on-policy AAC network:\n\n        # Separately encode than concatenate all `external` states modes, jointly encode every stream within mode:\n        self.on_aac_x_encoded = {}\n        for key in self.on_state_in.keys():\n            if \'external\' in key:\n                if isinstance(self.on_state_in[key], dict):  # got dictionary of data streams\n                    if self.share_encoder_params:\n                        layer_name_template = \'encoded_{}_shared\'\n                    else:\n                        layer_name_template = \'encoded_{}_{}\'\n                    encoded_streams = {\n                        name: tf.layers.flatten(\n                            self.state_encoder_class_ref(\n                                x=stream,\n                                ob_space=self.ob_space.shape[key][name],\n                                ac_space=self.ac_space,\n                                name=layer_name_template.format(key, name),\n                                reuse=self.reuse_encoder_params,  # shared params for all streams in mode\n                                **kwargs\n                            )\n                        )\n                        for name, stream in self.on_state_in[key].items()\n                    }\n                    encoded_mode = tf.concat(\n                        list(encoded_streams.values()),\n                        axis=-1,\n                        name=\'multi_encoded_{}\'.format(key)\n                    )\n                else:\n                    # Got single data stream:\n                    encoded_mode = tf.layers.flatten(\n                        self.state_encoder_class_ref(\n                            x=self.on_state_in[key],\n                            ob_space=self.ob_space.shape[key],\n                            ac_space=self.ac_space,\n                            name=\'encoded_{}\'.format(key),\n                            **kwargs\n                        )\n                    )\n                self.on_aac_x_encoded[key] = encoded_mode\n\n        self.debug[\'on_state_external_encoded_dict\'] = self.on_aac_x_encoded\n\n        on_aac_x = tf.concat(list(self.on_aac_x_encoded.values()), axis=-1, name=\'on_state_external_encoded\')\n\n        self.debug[\'on_state_external_encoded\'] = on_aac_x\n\n        # TODO: for encoder prediction test, output `naive` estimates for logits and value directly from encoder:\n        [self.on_simple_logits, self.on_simple_value, _] = dense_aac_network(\n            tf.layers.flatten(on_aac_x),\n            ac_space_depth=self.ac_space.one_hot_depth,\n            linear_layer_ref=linear_layer_ref,\n            name=\'aac_dense_simple_pi_v\'\n        )\n\n        # Reshape rnn inputs for batch training as: [rnn_batch_dim, rnn_time_dim, flattened_depth]:\n        x_shape_dynamic = tf.shape(on_aac_x)\n        max_seq_len = tf.cast(x_shape_dynamic[0] / self.on_batch_size, tf.int32)\n        x_shape_static = on_aac_x.get_shape().as_list()\n\n        on_last_action_in = tf.reshape(\n            self.on_last_a_in,\n            [self.on_batch_size, max_seq_len, self.ac_space.encoded_depth]\n        )\n        on_last_r_in = tf.reshape(self.on_last_reward_in, [self.on_batch_size, max_seq_len, 1])\n\n        on_aac_x = tf.reshape(on_aac_x, [self.on_batch_size, max_seq_len, np.prod(x_shape_static[1:])])\n\n        # Prepare `internal` state, if any:\n        if \'internal\' in list(self.on_state_in.keys()):\n            if self.encode_internal_state:\n                # Use convolution encoder:\n                on_x_internal = self.state_encoder_class_ref(\n                    x=self.on_state_in[\'internal\'],\n                    ob_space=self.ob_space.shape[\'internal\'],\n                    ac_space=self.ac_space,\n                    name=\'encoded_internal\',\n                    **kwargs\n                )\n                x_int_shape_static = on_x_internal.get_shape().as_list()\n                on_x_internal = [\n                    tf.reshape(on_x_internal, [self.on_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])])]\n                self.debug[\'on_state_internal_encoded\'] = on_x_internal\n\n            else:\n                # Feed as is:\n                x_int_shape_static = self.on_state_in[\'internal\'].get_shape().as_list()\n                on_x_internal = tf.reshape(\n                    self.on_state_in[\'internal\'],\n                    [self.on_batch_size, max_seq_len, np.prod(x_int_shape_static[1:])]\n                )\n                self.debug[\'on_state_internal_encoded\'] = on_x_internal\n                on_x_internal = [on_x_internal]\n\n        else:\n            on_x_internal = []\n\n        # Prepare datetime index if any:\n        if \'datetime\' in list(self.on_state_in.keys()):\n            x_dt_shape_static = self.on_state_in[\'datetime\'].get_shape().as_list()\n            on_x_dt = tf.reshape(\n                self.on_state_in[\'datetime\'],\n                [self.on_batch_size, max_seq_len, np.prod(x_dt_shape_static[1:])]\n            )\n            on_x_dt = [on_x_dt]\n\n        else:\n            on_x_dt = []\n\n        self.debug[\'on_state_dt_encoded\'] = on_x_dt\n        self.debug[\'conv_input_to_lstm1\'] = on_aac_x\n\n        # Feed last last_reward into LSTM_1 layer along with encoded `external` state features and datetime stamp:\n        # on_stage2_1_input = [on_aac_x, on_last_action_in, on_last_reward_in] + on_x_dt\n        on_stage2_1_input = [on_aac_x, on_last_r_in] #+ on_x_dt\n\n        # Feed last_action, encoded `external` state,  `internal` state, datetime stamp into LSTM_2:\n        # on_stage2_2_input = [on_aac_x, on_last_action_in, on_last_reward_in] + on_x_internal + on_x_dt\n        on_stage2_2_input = [on_aac_x, on_last_action_in] + on_x_internal #+ on_x_dt\n\n        # LSTM_1 full input:\n        on_aac_x = tf.concat(on_stage2_1_input, axis=-1)\n\n        self.debug[\'concat_input_to_lstm1\'] = on_aac_x\n\n        # First LSTM layer takes encoded `external` state:\n        [on_x_lstm_1_out, self.on_lstm_1_init_state, self.on_lstm_1_state_out, self.on_lstm_1_state_pl_flatten] =\\\n            lstm_network(\n                x=on_aac_x,\n                lstm_sequence_length=self.on_time_length,\n                lstm_class=lstm_class_ref,\n                lstm_layers=(lstm_layers[0],),\n                static=static_rnn,\n                dropout_keep_prob=self.dropout_keep_prob,\n                name=\'lstm_1\'\n            )\n\n        # var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n        # print(\'var_list: \', var_list)\n\n        self.debug[\'on_x_lstm_1_out\'] = on_x_lstm_1_out\n        self.debug[\'self.on_lstm_1_state_out\'] = self.on_lstm_1_state_out\n        self.debug[\'self.on_lstm_1_state_pl_flatten\'] = self.on_lstm_1_state_pl_flatten\n\n        # For time_flat only: Reshape on_lstm_1_state_out from [1,2,20,size] -->[20,1,2,size] --> [20,1, 2xsize]:\n        reshape_lstm_1_state_out = tf.transpose(self.on_lstm_1_state_out, [2, 0, 1, 3])\n        reshape_lstm_1_state_out_shape_static = reshape_lstm_1_state_out.get_shape().as_list()\n\n        # Take policy logits off first LSTM-dense layer:\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = on_x_lstm_1_out.get_shape().as_list()\n        rsh_on_x_lstm_1_out = tf.reshape(on_x_lstm_1_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        self.debug[\'reshaped_on_x_lstm_1_out\'] = rsh_on_x_lstm_1_out\n\n        if not shared_p_v:\n            # Aac policy output and action-sampling function:\n            [self.on_logits, _, self.on_sample] = dense_aac_network(\n                rsh_on_x_lstm_1_out,\n                ac_space_depth=self.ac_space.one_hot_depth,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_pi\'\n            )\n\n        # Second LSTM layer takes concatenated encoded \'external\' state, LSTM_1 output,\n        # last_action and `internal_state` (if present) tensors:\n        on_stage2_2_input += [on_x_lstm_1_out]\n\n        # Try: feed context instead of output\n        #on_stage2_2_input = [reshape_lstm_1_state_out] + on_stage2_1_input\n\n        # LSTM_2 full input:\n        on_aac_x = tf.concat(on_stage2_2_input, axis=-1)\n\n        self.debug[\'on_stage2_2_input\'] = on_aac_x\n\n        [on_x_lstm_2_out, self.on_lstm_2_init_state, self.on_lstm_2_state_out, self.on_lstm_2_state_pl_flatten] = \\\n            lstm_network(\n                x=on_aac_x,\n                lstm_sequence_length=self.on_time_length,\n                lstm_class=lstm_class_ref,\n                lstm_layers=(lstm_layers[-1],),\n                static=static_rnn,\n                dropout_keep_prob=self.dropout_keep_prob,\n                name=\'lstm_2\'\n            )\n\n        self.debug[\'on_x_lstm_2_out\'] = on_x_lstm_2_out\n        self.debug[\'self.on_lstm_2_state_out\'] = self.on_lstm_2_state_out\n        self.debug[\'self.on_lstm_2_state_pl_flatten\'] = self.on_lstm_2_state_pl_flatten\n\n        # Reshape back to [batch, flattened_depth], where batch = rnn_batch_dim * rnn_time_dim:\n        x_shape_static = on_x_lstm_2_out.get_shape().as_list()\n        rsh_on_x_lstm_2_out = tf.reshape(on_x_lstm_2_out, [x_shape_dynamic[0], x_shape_static[-1]])\n\n        self.debug[\'reshaped_on_x_lstm_2_out\'] = rsh_on_x_lstm_2_out\n\n        if shared_p_v:\n            # Take pi an value fn. estimates off second LSTM layer:\n            [self.on_logits, self.on_vf, self.on_sample] = dense_aac_network(\n                rsh_on_x_lstm_2_out,\n                ac_space_depth=self.ac_space.one_hot_depth,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_pi_vfn\'\n            )\n\n        else:\n            # Take pi off first LSTM layer, an value off second:\n            [_, self.on_vf, _] = dense_aac_network(\n                rsh_on_x_lstm_2_out,\n                ac_space_depth=self.ac_space.one_hot_depth,\n                linear_layer_ref=linear_layer_ref,\n                name=\'aac_dense_vfn\'\n            )\n\n        # Add test regression/ classification heads:\n        if self.regression_type == \'simple\':\n            # Naive regression (off encoder output):\n            # [self.regression, _, _] = dense_aac_network(\n            #     # tf.layers.flatten(self.debug[\'conv_input_to_lstm1\']),\n            #     tf.layers.flatten(self.debug[\'on_state_external_encoded\']),\n            #     # ac_space_depth=self.regression_depth,\n            #     ac_space_depth=self.regression_targets.shape.as_list()[-1],\n            #     linear_layer_ref=linear_layer_ref,\n            #     name=\'on_dense_simple_regression\'\n            # )\n            #self.regression = tf.layers.flatten(self.debug[\'on_state_external_encoded\'])\n            self.regression = linear(\n                x=tf.layers.flatten(self.debug[\'on_state_external_encoded\']),\n                size=self.regression_targets.shape.as_list()[-1],\n                initializer=normalized_columns_initializer(0.1),\n                name=\'on_dense_simple_regression\',\n                reuse=False\n            )\n        elif self.regression_type == \'rnn\':\n            # Context-aware regression (off LSTM bank):\n            # [self.regression, _, _] = dense_aac_network(\n            #     tf.layers.flatten(self.debug[\'reshaped_on_x_lstm_2_out\']),\n            #     # ac_space_depth=self.regression_depth,\n            #     ac_space_depth=self.regression_targets.shape.as_list()[-1],\n            #     linear_layer_ref=linear_layer_ref,\n            #     name=\'on_dense_rnn_regression\'\n            # )\n            self.regression = linear(\n                x=tf.layers.flatten(self.debug[\'reshaped_on_x_lstm_2_out\']),\n                size=self.regression_targets.shape.as_list()[-1],\n                initializer=normalized_columns_initializer(0.1),\n                name=\'on_dense_rnn_regression\',\n                reuse=False\n            )\n        else:\n            raise NotImplementedError(\'Unknown regression type `{}`\'.format(self.regression_type))\n\n        # Concatenate LSTM placeholders, init. states and context:\n        self.on_lstm_init_state = (self.on_lstm_1_init_state, self.on_lstm_2_init_state)\n        self.on_lstm_state_out = (self.on_lstm_1_state_out, self.on_lstm_2_state_out)\n        self.on_lstm_state_pl_flatten = self.on_lstm_1_state_pl_flatten + self.on_lstm_2_state_pl_flatten\n\n        # Batch-norm related:\n        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        # Add moving averages to save list:\n        moving_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name + \'.*moving.*\')\n        renorm_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name + \'.*renorm.*\')\n\n        # What to save:\n        self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n        self.var_list += moving_var_list + renorm_var_list\n\n        # RL2 related:\n        self.current_trial_num = -1  # always give initial context at first call\n        self.lstm_2_init_period = lstm_2_init_period\n        self.current_ep_num = 0\n\n    def get_initial_features(self, state, context=None):\n        """"""\n        Returns RNN initial context.\n        RNN_1 (lower) context is reset at every call.\n\n        RNN_2 (upper) context is reset:\n            - every `lstm_2_init_period\' episodes;\n            - episode  initial `state` `trial_num` metadata has been changed form last call (new train trial started);\n            - episode metatdata `type` is non-zero (test episode);\n            - no context arg is provided (initial episode of training);\n            - ... else carries context on to new episode;\n\n        Episode metadata are provided by DataTrialIterator, which is shaping Trial data distribution in this case,\n        and delivered through env.strategy as separate key in observation dictionary.\n\n        Args:\n            state:      initial episode state (result of env.reset())\n            context:    last previous episode RNN state (last_context of runner)\n\n        Returns:\n            2_RNN zero-state tuple.\n\n        Raises:\n            KeyError if [`metadata`]:[`trial_num`,`type`] keys not found\n        """"""\n        try:\n            sess = tf.get_default_session()\n            new_context = list(sess.run(self.on_lstm_init_state))\n            if state[\'metadata\'][\'trial_num\'] != self.current_trial_num\\\n                    or context is None\\\n                    or state[\'metadata\'][\'type\']\\\n                    or self.current_ep_num % self.lstm_2_init_period == 0:\n                # Assume new/initial trial or test sample, reset_1, 2 context:\n                pass #print(\'RL^2 policy context 1, 2 reset\')\n\n            else:\n                # Asssume same training trial, keep context_2 same as received:\n                new_context[-1] = context[-1]\n                #print(\'RL^2 policy context 1, reset\')\n            # Back to tuple:\n            new_context = tuple(new_context)\n            # Keep trial number:\n            self.current_trial_num = state[\'metadata\'][\'trial_num\']\n\n        except KeyError:\n            raise KeyError(\n                \'RL^2 policy: expected observation state dict. to have keys [`metadata`]:[`trial_num`,`type`]; got: {}\'.\n                format(state.keys())\n            )\n        self.current_ep_num +=1\n        return new_context\n\n    def act(self, observation, lstm_state, last_action, last_reward):\n        """"""\n        Predicts action.\n\n        Args:\n            observation:    dictionary containing single observation\n            lstm_state:     lstm context value\n            last_action:    action value from previous step\n            last_reward:    reward value previous step\n\n        Returns:\n            Action as dictionary of several action encodings, actions logits, V-fn value, output RNN state\n        """"""\n        sess = tf.get_default_session()\n        feeder = {pl: value for pl, value in zip(self.on_lstm_state_pl_flatten, flatten_nested(lstm_state))}\n        feeder.update(feed_dict_from_nested(self.on_state_in, observation, expand_batch=True))\n        feeder.update(\n            {\n                self.on_last_a_in: last_action,\n                self.on_last_reward_in: last_reward,\n                self.on_batch_size: 1,\n                self.on_time_length: 1,\n                self.train_phase: False\n            }\n        )\n        # action_one_hot, logits, value, context = sess.run(\n        #     [self.on_sample, self.on_logits, self.on_vf, self.on_lstm_state_out],\n        #     feeder\n        # )\n        # return action_one_hot, logits, value, context\n        logits, value, context, regression = sess.run(\n            [self.on_logits, self.on_vf, self.on_lstm_state_out, self.regression],\n            feeder\n        )\n        logits = logits[0, ...]\n        if self.ac_space.is_discrete:\n            # Use multinomial to get sample (discrete):\n            sample = np.random.multinomial(1, softmax(logits))\n            sample = self.ac_space._cat_to_vec(np.argmax(sample))\n\n        else:\n            # Use DP to get sample (continuous):\n            sample = sample_dp(logits, alpha=self.action_dp_alpha)\n\n        # Get all needed action encodings:\n        action = self.ac_space._vec_to_action(sample)\n        one_hot = self.ac_space._vec_to_one_hot(sample)\n        action_pack = {\n            \'environment\': action,\n            \'encoded\': self.ac_space.encode(action),\n            \'one_hot\': one_hot,\n        }\n        # print(\'action_pack: \', action_pack)\n\n        return action_pack, logits, value, context, regression\n\n'"
btgym/research/encoder_test/runner.py,0,"b'import numpy as np\n\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\nfrom btgym.algorithms.math_utils import softmax\n\n\nclass RegressionRunner(BaseSynchroRunner):\n    """"""\n    Runner with additional regression functionality.\n    """"""\n\n    def __init__(self, **kwargs):\n\n        super(RegressionRunner, self).__init__(\n            _implemented_aux_render_modes=(\n                \'regression\',\n                \'regression_targets\',\n                \'action_prob\',\n                \'value_fn\',\n                \'lstm_1_h\',\n                \'lstm_2_h\'\n            ),\n            **kwargs\n        )\n\n    def get_init_experience(self, policy, policy_sync_op=None, init_context=None, data_sample_config=None):\n        """"""\n        Starts new environment episode.\n\n        Args:\n            policy:                 policy to execute.\n            policy_sync_op:         operation copying local behavioural policy params from global one\n            init_context:           initial policy context for new episode.\n            data_sample_config:     configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n\n        Returns:\n            incomplete initial experience of episode as dictionary (misses bootstrapped R value),\n            next_state,\n            next, policy RNN context\n            action_reward\n        """"""\n        self.length = 0\n        self.reward_sum = 0\n        # Increment global and local episode counters:\n        self.sess.run(self.policy.inc_episode)\n        self.local_episode += 1\n\n        # self.log.warning(\'get_init_exp() data_sample_config: {}\'.format(data_sample_config))\n\n        if data_sample_config is None:\n            data_sample_config = policy.get_sample_config()\n\n        # Pass sample config to environment (.get_sample_config() is actually aac framework method):\n        init_state = self.env.reset(**data_sample_config)\n\n        # Master worker always resets context at the episode beginning:\n        # TODO: !\n        # if not self.data_sample_config[\'mode\']:\n        init_context = None\n\n        # self.log.warning(\'init_context_passed: {}\'.format(init_context))\n        # self.log.warning(\'state_metadata: {}\'.format(state[\'metadata\']))\n\n        init_action = self.env.action_space.encode(self.env.get_initial_action())\n        init_reward = np.asarray(0.0)\n\n        # Update policy:\n        if policy_sync_op is not None:\n            self.sess.run(policy_sync_op)\n\n        init_context = policy.get_initial_features(state=init_state, context=init_context)\n        action, logits, value, next_context, init_regression = policy.act(\n            init_state,\n            init_context,\n            init_action[None, ...],\n            init_reward[None, ...],\n        )\n        next_state, reward, terminal, self.info = self.env.step(action[\'environment\'])\n\n        experience = {\n            \'position\': {\'episode\': self.local_episode, \'step\': self.length},\n            \'state\': init_state,\n            \'action\': action[\'one_hot\'],\n            \'reward\': reward,\n            \'value\': value,\n            \'terminal\': terminal,\n            \'context\': init_context,\n            \'last_action\': init_action,\n            \'last_reward\': init_reward,\n            \'r\': None,  # to be updated\n            \'info\': self.info[-1],\n        }\n        # Execute user-defined callbacks to policy, if any:\n        for key, callback in policy.callback.items():\n            experience[key] = callback(**locals())\n\n        # reset per-episode  counters and accumulators:\n        self.ep_accum = {\n            \'logits\': [logits],\n            \'value\': [value],\n            \'context\': [init_context],\n            \'regression\': [init_regression],\n            \'regression_targets\': [init_state[\'regression_targets\']],\n        }\n        self.terminal_end = terminal\n        #self.log.warning(\'init_experience_context: {}\'.format(context))\n\n        # Take a nap:\n        self.sleep()\n\n        return experience, next_state, next_context, action[\'encoded\'], reward\n\n    def get_experience(self, policy, state, context, action, reward, policy_sync_op=None):\n        """"""\n        Get single experience (possibly terminal).\n\n        Returns:\n            incomplete experience as dictionary (misses bootstrapped R value),\n            next_state,\n            next, policy RNN context\n            action_reward\n        """"""\n        # Continue adding experiences to rollout:\n        next_action, logits, value, next_context, regression = policy.act(\n            state,\n            context,\n            action[None, ...],\n            reward[None, ...],\n        )\n\n        self.ep_accum[\'logits\'].append(logits)\n        self.ep_accum[\'value\'].append(value)\n        self.ep_accum[\'context\'].append(next_context)\n        self.ep_accum[\'regression\'].append(regression)\n        self.ep_accum[\'regression_targets\'].append(state[\'regression_targets\'])\n\n        # self.log.notice(\'context: {}\'.format(context))\n        next_state, next_reward, terminal, self.info = self.env.step(next_action[\'environment\'])\n\n        # Partially compose experience:\n        experience = {\n            \'position\': {\'episode\': self.local_episode, \'step\': self.length},\n            \'state\': state,\n            \'action\': next_action[\'one_hot\'],\n            \'reward\': next_reward,\n            \'value\': value,\n            \'terminal\': terminal,\n            \'context\': context,\n            \'last_action\': action,\n            \'last_reward\': reward,\n            \'r\': None,\n            \'info\': self.info[-1],\n        }\n        for key, callback in policy.callback.items():\n            experience[key] = callback(**locals())\n\n        # Housekeeping:\n        self.length += 1\n\n        # Take a nap:\n        # self.sleep()\n\n        return experience, next_state, next_context, next_action[\'encoded\'], next_reward\n\n    def get_ep_render(self, is_test=False):\n        """"""\n        Visualises episode environment and policy statistics.\n        Relies on environmnet renderer class methods,\n        so it is only valid when environment rendering is enabled (typically it is true for master runner).\n\n        Returns:\n            dictionary of images as rgb arrays\n\n        """"""\n        # Only render chief worker and test (slave) environment:\n        # if self.task < 1 and (\n        #     is_test or(\n        #         self.local_episode % self.env_render_freq == 0 and not self.data_sample_config[\'mode\']\n        #     )\n        # ):\n        if self.task < 1 and self.local_episode % self.env_render_freq == 0:\n\n            # Render environment (chief worker only):\n            render_stat = {\n                mode: self.env.render(mode)[None, :] for mode in self.env.render_modes\n            }\n            # Update renderings with aux:\n\n            # ep_a_logits = self.ep_accum[\'logits\']\n            # ep_value = self.ep_accum[\'value\']\n            # ep_regression = self.ep_accum[\'regression\']\n            # self.log.notice(\'ep_logits shape: {}\'.format(np.asarray(ep_a_logits).shape))\n            # self.log.notice(\'ep_value shape: {}\'.format(np.asarray(ep_value).shape))\n            # self.log.notice(\'ep_regression shape: {}\'.format(np.asarray(ep_regression).shape))\n\n            # ep_regression_targets = self.ep_accum[\'regression_targets\']\n            # self.log.notice(\'ep_regression_targets shape: {}\'.format(np.asarray(ep_regression_targets).shape))\n\n            # Unpack LSTM states:\n            rnn_1, rnn_2 = zip(*self.ep_accum[\'context\'])\n            rnn_1 = [state[0] for state in rnn_1]\n            rnn_2 = [state[0] for state in rnn_2]\n            c1, h1 = zip(*rnn_1)\n            c2, h2 = zip(*rnn_2)\n\n            r = np.exp(np.asarray(self.ep_accum[\'regression\'])[:, 0, :])\n            rt = np.asarray(self.ep_accum[\'regression_targets\'])\n\n            # Render everything implemented (doh!):\n            implemented_aux_images = {\n                \'action_prob\': self.env.renderer.draw_plot(\n                    # data=softmax(np.asarray(ep_a_logits)[:, 0, :] - np.asarray(ep_a_logits).max()),\n                    data=softmax(np.asarray(self.ep_accum[\'logits\'])), #[:, 0, :]),\n                    title=\'Episode actions probabilities\',\n                    figsize=(12, 4),\n                    box_text=\'\',\n                    xlabel=\'Backward env. steps\',\n                    ylabel=\'R+\',\n                    line_labels=[\'Hold\', \'Buy\', \'Sell\', \'Close\']\n                )[None, ...],\n                \'value_fn\': self.env.renderer.draw_plot(\n                    data=np.asarray(self.ep_accum[\'value\']),\n                    title=\'Episode Value function\',\n                    figsize=(12, 4),\n                    xlabel=\'Backward env. steps\',\n                    ylabel=\'R\',\n                    line_labels=[\'Value\']\n                )[None, ...],\n                # \'lstm_1_c\': norm_image(np.asarray(c1).T[None, :, 0, :, None]),\n                \'lstm_1_h\': self.norm_image(np.asarray(h1).T[None, :, 0, :, None]),\n                # \'lstm_2_c\': norm_image(np.asarray(c2).T[None, :, 0, :, None]),\n                \'lstm_2_h\': self.norm_image(np.asarray(h2).T[None, :, 0, :, None]),\n                \'regression\': self.env.renderer.draw_plot(\n                    data=r,\n                    title=\'Regression\',\n                    figsize=(12, 4),\n                    xlabel=\'Backward env. steps\',\n                    ylabel=\'R\',\n                    line_labels=[\'regressed values\'],\n                )[None, ...],\n                \'regression_targets\': self.env.renderer.draw_plot(\n                    data=rt,\n                    title=\'Regression targets\',\n                    figsize=(12, 4),\n                    xlabel=\'Backward env. steps\',\n                    ylabel=\'R\',\n                    line_labels=[\'target values\'],\n                )[None, ...],\n            }\n\n            self.log.notice(\'Mean regression value: {:.6f}, target: {:.6f}, mse: {:.6f}\'.format(r.mean(), rt.mean(), (((r-rt)**2).mean())**0.5))\n\n            # Pick what has been set:\n            aux_images = {summary: implemented_aux_images[summary] for summary in self.aux_render_modes}\n            render_stat.update(aux_images)\n\n        else:\n            render_stat = None\n\n        return render_stat'"
btgym/research/gps/__init__.py,0,b'\n'
btgym/research/gps/aac.py,2,"b'import tensorflow as tf\n\nfrom btgym.algorithms import BaseAAC\nfrom .loss import guided_aac_loss_def_0_0, guided_aac_loss_def_0_1, guided_aac_loss_def_0_3\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner, VerboseSynchroRunner\n\n\nclass GuidedAAC(BaseAAC):\n    """"""\n    Actor-critic framework augmented with expert actions imitation loss:\n    L_gps = aac_lambda * L_a3c + guided_lambda * L_im.\n\n    This implementation is loosely refereed as \'guided policy search\' after algorithm described in paper\n    by S. Levine and P. Abbeel `Learning Neural Network Policies with Guided PolicySearch under Unknown Dynamics`\n\n    in a sense that exploits idea of fitting \'local\' (here - single episode) oracle for environment with\n    generally unknown dynamics and use actions demonstrated by it to optimize trajectory distribution for training agent.\n\n    Note that this particular implementation of expert does not provides\n    complete action-state space trajectory for agent to follow.\n    Instead it estimates `advised` categorical distribution over actions conditioned on `external` (i.e. price dynamics)\n    state observations only.\n\n    Papers:\n        - Levine et al., \'Learning Neural Network Policies with Guided PolicySearch under Unknown Dynamics\'\n            https://people.eecs.berkeley.edu/~svlevine/papers/mfcgps.pdf\n\n        - Brys et al., \'Reinforcement Learning from Demonstration through Shaping\'\n            https://www.ijcai.org/Proceedings/15/Papers/472.pdf\n\n        - Wiewiora et al., \'Principled Methods for Advising Reinforcement Learning Agents\'\n            http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.6412&rep=rep1&type=pdf\n\n    """"""\n    def __init__(\n            self,\n            expert_loss=guided_aac_loss_def_0_3,\n            aac_lambda=1.0,\n            guided_lambda=1.0,\n            guided_decay_steps=None,\n            runner_config=None,\n            # aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            aux_render_modes=None,\n            name=\'GuidedA3C\',\n            **kwargs\n    ):\n        """"""\n\n        Args:\n            expert_loss:        callable returning tensor holding on_policy imitation loss graph and summaries\n            aac_lambda:         float, main on_policy a3c loss lambda\n            guided_lambda:      float, imitation loss lambda\n            guided_decay_steps: number of steps guided_lambda is annealed to zero\n            name:               str, name scope\n            **kwargs:           see BaseAAC kwargs\n        """"""\n        try:\n            self.expert_loss = expert_loss\n            self.aac_lambda = aac_lambda\n            self.guided_lambda = guided_lambda * 1.0\n            self.guided_decay_steps = guided_decay_steps\n            self.guided_lambda_decayed = None\n            self.train_guided_lambda = None\n            if runner_config is None:\n                runner_config = {\n                    \'class_ref\': BaseSynchroRunner,\n                    \'kwargs\': {\n                        \'aux_render_modes\': aux_render_modes,  # (\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n                    }\n                }\n\n            super(GuidedAAC, self).__init__(\n                runner_config=runner_config,\n                name=name,\n                aux_render_modes=aux_render_modes,\n                **kwargs\n            )\n        except:\n            msg = \'GuidedAAC.__init()__ exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def _make_loss(self, **kwargs):\n        """"""\n        Augments base loss with expert actions imitation loss\n\n        Returns:\n            tensor holding estimated loss graph\n            list of related summaries\n        """"""\n        aac_loss, summaries = self._make_base_loss(**kwargs)\n\n        # Guidance annealing:\n        if self.guided_decay_steps is not None:\n            self.guided_lambda_decayed = tf.train.polynomial_decay(\n                self.guided_lambda,\n                self.global_step + 1,\n                self.guided_decay_steps,\n                0.0,\n                power=1,\n                cycle=False,\n            )\n        else:\n            self.guided_lambda_decayed = self.guided_lambda\n        # Switch to zero when testing - prevents information leakage:\n        self.train_guided_lambda = self.guided_lambda_decayed * tf.cast(self.local_network.train_phase, tf.float32)\n\n        self.guided_loss, guided_summary = self.expert_loss(\n            pi_actions=self.local_network.on_logits,\n            expert_actions=self.local_network.expert_actions,\n            name=\'on_policy\',\n            verbose=True,\n            guided_lambda=self.train_guided_lambda\n        )\n        loss = self.aac_lambda * aac_loss + self.guided_loss\n\n        summaries += guided_summary\n\n        self.log.notice(\n            \'guided_lambda: {:1.6f}, guided_decay_steps: {}\'.format(self.guided_lambda, self.guided_decay_steps)\n        )\n\n        return loss, summaries\n\n\nclass VerboseGuidedAAC(GuidedAAC):\n    """"""\n    Extends parent `GuidedAAC` class with additional summaries.\n    """"""\n\n    def __init__(\n            self,\n            runner_config=None,\n            aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            name=\'VerboseGuidedA3C\',\n            **kwargs\n    ):\n        super(VerboseGuidedAAC, self).__init__(\n            name=name,\n            runner_config={\n                    \'class_ref\': VerboseSynchroRunner,\n                    \'kwargs\': {\n                        \'aux_render_modes\': aux_render_modes,\n                    }\n                },\n            aux_render_modes=aux_render_modes,\n            **kwargs\n        )\n\n'"
btgym/research/gps/loss.py,13,"b'import tensorflow as tf\n\n\ndef guided_aac_loss_def_0_0(pi_actions, expert_actions, name=\'on_policy/aac\', verbose=False, **kwargs):\n    """"""\n    Cross-entropy imitation loss on expert actions.\n\n    Args:\n        pi_actions:             tensor holding policy actions logits\n        expert_actions:         tensor holding expert actions probability distribution\n        name:           loss op name scope\n\n    Returns:\n        tensor holding estimated imitation loss;\n        list of related tensorboard summaries.\n    """"""\n    with tf.name_scope(name + \'/guided_loss\'):\n        # Loss over expert action\'s distribution:\n\n        neg_pi_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=pi_actions,\n            labels=tf.argmax(expert_actions, axis=-1)\n        )\n        loss = tf.reduce_mean(neg_pi_log_prob)\n\n        if verbose:\n            summaries = [tf.summary.scalar(\'actions_ce\', loss)]\n        else:\n            summaries = []\n\n    return loss, summaries\n\n\ndef guided_aac_loss_def_0_1(pi_actions, expert_actions, name=\'on_policy/aac\', verbose=False, **kwargs):\n    """"""\n    Cross-entropy imitation loss on {`buy`, `sell`} subset of expert actions.\n\n    Args:\n        pi_actions:             tensor holding policy actions logits\n        expert_actions:         tensor holding expert actions probability distribution\n        name:           loss op name scope\n\n    Returns:\n        tensor holding estimated imitation loss;\n        list of related tensorboard summaries.\n    """"""\n    with tf.name_scope(name + \'/guided_loss\'):\n        # Loss over expert buy/ sell:\n        # Cross-entropy on subset?...\n\n        neg_pi_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=pi_actions[..., 1:3],\n            labels=expert_actions[..., 1:3]\n        )\n        loss = tf.reduce_mean(neg_pi_log_prob)\n\n        if verbose:\n            summaries = [tf.summary.scalar(\'actions_ce\', loss)]\n        else:\n            summaries = []\n\n    return loss, summaries\n\n\ndef guided_aac_loss_def_0_3(pi_actions, expert_actions, name=\'on_policy/aac\', verbose=False, **kwargs):\n    """"""\n    MSE imitation loss on {`buy`, `sell`} subset of expert actions.\n\n    Args:\n        pi_actions:             tensor holding policy actions logits\n        expert_actions:         tensor holding expert actions probability distribution\n        name:           loss op name scope\n\n    Returns:\n        tensor holding estimated imitation loss;\n        list of related tensorboard summaries.\n    """"""\n    with tf.name_scope(name + \'/guided_loss\'):\n        if \'guided_lambda\' in kwargs.keys():\n            guided_lambda = kwargs[\'guided_lambda\']\n        else:\n            guided_lambda = 1.0\n\n        # Loss over expert buy/ sell:\n        loss = tf.losses.mean_squared_error(\n            labels=expert_actions[..., 1:3],\n            predictions=tf.nn.softmax(pi_actions)[..., 1:3],\n        ) * guided_lambda\n\n        if verbose:\n            summaries = [tf.summary.scalar(\'actions_mse\', loss)]\n        else:\n            summaries = []\n\n    return loss, summaries\n'"
btgym/research/gps/oracle.py,0,"b'import numpy as np\nfrom scipy import signal\n\n\nclass Oracle():\n    """"""\n    Irresponsible financial adviser.\n    """"""\n\n    def __init__(\n            self,\n            action_space=(0, 1, 2, 3),\n            time_threshold=5,\n            pips_threshold=10,\n            pips_scale=1e-4,\n            kernel_size=5,\n            kernel_stddev=1\n    ):\n        """"""\n\n        Args:\n            action_space:       actions to advice: 0 - hold, 1- buy, 2 - sell, 3 - close\n            time_threshold:     how many points (in number of ENVIRONMENT timesteps) on each side to use\n                                for the comparison to consider comparator(n, n+x) to be True\n            pips_threshold:     int, minimal peaks difference in pips\n                                to consider comparator(n, n+x) to be True\n            pips_scale:         actual single pip value wrt signal value\n            kernel_size:        gaussian convolution kernel size (used to compute distribution over actions)\n            kernel_stddev:      gaussian kernel standard deviation\n        """"""\n        self.action_space = action_space\n        self.time_threshold = time_threshold\n        self.value_threshold = pips_threshold * pips_scale\n        self.kernel_size = kernel_size\n        self.kernel = signal.gaussian(kernel_size, std=kernel_stddev)\n        self.data = None\n\n    def filter_by_margine(self, lst, threshold):\n        """"""\n        Filters out peaks by their \'value\' difference withing tolerance given.\n        Filtering is done from first to last index by removing every succeeding element of list from now on\n        if its value difference with value in hand is less than given threshold.\n\n        Args:\n            lst:        list of tuples; each tuple is (value, index)\n            threshold:  value filtering threshold\n\n        Returns:\n            filtered out list of tuples\n        """"""\n        if len(lst) == 1:\n            return lst\n        repeated = abs(lst[1][0] - lst[0][0]) < threshold\n        if repeated:\n            if len(lst) > 2:\n                filtered_tail = self.filter_by_margine([lst[0]] + lst[2:], threshold)\n            else:\n                filtered_tail = [lst[0]]\n        else:\n            filtered_tail = [lst[0]] + self.filter_by_margine(lst[1:], threshold)\n\n        return filtered_tail\n\n    def estimate_actions(self, episode_data):\n        """"""\n        Estimates hold/buy/sell signals based on local peaks filtered by time horizon and signal amplitude.\n\n        Args:\n            episode_data:   1D np.array of unscaled [but possibly resampled] price values in OHL[CV] format\n\n        Returns:\n            1D vector of signals of same length as episode_data\n        """"""\n        # Find local maxima and minima indices within time horizon:\n        max_ind = signal.argrelmax(episode_data, order=self.time_threshold)\n        min_ind = signal.argrelmin(episode_data, order=self.time_threshold)\n        indices = np.append(max_ind, min_ind)\n        # Append first and last points:\n        indices = np.append(indices, [0, episode_data.shape[0] - 1])\n        indices = np.sort(indices)\n\n        indices_and_values = []\n\n        for i in indices:\n            indices_and_values.append([episode_data[i], i])\n\n        # Filter by value:\n        indices_and_values = self.filter_by_margine(indices_and_values, self.value_threshold)\n\n        #print(\'filtered_indices_and_values:\', indices_and_values)\n\n        # Estimate advised actions (no \'close\' btw):\n        # Assume all \'hold\':\n        advice = np.ones(episode_data.shape[0], dtype=np.uint32) * self.action_space[0]\n\n        for num, (v, i) in enumerate(indices_and_values[:-1]):\n            if v < indices_and_values[num + 1][0]:\n                advice[i] = self.action_space[1]\n\n            else:\n                advice[i] = self.action_space[2]\n\n        return advice\n\n    def adjust_signals(self, signal):\n        """"""\n        Add simple heuristics (based on examining learnt policy actions distribution):\n        - repeat same buy or sell signal `kernel_size - 1` times.\n        """"""\n        i = 0\n        while i < signal.shape[0]:\n            j = 1\n            if signal[i] != 0:\n                # if got buy, sell or close  - repeat several times\n                while i + j < signal.shape[0] and j < self.kernel_size - 1:\n                    signal[i + j] = signal[i]\n                    j += 1\n            i = i + j\n\n        return signal\n\n    def fit(self, episode_data, resampling_factor=1):\n        """"""\n        Estimates `advised` actions probabilities distribution based on data received.\n\n        Args:\n            episode_data:           1D np.array of unscaled price values in OHL[CV] format\n            resampling_factor:      factor by which to resample given data\n                                    by taking min/max values inside every resampled bar\n\n        Returns:\n             Np.array of size [resampled_data_size, actions_space_size] of probabilities of advised actions, where\n             resampled_data_size = int(len(episode_data) / resampling_factor) + 1/0\n\n        """"""\n        # Vector of advised actions:\n        data = self.resample_data(episode_data, resampling_factor)\n        signals = self.estimate_actions(data)\n        signals = self.adjust_signals(signals)\n\n        # One-hot actions encoding:\n        actions_one_hot = np.zeros([signals.shape[0], len(self.action_space)])\n        actions_one_hot[np.arange(signals.shape[0]), signals] = 1\n\n        # Want a bit relaxed discrete distribution over actions instead of one hot (heuristic):\n        actions_distr = np.zeros(actions_one_hot.shape)\n\n        # For all actions except \'hold\' (due to heuristic skewness):\n        actions_distr[:, 0] = actions_one_hot[:, 0]\n\n        # ...spread out actions probabilities by convolving with gaussian kernel :\n        for channel in range(1, actions_one_hot.shape[-1]):\n            actions_distr[:, channel] = np.convolve(actions_one_hot[:, channel], self.kernel, mode=\'same\') + 0.1\n\n        # Normalize:\n        actions_distr /= actions_distr.sum(axis=-1)[..., None]\n\n        return actions_distr\n\n    def resample_data(self, episode_data, factor=1):\n        """"""\n        Resamples raw observations according to given skip_frame value\n        and estimates mean value of newly formed bars.\n\n        Args:\n            episode_data:   np.array of shape [episode_length, values]\n            factor:         scalar\n\n        Returns:\n            np.array of median Hi/Lo observations of size [int(episode_length/skip_frame) + 1, 1]\n        """"""\n        # Define resampled size and [possibly] extend\n        # to complete last bar by padding with values from very last column:\n        resampled_size = int(episode_data.shape[0] / factor)\n        #print(\'episode_data.shape:\', episode_data.shape)\n\n        if episode_data.shape[0] / factor > resampled_size:\n            resampled_size += 1\n            pad_size = resampled_size * factor - episode_data.shape[0]\n            #print(\'pad_size:\', pad_size)\n            episode_data = np.append(\n                episode_data,\n                np.zeros([pad_size, episode_data.shape[-1]]) + episode_data[-1, :][None,:],\n                axis=0\n            )\n        #print(\'new_episode_data.shape:\', episode_data.shape)\n\n        # Define HI and Low inside every new bar:\n        v_high = np.reshape(episode_data[:,1], [resampled_size, -1]).max(axis=-1)\n        v_low = np.reshape(episode_data[:, 2], [resampled_size, -1]).min(axis=-1)\n\n        # ...and yse Hi/Low mean along time_embedding:\n        data = np.stack([v_high, v_low], axis=-1).mean(axis=-1)\n\n        return data\n\n\nclass Oracle2():\n    """"""\n    [less]Irresponsible financial adviser.\n    """"""\n\n    def __init__(\n            self,\n            action_space=(0, 1, 2, 3),\n            gamma=1.0,\n            **kwargs\n    ):\n        """"""\n\n        Args:\n            action_space:       actions to advice: 0 - hold, 1- buy, 2 - sell, 3 - close\n            gamma:              price movement horizon discount, in (0, 1]\n        """"""\n        self.action_space = action_space\n        self.gamma = gamma\n        self.data = None\n\n    def p_up(self, x, gamma=1.0):\n        """"""\n        Discounted rise potential\n        """"""\n        if len(x) == 1:\n            return [0]\n\n        else:\n            p_x_n = self.p_up(x[1:], gamma)\n            delta = x[1] - x[0]\n            p_x = max([delta + max([gamma * p_x_n[0], 0]), 0])\n        return [p_x] + p_x_n\n\n    def p_down(self, x, gamma=1.0):\n        """"""\n        Discounted fall potential\n        """"""\n        if len(x) == 1:\n            return [0]\n\n        else:\n            p_x_n = self.p_down(x[1:], gamma)\n            delta = x[1] - x[0]\n            p_x = min([delta + min([gamma * p_x_n[0], 0]), 0])\n        return [p_x] + p_x_n\n\n    def fit(self, episode_data, resampling_factor=1):\n        """"""\n        Estimates `advised` actions probabilities distribution based on data received.\n\n        Args:\n            episode_data:           1D np.array of unscaled price values in OHL[CV] format\n            resampling_factor:      factor by which to resample given data\n                                    by taking min/max values inside every resampled bar\n\n        Returns:\n             Np.array of size [resampled_data_size, actions_space_size] of probabilities of advised actions,\n             where resampled_data_size = int(len(episode_data) / resampling_factor) + 1/0\n\n        """"""\n        # Vector of action unsacaled probabilities as future price movements potentials:\n        data = self.resample_data(episode_data, resampling_factor)\n\n        up_p = np.asarray(self.p_up(data, self.gamma))\n        down_p = - np.asarray(self.p_down(data, self.gamma))\n\n        mean_scale = (up_p.max() + down_p.max()) / 2\n\n        up_p /= mean_scale\n        down_p /= mean_scale\n\n        close_p = 1 - (up_p + down_p)\n        close_p[close_p < 0] = 0\n\n        hold_p = 0.2 * close_p\n\n        action_logits = np.stack(\n            [hold_p, up_p, down_p, close_p],\n            axis=-1\n        )\n\n        return action_logits / action_logits.sum(axis=-1)[..., None]\n\n    @staticmethod\n    def resample_data(episode_data, factor=1):\n        """"""\n        Resamples raw observations according to given skip_frame value\n        and estimates mean value of newly formed bars.\n\n        Args:\n            episode_data:   np.array of shape [episode_length, values]\n            factor:         scalar\n\n        Returns:\n            np.array of median Hi/Lo observations of size [int(episode_length/skip_frame) + 1, 1]\n        """"""\n        # Define resampled size and [possibly] extend\n        # to complete last bar by padding with values from very last column:\n        resampled_size = int(episode_data.shape[0] / factor)\n        # print(\'episode_data.shape:\', episode_data.shape)\n\n        if episode_data.shape[0] / factor > resampled_size:\n            resampled_size += 1\n            pad_size = resampled_size * factor - episode_data.shape[0]\n            # print(\'pad_size:\', pad_size)\n            episode_data = np.append(\n                episode_data,\n                np.zeros([pad_size, episode_data.shape[-1]]) + episode_data[-1, :][None, :],\n                axis=0\n            )\n        # print(\'new_episode_data.shape:\', episode_data.shape)\n\n        # Define HI and Low inside every new bar:\n        v_high = np.reshape(episode_data[:, 1], [resampled_size, -1]).max(axis=-1)\n        v_low = np.reshape(episode_data[:, 2], [resampled_size, -1]).min(axis=-1)\n\n        # ...and yse Hi/Low mean along time_embedding:\n        data = np.stack([v_high, v_low], axis=-1).mean(axis=-1)\n\n        return data\n\n\n\n\n\n'"
btgym/research/gps/policy.py,1,"b'import tensorflow as tf\nfrom btgym.algorithms.policy.stacked_lstm import AacStackedRL2Policy\nfrom btgym.algorithms.nn.layers import noisy_linear\n\n\nclass GuidedPolicy_0_0(AacStackedRL2Policy):\n    """"""\n    Guided policy: simple configuration wrapper around Stacked LSTM architecture.\n    """"""\n\n    def __init__(\n        self,\n        conv_2d_layer_config=(\n                (32, (3, 1), (2, 1)),\n                (32, (3, 1), (2, 1)),\n                (64, (3, 1), (2, 1)),\n                (64, (3, 1), (2, 1))\n            ),\n            lstm_class_ref=tf.contrib.rnn.LayerNormBasicLSTMCell,\n            lstm_layers=(256, 256),\n            lstm_2_init_period=50,\n            linear_layer_ref=noisy_linear,\n            **kwargs\n    ):\n        super(GuidedPolicy_0_0, self).__init__(\n            conv_2d_layer_config=conv_2d_layer_config,\n            lstm_class_ref=lstm_class_ref,\n            lstm_layers=lstm_layers,\n            lstm_2_init_period=lstm_2_init_period,\n            linear_layer_ref=linear_layer_ref,\n            **kwargs\n        )\n        self.expert_actions = self.on_state_in[\'expert\']\n\n\n'"
btgym/research/gps/strategy.py,0,"b'import numpy as np\n\nimport backtrader as bt\nfrom btgym.research.strategy_gen_4 import DevStrat_4_12\nfrom btgym.research.gps.oracle import Oracle, Oracle2\n\nfrom gym import spaces\nfrom btgym import DictSpace\n\n\nclass GuidedStrategy_0_0(DevStrat_4_12):\n    """"""\n    Augments observation state with expert actions predictions estimated by accessing entire episode data (=cheating).\n    """"""\n    # Time embedding period:\n    time_dim = 30  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of environment steps to skip before returning next response,\n    # e.g. if set to 10 -- agent will interact with environment every 10th step;\n    # every other step agent action is assumed to be \'hold\':\n    skip_frame = 10\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 20\n\n    # Possible agent actions:\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    gamma = 0.99  # fi_gamma, should be MDP gamma decay\n\n    reward_scale = 1  # reward multiplicator\n\n    state_ext_scale = np.linspace(3e3, 1e3, num=6)\n\n    params = dict(\n        # Note: fake `Width` dimension to use 2d conv etc.:\n        state_shape=\n        {\n            \'external\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, 6), dtype=np.float32),\n            \'internal\': spaces.Box(low=-2, high=2, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'datetime\': spaces.Box(low=0, high=1, shape=(1, 5), dtype=np.float32),\n            \'expert\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        drawdown_call=5,\n        target_call=19,\n        portfolio_actions=portfolio_actions,\n        skip_frame=skip_frame,\n        state_ext_scale=state_ext_scale,  # EURUSD\n        state_int_scale=1.0,\n        gamma=gamma,\n        reward_scale=1.0,\n        metadata={},\n        # Expert parameters:\n        expert_config=\n        {\n            \'time_threshold\': 5,  # minimum peak estimation radius in number of environment steps\n            \'pips_threshold\': 5,  # minimum peak estimation value in number of quota points\n            \'pips_scale\': 1e-4,   # value of single quota point relative to price value\n            \'kernel_size\': 5,     # gaussian_over_action tails size in number of env. steps\n            \'kernel_stddev\': 1,   # gaussian_over_action standard deviation\n        },\n    )\n\n    def __init__(self, **kwargs):\n        super(GuidedStrategy_0_0, self).__init__(**kwargs)\n        self.expert = Oracle(action_space=np.arange(len(self.p.portfolio_actions)), **self.p.expert_config)\n        # self.expert = Oracle2(action_space=np.arange(len(self.p.portfolio_actions)), **self.p.expert_config)\n        self.expert_actions = None\n        self.current_expert_action = None\n\n    def nextstart(self):\n        """"""\n        Overrides base method augmenting it with estimating expert actions before actual episode starts.\n        """"""\n        # This value shows how much episode records we need to spend\n        # to estimate first environment observation:\n        self.inner_embedding = self.data.close.buflen()\n        self.log.info(\'Inner time embedding: {}\'.format(self.inner_embedding))\n\n        # Now when we know exact maximum possible episode length -\n        #  can extract relevant episode data and make expert predictions:\n        data = self.datas[0].p.dataname.as_matrix()[self.inner_embedding:, :]\n\n        # Note: need to form sort of environment \'custom candels\' by taking min and max price values over every\n        # skip_frame period; this is done inside Oracle class;\n        # TODO: shift actions forward to eliminate one-point prediction lag?\n        # expert_actions is a matrix representing discrete distribution over actions probabilities\n        # of size [max_env_steps, action_space_size]:\n        self.expert_actions = self.expert.fit(episode_data=data, resampling_factor=self.p.skip_frame)\n\n    def get_expert_state(self):\n        self.current_expert_action = self.expert_actions[self.env_iteration]\n\n        #print(\'Strat_iteration:\', self.iteration)\n        #print(\'Env_iteration:\', self.env_iteration)\n\n        return self.current_expert_action\n\n    # def get_state(self):\n    #     # Update inner state statistic and compose state:\n    #     self.update_broker_stat()\n    #\n    #     self.state = {\n    #         \'external\': self.get_external_state(),\n    #         \'internal\': self.get_internal_state(),\n    #         \'datetime\': self.get_datetime_state(),\n    #         \'expert\': self.get_expert_state(),\n    #         \'metadata\': self.get_metadata_state(),\n    #     }\n    #\n    #     return self.state\n\n\nclass ExpertObserver(bt.observer.Observer):\n    """"""\n    Keeps track of expert-advised actions.\n    Single data_feed.\n    """"""\n\n    lines = (\'buy\', \'sell\', \'hold\', \'close\')\n    plotinfo = dict(plot=True, subplot=True, plotname=\'Expert Actions\', plotymargin=.8)\n    plotlines = dict(\n        buy=dict(marker=\'^\', markersize=4.0, color=\'cyan\', fillstyle=\'full\'),\n        sell=dict(marker=\'v\', markersize=4.0, color=\'magenta\', fillstyle=\'full\'),\n        hold=dict(marker=\'.\', markersize=1.0, color=\'gray\', fillstyle=\'full\'),\n        close=dict(marker=\'o\', markersize=4.0, color=\'blue\', fillstyle=\'full\')\n    )\n\n    def next(self):\n        action = np.argmax(self._owner.current_expert_action)\n        if action == 0:\n            self.lines.hold[0] = 0\n        elif action == 1:\n            self.lines.buy[0] = 1\n        elif action == 2:\n            self.lines.sell[0] = -1\n        elif action == 3:\n            self.lines.close[0] = 0\n\n'"
btgym/research/metalearn_2/__init__.py,0,"b""# This subpackage explores meta-learning approach based on idea if embedding learning algorithm in rnn activations.\n#\n# Papers:\n#\n# James Ting-Ho Lo: 'Adaptive Capability of Recurrent Neural Networks\n# with Fixed Weights for Series-Parallel System Identification',\n# http://www.math.umbc.edu/~jameslo/papers/accommodative.pdf\n#\n# Lee Feldkamp et al., 'Fixed-weight controller for multiple systems',\n# https://www.researchgate.net/publication/3705553_Fixed-weight_controller_for_multiple_systems\n#\n# Sepp Hohreiter, A. Steven Younger, Peter R. Conwell: 'Learning To Learn Using Gradient Desent',\n# http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.323&rep=rep1&type=pdf\n#\n#  A. Steven Younger, Sepp Hochreiter, Peter R. Conwell:  'Meta-Learning with Backpropagation',\n# http://www.bioinf.jku.at/publications/older/3904.pdf\n#\n# D. V. Prokhorov et al.: 'Adaptive behaviour with Fixed Weights in RNN: An Overview',\n# https://www.researchgate.net/profile/Ivan_Tyukin/publication/3950355_Adaptive_behavior_with_fixed_weights_in_RNN_An_overview\n#\n# JX Wang et al.: 'LEARNING TO REINFORCEMENT LEARN',\n# https://arxiv.org/pdf/1611.05763.pdf\n#\n# Sachin Ravi et al.: 'OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING',\n# https://openreview.net/pdf?id=rJY0-Kcll\n#\n# Marcin Andrychowicz et al., 'Learning to learn by gradient descent by gradient descent',\n# https://arxiv.org/pdf/1606.04474.pdf\n#\n# Yan Duan et al., 'RL^2: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING',\n# https://arxiv.org/pdf/1611.02779.pdf\n#\n# Flood Sung et al.: 'Learning to Learn: Meta-Critic Networksfor Sample Efficient Learning',\n# https://arxiv.org/pdf/1706.09529.pdf\n#\n# Nikhil Mishra et al., 'Meta-Learning with Temporal Convolutions',\n# https://arxiv.org/pdf/1707.03141.pdf\n"""
btgym/research/metalearn_2/_aac_t2d.py,0,"b'import  numpy as np\n\nfrom btgym.algorithms.utils import batch_stack, batch_gather, _show_struct\nfrom btgym.research.gps.aac import GuidedAAC\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\n\n\nclass AACt2d(GuidedAAC):\n    """"""\n    Trajectory2Distribution:\n    AAC class including methods enabling treating collected train data as empirical distribution rather than trajectory.\n\n    Note:\n        time_flat=True is a key ingredient here. See BaseAAC notes for details.\n    """"""\n    def __init__(\n            self,\n            runner_config=None,\n            name=\'AAC_T2D\',\n            **kwargs\n    ):\n        try:\n            if runner_config is None:\n                kwargs[\'runner_config\'] = {\n                \'class_ref\': BaseSynchroRunner,\n                \'kwargs\': {\n                    \'data_sample_config\': {\'mode\': 0},\n                    \'name\': \'t2d_synchro\',\n                },\n            }\n            else:\n                kwargs[\'runner_config\'] = runner_config\n            kwargs.update(\n                {\n                    \'time_flat\': True,\n                    \'name\': name,\n                    \'_aux_render_modes\': (\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n                }\n            )\n            super(AACt2d, self).__init__(**kwargs)\n            self.on_policy_batch = None\n            self.off_policy_batch = None\n            self.rp_batch = None\n\n        except:\n            msg = \'AAC_T2D.__init__() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def get_episode(self, **kwargs):\n        """"""\n         Get exactly one episode trajectory as single rollout. <-- DEAD WRONG\n\n        Args:\n            **kwargs:   see env.reset() method\n\n        Returns:\n\n        """"""\n        data_streams = [runner.get_episode(**kwargs) for runner in self.runners]\n        return {key: [stream[key] for stream in data_streams] for key in data_streams[0].keys()}\n\n    def get_batch(self, **kwargs):\n        """"""\n        Retrieves batch of rollouts from runners.\n\n        Args:\n            **kwargs:   see runner.get_batch() method.\n\n        Returns:\n            dictionary of batched rollouts\n        """"""\n        rollouts = []\n        terminal_context = []\n        for runner in self.runners:\n            batch = runner.get_batch(**kwargs)\n            for rollout in batch[\'data\']:\n                rollouts.append(rollout)\n\n            for context in batch[\'terminal_context\']:\n                terminal_context.append(context)\n\n        self.log.debug(\'rollouts_len: {}\'.format(len(rollouts)))\n\n        final_batch = {key: [rollout[key] for rollout in rollouts] for key in rollouts[0].keys()}\n        final_batch[\'terminal_context\'] = terminal_context\n\n        return final_batch\n\n    @staticmethod\n    def sample_batch(batch, sample_size):\n        """"""\n        Uniformly randomly samples mini-batch from (supposedly bigger) batch.\n\n        Args:\n            batch:          nested dict of experiences\n            sample_size:    mini-batch size\n\n        Returns:\n            nested dict of experiences of same structure as `batch` with number of experiences eq. to `sample_size`.\n        """"""\n        if batch is not None:\n            batch_size = batch[\'time_steps\'].shape[0]\n            indices = np.random.randint(0, batch_size, size=sample_size)\n\n            return batch_gather(batch, indices)\n\n        else:\n            return  None\n\n    def process_batch(self, sess, data):\n        """"""\n        Processes batched rollouts.\n        Makes every experience independent, enabling further shuffling or sampling.\n\n        Args:\n            sess:               tf session obj.\n            data (dict):        data dictionary\n            is_train (bool):    is data provided are train or test\n\n        Returns:\n            on-policy [, off-policy and rp] processed batched data.\n\n        """"""\n        # Process minibatch for on-policy train step:\n        on_policy_batch = self._process_rollouts(data[\'on_policy\'])\n\n        if self.use_memory:\n            # Process rollouts from replay memory:\n            off_policy_batch = self._process_rollouts(data[\'off_policy\'])\n\n            if self.use_reward_prediction:\n                # Rebalanced 50/50 sample for RP:\n                rp_rollouts = data[\'off_policy_rp\']\n                rp_batch = batch_stack([rp.process_rp(self.rp_reward_threshold) for rp in rp_rollouts])\n\n            else:\n                rp_batch = None\n\n        else:\n            off_policy_batch = None\n            rp_batch = None\n\n        return on_policy_batch, off_policy_batch, rp_batch\n\n    @staticmethod\n    def _check(batch):\n        """"""\n        Debug. utility.\n        """"""\n        print(\'Got data_dict:\')\n        for key in batch.keys():\n            try:\n                shape = np.asarray(batch[key]).shape\n            except:\n                shape = \'???\'\n            print(\'key: {}, shape: {}\'.format(key, shape))\n\n    def process(self, sess, **kwargs):\n        """"""\n        Usage example. Override.\n        """"""\n        try:\n            # Collect train trajectories:\n            train_data_batch = self.get_batch(size=30, require_terminal=True)\n\n            self._check(train_data_batch)\n            #print(\'train_data_batch_ep_summary: \', train_data_batch[\'ep_summary\'])\n            #print(\'train_data_batch_render_summary: \', train_data_batch[\'render_summary\'])\n\n            # Process time-flat alike (~iid) to treat as empirical data distribution over train task:\n            self.on_policy_batch, self.off_policy_batch, self.rp_batch = self.process_batch(sess, train_data_batch)\n\n            #self._check(self.on_policy_batch)\n\n            print(\'on_p_batch_size: {}\'.format(self.on_policy_batch[\'batch_size\']))\n\n            # Perform updates to\n            # Sample random batch of train data from train task:\n            on_policy_mini_batch = self.sample_batch(self.on_policy_batch, 17)\n\n            #self._check(on_policy_mini_batch)\n\n            print(\'on_p_mini_batch_size: {}\'.format(on_policy_mini_batch[\'batch_size\']))\n\n            feed_dict = self._get_main_feeder(sess, on_policy_mini_batch, None, None, True)\n\n            #self._check(feed_dict)\n\n        except:\n            msg = \'process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n\n'"
btgym/research/metalearn_2/_env_runner.py,1,"b'\nimport numpy as np\n\nfrom btgym.algorithms.rollout import Rollout\nfrom btgym.algorithms.memory import _DummyMemory\nfrom btgym.algorithms.math_utils import softmax\n\n\ndef MetaEnvRunnerFn(\n        sess,\n        env,\n        policy,\n        task,\n        rollout_length,\n        summary_writer,\n        episode_summary_freq,\n        env_render_freq,\n        atari_test,\n        ep_summary,\n        memory_config,\n        log,\n        aux_summaries=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n):\n    """"""\n    Meta-learning loop runtime logic of the thread runner.\n\n    Args:\n        env:                    environment instance\n        policy:                 policy instance\n        task:                   int\n        rollout_length:         int\n        episode_summary_freq:   int\n        env_render_freq:        int\n        atari_test:             bool, Atari or BTGyn\n        ep_summary:             dict of tf.summary op and placeholders\n        memory_config:          replay memory configuration dictionary\n        log:                    logbook logger\n        aux_summaries:          list of str, additional summaries to compute\n\n    Yelds:\n        collected data as dictionary of on_policy, off_policy rollouts, episode statistics and summaries.\n    """"""\n    if memory_config is not None:\n        memory = memory_config[\'class_ref\'](**memory_config[\'kwargs\'])\n\n    else:\n        memory = _DummyMemory()\n\n    # We want test data runner to be master:\n\n    if \'test\' in task:\n        mode = 1\n    else:\n        mode = 0\n    # Pass sample config to environment (.get_sample_config() is actually aac framework method):\n    log.warning(\'mode={}\'.format(mode))\n    last_state = env.reset(**policy.get_sample_config(mode))\n    last_action, last_reward, last_value, last_context = policy.get_initial_features(state=last_state)\n    length = 0\n    local_episode = 0\n    reward_sum = 0\n    last_action_reward = np.concatenate([last_action, np.asarray([last_reward])], axis=-1)\n\n    # Summary averages accumulators:\n    total_r = []\n    cpu_time = []\n    final_value = []\n    total_steps = []\n    total_steps_atari = []\n\n    # Aux accumulators:\n    ep_a_logits = []\n    ep_value = []\n    ep_context = []\n\n    ep_stat = None\n    test_ep_stat = None\n    render_stat = None\n\n    norm_image = lambda x: np.round((x - x.min()) / np.ptp(x) * 255)\n\n    if env.data_master is True:\n        # Hacky but we need env.renderer methods ready\n        env.renderer.initialize_pyplot()\n\n    log.notice(\'started data collection.\')\n    while True:\n        terminal_end = False\n        rollout = Rollout()\n\n        action, logits, value, context = policy.act(last_state, last_context, last_action_reward)\n\n        ep_a_logits.append(logits)\n        ep_value.append(value)\n        ep_context.append(context)\n\n        #log.debug(\'*: A: {}, V: {}, step: {} \'.format(action, value_, length))\n\n        # argmax to convert from one-hot:\n        state, reward, terminal, info = env.step(action.argmax())\n\n        # Partially collect first experience of rollout:\n        last_experience = {\n            \'position\': {\'episode\': local_episode, \'step\': length},\n            \'state\': last_state,\n            \'action\': action,\n            \'reward\': reward,\n            \'value\': value,\n            \'terminal\': terminal,\n            \'context\': last_context,\n            \'last_action_reward\': last_action_reward,\n        }\n        # Execute user-defined callbacks to policy, if any:\n        for key, callback in policy.callback.items():\n            last_experience[key] = callback(**locals())\n\n        length += 1\n        reward_sum += reward\n        last_state = state\n        last_context = context\n        last_action = action\n        last_reward = reward\n        last_action_reward = np.concatenate([last_action, np.asarray([last_reward])], axis=-1)\n\n        for roll_step in range(1, rollout_length):\n            if not terminal:\n                # Continue adding experiences to rollout:\n                action, logits, value, context = policy.act(last_state, last_context, last_action_reward)\n\n                #log.debug(\'A: {}, V: {}, step: {} \'.format(action, value_, length))\n\n                ep_a_logits.append(logits)\n                ep_value.append(value)\n                ep_context.append(context)\n\n                #log.notice(\'context: {}\'.format(context))\n\n                # Argmax to convert from one-hot:\n                state, reward, terminal, info = env.step(action.argmax())\n\n                # Partially collect next experience:\n                experience = {\n                    \'position\': {\'episode\': local_episode, \'step\': length},\n                    \'state\': last_state,\n                    \'action\': action,\n                    \'reward\': reward,\n                    \'value\': value,\n                    \'terminal\': terminal,\n                    \'context\': last_context,\n                    \'last_action_reward\': last_action_reward,\n                    #\'pixel_change\': 0 #policy.get_pc_target(state, last_state),\n                }\n                for key, callback in policy.callback.items():\n                    experience[key] = callback(**locals())\n\n                # Bootstrap to complete and push previous experience:\n                last_experience[\'r\'] = value\n                rollout.add(last_experience)\n                memory.add(last_experience)\n\n                # Housekeeping:\n                length += 1\n                reward_sum += reward\n                last_state = state\n                last_context = context\n                last_action = action\n                last_reward = reward\n                last_action_reward = np.concatenate([last_action, np.asarray([last_reward])], axis=-1)\n                last_experience = experience\n\n            if terminal:\n                # Finished episode within last taken step:\n                terminal_end = True\n                # All environment-specific summaries are here due to fact\n                # only runner allowed to interact with environment:\n                # Accumulate values for averaging:\n                total_r += [reward_sum]\n                total_steps_atari += [length]\n                if not atari_test:\n                    episode_stat = env.get_stat()  # get episode statistic\n                    last_i = info[-1]  # pull most recent info\n                    cpu_time += [episode_stat[\'runtime\'].total_seconds()]\n                    final_value += [last_i[\'broker_value\']]\n                    total_steps += [episode_stat[\'length\']]\n\n                # Episode statistics:\n                try:\n                    # Was it test episode ( `type` in metadata is not zero)?\n                    if not atari_test and state[\'metadata\'][\'type\']:\n                        is_test_episode = True\n\n                    else:\n                        is_test_episode = False\n\n                except KeyError:\n                    is_test_episode = False\n\n                if is_test_episode:\n                    test_ep_stat = dict(\n                        total_r=total_r[-1],\n                        final_value=final_value[-1],\n                        steps=total_steps[-1]\n                    )\n                else:\n                    if local_episode % episode_summary_freq == 0:\n                        if not atari_test:\n                            # BTgym:\n                            ep_stat = dict(\n                                total_r=np.average(total_r),\n                                cpu_time=np.average(cpu_time),\n                                final_value=np.average(final_value),\n                                steps=np.average(total_steps)\n                            )\n                        else:\n                            # Atari:\n                            ep_stat = dict(\n                                total_r=np.average(total_r),\n                                steps=np.average(total_steps_atari)\n                            )\n                        total_r = []\n                        cpu_time = []\n                        final_value = []\n                        total_steps = []\n                        total_steps_atari = []\n                # Only render chief worker and test environment:\n                if \'0\' in task and \'test\' in task and local_episode % env_render_freq == 0 :\n                    if not atari_test:\n                        # Render environment (chief worker only, and not in atari atari_test mode):\n                        render_stat = {\n                            mode: env.render(mode)[None,:] for mode in env.render_modes\n                        }\n                        # Update renderings with aux:\n\n                        # log.notice(\'ep_logits shape: {}\'.format(np.asarray(ep_a_logits).shape))\n                        # log.notice(\'ep_value shape: {}\'.format(np.asarray(ep_value).shape))\n\n                        # Unpack LSTM states:\n                        rnn_1, rnn_2 = zip(*ep_context)\n                        rnn_1 = [state[0] for state in rnn_1]\n                        rnn_2 = [state[0] for state in rnn_2]\n                        c1, h1 = zip(*rnn_1)\n                        c2, h2 = zip(*rnn_2)\n\n                        aux_images = {\n                            \'action_prob\':  env.renderer.draw_plot(\n                                # data=softmax(np.asarray(ep_a_logits)[:, 0, :] - np.asarray(ep_a_logits).max()),\n                                data=softmax(np.asarray(ep_a_logits)[:, 0, :]),\n                                title=\'Episode actions probabilities\',\n                                figsize=(12, 4),\n                                box_text=\'\',\n                                xlabel=\'Backward env. steps\',\n                                ylabel=\'R+\',\n                                line_labels=[\'Hold\', \'Buy\', \'Sell\', \'Close\']\n                            )[None, ...],\n                            \'value_fn\': env.renderer.draw_plot(\n                                data=np.asarray(ep_value),\n                                title=\'Episode Value function\',\n                                figsize=(12, 4),\n                                xlabel=\'Backward env. steps\',\n                                ylabel=\'R\',\n                                line_labels = [\'Value\']\n                            )[None, ...],\n                            #\'lstm_1_c\': norm_image(np.asarray(c1).T[None, :, 0, :, None]),\n                            \'lstm_1_h\': norm_image(np.asarray(h1).T[None, :, 0, :, None]),\n                            #\'lstm_2_c\': norm_image(np.asarray(c2).T[None, :, 0, :, None]),\n                            \'lstm_2_h\': norm_image(np.asarray(h2).T[None, :, 0, :, None])\n                        }\n\n                        render_stat.update(aux_images)\n\n                    else:\n                        # Atari:\n                        render_stat = dict(render_atari=state[\'external\'][None,:] * 255)\n\n                # New episode:\n                last_state = env.reset(**policy.get_sample_config(mode))\n                last_action, last_reward, last_value, last_context = policy.get_initial_features(state=last_state)\n                length = 0\n                reward_sum = 0\n                last_action_reward = np.concatenate([last_action, np.asarray([last_reward])], axis=-1)\n\n                # reset per-episode accumulators:\n                ep_a_logits = []\n                ep_value = []\n                ep_context = []\n\n                # Increment global and local episode counts:\n                sess.run(policy.inc_episode)\n                local_episode += 1\n                break\n\n        # After rolling `rollout_length` or less (if got `terminal`)\n        # complete final experience of the rollout:\n        if not terminal_end:\n            # Bootstrap:\n            last_experience[\'r\'] = np.asarray(\n                [policy.get_value(last_state, last_context, last_action_reward)]\n            )\n\n        else:\n            last_experience[\'r\'] = np.asarray([0.0])\n\n        rollout.add(last_experience)\n\n        # Only training rollouts are added to replay memory:\n        try:\n            # Was it test (`type` in metadata is not zero)?\n            if not atari_test and last_experience[\'state\'][\'metadata\'][\'type\']:\n                is_test = True\n\n            else:\n                is_test = False\n\n        except KeyError:\n            is_test = False\n\n        if not is_test:\n            memory.add(last_experience)\n\n        # Once we have enough experience and memory can be sampled, yield it,\n        # and have the ThreadRunner place it on a queue:\n        if memory.is_full():\n            data = dict(\n                on_policy=rollout,\n                off_policy=memory.sample_uniform(sequence_size=rollout_length),\n                off_policy_rp=memory.sample_priority(exact_size=True),\n                ep_summary=ep_stat,\n                test_ep_summary=test_ep_stat,\n                render_summary=render_stat,\n            )\n            yield data\n\n            ep_stat = None\n            test_ep_stat = None\n            render_stat = None\n\n'"
btgym/research/metalearn_2/_fwrnn_aac.py,0,"b'import tensorflow as tf\nimport time\n\nimport sys\nfrom logbook import Logger, StreamHandler\n\nfrom btgym.research.gps.aac import GuidedAAC\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\n\n\nclass MetaAAC_2_0(GuidedAAC):\n    """"""\n    RNN adaptation experiment\n    """"""\n\n    def __init__(\n            self,\n            trial_source_target_cycle=(1, 0),\n            num_episodes_per_trial=1,\n            name=\'AAC_FWRNN_Ada\',\n            **kwargs\n    ):\n        runner_config = {\n            \'class_ref\': BaseSynchroRunner,\n            \'kwargs\': {\n                \'data_sample_config\': {\'mode\': 0},\n                \'name\': \'\',\n            },\n        }\n        super(MetaAAC_2_0, self).__init__(\n            runner_config=runner_config,\n            name=name,\n            **kwargs\n        )\n\n        self.current_data = None\n        self.current_feed_dict = None\n\n        # Trials sampling control:\n        self.num_source_trials = trial_source_target_cycle[0]\n        self.num_target_trials = trial_source_target_cycle[-1]\n        self.num_episodes_per_trial = num_episodes_per_trial\n\n        # Note that only master (test runner) is requesting trials\n\n        self.current_source_trial = 0\n        self.current_target_trial = 0\n        self.current_trial_mode = 0  # source\n        self.current_episode = 0\n\n    def get_sample_config(self, mode=0, **kwargs):\n        """"""\n        Returns environment configuration parameters for next episode to sample.\n\n        Args:\n              mode:     bool, False for slave (train data), True for master (test data)\n\n        Returns:\n            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n        """"""\n\n        new_trial = 0\n\n        # Only master environment updates counters:\n        if self.current_episode >= self.num_episodes_per_trial:\n            # Reset episode counter:\n            self.current_episode = 0\n\n            # Request new trial:\n            new_trial = 1\n            # Decide on trial type (source/target):\n            if self.current_source_trial >= self.num_source_trials:\n                # Time to switch to target mode:\n                self.current_trial_mode = 1\n                # Reset counters:\n                self.current_source_trial = 0\n                self.current_target_trial = 0\n\n            if self.current_target_trial >= self.num_target_trials:\n                # Vise versa:\n                self.current_trial_mode = 0\n                self.current_source_trial = 0\n                self.current_target_trial = 0\n\n            # Update counter:\n            if self.current_trial_mode:\n                self.current_target_trial += 1\n            else:\n                self.current_source_trial += 1\n\n        self.current_episode += 1\n\n\n        # Compose btgym.datafeed.base.EnvResetConfig-consistent dict:\n        sample_config = dict(\n            episode_config=dict(\n                get_new=True,\n                sample_type=mode,\n                b_alpha=1.0,\n                b_beta=1.0\n            ),\n            trial_config=dict(\n                get_new=new_trial,\n                sample_type=self.current_trial_mode,\n                b_alpha=1.0,\n                b_beta=1.0\n            )\n        )\n        return sample_config\n\n    def get_episode(self, **kwargs):\n        data_streams = [runner.get_episode(**kwargs) for runner in self.runners]\n        return {key: [stream[key] for stream in data_streams] for key in data_streams[0].keys()}\n\n    def process(self, sess, **kwargs):\n        if self.task < 1:\n            self.process_test(sess)\n\n        else:\n            self.process_train(sess)\n\n    def process_test(self, sess):\n        """"""\n         test step.\n\n         Args:\n             sess (tensorflow.Session):   tf session obj.\n\n         """"""\n        # Copy from parameter server:\n        sess.run(self.sync_pi)\n        for i in range(1):\n            test_data = self.get_episode(init_context=0)\n            self.process_summary(sess, test_data)\n            #self.log.warning(\'self.current_episode: {}\'.format(self.current_episode))\n\n        #time.sleep(5)\n\n    def process_train(self, sess):\n        """"""\n        Train step.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            # Say `No` to redundant summaries:\n            wirte_model_summary = \\\n                self.local_steps % self.model_summary_freq == 0\n\n            # Collect train trajectory:\n            train_data = self.get_data()\n            feed_dict = self.process_data(sess,,,,, train_data,,\n                        # self.log.warning(\'Train data ok.\')\n\n                        # Copy from parameter server:\n                        sess.run(self.sync_pi)\n            # self.log.warning(\'Sync ok.\')\n\n            # Update pi_prime parameters wrt collected data:\n            if wirte_model_summary:\n                fetches = [self.train_op, self.model_summary_op, self.inc_step]\n            else:\n                fetches = [self.train_op, self.inc_step]\n\n            fetched = sess.run(fetches, feed_dict=feed_dict)\n\n            # self.log.warning(\'Train gradients ok.\')\n\n            if wirte_model_summary:\n                model_summary = fetched[-2]\n\n            else:\n                model_summary = None\n\n            # Write down summaries:\n            self.process_summary(sess, train_data, model_summary)\n            self.local_steps += 1\n\n        except:\n            msg = \'process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n\n\n'"
btgym/research/metalearn_2/_mldg_batch.py,14,"b'import tensorflow as tf\nimport numpy as np\n\nimport sys\nfrom logbook import Logger, StreamHandler\n\nfrom btgym.research.mldg.aac import SubAAC\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\n\n\nclass MLDG():\n    """"""\n    Asynchronous implementation of MLDG algorithm\n    for continuous adaptation in dynamically changing environments.\n\n    Papers:\n        Da Li et al.,\n         ""Learning to Generalize: Meta-Learning for Domain Generalization""\n         https://arxiv.org/abs/1710.03463\n\n        Maruan Al-Shedivat et al.,\n        ""Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments""\n        https://arxiv.org/abs/1710.03641\n\n    """"""\n    def __init__(\n            self,\n            env,\n            task,\n            log_level,\n            aac_class_ref=SubAAC,\n            runner_config=None,\n            aac_lambda=1.0,\n            guided_lambda=1.0,\n            rollout_length=20,\n            train_support=300,\n            fast_adapt_num_steps=10,\n            fast_adapt_batch_size=32,\n            trial_source_target_cycle=(1, 0),\n            num_episodes_per_trial=1,  # one-shot adaptation\n            _aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            name=\'MLDG\',\n            **kwargs\n    ):\n        try:\n            self.aac_class_ref = aac_class_ref\n            self.task = task\n            self.name = name\n            self.summary_writer = None\n\n            StreamHandler(sys.stdout).push_application()\n            self.log = Logger(\'{}_{}\'.format(name, task), level=log_level)\n\n            self.rollout_length = rollout_length\n            self.train_support = train_support  # number of train experiences to collect\n            self.train_batch_size = int(self.train_support / self.rollout_length)\n            self.fast_adapt_num_steps = fast_adapt_num_steps\n            self.fast_adapt_batch_size = fast_adapt_batch_size\n\n            if runner_config is None:\n                self.runner_config = {\n                    \'class_ref\': BaseSynchroRunner,\n                    \'kwargs\': {},\n                }\n            else:\n                self.runner_config = runner_config\n\n            self.env_list = env\n\n            assert isinstance(self.env_list, list) and len(self.env_list) == 2, \\\n                \'Expected pair of environments, got: {}\'.format(self.env_list)\n\n            # Instantiate two sub-trainers: one for test and one for train environments:\n\n            self.runner_config[\'kwargs\'][\'data_sample_config\'] = {\'mode\': 1}  # master\n            self.runner_config[\'kwargs\'][\'name\'] = \'master\'\n\n            self.train_aac = aac_class_ref(\n                env=self.env_list[0],  # train data will be master environment TODO: really dumb data control. improve.\n                task=self.task,\n                log_level=log_level,\n                runner_config=self.runner_config,\n                aac_lambda=aac_lambda,\n                guided_lambda=guided_lambda,\n                rollout_length=self.rollout_length,\n                trial_source_target_cycle=trial_source_target_cycle,\n                num_episodes_per_trial=num_episodes_per_trial,\n                _use_target_policy=False,\n                _use_global_network=True,\n                _aux_render_modes=_aux_render_modes,\n                name=self.name + \'_sub_Train\',\n                **kwargs\n            )\n\n            self.runner_config[\'kwargs\'][\'data_sample_config\'] = {\'mode\': 0}  # master\n            self.runner_config[\'kwargs\'][\'name\'] = \'slave\'\n\n            self.test_aac = aac_class_ref(\n                env=self.env_list[-1],  # test data -> slave env.\n                task=self.task,\n                log_level=log_level,\n                runner_config=self.runner_config,\n                aac_lambda=aac_lambda,\n                guided_lambda=guided_lambda,\n                rollout_length=self.rollout_length,\n                trial_source_target_cycle=trial_source_target_cycle,\n                num_episodes_per_trial=num_episodes_per_trial,\n                _use_target_policy=False,\n                _use_global_network=False,\n                global_step_op=self.train_aac.global_step,\n                global_episode_op=self.train_aac.global_episode,\n                inc_episode_op=self.train_aac.inc_episode,\n                _aux_render_modes=_aux_render_modes,\n                name=self.name + \'_sub_Test\',\n                **kwargs\n            )\n\n            self.local_steps = self.train_aac.local_steps\n            self.model_summary_freq = self.train_aac.model_summary_freq\n            #self.model_summary_op = self.train_aac.model_summary_op\n\n            self._make_train_op()\n            self.test_aac.model_summary_op = tf.summary.merge(\n                [self.test_aac.model_summary_op, self._combine_meta_summaries()],\n                name=\'meta_model_summary\'\n            )\n\n        except:\n            msg = \'MLDG.__init()__ exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def _make_train_op(self):\n        """"""\n\n        Defines:\n            tensors holding training op graph for sub trainers and self;\n        """"""\n        pi = self.train_aac.local_network\n        pi_prime = self.test_aac.local_network\n\n        self.test_aac.sync = self.test_aac.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi.var_list)]\n        )\n\n        self.global_step = self.train_aac.global_step\n        self.global_episode = self.train_aac.global_episode\n\n        self.test_aac.global_step = self.train_aac.global_step\n        self.test_aac.global_episode = self.train_aac.global_episode\n        self.test_aac.inc_episode = self.train_aac.inc_episode\n        self.train_aac.inc_episode = None\n        self.inc_step = self.train_aac.inc_step\n\n        # Meta-loss:\n        self.loss = 0.5 * self.train_aac.loss + 0.5 * self.test_aac.loss\n\n        # Clipped gradients:\n        self.train_aac.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.train_aac.loss, pi.var_list),\n            40.0\n        )\n        self.log.warning(\'self.train_aac.grads: {}\'.format(len(list(self.train_aac.grads))))\n\n        # self.test_aac.grads, _ = tf.clip_by_global_norm(\n        #     tf.gradients(self.test_aac.loss, pi_prime.var_list),\n        #     40.0\n        # )\n        # Meta-gradient:\n        grads_i, _ = tf.clip_by_global_norm(\n            tf.gradients(self.train_aac.loss, pi.var_list),\n            40.0\n        )\n\n        grads_i_next, _ = tf.clip_by_global_norm(\n            tf.gradients(self.test_aac.loss, pi_prime.var_list),\n            40.0\n        )\n\n        self.grads = []\n        for g1, g2 in zip(grads_i, grads_i_next):\n            if g1 is not None and g2 is not None:\n                meta_g = 0.5 * g1 + 0.5 * g2\n            else:\n                meta_g = None\n\n            self.grads.append(meta_g)\n\n        #self.log.warning(\'self.grads_len: {}\'.format(len(list(self.grads))))\n\n        # Gradients to update local copy of pi_prime (from train data):\n        train_grads_and_vars = list(zip(self.train_aac.grads, pi_prime.var_list))\n\n        # self.log.warning(\'train_grads_and_vars_len: {}\'.format(len(train_grads_and_vars)))\n\n        # Meta-gradients to be sent to parameter server:\n        meta_grads_and_vars = list(zip(self.grads, self.train_aac.network.var_list))\n\n        # self.log.warning(\'meta_grads_and_vars_len: {}\'.format(len(meta_grads_and_vars)))\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(self.train_aac.local_network.on_state_in.keys())\n\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        self.train_aac.inc_step = self.train_aac.global_step.assign_add(\n            tf.shape(self.train_aac.local_network.on_state_in[\'external\'])[0]\n        )\n\n        self.train_op = self.train_aac.optimizer.apply_gradients(train_grads_and_vars)\n\n        # Optimizer for meta-update:\n        self.optimizer = tf.train.AdamOptimizer(self.train_aac.train_learn_rate, epsilon=1e-5)\n        # TODO: own alpha-leran rate\n        self.meta_train_op = self.optimizer.apply_gradients(meta_grads_and_vars)\n\n        self.log.debug(\'meta_train_op defined\')\n\n    def _combine_meta_summaries(self):\n\n        meta_model_summaries = [\n            tf.summary.scalar(""meta_grad_global_norm"", tf.global_norm(self.grads)),\n            tf.summary.scalar(""total_meta_loss"", self.loss),\n        ]\n\n        return meta_model_summaries\n\n    def start(self, sess, summary_writer, **kwargs):\n        """"""\n        Executes all initializing operations,\n        starts environment runner[s].\n        Supposed to be called by parent worker just before training loop starts.\n\n        Args:\n            sess:           tf session object.\n            kwargs:         not used by default.\n        """"""\n        try:\n            # Copy weights from global to local:\n            sess.run(self.train_aac.sync_pi)\n            sess.run(self.test_aac.sync_pi)\n\n            # Start thread_runners:\n            self.train_aac._start_runners(   # master first\n                sess,\n                summary_writer,\n                init_context=None,\n                data_sample_config=self.train_aac.get_sample_config(mode=1)\n            )\n            self.test_aac._start_runners(\n                sess,\n                summary_writer,\n                init_context=None,\n                data_sample_config=self.test_aac.get_sample_config(mode=0)\n            )\n\n            self.summary_writer = summary_writer\n            self.log.notice(\'Runners started.\')\n\n        except:\n            msg = \'start() exception occurred\' + \\\n                \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def fast_adapt_step(self, sess, batch_size, on_policy_batch, off_policy_batch, rp_batch, make_summary=False):\n        """"""\n        One step of test_policy adaptation.\n\n        Args:\n            sess:                   tensorflow.Session obj.\n            batch_size:             train mini-batch size\n            on_policy_batch:        `on_policy` train data\n            off_policy_batch:       `off_policy` train data or None\n            rp_batch:               \'reward_prediction` train data or None\n            make_summary:           bool, if True - compute model summary\n\n        Returns:\n            model summary or None\n        """"""\n        # Sample from train distribution:\n        on_mini_batch = self.train_aac.sample_batch(on_policy_batch, batch_size)\n        off_mini_batch = self.train_aac.sample_batch(off_policy_batch, batch_size)\n        rp_mini_batch = self.train_aac.sample_batch(rp_batch, batch_size)\n\n        feed_dict = self.train_aac._get_main_feeder(sess, on_mini_batch, off_mini_batch, rp_mini_batch, True)\n\n        if make_summary:\n            fetches = [self.train_op, self.train_aac.model_summary_op]\n        else:\n            fetches = [self.train_op]\n\n        # Update pi_prime parameters wrt sampled data:\n        fetched = sess.run(fetches, feed_dict=feed_dict)\n\n        # self.log.warning(\'Train gradients ok.\')\n\n        if make_summary:\n            summary =  fetched[-1]\n\n        else:\n            summary = None\n\n        return summary\n\n    def train_step(self, sess, data_config):\n        """"""\n        Collects train task data and updates test policy parameters (fast adaptation).\n\n        Args:\n            sess:                   tensorflow.Session obj.\n            data_config:            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n\n        Returns:\n            batched train data\n\n        """"""\n        # Collect train distribution:\n        train_batch = self.train_aac.get_batch(\n            size=self.train_batch_size,\n            require_terminal=True,\n            same_trial=True,\n            data_sample_config=data_config\n        )\n\n        # for rollout in train_batch[\'on_policy\']:\n        #     self.log.warning(\n        #         \'Train data trial_num: {}\'.format(\n        #             np.asarray(rollout[\'state\'][\'metadata\'][\'trial_num\'])\n        #         )\n        #     )\n\n        # Process time-flat-alike (~iid) to treat as empirical data distribution over train task:\n        on_policy_batch, off_policy_batch, rp_batch = self.train_aac.process_batch(sess, train_batch)\n\n        # self.log.warning(\'Train data ok.\')\n\n        local_step = sess.run(self.global_step)\n        local_episode = sess.run(self.global_episode)\n        model_summary = None\n\n        # Extract all non-empty summaries:\n        ep_summary = [summary for summary in train_batch[\'ep_summary\'] if summary is not None]\n\n        # Perform number of test policy updates wrt. collected train data:\n        for i in range(self.fast_adapt_num_steps):\n            model_summary = self.fast_adapt_step(\n                sess,\n                batch_size=self.fast_adapt_batch_size,\n                on_policy_batch=on_policy_batch,\n                off_policy_batch=off_policy_batch,\n                rp_batch=rp_batch,\n                make_summary=(local_step + i) % self.model_summary_freq == 0\n            )\n            # self.log.warning(\'Batch {} Train gradients ok.\'.format(i))\n\n            # Write down summaries:\n            train_summary = dict(\n                render_summary=[None],\n                test_ep_summary=[None],\n                ep_summary=[ep_summary.pop() if len(ep_summary) > 0 else None]\n            )\n            self.train_aac.process_summary(\n                sess,\n                train_summary,\n                model_summary,\n                step=local_step + i,\n                episode=local_episode + i\n            )\n\n        return on_policy_batch, off_policy_batch, rp_batch\n\n    def meta_train_step(self, sess, data_config, on_policy_batch, off_policy_batch, rp_batch):\n        """"""\n        Collects data from source domain test task and performs meta-update to shared parameters vector.\n        Writes down relevant summaries.\n\n        Args:\n            sess:                   tensorflow.Session obj.\n            data_config:            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n            on_policy_batch:        `on_policy` train data\n            off_policy_batch:       `off_policy` train data or None\n            rp_batch:               \'reward_prediction` train data or None\n\n        """"""\n        done = False\n        while not done:\n            # Say `No` to redundant summaries:\n            wirte_model_summary = \\\n                self.local_steps % self.model_summary_freq == 0\n\n            # Collect test trajectory wrt updated test_policy parameters:\n            test_data = self.test_aac.get_data(\n                init_context=None,\n                data_sample_config=data_config\n            )\n            test_batch_size = 0  # TODO: adjust on/off/rp sizes\n            for rollout in test_data[\'on_policy\']:\n                test_batch_size += len(rollout[\'position\'])\n\n            test_feed_dict = self.test_aac.process_data(sess,,,,, test_data,,\n\n                             # self.log.warning(\'Test data rollout for step {} ok.\'.format(self.local_steps))\n                             #\n                             # self.log.warning(\n                             #     \'Test data trial_num: {}\'.format(\n                             #         np.asarray(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n                             #     )\n                             # )\n\n                             # Sample train data of same size:\n                             feed_dict = self.train_aac._get_main_feeder(\n                sess,\n                self.train_aac.sample_batch(on_policy_batch, test_batch_size),\n                self.train_aac.sample_batch(off_policy_batch, test_batch_size),\n                self.train_aac.sample_batch(rp_batch, test_batch_size),\n                True\n            )\n            # Add test trajectory:\n            feed_dict.update(test_feed_dict)\n\n            # Perform meta-update:\n            if wirte_model_summary:\n                meta_fetches = [self.meta_train_op, self.test_aac.model_summary_op, self.inc_step]\n            else:\n                meta_fetches = [self.meta_train_op, self.inc_step]\n\n            meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict)\n\n            # self.log.warning(\'Meta-gradients ok.\')\n\n            if wirte_model_summary:\n                meta_model_summary = meta_fetched[-2]\n\n            else:\n                meta_model_summary = None\n\n            # Write down summaries:\n            self.test_aac.process_summary(sess, test_data, meta_model_summary)\n            self.local_steps += 1\n\n            # If test episode ended?\n            done = np.asarray(test_data[\'terminal\']).any()\n\n    def meta_test_step(self, sess, data_config, on_policy_batch, off_policy_batch, rp_batch):\n        """"""\n        Validates adapted policy on data from target domain test task.\n        Writes down relevant summaries.\n\n        Args:\n            sess:                   tensorflow.Session obj.\n            data_config:            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n            on_policy_batch:        `on_policy` train data\n            off_policy_batch:       `off_policy` train data or None\n            rp_batch:               \'reward_prediction` train data or None\n\n        """"""\n        done = False\n        while not done:\n            # Collect test trajectory:\n            test_data = self.test_aac.get_data(\n                init_context=None,\n                data_sample_config=data_config\n            )\n\n            # self.log.warning(\'Target test rollout ok.\')\n            # self.log.warning(\n            #     \'Test data target trial_num: {}\'.format(\n            #         np.asarray(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n            #     )\n            # )\n            # self.log.warning(\'target_render_ep_summary: {}\'.format(test_data[\'render_summary\']))\n\n            # Write down summaries:\n            self.test_aac.process_summary(sess, test_data)\n\n            # If test episode ended?\n            done = np.asarray(test_data[\'terminal\']).any()\n\n    def process(self, sess):\n        """"""\n        Meta-train procedure for one-shot learning/\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            # Copy from parameter server:\n            sess.run(self.train_aac.sync_pi)\n            sess.run(self.test_aac.sync_pi)\n\n            #self.log.warning(\'Sync ok.\')\n\n            # Decide on data configuration for train/test trajectories,\n            # such as all data will come from same trial (maybe different episodes)\n            # and trial type as well (~from source or target domain):\n            # note: data_config counters get updated once per process() call\n            train_data_config = self.train_aac.get_sample_config(mode=1)  # master env., draws trial\n            test_data_config = self.train_aac.get_sample_config(mode=0)   # slave env, catches up with same trial\n\n            # If data comes from source or target domain:\n            is_target = train_data_config[\'trial_config\'][\'sample_type\']\n\n            # self.log.warning(\'PROCESS_train_data_config: {}\'.format(train_data_config))\n            # self.log.warning(\'PROCESS_test_data_config: {}\'.format(test_data_config))\n\n            # Fast adaptation step:\n            # collect train trajectories, process time-flat-alike (~iid) to treat as empirical data distribution\n            # over train task and adapt test_policy wrt. train experience:\n            on_policy_batch, off_policy_batch, rp_batch = self.train_step(sess, train_data_config)\n\n            # Slow adaptation step:\n            if is_target:\n                # Meta-test:\n                # self.log.warning(\'Running meta-test episode...\')\n                self.meta_test_step(sess,test_data_config, on_policy_batch, off_policy_batch, rp_batch)\n\n            else:\n                # Meta-train:\n                # self.log.warning(\'Running meta-train episode...\')\n                self.meta_train_step(sess,test_data_config, on_policy_batch, off_policy_batch, rp_batch)\n\n        except:\n            msg = \'process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n'"
btgym/research/metalearn_2/loss.py,14,"b""import tensorflow as tf\nimport  numpy as np\nfrom btgym.algorithms.math_utils import cat_entropy\n\ndef meta_loss_def_1_0(\n        act_target_train,\n        act_target_test,\n        adv_target_train,\n        adv_target_test,\n        r_target_train,\n        r_target_test,\n        pi_logits_train,\n        pi_logits_test,\n        pi_vf_train,\n        pi_vf_test,\n        pi_prime_logits,\n        entropy_beta,\n        epsilon=None,\n        name='_meta_',\n        verbose=False\n):\n    with tf.name_scope(name + '/meta'):\n        neg_pi_log_prob_train = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=pi_logits_train,\n            labels=act_target_train\n        )\n        neg_pi_log_prob_test = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=pi_logits_test,\n            labels=act_target_test\n        )\n        pi_loss = tf.reduce_mean(\n            (neg_pi_log_prob_train + neg_pi_log_prob_test) * adv_target_test\n        )\n        vf_loss_train = 0.5 * tf.losses.mean_squared_error(r_target_test, pi_vf_train)\n        vf_loss_test = 0.5 * tf.losses.mean_squared_error(r_target_test, pi_vf_test)\n\n        entropy = tf.reduce_mean(cat_entropy(pi_logits_test))\n\n        loss = pi_loss + vf_loss_test + vf_loss_train - entropy * entropy_beta\n\n        mean_vf_test = tf.reduce_mean(pi_vf_test)\n        mean_vf_train = tf.reduce_mean(pi_vf_train)\n\n        summaries = [\n            tf.summary.scalar('meta_policy_loss', pi_loss),\n            tf.summary.scalar('meta_value_loss_test', vf_loss_test),\n        ]\n        if verbose:\n            summaries += [\n                tf.summary.scalar('entropy', entropy),\n                tf.summary.scalar('value_fn_test', mean_vf_test),\n                tf.summary.scalar('value_fn_train', mean_vf_train)\n            ]\n\n    return loss, summaries\n"""
btgym/research/mldg/__init__.py,0,b''
btgym/research/mldg/aac.py,27,"b'import tensorflow as tf\nimport numpy as np\n\nimport sys\nfrom logbook import Logger, StreamHandler\nfrom btgym.research.gps.aac import GuidedAAC\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\n\nfrom btgym.algorithms.nn.layers import noisy_linear\n\n\nclass SubAAC(GuidedAAC):\n    """"""\n    Sub AAC trainer as lower-level part of meta-optimisation algorithm.\n    """"""\n\n    def __init__(\n            self,\n            trial_source_target_cycle=(1, 0),\n            num_episodes_per_trial=1,\n            **kwargs\n    ):\n        super(SubAAC, self).__init__(**kwargs)\n        self.current_data = None\n        self.current_feed_dict = None\n\n        # Trials sampling control:\n        self.num_source_trials = trial_source_target_cycle[0]\n        self.num_target_trials = trial_source_target_cycle[-1]\n        self.num_episodes_per_trial = num_episodes_per_trial\n\n        # Note that only master (test runner) is requesting trials\n\n        self.current_source_trial = 0\n        self.current_target_trial = 0\n        self.current_trial_mode = 0  # source\n        self.current_episode = 0\n\n    def process(self, sess):\n        """"""\n        self.process() logic is defined by meta-trainer.\n        """"""\n        pass\n\n    def get_sample_config(self, mode=0):\n        """"""\n        Returns environment configuration parameters for next episode to sample.\n\n        Args:\n              mode:     bool, False for slave (train data), True for master (test data)\n\n        Returns:\n            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n        """"""\n\n        new_trial = 0\n        if mode:\n            # Only master environment updates counters:\n            if self.current_episode >= self.num_episodes_per_trial:\n                # Reset episode counter:\n                self.current_episode = 0\n\n                # Request new trial:\n                new_trial = 1\n                # Decide on trial type (source/target):\n                if self.current_source_trial >= self.num_source_trials:\n                    # Time to switch to target mode:\n                    self.current_trial_mode = 1\n                    # Reset counters:\n                    self.current_source_trial = 0\n                    self.current_target_trial = 0\n\n                if self.current_target_trial >= self.num_target_trials:\n                    # Vise versa:\n                    self.current_trial_mode = 0\n                    self.current_source_trial = 0\n                    self.current_target_trial = 0\n\n                # Update counter:\n                if self.current_trial_mode:\n                    self.current_target_trial += 1\n                else:\n                    self.current_source_trial += 1\n\n            self.current_episode += 1\n        else:\n            new_trial = 1  # slave env. gets new trial anyway\n\n        # Compose btgym.datafeed.base.EnvResetConfig-consistent dict:\n        sample_config = dict(\n            episode_config=dict(\n                get_new=True,\n                sample_type=int(not mode),\n                b_alpha=1.0,\n                b_beta=1.0\n            ),\n            trial_config=dict(\n                get_new=new_trial,\n                sample_type=self.current_trial_mode,\n                b_alpha=1.0,\n                b_beta=1.0\n            )\n        )\n        return sample_config\n\n\nclass AMLDG():\n    """"""\n    Asynchronous implementation of MLDG algorithm (by Da Li et al.)\n    for one-shot adaptation in dynamically changing environments.\n\n    This class is AAC wrapper; relies on sub-AAC classes to make separate policy networks\n    for train/test data streams, performs data streams synchronization according to algorithm logic\n    via data_config dictionaries; performs actual data checks to prevent test information leakage.\n\n    Papers:\n        Da Li et al.,\n         ""Learning to Generalize: Meta-Learning for Domain Generalization""\n         https://arxiv.org/abs/1710.03463\n\n        Maruan Al-Shedivat et al.,\n        ""Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments""\n        https://arxiv.org/abs/1710.03641\n\n    """"""\n    def __init__(\n            self,\n            env,\n            task,\n            log_level,\n            aac_class_ref=SubAAC,\n            runner_config=None,\n            opt_decay_steps=None,\n            opt_end_learn_rate=None,\n            opt_learn_rate=1e-4,\n            fast_opt_learn_rate=1e-3,\n            opt_max_env_steps=10 ** 7,\n            aac_lambda=1.0,\n            guided_lambda=1.0,\n            rollout_length=20,\n            trial_source_target_cycle=(1, 0),\n            num_episodes_per_trial=1,  # one-shot adaptation\n            _aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            name=\'AMLDG\',\n            **kwargs\n    ):\n        try:\n            self.aac_class_ref = aac_class_ref\n            self.task = task\n            self.name = name\n            self.summary_writer = None\n\n            self.opt_learn_rate = opt_learn_rate\n            self.opt_max_env_steps = opt_max_env_steps\n            self.fast_opt_learn_rate = fast_opt_learn_rate\n\n            if opt_end_learn_rate is None:\n                self.opt_end_learn_rate = self.opt_learn_rate\n            else:\n                self.opt_end_learn_rate = opt_end_learn_rate\n\n            if opt_decay_steps is None:\n                self.opt_decay_steps = self.opt_max_env_steps\n            else:\n                self.opt_decay_steps = opt_decay_steps\n\n            StreamHandler(sys.stdout).push_application()\n            self.log = Logger(\'{}_{}\'.format(name, task), level=log_level)\n            self.rollout_length = rollout_length\n\n            if runner_config is None:\n                self.runner_config = {\n                    \'class_ref\': BaseSynchroRunner,\n                    \'kwargs\': {},\n                }\n            else:\n                self.runner_config = runner_config\n\n            self.env_list = env\n\n            assert isinstance(self.env_list, list) and len(self.env_list) == 2, \\\n                \'Expected pair of environments, got: {}\'.format(self.env_list)\n\n            # Instantiate two sub-trainers: one for meta-test and one for meta-train environments:\n\n            self.runner_config[\'kwargs\'][\'data_sample_config\'] = {\'mode\': 1}  # master\n            self.runner_config[\'kwargs\'][\'name\'] = \'master\'\n\n            self.train_aac = aac_class_ref(\n                env=self.env_list[0],  # train data will be master environment\n                task=self.task,\n                log_level=log_level,\n                runner_config=self.runner_config,\n                opt_learn_rate=self.fast_opt_learn_rate,  # non-decaying, used for fast pi_prime adaptation\n                opt_max_env_steps=self.opt_max_env_steps,\n                aac_lambda=aac_lambda,\n                guided_lambda=guided_lambda,\n                rollout_length=self.rollout_length,\n                trial_source_target_cycle=trial_source_target_cycle,\n                num_episodes_per_trial=num_episodes_per_trial,\n                _use_target_policy=False,\n                _use_global_network=True,\n                _aux_render_modes=_aux_render_modes,\n                name=self.name + \'/metaTrain\',\n                **kwargs\n            )\n\n            self.runner_config[\'kwargs\'][\'data_sample_config\'] = {\'mode\': 0}  # slave\n            self.runner_config[\'kwargs\'][\'name\'] = \'slave\'\n\n            self.test_aac = aac_class_ref(\n                env=self.env_list[-1],  # test data -> slave env.\n                task=self.task,\n                log_level=log_level,\n                runner_config=self.runner_config,\n                opt_learn_rate=0.0,  # test_aac.optimizer is not used\n                opt_max_env_steps=self.opt_max_env_steps,\n                aac_lambda=aac_lambda,\n                guided_lambda=guided_lambda,\n                rollout_length=self.rollout_length,\n                trial_source_target_cycle=trial_source_target_cycle,\n                num_episodes_per_trial=num_episodes_per_trial,\n                _use_target_policy=False,\n                _use_global_network=False,\n                global_step_op=self.train_aac.global_step,\n                global_episode_op=self.train_aac.global_episode,\n                inc_episode_op=self.train_aac.inc_episode,\n                _aux_render_modes=_aux_render_modes,\n                name=self.name + \'/metaTest\',\n                **kwargs\n            )\n\n            self.local_steps = self.train_aac.local_steps\n            self.model_summary_freq = self.train_aac.model_summary_freq\n\n            self._make_train_op()\n\n            self.test_aac.model_summary_op = tf.summary.merge(\n                [self.test_aac.model_summary_op, self._combine_meta_summaries()],\n                name=\'meta_model_summary\'\n            )\n\n        except:\n            msg = \'AMLDG.__init()__ exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def _make_train_op(self):\n        """"""\n        Defines tensors holding training op graph for meta-train, meta-test and meta-optimisation.\n        """"""\n        # Handy aliases:\n        pi = self.train_aac.local_network  # local meta-train policy\n        pi_prime = self.test_aac.local_network  # local meta-test policy\n        pi_global = self.train_aac.network  # global shared policy\n\n        self.test_aac.sync = self.test_aac.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi.var_list)]\n        )\n        self.test_aac.sync_pi_global = self.test_aac.sync_global = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi_global.var_list)]\n        )\n        self.train_aac.sync_pi_local = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_prime.var_list)]\n        )\n\n        # Shared counters:\n        self.global_step = self.train_aac.global_step\n        self.global_episode = self.train_aac.global_episode\n\n        self.test_aac.global_step = self.train_aac.global_step\n        self.test_aac.global_episode = self.train_aac.global_episode\n        self.test_aac.inc_episode = self.train_aac.inc_episode\n        self.train_aac.inc_episode = None\n\n        # Meta-opt. loss:\n        self.loss = self.train_aac.loss + self.test_aac.loss\n\n        # Clipped gradients:\n        self.train_aac.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.train_aac.loss, pi.var_list),\n            40.0\n        )\n        self.test_aac.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.test_aac.loss, pi_prime.var_list),\n            40.0\n        )\n        # Aliases:\n        pi.grads = self.train_aac.grads\n        pi_prime.grads = self.test_aac.grads\n\n        # Meta_optimisation gradients as an average of meta-train and meta-test gradients:\n        self.grads = []\n        for g1, g2 in zip(pi.grads, pi_prime.grads):\n            if g1 is not None and g2 is not None:\n                meta_g = (g1 + g2) / 2.0\n\n            else:\n                meta_g = None  # need to map correctly to vars\n\n            self.grads.append(meta_g)\n\n        # Gradients to update local meta-test policy (from train data):\n        train_grads_and_vars = list(zip(pi.grads, pi_prime.var_list))\n\n        # self.log.warning(\'train_grads_and_vars_len: {}\'.format(len(train_grads_and_vars)))\n\n        # Meta-gradients to be sent to parameter server:\n        meta_grads_and_vars = list(zip(self.grads, pi_global.var_list))\n\n        # Remove empty entries:\n        meta_grads_and_vars = [(g, v) for (g, v) in meta_grads_and_vars if g is not None]\n\n        # for item in meta_grads_and_vars:\n        #     self.log.warning(\'\\nmeta_g_v: {}\'.format(item))\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(self.train_aac.local_network.on_state_in.keys())\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        self.train_aac.inc_step = self.train_aac.global_step.assign_add(\n            tf.shape(self.test_aac.local_network.on_state_in[\'external\'])[0]\n        )\n        self.inc_step = self.train_aac.inc_step\n        # Pi to pi_prime local adaptation op:\n        # self.train_op = self.train_aac.optimizer.apply_gradients(train_grads_and_vars)\n\n        # self.fast_opt = tf.train.GradientDescentOptimizer(self.alpha_rate)\n        self.fast_opt = tf.train.GradientDescentOptimizer(self.fast_opt_learn_rate)\n        self.train_op = self.fast_opt.apply_gradients(train_grads_and_vars)\n\n        #  Learning rate annealing:\n        self.learn_rate_decayed = tf.train.polynomial_decay(\n            self.opt_learn_rate,\n            self.global_step + 1,\n            self.opt_decay_steps,\n            self.opt_end_learn_rate,\n            power=1,\n            cycle=False,\n        )\n\n        # Optimizer for meta-update, sharing same learn rate (change?):\n        self.optimizer = tf.train.AdamOptimizer(self.learn_rate_decayed, epsilon=1e-5)\n\n        # Global meta-optimisation op:\n        self.meta_train_op = self.optimizer.apply_gradients(meta_grads_and_vars)\n\n        self.log.debug(\'meta_train_op defined\')\n\n    def _combine_meta_summaries(self):\n        """"""\n        Additional summaries here.\n        """"""\n        meta_model_summaries = [\n            tf.summary.scalar(\'meta_grad_global_norm\', tf.global_norm(self.grads)),\n            tf.summary.scalar(\'total_meta_loss\', self.loss),\n            #tf.summary.scalar(\'alpha_learn_rate\', self.alpha_rate),\n            #tf.summary.scalar(\'alpha_learn_rate_loss\', self.alpha_rate_loss)\n        ]\n        return meta_model_summaries\n\n    def start(self, sess, summary_writer, **kwargs):\n        """"""\n        Executes all initializing operations,\n        starts environment runner[s].\n        Supposed to be called by parent worker just before training loop starts.\n\n        Args:\n            sess:           tf session object.\n            kwargs:         not used by default.\n        """"""\n        try:\n            # Copy weights from global to local:\n            sess.run(self.train_aac.sync_pi)\n            sess.run(self.test_aac.sync_pi)\n\n            # Start thread_runners:\n            self.train_aac._start_runners(   # master first\n                sess,\n                summary_writer,\n                init_context=None,\n                data_sample_config=self.train_aac.get_sample_config(mode=1)\n            )\n            self.test_aac._start_runners(\n                sess,\n                summary_writer,\n                init_context=None,\n                data_sample_config=self.test_aac.get_sample_config(mode=0)\n            )\n\n            self.summary_writer = summary_writer\n            self.log.notice(\'Runners started.\')\n\n        except:\n            msg = \'start() exception occurred\' + \\\n                \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def assert_trial_type(self, data, type):\n        """"""\n        Prevent information leakage:\n        check actual trial type consistency; if failed - possible data sampling logic fault, issue warning.\n\n        Args:\n            data:\n            type:   bool\n\n        Returns:\n\n        """"""\n        try:\n            assert (np.asarray(data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_type\']) == type).all()\n        except AssertionError:\n            self.log.warning(\n                \'Source trial assertion failed!\\nExpected: `trial_type`={}\\nGot metadata: {}\'.\\\n                    format(type, data[\'on_policy\'][0][\'state\'][\'metadata\'])\n            )\n\n    def assert_same_trial(self, train_data, test_data):\n        """"""\n        Prevent information leakage-II:\n        check if both data streams come from same trial.\n\n        Args:\n            train_data:\n            test_data:\n        """"""\n        train_trial_chksum = np.average(train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n        test_trial_chksum = np.average(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n        try:\n            assert train_trial_chksum == test_trial_chksum\n        except AssertionError:\n            msg1 = \'Trials match assertion failed!\\nGot train metadata: {},\\nGot test metadata:  {}\'. \\\n                format(\n                train_data[\'on_policy\'][0][\'state\'][\'metadata\'],\n                test_data[\'on_policy\'][0][\'state\'][\'metadata\']\n            )\n            msg2 = \'\\nTrain_trial_chksum: {}, test_trial_chksum: {}\'.format(train_trial_chksum, test_trial_chksum)\n            self.log.warning(msg1 + msg2)\n\n    def assert_episode_type(self, data, type):\n        """"""\n        Prevent information leakage-III:\n        check episode type for consistency; if failed - possible data sampling logic fault, issue warning.\n\n        Args:\n            train_data:\n            test_data:\n        """"""\n        try:\n            assert (np.asarray(data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\']) == type).all()\n\n        except AssertionError:\n            msg = \'Episode types assertion failed!\\nExpected episode_type: {},\\nGot episode metadata:  {}\'.\\\n                format(\n                    type,\n                    data[\'on_policy\'][0][\'state\'][\'metadata\']\n            )\n            self.log.warning(msg)\n\n    def process(self, sess):\n        """"""\n        Meta-train/test procedure for one-shot learning.\n        Single call runs single meta-test episode.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            # Copy from parameter server:\n            sess.run(self.train_aac.sync_pi)\n            sess.run(self.test_aac.sync_pi)\n            # self.log.warning(\'Init Sync ok.\')\n\n            # Get data configuration,\n            # (want both data streams come from  same trial,\n            # and trial type we got can be either from source or target domain);\n            # note: data_config counters get updated once per .process() call\n            train_data_config = self.train_aac.get_sample_config(mode=1)  # master env., samples trial\n            test_data_config = self.train_aac.get_sample_config(mode=0)   # slave env, catches up with same trial\n\n            # self.log.warning(\'train_data_config: {}\'.format(train_data_config))\n            # self.log.warning(\'test_data_config: {}\'.format(test_data_config))\n\n            # If this step data comes from source or target domain\n            # (i.e. is it either meta-optimised or true test episode):\n            is_target = train_data_config[\'trial_config\'][\'sample_type\']\n            done = False\n\n            # Collect initial meta-train trajectory rollout:\n            train_data = self.train_aac.get_data(data_sample_config=train_data_config, force_new_episode=True)\n            feed_dict = self.train_aac.process_data(sess, train_data, is_train=True,pi=self.train_aac.local_network)\n\n            # self.log.warning(\'Init Train data ok.\')\n\n            # Disable possibility of master data runner acquiring new trials,\n            # in case meta-train episode termintaes earlier than meta-test -\n            # we than need to get additional meta-train trajectories from exactly same distribution (trial):\n            train_data_config[\'trial_config\'][\'get_new\'] = 0\n\n            roll_num = 0\n\n            # Collect entire meta-test episode rollout by rollout:\n            while not done:\n                # self.log.warning(\'Roll #{}\'.format(roll_num))\n\n                wirte_model_summary = \\\n                    self.local_steps % self.model_summary_freq == 0\n\n                # self.log.warning(\n                #     \'Train data trial_num: {}\'.format(\n                #         np.asarray(train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n                #     )\n                # )\n\n                # Paranoid checks against data sampling logic faults to prevent possible cheating:\n                train_trial_chksum = np.average(train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n\n                # Update pi_prime parameters wrt collected train data:\n                if wirte_model_summary:\n                    fetches = [self.train_op, self.train_aac.model_summary_op]\n                else:\n                    fetches = [self.train_op]\n\n                fetched = sess.run(fetches, feed_dict=feed_dict)\n\n                # self.log.warning(\'Train gradients ok.\')\n\n                # Collect test rollout using updated pi_prime policy:\n                test_data = self.test_aac.get_data(data_sample_config=test_data_config)\n\n                # If meta-test episode has just ended?\n                done = np.asarray(test_data[\'terminal\']).any()\n\n                # self.log.warning(\n                #     \'Test data trial_num: {}\'.format(\n                #         np.asarray(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n                #     )\n                # )\n\n                test_trial_chksum = np.average(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n\n                # Ensure slave runner data consistency, can correct if episode just started:\n                if roll_num == 0 and train_trial_chksum != test_trial_chksum:\n                    test_data = self.test_aac.get_data(data_sample_config=test_data_config, force_new_episode=True)\n                    done = np.asarray(test_data[\'terminal\']).any()\n                    faulty_chksum = test_trial_chksum\n                    test_trial_chksum = np.average(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n\n                    self.log.warning(\n                        \'Test trial corrected: {} -> {}\'.format(faulty_chksum, test_trial_chksum)\n                    )\n\n                # self.log.warning(\n                #     \'roll # {}: train_trial_chksum: {}, test_trial_chksum: {}\'.\n                #         format(roll_num, train_trial_chksum, test_trial_chksum)\n                # )\n\n                if train_trial_chksum != test_trial_chksum:\n                    # Still got error? - highly probable algorithm logic fault. Issue warning.\n                    msg = \'Train/test trials mismatch found!\\nGot train trials: {},\\nTest trials: {}\'. \\\n                        format(\n                        train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'][0],\n                        test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'][0]\n                        )\n                    msg2 = \'Train data config: {}\\n Test data config: {}\'.format(train_data_config, test_data_config)\n\n                    self.log.warning(msg)\n                    self.log.warning(msg2)\n\n                # Check episode type for consistency; if failed - another data sampling logic fault, warn:\n                try:\n                    assert (np.asarray(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\']) == 1).all()\n                    assert (np.asarray(train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\']) == 0).all()\n                except AssertionError:\n                    msg = \'Train/test episodes types mismatch found!\\nGot train ep. type: {},\\nTest ep.type: {}\'. \\\n                        format(\n                        train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\'],\n                        test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\']\n                    )\n                    self.log.warning(msg)\n\n                # self.log.warning(\'Test data ok.\')\n\n                if not is_target:\n                    # Process test data and perform meta-optimisation step:\n                    feed_dict.update(\n                        self.test_aac.process_data(sess, test_data, is_train=True, pi=self.test_aac.local_network)\n                    )\n\n                    if wirte_model_summary:\n                        meta_fetches = [self.meta_train_op, self.test_aac.model_summary_op, self.inc_step]\n                    else:\n                        meta_fetches = [self.meta_train_op, self.inc_step]\n\n                    meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict)\n\n                    # self.log.warning(\'Meta-gradients ok.\')\n                else:\n                    # True test, no updates sent to parameter server:\n                    meta_fetched = [None, None]\n\n                    # self.log.warning(\'Meta-opt. rollout ok.\')\n\n                if wirte_model_summary:\n                    meta_model_summary = meta_fetched[-2]\n                    model_summary = fetched[-1]\n\n                else:\n                    meta_model_summary = None\n                    model_summary = None\n\n                # Next step housekeeping:\n                # copy from parameter server, not while testing:\n                if not is_target:\n                    sess.run(self.train_aac.sync_pi)\n\n                # Copy from pi to pi-prime:\n                sess.run(self.test_aac.sync_pi)\n                # self.log.warning(\'Sync ok.\')\n\n                # Collect next train trajectory rollout:\n                train_data = self.train_aac.get_data(data_sample_config=train_data_config)\n                feed_dict = self.train_aac.process_data(sess,train_data, is_train=True, pi=self.train_aac.local_network)\n                # self.log.warning(\'Train data ok.\')\n\n                # Write down summaries:\n                self.test_aac.process_summary(sess, test_data, meta_model_summary)\n                self.train_aac.process_summary(sess, train_data, model_summary)\n                self.local_steps += 1\n                roll_num += 1\n        except:\n            msg = \'process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n\nclass AMLDG_2(AMLDG):\n    """"""\n    FAILED do not use\n    """"""\n\n    def __init__(self, name=\'AMLDGv2\', **kwargs):\n        super(AMLDG_2, self).__init__(name=name, **kwargs)\n\n    def process(self, sess):\n        """"""\n        Meta-train/test procedure for one-shot learning.\n        Single call runs single meta-test episode.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            # Copy from parameter server:\n            sess.run(self.train_aac.sync_pi)\n            sess.run(self.test_aac.sync_pi)\n            # self.log.warning(\'Init Sync ok.\')\n\n            # Get data configuration,\n            #  Want data streams come from  different trials from same doman!\n            # note: data_config counters get updated once per .process() call\n            train_data_config = self.train_aac.get_sample_config(mode=1)  # master env., samples trial\n            test_data_config = self.train_aac.get_sample_config(mode=0)  # slave env, catches up with same trial\n\n            # self.log.warning(\'train_data_config: {}\'.format(train_data_config))\n            # self.log.warning(\'test_data_config: {}\'.format(test_data_config))\n\n            # If this step data comes from source or target domain\n            # (i.e. is it either meta-optimised or true test episode):\n            is_target = train_data_config[\'trial_config\'][\'sample_type\']\n            done = False\n\n            # Collect initial meta-train trajectory rollout:\n            train_data = self.train_aac.get_data(data_sample_config=train_data_config, force_new_episode=True)\n            feed_dict = self.train_aac.process_data(sess, train_data,is_train=True, pi=self.train_aac.local_network)\n\n            # self.log.warning(\'Init Train data ok.\')\n\n            # For target domain only:\n            # Disable possibility of master data runner acquiring new trials,\n            # in case meta-train episode termintaes earlier than meta-test -\n            # we than need to get additional meta-train trajectories from exactly same distribution (trial):\n            if is_target:\n                train_data_config[\'trial_config\'][\'get_new\'] = 0\n\n            roll_num = 0\n\n            # Collect entire meta-test episode rollout by rollout:\n            while not done:\n                # self.log.warning(\'Roll #{}\'.format(roll_num))\n\n                wirte_model_summary = \\\n                    self.local_steps % self.model_summary_freq == 0\n\n                # if not is_target:\n                # Update pi_prime parameters wrt collected train data:\n                if wirte_model_summary:\n                    fetches = [self.train_op, self.train_aac.model_summary_op]\n                else:\n                    fetches = [self.train_op]\n\n                fetched = sess.run(fetches, feed_dict=feed_dict)\n                # else:\n                #     # Target domain test, no local policy update:\n                #     fetched = [None, None]\n\n                # self.log.warning(\'Train gradients ok.\')\n\n                # Collect test rollout using [updated] pi_prime policy:\n                test_data = self.test_aac.get_data(data_sample_config=test_data_config)\n\n                # self.log.warning(\'Test_data:\')\n                # for k, v in test_data.items():\n                #     self.log.warning(\n                #         \'Key: {}, value_type: {},  value_shape: {}\'.format(k, type(v), np.asarray(v).shape)\n                #     )\n\n                # If meta-test episode has just ended?\n                done = np.asarray(test_data[\'terminal\']).any()\n\n                # # Reset master env to new trial to decorellate: TODO: quick fix, this one roll gets waisted, change\n                # if roll_num == 0:\n                #     train_data = self.train_aac.get_data(data_sample_config=train_data_config, force_new_episode=True)\n                #     self.assert_trial_type(train_data, is_target)\n                self.assert_trial_type(train_data, is_target)\n                self.assert_trial_type(test_data, is_target)\n\n                self.assert_episode_type(train_data, 0)\n                self.assert_episode_type(test_data, 1)\n\n                # self.assert_same_trial(train_data, test_data)\n\n                # self.log.warning(\'Test data ok.\')\n\n                if not is_target:\n                    # Process test data and perform meta-optimisation step:\n                    feed_dict.update(\n                        self.test_aac.process_data(sess, test_data,is_train=True, pi=self.test_aac.local_network)\n                    )\n\n                    if wirte_model_summary:\n                        meta_fetches = [self.meta_train_op, self.test_aac.model_summary_op, self.inc_step]\n                    else:\n                        meta_fetches = [self.meta_train_op, self.inc_step]\n\n                    meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict)\n\n                    # self.log.warning(\'Meta-gradients ok.\')\n                else:\n                    self.assert_same_trial(train_data, test_data)\n                    # Target domain test, no updates sent to parameter server:\n                    meta_fetched = [None, None]\n\n                    # self.log.warning(\'Meta-opt. rollout ok.\')\n\n                if wirte_model_summary:\n                    meta_model_summary = meta_fetched[-2]\n                    model_summary = fetched[-1]\n\n                else:\n                    meta_model_summary = None\n                    model_summary = None\n\n                # Next step housekeeping:\n                # copy from parameter server:\n                sess.run(self.train_aac.sync_pi)\n                sess.run(self.test_aac.sync_pi)  # TODO: maybe not?\n                # self.log.warning(\'Sync ok.\')\n\n                # if not is_target:\n                # Collect next train trajectory rollout:\n                train_data = self.train_aac.get_data(data_sample_config=train_data_config)\n\n                # Concatenate new train and previous step test data:\n                joined_data = {\n                    k: train_data[k] + test_data[k] for k in [\'on_policy\', \'terminal\', \'off_policy\', \'off_policy_rp\']\n                }\n                train_data.update(joined_data)\n\n                # self.log.warning(\'Train_data:\')\n                # for k, v in train_data.items():\n                #     self.log.warning(\n                #         \'Key: {}, value_type: {},  value_shape: {}\'.format(k, type(v), np.asarray(v).shape)\n                #     )\n\n                feed_dict = self.train_aac.process_data(sess, train_data,is_train=True, pi=self.train_aac.local_network)\n\n                # self.log.warning(\'Train data ok.\')\n                self.train_aac.process_summary(sess, train_data, model_summary)\n\n                # test summary anyway:\n                self.test_aac.process_summary(sess, test_data, meta_model_summary)\n\n                self.local_steps += 1\n                roll_num += 1\n        except:\n            msg = \'process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n\nclass AMLDG_3(AMLDG):\n    """"""\n    FAILED do not use\n    Closed-loop meta-update.\n    """"""\n\n    def __init__(self, fast_learn_rate_train=0.1, fast_learn_rate_test=0.1, name=\'AMLDGv3\', **kwargs):\n        self.fast_learn_rate_train = fast_learn_rate_train\n        self.fast_learn_rate_test = fast_learn_rate_test\n        super(AMLDG_3, self).__init__(name=name, **kwargs)\n\n    def _make_train_op(self):\n        """"""\n        Defines tensors holding training op graph for meta-train, meta-test and meta-optimisation.\n        """"""\n        # Handy aliases:\n        pi = self.train_aac.local_network  # local meta-train policy\n        pi_prime = self.test_aac.local_network  # local meta-test policy\n        pi_global = self.train_aac.network  # global shared policy\n\n        self.test_aac.sync = self.test_aac.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi.var_list)]\n        )\n        self.train_aac.sync_pi_local = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_prime.var_list)]\n        )\n\n        # Shared counters:\n        self.global_step = self.train_aac.global_step\n        self.global_episode = self.train_aac.global_episode\n\n        self.test_aac.global_step = self.train_aac.global_step\n        self.test_aac.global_episode = self.train_aac.global_episode\n        self.test_aac.inc_episode = self.train_aac.inc_episode\n        self.train_aac.inc_episode = None\n        self.inc_step = self.train_aac.inc_step\n\n        # Meta-opt. loss:\n        self.loss = self.train_aac.loss + self.test_aac.loss\n\n        # Clipped gradients:\n        self.train_aac.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.train_aac.loss, pi.var_list),\n            40.0\n        )\n        self.test_aac.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.test_aac.loss, pi_prime.var_list),\n            40.0\n        )\n        # Aliases:\n        pi.grads = self.train_aac.grads\n        pi_prime.grads = self.test_aac.grads\n\n        # Meta_optimisation gradients as sum of meta-train and meta-test gradients:\n        self.grads = []\n        for g1, g2 in zip(pi.grads, pi_prime.grads):\n            if g1 is not None and g2 is not None:\n                meta_g = g1 + g2\n                # meta_g = (1 - self.meta_grads_scale) * g1 + self.meta_grads_scale * g2\n            else:\n                meta_g = None  # need to map correctly to vars\n\n            self.grads.append(meta_g)\n\n        # Gradients to update local meta-test policy (from train data):\n        train_grads_and_vars = list(zip(pi.grads, pi_prime.var_list))\n\n        # self.log.warning(\'train_grads_and_vars_len: {}\'.format(len(train_grads_and_vars)))\n\n        # Meta-gradients to be sent to parameter server:\n        meta_grads_and_vars = list(zip(self.grads, pi_global.var_list))\n\n        # Remove empty entries:\n        meta_grads_and_vars = [(g, v) for (g, v) in meta_grads_and_vars if g is not None]\n\n        # for item in meta_grads_and_vars:\n        #     self.log.warning(\'\\nmeta_g_v: {}\'.format(item))\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(self.train_aac.local_network.on_state_in.keys())\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        self.train_aac.inc_step = self.train_aac.global_step.assign_add(\n            tf.shape(self.train_aac.local_network.on_state_in[\'external\'])[0]\n        )\n        # Simple SGD, no average statisitics:\n        self.fast_optimizer_train = tf.train.GradientDescentOptimizer(self.fast_learn_rate_train)\n        self.fast_optimizer_test = tf.train.GradientDescentOptimizer(self.fast_learn_rate_test)\n\n        # Pi to pi_prime local adaptation op:\n        self.train_op = self.fast_optimizer_train.apply_gradients(train_grads_and_vars)\n\n        # Optimizer for meta-update, sharing same learn rate (change?):\n        self.optimizer = tf.train.AdamOptimizer(self.train_aac.train_learn_rate, epsilon=1e-5)\n\n        # Global meta-optimisation op:\n        self.meta_train_op = self.optimizer.apply_gradients(meta_grads_and_vars)\n\n        # Local meta-optimisation:\n        local_meta_grads_and_vars = list(zip(self.grads, pi_prime.var_list))\n        local_meta_grads_and_vars = [(g, v) for (g, v) in local_meta_grads_and_vars if g is not None]\n\n        self.local_meta_train_op = self.fast_optimizer_test.apply_gradients(local_meta_grads_and_vars)\n\n        self.log.debug(\'meta_train_op defined\')\n\n    def process(self, sess):\n        """"""\n        Meta-train/test procedure for one-shot learning.\n        Single call runs single meta-test episode.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            # Copy from parameter server into both policies:\n            sess.run(self.train_aac.sync_pi)\n            sess.run(self.test_aac.sync_pi)\n            # self.log.warning(\'Init Sync ok.\')\n\n            # Get data configuration,\n            # (want both data streams come from  same trial,\n            # and trial type we got can be either from source or target domain);\n            # note: data_config counters get updated once per .process() call\n            train_data_config = self.train_aac.get_sample_config(mode=1)  # master env., samples trial\n            test_data_config = self.train_aac.get_sample_config(mode=0)   # slave env, catches up with same trial\n\n            # self.log.warning(\'train_data_config: {}\'.format(train_data_config))\n            # self.log.warning(\'test_data_config: {}\'.format(test_data_config))\n\n            # If this episode data comes from source or target domain\n            # (i.e. is it either meta-optimised or true test episode):\n            is_target = train_data_config[\'trial_config\'][\'sample_type\']\n            done = False\n\n            # Collect initial meta-train trajectory rollout:\n            train_data = self.train_aac.get_data(data_sample_config=train_data_config, force_new_episode=True)\n            feed_dict =self.train_aac.process_data(sess, train_data,is_train=True, pi=self.train_aac.local_network)\n\n            # self.log.warning(\'Init Train data ok.\')\n\n            # Disable possibility of master data runner acquiring new trials,\n            # in case meta-train episode terminates earlier than meta-test -\n            # we than need to get additional meta-train trajectories from exactly same distribution (trial):\n            train_data_config[\'trial_config\'][\'get_new\'] = 0\n\n            roll_num = 0\n\n            # Collect entire meta-test episode rollout by rollout:\n            while not done:\n                # self.log.warning(\'Roll #{}\'.format(roll_num))\n\n                wirte_model_summary = \\\n                    self.local_steps % self.model_summary_freq == 0\n\n                # Paranoid checks against data sampling logic faults to prevent possible cheating:\n                train_trial_chksum = np.average(train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n\n                # Update pi_prime parameters wrt collected train data:\n                if wirte_model_summary:\n                    fetches = [self.train_op, self.train_aac.model_summary_op]\n                else:\n                    fetches = [self.train_op]\n\n                fetched = sess.run(fetches, feed_dict=feed_dict)\n\n                # self.log.warning(\'Train gradients ok.\')\n\n                # Collect test rollout using updated pi_prime policy:\n                test_data = self.test_aac.get_data(data_sample_config=test_data_config)\n\n                # If meta-test episode has just ended?\n                done = np.asarray(test_data[\'terminal\']).any()\n\n                test_trial_chksum = np.average(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n\n                # Ensure slave runner data consistency, can correct if episode just started:\n                if roll_num == 0 and train_trial_chksum != test_trial_chksum:\n                    test_data = self.test_aac.get_data(data_sample_config=test_data_config, force_new_episode=True)\n                    done = np.asarray(test_data[\'terminal\']).any()\n                    faulty_chksum = test_trial_chksum\n                    test_trial_chksum = np.average(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'])\n\n                    self.log.warning(\n                        \'Test trial corrected: {} -> {}\'.format(faulty_chksum, test_trial_chksum)\n                    )\n\n                if train_trial_chksum != test_trial_chksum:\n                    # Still got error? - highly probable algorithm logic fault. Issue warning.\n                    msg = \'Train/test trials mismatch found!\\nGot train trials: {},\\nTest trials: {}\'. \\\n                        format(\n                        train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'][0],\n                        test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'trial_num\'][0]\n                        )\n                    msg2 = \'Train data config: {}\\n Test data config: {}\'.format(train_data_config, test_data_config)\n\n                    self.log.warning(msg)\n                    self.log.warning(msg2)\n\n                # Check episode type for consistency; if failed - another data sampling logic fault, warn:\n                try:\n                    assert (np.asarray(test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\']) == 1).any()\n                    assert (np.asarray(train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\']) == 0).any()\n                except AssertionError:\n                    msg = \'Train/test episodes types mismatch found!\\nGot train ep. type: {},\\nTest ep.type: {}\'. \\\n                        format(\n                        train_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\'],\n                        test_data[\'on_policy\'][0][\'state\'][\'metadata\'][\'type\']\n                    )\n                    self.log.warning(msg)\n\n                # self.log.warning(\'Test data ok.\')\n\n                # Process test data and perform meta-optimisation step:\n                feed_dict.update(\n                    self.test_aac.process_data(sess, test_data, is_train=True, pi=self.test_aac.local_network)\n                )\n\n                if not is_target:\n                    # Update local pi_prime (with fast_learn_rate) and global shared parameters (via slow_learn_rate):\n                    if wirte_model_summary:\n                        meta_fetches = [\n                            self.meta_train_op,\n                            self.local_meta_train_op,\n                            self.test_aac.model_summary_op,\n                            self.inc_step\n                        ]\n                    else:\n                        # Only update local pi_prime:\n                        meta_fetches = [\n                            self.meta_train_op,\n                            self.local_meta_train_op,\n                            self.inc_step\n                        ]\n\n                    meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict)\n\n                    # self.log.warning(\'Meta-gradients ok.\')\n                else:\n                    # True test, no updates sent to parameter server:\n                    meta_fetches = [self.local_meta_train_op]\n                    meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict) + [None, None]\n                    # self.log.warning(\'Meta-opt. rollout ok.\')\n\n                if wirte_model_summary:\n                    meta_model_summary = meta_fetched[-2]\n                    model_summary = fetched[-1]\n\n                else:\n                    meta_model_summary = None\n                    model_summary = None\n\n                # Copy pi_prime to pi:\n                sess.run(self.train_aac.sync_pi_local)\n                # sess.run(self.test_aac.sync_pi)\n                # self.log.warning(\'Sync ok.\')\n\n                # Collect next train trajectory rollout:\n                train_data = self.train_aac.get_data(data_sample_config=train_data_config)\n                feed_dict = self.train_aac.process_data(sess, train_data,is_train=True, pi=self.train_aac.local_network)\n                # self.log.warning(\'Train data ok.\')\n\n                # Write down summaries:\n                self.test_aac.process_summary(sess, test_data, meta_model_summary)\n                self.train_aac.process_summary(sess, train_data, model_summary)\n                self.local_steps += 1\n                roll_num += 1\n        except:\n            msg = \'process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n'"
btgym/research/mldg/aac_1.py,18,"b'import tensorflow as tf\nimport numpy as np\nfrom btgym.research.gps.aac import GuidedAAC\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\n\n\nclass AMLDG_1(GuidedAAC):\n    """"""\n    Asynchronous implementation of MLDG algorithm (by Da Li et al.)\n    for one-shot adaptation in dynamically changing environments.\n\n    MOD: Defines meta-test task as one-roll-ahead of train one.\n\n    Does not relies on sub-AAC classes.\n\n    TODO: Use per-episode replay buffer distribution support as in AMLDG_d instead of single previous rollout\n\n    Papers:\n        Da Li et al.,\n         ""Learning to Generalize: Meta-Learning for Domain Generalization""\n         https://arxiv.org/abs/1710.03463\n\n        Maruan Al-Shedivat et al.,\n        ""Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments""\n        https://arxiv.org/abs/1710.03641\n    """"""\n\n    def __init__(\n            self,\n            runner_config=None,\n            fast_opt_learn_rate=1e-3,\n            trial_source_target_cycle=(1, 0),\n            num_episodes_per_trial=1,  # one-shot adaptation\n            _aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            name=\'AMLDG1\',\n            **kwargs\n    ):\n        try:\n            if runner_config is None:\n                self.runner_config = {\n                    \'class_ref\': BaseSynchroRunner,\n                    \'kwargs\': {\n                        \'data_sample_config\': {\'mode\': 0},\n                        \'test_conditions\': {\n                            \'state\': {\n                                \'metadata\': {\n                                    \'trial_type\': 1  # any type of episode from target dom. considered test one\n                                }\n                            }\n                        },\n                        \'name\': \'\',\n                    },\n                }\n            else:\n                self.runner_config = runner_config\n\n            # Trials sampling control:\n            self.num_source_trials = trial_source_target_cycle[0]\n            self.num_target_trials = trial_source_target_cycle[-1]\n            self.num_episodes_per_trial = num_episodes_per_trial\n\n            self.current_source_trial = 0\n            self.current_target_trial = 0\n            self.current_trial_mode = 0  # source\n            self.current_episode = 0\n\n            self.fast_opt_learn_rate = fast_opt_learn_rate\n\n            super(AMLDG_1, self).__init__(\n                runner_config=self.runner_config,\n                _use_target_policy=True,\n                _aux_render_modes=_aux_render_modes,\n                name=name,\n                **kwargs\n            )\n            self.model_summary_op = tf.summary.merge(\n                [self.model_summary_op, self._combine_meta_summaries()],\n                name=\'meta_model_summary\'\n            )\n        except:\n            msg = \'{}.__init()__ exception occurred\'.format(name) + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def _make_loss(self, pi, pi_prime):\n        self.meta_train_loss, meta_train_summaries = self._make_base_loss(\n            pi=pi,\n            pi_prime=self.dummy_pi,\n            name=self.name + \'/meta_train\',\n            verbose=True\n        )\n        self.meta_test_loss, meta_test_summaries = self._make_base_loss(\n            pi=pi_prime,\n            pi_prime=self.dummy_pi,\n            name=self.name + \'/meta_test\',\n            verbose=True\n        )\n        # Guidance annealing:\n        if self.guided_decay_steps is not None:\n            self.guided_lambda_decayed = tf.train.polynomial_decay(\n                self.guided_lambda,\n                self.global_step + 1,\n                self.guided_decay_steps,\n                0,\n                power=1,\n                cycle=False,\n            )\n        else:\n            self.guided_lambda_decayed = self.guided_lambda\n        # Switch to zero when testing - prevents information leakage:\n        self.train_guided_lambda = self.guided_lambda_decayed * tf.cast(self.local_network.train_phase, tf.float32)\n\n        # Guided losses, need two of them:\n        # guided_train_loss, _ = self.expert_loss(\n        #     pi_actions=pi.on_logits,\n        #     expert_actions=pi.expert_actions,\n        #     name=\'on_policy\',\n        #     verbose=False,\n        #     guided_lambda=self.train_guided_lambda\n        # )\n        guided_test_loss, g_summary = self.expert_loss(\n            pi_actions=pi_prime.on_logits,\n            expert_actions=pi_prime.expert_actions,\n            name=\'on_policy\',\n            verbose=True,\n            guided_lambda=self.train_guided_lambda\n        )\n\n        # self.meta_train_loss += guided_train_loss\n        self.meta_test_loss += guided_test_loss\n\n        return self.meta_train_loss + self.meta_test_loss, meta_train_summaries + meta_test_summaries + g_summary\n\n    def _make_train_op(self, pi, pi_prime, pi_global):\n        """"""\n        Defines training op graph and supplementary sync operations.\n\n        Returns:\n            tensor holding training op graph;\n        """"""\n        # Copy weights from the parameter server to the local pi:\n        self.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_global.var_list)]\n        )\n        # From ps to pi_prime:\n        self.sync_pi_prime = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi_global.var_list)]\n        )\n        # From pi_prime to pi:\n        self.sync_pi_from_prime = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_prime.var_list)]\n        )\n        self.sync = [self.sync_pi, self.sync_pi_prime]\n        self.optimizer = tf.train.AdamOptimizer(self.train_learn_rate, epsilon=1e-5)\n        self.fast_optimizer = tf.train.GradientDescentOptimizer(self.fast_opt_learn_rate)\n\n        # Clipped gradients:\n        pi.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.meta_train_loss, pi.var_list),\n            40.0\n        )\n        pi_prime.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.meta_test_loss, pi_prime.var_list),\n            40.0\n        )\n        # Meta_optimisation gradients as sum of meta-train and meta-test gradients:\n        self.grads = []\n        for g1, g2 in zip(pi.grads, pi_prime.grads):\n            if g1 is not None and g2 is not None:\n                meta_g = g1 + g2  # if g1 is excluded - we got MAML\n\n            else:\n                meta_g = None  # need this to map correctly to vars\n\n            self.grads.append(meta_g)\n\n        # Gradients to update local meta-test policy (conditioned on train data):\n        train_grads_and_vars = list(zip(pi.grads, pi_prime.var_list))\n\n        # Meta-gradients to be sent to parameter server:\n        meta_grads_and_vars = list(zip(self.grads, pi_global.var_list))\n\n        # Remove empty entries:\n        meta_grads_and_vars = [(g, v) for (g, v) in meta_grads_and_vars if g is not None]\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(pi.on_state_in.keys())\n\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        self.inc_step = self.global_step.assign_add(tf.shape(self.local_network.on_state_in[\'external\'])[0])\n\n        # Local fast optimisation op:\n        self.fast_train_op = self.fast_optimizer.apply_gradients(train_grads_and_vars)\n\n        # Global meta-optimisation op:\n        self.meta_train_op = self.optimizer.apply_gradients(meta_grads_and_vars)\n\n        self.log.debug(\'train_op defined\')\n        return self.fast_train_op, self.meta_train_op\n\n    def _combine_meta_summaries(self):\n        """"""\n        Additional summaries here.\n        """"""\n        with tf.name_scope(self.name):\n            meta_model_summaries = [\n                tf.summary.scalar(\'meta_grad_global_norm\', tf.global_norm(self.grads)),\n                # tf.summary.scalar(\'total_meta_loss\', self.loss),\n                # tf.summary.scalar(\'alpha_learn_rate\', self.alpha_rate),\n                # tf.summary.scalar(\'alpha_learn_rate_loss\', self.alpha_rate_loss)\n            ]\n        return meta_model_summaries\n\n    def get_sample_config(self, **kwargs):\n        """"""\n        Returns environment configuration parameters for next episode to sample.\n\n        Always prescribes to sample train episode from source or target domain.\n\n        Args:\n              kwargs:     not used\n\n        Returns:\n            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n        """"""\n\n        new_trial = 0\n        if self.current_episode >= self.num_episodes_per_trial:\n            # Reset episode counter:\n            self.current_episode = 0\n\n            # Request new trial:\n            new_trial = 1\n            # Decide on trial type (source/target):\n            if self.current_source_trial >= self.num_source_trials:\n                # Time to switch to target mode:\n                self.current_trial_mode = 1\n                # Reset counters:\n                self.current_source_trial = 0\n                self.current_target_trial = 0\n\n            if self.current_target_trial >= self.num_target_trials:\n                # Vise versa:\n                self.current_trial_mode = 0\n                self.current_source_trial = 0\n                self.current_target_trial = 0\n\n            # Update counter:\n            if self.current_trial_mode:\n                self.current_target_trial += 1\n            else:\n                self.current_source_trial += 1\n\n        self.current_episode += 1\n\n        # Compose btgym.datafeed.base.EnvResetConfig-consistent dict:\n        sample_config = dict(\n            episode_config=dict(\n                get_new=True,\n                sample_type=0,\n                b_alpha=1.0,\n                b_beta=1.0\n            ),\n            trial_config=dict(\n                get_new=new_trial,\n                sample_type=self.current_trial_mode,\n                b_alpha=1.0,\n                b_beta=1.0\n            )\n        )\n        return sample_config\n\n    def start(self, sess, summary_writer, **kwargs):\n        """"""\n        Executes all initializing operations,\n        starts environment runner[s].\n        Supposed to be called by parent worker just before training loop starts.\n\n        Args:\n            sess:           tf session object.\n            kwargs:         not used by default.\n        """"""\n        try:\n            # Copy weights from global to local:\n            sess.run(self.sync)\n\n            # Start thread_runners:\n            self._start_runners(\n                sess,\n                summary_writer,\n                init_context=None,\n                data_sample_config=self.get_sample_config(mode=1)\n            )\n\n            self.summary_writer = summary_writer\n            self.log.notice(\'runner started.\')\n\n        except:\n            msg = \'.start() exception occurred\' + \\\n                \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def _process(self, sess):\n        """"""\n        Meta-train/test procedure for one-shot learning.\n        Single call runs single meta-test episode.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            sess.run(self.sync_pi)\n            sess.run(self.sync_pi_prime)\n\n            # Get data configuration,\n\n            data_config = self.get_sample_config(mode=1)\n\n            # self.log.warning(\'data_config: {}\'.format(data_config))\n\n            # If this step data comes from source or target domain\n            # (i.e. is it either meta-optimised or true test episode):\n            is_train = not data_config[\'trial_config\'][\'sample_type\']\n            done = False\n            roll_num = 0\n\n            #  ** Data leakage checks removed.\n\n            # Collect initial trajectory rollout:\n            train_data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config,\n                force_new_episode=True\n            )\n\n            # self.log.warning(\'initial_rollout_ok\')\n\n            while not done:\n                # self.log.warning(\'Roll #{}\'.format(roll_num))\n\n                wirte_model_summary = \\\n                    self.local_steps % self.model_summary_freq == 0\n\n                feed_dict = self.process_data(sess, train_data, is_train=is_train, pi=self.local_network)\n\n                fetches = [self.fast_train_op]\n\n                fetched = sess.run(fetches, feed_dict=feed_dict)\n\n                # self.log.warning(\'Train gradients ok.\')\n\n                # Collect test rollout using [updated] pi_prime policy:\n                test_data = self.get_data(\n                    policy=self.local_network_prime,\n                    data_sample_config=data_config\n                )\n\n                # self.log.debug(\'test_rollout_ok\')\n\n                # If meta-test episode has just ended?\n                done = np.asarray(test_data[\'terminal\']).any()\n\n                # TODO: paranoid check is_train ~ actual_data_trial_type\n\n                if is_train:\n                    # Process test data and perform meta-optimisation step:\n                    feed_dict.update(\n                        self.process_data(sess, test_data, is_train=True, pi=self.local_network_prime)\n                    )\n\n                    if wirte_model_summary:\n                        meta_fetches = [self.meta_train_op, self.model_summary_op, self.inc_step]\n                    else:\n                        meta_fetches = [self.meta_train_op, self.inc_step]\n\n                    meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict)\n\n                    # self.log.warning(\'Meta-gradients ok.\')\n                else:\n                    # True test, no updates sent to parameter server:\n                    meta_fetched = [None, None]\n\n                    # self.log.warning(\'Meta-opt. rollout ok.\')\n\n                if wirte_model_summary:\n                    meta_model_summary = meta_fetched[-2]\n                    model_summary = fetched[-1]\n\n                else:\n                    meta_model_summary = None\n                    model_summary = None\n\n                # Next step housekeeping:\n                sess.run(self.sync_pi_from_prime)\n\n                # TODO: ????\n                # sess.run(self.sync_pi_prime)\n\n                # Make this test trajectory next train:\n                train_data = test_data\n                # self.log.warning(\'Trajectories swapped.\')\n\n                # Write down summaries:\n                self.process_summary(sess, test_data, meta_model_summary)\n\n                self.local_steps += 1\n                roll_num += 1\n\n        except:\n            msg = \'.process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n'"
btgym/research/mldg/aac_1d.py,10,"b'import tensorflow as tf\nimport numpy as np\n\nfrom btgym.algorithms.utils import batch_stack, batch_gather\nfrom btgym.research.mldg.aac_1 import AMLDG_1\nfrom btgym.research.mldg.memory import LocalMemory\n\n\nclass AMLDG_1d(AMLDG_1):\n    """"""\n    AMLDG_1 + tunable g1 + t2d methods\n    """"""\n\n    def __init__(\n            self,\n            g1_lambda=1.0,\n            num_train_updates=1,\n            train_batch_size=64,\n            name=\'AMLDG1d\',\n            **kwargs\n         ):\n\n        self.g1_lambda = g1_lambda\n        self.train_batch_size = train_batch_size\n        self.num_train_updates = num_train_updates\n        self.episode_memory = LocalMemory()\n        super().__init__(name=name, **kwargs)\n\n    def half_process_data(self, sess, data, is_train, pi, pi_prime=None):\n        """"""\n        Processes data but returns batched data instead of train step feed dictionary.\n        Args:\n            sess:               tf session obj.\n            pi:                 policy to feed\n            pi_prime:           optional policy to feed\n            data (dict):        data dictionary\n            is_train (bool):    is data provided are train or test\n\n        Returns:\n            feed_dict (dict):   train step feed dictionary\n        """"""\n        # Process minibatch for on-policy train step:\n        on_policy_batch = self._process_rollouts(data[\'on_policy\'])\n\n        if self.use_memory:\n            # Process rollouts from replay memory:\n            off_policy_batch = self._process_rollouts(data[\'off_policy\'])\n\n            if self.use_reward_prediction:\n                # Rebalanced 50/50 sample for RP:\n                rp_rollouts = data[\'off_policy_rp\']\n                rp_batch = batch_stack([rp.process_rp(self.rp_reward_threshold) for rp in rp_rollouts])\n\n            else:\n                rp_batch = None\n\n        else:\n            off_policy_batch = None\n            rp_batch = None\n\n        return {\n            \'on_policy_batch\': on_policy_batch,\n            \'off_policy_batch\': off_policy_batch,\n            \'rp_batch\': rp_batch\n        }\n\n    @staticmethod\n    def _check(batch):\n        """"""\n        Debug. utility.\n        """"""\n        print(\'Got data_dict:\')\n        for key in batch.keys():\n            try:\n                shape = np.asarray(batch[key]).shape\n            except:\n                shape = \'???\'\n            print(\'key: {}, shape: {}\'.format(key, shape))\n\n    def _make_train_op(self, pi, pi_prime, pi_global):\n        """"""\n        Defines training op graph and supplementary sync operations.\n\n        Returns:\n            tensor holding training op graph;\n        """"""\n        # Copy weights from the parameter server to the local pi:\n        self.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_global.var_list)]\n        )\n        # From ps to pi_prime:\n        self.sync_pi_prime = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi_prime.var_list, pi_global.var_list)]\n        )\n        # From pi_prime to pi:\n        self.sync_pi_from_prime = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_prime.var_list)]\n        )\n        self.sync = [self.sync_pi, self.sync_pi_prime]\n        self.optimizer = tf.train.AdamOptimizer(self.train_learn_rate, epsilon=1e-5)\n        self.fast_optimizer = tf.train.GradientDescentOptimizer(self.fast_opt_learn_rate)\n\n        # Clipped gradients:\n        pi.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.meta_train_loss, pi.var_list),\n            40.0\n        )\n        pi_prime.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.meta_test_loss, pi_prime.var_list),\n            40.0\n        )\n        # Meta_optimisation gradients as sum of meta-train and meta-test gradients:\n        self.grads = []\n        for g1, g2 in zip(pi.grads, pi_prime.grads):\n            if g1 is not None and g2 is not None:\n                meta_g = g1 * self.g1_lambda + g2\n\n            else:\n                meta_g = None  # need this to map correctly to vars\n\n            self.grads.append(meta_g)\n\n        # Gradients to update local meta-test policy (conditioned on train data):\n        train_grads_and_vars = list(zip(pi.grads, pi_prime.var_list))\n\n        # Meta-gradients to be sent to parameter server:\n        meta_grads_and_vars = list(zip(self.grads, pi_global.var_list))\n\n        # Remove empty entries:\n        meta_grads_and_vars = [(g, v) for (g, v) in meta_grads_and_vars if g is not None]\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(pi.on_state_in.keys())\n\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        self.inc_step = self.global_step.assign_add(tf.shape(pi_prime.on_state_in[\'external\'])[0])\n\n        # Local fast optimisation op:\n        self.fast_train_op = self.fast_optimizer.apply_gradients(train_grads_and_vars)\n\n        # Global meta-optimisation op:\n        self.meta_train_op = self.optimizer.apply_gradients(meta_grads_and_vars)\n\n        self.log.debug(\'train_op defined\')\n        return self.fast_train_op, self.meta_train_op\n\n    def _process(self, sess):\n        """"""\n        Meta-train/test procedure for one-shot learning.\n        Single call runs single meta-test episode.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            sess.run(self.sync_pi)\n            sess.run(self.sync_pi_prime)\n\n            self.episode_memory.reset()\n\n            # Get data configuration,\n            data_config = self.get_sample_config(mode=1)\n\n            # self.log.warning(\'data_config: {}\'.format(data_config))\n\n            # If this step data comes from source or target domain\n            # (i.e. is it either meta-optimised or true test episode):\n            is_train = not data_config[\'trial_config\'][\'sample_type\']\n            done = False\n            roll_num = 0\n\n            #  ** Data leakage checks removed.\n\n            # Collect initial trajectory rollout:\n            train_data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config,\n                force_new_episode=True\n            )\n\n            # self.log.warning(\'initial_rollout_ok\')\n\n            while not done:\n                # self.log.warning(\'Roll #{}\'.format(roll_num))\n                feed_dict = {}\n                wirte_model_summary = \\\n                    self.local_steps % self.model_summary_freq == 0\n\n                self.episode_memory.add_batch(\n                    **self.half_process_data(sess, train_data, is_train=is_train, pi=self.local_network)\n                )\n                # self.log.warning(\'Train roll added to memory.\')\n\n                for i in range(self.num_train_updates):\n                    feed_dict = self._get_main_feeder(\n                        sess,\n                        **self.episode_memory.sample(self.train_batch_size),\n                        is_train=is_train,\n                        pi=self.local_network,\n                        pi_prime=self.local_network_prime)\n\n                    # self.log.warning(\'Train feed dict ok.\')\n\n                    fetches = [self.fast_train_op]\n\n                    fetched = sess.run(fetches, feed_dict=feed_dict)\n\n                    # self.log.warning(\'Pi_prime update {} ok.\'.format(i))\n\n                # Collect test rollout using [updated] pi_prime policy:\n                test_data = self.get_data(\n                    policy=self.local_network_prime,\n                    data_sample_config=data_config\n                )\n\n                # self.log.warning(\'test_rollout_ok\')\n\n                # If meta-test episode has just ended?\n                done = np.asarray(test_data[\'terminal\']).any()\n\n                # TODO: paranoid check is_train ~ actual_data_trial_type\n\n                if is_train:\n                    # Process test data and perform meta-optimisation step:\n                    feed_dict.update(\n                        self.process_data(sess, test_data, is_train=True, pi=self.local_network_prime)\n                    )\n\n                    if wirte_model_summary:\n                        meta_fetches = [self.meta_train_op, self.model_summary_op, self.inc_step]\n                    else:\n                        meta_fetches = [self.meta_train_op, self.inc_step]\n\n                    meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict)\n\n                    # self.log.warning(\'Meta-gradients ok.\')\n                else:\n                    # True test, no updates sent to parameter server:\n                    meta_fetched = [None, None]\n\n                    # self.log.warning(\'Meta-opt. rollout ok.\')\n\n                if wirte_model_summary:\n                    meta_model_summary = meta_fetched[-2]\n                    model_summary = fetched[-1]\n\n                else:\n                    meta_model_summary = None\n                    model_summary = None\n\n                # Next step housekeeping:\n                # sess.run(self.sync_pi_from_prime)\n\n                # TODO: ????\n                # sess.run(self.sync_pi_prime)\n\n                # Make this test trajectory next train:\n                train_data = test_data\n                # self.log.warning(\'Trajectories swapped.\')\n\n                # Write down summaries:\n                self.process_summary(sess, test_data, meta_model_summary)\n\n                self.local_steps += 1\n                roll_num += 1\n\n        except:\n            msg = \'.process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n'"
btgym/research/mldg/aac_1s.py,36,"b'import tensorflow as tf\nimport numpy as np\n\nfrom btgym.algorithms.utils import batch_stack, batch_gather\nfrom btgym.research.gps.aac import GuidedAAC\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\nfrom btgym.research.mldg.aac_1 import AMLDG_1\nfrom btgym.research.mldg.memory import LocalMemory2\n\n\nclass AMLDG_1s(GuidedAAC):\n    """"""\n    AMLDG_1 + t2d methods + another style of update\n    """"""\n\n    def __init__(\n            self,\n            num_train_updates=1,\n            train_batch_size=64,\n            runner_config=None,\n            fast_opt_learn_rate=1e-3,\n            trial_source_target_cycle=(1, 0),\n            num_episodes_per_trial=1,  # one-shot adaptation\n            _aux_render_modes=(\'action_prob\', \'value_fn\', \'lstm_1_h\', \'lstm_2_h\'),\n            name=\'AMLDG1s\',\n            **kwargs\n         ):\n        if runner_config is None:\n            self.runner_config = {\n                \'class_ref\': BaseSynchroRunner,\n                \'kwargs\': {\n                    \'data_sample_config\': {\'mode\': 0},\n                    \'test_conditions\': {\n                        \'state\': {\n                            \'metadata\': {\n                                \'trial_type\': 1  # any type of episode from target dom. considered test one\n                            }\n                        }\n                    },\n                    \'name\': \'\',\n                },\n            }\n        else:\n            self.runner_config = runner_config\n\n        # Trials sampling control:\n        self.num_source_trials = trial_source_target_cycle[0]\n        self.num_target_trials = trial_source_target_cycle[-1]\n        self.num_episodes_per_trial = num_episodes_per_trial\n\n        self.current_source_trial = 0\n        self.current_target_trial = 0\n        self.current_trial_mode = 0  # source\n        self.current_episode = 0\n\n        self.fast_opt_learn_rate = fast_opt_learn_rate\n        self.train_batch_size = train_batch_size\n        self.num_train_updates = num_train_updates\n        self.episode_memory = LocalMemory2()\n\n        super().__init__(\n            runner_config=self.runner_config,\n            use_off_policy_aac=True,\n            _use_target_policy=False,\n            _use_local_memory=True,\n            _aux_render_modes=_aux_render_modes,\n            name=name,\n            **kwargs\n        )\n\n    def half_process_data(self, sess, data, is_train, pi, pi_prime=None):\n        """"""\n        Processes data but returns batched data instead of train step feed dictionary.\n        Args:\n            sess:               tf session obj.\n            pi:                 policy to feed\n            pi_prime:           optional policy to feed\n            data (dict):        data dictionary\n            is_train (bool):    is data provided are train or test\n\n        Returns:\n            feed_dict (dict):   train step feed dictionary\n        """"""\n        # Process minibatch for on-policy train step:\n        on_policy_batch = self._process_rollouts(data[\'on_policy\'])\n\n        if self.use_memory:\n            # Process rollouts from replay memory:\n            off_policy_batch = self._process_rollouts(data[\'off_policy\'])\n\n            if self.use_reward_prediction:\n                # Rebalanced 50/50 sample for RP:\n                rp_rollouts = data[\'off_policy_rp\']\n                rp_batch = batch_stack([rp.process_rp(self.rp_reward_threshold) for rp in rp_rollouts])\n\n            else:\n                rp_batch = None\n\n        else:\n            off_policy_batch = None\n            rp_batch = None\n\n        return {\n            \'on_policy_batch\': on_policy_batch,\n            \'off_policy_batch\': off_policy_batch,\n            \'rp_batch\': rp_batch\n        }\n\n    @staticmethod\n    def _check(batch):\n        """"""\n        Debug. utility.\n        """"""\n        print(\'Got data_dict:\')\n        for key in batch.keys():\n            try:\n                shape = np.asarray(batch[key]).shape\n            except:\n                shape = \'???\'\n            print(\'key: {}, shape: {}\'.format(key, shape))\n\n    def get_sample_config(self, **kwargs):\n        """"""\n        Returns environment configuration parameters for next episode to sample.\n\n        Always prescribes to sample train episode from source or target domain.\n\n        Args:\n              kwargs:     not used\n\n        Returns:\n            configuration dictionary of type `btgym.datafeed.base.EnvResetConfig`\n        """"""\n\n        new_trial = 0\n        if self.current_episode >= self.num_episodes_per_trial:\n            # Reset episode counter:\n            self.current_episode = 0\n\n            # Request new trial:\n            new_trial = 1\n            # Decide on trial type (source/target):\n            if self.current_source_trial >= self.num_source_trials:\n                # Time to switch to target mode:\n                self.current_trial_mode = 1\n                # Reset counters:\n                self.current_source_trial = 0\n                self.current_target_trial = 0\n\n            if self.current_target_trial >= self.num_target_trials:\n                # Vise versa:\n                self.current_trial_mode = 0\n                self.current_source_trial = 0\n                self.current_target_trial = 0\n\n            # Update counter:\n            if self.current_trial_mode:\n                self.current_target_trial += 1\n            else:\n                self.current_source_trial += 1\n\n        self.current_episode += 1\n\n        # Compose btgym.datafeed.base.EnvResetConfig-consistent dict:\n        sample_config = dict(\n            episode_config=dict(\n                get_new=True,\n                sample_type=0,\n                b_alpha=1.0,\n                b_beta=1.0\n            ),\n            trial_config=dict(\n                get_new=new_trial,\n                sample_type=self.current_trial_mode,\n                b_alpha=1.0,\n                b_beta=1.0\n            )\n        )\n        return sample_config\n\n    def start(self, sess, summary_writer, **kwargs):\n        """"""\n        Executes all initializing operations,\n        starts environment runner[s].\n        Supposed to be called by parent worker just before training loop starts.\n\n        Args:\n            sess:           tf session object.\n            kwargs:         not used by default.\n        """"""\n        try:\n            # Copy weights from global to local:\n            sess.run(self.sync)\n\n            # Start thread_runners:\n            self._start_runners(\n                sess,\n                summary_writer,\n                init_context=None,\n                data_sample_config=self.get_sample_config(mode=1)\n            )\n\n            self.summary_writer = summary_writer\n            self.log.notice(\'runner started.\')\n\n        except:\n            msg = \'.start() exception occurred\' + \\\n                \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def _make_loss(self, pi, pi_prime, name=\'base\', verbose=True):\n        """"""\n        Defines base AAC on- and off-policy loss, optional VR loss[not used yet], placeholders and summaries.\n\n        Args:\n            pi:                 policy network obj.\n            pi_prime:           optional policy network obj.\n            name:               str, name scope\n            verbose:            summary level\n\n        Returns:\n            tensor holding estimated loss graph\n            list of related summaries\n        """"""\n        with tf.name_scope(name):\n\n            # Guidance annealing:\n            if self.guided_decay_steps is not None:\n                self.guided_lambda_decayed = tf.train.polynomial_decay(\n                    self.guided_lambda,\n                    self.global_step + 1,\n                    self.guided_decay_steps,\n                    0,\n                    power=1,\n                    cycle=False,\n                )\n            else:\n                self.guided_lambda_decayed = self.guided_lambda\n            # Switch to zero when testing - prevents information leakage:\n            self.train_guided_lambda = self.guided_lambda_decayed * tf.cast(self.local_network.train_phase, tf.float32)\n\n            guided_test_loss, g_summary = self.expert_loss(\n                pi_actions=pi.on_logits,\n                expert_actions=pi.expert_actions,\n                name=\'on_policy\',\n                verbose=True,\n                guided_lambda=self.train_guided_lambda\n            )\n\n            # On-policy AAC loss definition:\n            pi.on_pi_act_target = tf.placeholder(\n                tf.float32, [None, self.ref_env.action_space.n], name=""on_policy_action_pl""\n            )\n            pi.on_pi_adv_target = tf.placeholder(tf.float32, [None], name=""on_policy_advantage_pl"")\n            pi.on_pi_r_target = tf.placeholder(tf.float32, [None], name=""on_policy_return_pl"")\n\n            clip_epsilon = tf.cast(self.clip_epsilon * self.learn_rate_decayed / self.opt_learn_rate, tf.float32)\n\n            self.on_pi_loss, on_pi_summaries = self.on_policy_loss(\n                act_target=pi.on_pi_act_target,\n                adv_target=pi.on_pi_adv_target,\n                r_target=pi.on_pi_r_target,\n                pi_logits=pi.on_logits,\n                pi_vf=pi.on_vf,\n                pi_prime_logits=pi_prime.on_logits,\n                entropy_beta=self.model_beta,\n                epsilon=clip_epsilon,\n                name=\'on_policy\',\n                verbose=verbose\n            )\n            self.on_pi_loss += guided_test_loss\n            model_summaries = on_pi_summaries + g_summary\n\n            # Off-policy losses:\n            pi.off_pi_act_target = tf.placeholder(\n                tf.float32, [None, self.ref_env.action_space.n], name=""off_policy_action_pl"")\n            pi.off_pi_adv_target = tf.placeholder(tf.float32, [None], name=""off_policy_advantage_pl"")\n            pi.off_pi_r_target = tf.placeholder(tf.float32, [None], name=""off_policy_return_pl"")\n\n            if self.use_off_policy_aac:\n                # Off-policy AAC loss graph mirrors on-policy:\n                self.off_pi_loss, self.off_pi_summaries = self.off_policy_loss(\n                    act_target=pi.off_pi_act_target,\n                    adv_target=pi.off_pi_adv_target,\n                    r_target=pi.off_pi_r_target,\n                    pi_logits=pi.off_logits,\n                    pi_vf=pi.off_vf,\n                    pi_prime_logits=pi_prime.off_logits,\n                    entropy_beta=self.model_beta,\n                    epsilon=clip_epsilon,\n                    name=\'off_policy\',\n                    verbose=False\n                )\n                self.off_pi_loss *= self.off_aac_lambda\n\n            if self.use_value_replay:\n                # Value function replay loss:\n                pi.vr_target = tf.placeholder(tf.float32, [None], name=""vr_target"")\n                self.vr_loss, self.vr_summaries = self.vr_loss(\n                    r_target=pi.vr_target,\n                    pi_vf=pi.vr_value,\n                    name=\'off_policy\',\n                    verbose=verbose\n                )\n\n        return self.on_pi_loss, model_summaries\n\n    def _make_train_op(self, pi, pi_prime, pi_global):\n        """"""\n        Defines training op graph and supplementary sync operations.\n            TODO:\n                - joint on-policy(~meta-test) and sampled off-policy (~meta-train) optimization:\n                single optimizer, same learn rate;\n\n                - separate optimization with different rates;\n\n        Returns:\n            tensor holding training op graph;\n        """"""\n        # Copy weights from the parameter server to the local pi:\n        self.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_global.var_list)]\n        )\n        self.sync = self.sync_pi\n        self.optimizer = tf.train.AdamOptimizer(self.train_learn_rate, epsilon=1e-5)\n        self.fast_optimizer = tf.train.GradientDescentOptimizer(self.fast_opt_learn_rate)\n\n        # Clipped gradients:\n        pi.on_grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.on_pi_loss, pi.var_list),\n            40.0\n        )\n        pi.off_grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.off_pi_loss, pi.var_list),\n            40.0\n        )\n\n        pi.on_grads = [\n            g1 + g2 if g1 is not None and g2 is not None else g1 if g2 is None else g2\n            for g1, g2 in zip(pi.on_grads, pi.off_grads)\n        ]\n\n        self.grads = pi.on_grads\n\n        # Gradients to update local policy (conditioned on train, off-policy data):\n        local_grads_and_vars = list(zip(pi.off_grads, pi.var_list))\n\n        # Meta-gradients to be sent to parameter server:\n        global_grads_and_vars = list(zip(pi.on_grads, pi_global.var_list))\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(pi.on_state_in.keys())\n\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        self.inc_step = self.global_step.assign_add(tf.shape(pi.on_state_in[\'external\'])[0])\n\n        # Local fast optimisation op:\n        self.local_train_op = self.fast_optimizer.apply_gradients(local_grads_and_vars)\n\n        # Global meta-optimisation op:\n        self.global_train_op = self.optimizer.apply_gradients(global_grads_and_vars)\n\n        self.log.debug(\'train_op defined\')\n        return [self.local_train_op, self.global_train_op]\n\n    def _process(self, sess):\n        """"""\n        Meta-train/test procedure for one-shot learning.\n        Single call runs single meta-test episode.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            sess.run(self.sync_pi)\n\n            self.episode_memory.reset()\n\n            # Get data configuration,\n            data_config = self.get_sample_config(mode=1)\n\n            # self.log.warning(\'data_config: {}\'.format(data_config))\n\n            # If this step data comes from source or target domain\n            # (i.e. is it either meta-optimised or true test episode):\n            is_train = not data_config[\'trial_config\'][\'sample_type\']\n            done = False\n            roll_num = 0\n\n            #  ** Data leakage checks removed.\n\n            # Collect initial trajectory rollout:\n            train_data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config,\n                force_new_episode=True\n            )\n\n            # self.log.warning(\'initial_rollout_ok\')\n\n            while not done:\n                # self.log.warning(\'Roll #{}\'.format(roll_num))\n\n                wirte_model_summary = \\\n                    self.local_steps % self.model_summary_freq == 0\n\n                self.episode_memory.add_batch(\n                    **self.half_process_data(sess, train_data, is_train=is_train, pi=self.local_network)\n                )\n                # self.log.warning(\'train rollout added to memory.\')\n\n                # Collect test rollout using [updated by prev. step] policy:\n                test_data = self.get_data(\n                    policy=self.local_network,\n                    data_sample_config=data_config\n                )\n\n                # self.log.warning(\'test rollout collected.\')\n\n                # If meta-test episode has just ended?\n                done = np.asarray(test_data[\'terminal\']).any()\n\n                # TODO: paranoid check is_train ~ actual_data_trial_type\n\n                feed_dict = self.process_data(sess, test_data, is_train=is_train, pi=self.local_network)\n\n                # self._check(feed_dict)\n\n                feed_dict.update(\n                    self._get_main_feeder(\n                        sess,\n                        **self.episode_memory.sample(self.train_batch_size),\n                        is_train=is_train,\n                        pi=self.local_network,\n                    )\n                )\n\n                # self._check(feed_dict)\n\n                if is_train:\n                    train_op = [self.local_train_op, self.train_op]\n\n                else:\n                    train_op = [self.local_train_op]\n\n                if wirte_model_summary:\n                    meta_fetches = train_op + [self.model_summary_op, self.inc_step]\n                else:\n                    meta_fetches = train_op + [self.inc_step]\n\n                meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict)\n\n                if wirte_model_summary and is_train:\n                    meta_model_summary = meta_fetched[-2]\n\n                else:\n                    meta_model_summary = None\n\n                # Make this test trajectory next train:\n                train_data = test_data\n                # self.log.warning(\'Trajectories swapped.\')\n\n                # Write down summaries:\n                self.process_summary(sess, test_data, model_data=meta_model_summary)\n\n                self.local_steps += 1\n                roll_num += 1\n\n        except:\n            msg = \'.process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n\nclass AMLDG_1s_a(AMLDG_1s):\n    """"""\n    FAILED\n    """"""\n\n    def __init__(self, num_train_updates = 1, name=\'AMLDG1sa\', **kwargs):\n        super(AMLDG_1s_a, self).__init__(name=name, **kwargs)\n\n        self.num_train_updates = num_train_updates\n        # self.meta_summaries = self.combine_aux_summaries()\n\n    def _make_train_op(self, pi, pi_prime, pi_global):\n        """"""\n        Defines training op graph and supplementary sync operations.\n\n                - separate optimization with different rates;\n\n        Returns:\n            tensor holding training op graph;\n        """"""\n        # Copy weights from the parameter server to the local pi:\n        self.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi.var_list, pi_global.var_list)]\n        )\n        self.sync = self.sync_pi\n        self.optimizer = tf.train.AdamOptimizer(self.train_learn_rate, epsilon=1e-5)\n\n        # Clipped gradients:\n        pi.on_grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.on_pi_loss, pi.var_list),\n            40.0\n        )\n        pi.off_grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.off_pi_loss, pi.var_list),\n            40.0\n        )\n        self.grads = pi.on_grads\n\n        # Learnable fast rate:\n        #self.fast_learn_rate = tf.reduce_mean(pi.off_learn_alpha, name=\'mean_alpha_rate\') / 10\n        self.fast_optimizer = tf.train.GradientDescentOptimizer(self.fast_opt_learn_rate)\n        # self.alpha_rate_loss = tf.global_norm(pi.off_grads)\n        # self.alpha_grads, _ = tf.clip_by_global_norm(\n        #     tf.gradients(self.alpha_rate_loss, pi.var_list),\n        #     40.0\n        # )\n        # # Sum on_ and  second order alpha_ gradients:\n        # pi.off_grads = [\n        #     g1 + g2 if g1 is not None and g2 is not None else g1 if g2 is None else g2\n        #     for g1, g2 in zip(pi.off_grads, self.alpha_grads)\n        # ]\n\n        # Gradients to update local policy (conditioned on train, off-policy data):\n        local_grads_and_vars = list(zip(pi.off_grads, pi.var_list))\n\n        # Meta-gradients to be sent to parameter server:\n        global_grads_and_vars = list(zip(pi.on_grads, pi_global.var_list))\n\n        # Set global_step increment equal to observation space batch size:\n        obs_space_keys = list(pi.on_state_in.keys())\n\n        assert \'external\' in obs_space_keys, \\\n            \'Expected observation space to contain `external` mode, got: {}\'.format(obs_space_keys)\n        self.inc_step = self.global_step.assign_add(tf.shape(pi.on_state_in[\'external\'])[0])\n\n        # Local fast optimisation op:\n        self.local_train_op = self.fast_optimizer.apply_gradients(local_grads_and_vars)\n\n        # Global meta-optimisation op:\n        self.global_train_op = self.optimizer.apply_gradients(global_grads_and_vars)\n\n        self.log.debug(\'train_op defined\')\n        return [self.local_train_op, self.global_train_op]\n\n    # def combine_aux_summaries(self):\n    #     """"""\n    #     Additional summaries here.\n    #     """"""\n    #     off_model_summaries = tf.summary.merge(\n    #         [\n    #             tf.summary.scalar(\'alpha_rate\', self.fast_learn_rate),\n    #         ]\n    #     )\n    #     return off_model_summaries\n\n    def _process(self, sess):\n        """"""\n        Meta-train/test procedure for one-shot learning.\n        Single call runs single meta-test episode.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            sess.run(self.sync_pi)\n\n            self.episode_memory.reset()\n\n            # Get data configuration,\n            data_config = self.get_sample_config(mode=1)\n\n            # self.log.warning(\'data_config: {}\'.format(data_config))\n\n            # If this step data comes from source or target domain\n            # (i.e. is it either meta-optimised or true test episode):\n            is_train = not data_config[\'trial_config\'][\'sample_type\']\n            done = False\n            roll_num = 0\n\n            #  ** Data leakage checks removed.\n\n            # Collect initial trajectory rollout:\n            train_data = self.get_data(\n                policy=self.local_network,\n                data_sample_config=data_config,\n                force_new_episode=True\n            )\n\n            # self.log.warning(\'initial_rollout_ok\')\n\n            while not done:\n                # self.log.warning(\'Roll #{}\'.format(roll_num))\n\n                write_model_summary = \\\n                    self.local_steps % self.model_summary_freq == 0\n\n                # Add to replay buffer:\n                self.episode_memory.add_batch(\n                    **self.half_process_data(sess, train_data, is_train=is_train, pi=self.local_network)\n                )\n                # self.log.warning(\'train rollout added to memory.\')\n                feed_dict = {}\n                for i in range(self.num_train_updates):\n                    # Sample off policy data and make feeder:\n                    feed_dict = (\n                        self._get_main_feeder(\n                            sess,\n                            **self.episode_memory.sample(self.train_batch_size),\n                            is_train=is_train,\n                            pi=self.local_network,\n                        )\n                    )\n                    # self._check(feed_dict)\n\n                    fetches = [self.local_train_op]\n\n                    fetched = sess.run(fetches, feed_dict=feed_dict)\n\n                    # # Write down particular model summary:\n                    # if write_model_summary:\n                    #     self.summary_writer.add_summary(tf.Summary.FromString(fetched[-1]), sess.run(self.global_step))\n                    #     self.summary_writer.flush()\n\n                # Collect test on_policy rollout using [updated] policy:\n                test_data = self.get_data(\n                    policy=self.local_network,\n                    data_sample_config=data_config\n                )\n\n                # self.log.warning(\'test rollout collected.\')\n\n                # If meta-test episode has just ended?\n                done = np.asarray(test_data[\'terminal\']).any()\n\n                # TODO: paranoid check is_train ~ actual_data_trial_type\n\n                feed_dict.update(self.process_data(sess, test_data, is_train=is_train, pi=self.local_network))\n\n                # self._check(feed_dict)\n\n                if is_train:\n                    train_op = self.train_op\n\n                    if write_model_summary:\n                        meta_fetches = [train_op, self.model_summary_op, self.inc_step]\n\n                    else:\n                        meta_fetches = [train_op, self.inc_step]\n\n                    meta_fetched = sess.run(meta_fetches, feed_dict=feed_dict)\n\n                else:\n                    meta_fetched = [None, None]\n\n                if write_model_summary:\n\n                    meta_model_summary = meta_fetched[-2]\n\n                else:\n                    meta_model_summary = None\n\n                # Make this test trajectory next train:\n                train_data = test_data\n                # self.log.warning(\'Trajectories swapped.\')\n\n                # Write down summaries:\n                self.process_summary(sess, test_data, model_data=meta_model_summary)\n\n                self.local_steps += 1\n                roll_num += 1\n\n        except:\n            msg = \'.process() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n\n'"
btgym/research/mldg/memory.py,0,"b'import numpy as np\n\nfrom btgym.algorithms.utils import batch_stack, batch_gather\n\n\nclass LocalMemory:\n    """"""\n    Simple replay buffer allowing random sampling.\n    """"""\n\n    def __init__(self):\n        self.on_batch = None\n        self.off_batch = None\n        self.rp_batch = None\n\n    def reset(self):\n        """"""\n        Clears memory.\n        """"""\n        self.on_batch = None\n        self.off_batch = None\n        self.rp_batch = None\n\n    def add_batch(self, on_policy_batch, off_policy_batch, rp_batch):\n        """"""\n        Adds data to memory.\n\n        Args:\n            data:\n        """"""\n        if self.on_batch is None:\n            self.on_batch = on_policy_batch\n\n        else:\n            self.on_batch = batch_stack([self.on_batch, on_policy_batch])\n\n        if self.off_batch is None:\n            self.off_batch = off_policy_batch\n\n        else:\n            self.off_batch = batch_stack([self.off_batch, off_policy_batch])\n\n        if self.rp_batch is None:\n            self.rp_batch = rp_batch\n\n        else:\n            self.rp_batch = batch_stack([self.rp_batch, rp_batch])\n\n    def sample(self, sample_size):\n        """"""\n        Randomly samples experiences from memory.\n\n        Args:\n            sample_size:\n\n        Returns:\n            samples\n        """"""\n        if self.on_batch is not None:\n            batch_size = self.on_batch[\'time_steps\'].shape[0]\n            indices = np.random.randint(0, batch_size, size=sample_size)\n\n            on_policy_batch = batch_gather(self.on_batch, indices)\n\n        else:\n            on_policy_batch = None\n\n        if self.off_batch is not None:\n            batch_size = self.off_batch[\'time_steps\'].shape[0]\n            indices = np.random.randint(0, batch_size, size=sample_size)\n\n            off_policy_batch = batch_gather(self.off_batch, indices)\n\n        else:\n            off_policy_batch = None\n\n        if self.rp_batch is not None:\n            batch_size = self.rp_batch[\'time_steps\'].shape[0]\n            indices = np.random.randint(0, batch_size, size=sample_size)\n\n            rp_batch = batch_gather(self.rp_batch, indices)\n\n        else:\n            rp_batch = None\n\n        return {\n            \'on_policy_batch\': on_policy_batch,\n            \'off_policy_batch\': off_policy_batch,\n            \'rp_batch\': rp_batch\n        }\n\n\nclass LocalMemory2:\n    """"""\n    Simple replay buffer allowing random sampling.\n    """"""\n\n    def __init__(self):\n        self.batch = None\n\n    def reset(self):\n        """"""\n        Clears memory.\n        """"""\n        self.batch = None\n\n    def add_batch(self, on_policy_batch, **kwargs):\n        """"""\n        Adds data to memory.\n\n        Args:\n            data:\n        """"""\n        if self.batch is None:\n            self.batch = on_policy_batch\n\n        else:\n            self.batch = batch_stack([self.batch, on_policy_batch])\n\n    def sample(self, sample_size):\n        """"""\n        Randomly samples experiences from memory.\n\n        Args:\n            sample_size:\n\n        Returns:\n            samples\n        """"""\n        if self.batch is not None:\n            batch_size = self.batch[\'time_steps\'].shape[0]\n            indices = np.random.randint(0, batch_size, size=sample_size)\n\n            off_policy_batch = batch_gather(self.batch, indices)\n\n        else:\n            off_policy_batch = None\n\n        return {\n            \'on_policy_batch\': None,\n            \'off_policy_batch\': off_policy_batch,\n            \'rp_batch\': None\n        }\n\n'"
btgym/research/mldg/policy.py,1,"b'import tensorflow as tf\nfrom btgym.research.gps.policy import GuidedPolicy_0_0\nimport numpy as np\n\n\nclass AacStackedMetaPolicy(GuidedPolicy_0_0):\n    """"""\n    Enabling meta-learning by embedding learning algorithm in RNN activations.\n\n    Use in conjunction with btgym.research.meta_rnn_2.env_runner.MetaEnvRunnerFn;\n\n    Paper:\n    `FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING`,\n    https://arxiv.org/pdf/1611.02779.pdf\n\n    `Get_initial_features()` method has been modified to enamble meta-learning loop:\n    either to reset RNN critic (lstm_2) context to zero-state or return continue from the end of previous episode,\n    depending on episode metadata received.\n    """"""\n    def __init__(self, **kwargs):\n        super(AacStackedMetaPolicy, self).__init__(**kwargs)\n        self.current_trial_num = -1  # always give initial context at first call\n\n    def get_initial_features(self, state, context=None):\n        """"""\n        Returns initial RNN context\n        RNN_1 (lower, actor) context is reset at every call.\n        RNN_2 (upper, critic) context is reset if :\n            - episode  initial `state` `trial_num` metadata has been changed form last call (new trial started);\n            - no context arg is provided (initial episode of training);\n        ... else carries critic context on to new episode;\n\n        Episode metadata are provided by DataTrialIterator, which is shaping Trial data distribution in this case,\n        and delivered through env.strategy as separate key in observation dictionary.\n\n        Args:\n            state:      initial episode state (result of env.reset())\n            context:    last previous episode RNN state (last_context of runner)\n\n        Returns:\n            2 level RNN zero-state tuple.\n\n        Raises:\n            KeyError if [`metadata`]:[`trial_num`,`type`] keys not found\n        """"""\n        #print(\'Meta_policy_init_metadata:\', state[\'metadata\'])\n        #print(\'self.current_trial_num:\', self.current_trial_num)\n        try:\n            sess = tf.get_default_session()\n            new_context = list(sess.run(self.on_lstm_init_state))\n            if context is not None:\n                if state[\'metadata\'][\'trial_num\'] == self.current_trial_num or state[\'metadata\'][\'type\']:\n                    # Asssume same training trial or test episode pass, critic context intact to new episode:\n                    new_context[-1] = context[-1]\n                    # TODO: !\n                    # FULL context intact to new episode:\n                    #new_context = context\n\n                    #print(\'Meta_policy Actor context reset\')\n                else:\n                    #print(\'Meta_policy Actor and Critic context reset\')\n                    pass\n            # Back to tuple:\n            new_context = tuple(new_context)\n            # Keep trial number:\n            self.current_trial_num = state[\'metadata\'][\'trial_num\']\n\n        except KeyError:\n            raise KeyError(\n                \'Meta_policy: expected observation state dict. to have keys [`metadata`]:[`trial_num`,`type`]; got: {}\'.\n                format(state.keys())\n            )\n\n        return new_context\n\n'"
btgym/research/model_based/__init__.py,0,b''
btgym/research/model_based/aac.py,36,"b'import sys\nfrom logbook import Logger, StreamHandler\n\nimport numpy as np\nimport tensorflow as tf\nfrom btgym.research.gps.aac import GuidedAAC\nfrom .runner import OUpRunner\n\n\nclass OUpAAC(GuidedAAC):\n    """"""\n    Extends parent `GuidedAAC` class with additional summaries related to Orn-Uhl. data generating process.\n    """"""\n\n    def __init__(\n            self,\n            runner_config=None,\n            aac_lambda=1.0,\n            guided_lambda=0.0,\n            name=\'OUpA3C\',\n            **kwargs\n    ):\n        if runner_config is None:\n            runner_config = {\n                \'class_ref\': OUpRunner,\n                \'kwargs\': {}\n            }\n        super(OUpAAC, self).__init__(\n            aac_lambda=aac_lambda,\n            guided_lambda=guided_lambda,\n            name=name,\n            runner_config=runner_config,\n            **kwargs,\n        )\n\n    def _combine_summaries(self, policy=None, model_summaries=None):\n        """"""\n        Defines model-wide and episode-related summaries\n\n        Returns:\n            model_summary op\n            episode_summary op\n        """"""\n        if model_summaries is not None:\n            if self.use_global_network:\n                # Model-wide statistics:\n                with tf.name_scope(\'model\'):\n                    model_summaries += [\n                        tf.summary.scalar(""grad_global_norm"", self.grads_global_norm),\n                        tf.summary.scalar(""learn_rate"", self.learn_rate_decayed),\n                        # cause actual rate is a jaggy due to test freezes\n                        tf.summary.scalar(""total_loss"", self.loss),\n                    ]\n                    if policy is not None:\n                        model_summaries += [tf.summary.scalar(""var_global_norm"", tf.global_norm(policy.var_list))]\n        else:\n            model_summaries = []\n        # Model stat. summary:\n        model_summary = tf.summary.merge(model_summaries, name=\'model_summary\')\n\n        # Episode-related summaries:\n        ep_summary = dict(\n            # Summary placeholders\n            render_atari=tf.placeholder(tf.uint8, [None, None, None, 1]),\n            total_r=tf.placeholder(tf.float32, ),\n            cpu_time=tf.placeholder(tf.float32, ),\n            final_value=tf.placeholder(tf.float32, ),\n            steps=tf.placeholder(tf.int32, ),\n            ou_lambda=tf.placeholder(tf.float32, ),\n            ou_sigma=tf.placeholder(tf.float32, ),\n            ou_mu=tf.placeholder(tf.float32, ),\n\n        )\n        ep_summary.update(\n            {\n                mode: tf.placeholder(tf.uint8, [None, None, None, None], name=mode + \'_pl\')\n                for mode in self.env_list[0].render_modes + self.aux_render_modes\n            }\n        )\n        ep_summary[\'render_op\'] = tf.summary.merge(\n            [tf.summary.image(mode, ep_summary[mode])\n             for mode in self.env_list[0].render_modes + self.aux_render_modes]\n        )\n        # Episode stat. summary:\n        ep_summary[\'btgym_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode_train/cpu_time_sec\', ep_summary[\'cpu_time\']),\n                tf.summary.scalar(\'episode_train/final_value\', ep_summary[\'final_value\']),\n                tf.summary.scalar(\'episode_train/total_reward\', ep_summary[\'total_r\']),\n                tf.summary.scalar(\'episode_train/ou_lambda\', ep_summary[\'ou_lambda\']),\n                tf.summary.scalar(\'episode_train/ou_sigma\', ep_summary[\'ou_sigma\']),\n                tf.summary.scalar(\'episode_train/ou_mu\', ep_summary[\'ou_mu\']),\n            ],\n            name=\'episode_train_btgym\'\n        )\n        # Test episode stat. summary:\n        ep_summary[\'test_btgym_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode_test/total_reward\', ep_summary[\'total_r\']),\n                tf.summary.scalar(\'episode_test/final_value\', ep_summary[\'final_value\']),\n            ],\n            name=\'episode_test_btgym\'\n        )\n        ep_summary[\'atari_stat_op\'] = tf.summary.merge(\n            [\n                tf.summary.scalar(\'episode/total_reward\', ep_summary[\'total_r\']),\n                tf.summary.scalar(\'episode/steps\', ep_summary[\'steps\'])\n            ],\n            name=\'episode_atari\'\n        )\n        self.log.debug(\'model-wide and episode summaries ok.\')\n        return model_summary, ep_summary\n\n\nclass AMLDG:\n    """"""\n    Train framework for combined model-based/model-free setup with non-parametric data model.\n    Compensates model bias by jointly learning optimal policy for modelled data (generated trajectories) and\n    real data model is based upon.\n    Based on objective identical to one of MLDG algorithm (by Da Li et al.).\n\n    This class is basically an AAC wrapper: it relies on two sub-AAC classes to make separate policy networks\n    and training loops.\n\n    Note that \'actor\' and \'critic\' names used here are not related to same named entities used in A3C and\n    other actor-critic RL algorithms; it rather relevant to \'generator\' and \'discriminator\' terms used\n    in adversarial training and mean that \'actor trainer\' is optimising RL objective on synthetic data,\n    generated by some model while \'critic trainer\' tries to compensate model bias by optimizing same objective on\n    real data model has been fitted with.\n\n    Papers:\n        Da Li et al.,\n         ""Learning to Generalize: Meta-Learning for Domain Generalization""\n         https://arxiv.org/abs/1710.03463\n\n        Maruan Al-Shedivat et al.,\n        ""Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments""\n        https://arxiv.org/abs/1710.03641\n\n    """"""\n    def __init__(\n            self,\n            env,\n            task,\n            log_level,\n            aac_class_ref=OUpAAC,\n            runner_config=None,\n            opt_decay_steps=None,\n            opt_end_learn_rate=None,\n            opt_learn_rate=1e-4,\n            opt_max_env_steps=10 ** 7,\n            aac_lambda=1.0,\n            guided_lambda=0.0,\n            tau=.5,\n            rollout_length=20,\n            train_phase=True,\n            name=\'TrainAMLDG\',\n            **kwargs\n    ):\n        try:\n            self.aac_class_ref = aac_class_ref\n            self.task = task\n            self.name = name\n            self.summary_writer = None\n            self.train_phase = train_phase\n            self.tau = tau\n\n            self.opt_learn_rate = opt_learn_rate\n            self.opt_max_env_steps = opt_max_env_steps\n\n            if opt_end_learn_rate is None:\n                self.opt_end_learn_rate = self.opt_learn_rate\n            else:\n                self.opt_end_learn_rate = opt_end_learn_rate\n\n            if opt_decay_steps is None:\n                self.opt_decay_steps = self.opt_max_env_steps\n            else:\n                self.opt_decay_steps = opt_decay_steps\n\n            StreamHandler(sys.stdout).push_application()\n            self.log = Logger(\'{}_{}\'.format(name, task), level=log_level)\n            self.rollout_length = rollout_length\n\n            if runner_config is None:\n                self.runner_config = {\n                    \'class_ref\': OUpRunner,\n                    \'kwargs\': {},\n                }\n            else:\n                self.runner_config = runner_config\n\n            self.env_list = env\n\n            assert isinstance(self.env_list, list) and len(self.env_list) == 2, \\\n                \'Expected pair of environments, got: {}\'.format(self.env_list)\n\n            # Instantiate two sub-trainers: one for training on modeled data (actor, or generator) and one\n            # for training on real data (critic, or discriminator):\n            self.runner_config[\'kwargs\'] = {\n                \'data_sample_config\': {\'mode\': 0},  # synthetic train data\n                \'name\': \'actor\',\n                \'test_deterministic\': not self.train_phase,\n            }\n            self.actor_aac = aac_class_ref(\n                env=self.env_list[-1],  # test data -> slave env.\n                task=self.task,\n                log_level=log_level,\n                runner_config=self.runner_config,\n                opt_learn_rate=self.opt_learn_rate,\n                opt_max_env_steps=self.opt_max_env_steps,\n                opt_end_learn_rate=self.opt_end_learn_rate,\n                aac_lambda=aac_lambda,\n                guided_lambda=guided_lambda,\n                rollout_length=self.rollout_length,\n                episode_train_test_cycle=(1, 0) if self.train_phase else (0, 1),\n                _use_target_policy=False,\n                _use_global_network=True,\n                name=self.name + \'/actor\',\n                **kwargs\n            )\n            # Change for critic:\n            self.runner_config[\'kwargs\'] = {\n                \'data_sample_config\': {\'mode\': 1},  # real train data\n                \'name\': \'critic\',\n                \'test_deterministic\': not self.train_phase,  # enable train exploration on [formally] test data\n            }\n            self.critic_aac = aac_class_ref(\n                env=self.env_list[0],  # real train data will be master environment\n                task=self.task,\n                log_level=log_level,\n                runner_config=self.runner_config,\n                opt_learn_rate=self.opt_learn_rate,\n                opt_max_env_steps=self.opt_max_env_steps,\n                opt_end_learn_rate=self.opt_end_learn_rate,\n                aac_lambda=aac_lambda,\n                guided_lambda=guided_lambda,\n                rollout_length=self.rollout_length,\n                episode_train_test_cycle=(0, 1),  # always real\n                _use_target_policy=False,\n                _use_global_network=False,\n                global_step_op=self.actor_aac.global_step,\n                global_episode_op=self.actor_aac.global_episode,\n                inc_episode_op=self.actor_aac.inc_episode,\n                name=self.name + \'/critic\',\n                **kwargs\n            )\n\n            self.local_steps = self.critic_aac.local_steps\n            self.model_summary_freq = self.critic_aac.model_summary_freq\n\n            self._make_train_op()\n\n        except Exception as e:\n            msg = \'AMLDG.__init()__ exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise e\n\n    def _make_train_op(self):\n        """"""\n        Defines tensors holding training ops.\n        """"""\n        # Handy aliases:\n        pi_critic = self.critic_aac.local_network  # local critic policy\n        pi_actor = self.actor_aac.local_network  # local actor policy\n        pi_global = self.actor_aac.network  # global shared policy\n\n        # From local actor to local critic:\n        self.critic_aac.sync_pi = tf.group(\n            *[v1.assign(v2) for v1, v2 in zip(pi_critic.var_list, pi_actor.var_list)]\n        )\n\n        # Inherited counters:\n        self.global_step = self.actor_aac.global_step\n        self.global_episode = self.actor_aac.global_episode\n        self.inc_episode = self.actor_aac.inc_episode\n        self.reset_global_step = self.actor_aac.reset_global_step\n\n        # Clipped gradients for critic (critic\'s train op is disabled by `_use_global_network=False`\n        # to avoid actor\'s name scope violation):\n        self.critic_aac.grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self.critic_aac.loss, pi_critic.var_list),\n            40.0\n        )\n        # Placeholders for stored gradients values, include None\'s to correctly map Vars:\n        self.actor_aac.grads_placeholders = [\n            tf.placeholder(shape=grad.shape, dtype=grad.dtype) if grad is not None else None\n            for grad in self.actor_aac.grads\n        ]\n        self.critic_aac.grads_placeholders = [\n            tf.placeholder(shape=grad.shape, dtype=grad.dtype) if grad is not None else None\n            for grad in self.critic_aac.grads\n        ]\n\n        # Gradients to update local critic policy with stored actor\'s gradients:\n        critic_grads_and_vars = list(zip(self.actor_aac.grads_placeholders, pi_critic.var_list))\n\n        # Final gradients to be sent to parameter server:\n        self.grads = [\n            self.tau * g1 + (1 - self.tau) * g2 if g1 is not None and g2 is not None else None\n            for g1, g2 in zip(self.actor_aac.grads_placeholders, self.critic_aac.grads_placeholders)\n        ]\n        global_grads_and_vars = list(zip(self.grads, pi_global.var_list))\n\n        # debug_global_grads_and_vars = list(zip(self.actor_aac.grads_placeholders, pi_global.var_list))\n        # debug_global_grads_and_vars = [(g, v) for (g, v) in debug_global_grads_and_vars if g is not None]\n\n        # Remove None entries:\n        global_grads_and_vars = [(g, v) for (g, v) in global_grads_and_vars if g is not None ]\n        critic_grads_and_vars = [(g, v) for (g, v) in critic_grads_and_vars if g is not None]\n        self.actor_aac.grads = [g for g in self.actor_aac.grads if g is not None]\n        self.critic_aac.grads = [g for g in self.critic_aac.grads if g is not None]\n        self.actor_aac.grads_placeholders = [pl for pl in self.actor_aac.grads_placeholders if pl is not None]\n        self.critic_aac.grads_placeholders = [pl for pl in self.critic_aac.grads_placeholders if pl is not None]\n\n        self.inc_step = self.actor_aac.inc_step\n\n        # Op to update critic with gradients from actor:\n        self.critic_aac.optimizer = tf.train.AdamOptimizer(self.actor_aac.learn_rate_decayed, epsilon=1e-5)\n        self.update_critic_op = self.critic_aac.optimizer.apply_gradients(critic_grads_and_vars)\n\n        # Use actor optimizer to update global policy instance:\n        self.train_op = self.actor_aac.optimizer.apply_gradients(global_grads_and_vars)\n\n        self.log.debug(\'all_train_ops defined\')\n\n    def start(self, sess, summary_writer, **kwargs):\n        """"""\n        Executes all initializing operations,\n        starts environment runner[s].\n        Supposed to be called by parent worker just before training loop starts.\n\n        Args:\n            sess:           tf session object.\n            kwargs:         not used by default.\n        """"""\n        try:\n            # Copy weights from global to local:\n            sess.run(self.critic_aac.sync_pi)\n            sess.run(self.actor_aac.sync_pi)\n\n            # Start thread_runners:\n            self.critic_aac._start_runners(   # master first\n                sess,\n                summary_writer,\n                init_context=None,\n                data_sample_config=self.critic_aac.get_sample_config(mode=1)\n            )\n            self.actor_aac._start_runners(\n                sess,\n                summary_writer,\n                init_context=None,\n                data_sample_config=self.actor_aac.get_sample_config(mode=0)\n            )\n\n            self.summary_writer = summary_writer\n            self.log.notice(\'Runners started.\')\n\n        except:\n            msg = \'start() exception occurred\' + \\\n                \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise RuntimeError(msg)\n\n    def process(self, sess):\n        if self.train_phase:\n            self.process_train(sess)\n\n        else:\n            self.process_test(sess)\n\n    def process_test(self, sess):\n        """"""\n        Evaluation loop.\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n        """"""\n        try:\n            # sess.run(self.critic_aac.sync_pi)\n            # sess.run(self.actor_aac.sync_pi)\n\n            actor_data = self.actor_aac.get_data()\n            critic_data = self.critic_aac.get_data()\n\n            # Write down summaries:\n            self.actor_aac.process_summary(sess, actor_data)\n            self.critic_aac.process_summary(sess, critic_data)\n            self.local_steps += 1\n\n        except Exception as e:\n            msg = \'process_test() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise e\n\n    def process_train(self, sess):\n        """"""\n        Train procedure.\n\n        Args:\n            sess (tensorflow.Session):   tf session obj.\n\n        """"""\n        try:\n            # Copy from parameter server:\n            sess.run(self.critic_aac.sync_pi)\n            sess.run(self.actor_aac.sync_pi)\n            # self.log.warning(\'Train Sync ok.\')\n\n            # Get data configuration (redundant):\n            actor_data_config = {\n                \'episode_config\': {\'get_new\': 1, \'sample_type\': 0, \'b_alpha\': 1.0, \'b_beta\': 1.0},\n                \'trial_config\': {\'get_new\': 1, \'sample_type\': 0, \'b_alpha\': 1.0, \'b_beta\': 1.0}\n            }\n            critic_data_config = {\n                \'episode_config\': {\'get_new\': 1, \'sample_type\': 1, \'b_alpha\': 1.0, \'b_beta\': 1.0},\n                \'trial_config\': {\'get_new\': 1, \'sample_type\': 1, \'b_alpha\': 1.0, \'b_beta\': 1.0}\n            }\n\n            # self.log.warning(\'actor_data_config: {}\'.format(actor_data_config))\n            # self.log.warning(\'critic_data_config: {}\'.format(critic_data_config))\n\n            # Collect synthetic train trajectory rollout:\n            actor_data = self.actor_aac.get_data(data_sample_config=actor_data_config)\n            actor_feed_dict = self.actor_aac.process_data(\n                sess,\n                actor_data,\n                is_train=True,\n                pi=self.actor_aac.local_network\n            )\n\n            # self.log.warning(\'Actor data ok.\')\n\n            wirte_model_summary = \\\n                self.local_steps % self.model_summary_freq == 0\n\n            # Get gradients from actor:\n            if wirte_model_summary:\n                actor_fetches = [self.actor_aac.grads, self.inc_step, self.actor_aac.model_summary_op]\n\n            else:\n                actor_fetches = [self.actor_aac.grads, self.inc_step]\n\n            # self.log.warning(\'self.actor_aac.grads: \\n{}\'.format(self.actor_aac.grads))\n            # self.log.warning(\'self.actor_aac.model_summary_op: \\n{}\'.format(self.actor_aac.model_summary_op))\n\n            actor_fetched = sess.run(actor_fetches, feed_dict=actor_feed_dict)\n            actor_grads_values = actor_fetched[0]\n            # self.log.warning(\'Actor gradients ok.\')\n\n            # Start preparing gradients feeder:\n            grads_feed_dict = {\n                self.actor_aac.local_network.train_phase: True,\n                self.critic_aac.local_network.train_phase: True,\n            }\n            grads_feed_dict.update(\n                {pl: value for pl, value in zip(self.actor_aac.grads_placeholders, actor_grads_values)}\n            )\n\n            # Update critic with gradients collected from generated data:\n            sess.run(self.update_critic_op, feed_dict=grads_feed_dict)\n            # self.log.warning(\'Critic update ok.\')\n\n            # Collect real train trajectory rollout using updated critic policy:\n            critic_data = self.critic_aac.get_data(data_sample_config=critic_data_config)\n            critic_feed_dict = self.critic_aac.process_data(\n                sess,\n                critic_data,\n                is_train=True,\n                pi=self.critic_aac.local_network\n            )\n            # self.log.warning(\'Critic data ok.\')\n\n            # Get gradients from critic:\n            if wirte_model_summary:\n                critic_fetches = [self.critic_aac.grads, self.critic_aac.model_summary_op]\n\n            else:\n                critic_fetches = [self.critic_aac.grads]\n\n            critic_fetched = sess.run(critic_fetches, feed_dict=critic_feed_dict)\n            critic_grads_values = critic_fetched[0]\n            # self.log.warning(\'Critic gradients ok.\')\n\n            # Update gradients feeder with critic\'s:\n            grads_feed_dict.update(\n                {pl: value for pl, value in zip(self.critic_aac.grads_placeholders, critic_grads_values)}\n            )\n\n            # Finally send combined gradients update to parameters server:\n            sess.run([self.train_op], feed_dict=grads_feed_dict)\n            # sess.run([self.actor_aac.train_op], feed_dict=actor_feed_dict)\n\n            # self.log.warning(\'Final gradients ok.\')\n\n            if wirte_model_summary:\n                critic_model_summary = critic_fetched[-1]\n                actor_model_summary = actor_fetched[-1]\n\n            else:\n                critic_model_summary = None\n                actor_model_summary = None\n\n            # Write down summaries:\n            self.actor_aac.process_summary(sess, actor_data, actor_model_summary)\n            self.critic_aac.process_summary(sess, critic_data, critic_model_summary)\n            self.local_steps += 1\n\n        except Exception as e:\n            msg = \'process_train() exception occurred\' + \\\n                  \'\\n\\nPress `Ctrl-C` or jupyter:[Kernel]->[Interrupt] for clean exit.\\n\'\n            self.log.exception(msg)\n            raise e\n\n\nclass TestAMLDG(AMLDG):\n    """"""\n    Convenience evaluating wrapper.\n    """"""\n    def __init__(self, *args, train_phase=None, name=None, **kwargs):\n        super().__init__(*args, train_phase=False, name=\'TestAMLDG\', **kwargs)\n\n\nclass TrainAMLDG(AMLDG):\n    """"""\n    Convenience training wrapper.\n    """"""\n    def __init__(self, *args, train_phase=None, name=None, **kwargs):\n        super().__init__(*args, train_phase=True, name=\'TrainAMLDG\', **kwargs)'"
btgym/research/model_based/runner.py,0,"b'import numpy as np\nfrom btgym.algorithms.runner.synchro import BaseSynchroRunner\n\n\nclass OUpRunner(BaseSynchroRunner):\n    """"""\n    Extends `BaseSynchroRunner` class with additional summaries related to Orn-Uhl. data generating process.\n    """"""\n\n    def __init__(self, name=\'OUp_synchro\', **kwargs):\n        super(OUpRunner, self).__init__(name=name, **kwargs)\n\n        # True data_generating_process params:\n        self.dgp_params = {key: [] for key in self.env.observation_space.shape[\'metadata\'][\'generator\'].keys()}\n        self.dgp_dict = {key: 0 for key in self.env.observation_space.shape[\'metadata\'][\'generator\'].keys()}\n        # print(\'self.dgp_params_00: \', self.dgp_params)\n\n        self.policy.callback[\'dgp_params\'] = self.pull_dgp_params\n\n    @staticmethod\n    def pull_dgp_params(self, **kwargs):\n        """"""Self is ok.""""""\n        # print(\'metadata: \', kwargs[\'experience\'][\'state\'][\'metadata\'][\'generator\'])\n        self.dgp_dict = kwargs[\'experience\'][\'state\'][\'metadata\'][\'generator\']\n\n    def get_train_stat(self, is_test=False):\n        """"""\n        Updates and computes average statistics for train episodes.\n        Args:\n            is_test: bool, current episode type\n\n        Returns:\n            dict of stats\n        """"""\n        ep_stat = {}\n        if not is_test:\n            self.total_r += [self.reward_sum]\n            episode_stat = self.env.get_stat()  # get episode statistic\n            last_i = self.info[-1]  # pull most recent info\n            self.cpu_time += [episode_stat[\'runtime\'].total_seconds()]\n            self.final_value += [last_i[\'broker_value\']]\n            self.total_steps += [episode_stat[\'length\']]\n\n            for key, accum in self.dgp_params.items():\n                accum.append(self.dgp_dict[key])\n\n            if self.local_episode % self.episode_summary_freq == 0:\n                ep_stat = dict(\n                    total_r=np.average(self.total_r),\n                    cpu_time=np.average(self.cpu_time),\n                    final_value=np.average(self.final_value),\n                    steps=np.average(self.total_steps),\n                    ou_lambda=np.average(self.dgp_params[\'l\']),\n                    ou_sigma=np.average(self.dgp_params[\'sigma\']),\n                    ou_mu=np.average(self.dgp_params[\'mu\']),\n                )\n                self.total_r = []\n                self.cpu_time = []\n                self.final_value = []\n                self.total_steps = []\n                self.total_steps_atari = []\n                self.dgp_params = {key: [] for key in self.env.observation_space.shape[\'metadata\'][\'generator\'].keys()}\n                # print(\'ep_stat: \', ep_stat)\n        return ep_stat\n\n'"
btgym/research/model_based/strategy.py,0,"b'import backtrader.indicators as btind\n\nfrom gym import spaces\nfrom btgym import DictSpace\n\nimport numpy as np\nfrom scipy import stats\nfrom pykalman import KalmanFilter\n\nfrom btgym.research.strategy_gen_6.base import BaseStrategy6, Zscore, NormalisationState\nfrom btgym.research.strategy_gen_6.utils import SpreadSizer, SpreadConstructor, CumSumReward\nfrom btgym.research.model_based.model.bivariate import BivariatePriceModel\nfrom btgym.research.model_based.model.utils import cov2corr, log_stat2stat\n\n\nclass PairSpreadStrategy_0(BaseStrategy6):\n    """"""\n    Expects pair of data streams. Forms spread as only virtual trading asset.\n    """"""\n\n    # Time embedding period:\n    time_dim = 128  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 64\n\n    # Possible agent actions;  Note: place \'hold\' first! :\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    features_parameters = (1, 4, 16, 64, 256, 1024)\n    num_features = len(features_parameters)\n\n    params = dict(\n        state_shape={\n            \'external\': spaces.Box(low=-10, high=10, shape=(time_dim, 1, num_features*2), dtype=np.float32),\n            \'internal\': spaces.Box(low=-100, high=100, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'expert\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n            \'stat\': spaces.Box(low=-100, high=100, shape=(3, 1), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10**10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10**10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10**10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                    # TODO: make generator parameters names standard\n                    \'generator\': DictSpace(\n                        {\n                            \'mu\': spaces.Box(\n                                shape=(),\n                                low=np.finfo(np.float64).min,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'l\': spaces.Box(\n                                shape=(),\n                                low=0,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'sigma\': spaces.Box(\n                                shape=(),\n                                low=0,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'x0\': spaces.Box(\n                                shape=(),\n                                low=np.finfo(np.float64).min,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            )\n                        }\n                    )\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        slippage=None,\n        leverage=1.0,\n        gamma=1.0,              # fi_gamma, ~ should match MDP gamma decay\n        reward_scale=1,         # reward multiplicator\n        norm_alpha=0.001,       # renormalisation tracking decay in []0, 1]\n        norm_alpha_2=0.01,      # float in []0, 1], tracking decay for original prices\n        drawdown_call=10,       # finish episode when hitting drawdown treshghold, in percent.\n        dataset_stat=None,      # Summary descriptive statistics for entire dataset and\n        episode_stat=None,      # current episode. Got updated by server.\n        time_dim=time_dim,      # time embedding period\n        avg_period=avg_period,  # number of time steps reward estimation statistics are averaged over\n        features_parameters=features_parameters,\n        num_features=num_features,\n        metadata={},\n        broadcast_message={},\n        trial_stat=None,\n        trial_metadata=None,\n        portfolio_actions=portfolio_actions,\n        skip_frame=1,       # number of environment steps to skip before returning next environment response\n        position_max_depth=1,\n        order_size=1,  # legacy plug, to be removed <-- rework gen_6.__init__\n        initial_action=None,\n        initial_portfolio_action=None,\n        state_int_scale=1,\n        state_ext_scale=1,\n    )\n\n    def __init__(self, **kwargs):\n        super(PairSpreadStrategy_0, self).__init__(**kwargs)\n\n        assert len(self.p.asset_names) == 1, \'Only one derivative spread asset is supported\'\n        assert len(self.getdatanames()) == 2, \\\n            \'Expected exactly two input datalines but {} where given\'.format(self.getdatanames())\n\n        if isinstance(self.p.asset_names, str):\n            self.p.asset_names = [self.p.asset_names]\n        self.action_key = list(self.p.asset_names)[0]\n\n        self.current_expert_action = np.zeros(len(self.p.portfolio_actions))\n        self.state[\'metadata\'] = self.metadata\n\n        # Infer OU generator params:\n        generator_keys = self.p.state_shape[\'metadata\'].spaces[\'generator\'].spaces.keys()\n        if \'generator\' not in self.p.metadata.keys() or self.p.metadata[\'generator\'] == {}:\n            self.metadata[\'generator\'] = {key: np.asarray(0) for key in generator_keys}\n\n        else:\n            # self.metadata[\'generator\'] = {key: self.p.metadata[\'generator\'][key] for key in generator_keys}\n\n            # TODO: clean up this mess, refine names:\n\n            self.metadata[\'generator\'] = {\n                \'l\': self.p.metadata[\'generator\'][\'ou_lambda\'],\n                \'mu\': self.p.metadata[\'generator\'][\'ou_mu\'],\n                \'sigma\': self.p.metadata[\'generator\'][\'ou_sigma\'],\n                \'x0\': 0,\n            }\n\n            # Make scalars np arrays to comply gym.spaces.Box specs:\n            for k, v in self.metadata[\'generator\'].items():\n                self.metadata[\'generator\'][k] = np.asarray(v)\n\n        self.last_delta_total_pnl = 0\n        self.last_pnl = 0\n\n        self.log.debug(\'startegy got broadcast_msg: <<{}>>\'.format(self.p.broadcast_message))\n\n        # Track original prices statistics, let base self.norm_stat_tracker track spread (=stat_asset) itself:\n        self.norm_stat_tracker_2 = Zscore(2, self.p.norm_alpha_2)\n\n        # Synthetic spread order size estimator:\n        self.spread_sizer = SpreadSizer(\n            init_cash=self.p.start_cash,\n            position_max_depth=self.p.position_max_depth,\n            leverage=self.p.leverage,\n            margin_reserve=self.margin_reserve,\n        )\n        self.last_action = None\n\n        # Keeps track of virtual spread position\n        # long_ spread: >0, short_spread: <0, no positions: 0\n        self.spread_position_size = 0\n\n        # Reward signal filtering:\n        self.kf = KalmanFilter(\n            initial_state_mean=0,\n            transition_covariance=.01,\n            observation_covariance=1,\n            n_dim_obs=1\n        )\n        self.kf_state = [0, 0]\n\n    def set_datalines(self):\n        # Override stat line:\n        self.stat_asset = self.data.spread = SpreadConstructor()\n\n        # Spy on reward behaviour:\n        self.reward_tracker = CumSumReward()\n\n        self.data.std = btind.StdDev(self.data.spread, period=self.p.time_dim, safepow=True)\n        self.data.std.plotinfo.plot = False\n\n        self.data.features = [btind.EMA(self.data.spread, period=period) for period in self.p.features_parameters]\n        initial_time_period = np.asarray(self.p.features_parameters).max() + self.p.time_dim\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=initial_time_period\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n    def get_broadcast_message(self):\n        """"""\n        Not used.\n        """"""\n        return {\n            \'data_model_psi\': np.zeros([2, 3]),\n            \'iteration\': self.iteration\n        }\n\n    def get_expert_state(self):\n        """"""\n        Not used.\n        """"""\n        return np.zeros(len(self.p.portfolio_actions))\n\n    def prenext(self):\n        if self.pre_iteration + 2 > self.p.time_dim - self.avg_period:\n            self.update_broker_stat()\n            x_upd = np.stack(\n                [\n                    np.asarray(self.datas[0].get(size=1)),\n                    np.asarray(self.datas[1].get(size=1))\n                ],\n                axis=0\n            )\n            _ = self.norm_stat_tracker_2.update(x_upd)  # doubles update_broker_stat() but helps faster stabilization\n\n        elif self.pre_iteration + 2 == self.p.time_dim - self.avg_period:\n            # Initialize all trackers:\n            x_init = np.stack(\n                [\n                    np.asarray(self.datas[0].get(size=self.data.close.buflen())),\n                    np.asarray(self.datas[1].get(size=self.data.close.buflen()))\n                ],\n                axis=0\n            )\n            _ = self.norm_stat_tracker_2.reset(x_init)\n            _ = self.norm_stat_tracker.reset(np.asarray(self.stat_asset.get(size=self.data.close.buflen()))[None, :])\n            # _ = self.norm_stat_tracker.reset(np.asarray(self.stat_asset.get(size=1))[None, :])\n\n        self.pre_iteration += 1\n\n    def nextstart(self):\n        self.inner_embedding = self.data.close.buflen()\n        self.log.debug(\'Inner time embedding: {}\'.format(self.inner_embedding))\n\n    def get_normalisation(self):\n        """"""\n        Estimates current normalisation constants, updates `normalisation_state` attr.\n\n        Returns:\n            instance of NormalisationState tuple\n        """"""\n        # Update synth. spread rolling normalizers:\n        x_upd = np.stack(\n            [\n                np.asarray(self.datas[0].get(size=1)),\n                np.asarray(self.datas[1].get(size=1))\n            ],\n            axis=0\n        )\n        _ = self.norm_stat_tracker_2.update(x_upd)\n\n        # ...and use [normalised] spread rolling mean and variance to estimate NormalisationState\n        # used to normalize all broker statistics and reward:\n        spread_data = np.asarray(self.stat_asset.get(size=1))\n\n        mean, var = self.norm_stat_tracker.update(spread_data[None, :])\n        var = np.clip(var, 1e-8, None)\n\n        # Use 99% N(stat_data_mean, stat_data_std) intervals as normalisation interval:\n        intervals = stats.norm.interval(.99, mean, var ** .5)\n        self.normalisation_state = NormalisationState(\n            mean=float(mean),\n            variance=float(var),\n            low_interval=intervals[0][0],\n            up_interval=intervals[1][0]\n        )\n        return self.normalisation_state\n\n    def get_stat_state(self):\n        return np.concatenate(\n            [np.asarray(self.norm_stat_tracker.get_state()), np.asarray(self.stat_asset.get())[None, :]],\n            axis=0\n        )\n\n    def get_external_state(self):\n        """"""\n        Attempt to include avg decomp. of original normalised spread\n        """"""\n        x_sma = np.stack(\n            [\n                feature.get(size=self.p.time_dim) for feature in self.data.features\n            ],\n            axis=-1\n        )\n        scale = 1 / np.clip(self.data.std[0], 1e-10, None)\n        x_sma *= scale  # <-- more or less ok\n\n        # Gradient along features axis:\n        dx = np.gradient(x_sma, axis=-1)\n\n        # TODO: different conv. encoders for these two types of features:\n        x = np.concatenate([x_sma, dx], axis=-1)\n\n        # Crop outliers:\n        x = np.clip(x, -10, 10)\n        return x[:, None, :]\n\n    def get_order_sizes(self):\n        """"""\n        Estimates current order sizes for assets in trade, updates attribute.\n\n        Returns:\n            array-like of floats\n        """"""\n        s = self.norm_stat_tracker_2.get_state()\n        self.current_order_sizes = np.asarray(\n            self.spread_sizer.get_sizing(self.env.broker.get_value(), s.mean, s.variance),\n            dtype=np.float\n        )\n        return self.current_order_sizes\n\n    def long_spread(self):\n        """"""\n        Opens or adds up long spread `virtual position`.\n        """"""\n        # Get current sizes:\n        order_sizes = self.get_order_sizes()\n\n        if self.spread_position_size >= 0:\n            if not self.can_add_up(order_sizes[0], order_sizes[1]):\n                self.order_failed += 1\n                # self.log.warning(\n                #     \'Adding Long spread to existing {} hit margin, ignored\'.format(self.spread_position_size)\n                # )\n                return\n\n        elif self.spread_position_size == -1:\n            # Currently in single short -> just close to prevent disballance:\n            return self.close_spread()\n\n        name1 = self.datas[0]._name\n        name2 = self.datas[1]._name\n\n        self.order = self.buy(data=name1, size=order_sizes[0])\n        self.order = self.sell(data=name2, size=order_sizes[1])\n        self.spread_position_size += 1\n        # self.log.warning(\'long spread submitted, new pos. size: {}\'.format(self.spread_position_size))\n\n    def short_spread(self):\n        order_sizes = self.get_order_sizes()\n\n        if self.spread_position_size <= 0:\n            if not self.can_add_up(order_sizes[0], order_sizes[1]):\n                self.order_failed += 1\n                # self.log.warning(\n                #     \'Adding Short spread to existing {} hit margin, ignored\'.format(self.spread_position_size)\n                # )\n                return\n\n        elif self.spread_position_size == 1:\n            # Currently in single long:\n            return self.close_spread()\n\n        name1 = self.datas[0]._name\n        name2 = self.datas[1]._name\n\n        self.order = self.sell(data=name1, size=order_sizes[0])\n        self.order = self.buy(data=name2, size=order_sizes[1])\n        self.spread_position_size -= 1\n        # self.log.warning(\'short spread submitted, new pos. size: {}\'.format(self.spread_position_size))\n\n    def close_spread(self):\n        self.order = self.close(data=self.datas[0]._name)\n        self.order = self.close(data=self.datas[1]._name)\n        self.spread_position_size = 0\n        # self.log.warning(\'close spread submitted, new pos. size: {}\'.format(self.spread_position_size))\n\n    def can_add_up(self, order_0_size=None, order_1_size=None):\n        """"""\n        Checks if there enough cash left to open synthetic spread position\n\n        Args:\n            order_0_size:   float, order size for data0 asset or None\n            order_1_size:   float, order size for data1 asset or None\n\n        Returns:\n            True if possible, False otherwise\n        """"""\n        if order_1_size is None or order_0_size is None:\n            order_sizes = self.get_order_sizes()\n            order_0_size = order_sizes[0]\n            order_1_size = order_sizes[1]\n\n        # Get full operation cost:\n        # TODO: it can be two commissions schemes\n        op_cost = [\n            self.env.broker.comminfo[None].getoperationcost(\n                size=size,\n                price=self.getdatabyname(name).high[0]\n            ) / self.env.broker.comminfo[None].get_leverage() +\n            self.env.broker.comminfo[None].getcommission(\n                size=size,\n                price=self.getdatabyname(name).high[0]\n            )\n            for size, name in zip([order_0_size, order_1_size], [self.datas[0]._name, self.datas[1]._name])\n        ]\n        # self.log.warning(\'op_cost+comm+reserve: {:.4f}\'.format(np.asarray(op_cost).sum() + self.margin_reserve))\n        # self.log.warning(\'order sizes: {:.4f}; {:.4f}\'.format(order_0_size, order_1_size))\n        # self.log.warning(\'leverage: {}\'.format(self.env.broker.comminfo[None].get_leverage()))\n        # self.log.warning(\n        #     \'commision: {:.4f} + {:.4f}\'.format(\n        #         self.env.broker.comminfo[None].getcommission(\n        #             size=order_0_size,\n        #             price=self.getdatabyname(self.datas[0]._name).high[0]\n        #         ),\n        #         self.env.broker.comminfo[None].getcommission(\n        #             size=order_1_size,\n        #             price=self.getdatabyname(self.datas[1]._name).high[0]\n        #         ),\n        #     )\n        # )\n        # self.log.warning(\'current_cash: {}\'.format(self.env.broker.get_cash()))\n        if np.asarray(op_cost).sum() + self.margin_reserve >= self.env.broker.get_cash() * (1 - self.margin_reserve):\n            # self.log.warning(\'add_up check failed\')\n            return False\n\n        else:\n            # self.log.warning(\'add_up check ok\')\n            return True\n\n    def get_broker_pos_duration(self, **kwargs):\n        """"""\n        Position duration is measured w.r.t. virtual spread position, not broker account exposure\n        """"""\n        if self.spread_position_size == 0:\n            self.current_pos_duration = 0\n            # self.log.warning(\'zero position\')\n\n        else:\n            self.current_pos_duration += 1\n            # self.log.warning(\'position duration: {}\'.format(self.current_pos_duration))\n\n        return self.current_pos_duration\n\n    def notify_order(self, order):\n        """"""\n        Shamelessly taken from backtrader tutorial.\n        TODO: better multi data support\n        """"""\n        if order.status in [order.Submitted, order.Accepted]:\n            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n            return\n        # Check if an order has been completed\n        # Attention: broker could reject order if not enough cash\n        if order.status in [order.Completed]:\n            if order.isbuy():\n                self.broker_message = \'BUY executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n                self.buyprice = order.executed.price\n                self.buycomm = order.executed.comm\n\n            else:  # Sell\n                self.broker_message = \'SELL executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n            self.bar_executed = len(self)\n\n        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n            self.broker_message = \'ORDER FAILED with status: \' + str(order.getstatusname())\n\n        # self.log.warning(\'BM: {}\'.format(self.broker_message))\n        self.order = None\n\n    def get_reward(self):\n        """"""\n        Shapes reward function as normalized single trade realized profit/loss,\n        augmented with potential-based reward shaping functions in form of:\n        F(s, a, s`) = gamma * FI(s`) - FI(s);\n        Potential FI_1 is current normalized unrealized profit/loss.\n\n        Paper:\n            ""Policy invariance under reward transformations:\n             Theory and application to reward shaping"" by A. Ng et al., 1999;\n             http://www.robotics.stanford.edu/~ang/papers/shaping-icml99.pdf\n        """"""\n\n        # All sliding statistics for this step are already updated by get_state().\n\n        # Potential-based shaping function 1:\n        # based on potential of averaged profit/loss for current opened trade (unrealized p/l):\n        unrealised_pnl = np.asarray(self.broker_stat[\'unrealized_pnl\'])\n        current_pos_duration = self.broker_stat[\'pos_duration\'][-1]\n\n        # We want to estimate potential `fi = gamma*fi_prime - fi` of current opened position,\n        # thus need to consider different cases given skip_fame parameter:\n        if current_pos_duration == 0:\n            # Set potential term to zero if there is no opened positions:\n            fi_1 = 0\n            fi_1_prime = 0\n            # Reset filter state:\n            self.kf_state = [0, 0]\n        else:\n            fi_1 = self.last_pnl\n            # fi_1_prime = np.average(unrealised_pnl[-1])\n            self.kf_state = self.kf.filter_update(\n                filtered_state_mean=self.kf_state[0],\n                filtered_state_covariance=self.kf_state[1],\n                observation=unrealised_pnl[-1],\n            )\n            fi_1_prime = np.squeeze(self.kf_state[0])\n\n        # Potential term 1:\n        f1 = self.p.gamma * fi_1_prime - fi_1\n        self.last_pnl = fi_1_prime\n\n        # Potential-based shaping function 2:\n        # based on potential of averaged profit/loss for global unrealized pnl:\n        total_pnl = np.asarray(self.broker_stat[\'total_unrealized_pnl\'])\n        delta_total_pnl = np.average(total_pnl[-self.p.skip_frame:]) - np.average(total_pnl[:-self.p.skip_frame])\n\n        fi_2 = delta_total_pnl\n        fi_2_prime = self.last_delta_total_pnl\n\n        # Potential term 2:\n        f2 = self.p.gamma * fi_2_prime - fi_2\n        self.last_delta_total_pnl = delta_total_pnl\n\n        # Potential term 3:\n        # f3 = 1 + 0.5 * np.log(1 + current_pos_duration)\n        f3 = 1.0\n\n        # Main reward function: normalized realized profit/loss:\n        realized_pnl = np.asarray(self.broker_stat[\'realized_pnl\'])[-self.p.skip_frame:].sum()\n\n        # Weights are subject to tune:\n        self.reward = (0.1 * f1 * f3 + 1.0 * realized_pnl) * self.p.reward_scale #/ self.normalizer\n        # self.reward = np.clip(self.reward, -self.p.reward_scale, self.p.reward_scale)\n\n        self.reward = np.clip(self.reward, -1e3, 1e3)\n\n        return self.reward\n\n    def _next_discrete(self, action):\n        """"""\n        Manages spread virtual positions.\n\n        Args:\n            action:     dict, string encoding of btgym.spaces.ActionDictSpace\n\n        """"""\n        # Here we expect action dict to contain single key:\n        single_action = action[self.action_key]\n\n        if single_action == \'hold\' or self.is_done_enabled:\n            pass\n        elif single_action == \'buy\':\n            self.long_spread()\n            self.broker_message = \'new {}_LONG created; \'.format(self.action_key) + self.broker_message\n        elif single_action == \'sell\':\n            self.short_spread()\n            self.broker_message = \'new {}_SHORT created; \'.format(self.action_key) + self.broker_message\n        elif single_action == \'close\':\n            self.close_spread()\n            self.broker_message = \'new {}_CLOSE created; \'.format(self.action_key) + self.broker_message\n\n\nclass PairSpreadStrategy_1(PairSpreadStrategy_0):\n    """"""\n    Expects pair of data streams. Encodes each asset independently.\n    """"""\n\n    # Time embedding period:\n    time_dim = 128  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = 64\n\n    # Possible agent actions;  Note: place \'hold\' first! :\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    features_parameters = (1, 4, 16, 64, 256, 1024)\n    num_features = len(features_parameters)\n\n    params = dict(\n        state_shape={\n            \'external\': DictSpace(\n                {\n                    \'asset1\': spaces.Box(low=-10, high=10, shape=(time_dim, 1, num_features), dtype=np.float32),\n                    \'asset2\': spaces.Box(low=-10, high=10, shape=(time_dim, 1, num_features), dtype=np.float32),\n                }\n            ),\n\n            \'internal\': spaces.Box(low=-100, high=100, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'expert\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),\n            \'stat\': spaces.Box(low=-100, high=100, shape=(3, 1), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                    # TODO: make generator parameters names standard\n                    \'generator\': DictSpace(\n                        {\n                            \'mu\': spaces.Box(\n                                shape=(),\n                                low=np.finfo(np.float64).min,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'l\': spaces.Box(\n                                shape=(),\n                                low=0,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'sigma\': spaces.Box(\n                                shape=(),\n                                low=0,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'x0\': spaces.Box(\n                                shape=(),\n                                low=np.finfo(np.float64).min,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            )\n                        }\n                    )\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        slippage=None,\n        leverage=1.0,\n        gamma=1.0,  # fi_gamma, ~ should match MDP gamma decay\n        reward_scale=1,  # reward multiplicator\n        norm_alpha=0.001,  # renormalisation tracking decay in []0, 1]\n        norm_alpha_2=0.01,  # float in []0, 1], tracking decay for original prices\n        drawdown_call=10,  # finish episode when hitting drawdown treshghold, in percent.\n        dataset_stat=None,  # Summary descriptive statistics for entire dataset and\n        episode_stat=None,  # current episode. Got updated by server.\n        time_dim=time_dim,  # time embedding period\n        avg_period=avg_period,  # number of time steps reward estimation statistics are averaged over\n        features_parameters=features_parameters,\n        num_features=num_features,\n        metadata={},\n        broadcast_message={},\n        trial_stat=None,\n        trial_metadata=None,\n        portfolio_actions=portfolio_actions,\n        skip_frame=1,  # number of environment steps to skip before returning next environment response\n        position_max_depth=1,\n        order_size=1,  # legacy plug, to be removed <-- rework gen_6.__init__\n        initial_action=None,\n        initial_portfolio_action=None,\n        state_int_scale=1,\n        state_ext_scale=1,\n    )\n\n    def set_datalines(self):\n        # Override stat line:\n        self.stat_asset = self.data.spread = SpreadConstructor()\n\n        # Spy on reward behaviour:\n        self.reward_tracker = CumSumReward()\n\n        self.data.std1 = btind.StdDev(self.datas[0], period=self.p.time_dim, safepow=True)\n        self.data.std1.plotinfo.plot = False\n\n        self.data.std2 = btind.StdDev(self.datas[1], period=self.p.time_dim, safepow=True)\n        self.data.std2.plotinfo.plot = False\n\n        self.data.features1 = [btind.EMA(self.datas[0], period=period) for period in self.p.features_parameters]\n        self.data.features2 = [btind.EMA(self.datas[1], period=period) for period in self.p.features_parameters]\n\n        initial_time_period = np.asarray(self.p.features_parameters).max() + self.p.time_dim\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=initial_time_period\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n    def get_external_state(self):\n        """"""\n        Attempt to include avg decomp. of original normalised spread\n        """"""\n        x_sma1 = np.stack(\n            [\n                feature.get(size=self.p.time_dim) for feature in self.data.features1\n            ],\n            axis=-1\n        )\n        scale = 1 / np.clip(self.data.std1[0], 1e-10, None)\n        x_sma1 *= scale  # <-- more or less ok\n\n        # Gradient along features axis:\n        dx1 = np.gradient(x_sma1, axis=-1)\n        dx1 = np.clip(dx1, -10, 10)\n\n        x_sma2 = np.stack(\n            [\n                feature.get(size=self.p.time_dim) for feature in self.data.features2\n            ],\n            axis=-1\n        )\n        scale = 1 / np.clip(self.data.std2[0], 1e-10, None)\n        x_sma2 *= scale  # <-- more or less ok\n\n        # Gradient along features axis:\n        dx2 = np.gradient(x_sma2, axis=-1)\n        dx2 = np.clip(dx2, -10, 10)\n\n        return {\'asset1\': dx1[:, None, :], \'asset2\': dx2[:, None, :],}\n\n\nclass SSAStrategy_0(PairSpreadStrategy_0):\n    """"""\n    BivariateTimeSeriesModel decomposition based.\n    """"""\n    time_dim = 128\n    avg_period = 16\n    model_time_dim = 16\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n    features_parameters = None\n    num_features = 4\n\n    params = dict(\n        state_shape={\n            \'external\': DictSpace(\n                {\n                    \'ssa\': spaces.Box(low=-100, high=100, shape=(time_dim, 1, num_features), dtype=np.float32),\n\n                }\n            ),\n            \'internal\': DictSpace(\n                {\n                    \'broker\': spaces.Box(low=-100, high=100, shape=(avg_period, 1, 5), dtype=np.float32),\n                    \'model\': spaces.Box(low=-100, high=100, shape=(model_time_dim, 1, 9), dtype=np.float32),\n                }\n            ),\n            \'expert\': spaces.Box(low=0, high=10, shape=(len(portfolio_actions),), dtype=np.float32),  # not used\n            \'stat\': spaces.Box(low=-1e6, high=1e6, shape=(3, 1), dtype=np.float32),  # for debug. proposes only\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                    \'generator\': DictSpace(  # ~ S-generator params.\n                        {\n                            \'mu\': spaces.Box(\n                                shape=(),\n                                low=np.finfo(np.float64).min,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'l\': spaces.Box(\n                                shape=(),\n                                low=0,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'sigma\': spaces.Box(\n                                shape=(),\n                                low=0,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            ),\n                            \'x0\': spaces.Box(\n                                shape=(),\n                                low=np.finfo(np.float64).min,\n                                high=np.finfo(np.float64).max,\n                                dtype=np.float64\n                            )\n                        }\n                    )\n                }\n            )\n        },\n        data_model_params=dict(\n            alpha=.001,\n            stat_alpha=.0001,\n            filter_alpha=.05,\n            max_length=time_dim * 2,\n            analyzer_window=10,\n            p_analyzer_grouping=[[0, 1], [1, 2], [2, 3], [3, None]],\n            s_analyzer_grouping=[[0, 1], [1, 2], [2, 3], [3, None]]\n        ),\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        slippage=None,\n        leverage=1.0,\n        gamma=1.0,              # fi_gamma, should match MDP gamma decay\n        reward_scale=1,         # reward multiplicator\n        norm_alpha=0.001,       # float in []0, 1], renormalisation tracking decay (for synth. spread)\n        norm_alpha_2=0.01,     # float in []0, 1], tracking decay for original prices\n        drawdown_call=10,       # finish episode when hitting drawdown threshold, in percent.\n        dataset_stat=None,      # Summary descriptive statistics for entire dataset and\n        episode_stat=None,      # current episode. Got updated by server.\n        time_dim=time_dim,      # time embedding period\n        avg_period=avg_period,  # number of time steps reward estimation statistics are averaged over\n        features_parameters=features_parameters,\n        num_features=num_features,\n        metadata={},\n        broadcast_message={},\n        trial_stat=None,\n        trial_metadata=None,\n        portfolio_actions=portfolio_actions,\n        skip_frame=1,  # number of environment steps to skip before returning next environment response\n        position_max_depth=1,\n        order_size=1,  # legacy plug, to be removed <-- rework gen_6.__init__\n        initial_action=None,\n        initial_portfolio_action=None,\n        state_int_scale=1,\n        state_ext_scale=1,\n    )\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # Bivariate model:\n        self.data_model = BivariatePriceModel(**self.p.data_model_params)\n\n        # Accumulators for \'model\' observation mode:\n        self.external_model_state = np.zeros([self.model_time_dim, 1, 9])\n\n\n    def set_datalines(self):\n        # Discard superclass dataline, use SpreadConstructor instead:\n        self.data.spread = None\n\n        # Override stat line:\n        self.stat_asset = self.SpreadConstructor()\n\n        # Spy on reward behaviour:\n        self.reward_tracker = self.CumSumReward()\n\n        initial_time_period = self.p.time_dim\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=initial_time_period\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n    def prenext(self):\n        if self.pre_iteration + 2 > self.p.time_dim - self.avg_period:\n            self.update_broker_stat()\n            x_upd = np.stack(\n                [\n                    np.asarray(self.datas[0].get(size=1)),\n                    np.asarray(self.datas[1].get(size=1))\n                ],\n                axis=0\n            )\n            _ = self.norm_stat_tracker_2.update(x_upd)  # doubles update_broker_stat() but helps faster stabilization\n            self.data_model.update(x_upd)\n\n        elif self.pre_iteration + 2 == self.p.time_dim - self.avg_period:\n            # Initialize all trackers:\n            x_init = np.stack(\n                [\n                    np.asarray(self.datas[0].get(size=self.data.close.buflen())),\n                    np.asarray(self.datas[1].get(size=self.data.close.buflen()))\n                ],\n                axis=0\n            )\n            _ = self.norm_stat_tracker_2.reset(x_init)\n            _ = self.norm_stat_tracker.reset(np.asarray(self.stat_asset.get(size=self.data.close.buflen()))[None, :])\n            # _ = self.norm_stat_tracker.reset(np.asarray(self.stat_asset.get(size=1))[None, :])\n            self.data_model.reset(x_init)\n\n        self.pre_iteration += 1\n\n    def nextstart(self):\n        self.inner_embedding = self.data.close.buflen()\n        self.log.debug(\'Inner time embedding: {}\'.format(self.inner_embedding))\n\n        # self.log.warning(\n        #     \'Pos. max. depth: {}, leverage: {}, order sizes: {:.4f}, {:.4f}\'.format(\n        #         self.p.position_max_depth,\n        #         self.p.leverage,\n        #         size_0,\n        #         size_1\n        #     )\n        # )\n\n    def get_normalisation(self):\n        """"""\n        Estimates current normalisation constants, updates `normalisation_state` attr.\n\n        Returns:\n            instance of NormalisationState tuple\n        """"""\n        # Update synth. spread rolling normalizers:\n        x_upd = np.stack(\n            [\n                np.asarray(self.datas[0].get(size=1)),\n                np.asarray(self.datas[1].get(size=1))\n            ],\n            axis=0\n        )\n        _ = self.norm_stat_tracker_2.update(x_upd)\n\n        # ...and use [normalised] spread rolling mean and variance to estimate NormalisationState\n        # used to normalize all broker statistics and reward:\n        spread_data = np.asarray(self.stat_asset.get(size=1))\n\n        mean, var = self.norm_stat_tracker.update(spread_data[None, :])\n        var = np.clip(var, 1e-8, None)\n\n        # Use 99% N(stat_data_mean, stat_data_std) intervals as normalisation interval:\n        intervals = stats.norm.interval(.99, mean, var ** .5)\n        self.normalisation_state = NormalisationState(\n            mean=float(mean),\n            variance=float(var),\n            low_interval=intervals[0][0],\n            up_interval=intervals[1][0]\n        )\n        return self.normalisation_state\n\n    def get_external_state(self):\n        return dict(\n            ssa=self.get_external_ssa_state(),\n        )\n\n    def get_internal_state(self):\n        return dict(\n            broker=self.get_internal_broker_state(),\n            model=self.get_data_model_state(),\n        )\n\n    def get_external_ssa_state(self):\n        """"""\n        Spread SSA decomposition.\n        """"""\n        x_upd = np.stack(\n            [\n                np.asarray(self.datas[0].get(size=self.p.skip_frame)),\n                np.asarray(self.datas[1].get(size=self.p.skip_frame))\n            ],\n            axis=0\n        )\n        # self.log.warning(\'x_upd: {}\'.format(x_upd.shape))\n        self.data_model.update(x_upd)\n\n        x_ssa = self.data_model.s.transform(size=self.p.time_dim).T  #* self.normalizer\n\n        # Gradient along features axis:\n        # dx = np.gradient(x_ssa, axis=-1)\n        #\n        # # Add up: gradient  along time axis:\n        # # dx2 = np.gradient(dx, axis=0)\n        #\n        # x = np.concatenate([x_ssa_bank, dx], axis=-1)\n\n        # Crop outliers:\n        x_ssa = np.clip(x_ssa, -10, 10)\n        # x_ssa = np.clip(dx, -10, 10)\n        return x_ssa[:, None, :]\n\n    def get_data_model_state(self):\n        """"""\n         Spread stochastic model parameters.\n        """"""\n        state = self.data_model.s.process.get_state()\n        cross_corr = cov2corr(state.filtered.covariance)[[0, 0, 1], [1, 2, 2]]\n        update = np.concatenate(\n            [\n                state.filtered.mean.flatten(),\n                state.filtered.variance.flatten(),\n                cross_corr,\n            ]\n        )\n        self.external_model_state = np.concatenate(\n            [\n                self.external_model_state[1:, :, :],\n                update[None, None, :]\n            ],\n            axis=0\n        )\n        # self.external_model_state = np.gradient(self.external_model_state, axis=-1)\n        return self.external_model_state\n\n    def get_internal_broker_state(self):\n        stat_lines = (\'value\', \'unrealized_pnl\', \'realized_pnl\', \'cash\', \'exposure\')\n        x_broker = np.stack(\n            [np.asarray(self.broker_stat[name]) for name in stat_lines],\n            axis=-1\n        )\n        # self.log.warning(\'broker: {}\'.format(x_broker))\n        # self.log.warning(\'Ns: {}\'.format(self.normalisation_state))\n        # x_broker = np.gradient(x_broker, axis=-1)\n        return np.clip(x_broker[:, None, :], -100, 100)\n\n\n\n\n\n'"
btgym/research/strategy_gen_5/__init__.py,0,b''
btgym/research/strategy_gen_5/base.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017-2018 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nimport backtrader as bt\nimport backtrader.indicators as btind\n\nfrom gym import spaces\nfrom btgym import DictSpace\n\nimport numpy as np\nfrom collections import deque\n\nfrom btgym.strategy.utils import norm_value, decayed_result, exp_scale\n\n\n############################## Base BTgymStrategy Class ###################\n\n\nclass BaseStrategy5(bt.Strategy):\n    """"""\n    \'New and improved\' base startegy class.\n    Incorporates state declaration and preprocessing improvements.\n    Current candidate to replace current BTgymBaseStrategy.\n\n    Controls Environment inner dynamics and backtesting logic. Provides gym\'my (State, Action, Reward, Done, Info) data.\n    Any State, Reward and Info computation logic can be implemented by subclassing BTgymStrategy and overriding\n    get_[mode]_state(), get_reward(), get_info(), is_done() and set_datalines() methods.\n    One can always go deeper and override __init__ () and next() methods for desired\n    server cerebro engine behaviour, including order execution logic etc.\n\n    Note:\n        - base class supports single asset iteration via default data_line named \'base_asset\', see derived classes\n          multi-asset support\n        - bt.observers.DrawDown observer will be automatically added to BTgymStrategy instance at runtime.\n        - Since it is bt.Strategy subclass, refer to https://www.backtrader.com/docu/strategy.html for more information.\n    """"""\n    # Time embedding period:\n    time_dim = 4  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = time_dim\n\n    # Possible agent actions;  Note: place \'hold\' first! :\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    features_parameters = ()\n    num_features = len(features_parameters)\n\n    params = dict(\n        # Observation state shape is dictionary of Gym spaces,\n        # at least should contain `raw_state` field.\n        # By convention first dimension of every Gym Box space is time embedding one;\n        # one can define any shape; should match env.observation_space.shape.\n        # observation space state min/max values,\n        # For `raw_state\' (default) - absolute min/max values from BTgymDataset will be used.\n        state_shape={\n            \'raw\': spaces.Box(\n                shape=(time_dim, 4),\n                low=0,  # will get overridden.\n                high=0,\n                dtype=np.float32,\n            ),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        slippage=None,\n        leverage=1.0,\n        gamma=0.99,             # fi_gamma, should match MDP gamma decay\n        reward_scale=1,         # reward multiplicator\n        drawdown_call=10,       # finish episode when hitting drawdown treshghold , in percent.\n        target_call=10,         # finish episode when reaching profit target, in percent.\n        dataset_stat=None,      # Summary descriptive statistics for entire dataset and\n        episode_stat=None,      # current episode. Got updated by server.\n        time_dim=time_dim,      # time embedding period\n        avg_period=avg_period,  # number of time steps reward estimation statistics are averaged over\n        features_parameters=features_parameters,\n        num_features=num_features,\n        metadata={},\n        broadcast_message={},\n        trial_stat=None,\n        trial_metadata=None,\n        portfolio_actions=portfolio_actions,\n        skip_frame=1,       # number of environment steps to skip before returning next environment response\n        order_size=None,\n        initial_action=None,\n        initial_portfolio_action=None,\n        state_int_scale=1,\n        state_ext_scale=1,\n    )\n\n    def __init__(self, **kwargs):\n        """"""\n        Keyword Args:\n            params (dict):          parameters dictionary, see Note below.\n\n            Notes:\n                Due to backtrader convention, any strategy arguments should be defined inside `params` dictionary\n                or passed as kwargs to bt.Cerebro() class via .addstrategy() method. Parameter dictionary\n                should contain at least these keys::\n\n                    state_shape:        Observation state shape is dictionary of Gym spaces, by convention\n                                        first dimension of every Gym Box space is time embedding one;\n                    cash_name:          str, name for cash asset\n                    asset_names:        iterable of str, names for assets\n                    start_cash:         float, broker starting cash\n                    commission:         float, broker commission value, .01 stands for 1%\n                    leverage:           float, broker leverage\n                    slippage:           float, broker execution slippage\n                    order_size:         dict of fixed order stakes (floats); keys should match assets names.\n                    drawdown_call:      finish episode when hitting this drawdown treshghold , in percent.\n                    target_call:        finish episode when reaching this profit target, in percent.\n                    portfolio_actions:  possible agent actions.\n                    skip_frame:         number of environment steps to skip before returning next response,\n                                        e.g. if set to 10 -- agent will interact with environment every 10th step;\n                                        every other step agent action is assumed to be \'hold\'.\n\n                Default values are::\n\n                    state_shape=dict(raw_state=spaces.Box(shape=(4, 4), low=0, high=0,))\n                    cash_name=\'default_cash\'\n                    asset_names=[\'default_asset\']\n                    start_cash=None\n                    commission=None\n                    slippage=None,\n                    leverage=1.0\n                    drawdown_call=10\n                    target_call=10\n                    dataset_stat=None\n                    episode_stat=None\n                    portfolio_actions=(\'hold\', \'buy\', \'sell\', \'close\')\n                    skip_frame=1\n                    order_size=None\n        """"""\n        # Inherit logger from cerebro:\n        self.log = self.env._log\n\n        self.skip_frame = self.p.skip_frame\n\n        self.iteration = 0\n        self.env_iteration = 0\n        self.inner_embedding = 1\n        self.is_done = False\n        self.is_done_enabled = False\n        self.steps_till_is_done = 2  # extra steps to make when episode terminal conditions are met\n        self.action = self.p.initial_portfolio_action\n        self.action_to_repeat = self.p.initial_portfolio_action\n        self.action_repeated = 0\n        self.num_action_repeats = None\n        self.reward = 0\n        self.order = None\n        self.order_failed = 0\n        self.broker_message = \'_\'\n        self.final_message = \'_\'\n        self.raw_state = None\n        self.time_stamp = 0\n\n        # Configure state_shape:\n        if self.p.state_shape is None:\n            self.p.state_shape = self.set_state_shape()\n\n        # Prepare broker:\n        if self.p.start_cash is not None:\n            self.env.broker.setcash(self.p.start_cash)\n\n        if self.p.commission is not None:\n            self.env.broker.setcommission(commission=self.p.commission, leverage=self.p.leverage)\n\n        if self.p.slippage is not None:\n            # Bid/ask workaround: set overkill 10% slippage + slip_out=False\n            # ensuring we always buy at current \'high\'~\'ask\' and sell at \'low\'~\'bid\':\n            self.env.broker.set_slippage_perc(self.p.slippage, slip_open=True, slip_match=True, slip_out=False)\n\n        # Normalisation constant for statistics derived from account value:\n        self.broker_value_normalizer = 1 / \\\n            self.env.broker.startingcash / (self.p.drawdown_call + self.p.target_call) * 100\n\n        self.target_value = self.env.broker.startingcash * (1 + self.p.target_call / 100)\n\n        # Try to define stake, if no self.p.order_size dict has been set:\n        if self.p.order_size is None:\n            # If no order size has been set for every data_line,\n            # try to infer stake size from sizer set by bt.Cerebro.addsizer() method:\n            try:\n                assert len(list(self.env.sizers.values())) == 1\n                env_sizer_params = list(self.env.sizers.values())[0][-1]  # pull dict of outer set sizer params\n                assert \'stake\' in env_sizer_params.keys()\n\n            except (AssertionError, KeyError) as e:\n                msg = \'Order stake is not set neither via strategy.param.order_size nor via bt.Cerebro.addsizer method.\'\n                self.log.error(msg)\n                raise ValueError(msg)\n\n            self.p.order_size = {name: env_sizer_params[\'stake\'] for name in self.p.asset_names}\n\n        elif isinstance(self.p.order_size, int) or isinstance(self.p.order_size, float):\n            unimodal_stake = {name: self.p.order_size for name in self.getdatanames()}\n            self.p.order_size = unimodal_stake\n\n        # self.log.warning(\'asset names: {}\'.format(self.p.asset_names))\n        # self.log.warning(\'data names: {}\'.format(self.getdatanames()))\n\n        self.trade_just_closed = False\n        self.trade_result = 0\n\n        self.unrealized_pnl = None\n        self.norm_broker_value = None\n        self.realized_pnl = None\n\n        self.current_pos_duration = 0\n        self.current_pos_min_value = 0\n        self.current_pos_max_value = 0\n\n        self.realized_broker_value = self.env.broker.startingcash\n        self.episode_result = 0  # not used\n\n        # Service sma to get correct first features values:\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=self.p.time_dim\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n        # self.log.warning(\'self.p.dir: {}\'.format(dir(self.params)))\n\n        # Episode-wide metadata:\n        self.metadata = {\n            \'type\': np.asarray(self.p.metadata[\'type\']),\n            \'trial_num\': np.asarray(self.p.metadata[\'parent_sample_num\']),\n            \'trial_type\': np.asarray(self.p.metadata[\'parent_sample_type\']),\n            \'sample_num\': np.asarray(self.p.metadata[\'sample_num\']),\n            \'first_row\': np.asarray(self.p.metadata[\'first_row\']),\n            \'timestamp\': np.asarray(self.time_stamp, dtype=np.float64)\n        }\n        self.state = {\n            \'raw\': None,\n            \'metadata\': None\n        }\n\n        # If it is train or test episode?\n        # default logic: true iff. it is test episode from target domain:\n        self.is_test = self.metadata[\'type\'] and self.metadata[\'trial_type\']\n\n        # This flag shows to the outer world if this episode can broadcast world-state information, e.g. move global\n        # time forward (see: btgym.server._BTgymAnalyzer.next() method);\n        self.can_broadcast = self.is_test\n\n        self.log.debug(\'strategy.metadata: {}\'.format(self.metadata))\n        self.log.debug(\'is_test: {}\'.format(self.is_test))\n\n        # Broker data lines of interest (used for estimation inner state of agent:\n        self.broker_datalines = [\n            \'cash\',\n            \'value\',\n            \'exposure\',\n            \'drawdown\',\n            \'pos_duration\',\n            \'realized_pnl\',\n            \'unrealized_pnl\',\n            \'min_unrealized_pnl\',\n            \'max_unrealized_pnl\',\n            \'total_unrealized_pnl\',\n        ]\n        # Define flat collection dictionary looking up for methods for estimating broker statistics,\n        # one method for one mode, should be named .get_broker_[mode_name]():\n        self.collection_get_broker_stat_methods = {}\n        for line in self.broker_datalines:\n            try:\n                self.collection_get_broker_stat_methods[line] = getattr(self, \'get_broker_{}\'.format(line))\n\n            except AttributeError:\n                raise NotImplementedError(\'Callable get_broker_{}.() not found\'.format(line))\n\n        # Broker and account related sliding statistics accumulators, globally normalized last `avg_perod` values,\n        # so it\'s a bit more computationally efficient than use of bt.Observers:\n        self.broker_stat = {key: deque(maxlen=self.avg_period) for key in self.broker_datalines}\n\n        # Add custom data Lines if any (convenience wrapper):\n        self.set_datalines()\n        self.log.debug(\'Kwargs:\\n{}\\n\'.format(str(kwargs)))\n\n        # Define flat collection dictionary looking for methods for estimating observation state,\n        # one method for one mode, should be named .get_[mode_name]_state():\n        self.collection_get_state_methods = {}\n        for key in self.p.state_shape.keys():\n            try:\n                self.collection_get_state_methods[key] = getattr(self, \'get_{}_state\'.format(key))\n\n            except AttributeError:\n                raise NotImplementedError(\'Callable get_{}_state.() not found\'.format(key))\n\n        for data in self.datas:\n            self.log.debug(\'data_name: {}\'.format(data._name))\n\n        self.log.debug(\'stake size: {}\'.format(self.p.order_size))\n\n        # Define how this strategy should handle actions: either as discrete or continuous:\n        if self.p.portfolio_actions is None or set(self.p.portfolio_actions) == {}:\n            # No discrete actions provided, assume continuous:\n            try:\n                assert self.p.skip_frame > 1\n\n            except AssertionError:\n                msg = \'For continuous actions it is essential to set `skip_frame` parameter > 1, got: {}\'.format(\n                    self.p.skip_frame\n                )\n                self.log.error(msg)\n                raise ValueError(msg)\n            # Disable broker checking margin,\n            # see: https://community.backtrader.com/topic/152/multi-asset-ranking-and-rebalancing/2?page=1\n            self.env.broker.set_checksubmit(False)\n            self.next_process_fn = self._next_target_percent\n            # Repeat action 2 times:\n            self.num_action_repeats = 2\n\n        else:\n            # Use discrete handling method otherwise:\n            self.env.broker.set_checksubmit(True)\n            self.next_process_fn = self._next_discrete\n            # self.log.warning(\'DISCRETE\')\n            # Do not repeat action for discrete:\n            self.num_action_repeats = 0\n\n    def prenext(self):\n        self.update_broker_stat()\n\n    def nextstart(self):\n        self.inner_embedding = self.data.close.buflen()\n        self.log.debug(\'Inner time embedding: {}\'.format(self.inner_embedding))\n\n    def next(self):\n        """"""\n        Default implementation for built-in backtrader method.\n        Defines one step environment routine;\n        Handles order execution logic according to action received.\n        Note that orders can only be submitted for data_lines in action_space (assets).\n        `self.action` attr. is updated by btgym.server._BTgymAnalyzer, and `None` actions\n        are emitted while doing `skip_frame` loop.\n        """"""\n        self.update_broker_stat()\n\n        if \'_skip_this\' in self.action.keys():\n            # print(\'a_skip, b_message: \', self.broker_message)\n            if self.action_repeated < self.num_action_repeats:\n                self.next_process_fn(self.action_to_repeat)\n                self.action_repeated += 1\n\n        else:\n            self.next_process_fn(self.action)\n            self.action_repeated = 0\n            self.action_to_repeat = self.action\n            # print(\'a_process, b_message: \', self.broker_message)\n\n    def notify_trade(self, trade):\n        if trade.isclosed:\n            # Set trade flags: True if trade have been closed just now and within last frame-skip period,\n            # and store trade result:\n            self.trade_just_closed = True\n            # Note: `trade_just_closed` flag has to be reset manually after evaluating.\n            self.trade_result += trade.pnlcomm\n\n            # Store realized prtfolio value:\n            self.realized_broker_value = self.broker.get_value()\n            # self.log.warning(\'notify_trade: trade_pnl: {}, cum_trade_result: {}, realized_value: {}\'.format(\n            #     trade.pnlcomm, self.trade_result, self.realized_broker_value)\n            # )\n\n    def update_broker_stat(self):\n        """"""\n        Updates all sliding broker statistics deques with latest-step values such as:\n            - normalized broker value\n            - normalized broker cash\n            - normalized exposure (position size)\n            - exp. scaled episode duration in steps, normalized wrt. max possible episode steps\n            - normalized realized profit/loss for last closed trade (is zero if no pos. closures within last env. step)\n            - normalized profit/loss for current opened trade (unrealized p/l);\n        """"""\n        # Current account value:\n        current_value = self.env.broker.get_value()\n\n        # Individual positions for each instrument traded:\n        positions = [self.env.broker.getposition(data) for data in self.datas]\n        exposure = sum([abs(pos.size) for pos in positions])\n\n        for key, method in self.collection_get_broker_stat_methods.items():\n            self.broker_stat[key].append(\n                method(\n                    current_value=current_value,\n                    positions=positions,\n                    exposure=exposure,\n                )\n            )\n\n        # Reset one-time flags:\n        self.trade_just_closed = False\n        self.trade_result = 0\n\n    def get_broker_value(self, current_value, **kwargs):\n        """"""\n\n        Args:\n            current_value:  current portfolio value\n\n        Returns:\n            normalized broker value.\n        """"""\n        return norm_value(\n            current_value,\n            self.env.broker.startingcash,\n            self.p.drawdown_call,\n            self.p.target_call,\n        )\n\n    def get_broker_cash(self, **kwargs):\n        """"""\n\n        Returns:\n            normalized broker cash\n        """"""\n        return norm_value(\n            self.env.broker.get_cash(),\n            self.env.broker.startingcash,\n            99.0,\n            self.p.target_call,\n        )\n\n    def get_broker_exposure(self, exposure, **kwargs):\n        """"""\n\n        Returns:\n            normalized exposure (position size)\n        """"""\n        return exposure / (self.env.broker.startingcash * self.env.broker.get_leverage() + 1e-2)\n\n    def get_broker_realized_pnl(self, current_value, **kwargs):\n        """"""\n\n        Args:\n            current_value: current portfolio value\n\n        Returns:\n            normalized realized profit/loss for last closed trade (is zero if no pos. closures within last env. step)\n        """"""\n\n        if self.trade_just_closed:\n            pnl = decayed_result(\n                self.trade_result,\n                current_value,\n                self.env.broker.startingcash,\n                self.p.drawdown_call,\n                self.p.target_call,\n                gamma=1\n            )\n            # self.log.warning(\'get_broker_realized_pnl: got result: {} --> pnl: {}\'.format(self.trade_result, pnl))\n            # Reset flag:\n            # self.trade_just_closed = False\n            # print(\'broker_realized_pnl: step {}, just closed.\'.format(self.iteration))\n\n        else:\n            pnl = 0.0\n        return pnl\n\n    def get_broker_unrealized_pnl(self, current_value, **kwargs):\n        """"""\n\n        Args:\n            current_value: current portfolio value\n\n        Returns:\n            normalized profit/loss for current opened trade\n        """"""\n        return (current_value - self.realized_broker_value) * self.broker_value_normalizer\n\n    def get_broker_total_unrealized_pnl(self, current_value, **kwargs):\n        """"""\n\n        Args:\n            current_value: current portfolio value\n\n        Returns:\n            normalized profit/loss wrt. initial portfolio value\n        """"""\n        return (current_value - self.env.broker.startingcash) * self.broker_value_normalizer\n\n    def get_broker_episode_step(self, **kwargs):\n        """"""\n\n        Returns:\n            exp. scaled episode duration in steps, normalized wrt. max possible episode steps\n        """"""\n        return exp_scale(\n            self.iteration / (self.data.numrecords - self.inner_embedding),\n            gamma=3\n        )\n\n    def get_broker_drawdown(self, **kwargs):\n        """"""\n\n        Returns:\n            current drawdown value\n        """"""\n        try:\n            dd = self.stats.drawdown.drawdown[-1]  # / self.p.drawdown_call\n        except IndexError:\n            dd = 0.0\n        return dd\n\n    def get_broker_pos_duration(self, exposure, **kwargs):\n\n        if exposure == 0:\n            self.current_pos_duration = 0\n            # print(\'ZERO_POSITION\\n\')\n\n        else:\n            self.current_pos_duration += 1\n\n        return self.current_pos_duration\n\n    def get_broker_max_unrealized_pnl(self, current_value, exposure, **kwargs):\n        if exposure == 0:\n            self.current_pos_max_value = current_value\n\n        else:\n            if self.current_pos_max_value < current_value:\n                self.current_pos_max_value = current_value\n        return (self.current_pos_max_value - self.realized_broker_value) * self.broker_value_normalizer\n\n    def get_broker_min_unrealized_pnl(self, current_value, exposure, **kwargs):\n        if exposure == 0:\n            self.current_pos_min_value = current_value\n\n        else:\n            if self.current_pos_min_value > current_value:\n                self.current_pos_min_value = current_value\n        return (self.current_pos_min_value - self.realized_broker_value) * self.broker_value_normalizer\n\n    def set_datalines(self):\n        """"""\n        Default datalines are: Open, Low, High, Close, Volume.\n        Any other custom data lines, indicators, etc. should be explicitly defined by overriding this method.\n        Invoked once by Strategy.__init__().\n        """"""\n        pass\n\n    def get_raw_state(self):\n        """"""\n        Default state observation composer.\n\n        Returns:\n             and updates time-embedded environment state observation as [n,4] numpy matrix, where:\n                4 - number of signal features  == state_shape[1],\n                n - time-embedding length  == state_shape[0] == <set by user>.\n\n        Note:\n            `self.raw_state` is used to render environment `human` mode and should not be modified.\n\n        """"""\n        self.raw_state = np.row_stack(\n            (\n                np.frombuffer(self.data.open.get(size=self.time_dim)),\n                np.frombuffer(self.data.high.get(size=self.time_dim)),\n                np.frombuffer(self.data.low.get(size=self.time_dim)),\n                np.frombuffer(self.data.close.get(size=self.time_dim)),\n            )\n        ).T\n\n        return self.raw_state\n\n    def get_internal_state(self):\n        """"""\n        Composes internal state tensor by calling all statistics from broker_stat dictionary.\n        Generally, this method should not be modified, implement corresponding get_broker_[mode]() methods.\n\n        """"""\n        x_broker = np.concatenate(\n            [np.asarray(stat)[..., None] for stat in self.broker_stat.values()],\n            axis=-1\n        )\n        return x_broker[:, None, :]\n\n    def get_metadata_state(self):\n        self.metadata[\'timestamp\'] = np.asarray(self._get_timestamp())\n\n        return self.metadata\n\n    def _get_time(self):\n        """"""\n        Retrieves current time point of the episode data.\n\n        Returns:\n            datetime object\n        """"""\n        return self.data.datetime.datetime()\n\n    def _get_timestamp(self):\n        """"""\n        Sets attr. and returns current data timestamp.\n\n        Returns:\n            POSIX timestamp\n        """"""\n        self.time_stamp = self._get_time().timestamp()\n\n        return self.time_stamp\n\n    def _get_broadcast_info(self):\n        """"""\n        Transmits broadcasting message.\n\n        Returns:\n            dictionary  or None\n        """"""\n        try:\n            return self.get_broadcast_message()\n\n        except AttributeError:\n            return None\n\n    def get_broadcast_message(self):\n        """"""\n        Override this.\n\n        Returns:\n            dictionary or None\n        """"""\n        return None\n\n    def get_state(self):\n        """"""\n        Collects estimated values for every mode of observation space by calling methods from\n        `collection_get_state_methods` dictionary.\n        As a rule, this method should not be modified, override or implement corresponding get_[mode]_state() methods,\n        defining necessary calculations and return arbitrary shaped tensors for every space mode.\n\n        Note:\n            - \'data\' referes to bt.startegy datafeeds and should be treated as such.\n                Datafeed Lines that are not default to BTgymStrategy should be explicitly defined by\n                 __init__() or define_datalines().\n        """"""\n        # Update inner state statistic and compose state: <- moved to .next()\n        # self.update_broker_stat()\n        self.state = {key: method() for key, method in self.collection_get_state_methods.items()}\n        return self.state\n\n    def get_reward(self):\n        """"""\n        Shapes reward function as normalized single trade realized profit/loss,\n        augmented with potential-based reward shaping functions in form of:\n        F(s, a, s`) = gamma * FI(s`) - FI(s);\n        Potential FI_1 is current normalized unrealized profit/loss.\n\n        Paper:\n            ""Policy invariance under reward transformations:\n             Theory and application to reward shaping"" by A. Ng et al., 1999;\n             http://www.robotics.stanford.edu/~ang/papers/shaping-icml99.pdf\n        """"""\n\n        # All sliding statistics for this step are already updated by get_state().\n\n        # Potential-based shaping function 1:\n        # based on potential of averaged profit/loss for current opened trade (unrealized p/l):\n        unrealised_pnl = np.asarray(self.broker_stat[\'unrealized_pnl\'])\n        current_pos_duration = self.broker_stat[\'pos_duration\'][-1]\n\n        # We want to estimate potential `fi = gamma*fi_prime - fi` of current opened position,\n        # thus need to consider different cases given skip_fame parameter:\n        if current_pos_duration == 0:\n            # Set potential term to zero if there is no opened positions:\n            f1 = 0\n            fi_1_prime = 0\n        else:\n            if current_pos_duration < self.p.skip_frame:\n                fi_1 = 0\n                fi_1_prime = np.average(unrealised_pnl[-current_pos_duration:])\n\n            elif current_pos_duration < 2 * self.p.skip_frame:\n                fi_1 = np.average(\n                    unrealised_pnl[-(self.p.skip_frame + current_pos_duration):-self.p.skip_frame]\n                )\n                fi_1_prime = np.average(unrealised_pnl[-self.p.skip_frame:])\n\n            else:\n                fi_1 = np.average(\n                    unrealised_pnl[-2 * self.p.skip_frame:-self.p.skip_frame]\n                )\n                fi_1_prime = np.average(unrealised_pnl[-self.p.skip_frame:])\n\n            # Potential term:\n            f1 = self.p.gamma * fi_1_prime - fi_1\n\n        # Main reward function: normalized realized profit/loss:\n        realized_pnl = np.asarray(self.broker_stat[\'realized_pnl\'])[-self.p.skip_frame:].sum()\n\n        # Weights are subject to tune:\n        self.reward = (10.0 * f1 + 10.0 * realized_pnl) * self.p.reward_scale\n        self.reward = np.clip(self.reward, -self.p.reward_scale, self.p.reward_scale)\n\n        return self.reward\n\n    def get_info(self):\n        """"""\n        Composes information part of environment response,\n        can be any object. Override to own taste.\n\n        Note:\n            Due to \'skip_frame\' feature, INFO part of environment response transmitted by server can be  a list\n            containing either all skipped frame\'s info objects, i.e. [info[-9], info[-8], ..., info[0]] or\n            just latest one, [info[0]]. This behaviour is set inside btgym.server._BTgymAnalyzer().next() method.\n        """"""\n        return dict(\n            step=self.iteration,\n            time=self.data.datetime.datetime(),\n            action=self.action,\n            broker_message=self.broker_message,\n            broker_cash=self.stats.broker.cash[0],\n            broker_value=self.stats.broker.value[0],\n            drawdown=self.stats.drawdown.drawdown[0],\n            max_drawdown=self.stats.drawdown.maxdrawdown[0],\n        )\n\n    def get_done(self):\n        """"""\n        Episode termination estimator,\n        defines any trading logic conditions episode stop is called upon, e.g. <OMG! Stop it, we became too rich!>.\n        It is just a structural a convention method. Default method is empty.\n\n        Expected to return:\n            tuple (<is_done, type=bool>, <message, type=str>).\n        """"""\n        return False, \'-\'\n\n    def _get_done(self):\n        """"""\n        Default episode termination method,\n        checks base conditions episode stop is called upon:\n            1. Reached maximum episode duration. Need to check it explicitly, because <self.is_done> flag\n               is sent as part of environment response.\n            2. Got \'_done\' signal from outside. E.g. via env.reset() method invoked by outer RL algorithm.\n            3. Hit drawdown threshold.\n            4. Hit target profit threshold.\n\n        This method shouldn\'t be overridden or called explicitly.\n\n        Runtime execution logic is:\n            terminate episode if:\n                get_done() returned (True, \'something\')\n                OR\n                ANY _get_done() default condition is met.\n        """"""\n        if not self.is_done_enabled:\n            # Episode is on its way,\n            # apply base episode termination rules:\n            is_done_rules = [\n                # Do we approaching the end of the episode?:\n                (self.iteration >= \\\n                 self.data.numrecords - self.inner_embedding - self.p.skip_frame - self.steps_till_is_done,\n                 \'END OF DATA\'),\n                # Any money left?:\n                (self.stats.drawdown.maxdrawdown[0] >= self.p.drawdown_call, \'DRAWDOWN CALL\'),\n                # Party time?\n                (self.env.broker.get_value() > self.target_value, \'TARGET REACHED\'),\n            ]\n            # Append custom get_done() results, if any:\n            is_done_rules += [self.get_done()]\n\n            # Sweep through rules:\n            for (condition, message) in is_done_rules:\n                if condition:\n                    # Start episode termination countdown for clean exit:\n                    # to forcefully execute final `close` order and compute proper reward\n                    # we need to make `steps_till_is_done` number of steps until `is_done` flag can be safely risen:\n                    self.is_done_enabled = True\n                    self.broker_message += message\n                    self.final_message = message\n                    self.order = self.close()\n                    self.log.debug(\n                        \'Episode countdown started at: {}, {}, r:{}\'.format(self.iteration, message, self.reward)\n                    )\n\n        else:\n            # Now in episode termination phase,\n            # just keep hitting `Close` button:\n            self.steps_till_is_done -= 1\n            self.broker_message = \'CLOSE, {}\'.format(self.final_message)\n            self.order = self.close()\n            self.log.debug(\n                \'Episode countdown contd. at: {}, {}, r:{}\'.format(self.iteration, self.broker_message, self.reward)\n            )\n\n        if self.steps_till_is_done <= 0:\n            # Now we\'ve done, terminate:\n            self.is_done = True\n\n        return self.is_done\n\n    def notify_order(self, order):\n        """"""\n        Shamelessly taken from backtrader tutorial.\n        TODO: better multi data support\n        """"""\n        if order.status in [order.Submitted, order.Accepted]:\n            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n            return\n        # Check if an order has been completed\n        # Attention: broker could reject order if not enough cash\n        if order.status in [order.Completed]:\n            if order.isbuy():\n                self.broker_message = \'BUY executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n                self.buyprice = order.executed.price\n                self.buycomm = order.executed.comm\n\n            else:  # Sell\n                self.broker_message = \'SELL executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n            self.bar_executed = len(self)\n\n        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n            self.broker_message = \'ORDER FAILED with status: \' + str(order.getstatusname())\n            # Rise order_failed flag until get_reward() will [hopefully] use and reset it:\n            self.order_failed += 1\n        # self.log.warning(\'BM: {}\'.format(self.broker_message))\n        self.order = None\n\n    def _next_discrete(self, action):\n        """"""\n        Default implementation for discrete actions.\n        Note that orders can be submitted only for data_lines in action_space (assets).\n\n        Args:\n            action:     dict, string encoding of btgym.spaces.ActionDictSpace\n\n        """"""\n        for key, single_action in action.items():\n            # Simple action-to-order logic:\n            if single_action == \'hold\' or self.is_done_enabled:\n                pass\n            elif single_action == \'buy\':\n                self.order = self.buy(data=key, size=self.p.order_size[key])\n                self.broker_message = \'new {}_BUY created; \'.format(key) + self.broker_message\n            elif single_action == \'sell\':\n                self.order = self.sell(data=key, size=self.p.order_size[key])\n                self.broker_message = \'new {}_SELL created; \'.format(key) + self.broker_message\n            elif single_action == \'close\':\n                self.order = self.close(data=key)\n                self.broker_message = \'new {}_CLOSE created; \'.format(key) + self.broker_message\n\n        # Somewhere after this point, server-side _BTgymAnalyzer() is exchanging information with environment wrapper,\n        # obtaining <self.action> , composing and sending <state,reward,done,info> etc... never mind.\n\n    def _next_target_percent(self, action):\n        """"""\n        Uses `order_target_percent` method to rebalance assets to given ratios. Expects action for every asset to be\n        a float scalar in [0,1], with actions sum to 1 over all assets (including base one).\n        Note that action for base asset (cash) is ignored.\n        For details refer to: https://www.backtrader.com/docu/order_target/order_target.html\n        """"""\n        # TODO 1: filter similar actions to prevent excessive orders issue e.g by DKL on two consecutive ones\n        # TODO 2: actions discretesation on level of execution\n        for asset in self.p.asset_names:\n            # Reducing assets positions subj to 5% margin reserve:\n            single_action = round(float(action[asset]) * 0.9, 2)\n            self.order = self.order_target_percent(data=asset, target=single_action)\n            self.broker_message += \' new {}->{:1.0f}% created; \'.format(asset, single_action * 100)\n\n'"
btgym/research/strategy_gen_6/__init__.py,0,b''
btgym/research/strategy_gen_6/base.py,0,"b'\nimport backtrader as bt\nimport backtrader.indicators as btind\n\nfrom gym import spaces\nfrom btgym import DictSpace\n\nimport numpy as np\nfrom scipy import stats\nfrom collections import namedtuple\n\nfrom btgym.research.model_based.model.rec import Zscore\n\n\nNormalisationState = namedtuple(\'NormalisationState\', [\'mean\', \'variance\', \'low_interval\', \'up_interval\'])\n\n\nclass BaseStrategy6(bt.Strategy):\n    """"""\n    Added for gen.6:\n        traded asset volatility-based rescaling for all broker statistics and, consequently, reward fn\n        self.p.norm_alpha - tracking smoothing decay parameter added\n        self.p.target_call  - upper limit arg. is removed\n        TODO: auto sizer inference, co-integration coeff. inference\n\n    Controls Environment inner dynamics and backtesting logic. Provides gym\'my (State, Action, Reward, Done, Info) data.\n    Any State, Reward and Info computation logic can be implemented by subclassing BTgymStrategy and overriding\n    get_[mode]_state(), get_reward(), get_info(), is_done() and set_datalines() methods.\n    One can always go deeper and override __init__ () and next() methods for desired\n    server cerebro engine behaviour, including order execution logic etc.\n\n    Note:\n        - base class supports single asset iteration via default data_line named \'base_asset\', see derived classes\n          multi-asset support\n        - bt.observers.DrawDown observer will be automatically added to BTgymStrategy instance at runtime.\n        - Since it is bt.Strategy subclass, refer to https://www.backtrader.com/docu/strategy.html for more information.\n    """"""\n    # Time embedding period:\n    time_dim = 32  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = int(time_dim / 2)\n\n    # Possible agent actions;  Note: place \'hold\' first! :\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    features_parameters = ()\n    num_features = len(features_parameters)\n\n    params = dict(\n        # Observation state shape is dictionary of Gym spaces,\n        # at least should contain `raw_state` field.\n        # By convention first dimension of every Gym Box space is time embedding one;\n        # one can define any shape; should match env.observation_space.shape.\n        # observation space state min/max values,\n        # For `raw_state\' (default) - absolute min/max values from BTgymDataset will be used.\n        state_shape={\n            \'raw\': spaces.Box(\n                shape=(time_dim, 4),\n                low=0,  # will get overridden.\n                high=0,\n                dtype=np.float32,\n            ),\n            \'internal\': spaces.Box(low=-100, high=100, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'stat\': spaces.Box(low=-100, high=100, shape=(2, 1), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        slippage=None,\n        leverage=1.0,\n        gamma=0.99,             # fi_gamma, should match MDP gamma decay\n        reward_scale=1.0,       # reward multiplicator\n        norm_alpha=0.001,       # renormalisation tracking decay in []0, 1]\n        drawdown_call=10,       # finish episode when hitting drawdown treshghold, in percent to initial cash.\n        dataset_stat=None,      # Summary descriptive statistics for entire dataset and\n        episode_stat=None,      # current episode. Got updated by server.\n        time_dim=time_dim,      # time embedding period\n        avg_period=avg_period,  # number of time steps reward estimation statistics are tracked over\n        features_parameters=features_parameters,\n        num_features=num_features,\n        metadata={},\n        broadcast_message={},\n        trial_stat=None,\n        trial_metadata=None,\n        portfolio_actions=portfolio_actions,\n        skip_frame=1,       # number of environment steps to skip before returning next environment response\n        order_size=None,\n        initial_action=None,\n        initial_portfolio_action=None,\n        state_int_scale=1,\n        state_ext_scale=1,\n    )\n\n    def __init__(self, **kwargs):\n        """"""\n        Keyword Args:\n            params (dict):          parameters dictionary, see Note below.\n\n            Notes:\n                Due to backtrader convention, any strategy arguments should be defined inside `params` dictionary\n                or passed as kwargs to bt.Cerebro() class via .addstrategy() method. Parameter dictionary\n                should contain at least these keys::\n\n                    state_shape:        Observation state shape is dictionary of Gym spaces, by convention\n                                        first dimension of every Gym Box space is time embedding one;\n                    cash_name:          str, name for cash asset\n                    asset_names:        iterable of str, names for assets\n                    start_cash:         float, broker starting cash\n                    commission:         float, broker commission value, .01 stands for 1%\n                    leverage:           float, broker leverage\n                    slippage:           float, broker execution slippage\n                    order_size:         dict of fixed order stakes (floats); keys should match assets names.\n                    drawdown_call:      finish episode when hitting this drawdown treshghold , in percent.\n                    portfolio_actions:  possible agent actions.\n                    skip_frame:         number of environment steps to skip before returning next response,\n                                        e.g. if set to 10 -- agent will interact with environment every 10th step;\n                                        every other step agent action is assumed to be \'hold\'.\n\n                Default values are::\n\n                    state_shape=dict(raw_state=spaces.Box(shape=(4, 4), low=0, high=0,))\n                    cash_name=\'default_cash\'\n                    asset_names=[\'default_asset\']\n                    start_cash=None\n                    commission=None\n                    slippage=None,\n                    leverage=1.0\n                    drawdown_call=10\n                    dataset_stat=None\n                    episode_stat=None\n                    portfolio_actions=(\'hold\', \'buy\', \'sell\', \'close\')\n                    skip_frame=1\n                    order_size=None\n        """"""\n        # Inherit logger from cerebro:\n        self.log = self.env._log\n\n        assert self.p.avg_period + 2 < self.p.time_dim, \'Doh!\'\n\n        self.skip_frame = self.p.skip_frame\n\n        self.iteration = 0\n        self.pre_iteration = 0\n        self.env_iteration = 0\n        self.inner_embedding = 1\n        self.is_done = False\n        self.is_done_enabled = False\n        self.steps_till_is_done = 2  # extra steps to make when episode terminal conditions are met\n        self.action = self.p.initial_portfolio_action\n        self.action_to_repeat = self.p.initial_portfolio_action\n        self.action_repeated = 0\n        self.num_action_repeats = None\n        self.reward = 0\n        self.order = None\n        self.order_failed = 0\n        self.broker_message = \'_\'\n        self.final_message = \'_\'\n        self.raw_state = None\n        self.time_stamp = 0\n\n        # Prepare broker:\n        if self.p.start_cash is not None:\n            self.env.broker.setcash(self.p.start_cash)\n\n        if self.p.commission is not None:\n            self.env.broker.setcommission(commission=self.p.commission, leverage=self.p.leverage)\n\n        if self.p.slippage is not None:\n            # Bid/ask workaround: set overkill 10% slippage + slip_out=False\n            # ensuring we always buy at current \'high\'~\'ask\' and sell at \'low\'~\'bid\':\n            self.env.broker.set_slippage_perc(self.p.slippage, slip_open=True, slip_match=True, slip_out=False)\n\n        # self.target_value = self.env.broker.startingcash * (1 + self.p.target_call / 100)\n\n        # Try to define stake, if no self.p.order_size dict has been set:\n        if self.p.order_size is None:\n            # If no order size has been set for every data_line,\n            # try to infer stake size from sizer set by bt.Cerebro.addsizer() method:\n            try:\n                assert len(list(self.env.sizers.values())) == 1\n                env_sizer_params = list(self.env.sizers.values())[0][-1]  # pull dict of outer set sizer params\n                assert \'stake\' in env_sizer_params.keys()\n\n            except (AssertionError, KeyError) as e:\n                msg = \'Order stake is not set neither via strategy.param.order_size nor via bt.Cerebro.addsizer method.\'\n                self.log.error(msg)\n                raise ValueError(msg)\n\n            self.p.order_size = {name: env_sizer_params[\'stake\'] for name in self.p.asset_names}\n\n        elif isinstance(self.p.order_size, int) or isinstance(self.p.order_size, float):\n            unimodal_stake = {name: self.p.order_size for name in self.getdatanames()}\n            self.p.order_size = unimodal_stake\n\n        # Current effective order sizes:\n        self.current_order_sizes = None\n        self.margin_reserve = 0.01\n\n        # Current stat normalisation:\n        self.normalizer = 1.0\n\n        # self.log.warning(\'asset names: {}\'.format(self.p.asset_names))\n        # self.log.warning(\'data names: {}\'.format(self.getdatanames()))\n\n        self.trade_just_closed = False\n        self.trade_result = 0\n\n        self.unrealized_pnl = None\n        self.norm_broker_value = None\n        self.realized_pnl = None\n\n        self.current_pos_duration = 0\n        self.current_pos_min_value = 0\n        self.current_pos_max_value = 0\n\n        self.realized_broker_value = self.env.broker.startingcash\n        self.episode_result = 0  # not used\n\n        # Service sma to get correct first features values:\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=self.p.time_dim\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n        # self.log.warning(\'self.p.dir: {}\'.format(dir(self.params)))\n\n        # Episode-wide metadata:\n        self.metadata = {\n            \'type\': np.asarray(self.p.metadata[\'type\']),\n            \'trial_num\': np.asarray(self.p.metadata[\'parent_sample_num\']),\n            \'trial_type\': np.asarray(self.p.metadata[\'parent_sample_type\']),\n            \'sample_num\': np.asarray(self.p.metadata[\'sample_num\']),\n            \'first_row\': np.asarray(self.p.metadata[\'first_row\']),\n            \'timestamp\': np.asarray(self.time_stamp, dtype=np.float64)\n        }\n        self.state = {\n            \'raw\': None,\n            \'metadata\': None\n        }\n\n        # If it is train or test episode?\n        # default logic: true iff. it is test episode from target domain:\n        self.is_test = self.metadata[\'type\'] and self.metadata[\'trial_type\']\n\n        # This flag shows to the outer world if this episode can broadcast world-state information, e.g. move global\n        # time forward (see: btgym.server._BTgymAnalyzer.next() method);\n        self.can_broadcast = self.is_test\n\n        self.log.debug(\'strategy.metadata: {}\'.format(self.metadata))\n        self.log.debug(\'is_test: {}\'.format(self.is_test))\n\n        # Broker data lines of interest (used for estimation inner state of agent:\n        self.broker_datalines = [\n            \'cash\',\n            \'value\',\n            \'exposure\',\n            \'drawdown\',\n            \'pos_duration\',\n            \'realized_pnl\',\n            \'unrealized_pnl\',\n            \'min_unrealized_pnl\',\n            \'max_unrealized_pnl\',\n            \'total_unrealized_pnl\',\n        ]\n        # Define flat collection dictionary looking up for methods for estimating broker statistics,\n        # one method for one mode, should be named .get_broker_[mode_name]():\n        self.collection_get_broker_stat_methods = {}\n        for line in self.broker_datalines:\n            try:\n                self.collection_get_broker_stat_methods[line] = getattr(self, \'get_broker_{}\'.format(line))\n\n            except AttributeError:\n                raise NotImplementedError(\'Callable get_broker_{}.() not found\'.format(line))\n\n        # Broker and account related sliding statistics accumulators:\n        self.broker_stat = {key: np.zeros(self.avg_period) for key in self.broker_datalines}\n\n        # This data line will be used to by default to\n        # define normalisation bounds (can be overiden via .set_datalines()):\n        self.stat_asset = self.data.open\n\n        # Add custom data Lines if any [and possibly redefine stat_asset and order_size_normalizer]:\n        self.set_datalines()\n\n        # Normalisation statistics estimator (updated via update_broker_stat.()):\n        self.norm_stat_tracker = Zscore(1, alpha=self.p.norm_alpha)\n        self.normalisation_state = NormalisationState(0, 0, .9, 1.1)\n\n        # State exp. smoothing params:\n        # self.internal_state_discount = np.cumprod(np.tile(1 - 1 / self.p.avg_period, self.p.avg_period))[::-1]\n        # self.external_state_discount = None  # not used\n\n        # Define flat collection dictionary looking for methods for estimating observation state,\n        # one method per one mode, should be named .get_[mode_name]_state():\n        self.collection_get_state_methods = {}\n        for key in self.p.state_shape.keys():\n            try:\n                self.collection_get_state_methods[key] = getattr(self, \'get_{}_state\'.format(key))\n\n            except AttributeError:\n                raise NotImplementedError(\'Callable get_{}_state.() not found\'.format(key))\n\n        for data in self.datas:\n            self.log.debug(\'data_name: {}\'.format(data._name))\n\n        self.log.debug(\'stake size: {}\'.format(self.p.order_size))\n\n        # Define how this strategy should handle actions: either as discrete or continuous:\n        if self.p.portfolio_actions is None or set(self.p.portfolio_actions) == {}:\n            # No discrete actions provided, assume continuous:\n            try:\n                assert self.p.skip_frame > 1\n\n            except AssertionError:\n                msg = \'For continuous actions it is essential to set `skip_frame` parameter > 1, got: {}\'.format(\n                    self.p.skip_frame\n                )\n                self.log.error(msg)\n                raise ValueError(msg)\n            # Disable broker checking margin,\n            # see: https://community.backtrader.com/topic/152/multi-asset-ranking-and-rebalancing/2?page=1\n            self.env.broker.set_checksubmit(False)\n            self.next_process_fn = self._next_target_percent\n            # Repeat action 2 times:\n            self.num_action_repeats = 2\n\n        else:\n            # Use discrete handling method otherwise:\n            self.env.broker.set_checksubmit(True)\n            self.next_process_fn = self._next_discrete\n            # self.log.warning(\'DISCRETE\')\n            # Do not repeat action for discrete:\n            self.num_action_repeats = 0\n\n    def prenext(self):\n        if self.pre_iteration + 2 > self.p.time_dim - self.avg_period:\n            self.update_broker_stat()\n\n        elif self.pre_iteration + 2 == self.p.time_dim - self.avg_period:\n            _ = self.norm_stat_tracker.reset(\n                np.asarray(self.stat_asset.get(size=self.data.close.buflen()))[None, :]\n            )\n\n        self.pre_iteration += 1\n\n    def nextstart(self):\n        self.inner_embedding = self.data.close.buflen()\n        # self.log.warning(\'Inner time embedding: {}\'.format(self.inner_embedding))\n        # for k, v in self.broker_stat.items():\n        #     self.log.warning(\'{}: {}\'.format(k, len(v)))\n\n    def next(self):\n        """"""\n        Default implementation for built-in backtrader method.\n        Defines one step environment routine;\n        Handles order execution logic according to action received.\n        Note that orders can only be submitted for data_lines in action_space (assets).\n        `self.action` attr. is updated by btgym.server._BTgymAnalyzer, and `None` actions\n        are emitted while doing `skip_frame` loop.\n        """"""\n        self.update_broker_stat()\n\n        if \'_skip_this\' in self.action.keys():\n            # print(\'a_skip, b_message: \', self.broker_message)\n            if self.action_repeated < self.num_action_repeats:\n                self.next_process_fn(self.action_to_repeat)\n                self.action_repeated += 1\n\n        else:\n            self.next_process_fn(self.action)\n            self.action_repeated = 0\n            self.action_to_repeat = self.action\n            # print(\'a_process, b_message: \', self.broker_message)\n\n    def notify_trade(self, trade):\n        if trade.isclosed:\n            # Set trade flags: True if trade have been closed just now and within last frame-skip period,\n            # and store trade result:\n            self.trade_just_closed = True\n            # Note: `trade_just_closed` flag has to be reset manually after evaluating.\n            self.trade_result += trade.pnlcomm\n\n            # Store realized prtfolio value:\n            self.realized_broker_value = self.broker.get_value()\n            # self.log.warning(\'notify_trade: trade_pnl: {}, cum_trade_result: {}, realized_value: {}\'.format(\n            #     trade.pnlcomm, self.trade_result, self.realized_broker_value)\n            # )\n\n    def update_broker_stat(self):\n        """"""\n        Updates all sliding broker statistics with latest-step values such as:\n            - normalized broker value\n            - normalized broker cash\n            - normalized exposure (position size)\n            - exp. scaled episode duration in steps, normalized wrt. max possible episode steps\n            - normalized realized profit/loss for last closed trade (is zero if no pos. closures within last env. step)\n            - normalized profit/loss for current opened trade (unrealized p/l);\n        """"""\n        # Update current account value:\n        current_value = self.env.broker.get_value()\n\n        # ...normalisation bounds:\n        norm_state = self.get_normalisation()\n\n        # ..current order sizes:\n\n        # order_sizes = self.get_order_sizes()\n\n\n        # ...individual positions for each instrument traded:\n        positions = [self.env.broker.getposition(data) for data in self.datas]\n\n        # ... total cash exposure:\n        exposure = sum([abs(pos.size) for pos in positions])\n\n        # ... tracking normalisation constant:\n\n        self.normalizer = 1 / np.clip(\n            (norm_state.up_interval - norm_state.low_interval),\n            1e-8,\n            None\n        )\n\n        # print(\'norm_state: \', norm_state)\n        # print(\'normalizer: \', normalizer)\n        # print(\'self.current_order_sizes: \', self.current_order_sizes)\n\n        for key, method in self.collection_get_broker_stat_methods.items():\n            update = method(\n                current_value=current_value,\n                positions=positions,\n                exposure=exposure,\n                lower_bound=norm_state.low_interval,\n                upper_bound=norm_state.up_interval,\n                normalizer=self.normalizer,\n            )\n            # Update accumulator:\n            self.broker_stat[key] = np.concatenate([self.broker_stat[key][1:], np.asarray([float(update)])])\n\n        # Reset one-time flags:\n        self.trade_just_closed = False\n        self.trade_result = 0\n\n    def get_normalisation(self):\n        """"""\n        Estimates current normalisation constants, updates `normalisation_state` attr.\n\n        Returns:\n            instance of NormalisationState tuple\n        """"""\n        # Update normalizer stat:\n        stat_data = np.asarray(self.stat_asset.get(size=1))\n        mean, var = self.norm_stat_tracker.update(stat_data[None, :])\n        var = np.clip(var, 1e-8, None)\n\n        # Use 99% N(stat_data_mean, stat_data_std) intervals as normalisation interval:\n        intervals = stats.norm.interval(.99, mean, var ** .5)\n        self.normalisation_state = NormalisationState(\n            mean=float(mean),\n            variance=float(var),\n            low_interval=intervals[0][0],\n            up_interval=intervals[1][0]\n        )\n        return self.normalisation_state\n\n    def get_order_sizes(self):\n        """"""\n        Estimates current order sizes for assets in trade, sets attribute.\n\n        Returns:\n            array-like of floats\n        """"""\n        # Default implementation for fixed-size orders:\n        self.current_order_sizes = np.fromiter(self.p.order_size.values(), dtype=np.float)\n        return self.current_order_sizes\n\n    def get_broker_value(self, current_value, normalizer, **kwargs):\n        """"""\n\n        Args:\n            current_value:  float, current portfolio value\n            lower_bound:    float, lower normalisation constant\n            upper_bound:    float, upper normalisation constant\n\n        Returns:\n            broker value normalized w.r.t. start value.\n        """"""\n        return (current_value - self.env.broker.startingcash) / self.env.broker.startingcash / self.p.leverage #* normalizer\n\n    def get_broker_cash(self, current_value, **kwargs):\n        """"""\n        Args:\n            current_value:    float, current portfolio value\n\n        Returns:\n            broker cash normalized w.r.t. current value.\n        """"""\n        return self.env.broker.get_cash() / current_value\n\n    def get_broker_exposure(self, exposure, normalizer, **kwargs):\n        """"""\n        Args:\n            exposure:   float, current total position exposure\n\n        Returns:\n            exposure (position size) normalized w.r.t. single order size.\n        """"""\n        return exposure * normalizer #/ self.current_order_sizes.mean()\n\n    def get_broker_realized_pnl(self, normalizer, **kwargs):\n        """"""\n\n        Args:\n            normalizer:     float, normalisation constant\n\n        Returns:\n            normalized realized profit/loss for last closed trade (is zero if no pos. closures within last env. step)\n        """"""\n\n        if self.trade_just_closed:\n            pnl = self.trade_result * normalizer\n\n        else:\n            pnl = 0.0\n        return pnl\n\n    def get_broker_unrealized_pnl(self, current_value, normalizer, **kwargs):\n        """"""\n\n        Args:\n            current_value:  float, current portfolio value\n            normalizer:     float, normalisation constant\n\n        Returns:\n            normalized profit/loss for current opened trade\n        """"""\n        pnl = (current_value - self.realized_broker_value) * normalizer\n\n        return pnl\n\n    def get_broker_total_unrealized_pnl(self, current_value, normalizer, **kwargs):\n        """"""\n        REDUNDANT\n        Args:\n            current_value:  float, current portfolio value\n            normalizer:     float, normalisation constant\n\n\n        Returns:\n            normalized profit/loss wrt. initial portfolio value\n        """"""\n        pnl = (current_value - self.env.broker.startingcash) * self.env.broker.startingcash\n\n        return pnl\n\n    def get_broker_drawdown(self, **kwargs):\n        """"""\n\n        Returns:\n            current drawdown value\n        """"""\n        try:\n            dd = self.stats.drawdown.drawdown[-1] / self.p.drawdown_call\n        except IndexError:\n            dd = 0.0\n        return dd\n\n    def get_broker_pos_duration(self, exposure, **kwargs):\n        """"""\n\n        Args:\n            exposure:   float, current total positions exposure\n\n        Returns:\n            int, number of ticks current position is being held\n        """"""\n        if exposure == 0:\n            self.current_pos_duration = 0\n            # print(\'ZERO_POSITION\\n\')\n\n        else:\n            self.current_pos_duration += 1\n\n        return self.current_pos_duration\n\n    def get_broker_max_unrealized_pnl(self, current_value, exposure, normalizer, **kwargs):\n        """"""\n\n        Args:\n            exposure:       float, current total positions exposure\n            current_value:  float, current portfolio value\n            normalizer:     float, normalisation constant\n\n        Returns:\n            best unrealised PnL achieved within current opened position\n\n        """"""\n        if exposure == 0:\n            self.current_pos_max_value = current_value\n\n        else:\n            if self.current_pos_max_value < current_value:\n                self.current_pos_max_value = current_value\n\n        pnl = (self.current_pos_max_value - self.realized_broker_value) * normalizer\n\n        return pnl\n\n    def get_broker_min_unrealized_pnl(self, current_value, exposure, normalizer, **kwargs):\n        """"""\n\n        Args:\n            exposure:       float, current total positions exposure\n            current_value:  float, current portfolio value\n            normalizer:     float, normalisation constant\n\n        Returns:\n            worst unrealised PnL achieved within current opened position\n        """"""\n        if exposure == 0:\n            self.current_pos_min_value = current_value\n\n        else:\n            if self.current_pos_min_value > current_value:\n                self.current_pos_min_value = current_value\n\n        pnl = (self.current_pos_min_value - self.realized_broker_value) * normalizer\n\n        return pnl\n\n    def set_datalines(self):\n        """"""\n        Default datalines are: Open, Low, High, Close, Volume.\n        Any other custom data lines, indicators, etc. should be explicitly defined by overriding this method.\n        Invoked once by Strategy.__init__().\n        """"""\n        pass\n\n    def get_raw_state(self):\n        """"""\n        Default state observation composer.\n\n        Returns:\n             and updates time-embedded environment state observation as [n, 4] numpy matrix, where:\n                4 - number of signal features  == state_shape[1],\n                n - time-embedding length  == state_shape[0] == <set by user>.\n\n        Note:\n            `self.raw_state` is used to render environment `human` mode and should not be modified.\n\n        """"""\n        self.raw_state = np.row_stack(\n            (\n                np.frombuffer(self.data.open.get(size=self.time_dim)),\n                np.frombuffer(self.data.high.get(size=self.time_dim)),\n                np.frombuffer(self.data.low.get(size=self.time_dim)),\n                np.frombuffer(self.data.close.get(size=self.time_dim)),\n            )\n        ).T\n\n        return self.raw_state\n\n    def get_stat_state(self):\n        return np.asarray(self.norm_stat_tracker.get_state())\n\n    def get_internal_state(self):\n        stat_lines = (\'value\', \'unrealized_pnl\', \'realized_pnl\', \'cash\', \'exposure\')\n        # Use smoothed values:\n        x_broker = np.stack(\n            [np.asarray(self.broker_stat[name]) for name in stat_lines],\n            axis=-1\n        )\n        # x_broker = np.gradient(x_broker, axis=-1)\n        return np.clip(x_broker[:, None, :], -100, 100)\n\n    def get_metadata_state(self):\n        self.metadata[\'timestamp\'] = np.asarray(self._get_timestamp())\n\n        return self.metadata\n\n    def _get_time(self):\n        """"""\n        Retrieves current time point of the episode data.\n\n        Returns:\n            datetime object\n        """"""\n        return self.data.datetime.datetime()\n\n    def _get_timestamp(self):\n        """"""\n        Sets attr. and returns current data timestamp.\n\n        Returns:\n            POSIX timestamp\n        """"""\n        self.time_stamp = self._get_time().timestamp()\n\n        return self.time_stamp\n\n    def _get_broadcast_info(self):\n        """"""\n        Transmits broadcasting message.\n\n        Returns:\n            dictionary  or None\n        """"""\n        try:\n            return self.get_broadcast_message()\n\n        except AttributeError:\n            return None\n\n    def get_broadcast_message(self):\n        """"""\n        Override this.\n\n        Returns:\n            dictionary or None\n        """"""\n        return None\n\n    def get_state(self):\n        """"""\n        Collects estimated values for every mode of observation space by calling methods from\n        `collection_get_state_methods` dictionary.\n        As a rule, this method should not be modified, override or implement corresponding get_[mode]_state() methods,\n        defining necessary calculations and return properly shaped tensors for every space mode.\n\n        Note:\n            - \'data\' referes to bt.startegy datafeeds and should be treated as such.\n                Datafeed Lines that are not default to BTgymStrategy should be explicitly defined by\n                 __init__() or define_datalines().\n        """"""\n        # Update inner state statistic and compose state: <- moved to .next()\n        # self.update_broker_stat()\n        self.state = {key: method() for key, method in self.collection_get_state_methods.items()}\n        return self.state\n\n    def get_reward(self):\n        """"""\n        Shapes reward function as normalized single trade realized profit/loss,\n        augmented with potential-based reward shaping functions in form of:\n        F(s, a, s`) = gamma * FI(s`) - FI(s);\n        Potential FI_1 is current normalized unrealized profit/loss.\n\n        Paper:\n            ""Policy invariance under reward transformations:\n             Theory and application to reward shaping"" by A. Ng et al., 1999;\n             http://www.robotics.stanford.edu/~ang/papers/shaping-icml99.pdf\n        """"""\n\n        # All sliding statistics for this step are already updated by get_state().\n\n        # Potential-based shaping function 1:\n        # based on potential of averaged profit/loss for current opened trade (unrealized p/l):\n        unrealised_pnl = np.asarray(self.broker_stat[\'unrealized_pnl\'])\n        current_pos_duration = int(self.broker_stat[\'pos_duration\'][-1])\n\n        #self.log.warning(\'current_pos_duration: {}\'.format(current_pos_duration))\n\n        # We want to estimate potential `fi = gamma*fi_prime - fi` of current opened position,\n        # thus need to consider different cases given skip_fame parameter:\n        if current_pos_duration == 0:\n            # Set potential term to zero if there is no opened positions:\n            f1 = 0\n            fi_1_prime = 0\n        else:\n            if current_pos_duration < self.p.skip_frame:\n                fi_1 = 0\n                fi_1_prime = np.average(unrealised_pnl[-current_pos_duration:])\n\n            elif current_pos_duration < 2 * self.p.skip_frame:\n                fi_1 = np.average(\n                    unrealised_pnl[-(self.p.skip_frame + current_pos_duration):-self.p.skip_frame]\n                )\n                fi_1_prime = np.average(unrealised_pnl[-self.p.skip_frame:])\n\n            else:\n                fi_1 = np.average(\n                    unrealised_pnl[-2 * self.p.skip_frame:-self.p.skip_frame]\n                )\n                fi_1_prime = np.average(unrealised_pnl[-self.p.skip_frame:])\n\n            # Potential term:\n            f1 = self.p.gamma * fi_1_prime - fi_1\n\n        # Main reward function: normalized realized profit/loss:\n        realized_pnl = np.asarray(self.broker_stat[\'realized_pnl\'])[-self.p.skip_frame:].sum()\n\n        # Weights are subject to tune:\n        self.reward = (0.1 * f1 + 1.0 * realized_pnl) * self.p.reward_scale #/ self.normalizer\n        # self.reward = np.clip(self.reward, -self.p.reward_scale, self.p.reward_scale)\n        self.reward = np.clip(self.reward, -1e3, 1e3)\n\n        return self.reward\n\n    def get_info(self):\n        """"""\n        Composes information part of environment response,\n        can be any object. Override to own taste.\n\n        Note:\n            Due to \'skip_frame\' feature, INFO part of environment response transmitted by server can be  a list\n            containing either all skipped frame\'s info objects, i.e. [info[-9], info[-8], ..., info[0]] or\n            just latest one, [info[0]]. This behaviour is set inside btgym.server._BTgymAnalyzer().next() method.\n        """"""\n        return dict(\n            step=self.iteration,\n            time=self.data.datetime.datetime(),\n            action=self.action,\n            broker_message=self.broker_message,\n            broker_cash=self.stats.broker.cash[0],\n            broker_value=self.stats.broker.value[0],\n            drawdown=self.stats.drawdown.drawdown[0],\n            max_drawdown=self.stats.drawdown.maxdrawdown[0],\n        )\n\n    def get_done(self):\n        """"""\n        Episode termination estimator,\n        defines any trading logic conditions episode stop is called upon, e.g. <OMG! Stop it, we became too rich!>.\n        It is just a structural a convention method. Default method is empty.\n\n        Expected to return:\n            tuple (<is_done, type=bool>, <message, type=str>).\n        """"""\n        return False, \'-\'\n\n    def _get_done(self):\n        """"""\n        Default episode termination method,\n        checks base conditions episode stop is called upon:\n            1. Reached maximum episode duration. Need to check it explicitly, because <self.is_done> flag\n               is sent as part of environment response.\n            2. Got \'_done\' signal from outside. E.g. via env.reset() method invoked by outer RL algorithm.\n            3. Hit `drawdown` threshold.\n\n        This method shouldn\'t be overridden or called explicitly.\n\n        Runtime execution logic is:\n            terminate episode if:\n                get_done() returned (True, \'something\')\n                OR\n                ANY _get_done() default condition is met.\n        """"""\n        if not self.is_done_enabled:\n            # Episode is on its way,\n            # apply base episode termination rules:\n            is_done_rules = [\n                # Do we approaching the end of the episode?:\n                (self.iteration >= \\\n                 self.data.numrecords - self.inner_embedding - self.p.skip_frame - self.steps_till_is_done,\n                 \'END OF DATA\'),\n                # Any money left?:\n                (self.stats.drawdown.maxdrawdown[0] >= self.p.drawdown_call, \'DRAWDOWN CALL\'),\n            ]\n            # Append custom get_done() results, if any:\n            is_done_rules += [self.get_done()]\n\n            # self.log.debug(\n            #     \'iteration: {}, condition: {}\'.format(\n            #         self.iteration,\n            #         self.data.numrecords - self.inner_embedding - self.p.skip_frame - self.steps_till_is_done\n            #     )\n            # )\n\n            # Sweep through rules:\n            for (condition, message) in is_done_rules:\n                if condition:\n                    # Start episode termination countdown for clean exit:\n                    # to forcefully execute final `close` order and compute proper reward\n                    # we need to make `steps_till_is_done` number of steps until `is_done` flag can be safely risen:\n                    self.is_done_enabled = True\n                    self.broker_message += message\n                    self.final_message = message\n                    self.order = self.close()\n                    self.log.debug(\n                        \'Episode countdown started at: {}, {}, r:{}\'.format(self.iteration, message, self.reward)\n                    )\n\n        else:\n            # Now in episode termination phase,\n            # just keep hitting `Close` button:\n            self.steps_till_is_done -= 1\n            self.broker_message = \'CLOSE, {}\'.format(self.final_message)\n            self.order = self.close()\n            self.log.debug(\n                \'Episode countdown contd. at: {}, {}, r:{}\'.format(self.iteration, self.broker_message, self.reward)\n            )\n\n        if self.steps_till_is_done <= 0:\n            # Now we\'ve done, terminate:\n            self.is_done = True\n\n        return self.is_done\n\n    def notify_order(self, order):\n        """"""\n        Shamelessly taken from backtrader tutorial.\n        TODO: better multi data support\n        """"""\n        if order.status in [order.Submitted, order.Accepted]:\n            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n            return\n        # Check if an order has been completed\n        # Attention: broker could reject order if not enough cash\n        if order.status in [order.Completed]:\n            if order.isbuy():\n                self.broker_message = \'BUY executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n                self.buyprice = order.executed.price\n                self.buycomm = order.executed.comm\n\n            else:  # Sell\n                self.broker_message = \'SELL executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n            self.bar_executed = len(self)\n\n        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n            self.broker_message = \'ORDER FAILED with status: \' + str(order.getstatusname())\n            # Rise order_failed flag until get_reward() will [hopefully] use and reset it:\n            self.order_failed += 1\n        # self.log.warning(\'BM: {}\'.format(self.broker_message))\n        self.order = None\n\n    def _next_discrete(self, action):\n        """"""\n        Default implementation for discrete actions.\n        Note that orders can be submitted only for data_lines in action_space (assets).\n\n        Args:\n            action:     dict, string encoding of btgym.spaces.ActionDictSpace\n\n        """"""\n        for key, single_action in action.items():\n            # Simple action-to-order logic:\n            if single_action == \'hold\' or self.is_done_enabled:\n                pass\n            elif single_action == \'buy\':\n                self.order = self.buy(data=key, size=self.p.order_size[key])\n                self.broker_message = \'new {}_BUY created; \'.format(key) + self.broker_message\n            elif single_action == \'sell\':\n                self.order = self.sell(data=key, size=self.p.order_size[key])\n                self.broker_message = \'new {}_SELL created; \'.format(key) + self.broker_message\n            elif single_action == \'close\':\n                self.order = self.close(data=key)\n                self.broker_message = \'new {}_CLOSE created; \'.format(key) + self.broker_message\n\n        # Somewhere after this point, server-side _BTgymAnalyzer() is exchanging information with environment wrapper,\n        # obtaining <self.action> , composing and sending <state,reward,done,info> etc... never mind.\n\n    def _next_target_percent(self, action):\n        """"""\n        Uses `order_target_percent` method to rebalance assets to given ratios. Expects action for every asset to be\n        a float scalar in [0,1], with actions sum to 1 over all assets (including base one).\n        Note that action for base asset (cash) is ignored.\n        For details refer to: https://www.backtrader.com/docu/order_target/order_target.html\n        """"""\n        # TODO 1: filter similar actions to prevent excessive orders issue e.g by DKL on two consecutive ones\n        # TODO 2: actions discretisation on level of execution\n        for asset in self.p.asset_names:\n            # Reducing assets positions subj to 5% margin reserve:\n            single_action = round(float(action[asset]) * 0.9, 2)\n            self.order = self.order_target_percent(data=asset, target=single_action)\n            self.broker_message += \' new {}->{:1.0f}% created; \'.format(asset, single_action * 100)\n\n'"
btgym/research/strategy_gen_6/utils.py,0,"b'import numpy as np\nfrom backtrader import Indicator\n\n\nclass SpreadConstructor(Indicator):\n    """"""\n    Normalised Synthetic spread estimation and plotting.\n    Uses norm_stat_tracker_2.\n    """"""\n    lines = (\'spread\',)\n    plotinfo = dict(\n        subplot=True,\n        plotabove=True,\n        plotname=\'Norm. Synthetic Spread\',\n    )\n    plotlines = dict(\n        spread=dict(_name=\'SPREAD\', color=\'darkmagenta\'),\n    )\n\n    def next(self):\n        s = self._owner.norm_stat_tracker_2.get_state()\n        if s.mean is None or s.variance is None:\n            self.lines.spread[0] = np.random.normal(0, 0.39)\n\n        else:\n            self.lines.spread[0] = (self._owner.datas[1] - s.mean[1]) / s.variance[1] ** .5 \\\n                                   - (self._owner.datas[0] - s.mean[0]) / s.variance[0] ** .5\n\n\nclass CumSumReward(Indicator):\n    """"""\n    Cumulative reward tracking.\n    """"""\n    lines = (\'cum_reward\',)\n    plotinfo = dict(\n        subplot=True,\n        plotabove=True,\n        plotname=\'Cumulative Reward\',\n    )\n    plotlines = dict(\n        cum_reward=dict(_name=\'REWARD\', color=\'darkblue\'),\n    )\n    total_reward = 0.0\n\n    def next(self):\n        self.total_reward += self._owner.reward / self._owner.p.skip_frame\n        self.lines.cum_reward[0] = self.total_reward\n\n\nclass SpreadSizer:\n    """"""\n    For a given pair of [supposedly co-integrated] assets\n    and initial account conditions\n    estimates order sizes to form a [locally] balanced ""synthetic spread"" position,\n    (when orders are executed in opposite directions).\n    Note, that it is supposed that fractional orders are supported (ok for backtrader broker)\n    """"""\n\n    def __init__(self, init_cash, position_max_depth, leverage=1, margin_reserve=.01):\n        """"""\n\n        Args:\n            init_cash:              uint, initial [backtrader] broker cash\n            position_max_depth:     uint, maximum compound number of same direction \'synthetic orders\' to exhaust\n                                    all available account cash (including unrealized broker value) w.r.t.\n                                    current asset prices and co-integration coefficient.\n            leverage:               float, broker leverage\n            margin_reserve:         float < 1, amount to reserve in percent / 100\n        """"""\n        self.init_cash = init_cash\n        self.position_max_depth = position_max_depth\n        self.leverage = leverage\n        self.margin_reserve = margin_reserve\n        self.unit_order_size = 1\n        self.alpha = 1\n        # TODO: if account value rises/ drops -\n        # TODO: either we scale order sizes to keep position_depth constant or  <-- currently this option\n        # TODO: adjust depth to keep order sizes constant?\n\n    def get_init_sizing(self, init_price, init_variance):\n        """"""\n        Returns position sizes w.r.t initial account value.\n\n        Args:\n            init_price:         array of floats of size [2] - initial assets prices\n            init_variance:      array of floats of size [2] - initial prices variances\n\n        Returns:\n            (float, float) - position sizes for both assets\n        """"""\n        # Infer volatility-balanced order costs via current co-integrating relation,\n        # which is [approximately] estimated here as ratio of standard deviations of asset prices:\n        base_order1_cost = init_price[0] * init_variance[1] ** .5\n        base_order2_cost = init_price[1] * init_variance[0] ** .5\n\n        # Scaling coefficient to match required position depth:\n        self.alpha = self.leverage * self.init_cash \\\n            / (self.unit_order_size * self.position_max_depth * (base_order1_cost + base_order2_cost))\n        self.alpha *= 1 - self.margin_reserve\n\n        base_order1_size = self.unit_order_size * init_variance[1] ** .5\n        base_order2_size = self.unit_order_size * init_variance[0] ** .5\n\n        return self.alpha * base_order1_size, self.alpha * base_order2_size\n\n    def get_sizing(self, current_broker_value, price, variance):\n        """"""\n        Returns position sizes w.r.t current account value.\n\n        Args:\n            current_broker_value:   current account value (cash + unrealised PnL)\n            price:                  array of floats of size [2] - current assets prices\n            variance:               array of floats of size [2] - current prices variances\n\n        Returns:\n            (float, float) - position sizes for both assets\n        """"""\n        base_order1_cost = price[0] * variance[1] ** .5\n        base_order2_cost = price[1] * variance[0] ** .5\n\n        # Same as before but w.r.t. current value:\n        self.alpha = self.leverage * current_broker_value\\\n            / (self.unit_order_size * self.position_max_depth * (base_order1_cost + base_order2_cost))\n        self.alpha *= 1 - self.margin_reserve\n\n        base_order1_size = self.unit_order_size * variance[1] ** .5\n        base_order2_size = self.unit_order_size * variance[0] ** .5\n\n        return self.alpha * base_order1_size, self.alpha * base_order2_size\n\n'"
btgym/research/strategy_gen_7/__init__.py,0,b''
btgym/research/strategy_gen_7/base.py,0,"b'\nimport backtrader as bt\nimport backtrader.indicators as btind\n\nfrom gym import spaces\nfrom btgym import DictSpace\n\nimport numpy as np\nfrom scipy import stats\nfrom collections import namedtuple\n\nfrom btgym.research.model_based.model.rec import Zscore\n\n\nNormalisationState = namedtuple(\'NormalisationState\', [\'mean\', \'variance\', \'low_interval\', \'up_interval\'])\n\n\nclass BaseStrategy7(bt.Strategy):\n    """"""\n    Changes in gen.7:\n        Broker Stat Rework: (https://github.com/Kismuz/btgym/issues/117)\n        +   PnL calculation had been change from \'Per Trade\' evaluation to \'Per Filled Order\' (AKA sub trade).\n            The motivation was to make the reward denser, by introducing the realized pnl as soon as possible (next skip_frame)\n            without having to wait for the trade to end (which is indicated by position crossing zero and is undetermined in time)\n        +   Unrealized PnL calculation had been redesign. now every position being held is evaluated by price change of previous bar.\n        -   Broker stat and variables that wasn\'t used/became redundant had been removed\n\n    Added for gen.6:\n        traded asset volatility-based rescaling for all broker statistics and, consequently, reward fn\n        self.p.norm_alpha - tracking smoothing decay parameter added\n        self.p.target_call  - upper limit arg. is removed\n        TODO: auto sizer inference, co-integration coeff. inference\n\n    Controls Environment inner dynamics and backtesting logic. Provides gym\'my (State, Action, Reward, Done, Info) data.\n    Any State, Reward and Info computation logic can be implemented by subclassing BTgymStrategy and overriding\n    get_[mode]_state(), get_reward(), get_info(), is_done() and set_datalines() methods.\n    One can always go deeper and override __init__ () and next() methods for desired\n    server cerebro engine behaviour, including order execution logic etc.\n\n    Note:\n        - base class supports single asset iteration via default data_line named \'base_asset\', see derived classes\n          multi-asset support\n        - bt.observers.DrawDown observer will be automatically added to BTgymStrategy instance at runtime.\n        - Since it is bt.Strategy subclass, refer to https://www.backtrader.com/docu/strategy.html for more information.\n    """"""\n    # Time embedding period:\n    time_dim = 32  # NOTE: changed this --> change Policy  UNREAL for aux. pix control task upsampling params\n\n    # Number of timesteps reward estimation statistics are averaged over, should be:\n    # skip_frame_period <= avg_period <= time_embedding_period:\n    avg_period = int(time_dim / 2)\n\n    # Possible agent actions;  Note: place \'hold\' first! :\n    portfolio_actions = (\'hold\', \'buy\', \'sell\', \'close\')\n\n    features_parameters = ()\n    num_features = len(features_parameters)\n\n    params = dict(\n        # Observation state shape is dictionary of Gym spaces,\n        # at least should contain `raw_state` field.\n        # By convention first dimension of every Gym Box space is time embedding one;\n        # one can define any shape; should match env.observation_space.shape.\n        # observation space state min/max values,\n        # For `raw_state\' (default) - absolute min/max values from BTgymDataset will be used.\n        state_shape={\n            \'raw\': spaces.Box(\n                shape=(time_dim, 4),\n                low=0,  # will get overridden.\n                high=0,\n                dtype=np.float32,\n            ),\n            \'internal\': spaces.Box(low=-100, high=100, shape=(avg_period, 1, 5), dtype=np.float32),\n            \'stat\': spaces.Box(low=-100, high=100, shape=(2, 1), dtype=np.float32),\n            \'metadata\': DictSpace(\n                {\n                    \'type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'trial_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'trial_type\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=1,\n                        dtype=np.uint32\n                    ),\n                    \'sample_num\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'first_row\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=10 ** 10,\n                        dtype=np.uint32\n                    ),\n                    \'timestamp\': spaces.Box(\n                        shape=(),\n                        low=0,\n                        high=np.finfo(np.float64).max,\n                        dtype=np.float64\n                    ),\n                }\n            )\n        },\n        cash_name=\'default_cash\',\n        asset_names=[\'default_asset\'],\n        start_cash=None,\n        commission=None,\n        slippage=None,\n        leverage=1.0,\n        gamma=0.99,             # fi_gamma, should match MDP gamma decay\n        reward_scale=1.0,       # reward multiplicator\n        norm_alpha=0.001,       # renormalisation tracking decay in []0, 1]\n        drawdown_call=10,       # finish episode when hitting drawdown treshghold, in percent to initial cash.\n        dataset_stat=None,      # Summary descriptive statistics for entire dataset and\n        episode_stat=None,      # current episode. Got updated by server.\n        time_dim=time_dim,      # time embedding period\n        avg_period=avg_period,  # number of time steps reward estimation statistics are tracked over\n        features_parameters=features_parameters,\n        num_features=num_features,\n        metadata={},\n        broadcast_message={},\n        trial_stat=None,\n        trial_metadata=None,\n        portfolio_actions=portfolio_actions,\n        skip_frame=1,       # number of environment steps to skip before returning next environment response\n        order_size=None,\n        initial_action=None,\n        initial_portfolio_action=None,\n        state_int_scale=1,\n        state_ext_scale=1,\n    )\n\n    def __init__(self, **kwargs):\n        """"""\n        Keyword Args:\n            params (dict):          parameters dictionary, see Note below.\n\n            Notes:\n                Due to backtrader convention, any strategy arguments should be defined inside `params` dictionary\n                or passed as kwargs to bt.Cerebro() class via .addstrategy() method. Parameter dictionary\n                should contain at least these keys::\n\n                    state_shape:        Observation state shape is dictionary of Gym spaces, by convention\n                                        first dimension of every Gym Box space is time embedding one;\n                    cash_name:          str, name for cash asset\n                    asset_names:        iterable of str, names for assets\n                    start_cash:         float, broker starting cash\n                    commission:         float, broker commission value, .01 stands for 1%\n                    leverage:           float, broker leverage\n                    slippage:           float, broker execution slippage\n                    order_size:         dict of fixed order stakes (floats); keys should match assets names.\n                    drawdown_call:      finish episode when hitting this drawdown treshghold , in percent.\n                    portfolio_actions:  possible agent actions.\n                    skip_frame:         number of environment steps to skip before returning next response,\n                                        e.g. if set to 10 -- agent will interact with environment every 10th step;\n                                        every other step agent action is assumed to be \'hold\'.\n\n                Default values are::\n\n                    state_shape=dict(raw_state=spaces.Box(shape=(4, 4), low=0, high=0,))\n                    cash_name=\'default_cash\'\n                    asset_names=[\'default_asset\']\n                    start_cash=None\n                    commission=None\n                    slippage=None,\n                    leverage=1.0\n                    drawdown_call=10\n                    dataset_stat=None\n                    episode_stat=None\n                    portfolio_actions=(\'hold\', \'buy\', \'sell\', \'close\')\n                    skip_frame=1\n                    order_size=None\n        """"""\n        # Inherit logger from cerebro:\n        self.log = self.env._log\n\n        assert self.p.avg_period + 2 < self.p.time_dim, \'Doh!\'\n\n        self.skip_frame = self.p.skip_frame\n\n        self.iteration = 0\n        self.pre_iteration = 0\n        self.env_iteration = 0\n        self.inner_embedding = 1\n        self.is_done = False\n        self.is_done_enabled = False\n        self.steps_till_is_done = 2  # extra steps to make when episode terminal conditions are met\n        self.action = self.p.initial_portfolio_action\n        self.action_to_repeat = self.p.initial_portfolio_action\n        self.action_repeated = 0\n        self.num_action_repeats = None\n        self.reward = 0\n        self.order = None\n        self.order_failed = 0\n        self.broker_message = \'_\'\n        self.final_message = \'_\'\n        self.raw_state = None\n        self.time_stamp = 0\n\n        self.trade = None\n        self.sub_trade = {}\n        self.last_sub_trade_count = 0  # count how many filled orders the current trade contain\n\n        # Prepare broker:\n        if self.p.start_cash is not None:\n            self.env.broker.setcash(self.p.start_cash)\n\n        if self.p.commission is not None:\n            self.env.broker.setcommission(commission=self.p.commission, leverage=self.p.leverage)\n\n        if self.p.slippage is not None:\n            # Bid/ask workaround: set overkill 10% slippage + slip_out=False\n            # ensuring we always buy at current \'high\'~\'ask\' and sell at \'low\'~\'bid\':\n            self.env.broker.set_slippage_perc(self.p.slippage, slip_open=True, slip_match=True, slip_out=False)\n\n        # self.target_value = self.env.broker.startingcash * (1 + self.p.target_call / 100)\n\n        # Try to define stake, if no self.p.order_size dict has been set:\n        if self.p.order_size is None:\n            # If no order size has been set for every data_line,\n            # try to infer stake size from sizer set by bt.Cerebro.addsizer() method:\n            try:\n                assert len(list(self.env.sizers.values())) == 1\n                env_sizer_params = list(self.env.sizers.values())[0][-1]  # pull dict of outer set sizer params\n                assert \'stake\' in env_sizer_params.keys()\n\n            except (AssertionError, KeyError) as e:\n                msg = \'Order stake is not set neither via strategy.param.order_size nor via bt.Cerebro.addsizer method.\'\n                self.log.error(msg)\n                raise ValueError(msg)\n\n            self.p.order_size = {name: env_sizer_params[\'stake\'] for name in self.p.asset_names}\n\n        elif isinstance(self.p.order_size, int) or isinstance(self.p.order_size, float):\n            unimodal_stake = {name: self.p.order_size for name in self.getdatanames()}\n            self.p.order_size = unimodal_stake\n\n        # Current effective order sizes:\n        self.current_order_sizes = None\n        self.margin_reserve = 0.01\n\n        # Current stat normalisation:\n        self.normalizer = 1.0\n\n        # self.log.warning(\'asset names: {}\'.format(self.p.asset_names))\n        # self.log.warning(\'data names: {}\'.format(self.getdatanames()))\n\n        # Service sma to get correct first features values:\n        self.data.dim_sma = btind.SimpleMovingAverage(\n            self.datas[0],\n            period=self.p.time_dim\n        )\n        self.data.dim_sma.plotinfo.plot = False\n\n        # self.log.warning(\'self.p.dir: {}\'.format(dir(self.params)))\n\n        # Episode-wide metadata:\n        self.metadata = {\n            \'type\': np.asarray(self.p.metadata[\'type\']),\n            \'trial_num\': np.asarray(self.p.metadata[\'parent_sample_num\']),\n            \'trial_type\': np.asarray(self.p.metadata[\'parent_sample_type\']),\n            \'sample_num\': np.asarray(self.p.metadata[\'sample_num\']),\n            \'first_row\': np.asarray(self.p.metadata[\'first_row\']),\n            \'timestamp\': np.asarray(self.time_stamp, dtype=np.float64)\n        }\n        self.state = {\n            \'raw\': None,\n            \'metadata\': None\n        }\n\n        # If it is train or test episode?\n        # default logic: true iff. it is test episode from target domain:\n        self.is_test = self.metadata[\'type\'] and self.metadata[\'trial_type\']\n\n        # This flag shows to the outer world if this episode can broadcast world-state information, e.g. move global\n        # time forward (see: btgym.server._BTgymAnalyzer.next() method);\n        self.can_broadcast = self.is_test\n\n        self.log.debug(\'strategy.metadata: {}\'.format(self.metadata))\n        self.log.debug(\'is_test: {}\'.format(self.is_test))\n\n        # Broker data lines of interest (used for estimation inner state of agent:\n        self.broker_datalines = []\n        self.set_broker_datalines()\n        \n        # Define flat collection dictionary looking up for methods for estimating broker statistics,\n        # one method for one mode, should be named .get_broker_[mode_name]():\n        self.collection_get_broker_stat_methods = {}\n        for line in self.broker_datalines:\n            try:\n                self.collection_get_broker_stat_methods[line] = getattr(self, \'get_broker_{}\'.format(line))\n\n            except AttributeError:\n                raise NotImplementedError(\'Callable get_broker_{}.() not found\'.format(line))\n\n        # Broker and account related sliding statistics accumulators:\n        self.broker_stat = {key: np.zeros(self.avg_period) for key in self.broker_datalines}\n\n        # This data line will be used to by default to\n        # define normalisation bounds (can be overiden via .set_datalines()):\n        self.stat_asset = self.data.open\n\n        # Add custom data Lines if any [and possibly redefine stat_asset and order_size_normalizer]:\n        self.set_datalines()\n\n        # Normalisation statistics estimator (updated via update_broker_stat.()):\n        self.norm_stat_tracker = Zscore(1, alpha=self.p.norm_alpha)\n        self.normalisation_state = NormalisationState(0, 0, .9, 1.1)\n\n        # State exp. smoothing params:\n        # self.internal_state_discount = np.cumprod(np.tile(1 - 1 / self.p.avg_period, self.p.avg_period))[::-1]\n        # self.external_state_discount = None  # not used\n\n        # Define flat collection dictionary looking for methods for estimating observation state,\n        # one method per one mode, should be named .get_[mode_name]_state():\n        self.collection_get_state_methods = {}\n        for key in self.p.state_shape.keys():\n            try:\n                self.collection_get_state_methods[key] = getattr(self, \'get_{}_state\'.format(key))\n\n            except AttributeError:\n                raise NotImplementedError(\'Callable get_{}_state.() not found\'.format(key))\n\n        for data in self.datas:\n            self.log.debug(\'data_name: {}\'.format(data._name))\n\n        self.log.debug(\'stake size: {}\'.format(self.p.order_size))\n\n        # Define how this strategy should handle actions: either as discrete or continuous:\n        if self.p.portfolio_actions is None or set(self.p.portfolio_actions) == {}:\n            # No discrete actions provided, assume continuous:\n            try:\n                assert self.p.skip_frame > 1\n\n            except AssertionError:\n                msg = \'For continuous actions it is essential to set `skip_frame` parameter > 1, got: {}\'.format(\n                    self.p.skip_frame\n                )\n                self.log.error(msg)\n                raise ValueError(msg)\n            # Disable broker checking margin,\n            # see: https://community.backtrader.com/topic/152/multi-asset-ranking-and-rebalancing/2?page=1\n            self.env.broker.set_checksubmit(False)\n            self.next_process_fn = self._next_target_percent\n            # Repeat action 2 times:\n            self.num_action_repeats = 2\n\n        else:\n            # Use discrete handling method otherwise:\n            self.env.broker.set_checksubmit(True)\n            self.next_process_fn = self._next_discrete\n            # self.log.warning(\'DISCRETE\')\n            # Do not repeat action for discrete:\n            self.num_action_repeats = 0\n\n    def prenext(self):\n        if self.pre_iteration + 2 > self.p.time_dim - self.avg_period:\n            self.update_broker_stat()\n\n        elif self.pre_iteration + 2 == self.p.time_dim - self.avg_period:\n            _ = self.norm_stat_tracker.reset(\n                np.asarray(self.stat_asset.get(size=self.data.close.buflen()))[None, :]\n            )\n\n        self.pre_iteration += 1\n\n    def nextstart(self):\n        self.inner_embedding = self.data.close.buflen()\n        # self.log.warning(\'Inner time embedding: {}\'.format(self.inner_embedding))\n        # for k, v in self.broker_stat.items():\n        #     self.log.warning(\'{}: {}\'.format(k, len(v)))\n\n    def next(self):\n        """"""\n        Default implementation for built-in backtrader method.\n        Defines one step environment routine;\n        Handles order execution logic according to action received.\n        Note that orders can only be submitted for data_lines in action_space (assets).\n        `self.action` attr. is updated by btgym.server._BTgymAnalyzer, and `None` actions\n        are emitted while doing `skip_frame` loop.\n        """"""\n        self.update_broker_stat()\n\n        if \'_skip_this\' in self.action.keys():\n            # print(\'a_skip, b_message: \', self.broker_message)\n            if self.action_repeated < self.num_action_repeats:\n                self.next_process_fn(self.action_to_repeat)\n                self.action_repeated += 1\n\n        else:\n            self.next_process_fn(self.action)\n            self.action_repeated = 0\n            self.action_to_repeat = self.action\n            # print(\'a_process, b_message: \', self.broker_message)\n\n    def notify_trade(self, trade):\n        if trade.justopened:\n            self.trade = trade\n\n    def update_broker_stat(self):\n        """"""\n        Updates all sliding broker statistics with latest-step values such as:\n            - normalized broker value\n            - normalized broker cash\n            - normalized exposure (position size)\n            - exp. scaled episode duration in steps, normalized wrt. max possible episode steps\n            - normalized realized profit/loss for last closed trade (is zero if no pos. closures within last env. step)\n            - normalized profit/loss for current opened trade (unrealized p/l);\n        """"""\n\n        self.update_sub_trade()\n\n        # Update current account value:\n        current_value = self.env.broker.get_value()\n\n        # ...normalisation bounds:\n        norm_state = self.get_normalisation()\n\n        # ..current order sizes:\n\n        # order_sizes = self.get_order_sizes()\n\n        # ...individual positions for each instrument traded:\n        positions = [self.env.broker.getposition(data) for data in self.datas]\n\n        # ... total cash exposure:\n        exposure = sum([abs(pos.size) for pos in positions])\n\n        # ... tracking normalisation constant:\n\n        self.normalizer = 1 / np.clip(\n            (norm_state.up_interval - norm_state.low_interval),\n            1e-8,\n            None\n        )\n\n        # print(\'norm_state: \', norm_state)\n        # print(\'normalizer: \', normalizer)\n        # print(\'self.current_order_sizes: \', self.current_order_sizes)\n\n        for key, method in self.collection_get_broker_stat_methods.items():\n            update = method(\n                current_value=current_value,\n                positions=positions,\n                exposure=exposure,\n                lower_bound=norm_state.low_interval,\n                upper_bound=norm_state.up_interval,\n                normalizer=self.normalizer,\n            )\n            # Update accumulator:\n            self.broker_stat[key] = np.concatenate([self.broker_stat[key][1:], np.asarray([float(update)])])\n\n    def update_sub_trade(self):\n        """"""\n        Extract individual filled orders (AKA sub trade) from a trade and return a sub trade statistic\n        Each sub trade update will update according to current stat of the trade.\n\n        Note: should only be called once from within \'update_broker_stat\'\n        """"""\n\n        self.sub_trade[\'pnl\'] = 0.0\n        self.sub_trade[\'pnlcomm\'] = 0.0\n        self.sub_trade[\'commission\'] = 0.0\n\n        if self.trade is None:\n            return\n\n        current_sub_trade_count = len(self.trade.history)\n\n        # reset count - check if changed to a new trade\n        if current_sub_trade_count < self.last_sub_trade_count:\n            self.last_sub_trade_count = 0\n\n        # update sub trade on every new entry in trade.history\n        if current_sub_trade_count > self.last_sub_trade_count:\n\n            # pnl data from trade.history get aggregated as trade evolve but we need the unaggregated terms\n            if current_sub_trade_count > 1:\n                self.sub_trade[\'pnl\'] = self.trade.history[-1].status.pnl - self.trade.history[-2].status.pnl\n\n            self.sub_trade[\'commission\'] = self.trade.history[-1].event.commission\n\n            # commission is subtracted instantly after each trade. allowing first order to have negative pnl\n            # Note: logic is different from pnlcomm used in the trade object\n            self.sub_trade[\'pnlcomm\'] = self.sub_trade[\'pnl\'] - self.sub_trade[\'commission\']\n\n        # keep count on the number of sub trades\n        self.last_sub_trade_count = current_sub_trade_count\n\n    def get_normalisation(self):\n        """"""\n        Estimates current normalisation constants, updates `normalisation_state` attr.\n\n        Returns:\n            instance of NormalisationState tuple\n        """"""\n        # Update normalizer stat:\n        stat_data = np.asarray(self.stat_asset.get(size=1))\n        mean, var = self.norm_stat_tracker.update(stat_data[None, :])\n        var = np.clip(var, 1e-8, None)\n\n        # Use 99% N(stat_data_mean, stat_data_std) intervals as normalisation interval:\n        intervals = stats.norm.interval(.99, mean, var ** .5)\n        self.normalisation_state = NormalisationState(\n            mean=float(mean),\n            variance=float(var),\n            low_interval=intervals[0][0],\n            up_interval=intervals[1][0]\n        )\n        return self.normalisation_state\n\n    def get_order_sizes(self):\n        """"""\n        Estimates current order sizes for assets in trade, sets attribute.\n\n        Returns:\n            array-like of floats\n        """"""\n        # Default implementation for fixed-size orders:\n        self.current_order_sizes = np.fromiter(self.p.order_size.values(), dtype=np.float)\n        return self.current_order_sizes\n\n    def set_broker_datalines(self):\n        """"""\n        The following broker datalines are the baseline for any strategy.\n        Any other custom data lines, should be explicitly defined by overriding this method with a call to super().\n        Any new data line should have a corresponding method of form \'get_broker_{}\'.\n        Invoked once by Strategy.__init__().\n        Data can then be retrieved by self.broker_stat[]\n        """"""\n        self.broker_datalines = [\n            \'cash\',\n            \'value\',\n            \'exposure\',\n            \'drawdown\',\n            \'realized_pnl\',\n            \'unrealized_pnl\',\n        ]\n    \n    def get_broker_value(self, current_value, normalizer, **kwargs):\n        """"""\n        Args:\n            current_value:  float, current portfolio value\n\n        Returns:\n            broker value normalized w.r.t. start value.\n        """"""\n        return (current_value - self.env.broker.startingcash) / self.env.broker.startingcash / self.p.leverage\n\n    def get_broker_cash(self, current_value, **kwargs):\n        """"""\n        Args:\n            current_value:    float, current portfolio value\n\n        Returns:\n            broker cash normalized w.r.t. current value.\n        """"""\n        return self.env.broker.get_cash() / current_value\n\n    def get_broker_exposure(self, exposure, normalizer, **kwargs):\n        """"""\n        Args:\n            exposure:   float, current total position exposure\n\n        Returns:\n            exposure (position size) normalized w.r.t. single order size.\n        """"""\n        return exposure * normalizer\n\n    def get_broker_realized_pnl(self, normalizer, **kwargs):\n        """"""\n        Args:\n            normalizer:     float, normalisation constant\n\n        Returns:\n            normalized realized profit/loss for filled order that reduce exposure\n        """"""\n\n        return self.sub_trade[\'pnlcomm\'] * normalizer\n\n    def get_broker_unrealized_pnl(self, current_value, normalizer, **kwargs):\n        """"""\n        Args:\n            current_value:  float, current portfolio value\n            normalizer:     float, normalisation constant\n\n        Returns:\n            normalized profit/loss for current opened trade\n        """"""\n\n        unrealized = 0.0\n\n        for data in self.datas:\n            comminfo = self.env.broker.getcommissioninfo(data)\n            position = self.env.broker.getposition(data)\n\n            # calculate the differential unrealized pnl between previous and current price\n            dunrealized = comminfo.profitandloss(position.size, position.price, data.close[0])\n            unrealized += dunrealized\n\n        return unrealized * normalizer\n\n    def get_broker_drawdown(self, **kwargs):\n        """"""\n        Returns:\n            current drawdown value\n        """"""\n        try:\n            dd = self.stats.drawdown.drawdown[-1] / self.p.drawdown_call\n        except IndexError:\n            dd = 0.0\n        return dd\n\n    def set_datalines(self):\n        """"""\n        Default datalines are: Open, Low, High, Close, Volume.\n        Any other custom data lines, indicators, etc. should be explicitly defined by overriding this method.\n        Invoked once by Strategy.__init__().\n        """"""\n        pass\n\n    def get_raw_state(self):\n        """"""\n        Default state observation composer.\n\n        Returns:\n             and updates time-embedded environment state observation as [n, 4] numpy matrix, where:\n                4 - number of signal features  == state_shape[1],\n                n - time-embedding length  == state_shape[0] == <set by user>.\n\n        Note:\n            `self.raw_state` is used to render environment `human` mode and should not be modified.\n\n        """"""\n        self.raw_state = np.row_stack(\n            (\n                np.frombuffer(self.data.open.get(size=self.time_dim)),\n                np.frombuffer(self.data.high.get(size=self.time_dim)),\n                np.frombuffer(self.data.low.get(size=self.time_dim)),\n                np.frombuffer(self.data.close.get(size=self.time_dim)),\n            )\n        ).T\n\n        return self.raw_state\n\n    def get_stat_state(self):\n        return np.asarray(self.norm_stat_tracker.get_state())\n\n    def get_internal_state(self):\n        stat_lines = (\'value\', \'unrealized_pnl\', \'realized_pnl\', \'cash\', \'exposure\')\n        # Use smoothed values:\n        x_broker = np.stack(\n            [np.asarray(self.broker_stat[name]) for name in stat_lines],\n            axis=-1\n        )\n        # x_broker = np.gradient(x_broker, axis=-1)\n        return np.clip(x_broker[:, None, :], -100, 100)\n\n    def get_metadata_state(self):\n        self.metadata[\'timestamp\'] = np.asarray(self._get_timestamp())\n\n        return self.metadata\n\n    def _get_time(self):\n        """"""\n        Retrieves current time point of the episode data.\n\n        Returns:\n            datetime object\n        """"""\n        return self.data.datetime.datetime()\n\n    def _get_timestamp(self):\n        """"""\n        Sets attr. and returns current data timestamp.\n\n        Returns:\n            POSIX timestamp\n        """"""\n        self.time_stamp = self._get_time().timestamp()\n\n        return self.time_stamp\n\n    def _get_broadcast_info(self):\n        """"""\n        Transmits broadcasting message.\n\n        Returns:\n            dictionary  or None\n        """"""\n        try:\n            return self.get_broadcast_message()\n\n        except AttributeError:\n            return None\n\n    def get_broadcast_message(self):\n        """"""\n        Override this.\n\n        Returns:\n            dictionary or None\n        """"""\n        return None\n\n    def get_state(self):\n        """"""\n        Collects estimated values for every mode of observation space by calling methods from\n        `collection_get_state_methods` dictionary.\n        As a rule, this method should not be modified, override or implement corresponding get_[mode]_state() methods,\n        defining necessary calculations and return properly shaped tensors for every space mode.\n\n        Note:\n            - \'data\' referes to bt.startegy datafeeds and should be treated as such.\n                Datafeed Lines that are not default to BTgymStrategy should be explicitly defined by\n                 __init__() or define_datalines().\n        """"""\n        # Update inner state statistic and compose state: <- moved to .next()\n        # self.update_broker_stat()\n        self.state = {key: method() for key, method in self.collection_get_state_methods.items()}\n        return self.state\n\n    def get_reward(self):\n        """"""\n        Shapes reward function as normalized single trade realized profit/loss,\n        augmented with potential-based reward shaping functions in form of:\n        F(s, a, s`) = gamma * FI(s`) - FI(s);\n        Potential FI_1 is current normalized unrealized profit/loss.\n\n        Paper:\n            ""Policy invariance under reward transformations:\n             Theory and application to reward shaping"" by A. Ng et al., 1999;\n             http://www.robotics.stanford.edu/~ang/papers/shaping-icml99.pdf\n        """"""\n\n        # All sliding statistics for this step are already updated by get_state().\n\n        # Potential-based shaping function 1:\n        # based on potential of averaged profit/loss for the last skip_frame (unrealized p/l):\n        unrealised_pnl = np.asarray(self.broker_stat[\'unrealized_pnl\'])\n        current_exposure = self.broker_stat[\'exposure\'][-1]\n\n        # We want to estimate potential `fi = gamma*fi_prime - fi` of current opened position\n        if current_exposure == 0:\n            # Set potential term to zero if there is no opened positions\n            fi_1 = 0\n            fi_1_prime = 0\n        else:\n            fi_1 = np.average(unrealised_pnl[-2 * self.p.skip_frame:-self.p.skip_frame])\n            fi_1_prime = np.average(unrealised_pnl[-self.p.skip_frame:])\n\n        # Potential term:\n        f1 = self.p.gamma * fi_1_prime - fi_1\n\n        # Main reward function: normalized realized profit/loss:\n        realized_pnl = np.asarray(self.broker_stat[\'realized_pnl\'])[-self.p.skip_frame:].sum()\n\n        # Weights are subject to tune:\n        self.reward = (1.0 * f1 + 1.0 * realized_pnl) * self.p.reward_scale\n        self.reward = np.clip(self.reward, -1e3, 1e3)\n\n        return self.reward\n\n    def get_info(self):\n        """"""\n        Composes information part of environment response,\n        can be any object. Override to own taste.\n\n        Note:\n            Due to \'skip_frame\' feature, INFO part of environment response transmitted by server can be  a list\n            containing either all skipped frame\'s info objects, i.e. [info[-9], info[-8], ..., info[0]] or\n            just latest one, [info[0]]. This behaviour is set inside btgym.server._BTgymAnalyzer().next() method.\n        """"""\n        return dict(\n            step=self.iteration,\n            time=self.data.datetime.datetime(),\n            action=self.action,\n            broker_message=self.broker_message,\n            broker_cash=self.stats.broker.cash[0],\n            broker_value=self.stats.broker.value[0],\n            drawdown=self.stats.drawdown.drawdown[0],\n            max_drawdown=self.stats.drawdown.maxdrawdown[0],\n        )\n\n    def get_done(self):\n        """"""\n        Episode termination estimator,\n        defines any trading logic conditions episode stop is called upon, e.g. <OMG! Stop it, we became too rich!>.\n        It is just a structural a convention method. Default method is empty.\n\n        Expected to return:\n            tuple (<is_done, type=bool>, <message, type=str>).\n        """"""\n        return False, \'-\'\n\n    def _get_done(self):\n        """"""\n        Default episode termination method,\n        checks base conditions episode stop is called upon:\n            1. Reached maximum episode duration. Need to check it explicitly, because <self.is_done> flag\n               is sent as part of environment response.\n            2. Got \'_done\' signal from outside. E.g. via env.reset() method invoked by outer RL algorithm.\n            3. Hit `drawdown` threshold.\n\n        This method shouldn\'t be overridden or called explicitly.\n\n        Runtime execution logic is:\n            terminate episode if:\n                get_done() returned (True, \'something\')\n                OR\n                ANY _get_done() default condition is met.\n        """"""\n        if not self.is_done_enabled:\n            # Episode is on its way,\n            # apply base episode termination rules:\n            is_done_rules = [\n                # Do we approaching the end of the episode?:\n                (self.iteration >= \\\n                 self.data.numrecords - self.inner_embedding - self.p.skip_frame - self.steps_till_is_done,\n                 \'END OF DATA\'),\n                # Any money left?:\n                (self.stats.drawdown.maxdrawdown[0] >= self.p.drawdown_call, \'DRAWDOWN CALL\'),\n            ]\n            # Append custom get_done() results, if any:\n            is_done_rules += [self.get_done()]\n\n            # self.log.debug(\n            #     \'iteration: {}, condition: {}\'.format(\n            #         self.iteration,\n            #         self.data.numrecords - self.inner_embedding - self.p.skip_frame - self.steps_till_is_done\n            #     )\n            # )\n\n            # Sweep through rules:\n            for (condition, message) in is_done_rules:\n                if condition:\n                    # Start episode termination countdown for clean exit:\n                    # to forcefully execute final `close` order and compute proper reward\n                    # we need to make `steps_till_is_done` number of steps until `is_done` flag can be safely risen:\n                    self.is_done_enabled = True\n                    self.broker_message += message\n                    self.final_message = message\n                    self.order = self.close()\n                    self.log.debug(\n                        \'Episode countdown started at: {}, {}, r:{}\'.format(self.iteration, message, self.reward)\n                    )\n\n        else:\n            # Now in episode termination phase,\n            # just keep hitting `Close` button:\n            self.steps_till_is_done -= 1\n            self.broker_message = \'CLOSE, {}\'.format(self.final_message)\n            self.order = self.close()\n            self.log.debug(\n                \'Episode countdown contd. at: {}, {}, r:{}\'.format(self.iteration, self.broker_message, self.reward)\n            )\n\n        if self.steps_till_is_done <= 0:\n            # Now we\'ve done, terminate:\n            self.is_done = True\n\n        return self.is_done\n\n    def notify_order(self, order):\n        """"""\n        Shamelessly taken from backtrader tutorial.\n        TODO: better multi data support\n        """"""\n        if order.status in [order.Submitted, order.Accepted]:\n            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n            return\n        # Check if an order has been completed\n        # Attention: broker could reject order if not enough cash\n        if order.status in [order.Completed]:\n            if order.isbuy():\n                self.broker_message = \'BUY executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n                self.buyprice = order.executed.price\n                self.buycomm = order.executed.comm\n\n            else:  # Sell\n                self.broker_message = \'SELL executed,\\nPrice: {:.5f}, Cost: {:.4f}, Comm: {:.4f}\'. \\\n                    format(order.executed.price,\n                           order.executed.value,\n                           order.executed.comm)\n            self.bar_executed = len(self)\n\n        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n            self.broker_message = \'ORDER FAILED with status: \' + str(order.getstatusname())\n            # Rise order_failed flag until get_reward() will [hopefully] use and reset it:\n            self.order_failed += 1\n        # self.log.warning(\'BM: {}\'.format(self.broker_message))\n        self.order = None\n\n    def _next_discrete(self, action):\n        """"""\n        Default implementation for discrete actions.\n        Note that orders can be submitted only for data_lines in action_space (assets).\n\n        Args:\n            action:     dict, string encoding of btgym.spaces.ActionDictSpace\n\n        """"""\n        for key, single_action in action.items():\n            # Simple action-to-order logic:\n            if single_action == \'hold\' or self.is_done_enabled:\n                pass\n            elif single_action == \'buy\':\n                self.order = self.buy(data=key, size=self.p.order_size[key])\n                self.broker_message = \'new {}_BUY created; \'.format(key) + self.broker_message\n            elif single_action == \'sell\':\n                self.order = self.sell(data=key, size=self.p.order_size[key])\n                self.broker_message = \'new {}_SELL created; \'.format(key) + self.broker_message\n            elif single_action == \'close\':\n                self.order = self.close(data=key)\n                self.broker_message = \'new {}_CLOSE created; \'.format(key) + self.broker_message\n\n        # Somewhere after this point, server-side _BTgymAnalyzer() is exchanging information with environment wrapper,\n        # obtaining <self.action> , composing and sending <state,reward,done,info> etc... never mind.\n\n    def _next_target_percent(self, action):\n        """"""\n        Uses `order_target_percent` method to rebalance assets to given ratios. Expects action for every asset to be\n        a float scalar in [0,1], with actions sum to 1 over all assets (including base one).\n        Note that action for base asset (cash) is ignored.\n        For details refer to: https://www.backtrader.com/docu/order_target/order_target.html\n        """"""\n        # TODO 1: filter similar actions to prevent excessive orders issue e.g by DKL on two consecutive ones\n        # TODO 2: actions discretisation on level of execution\n        for asset in self.p.asset_names:\n            # Reducing assets positions subj to 5% margin reserve:\n            single_action = round(float(action[asset]) * 0.9, 2)\n            self.order = self.order_target_percent(data=asset, target=single_action)\n            self.broker_message += \' new {}->{:1.0f}% created; \'.format(asset, single_action * 100)\n'"
btgym/research/model_based/datafeed/__init__.py,0,b''
btgym/research/model_based/datafeed/base.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017, 2018 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nfrom logbook import Logger, StreamHandler, WARNING\n\nimport datetime\nimport sys, os\nimport copy\n\nimport backtrader.feeds as btfeeds\nimport numpy as np\nimport pandas as pd\n\nfrom btgym.datafeed.derivative import BTgymDataset2\nfrom btgym.datafeed.multi import BTgymMultiData\n\n\ndef base_random_generator_fn(num_points=10, **kwargs):\n    """"""\n    Base random uniform generating function. Provides synthetic data points.\n\n    Args:\n        num_points: trajectory length\n        kwargs:     any function parameters, not used here\n\n    Returns:\n        1d array of generated values; here: randoms in [0,1]\n    """"""\n    return np.random.random(num_points)\n\n\ndef base_bias_generator_fn(num_points=10, bias=1, **kwargs):\n    """"""\n    Base bias generating function. Provides constant synthetic data points.\n\n    Args:\n        num_points: trajectory length\n        bias:       data point constant value >=0\n        kwargs:     any function parameters, not used here\n\n    Returns:\n        1d array of generated values; here: randoms in [0,1]\n    """"""\n    assert bias >= 0, \'Only positive bias allowed, got: {}\'.format(bias)\n    return np.ones(num_points) * bias\n\n\ndef base_generator_parameters_fn(**kwargs):\n    """"""\n    Base parameters generating function. Provides arguments for data generating function.\n    It itself accept arguments specified via `generator_parameters_config` dictionary;\n\n    Returns:\n        dictionary of kwargs consistent with generating function used.\n    """"""\n    return dict()\n\n\ndef base_random_uniform_parameters_fn(**kwargs):\n    """"""\n    Provides samples for kwargs given.\n    If parameter is set as float - returns exactly given value;\n    if parameter is set as iterable of form [a, b] - uniformly randomly samples parameters value\n    form given interval.\n\n    Args:\n        **kwargs: any kwarg specifying float or iterable of two ordered floats\n\n    Returns:\n        dictionary of kwargs holding sampled values\n    """"""\n    samples = {}\n    for key, value in kwargs.items():\n        if type(value) in [int, float, np.float64]:\n            interval = [value, value]\n        else:\n            interval = list(value)\n\n        assert len(interval) == 2 and interval[0] <= interval[-1], \\\n            \'Expected parameter <{}> be float or ordered interval, got: {}\'.format(key, value)\n\n        samples[key] = np.random.uniform(low=interval[0], high=interval[-1])\n    return samples\n\n\ndef base_spread_generator_fn(num_points=10, alpha=1, beta=1, minimum=0, maximum=0):\n    """"""\n    Generates spread values for single synthetic tragectory. Samples drawn from parametrized beta-distribution;\n    If base generated trajectory P is given, than High/Ask value = P + 1/2 * Spread; Low/Bid value = P - 1/2* Spread\n\n    Args:\n        num_points: trajectory length\n        alpha:      beta-distribution alpha param.\n        beta:       beta-distribution beta param.\n        minimum:    spread minimum value\n        maximum:    spread maximum value\n\n    Returns:\n        1d array of generated values;\n    """"""\n    assert alpha > 0 and beta > 0, \'Beta-distribution parameters should be non-negative, got: {},{}\'.format(alpha, beta)\n    assert minimum <= maximum, \'Spread min/max values should form ordered pair, got: {}/{}\'.format(minimum, maximum)\n    return minimum + np.random.beta(a=alpha, b=beta, size=num_points) * (maximum - minimum)\n\n\nclass BaseDataGenerator:\n    """"""\n    Base synthetic data provider class.\n    """"""\n    def __init__(\n            self,\n            episode_duration=None,\n            timeframe=1,\n            generator_fn=base_random_generator_fn,\n            generator_parameters_fn=base_generator_parameters_fn,\n            generator_parameters_config=None,\n            spread_generator_fn=None,\n            spread_generator_parameters=None,\n            name=\'BaseSyntheticDataGenerator\',\n            data_names=(\'default_asset\',),\n            parsing_params=None,\n            target_period=-1,\n            global_time=None,\n            task=0,\n            log_level=WARNING,\n            _nested_class_ref=None,\n            _nested_params=None,\n            **kwargs\n    ):\n        """"""\n\n        Args:\n            episode_duration:               dict, duration of episode in days/hours/mins\n            generator_fn                    callabale, should return generated data as 1D np.array\n            generator_parameters_fn:        callable, should return dictionary of generator_fn kwargs\n            generator_parameters_config:    dict, generator_parameters_fn args\n            spread_generator_fn:            callable, should return values of spread to form {High, Low}\n            spread_generator_parameters:    dict, spread_generator_fn args\n            timeframe:                      int, data periodicity in minutes\n            name:                           str\n            data_names:                     iterable of str\n            target_period:                  int or dict, if set to -1 - disables `test` sampling\n            global_time:                    dict {y, m, d} to set custom global time (only for plotting)\n            task:                           int\n            log_level:                      logbook.Logger level\n            **kwargs:\n\n        """"""\n        # Logging:\n        self.log_level = log_level\n        self.task = task\n        self.name = name\n        self.filename = self.name + \'_sample\'\n        self.target_period = target_period\n\n        self.data_names = data_names\n        self.data_name = self.data_names[0]\n        self.sample_instance = None\n        self.metadata = {\'sample_num\': 0, \'type\': None, \'parent_sample_type\': None}\n\n        self.data = None\n        self.data_stat = None\n\n        self.sample_num = 0\n        self.is_ready = False\n\n        if _nested_class_ref is None:\n            self.nested_class_ref = BaseDataGenerator\n        else:\n            self.nested_class_ref = _nested_class_ref\n\n        if _nested_params is None:\n            self.nested_params = dict(\n                episode_duration=episode_duration,\n                timeframe=timeframe,\n                generator_fn=generator_fn,\n                generator_parameters_fn=generator_parameters_fn,\n                generator_parameters_config=generator_parameters_config,\n                name=name,\n                data_names=data_names,\n                task=task,\n                log_level=log_level,\n                _nested_class_ref=_nested_class_ref,\n                _nested_params=_nested_params,\n            )\n        else:\n            self.nested_params = _nested_params\n\n        StreamHandler(sys.stdout).push_application()\n        self.log = Logger(\'{}_{}\'.format(self.name, self.task), level=self.log_level)\n\n        # Default sample time duration:\n        if episode_duration is None:\n            self.episode_duration = dict(\n                    days=0,\n                    hours=23,\n                    minutes=55,\n                )\n        else:\n            self.episode_duration = episode_duration\n\n        # Btfeed parsing setup:\n        if parsing_params is None:\n            self.parsing_params = dict(\n                names=[\'ask\', \'bid\', \'mid\'],\n                datetime=0,\n                timeframe=1,\n                open=\'mid\',\n                high=\'ask\',\n                low=\'bid\',\n                close=\'mid\',\n                volume=-1,\n                openinterest=-1\n            )\n        else:\n            self.parsing_params = parsing_params\n\n        self.columns_map = {\n            \'open\': \'mean\',\n            \'high\': \'maximum\',\n            \'low\': \'minimum\',\n            \'close\': \'mean\',\n            \'bid\': \'minimum\',\n            \'ask\': \'maximum\',\n            \'mid\': \'mean\',\n            \'volume\': \'nothing\',\n        }\n        self.nested_params[\'parsing_params\'] = self.parsing_params\n\n        for key, value in self.parsing_params.items():\n            setattr(self, key, value)\n\n        # base data feed related:\n        self.params = {}\n        if global_time is None:\n            self.global_time = datetime.datetime(year=2018, month=1, day=1)\n        else:\n            self.global_time = datetime.datetime(**global_time)\n\n        self.global_timestamp = self.global_time.timestamp()\n\n        # Infer time indexes and sample number of records:\n        self.train_index = pd.timedelta_range(\n            start=datetime.timedelta(days=0, hours=0, minutes=0),\n            end=datetime.timedelta(**self.episode_duration),\n            freq=\'{}min\'.format(self.timeframe)\n        )\n        self.test_index = pd.timedelta_range(\n            start=self.train_index[-1] + datetime.timedelta(minutes=self.timeframe),\n            periods=len(self.train_index),\n            freq=\'{}min\'.format(self.timeframe)\n        )\n        self.train_index += self.global_time\n        self.test_index += self.global_time\n        self.episode_num_records = len(self.train_index)\n\n        self.generator_fn = generator_fn\n        self.generator_parameters_fn = generator_parameters_fn\n\n        if generator_parameters_config is not None:\n            self.generator_parameters_config = generator_parameters_config\n\n        else:\n            self.generator_parameters_config = {}\n\n        self.spread_generator_fn = spread_generator_fn\n\n        if spread_generator_parameters is not None:\n            self.spread_generator_parameters = spread_generator_parameters\n\n        else:\n            self.spread_generator_parameters = {}\n\n    def set_logger(self, level=None, task=None):\n        """"""\n        Sets logbook logger.\n\n        Args:\n            level:  logbook.level, int\n            task:   task id, int\n\n        """"""\n        if task is not None:\n            self.task = task\n\n        if level is not None:\n            self.log = Logger(\'{}_{}\'.format(self.name, self.task), level=level)\n\n    def reset(self,  **kwargs):\n        self.read_csv()\n        self.sample_num = 0\n        self.is_ready = True\n\n    def read_csv(self, **kwargs):\n        self.data = self.generate_data(self.generator_parameters_fn(**self.generator_parameters_config))\n\n    def generate_data(self, generator_params, sample_type=0):\n        """"""\n        Generates data trajectory, performs base consistency checks.\n\n        Args:\n            generator_params:       dict, data_generating_function parameters\n            sample_type:            0 - generate train data | 1 - generate test data\n\n        Returns:\n            data as pandas dataframe\n        """"""\n        assert sample_type in [0, 1],\\\n            \'Expected sample type be either 0 (train), or 1 (test) got: {}\'.format(sample_type)\n        # Generate data points:\n        data_array = self.generator_fn(num_points=self.episode_num_records, **generator_params)\n\n        assert len(data_array.shape) == 1 and data_array.shape[0] == self.episode_num_records,\\\n            \'Expected generated data to be 1D array of length {},  got data shape: {}\'.format(\n                self.episode_num_records,\n                data_array.shape\n            )\n        if self.spread_generator_fn is not None:\n            spread_array = self.spread_generator_fn(\n                num_points=self.episode_num_records,\n                **self.spread_generator_parameters\n            )\n            assert len(spread_array.shape) == 1 and spread_array.shape[0] == self.episode_num_records, \\\n                \'Expected generated spread to be 1D array of length {},  got data shape: {}\'.format(\n                    self.episode_num_records,\n                    spread_array.shape\n                )\n        else:\n            spread_array = np.zeros(self.episode_num_records)\n\n        data_dict = {\n            \'mean\': data_array,\n            \'maximum\': data_array + .5 * spread_array,\n            \'minimum\': data_array - .5 * spread_array,\n            \'nothing\': data_array * 0.0,\n        }\n\n        # negs = data_dict[\'minimum\'] < 0\n        # if negs.any():\n        #     self.log.warning(\'{} negative generated values detected\'.format(negs.shape[0]))\n\n        # Make dataframe:\n        if sample_type:\n            index = self.test_index\n        else:\n            index = self.train_index\n        # Map dictionary of data to dataframe columns:\n        df = pd.DataFrame(data={name: data_dict[self.columns_map[name]] for name in self.names}, index=index)\n        # df = df.set_index(\'hh:mm:ss\')\n        return df\n\n    def sample(self, get_new=True, sample_type=0,  **kwargs):\n        """"""\n        Samples continuous subset of data.\n\n        Args:\n            get_new (bool):                 not used;\n            sample_type (int or bool):      0 (train) or 1 (test) - get sample from train or test data subsets\n                                            respectively.\n\n        Returns:\n            Dataset instance with number of records ~ max_episode_len.\n\n        """"""\n        try:\n            assert sample_type in [0, 1]\n\n        except AssertionError:\n            msg = \'Sampling attempt: expected sample type be in {}, got: {}\'.format([0, 1], sample_type)\n            self.log.error(msg)\n            raise ValueError(msg)\n\n        if self.target_period == -1 and sample_type:\n            msg = \'Attempt to sample type {} given disabled target_period\'.format(sample_type)\n            self.log.error(msg)\n            raise ValueError(msg)\n\n        if self.metadata[\'type\'] is not None:\n            if self.metadata[\'type\'] != sample_type:\n                self.log.warning(\n                    \'Attempt to sample type {} given current sample type {}, overriden.\'.format(\n                        sample_type,\n                        self.metadata[\'type\']\n                    )\n                )\n                sample_type = self.metadata[\'type\']\n\n        # Get sample:\n        self.sample_instance = self.sample_synthetic(sample_type)\n\n        self.sample_instance.metadata[\'type\'] = sample_type\n        self.sample_instance.metadata[\'sample_num\'] = self.sample_num\n        self.sample_instance.metadata[\'parent_sample_num\'] = self.metadata[\'sample_num\']\n        self.sample_instance.metadata[\'parent_sample_type\'] = self.metadata[\'type\']\n        self.sample_num += 1\n\n        return self.sample_instance\n\n    def sample_synthetic(self, sample_type=0):\n        """"""\n        Get data_generator instance containing synthetic data.\n\n        Args:\n            sample_type (int or bool):      0 (train) or 1 (test) - get sample with train or test time periods\n                                            respectively.\n\n        Returns:\n            nested_class_ref instance\n        """"""\n        # Generate data:\n        generator_params = self.generator_parameters_fn(**self.generator_parameters_config)\n        data = self.generate_data(generator_params, sample_type=sample_type)\n\n        # Make data_class instance:\n        sample_instance = self.nested_class_ref(**self.nested_params)\n        sample_instance.filename += \'_{}\'.format(self.sample_num)\n        self.log.info(\'New sample id: <{}>.\'.format(sample_instance.filename))\n\n        # Add data and metadata:\n        sample_instance.data = data\n        sample_instance.metadata[\'generator\'] = generator_params\n        sample_instance.metadata[\'first_row\'] = 0\n        sample_instance.metadata[\'last_row\'] = self.episode_num_records\n\n        return sample_instance\n\n    def describe(self):\n        """"""\n        Returns summary dataset statistic as pandas dataframe:\n\n            - records count,\n            - data mean,\n            - data std dev,\n            - min value,\n            - 25% percentile,\n            - 50% percentile,\n            - 75% percentile,\n            - max value\n\n        for every data column.\n        """"""\n        # Pretty straightforward, using standard pandas utility.\n        # The only caveat here is that if actual data has not been loaded yet, need to load, describe and unload again,\n        # thus avoiding passing big files to BT server:\n        flush_data = False\n        try:\n            assert not self.data.empty\n            pass\n\n        except (AssertionError, AttributeError) as e:\n            self.read_csv()\n            flush_data = True\n\n        self.data_stat = self.data.describe()\n        self.log.info(\'Data summary:\\n{}\'.format(self.data_stat.to_string()))\n\n        if flush_data:\n            self.data = None\n            self.log.info(\'Flushed data.\')\n\n        return self.data_stat\n\n    def to_btfeed(self):\n        """"""\n        Performs BTgymData-->bt.feed conversion.\n\n        Returns:\n             dict of type: {data_line_name: bt.datafeed instance}.\n        """"""\n        try:\n            assert not self.data.empty\n            btfeed = btfeeds.PandasDirectData(\n                dataname=self.data,\n                timeframe=self.timeframe,\n                datetime=self.datetime,\n                open=self.open,\n                high=self.high,\n                low=self.low,\n                close=self.close,\n                volume=self.volume,\n                openinterest=self.openinterest\n            )\n            btfeed.numrecords = self.data.shape[0]\n            return {self.data_name: btfeed}\n\n        except (AssertionError, AttributeError) as e:\n            msg = \'Instance holds no data. Hint: forgot to call .read_csv()?\'\n            self.log.error(msg)\n            raise AssertionError(msg)\n\n    def set_global_timestamp(self, timestamp):\n        pass\n\n\nclass BaseCombinedDataSet:\n    """"""\n    Data provider class wrapper incorporates synthetic train and real test data streams.\n    """"""\n    def __init__(\n            self,\n            train_data_config,\n            test_data_config,\n            train_class_ref=BaseDataGenerator,\n            test_class_ref=BTgymDataset2,\n            name=\'CombinedDataSet\',\n            **kwargs\n    ):\n        """"""\n\n        Args:\n            filename:                       str, test data filename\n            parsing_params:                 dict test data parsing params\n            episode_duration_train:         dict, duration of train episode in days/hours/mins\n            episode_duration_test:          dict, duration of test episode in days/hours/mins\n            time_gap:                       dict test episode duration tolerance\n            start_00:                       bool, def=False\n            generator_fn                    callabale, should return generated data as 1D np.array\n            generator_parameters_fn:        callable, should return dictionary of generator_fn kwargs\n            generator_parameters_config:    dict, generator_parameters_fn args\n            timeframe:                      int, data periodicity in minutes\n            name:                           str\n            data_names:                     iterable of str\n            global_time:                    dict {y, m, d} to set custom global time (here for plotting only)\n            task:                           int\n            log_level:                      logbook.Logger level\n            **kwargs:                       common kwargs\n\n        """"""\n        self.name = name\n        self.log = None\n\n        try:\n            self.task = kwargs[\'task\']\n        except KeyError:\n            self.task = None\n\n        self.train_data_config = train_data_config\n        self.test_data_config = test_data_config\n        self.train_data_config.update(kwargs)\n        self.test_data_config.update(kwargs)\n        self.train_data_config[\'name\'] = self.name + \'/train\'\n        self.test_data_config[\'name\'] = self.name + \'/test\'\n\n        # Declare all test data come from target domain:\n        self.test_data_config[\'target_period\'] = -1\n        self.test_data_config[\'test_period\'] = -1\n\n        self.streams = {\n            \'train\': train_class_ref(**self.train_data_config),\n            \'test\': test_class_ref(**self.test_data_config),\n        }\n\n        self.sample_instance = None\n        self.sample_num = 0\n        self.is_ready = False\n\n        # Legacy parameters, left here for BTgym API_shell:\n        try:\n            self.parsing_params = kwargs[\'parsing_params\']\n\n        except KeyError:\n            self.parsing_params = dict(\n                sep=\',\',\n                header=0,\n                index_col=0,\n                parse_dates=True,\n                names=[\'ask\', \'bid\', \'mid\'],\n                dataname=None,\n                datetime=0,\n                nullvalue=0.0,\n                timeframe=1,\n                high=1,  # \'ask\',\n                low=2,  # \'bid\',\n                open=3,  # \'mid\',\n                close=3,  # \'mid\',\n                volume=-1,\n                openinterest=-1,\n            )\n\n        try:\n            self.sampling_params = kwargs[\'sampling_params\']\n\n        except KeyError:\n            self.sampling_params = {}\n\n        self.params = {}\n        self.params.update(self.parsing_params)\n        self.params.update(self.sampling_params)\n\n        self.set_params(self.params)\n        self.data_names = self.streams[\'test\'].data_names\n        self.global_timestamp = 0\n\n    def set_params(self, params_dict):\n        """"""\n        Batch attribute setter.\n\n        Args:\n            params_dict: dictionary of parameters to be set as instance attributes.\n        """"""\n        for key, value in params_dict.items():\n            setattr(self, key, value)\n\n    def set_logger(self, *args, **kwargs):\n        for stream in self.streams.values():\n            stream.set_logger(*args, **kwargs)\n        self.log = self.streams[\'test\'].log\n\n    def reset(self, *args, **kwargs):\n        for stream in self.streams.values():\n            stream.reset(*args, **kwargs)\n        self.task = self.streams[\'test\'].task\n        self.global_timestamp = self.streams[\'test\'].global_timestamp\n        self.sample_num = 0\n        self.is_ready = True\n\n    def read_csv(self, *args, **kwargs):\n        for stream in self.streams.values():\n            stream.read_csv(*args, **kwargs)\n\n    def describe(self,*args, **kwargs):\n        return self.streams[\'test\'].describe()\n\n    def set_global_timestamp(self, *args, **kwargs):\n        for stream in self.streams.values():\n            stream.set_global_timestamp(*args, **kwargs)\n        self.global_timestamp = self.streams[\'test\'].global_timestamp\n\n    def to_btfeed(self):\n        raise NotImplementedError\n\n    def sample(self, sample_type=0,  **kwargs):\n        """"""\n        Samples continuous subset of data.\n\n        Args:\n            sample_type (int or bool):      0 (train) or 1 (test) - get sample from train or test data subsets\n                                            respectively.\n\n        Returns:\n            Dataset instance with number of records ~ max_episode_len,\n\n        """"""\n        try:\n            assert sample_type in [0, 1]\n\n        except AssertionError:\n            self.log.exception(\n                \'Sampling attempt: expected sample type be in {}, got: {}\'.\\\n                format([0, 1], sample_type)\n            )\n            raise AssertionError\n\n        if sample_type:\n            self.sample_instance = self.streams[\'test\'].sample(sample_type=sample_type, **kwargs)\n            self.sample_instance.metadata[\'generator\'] = {}\n\n        else:\n            self.sample_instance = self.streams[\'train\'].sample(sample_type=sample_type, **kwargs)\n\n        # Common metadata:\n        self.sample_instance.metadata[\'type\'] = sample_type\n        self.sample_instance.metadata[\'sample_num\'] = self.sample_num\n        self.sample_instance.metadata[\'parent_sample_num\'] = 0\n        self.sample_instance.metadata[\'parent_sample_type\'] = None\n        self.sample_num += 1\n\n        return self.sample_instance\n\n\nclass BasePairDataGenerator(BTgymMultiData):\n    """"""\n    Generates pair of data streams driven by single 2-level generating process.\n    TODO: make data generating process single stand-along function or class method, do not use BaseDataGenerator\'s\n    """"""\n    def __init__(\n            self,\n            data_names,\n            process1_config=None,  # bias generator\n            process2_config=None,  # spread generator\n            data_class_ref=BaseDataGenerator,\n            name=\'PairDataGenerator\',\n            _top_level=True,\n            **kwargs\n    ):\n        assert len(list(data_names)) == 2, \'Expected `data_names` be pair of `str`, got: {}\'.format(data_names)\n        if process1_config is None:\n            self.process1_config = {\n                \'generator_fn\': base_bias_generator_fn,\n                \'generator_parameters_fn\': base_generator_parameters_fn,\n                \'generator_parameters_config\': None,\n            }\n        else:\n            self.process1_config = process1_config\n\n        if process2_config is None:\n            self.process2_config = {\n                \'generator_fn\': base_random_generator_fn,\n                \'generator_parameters_fn\': base_generator_parameters_fn,\n                \'generator_parameters_config\': None,\n            }\n        else:\n            self.process2_config = process2_config\n\n        data_config = {name: {\'filename\': None, \'config\': {}} for name in data_names}\n\n        # Let first asset hold p1 generating process:\n        self.a1_name = data_names[0]\n        data_config[self.a1_name][\'config\'].update(self.process1_config)\n\n        # Second asset will hold p2 generating process:\n        self.a2_name = data_names[-1]\n        data_config[self.a2_name][\'config\'].update(self.process2_config)\n\n        self.nested_kwargs = kwargs\n        self.get_new_sample = not _top_level\n\n        super(BasePairDataGenerator, self).__init__(\n            data_config=data_config,\n            data_names=data_names,\n            data_class_ref=data_class_ref,\n            name=name,\n            **kwargs\n        )\n\n    def sample(self, sample_type=0, **kwargs):\n        if self.get_new_sample:\n            # Get process1 trajectory:\n            p1_sample = self.data[self.a1_name].sample(sample_type=sample_type, **kwargs)\n\n            # Get p2 trajectory:\n            p2_sample = self.data[self.a2_name].sample(sample_type=sample_type, **kwargs)\n\n            idx_intersected = p1_sample.data.index.intersection(p2_sample.data.index)\n\n            self.log.info(\'p1/p2 shared num. records: {}\'.format(len(idx_intersected)))\n            # TODO: move this generating process to stand-along function\n\n            # Combine processes:\n            data1 = p1_sample.data + 0.5 * p2_sample.data\n            data2 = p1_sample.data - 0.5 * p2_sample.data\n            metadata = copy.deepcopy(p2_sample.metadata)\n\n        else:\n            data1 = None\n            data2 = None\n            metadata = {}\n\n        metadata.update(\n            {\'type\': sample_type, \'sample_num\': self.sample_num, \'parent_sample_type\': self.sample_num, \'parent_sample_num\': sample_type}\n        )\n        # Prepare empty instance of multi_stream data:\n        sample = BasePairDataGenerator(\n            data_names=self.data_names,\n            process1_config=self.process1_config,\n            process2_config=self.process2_config,\n            data_class_ref=self.data_class_ref,\n            # task=self.task,\n            # log_level=self.log_level,\n            name=\'sub_\' + self.name,\n            _top_level=False,\n            **self.nested_kwargs\n        )\n        # TODO: maybe add p1 metadata\n        sample.metadata = copy.deepcopy(metadata)\n\n        # Populate sample with data:\n        sample.data[self.a1_name].data = data1\n        sample.data[self.a2_name].data = data2\n\n        sample.filename = {key: stream.filename for key, stream in self.data.items()}\n        self.sample_num += 1\n        return sample\n\n\nclass BasePairCombinedDataSet(BaseCombinedDataSet):\n    """"""\n    Provides doubled streams of simulated train / real test data.\n    Suited for pairs or spread trading setup.\n    """"""\n    def __init__(\n            self,\n            assets_filenames,\n            process1_config=None,\n            process2_config=None,\n            train_episode_duration=None,\n            test_episode_duration=None,\n            train_class_ref=BasePairDataGenerator,\n            test_class_ref=BTgymMultiData,\n            name=\'PairCombinedDataSet\',\n            **kwargs\n    ):\n        assert isinstance(assets_filenames, dict),\\\n            \'Expected `assets_filenames` type `dict`, got {} \'.format(type(assets_filenames))\n\n        data_names = [name for name in assets_filenames.keys()]\n        assert len(data_names) == 2, \'Expected exactly two assets, got: {}\'.format(data_names)\n\n        train_data_config = dict(\n            data_names=data_names,\n            process1_config=process1_config,\n            process2_config=process2_config,\n            data_class_ref=BaseDataGenerator,\n            episode_duration=train_episode_duration,\n            # name=name,\n        )\n        test_data_config = dict(\n            data_class_ref=BTgymDataset2,\n            data_config={asset_name: {\'filename\': file_name} for asset_name, file_name in assets_filenames.items()},\n            episode_duration=test_episode_duration,\n            # name=name,\n        )\n\n        super(BasePairCombinedDataSet, self).__init__(\n            train_data_config=train_data_config,\n            test_data_config=test_data_config,\n            train_class_ref=train_class_ref,\n            test_class_ref=test_class_ref,\n            name=name,\n            **kwargs\n        )\n\n'"
btgym/research/model_based/datafeed/bivariate.py,2,"b'import numpy as np\nimport copy\nimport pandas as pd\nimport random\n\nfrom btgym.datafeed.derivative import BTgymDataset2\nfrom btgym.datafeed.multi import BTgymMultiData\n\nfrom btgym.research.model_based.datafeed.base import BasePairDataGenerator, BaseCombinedDataSet\nfrom btgym.research.model_based.model.bivariate import BivariatePriceModel\n\n\ndef bivariate_generator_fn(num_points, state, keep_decimals=6, **kwargs):\n    """"""\n    Wrapper around data modeling class generative method.\n\n    Args:\n        num_points:         trajectory length to draw\n        state:              model state, instance of BivariateTSModelState\n        keep_decimals:      number of decimal places to round to\n\n    Returns:\n        generated time-series of size [1, 2, size]\n    """"""\n    _, x = BivariatePriceModel.generate_bivariate_trajectory_fn(1, num_points, state, True, BivariatePriceModel.u_recon)\n    return np.around(x, decimals=keep_decimals)\n\n\ndef bivariate_random_state_fn(*args, **kwargs):\n    """"""\n    Samples random model state.\n    Args:\n        *args:          same as args for: BivariateTSModel.get_random_state\n        **kwargs:       same as args for: BivariateTSModel.get_random_state\n\n    Returns:\n        dictionary holding instance of BivariateTSModelState and auxillary fields\n    """"""\n    state = BivariatePriceModel.get_random_state(*args, **kwargs)\n    return dict(\n        state=state,\n        # for tf.summaries via strategy:\n        ou_mu=state.s.process.observation.mu,\n        ou_lambda=np.exp(state.s.process.observation.log_theta),\n        ou_sigma=np.exp(state.s.process.observation.log_sigma),\n        x0=state.s.process.observation.mu,\n    )\n\n\ndef bivariate_state_set_iterator_fn(states_set, **kwargs):\n    """"""\n    Randomly sample state from given set of states.\n\n    Args:\n        states_set:     iterable containing instances of BivariateTSModelState\n\n    Returns:\n        dictionary holding instance of BivariateTSModelState and auxillary fields\n    """"""\n    state = random.choice(tuple(states_set))\n\n    return dict(\n        state=state,\n        # for tf.summaries via strategy:\n        ou_mu=state.s.process.observation.mu,\n        ou_lambda=np.exp(state.s.process.observation.log_theta),\n        ou_sigma=np.exp(state.s.process.observation.log_sigma),\n        x0=state.s.process.observation.mu,\n    )\n\n\nclass BivariateGenerator(BasePairDataGenerator):\n    """"""\n    Generates O=H=L=C price data driven by `BivariatePriceModel`\n    """"""\n    def __init__(\n            self,\n            data_names,\n            generator_parameters_config,\n            generator_fn=bivariate_generator_fn,\n            generator_parameters_fn=bivariate_random_state_fn,\n            name=\'BivariateGenerator\',\n            **kwargs\n\n    ):\n        super(BivariateGenerator, self).__init__(\n            data_names,\n            process1_config=None,  # bias generator\n            process2_config=None,  # spread generator\n            name=name,\n            **kwargs\n        )\n        self.generator_fn = generator_fn\n        self.generator_parameters_fn = generator_parameters_fn\n        self.generator_parameters_config = generator_parameters_config\n\n        self.columns_map = {\n            \'open\': \'mean\',\n            \'high\': \'maximum\',\n            \'low\': \'minimum\',\n            \'close\': \'last\',\n            \'bid\': \'minimum\',\n            \'ask\': \'maximum\',\n            \'mid\': \'mean\',\n            \'volume\': \'nothing\',\n        }\n\n    def generate_data(self, generator_params, sample_type=0):\n        """"""\n        Generates data trajectory.\n\n        Args:\n            generator_params:       dict, data_generating_function parameters\n            sample_type:            0 - generate train data | 1 - generate test data\n\n        Returns:\n            tuple of two pandas.dataframe objects\n        """"""\n        # Get data shaped [1, 2, num_points] and map to OHLC pattern:\n        data = self.generator_fn(num_points=self.data[self.a1_name].episode_num_records, **generator_params)\n\n        # No fancy OHLC modelling yet:\n        p1_dict = {\n            \'mean\': data[0, 0, :],\n            \'maximum\': data[0, 0, :],\n            \'minimum\': data[0, 0, :],\n            \'last\': data[0, 0, :],\n            \'nothing\': np.zeros(data[0, 1, :].shape)\n        }\n        p2_dict = {\n            \'mean\': data[0, 1, :],\n            \'maximum\': data[0, 1, :],\n            \'minimum\': data[0, 1, :],\n            \'last\': data[0, 1, :],\n            \'nothing\': np.zeros(data[0, 1, :].shape)\n        }\n        # Make dataframes:\n        if sample_type:\n            index = self.data[self.a1_name].test_index\n        else:\n            index = self.data[self.a1_name].train_index\n        # Map dictionary of data to dataframe columns:\n        df1 = pd.DataFrame(data={name: p1_dict[self.columns_map[name]] for name in self.names}, index=index)\n        df2 = pd.DataFrame(data={name: p2_dict[self.columns_map[name]] for name in self.names}, index=index)\n\n        return df1, df2\n\n    def sample(self, sample_type=0, broadcast_message=None, **kwargs):\n        """"""\n        Generates pair of price trajectories and pack it as DataSet_type object.\n\n        Args:\n            sample_type:        bool, train/test\n            broadcast_message:  <reserved for future param.>\n            **kwargs:\n\n        Returns:\n            sample as SimpleBivariateGenerator instance\n        """"""\n        # self.log.debug(\'broadcast_message: <<{}>>\'.format(broadcast_message))\n\n        if self.metadata[\'type\'] is not None:\n            if self.metadata[\'type\'] != sample_type:\n                self.log.warning(\n                    \'Attempt to sample type {} given current sample type {}, overriden.\'.format(\n                        sample_type,\n                        self.metadata[\'type\']\n                    )\n                )\n                sample_type = self.metadata[\'type\']\n\n        # Prepare empty instance of multi_stream data:\n        sample = type(self)(\n            data_names=self.data_names,\n            generator_parameters_config=self.generator_parameters_config,\n            data_class_ref=self.data_class_ref,\n            name=\'sub_\' + self.name,\n            _top_level=False,\n            **self.nested_kwargs\n        )\n        # TODO: WTF?\n        sample.names = self.names\n\n        if self.get_new_sample:\n            # get parameters:\n            params = self.generator_parameters_fn(**self.generator_parameters_config)\n\n            data1, data2 = self.generate_data(params, sample_type=sample_type)\n\n            metadata = {\'generator\': params}\n\n        else:\n            data1 = None\n            data2 = None\n            metadata = {}\n\n        metadata.update(\n            {\n                \'type\': sample_type,\n                \'sample_num\': self.sample_num,\n                \'parent_sample_type\': self.metadata[\'type\'],\n                \'parent_sample_num\': self.sample_num,\n                \'first_row\': 0,\n                \'last_row\': self.data[self.a1_name].episode_num_records,\n            }\n        )\n\n        sample.metadata = copy.deepcopy(metadata)\n\n        # Populate sample with data:\n        sample.data[self.a1_name].data = data1\n        sample.data[self.a2_name].data = data2\n\n        sample.filename = {key: stream.filename for key, stream in self.data.items()}\n        self.sample_num += 1\n        return sample\n\n\nclass BivariateDataSet(BaseCombinedDataSet):\n    """"""\n    Combined data iterator provides:\n    - train data as two trajectories of OHLC prices generated by \'BivariatePriceModel\';\n    - test data as two historic timeindex-matching OHLC data lines;\n\n    """"""\n\n    def __init__(\n            self,\n            model_params,\n            assets_filenames=None,\n            assets_dataframes=None,\n            train_episode_duration=None,\n            test_episode_duration=None,\n            name=\'BivariateDataSet\',\n            _train_class_ref=BivariateGenerator,\n            _test_class_ref=BTgymMultiData,\n            **kwargs\n    ):\n        """"""\n\n        Args:\n            model_params:               dict holding generative model parameters,\n                                        same as kwargs for: BivariatePriceModel.get_random_state() method\n            assets_filenames:           dict. of two keys in form of: {\'asset_name`: \'data_file_name\'},\n                                        test data or None, ignored when `assets_dataframes` arg. is given\n            assets_dataframes:          dict. of two keys in form of {\'asset_name`: pd.dataframe},\n                                        an alternative way to provide test data as pandas.dataframes instances,\n                                        overrides `assets_filenames`\n            train_episode_duration:     dict of keys {\'days\', \'hours\', \'minutes\'} - train sample duration\n            test_episode_duration:      dict of keys {\'days\', \'hours\', \'minutes\'} - test sample duration\n        """"""\n        if assets_dataframes is None:\n            assert isinstance(assets_filenames, dict), \\\n                \'Expected `assets_filenames` type `dict`, got {} \'.format(type(assets_filenames))\n\n            data_config = {\n                asset_name: {\'filename\': f_name, \'dataframe\': None} for asset_name, f_name in assets_filenames.items()}\n\n        else:\n            assert assets_dataframes is not None, \'Expected either `assets_filenames` or `assets_dataframes`\'\n            assert isinstance(assets_dataframes, dict), \\\n                \'Expected `assets_filenames` type `dict`, got {} \'.format(type(assets_filenames))\n\n            data_config = {\n                asset_name: {\'dataframe\': df, \'filename\': None} for asset_name, df in assets_dataframes.items()\n            }\n\n        data_names = [name for name in data_config.keys()]\n        assert len(data_names) == 2, \'Expected exactly two assets, got: {}\'.format(data_names)\n\n        self.train_class_ref = _train_class_ref\n        self.test_class_ref = _test_class_ref\n\n        train_data_config = dict(\n            data_names=data_names,\n            generator_parameters_config=model_params,\n            episode_duration=train_episode_duration,\n        )\n        test_data_config = dict(\n            data_class_ref=BTgymDataset2,\n            data_config=data_config,\n            episode_duration=test_episode_duration,\n        )\n        super(BivariateDataSet, self).__init__(\n            train_data_config=train_data_config,\n            test_data_config=test_data_config,\n            train_class_ref=self.train_class_ref,\n            test_class_ref=self.test_class_ref,\n            name=name,\n            **kwargs\n        )\n\n\nclass BivariateStateSetGenerator(BivariateGenerator):\n\n    def __init__(\n            self,\n            data_names,\n            generator_parameters_config,\n            name=\'BivariateStateSetGenerator\',\n            **kwargs\n\n    ):\n        super().__init__(\n            data_names,\n            generator_parameters_config,\n            generator_fn=bivariate_generator_fn,\n            generator_parameters_fn=bivariate_state_set_iterator_fn,\n            name=name,\n            **kwargs\n        )\n\n\nclass BivariateStateDataSet(BivariateDataSet):\n\n    def __init__(self, *args, **kwargs):\n        """"""\n\n        Args:\n            model_params:               dict holding generative model parameters,\n                                        same as args for: bivariate_state_set_iterator_fn\n            assets_filenames:           dict. of two keys in form of: {\'asset_name`: \'data_file_name\'},\n                                        test data or None, ignored when `assets_dataframes` arg. is given\n            assets_dataframes:          dict. of two keys in form of {\'asset_name`: pd.dataframe},\n                                        an alternative way to provide test data as pandas.dataframes instances,\n                                        overrides `assets_filenames`\n            train_episode_duration:     dict of keys {\'days\', \'hours\', \'minutes\'} - train sample duration\n            test_episode_duration:      dict of keys {\'days\', \'hours\', \'minutes\'} - test sample duration\n        """"""\n        super().__init__(*args, _train_class_ref=BivariateStateSetGenerator, **kwargs)\n\n'"
btgym/research/model_based/datafeed/ou.py,0,"b'###############################################################################\n#\n# Copyright (C) 2017, 2018 Andrew Muzikin\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nimport copy\nimport pandas as pd\n\nfrom .base import BaseCombinedDataSet, BasePairCombinedDataSet, BasePairDataGenerator\nfrom .base import base_random_uniform_parameters_fn, base_spread_generator_fn\nfrom btgym.research.model_based.model.stochastic import ornshtein_uhlenbeck_process_fn, ornshtein_uhlenbeck_uniform_parameters_fn\nfrom btgym.research.model_based.model.stochastic import ornshtein_uhlenbeck_log_uniform_parameters_fn\nfrom btgym.research.model_based.model.stochastic import weiner_process_fn, weiner_process_uniform_parameters_fn\nfrom btgym.research.model_based.model.stochastic import coupled_wave_pair_generator_fn\nfrom btgym.datafeed.derivative import BTgymDataset2\nfrom btgym.datafeed.multi import BTgymMultiData\n\n\nclass UniformOUGenerator(BaseCombinedDataSet):\n    """"""\n    Combined data iterator provides:\n    - realisations of Ornstein-Uhlenbeck process as train data;\n    - real historic data as test data;\n\n    OUp. paramters are randomly uniformly sampled from given intervals.\n    """"""\n    def __init__(self, ou_mu, ou_lambda, ou_sigma, ou_x0=None, name=\'UniformOUData\', **kwargs):\n        """"""\n\n        Args:\n            ou_mu:                      float or iterable of 2 floats, Ornstein-Uhlenbeck process mean value or interval\n            ou_lambda:                  float or iterable of 2 floats, OUp. mean-reverting rate or interval\n            ou_sigma:                   float or iterable of 2 floats, OUp. volatility value or interval\n            ou_x0:                      float or iterable of 2 floats, OUp. trajectory start value or interval\n            filename:                   str, test data filename\n            parsing_params:             dict test data parsing params\n            episode_duration_train:     dict, duration of train episode in days/hours/mins\n            episode_duration_test:      dict, duration of test episode in days/hours/mins\n            time_gap:                   dict test episode duration tolerance\n            start_00:                   bool, def=False\n            timeframe:                  int, data periodicity in minutes\n            name:                       str\n            data_names:                 iterable of str\n            global_time:                dict {y, m, d} to set custom global time (here for plotting only)\n            task:                       int\n            log_level:                  logbook.Logger level\n        """"""\n        super(UniformOUGenerator, self).__init__(\n            generator_fn=ornshtein_uhlenbeck_process_fn,\n            generator_parameters_fn=ornshtein_uhlenbeck_uniform_parameters_fn,\n            generator_parameters_config={\'mu\': ou_mu, \'l\': ou_lambda, \'sigma\': ou_sigma, \'x0\': ou_x0},\n            name=name,\n            **kwargs\n        )\n\n\nclass LogUniformOUGenerator(BaseCombinedDataSet):\n    """"""\n    Combined data iterator provides:\n    - realisations of Ornstein-Uhlenbeck process as train data;\n    - real historic data as test data;\n\n    Lambda parameter is sampled from log-uniform distribution,\n    Sigma, Mu parameters are sampled from uniform distributions defined by given intervals.\n    """"""\n    def __init__(self, ou_mu, ou_lambda, ou_sigma, ou_x0=None, name=\'LogUniformOUData\', **kwargs):\n        """"""\n\n        Args:\n            ou_mu:                      float or iterable of 2 floats, Ornstein-Uhlenbeck process mean value or interval\n            ou_lambda:                  float or iterable of 2 floats, OUp. mean-reverting rate or interval\n            ou_sigma:                   float or iterable of 2 floats, OUp. volatility value or interval\n            ou_x0:                      float or iterable of 2 floats, OUp. trajectory start value or interval\n            filename:                   str, test data filename\n            parsing_params:             dict test data parsing params\n            episode_duration_train:     dict, duration of train episode in days/hours/mins\n            episode_duration_test:      dict, duration of test episode in days/hours/mins\n            time_gap:                   dict test episode duration tolerance\n            start_00:                   bool, def=False\n            timeframe:                  int, data periodicity in minutes\n            name:                       str\n            data_names:                 iterable of str\n            global_time:                dict {y, m, d} to set custom global time (here for plotting only)\n            task:                       int\n            log_level:                  logbook.Logger level\n        """"""\n        super(LogUniformOUGenerator, self).__init__(\n            generator_fn=ornshtein_uhlenbeck_process_fn,\n            generator_parameters_fn=ornshtein_uhlenbeck_log_uniform_parameters_fn,\n            generator_parameters_config={\'mu\': ou_mu, \'l\': ou_lambda, \'sigma\': ou_sigma, \'x0\': ou_x0},\n            name=name,\n            **kwargs\n        )\n\n\nclass OUGenerator(BaseCombinedDataSet):\n    """"""\n    Combined data iterator provides:\n    - realisations of Ornstein-Uhlenbeck process as train data;\n    - real historic data as test data;\n\n    This class expects OUp. parameters no-args callable to be explicitly provided.\n    """"""\n    def __init__(self, generator_parameters_fn, name=\'OUData\', **kwargs):\n        """"""\n\n        Args:\n            generator_parameters_fn:    callable, should return dictionary of generator_fn kwargs: {l, mu, sigma};\n                                        this callable itself should not require any args\n            filename:                   str, test data filename\n            parsing_params:             dict test data parsing params\n            episode_duration_train:     dict, duration of train episode in days/hours/mins\n            episode_duration_test:      dict, duration of test episode in days/hours/mins\n            time_gap:                   dict test episode duration tolerance\n            start_00:                   bool, def=False\n            timeframe:                  int, data periodicity in minutes\n            name:                       str\n            data_names:                 iterable of str\n            global_time:                dict {y, m, d} to set custom global time (here for plotting only)\n            task:                       int\n            log_level:                  logbook.Logger level\n        """"""\n        super(OUGenerator, self).__init__(\n            generator_fn=ornshtein_uhlenbeck_process_fn,\n            generator_parameters_fn=generator_parameters_fn,\n            name=name,\n            **kwargs\n        )\n\n\nclass PairOUDataSet(BasePairCombinedDataSet):\n    """"""\n    Combined data iterator provides:\n    Train:\n        two integrated synthetic data lines composed as:\n        line2 = Weiner_tragectory + .5 * OU_tragectory\n        line2 = Weiner_tragectory - .5 * OU_tragectory\n\n    Test:\n        two real historic time-consistent data lines;\n    """"""\n\n    def __init__(\n            self,\n            assets_filenames,\n            ou_lambda,\n            ou_sigma,\n            ou_mu,\n            weiner_delta,\n            x0,\n            spread_alpha,\n            spread_beta,\n            spread_max,\n            spread_min,\n            name=\'PairedOuDataSet\',\n            **kwargs\n    ):\n        """"""\n\n        Args:\n            assets_filenames:           (req.) dict of str, test data filenames (two files expected)\n            ou_mu:                      float or iterable of 2 floats, Ornstein-Uhlenbeck process mean value or interval\n            ou_lambda:                  float or iterable of 2 floats, OUp. mean-reverting rate or interval\n            ou_sigma:                   float or iterable of 2 floats, OUp. volatility value or interval\n            ou_x0:                      float or iterable of 2 floats, OUp. trajectory start value or interval\n            weiner_delta:               float or iterable of 2 floats, Weiner p. trajectory speed parameter or interval\n            spread_alpha:               float, random spread beta-distributed alpha param\n            spread_beta:                float, random spread beta-distributed beta param\n            spread_max:                 float, random spread min. value\n            spread_min:                 float, random spread max. value\n            train_episode_duration:     dict, duration of train episode in days/hours/mins\n            test_episode_duration:      dict, duration of test episode in days/hours/mins\n        """"""\n        process_1_config = dict(\n            generator_fn=weiner_process_fn,\n            generator_parameters_fn=weiner_process_uniform_parameters_fn,\n            generator_parameters_config={\'delta\': weiner_delta, \'x0\': x0},\n            spread_generator_fn=base_spread_generator_fn,\n            spread_generator_parameters={\n                \'alpha\': spread_alpha,\n                \'beta\': spread_beta,\n                \'minimum\': spread_min,\n                \'maximum\': spread_max\n            },\n        )\n        process_2_config = dict(\n            generator_fn=ornshtein_uhlenbeck_process_fn,\n            generator_parameters_fn=ornshtein_uhlenbeck_uniform_parameters_fn,\n            generator_parameters_config={\'mu\': ou_mu, \'l\': ou_lambda, \'sigma\': ou_sigma, \'x0\': None},\n            spread_generator_fn=None,\n        )\n        super(PairOUDataSet, self).__init__(\n            assets_filenames=assets_filenames,\n            process1_config=process_1_config,\n            process2_config=process_2_config,\n            name=name,\n            **kwargs\n        )\n\n\nclass PairWaveModelGenerator(BasePairDataGenerator):\n    """"""\n    More-or-less realistic OHLC model.\n    Utilizes single stochastic model to generate two integrated trajectories.\n    """"""\n    def __init__(\n            self,\n            data_names,\n            generator_parameters_config,\n            generator_fn=coupled_wave_pair_generator_fn,\n            generator_parameters_fn=base_random_uniform_parameters_fn,\n            name=\'PairWaveModelGenerator\',\n            **kwargs\n\n    ):\n        super(PairWaveModelGenerator, self).__init__(\n            data_names,\n            process1_config=None,  # bias generator\n            process2_config=None,  # spread generator\n            name=name,\n            **kwargs\n        )\n        self.generator_fn = generator_fn\n        self.generator_parameters_fn = generator_parameters_fn\n        self.generator_parameters_config = generator_parameters_config\n\n        self.columns_map = {\n            \'open\': \'mean\',\n            \'high\': \'maximum\',\n            \'low\': \'minimum\',\n            \'close\': \'last\',\n            \'bid\': \'minimum\',\n            \'ask\': \'maximum\',\n            \'mid\': \'mean\',\n        }\n\n    def generate_data(self, generator_params, sample_type=0):\n        """"""\n        Generates data trajectory.\n\n        Args:\n            generator_params:       dict, data_generating_function parameters\n            sample_type:            0 - generate train data | 1 - generate test data\n\n        Returns:\n            data as two pandas dataframes\n        """"""\n        # Get data shaped [2, 4, num_points] and map to OHLC pattern:\n        data = self.generator_fn(num_points=self.data[self.a1_name].episode_num_records, **generator_params)\n        p1_dict = {\n            \'mean\': data[0, 0, :],\n            \'maximum\': data[0, 1, :],\n            \'minimum\': data[0, 2, :],\n            \'last\': data[0, 3, :],\n        }\n        p2_dict = {\n            \'mean\': data[1, 0, :],\n            \'maximum\': data[1, 1, :],\n            \'minimum\': data[1, 2, :],\n            \'last\': data[1, 3, :],\n        }\n        # Make dataframes:\n        if sample_type:\n            index = self.data[self.a1_name].test_index\n        else:\n            index = self.data[self.a1_name].train_index\n        # Map dictionary of data to dataframe columns:\n        df1 = pd.DataFrame(data={name: p1_dict[self.columns_map[name]] for name in self.names}, index=index)\n        df2 = pd.DataFrame(data={name: p2_dict[self.columns_map[name]] for name in self.names}, index=index)\n\n        return df1, df2\n\n    def sample(self, sample_type=0, broadcast_message=None, **kwargs):\n        """"""\n        Overrides base method by employing single underlying stochastic process to generate two tragectories\n        Args:\n            sample_type:    bool, train/test\n            **kwargs:\n\n        Returns:\n            sample as PairWaveModelGenerator instance\n        """"""\n        # self.log.debug(\'broadcast_message: <<{}>>\'.format(broadcast_message))\n\n        if self.metadata[\'type\'] is not None:\n            if self.metadata[\'type\'] != sample_type:\n                self.log.warning(\n                    \'Attempt to sample type {} given current sample type {}, overriden.\'.format(\n                        sample_type,\n                        self.metadata[\'type\']\n                    )\n                )\n                sample_type = self.metadata[\'type\']\n\n        # Prepare empty instance of multi_stream data:\n        sample = PairWaveModelGenerator(\n            data_names=self.data_names,\n            generator_parameters_config=self.generator_parameters_config,\n            data_class_ref=self.data_class_ref,\n            name=\'sub_\' + self.name,\n            _top_level=False,\n            **self.nested_kwargs\n        )\n        # TODO: WTF?\n        sample.names = self.names\n\n        if self.get_new_sample:\n            # get parameters:\n            params = self.generator_parameters_fn(**self.generator_parameters_config)\n\n            data1, data2 = self.generate_data(params, sample_type=sample_type)\n\n            metadata = {\'generator\': params}\n\n        else:\n            data1 = None\n            data2 = None\n            metadata = {}\n\n        metadata.update(\n            {\n                \'type\': sample_type,\n                \'sample_num\': self.sample_num,\n                \'parent_sample_type\': self.metadata[\'type\'],\n                \'parent_sample_num\': self.sample_num,\n                \'first_row\': 0,\n                \'last_row\': self.data[self.a1_name].episode_num_records,\n            }\n        )\n\n        sample.metadata = copy.deepcopy(metadata)\n\n        # Populate sample with data:\n        sample.data[self.a1_name].data = data1\n        sample.data[self.a2_name].data = data2\n\n        sample.filename = {key: stream.filename for key, stream in self.data.items()}\n        self.sample_num += 1\n        return sample\n\n\nclass PairWaveModelDataSet(BaseCombinedDataSet):\n    """"""\n    Combined data iterator provides:\n    Train:\n        two trajectories of OHLC prices modelled by OU process with stochastic drift;\n        High-Low spread values for each price line independently generated by \'coupled wave model\';\n\n    Test:\n        two real historic time-consistent data lines;\n\n    """"""\n    def __init__(\n            self,\n            assets_filenames,\n            drift_sigma,\n            ou_sigma,\n            ou_lambda,\n            ou_mu,\n            spread_sigma_1,\n            spread_sigma_2,\n            spread_mean_1,\n            spread_mean_2,\n            bias,\n            train_episode_duration=None,\n            test_episode_duration=None,\n            name=\'PairedWaveData\',\n            **kwargs\n    ):\n        """"""\n\n        Args:\n            assets_filenames:           dict of two keys in form of {\'asset_name`: \'data_file_name\'}, test data\n            drift_sigma:                ufloat, stichastic drift sigma\n            ou_sigma:                   ufloat, base OU process sigma\n            ou_lambda:                  ufloat, base OU mean-reverting speed parameter\n            ou_mu:                      float, base OU mean parameter\n            spread_sigma_1:             ufloat, Hi-Lo spread generating sigma1\n            spread_sigma_2:             ufloat, Hi-Lo spread generating sigma2\n            spread_mean_1:              float, Hi-Lo spread generating mean1\n            spread_mean_2:              float, Hi-Lo spread generating mean2\n            bias:                       ufloat, process starting point\n            train_episode_duration:     dict of keys {\'days\', \'hours\', \'minutes\'} - train sample duration\n            test_episode_duration:      dict of keys {\'days\', \'hours\', \'minutes\'} - test sample duration\n        """"""\n        assert isinstance(assets_filenames, dict), \\\n            \'Expected `assets_filenames` type `dict`, got {} \'.format(type(assets_filenames))\n\n        data_names = [name for name in assets_filenames.keys()]\n        assert len(data_names) == 2, \'Expected exactly two assets, got: {}\'.format(data_names)\n\n        assert isinstance(assets_filenames, dict), \\\n            \'Expected `assets_filenames` type `dict`, got {} \'.format(type(assets_filenames))\n\n        data_names = [name for name in assets_filenames.keys()]\n        assert len(data_names) == 2, \'Expected exactly two assets, got: {}\'.format(data_names)\n\n        generator_parameters_config = dict(\n            drift_sigma=drift_sigma,\n            ou_sigma=ou_sigma,\n            ou_lambda=ou_lambda,\n            ou_mu=ou_mu,\n            spread_sigma_1=spread_sigma_1,\n            spread_sigma_2=spread_sigma_2,\n            spread_mean_1=spread_mean_1,\n            spread_mean_2=spread_mean_2,\n            bias=bias,\n        )\n        train_data_config = dict(\n            data_names=data_names,\n            generator_parameters_config=generator_parameters_config,\n            episode_duration=train_episode_duration,\n        )\n        test_data_config = dict(\n            data_class_ref=BTgymDataset2,\n            data_config={asset_name: {\'filename\': file_name} for asset_name, file_name in assets_filenames.items()},\n            episode_duration=test_episode_duration,\n        )\n        super(PairWaveModelDataSet, self).__init__(\n            train_data_config=train_data_config,\n            test_data_config=test_data_config,\n            train_class_ref=PairWaveModelGenerator,\n            test_class_ref=BTgymMultiData,\n            name=name,\n            **kwargs\n        )\n\n\n'"
btgym/research/model_based/model/__init__.py,0,b''
btgym/research/model_based/model/bivariate.py,0,"b'import numpy as np\nfrom collections import namedtuple\n\nfrom btgym.research.model_based.model.rec import Zscore, ZscoreState, Covariance, CovarianceState\nfrom btgym.research.model_based.model.rec import OUEstimatorState\n\nfrom btgym.research.model_based.model.univariate import OUProcess, TimeSeriesModel\n\n\nBivariateTSModelState = namedtuple(\'BivariateTSModelState\', [\'p\', \'s\', \'stat\', \'ps_stat\'])\n\n\nclass BivariateTSModel:\n    """"""\n    Two-factor bivariate time-series model.\n\n    Motivating papers:\n        Eduardo Schwartz, James E. Smith, ""Short-Term Variations and Long-Term Dynamics in Commodity Prices"",\n        in ""Management Science"", Vol. 46, No. 7, July 2000 pp. 893\xe2\x80\x93911\n\n        Harris, D., ""Principal components analysis of cointegrated time series,"" in ""Econometric Theory"", Vol. 13, 1997\n    """"""\n    # TODO: trajectory generator uses simplified algorithm: entire trajectory is generated out of single model state\n    # TODO: proper state-space model approach\n    # TODO: should be: sample [randomized?] trajectory of states -> sample realisation trajectory of same length\n    # Decomposition matrix:\n    u_decomp = np.asarray([[.5, .5], [.5, -.5]])\n\n    # Reconstruction (inverse u_decomp):\n    u_recon = np.asarray([[1.,  1.], [1., -1.]])\n\n    def __init__(\n            self,\n            max_length,\n            analyzer_window,\n            p_analyzer_grouping=None,\n            s_analyzer_grouping=None,\n            alpha=None,\n            filter_alpha=None,\n            stat_alpha=None,\n            ps_alpha=None,\n    ):\n        """"""\n\n        Args:\n            max_length:             uint, maximum time-series trajectory length to keep;\n            analyzer_window:        uint, SSA embedding window (shared for P and S analyzers);\n            p_analyzer_grouping:    P process SSA decomposition triples grouping,\n                                    iterable of pairs convertible to python slices, i.e.:\n                                    grouping=[[0,1], [1,2], [2, None]];\n            s_analyzer_grouping:    P process SSA decomposition triples grouping, se above;\n            alpha:                  float in [0, 1], SSA and processes estimators decaying factor;\n            filter_alpha:           float in [0, 1], processes smoothing decaying factor;\n            stat_alpha:             float in [0, 1], time-series statistics tracking decaying factor;\n            ps_alpha:               float in [0, 1], P|S processes covariance tracking decaying factor;\n        """"""\n        max_length = np.atleast_1d(max_length)\n        analyzer_window = np.atleast_1d(analyzer_window)\n        alpha = np.atleast_1d(alpha)\n        filter_alpha = np.atleast_1d(filter_alpha)\n\n        # Max. variance factor component (average):\n        self.p = TimeSeriesModel(\n            max_length[0],\n            analyzer_window[0],\n            p_analyzer_grouping,\n            alpha[0],\n            filter_alpha[0]\n        )\n\n        # Max. stationarity factor component (difference):\n        self.s = TimeSeriesModel(\n            max_length[-1],\n            analyzer_window[-1],\n            s_analyzer_grouping,\n            alpha[-1],\n            filter_alpha[-1]\n        )\n\n        # Statistics of original data:\n        self.stat = Zscore(2, stat_alpha)\n\n        # Stochastic processes covariance:\n        self.ps_stat = Covariance(2, ps_alpha)\n\n    def ready(self):\n        return self.s.ready() and self.p.ready()\n\n    def get_state(self):\n        return BivariateTSModelState(\n            p=self.p.get_state(),\n            s=self.s.get_state(),\n            stat=self.stat.get_state(),\n            ps_stat=self.ps_stat.get_state()\n        )\n\n    @staticmethod\n    def get_random_state(p_params, s_params, mean=(100, 100), variance=(1, 1), ps_corrcoef=(-1, 1)):\n        """"""\n        Samples random uniform model state w.r.t. parameters intervals given.\n\n        Args:\n            p_params:       dict, P stochastic process parameters, see kwargs at: OUProcess.get_random_state\n            s_params:       dict, S stochastic process parameters, see kwargs at: OUProcess.get_random_state\n            mean:           iterable of floats as [lower_bound, upper_bound], time-series means sampling interval.\n            variance:       iterable of floats as [lower_bound, upper_bound], time-series variances sampling interval.\n            ps_corrcoef:    iterable of floats as [lower_bound, upper_bound], correlation coefficient\n                            for P and S process innovations, -1 <= ps_corrcoef <= 1\n\n        Returns:\n            instance of BivariateTSModelState\n\n        Note:\n            negative means are allowed.\n        """"""\n        sample = dict()\n        for name, param, low_threshold in zip(\n                [\'mean\', \'variance\', \'ps_corrcoef\'], [mean, variance, ps_corrcoef], [-np.inf, 1e-8, -1.0]):\n            interval = np.asarray(param)\n            assert interval.ndim == 1 and interval[0] <= interval[-1], \\\n                \' Expected param `{}` as iterable of ordered values as: [lower_bound, upper_bound], got: {}\'.format(\n                    name, interval\n                )\n            assert interval[0] >= low_threshold, \\\n                \'Expected param `{}` lower bound be no less than {}, got: {}\'.format(name, low_threshold, interval[0])\n\n            sample[name] = np.random.uniform(low=interval[0], high=interval[-1], size=2)\n\n        # Correlation matrix instead of covariance - it is ok as it gets normalized when sampling anyway:\n        rho = np.eye(2)\n        rho[0, 1] = rho[1, 0] = sample[\'ps_corrcoef\'][0]\n        # TODO: log-uniform sampling for s, p params\n        return BivariateTSModelState(\n            p=TimeSeriesModel.get_random_state(**p_params),\n            s=TimeSeriesModel.get_random_state(**s_params),\n            stat=ZscoreState(\n                mean=sample[\'mean\'],\n                variance=sample[\'variance\']\n            ),\n            ps_stat=CovarianceState(\n                mean=np.zeros(2),\n                variance=np.ones(2),\n                covariance=rho,\n            ),\n        )\n\n    @staticmethod\n    def _decompose(trajectory, mean, variance, u):\n        """"""\n        Returns orthonormal decomposition of pair [X1, X2].\n        Static method, can be used as stand-along function.\n\n        Args:\n            trajectory: time-series data of shape [2, num_points]\n            mean:       data mean of size [2]\n            variance:   data variance of size [2]\n            u:          [2, 2] decomposition matrix\n\n        Returns:\n            data projection of size [2, num_pints], where first (P) component is average and second (S) is difference\n            of original time-series.\n        """"""\n        assert len(trajectory.shape) == 2 and trajectory.shape[0] == 2, \\\n            \'Expected data as array of size [2, num_points], got: {}\'.format(trajectory.shape)\n\n        assert mean.shape == (2,) and variance.shape == (2,), \\\n            \'Expected mean and variance as vectors of size [2], got: {}, {}\'.format(mean.shape, variance.shape)\n\n        assert u.shape == (2, 2), \'Expected U as 2x2 matrix, got: {}\'.format(u.shape)\n\n        # Z-score data:\n        # Mind swapped STD!\n        norm_data = (trajectory - mean[:, None]) / np.clip(variance[:, None], 1e-8, None) ** .5\n        ps_decomposition = np.matmul(u, norm_data)\n\n        return ps_decomposition\n\n    @staticmethod\n    def _reconstruct(ps_decomposition, mean, variance, u):\n        """"""\n        Returns original data [X1, X2] given orthonormal P|S decomposition .\n        Static method, can be used as stand-along function.\n\n        Args:\n            ps_decomposition:   data ps-decomposition of size [2, num_points]\n            mean:               original data mean of size [2]\n            variance:           original data variance of size [2]\n            u:                  [2, 2] reconstruction matrix\n\n        Returns:\n            reconstructed data of size [2, num_pints]\n        """"""\n        assert len(ps_decomposition.shape) == 2 and ps_decomposition.shape[0] == 2, \\\n            \'Expected data as array of size [2, num_points], got: {}\'.format(ps_decomposition.shape)\n\n        assert mean.shape == (2,) and variance.shape == (2,), \\\n            \'Expected mean and variance as vectors of size [2], got: {}, {}\'.format(mean.shape, variance.shape)\n\n        assert u.shape == (2, 2), \'Expected U as 2x2 matrix, got: {}\'.format(u.shape)\n\n        return np.matmul(u, ps_decomposition) * variance[:, None] ** .5 + mean[:, None]\n\n    def decompose(self, trajectory):\n        """"""\n        Returns orthonormal decomposition of pair [X1, X2] w.r.t current statistics.\n\n        Args:\n            trajectory: time-series data of shape [2, num_points]\n\n        Returns:\n            tuple (P, S), where first (P) component is average and second (S) is difference\n            of original time-series, of size [num_points] each\n        """"""\n        ps_decomp = self._decompose(trajectory, self.stat.mean, self.stat.variance, self.u_decomp)\n        return ps_decomp[0, :], ps_decomp[1, :]\n\n    def reconstruct(self, p, s, mean=None, variance=None):\n        """"""\n        Returns original data [X1, X2] given orthonormal P|S decomposition.\n\n        Args:\n            p:          data p-component of shape [num_points]\n            s:          data s-component of shape [num_points]\n            mean:       original data mean of size [2] or None\n            variance:   original data variance of size [2] or None\n\n        Returns:\n            reconstructed data of size [2, num_pints]\n\n        Notes:\n            if either mean or variance arg is not given - stored mean and variance are used.\n        """"""\n        assert p.shape == s.shape, \' Expected components be same size but got: {} and {}\'.format(p.shape, s.shape)\n\n        if mean is None or variance is None:\n            mean = self.stat.mean\n            variance = self.stat.variance\n\n        ps = np.stack([p, s], axis=0)\n        return self._reconstruct(ps, mean, variance, self.u_recon)\n\n    def reset(self, init_trajectory):\n        """"""\n        Resets model parameters and trajectories given initial data.\n\n        Args:\n            init_trajectory:    initial time-series observations of size [2, num_points]\n        """"""\n        _ = self.stat.reset(init_trajectory)\n        p_data, s_data = self.decompose(init_trajectory)\n        self.p.reset(p_data)\n        self.s.reset(s_data)\n        residuals = np.stack(\n            [self.p.process.estimator.residuals, self.s.process.estimator.residuals],\n            axis=0\n        )\n        _ = self.ps_stat.reset(residuals)\n\n    def update(self, trajectory, disjoint=False):\n        """"""\n        Updates model parameters and trajectories given new data.\n\n        Args:\n            trajectory: time-series update observations of size [2, num_points], where:\n                        num_points <= min{p_params[max_length], s_params[max_length]} is necessary\n                        to keep model trajectory continuous\n            disjoint:   bool, indicates whether update given is continuous or disjoint w.r.t. previous one\n        """"""\n        _ = self.stat.update(trajectory)  # todo: this stat.estimator does not respect `disjoint` arg.; ?!!\n        p_data, s_data = self.decompose(trajectory)\n        self.p.update(p_data, disjoint)\n        self.s.update(s_data, disjoint)\n        residuals = np.stack(\n            [self.p.process.estimator.residuals, self.s.process.estimator.residuals],\n            axis=0\n        )\n        _ = self.ps_stat.update(residuals)\n\n    def transform(self, trajectory=None, state=None, size=None):\n        """"""\n        Returns per-component analyzer data decomposition.\n\n        Args:\n            trajectory:     bivariate data to decompose of size [2, num_points] or None\n            state:          instance of BivariateTSModelState or None\n            size:           uint, size of decomposition to get, or None\n\n        Returns:\n            array of [size or num_points], array of [size or num_points], ZscoreState(2)\n\n            - SSA transformations of P, S components of given trajectory w.r.t. given state\n            - bivariate trajectory statistics (means and variances)\n\n        Notes:\n            if no `trajectory` is given - returns stored data decomposition\n            if no `state` arg. is given - uses stored analyzer state.\n            if no \'size` arg is given - decomposes full [stored or given] trajectory\n        """"""\n        if state is not None:\n            assert isinstance(state, BivariateTSModelState),\\\n                \'Expected `state as instance of BivariateTSModelState but got: {}`\'.format(type(state))\n            s_state = state.s\n            p_state = state.p\n            stat = state.stat\n\n        else:\n            assert trajectory is None, \'When `trajectory` arg. is given, `state` is required\'\n            p_state = None\n            s_state = None\n            stat = self.stat.get_state()\n\n        if trajectory is not None:\n            ps_data = self._decompose(trajectory, stat.mean, stat.variance, self.u_decomp)\n            p_data = ps_data[0, :]\n            s_data = ps_data[1, :]\n\n        else:\n            p_data = None\n            s_data = None\n\n        p_transformed = self.p.transform(p_data, p_state, size)\n        s_transformed = self.s.transform(s_data, s_state, size)\n\n        return p_transformed, s_transformed, stat\n\n    def get_trajectory(self, size=None, reconstruct=True):\n        """"""\n        Returns stored decomposition fragment and [optionally] time-series reconstruction.\n        TODO: reconstruction is freaky due to only last stored statistic is used\n\n        Args:\n            size:           uint, fragment length to get in [1, ..., max_length] or None\n            reconstruct:    bool, if True - also return data reconstruction\n\n        Returns:\n            array of [size ... max_length], array of [size ... max_length], array of size [2, size ... max_length]\n            or\n            array of [size ... max_length], array of [size ... max_length], None\n\n            P,C [, and 2D trajectory] series as [ x[-size], x[-size+1], ... x[-1] ], up to length [size];\n            if no `size` arg. is given - returns entire stored trajectory, up to length [max_length].\n\n        """"""\n        p_data = self.p.get_trajectory(size)\n        s_data = self.s.get_trajectory(size)\n\n        if reconstruct:\n            trajectory = self.reconstruct(p_data, s_data)\n\n        else:\n            trajectory = None\n\n        return p_data, s_data, trajectory\n\n    @staticmethod\n    def generate_trajectory_fn(batch_size, size, state, reconstruct=False, u_recon=None):\n        """"""\n        Generates batch of time-series realisations given model state.\n        Static method, can be used as stand-along function.\n\n        Args:\n            batch_size:     uint, number of trajectories to generates\n            size:           uint, trajectory length to generate\n            state:          instance of BivariateTSModelState;\n            reconstruct:    bool, if True - return time-series along with P, S trajectories, return None otherwise\n            u_recon:        reconstruction matrix of size [2, 2] or None; required if reconstruct=True;\n\n        Returns:\n            generated P and S processes realisations of size [batch_size, 2, size];\n            generated time-series reconstructions of size [batch_size, 2, size] or None;\n        """"""\n        assert isinstance(state, BivariateTSModelState), \\\n            \'Expected `state` as instance of BivariateTSModelState, got: {}\'.format(type(state))\n\n        if reconstruct:\n            assert u_recon is not None, \'reconstruct=True but reconstruction matrix is not provided.\'\n\n        # Unpack:\n        p_state = state.p.process\n        s_state = state.s.process\n\n        # Get all samples for single batch (faster):\n        p_params = OUProcess.sample_naive_unbiased(p_state, batch_size)\n        s_params = OUProcess.sample_naive_unbiased(s_state, batch_size)\n\n        # Concatenate batch-wise:\n        parameters = OUEstimatorState(\n            mu=np.concatenate([p_params.mu, s_params.mu]),\n            log_theta=np.concatenate([p_params.log_theta, s_params.log_theta]),\n            log_sigma=np.concatenate([p_params.log_sigma, s_params.log_sigma]),\n        )\n        driver_df = np.concatenate(\n            [\n                np.tile(p_state.driver_df, batch_size),\n                np.tile(s_state.driver_df, batch_size),\n            ]\n        )\n        # Access multivariate generator_fn directly to get batch of bivariate OU:\n        batch_2x = OUProcess.generate_trajectory_fn(2 * batch_size, size, parameters, driver_df)\n        batch_2x = np.reshape(batch_2x, [2, batch_size, -1])\n        batch_2x = np.swapaxes(batch_2x, 0, 1)\n\n        if reconstruct:\n            x = np.matmul(u_recon, batch_2x) * state.stat.variance[None, :, None] ** .5 \\\n                + state.stat.mean[None, :, None]\n\n        else:\n            x = None\n\n        return batch_2x, x\n\n    @staticmethod\n    def generate_bivariate_trajectory_fn(batch_size, size, state, reconstruct=False, u_recon=None):\n        """"""\n        Generates batch of time-series realisations given model state.\n        Static method, can be used as stand-along function.\n\n        Args:\n            batch_size:     uint, number of trajectories to generates\n            size:           uint, trajectory length to generate\n            state:          instance of BivariateTSModelState;\n            reconstruct:    bool, if True - return time-series along with P, S trajectories, return None otherwise\n            u_recon:        reconstruction matrix of size [2, 2] or None; required if reconstruct=True;\n\n        Returns:\n            generated P and S processes realisations of size [batch_size, 2, size];\n            generated time-series reconstructions of size [batch_size, 2, size] or None;\n        """"""\n        assert isinstance(state, BivariateTSModelState), \\\n            \'Expected `state` as instance of BivariateTSModelState, got: {}\'.format(type(state))\n\n        if reconstruct:\n            assert u_recon is not None, \'reconstruct=True but reconstruction matrix is not provided.\'\n\n        # Unpack:\n        p_state = state.p.process\n        s_state = state.s.process\n\n        # Get all samples for single batch (faster):\n        p_params = OUProcess.sample_naive_unbiased(p_state, 1)\n        s_params = OUProcess.sample_naive_unbiased(s_state, 1)\n\n        # Concatenate batch-wise:\n        parameters = OUEstimatorState(\n            mu=np.concatenate([p_params.mu, s_params.mu]),\n            log_theta=np.concatenate([p_params.log_theta, s_params.log_theta]),\n            log_sigma=np.concatenate([p_params.log_sigma, s_params.log_sigma]),\n        )\n        driver_df = np.asarray([p_state.driver_df, s_state.driver_df])\n\n        # Access multivariate generator_fn directly to get batch of 2d correlated OU\'s:\n        batch_2d = OUProcess.generate_multivariate_trajectory_fn(\n            batch_size=batch_size,\n            size=size,\n            parameters=parameters,\n            t_df=driver_df,\n            covariance=state.ps_stat.covariance\n        )\n        batch_2d = np.swapaxes(batch_2d, 1, 2)\n\n        if reconstruct:\n            x = np.matmul(u_recon, batch_2d) * state.stat.variance[None, :, None] ** .5 \\\n                + state.stat.mean[None, :, None]\n\n        else:\n            x = None\n\n        return batch_2d, x\n\n    def generate(self, batch_size, size, state=None, reconstruct=True):\n        """"""\n        Generates batch of time-series realisations given model state.\n\n        Args:\n            batch_size:     uint, number of trajectories to generates\n            size:           uint, trajectory length to generate\n            state:          instance of BivariateTSModelState or None;\n                            if no state provided - current state is used.\n            reconstruct:    bool, if True - return time-series along with P, S trajectories, return None otherwise\n\n        Returns:\n            generated P and S processes realisations of size [batch_size, 2, size];\n            generated time-series reconstructions of size [batch_size, 2, size] or None;\n        """"""\n        if state is None:\n            # Fit student-t df:\n            _ = self.p.process.driver_estimator.fit()\n            _ = self.s.process.driver_estimator.fit()\n\n            state = self.get_state()\n\n        # return self.generate_trajectory_fn(batch_size, size, state, reconstruct, self.u_recon)\n        return self.generate_bivariate_trajectory_fn(batch_size, size, state, reconstruct, self.u_recon)\n\n\nclass BivariatePriceModel(BivariateTSModel):\n    """"""\n    Wrapper class for positive-valued time-series.\n    Internally works with log-transformed data.\n    """"""\n\n    def reset(self, init_trajectory):\n        """"""\n        Resets model parameters and trajectories given initial data.\n\n        Args:\n            init_trajectory:    initial time-series observations of size [2, num_points]\n        """"""\n        return super().reset(np.log(init_trajectory))\n\n    def update(self, trajectory, disjoint=False):\n        """"""\n        Updates model parameters and trajectories given new data.\n\n        Args:\n            trajectory: time-series update observations of size [2, num_points], where:\n                        num_points <= min{p_params[max_length], s_params[max_length]} is necessary\n                        to keep model trajectory continuous\n            disjoint:   bool, indicates whether update given is continuous or disjoint w.r.t. previous one\n        """"""\n        return super().update(np.log(trajectory), disjoint)\n\n    def transform(self, trajectory=None, state=None, size=None):\n        """"""\n        Returns per-component analyzer data decomposition.\n\n        Args:\n            trajectory:     data to decompose of size [2, num_points] or None\n            state:          instance of BivariateTSModelState or None\n            size:           uint, size of decomposition to get, or None\n\n        Returns:\n            array of [size or num_points], array of [size or num_points], ZscoreState(2)\n\n            - SSA transformations of P, S components of given trajectory w.r.t. given state\n            - bivariate trajectory statistics (means and variances)\n\n        Notes:\n            if no `trajectory` is given - returns stored data decomposition\n            if no `state` arg. is given - uses stored analyzer state.\n            if no \'size` arg is given - decomposes full [stored or given] trajectory\n        """"""\n        if trajectory is not None:\n            trajectory = np.log(trajectory)\n\n        return super().transform(trajectory, state, size)\n\n    def get_trajectory(self, size=None, reconstruct=True):\n        """"""\n        Returns stored decomposition fragment and [optionally] time-series reconstruction.\n        TODO: reconstruction is freaky due to only last stored statistic is used\n\n        Args:\n            size:           uint, fragment length to get in [1, ..., max_length] or None\n            reconstruct:    bool, if True - also return data reconstruction\n\n        Returns:\n            array of [size ... max_length], array of [size ... max_length], array of size [2, size ... max_length]\n            or\n            array of [size ... max_length], array of [size ... max_length], None\n\n            P,C [, and 2D trajectory] series as [ x[-size], x[-size+1], ... x[-1] ], up to length [size];\n            if no `size` arg. is given - returns entire stored trajectory, up to length [max_length].\n\n        """"""\n        p_data, s_data, trajectory = super().get_trajectory(size, reconstruct)\n\n        if reconstruct:\n            trajectory = np.exp(trajectory)\n\n        return p_data, s_data, trajectory\n\n    @staticmethod\n    def get_random_state(p_params, s_params, mean=(100, 100), variance=(1, 1), ps_corrcoef=(-1, 1)):\n        """"""\n        Samples random uniform model state w.r.t. intervals given.\n\n        Args:\n            p_params:       dict, P stochastic process parameters, see kwargs at: OUProcess.get_random_state\n            s_params:       dict, S stochastic process parameters, see kwargs at: OUProcess.get_random_state\n            mean:           iterable of floats as [0 < lower_bound, upper_bound], time-series means sampling interval.\n            variance:       iterable of floats as [0 < lower_bound, upper_bound], time-series variances sampling interval.\n            ps_corrcoef:    iterable of floats as [-1 <= lower_bound, upper_bound <= 1], correlation coefficient\n                            for P and S process innovations.\n\n        Returns:\n            instance of BivariateTSModelState\n\n        Note:\n            negative means are rejected;\n            P and S processes fitted over log_transformed data;\n        """"""\n        sample = dict()\n        for name, param, low_threshold in zip(\n                [\'mean\', \'variance\', \'ps_corrcoef\'], [mean, variance, ps_corrcoef], [1e-8, 1e-8, -1.0]):\n            interval = np.asarray(param)\n            assert interval.ndim == 1 and interval[0] <= interval[-1], \\\n                \' Expected param `{}` as iterable of ordered values as: [lower_bound, upper_bound], got: {}\'.format(\n                    name, interval\n                )\n            assert interval[0] >= low_threshold, \\\n                \'Expected param `{}` lower bound be no less than {}, got: {}\'.format(name, low_threshold, interval[0])\n\n            sample[name] = np.random.uniform(low=interval[0], high=interval[-1], size=2)\n\n        # Correlation matrix instead of covariance - it is ok as it gets normalized when sampling anyway:\n        rho = np.eye(2)\n        rho[0, 1] = rho[1, 0] = sample[\'ps_corrcoef\'][0]\n\n        # Log_transform mean and variance (those is biased estimates but ok for rnd. samples):\n        log_variance = np.log(sample[\'variance\'] / sample[\'mean\'] ** 2 + 1)\n        log_mean = np.log(sample[\'mean\']) - .5 * log_variance\n\n        # Inverse transform memo:\n        # mean = exp(log_mean + 0.5 * log_var)\n        # var = mean**2 * (exp(log_var) -1)\n\n        return BivariateTSModelState(\n            p=TimeSeriesModel.get_random_state(**p_params),\n            s=TimeSeriesModel.get_random_state(**s_params),\n            stat=ZscoreState(\n                mean=log_mean,\n                variance=log_variance\n            ),\n            ps_stat=CovarianceState(\n                mean=np.zeros(2),\n                variance=np.ones(2),\n                covariance=rho,\n            ),\n        )\n\n    @staticmethod\n    def generate_trajectory_fn(batch_size, size, state, reconstruct=False, u_recon=None):\n        """"""\n        Generates batch of time-series realisations given model state.\n        Static method, can be used as stand-along function.\n\n        Args:\n            batch_size:     uint, number of trajectories to generates\n            size:           uint, trajectory length to generate\n            state:          instance of BivariateTSModelState;\n            reconstruct:    bool, if True - return time-series along with P, S trajectories, return None otherwise\n            u_recon:        reconstruction matrix of size [2, 2] or None; required if reconstruct=True;\n\n        Returns:\n            generated P and S processes realisations of size [batch_size, 2, size];\n            generated time-series reconstructions of size [batch_size, 2, size] or None;\n        """"""\n        batch_2x, x = BivariateTSModel.generate_trajectory_fn(batch_size, size, state, reconstruct, u_recon)\n\n        if reconstruct:\n            x = np.exp(x)\n\n        return batch_2x, x\n\n    @staticmethod\n    def generate_bivariate_trajectory_fn(batch_size, size, state, reconstruct=False, u_recon=None):\n        """"""\n        Generates batch of time-series realisations given model state.\n        Static method, can be used as stand-along function.\n\n        Args:\n            batch_size:     uint, number of trajectories to generates\n            size:           uint, trajectory length to generate\n            state:          instance of BivariateTSModelState;\n            reconstruct:    bool, if True - return time-series along with P, S trajectories, return None otherwise\n            u_recon:        reconstruction matrix of size [2, 2] or None; required if reconstruct=True;\n\n        Returns:\n            generated P and S processes realisations of size [batch_size, 2, size];\n            generated time-series reconstructions of size [batch_size, 2, size] or None;\n        """"""\n        batch_2d, x = BivariateTSModel.generate_bivariate_trajectory_fn(batch_size, size, state, reconstruct, u_recon)\n\n        if reconstruct:\n            x = np.exp(x)\n\n        return batch_2d, x\n\n\nclass BPM(BivariatePriceModel):\n    """"""\n    Wrapper class with de-facto disabled analyzer\n    in favor to state lightness an computation speed.\n    """"""\n\n    def __init__(\n            self,\n            *args,\n            analyzer_window=None,\n            p_analyzer_grouping=None,\n            s_analyzer_grouping=None,\n            **kwargs\n    ):\n        super().__init__(\n            *args,\n            analyzer_window=[2, 2],\n            p_analyzer_grouping=None,\n            s_analyzer_grouping=None,\n            **kwargs\n        )\n\n\n\n\n\n\n'"
btgym/research/model_based/model/rec.py,0,"b'# Exponentially smoothed recursive versions\n# of some useful statistics and estimators\n# for time-series analysis\n\nimport numpy as np\nfrom scipy.linalg import toeplitz\nfrom scipy.stats import t as student_t\nimport copy\nfrom collections import namedtuple\n\n\nSSAState = namedtuple(\n    \'SSAState\',\n    [\'window\', \'max_length\', \'grouping\', \'alpha\', \'covariance\', \'u\', \'singular_values\', \'mean\', \'variance\']\n)\n\n\nclass SSA:\n    """"""\n    Recursive toeplitz-style Singular Spectrum Analysis estimation of one-dimensional signal\n    with arbitrary consecutive updates length.\n\n    See:\n    https://en.wikipedia.org/wiki/Singular_spectrum_analysis\n\n    Golyandina, N. et. al., ""Basic Singular Spectrum Analysis and Forecasting with R"", 2012,\n    https://arxiv.org/abs/1206.6910\n\n    Golyandina, N., ""Singular Spectrum Analysis for time series"", 2004 [in Russian]:\n    http://www.gistatgroup.com/gus/ssa_an.pdf\n\n    Dimitrios D. Thomakos, ""Optimal Linear Filtering, Smoothing and Trend Extraction\n    for Processes with Unit Roots and Cointegration"", 2008,\n    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1113331\n    """"""\n\n    def __init__(self, window, max_length, grouping=None, alpha=None):\n        """"""\n\n        Args:\n            window:         uint, time embedding window\n            max_length:     uint, maximum embedded signal trajectory length to keep, should be > window\n            grouping:       SSA decomposition triples grouping, iterable of pairs convertible to python slices, i.e.:\n                            grouping=[[0,1], [1,2], [2, None]]\n            alpha:          float in [0, 1], decaying factor;\n\n            Notes:\n                alpha ~ 1 / effective_window_length;\n                alpha ~ 1 - forgetting_factor,  in terms of Recursive Least Squares Filter\n        """"""\n        self.window = window\n        assert max_length > window,\\\n            \'Expected max_length_to_keep > window, got {} and {}\'.format(max_length, window)\n        self.max_length = max_length\n        self.max_length_adj = max_length - window + 1\n        self.grouping = grouping\n        self.x_embedded = None\n        self.cov_estimator = Covariance(window, alpha=alpha)\n        self.covariance = None\n        self.mean = None\n        self.variance = None\n        self.u = None\n        self.singular_values = None\n        self.v = None\n        self.state = None\n        self.is_ready = False\n\n    def ready(self):\n        assert self.is_ready, \'SSA is not initialized. Hint: forgot to call .reset()?\'\n\n    def get_state(self):\n        """"""\n        Convenience wrapper: pack and send everything but trajectory.\n\n        Returns:\n            instance of SSAstate: named tuple holding current estimator statistics\n        """"""\n        self.state = SSAState(\n            self.window,\n            self.max_length,\n            self.grouping,\n            self.cov_estimator.stat.alpha,\n            self.covariance,\n            self.u,\n            self.singular_values,\n            self.mean,\n            self.variance,\n        )\n        return self.state\n\n    def reset(self, init_trajectory):\n        """"""\n        Resets estimator state and stored trajectory.\n\n        Args:\n            init_trajectory:    initial trajectory of size [init_num_points],\n                                such as: length + window > init_num_points > window\n\n        Returns:\n            embedded trajectory of size [window, init_num_points - window + 1]\n        """"""\n\n        assert self.max_length >= init_trajectory.shape[0] > self.window, \\\n            \'Expected initial trajectory length be in [{}, ..., {}], got: {}\'.format(\n                self.window + 1, self.max_length, init_trajectory.shape[0]\n            )\n        init_embedding = self._update_embed(init_trajectory, disjoint=True)\n        self.covariance, self.mean, self.variance = self.cov_estimator.reset(init_embedding)\n        self._update_svd()\n        self.is_ready = True\n\n        return init_embedding\n\n    def update(self, trajectory, disjoint=False):\n        """"""\n        Updates estimator state and embedded trajectory.\n\n        Args:\n            trajectory: observation trajectory of size [num_points], such as: length >= num_points > 0\n            disjoint:   bool, indicates whether update given is continuous or disjoint w.r.t. previous update\n                        if set to True - discards embedded trajectory already being kept.\n\n        Returns:\n            embedded update trajectory of size [window, num_points]\n        """"""\n\n        self.ready()\n        if not disjoint:\n            assert self.max_length_adj >= trajectory.shape[0] > 0,\\\n                \'Expected continuous update length be less than: {}, got: {}\'.format(self.max_length, trajectory.shape[0])\n        embedded_update = self._update_embed(trajectory, disjoint=disjoint)\n        self.covariance, self.mean, self.variance = self.cov_estimator.update(embedded_update)\n        self._update_svd()\n\n        return embedded_update\n\n    def transform(self, x_embedded=None, state=None, size=None):\n        """"""\n        Return SSA signal decomposition.\n\n        Args:\n            x_embedded:      lag-embedded signal of size [window, length] or None\n            state:  instance of SSAstate or None\n            size:   uint or None, if given - trajectory size to transform, counting from most recent observation\n\n        Returns:\n            SSA signal decomposition of given X w.r.t. state\n            if no arguments provided - returns decomposition of kept trajectory;\n        """"""\n        if x_embedded is None:\n            x_embedded = self.x_embedded\n\n        else:\n            assert state is not None, \'SSAstate is expected when outer X is given, but got: None\'\n\n        if state is None:\n            state = self.get_state()\n\n        if size is not None:\n            assert size > self.window - 1, \'Expected `size` no less than: {} but got: {}\'.format(self.window, size)\n            idx = - size + self.window - 1\n            if - idx > x_embedded.shape[-1]:\n                idx = None\n\n        else:\n            idx = None\n\n        return self._transform(x_embedded[:, idx:], state)\n\n    def _update_embed(self, x, disjoint=False):\n        """"""\n        Arbitrary length update of embedding matrix.\n\n        Args:\n            x:          observation trajectory of size [num_points], such as: length >= num_points > 0\n            disjoint:   bool, indicates whether update given is continuous or disjoint w.r.t. previous update\n                        if set to True - discards embedded trajectory already being kept.\n\n        Returns:\n            embedded update/home/muzikin/Repos/ml_serving\n\n        """"""\n        assert len(x.shape) == 1, \'Expected 1d trajectory but got input shaped: {}\'.format(x.shape)\n        if disjoint:\n            # Been told trajectory given is NOT continuous input:\n            assert self.max_length >= x.shape[0] > self.window, \\\n                            \'Expected disjoint/initial trajectory length be in [{}, ..., {}], got: {}\'.format(\n                                self.window + 1, self.max_length, x.shape[0]\n            )\n            self.x_embedded = self._delay_embed(x, self.window)\n            return self.x_embedded\n\n        else:\n            head = self.x_embedded[-1, 1 - self.window:]\n\n            upd = np.concatenate([head, x])\n\n            upd_embedding = self._delay_embed(upd, self.window)\n\n            truncate_idx = np.clip(\n                self.x_embedded.shape[-1] + upd_embedding.shape[-1] - self.max_length_adj,\n                0,\n                None\n            )\n            self.x_embedded = np.concatenate(\n                [self.x_embedded[:, truncate_idx:], upd_embedding],\n                axis=1\n            )\n            return upd_embedding\n\n    def _update_svd(self):\n        """"""\n        Toeplitz variant of SSA decomposition (based on covariance matrix).\n        """"""\n        self.u, self.singular_values, self.v = np.linalg.svd(self.covariance)\n\n    @staticmethod\n    def _delay_embed(x, w):\n        """"""\n        Time-embedding with window size `w` and lag 1\n        """"""\n        g = 0\n        return x[(np.arange(w) * (g + 1)) + np.arange(np.max(x.shape[0] - (w - 1) * (g + 1), 0)).reshape(-1, 1)].T\n\n    @staticmethod\n    def _henkel_diag_average(x, n, window):\n        """"""\n        Computes  diagonal averaging operator D.\n        Usage: D = J.T.dot(B)*s, see:\n        Dimitrios D. Thomakos, `Optimal Linear Filtering, Smoothing and Trend Extraction\n        for Processes with Unit Roots and Cointegration`, 2008; pt. 2.2\n        """"""\n        J = np.ones([n - window + 1, 1])\n        h = x.shape[0]\n\n        pad = np.zeros([h, h - 1])\n        pad.fill(np.nan)\n\n        padded_x = np.r_[\'-1\', x, pad]\n        s0, s1 = padded_x.strides\n\n        B = copy.copy(\n            np.lib.stride_tricks.as_strided(padded_x, [h, n], [s0 - s1, s1], writeable=False)\n        )\n        B = np.ma.masked_array(B, mask=np.isnan(B))\n        s = 1 / np.logical_not(B.mask).sum(axis=0)\n        B[B.mask] = 0.0\n        return B.data, J, s\n\n    @staticmethod\n    def _transform(x, state):\n        """"""\n        Returns SSA decomposition w.r.t. given grouping.\n\n        Args:\n            x:      embedded vector\n            state:  instance of `SSAstate` holding fitted decomposition parameters\n        """"""\n        assert isinstance(state, SSAState),\\\n            \'Expected `state` be instance of SSAstate, got {}\'.format(type(state))\n\n        n = x.shape[-1] + state.window - 1\n\n        if state.grouping is None:\n            grouping = [[i] for i in range(state.u.shape[-1])]\n        else:\n            grouping = state.grouping\n\n        x_comp = []\n        for group in grouping:\n            d_u = state.u[:, slice(*group)]\n            d_x = d_u.dot(d_u.T).dot(x)\n            B, J, s = SSA._henkel_diag_average(d_x.T, n, state.window)\n            x_comp.append(np.squeeze(J.T.dot(B) * s))\n\n        return np.asarray(x_comp)\n\n    def get_trajectory(self, size=None):\n        """"""\n        Retrieve stored fragment of original time-series data.\n\n        Args:\n            size:   uint, fragment length in [1, ..., max_length] or None\n\n        Returns:\n            1d series as [ x[-size], x[-size+1], ... x[-1] ], up to length [size]\n            if no size arg is given - returns entire stored trajectory, up to length [max_length]\n        """"""\n        self.ready()\n        if size is not None:\n            assert 1 <= size <= self.max_length, \\\n                \'Can only retrieve from 1 up to {} last values, but got size: {}\'.format(self.max_length, size)\n        else:\n            size = self.max_length\n\n        v, h = self.x_embedded.shape\n\n        if size > v + h - 1:\n            size = v + h - 1\n\n        if size <= h:\n            i_1 = - size\n            i_0 = -1\n\n        else:\n            i_1 = None\n            i_0 = h - size - 1\n\n        return np.concatenate([self.x_embedded[i_0:-1, 0], self.x_embedded[-1, i_1:]], axis=-1)\n\n\nZscoreState = namedtuple(\'ZscoreState\', [\'mean\', \'variance\'])\n\n\nclass Zscore:\n    """"""\n    Recursive exponentially decayed mean and variance estimation for time-series\n    with arbitrary consecutive updates length.\n\n    Paper:\n    Tony Finch: ""Incremental calculation of weighted mean and variance"", notes, 2009\n    """"""\n\n    def __init__(self, dim, alpha):\n        """"""\n\n        Args:\n            dim:        observation dimensionality\n            alpha:      float, decaying factor in [0, 1]\n\n        """"""\n        self.dim = dim\n        if alpha is None:\n            self.alpha = 1\n            self.is_decayed = False\n        else:\n            self.alpha = alpha\n            self.is_decayed = True\n\n        self.mean = None\n        self.variance = None\n        self.g = None\n        self.dx = None\n        self.num_obs = 0\n\n    def get_state(self):\n        """"""\n        Convenience wrapper.\n\n        Returns:\n            current state as instance of ZscoreState tuple\n        """"""\n        return ZscoreState(\n            mean=self.mean,\n            variance=self.variance,\n        )\n\n    def reset(self, init_x):\n        """"""\n        Resets statistics estimates.\n\n        Args:\n            init_x:  np.array of initial observations of size [dim, num_init_observations]\n\n        Returns:\n            initial dimension-wise mean and variance estimates of sizes [dim, 1], [dim, 1]\n\n        """"""\n        if init_x is None:\n            self.mean = np.zeros(self.dim)\n            self.variance = np.ones(self.dim) * 1e-8\n            self.g = np.zeros(self.dim)\n            self.dx = np.zeros(self.dim)\n            self.num_obs = 1\n            if not self.is_decayed:\n                self.alpha = 1\n\n        else:\n            assert init_x.shape[0] == self.dim\n            self.mean = init_x.mean(axis=-1)\n            self.variance = init_x.var(axis=-1)\n            self.num_obs = init_x.shape[-1]\n\n            if not self.is_decayed:\n                self.alpha = 1 / (self.num_obs - 1)\n\n        return self.mean, self.variance\n\n    def update(self, x):\n        """"""\n        Updates statistics estimates.\n\n        Args:\n            x: np.array, partial trajectory of shape [dim, num_updating_points]\n\n        Returns:\n            current dimension-wise mean and variance estimates of sizes [dim, 1], [dim, 1]\n        """"""\n        assert len(x.shape) == 2 and x.shape[0] == self.dim\n\n        # Update length:\n        k = x.shape[-1]\n\n        self.num_obs += k\n        if not self.is_decayed:\n            self.alpha = 1 / (self.num_obs - 1)\n\n        # Mean estimation:\n\n        # Broadcast input to [dim, update_len, update_len]:\n        xx = np.tile(x[:, None, :], [1, k, 1])\n\n        gamma = 1 - self.alpha\n\n        # Exp. decays as powers of (1-alpha):\n        g = np.cumprod(np.repeat(gamma, k))\n\n        # Diag. matrix of decayed coeff:\n        tp = toeplitz(g / gamma, r=np.zeros(k))[::-1, ::1]\n\n        # Backward-ordered mean updates as sums of decayed inputs:\n        k_step_mean_update = np.sum(xx * tp[None, ...], axis=2)  # tp expanded for sure broadcast\n\n        # Broadcast stored value of mean to [dim, 1] and apply decay:\n        k_decayed_old_mean = (np.tile(self.mean[..., None], [1, k]) * g)\n\n        # Get backward-recursive array of mean values from (num_obs - update_len) to (num_obs):\n        means = k_decayed_old_mean + self.alpha * k_step_mean_update[:, ::-1]\n\n        # Variance estimation:\n\n        # Get deviations of update:\n        dx = x - np.concatenate([self.mean[..., None], means[:, :-1]], axis=1)\n\n        # Get new variance value at (num_obs) point:\n        k_decayed_old_var = gamma ** k * self.variance\n        k_step_var_update = np.sum(g[::-1] * dx ** 2, axis=1)\n\n        variance = k_decayed_old_var + self.alpha * k_step_var_update\n\n        # Update current values:\n        self.mean = means[:, -1]\n        self.variance = variance\n\n        # Keep g and dx:\n        self.g = g\n        self.dx = dx\n\n        return self.mean, self.variance\n\n\nCovarianceState = namedtuple(\'CovarianceState\', [\'covariance\', \'mean\', \'variance\'])\n\n\nclass Covariance:\n    """"""\n    Recursive exponentially decaying mean, variance and covariance matrix estimation for time-series\n    with arbitrary consecutive updates length.\n    """"""\n\n    def __init__(self, dim, alpha=None):\n        """"""\n\n        Args:\n            dim:        observation dimensionality\n            alpha:      float, decaying factor in [0, 1]\n\n        """"""\n        self.stat = Zscore(dim, alpha)\n        self.covariance = None\n        self.mean = self.stat.mean\n        self.variance = self.stat.variance\n\n    def get_state(self):\n        """"""\n        Convenience wrapper.\n\n        Returns:\n            current state as instance of CovarianceState tuple\n        """"""\n        return CovarianceState(\n            covariance=self.covariance,\n            mean=self.mean,\n            variance=self.variance,\n        )\n\n    def reset(self, init_x):\n        """"""\n        Resets statistics estimates.\n\n        Args:\n            init_x:   np.array of initial observations of size [dim, num_init_observations]\n\n        Returns:\n            initial covariance matrix estimate of size [dim, dim]\n            initial dimension-wise means and variances of sizes [dim, 1], [dim, 1]\n\n        """"""\n        self.mean, self.variance = self.stat.reset(init_x)\n\n        if init_x is None:\n            self.covariance = np.eye(self.stat.dim)\n\n        else:\n            self.covariance = np.cov(init_x)\n\n        return self.covariance, self.mean, self.variance\n\n    def update(self, x):\n        """"""\n        Updates statistics estimates.\n\n        Args:\n            x: np.array, partial trajectory of shape [dim, num_updating_points]\n\n        Returns:\n            current covariance matrix estimate of size [dim, dim]\n            current dimension-wise means and variances of sizes [dim, 1], [dim, 1]\n\n        """"""\n        k = x.shape[-1]\n        self.mean, self.variance = self.stat.update(x)\n        dx = self.stat.dx.T\n\n        g = self.stat.g\n\n        k_decayed_covariance = (1 - self.stat.alpha) ** k * self.covariance\n\n        k_step_update = np.sum(g[::-1, None, None] * np.matmul(dx[:, :, None], dx[:, None, :]), axis=0)\n\n        self.covariance = k_decayed_covariance + self.stat.alpha * k_step_update\n\n        return self.covariance, self.mean, self.variance\n\n\nOUEstimatorState = namedtuple(\'OUEstimatorState\', [\'mu\', \'log_theta\', \'log_sigma\'])\n\n\nclass OUEstimator:\n    """"""\n    Recursive Ornstein-Uhlenbeck process parameters estimation in exponentially decaying window\n    with arbitrary consecutive updates length.\n    """"""\n\n    def __init__(self, alpha):\n        """"""\n\n        Args:\n            alpha:  float in [0, 1], decaying window factor.\n\n        Notes:\n            alpha ~ 1 / effective_window_length;\n            parameters fitted are: Mu, Log_Theta, Log_Sigma, for process: dX = -Theta *(X - Mu) + Sigma * dW;\n        """"""\n        self.alpha = alpha\n        self.covariance_estimator = Covariance(2, alpha)\n        self.residuals_stat = Zscore(1, alpha)\n        self.residuals = None\n        self.ls_a = None\n        self.ls_b = None\n        self.mu = None\n        self.log_theta = None\n        self.log_sigma = None\n        self.x_prev = 0.0\n\n    def get_state(self):\n        """"""\n        Convenience wrapper.\n\n        Returns:\n            current state as instance of OUProcessParams tuple\n        """"""\n        return OUEstimatorState(\n            mu=self.mu,\n            log_theta=self.log_theta,\n            log_sigma=self.log_sigma\n        )\n\n    def reset(self, trajectory):\n        """"""\n        Resets estimator parameters for process dX = -Theta *(X - Mu) + Sigma * dW\n        given initial data.\n\n        Args:\n            trajectory:     initial 1D process observations trajectory of size [num_points]\n\n        Returns:\n            current estimated Mu, Theta, Sigma\n        """"""\n        # Fit trajectory:\n        x = trajectory[:-1]\n        y = trajectory[1:]\n        xy = np.stack([x, y], axis=0)\n\n        self.ls_a, self.ls_b = self.fit_ls_estimate(*self.covariance_estimator.reset(xy))\n\n        self.residuals = y - (self.ls_a * x + self.ls_b)\n\n        _, residuals_var = self.residuals_stat.reset(self.residuals[None, :])\n\n        _, self.log_theta, self.log_sigma = self.fit_ou_estimate(self.ls_a, self.ls_b, residuals_var)\n\n        self.mu = self.covariance_estimator.mean.mean()\n\n        self.x_prev = trajectory[-1]\n\n        return self.mu, self.log_theta, self.log_sigma\n\n    def update(self, trajectory, disjoint=False):\n        """"""\n        Updates OU parameters values for process dX = -Theta *(X - Mu) + Sigma * dW\n        given new observations.\n\n        Args:\n            trajectory:  1D process observations trajectory update of size [num_points]\n            disjoint:    bool, indicates whether update given is continuous or disjoint w.r.t. previous one\n\n        Returns:\n            current estimated  Mu, Theta, Sigma\n        """"""\n        if disjoint:\n            x = trajectory[:-1]\n            y = trajectory[1:]\n\n        else:\n            # continuous update, can use backed-up value:\n            x = np.concatenate([[self.x_prev], trajectory[:-1]])\n            y = trajectory\n\n        xy = np.stack([x, y], axis=0)\n\n        # Fit least squares params:\n        self.ls_a, self.ls_b = self.fit_ls_estimate(*self.covariance_estimator.update(xy))\n\n        # Get LS errors and variance:\n        self.residuals = y - (self.ls_a * x + self.ls_b)\n        _, residuals_var = self.residuals_stat.update(self.residuals[None, :])\n\n        # Get OU params:\n        _, self.log_theta, self.log_sigma = self.fit_ou_estimate(self.ls_a, self.ls_b, np.squeeze(residuals_var))\n        # Stable mean:\n        self.mu = self.covariance_estimator.mean.mean()\n\n        self.x_prev = trajectory[-1]\n\n        return self.mu, self.log_theta, self.log_sigma\n\n    @staticmethod\n    def fit_ls_estimate(sigma_xy, mean, variance):\n        """"""\n        Computes LS parameters given data covariance matrix, mean and variance: y = a*x + b + e\n\n        Args:\n            sigma_xy:   x, y covariance matrix of size [2, 2]\n            mean:       x, y mean of size [2]\n            variance:   x, y variance of size [2]\n\n        Returns:\n            fitted least squares parameters\n        """"""\n        a = (sigma_xy / np.clip((variance[0] * variance[1]) ** .5, 1e-6, None))[0, 1]\n        b = mean[1] - mean[0] * a\n\n        return np.clip(a, 1e-6, 0.999999), b\n\n    @staticmethod\n    def fit_ou_estimate(a, b, err_var, dt=1):\n        """"""\n        Given least squares parameters of data and errors variance,\n        returns parameters of OU process.\n\n        Args:\n            a:          ls slope value\n            b:          ls bias value\n            err_var:    error variance\n            dt:         time increment\n\n        Returns:\n            mu, log_theta, log_sigma\n        """"""\n        theta = float(- np.log(a) / dt)\n        mu = 0.0  # b / (1 - a)  # unstable for a --> 0\n        sigma = float((err_var * -2 * np.log(a) / (dt * (1 - a ** 2))) ** .5)\n        return mu, np.log(np.clip(theta, 1e-8, None)), np.log(np.clip(sigma, 1e-10, None))\n\n\nclass EMA:\n    """"""\n    Recursive exponentially decayed mean estimation for time-series\n    with arbitrary consecutive updates length.\n    """"""\n\n    def __init__(self, dim, alpha):\n        """"""\n\n        Args:\n            dim:        observation dimensionality\n            alpha:      float, decaying factor in [0, 1]\n\n        """"""\n        self.dim = dim\n        if alpha is None:\n            self.alpha = 1\n            self.is_decayed = False\n        else:\n            self.alpha = alpha\n            self.is_decayed = True\n\n        self.mean = None\n        self.g = None\n        self.num_obs = 0\n\n    def get_state(self):\n        """"""\n\n        Returns:\n            current mean value\n        """"""\n        return self.mean\n\n    def reset(self, init_x):\n        """"""\n        Resets statistics estimates.\n\n        Args:\n            init_x:  np.array of initial observations of size [dim, num_init_observations]\n\n        Returns:\n            initial dimension-wise mean estimates of sizes [dim, 1]\n\n        """"""\n        if init_x is None:\n            self.mean = np.zeros(self.dim)\n            self.g = np.zeros(self.dim)\n            self.num_obs = 1\n            if not self.is_decayed:\n                self.alpha = 1\n\n        else:\n            assert init_x.ndim == 2 and init_x.shape[0] == self.dim, \\\n                \'Expected init. value as 2D array of size: [{}, num_init_points], got: {}\'.format(self.dim,\n                                                                                                  init_x.shape)\n            self.mean = init_x.mean(axis=-1)\n            self.num_obs = init_x.shape[-1]\n\n            if not self.is_decayed:\n                self.alpha = 1 / (self.num_obs - 1)\n\n        return self.mean[:, None]\n\n    def update(self, x):\n        """"""\n        Updates statistics estimates.\n\n        Args:\n            x: np.array, partial trajectory of shape [dim, num_updating_points]\n\n        Returns:\n            current dimension-wise mean estimates of size [dim, num_updating_points]\n        """"""\n        assert x.ndim == 2 and x.shape[0] == self.dim, \\\n            \'Expected update value as 2D array of size: [{}, num_points], got: {}\'.format(self.dim, x.shape)\n\n        # Update length:\n        k = x.shape[-1]\n\n        self.num_obs += k\n        if not self.is_decayed:\n            self.alpha = 1 / (self.num_obs - 1)\n\n        # Mean estimation:\n\n        # Broadcast input to [dim, update_len, update_len]:\n        xx = np.tile(x[:, None, :], [1, k, 1])\n\n        gamma = 1 - self.alpha\n\n        # Exp. decays as powers of (1-alpha):\n        g = np.cumprod(np.repeat(gamma, k))\n\n        # Diag. matrix of decayed coeff:\n        tp = toeplitz(g / gamma, r=np.zeros(k))[::-1, ::1]\n\n        # Backward-ordered mean updates as sums of decayed inputs:\n        k_step_mean_update = np.sum(xx * tp[None, ...], axis=2)  # tp expanded for sure broadcast\n\n        # Broadcast stored value of mean to [dim, 1] and apply decay:\n        k_decayed_old_mean = (np.tile(self.mean[..., None], [1, k]) * g)\n\n        # Get backward-recursive array of mean values from (num_obs - update_len) to (num_obs):\n        means = k_decayed_old_mean + self.alpha * k_step_mean_update[:, ::-1]\n\n        self.g = g\n\n        # Update current value:\n        self.mean = means[:, -1]\n\n        return means\n\n\nSTEstimatorState = namedtuple(\'STEstimatorState\', [\'df\', \'loc\', \'scale\'])\n\n\nclass STEstimator:\n    """"""\n    Standart student-t parameters estimation.\n    """"""\n    # TODO: implement true recursive decaying algorithm\n\n    def __init__(self, alpha):\n        """"""\n\n        Args:\n            alpha:  float in [0, 1], decaying window factor.\n\n        Notes:\n            alpha ~ 1 / effective_window_size;\n            parameters fitted are: df, loc, scale  - degree of freedom\n        """"""\n        assert alpha is not None and 0 < alpha <= 1.0,\\\n            ""expected alpha as float in [0, 1], got: {}"".format(alpha)\n\n        self.alpha = alpha\n        # Half-effective tracking window:\n        self.window_size = int(np.clip(.5//alpha, 2, None))\n        self.trajectory = np.zeros(self.window_size)\n        self.mask_idx = 2\n        self.df = None\n        self.loc = None\n        self.scale = None\n\n    def get_state(self):\n        """"""\n        Convenience wrapper.\n\n        Returns:\n            current state as instance of STEstimatorState tuple\n        """"""\n        return STEstimatorState(\n            df=self.df,\n            loc=self.loc,\n            scale=self.scale\n        )\n\n    def fit(self, trajectory=None):\n        """"""\n        Fits parameters to currently stored or provided data.\n\n        Args:\n            trajectory:     array_like, data to fit or None\n\n        Returns:\n            fitted parameters: fd, loc, state\n        """"""\n        if trajectory is None:\n            if self.df is None:\n                self.df, self.loc, self.scale = student_t.fit(self.trajectory[-self.mask_idx:])\n                self.df = np.clip(self.df, 3, None)\n\n            return self.df, self.loc, self.scale\n\n        else:\n            df, loc, scale = student_t.fit(trajectory)\n            df = np.clip(df, 3, None)\n\n            return df, loc, scale\n\n    def reset(self, init_trajectory):\n        """"""\n        Resets estimator trajectory and parameters with initial data.\n\n        Args:\n            init_trajectory:     initial 1D process observations trajectory of size [num_points]\n        """"""\n        assert init_trajectory.ndim == 1, \'Expected 1D data, got shape: {}\'.format(init_trajectory.shape)\n        self.trajectory = np.zeros(self.window_size)\n        self.mask_idx = np.clip(len(init_trajectory), None, self.window_size)\n        self.trajectory[-self.mask_idx:] = init_trajectory[-self.mask_idx:]\n\n        self.df = None\n        self.loc = None\n        self.scale = None\n\n    def update(self, trajectory):\n        """"""\n        Updates stored trajectory with new observations.\n\n        Args:\n            trajectory:  1D process observations trajectory update of size [num_points]\n\n        """"""\n        assert trajectory.ndim == 1, \'Expected 1D data, got shape: {}\'.format(trajectory.shape)\n        upd_len = np.clip(len(trajectory), None, self.window_size)\n        self.mask_idx = np.clip(self.mask_idx + upd_len, None, self.window_size)\n        self.trajectory = np.concatenate([self.trajectory[upd_len:], trajectory[-upd_len:]])\n\n        self.df = None\n        self.loc = None\n        self.scale = None\n\n\n\n\n\n\n'"
btgym/research/model_based/model/stochastic.py,0,"b'import numpy as np\nfrom scipy.stats import norm\nfrom btgym.research.model_based.model.utils import log_uniform, multivariate_t_rvs, cov2corr\n\n\ndef weiner_process_fn(num_points, delta, x0=0, dt=1):\n    """"""\n    Generates Weiner process realisation trajectory.\n\n    Args:\n        num_points:     int, trajectory length;\n        delta:          float, speed parameter;\n        x0:             float, starting point;\n        dt:             int, time increment;\n\n    Returns:\n        generated data as 1D np.array\n    """"""\n    x0 = np.asarray(x0)\n    r = norm.rvs(size=x0.shape + (num_points,), scale=delta * (dt**.5))\n\n    return np.cumsum(r, axis=-1) + np.expand_dims(x0, axis=-1)\n\n\ndef weiner_process_uniform_parameters_fn(delta, x0, dt=1):\n    """"""\n    Provides parameters for Weiner process.\n    If parameter is set as iterable of form [a, b] - uniformly randomly samples parameters value\n    form given interval.\n\n    Args:\n        delta:          float or iterable of 2 floats, speed parameter;\n        x0:             float or iterable of 2 floats, starting point;\n\n    Returns:\n\n    """"""\n    if type(delta) in [int, float, np.float64]:\n        delta = [delta, delta]\n    else:\n        delta = list(delta)\n\n    if type(x0) in [int, float, np.float64]:\n        x0 = [x0, x0]\n    else:\n        x0 = list(x0)\n\n    assert len(delta) == 2 and 0 <= delta[0] <= delta[-1], \\\n        \'Expected Weiner delta be non-negative float or ordered interval, got: {}\'.format(delta)\n\n    assert len(x0) == 2 and 0 <= x0[0] <= x0[-1], \\\n        \'Expected Weiner starting x0 be non-negative float or ordered interval, got: {}\'.format(x0)\n\n    delta_value = np.random.uniform(low=delta[0], high=delta[-1])\n    x0_value = np.random.uniform(low=x0[0], high=x0[-1])\n\n    return dict(\n        delta=delta_value,\n        x0=x0_value\n    )\n\n\ndef ornshtein_uhlenbeck_process_fn(num_points, mu, l, sigma, x0=0, dt=1):\n    """"""\n    Generates Ornshtein-Uhlenbeck process realisation trajectory.\n\n    Args:\n        num_points:     int, trajectory length\n        mu:             float, mean;\n        l:              float, lambda, mean reversion rate;\n        sigma:          float, volatility;\n        x0:             float, starting point;\n        dt:             int, time increment;\n\n    Returns:\n        generated data as 1D np.array\n    """"""\n    # print(\'OU_p_fn got:: l: {}, sigma: {}, mu: {}\'.format(l, sigma, mu))\n\n    n = num_points + 1\n    x = np.zeros(n)\n    x[0] = x0\n    for i in range(1, n):\n        x[i] = x[i - 1] * np.exp(-l * dt) + mu * (1 - np.exp(-l * dt)) + \\\n               sigma * ((1 - np.exp(-2 * l * dt)) / (2 * l)) ** .5 * np.random.normal(0, 1)\n\n    return x[1:]\n\n\ndef ornshtein_uhlenbeck_process_batch_fn(num_points, mu, l, sigma, x0, dt=1):\n    """"""\n    Generates batch of Ornshtein-Uhlenbeck process realisation trajectories.\n\n    Args:\n        num_points:     int, trajectory length\n        mu:             float or array of shape [batch_dim], mean;\n        l:              float or array of shape [batch_dim], lambda, mean reversion rate;\n        sigma:          float or array of shape [batch_dim], volatility;\n        x0:             float or array of shape [batch_dim], starting point;\n        dt:             int, time increment;\n\n    Returns:\n        generated data as np.array of shape [batch_dim, num_points]\n    """"""\n    # print(\'OU_p_fn got:: l: {}, sigma: {}, mu: {}\'.format(l, sigma, mu))\n\n    n = num_points + 1\n    try:\n        batch_dim = x0.shape[0]\n        x = np.zeros([n, batch_dim])\n        x[0, :] = np.squeeze(x0)\n    except (AttributeError, IndexError) as e:\n        batch_dim = None\n        x = np.zeros([n, 1])\n        x[0, :] = x0\n\n    for i in range(1, n):\n        x[i, :] = x[i - 1, :] * np.exp(-l * dt) + mu * (1 - np.exp(-l * dt)) + \\\n               sigma * ((1 - np.exp(-2 * l * dt)) / (2 * l)) ** .5 * np.random.normal(0, 1, size=batch_dim)\n\n    return x[1:, :]\n\n\ndef ou_process_t_driver_batch_fn(num_points, mu, l, sigma, df, x0, dt=1):\n    """"""\n    Generates batch of realisation trajectories of Ornshtein-Uhlenbeck process\n    driven by t-distributed innovations.\n\n    Args:\n        num_points:     int, trajectory length\n        mu:             float or array of shape [batch_dim], mean;\n        l:              float or array of shape [batch_dim], lambda, mean reversion rate;\n        sigma:          float or array of shape [batch_dim], volatility;\n        df:             float or array of shape [batch_dim] > 2.0, standart Student-t degrees of freedom param.;\n        x0:             float or array of shape [batch_dim], starting point;\n        dt:             int, time increment;\n\n    Returns:\n        generated data as np.array of shape [batch_dim, num_points]\n    """"""\n\n    n = num_points + 1\n    try:\n        batch_dim = x0.shape[0]\n        x = np.zeros([n, batch_dim])\n        x[0, :] = np.squeeze(x0)\n    except (AttributeError, IndexError) as e:\n        batch_dim = None\n        x = np.zeros([n, 1])\n        x[0, :] = x0\n\n    for i in range(1, n):\n        driver = np.random.standard_t(df, size=df.size) * ((df - 2) / df) ** .5\n        # driver = stats.t.rvs(df, loc, scale, size=batch_dim)\n        # x_vol = df / (df - 2)\n        # driver = (driver - loc) / scale / x_vol**.5\n        x[i, :] = x[i - 1, :] * np.exp(-l * dt) + mu * (1 - np.exp(-l * dt)) + \\\n            sigma * ((1 - np.exp(-2 * l * dt)) / (2 * l)) ** .5 * driver\n\n    return x[1:, :]\n\n\ndef multivariate_ou_process_t_driver_batch_fn(batch_size, num_points, mu, theta, sigma, cov, df, x0, dt=1):\n    """"""\n    Generates batch of realisations of multivariate Ornshtein-Uhlenbeck process\n    driven by t-distributed innovations.\n\n    Args:\n        batch_size:     int, batch_size\n        num_points:     int, trajectory length\n        mu:             array of shape [process_dim], process mean;\n        theta:          array of shape [process_dim], mean reversion rate;\n        sigma:          array of shape [process_dim], process deviation parameter;\n        cov:            array of shape [process_dim, process_dim],\n                        covariance or correlation matrix of process innovations\n        df:             array of shape [process_dim] > 2.0, Student-t degree of freedom params.;\n        x0:             array of shape [process_dim], starting points;\n        dt:             int, time increment;\n\n    Returns:\n        generated data as np.array of size: [batch_size,  num_points, process_dim]\n    """"""\n    assert mu.shape == theta.shape == sigma.shape == df.shape == x0.shape and mu.ndim == 1, \\\n        \'Expected parameters mu, theta, sigma, df, x0 as 1d vectors o same size...\'\n\n    dim = mu.shape[0]\n    n = num_points + 1\n\n    x = np.zeros([batch_size, n, dim])\n    x[:, 0, :] = x0[None, :]\n\n    # Normalize covariance:\n    rho = cov2corr(cov)\n\n    # Get multivariate t-innovations scaled to unit variance:\n    driver = multivariate_t_rvs(mean=np.zeros(dim), cov=rho, df=df, size=[batch_size, n]) * ((df - 2) / df) ** .5\n\n    # Get trajectory:\n    for i in range(1, n):\n        x[:, i, :] = x[:, i - 1, :] * np.exp(-theta * dt) + mu * (1 - np.exp(-theta * dt))\n\n        # Scale to OU innovation amplitude:\n        innovation = sigma * ((1 - np.exp(-2 * theta * dt)) / (2 * theta)) ** .5 * driver[:, i, :]\n\n        x[:, i, :] += innovation\n\n    return x[:, 1:, :]\n\n\ndef ornshtein_uhlenbeck_uniform_parameters_fn(mu, l, sigma, x0=None, dt=1):\n    """"""\n    Provides parameters for OU process.\n    If parameter is set as iterable of form [a, b] - uniformly randomly samples parameters value\n    form given interval.\n\n    Args:\n        mu:             float or iterable of 2 floats, mean;\n        l:              float or iterable of 2 floats, lambda, mean reversion rate;\n        sigma:          float or iterable of 2 floats, volatility;\n        x0:             float or iterable of 2 floats, starting point;\n        dt:             not used | int, time increment;\n\n    Returns:\n        dictionary of sampled values\n    """"""\n    if type(l) in [int, float, np.float64]:\n        l = [l, l]\n    else:\n        l = list(l)\n\n    if type(sigma) in [int, float, np.float64]:\n        sigma = [sigma, sigma]\n    else:\n        sigma = list(sigma)\n\n    if type(mu) in [int, float, np.float64]:\n        mu = [mu, mu]\n    else:\n        mu = list(mu)\n\n    # Sanity checks:\n    assert len(l) == 2 and 0 < l[0] <= l[-1], \\\n        \'Expected OU mean reversion rate be positive float or ordered interval, got: {}\'.format(l)\n    assert len(sigma) == 2 and 0 <= sigma[0] <= sigma[-1], \\\n        \'Expected OU sigma be non-negative float or ordered interval, got: {}\'.format(sigma)\n    assert len(mu) == 2 and mu[0] <= mu[-1], \\\n        \'Expected OU mu be float or ordered interval, got: {}\'.format(mu)\n\n    # Uniformly sample params:\n    l_value = np.random.uniform(low=l[0], high=l[-1])\n    sigma_value = np.random.uniform(low=sigma[0], high=sigma[-1])\n    mu_value = np.random.uniform(low=mu[0], high=mu[-1])\n\n    if x0 is None:\n        # Choose starting point equal to mean:\n        x0_value = mu_value\n\n    else:\n        if type(x0) in [int, float, np.float64]:\n            x0 = [x0, x0]\n        else:\n            x0 = list(x0)\n\n        assert len(x0) == 2 and x0[0] <= x0[-1], \\\n            \'Expected OU x0 be float or ordered interval, got: {}\'.format(x0)\n\n        x0_value = np.random.uniform(low=x0[0], high=x0[-1])\n\n    # print(\'OU_params_fn sample intervals:: l: {}, sigma: {}, mu: {}, x0: {}\'.format(l, sigma, mu, x0))\n    # print(\'OU_params_fn passed:: l: {}, sigma: {}, mu: {}, x0: {}\'.format(l_value, sigma_value, mu_value, x0_value))\n\n    return dict(\n        l=l_value,\n        sigma=sigma_value,\n        mu=mu_value,\n        x0=x0_value,\n        #dt=dt\n    )\n\n\ndef ornshtein_uhlenbeck_log_uniform_parameters_fn(mu, l, sigma, x0=None, dt=1):\n    """"""\n    Provides parameters for OU process.\n    If `mu`, `sigma` is set as iterable of form [a, b] - uniformly randomly samples parameters value\n    form given interval; `l` is sampled from log-uniform distribution\n\n\n    Args:\n        mu:             float or iterable of 2 floats, mean;\n        l:              float or iterable of 2 floats, lambda, mean reversion rate;\n        sigma:          float or iterable of 2 floats, volatility;\n        x0:             float or iterable of 2 floats, starting point;\n        dt:             not used | int, time increment;\n\n    Returns:\n        dictionary of sampled values\n    """"""\n    if type(l) in [int, float, np.float64]:\n        l = [l, l]\n    else:\n        l = list(l)\n\n    if type(sigma) in [int, float, np.float64]:\n        sigma = [sigma, sigma]\n    else:\n        sigma = list(sigma)\n\n    if type(mu) in [int, float, np.float64]:\n        mu = [mu, mu]\n    else:\n        mu = list(mu)\n\n    # Sanity checks:\n    assert len(l) == 2 and 0 < l[0] <= l[-1], \\\n        \'Expected OU mean reversion rate be positive float or ordered interval, got: {}\'.format(l)\n    assert len(sigma) == 2 and 0 <= sigma[0] <= sigma[-1], \\\n        \'Expected OU sigma be non-negative float or ordered interval, got: {}\'.format(sigma)\n    assert len(mu) == 2 and mu[0] <= mu[-1], \\\n        \'Expected OU mu be float or ordered interval, got: {}\'.format(mu)\n\n    # Uniformly sample params:\n    l_value = log_uniform(l, 1)\n    sigma_value = np.random.uniform(low=sigma[0], high=sigma[-1])\n    mu_value = np.random.uniform(low=mu[0], high=mu[-1])\n\n    if x0 is None:\n        # Choose starting point equal to mean:\n        x0_value = mu_value\n\n    else:\n        if type(x0) in [int, float, np.float64]:\n            x0 = [x0, x0]\n        else:\n            x0 = list(x0)\n\n        assert len(x0) == 2 and x0[0] <= x0[-1], \\\n            \'Expected OU x0 be float or ordered interval, got: {}\'.format(x0)\n\n        x0_value = np.random.uniform(low=x0[0], high=x0[-1])\n\n    # print(\'OU_params_fn sample intervals:: l: {}, sigma: {}, mu: {}, x0: {}\'.format(l, sigma, mu, x0))\n    # print(\'OU_params_fn passed:: l: {}, sigma: {}, mu: {}, x0: {}\'.format(l_value, sigma_value, mu_value, x0_value))\n\n    return dict(\n        l=l_value,\n        sigma=sigma_value,\n        mu=mu_value,\n        x0=x0_value,\n        #dt=dt\n    )\n\n\ndef coupled_wave_pair_generator_fn(\n        num_points,\n        drift_sigma,\n        ou_sigma,\n        ou_lambda,\n        ou_mu,\n        spread_sigma_1,\n        spread_sigma_2,\n        spread_mean_1,\n        spread_mean_2,\n        bias,\n        keep_decimals=6,\n):\n    """"""\n    Generates two integrated trajectories of OHLC prices.\n    Prices are modelled by OU process with stochastic drift;\n    High-Low spread values for each price line independently generated by \'coupled wave model\',\n    see  formulae (18a-c) - (20),  pp. 10-11 in:\n    ""Spread, volatility, and volume relationship in\n    financial markets and market maker\xe2\x80\x99s profit optimization"" by Jack Sarkissian, 2016;\n    https://arxiv.org/abs/1606.07381\n\n    Args:\n        num_points:         int, trajectory length\n        drift_sigma:        ufloat, stichastic drift sigma\n        ou_sigma:           ufloat, base OU process sigma\n        ou_lambda:          ufloat, base OU mean-reverting speed parameter\n        ou_mu:              float, base OU mean parameter\n        spread_sigma_1:     ufloat, Hi-Lo spread generating sigma1\n        spread_sigma_2:     ufloat, Hi-Lo spread generating sigma2\n        spread_mean_1:      float, Hi-Lo spread generating mean1\n        spread_mean_2:      float, Hi-Lo spread generating mean2\n        bias:               ufloat, process starting point\n        keep_decimals:      uint, number of decimal places to keep in generated data\n\n    Returns:\n        3d array of generated values of shape [2, 4, num_points]\n    """"""\n    # Price iteration by ""coupled-wave model"", formulae (18a-c) - (20),  pp. 10-11:\n    s_mid = lambda s_last, sigma: s_last * (1 + np.random.normal(loc=0.0, scale=sigma, size=None))\n\n    h = lambda sigma1, sigma2, mean1, mean2: np.clip((\n        np.random.normal(loc=mean1, scale=sigma1, size=None) ** 2 +\n        np.random.normal(loc=mean2, scale=sigma2, size=None) ** 2\n         ) ** .5, mean1, None)\n\n    s_low = lambda x_mid, h_val: x_mid - h_val / 2\n\n    s_high = lambda x_mid, h_val: x_mid + h_val / 2\n\n    s_last = lambda x_low, x_high: np.random.uniform(low=x_low, high=x_high, size=None)\n\n    delta_ou = lambda s, l, sigma: (ou_mu - s) * (1 - np.exp(-l)) + sigma * (\n                (1 - np.exp(-2 * l)) / (2 * l)) ** .5 * np.random.normal(0, 1)\n\n    x_mid1 = [bias]\n    x_low1 = [bias]\n    x_high1 = [bias]\n    x_last1 = [bias + ou_mu/2]\n    x_mid2 = [bias]\n    x_low2 = [bias]\n    x_high2 = [bias]\n    x_last2 = [bias - ou_mu/2]\n\n    # Generate trajectory:\n    for i in range(num_points):\n        x_last_ou = (x_last1[-1] - x_last2[-1])\n\n        d_s = delta_ou(x_last_ou, ou_lambda, ou_sigma)\n\n        drift1 = np.random.normal(loc=0.0, scale=drift_sigma, size=None)\n        #drift2 = np.random.normal(loc=0.0, scale=drift_sigma, size=None)\n\n        x_mid1.append(x_last1[-1] * (1 + drift1) + d_s / 2)\n        x_mid2.append(x_last2[-1] * (1 + drift1) - d_s / 2)\n\n        #     dd = np.random.uniform(0,1)\n        #     x_mid1.append(x_last1[-1] + d_s*(1-dd))\n        #     x_mid2.append(x_last2[-1] - d_s*dd)\n\n        h1_val = h(spread_sigma_1, spread_sigma_2, spread_mean_1, spread_mean_2)\n        h2_val = h(spread_sigma_1, spread_sigma_2, spread_mean_1, spread_mean_2)\n\n        x_low1.append(s_low(x_mid1[-1], h1_val))\n        x_high1.append(s_high(x_mid1[-1], h1_val))\n        x_last1.append(s_last(x_low1[-1], x_high1[-1]))\n\n        x_low2.append(s_low(x_mid2[-1], h2_val))\n        x_high2.append(s_high(x_mid2[-1], h2_val))\n        x_last2.append(s_last(x_low2[-1], x_high2[-1]))\n\n    x = np.asarray([[x_mid1, x_high1, x_low1, x_last1], [x_mid2, x_high2, x_low2, x_last2]])[:, :, 1:]\n\n    return np.around(x, decimals=keep_decimals)\n\n\n'"
btgym/research/model_based/model/univariate.py,0,"b'import numpy as np\nfrom scipy import stats\nfrom collections import namedtuple\n\nfrom btgym.research.model_based.model.stochastic import ou_process_t_driver_batch_fn\nfrom btgym.research.model_based.model.stochastic import multivariate_ou_process_t_driver_batch_fn\n\nfrom btgym.research.model_based.model.rec import Zscore, ZscoreState, Covariance, CovarianceState\nfrom btgym.research.model_based.model.rec import SSA, OUEstimator, OUEstimatorState, STEstimator\n\nOUProcessState = namedtuple(\'OUProcessState\', [\'observation\', \'filtered\', \'driver_df\'])\n\n\nclass OUProcess:\n    """"""\n    Provides essential functionality for recursive time series modeling\n    as Ornshteinh-Uhlenbeck stochastic process:\n    parameters estimation, state filtering and sampling, trajectories generation.\n    """"""\n    def __init__(self, alpha=None, filter_alpha=None):\n        self.alpha = alpha\n        self.filter_alpha = filter_alpha\n        self.estimator = OUEstimator(alpha)\n\n        # Just use exponential smoothing as state-space trajectory filter:\n        self.filter = Covariance(3, alpha=filter_alpha)\n\n        # Driver is Student-t:\n        self.driver_estimator = STEstimator(alpha)\n\n        # Empirical statistics tracker (debug, mostly for accuracy checking, not included in OUProcessState):\n        self.stat = Zscore(1, alpha)\n\n        self.is_ready = False\n\n    def ready(self):\n        assert self.is_ready, \'OUProcess is not initialized. Hint: forgot to call .reset()?\'\n\n    def get_state(self):\n        """"""\n        Returns model state tuple.\n\n        Returns:\n            current state as instance of OUProcessState\n        """"""\n        self.ready()\n        return OUProcessState(\n            observation=self.estimator.get_state(),\n            filtered=self.filter.get_state(),\n            driver_df=self.driver_estimator.df,\n        )\n\n    @staticmethod\n    def get_random_state(mu=(0, 0), theta=(.1, 1), sigma=(0.1, 1), driver_df=(3, 50), variance=1e-2):\n        """"""\n        Samples random uniform process state w.r.t. parameters intervals given.\n\n        Args:\n            mu:         iterable of floats as [lower_bound, upper_bound], OU Mu sampling interval\n            theta:      iterable of positive floats as [lower_bound, upper_bound], OU Theta sampling interval\n            sigma:      iterable of positive floats as [lower_bound, upper_bound], OU Sigma sampling interval\n            driver_df:  iterable of positive floats as [lower_bound > 2, upper_bound],\n                        student-t driver degrees of freedom sampling interval\n            variance:   filtered observation variance (same and fixed for all params., covariance assumed diagonal)\n\n        Returns:\n            instance of OUProcessState\n        """"""\n        # TODO: random log-uniform sampling for mu, theta, sigma i.f.f. log_prices are used as in BivariatePriceModel\n        sample = dict()\n        for name, param, low_threshold in zip(\n                [\'mu\', \'theta\', \'sigma\', \'driver_df\'],\n                [mu, theta, sigma, driver_df],\n                [-np.inf, 1e-8, 1e-8, 2.999],\n        ):\n            interval = np.asarray(param)\n            assert interval.ndim == 1 and interval[0] <= interval[-1], \\\n                \' Expected param `{}` as iterable of ordered values as: [lower_bound, upper_bound], got: {}\'.format(\n                    name, interval\n                )\n            assert interval[0] > low_threshold, \\\n                \'Expected param `{}` lower bound be bigger than {}, got: {}\'.format(name, low_threshold, interval[0])\n            sample[name] = np.random.uniform(low=interval[0], high=interval[-1])\n\n        observation = OUEstimatorState(\n            mu=sample[\'mu\'],\n            log_theta=np.log(sample[\'theta\']),\n            log_sigma=np.log(sample[\'sigma\'])\n        )\n        filtered = CovarianceState(\n            mean=np.asarray(observation),\n            variance=np.ones(3) * variance,\n            covariance=np.eye(3) * variance,\n        )\n        return OUProcessState(\n            observation=observation,\n            filtered=filtered,\n            driver_df=sample[\'driver_df\'],\n        )\n\n    def fit_driver(self, trajectory=None):\n        """"""\n        Updates Student-t driver shape parameter. Needs entire trajectory for correct estimation.\n        TODO: make recursive update.\n\n        Args:\n            trajectory: full observed data of size ~[max_length] or None\n\n        Returns:\n            Estimated shape parameter.\n        """"""\n        self.ready()\n        driver_df, _, _ = self.driver_estimator.fit(trajectory)\n\n        return driver_df\n\n    def reset(self, init_trajectory):\n        """"""\n        Resets model parameters for process dX = -Theta *(X - Mu) + Sigma * dW\n        and starts new trajectory given initial data.\n\n        Args:\n            init_trajectory:    initial 1D process observations trajectory of size [num_points]\n        """"""\n        _ = self.stat.reset(init_trajectory[None, :])\n\n        init_observation = np.asarray(self.estimator.reset(init_trajectory))\n        # 2x observation to get initial covariance matrix estimate:\n        init_observation = np.stack([init_observation, init_observation], axis=-1)\n        _ = self.filter.reset(init_observation)\n\n        self.driver_estimator.reset(self.estimator.residuals)\n        self.is_ready = True\n\n    def update(self, trajectory, disjoint=False):\n        """"""\n        Updates model parameters estimates for process dX = -Theta *(X - Mu) + Sigma * dW\n        given new observations.\n\n        Args:\n            trajectory:  1D process observations trajectory update of size [num_points]\n            disjoint:    bool, indicates whether update given is continuous or disjoint w.r.t. previous one\n        """"""\n        self.ready()\n        _ = self.stat.update(trajectory[None, :])  # todo: disjoint is ignored or reset stat?\n\n        # Get new state-space observation:\n        observation = self.estimator.update(trajectory, disjoint)\n\n        # Smooth and make it random variable:\n        _ = self.filter.update(np.asarray(observation)[:, None])\n\n        # Residuals distr. shape update but do not fit:\n        self.driver_estimator.update(self.estimator.residuals)\n\n    @staticmethod\n    def sample_from_filtered(filter_state, size=1):\n        """"""\n        Samples process parameters values given smoothed observations.\n        Static method, can be used as stand-along function.\n\n        Args:\n            filter_state:  instance of CovarianceState of dimensionality 3\n            size:          int or None, number of samples to draw\n\n        Returns:\n            sampled process parameters of size [size] each, packed as OUEstimatorState tuple\n\n        """"""\n        assert isinstance(filter_state, CovarianceState),\\\n            \'Expected filter_state as instance of CovarianceState, got: {}\'.format(type(filter_state))\n\n        sample = np.random.multivariate_normal(filter_state.mean, filter_state.covariance, size=size)\n\n        return OUEstimatorState(\n            mu=sample[:, 0],\n            log_theta=sample[:, 1],\n            log_sigma=sample[:, 2],\n        )\n\n    @staticmethod\n    def sample_naive_unbiased(state, size=1):\n        """"""\n        Samples process parameters values given observed values and smoothed covariance.\n        Static method, can be used as stand-along function.\n\n        Args:\n            state:  instance of OUProcessState\n            size:   int or None, number of samples to draw\n\n        Returns:\n            sampled process parameters of size [size] each, packed as OUEstimatorState tuple\n\n        """"""\n        assert isinstance(state, OUProcessState), \\\n            \'Expected filter_state as instance of `OUProcessState`, got: {}\'.format(type(state))\n\n        # naive_mean = (np.asarray(state.observation) + state.filtered.mean) / 2\n        naive_mean = np.asarray(state.observation)\n        sample = np.random.multivariate_normal(naive_mean, state.filtered.covariance, size=size)\n\n        return OUEstimatorState(\n            mu=sample[:, 0],\n            log_theta=sample[:, 1],\n            log_sigma=sample[:, 2],\n        )\n\n    def sample_parameters(self, state=None, size=1):\n        """"""\n        Samples process parameters values given process state;\n\n        Args:\n            state:  instance of OUProcessState or None;\n                    if no state provided - current state is used;\n            size:   number of samples to draw;\n\n        Returns:\n            sampled process parameters of size [size] each, packed as OUEstimatorState tuple\n        """"""\n        if state is None:\n            state = self.get_state()\n\n        else:\n            assert isinstance(state, OUProcessState),\\\n                \'Expected state as instance of OUProcessState, got: {}\'.format(type(state))\n\n        # return self.sample_from_filtered(state.filtered, size=size)\n        return self.sample_naive_unbiased(state, size=size)\n\n    @staticmethod\n    def generate_trajectory_fn(batch_size, size, parameters, t_df):\n        """"""\n        Generates batch of univariate process realisations given process parameters.\n        Static method, can be used as stand-along function.\n\n        Args:\n            batch_size:     uint, number of trajectories to generates\n            size:           uint, trajectory length to generate\n            parameters:     instance of OUEstimatorState of size [batch_size] for each parameter\n            t_df:           float > 3.0, driver shape param.\n\n        Returns:\n            process realisations as 2d array of size [batch_size, size]\n        """"""\n        assert isinstance(parameters, OUEstimatorState), \\\n            \'Expected `parameters` as instance of OUEstimatorState, got: {}\'.format(type(parameters))\n\n        for param in parameters:\n            assert param.shape[0] == batch_size,\\\n                \'Given `parameters` length: {} and `batch_size`: {} does not match.\'.format(param.shape[0], batch_size)\n\n        if isinstance(t_df, float) or isinstance(t_df, int):\n            t_df = np.tile(t_df, batch_size)\n\n        else:\n            assert t_df.shape[0] == batch_size, \\\n                \'Given `t_df` parameters length: {} and `batch_size`: {} does not match.\'.format(t_df.shape[0], batch_size)\n\n        trajectory = ou_process_t_driver_batch_fn(\n            size,\n            mu=parameters.mu,\n            l=np.exp(parameters.log_theta),\n            sigma=np.exp(parameters.log_sigma),\n            df=t_df,\n            x0=parameters.mu,\n        )\n        return trajectory.T\n\n    @staticmethod\n    def generate_multivariate_trajectory_fn(batch_size, size, parameters, t_df, covariance):\n        """"""\n        Generates batch of realisations of multivariate Ornshtein-Uhlenbeck process.\n        Note differences in parameters dimensionality w.r.t. univarite case!\n        Static method, can be used as stand-along function.\n\n        Args:\n            batch_size:     uint, number of trajectories to generates\n            size:           uint, trajectory length to generate\n            parameters:     instance of OUEstimatorState of size [process_dim] for each parameter\n            t_df:           array_like, driver shape param. vector of size [process_dim]\n            covariance:     process innovations covariance matrix of size [process_dim, process_dim]\n\n        Returns:\n            process realisations as array of size [batch_size, size, process_dim]\n        """"""\n        assert isinstance(parameters, OUEstimatorState), \\\n            \'Expected `parameters` as instance of OUEstimatorState, got: {}\'.format(type(parameters))\n\n        trajectory = multivariate_ou_process_t_driver_batch_fn(\n            batch_size=batch_size,\n            num_points=size,\n            mu=parameters.mu,\n            theta=np.exp(parameters.log_theta),\n            sigma=np.exp(parameters.log_sigma),\n            cov=covariance,\n            df=t_df,\n            x0=parameters.mu,\n        )\n        return trajectory\n\n    def generate(self, batch_size, size, state=None, driver_df=None):\n        """"""\n        Generates batch of realisations given process state.\n\n        Args:\n            batch_size:     uint, number of trajectories to generates\n            size:           uint, trajectory length to generate\n            state:          instance of OUProcessState or None;\n                            if no state provided - current state is used.\n            driver_df:      t-student process driver degree of freedom parameter or None;\n                            if no value provided - current value is used;\n\n        Returns:\n            process realisations of size [batch_size, size]\n\n        """"""\n        self.ready()\n        parameters = self.sample_parameters(state, size=batch_size)\n\n        if driver_df is None:\n            driver_df, _, _ = self.driver_estimator.fit()\n        print(\'driver_df: \', driver_df)\n        return self.generate_trajectory_fn(batch_size, size, parameters, driver_df)\n\n\nTimeSeriesModelState = namedtuple(\'TimeSeriesModelState\', [\'process\', \'analyzer\'])\n\n\nclass TimeSeriesModel:\n    """"""\n    Base time-series modeling and decomposition wrapper class.\n\n    Consist of two [independent] functional parts:\n    - stochastic process modeling (fitting and tracking of unobserved parameters, new data generation);\n    - realisation trajectory analysis and decomposition;\n\n    Note that there this base class does not include data pre-processing, methods (normalisation, log_transform etc.).\n    Later should be added at user_level child classes.\n\n    """"""\n\n    def __init__(self, max_length, analyzer_window, analyzer_grouping=None, alpha=None, filter_alpha=None):\n        """"""\n\n        Args:\n            max_length:         uint, maximum trajectory length to keep;\n            analyzer_window:    uint, SSA embedding window;\n            analyzer_grouping:  SSA decomposition triples grouping,\n                                iterable of pairs convertible to python slices, i.e.:\n                                grouping=[[0,1], [1,2], [2, None]];\n            alpha:              float in [0, 1], SSA and process estimator decaying factor;\n            filter_alpha:       float in [0, 1], process smoothing decaying factor;\n        """"""\n        self.process = OUProcess(alpha=alpha, filter_alpha=filter_alpha)\n        self.analyzer = SSA(window=analyzer_window, max_length=max_length, grouping=analyzer_grouping, alpha=alpha)\n\n    def get_state(self):\n        return TimeSeriesModelState(\n            process=self.process.get_state(),\n            analyzer=self.analyzer.get_state(),\n        )\n\n    @staticmethod\n    def get_random_state(**kwargs):\n        """"""\n        Random state sample wrapper.\n\n        Args:\n            kwargs:   dict, stochastic process parameters, see kwargs at: OUProcess.get_random_state\n\n        Returns:\n            instance of TimeSeriesModelState with `analyser` set to None\n        """"""\n        return TimeSeriesModelState(\n            process=OUProcess.get_random_state(**kwargs),\n            analyzer=None,\n        )\n\n    def ready(self):\n        assert self.process.is_ready and self.analyzer.is_ready,\\\n            \'TimeSeriesModel is not initialized. Hint: forgot to call .reset()?\'\n\n    def reset(self, init_trajectory):\n        """"""\n        Resets model parameters and trajectory given initial data.\n\n        Args:\n            init_trajectory:    initial time-series observations of size from [1] to [num_points]\n        """"""\n        self.process.reset(init_trajectory)\n        _ = self.analyzer.reset(init_trajectory)\n\n    def update(self, trajectory, disjoint=False):\n        """"""\n        Updates model parameters and trajectory given new data.\n\n        Args:\n            trajectory: time-series update observations of size from [1] to [num_points],\n                        where num_points <= max_length to keep model trajectory continuous\n            disjoint:   bool, indicates whether update given is continuous or disjoint w.r.t. previous one\n        """"""\n        _ = self.analyzer.update(trajectory, disjoint)\n        self.process.update(trajectory, disjoint)\n\n    def transform(self, trajectory=None, state=None, size=None):\n        """"""\n        Returns analyzer data decomposition.\n\n        Args:\n            trajectory:     data to decompose of size [num_points] or None\n            state:          instance of TimeSeriesModelState or None\n            size:           uint, size of decomposition to get, or None\n\n        Returns:\n            SSA decomposition of given trajectory w.r.t. given state\n            if no `trajectory` is given - returns stored data decomposition\n            if no `state` arg. is given - uses stored analyzer state.\n            if no \'size` arg is given - decomposes full [stored or given] trajectory\n        """"""\n        # Ff 1d signal is given - need to embed it first:\n        if trajectory is not None:\n            trajectory = np.squeeze(trajectory)\n            assert trajectory.ndim == 1, \'Expected 1D array but got shape: {}\'.format(trajectory.shape)\n            x_embedded = self.analyzer._delay_embed(trajectory, self.analyzer.window)\n        else:\n            x_embedded = None\n\n        if state is not None:\n            assert isinstance(state, TimeSeriesModelState), \\\n                \'Expected `state` as instance of TimeSeriesModelState, got: {}\'.format(type(state))\n            # Unpack:\n            state = state.analyzer\n\n        return self.analyzer.transform(x_embedded, state, size)\n\n    def get_trajectory(self, size=None):\n        """"""\n        Returns stored fragment of original time-series data.\n\n        Args:\n            size:   uint, fragment length in [1, ..., max_length] or None\n\n        Returns:\n            1d series as [ x[-size], x[-size+1], ... x[-1] ], up to length [size];\n            if no `size` arg. is given - returns entire stored trajectory, up to length [max_length].\n        """"""\n        return self.analyzer.get_trajectory(size)\n\n    def generate(self, batch_size, size, state=None, driver_df=None):\n        """"""\n        Generates batch of realisations given process parameters.\n\n        Args:\n            batch_size:     uint, number of realisations to draw\n            size:           uint, length of each one\n            state:          instance TimeSeriesModelState or None, model parameters to use\n            driver_df:      t-student process driver degree of freedom parameter or None\n\n        Returns:\n            process realisations batch of size [batch_size, size]\n        """"""\n        if state is not None:\n            assert isinstance(state, TimeSeriesModelState), \\\n                \'Expected `state` as instance of TimeSeriesModelState, got: {}\'.format(type(state))\n            # Unpack:\n            process_state = state.process\n            if driver_df is None:\n                # Get driver param from given state:\n                driver_df = state.process.driver_estimator.df\n        else:\n            process_state = None\n\n        return self.process.generate(batch_size, size, process_state, driver_df)\n\n\nPriceModelState = namedtuple(\'PriceModelState\', [\'process\', \'analyzer\', \'stat\'])\n\n\nclass PriceModel(TimeSeriesModel):\n    """"""\n    Wrapper class for positive-valued time-series.\n    Internally works with normalised log-transformed data.\n    """"""\n    def __init__(\n            self,\n            max_length,\n            analyzer_window,\n            analyzer_grouping=None,\n            alpha=None,\n            filter_alpha=None,\n            stat_alpha=None\n    ):\n        """"""\n\n        Args:\n            max_length:         uint, maximum trajectory length to keep;\n            analyzer_window:    uint, SSA embedding window;\n            analyzer_grouping:  SSA decomposition triples grouping,\n                                iterable of pairs convertible to python slices, i.e.:\n                                grouping=[[0,1], [1,2], [2, None]];\n            alpha:              float in [0, 1], SSA and process estimator decaying factor;\n            filter_alpha:       float in [0, 1], process smoothing decaying factor;\n            stat_alpha:         float in [0, 1], time-series statistics tracking decaying factor;\n        """"""\n        super().__init__(max_length, analyzer_window, analyzer_grouping, alpha, filter_alpha)\n\n        # Statistics of original data:\n        self.stat = Zscore(1, stat_alpha)\n\n    def get_state(self):\n        """"""\n        Returns model state tuple.\n\n        Returns:\n            current state as instance of PriceModelState\n        """"""\n        return PriceModelState(\n            process=self.process.get_state(),\n            analyzer=self.analyzer.get_state(),\n            stat=self.stat.get_state(),\n        )\n\n    @staticmethod\n    def normalise(trajectory, mean, variance):\n        return (trajectory - mean) / np.clip(variance, 1e-8, None) ** .5\n\n    @staticmethod\n    def denormalize(trajectory, mean, variance):\n        return trajectory * variance ** .5 + mean\n\n    def reset(self, init_trajectory):\n        """"""\n        Resets model parameters and trajectory given initial data.\n\n        Args:\n            init_trajectory:    initial time-series observations of size from [1] to [num_points]\n        """"""\n        log_data = np.log(init_trajectory)\n        mean, variance = self.stat.reset(log_data[None, :])\n        return super().reset(self.normalise(log_data, mean, variance))\n\n    def update(self, trajectory, disjoint=False):\n        """"""\n        Updates model parameters and trajectory given new data.\n\n        Args:\n            trajectory: time-series update observations of size from [1] to [num_points],\n                        where num_points <= max_length to keep model trajectory continuous\n            disjoint:   bool, indicates whether update given is continuous or disjoint w.r.t. previous one\n        """"""\n        log_data = np.log(trajectory)\n        mean, variance = self.stat.update(log_data[None, :])\n        return super().update(self.normalise(log_data, mean, variance), disjoint)\n\n    def transform(self, trajectory=None, state=None, size=None):\n        """"""\n        Returns analyzer data decomposition.\n\n        Args:\n            trajectory:     data to decompose of size [num_points] or None\n            state:          instance of PriceModelState or None\n            size:           uint, size of decomposition to get, or None\n\n        Returns:\n            SSA decomposition of given trajectory w.r.t. given state\n            if no `trajectory` is given - returns stored data decomposition\n            if no `state` arg. is given - uses stored analyzer state.\n            if no \'size` arg is given - decomposes full [stored or given] trajectory\n        """"""\n        if state is not None:\n            assert isinstance(state, PriceModelState), \\\n                \'Expected `state` as instance of PriceModelState, got: {}\'.format(type(state))\n            # Unpack state:\n            state_base = TimeSeriesModelState(\n                analyzer=state.analyzer,\n                process=state.process\n            )\n        else:\n            state_base = None\n\n        # If 1d signal is given - need to normalize:\n        if trajectory is not None:\n            assert state is not None, \'State is expected when trajectory is given\'\n            trajectory = self.normalise(np.log(trajectory), state.stat.mean, state.stat.variance)\n\n        return super().transform(trajectory, state_base, size)\n\n    def get_trajectory(self, size=None):\n        """"""\n        Returns stored fragment of original time-series data.\n\n        Args:\n            size:   uint, fragment length in [1, ..., max_length] or None\n\n        Returns:\n            1d series as [ x[-size], x[-size+1], ... x[-1] ], up to length [size];\n            if no `size` arg. is given - returns entire stored trajectory, up to length [max_length].\n        """"""\n        # TODO: reconstruction is freaky due to only last stored statistic is used\n        trajectory = super().get_trajectory(size)\n        state = self.get_state()\n\n        return np.exp(self.denormalize(trajectory, state.stat.mean, state.stat.variance))\n\n    def generate(self, batch_size, size, state=None, driver_df=None):\n        """"""\n        Generates batch of realisations given process parameters.\n\n        Args:\n            batch_size:     uint, number of realisations to draw\n            size:           uint, length of each one\n            state:          instance PriceModelState or None, model parameters to use\n            driver_df:      t-student process driver degree of freedom parameter or None\n\n        Returns:\n            process realisations batch of size [batch_size, size]\n        """"""\n        if state is not None:\n            assert isinstance(state, PriceModelState), \\\n                \'Expected `state` as instance of PriceModelState, got: {}\'.format(type(state))\n            # Unpack:\n            state_base = TimeSeriesModelState(\n                analyzer=state.analyzer,\n                process=state.process\n            )\n        else:\n            state = self.get_state()\n            state_base = None\n\n        trajectory = super().generate(batch_size, size, state_base, driver_df)\n\n        return np.exp(self.denormalize(trajectory, state.stat.mean, state.stat.variance))\n\n    @staticmethod\n    def get_random_state(p_params, mean=(100, 100), variance=(1, 1)):\n        """"""\n        Samples random uniform model state w.r.t. intervals given.\n\n        Args:\n            p_params:       dict, stochastic process parameters, see kwargs at: OUProcess.get_random_state\n            mean:           iterable of floats as [0 < lower_bound, upper_bound], time-series means sampling interval.\n            variance:       iterable of floats as [0 < lower_bound, upper_bound], time-series variances sampling interval.\n\n        Returns:\n            instance of PriceModelState with `analyser` set to None\n\n        Note:\n            negative means are rejected;\n            stochastic process fitted on log_normalized data;\n        """"""\n        sample = dict()\n        for name, param, low_threshold in zip(\n                [\'mean\', \'variance\', ], [mean, variance], [1e-8, 1e-8]):\n            interval = np.asarray(param)\n            assert interval.ndim == 1 and interval[0] <= interval[-1], \\\n                \' Expected param `{}` as iterable of ordered values as: [lower_bound, upper_bound], got: {}\'.format(\n                    name, interval\n                )\n            assert interval[0] >= low_threshold, \\\n                \'Expected param `{}` lower bound be no less than {}, got: {}\'.format(name, low_threshold, interval[0])\n\n            sample[name] = np.random.uniform(low=interval[0], high=interval[-1], size=1)\n\n        # Log_transform mean and variance (those is biased estimates but ok for rnd. samples):\n        log_variance = np.log(sample[\'variance\'] / sample[\'mean\'] ** 2 + 1)\n        log_mean = np.log(sample[\'mean\']) - .5 * log_variance\n\n        # Inverse transform memo:\n        # mean = exp(log_mean + 0.5 * log_var)\n        # var = mean**2 * (exp(log_var) -1)\n\n        return PriceModelState(\n            process=OUProcess.get_random_state(**p_params),\n            analyzer=None,\n            stat=ZscoreState(\n                mean=log_mean,\n                variance=log_variance\n            ),\n        )\n\n'"
btgym/research/model_based/model/utils.py,0,"b'import numpy as np\n\n\ndef log_uniform(lo_hi, size):\n    """"""\n    Samples from log-uniform distribution in range specified by `lo_hi`.\n    Takes:\n        lo_hi: either scalar or iterable in form [low_value, high_value]\n        size: sample size\n    Returns:\n         np.array or np.float (if size=1).\n    """"""\n    r = np.asarray(lo_hi)\n    try:\n        lo = r[0]\n        hi = r[-1]\n    except IndexError:\n        lo = hi = r\n    x = np.random.random(size)\n    log_lo = np.log(lo + 1e-12)\n    log_hi = np.log(hi + 1e-12)\n    v = log_lo * (1 - x) + log_hi * x\n    if size > 1:\n        return np.exp(v)\n    else:\n        return np.exp(v)[0]\n\n\ndef ou_mle_estimator(data, dt=1, force_zero_mean=True):\n    """"""\n    Estimates vanilla OU max. log-likelihood parameters from given data of size [num_trajectories, num_points].\n\n    Returns:\n         tuple of vectors (mu, lambda, sigma) of size [num_trajectories] each.\n\n    Note:\n        robust against:\n            highly biased data i.e. where data.mean / data.std  >> 1\n            border conditions i.e. OU --> Weiner (unit root process)\n    """"""\n    if len(data.shape) == 1:\n        data = data[None, :]\n    elif len(data.shape) > 2:\n        raise AssertionError(\'Only 1D and 2D data accepted\')\n\n    # Center every trajectory:\n    bias = data.mean(axis=-1)\n    data -= bias[:, None]\n\n    n = data.shape[-1] - 1\n    x = data[:, :-1]\n    y = data[:, 1:]\n    sx = x.sum(axis=-1)\n    sy = y.sum(axis=-1)\n    sxx = (x ** 2).sum(axis=-1)\n    sxy = (x * y).sum(axis=-1)\n    syy = (y ** 2).sum(axis=-1)\n\n    if force_zero_mean:\n        # Assume OU mean is zero for centered data, compromises MLE but prevents\n        # trashy MU values for unit-root processes:\n        mu = 0\n\n    else:\n        mu_denom = (n * (sxx - sxy) - (sx**2 - sx * sy))\n        mu_denom[np.logical_and(0 <= mu_denom, mu_denom < 1e-10)] = 1e-10\n        mu_denom[np.logical_and(-1e-10 < mu_denom, mu_denom < 0)] = -1e-10\n        mu = (sy * sxx - sx * sxy) / mu_denom\n\n    l_denom = sxx - 2 * mu * sx + n * (mu ** 2)\n    l_denom[np.logical_and(0 <= l_denom, l_denom < 1e-10)] = 1e-10\n    l_denom[np.logical_and(-1e-10 < l_denom, l_denom < 0)] = -1e-10\n\n    l = - (1 / dt) * np.log(\n        np.clip(\n            (sxy - mu * sx - mu * sy + n * (mu ** 2)) / l_denom,\n            1e-10,\n            None\n        )\n    )\n    l = np.clip(l, 1e-10, None)\n\n    a = np.exp(-l * dt)\n\n    sigma_sq_hat = (1 / n) * (\n            syy - 2 * a * sxy + a ** 2 * sxx - 2 * mu * (1 - a) * (sy - a * sx) + n * (mu ** 2) * (1 - a) ** 2\n    )\n\n    sigma_sq_denom = 1 - a ** 2\n    sigma_sq_denom = np.clip(sigma_sq_denom, 1e-10, None)\n\n    sigma_sq = sigma_sq_hat * (2 * l / sigma_sq_denom)\n\n    sigma = np.clip(sigma_sq, 1e-10, None) ** .5\n\n    # Set bias back:\n    mu += bias\n    data += bias[:, None]  # in_place cleanup\n\n    return np.squeeze(mu), np.squeeze(l), np.squeeze(sigma)\n\n\ndef ou_lsr_estimator(data, dt=1):\n    """"""\n    Estimates vanilla OU parameters via least squares method from given data of size [num_trajectories, num_points].\n    Returns tuple of vectors (mu, lambda, sigma) of size [num_trajectories] each.\n    Note: robust against highly biased data i.e. where data.mean / data.std  >> 1\n    """"""\n    if len(data.shape) == 1:\n        data = data[None, :]\n    elif len(data.shape) > 2:\n        raise AssertionError(\'Only 1D and 2D data accepted\')\n\n    # Remove bias from every trajectory:\n    bias = data.mean(axis=-1)\n    data -= bias[:, None]\n\n    n = data.shape[-1] - 1\n    x = data[:, :-1]\n    y = data[:, 1:]\n    sx = x.sum(axis=-1)\n    sy = y.sum(axis=-1)\n    sxx = (x ** 2).sum(axis=-1)\n    sxy = (x * y).sum(axis=-1)\n    syy = (y ** 2).sum(axis=-1)\n\n    a = (n * sxy - sx * sy) / (n * sxx - sx ** 2)\n    b = (sy - a * sx) / n\n    sd_e = ((n * syy - sy ** 2 - a * (n * sxy - sx * sy)) / (n * (n - 2))) ** .5\n\n    l = - np.log(a) / dt\n    mu = b / (1 - a)\n    sigma = sd_e * (-2 * np.log(a) / (dt * (1 - a ** 2))) ** .5\n\n    # Set bias back:\n    mu += bias\n    data += bias[:, None]  # in_place cleanup\n\n    return np.squeeze(mu), np.squeeze(l), np.squeeze(sigma)\n\n\ndef ou_variance(l, sigma, **kwargs):\n    """"""\n    Returns true OU process variance.\n    """"""\n    return np.clip(sigma**2, 0, None) / (2 * np.clip(l, 1e-10, None))\n\n\ndef ou_log_likelihood(mu, l, sigma, data):\n    """"""\n    Estimates OU model parameters log likelihood given data log[P(mu, lambda, sigma|X)]\n    """"""\n    x = data[1:]\n    x_prev = data[:-1]\n    logL = - .5 * np.log(2 * np.pi) - np.log(sigma) \\\n           - 1 / (2 * sigma ** 2) * ((x - x_prev * np.exp(-l) - mu * (1 - np.exp(-l))) ** 2).mean(axis=-1)\n    return logL\n\n\ndef batch_covariance(x):\n    """"""\n    Batched covariance matrix estimation.\n    Credit to: Divakar@stackoverflow.com, see:\n    https://stackoverflow.com/questions/40394775/vectorizing-numpy-covariance-for-3d-array\n\n    Args:\n        x:  array of size [batch_dim, num_variables, num_observations]\n\n    Returns:\n        estimated covariance matrix of size [batch_dim, num_variables, num_variables]\n    """"""\n    n = x.shape[2]\n    m1 = x - x.sum(2, keepdims=1) / n\n    return np.einsum(\'ijk,ilk->ijl\', m1, m1) / (n - 1)\n\n\ndef multivariate_t_rvs(mean, cov, df, size):\n    """"""\n    Student\'s T random variable.\n    Generates random samples from multivariate t distribution.\n\n    Code credit:\n    written by Enzo Michelangeli, style changes by josef-pktd;\n    https://github.com/statsmodels/statsmodels/blob/master/statsmodels/sandbox/distributions/multivariate.py#L90\n\n    Args:\n        mean:   array_like, mean of random variable of size [dim], length determines dimensionality of random variable\n        cov:    array_like, covariance  matrix of size [dim, dim]\n        df:     array_like > 0, degrees of freedom of size [dim]\n        size:   array_like, size of observations to draw\n\n    Returns:\n        rvs as ndarray of size: size + [dim], i.e. if size=[m, n] than returned sample is: [m, n, dim]\n    """"""\n    # t-variance memo: ((df - 2) / df) ** .5\n    mean = np.asarray(mean)\n    df = np.asarray(df)\n\n    if type(size) in [int, float]:\n        size = [int(size)]\n    else:\n        size = list(size)\n\n    assert mean.ndim == 1 and df.shape == mean.shape, \\\n        \'Expected `mean` and `df` be 1d array_like of same size, got shapes: {} and {}\'.format(mean.shape, df.shape)\n\n    d = len(mean)\n\n    assert cov.shape == (d, d), \'Dimensionality: {} does not match covariance shape: {}\'.format(d, cov.shape)\n\n    x = np.random.chisquare(df, size + [d]) / df\n    z = np.random.multivariate_normal(np.zeros(d), cov, size)\n\n    return mean[None, :] + z / np.sqrt(x)\n\n\ndef cov2corr(cov):\n    """"""\n    Converts covariance matrix to correlation matrix.\n\n    Args:\n        cov:    square matrix\n\n    Returns:\n        correlation matrix of the same size.\n    """"""\n    cov = np.asanyarray(cov)\n    std = np.sqrt(np.clip(np.diag(cov), 1e-16, None))\n    corr = cov / np.outer(std, std)\n    return corr\n\n\ndef log_stat2stat(log_mean, log_variance):\n    """"""\n    Converts mean and variance of log_transformed RV\n    to mean and variance of original near-normally distributed RV.\n\n    Args:\n        log_mean:       array_like\n        log_variance:   array_like\n\n    Returns:\n        mean, variance of the same size\n    """"""\n    mean = np.exp(log_mean + 0.5 * log_variance)\n    variance = mean**2 * (np.exp(log_variance) - 1)\n\n    return mean, variance\n\n\n\n\n\n\n\n\n'"
