file_path,api_count,code
dataio/__init__.py,0,b'\n'
dataio/data_reader.py,0,"b""'''\r\nCreated on Mar 1, 2017\r\n\r\n@author: v-lianji\r\n'''\r\n\r\nimport pandas\r\nimport codecs\r\nimport pickle\r\nfrom scipy.sparse import find\r\nimport numpy as np\r\n\r\nclass movie_lens_data_repos:\r\n\tdef __init__(self, file):\r\n\t\twith codecs.open(file,'rb') as f:\r\n\t\t\ttrain,validate,test,user_content,item_content = pickle.load(f)\r\n\t\t\r\n\t\ttrain = train.reindex(np.random.permutation(train.index))\r\n\t\t\r\n\t\tself.training_ratings_user = train.loc[:,'user']\r\n\t\tself.training_ratings_item = train.loc[:,'item']\r\n\t\tself.training_ratings_score = train.loc[:,'rate'] \r\n\t\tself.test_ratings_user = validate.loc[:,'user']\r\n\t\tself.test_ratings_item = validate.loc[:,'item']\r\n\t\tself.test_ratings_score = validate.loc[:,'rate'] \r\n\t\tself.eval_ratings_user = test.loc[:,'user']\r\n\t\tself.eval_ratings_item = test.loc[:,'item']\r\n\t\tself.eval_ratings_score = test.loc[:,'rate'] \r\n\t\t\r\n\t\tself.n_user = max([self.training_ratings_user.max(),self.test_ratings_user.max(),self.eval_ratings_user.max()])+1\r\n\t\tself.n_item = max([self.training_ratings_item.max(),self.test_ratings_item.max(),self.eval_ratings_item.max()])+1\r\n\r\n\t\tself.n_user_attr, self.n_item_attr = user_content.shape[1], item_content.shape[1]\r\n\t\tprint('n_user=%d n_item=%d n_user_attr=%d n_item_attr=%d' %(self.n_user,self.n_item,self.n_user_attr, self.n_item_attr))\r\n\t\t\r\n\t\tself.user_attr = self.BuildAttributeFromSPMatrix(user_content,self.n_user,self.n_user_attr)\r\n\t\tself.item_attr = self.BuildAttributeFromSPMatrix(item_content,self.n_item,self.n_item_attr)\r\n\t\t\r\n\tdef BuildAttributeFromSPMatrix(self, sp_matrix, n, m):\r\n\t\tres = [] \r\n\t\tfor _ in range(n):\r\n\t\t\tres.append([])\r\n\t\t(row,col,value) = find(sp_matrix)\r\n\t\tfor r,c,v in zip(row,col,value):\r\n\t\t\tres[r].append([c,float(v)])\r\n\t\treturn res\r\n\t\t\t\r\n\t\t\r\n\r\nclass sparse_data_repos:\r\n\tdef __init__(self, n_user, n_item, n_user_attr = 0, n_item_attr = 0):\r\n\t\tself.n_user = n_user \r\n\t\tself.n_item = n_item \r\n\t\tself.n_user_attr = n_user_attr \r\n\t\tself.n_item_attr = n_item_attr \t\r\n\t\tself.user_attr = []\r\n\t\tself.item_attr = [] \t\r\n\t\tself.training_ratings_user = []\r\n\t\tself.training_ratings_item = []\r\n\t\tself.training_ratings_item02 = []\r\n\t\tself.training_ratings_score = []\r\n\t\tself.test_ratings_user = []\r\n\t\tself.test_ratings_item = []\r\n\t\tself.test_ratings_item02 = []\r\n\t\tself.test_ratings_score = []\r\n\t\tself.eval_ratings_user=[]\r\n\t\tself.eval_ratings_item=[]\r\n\t\tself.eval_ratings_score=[]\r\n\t\r\n\tdef load_user_attributes(self, infile,spliter='\\t'):\r\n\t\tself.load_attributes(self.user_attr, self.n_user, self.n_user_attr, infile,spliter)\r\n\t\r\n\tdef load_item_attributes(self, infile,spliter='\\t'):\r\n\t\tself.load_attributes(self.item_attr, self.n_item, self.n_item_attr, infile,spliter)\r\n\t\t\r\n\t\t\t\r\n\tdef load_attributes(self, res, n, m, infile,spliter):\t\t\r\n\t\tdel res[:]\r\n\t\tfor i in range(n):\r\n\t\t\tres.append([]) \r\n\t\t\r\n\t\twith open(infile, 'r') as rd:\r\n\t\t\twhile True:\r\n\t\t\t\tline = rd.readline()\r\n\t\t\t\tif not line:\r\n\t\t\t\t\tbreak \r\n\t\t\t\twords = line.replace('\\r\\n','').replace('\\n','').split(spliter)\r\n\t\t\t\tuid = int(words[0])\r\n\t\t\t\tfor i in range(len(words)-1):\r\n\t\t\t\t\ttokens = words[i+1].split(':')\r\n\t\t\t\t\tres[uid].append([int(tokens[0]),float(tokens[1])]) \r\n\t\t\t\t\t\r\n\t\t\t\t\t\r\n\tdef load_trainging_ratings(self, infile, spliter = '\\t'):\r\n\t\tself.load_rating_file(infile,self.training_ratings_user, self.training_ratings_item, self.training_ratings_score, spliter)\r\n\t\r\n\tdef load_test_ratings(self, infile, spliter = '\\t'):\r\n\t\tself.load_rating_file(infile,self.test_ratings_user, self.test_ratings_item, self.test_ratings_score, spliter)\r\n\t\t\r\n\tdef load_eval_ratings(self, infile, spliter = '\\t'):\r\n\t\tself.load_rating_file(infile,self.eval_ratings_user, self.eval_ratings_item, self.eval_ratings_score, spliter)\r\n\t\r\n\tdef load_rating_file(self,infile,rating_user, rating_item, rating_score,spliter):\r\n\t\tdel rating_user[:]\r\n\t\tdel rating_item[:]\r\n\t\tdel rating_score[:]\r\n\t\t\r\n\t\twith open(infile,'r') as rd:\r\n\t\t\twhile True:\r\n\t\t\t\tline = rd.readline()\r\n\t\t\t\tif not line:\r\n\t\t\t\t\tbreak \r\n\t\t\t\twords = line.replace('\\r\\n','').replace('\\n','').split(spliter)\r\n\t\t\t\trating_user.append(int(words[0]))\r\n\t\t\t\trating_item.append(int(words[1]))\r\n\t\t\t\trating_score.append(float(words[2]))\r\n\t\t#print(rating_list)\t\t\r\n\t\r\n\t\t\t\t\t\r\n\tdef load_trainging_pairwise_ratings(self, infile, spliter = '\\t'):\r\n\t\tself.load_pairwise_rating_file(infile,self.training_ratings_user, self.training_ratings_item, self.training_ratings_item02, self.training_ratings_score, spliter)\r\n\t\r\n\tdef load_test_pairwise_ratings(self, infile, spliter = '\\t'):\r\n\t\tself.load_pairwise_rating_file(infile,self.test_ratings_user, self.test_ratings_item, self.test_ratings_item02, self.test_ratings_score, spliter)\r\n\r\n\tdef load_pairwise_rating_file(self,infile,rating_user,rating_item01,rating_item02,rating_score, spliter):\r\n\t\tdel rating_user[:]\r\n\t\tdel rating_item01[:]\r\n\t\tdel rating_item02[:]\r\n\t\tdel rating_score[:]\r\n\t\t\r\n\t\t\r\n\t\twith open(infile,'r') as rd:\r\n\t\t\twhile True:\r\n\t\t\t\tline = rd.readline()\r\n\t\t\t\tif not line:\r\n\t\t\t\t\tbreak \r\n\t\t\t\twords = line.replace('\\r\\n','').replace('\\n','').split(spliter)\r\n\t\t\t\trating_user.append(int(words[0]))\r\n\t\t\t\trating_item01.append(int(words[1]))\r\n\t\t\t\trating_item02.append(int(words[2]))\r\n\t\t\t\trating_score.append(float(words[3]))\r\n\t\t\t\r\n\t\t\t\t\t\t\t\r\n\r\nclass dense_data_repos:\r\n\tdef __init__(self, n_user, n_item, n_user_attr = 0, n_item_attr = 0):\r\n\t\tself.n_user = n_user \r\n\t\tself.n_item = n_item \r\n\t\tself.n_user_attr = n_user_attr \r\n\t\tself.n_item_attr = n_item_attr \t\r\n\t\tself.user_attr = []\r\n\t\tself.item_attr = [] \t\r\n\t\tself.training_ratings = []\r\n\t\tself.test_ratings = [] \r\n\t\r\n\tdef load_user_attributes(self, infile,spliter='\\t'):\r\n\t\tself.load_attributes(self.user_attr, self.n_user, self.n_user_attr, infile,spliter)\r\n\t\r\n\tdef load_item_attributes(self, infile,spliter='\\t'):\r\n\t\tself.load_attributes(self.item_attr, self.n_item, self.n_item_attr, infile,spliter)\r\n\t\r\n\tdef load_attributes(self, res, n, m, infile,spliter):\r\n\t\t#res = [[0.0]*m for i in range(n)]\r\n\t\tdel res[:]\r\n\t\tfor i in range(n):\r\n\t\t\tres.append([0.0]*m) \r\n\t\t\r\n\t\twith open(infile, 'r') as rd:\r\n\t\t\twhile True:\r\n\t\t\t\tline = rd.readline()\r\n\t\t\t\tif not line:\r\n\t\t\t\t\tbreak \r\n\t\t\t\twords = line.replace('\\r\\n','').replace('\\n','').split(spliter)\r\n\t\t\t\tuid = int(words[0])\r\n\t\t\t\tfor i in range(len(words)-1):\r\n\t\t\t\t\ttokens = words[i+1].split(':')\r\n\t\t\t\t\tres[uid][int(tokens[0])] = float(tokens[1])\r\n\t\t\t\t\t\r\n\tdef load_trainging_ratings(self, infile, spliter = '\\t'):\r\n\t\tself.load_rating_file(infile,self.training_ratings,spliter)\r\n\t\r\n\tdef load_test_ratings(self, infile, spliter = '\\t'):\r\n\t\tself.load_rating_file(infile, self.test_ratings, spliter)\r\n\r\n\tdef load_rating_file(self,infile,rating_list,spliter):\r\n\t\tdel rating_list[:]\r\n\t\twith open(infile,'r') as rd:\r\n\t\t\twhile True:\r\n\t\t\t\tline = rd.readline()\r\n\t\t\t\tif not line:\r\n\t\t\t\t\tbreak \r\n\t\t\t\twords = line.replace('\\r\\n','').replace('\\n','').split(spliter)\r\n\t\t\t\trating_list.append([int(words[0]),int(words[1]),float(words[2])])\r\n\t\t#print(rating_list)\t\t\r\n\r\n\r\ndef load_rating_tsv(filename):\r\n\t'''\r\n\tres: [ [uid,iid,score], ... ]\r\n\t'''\r\n\tres = []\r\n\twith open(filename,'r') as rd:\r\n\t\twhile True:\r\n\t\t\tline = rd.readline()\r\n\t\t\tif not line:\r\n\t\t\t\tbreak \r\n\t\t\twords = line.replace('\\r\\n','').replace('\\n','').split('\\t')\r\n\t\t\tres.append([words[0],words[1],float(words[2])])\r\n\treturn res\r\n\r\ndef load_content_tsv(filename):\r\n\t'''\r\n\tres: dict --> uid : [ [tag,value], ...]\r\n\t'''\r\n\tres = {}\r\n\twith open(filename,'r') as rd:\r\n\t\twhile True:\r\n\t\t\tline = rd.readline()\r\n\t\t\tif not line:\r\n\t\t\t\tbreak \r\n\t\t\twords = line.replace('\\r\\n','').replace('\\n','').split('\\t')\r\n\t\t\tres[words[0]]=[]\r\n\t\t\tfor i in range(len(words)-1):\r\n\t\t\t\ttokens = words[i+1].split(':')\r\n\t\t\t\tres[words[0]].append([tokens[0],float(tokens[1])])\r\n\treturn res\r\n\r\n\r\nif __name__ == '__main__':\r\n\tpass"""
models/RNN-News-RecSys.py,0,"b'see the repository here https://github.com/Leavingseason/rnn_recsys\nOur implementation of one research paper ""Embedding-based News Recommendation for Millions of Users"" https://dl.acm.org/citation.cfm?id=3098108 Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD \'17).\n'"
models/__init__.py,0,b'\n'
models/bmf.py,21,"b'\'\'\'\r\nCreated on Mar 3, 2017\r\n\r\n@author: v-lianji\r\n\'\'\'\r\n\r\n\r\nimport tensorflow as tf\r\nfrom dataio import data_reader \r\nimport math\r\nfrom time import clock\r\nimport numpy as np\r\n\r\n\r\ndef build_model(user_indices, item_indices, rank, ratings, user_cnt, item_cnt, lr, lamb, mu, init_value):\r\n\t\r\n\t\r\n\tW_user = tf.Variable(tf.truncated_normal([user_cnt, rank], stddev=init_value/math.sqrt(float(rank)), mean=0), name = \'user_embedding\', dtype=tf.float32)\r\n\tW_item = tf.Variable(tf.truncated_normal([item_cnt, rank], stddev=init_value/math.sqrt(float(rank)), mean=0), name = \'item_embedding\', dtype=tf.float32)\r\n\t\r\n\tW_user_bias = tf.concat([W_user, tf.ones((user_cnt,1), dtype=tf.float32)], 1, name=\'user_embedding_bias\')\r\n\tW_item_bias = tf.concat([tf.ones((item_cnt,1), dtype=tf.float32), W_item], 1, name=\'item_embedding_bias\')\r\n\t\r\n\tuser_feature = tf.nn.embedding_lookup(W_user_bias, user_indices, name = \'user_feature\')\r\n\titem_feature = tf.nn.embedding_lookup(W_item_bias, item_indices, name = \'item_feature\')\t\r\n\t\r\n\t\r\n\tpreds = tf.add(tf.reduce_sum( tf.multiply(user_feature , item_feature) , 1), mu)\r\n\t\r\n\tsquare_error = tf.sqrt(tf.reduce_mean( tf.squared_difference(preds, ratings)))\r\n\tloss = square_error + lamb*(tf.reduce_mean(tf.nn.l2_loss(W_user)) + tf.reduce_mean(tf.nn.l2_loss(W_item)))\r\n\t\t\r\n\ttf.summary.scalar(\'square_error\', square_error)\r\n\ttf.summary.scalar(\'loss\', loss)\r\n\tmerged_summary = tf.summary.merge_all()\r\n\t#tf.global_variables_initializer()\r\n\ttrain_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)   # tf.train.AdadeltaOptimizer(learning_rate=lr).minimize(loss)    #\r\n\r\n\treturn train_step, square_error, loss, merged_summary\r\n\r\ndef grid_search_params():\r\n\r\n\tdataset = data_reader.sparse_data_repos(10000,10005)\r\n\tdataset.load_trainging_ratings(r\'data/userbook_unique_compactid_train.txt\')\r\n\tdataset.load_test_ratings(r\'data/userbook_unique_compactid_valid.txt\')\r\n\tdataset.load_eval_ratings(r\'data/userbook_unique_compactid_test.txt\')\r\n\tlog_file = r\'logs/BMF_book.csv\'\r\n\t\r\n\twt = open(log_file,\'w\')\r\n\trank = 16\r\n\tlambs=[0.00003,0.00005,0.0001]\r\n\tbatch_sizes=[500]\r\n\tn_eopch=2000\r\n\tlrs=[0.1]\r\n\tinit_values = [0.01]\r\n\t#mu=dataset.training_ratings_score.mean()\r\n\tmu = np.asarray(dataset.training_ratings_score, dtype=np.float32).mean() \r\n\twt.write(\'rank,lr,lamb,mu,n_eopch,batch_size,best_train_rmse,best_test_rmse,best_eval_rmse,best_epoch,init_value,minutes\\n\')\r\n\tfor lamb in lambs:\r\n\t\tfor lr in lrs:\r\n\t\t\tfor init_value in init_values:\r\n\t\t\t\tfor batch_size in batch_sizes:\r\n\t\t\t\t\trun_with_parameter(dataset,rank,lr,lamb,mu,n_eopch,batch_size,wt, init_value)\r\n\twt.close()\r\n\r\ndef run_with_parameter(dataset,rank,lr,lamb,mu,n_eopch,batch_size,wt, init_value):\r\n\tstart = clock()\r\n\ttf.reset_default_graph()\r\n\tbest_train_rmse, best_test_rmse, best_eval_rmse, best_eopch_idx = single_run(dataset,rank,dataset.n_user,dataset.n_item,lr,lamb,mu,n_eopch,batch_size,True, init_value)\r\n\tend = clock()\r\n\twt.write(\'%d,%f,%f,%f,%d,%d,%f,%f,%f,%d,%f,%f\\n\' %(rank,lr,lamb,mu,n_eopch,batch_size,best_train_rmse, best_test_rmse, best_eval_rmse,best_eopch_idx,init_value,(end-start)/60))\r\n\twt.flush()\r\n\r\n\r\n\r\n\r\ndef single_run(dataset,rank,user_cnt,item_cnt,lr,lamb,mu,n_eopch,batch_size,is_eval_on, init_value):\r\n\t\r\n\tuser_indices =  tf.placeholder(tf.int32,[None])\r\n\titem_indices =  tf.placeholder(tf.int32,[None])\r\n\tratings = tf.placeholder(tf.float32, [None])\t\r\n\r\n\r\n\ttrain_step, square_error, loss, merged_summary = build_model(user_indices, item_indices, rank, ratings, user_cnt, item_cnt, lr, lamb, mu, init_value)\r\n\t\r\n\tsess = tf.Session()\r\n\tinit = tf.global_variables_initializer()\r\n\tsess.run(init) \r\n\t\r\n\t#print(sess.run(user_embeddings))\r\n\t\r\n\ttrain_writer = tf.summary.FileWriter(r\'logs\', sess.graph)\r\n\t\r\n\tn_instances = len(dataset.training_ratings_user)\r\n\r\n\tbest_train_rmse, best_test_rmse, best_eval_rmse = -1, -1, -1\r\n\tbest_eopch_idx = -1 \r\n\tfor ite in range(n_eopch):\r\n\t\t#print(ite)\r\n\t\tstart = clock()\r\n\t\tfor i in range(n_instances//batch_size):\r\n\t\t\tstart_idx = i * batch_size \r\n\t\t\tend_idx = start_idx + batch_size\r\n\t\t\tcur_user_indices, cur_item_indices, cur_label = dataset.training_ratings_user[start_idx:end_idx], dataset.training_ratings_item[start_idx:end_idx],dataset.training_ratings_score[start_idx:end_idx]\r\n\t\t\t\r\n\t\t\tsess.run(train_step, { user_indices : cur_user_indices, item_indices : cur_item_indices, ratings : cur_label})\t\r\n\t\t\t\r\n\t\terror_traing = sess.run(square_error, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\r\n\t\terror_test = sess.run(square_error, { user_indices : dataset.test_ratings_user, item_indices : dataset.test_ratings_item, ratings : dataset.test_ratings_score})\r\n\t\tif is_eval_on:\r\n\t\t\terror_eval = sess.run(square_error, { user_indices : dataset.eval_ratings_user, item_indices : dataset.eval_ratings_item, ratings : dataset.eval_ratings_score})\r\n\t\telse: \r\n\t\t\terror_eval = -1\r\n\t\t\t\r\n\t\tif best_test_rmse<0 or best_test_rmse>error_test:\r\n\t\t\tbest_train_rmse, best_test_rmse, best_eval_rmse = error_traing,error_test, error_eval \r\n\t\t\tbest_eopch_idx = ite \r\n\t\telse:\r\n\t\t\tif ite - best_eopch_idx>10:\r\n\t\t\t\tbreak \r\n\t\t\t\r\n\t\tloss_traing = sess.run(loss, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\r\n\t\t#loss_test = sess.run(loss, { user_feature : test_user_feature, item_feature : test_item_feature, ratings : test_label})\r\n\t\tsummary = sess.run(merged_summary, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\r\n\t\ttrain_writer.add_summary(summary, ite)\r\n\t\tend = clock()\r\n\t\tprint(""Iteration %d  RMSE(train): %f  RMSE(test): %f   RMSE(eval): %f   LOSS(train): %f  minutes: %f"" %(ite, error_traing, error_test, error_eval, loss_traing, (end-start)/60))\r\n\t\t\r\n\t\r\n\ttrain_writer.close()\r\n\t\r\n\treturn best_train_rmse, best_test_rmse, best_eval_rmse,best_eopch_idx\r\n\r\nif __name__ == \'__main__\':\r\n\t\r\n\tgrid_search_params()\r\n\t#run()\r\n\tpass \r\n'"
models/ccf_net.py,47,"b'\'\'\'\r\nCreated on June 3, 2017\r\n\r\n@author: v-lianji\r\n\'\'\'\r\n\r\n\r\nimport tensorflow as tf\r\nfrom dataio import data_reader \r\nfrom dataio import adapter\r\nimport math\r\nfrom time import clock\r\nimport numpy as np \r\n\r\n\r\ndef grid_search(infile,logfile):\r\n\t\r\n\t#default params:\r\n\tparams={\r\n\t\t\'cf_dim\':16, \r\n\t\t\'user_attr_rank\':16, \r\n\t\t\'item_attr_rank\':16, \r\n\t\t\'layer_sizes\':[16,8], \r\n\t\t\'lr\':0.1, \r\n\t\t\'lamb\':0.001, \r\n\t\t\'mu\':4.0, \r\n\t\t\'n_eopch\':2000 , \r\n\t\t\'batch_size\':500, \r\n\t\t\'init_value \':0.01\r\n\t\t}\r\n\t\r\n\tdataset = data_reader.movie_lens_data_repos(infile) \r\n\twt = open(logfile,\'w\')\r\n\t\r\n\tlambs=[0.001,0.0001,0.0005,0.005]\t\r\n\tlrs=[0.1,0.05]\r\n\tlayer_sizes_list = [[16],[16,8]]\r\n\tinit_values = [0.01,0.1]\r\n\tmu=dataset.training_ratings_score.mean() \r\n\t\r\n\t#wt.write(\'cf_dim,user_attr_rank,item_attr_rank,lr,lamb,mu,n_eopch,batch_size,best_train_rmse,best_test_rmse,best_eval_rmse,best_epoch,init_value,layer_cnt,minutes\\n\')\r\n\t\r\n\tfor lamb in lambs:\r\n\t\tfor lr in lrs:\r\n\t\t\tfor init_value in init_values:\r\n\t\t\t\tfor layer_sizes in layer_sizes_list:\r\n\t\t\t\t\tparams[\'lamb\']=lamb\r\n\t\t\t\t\tparams[\'lr\']=lr\r\n\t\t\t\t\tparams[\'init_value\']=init_value\r\n\t\t\t\t\tparams[\'layer_sizes\']=layer_sizes\r\n\t\t\t\t\tparams[\'mu\']=mu\r\n\t\t\t\t\trun_with_parameters(dataset, params, wt)\r\n\t\t\t\t\trun_with_parameters(dataset, params, wt)\r\n\twt.close()\r\n\r\ndef run_with_parameters(dataset, params, wt):\r\n\t\r\n\tstart = clock()\r\n\ttf.reset_default_graph()\r\n\tbest_train_rmse, best_test_rmse, best_eval_rmse, best_eopch_idx = single_run(dataset, params)\r\n\tend = clock()\r\n\twt.write(\'%f,%f,%f,%d,%f,%s\\n\' %(best_train_rmse, best_test_rmse, best_eval_rmse, best_eopch_idx,(end-start)/60, str(params)))\r\n\twt.flush()\r\n\r\ndef single_run(dataset, params):\r\n\tcf_dim, user_attr_rank, item_attr_rank, layer_sizes, lr, lamb, mu, n_eopch , batch_size, init_value =  params[\'cf_dim\'], params[\'user_attr_rank\'],params[\'item_attr_rank\'],params[\'layer_sizes\'],params[\'lr\'],params[\'lamb\'],params[\'mu\'],params[\'n_eopch\'],params[\'batch_size\'],params[\'init_value\']\r\n\t##  compose features from SVD\r\n\tuser_cnt,user_attr_cnt = dataset.n_user, dataset.n_user_attr\r\n\titem_cnt,item_attr_cnt = dataset.n_item, dataset.n_item_attr\r\n\t\r\n\tW_user = tf.Variable(tf.truncated_normal([user_cnt, cf_dim], stddev=init_value/math.sqrt(float(cf_dim)), mean=0), name = \'user_cf_embedding\', dtype=tf.float32)\r\n\tW_item = tf.Variable(tf.truncated_normal([item_cnt, cf_dim], stddev=init_value/math.sqrt(float(cf_dim)), mean=0), name = \'item_cf_embedding\', dtype=tf.float32)\r\n\t\r\n\tW_user_bias = tf.concat([W_user, tf.ones((user_cnt,1), dtype=tf.float32)], 1, name=\'user_cf_embedding_bias\')\r\n\tW_item_bias = tf.concat([tf.ones((item_cnt,1), dtype=tf.float32), W_item], 1, name=\'item_cf_embedding_bias\')\r\n\t\r\n\t##  compose features from attributes\r\n\tuser_attr_indices, user_attr_indices_values, user_attr_indices_weights =  compose_vector_for_sparse_tensor(dataset.user_attr)\r\n\titem_attr_indices, item_attr_indices_values, item_attr_indices_weights =  compose_vector_for_sparse_tensor(dataset.item_attr)\r\n\t\r\n\tuser_sp_ids = tf.SparseTensor(indices=user_attr_indices, values = user_attr_indices_values, dense_shape=[user_cnt,user_attr_cnt])\r\n\tuser_sp_weights = tf.SparseTensor(indices = user_attr_indices, values = user_attr_indices_weights, dense_shape=[user_cnt, user_attr_cnt])\r\n\t\r\n\t\r\n\titem_sp_ids = tf.SparseTensor(indices=item_attr_indices, values = item_attr_indices_values, dense_shape=[item_cnt,item_attr_cnt])\r\n\titem_sp_weights = tf.SparseTensor(indices = item_attr_indices, values = item_attr_indices_weights, dense_shape=[item_cnt, item_attr_cnt])\r\n\t\r\n\tW_user_attr = tf.Variable(tf.truncated_normal([user_attr_cnt, user_attr_rank], stddev=init_value/math.sqrt(float(user_attr_rank)), mean=0), name = \'user_attr_embedding\',dtype=tf.float32)\r\n\tW_item_attr = tf.Variable(tf.truncated_normal([item_attr_cnt, item_attr_rank], stddev=init_value/math.sqrt(float(item_attr_rank)), mean=0), name = \'item_attr_embedding\',dtype=tf.float32)\r\n\t\r\n\tuser_embeddings = tf.nn.embedding_lookup_sparse(W_user_attr, user_sp_ids, user_sp_weights,  name=\'user_embeddings\', combiner=\'sum\')\r\n\titem_embeddings = tf.nn.embedding_lookup_sparse(W_item_attr, item_sp_ids, item_sp_weights,  name=\'item_embeddings\', combiner=\'sum\')\r\n\t\t\r\n\t\r\n\tuser_indices =  tf.placeholder(tf.int32,[None])\r\n\titem_indices =  tf.placeholder(tf.int32,[None])\r\n\tratings = tf.placeholder(tf.float32, [None])\r\n\t\r\n\tuser_cf_feature = tf.nn.embedding_lookup(W_user_bias, user_indices, name = \'user_feature\')\r\n\titem_cf_feature = tf.nn.embedding_lookup(W_item_bias, item_indices, name = \'item_feature\')\t\r\n\t\r\n\t\r\n\tuser_attr_feature = tf.nn.embedding_lookup(user_embeddings, user_indices, name = \'user_feature\')\r\n\titem_attr_feature = tf.nn.embedding_lookup(item_embeddings, item_indices, name = \'item_feature\')\r\n\t\r\n\t#tf.summary.image(\'user_feautre\', user_feature)\r\n\t\r\n\r\n\ttrain_step, square_error, loss, merged_summary = build_model(user_cf_feature, user_attr_feature, user_attr_rank, \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\titem_cf_feature,item_attr_feature, item_attr_rank, \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tratings, layer_sizes, \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tW_user, W_item,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tW_user_attr,W_item_attr,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlamb,lr, mu)\r\n\t\r\n\tsess = tf.Session()\r\n\tinit = tf.global_variables_initializer()\r\n\tsess.run(init) \r\n\t\r\n\t#print(sess.run(user_embeddings))\r\n\t\r\n\ttrain_writer = tf.summary.FileWriter(r\'\\\\mlsdata\\e$\\Users\\v-lianji\\DeepRecsys\\Test\\logs\', sess.graph)\r\n\t\r\n\t\r\n\tn_instances = len(dataset.training_ratings_user)\r\n\r\n\tbest_train_rmse, best_test_rmse, best_eval_rmse = -1, -1, -1\r\n\tbest_eopch_idx = -1 \r\n\t\r\n\tfor ite in range(n_eopch):\r\n\t\t#print(ite)\r\n\t\tstart = clock()\r\n\t\tfor i in range(n_instances//batch_size):\r\n\t\t\tstart_idx = i * batch_size \r\n\t\t\tend_idx = start_idx + batch_size\r\n\t\t\tcur_user_indices, cur_item_indices, cur_label = dataset.training_ratings_user[start_idx:end_idx], dataset.training_ratings_item[start_idx:end_idx],dataset.training_ratings_score[start_idx:end_idx]\r\n\t\t\t\r\n\t\t\tsess.run(train_step, { user_indices : cur_user_indices, item_indices : cur_item_indices, ratings : cur_label})\t\r\n\t\t\t\r\n\t\terror_traing = sess.run(square_error, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\r\n\t\terror_test = sess.run(square_error, { user_indices : dataset.test_ratings_user, item_indices : dataset.test_ratings_item, ratings : dataset.test_ratings_score})\r\n\t\terror_eval = sess.run(square_error, { user_indices : dataset.eval_ratings_user, item_indices : dataset.eval_ratings_item, ratings : dataset.eval_ratings_score})\r\n\t\t\r\n\t\tloss_traing = sess.run(loss, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\r\n\t\t#loss_test = sess.run(loss, { user_feature : test_user_feature, item_feature : test_item_feature, ratings : test_label})\r\n\t\t\r\n\t\t\r\n\t\tsummary = sess.run(merged_summary, { user_indices : dataset.training_ratings_user, item_indices : dataset.training_ratings_item, ratings : dataset.training_ratings_score})\r\n\t\ttrain_writer.add_summary(summary, ite)\r\n\t\tend = clock()\r\n\t\tprint(""Iteration %d  RMSE(train): %f  RMSE(test): %f   RMSE(eval): %f   LOSS(train): %f  minutes: %f"" %(ite, error_traing, error_test, error_eval, loss_traing, (end-start)/60))\r\n\t\t\r\n\t\tif best_test_rmse<0 or best_test_rmse>error_test:\r\n\t\t\tbest_train_rmse, best_test_rmse, best_eval_rmse = error_traing,error_test, error_eval \r\n\t\t\tbest_eopch_idx = ite \r\n\t\telse:\r\n\t\t\tif ite - best_eopch_idx>10:\r\n\t\t\t\tbreak \r\n\t\t\t\r\n\t\r\n\ttrain_writer.close()\r\n\t\r\n\treturn best_train_rmse, best_test_rmse, best_eval_rmse, best_eopch_idx\r\n\r\ndef build_model(user_cf_feature, user_attr_feature, user_attr_rank, \r\n\t\t\titem_cf_feature, item_attr_feature, item_attr_rank, \r\n\t\t\tratings, layer_size, \r\n\t\t\tW_user, W_item,\r\n\t\t\tW_user_attr, W_item_attr, lamb , lr, mu ):\r\n\t\r\n\tlayer_cnt = len(layer_size)\r\n\thiddens_user = [] \r\n\thiddens_item = [] \r\n\t\r\n\thiddens_user.append(user_attr_feature)\r\n\thiddens_item.append(item_attr_feature)\r\n\t\r\n\tb_user_list = []\r\n\tb_item_list = []\r\n\tW_user_list = [] \r\n\tW_item_list = []\r\n\t\r\n\tfor i in range(layer_cnt):\r\n\t\twith tf.name_scope(\'layer_\'+str(i)):\r\n\t\t\tb_user_list.append(tf.Variable(tf.truncated_normal([layer_size[i]]),name=\'user_bias\'))\r\n\t\t\tb_item_list.append(tf.Variable(tf.truncated_normal([layer_size[i]]),name=\'item_bias\'))\r\n\t\t\tif i==0:\r\n\t\t\t\tW_user_list.append(tf.Variable(tf.truncated_normal([user_attr_rank, layer_size[i]], stddev=1/math.sqrt(float(layer_size[i])), mean=0), name = \'W_user\'))\r\n\t\t\t\tW_item_list.append(tf.Variable(tf.truncated_normal([item_attr_rank, layer_size[i]], stddev=1/math.sqrt(float(layer_size[i])), mean=0), name= \'W_item\'))\r\n\t\t\t\t\r\n\t\t\t\tuser_middle = tf.matmul(user_attr_feature,W_user_list[i]) + b_user_list[i]\r\n\t\t\t\titem_middle = tf.matmul(item_attr_feature,W_item_list[i]) + b_item_list[i]  \r\n\t\t\t\t\r\n\t\t\telse:\r\n\t\t\t\tW_user_list.append(tf.Variable(tf.truncated_normal([layer_size[i-1], layer_size[i]], stddev=1/math.sqrt(float(layer_size[i])), mean=0), name = \'W_user\'))\r\n\t\t\t\tW_item_list.append(tf.Variable(tf.truncated_normal([layer_size[i-1], layer_size[i]], stddev=1/math.sqrt(float(layer_size[i])), mean=0), name= \'W_item\'))\r\n\t\t\t\t\r\n\t\t\t\tuser_middle =tf.matmul(hiddens_user[i],W_user_list[i]) + b_user_list[i]\r\n\t\t\t\titem_middle =tf.matmul(hiddens_item[i],W_item_list[i]) + b_item_list[i]\r\n\t\t\t\r\n\t\t\thiddens_user.append(tf.identity(user_middle, name = \'factor_user\')) #identity ,sigmoid\r\n\t\t\thiddens_item.append(tf.identity(item_middle, name = \'factor_item\'))\r\n\t\t\r\n\t\t\r\n\tfactor_user = hiddens_user[layer_cnt] \r\n\tfactor_item = hiddens_item[layer_cnt] \r\n\t\r\n\t\r\n\t\r\n\tpreds = tf.reduce_sum( tf.multiply(user_cf_feature , item_cf_feature) , 1) +  tf.reduce_sum( tf.multiply(factor_user , factor_item) , 1) + mu\r\n\t# TODO:  bound the prediction within [min_score, max_score]\r\n\t\r\n\tsquare_error = tf.sqrt(tf.reduce_mean( tf.squared_difference(preds, ratings)))\r\n\tloss = square_error \r\n\tfor i in range(layer_cnt):\r\n\t\tloss = loss + lamb*(\r\n\t\t\t\t\t\t\ttf.reduce_mean(tf.nn.l2_loss(W_user)) + tf.reduce_mean(tf.nn.l2_loss(W_item)) +\r\n\t\t\t\t\t\t\ttf.reduce_mean(tf.nn.l2_loss(W_user_attr)) + tf.reduce_mean(tf.nn.l2_loss(W_item_attr)) + \r\n\t\t\t\t\t\t\ttf.reduce_mean(tf.nn.l2_loss(W_user_list[i])) + tf.reduce_mean(tf.nn.l2_loss(W_item_list[i])) + tf.reduce_mean(tf.nn.l2_loss(b_user_list[i])) + tf.reduce_mean(tf.nn.l2_loss(b_item_list[i]))\r\n\t\t\t\t\t\t)\r\n\t\t\r\n\ttf.summary.scalar(\'square_error\', square_error)\r\n\ttf.summary.scalar(\'loss\', loss)\r\n\tmerged_summary = tf.summary.merge_all()\r\n\t#tf.global_variables_initializer()\r\n\ttrain_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\r\n\r\n\treturn train_step, square_error, loss, merged_summary\r\n\r\n\r\n\r\ndef compose_vector_for_sparse_tensor(entity2attr_list):\r\n\tindices = [] \r\n\tindices_values = []\r\n\tweight_values = [] \r\n\t\r\n\tN = len(entity2attr_list)\r\n\tfor i in range(N):\r\n\t\tif len(entity2attr_list[i])>0:\r\n\t\t\tcnt = 0 \r\n\t\t\tfor attr_pair in entity2attr_list[i]:\r\n\t\t\t\t#print(entity2attr_list)\r\n\t\t\t\tindices.append([i,cnt])\r\n\t\t\t\tindices_values.append(attr_pair[0])\r\n\t\t\t\tweight_values.append(attr_pair[1])\r\n\t\t\t\tcnt+=1\r\n\t\telse:\r\n\t\t\tindices.append([i,0])\r\n\t\t\tindices_values.append(0)\r\n\t\t\tweight_values.append(0)\r\n\treturn indices, indices_values, weight_values\r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\t\r\n\tgrid_search(r\'data/movielens_100k.pkl\',\r\n\t\t\tr\'logs/CCFNet_movielens10m.csv\')\r\n\tpass \r\n'"
models/deepFM.py,79,"b'\'\'\'\r\nCreated on June 04, 2017\r\n\r\n@author: v-lianji\r\n\'\'\'\r\n\r\nimport tensorflow as tf\r\nimport math\r\nfrom time import clock\r\nimport numpy as np\r\nimport sys\r\nimport os\r\nimport pickle\r\nimport sys\r\nfrom sklearn import metrics\r\nfrom sklearn.metrics import roc_auc_score\r\nfrom datetime import datetime\r\n\r\n\r\ndef load_data_from_file_batching(file, batch_size):\r\n    labels = []\r\n    features = []\r\n    cnt = 0\r\n    with open(file, \'r\') as rd:\r\n        while True:\r\n            line = rd.readline()\r\n            if not line:\r\n                break\r\n            cnt += 1\r\n            if \'#\' in line:\r\n                punc_idx = line.index(\'#\')\r\n            else:\r\n                punc_idx = len(line)\r\n            label = float(line[0:1])\r\n            if label>1:\r\n                label=1\r\n            feature_line = line[2:punc_idx]\r\n            words = feature_line.split(\' \')\r\n            cur_feature_list = []\r\n            for word in words:\r\n                if not word:\r\n                    continue\r\n                tokens = word.split(\':\')\r\n\r\n                # if tokens[0]==\'4532\':\r\n                #    print(\'line \', cnt, \':    \',word, \'    line:\', line)\r\n                if len(tokens[1]) <= 0:\r\n                    tokens[1] = \'0\'\r\n                cur_feature_list.append([int(tokens[0]) - 1, float(tokens[1])])\r\n            features.append(cur_feature_list)\r\n            labels.append(label)\r\n            if cnt == batch_size:\r\n                yield labels, features\r\n                labels = []\r\n                features = []\r\n                cnt = 0\r\n    if cnt > 0:\r\n        yield labels, features\r\n\r\n\r\ndef prepare_data_4_sp(labels, features, dim):\r\n    instance_cnt = len(labels)\r\n\r\n    indices = []\r\n    values = []\r\n    values_2 = []\r\n    shape = [instance_cnt, dim]\r\n    feature_indices = []\r\n\r\n    for i in range(instance_cnt):\r\n        m = len(features[i])\r\n        for j in range(m):\r\n            indices.append([i, features[i][j][0]])\r\n            values.append(features[i][j][1])\r\n            values_2.append(features[i][j][1] * features[i][j][1])\r\n            feature_indices.append(features[i][j][0])\r\n\r\n    res = {}\r\n\r\n    res[\'indices\'] = np.asarray(indices, dtype=np.int64)\r\n    res[\'values\'] = np.asarray(values, dtype=np.float32)\r\n    res[\'values2\'] = np.asarray(values_2, dtype=np.float32)\r\n    res[\'shape\'] = np.asarray(shape, dtype=np.int64)\r\n    res[\'labels\'] = np.asarray([[label] for label in labels], dtype=np.float32)\r\n    res[\'feature_indices\'] = np.asarray(feature_indices, dtype=np.int64)\r\n\r\n    return res\r\n\r\n\r\ndef load_data_cache(filename):\r\n    with open(filename, ""rb"") as f:\r\n        while True:\r\n            try:\r\n                yield pickle.load(f)\r\n            except EOFError:\r\n                break\r\n\r\n\r\ndef pre_build_data_cache(infile, outfile, feature_cnt, batch_size):\r\n    wt = open(outfile, \'wb\')\r\n    for labels, features in load_data_from_file_batching(infile, batch_size):\r\n        input_in_sp = prepare_data_4_sp(labels, features, feature_cnt)\r\n        pickle.dump(input_in_sp, wt)\r\n    wt.close()\r\n\r\n\r\ndef single_run(feature_cnt, field_cnt,  params):\r\n\r\n    print (params)\r\n\r\n    pre_build_data_cache_if_need(params[\'train_file\'], feature_cnt, params[\'batch_size\'])\r\n    pre_build_data_cache_if_need(params[\'test_file\'], feature_cnt, params[\'batch_size\'])\r\n    \r\n    params[\'train_file\'] = params[\'train_file\'].replace(\'.csv\',\'.pkl\').replace(\'.txt\',\'.pkl\')\r\n    params[\'test_file\'] = params[\'test_file\'].replace(\'.csv\',\'.pkl\').replace(\'.txt\',\'.pkl\')\r\n  \r\n    print(\'start single_run\')\r\n    \r\n    tf.reset_default_graph()\r\n\r\n    n_epoch = params[\'n_epoch\']\r\n    batch_size = params[\'batch_size\']\r\n\r\n    _indices = tf.placeholder(tf.int64, shape=[None, 2], name=\'raw_indices\')\r\n    _values = tf.placeholder(tf.float32, shape=[None], name=\'raw_values\')\r\n    _values2 = tf.placeholder(tf.float32, shape=[None], name=\'raw_values_square\')\r\n    _shape = tf.placeholder(tf.int64, shape=[2], name=\'raw_shape\')\r\n\r\n    _y = tf.placeholder(tf.float32, shape=[None, 1], name=\'Y\')\r\n    _ind = tf.placeholder(tf.int64, shape=[None])\r\n    \r\n\r\n    _keep_probs = tf.placeholder(tf.float32, shape=[len(params[\'keep_probs\'])], name = \'dropout_keep_probability\')\r\n    \r\n    train_step, loss, error, preds, merged_summary, tmp = build_model(_indices, _values, _values2, _shape, _y, _ind, _keep_probs,\r\n                                                                 feature_cnt, field_cnt, params)\r\n\r\n    # auc = tf.metrics.auc(_y, preds)\r\n\r\n\r\n    saver = tf.train.Saver()\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init) \r\n\r\n    log_writer = tf.summary.FileWriter(params[\'log_path\'], graph=sess.graph)\r\n\r\n    glo_ite = 0\r\n\r\n    #saver.restore(sess, \'models/[500, 100]0.001-36\')\r\n\r\n    for eopch in range(n_epoch):\r\n        iteration = -1\r\n        start = clock()\r\n\r\n        time_load_data, time_sess = 0, 0\r\n        time_cp02 = clock()\r\n        \r\n        train_loss_per_epoch = 0\r\n       \r\n        for training_input_in_sp in load_data_cache(params[\'train_file\']):            \r\n            time_cp01 = clock()\r\n            time_load_data += time_cp01 - time_cp02\r\n            iteration += 1\r\n            glo_ite += 1\r\n            _,  cur_loss, summary, _tmp = sess.run([train_step,  loss, merged_summary, tmp], feed_dict={\r\n                _indices: training_input_in_sp[\'indices\'], _values: training_input_in_sp[\'values\'],\r\n                _shape: training_input_in_sp[\'shape\'], _y: training_input_in_sp[\'labels\'],\r\n                _values2: training_input_in_sp[\'values2\'], _ind: training_input_in_sp[\'feature_indices\'],\r\n                _keep_probs: np.asarray(params[\'keep_probs\'])\r\n            })\r\n\r\n            time_cp02 = clock()\r\n\r\n            time_sess += time_cp02 - time_cp01\r\n\r\n            train_loss_per_epoch += cur_loss\r\n            \r\n\r\n            log_writer.add_summary(summary, glo_ite)\r\n        end = clock()\r\n        #print(\'time for eopch \', eopch, \' \', ""{0:.4f}min"".format((end - start) / 60.0), \' time_load_data:\', ""{0:.4f}"".format(time_load_data), \' time_sess:\',\r\n        #      ""{0:.4f}"".format(time_sess), \' train_loss: \', train_loss_per_epoch, \' train_error: \', train_error_per_epoch)\r\n        if eopch % 5 == 0:\r\n            model_path = params[\'model_path\'] + ""/"" + str(params[\'layer_sizes\']).replace(\':\', \'_\') + str(\r\n                params[\'reg_w_linear\']).replace(\':\', \'_\')\r\n            os.makedirs(model_path, exist_ok=True)\r\n            saver.save(sess, model_path, global_step=eopch)            \r\n            auc=predict_test_file(preds, sess, params[\'test_file\'], feature_cnt, _indices, _values, _shape, _y,\r\n                              _values2, _ind, _keep_probs,  eopch, batch_size, \'test\', model_path, params[\'output_predictions\'], params)\r\n            print(\'auc is \', auc, \', at epoch  \', eopch, \', time is {0:.4f} min\'.format((end - start) / 60.0)\r\n                  , \', train_loss is {0:.2f}\'.format(train_loss_per_epoch))\r\n            \r\n\r\n    log_writer.close()\r\n\r\n\r\ndef predict_test_file(preds, sess, test_file, feature_cnt, _indices, _values, _shape, _y, _values2, _ind, _keep_probs,  epoch,\r\n                      batch_size, tag, path, output_prediction = True, params = None):\r\n    if output_prediction:\r\n        wt = open(path + \'/deepFM_pred_\' + tag + str(epoch) + \'.txt\', \'w\')\r\n\r\n    gt_scores = []\r\n    pred_scores = []\r\n\r\n    for test_input_in_sp in load_data_cache(test_file):\r\n        predictios = sess.run(preds, feed_dict={\r\n            _indices: test_input_in_sp[\'indices\'], _values: test_input_in_sp[\'values\'],\r\n            _shape: test_input_in_sp[\'shape\'], _y: test_input_in_sp[\'labels\'], _values2: test_input_in_sp[\'values2\'],\r\n            _ind: test_input_in_sp[\'feature_indices\'],\r\n            _keep_probs: np.ones_like(params[\'keep_probs\'])\r\n        }).reshape(-1).tolist()\r\n        \r\n        if output_prediction:\r\n            for (gt, preded) in zip(test_input_in_sp[\'labels\'].reshape(-1).tolist(), predictios):\r\n                wt.write(\'{0:d},{1:f}\\n\'.format(int(gt), preded))\r\n                gt_scores.append(gt)\r\n                #pred_scores.append(1.0 if preded >= 0.5 else 0.0)\r\n                pred_scores.append(preded)\r\n        else:\r\n            gt_scores.extend(test_input_in_sp[\'labels\'].reshape(-1).tolist())\r\n            pred_scores.extend(predictios)\r\n\r\n    auc = roc_auc_score(np.asarray(gt_scores), np.asarray(pred_scores))\r\n    #print(\'auc is \', auc, \', at epoch  \', epoch)\r\n    if output_prediction:\r\n        wt.close()\r\n    return auc\r\n\r\n\r\ndef build_model(_indices, _values, _values2, _shape, _y, _ind, keep_probs, feature_cnt, field_cnt, params):\r\n    eta = tf.constant(params[\'eta\'])\r\n    _x = tf.SparseTensor(_indices, _values, _shape)  # m * feature_cnt sparse tensor\r\n    _xx = tf.SparseTensor(_indices, _values2, _shape)\r\n\r\n    model_params = []\r\n    tmp = []\r\n\r\n    init_value = params[\'init_value\']\r\n    dim = params[\'dim\']\r\n    layer_sizes = params[\'layer_sizes\']\r\n     \r\n\r\n    # w_linear = tf.Variable(tf.truncated_normal([feature_cnt, 1], stddev=init_value, mean=0), name=\'w_linear\',\r\n    #                        dtype=tf.float32)\r\n    w_linear = tf.Variable(tf.truncated_normal([feature_cnt, 1], stddev=init_value, mean=0),  #tf.random_uniform([feature_cnt, 1], minval=-0.05, maxval=0.05), \r\n                        name=\'w_linear\', dtype=tf.float32)\r\n\r\n    bias = tf.Variable(tf.truncated_normal([1], stddev=init_value, mean=0), name=\'bias\')\r\n    model_params.append(bias)\r\n    model_params.append(w_linear)\r\n    preds = bias\r\n    # linear part\r\n    preds += tf.sparse_tensor_dense_matmul(_x, w_linear, name=\'contr_from_linear\')\r\n\r\n    w_fm = tf.Variable(tf.truncated_normal([feature_cnt, dim], stddev=init_value / math.sqrt(float(dim)), mean=0),\r\n                           name=\'w_fm\', dtype=tf.float32)\r\n    model_params.append(w_fm)\r\n    # fm order 2 interactions\r\n    if params[\'is_use_fm_part\']:  \r\n        preds = preds + 0.5 * tf.reduce_sum(\r\n            tf.pow(tf.sparse_tensor_dense_matmul(_x, w_fm), 2) - tf.sparse_tensor_dense_matmul(_xx, tf.pow(w_fm, 2)), 1,\r\n            keep_dims=True)\r\n\r\n    ## deep neural network\r\n    if params[\'is_use_dnn_part\']:\r\n        w_fm_nn_input = tf.reshape(tf.gather(w_fm, _ind) * tf.expand_dims(_values, 1), [-1, field_cnt * dim])\r\n        print(w_fm_nn_input.shape)\r\n\r\n        # tmp.append(tf.shape(tf.expand_dims(_values, 1)))\r\n        # tmp.append(tf.shape(w_fm_nn_input))\r\n        # tmp.append(tf.shape(tf.gather(w_fm, _ind) * tf.expand_dims(_values, 1)))\r\n        # tmp.append(tf.shape(tf.gather(w_fm, _ind)))\r\n\r\n\r\n        #w_nn_layers = []\r\n        hidden_nn_layers = []\r\n        hidden_nn_layers.append(w_fm_nn_input)\r\n        last_layer_size = field_cnt * dim\r\n        layer_idx = 0\r\n\r\n        w_nn_params = []\r\n        b_nn_params = []\r\n\r\n        for layer_size in layer_sizes:\r\n            cur_w_nn_layer = tf.Variable(\r\n                tf.truncated_normal([last_layer_size, layer_size], stddev=init_value / math.sqrt(float(10)), mean=0),\r\n                name=\'w_nn_layer\' + str(layer_idx), dtype=tf.float32)\r\n\r\n            cur_b_nn_layer = tf.Variable(tf.truncated_normal([layer_size], stddev=init_value, mean=0), name=\'b_nn_layer\' + str(layer_idx)) #tf.get_variable(\'b_nn_layer\' + str(layer_idx), [layer_size], initializer=tf.constant_initializer(0.0)) \r\n\r\n            cur_hidden_nn_layer = tf.nn.xw_plus_b(hidden_nn_layers[layer_idx], cur_w_nn_layer, cur_b_nn_layer)\r\n            \r\n            cur_hidden_nn_layer = tf.nn.dropout(cur_hidden_nn_layer, keep_probs[layer_idx])\r\n            \r\n            if params[\'activations\'][layer_idx]==\'tanh\':\r\n                cur_hidden_nn_layer = tf.nn.tanh(cur_hidden_nn_layer)\r\n            elif params[\'activations\'][layer_idx]==\'sigmoid\':\r\n                cur_hidden_nn_layer = tf.nn.sigmoid(cur_hidden_nn_layer)\r\n            elif params[\'activations\'][layer_idx]==\'relu\':\r\n                cur_hidden_nn_layer = tf.nn.relu(cur_hidden_nn_layer)\r\n            \r\n            #cur_hidden_nn_layer = tf.matmul(hidden_nn_layers[layer_idx], cur_w_nn_layer)\r\n            #w_nn_layers.append(cur_w_nn_layer)\r\n            hidden_nn_layers.append(cur_hidden_nn_layer)\r\n\r\n            layer_idx += 1\r\n            last_layer_size = layer_size\r\n\r\n            model_params.append(cur_w_nn_layer)\r\n            model_params.append(cur_b_nn_layer)\r\n            w_nn_params.append(cur_w_nn_layer)\r\n            b_nn_params.append(cur_b_nn_layer)\r\n\r\n\r\n        w_nn_output = tf.Variable(tf.truncated_normal([last_layer_size, 1], stddev=init_value, mean=0), name=\'w_nn_output\',\r\n                                  dtype=tf.float32)\r\n        nn_output = tf.matmul(hidden_nn_layers[-1], w_nn_output)\r\n        model_params.append(w_nn_output)\r\n        w_nn_params.append(w_nn_output)\r\n\r\n        preds += nn_output\r\n\r\n    if params[\'loss\'] == \'cross_entropy_loss\': # \'loss\': \'log_loss\'\r\n        error = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(preds,[-1])\r\n                                                                       , labels=tf.reshape(_y,[-1])))\r\n    elif params[\'loss\'] == \'square_loss\':\r\n        preds = tf.sigmoid(preds)\r\n        error = tf.reduce_mean(tf.squared_difference(preds, _y))  \r\n    elif params[\'loss\'] == \'log_loss\':\r\n        preds = tf.sigmoid(preds)\r\n        error = tf.reduce_mean(tf.losses.log_loss(predictions=preds,labels=_y))\r\n\r\n    lambda_w_linear = tf.constant(params[\'reg_w_linear\'], name=\'lambda_w_linear\')\r\n    lambda_w_fm = tf.constant(params[\'reg_w_fm\'], name=\'lambda_w_fm\')\r\n    lambda_w_nn = tf.constant(params[\'reg_w_nn\'], name=\'lambda_nn_fm\')\r\n    lambda_w_l1 = tf.constant(params[\'reg_w_l1\'], name=\'lambda_w_l1\')\r\n\r\n    # l2_norm = tf.multiply(lambda_w_linear, tf.pow(bias, 2)) + tf.reduce_sum(\r\n    #     tf.add(tf.multiply(lambda_w_linear, tf.pow(w_linear, 2)),\r\n    #            tf.multiply(lambda_w_fm, tf.pow(w_fm, 2)))) + tf.reduce_sum(\r\n    #     tf.multiply(lambda_w_nn, tf.pow(w_nn_output, 2)))\r\n\r\n    # l2_norm = tf.multiply(lambda_w_linear, tf.pow(bias, 2)) \\\r\n    #           + tf.multiply(lambda_w_linear, tf.reduce_sum(tf.pow(w_linear, 2)))\r\n\r\n    l2_norm = tf.multiply(lambda_w_linear, tf.reduce_sum(tf.pow(w_linear, 2))) \r\n    l2_norm += tf.multiply(lambda_w_l1, tf.reduce_sum(tf.abs(w_linear)))\r\n\r\n    if params[\'is_use_fm_part\'] or params[\'is_use_dnn_part\']:\r\n        l2_norm += tf.multiply(lambda_w_fm, tf.reduce_sum(tf.pow(w_fm, 2)))\r\n\r\n    if params[\'is_use_dnn_part\']:\r\n        for i in range(len(w_nn_params)):\r\n            l2_norm += tf.multiply(lambda_w_nn, tf.reduce_sum(tf.pow(w_nn_params[i], 2)))\r\n\r\n        for i in range(len(b_nn_params)):\r\n            l2_norm += tf.multiply(lambda_w_nn, tf.reduce_sum(tf.pow(b_nn_params[i], 2)))\r\n\r\n\r\n    # tmp.append(tf.shape(tf.pow(w_linear, 2)))\r\n    # tmp.append(tf.shape(tf.pow(w_fm, 2)))\r\n\r\n\r\n\r\n    loss = tf.add(error, l2_norm)\r\n    if params[\'optimizer\']==\'adadelta\':\t\r\n        train_step = tf.train.AdadeltaOptimizer(eta).minimize(loss,var_list=model_params)#\r\n    elif params[\'optimizer\']==\'sgd\':\r\n        train_step = tf.train.GradientDescentOptimizer(params[\'learning_rate\']).minimize(loss,var_list=model_params)\r\n    elif params[\'optimizer\']==\'adam\':\r\n        train_step = tf.train.AdamOptimizer(params[\'learning_rate\']).minimize(loss,var_list=model_params)\r\n    elif params[\'optimizer\']==\'ftrl\':\r\n        train_step = tf.train.FtrlOptimizer(params[\'learning_rate\']).minimize(loss,var_list=model_params)\r\n    else:\r\n        train_step = tf.train.GradientDescentOptimizer(params[\'learning_rate\']).minimize(loss,var_list=model_params)\t    \r\n\r\n    tf.summary.scalar(\'square_error\', error)\r\n    tf.summary.scalar(\'loss\', loss)\r\n    tf.summary.histogram(\'linear_weights_hist\', w_linear)\r\n\r\n    if params[\'is_use_fm_part\']:\r\n        tf.summary.histogram(\'fm_weights_hist\', w_fm)\r\n    if params[\'is_use_dnn_part\']:\r\n        for idx in range(len(w_nn_params))  :\r\n            tf.summary.histogram(\'nn_layer\'+str(idx)+\'_weights\', w_nn_params[idx])\r\n        \r\n    merged_summary = tf.summary.merge_all()\r\n\r\n\r\n    return train_step, loss, error, preds, merged_summary, tmp\r\n\r\ndef pre_build_data_cache_if_need(infile, feature_cnt, batch_size):\r\n    outfile = infile.replace(\'.csv\',\'.pkl\').replace(\'.txt\',\'.pkl\')\r\n    if not os.path.isfile(outfile):\r\n        print(\'pre_build_data_cache for \', infile)\r\n        pre_build_data_cache(infile, outfile, feature_cnt, batch_size)\r\n        print(\'pre_build_data_cache finished.\' )\r\n\r\ndef run():\r\n    # train_file = r\'\\\\mlsdata\\e$\\Users\\v-lianji\\DeepRecsys\\DeepFM\\part01.svmlight_balanced.csv\'\r\n    # test_file = r\'\\\\mlsdata\\e$\\Users\\v-lianji\\DeepRecsys\\DeepFM\\part02.svmlight.csv\'\r\n\r\n    print (\'begin running\')\r\n\r\n    field_cnt = 46 #83\r\n    feature_cnt = 46 #5000\r\n\r\n\r\n    params = {\r\n        \'reg_w_linear\': 0.00010, \'reg_w_fm\':0.0001, \'reg_w_nn\': 0.0001,  #0.001\r\n        \'reg_w_l1\': 0.0001,\r\n        \'init_value\': 0.1,\r\n        \'layer_sizes\': [10, 5],\r\n        \'keep_probs\':[0.7,0.7],  # dropout setting\r\n        \'activations\':[\'tanh\',\'tanh\'],\r\n        \'eta\': 0.1,\r\n        \'n_epoch\': 5000,  # 500\r\n        \'batch_size\': 50,\r\n        \'dim\': 8,\r\n        \'model_path\': \'models\',\r\n        \'log_path\': \'logs/\' + datetime.utcnow().strftime(\'%Y-%m-%d_%H_%M_%S\'),\r\n        \'train_file\':  \'data/S1_4.txt\',  #\'data/part01.svmlight_balanced.csv\',\r\n        \'test_file\':    \'data/S5.txt\',#\'data/part02.svmlight.csv\',\r\n        \'output_predictions\':False,\r\n        \'is_use_fm_part\':True,\r\n        \'is_use_dnn_part\':True,\r\n        \'learning_rate\':0.01, # [0.001, 0.01]\r\n        \'loss\': \'log_loss\', # [cross_entropy_loss, square_loss, log_loss]\r\n        \'optimizer\':\'sgd\' # [adam, ftrl, sgd]\r\n    }\r\n  \r\n\r\n    single_run(feature_cnt, field_cnt, params)\r\n\r\n        \r\nif __name__ == \'__main__\':\r\n    run()\r\n\r\n'"
models/deepFM_BOW.py,75,"b'\'\'\'\r\nCreated on May 14, 2017\r\n\r\n@author: v-lianji\r\n\r\nUpon the deepFM.py, in this file, we make the model support multiple sparse values within one field.\r\nThe input format is : label field_idx:feature_idx:value ...  \r\nFor all instances, number of field is fixed. However, different instances may have different number of active feautres under each field.\r\n\r\nReferring https://github.com/Leavingseason/OpenLearning4DeepRecsys/issues/10 \r\n\'\'\'\r\n\r\nimport tensorflow as tf\r\nimport math\r\nfrom time import clock\r\nimport numpy as np\r\nimport sys\r\nimport os\r\nimport pickle\r\nimport sys\r\nfrom sklearn import metrics\r\nfrom sklearn.metrics import roc_auc_score\r\nfrom datetime import datetime\r\nimport logging\r\nimport platform\r\nimport random\r\n\r\n\r\nFIELD_COUNT =  45 #46\r\nFEATURE_COUNT = 100000 #46\r\n\r\nos.makedirs(\'logs/\', exist_ok=True)\r\nlogging_filename = \'logs/\' + platform.node() + \'__\' + datetime.utcnow().strftime(\'%Y-%m-%d_%H_%M_%S\') + \'.log\'\r\nlogger = logging.getLogger(__name__)\r\nlogger.setLevel(logging.INFO)\r\nhandler = logging.FileHandler(logging_filename)\r\nhandler.setLevel(logging.INFO)\r\nformatter = logging.Formatter(\'%(message)s\')\r\nhandler.setFormatter(formatter)\r\nlogger.addHandler(handler)\r\n\r\n\r\nhandler02 = logging.StreamHandler()\r\nhandler02.setLevel(logging.INFO)\r\nhandler02.setFormatter(formatter)\r\nlogger.addHandler(handler02)\r\n\r\n\r\n\r\ndef load_data_from_file_batching(file, batch_size):\r\n    labels = []\r\n    features = []\r\n    qids = []\r\n    docids = []\r\n    cnt = 0\r\n    with open(file, \'r\') as rd:\r\n        while True:\r\n            line = rd.readline().strip()\r\n            if not line:\r\n                break\r\n            cnt += 1\r\n            if \'#\' in line:\r\n                punc_idx = line.index(\'#\')\r\n            else:\r\n                punc_idx = len(line)\r\n\r\n            before_comment_line = line[:punc_idx].strip()\r\n            after_comment_line = line[punc_idx + 1:].strip()\r\n\r\n            cols = before_comment_line.split()\r\n            label = float(cols[0])\r\n            if label > 0:\r\n                label = 1\r\n            else:\r\n                label = 0\r\n            words = []\r\n            for col in cols[1:]:\r\n                if col.startswith(\'qid:\'):\r\n                    qids.append(col)\r\n                else:\r\n                    words.append(col)\r\n            cur_feature_list = []\r\n            for word in words:\r\n                if not word:\r\n                    continue\r\n                tokens = word.split(\':\')\r\n\r\n                if len(tokens[2]) <= 0:\r\n                    tokens[2] = \'0\'\r\n                cur_feature_list.append([int(tokens[0]) - 1, int(tokens[1]) - 1, float(tokens[2])])\r\n            features.append(cur_feature_list)\r\n            labels.append(label)\r\n            if len(after_comment_line) > 0:\r\n                docids.append(after_comment_line)\r\n\r\n            if len(qids)<len(labels):\r\n                qids.append(\'qid:fake\')\r\n                \r\n            if cnt == batch_size:\r\n                yield labels, features, qids, docids\r\n                labels = []\r\n                features = []\r\n                qids = []\r\n                docids = []\r\n                cnt = 0\r\n    if cnt > 0:\r\n        yield labels, features, qids, docids\r\n\r\n\r\ndef prepare_data_4_sp(labels, features, dim):\r\n    instance_cnt = len(labels)\r\n\r\n    indices = []\r\n    values = []\r\n    values_2 = []\r\n    shape = [instance_cnt, dim]\r\n    field2feature_indices = []\r\n    field2feature_values = []\r\n    field2feature_weights = []\r\n    filed2feature_shape = [instance_cnt * FIELD_COUNT, -1]\r\n\r\n    lastidx = 0 \r\n    for i in range(instance_cnt):\r\n        m = len(features[i])\r\n        field2features_dic = {}\r\n        for j in range(m):\r\n            indices.append([i, features[i][j][1]])\r\n            values.append(features[i][j][2])\r\n            values_2.append(features[i][j][2] * features[i][j][2])\r\n            #feature_indices.append(features[i][j][1])\r\n            if features[i][j][0] not in field2features_dic:\r\n                field2features_dic[features[i][j][0]] = 0\r\n            else:\r\n                field2features_dic[features[i][j][0]] += 1\r\n            cur_idx = i * FIELD_COUNT + features[i][j][0] \r\n            #if lastidx<cur_idx-1 or lastidx>cur_idx:\r\n            #    print(\'lastidx \',lastidx, \' curidx \',cur_idx, \' fieldidx \',features[i][j][0], \'features \',features[i] )\r\n            if lastidx<cur_idx:\r\n                lastidx = cur_idx\r\n            field2feature_indices.append([i * FIELD_COUNT + features[i][j][0], field2features_dic[features[i][j][0]]])\r\n            field2feature_values.append(features[i][j][1])\r\n            field2feature_weights.append(features[i][j][2] ) \r\n            if filed2feature_shape[1] < field2features_dic[features[i][j][0]]:\r\n                filed2feature_shape[1] = field2features_dic[features[i][j][0]]\r\n    filed2feature_shape[1] += 1\r\n\r\n    sorted_index = sorted(range(len(field2feature_indices)), key=lambda k: (field2feature_indices[k][0],field2feature_indices[k][1]))\r\n\r\n\r\n\r\n    res = {}\r\n    res[\'indices\'] = np.asarray(indices, dtype=np.int64)\r\n    res[\'values\'] = np.asarray(values, dtype=np.float32)\r\n    res[\'values2\'] = np.asarray(values_2, dtype=np.float32)\r\n    res[\'shape\'] = np.asarray(shape, dtype=np.int64)\r\n    res[\'labels\'] = np.asarray([[label] for label in labels], dtype=np.float32)\r\n    res[\'field2feature_indices\'] = np.asarray(field2feature_indices, dtype=np.int64)[sorted_index]\r\n    res[\'field2feature_values\'] = np.asarray(field2feature_values, dtype=np.int64)[sorted_index]\r\n    res[\'field2feature_weights\'] = np.asarray(field2feature_weights, dtype=np.float32)[sorted_index]\r\n    res[\'filed2feature_shape\'] = np.asarray(filed2feature_shape, dtype=np.int64)\r\n\r\n    return res\r\n\r\n\r\ndef load_data_cache(filename):\r\n    with open(filename, ""rb"") as f:\r\n        while True:\r\n            try:\r\n                yield pickle.load(f)\r\n            except EOFError:\r\n                break\r\n\r\n\r\ndef pre_build_data_cache(infile, outfile, batch_size):\r\n    wt = open(outfile, \'wb\')\r\n    for labels, features, qids, docids in load_data_from_file_batching(infile, batch_size):\r\n        input_in_sp = prepare_data_4_sp(labels, features, FEATURE_COUNT)\r\n        pickle.dump((input_in_sp, qids, docids), wt)\r\n    wt.close()\r\n\r\n\r\ndef single_run(params):\r\n \r\n    logger.info(\'\\n\\n\')\r\n    logger.info(params)\r\n    logger.info(\'\\n\\n\')\r\n\r\n\r\n    pre_build_data_cache_if_need(params[\'train_file\'], params[\'batch_size\'], params[\'clean_cache\'] if \'clean_cache\' in params else False)\r\n    pre_build_data_cache_if_need(params[\'test_file\'], params[\'batch_size\'], params[\'clean_cache\'] if \'clean_cache\' in params else False)\r\n    \r\n    params[\'train_file\'] = params[\'train_file\'].replace(\'.csv\',\'.pkl\').replace(\'.txt\',\'.pkl\')\r\n    params[\'test_file\'] = params[\'test_file\'].replace(\'.csv\',\'.pkl\').replace(\'.txt\',\'.pkl\')\r\n  \r\n    print(\'start single_run\')\r\n    \r\n    tf.reset_default_graph()\r\n\r\n    n_epoch = params[\'n_epoch\']\r\n    batch_size = params[\'batch_size\']\r\n\r\n    _indices = tf.placeholder(tf.int64, shape=[None, 2], name=\'raw_indices\')\r\n    _values = tf.placeholder(tf.float32, shape=[None], name=\'raw_values\')\r\n    _values2 = tf.placeholder(tf.float32, shape=[None], name=\'raw_values_square\')\r\n    _shape = tf.placeholder(tf.int64, shape=[2], name=\'raw_shape\')\r\n\r\n    _field2feature_indices = tf.placeholder(tf.int64, shape=[None, 2], name=\'field2feature_indices\')\r\n    _field2feature_values = tf.placeholder(tf.int64, shape=[None], name=\'field2feature_values\')\r\n    _field2feature_weights = tf.placeholder(tf.float32, shape=[None], name=\'field2feature_weights\')\r\n    _field2feature_shape = tf.placeholder(tf.int64, shape=[2], name=\'field2feature_shape\')\r\n\r\n    _y = tf.placeholder(tf.float32, shape=[None, 1], name=\'Y\')\r\n\r\n\r\n    train_step, loss, error, preds, tmp = build_model(_indices, _values, _values2, _shape\r\n                                                                      , _field2feature_indices, _field2feature_values, _field2feature_weights, _field2feature_shape\r\n                                                                      , _y, params)\r\n\r\n    # auc = tf.metrics.auc(_y, preds)\r\n\r\n\r\n    saver = tf.train.Saver()\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    # log_writer = tf.summary.FileWriter(params[\'log_path\'], graph=sess.graph)\r\n\r\n    glo_ite = 0\r\n\r\n    last_best_auc = None\r\n    max_stop_grow_torrelence = 50\r\n    stop_grow_cnt = 0\r\n\r\n\r\n    #saver.restore(sess, \'models/[500, 100]0.001-36\')\r\n    start = clock()\r\n    for eopch in range(n_epoch):\r\n        iteration = -1\r\n        \r\n\r\n        time_load_data, time_sess = 0, 0\r\n        time_cp02 = clock()\r\n        \r\n        train_loss_per_epoch = 0\r\n       \r\n        for training_input_in_sp, qids, docids in load_data_cache(params[\'train_file\']):\r\n            \r\n            #if random.random()<0.8:\r\n            #    continue\r\n            \r\n            #print(\'training_input_in_sp=\',training_input_in_sp)\r\n            #sys.exit()\r\n            time_cp01 = clock()\r\n            time_load_data += time_cp01 - time_cp02\r\n            iteration += 1\r\n            glo_ite += 1\r\n            _,  cur_loss = sess.run([train_step,  loss], feed_dict={\r\n                _indices: training_input_in_sp[\'indices\'], _values: training_input_in_sp[\'values\'],\r\n                _shape: training_input_in_sp[\'shape\'], _y: training_input_in_sp[\'labels\'],\r\n                _values2: training_input_in_sp[\'values2\']\r\n                , _field2feature_indices: training_input_in_sp[\'field2feature_indices\']\r\n                , _field2feature_values: training_input_in_sp[\'field2feature_values\']\r\n                , _field2feature_weights: training_input_in_sp[\'field2feature_weights\']\r\n                , _field2feature_shape: training_input_in_sp[\'filed2feature_shape\']\r\n            })\r\n\r\n\r\n            time_cp02 = clock()\r\n\r\n            time_sess += time_cp02 - time_cp01\r\n\r\n            train_loss_per_epoch += cur_loss\r\n            \r\n\r\n            # log_writer.add_summary(summary, glo_ite)\r\n        end = clock()\r\n        #print(\'time for eopch \', eopch, \' \', ""{0:.4f}min"".format((end - start) / 60.0), \' time_load_data:\', ""{0:.4f}"".format(time_load_data), \' time_sess:\',\r\n        #      ""{0:.4f}"".format(time_sess), \' train_loss: \', train_loss_per_epoch, \' train_error: \', train_error_per_epoch)\r\n        if eopch % 1 == 0:\r\n            model_path = params[\'model_path\'] + ""/"" + str(params[\'layer_sizes\']).replace(\':\', \'_\') + str(\r\n                params[\'reg_w_linear\']).replace(\':\', \'_\')\r\n            os.makedirs(model_path, exist_ok=True)\r\n            saver.save(sess, model_path, global_step=eopch)            \r\n            metrics=predict_test_file(preds, sess, params[\'test_file\'], _indices, _values, _shape, _y,\r\n                              _values2, _field2feature_indices, _field2feature_values,_field2feature_weights\r\n                                  , _field2feature_shape, eopch, batch_size, \'test\', model_path, params[\'output_predictions\']\r\n                                  , params)\r\n            metrics_strs = []\r\n            auc = 0\r\n            for metric_name in metrics:\r\n                metrics_strs.append(\'{0} is {1:.5f}\'.format(metric_name, metrics[metric_name]))\r\n                if metric_name == \'global_auc\':\r\n                    auc = metrics[metric_name]\r\n                \r\n            if last_best_auc is None or auc>last_best_auc:\r\n                last_best_auc = auc \r\n                stop_grow_cnt = 0 \r\n            else:\r\n                stop_grow_cnt+=1\r\n                \r\n            res_str = \' ,\'.join(metrics_strs) + \', at epoch {0:d}, time is {1:.4f} min, train_loss is {2:.2f}\'.format(eopch, (end -start) / 60.0, train_loss_per_epoch)\r\n\r\n            logger.info(res_str)\r\n            start = clock()\r\n            \r\n            if stop_grow_cnt>max_stop_grow_torrelence:\r\n                break \r\n\r\n \r\n\r\ndef predict_test_file(preds, sess, test_file, _indices, _values, _shape, _y, _values2, _field2feature_indices, _field2feature_values,_field2feature_weights\r\n                                  , _field2feature_shape, epoch,\r\n                      batch_size, tag, path, output_prediction, params):\r\n    if output_prediction:\r\n        wt = open(path + \'/deepFM_pred_\' + tag + str(epoch) + \'.txt\', \'w\')\r\n\r\n    gt_scores = []\r\n    pred_scores = []\r\n\r\n    query2res = {}\r\n\r\n    for test_input_in_sp, qids, docids in load_data_cache(test_file):\r\n\r\n        predictios = sess.run(preds, feed_dict={\r\n            _indices: test_input_in_sp[\'indices\'], _values: test_input_in_sp[\'values\'],\r\n            _shape: test_input_in_sp[\'shape\'], _y: test_input_in_sp[\'labels\'], _values2: test_input_in_sp[\'values2\'],\r\n                _field2feature_indices: test_input_in_sp[\'field2feature_indices\']\r\n                , _field2feature_values: test_input_in_sp[\'field2feature_values\']\r\n                , _field2feature_weights: test_input_in_sp[\'field2feature_weights\']\r\n                , _field2feature_shape: test_input_in_sp[\'filed2feature_shape\']\r\n        }).reshape(-1).tolist()\r\n        \r\n        if output_prediction:\r\n            for (gt, preded, qid) in zip(test_input_in_sp[\'labels\'].reshape(-1).tolist(), predictios, qids):\r\n                wt.write(\'{0:d},{1:f}\\n\'.format(int(gt), preded))\r\n                gt_scores.append(gt)\r\n                #pred_scores.append(1.0 if preded >= 0.5 else 0.0)\r\n                pred_scores.append(preded)\r\n        else:\r\n            for (gt, preded, qid) in zip(test_input_in_sp[\'labels\'].reshape(-1).tolist(), predictios, qids):\r\n                if qid not in query2res:\r\n                    query2res[qid] = []\r\n                query2res[qid].append([gt, preded])\r\n\r\n    metrics = compute_metric(query2res, params)\r\n\r\n    if output_prediction:\r\n        wt.close()\r\n    return metrics\r\n\r\n\r\ndef compute_metric(query2res, params):\r\n    result = {}\r\n\r\n    for m in params[\'metrics\']:\r\n        if \'global_auc\' in m[\'name\']:\r\n            gt_scores = []\r\n            pred_scores = []\r\n            for qid in query2res:\r\n                gt_scores.extend([x[0] for x in query2res[qid]] )\r\n                pred_scores.extend([x[1] for x in query2res[qid]] )\r\n            #print(\'gt_scores \',gt_scores) \r\n            #print(\'pred_scores \',pred_scores)   \r\n            result[\'global_auc\'] = roc_auc_score(np.asarray(gt_scores), np.asarray(pred_scores))\r\n        elif \'individual_auc\' in m[\'name\']:\r\n            aucs = []\r\n            for qid in query2res:\r\n                gt_scores = np.asarray([x[0] for x in query2res[qid]])\r\n                if gt_scores.min() > 0 or gt_scores.max() < 1:\r\n                    continue\r\n                pred_scores = [x[1] for x in query2res[qid]]\r\n                aucs.append(roc_auc_score(gt_scores, np.asarray(pred_scores)))\r\n            result[\'individual_auc\'] = np.asarray(aucs).mean()\r\n        elif \'precision\' in m[\'name\']:\r\n            precisions = []\r\n            for qid in query2res:\r\n                k = min(m[\'k\'], len(query2res[qid]))\r\n                gt_scores = np.asarray([x[0] for x in query2res[qid]])\r\n                pred_scores = np.asarray([x[1] for x in query2res[qid]])\r\n                precision = gt_scores[np.argsort(pred_scores)[::-1][:k]].mean()\r\n\r\n                precisions.append(precision)\r\n            result[\'precision_at_\' + str(m[\'k\'])] = np.asarray(precisions).mean()\r\n\r\n\r\n    return result\r\n\r\n\r\ndef build_model(_indices, _values, _values2, _shape, _field2feature_indices, _field2feature_values,_field2feature_weights, _field2feature_shape, _y, params):\r\n    eta = tf.constant(params[\'eta\'])\r\n    _x = tf.SparseTensor(_indices, _values, _shape)  # m * FEATURE_COUNT sparse tensor\r\n    _xx = tf.SparseTensor(_indices, _values2, _shape)\r\n\r\n    model_params = []\r\n    tmp = []\r\n\r\n    init_value = params[\'init_value\']\r\n    dim = params[\'dim\']\r\n    layer_sizes = params[\'layer_sizes\']\r\n\r\n    # w_linear = tf.Variable(tf.truncated_normal([feature_cnt, 1], stddev=init_value, mean=0), name=\'w_linear\',\r\n    #                        dtype=tf.float32)\r\n    w_linear = tf.Variable(tf.truncated_normal([FEATURE_COUNT, 1], stddev=init_value, mean=0),  #tf.random_uniform([FEATURE_COUNT, 1], minval=-0.05, maxval=0.05),\r\n                        name=\'w_linear\', dtype=tf.float32)\r\n\r\n    bias = tf.Variable(tf.truncated_normal([1], stddev=init_value, mean=0), name=\'bias\')\r\n    model_params.append(bias)\r\n    model_params.append(w_linear)\r\n    preds = bias\r\n    # linear part\r\n    preds += tf.sparse_tensor_dense_matmul(_x, w_linear, name=\'contr_from_linear\')\r\n\r\n    w_fm = tf.Variable(tf.truncated_normal([FEATURE_COUNT, dim], stddev=init_value / math.sqrt(float(dim)), mean=0),\r\n                           name=\'w_fm\', dtype=tf.float32)\r\n    model_params.append(w_fm)\r\n    # fm order 2 interactions\r\n    if params[\'is_use_fm_part\']:  \r\n        preds = preds + 0.5 * tf.reduce_sum(\r\n            tf.pow(tf.sparse_tensor_dense_matmul(_x, w_fm), 2) - tf.sparse_tensor_dense_matmul(_xx, tf.pow(w_fm, 2)), 1,\r\n            keep_dims=True)\r\n\r\n    w_nn_params = []\r\n    b_nn_params = []\r\n    ## deep neural network  \r\n    if params[\'is_use_dnn_part\']:\r\n        # w_fm_nn_input = tf.reshape(tf.gather(w_fm, _ind) * tf.expand_dims(_values, 1), [-1, FIELD_COUNT * dim])\r\n        # print(w_fm_nn_input.shape)\r\n\r\n        w_fm_sparseIndexs = tf.SparseTensor(_field2feature_indices, _field2feature_values, _field2feature_shape)\r\n        w_fm_sparseWeights = tf.SparseTensor(_field2feature_indices, _field2feature_weights, _field2feature_shape)\r\n        w_fm_nn_input_orgin = tf.nn.embedding_lookup_sparse(w_fm, w_fm_sparseIndexs,w_fm_sparseWeights,combiner=""sum"")\r\n        w_fm_nn_input = tf.reshape(w_fm_nn_input_orgin, [-1, dim * FIELD_COUNT]) \r\n\r\n\r\n        hidden_nn_layers = []\r\n        hidden_nn_layers.append(w_fm_nn_input)\r\n        last_layer_size = FIELD_COUNT * dim\r\n        layer_idx = 0\r\n \r\n\r\n        for layer_size in layer_sizes:\r\n            cur_w_nn_layer = tf.Variable(\r\n                tf.truncated_normal([last_layer_size, layer_size], stddev=init_value / math.sqrt(float(10)), mean=0),\r\n                name=\'w_nn_layer\' + str(layer_idx), dtype=tf.float32)\r\n\r\n            cur_b_nn_layer = tf.Variable(tf.truncated_normal([layer_size], stddev=init_value, mean=0), name=\'b_nn_layer\' + str(layer_idx)) #tf.get_variable(\'b_nn_layer\' + str(layer_idx), [layer_size], initializer=tf.constant_initializer(0.0)) \r\n\r\n            cur_hidden_nn_layer = tf.nn.xw_plus_b(hidden_nn_layers[layer_idx], cur_w_nn_layer, cur_b_nn_layer)\r\n            \r\n            if params[\'activations\'][layer_idx]==\'tanh\':\r\n                cur_hidden_nn_layer = tf.nn.tanh(cur_hidden_nn_layer)\r\n            elif params[\'activations\'][layer_idx]==\'sigmoid\':\r\n                cur_hidden_nn_layer = tf.nn.sigmoid(cur_hidden_nn_layer)\r\n            elif params[\'activations\'][layer_idx]==\'relu\':\r\n                cur_hidden_nn_layer = tf.nn.relu(cur_hidden_nn_layer)\r\n            \r\n            #cur_hidden_nn_layer = tf.matmul(hidden_nn_layers[layer_idx], cur_w_nn_layer)\r\n            #w_nn_layers.append(cur_w_nn_layer)\r\n            hidden_nn_layers.append(cur_hidden_nn_layer)\r\n\r\n            layer_idx += 1\r\n            last_layer_size = layer_size\r\n\r\n            model_params.append(cur_w_nn_layer)\r\n            model_params.append(cur_b_nn_layer)\r\n            w_nn_params.append(cur_w_nn_layer)\r\n            b_nn_params.append(cur_b_nn_layer)\r\n\r\n\r\n        w_nn_output = tf.Variable(tf.truncated_normal([last_layer_size, 1], stddev=init_value, mean=0), name=\'w_nn_output\',\r\n                                  dtype=tf.float32)\r\n        nn_output = tf.matmul(hidden_nn_layers[-1], w_nn_output)\r\n        model_params.append(w_nn_output)\r\n        w_nn_params.append(w_nn_output)\r\n\r\n        preds += nn_output\r\n\r\n    if params[\'loss\'] == \'cross_entropy_loss\': # \'loss\': \'log_loss\'\r\n        error = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(preds,[-1])\r\n                                                                       , labels=tf.reshape(_y,[-1])))\r\n    elif params[\'loss\'] == \'square_loss\':\r\n        preds = tf.sigmoid(preds)\r\n        error = tf.reduce_mean(tf.squared_difference(preds, _y))  \r\n    elif params[\'loss\'] == \'log_loss\':\r\n        preds = tf.sigmoid(preds)\r\n        error = tf.reduce_mean(tf.losses.log_loss(predictions=preds,labels=_y))\r\n\r\n    lambda_w_linear = tf.constant(params[\'reg_w_linear\'], name=\'lambda_w_linear\')\r\n    lambda_w_fm = tf.constant(params[\'reg_w_fm\'], name=\'lambda_w_fm\')\r\n    lambda_w_nn = tf.constant(params[\'reg_w_nn\'], name=\'lambda_nn_fm\')\r\n    lambda_w_l1 = tf.constant(params[\'reg_w_l1\'], name=\'lambda_w_l1\')\r\n\r\n    l2_norm = tf.multiply(lambda_w_linear, tf.reduce_sum(tf.pow(w_linear, 2))) \r\n    l2_norm += tf.multiply(lambda_w_l1, tf.reduce_sum(tf.abs(w_linear)))\r\n\r\n\r\n    if params[\'is_use_fm_part\'] or params[\'is_use_dnn_part\'] or params[\'is_multi_level\']:\r\n        l2_norm += lambda_w_fm * tf.nn.l2_loss(w_fm)\r\n        #l2_norm += tf.multiply(lambda_w_fm, tf.reduce_sum(tf.pow(w_fm, 2)))\r\n\r\n    if params[\'is_use_dnn_part\'] or params[\'is_multi_level\']:\r\n        for i in range(len(w_nn_params)):\r\n            l2_norm += lambda_w_nn * tf.nn.l2_loss(w_nn_params[i])\r\n            #l2_norm += tf.multiply(lambda_w_nn, tf.reduce_sum(tf.pow(w_nn_params[i], 2)))\r\n\r\n        for i in range(len(b_nn_params)):\r\n            l2_norm += lambda_w_nn * tf.nn.l2_loss(b_nn_params[i])\r\n            #l2_norm += tf.multiply(lambda_w_nn, tf.reduce_sum(tf.pow(b_nn_params[i], 2)))\r\n\r\n\r\n\r\n    loss = tf.add(error, l2_norm)\r\n    if params[\'optimizer\']==\'adadelta\':    \r\n        train_step = tf.train.AdadeltaOptimizer(eta).minimize(loss,var_list=model_params)#\r\n    elif params[\'optimizer\']==\'sgd\':\r\n        train_step = tf.train.GradientDescentOptimizer(params[\'learning_rate\']).minimize(loss,var_list=model_params)\r\n    elif params[\'optimizer\']==\'adam\':\r\n        train_step = tf.train.AdamOptimizer(params[\'learning_rate\']).minimize(loss,var_list=model_params)\r\n    elif params[\'optimizer\']==\'ftrl\':\r\n        train_step = tf.train.FtrlOptimizer(params[\'learning_rate\']).minimize(loss,var_list=model_params)\r\n    else:\r\n        train_step = tf.train.GradientDescentOptimizer(params[\'learning_rate\']).minimize(loss,var_list=model_params)        \r\n\r\n    # tf.summary.scalar(\'square_error\', error)\r\n    # tf.summary.scalar(\'loss\', loss)\r\n    # tf.summary.histogram(\'linear_weights_hist\', w_linear)\r\n    #\r\n    # if params[\'is_use_fm_part\']:\r\n    #     tf.summary.histogram(\'fm_weights_hist\', w_fm)\r\n    # if params[\'is_use_dnn_part\']:\r\n    #     for idx in range(len(w_nn_params))  :\r\n    #         tf.summary.histogram(\'nn_layer\'+str(idx)+\'_weights\', w_nn_params[idx])\r\n    #\r\n    # merged_summary = tf.summary.merge_all()\r\n\r\n\r\n    #return train_step, loss, error, preds, merged_summary, tmp\r\n    return train_step, loss, error, preds, tmp\r\n\r\ndef pre_build_data_cache_if_need(infile, batch_size, rebuild_cache):\r\n    outfile = infile.replace(\'.csv\',\'.pkl\').replace(\'.txt\',\'.pkl\')\r\n    if not os.path.isfile(outfile) or rebuild_cache:\r\n        print(\'pre_build_data_cache for \', infile)\r\n        pre_build_data_cache(infile, outfile, batch_size)\r\n        print(\'pre_build_data_cache finished.\' )\r\n\r\ndef run():\r\n    print (\'begin running\')\r\n\r\n    params = {\r\n        \'reg_w_linear\': 0.0001, \'reg_w_fm\':0.0001, \'reg_w_nn\': 0.0001,  #0.001\r\n        \'reg_w_l1\': 0.0001,\r\n        \'init_value\': 0.001,\r\n        \'layer_sizes\': [100,500],\r\n        \'activations\':[\'relu\',\'tanh\'],#\r\n        \'eta\': 0.1,\r\n        \'n_epoch\': 5000,  # 500\r\n        \'batch_size\': 256,\r\n        \'dim\': 15,\r\n        \'model_path\': \'models\',\r\n        \'train_file\':  \'data/demodata.fieldwise.txt\',  \r\n        \'test_file\':    \'data/demodata.fieldwise.txt\',\r\n        \'output_predictions\':False,\r\n        \'is_use_fm_part\':True,\r\n        \'is_use_dnn_part\':True, \r\n        \'multi_level_num\':1,\r\n        \'learning_rate\':0.0001, # [0.001, 0.01]\r\n        \'loss\': \'log_loss\', # [cross_entropy_loss, square_loss, log_loss]\r\n        \'optimizer\':\'adam\', # [adam, ftrl, sgd]\r\n        \'clean_cache\':True,\r\n        \'metrics\': [\r\n            #{\'name\': \'individual_auc\'},\r\n             {\'name\': \'global_auc\'}\r\n            #, {\'name\': \'precision\', \'k\': 1}\r\n            #, {\'name\': \'precision\', \'k\': 5}\r\n            # , {\'name\': \'precision\', \'k\': 10}\r\n        ]\r\n\r\n    }\r\n   \r\n\r\n    #single_run(feature_cnt, field_cnt, params)\r\n    grid_search( params)\r\n\r\ndef grid_search( params):\r\n    single_run(  params)\r\n    \'\'\'\r\n    for i in range(0,5):      \r\n        params[\'dim\'] = pow(2,i)    \r\n        \r\n        for _ in range(3):\r\n            single_run(  params)\r\n    \'\'\'\r\n           \r\nif __name__ == \'__main__\':\r\n    run()\r\n\r\n'"
models/data/__init__.py,0,b' \n'
models/NeuralCF_tensorflow/dataio/ImpDataset.py,0,"b'\'\'\'\r\nCreated on Jan 23, 2018\r\n\r\n@author: v-lianji\r\n\'\'\'\r\n\r\nimport numpy as np\r\n\r\nclass ImpDataset(object):\r\n    \r\n    def __init__(self,path):\r\n        self.trainPosSet, self.num_users , self.num_items  = self.load_rating_file_as_set(path + ""train.tsv"")\r\n        self.testPosSet, _, _ = self.load_rating_file_as_set(path + ""test.tsv"")\r\n        self.testPair2NegList = self.load_negative_file_as_dict(path + ""test.negative.tsv"")\r\n    \r\n    \r\n    def load_rating_file_as_set(self, filename):  \r\n        num_users, num_items = 0, 0 \r\n        res = set() \r\n        with open(filename, \'r\') as rd:\r\n            while True:\r\n                line = rd.readline()  \r\n                if not line:\r\n                    break \r\n                words = line.strip().split(\'\\t\')\r\n                u, i = int(words[0]), int(words[1])\r\n                num_users = max(num_users, u) \r\n                num_items = max(num_items, i)\r\n                key = (u,i) \r\n                if key not in res:\r\n                    res.add(key)\r\n        \r\n        return res, num_users + 1, num_items + 1 \r\n         \r\n    def  load_negative_file_as_dict(self, filename):  \r\n        res = dict() \r\n        with open(filename, \'r\') as rd:\r\n            while True:\r\n                line = rd.readline() \r\n                if not line:\r\n                    break \r\n                words = line.strip().split(\'\\t\')\r\n                key = eval(words[0])\r\n                if key in res :\r\n                    continue \r\n                res[key] = [int(i) for i in words[1:]] \r\n                res[key].append(key[1])\r\n                res[key] = np.asarray(res[key], dtype = np.int32)\r\n                np.random.shuffle(res[key])\r\n        return res     \r\n            \r\n    def make_training_instances(self, neg_k): \r\n        user_input, item_input, labels = [],[],[]\r\n        for (u,i) in self.trainPosSet:\r\n            user_input.append(u)\r\n            item_input.append(i)\r\n            labels.append(1.0)\r\n            \r\n            for _ in range(neg_k):\r\n                j = np.random.randint(self.num_items)\r\n                while (u,j) in self.trainPosSet:\r\n                    j = np.random.randint(self.num_items)\r\n                user_input.append(u)\r\n                item_input.append(j)\r\n                labels.append(0.0)\r\n           \r\n        num_inst = len(user_input) \r\n        user_input, item_input, labels = np.asarray(user_input, np.int32),np.asarray(item_input, np.int32),np.asarray(labels, np.float32)\r\n        indices = np.arange(num_inst) \r\n        np.random.shuffle(indices)\r\n        return user_input[indices], item_input[indices], labels[indices], num_inst\r\n           '"
models/NeuralCF_tensorflow/models/BaseModel.py,23,"b""'''\r\nCreated on Jan 23, 2018\r\n\r\n@author: v-lianji\r\n'''\r\nfrom models import utils \r\nfrom dataio import ImpDataset\r\nimport tensorflow as tf \r\n \r\n\r\nclass BaseModel(object):\r\n    def __init__(self, args, num_users, num_items):\r\n        self.num_users, self.num_items = num_users, num_items \r\n        self.lr = args.lr \r\n        self.learner = args.learner \r\n        self.init_stddev = args.init_stddev\r\n        self.loss = args.loss\r\n        self.lambda_id_emb = args.reg_id_embedding\r\n        self.lambda_others = args.reg_others\r\n        self.eta = args.eta\r\n    \r\n\r\n    def build_train_model(self, model_vector, model_len, ratings, model_params):   \r\n        init_value = self.init_stddev \r\n        \r\n        w_output = tf.Variable(tf.truncated_normal([model_len, 1], stddev=init_value, mean=0), name='w_output', dtype=tf.float32)\r\n        b_output =  tf.Variable(tf.truncated_normal([1], stddev=init_value*0.01, mean=0), name='b_output', dtype=tf.float32)\r\n        model_params.append(w_output)\r\n        model_params.append(b_output)\r\n        raw_predictions = tf.nn.xw_plus_b(model_vector, w_output, b_output, name='output')\r\n        \r\n        output = tf.reshape(tf.sigmoid(raw_predictions), [-1]) \r\n        \r\n        with tf.name_scope('error'): \r\n            type_of_loss = self.loss \r\n            if type_of_loss == 'cross_entropy_loss':\r\n                raw_error = tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.reshape(raw_predictions, [-1]), labels=tf.reshape(self.ratings, [-1]))\r\n                error = tf.reduce_mean(\r\n                                   raw_error,\r\n                                   name='error/cross_entropy_loss'\r\n                                   )\r\n            elif type_of_loss == 'square_loss' or type_of_loss == 'rmse':\r\n                raw_error = tf.squared_difference(output, ratings, name='error/squared_diff')\r\n                error = tf.reduce_mean(raw_error, name='error/mean_squared_diff')\r\n            elif type_of_loss == 'log_loss':\r\n                raw_error = tf.losses.log_loss(predictions=output, labels=ratings)\r\n                error = tf.reduce_mean(raw_error, name='error/mean_log_loss')\r\n\r\n        \r\n            l2_norm = 0\r\n            for par in model_params:\r\n                l2_norm +=  tf.nn.l2_loss(par) * self.lambda_others\r\n            r'''\r\n            l2_norm += tf.nn.l2_loss(emb_user) * self.lambda_id_emb\r\n            l2_norm += tf.nn.l2_loss(emb_item) * self.lambda_id_emb\r\n            l2_norm += tf.nn.l2_loss(w_output) * self.lambda_others\r\n            l2_norm += tf.nn.l2_loss(b_output) * self.lambda_others\r\n            '''\r\n            \r\n            loss = error + l2_norm    \r\n        \r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) ##--\r\n        with tf.control_dependencies(update_ops):\r\n            type_of_opt = self.learner\r\n            if type_of_opt == 'adadelta':  \r\n                train_step = tf.train.AdadeltaOptimizer(self.eta).minimize(loss,var_list=model_params)#\r\n            elif type_of_opt == 'sgd':\r\n                train_step = tf.train.GradientDescentOptimizer(self.lr).minimize(loss,var_list=model_params)\r\n            elif type_of_opt =='adam':\r\n                train_step = tf.train.AdamOptimizer(self.lr).minimize(loss, var_list=model_params)\r\n            elif type_of_opt =='ftrl':\r\n                train_step = tf.train.FtrlOptimizer(self.lr).minimize(loss,var_list=model_params)\r\n            else:\r\n                train_step = tf.train.GradientDescentOptimizer(self.lr).minimize(loss,var_list=model_params)          \r\n        \r\n        return output, loss, error, raw_error, train_step\r\n            \r\n\r\n """
models/NeuralCF_tensorflow/models/GMF.py,10,"b""'''\r\nCreated on Jan 23, 2018\r\n\r\n@author: v-lianji\r\n'''\r\n\r\nfrom models import utils \r\nfrom dataio import ImpDataset\r\nimport math \r\nimport tensorflow as tf \r\nfrom models.evaluation import *\r\nfrom time import time\r\nfrom models.BaseModel import *\r\n\r\n\r\nclass GMF(BaseModel):\r\n    def __init__(self, args, num_users, num_items):\r\n        BaseModel.__init__(self, args, num_users, num_items)\r\n        self.num_factors = args.num_factors \r\n        \r\n\r\n    def build_core_model(self, user_indices , item_indices): \r\n        \r\n        init_value = self.init_stddev  \r\n        \r\n        emb_user = tf.Variable(tf.truncated_normal([self.num_users, self.num_factors], stddev=init_value/math.sqrt(float(self.num_factors)), mean=0), name = 'user_embedding', dtype=tf.float32)\r\n        emb_item = tf.Variable(tf.truncated_normal([self.num_items, self.num_factors], stddev=init_value/math.sqrt(float(self.num_factors)), mean=0), name = 'item_embedding', dtype=tf.float32)\r\n        \r\n        emb_user_bias = tf.concat([emb_user, tf.ones((self.num_users,1) , dtype=tf.float32)* 0.1], 1, name='user_embedding_bias')\r\n        emb_item_bias = tf.concat([tf.ones((self.num_items,1), dtype=tf.float32)* 0.1, emb_item], 1, name='item_embedding_bias')\r\n        \r\n        user_feature = tf.nn.embedding_lookup(emb_user_bias, user_indices, name = 'user_feature') \r\n        item_feature = tf.nn.embedding_lookup(emb_item_bias, item_indices, name = 'item_feature')   \r\n        \r\n        product_vector = tf.multiply(user_feature , item_feature)\r\n           \r\n        model_params = [emb_user,emb_item]  \r\n\r\n        return product_vector, self.num_factors+1, model_params\r\n\r\n    def build_model(self, user_indices = None, item_indices = None):  \r\n         \r\n        if not user_indices:\r\n            user_indices =  tf.placeholder(tf.int32,[None])\r\n        self.user_indices = user_indices\r\n        if not item_indices:\r\n            item_indices =  tf.placeholder(tf.int32,[None])\r\n        self.item_indices = item_indices\r\n        \r\n        self.ratings = tf.placeholder(tf.float32, [None])\r\n        \r\n        model_vector, model_len, model_params = self.build_core_model(user_indices , item_indices)\r\n        \r\n        self.output, self.loss, self.error, self.raw_error, self.train_step = self.build_train_model( model_vector, model_len, self.ratings, model_params)  \r\n     """
models/NeuralCF_tensorflow/models/MLP.py,12,"b""'''\r\nCreated on Jan 23, 2018\r\n\r\n@author: v-lianji\r\n'''\r\n\r\nfrom models import utils \r\nfrom dataio import ImpDataset\r\nimport math \r\nimport tensorflow as tf \r\nfrom models.evaluation import *\r\nfrom time import time\r\nfrom models.BaseModel import *\r\n\r\nclass MLP(BaseModel):\r\n    def __init__(self, args, num_users, num_items):\r\n        BaseModel.__init__(self, args, num_users, num_items)\r\n        self.layers = eval(args.layers) \r\n        self.lambda_layers = eval(args.reg_layers)\r\n        \r\n        #self.build_model()\r\n    \r\n    def build_core_model(self, user_indices , item_indices): \r\n        \r\n        init_value = self.init_stddev  \r\n        \r\n        emb_user = tf.Variable(tf.truncated_normal([self.num_users, self.layers[0]//2], stddev=init_value/math.sqrt(float(self.layers[0]//2)), mean=0), name = 'user_embedding', dtype=tf.float32)\r\n        emb_item = tf.Variable(tf.truncated_normal([self.num_items, self.layers[0]//2], stddev=init_value/math.sqrt(float(self.layers[0]//2)), mean=0), name = 'item_embedding', dtype=tf.float32)\r\n        user_feature = tf.nn.embedding_lookup(emb_user, user_indices, name = 'user_feature') \r\n        item_feature = tf.nn.embedding_lookup(emb_item, item_indices, name = 'item_feature')   \r\n        \r\n        hidden_layers = [tf.concat([user_feature,item_feature],1)]\r\n        \r\n        model_params = [emb_user,emb_item]  \r\n        \r\n        for i in range(1,len(self.layers)):\r\n            w_hidden_layer = tf.Variable(tf.truncated_normal([self.layers[i-1],self.layers[i]], stddev = init_value, mean = 0), name = 'w_hidden_'+ str(i), dtype=tf.float32) \r\n            b_hidden_layer = tf.Variable(tf.truncated_normal([self.layers[i]], stddev = init_value*0.1, mean = 0), name = 'b_hidden_'+ str(i), dtype=tf.float32)\r\n            cur_layer = tf.nn.xw_plus_b(hidden_layers[-1], w_hidden_layer, b_hidden_layer)\r\n            cur_layer = tf.nn.relu(cur_layer)\r\n            hidden_layers.append(cur_layer)\r\n            model_params.append(w_hidden_layer)\r\n            model_params.append(b_hidden_layer)     \r\n         \r\n        return hidden_layers[-1], self.layers[-1], model_params\r\n\r\n    def build_model(self, user_indices = None, item_indices = None):  \r\n        \r\n        \r\n        if not user_indices:\r\n            user_indices =  tf.placeholder(tf.int32,[None])\r\n        self.user_indices = user_indices\r\n        if not item_indices:\r\n            item_indices =  tf.placeholder(tf.int32,[None])\r\n        self.item_indices = item_indices\r\n        \r\n        self.ratings = tf.placeholder(tf.float32, [None])\r\n        \r\n        model_vector, model_len, model_params = self.build_core_model(user_indices , item_indices)\r\n        \r\n        self.output, self.loss, self.error, self.raw_error, self.train_step = self.build_train_model( model_vector, model_len, self.ratings, model_params)  \r\n        """
models/NeuralCF_tensorflow/models/NeuMF.py,4,"b""'''\r\nCreated on Jan 23, 2018\r\n\r\n@author: v-lianji\r\n'''\r\n\r\n\r\nfrom models import utils \r\nfrom dataio import ImpDataset\r\nimport math \r\nimport tensorflow as tf \r\nfrom models.evaluation import *\r\nfrom time import time\r\nfrom models.BaseModel import *\r\nfrom models.GMF import *\r\nfrom models.MLP import *\r\n\r\nclass NeuMF(BaseModel):\r\n    def __init__(self, args, num_users, num_items):\r\n        BaseModel.__init__(self, args, num_users, num_items)\r\n        self.layers = eval(args.layers) \r\n        self.lambda_layers = eval(args.reg_layers)\r\n        self.num_factors = args.num_factors \r\n        self.model_GMF = GMF(args, num_users, num_items)\r\n        self.model_MLP = MLP(args, num_users, num_items)\r\n    \r\n    def build_core_model(self, user_indices , item_indices): \r\n        \r\n        vector_GMF, len_GMF, params_GMF = self.model_GMF.build_core_model(user_indices, item_indices)  \r\n        vector_MLP, len_MLP, params_MLP = self.model_MLP.build_core_model(user_indices, item_indices)\r\n        \r\n        model_vector = tf.concat([vector_GMF,vector_MLP],1)\r\n        model_len = len_GMF + len_MLP\r\n        \r\n        model_params = [] \r\n        model_params.extend(params_GMF)\r\n        model_params.extend(params_MLP)\r\n         \r\n        return model_vector,model_len,model_params\r\n\r\n    def build_model(self, user_indices = None, item_indices = None):  \r\n        \r\n        if not user_indices:\r\n            user_indices =  tf.placeholder(tf.int32,[None])\r\n        self.user_indices = user_indices\r\n        if not item_indices:\r\n            item_indices =  tf.placeholder(tf.int32,[None])\r\n        self.item_indices = item_indices\r\n        \r\n        self.ratings = tf.placeholder(tf.float32, [None])\r\n        \r\n        model_vector, model_len, model_params = self.build_core_model(user_indices , item_indices)\r\n        \r\n        self.output, self.loss, self.error, self.raw_error, self.train_step = self.build_train_model( model_vector, model_len, self.ratings, model_params)  \r\n        """
models/NeuralCF_tensorflow/models/evaluation.py,0,"b""'''\r\nCreated on Jan 23, 2018\r\n\r\n@author: v-lianji\r\n'''\r\n\r\nimport numpy as np\r\nimport math \r\n\r\ndef evaluate_one_case(u,i, key2candidates, sess, model, topk):\r\n    key = (u,i) \r\n    assert(key in key2candidates)\r\n    items = key2candidates[key] \r\n    users = np.full(len(items), key[0], dtype=np.int32)\r\n    predictions = sess.run(model.output, { model.user_indices : users, model.item_indices : items})\r\n    #print(predictions)\r\n    k = min(topk, len(items))\r\n    sorted_idx = np.argsort(predictions)[::-1]\r\n    selected_items = items[sorted_idx[0:k]]\r\n    #print(sorted_idx)\r\n    #print(i,items[sorted_idx[0]])\r\n    ndcg = getNDCG(selected_items,i)\r\n    hit = getHitRatio(selected_items,i)\r\n    return hit,ndcg\r\n    \r\ndef getHitRatio(items,iid):\r\n    if iid in items:\r\n        return 1.0 \r\n    else:\r\n        return 0.0\r\n        \r\n\r\ndef getNDCG(items,iid):\r\n    for i in range(len(items)):\r\n        if items[i]==iid:\r\n            return math.log(2)/math.log(i+2) \r\n    return 0.\r\n\r\ndef evaluate_model(sess, model, dataset, topk):\r\n    hits, ndcgs = [],[]\r\n    for (u,i) in dataset.testPosSet:\r\n        hit,ndcg = evaluate_one_case(u,i,dataset.testPair2NegList,  sess, model, topk)\r\n        hits.append(hit)\r\n        ndcgs.append(ndcg)\r\n        #break \r\n    return np.asarray(hits).mean(), np.asarray(ndcgs).mean()\r\n        """
models/NeuralCF_tensorflow/models/main.py,2,"b'\'\'\'\r\nCreated on Jan 23, 2018\r\n\r\n@author: v-lianji\r\n\'\'\'\r\n\r\n\r\nfrom models import utils \r\nfrom dataio import ImpDataset\r\nimport math \r\nimport tensorflow as tf \r\nfrom models.evaluation import *\r\nfrom time import time\r\nfrom models.GMF import *\r\nfrom models.MLP import *\r\nfrom models.NeuMF import *\r\n\r\n\r\n\r\ndef single_run(args, dataset):\r\n    \r\n    model = NeuMF(args, dataset.num_users, dataset.num_items)\r\n    model.build_model()\r\n\r\n    sess = tf.Session() \r\n    init = tf.global_variables_initializer() \r\n    sess.run(init) \r\n    \r\n    t1 = time()\r\n    ahit, andcg = evaluate_model(sess, model, dataset, args.topk)\r\n    best_hr, best_ndcg, best_iter = ahit, andcg, -1\r\n    print(\'Init: HR = %.4f, NDCG = %.4f\\t [%.1f s]\' % (ahit, andcg, time()-t1))\r\n    \r\n    for epoch in range(args.epochs):\r\n        t1 = time()\r\n        train_users, train_items, train_labels, num_inst = dataset.make_training_instances(args.num_neg_inst) \r\n        #print(train_labels[0:20])\r\n        loss_per_epoch , error_per_epoch = 0, 0 \r\n        for ite in range((num_inst-1)//args.batch_size+1):\r\n            start_idx = ite * args.batch_size \r\n            end_idx = min((ite+1)*args.batch_size , num_inst) \r\n            cur_user_indices, cur_item_indices, cur_label = train_users[start_idx:end_idx], train_items[start_idx:end_idx],train_labels[start_idx:end_idx]\r\n            \r\n            _, loss, error = sess.run([model.train_step, model.loss, model.raw_error], { model.user_indices : cur_user_indices, model.item_indices : cur_item_indices, model.ratings : cur_label})\r\n            loss_per_epoch +=loss\r\n            error_per_epoch += error \r\n        error_per_epoch /= num_inst\r\n        t2 = time()\r\n        if epoch % args.verbose == 0:\r\n            ahit, andcg = evaluate_model(sess, model, dataset, args.topk)\r\n            print(\'epoch %d   \\t[%.1f s]: HR= %.4f\\tNDCG= %.4f\\tloss= %.4f\\terror= %.4f\\t[%.1f s]\' %(epoch, t2-t1, ahit, andcg, loss_per_epoch, error_per_epoch, time()-t2))\r\n            if ahit > best_hr :\r\n                best_hr = ahit \r\n                best_iter = epoch \r\n            if andcg > best_ndcg :\r\n                best_ndcg = andcg \r\n                \r\n    print(""End. Best Epoch %d:  HR = %.4f, NDCG = %.4f. "" %(best_iter, best_hr, best_ndcg))    \r\n        \r\n\r\nif __name__ == \'__main__\':\r\n    args = utils.parse_args() \r\n    \r\n    print(""runtime arguments: %s"" %(args))\r\n    \r\n    dataset = ImpDataset.ImpDataset(args.path)\r\n    \r\n    print(dataset.num_users,dataset.num_items)\r\n    \r\n    single_run(args, dataset) \r\n    \r\n    \r\n    \r\n    '"
models/NeuralCF_tensorflow/models/utils.py,0,"b'\'\'\'\r\nCreated on Jan 23, 2018\r\n\r\n@author: v-lianji\r\n\'\'\'\r\nimport argparse\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'--path\', nargs=\'?\', default=\'../Data/ml-1m/\',\r\n                        help=\'Input data path.\')\r\n    parser.add_argument(\'--epochs\', type=int, default=20,\r\n                        help=\'Number of epochs.\')\r\n    parser.add_argument(\'--batch_size\', type=int, default=256,\r\n                        help=\'Batch size.\')\r\n    parser.add_argument(\'--num_factors\', type=int, default=32,\r\n                        help=\'Embedding size.\')\r\n    parser.add_argument(\'--reg_id_embedding\', nargs=\'?\', default=0.0, type=int,\r\n                        help=""Regularization for user and item embeddings."")\r\n    parser.add_argument(\'--reg_others\', nargs=\'?\', default=0.0,  type=float,\r\n                        help=""Regularization for general variables."")\r\n    parser.add_argument(\'--init_stddev\', nargs=\'?\', default=0.1,  type=float,\r\n                        help=""Init stddev value for variables."")\r\n    parser.add_argument(\'--num_neg_inst\', type=int, default=4,\r\n                        help=\'Number of negative instances to pair with a positive instance.\')\r\n    parser.add_argument(\'--lr\', type=float, default=0.001,\r\n                        help=\'Learning rate.\')\r\n    parser.add_argument(\'--learner\', nargs=\'?\', default=\'adam\',\r\n                        help=\'Specify an optimizer: adagrad, adam, rmsprop, sgd\')\r\n    parser.add_argument(\'--verbose\', type=int, default=1,\r\n                        help=\'Show performance per X iterations\')\r\n    parser.add_argument(\'--out\', type=int, default=1,\r\n                        help=\'Whether to save the trained model.\')\r\n    parser.add_argument(\'--loss\',   default=\'log_loss\',\r\n                        help=\'type of loss function.\')\r\n    parser.add_argument(\'--eta\', type=float, default=0.1,\r\n                        help=\'eta of adadelta\')\r\n    parser.add_argument(\'--topk\', type=int, default=10,\r\n                        help=\'Evaluate the top k items.\')\r\n    \r\n    parser.add_argument(\'--layers\', nargs=\'?\', default=\'[64,32,16,8]\',\r\n                        help=""Size of each layer. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size."")\r\n    parser.add_argument(\'--reg_layers\', nargs=\'?\', default=\'[0,0,0,0]\',\r\n                        help=""Regularization for each layer"")\r\n   \r\n    return parser.parse_args()\r\n'"
