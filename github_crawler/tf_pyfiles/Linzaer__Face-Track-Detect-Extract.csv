file_path,api_count,code
project_root_dir.py,0,b'import os\n\nproject_dir = os.path.dirname(os.path.abspath(__file__))\n'
start.py,2,"b'import argparse\nimport os\nfrom time import time\n\nimport align.detect_face as detect_face\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom lib.face_utils import judge_side_face\nfrom lib.utils import Logger, mkdir\nfrom project_root_dir import project_dir\nfrom src.sort import Sort\n\nlogger = Logger()\n\n\ndef main():\n    global colours, img_size\n    args = parse_args()\n    videos_dir = args.videos_dir\n    output_path = args.output_path\n    no_display = args.no_display\n    detect_interval = args.detect_interval  # you need to keep a balance between performance and fluency\n    margin = args.margin  # if the face is big in your video ,you can set it bigger for tracking easiler\n    scale_rate = args.scale_rate  # if set it smaller will make input frames smaller\n    show_rate = args.show_rate  # if set it smaller will dispaly smaller frames\n    face_score_threshold = args.face_score_threshold\n\n    mkdir(output_path)\n    # for display\n    if not no_display:\n        colours = np.random.rand(32, 3)\n\n    # init tracker\n    tracker = Sort()  # create instance of the SORT tracker\n\n    logger.info(\'Start track and extract......\')\n    with tf.Graph().as_default():\n        with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True),\n                                              log_device_placement=False)) as sess:\n            pnet, rnet, onet = detect_face.create_mtcnn(sess, os.path.join(project_dir, ""align""))\n\n            minsize = 40  # minimum size of face for mtcnn to detect\n            threshold = [0.6, 0.7, 0.7]  # three steps\'s threshold\n            factor = 0.709  # scale factor\n\n            for filename in os.listdir(videos_dir):\n                logger.info(\'All files:{}\'.format(filename))\n            for filename in os.listdir(videos_dir):\n                suffix = filename.split(\'.\')[1]\n                if suffix != \'mp4\' and suffix != \'avi\':  # you can specify more video formats if you need\n                    continue\n                video_name = os.path.join(videos_dir, filename)\n                directoryname = os.path.join(output_path, filename.split(\'.\')[0])\n                logger.info(\'Video_name:{}\'.format(video_name))\n                cam = cv2.VideoCapture(video_name)\n                c = 0\n                while True:\n                    final_faces = []\n                    addtional_attribute_list = []\n                    ret, frame = cam.read()\n                    if not ret:\n                        logger.warning(""ret false"")\n                        break\n                    if frame is None:\n                        logger.warning(""frame drop"")\n                        break\n\n                    frame = cv2.resize(frame, (0, 0), fx=scale_rate, fy=scale_rate)\n                    r_g_b_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    if c % detect_interval == 0:\n                        img_size = np.asarray(frame.shape)[0:2]\n                        mtcnn_starttime = time()\n                        faces, points = detect_face.detect_face(r_g_b_frame, minsize, pnet, rnet, onet, threshold,\n                                                                factor)\n                        logger.info(""MTCNN detect face cost time : {} s"".format(\n                            round(time() - mtcnn_starttime, 3)))  # mtcnn detect ,slow\n                        face_sums = faces.shape[0]\n                        if face_sums > 0:\n                            face_list = []\n                            for i, item in enumerate(faces):\n                                score = round(faces[i, 4], 6)\n                                if score > face_score_threshold:\n                                    det = np.squeeze(faces[i, 0:4])\n\n                                    # face rectangle\n                                    det[0] = np.maximum(det[0] - margin, 0)\n                                    det[1] = np.maximum(det[1] - margin, 0)\n                                    det[2] = np.minimum(det[2] + margin, img_size[1])\n                                    det[3] = np.minimum(det[3] + margin, img_size[0])\n                                    face_list.append(item)\n\n                                    # face cropped\n                                    bb = np.array(det, dtype=np.int32)\n\n                                    # use 5 face landmarks  to judge the face is front or side\n                                    squeeze_points = np.squeeze(points[:, i])\n                                    tolist = squeeze_points.tolist()\n                                    facial_landmarks = []\n                                    for j in range(5):\n                                        item = [tolist[j], tolist[(j + 5)]]\n                                        facial_landmarks.append(item)\n                                    if args.face_landmarks:\n                                        for (x, y) in facial_landmarks:\n                                            cv2.circle(frame, (int(x), int(y)), 3, (0, 255, 0), -1)\n                                    cropped = frame[bb[1]:bb[3], bb[0]:bb[2], :].copy()\n\n                                    dist_rate, high_ratio_variance, width_rate = judge_side_face(\n                                        np.array(facial_landmarks))\n\n                                    # face addtional attribute(index 0:face score; index 1:0 represents front face and 1 for side face )\n                                    item_list = [cropped, score, dist_rate, high_ratio_variance, width_rate]\n                                    addtional_attribute_list.append(item_list)\n\n                            final_faces = np.array(face_list)\n\n                    trackers = tracker.update(final_faces, img_size, directoryname, addtional_attribute_list, detect_interval)\n\n                    c += 1\n\n                    for d in trackers:\n                        if not no_display:\n                            d = d.astype(np.int32)\n                            cv2.rectangle(frame, (d[0], d[1]), (d[2], d[3]), colours[d[4] % 32, :] * 255, 3)\n                            if final_faces != []:\n                                cv2.putText(frame, \'ID : %d  DETECT\' % (d[4]), (d[0] - 10, d[1] - 10),\n                                            cv2.FONT_HERSHEY_SIMPLEX,\n                                            0.75,\n                                            colours[d[4] % 32, :] * 255, 2)\n                                cv2.putText(frame, \'DETECTOR\', (5, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n                                            (1, 1, 1), 2)\n                            else:\n                                cv2.putText(frame, \'ID : %d\' % (d[4]), (d[0] - 10, d[1] - 10), cv2.FONT_HERSHEY_SIMPLEX,\n                                            0.75,\n                                            colours[d[4] % 32, :] * 255, 2)\n\n                    if not no_display:\n                        frame = cv2.resize(frame, (0, 0), fx=show_rate, fy=show_rate)\n                        cv2.imshow(""Frame"", frame)\n                        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                            break\n\n\ndef parse_args():\n    """"""Parse input arguments.""""""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--videos_dir"", type=str,\n                        help=\'Path to the data directory containing aligned your face patches.\', default=\'videos\')\n    parser.add_argument(\'--output_path\', type=str,\n                        help=\'Path to save face\',\n                        default=\'facepics\')\n    parser.add_argument(\'--detect_interval\',\n                        help=\'how many frames to make a detection\',\n                        type=int, default=1)\n    parser.add_argument(\'--margin\',\n                        help=\'add margin for face\',\n                        type=int, default=10)\n    parser.add_argument(\'--scale_rate\',\n                        help=\'Scale down or enlarge the original video img\',\n                        type=float, default=0.7)\n    parser.add_argument(\'--show_rate\',\n                        help=\'Scale down or enlarge the imgs drawn by opencv\',\n                        type=float, default=1)\n    parser.add_argument(\'--face_score_threshold\',\n                        help=\'The threshold of the extracted faces,range 0<x<=1\',\n                        type=float, default=0.85)\n    parser.add_argument(\'--face_landmarks\',\n                        help=\'Draw five face landmarks on extracted face or not \', action=""store_true"")\n    parser.add_argument(\'--no_display\',\n                        help=\'Display or not\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    main()\n'"
align/__init__.py,0,b''
align/detect_face.py,23,"b'"""""" Tensorflow implementation of the face detection / alignment algorithm found at\nhttps://github.com/kpzhang93/MTCNN_face_detection_alignment\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n# from math import floor\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom six import string_types, iteritems\n\n\ndef layer(op):\n    \'\'\'Decorator for composable network layers.\'\'\'\n\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.terminals) == 0:\n            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n        elif len(self.terminals) == 1:\n            layer_input = self.terminals[0]\n        else:\n            layer_input = list(self.terminals)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n\n    return layer_decorated\n\n\nclass Network(object):\n    def __init__(self, inputs, trainable=True):\n        # The input nodes for this network\n        self.inputs = inputs\n        # The current list of terminal nodes\n        self.terminals = []\n        # Mapping from layer names to layers\n        self.layers = dict(inputs)\n        # If true, the resulting variables are set as trainable\n        self.trainable = trainable\n\n        self.setup()\n\n    def setup(self):\n        \'\'\'Construct the network. \'\'\'\n        raise NotImplementedError(\'Must be implemented by the subclass.\')\n\n    def load(self, data_path, session, ignore_missing=False):\n        \'\'\'Load network weights.\n        data_path: The path to the numpy-serialized network weights\n        session: The current TensorFlow session\n        ignore_missing: If true, serialized weights for missing layers are ignored.\n        \'\'\'\n        data_dict = np.load(data_path, encoding=\'latin1\').item()  # pylint: disable=no-member\n\n        for op_name in data_dict:\n            with tf.variable_scope(op_name, reuse=True):\n                for param_name, data in iteritems(data_dict[op_name]):\n                    try:\n                        var = tf.get_variable(param_name)\n                        session.run(var.assign(data))\n                    except ValueError:\n                        if not ignore_missing:\n                            raise\n\n    def feed(self, *args):\n        \'\'\'Set the input(s) for the next operation by replacing the terminal nodes.\n        The arguments can be either layer names or the actual layers.\n        \'\'\'\n        assert len(args) != 0\n        self.terminals = []\n        for fed_layer in args:\n            if isinstance(fed_layer, string_types):\n                try:\n                    fed_layer = self.layers[fed_layer]\n                except KeyError:\n                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n            self.terminals.append(fed_layer)\n        return self\n\n    def get_output(self):\n        \'\'\'Returns the current network output.\'\'\'\n        return self.terminals[-1]\n\n    def get_unique_name(self, prefix):\n        \'\'\'Returns an index-suffixed unique name for the given prefix.\n        This is used for auto-generating layer names based on the type-prefix.\n        \'\'\'\n        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n        return \'%s_%d\' % (prefix, ident)\n\n    def make_var(self, name, shape):\n        \'\'\'Creates a new TensorFlow variable.\'\'\'\n        return tf.get_variable(name, shape, trainable=self.trainable)\n\n    def validate_padding(self, padding):\n        \'\'\'Verifies that the padding is one of the supported ones.\'\'\'\n        assert padding in (\'SAME\', \'VALID\')\n\n    @layer\n    def conv(self,\n             inp,\n             k_h,\n             k_w,\n             c_o,\n             s_h,\n             s_w,\n             name,\n             relu=True,\n             padding=\'SAME\',\n             group=1,\n             biased=True):\n        # Verify that the padding is acceptable\n        self.validate_padding(padding)\n        # Get the number of channels in the input\n        c_i = int(inp.get_shape()[-1])\n        # Verify that the grouping parameter is valid\n        assert c_i % group == 0\n        assert c_o % group == 0\n        # Convolution for a given input and kernel\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n            kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n            # This is the common-case. Convolve the input without any further complications.\n            output = convolve(inp, kernel)\n            # Add the biases\n            if biased:\n                biases = self.make_var(\'biases\', [c_o])\n                output = tf.nn.bias_add(output, biases)\n            if relu:\n                # ReLU non-linearity\n                output = tf.nn.relu(output, name=scope.name)\n            return output\n\n    @layer\n    def prelu(self, inp, name):\n        with tf.variable_scope(name):\n            i = int(inp.get_shape()[-1])\n            alpha = self.make_var(\'alpha\', shape=(i,))\n            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\n        return output\n\n    @layer\n    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\'SAME\'):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(inp,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def fc(self, inp, num_out, name, relu=True):\n        with tf.variable_scope(name):\n            input_shape = inp.get_shape()\n            if input_shape.ndims == 4:\n                # The input is spatial. Vectorize it first.\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= int(d)\n                feed_in = tf.reshape(inp, [-1, dim])\n            else:\n                feed_in, dim = (inp, input_shape[-1].value)\n            weights = self.make_var(\'weights\', shape=[dim, num_out])\n            biases = self.make_var(\'biases\', [num_out])\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=name)\n            return fc\n\n    """"""\n    Multi dimensional softmax,\n    refer to https://github.com/tensorflow/tensorflow/issues/210\n    compute softmax along the dimension of target\n    the native softmax only supports batch_size x dimension\n    """"""\n\n    @layer\n    def softmax(self, target, axis, name=None):\n        max_axis = tf.reduce_max(target, axis, keep_dims=True)\n        target_exp = tf.exp(target - max_axis)\n        normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)\n        softmax = tf.div(target_exp, normalize, name)\n        return softmax\n\n\nclass PNet(Network):\n    def setup(self):\n        (self.feed(\'data\')  # pylint: disable=no-value-for-parameter, no-member\n         .conv(3, 3, 10, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n         .prelu(name=\'PReLU1\')\n         .max_pool(2, 2, 2, 2, name=\'pool1\')\n         .conv(3, 3, 16, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n         .prelu(name=\'PReLU2\')\n         .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n         .prelu(name=\'PReLU3\')\n         .conv(1, 1, 2, 1, 1, relu=False, name=\'conv4-1\')\n         .softmax(3, name=\'prob1\'))\n\n        (self.feed(\'PReLU3\')  # pylint: disable=no-value-for-parameter\n         .conv(1, 1, 4, 1, 1, relu=False, name=\'conv4-2\'))\n\n\nclass RNet(Network):\n    def setup(self):\n        (self.feed(\'data\')  # pylint: disable=no-value-for-parameter, no-member\n         .conv(3, 3, 28, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n         .prelu(name=\'prelu1\')\n         .max_pool(3, 3, 2, 2, name=\'pool1\')\n         .conv(3, 3, 48, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n         .prelu(name=\'prelu2\')\n         .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n         .conv(2, 2, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n         .prelu(name=\'prelu3\')\n         .fc(128, relu=False, name=\'conv4\')\n         .prelu(name=\'prelu4\')\n         .fc(2, relu=False, name=\'conv5-1\')\n         .softmax(1, name=\'prob1\'))\n\n        (self.feed(\'prelu4\')  # pylint: disable=no-value-for-parameter\n         .fc(4, relu=False, name=\'conv5-2\'))\n\n\nclass ONet(Network):\n    def setup(self):\n        (self.feed(\'data\')  # pylint: disable=no-value-for-parameter, no-member\n         .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n         .prelu(name=\'prelu1\')\n         .max_pool(3, 3, 2, 2, name=\'pool1\')\n         .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n         .prelu(name=\'prelu2\')\n         .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n         .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n         .prelu(name=\'prelu3\')\n         .max_pool(2, 2, 2, 2, name=\'pool3\')\n         .conv(2, 2, 128, 1, 1, padding=\'VALID\', relu=False, name=\'conv4\')\n         .prelu(name=\'prelu4\')\n         .fc(256, relu=False, name=\'conv5\')\n         .prelu(name=\'prelu5\')\n         .fc(2, relu=False, name=\'conv6-1\')\n         .softmax(1, name=\'prob1\'))\n\n        (self.feed(\'prelu5\')  # pylint: disable=no-value-for-parameter\n         .fc(4, relu=False, name=\'conv6-2\'))\n\n        (self.feed(\'prelu5\')  # pylint: disable=no-value-for-parameter\n         .fc(10, relu=False, name=\'conv6-3\'))\n\n\ndef create_mtcnn(sess, model_path):\n    if not model_path:\n        model_path, _ = os.path.split(os.path.realpath(__file__))\n\n    with tf.variable_scope(\'pnet\'):\n        data = tf.placeholder(tf.float32, (None, None, None, 3), \'input\')\n        pnet = PNet({\'data\': data})\n        pnet.load(os.path.join(model_path, \'det1.npy\'), sess)\n    with tf.variable_scope(\'rnet\'):\n        data = tf.placeholder(tf.float32, (None, 24, 24, 3), \'input\')\n        rnet = RNet({\'data\': data})\n        rnet.load(os.path.join(model_path, \'det2.npy\'), sess)\n    with tf.variable_scope(\'onet\'):\n        data = tf.placeholder(tf.float32, (None, 48, 48, 3), \'input\')\n        onet = ONet({\'data\': data})\n        onet.load(os.path.join(model_path, \'det3.npy\'), sess)\n\n    pnet_fun = lambda img: sess.run((\'pnet/conv4-2/BiasAdd:0\', \'pnet/prob1:0\'), feed_dict={\'pnet/input:0\': img})\n    rnet_fun = lambda img: sess.run((\'rnet/conv5-2/conv5-2:0\', \'rnet/prob1:0\'), feed_dict={\'rnet/input:0\': img})\n    onet_fun = lambda img: sess.run((\'onet/conv6-2/conv6-2:0\', \'onet/conv6-3/conv6-3:0\', \'onet/prob1:0\'),\n                                    feed_dict={\'onet/input:0\': img})\n    return pnet_fun, rnet_fun, onet_fun\n\n\ndef detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n    # im: input image\n    # minsize: minimum of faces\' size\n    # pnet, rnet, onet: caffemodel\n    # threshold: threshold=[th1 th2 th3], th1-3 are three steps\'s threshold\n    # fastresize: resize img from last scale (using in high-resolution images) if fastresize==true\n    factor_count = 0\n    total_boxes = np.empty((0, 9))\n    points = np.empty(0)\n    h = img.shape[0]\n    w = img.shape[1]\n    minl = np.amin([h, w])\n    m = 12.0 / minsize\n    minl = minl * m\n    # creat scale pyramid\n    scales = []\n    while minl >= 12:\n        scales += [m * np.power(factor, factor_count)]\n        minl = minl * factor\n        factor_count += 1\n\n    # first stage\n    for j in range(len(scales)):\n        scale = scales[j]\n        hs = int(np.ceil(h * scale))\n        ws = int(np.ceil(w * scale))\n        im_data = imresample(img, (hs, ws))\n        im_data = (im_data - 127.5) * 0.0078125\n        img_x = np.expand_dims(im_data, 0)\n        img_y = np.transpose(img_x, (0, 2, 1, 3))\n        out = pnet(img_y)\n        out0 = np.transpose(out[0], (0, 2, 1, 3))\n        out1 = np.transpose(out[1], (0, 2, 1, 3))\n\n        boxes, _ = generateBoundingBox(out1[0, :, :, 1].copy(), out0[0, :, :, :].copy(), scale, threshold[0])\n\n        # inter-scale nms\n        pick = nms(boxes.copy(), 0.5, \'Union\')\n        if boxes.size > 0 and pick.size > 0:\n            boxes = boxes[pick, :]\n            total_boxes = np.append(total_boxes, boxes, axis=0)\n\n    numbox = total_boxes.shape[0]\n    if numbox > 0:\n        pick = nms(total_boxes.copy(), 0.7, \'Union\')\n        total_boxes = total_boxes[pick, :]\n        regw = total_boxes[:, 2] - total_boxes[:, 0]\n        regh = total_boxes[:, 3] - total_boxes[:, 1]\n        qq1 = total_boxes[:, 0] + total_boxes[:, 5] * regw\n        qq2 = total_boxes[:, 1] + total_boxes[:, 6] * regh\n        qq3 = total_boxes[:, 2] + total_boxes[:, 7] * regw\n        qq4 = total_boxes[:, 3] + total_boxes[:, 8] * regh\n        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:, 4]]))\n        total_boxes = rerec(total_boxes.copy())\n        total_boxes[:, 0:4] = np.fix(total_boxes[:, 0:4]).astype(np.int32)\n        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n\n    numbox = total_boxes.shape[0]\n    if numbox > 0:\n        # second stage\n        tempimg = np.zeros((24, 24, 3, numbox))\n        for k in range(0, numbox):\n            tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n            tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = img[y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n            if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n            else:\n                return np.empty()\n        tempimg = (tempimg - 127.5) * 0.0078125\n        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))\n        out = rnet(tempimg1)\n        out0 = np.transpose(out[0])\n        out1 = np.transpose(out[1])\n        score = out1[1, :]\n        ipass = np.where(score > threshold[1])\n        total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(), np.expand_dims(score[ipass].copy(), 1)])\n        mv = out0[:, ipass[0]]\n        if total_boxes.shape[0] > 0:\n            pick = nms(total_boxes, 0.7, \'Union\')\n            total_boxes = total_boxes[pick, :]\n            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:, pick]))\n            total_boxes = rerec(total_boxes.copy())\n\n    numbox = total_boxes.shape[0]\n    if numbox > 0:\n        # third stage\n        total_boxes = np.fix(total_boxes).astype(np.int32)\n        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n        tempimg = np.zeros((48, 48, 3, numbox))\n        for k in range(0, numbox):\n            tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n            tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = img[y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n            if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n            else:\n                return np.empty()\n        tempimg = (tempimg - 127.5) * 0.0078125\n        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))\n        out = onet(tempimg1)\n        out0 = np.transpose(out[0])\n        out1 = np.transpose(out[1])\n        out2 = np.transpose(out[2])\n        score = out2[1, :]\n        points = out1\n        ipass = np.where(score > threshold[2])\n        points = points[:, ipass[0]]\n        total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(), np.expand_dims(score[ipass].copy(), 1)])\n        mv = out0[:, ipass[0]]\n\n        w = total_boxes[:, 2] - total_boxes[:, 0] + 1\n        h = total_boxes[:, 3] - total_boxes[:, 1] + 1\n        points[0:5, :] = np.tile(w, (5, 1)) * points[0:5, :] + np.tile(total_boxes[:, 0], (5, 1)) - 1\n        points[5:10, :] = np.tile(h, (5, 1)) * points[5:10, :] + np.tile(total_boxes[:, 1], (5, 1)) - 1\n        if total_boxes.shape[0] > 0:\n            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n            pick = nms(total_boxes.copy(), 0.7, \'Min\')\n            total_boxes = total_boxes[pick, :]\n            points = points[:, pick]\n\n    return total_boxes, points\n\n\ndef bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor):\n    # im: input image\n    # minsize: minimum of faces\' size\n    # pnet, rnet, onet: caffemodel\n    # threshold: threshold=[th1 th2 th3], th1-3 are three steps\'s threshold [0-1]\n\n    all_scales = [None] * len(images)\n    images_with_boxes = [None] * len(images)\n\n    for i in range(len(images)):\n        images_with_boxes[i] = {\'total_boxes\': np.empty((0, 9))}\n\n    # create scale pyramid\n    for index, img in enumerate(images):\n        all_scales[index] = []\n        h = img.shape[0]\n        w = img.shape[1]\n        minsize = int(detection_window_size_ratio * np.minimum(w, h))\n        factor_count = 0\n        minl = np.amin([h, w])\n        if minsize <= 12:\n            minsize = 12\n\n        m = 12.0 / minsize\n        minl = minl * m\n        while minl >= 12:\n            all_scales[index].append(m * np.power(factor, factor_count))\n            minl = minl * factor\n            factor_count += 1\n\n    # # # # # # # # # # # # #\n    # first stage - fast proposal network (pnet) to obtain face candidates\n    # # # # # # # # # # # # #\n\n    images_obj_per_resolution = {}\n\n    # TODO: use some type of rounding to number module 8 to increase probability that pyramid images will have the same resolution across input images\n\n    for index, scales in enumerate(all_scales):\n        h = images[index].shape[0]\n        w = images[index].shape[1]\n\n        for scale in scales:\n            hs = int(np.ceil(h * scale))\n            ws = int(np.ceil(w * scale))\n\n            if (ws, hs) not in images_obj_per_resolution:\n                images_obj_per_resolution[(ws, hs)] = []\n\n            im_data = imresample(images[index], (hs, ws))\n            im_data = (im_data - 127.5) * 0.0078125\n            img_y = np.transpose(im_data, (1, 0, 2))  # caffe uses different dimensions ordering\n            images_obj_per_resolution[(ws, hs)].append({\'scale\': scale, \'image\': img_y, \'index\': index})\n\n    for resolution in images_obj_per_resolution:\n        images_per_resolution = [i[\'image\'] for i in images_obj_per_resolution[resolution]]\n        outs = pnet(images_per_resolution)\n\n        for index in range(len(outs[0])):\n            scale = images_obj_per_resolution[resolution][index][\'scale\']\n            image_index = images_obj_per_resolution[resolution][index][\'index\']\n            out0 = np.transpose(outs[0][index], (1, 0, 2))\n            out1 = np.transpose(outs[1][index], (1, 0, 2))\n\n            boxes, _ = generateBoundingBox(out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0])\n\n            # inter-scale nms\n            pick = nms(boxes.copy(), 0.5, \'Union\')\n            if boxes.size > 0 and pick.size > 0:\n                boxes = boxes[pick, :]\n                images_with_boxes[image_index][\'total_boxes\'] = np.append(images_with_boxes[image_index][\'total_boxes\'],\n                                                                          boxes,\n                                                                          axis=0)\n\n    for index, image_obj in enumerate(images_with_boxes):\n        numbox = image_obj[\'total_boxes\'].shape[0]\n        if numbox > 0:\n            h = images[index].shape[0]\n            w = images[index].shape[1]\n            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Union\')\n            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n            regw = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0]\n            regh = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1]\n            qq1 = image_obj[\'total_boxes\'][:, 0] + image_obj[\'total_boxes\'][:, 5] * regw\n            qq2 = image_obj[\'total_boxes\'][:, 1] + image_obj[\'total_boxes\'][:, 6] * regh\n            qq3 = image_obj[\'total_boxes\'][:, 2] + image_obj[\'total_boxes\'][:, 7] * regw\n            qq4 = image_obj[\'total_boxes\'][:, 3] + image_obj[\'total_boxes\'][:, 8] * regh\n            image_obj[\'total_boxes\'] = np.transpose(np.vstack([qq1, qq2, qq3, qq4, image_obj[\'total_boxes\'][:, 4]]))\n            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n            image_obj[\'total_boxes\'][:, 0:4] = np.fix(image_obj[\'total_boxes\'][:, 0:4]).astype(np.int32)\n            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n\n            numbox = image_obj[\'total_boxes\'].shape[0]\n            tempimg = np.zeros((24, 24, 3, numbox))\n\n            if numbox > 0:\n                for k in range(0, numbox):\n                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                        tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n                    else:\n                        return np.empty()\n\n                tempimg = (tempimg - 127.5) * 0.0078125\n                image_obj[\'rnet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n\n    # # # # # # # # # # # # #\n    # second stage - refinement of face candidates with rnet\n    # # # # # # # # # # # # #\n\n    bulk_rnet_input = np.empty((0, 24, 24, 3))\n    for index, image_obj in enumerate(images_with_boxes):\n        if \'rnet_input\' in image_obj:\n            bulk_rnet_input = np.append(bulk_rnet_input, image_obj[\'rnet_input\'], axis=0)\n\n    out = rnet(bulk_rnet_input)\n    out0 = np.transpose(out[0])\n    out1 = np.transpose(out[1])\n    score = out1[1, :]\n\n    i = 0\n    for index, image_obj in enumerate(images_with_boxes):\n        if \'rnet_input\' not in image_obj:\n            continue\n\n        rnet_input_count = image_obj[\'rnet_input\'].shape[0]\n        score_per_image = score[i:i + rnet_input_count]\n        out0_per_image = out0[:, i:i + rnet_input_count]\n\n        ipass = np.where(score_per_image > threshold[1])\n        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n\n        mv = out0_per_image[:, ipass[0]]\n\n        if image_obj[\'total_boxes\'].shape[0] > 0:\n            h = images[index].shape[0]\n            w = images[index].shape[1]\n            pick = nms(image_obj[\'total_boxes\'], 0.7, \'Union\')\n            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv[:, pick]))\n            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n\n            numbox = image_obj[\'total_boxes\'].shape[0]\n\n            if numbox > 0:\n                tempimg = np.zeros((48, 48, 3, numbox))\n                image_obj[\'total_boxes\'] = np.fix(image_obj[\'total_boxes\']).astype(np.int32)\n                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n\n                for k in range(0, numbox):\n                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                        tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n                    else:\n                        return np.empty()\n                tempimg = (tempimg - 127.5) * 0.0078125\n                image_obj[\'onet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n\n        i += rnet_input_count\n\n    # # # # # # # # # # # # #\n    # third stage - further refinement and facial landmarks positions with onet\n    # # # # # # # # # # # # #\n\n    bulk_onet_input = np.empty((0, 48, 48, 3))\n    for index, image_obj in enumerate(images_with_boxes):\n        if \'onet_input\' in image_obj:\n            bulk_onet_input = np.append(bulk_onet_input, image_obj[\'onet_input\'], axis=0)\n\n    out = onet(bulk_onet_input)\n\n    out0 = np.transpose(out[0])\n    out1 = np.transpose(out[1])\n    out2 = np.transpose(out[2])\n    score = out2[1, :]\n    points = out1\n\n    i = 0\n    ret = []\n    for index, image_obj in enumerate(images_with_boxes):\n        if \'onet_input\' not in image_obj:\n            ret.append(None)\n            continue\n\n        onet_input_count = image_obj[\'onet_input\'].shape[0]\n\n        out0_per_image = out0[:, i:i + onet_input_count]\n        score_per_image = score[i:i + onet_input_count]\n        points_per_image = points[:, i:i + onet_input_count]\n\n        ipass = np.where(score_per_image > threshold[2])\n        points_per_image = points_per_image[:, ipass[0]]\n\n        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n        mv = out0_per_image[:, ipass[0]]\n\n        w = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0] + 1\n        h = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1] + 1\n        points_per_image[0:5, :] = np.tile(w, (5, 1)) * points_per_image[0:5, :] + np.tile(\n            image_obj[\'total_boxes\'][:, 0], (5, 1)) - 1\n        points_per_image[5:10, :] = np.tile(h, (5, 1)) * points_per_image[5:10, :] + np.tile(\n            image_obj[\'total_boxes\'][:, 1], (5, 1)) - 1\n\n        if image_obj[\'total_boxes\'].shape[0] > 0:\n            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv))\n            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Min\')\n            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n            points_per_image = points_per_image[:, pick]\n\n            ret.append((image_obj[\'total_boxes\'], points_per_image))\n        else:\n            ret.append(None)\n\n        i += onet_input_count\n\n    return ret\n\n\n# function [boundingbox] = bbreg(boundingbox,reg)\ndef bbreg(boundingbox, reg):\n    # calibrate bounding boxes\n    if reg.shape[1] == 1:\n        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n\n    w = boundingbox[:, 2] - boundingbox[:, 0] + 1\n    h = boundingbox[:, 3] - boundingbox[:, 1] + 1\n    b1 = boundingbox[:, 0] + reg[:, 0] * w\n    b2 = boundingbox[:, 1] + reg[:, 1] * h\n    b3 = boundingbox[:, 2] + reg[:, 2] * w\n    b4 = boundingbox[:, 3] + reg[:, 3] * h\n    boundingbox[:, 0:4] = np.transpose(np.vstack([b1, b2, b3, b4]))\n    return boundingbox\n\n\ndef generateBoundingBox(imap, reg, scale, t):\n    # use heatmap to generate bounding boxes\n    stride = 2\n    cellsize = 12\n\n    imap = np.transpose(imap)\n    dx1 = np.transpose(reg[:, :, 0])\n    dy1 = np.transpose(reg[:, :, 1])\n    dx2 = np.transpose(reg[:, :, 2])\n    dy2 = np.transpose(reg[:, :, 3])\n    y, x = np.where(imap >= t)\n    if y.shape[0] == 1:\n        dx1 = np.flipud(dx1)\n        dy1 = np.flipud(dy1)\n        dx2 = np.flipud(dx2)\n        dy2 = np.flipud(dy2)\n    score = imap[(y, x)]\n    reg = np.transpose(np.vstack([dx1[(y, x)], dy1[(y, x)], dx2[(y, x)], dy2[(y, x)]]))\n    if reg.size == 0:\n        reg = np.empty((0, 3))\n    bb = np.transpose(np.vstack([y, x]))\n    q1 = np.fix((stride * bb + 1) / scale)\n    q2 = np.fix((stride * bb + cellsize - 1 + 1) / scale)\n    boundingbox = np.hstack([q1, q2, np.expand_dims(score, 1), reg])\n    return boundingbox, reg\n\n\n# function pick = nms(boxes,threshold,type)\ndef nms(boxes, threshold, method):\n    if boxes.size == 0:\n        return np.empty((0, 3))\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    s = boxes[:, 4]\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    I = np.argsort(s)\n    pick = np.zeros_like(s, dtype=np.int16)\n    counter = 0\n    while I.size > 0:\n        i = I[-1]\n        pick[counter] = i\n        counter += 1\n        idx = I[0:-1]\n        xx1 = np.maximum(x1[i], x1[idx])\n        yy1 = np.maximum(y1[i], y1[idx])\n        xx2 = np.minimum(x2[i], x2[idx])\n        yy2 = np.minimum(y2[i], y2[idx])\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        if method is \'Min\':\n            o = inter / np.minimum(area[i], area[idx])\n        else:\n            o = inter / (area[i] + area[idx] - inter)\n        I = I[np.where(o <= threshold)]\n    pick = pick[0:counter]\n    return pick\n\n\n# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\ndef pad(total_boxes, w, h):\n    # compute the padding coordinates (pad the bounding boxes to square)\n    tmpw = (total_boxes[:, 2] - total_boxes[:, 0] + 1).astype(np.int32)\n    tmph = (total_boxes[:, 3] - total_boxes[:, 1] + 1).astype(np.int32)\n    numbox = total_boxes.shape[0]\n\n    dx = np.ones((numbox), dtype=np.int32)\n    dy = np.ones((numbox), dtype=np.int32)\n    edx = tmpw.copy().astype(np.int32)\n    edy = tmph.copy().astype(np.int32)\n\n    x = total_boxes[:, 0].copy().astype(np.int32)\n    y = total_boxes[:, 1].copy().astype(np.int32)\n    ex = total_boxes[:, 2].copy().astype(np.int32)\n    ey = total_boxes[:, 3].copy().astype(np.int32)\n\n    tmp = np.where(ex > w)\n    edx.flat[tmp] = np.expand_dims(-ex[tmp] + w + tmpw[tmp], 1)\n    ex[tmp] = w\n\n    tmp = np.where(ey > h)\n    edy.flat[tmp] = np.expand_dims(-ey[tmp] + h + tmph[tmp], 1)\n    ey[tmp] = h\n\n    tmp = np.where(x < 1)\n    dx.flat[tmp] = np.expand_dims(2 - x[tmp], 1)\n    x[tmp] = 1\n\n    tmp = np.where(y < 1)\n    dy.flat[tmp] = np.expand_dims(2 - y[tmp], 1)\n    y[tmp] = 1\n\n    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n\n\n# function [bboxA] = rerec(bboxA)\ndef rerec(bboxA):\n    # convert bboxA to square\n    h = bboxA[:, 3] - bboxA[:, 1]\n    w = bboxA[:, 2] - bboxA[:, 0]\n    l = np.maximum(w, h)\n    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5\n    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5\n    bboxA[:, 2:4] = bboxA[:, 0:2] + np.transpose(np.tile(l, (2, 1)))\n    return bboxA\n\n\ndef imresample(img, sz):\n    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA)  # @UndefinedVariable\n    return im_data\n\n    # This method is kept for debugging purpose\n\n#     h=img.shape[0]\n#     w=img.shape[1]\n#     hs, ws = sz\n#     dx = float(w) / ws\n#     dy = float(h) / hs\n#     im_data = np.zeros((hs,ws,3))\n#     for a1 in range(0,hs):\n#         for a2 in range(0,ws):\n#             for a3 in range(0,3):\n#                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\n#     return im_data\n'"
lib/__init__.py,0,b''
lib/face_utils.py,0,"b'import numpy as np\n\n\ndef judge_side_face(facial_landmarks):\n    wide_dist = np.linalg.norm(facial_landmarks[0] - facial_landmarks[1])\n    high_dist = np.linalg.norm(facial_landmarks[0] - facial_landmarks[3])\n    dist_rate = high_dist / wide_dist\n\n    # cal std\n    vec_A = facial_landmarks[0] - facial_landmarks[2]\n    vec_B = facial_landmarks[1] - facial_landmarks[2]\n    vec_C = facial_landmarks[3] - facial_landmarks[2]\n    vec_D = facial_landmarks[4] - facial_landmarks[2]\n    dist_A = np.linalg.norm(vec_A)\n    dist_B = np.linalg.norm(vec_B)\n    dist_C = np.linalg.norm(vec_C)\n    dist_D = np.linalg.norm(vec_D)\n\n    # cal rate\n    high_rate = dist_A / dist_C\n    width_rate = dist_C / dist_D\n    high_ratio_variance = np.fabs(high_rate - 1.1)  # smaller is better\n    width_ratio_variance = np.fabs(width_rate - 1)\n\n    return dist_rate, high_ratio_variance, width_ratio_variance\n'"
lib/utils.py,0,"b'import logging\nimport os\nimport time\nimport uuid\nfrom operator import itemgetter\n\nimport cv2\nimport project_root_dir\n\nlog_file_root_path = os.path.join(project_root_dir.project_dir, \'logs\')\nlog_time = time.strftime(\'%Y_%m_%d_%H_%M\', time.localtime(time.time()))\n\n\ndef mkdir(path):\n    path.strip()\n    path.rstrip(\'\\\\\')\n    isExists = os.path.exists(path)\n    if not isExists:\n        os.makedirs(path)\n\n\ndef save_to_file(root_dic, tracker):\n    filter_face_addtional_attribute_list = []\n    for item in tracker.face_addtional_attribute:\n        if item[2] < 1.4 and item[4] < 1:  # recommended thresold value\n            filter_face_addtional_attribute_list.append(item)\n    if len(filter_face_addtional_attribute_list) > 0:\n        score_reverse_sorted_list = sorted(filter_face_addtional_attribute_list, key=itemgetter(4))\n        mkdir(root_dic)\n        cv2.imwrite(""{0}/{1}.jpg"".format(root_dic, str(uuid.uuid1())), score_reverse_sorted_list[0][0])\n\n\nclass Logger:\n\n    def __init__(self, module_name=""MOT""):\n        super().__init__()\n        path_join = os.path.join(log_file_root_path, module_name)\n        mkdir(path_join)\n\n        self.logger = logging.getLogger(module_name)\n        self.logger.setLevel(logging.INFO)\n        log_file = os.path.join(path_join, \'{}.log\'.format(log_time))\n        if not self.logger.handlers:\n            fh = logging.FileHandler(log_file, encoding=\'utf-8\')\n            fh.setLevel(logging.INFO)\n\n            ch = logging.StreamHandler()\n            ch.setLevel(logging.INFO)\n            formatter = logging.Formatter(\n                ""%(asctime)s - %(name)s - %(levelname)s - %(message)s -  %(threadName)s - %(process)d "")\n            ch.setFormatter(formatter)\n            fh.setFormatter(formatter)\n            self.logger.addHandler(ch)\n            self.logger.addHandler(fh)\n\n    def error(self, msg, *args, **kwargs):\n        if self.logger is not None:\n            self.logger.error(msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        if self.logger is not None:\n            self.logger.info(msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        if self.logger is not None:\n            self.logger.warning(msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        if self.logger is not None:\n            self.logger.warning(msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        if self.logger is not None:\n            self.logger.exception(msg, *args, exc_info=True, **kwargs)\n'"
src/__init__.py,0,b''
src/data_association.py,0,"b'""""""\nAs implemented in https://github.com/abewley/sort but with some modifications\n\nFor each detected item, it computes the intersection over union (IOU) w.r.t. each tracked object. (IOU matrix)\nThen, it applies the Hungarian algorithm (via linear_assignment) to assign each det. item to the best possible\ntracked item (i.e. to the one with max. IOU).\n\nNote: a more recent approach uses a Deep Association Metric instead.\nsee https://github.com/nwojke/deep_sort\n""""""\n\nimport numpy as np\nfrom numba import jit\nfrom sklearn.utils.linear_assignment_ import linear_assignment\n\n\n@jit\ndef iou(bb_test, bb_gt):\n    """"""\n    Computes IUO between two bboxes in the form [x1,y1,x2,y2]\n    """"""\n    xx1 = np.maximum(bb_test[0], bb_gt[0])\n    yy1 = np.maximum(bb_test[1], bb_gt[1])\n    xx2 = np.minimum(bb_test[2], bb_gt[2])\n    yy2 = np.minimum(bb_test[3], bb_gt[3])\n    w = np.maximum(0., xx2 - xx1)\n    h = np.maximum(0., yy2 - yy1)\n    wh = w * h\n    o = wh / ((bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1])\n              + (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1]) - wh)\n    return (o)\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.25):\n    """"""\n    Assigns detections to tracked object (both represented as bounding boxes)\n\n    Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n    """"""\n    if len(trackers) == 0:\n        return np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int)\n    iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)\n\n    for d, det in enumerate(detections):\n        for t, trk in enumerate(trackers):\n            iou_matrix[d, t] = iou(det, trk)\n    \'\'\'The linear assignment module tries to minimise the total assignment cost.\n    In our case we pass -iou_matrix as we want to maximise the total IOU between track predictions and the frame detection.\'\'\'\n    matched_indices = linear_assignment(-iou_matrix)\n\n    unmatched_detections = []\n    for d, det in enumerate(detections):\n        if d not in matched_indices[:, 0]:\n            unmatched_detections.append(d)\n    unmatched_trackers = []\n    for t, trk in enumerate(trackers):\n        if t not in matched_indices[:, 1]:\n            unmatched_trackers.append(t)\n\n    # filter out matched with low IOU\n    matches = []\n    for m in matched_indices:\n        if iou_matrix[m[0], m[1]] < iou_threshold:\n            unmatched_detections.append(m[0])\n            unmatched_trackers.append(m[1])\n        else:\n            matches.append(m.reshape(1, 2))\n    if len(matches) == 0:\n        matches = np.empty((0, 2), dtype=int)\n    else:\n        matches = np.concatenate(matches, axis=0)\n\n    return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n'"
src/kalman_tracker.py,0,"b'""""""\nAs implemented in https://github.com/abewley/sort but with some modifications\n""""""\n\nimport numpy as np\nfrom filterpy.kalman import KalmanFilter\n\n\'\'\'Motion Model\'\'\'\n\n\nclass KalmanBoxTracker(object):\n    """"""\n    This class represents the internal state of individual tracked objects observed as bbox.\n    """"""\n    count = 0\n\n    def __init__(self, bbox):\n        """"""\n        Initialises a tracker using initial bounding box.\n        """"""\n        # define constant velocity model\n        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n        self.kf.F = np.array(\n            [[1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0],\n             [0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1]])\n        self.kf.H = np.array(\n            [[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0]])\n\n        self.kf.R[2:, 2:] *= 10.\n        self.kf.P[4:, 4:] *= 1000.  # give high uncertainty to the unobservable initial velocities\n        self.kf.P *= 10.\n        self.kf.Q[-1, -1] *= 0.01\n        self.kf.Q[4:, 4:] *= 0.01\n\n        self.kf.x[:4] = convert_bbox_to_z(bbox)\n        self.time_since_update = 0\n        self.id = KalmanBoxTracker.count\n        KalmanBoxTracker.count += 1\n        self.history = []\n        self.hits = 0\n        self.hit_streak = 0\n        self.age = 0\n\n        self.predict_num = 0  # \xe8\xa7\xa3\xe5\x86\xb3\xe7\x94\xbb\xe9\x9d\xa2\xe4\xb8\xad\xe6\x97\xa0\xe4\xba\xba\xe8\x84\xb8\xe6\xa3\x80\xe6\xb5\x8b\xe5\x88\xb0\xe6\x97\xb6\xe8\x80\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe7\x9a\x84\xe5\x8e\x9f\xe6\x9c\x89\xe8\xbf\xbd\xe8\xb8\xaa\xe5\x99\xa8\xe4\xba\xba\xe5\x83\x8f\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xbc\x82\xe7\xa7\xbbbug\n\n        # addtional fields\n        self.face_addtional_attribute = []\n\n    def update(self, bbox):\n        """"""\n        Updates the state vector with observed bbox.\n        """"""\n        self.time_since_update = 0\n        self.history = []\n        self.hits += 1\n        self.hit_streak += 1\n        if bbox != []:\n            self.kf.update(convert_bbox_to_z(bbox))\n            self.predict_num = 0\n        else:\n            self.predict_num += 1\n\n    def predict(self):\n        """"""\n        Advances the state vector and returns the predicted bounding box estimate.\n        """"""\n        if (self.kf.x[6] + self.kf.x[2]) <= 0:\n            self.kf.x[6] *= 0.0\n        self.kf.predict()\n        self.age += 1\n        if self.time_since_update > 0:\n            self.hit_streak = 0\n        self.time_since_update += 1\n        self.history.append(convert_x_to_bbox(self.kf.x))\n        return self.history[-1][0]\n\n    def get_state(self):\n        """"""\n        Returns the current bounding box estimate.\n        """"""\n        return convert_x_to_bbox(self.kf.x)[0]\n\n\ndef convert_bbox_to_z(bbox):\n    """"""\n    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n      [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n      the aspect ratio\n    """"""\n    w = bbox[2] - bbox[0]\n    h = bbox[3] - bbox[1]\n    x = bbox[0] + w / 2.\n    y = bbox[1] + h / 2.\n    s = w * h  # scale is just area\n    r = w / float(h)\n    return np.array([x, y, s, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n    """"""\n    Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n      [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n    """"""\n    w = np.sqrt(x[2] * x[3])\n    h = x[2] / w\n    if score is None:\n        return np.array([x[0] - w / 2., x[1] - h / 2., x[0] + w / 2., x[1] + h / 2.]).reshape((1, 4))\n    else:\n        return np.array([x[0] - w / 2., x[1] - h / 2., x[0] + w / 2., x[1] + h / 2., score]).reshape((1, 5))\n'"
src/sort.py,0,"b'""""""\nAs implemented in https://github.com/abewley/sort but with some modifications\n""""""\n\nfrom __future__ import print_function\n\nimport lib.utils as utils\nimport numpy as np\nfrom src.data_association import associate_detections_to_trackers\nfrom src.kalman_tracker import KalmanBoxTracker\n\nlogger = utils.Logger(""MOT"")\n\n\nclass Sort:\n\n    def __init__(self, max_age=1, min_hits=3):\n        """"""\n        Sets key parameters for SORT\n        """"""\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.trackers = []\n        self.frame_count = 0\n\n    def update(self, dets, img_size, root_dic, addtional_attribute_list, predict_num):\n        """"""\n        Params:\n          dets - a numpy array of detections in the format [[x,y,w,h,score],[x,y,w,h,score],...]\n        Requires: this method must be called once for each frame even with empty detections.\n        Returns the a similar array, where the last column is the object ID.\n\n        NOTE:as in practical realtime MOT, the detector doesn\'t run on every single frame\n        """"""\n        self.frame_count += 1\n        # get predicted locations from existing trackers.\n        trks = np.zeros((len(self.trackers), 5))\n        to_del = []\n        ret = []\n        for t, trk in enumerate(trks):\n            pos = self.trackers[t].predict()  # kalman predict ,very fast ,<1ms\n            trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n            if np.any(np.isnan(pos)):\n                to_del.append(t)\n        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n        for t in reversed(to_del):\n            self.trackers.pop(t)\n        if dets != []:\n            matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets, trks)\n\n            # update matched trackers with assigned detections\n            for t, trk in enumerate(self.trackers):\n                if t not in unmatched_trks:\n                    d = matched[np.where(matched[:, 1] == t)[0], 0]\n                    trk.update(dets[d, :][0])\n                    trk.face_addtional_attribute.append(addtional_attribute_list[d[0]])\n\n            # create and initialise new trackers for unmatched detections\n            for i in unmatched_dets:\n                trk = KalmanBoxTracker(dets[i, :])\n                trk.face_addtional_attribute.append(addtional_attribute_list[i])\n                logger.info(""new Tracker: {0}"".format(trk.id + 1))\n                self.trackers.append(trk)\n\n        i = len(self.trackers)\n        for trk in reversed(self.trackers):\n            if dets == []:\n                trk.update([])\n            d = trk.get_state()\n            if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n                ret.append(np.concatenate((d, [trk.id + 1])).reshape(1, -1))  # +1 as MOT benchmark requires positive\n            i -= 1\n            # remove dead tracklet\n            if trk.time_since_update >= self.max_age or trk.predict_num >= predict_num or d[2] < 0 or d[3] < 0 or d[0] > img_size[1] or d[1] > img_size[0]:\n                if len(trk.face_addtional_attribute) >= 5:\n                    utils.save_to_file(root_dic, trk)\n                logger.info(\'remove tracker: {0}\'.format(trk.id + 1))\n                self.trackers.pop(i)\n        if len(ret) > 0:\n            return np.concatenate(ret)\n        return np.empty((0, 5))\n'"
