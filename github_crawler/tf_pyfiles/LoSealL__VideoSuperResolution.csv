file_path,api_count,code
prepare_data.py,0,"b'""""""\nCopyright: Wenyi Tang 2017-2019\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 20th 2018\n\nPrepare datasets and install VSR package for users.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport re\nimport sys\nimport tarfile\nimport zipfile\nfrom pathlib import Path\n\nfrom tensorflow import keras\n\nkutils = keras.utils\n\n# For now VSR requires python>=3.5\nif sys.version_info.major == 3 and sys.version_info.minor < 6:\n  print(""Python version is required >=3.6!"")\n  exit(-1)\n\n_DEFAULT_DATASET_PATH = \'/mnt/data/datasets\'\n_DEFAULT_DOWNLOAD_DIR = \'/tmp/downloads\'\n_DEFAULT_WEIGHTS_DIR = \'./Results\'\n# Contact me if any of these links un-accessed\nDATASETS = {\n  \'DIV2K\': {\n    \'DIV2K_train_HR.zip\': \'http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\',\n    \'DIV2K_valid_HR.zip\': \'http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip\',\n    \'DIV2K_train_LR_unknown_X4.zip\': \'http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_unknown_X4.zip\',\n    \'DIV2K_valid_LR_unknown_X4.zip\': \'http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_unknown_X4.zip\',\n  },\n  \'SET5.zip\': \'https://uofi.box.com/shared/static/kfahv87nfe8ax910l85dksyl2q212voc.zip\',\n  \'SET14.zip\': \'https://uofi.box.com/shared/static/igsnfieh4lz68l926l8xbklwsnnk8we9.zip\',\n  \'SunHay80.zip\': \'https://uofi.box.com/shared/static/rirohj4773jl7ef752r330rtqw23djt8.zip\',\n  \'Urban100.zip\': \'https://uofi.box.com/shared/static/65upg43jjd0a4cwsiqgl6o6ixube6klm.zip\',\n  # \'VID4.zip\': \'https://people.csail.mit.edu/celiu/CVPR2011/videoSR.zip\',\n  \'BSD300.tgz\': \'https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300-images.tgz\',\n  \'BSD500.tgz\': \'http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\',\n  \'91image.rar\': \'http://www.ifp.illinois.edu/~jyang29/codes/ScSR.rar\',\n  \'waterloo.rar\': \'http://ivc.uwaterloo.ca/database/WaterlooExploration/exploration_database_and_code.rar\',\n  \'GOPRO_Large.zip\': \'1H0PIXvJH4c40pk7ou6nAwoxuR4Qh_Sa2\',\n  \'MCL-V.rar\': \'1z41hdqR-bqNLcUWllPePzkfQW-I_A9ny\',\n  \'vid4.zip\': \'1ogEdifL_krqJnFAHfGNqOSMuUg_Ud6fb\',\n}\nWEIGHTS = {\n  \'srcnn.tar.gz\': \'https://github.com/LoSealL/Model/releases/download/srcnn/srcnn.tar.gz\',\n  \'espcn.tar.gz\': \'https://github.com/LoSealL/Model/releases/download/espcn/espcn.tar.gz\',\n  \'edsr.zip\': \'https://github.com/LoSealL/Model/releases/download/edsr/edsr.zip\',\n  \'dncnn.zip\': \'https://github.com/LoSealL/Model/releases/download/DnCNN/dncnn.zip\',\n  \'carn.zip\': \'https://github.com/LoSealL/Model/releases/download/carn/carn.zip\',\n  # Google Drive File ID.\n  # If you can\'t download from this file, visit url https://drive.google.com/open?id=<id>\n  # paste the file id into position <id>.\n  \'srdensenet.zip\': \'1aXAfRqZieY6mTfZUnErG84-9NfkQSeDw\',\n  \'vdsr.zip\': \'1hW5YDxXpmjO2IfAy8f29O7yf1M3fPIg1\',\n  \'msrn.zip\': \'1A0LoY3oB_VnArP3GzI1ILUNJbLAEjdtJ\',\n  \'vespcn.zip\': \'19u4YpsyThxW5dv4fhpMj7c5gZeEDKthm\',\n  \'dbpn.zip\': \'1ymtlOjhkGmad-od0zw7yTf17nWD4KMVi\',\n  \'idn.zip\': \'1Fh3rtvrKKLAK27r518T1M_JET_LWZAFQ\',\n  \'drsr_v2.zip\': \'1UrVNE6QMcQTW9Ks4P__JrRClb4IGTMYp\',\n  \'drsr_sc2.zip\': \'1xIRVG7jbTM9fcLQkwyGyJIjwF2rTbNEJ\',\n  \'drsr_sc4.zip\': \'1W-222rR2D2o-E99B4cXuUPBz2aCLuY_Z\',\n  # GAN weights\n  \'gangp.zip\': \'1UHiSLjaU5Yeiltl9cQsR3-EKta3yt0dI\',\n  \'lsgan.zip\': \'15dsubMpvTeCoSCIfPCcKjhnk7UMyuljt\',\n  \'ragan.zip\': \'1HWR2m3cFH-Fze1zkioj20ugDXRmjGQEH\',\n  \'ragangp.zip\': \'1lf3Rj3Lk1qISbQiIQiSJt03DVV5pp5Ml\',\n  \'ralsgan.zip\': \'180qrnH8_MdFvLlSl5MSP8sQCPLbbevsr\',\n  \'rgan.zip\': \'1ZwCB1Fa9UIybOq1SfgOeBKJ8g63KMYEK\',\n  \'rgangp.zip\': \'1QSBVscdfJvf_dMRRiBA_lCq39gX9mDZJ\',\n  \'rlsgan.zip\': \'1siDKxGvlb0p2E2_EmAJoT8knFMuQRivj\',\n  \'sgan.zip\': \'1spClB26QJNQEio_DktobQq9ALT-PHfg3\',\n  \'wgangp.zip\': \'1jyngiCyU1Js4DH5yUhug4gTPy2bQoETO\',\n  # PyTorch weights (Prefix ""T"")\n  \'Tsrcnn.zip\': \'https://github.com/LoSealL/Model/releases/download/srcnn/Tsrcnn.zip\',\n  \'Tespcn.zip\': \'https://github.com/LoSealL/Model/releases/download/espcn/Tespcn.zip\',\n  \'Tvdsr.zip\': \'https://github.com/LoSealL/Model/releases/download/vdsr/Tvdsr.zip\',\n  \'Tdrcn.zip\': \'https://github.com/LoSealL/Model/releases/download/drcn/Tdrcn.zip\',\n  \'Tdrrn.zip\': \'https://github.com/LoSealL/Model/releases/download/drrn/Tdrrn.zip\',\n  \'Tsofvsr.zip\': \'https://github.com/LoSealL/Model/releases/download/sofvsr/SOFVSR_x4.zip\',\n  \'Tcarn.zip\': \'https://github.com/LoSealL/Model/releases/download/carn/tcarn.zip\',\n  \'Tesrgan.zip\': \'https://github.com/LoSealL/Model/releases/download/esrgan/esrgan.zip\',\n  \'Ttecogan.zip\': \'https://github.com/LoSealL/Model/releases/download/tecogan/tecogan.zip\',\n  \'Tfrvsr.zip\': \'https://github.com/LoSealL/Model/releases/download/frvsr/FRVSR.zip\',\n  \'Tmldn.zip\': \'https://github.com/LoSealL/Model/releases/download/mldn/drn.zip\',\n  \'Tcrdn.zip\': \'https://github.com/LoSealL/Model/releases/download/crdn/rsr.zip\',\n  \'Trbpn.zip\': \'1Ozp5j-DBWJSpXY5GvxiEPKdfCaAbOXqu\',\n  \'Tspmc.zip\': \'https://github.com/LoSealL/Model/releases/download/spmc/spmc.zip\',\n  \'Tvespcn.zip\': \'https://github.com/LoSealL/Model/releases/download/vespcn/Tvespcn.zip\',\n  \'Tsrmd.zip\': \'1ORKH05-aLSbQaWB4qQulIm2INoRufuD_\',\n  \'Tdbpn.zip\': \'1PbhtuMz1zF3-d16dthurJ0xIQ9uyMvkz\'\n}\n\n\ndef get_input(question):\n  try:\n    ans = input(question)\n  except KeyboardInterrupt:\n    ans = None\n    print(\'\\n\', flush=True)  # user exit\n    exit(0)\n  return ans\n\n\ndef matches(str1, pattern):\n  if not pattern:\n    return str1\n  ret = re.match(pattern.lower(), str1.lower())\n  if ret:\n    return str1\n\n\ndef user_input(name, defaults=False, pattern=None):\n  _name = name\n  for _pat in pattern:\n    _name = matches(name, _pat)\n    if _name is not None:\n      break\n  if not _name:\n    return\n  question = \'Do you wish to download {}? \'.format(_name)\n  if defaults:\n    return True\n  else:\n    question += \'[y/N] \'\n  var = None\n  while var is None:\n    raw_ans = get_input(question)\n    if raw_ans is None:\n      print(\'\\n\', flush=True)  # user exit\n      break\n    elif raw_ans == \'\':\n      var = defaults\n      break\n    ans = raw_ans.lower()\n    if ans == \'y\':\n      var = True\n    elif ans == \'n\':\n      var = False\n    else:\n      print(\'Invalid selection: {}\'.format(raw_ans))\n  return var\n\n\ndef download(name, url, path):\n  fname = str(Path(path).resolve() / name)\n  try:\n    file = kutils.get_file(fname, url)\n    return file\n  except Exception:\n    print(\'Unable to get file {}\'.format(name))\n\n\ndef drive_download(name, url, path):\n  print(f""Google Drive Download API has been expired, ""\n        f""please download {name} from https://drive.google.com/open?id={url} directly.""\n        ""\\nFor more question, please message me at https://gitub.com/loseall."")\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--download_dir"", type=str,\n                      default=_DEFAULT_DOWNLOAD_DIR,\n                      help=""Specify download directory. ""\n                           ""[{}]"".format(_DEFAULT_DOWNLOAD_DIR))\n  parser.add_argument(""--data_dir"", type=str,\n                      default=_DEFAULT_DATASET_PATH,\n                      help=""Specify dataset extracted directory. ""\n                           ""[{}]"".format(_DEFAULT_DATASET_PATH))\n  parser.add_argument(""--weights_dir"", type=str,\n                      default=_DEFAULT_WEIGHTS_DIR,\n                      help=""Specify weights extracted directory. ""\n                           ""[{}]"".format(_DEFAULT_WEIGHTS_DIR))\n  parser.add_argument(""--filter"", nargs=\'*\', default=[],\n                      help=""an re pattern to filter candidates."")\n  parser.add_argument(""-q"", ""--quiet"", action=""store_true"",\n                      help=""download quietly"")\n  args, _ = parser.parse_known_args()\n  # make work dir\n  Path(args.download_dir).mkdir(exist_ok=True, parents=True)\n\n  def get_leaf(key: str, node: dict):\n    for k, v in node.items():\n      if isinstance(v, dict):\n        for k2, v2 in get_leaf(k, v):\n          yield Path(key) / k2, v2\n      else:\n        yield Path(key) / k, v\n\n  need_to_download = {}\n  try:\n    Path(args.data_dir).mkdir(exist_ok=True, parents=True)\n    for k, v in get_leaf(args.data_dir, DATASETS):\n      if user_input(k.stem, args.quiet, args.filter):\n        need_to_download[k] = v\n  except (FileNotFoundError, OSError):\n    pass\n  for k, v in get_leaf(args.weights_dir, WEIGHTS):\n    if user_input(k.stem, args.quiet, args.filter):\n      need_to_download[k] = v\n  need_to_extract = {}\n  for k, v in need_to_download.items():\n    if v[:4] == \'http\':\n      need_to_extract[k] = (k.parent,\n                            download(k.name, v, args.download_dir))\n    else:\n      need_to_extract[k] = (k.parent,\n                            drive_download(k.name, v, args.download_dir))\n  for k, v in need_to_extract.values():\n    if v is None:\n      continue\n    ext = Path(v).suffix\n    if ext in (\'.tar\', \'.tgz\', \'.gz\', \'.bz\'):\n      open_fn = tarfile.open\n      is_match_fn = tarfile.is_tarfile\n    elif ext in (\'.zip\',):\n      open_fn = zipfile.ZipFile\n      is_match_fn = zipfile.is_zipfile\n    else:\n      raise TypeError(""Unrecognized extension: {}"".format(ext))\n    if is_match_fn(v):\n      with open_fn(v) as fd:\n        try:\n          fd.extractall(str(k.resolve()))\n        except (tarfile.TarError, RuntimeError, KeyboardInterrupt):\n          # TBD...\n          pass\n    else:\n      print(""[WARN] {} have to be uncompressed manually."".format(v))\n\n\nif __name__ == \'__main__\':\n  main()\n'"
setup.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 16\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n# Get version from CHANGELOG\ntry:\n  with open(\'CHANGELOG.md\') as fd:\n    VERSION = fd.readline()[:-1]\nexcept IOError:\n  VERSION = \'0.0.0\'\n\nREQUIRED_PACKAGES = [\n  \'numpy\',\n  \'scipy\',\n  \'scikit-image\',\n  \'matplotlib\',\n  \'pillow\',\n  \'pypng\',\n  \'pytest\',\n  \'PyYAML\',\n  \'psutil\',\n  \'tqdm\',\n  \'h5py\',\n  \'easydict >= 1.9\',\n]\n\nwith open(\'README.md\', \'r\', encoding=\'utf-8\') as fd:\n  long_desp = fd.read()\n\nsetup(\n    name=\'VSR\',\n    version=VERSION,\n    description=\'Video Super-Resolution Framework\',\n    long_description=long_desp,\n    long_description_content_type=""text/markdown"",\n    url=\'https://github.com/LoSealL/VideoSuperResolution\',\n    packages=find_packages(),\n    install_requires=REQUIRED_PACKAGES,\n    license=\'MIT\',\n    author=\'Wenyi Tang\',\n    author_email=\'wenyitang@outlook.com\',\n    keywords=""super-resolution sr vsr cnn srcnn vespcn"",\n    classifiers=[\n      ""Programming Language :: Python :: 3"",\n      ""License :: OSI Approved :: MIT License"",\n      ""Operating System :: OS Independent"",\n    ],\n    python_requires=\'>=3.6\',\n)\n'"
Tests/correlation_test.py,3,"b""import os\nimport unittest\n\nif not os.getcwd().endswith('Tests'):\n  os.chdir('Tests')\n\nimport tensorflow as tf\nfrom VSR.Backend.TF.Util import _make_vector, _make_displacement, correlation\n\n\nclass CorrelationTest(unittest.TestCase):\n  @staticmethod\n  def constant():\n    return tf.constant([[\n      [[1, 1.1, 1.2], [2, 2.1, 2.2], [3, 3.1, 3.2]],\n      [[4, 4.1, 4.2], [5, 5.1, 5.2], [6, 6.1, 6.2]],\n      [[7, 7.1, 7.2], [8, 8.1, 8.2], [9, 9.1, 9.2]]\n    ]], 'float32')\n\n  def test_correlation(self):\n    with tf.Session() as sess:\n      vec = _make_vector(self.constant()).eval()\n      disp = _make_displacement(self.constant()).eval()\n      x = self.constant()\n      corr = correlation(x, x, 3, 1).eval()\n      x = tf.ones([1, 5, 5, 1], 'float32')\n      corr_stride = correlation(x, x, 3, 2, 2, 2).eval()\n    self.assertEqual(vec.shape, [1, 3, 3, 27])\n    self.assertEqual(disp.shape, [1, 3, 3, 1])\n    self.assertEqual(corr.shape, [1, 3, 3, 4])\n    self.assertEqual(corr_stride.shape, [1, 3, 3, 4])\n\n\nif __name__ == '__main__':\n  unittest.main()\n"""
Tests/dataset_test.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 16\n\nimport os\nimport unittest\n\nif not os.getcwd().endswith('Tests'):\n  os.chdir('Tests')\nfrom VSR.DataLoader.Dataset import Dataset, load_datasets\n\n\nclass DatasetTest(unittest.TestCase):\n  def test_image_data(self):\n    d = Dataset('data/set5_x2')\n    data = d.compile()\n    self.assertEqual(len(data), 5)\n    self.assertEqual(data.capacity, 983040)\n\n  def test_video_data(self):\n    d = Dataset('data/video/custom_pair').use_like_video()\n    data = d.compile()\n    self.assertEqual(len(data), 2)\n\n  def test_multi_url(self):\n    d = Dataset('data/set5_x2', 'data/kitti_car')\n    data = d.compile()\n    self.assertEqual(len(data), 8)\n\n  def test_include_exclude(self):\n    d = Dataset('data')\n    d.include_('xiuxian*')\n    data1 = d.compile()\n    d = Dataset('data')\n    d.include_reg_('set5')\n    data2 = d.compile()\n    d = Dataset('data').include_reg('set5').exclude('png')\n    data3 = d.compile()\n\n    self.assertEqual(len(data1), 6)\n    self.assertEqual(len(data2), 5)\n    self.assertEqual(len(data3), 0)\n\n  def test_dataset_desc_file(self):\n    ddf = 'data/fake_datasets.yml'\n    datasets = load_datasets(ddf)\n    self.assertEqual(len(datasets), 9)\n    self.assertEqual(len(datasets.NONE.train.hr.compile()), 0)\n    self.assertEqual(len(datasets.NORMAL.train.hr.compile()), 7)\n    self.assertEqual(len(datasets.NORMAL.val.hr.compile()), 5)\n    self.assertEqual(len(datasets.NORMAL.test.hr.compile()), 1)\n    self.assertEqual(len(datasets.PAIR.train.hr.compile()), 2)\n    self.assertEqual(len(datasets.PAIR.train.lr.compile()), 2)\n    self.assertEqual(len(datasets.VIDEOPAIR.train.hr.compile()), 1)\n    self.assertEqual(len(datasets.VIDEOPAIR.train.lr.compile()), 1)\n    self.assertEqual(len(datasets.VIDEOPAIR.val.hr.compile()), 1)\n    self.assertEqual(len(datasets.VIDEOPAIR.val.lr.compile()), 1)\n    self.assertEqual(len(datasets.VIDEOPAIR.test.hr.compile()), 1)\n    self.assertEqual(len(datasets.VIDEOPAIR.test.lr.compile()), 1)\n    self.assertEqual(len(datasets.FOO.test.hr.compile()), 2)\n    self.assertEqual(len(datasets.BAR.test.hr.compile()), 5)\n    self.assertTrue(datasets.VIDEOPAIR.train.hr.as_video)\n    self.assertTrue(datasets.XIUXIAN.test.hr.as_video)\n\n    raw = load_datasets(ddf, 'RAW')\n    self.assertEqual(len(raw.train.hr.compile()), 1)\n    self.assertEqual(len(raw.val.hr.compile()), 1)\n    self.assertEqual(len(raw.test.hr.compile()), 1)\n    self.assertTrue(raw.train.hr.as_video)\n\n\nif __name__ == '__main__':\n  unittest.main()\n"""
Tests/googledrive_test.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 16\n\nimport os\nimport unittest\n\nif not os.getcwd().endswith(\'Tests\'):\n  os.chdir(\'Tests\')\n\ntry:\n  from googleapiclient.discovery import build\n  from httplib2 import Http\n  from oauth2client import file, client, tools\nexcept ImportError as ex:\n  print(f""[!] {ex}"")\n  exit(-1)\n\n# If modifying these scopes, delete the file token.json.\nSCOPES = \'https://www.googleapis.com/auth/drive.readonly\'\n\n\nclass FetchGoogleDriveTest(unittest.TestCase):\n  def test_downloads(self):\n    """"""Shows basic usage of the Drive v3 API.\n       Prints the names and ids of the first 10 files the user has access to.\n    """"""\n    # The file token.json stores the user\'s access and refresh tokens, and is\n    # created automatically when the authorization flow completes for the first\n    # time.\n    store = file.Storage(\'/tmp/token.json\')\n    creds = store.get()\n    if not creds or creds.invalid:\n      flow = client.flow_from_clientsecrets(\'../Data/credentials.json\', SCOPES)\n      creds = tools.run_flow(flow, store)\n    service = build(\'drive\', \'v3\', http=creds.authorize(Http()))\n    file_id = \'1H0PIXvJH4c40pk7ou6nAwoxuR4Qh_Sa2\'\n    request = service.files().get_media(fileId=file_id)\n    request.execute()\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
Tests/image_test.py,4,"b'import os\nimport unittest\n\nif not os.getcwd().endswith(\'Tests\'):\n  os.chdir(\'Tests\')\n\nimport numpy as np\nfrom PIL import Image\nfrom VSR.Util import imread, rgb_to_yuv\nfrom VSR.Backend import BACKEND\n\nURL = \'data/set5_x2/img_001_SRF_2_LR.png\'\n\n\nclass ImageTest(unittest.TestCase):\n  def psnr(self, x: np.ndarray, y: np.ndarray):\n    mse = np.mean((x - y) ** 2)\n    if mse == 0:\n      return np.inf\n    psnr = np.log10(255 * 255 / mse) * 10\n    return psnr\n\n  def test_rgb2yuv(self):\n    img = imread(URL, mode=\'RGB\').astype(\'float32\')\n    yuv = rgb_to_yuv(img, 255, \'matlab\')\n    yuv_ref = imread(URL, mode=\'YCbCr\').astype(\'float32\')\n    # should have the same shape\n    self.assertEqual(yuv.shape, img.shape)\n    self.assertGreaterEqual(self.psnr(yuv, yuv_ref), 30)\n\n  def test_resize_upsample_tf(self):\n    if BACKEND != \'tensorflow\':\n      return\n    import tensorflow as tf\n    tf.enable_eager_execution()\n    from VSR.Backend.TF.Util import upsample\n\n    Im = Image.open(URL)\n    for X in [Im, Im.convert(\'L\')]:\n      w = X.width\n      h = X.height\n      for ss in [2, 3, 4, 5, 6]:\n        GT = X.resize([w * ss, h * ss], Image.BICUBIC)\n        gt = np.asarray(GT, dtype=\'float32\') / 255\n        x = tf.constant(np.asarray(X), dtype=\'float32\') / 255\n        y = upsample(x, ss).numpy().clip(0, 1)\n        self.assertGreaterEqual(self.psnr(y, gt), 30, f""{X.mode}, {ss}"")\n\n  def test_resize_downsample_tf(self):\n    if BACKEND != \'tensorflow\':\n      return\n    import tensorflow as tf\n    tf.enable_eager_execution()\n    from VSR.Backend.TF.Util import downsample\n\n    Im = Image.open(URL)\n    for X in [Im, Im.convert(\'L\')]:\n      w = X.width\n      h = X.height\n      for ss in [2, 4, 6, 8]:\n        w_ = w - w % ss\n        h_ = h - h % ss\n        X = X.crop([0, 0, w_, h_])\n        GT = X.resize([w_ // ss, h_ // ss], Image.BICUBIC)\n        gt = np.asarray(GT, dtype=\'float32\') / 255\n        x = tf.constant(np.asarray(X), dtype=\'float32\') / 255\n        y = downsample(x, ss).numpy().clip(0, 1)\n        self.assertGreaterEqual(self.psnr(y, gt), 30, f""{X.mode}, {ss}"")\n\n  def test_resize_upsample_torch(self):\n    if BACKEND != \'pytorch\':\n      return\n    from VSR.Backend.Torch.Util.Utility import upsample\n    import torchvision\n\n    Im = Image.open(URL)\n    trans = torchvision.transforms.ToTensor()\n    for X in [Im, Im.convert(\'L\')]:\n      w = X.width\n      h = X.height\n      for ss in [2, 3, 4, 5, 6]:\n        GT = X.resize([w * ss, h * ss], Image.BICUBIC)\n        gt = trans(GT).numpy()\n        x = trans(X)\n        y = upsample(x, ss).numpy().clip(0, 1)\n        self.assertGreaterEqual(self.psnr(y, gt), 30, f""{X.mode}, {ss}"")\n\n  def test_resize_downsample_torch(self):\n    if BACKEND != \'pytorch\':\n      return\n    from VSR.Backend.Torch.Util.Utility import downsample\n    import torchvision\n\n    Im = Image.open(URL)\n    trans = torchvision.transforms.ToTensor()\n    for X in [Im, Im.convert(\'L\')]:\n      w = X.width\n      h = X.height\n      for ss in [2, 4, 6, 8]:\n        w_ = w - w % ss\n        h_ = h - h % ss\n        X = X.crop([0, 0, w_, h_])\n        GT = X.resize([w_ // ss, h_ // ss], Image.BICUBIC)\n        gt = trans(GT).numpy()\n        x = trans(X)\n        y = downsample(x, ss).numpy().clip(0, 1)\n        self.assertGreaterEqual(self.psnr(y, gt), 30, f""{X.mode}, {ss}"")\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
Tests/imfilter_test.py,4,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 13\n\nimport unittest\n\nimport numpy as np\n\nfrom VSR.Util.Math import (anisotropic_gaussian_kernel, gaussian_kernel)\nfrom VSR.Backend import BACKEND\n\n_K1 = gaussian_kernel(15, 2)\n_K2 = anisotropic_gaussian_kernel(15, 1, 5, 3)\n\n\nclass ImFilter(unittest.TestCase):\n  # For ones([4, 4])\n  y_gold = np.array([\n    [0.3151, 0.3776, 0.3776, 0.3151],\n    [0.3776, 0.4524, 0.4524, 0.3776],\n    [0.3776, 0.4524, 0.4524, 0.3776],\n    [0.3151, 0.3776, 0.3776, 0.3151]\n  ])\n  z_gold = np.array([\n    [0.3391, 0.3950, 0.3774, 0.2950],\n    [0.3850, 0.4627, 0.4557, 0.3677],\n    [0.3677, 0.4557, 0.4627, 0.3850],\n    [0.2950, 0.3774, 0.3950, 0.3391]\n  ])\n\n  def test_torch(self):\n    if BACKEND != 'pytorch':\n      return\n    import torch\n    from VSR.Backend.Torch.Util.Utility import imfilter\n\n    tk1 = torch.tensor(_K1, dtype=torch.float32)\n    tk2 = torch.tensor(_K2, dtype=torch.float32)\n    x = torch.ones(2, 3, 4, 4, dtype=torch.float32)\n    y = imfilter(x, tk1)\n    z = imfilter(x, torch.stack([tk1, tk2]))\n    y_ = y.detach().numpy()\n    z_ = z.detach().numpy()\n    self.assertTrue(np.all(y_[0] == z_[0]))\n    self.assertTrue(np.all(np.abs(y_[0, 0] - self.y_gold) <= 1e-4))\n    self.assertTrue(np.all(np.abs(z_[1, 0] - self.z_gold) <= 1e-4))\n\n  def test_tf(self):\n    if BACKEND != 'tensorflow':\n      return\n    import tensorflow as tf\n    from VSR.Backend.TF.Util import imfilter\n\n    tk1 = tf.constant(_K1, dtype=tf.float32)\n    tk2 = tf.constant(_K2, dtype=tf.float32)\n    x = tf.ones([2, 4, 4, 3], dtype=tf.float32)\n    y = imfilter(x, tk1)\n    z = imfilter(x, tk2)\n    with tf.Session() as sess:\n      y_, z_ = sess.run([y, z])\n      self.assertTrue(np.all(np.abs(y_[0, ..., 0] - self.y_gold) <= 1e-4))\n      self.assertTrue(np.all(np.abs(z_[0, ..., 0] - self.z_gold) <= 1e-4))\n\n\nif __name__ == '__main__':\n  unittest.main()\n"""
Tests/initializer_test.py,5,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 5\n\nimport os\nimport unittest\n\nif not os.getcwd().endswith(\'Tests\'):\n  os.chdir(\'Tests\')\nimport numpy as np\n\ntry:\n  from torch import nn\n  import tensorflow as tf\n\n  tf.enable_eager_execution()\nexcept (ImportError, AttributeError) as ex:\n  print(f""[!] Cross validation is needed"")\n  print(ex)\n  exit(0)\n\nfrom VSR.Backend.TF.Util import TorchInitializer\n\n\nclass TestInitializer(unittest.TestCase):\n  def test_torch_initializer(self):\n    x = np.ones([4, 16, 16, 16], np.float32)\n    c2dtf = tf.layers.Conv2D(16, 3, padding=\'same\',\n                             kernel_initializer=TorchInitializer(),\n                             bias_initializer=TorchInitializer(9 * 16))\n    c2dtf.build(x.shape)\n    w1 = c2dtf.kernel\n    y1 = c2dtf.apply(x)\n\n    c2dnn = nn.Conv2d(16, 16, 3, padding=1)\n    w2 = c2dnn.weight\n    # TODO: how to test distribution?\n    assert True\n'"
Tests/loader_test.py,0,"b""#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x888:28\n\nimport os\nimport unittest\n\nif not os.getcwd().endswith('Tests'):\n  os.chdir('Tests')\n\nimport numpy as np\n\nfrom VSR.DataLoader.Loader import Loader\nfrom VSR.DataLoader.Dataset import Dataset\nfrom VSR.DataLoader.Crop import RandomCrop, CenterCrop\nfrom VSR.Util.ImageProcess import imresize\n\n\nclass LoaderTest(unittest.TestCase):\n  def assert_psnr(self, ret):\n    x = ret[0]['hr'][0, 0]\n    y = ret[0]['lr'][0, 0]\n    y = imresize(y, x.shape[-1] // y.shape[-1])\n    mse = np.mean((x - y) ** 2)\n    psnr = np.log10(255 * 255 / mse) * 10\n    self.assertGreaterEqual(psnr, 28)\n\n  def test_simplest_loader(self):\n    d = Dataset('data/set5_x2')\n    ld = Loader(d, scale=2, threads=4)\n    itr = ld.make_one_shot_iterator([4, 3, 4, 4], 10, True)\n    self.assertEqual(len(itr), 10)\n    ret = list(itr)\n    self.assertEqual(len(ret), 10)\n    itr = ld.make_one_shot_iterator([4, 3, 16, 16], 10, True)\n    self.assertEqual(len(itr), 10)\n    ret = list(itr)\n    self.assertEqual(len(ret), 10)\n    self.assert_psnr(ret)\n\n  def test_complex_loader(self):\n    d = Dataset('data').use_like_video().include_reg('hr/xiuxian')\n    hr = d.compile()\n    d = Dataset('data').use_like_video().include_reg('lr/xiuxian')\n    lr = d.compile()\n    ld = Loader(hr, lr, threads=4)\n    ld.image_augmentation()\n    ld.cropper(RandomCrop(2))\n    itr = ld.make_one_shot_iterator([4, 3, 3, 16, 16], 10, shuffle=True)\n    ret = list(itr)\n    self.assertEqual(len(ret), 10)\n    self.assert_psnr(ret)\n\n  def test_memory_limit(self):\n    d = Dataset('data/')\n    d = d.include('*.png')\n    data = d.compile()\n    ld = Loader(data, data, threads=4)\n    ld.cropper(RandomCrop(1))\n    ld.image_augmentation()\n    itr = ld.make_one_shot_iterator([4, 3, 16, 16], 10, True, data.capacity / 2)\n    ret = list(itr)\n    self.assertEqual(len(ret), 10)\n    self.assert_psnr(ret)\n    itr = ld.make_one_shot_iterator([4, 3, 16, 16], 10, True, data.capacity / 2)\n    ret = list(itr)\n    self.assertEqual(len(ret), 10)\n    self.assert_psnr(ret)\n\n  def test_no_shuffle(self):\n    d = Dataset('data/').include('*.png')\n    data = d.compile()\n    ld = Loader(data, data, threads=4)\n    ld.cropper(CenterCrop(1))\n    itr1 = ld.make_one_shot_iterator([1, 3, 16, 16], -1, False)\n    ret1 = list(itr1)\n    self.assertEqual(len(ret1), 16)\n    self.assert_psnr(ret1)\n    itr2 = ld.make_one_shot_iterator([1, 3, 16, 16], -1, False)\n    ret2 = list(itr2)\n    self.assertEqual(len(ret2), 16)\n    self.assert_psnr(ret2)\n    for x, y in zip(ret1, ret2):\n      self.assertTrue(np.all((x['hr'] - y['hr']) < 1e-4))\n\n  def test_no_shuffle_limit(self):\n    d = Dataset('data/')\n    d = d.include('*.png')\n    data = d.compile()\n    ld = Loader(data, data, threads=4)\n    ld.cropper(RandomCrop(1))\n    ld.image_augmentation()\n    itr = ld.make_one_shot_iterator([4, 3, 16, 16], 10, False,\n                                    data.capacity / 2)\n    ret = list(itr)\n    self.assertEqual(len(ret), 10)\n    self.assert_psnr(ret)\n    itr = ld.make_one_shot_iterator([4, 3, 16, 16], 10, False,\n                                    data.capacity / 2)\n    ret = list(itr)\n    self.assertEqual(len(ret), 10)\n    self.assert_psnr(ret)\n\n  def test_auto_deduce_shape(self):\n    d = Dataset('data').include_reg('set5')\n    ld = Loader(d, scale=1)\n    itr = ld.make_one_shot_iterator([1, -1, -1, -1], -1)\n    ret = list(itr)\n    self.assertEqual(len(ret), 5)\n    self.assert_psnr(ret)\n\n  def test_load_empty_data(self):\n    d = Dataset('not-found')\n    ld = Loader(d, scale=1)\n    itr = ld.make_one_shot_iterator([1, -1, -1, -1], -1)\n    self.assertEqual(len(list(itr)), 0)\n    itr = ld.make_one_shot_iterator([4, 3, 16, 16], 10)\n    ret = list(itr)\n    self.assertEqual(len(ret), 10)\n    self.assertFalse(ret[0]['hr'])\n    self.assertFalse(ret[0]['lr'])\n    self.assertFalse(ret[0]['name'])\n\n\nif __name__ == '__main__':\n  unittest.main()\n"""
Tests/model_test.py,0,"b'# ##############################################################################\n#  Copyright (c) 2020. LoSealL All Rights Reserved.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Date: 2020 - 2 - 5\n# ##############################################################################\nimport os\nimport unittest\n\nif not os.getcwd().endswith(\'Tests\'):\n  os.chdir(\'Tests\')\nfrom VSR.DataLoader import Dataset, Loader\nfrom VSR.Model import get_model\nfrom VSR.Backend import DATA_FORMAT\n\n\nclass ModelTest(unittest.TestCase):\n  def test_train_srcnn(self):\n    data = Dataset(\'data\').include_reg(\'set5\')\n    ld = Loader(data, scale=2)\n    ld.set_color_space(\'lr\', \'L\')\n    ld.set_color_space(\'hr\', \'L\')\n    m = get_model(\'srcnn\')(scale=2, channel=1)\n    with m.executor as t:\n      config = t.query_config({})\n      config.epochs = 5\n      config.steps = 10\n      if DATA_FORMAT == \'channels_first\':\n        config.batch_shape = [16, 1, 16, 16]\n      else:\n        config.batch_shape = [16, 16, 16, 1]\n      t.fit([ld, None], config)\n\n  def test_infer_srcnn(self):\n    m = get_model(\'srcnn\')(scale=2, channel=3)\n    data = Dataset(\'data\').include_reg(\'set5\')\n    ld = Loader(data, scale=2)\n    with m.executor as t:\n      config = t.query_config({})\n      t.infer(ld, config)\n\n  def test_train_vespcn(self):\n    data = Dataset(\'data/video\').include_reg(""xiuxian"").use_like_video()\n    ld = Loader(data, scale=2)\n    m = get_model(\'vespcn\')(scale=2, channel=3)\n    with m.executor as t:\n      config = t.query_config({})\n      config.epochs = 1\n      config.steps = 10\n      if DATA_FORMAT == \'channels_first\':\n        config.batch_shape = [16, 3, 3, 16, 16]\n      else:\n        config.batch_shape = [16, 3, 16, 16, 3]\n      t.fit([ld, None], config)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
Tests/motion_test.py,6,"b'""""""\nunit test for VSR.Framework.Motion package\n""""""\nimport os\nimport unittest\n\nif not os.getcwd().endswith(\'Tests\'):\n  os.chdir(\'Tests\')\nfrom VSR.Backend.TF.Framework import Motion as M\nfrom VSR.Backend.Torch.Models.video import motion as MT\nfrom VSR.DataLoader.FloDecoder import open_flo, KITTI\n\nimport tensorflow as tf\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nTEST_FLO_FILE = \'./data/flying_chair/flow/0000.flo\'\nTEST_PNG16_FILE = \'./data/kitti_car/f_01.png\'\n\n\nclass MotionTest(unittest.TestCase):\n  def _assert_same(self, x, y, epsilon=1e-6):\n    if isinstance(x, torch.Tensor):\n      d = (x - y).abs().mean()\n      self.assertLessEqual(d.cpu().numpy(), epsilon)\n    elif isinstance(x, tf.Tensor):\n      d = tf.reduce_mean(tf.abs(x - y))\n      self.assertLessEqual(d.eval(), epsilon)\n\n  def test_open_flo(self):\n    X = open_flo(TEST_FLO_FILE)\n    self.assertEqual(X.shape, (384, 512, 2))\n    self.assertLessEqual(np.abs(X[..., 0]).max(), 512)\n    self.assertLessEqual(np.abs(X[..., 1]).max(), 384)\n\n  def test_open_png16(self):\n    X = KITTI.open_png16(TEST_PNG16_FILE)\n    self.assertEqual(X.shape, (375, 1242, 3))\n    X = KITTI.open_flow(TEST_PNG16_FILE)\n    self.assertEqual(X.shape, (375, 1242, 2))\n    self.assertLessEqual(np.abs(X[..., 0]).max(), 1242)\n    self.assertLessEqual(np.abs(X[..., 1]).max(), 375)\n\n  def test_grid(self):\n    G = np.meshgrid(range(5), range(5))\n    G = np.stack(G, -1)\n    G_bar = M._grid(5, 5)[0]\n    with tf.Session() as sess:\n      G_bar = sess.run(G_bar)\n    self.assertTrue(np.all(G == G_bar.transpose([1, 0, 2])))\n\n  def test_sample(self):\n    G = np.meshgrid(range(5), range(5))\n    G = np.stack(G, -1)\n    G = np.expand_dims(G, 0)\n    G.transpose([0, 2, 1, 3])\n\n    X = np.random.rand(1, 5, 5, 3).astype(\'float32\')\n    X_bar = M._sample(X, G[..., 0], G[..., 1])\n\n    with tf.Session() as sess:\n      X_bar = sess.run(X_bar)\n    self.assertTrue(np.all(X == X_bar))\n\n  def test_warp_car(self):\n    flow = KITTI.open_flow(TEST_PNG16_FILE)\n    car = KITTI.open_png16(\'./data/kitti_car/c_11.png\')\n    flow = flow.reshape([1, *flow.shape])\n    car = car.reshape([1, *car.shape])\n    car_bar = M.warp(car, flow[..., 0], flow[..., 1], True)\n    with tf.Session() as sess:\n      car_bar = sess.run(car_bar)[0]\n      car_bar = car_bar.astype(\'uint8\')\n      # Image.fromarray(car_bar, \'RGB\').show()\n\n  def test_warp_chair(self):\n    flow = open_flo(TEST_FLO_FILE)\n    img1 = Image.open(\'./data/flying_chair/pair/0000/img1.png\')\n    ch0 = np.array(img1).astype(\'float32\')\n    flow = flow.reshape([1, *flow.shape])\n    ch0 = np.expand_dims(ch0, 0)\n    ch1 = M.warp(ch0, flow[..., 0], flow[..., 1], True)\n    with tf.Session() as sess:\n      ch1 = sess.run(ch1)[0]\n      ch1 = ch1.astype(\'uint8\')\n      # Image.fromarray(ch1, \'RGB\').show()\n\n  def test_sttn(self):\n    f0 = torch.ones(1, 1, 8, 8) * 4\n    f1 = torch.ones(1, 1, 8, 8) * 8\n    f2 = torch.stack([f1, f0], dim=2)  # NCTHW\n    tr = MT.STTN(padding_mode=\'border\')\n    d = torch.zeros(1, 8, 8)\n    u = torch.zeros(1, 8, 8)\n    v = torch.zeros(1, 8, 8)\n    f3 = tr(f2, d, u, v).squeeze(2)\n    self._assert_same(f3, f1)\n    d = torch.ones(1, 8, 8) * 2\n    f4 = tr(f2, d, u, v).squeeze(2)\n    self._assert_same(f4, f0)\n\n  def test_sttn_permute(self):\n    f0 = torch.ones(1, 1, 8, 8) * 4\n    f1 = torch.ones(1, 1, 8, 8) * 8\n    f2 = torch.stack([f1, f0], dim=1)  # NTCHW\n    tr = MT.STTN([0, 2, 1, 3, 4], padding_mode=\'border\')\n    d = torch.zeros(1, 8, 8)\n    u = torch.zeros(1, 8, 8)\n    v = torch.zeros(1, 8, 8)\n    f3 = tr(f2, d, u, v).squeeze(2)\n    self._assert_same(f3, f1)\n    d = torch.ones(1, 8, 8) * 2\n    f4 = tr(f2, d, u, v).squeeze(2)\n    self._assert_same(f4, f0)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
Tests/space_to_depth_test.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 16\n\nimport os\nimport unittest\n\nif not os.getcwd().endswith('Tests'):\n  os.chdir('Tests')\n\ntry:\n  import torch\n  import torchvision\n  from torch.nn import PixelShuffle\n  from VSR.Backend.Torch.Models.Arch import SpaceToDim\nexcept ImportError:\n  exit(0)\n\nimport numpy as np\nfrom PIL import Image\n\n\nclass SpaceToDimTest(unittest.TestCase):\n  def test_space_to_depth(self):\n    f1 = SpaceToDim(2, dim=1)\n    ff = PixelShuffle(2)\n    x = Image.open('data/set5_x2/img_001_SRF_2_LR.png')\n    g = torchvision.transforms.ToTensor()\n    h = torchvision.transforms.ToPILImage()\n    z = f1(g(x).unsqueeze(0))\n    y = h(ff(z)[0])\n    self.assertTrue(np.all(np.array(x) == np.array(y)))\n\n  def dummy_test_space_to_x(self):\n    f1 = SpaceToDim(2, (1, 2), dim=3)\n    x = torch.ones(1, 4, 4, 3)\n    y = f1(x)\n    self.assertEqual(y.shape, torch.Size([1, 2, 2, 12]))\n    f2 = SpaceToDim(2, (1, 2), dim=0)\n    y = f2(x)\n    self.assertEqual(y.shape, torch.Size([4, 2, 2, 3]))\n\n\nif __name__ == '__main__':\n  unittest.main()\n"""
Tests/training_test.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/17 \xe4\xb8\x8b\xe5\x8d\x885:53\n\nimport os\nimport subprocess\nfrom VSR.Model import list_supported_models\n\nif not os.getcwd().endswith(\'Tests\'):\n  os.chdir(\'Tests\')\n\n_WORKDIR = r""/tmp/vsr/utest/""\n_TCMD = r""python train.py {} --data_config=../Tests/data/fake_datasets.yml --dataset=normal --epochs=1 --steps=1 --save_dir={}""\n_ECMD = r""python eval.py {} --save_dir={} --ensemble -t=../Tests/data/set5_x2""\n\n\ndef train(model_name: str):\n  cmd = _TCMD.format(str(model_name), _WORKDIR)\n  cwd = \'../Train\'\n  subprocess.call(cmd, stderr=subprocess.DEVNULL, cwd=cwd, shell=True)\n\n\ndef eval(model_name: str):\n  cmd = _ECMD.format(str(model_name), _WORKDIR)\n  cwd = \'../Train\'\n  subprocess.call(cmd, stderr=subprocess.DEVNULL, cwd=cwd, shell=True)\n\n\ndef test_train_srcnn():\n  train(\'srcnn\')\n  eval(\'srcnn\')\n\n\ndef test_train_espcn():\n  train(\'espcn\')\n  eval(\'espcn\')\n\n\ndef test_other_models():\n  for k in list_supported_models():\n    if k in (\n        \'sofvsr\', \'vespcn\', \'frvsr\', \'qprn\', \'ufvsr\', \'yovsr\', \'tecogan\',\n        \'spmc\', \'rbpn\'\n    ):\n      continue\n    train(k)\n    eval(k)\n\n\nif __name__ == \'__main__\':\n  test_other_models()\n'"
Tests/utility_test.py,0,"b'import os\nimport unittest\n\nif not os.getcwd().endswith(\'Tests\'):\n  os.chdir(\'Tests\')\n\nfrom VSR.Util import str_to_bytes, Config\n\nTEST_STR = (\n  \'1.3\', \'2kb\', \'3 mb\', \'4GB\', \'9Zb\', \'2.3pB\'\n)\nANS = (\n  1.3, 2048.0, 3145728.0, 4294967296.0, 10625324586456701730816.0,\n  2589569785738035.2\n)\n\n\nclass UtilityTest(unittest.TestCase):\n  def test_str_to_bytes(self):\n    for t, a in zip(TEST_STR, ANS):\n      ans = str_to_bytes(t)\n      self.assertEqual(ans, a, f""{t} != {a}"")\n\n  def test_config(self):\n    d = Config(a=1, b=2)\n    self.assertTrue(hasattr(d, \'a\'))\n    self.assertTrue(hasattr(d, \'b\'))\n    self.assertTrue(hasattr(d, \'non-exist\'))\n    self.assertIs(d.a, 1)\n    self.assertIs(d.b, 2)\n    d.update(a=2, b=3)\n    self.assertIs(d.a, 2)\n    self.assertIs(d.b, 3)\n    d.a = 9\n    self.assertIs(d.a, 9)\n    d.update(Config(b=6, f=5))\n    self.assertIs(d.b, 6)\n    self.assertIs(d.f, 5)\n    d.pop(\'b\')\n    self.assertIsNone(d.b)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
Tests/vgg_test.py,2,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 16\n\nimport os\nimport unittest\n\nif not os.getcwd().endswith('Tests'):\n  os.chdir('Tests')\n\nimport numpy as np\nfrom PIL import Image\n\nfrom VSR.Backend.TF.Util import Vgg\n\nURL = 'data/set5_x2/img_001_SRF_2_LR.png'\nimage_boy = np.asarray(Image.open(URL))\n\n\nclass VggTest(unittest.TestCase):\n  def import_tf(self):\n    import tensorflow as tf\n    return tf\n\n  def test_vgg16(self):\n    vgg = Vgg(False, vgg=Vgg.VGG16)\n    x = np.random.normal(size=[16, 128, 128, 3])\n    y = vgg(x)\n    self.assertEqual(y.shape, (16,))\n\n  def test_vgg19(self):\n    vgg = Vgg(False, vgg=Vgg.VGG19)\n    x = np.random.normal(size=[16, 128, 128, 3])\n    y = vgg(x, 'block2_conv2')\n    self.assertEqual(y.shape, (16, 64, 64, 128))\n\n  def test_vgg_classify(self):\n    vgg16 = Vgg(True, vgg=Vgg.VGG16)\n    vgg19 = Vgg(True, vgg=Vgg.VGG19)\n    x = np.expand_dims(image_boy, 0)\n    y1 = vgg16(x)\n    y2 = vgg19(x)\n    tf = self.import_tf()\n    with tf.Session() as sess:\n      y1, y2 = sess.run([y1, y2])\n      self.assertEqual(y2[0].tolist().index(y2.max()),\n                       y1[0].tolist().index(y1.max()))\n\n  def test_multiple_call(self):\n    vgg1 = Vgg(False, vgg=Vgg.VGG16)\n    vgg2 = Vgg(False, vgg=Vgg.VGG16)\n    x = np.expand_dims(image_boy, 0)\n    y1 = vgg1(x)\n    y2 = vgg2(x)\n    y3 = vgg2(x.copy())\n    tf = self.import_tf()\n    with tf.Session() as sess:\n      y1, y2, y3 = sess.run([y1, y2, y3])\n      self.assertEqual(y1, y2)\n      self.assertEqual(y2, y3)\n\n\nif __name__ == '__main__':\n  unittest.main()\n"""
Tests/virtualfile_test.py,0,"b'""""""\nUnit test for DataLoader.VirtualFile\n""""""\nimport os\nimport unittest\n\nif not os.getcwd().endswith(\'Tests\'):\n  os.chdir(\'Tests\')\nfrom VSR.DataLoader.VirtualFile import *\nfrom VSR.Util.ImageProcess import img_to_array\n\nRAW = \'data/video/raw_32x32.yv12\'\nIMG = \'data/set5_x2/img_001_SRF_2_LR.png\'\nVID = \'data/video/custom_pair/lr/xiuxian\'\n\n\nclass VirtualFileTest(unittest.TestCase):\n  def test_image_read(self):\n    vf = ImageFile(IMG)\n    self.assertEqual(vf.name, \'img_001_SRF_2_LR\')\n    img = vf.read_frame()\n    self.assertIsInstance(img, list)\n    self.assertEqual(len(img), 1)\n    self.assertIsInstance(img[0], Image.Image)\n    self.assertEqual(img[0].width, 256)\n    self.assertEqual(img[0].height, 256)\n\n  def test_video_read(self):\n    vf = ImageFile(VID)\n    self.assertEqual(vf.name, \'xiuxian\')\n    vid = vf.read_frame(3)\n    self.assertIsInstance(vid, list)\n    self.assertEqual(len(vid), 3)\n    self.assertEqual(vid[0].width, 240)\n    self.assertEqual(vid[0].height, 135)\n\n  def test_raw_read(self):\n    vf = RawFile(RAW, \'YV12\', [32, 32])\n    self.assertEqual(vf.name, \'raw_32x32\')\n    raw = vf.read_frame(vf.frames)\n    self.assertEqual(len(raw), vf.frames)\n    self.assertEqual(raw[0].width, 32)\n    self.assertEqual(raw[0].height, 32)\n\n  def test_image_seek(self):\n    vf = ImageFile(IMG, False)\n    f1 = vf.read_frame(1)[0]\n    vf.seek(0, SEEK_SET)\n    f2 = vf.read_frame(1)[0]\n    vf.seek(-1, SEEK_CUR)\n    f3 = vf.read_frame(1)[0]\n    vf.seek(-1, SEEK_END)\n    f4 = vf.read_frame(1)[0]\n    vf.seek(-2, SEEK_END)\n    f5 = vf.read_frame(1)[0]\n\n    F = [f1, f2, f3, f4, f5]\n    F = [img_to_array(f) for f in F]\n    self.assertTrue(np.all(F[0] == F[1]))\n    self.assertTrue(np.all(F[1] == F[2]))\n    self.assertTrue(np.all(F[3] == F[4]))\n\n  def test_vid_seek(self):\n    vf = ImageFile(VID, False)\n    f1 = vf.read_frame(1)[0]\n    vf.seek(0, SEEK_SET)\n    f2 = vf.read_frame(1)[0]\n    vf.seek(-1, SEEK_CUR)\n    f3 = vf.read_frame(1)[0]\n    vf.seek(-1, SEEK_END)\n    f4 = vf.read_frame(1)[0]\n    vf.seek(2, SEEK_SET)\n    f5 = vf.read_frame(1)[0]\n\n    F = [f1, f2, f3, f4, f5]\n    F = [img_to_array(f) for f in F]\n    self.assertTrue(np.all(F[0] == F[1]))\n    self.assertTrue(np.all(F[1] == F[2]))\n    self.assertTrue(np.all(F[3] == F[4]))\n\n  def test_raw_seek(self):\n    vf = RawFile(RAW, \'YV12\', [32, 32])\n    f1 = vf.read_frame(1)[0]\n    vf.seek(0, SEEK_SET)\n    f2 = vf.read_frame(1)[0]\n    vf.seek(-1, SEEK_CUR)\n    f3 = vf.read_frame(1)[0]\n    vf.seek(-1, SEEK_END)\n    f4 = vf.read_frame(1)[0]\n    vf.seek(-2, SEEK_END)\n    vf.seek(1, SEEK_CUR)\n    f5 = vf.read_frame(1)[0]\n\n    F = [f1, f2, f3, f4, f5]\n    F = [img_to_array(f) for f in F]\n    self.assertTrue(np.all(F[0] == F[1]))\n    self.assertTrue(np.all(F[1] == F[2]))\n    self.assertTrue(np.all(F[3] == F[4]))\n\n  def test_vf_copy(self):\n    import copy\n    vf0 = ImageFile(IMG, False)\n    vf1 = copy.deepcopy(vf0)\n    vf0.read_frame(1)\n    try:\n      vf0.read_frame(1)\n      self.assertFalse(True, ""Unreachable code"")\n    except EOFError:\n      pass\n    vf1.read_frame(1)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
Tools/CelebA.py,0,"b'""""""\nCopyright: Wenyi Tang 2017-2019\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Jan 3rd, 2019\n\nPre-processing CelebA dataset:\n- Crop and resize to WxH\n- Randomly split into (train, test)\n""""""\n\n#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:03\n\nimport argparse\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""celeba"", help=""CelebA root folder."")\nparser.add_argument(""-W"", type=int, default=64, help=""width"")\nparser.add_argument(""-H"", type=int, default=64, help=""height"")\nparser.add_argument(""--n_test"", type=int, default=10000, help=""test samples"")\nargs = parser.parse_args()\n\n\ndef main():\n  root = Path(args.celeba)\n  if not root.exists():\n    raise FileNotFoundError(""Root of CelebA does not exist!"")\n\n  images = list(root.rglob(\'*.jpg\'))\n  resize_dir = root / \'resize{}\'.format(args.W)\n  test_dir = root / \'test{}\'.format(args.W)\n  resize_dir.mkdir(parents=True, exist_ok=False)\n  test_dir.mkdir(parents=True, exist_ok=False)\n\n  np.random.shuffle(images)\n  for img in images[:args.n_test]:\n    x = Image.open(img)\n    dw = (x.width - args.W) // 2\n    dh = (x.height - args.H) // 2\n    box = [dw, dh, x.width - dw, x.height - dh]\n    x.crop(box).save(str(test_dir) + \'/{}.png\'.format(img.stem))\n\n  for img in images[args.n_test:]:\n    x = Image.open(img)\n    dw = (x.width - args.W) // 2\n    dh = (x.height - args.H) // 2\n    box = [dw, dh, x.width - dw, x.height - dh]\n    x.crop(box).save(str(resize_dir) + \'/{}.png\'.format(img.stem))\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Tools/DND.py,0,"b'""""""\n# Author: Tobias Pl\xc3\xb6tz, TU Darmstadt (tobias.ploetz@visinf.tu-darmstadt.de)\n\n # This file is part of the implementation as described in the CVPR 2017 paper:\n # Tobias Pl\xc3\xb6tz and Stefan Roth, Benchmarking Denoising Algorithms with Real Photographs.\n # Please see the file LICENSE.txt for the license governing this code.\n""""""\n#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:03\n\n# Pre-processing DND dataset:\n# - Crop each image into 20 small patches (1000 in total)\n# - Convert .mat into png files\n# - Submit denoised .png files into bundled .mat files.\nimport argparse\nimport os\nfrom pathlib import Path\n\nimport h5py\nimport numpy as np\nimport scipy.io as sio\nimport tqdm\nfrom PIL import Image\n\n\ndef load_nlf(info, img_id):\n  nlf = {}\n  nlf_h5 = info[info[""nlf""][0][img_id]]\n  nlf[""a""] = nlf_h5[""a""][0][0]\n  nlf[""b""] = nlf_h5[""b""][0][0]\n  return nlf\n\n\ndef load_sigma_raw(info, img_id, bb, yy, xx):\n  nlf_h5 = info[info[""sigma_raw""][0][img_id]]\n  sigma = nlf_h5[xx, yy, bb]\n  return sigma\n\n\ndef load_sigma_srgb(info, img_id, bb):\n  nlf_h5 = info[info[""sigma_srgb""][0][img_id]]\n  sigma = nlf_h5[0, bb]\n  return sigma\n\n\ndef denoise_raw(denoiser, data_folder, out_folder):\n  \'\'\'\n  Utility function for denoising all bounding boxes in all raw images of\n  the DND dataset.\n\n  denoiser      Function handle\n                It is called as Idenoised = denoiser(Inoisy, nlf) where Inoisy is the noisy image patch\n                and nlf is a dictionary containing the parameters of the noise level\n                function (nlf[""a""], nlf[""b""]) and a mean noise strength (nlf[""sigma""])\n  data_folder   Folder where the DND dataset resides\n  out_folder    Folder where denoised output should be written to\n  \'\'\'\n  try:\n    os.makedirs(out_folder)\n  except:\n    pass\n\n  # load info\n  infos = h5py.File(os.path.join(data_folder, \'info.mat\'), \'r\')\n  info = infos[\'info\']\n  bb = info[\'boundingboxes\']\n  print(\'info loaded\\n\')\n  # process data\n  for i in range(50):\n    filename = os.path.join(data_folder, \'images_raw\', \'%04d.mat\' % (i + 1))\n    img = h5py.File(filename, \'r\')\n    Inoisy = np.float32(np.array(img[\'Inoisy\']).T)\n    # bounding box\n    ref = bb[0][i]\n    boxes = np.array(info[ref]).T\n    for k in range(20):\n      idx = [int(boxes[k, 0] - 1), int(boxes[k, 2]), int(boxes[k, 1] - 1),\n             int(boxes[k, 3])]\n      Inoisy_crop = Inoisy[idx[0]:idx[1], idx[2]:idx[3]].copy()\n      Idenoised_crop = Inoisy_crop.copy()\n      H = Inoisy_crop.shape[0]\n      W = Inoisy_crop.shape[1]\n      nlf = load_nlf(info, i)\n      for yy in range(2):\n        for xx in range(2):\n          nlf[""sigma""] = load_sigma_raw(info, i, k, yy, xx)\n          Inoisy_crop_c = Inoisy_crop[yy:H:2, xx:W:2].copy()\n          Idenoised_crop_c = denoiser(Inoisy_crop_c, nlf)\n          Idenoised_crop[yy:H:2, xx:W:2] = Idenoised_crop_c\n      # save denoised data\n      Idenoised_crop = np.float32(Idenoised_crop)\n      save_file = os.path.join(out_folder,\n                               \'%04d_%02d.mat\' % (i + 1, k + 1))\n      sio.savemat(save_file, {\'Idenoised_crop\': Idenoised_crop})\n      print(\'%s crop %d/%d\' % (filename, k + 1, 20))\n    print(\'[%d/%d] %s done\\n\' % (i + 1, 50, filename))\n\n\ndef denoise_srgb(denoiser, data_folder, out_folder):\n  \'\'\'\n  Utility function for denoising all bounding boxes in all sRGB images of\n  the DND dataset.\n\n  denoiser      Function handle\n                It is called as Idenoised = denoiser(Inoisy, nlf) where Inoisy is the noisy image patch\n                and nlf is a dictionary containing the  mean noise strength (nlf[""sigma""])\n  data_folder   Folder where the DND dataset resides\n  out_folder    Folder where denoised output should be written to\n  \'\'\'\n  try:\n    os.makedirs(out_folder)\n  except:\n    pass\n\n  print(\'model loaded\\n\')\n  # load info\n  infos = h5py.File(os.path.join(data_folder, \'info.mat\'), \'r\')\n  info = infos[\'info\']\n  bb = info[\'boundingboxes\']\n  print(\'info loaded\\n\')\n  # process data\n  for i in range(50):\n    filename = os.path.join(data_folder, \'images_srgb\',\n                            \'%04d.mat\' % (i + 1))\n    img = h5py.File(filename, \'r\')\n    Inoisy = np.float32(np.array(img[\'InoisySRGB\']).T)\n    # bounding box\n    ref = bb[0][i]\n    boxes = np.array(info[ref]).T\n    for k in range(20):\n      idx = [int(boxes[k, 0] - 1), int(boxes[k, 2]), int(boxes[k, 1] - 1),\n             int(boxes[k, 3])]\n      Inoisy_crop = Inoisy[idx[0]:idx[1], idx[2]:idx[3], :].copy()\n      H = Inoisy_crop.shape[0]\n      W = Inoisy_crop.shape[1]\n      nlf = load_nlf(info, i)\n      for yy in range(2):\n        for xx in range(2):\n          nlf[""sigma""] = load_sigma_srgb(info, i, k)\n          Idenoised_crop = denoiser(Inoisy_crop, nlf, index=(i, k))\n      # save denoised data\n      Idenoised_crop = np.float32(Idenoised_crop)\n      save_file = os.path.join(out_folder,\n                               \'%04d_%02d.mat\' % (i + 1, k + 1))\n      sio.savemat(save_file, {\'Idenoised_crop\': Idenoised_crop})\n      print(\'%s crop %d/%d\' % (filename, k + 1, 20))\n    print(\'[%d/%d] %s done\\n\' % (i + 1, 50, filename))\n\n\ndef bundle_submissions_raw(submission_folder):\n  \'\'\'\n  Bundles submission data for raw denoising\n\n  submission_folder Folder where denoised images reside\n\n  Output is written to <submission_folder>/bundled/. Please submit\n  the content of this folder.\n  \'\'\'\n\n  out_folder = os.path.join(submission_folder, ""bundled/"")\n  try:\n    os.mkdir(out_folder)\n  except:\n    pass\n\n  israw = True\n  eval_version = ""1.0""\n\n  for i in range(50):\n    Idenoised = np.zeros((20,), dtype=np.object)\n    for bb in range(20):\n      filename = \'%04d_%02d.mat\' % (i + 1, bb + 1)\n      s = sio.loadmat(os.path.join(submission_folder, filename))\n      Idenoised_crop = s[""Idenoised_crop""]\n      Idenoised[bb] = Idenoised_crop\n    filename = \'%04d.mat\' % (i + 1)\n    sio.savemat(os.path.join(out_folder, filename),\n                {""Idenoised"": Idenoised,\n                 ""israw"": israw,\n                 ""eval_version"": eval_version},\n                )\n\n\ndef bundle_submissions_srgb(submission_folder):\n  \'\'\'\n  Bundles submission data for sRGB denoising\n\n  submission_folder Folder where denoised images reside\n\n  Output is written to <submission_folder>/bundled/. Please submit\n  the content of this folder.\n  \'\'\'\n  out_folder = os.path.join(submission_folder, ""bundled/"")\n  try:\n    os.mkdir(out_folder)\n  except:\n    pass\n  israw = False\n  eval_version = ""1.0""\n\n  for i in range(50):\n    Idenoised = np.zeros((20,), dtype=np.object)\n    for bb in range(20):\n      filename = \'%04d_%02d.mat\' % (i + 1, bb + 1)\n      s = sio.loadmat(os.path.join(submission_folder, filename))\n      Idenoised_crop = s[""Idenoised_crop""]\n      Idenoised[bb] = Idenoised_crop\n    filename = \'%04d.mat\' % (i + 1)\n    sio.savemat(os.path.join(out_folder, filename),\n                {""Idenoised"": Idenoised,\n                 ""israw"": israw,\n                 ""eval_version"": eval_version},\n                )\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(description=""DND submission tool"")\n  parser.add_argument(""--data_dir"", type=str, default=None,\n                      help=""dnd data folder (root folder)."")\n  parser.add_argument(""--save_dir"", type=str, default=\'./outputs\',\n                      help=""output cropped patches."")\n  parser.add_argument(""--submission_dir"", type=str, default=None,\n                      help=""denoised images (.png)."")\n  parser.add_argument(""--bundle_dir"", type=str, default=\'./bundled\')\n  args = parser.parse_args()\n\n\n  def denoiser(image, nlf, index):\n    i, k = index\n    name = \'%04d_%02d_hr.png\' % (i + 1, k + 1)\n    image_u = np.round(image * 255).astype(np.uint8)\n    image_u = Image.fromarray(image_u, \'RGB\')\n    image_u.save(os.path.join(args.save_dir, name))\n    name = \'%04d_%02d_lr.png\' % (i + 1, k + 1)\n    image_u.resize([image_u.width // 4, image_u.height // 4],\n                   Image.BICUBIC).save(os.path.join(args.save_dir, name))\n    return image\n\n\n  if args.data_dir:\n    denoise_srgb(denoiser, args.data_dir, args.save_dir)\n  if args.submission_dir:\n    bundled = Path(args.bundle_dir)\n    bundled.mkdir(exist_ok=True, parents=True)\n    for img_file in tqdm.tqdm(Path(args.submission_dir).glob(\'*.png\')):\n      img_u = Image.open(img_file)\n      img = np.asarray(img_u, np.float32) / 255\n      path = bundled / (img_file.stem[:7] + \'.mat\')\n      sio.savemat(str(path), {\'Idenoised_crop\': img})\n    bundle_submissions_srgb(str(bundled))\n'"
Tools/FFmpegHelper.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:03\n\nimport argparse\nimport subprocess\nimport uuid\nfrom pathlib import Path\n\nimport tqdm\n\nparser = argparse.ArgumentParser(\n  description=""Helper tool to use ffmpeg for transcoding."")\nparser.add_argument(""input_dir"", help=""root folder of the raw videos."")\nparser.add_argument(""output_dir"", help=""root folder of the targets."")\nparser.add_argument(""--gop"", type=int, default=1,\n                    help=""[Enc] group of pictures (1)"")\nparser.add_argument(""--bf"", type=int, default=0, help=""[Enc] # B frames (0)."")\nparser.add_argument(""--codec"", default=""libx264"",\n                    help=""[Enc] encoder codec (libx264)."")\nparser.add_argument(""--qp"", type=int, default=0,\n                    help=""[Enc] quality index. [0, 51]"")\nFLAGS = parser.parse_args()\n\n\ndef _check_ffmpeg():\n  import shutil\n  if shutil.which(\'ffmpeg\') is None:\n    raise FileNotFoundError(""Couldn\'t find ffmpeg!"")\n\n\ndef parse_inputfile(path):\n  filename = Path(path).stem\n  suffix = Path(path).suffix\n  size = filename.split(\'_\')[-1]\n  _size = [int(i) for i in size.split(\'x\')]\n  assert len(_size) == 2\n  return size, suffix[1:].lower()\n\n\ndef encode(file, work_dir):\n  cmd = \'ffmpeg -f rawvideo\'\n  size, fmt = parse_inputfile(file)\n  tmp_name = work_dir / f\'{str(uuid.uuid4())}.264\'\n  cmd += f\' -pix_fmt {fmt}\'\n  cmd += f\' -s:v {size}\'\n  cmd += f\' -i {str(file)}\'\n  cmd += f\' -vcodec {FLAGS.codec}\'\n  cmd += f\' -g {FLAGS.gop}\'\n  cmd += f\' -bf {FLAGS.bf}\'\n  cmd += f\' -qp {FLAGS.qp}\'\n  cmd += f\' -f rawvideo {str(tmp_name)}\'\n  # print(cmd)\n  subprocess.call(cmd.split(\' \'), stderr=subprocess.DEVNULL)\n  return tmp_name, fmt\n\n\ndef decode(file, output_dir, name, fmt):\n  cmd = f\'ffmpeg -i {str(file)}\'\n  output_name = output_dir / f\'{name}_{FLAGS.qp}.{fmt}\'\n  cmd += f\' -f rawvideo -pix_fmt {fmt}\'\n  cmd += f\' {str(output_name)} -y\'\n  # print(cmd)\n  subprocess.call(cmd.split(\' \'), stderr=subprocess.DEVNULL)\n\n\ndef main():\n  _check_ffmpeg()\n  raw_videos = filter(lambda f: f.is_file(), Path(FLAGS.input_dir).rglob(\'*\'))\n  raw_videos = list(raw_videos)\n  if not raw_videos:\n    raw_videos = filter(lambda f: f.is_file(), [Path(FLAGS.input_dir)])\n  tmp_dir = Path(\'/tmp/vsr/tools/_ffmpeg\')\n  tmp_dir.mkdir(exist_ok=True, parents=True)\n  save_dir = Path(FLAGS.output_dir)\n  save_dir.mkdir(exist_ok=True, parents=True)\n  with tqdm.tqdm(sorted(raw_videos), ascii=True, unit=\' video\') as r:\n    for fp in r:\n      r.set_postfix({\'name\': fp.name})\n      stream, fmt = encode(fp, tmp_dir)\n      decode(stream, save_dir, fp.stem, fmt)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Tools/FastMetrics.py,0,"b'#   Copyright (c): Wenyi Tang 2017-2019.\n#   Author: Wenyi Tang\n#   Email: wenyi.tang@intel.com\n#   Update Date: 6/6/19, 10:35 AM\n\nimport argparse\nimport multiprocessing as mp\nfrom pathlib import Path\n\nimport numpy as np\nimport tqdm\nfrom PIL import Image\nfrom skimage.measure import compare_ssim\n\nfrom VSR.Util.ImageProcess import rgb_to_yuv\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""input_dir"", help=""target folder"")\nparser.add_argument(""reference_dir"", help=""reference folder"")\nparser.add_argument(""--ssim"", action=\'store_true\')\nparser.add_argument(""--l_only"", action=\'store_true\', help=""luminance only"")\nparser.add_argument(""--shave"", type=int, default=0, help=""shave border pixels"")\nparser.add_argument(""--multithread"", action=\'store_true\')\nFLAGS = parser.parse_args()\n\n\ndef gen():\n  d1 = Path(FLAGS.input_dir)\n  d2 = Path(FLAGS.reference_dir)\n\n  assert d1.exists() and d2.exists(), ""Path not found!""\n  assert len(list(d1.iterdir())) == len(list(d2.iterdir())), f""{d1} v {d2}""\n\n  for x, y in zip(sorted(d1.iterdir()), sorted(d2.iterdir())):\n    if x.is_dir() and y.is_dir():\n      assert len(list(x.iterdir())) == len(list(y.iterdir())), f""{x} v {y}""\n      for i, j in zip(sorted(x.iterdir()), sorted(y.iterdir())):\n        if i.is_file() and j.is_file():\n          yield i, j\n        else:\n          print(f"" [!] Found {i} v.s. {j} not file."")\n    elif x.is_file() and y.is_file():\n      yield x, y\n    else:\n      print(f"" [!] Found {x} v.s. {y} mismatch."")\n\n\ndef main():\n  mmse = 0\n  apsnr = 0\n  assim = 0\n  count = 0\n\n  def action(x, y):\n    xname = f\'{x.parent.name}/{x.stem}\'\n    yname = f\'{y.parent.name}/{y.stem}\'\n    x = Image.open(x)\n    y = Image.open(y)\n    assert x.width == y.width and x.height == y.height, ""Image size mismatch!""\n    xx = np.asarray(x, dtype=np.float) / 255.0\n    yy = np.asarray(y, dtype=np.float) / 255.0\n    if FLAGS.l_only:\n      xx = rgb_to_yuv(xx, standard=\'matlab\')[..., :1]\n      yy = rgb_to_yuv(yy, standard=\'matlab\')[..., :1]\n    if FLAGS.shave:\n      xx = xx[..., FLAGS.shave:-FLAGS.shave, FLAGS.shave:-FLAGS.shave, :]\n      yy = yy[..., FLAGS.shave:-FLAGS.shave, FLAGS.shave:-FLAGS.shave, :]\n    mse = np.mean((xx - yy) ** 2)\n    psnr = np.log10(1.0 / mse) * 10.0\n    info = {""x"": xname, ""y"": yname}\n    if FLAGS.ssim:\n      ssim = compare_ssim(xx, yy, multichannel=True)\n      info.update(SSIM=ssim)\n    info.update(PSNR=psnr)\n    info.update(MSE=mse)\n    return info\n\n  if FLAGS.multithread:\n    pool = mp.pool.ThreadPool()\n    results = [pool.apply_async(action, (i, j)) for i, j in gen()]\n    with tqdm.tqdm(results) as r:\n      for info in r:\n        info = info.get()\n        if FLAGS.ssim:\n          assim += info[\'SSIM\']\n        mmse += info[\'MSE\']\n        apsnr += info[\'PSNR\']\n        count += 1\n        r.set_postfix(info)\n  else:\n    with tqdm.tqdm(gen()) as r:\n      for x, y in r:\n        info = action(x, y)\n        if FLAGS.ssim:\n          assim += info[\'SSIM\']\n        mmse += info[\'MSE\']\n        apsnr += info[\'PSNR\']\n        count += 1\n        r.set_postfix(info)\n  mmse /= count\n  apsnr /= count\n  assim /= count\n  mpsnr = np.log10(1.0 / mmse) * 10.0\n  print(f""[*] Mean PSNR(MMSE): {mpsnr:.2f}"")\n  print(f""[*] Avg PSNR: {apsnr:.2f}"")\n  print(f""[*] Avg SSIM: {assim:.4f}"")\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Tools/Image2Raw.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/4 \xe4\xb8\x8b\xe5\x8d\x882:42\n\nimport argparse\nfrom pathlib import Path\n\nimport numpy as np\nimport tqdm\nfrom PIL import Image\n\nfrom Tools.Misc import YUVConverter\n\n_YUV_COLOR = (""NV12"", ""NV21"", ""YV12"", ""YV21"")\n_RGB_COLOR = (""RGBA"",)\n\nparser = argparse.ArgumentParser(\n  usage=r\'\'\'python Image2Raw.py input_dir output_dir [--options]\'\'\',\n  description=r\'\'\'Convert a folder of images to raw video format (FOURCC).\'\'\')\nparser.add_argument(""input_dir"", help=""root of the input folder, ""\n                                      ""the leaf will be the last child-folder ""\n                                      ""containing individual image files."")\nparser.add_argument(""output_dir"", help=""root of the output path."")\nparser.add_argument(""--color_fmt"", choices=_YUV_COLOR + _RGB_COLOR,\n                    default=\'NV12\',\n                    help=""output color format"")\nFLAGS = parser.parse_args()\n\n\ndef parse_video_clips(path):\n  _path = Path(path)\n  if not _path.exists():\n    raise FileNotFoundError(f""{path} doesn\'t exist!!"")\n\n  files = _path.rglob(\'*\')\n  parents = set(f.parent for f in filter(lambda f: f.is_file(), files))\n  return parents\n\n\ndef read_video_frames(path):\n  _path = Path(path)\n  files = sorted(_path.glob(\'*\'))\n  images = [Image.open(f) for f in files]\n  if FLAGS.color_fmt in _YUV_COLOR:\n    images = [m.convert(\'YCbCr\') for m in images]\n  mat = np.stack(images).transpose([0, 3, 1, 2])  # [NCHW]\n  return {\n    \'data\': mat,\n    \'length\': mat.shape[0],\n    \'name\': path.stem,\n    \'width\': mat.shape[3],\n    \'height\': mat.shape[2],\n  }\n\n\ndef main():\n  videos = parse_video_clips(FLAGS.input_dir)\n  root = Path(FLAGS.output_dir)\n  root.mkdir(exist_ok=True, parents=True)\n  print(f"" [*] Total videos found: {len(videos)}."")\n  with tqdm.tqdm(videos, ascii=True, unit=\' video\') as r:\n    for fp in r:\n      data = read_video_frames(fp)\n      r.set_postfix({""name"": data[\'name\']})\n      if FLAGS.color_fmt in _YUV_COLOR:\n        cvt = YUVConverter(data[\'data\'])\n        bytes = cvt.to(FLAGS.color_fmt).getbuffer().tobytes()\n      else:\n        raise NotImplementedError\n      nm = f""{data[\'name\']}_{cvt.width}x{cvt.height}.{FLAGS.color_fmt}""\n      save_path = root / nm\n      with save_path.open(\'wb\') as fd:\n        fd.write(bytes)\n\n\nif __name__ == \'__main__\':\n  main()\n  exit(0)\n'"
Tools/MakeHDF.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:03\n\nimport argparse\nimport time\nfrom pathlib import Path\n\nimport h5py\nimport numpy as np\nimport tqdm\nfrom PIL import Image\n\n__all__ = [""gather_videos_vqp"", ""gather_videos"", ""print_dataset""]\n\nparser = argparse.ArgumentParser(description=""Make HDF5 datasets"")\nparser.add_argument(""input_dir"", help=""path of the input root folder."")\nparser.add_argument(""-o"", ""--output"", help=""output hdf file path."")\nparser.add_argument(""-a"", ""--append"", action=\'store_true\')\nparser.add_argument(""-t"", ""--task_name"", choices=__all__, help=""task name"")\nparser.add_argument(""--compression"", type=int, default=None)\nparser.add_argument(""--glob"", help=""glob pattern to gather files inside input.""\n                                   ""For recursively glob, use **/*."")\nparser.add_argument(""--data_format"",\n                    choices=(\'channels_first\', \'channels_last\'),\n                    default=\'channels_first\', help=""data format (default: CHW)"")\nFLAGS, args = parser.parse_known_args()\n\n\ndef make_hdf_header():\n  if FLAGS.output:\n    if FLAGS.append:\n      fd = h5py.File(FLAGS.output, \'a\')\n    else:\n      fd = h5py.File(FLAGS.output, \'w\')\n    fd.attrs[\'author\'] = \'LoSealL\'\n    fd.attrs[\'email\'] = \'wenyi.tang@intel.com\'\n    fd.attrs[\'date\'] = time.strftime(""%Y-%m-%d"")\n    fd.attrs[\'data_format\'] = FLAGS.data_format\n\n    return fd\n\n\ndef flush_hdf(fd: h5py.File):\n  if isinstance(fd, h5py.File):\n    fd.close()\n\n\ndef gather_videos_vqp(fd: h5py.File):\n  """"""Specified for VQP""""""\n  root = Path(FLAGS.input_dir)\n  glob = FLAGS.glob or \'*\'\n  inputs = sorted(root.glob(glob))\n  candidates = set(i.parent for i in filter(lambda f: f.is_file(), inputs))\n  frames_info = {}\n  for p in tqdm.tqdm(candidates):\n    seq = [Image.open(f) for f in\n           filter(lambda f: f.is_file(), sorted(p.rglob(\'*\')))]\n    cube = np.stack(seq)\n    if FLAGS.data_format == \'channels_first\':\n      cube = cube.transpose([0, 3, 1, 2])\n    cube = np.expand_dims(cube, 0)\n    path = p.relative_to(root)\n    # ugly\n    path = path.parent / path.stem.split(\'_\')[0]\n    key = str(path.as_posix())\n    if not key in fd:\n      fd.create_dataset(key, data=cube,\n                        maxshape=(52,) + cube.shape[1:],\n                        compression=FLAGS.compression)\n      frames_info[key] = len(seq)\n    else:\n      d = fd[key]\n      cnt = d.shape[0] + 1\n      d.resize(cnt, 0)\n      d[-1] = cube\n    del cube\n\n\ndef gather_videos(fd: h5py.File):\n  """"""Gather videos. Video is defined in a folder containing sequential images.""""""\n  root = Path(FLAGS.input_dir)\n  glob = FLAGS.glob or \'*\'\n  inputs = sorted(root.glob(glob))\n  candidates = set(i.parent for i in filter(lambda f: f.is_file(), inputs))\n  frames_info = {}\n  for p in tqdm.tqdm(candidates):\n    seq = [Image.open(f) for f in\n           filter(lambda f: f.is_file(), sorted(p.rglob(\'*\')))]\n    cube = np.stack(seq)\n    if FLAGS.data_format == \'channels_first\':\n      cube = cube.transpose([0, 3, 1, 2])\n    path = p.relative_to(root)\n    key = str(path.as_posix())\n    fd.create_dataset(key, data=cube, compression=FLAGS.compression)\n    frames_info[key] = len(seq)\n    del cube\n  fd.attrs[\'frames_info\'] = list(frames_info.items())\n\n\ndef print_dataset(*args):\n  def _print(name, obj):\n    print(f""key: [{name}], shape: {obj.shape}"")\n\n  fd = Path(FLAGS.input_dir)\n  if fd.exists():\n    with h5py.File(str(fd), \'r\') as fd:\n      fd.visititems(_print)\n\n\ndef main():\n  fd = make_hdf_header()\n  globals()[FLAGS.task_name](fd)\n  flush_hdf(fd)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Tools/Misc.py,3,"b'""""""\nCopyright: Wenyi Tang 2017-2019\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Jan 7th, 2019\n\nMisc utility tools\n- make TFRecords files\n""""""\n\n#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:03\n\nimport io\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef make_tensor_label_records(tensors, labels, writer):\n  assert isinstance(tensors, (list, tuple))\n  assert isinstance(labels, (list, tuple))\n  assert len(tensors) == len(labels)\n\n  example = tf.train.Example(features=tf.train.Features())\n  for _t, _l in zip(tensors, labels):\n    assert isinstance(_t, bytes)\n    assert isinstance(_l, str)\n\n    bl = tf.train.BytesList(value=[_t])\n    ff = example.features.feature.get_or_create(_l)\n    ff.MergeFrom(tf.train.Feature(bytes_list=bl))\n  writer.write(example.SerializeToString())\n\n\nclass YUVConverter:\n  def __init__(self, frame):\n    self.data = frame\n    self.length = frame.shape[0]\n    self.height = frame.shape[2]\n    self.width = frame.shape[3]\n\n  def to_nv12(self):\n    # YUV -> NV12\n    h_tail = self.height % 2\n    w_tail = self.width % 2\n    y = np.pad(self.data[:, 0], [[0, 0], [0, h_tail], [0, w_tail]],\n               mode=\'reflect\')\n    u = self.data[:, 1, ::2, ::2]\n    v = self.data[:, 2, ::2, ::2]\n    buffer = io.BytesIO()\n    for i in range(self.length):\n      plain = y[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = np.stack([u[i], v[i]], -1).flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n    if buffer.tell() != np.prod(self.data.shape) // 2:\n      print("" [$] warning: even frame size, crop 1 pixel out"")\n      assert buffer.tell() == np.prod(y.shape) + np.prod(u.shape) + np.prod(\n        v.shape)\n      self.width = y.shape[2]\n      self.height = y.shape[1]\n    return buffer\n\n  def to_nv21(self):\n    # YUV -> NV21\n    h_tail = self.height % 2\n    w_tail = self.width % 2\n    y = np.pad(self.data[:, 0], [[0, 0], [0, h_tail], [0, w_tail]],\n               mode=\'reflect\')\n    u = self.data[:, 2, ::2, ::2]\n    v = self.data[:, 1, ::2, ::2]\n    buffer = io.BytesIO()\n    for i in range(self.length):\n      plain = y[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = np.stack([u[i], v[i]], -1).flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n    if buffer.tell() != np.prod(self.data.shape) // 2:\n      print("" [$] warning: even frame size, crop 1 pixel out"")\n      assert buffer.tell() == np.prod(y.shape) + np.prod(u.shape) + np.prod(\n        v.shape)\n      self.width = y.shape[2]\n      self.height = y.shape[1]\n    return buffer\n\n  def to_yv12(self):\n    h_tail = self.height % 2\n    w_tail = self.width % 2\n    y = np.pad(self.data[:, 0], [[0, 0], [0, h_tail], [0, w_tail]],\n               mode=\'reflect\')\n    u = self.data[:, 1, ::2, ::2]\n    v = self.data[:, 2, ::2, ::2]\n    buffer = io.BytesIO()\n    for i in range(self.length):\n      plain = y[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = u[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = v[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n    if buffer.tell() != np.prod(self.data.shape) // 2:\n      print("" [$] warning: even frame size, crop 1 pixel out"")\n      assert buffer.tell() == np.prod(y.shape) + np.prod(u.shape) + np.prod(\n        v.shape)\n      self.width = y.shape[2]\n      self.height = y.shape[1]\n    return buffer\n\n  def to_yv21(self):\n    h_tail = self.height % 2\n    w_tail = self.width % 2\n    y = np.pad(self.data[:, 0], [[0, 0], [0, h_tail], [0, w_tail]],\n               mode=\'reflect\')\n    u = self.data[:, 1, ::2, ::2]\n    v = self.data[:, 2, ::2, ::2]\n    buffer = io.BytesIO()\n    for i in range(self.length):\n      plain = y[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = v[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = u[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n    if buffer.tell() != np.prod(self.data.shape) // 2:\n      print("" [$] warning: even frame size, crop 1 pixel out"")\n      assert buffer.tell() == np.prod(y.shape) + np.prod(u.shape) + np.prod(\n        v.shape)\n      self.width = y.shape[2]\n      self.height = y.shape[1]\n    return buffer\n\n  def to_yv12(self):\n    h_tail = -1 if self.height % 2 else None\n    w_tail = -1 if self.width % 2 else None\n    y = self.data[:, 0, :h_tail, :w_tail]\n    u = self.data[:, 1, ::2, ::2]\n    v = self.data[:, 2, ::2, ::2]\n    buffer = io.BytesIO()\n    for i in range(self.length):\n      plain = y[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = u[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = v[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n    if buffer.tell() != np.prod(self.data.shape) // 2:\n      print("" [$] warning: even frame size, crop 1 pixel out"")\n      assert buffer.tell() == np.prod(y.shape) + np.prod(u.shape) + np.prod(\n        v.shape)\n    return buffer\n\n  def to_yv21(self):\n    h_tail = -1 if self.height % 2 else None\n    w_tail = -1 if self.width % 2 else None\n    y = self.data[:, 0, :h_tail, :w_tail]\n    u = self.data[:, 1, ::2, ::2]\n    v = self.data[:, 2, ::2, ::2]\n    buffer = io.BytesIO()\n    for i in range(self.length):\n      plain = y[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = v[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n      plain = u[i].flatten().astype(\'uint8\').tobytes()\n      buffer.write(plain)\n    if buffer.tell() != np.prod(self.data.shape) // 2:\n      print("" [$] warning: even frame size, crop 1 pixel out"")\n      assert buffer.tell() == np.prod(y.shape) + np.prod(u.shape) + np.prod(\n        v.shape)\n    return buffer\n\n  def to(self, fmt):\n    func_name = \'to_\' + fmt.lower()\n    if hasattr(self, func_name):\n      return getattr(self, func_name)()\n    raise NotImplementedError(f""Unsupported color format: {fmt}!"")\n'"
Tools/NtireHelper.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:03\n\nr""""""NTIRE Data helpers for NTIRE 2019\nContaining tasks for:\n- [Real Image Super-Resolution](https://competitions.codalab.org/competitions/21439)\n- [Real Image Denoising (sRGB)](https://competitions.codalab.org/competitions/21266)\n- [Image Dehazing](https://competitions.codalab.org/competitions/21163)\n\nFor reports of my related works, please refer to [README_NTIRE19.md](../Docs/README_NTIRE19.md)\n""""""\n\nimport argparse\nfrom pathlib import Path\n\nimport numpy as np\nimport tqdm\nfrom PIL import Image\nfrom scipy.io import loadmat, savemat\n\nfrom VSR.Util.ImageProcess import array_to_img, img_to_array\n\n__all__ = [""parse_mat"", ""group_mat"", ""divide"", ""combine""]\n\n\ndef main():\n  parser = argparse.ArgumentParser(\n      description=r""""""NTIRE Data helpers for NTIRE 2019\n    Containing tasks for:\n    - [Real Image Super-Resolution](https://competitions.codalab.org/competitions/21439)\n    - [Real Image Denoising (sRGB)](https://competitions.codalab.org/competitions/21266)\n    - [Image Dehazing](https://competitions.codalab.org/competitions/21163)\n    \n    For reports of my related works, please refer to [README_NTIRE19.md](../Docs/README_NTIRE19.md)\n    """""")\n  parser.add_argument(""input_dir"")\n  parser.add_argument(""save_dir"")\n  parser.add_argument(""--task"", choices=__all__, default=None)\n  group0 = parser.add_argument_group(""divide"")\n  group0.add_argument(""--patch"", type=int, default=0,\n                      help=""Patch size for dividing/combining sub-images"")\n  group0.add_argument(""--stride"", type=int, default=0,\n                      help=""Stride for dividing/combining sub-images"")\n  group1 = parser.add_argument_group(""denoise"")\n  group1.add_argument(""--metadata"", help=""Path to denoising metadata directory"")\n  group2 = parser.add_argument_group(""combine"")\n  group2.add_argument(""--ref"", help=""Path to referenced original images"")\n  flags = parser.parse_args()\n\n  if flags.task:\n    functor = globals()[flags.task]\n  else:\n    # guess\n    inputs = Path(flags.input_dir)\n    if inputs.suffix == \'.mat\':\n      functor = parse_mat\n    elif flags.patch > 0 and flags.stride > 0:\n      functor = divide\n    elif flags.stride > 0:\n      functor = combine\n    else:\n      raise RuntimeError(""Should specify a running task!"")\n  return functor(flags)\n\n\ndef parse_mat(flags):\n  """"""Parse MAT and its corresponding metadata file. Extract and save into png\n    files, named using meta-data elements\n\n  Challenge: Real Image Denoising\n\n  Args:\n    input_dir: path of the .MAT file\n    metadata: folder that containing .mat format meta-data\n  """"""\n\n  save_dir = Path(flags.save_dir)\n  save_dir.mkdir(exist_ok=True, parents=True)\n  mat_dir = Path(flags.input_dir)\n  if not mat_dir.exists():\n    raise ValueError(f""Can\'t find {mat_dir.name}!"")\n  metadata = flags.metadata\n  if metadata:\n    metadata = sorted(Path(flags.metadata).rglob(\'*.MAT\'))\n    metadata = [loadmat(str(m))[\'metadata\'] for m in metadata]\n    metadata = [m[0, 0][0][0] for m in metadata]\n    metadata = [Path(m).parent.parent.stem for m in metadata]\n    # wrong name in original mat file\n    metadata[33] = ""0158_007_GP_03200_03200_5500_N""\n    metadata = np.asarray([m.split(\'_\') for m in metadata])\n    assert metadata.shape[1] == 7, ""Probably a wrong metadata folder.""\n  mat = loadmat(str(mat_dir))\n  key = list(mat.keys())[-1]\n  print(f""find key: [{key}]"")\n  val_mat = mat[key]\n  assert val_mat.shape == (40, 32, 256, 256, 3), ""Probably a wrong mat file.""\n  assert val_mat.dtype == \'uint8\'\n  g = enumerate(val_mat.reshape([-1, 256, 256, 3]))\n  for i, img in tqdm.tqdm(g, total=40 * 32, ascii=True):\n    img = Image.fromarray(img, \'RGB\')\n    if metadata is not None:\n      suffix = ""{}_{}_{}_{}_{}_{}"".format(*metadata[i // 32][1:])\n      img.save(""{}/{:04d}_{}.png"".format(save_dir, i, suffix))\n    else:\n      img.save(""{}/{:04d}.png"".format(save_dir, i))\n\n\ndef group_mat(flags):\n  """"""Group denoised images into required mat format.\n\n  Challenge: Real Image Denoising\n\n  Args:\n    input_dir: path of the result images.\n    save_dir: saving folder or file name of .mat file.\n  """"""\n\n  save_dir = Path(flags.save_dir).resolve()\n  save_dir.mkdir(exist_ok=True, parents=True)\n  if save_dir.is_dir():\n    save_dir /= \'results\'\n  results = []\n  g = sorted(Path(flags.input_dir).glob(\'*.png\'))\n  assert len(g) == 40 * 32, ""Not enough image files!""\n  print("" [*] Appending results..."")\n  for img in tqdm.tqdm(g, ascii=True):\n    img = Image.open(img)\n    if img.width != 256 or img.height != 256:\n      img = img.resize([256, 256], Image.BICUBIC)\n    results.append(img_to_array(img))\n  results = np.stack(results).reshape([40, 32, 256, 256, 3])\n  savemat(str(save_dir), {""results"": results})\n  print("" [*] Saved to {}.mat"".format(save_dir))\n\n\ndef divide(flags):\n  """"""Divide given images to small patches.\n\n  Challenge: can be used to all challenges.\n\n  Args:\n    input_dir: path of images to be divided\n    patch: dividing patch size\n    stride: dividing stride (usually smaller than `patch`)\n  """"""\n\n  def _divide(img: Image, stride: int, size: int) -> list:\n    w = img.width\n    h = img.height\n    img = img_to_array(img, data_format=\'channels_last\')\n    patches = []\n    img = np.pad(img, [[0, size - h % stride or stride],\n                       [0, size - w % stride or stride], [0, 0]],\n                 mode=\'reflect\')\n    size - w % stride\n    for i in np.arange(0, h, stride):\n      for j in np.arange(0, w, stride):\n        patches.append(img[i:i + size, j:j + size])\n    return patches\n\n  save_dir = Path(flags.save_dir).resolve()\n  save_dir.mkdir(exist_ok=True, parents=True)\n  files = sorted(Path(flags.input_dir).glob(""*.png""))\n  print("" [*] Dividing...\\n"")\n  for f in tqdm.tqdm(files, ascii=True):\n    pf = _divide(Image.open(f), flags.stride, flags.patch)\n    for i, p in enumerate(pf):\n      array_to_img(p, \'RGB\', data_format=\'channels_last\').save(\n          f""{save_dir}/{f.stem}_{i:04d}.png"")\n\n\ndef combine(flags):\n  """"""Combining the divided small patches into entire big image.\n    Used as a pair to `divide`.\n\n  Args:\n    input_dir: path of processed fragile images\n    ref: path to referenced original images (target name, width/height)\n    stride: stride used to divide them\n  """"""\n\n  def _combine(ref: Image, sub: list, stride) -> Image:\n    w = ref.width\n    h = ref.height\n    blank = np.zeros([h, w, 3], \'float32\')\n    count = np.zeros([h, w, 1])\n    k = 0\n    for i in np.arange(0, h, stride):\n      for j in np.arange(0, w, stride):\n        p = sub[k]\n        k += 1\n        try:\n          blank[i:i + p.height, j:j + p.width] += img_to_array(\n              p, \'channels_last\')\n        except ValueError:\n          blank[i:i + p.height, j:j + p.width] += img_to_array(\n              p, \'channels_last\')[:h - i, :w - j]\n        count[i:i + p.height, j:j + p.width] += 1\n    blank /= count\n    return array_to_img(blank, \'RGB\', \'channels_last\')\n\n  save_dir = Path(flags.save_dir)\n  save_dir.mkdir(exist_ok=True, parents=True)\n  files = sorted(Path(flags.ref).glob(""*.png""))\n  print("" [!] Combining...\\n"")\n  results = Path(flags.input_dir)\n  for f in tqdm.tqdm(files, ascii=True):\n    sub = list(results.glob(""{}_*.png"".format(f.stem)))\n    sub.sort(key=lambda x: int(x.stem[-4:]))\n    sub = [Image.open(s) for s in sub]\n    img = _combine(Image.open(f), sub, flags.stride)\n    img.save(""{}/{}.png"".format(save_dir, f.stem))\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Tools/Raw2Image.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:03\n\nimport argparse\nimport multiprocessing as mp\nimport re\nfrom pathlib import Path\n\nimport tqdm\n\nfrom VSR.DataLoader.VirtualFile import RawFile\n\nparser = argparse.ArgumentParser(\n  description=""Convert a raw video to a folder of images."")\nparser.add_argument(""input_dir"", help=""root folder of raw videos."")\nparser.add_argument(""output_dir"", help=""root folder of images"")\nparser.add_argument(""--width"", type=int, default=0,\n                    help=""default 0. Auto detect from file name."")\nparser.add_argument(""--height"", type=int, default=0,\n                    help=""default 0. Auto detect from file name."")\nparser.add_argument(""--overwrite"", action=\'store_true\',\n                    help=""overwrite existing files with same name."")\nFLAGS = parser.parse_args()\n\n\ndef guess_file_size(file):\n  name = file.name\n  rept = re.compile(""\\d+[xX]\\d+"")\n  for i in name.split(\'_\'):\n    ans = re.findall(rept, i)\n    if ans:\n      size = ans[0].lower().split(\'x\')\n      return int(size[0]), int(size[1])\n  return -1, -1\n\n\ndef parse_format(fmt):\n  if fmt.upper() in (\'YUV\', \'YUV420P\'):\n    return \'YV12\'\n  return fmt.upper()\n\n\ndef encode(file, save_dir):\n  w = FLAGS.width\n  h = FLAGS.height\n  if w == 0 or h == 0:\n    w, h = guess_file_size(file)\n  if w <= 0 or h <= 0:\n    raise ValueError(""No width/height can be retrieved!"")\n  fmt = file.suffix[1:]\n  fmt = parse_format(fmt)\n  save_dir /= file.stem\n  save_dir.mkdir(exist_ok=FLAGS.overwrite, parents=True)\n  fd = RawFile(file, fmt, [w, h])\n  frames = fd.read_frame(fd.frames)\n  for i, f in enumerate(frames):\n    f.convert(\'RGB\').save(f\'{str(save_dir)}/{i:05d}.png\')\n  return file.stem\n\n\ndef main():\n  input_dir = Path(FLAGS.input_dir)\n  if input_dir.is_dir():\n    raw_videos = filter(lambda f: f.is_file(), input_dir.rglob(\'*\'))\n  else:\n    assert input_dir.is_file()\n    raw_videos = [input_dir]\n  raw_videos = sorted(raw_videos)\n  save_dir = Path(FLAGS.output_dir)\n  save_dir.mkdir(exist_ok=True, parents=True)\n  pool = mp.pool.ThreadPool()\n  results = []\n  for fp in raw_videos:\n    results.append(pool.apply_async(encode, (fp, save_dir)))\n  with tqdm.tqdm(results, ascii=True, unit=\'image\') as r:\n    for i in r:\n      name = i.get()\n      r.set_postfix({\'name\': name})\n  pool.close()\n  pool.join()\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Tools/SeqVisual.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/22 \xe4\xb8\x8b\xe5\x8d\x883:54\n\nimport argparse\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\nfrom VSR.DataLoader.VirtualFile import ImageFile\n\nparser = argparse.ArgumentParser(description=""Sequential Visualizer"")\nparser.add_argument(""input_dir"", help=""input folder"")\nparser.add_argument(""output"", help=""output file path"")\nparser.add_argument(""--row"", \'-r\', type=int, default=-1, help=""row number"")\nparser.add_argument(""--col"", \'-c\', type=int, default=-1, help=""column number"")\nparser.add_argument(""--zoom"", nargs=4, type=int, help=\'zoom coordinate\')\nparser.add_argument(""--compose_id"", type=int, default=None)\n\n\ndef main():\n  flags = parser.parse_args()\n  fp = ImageFile(flags.input_dir)\n  frames = np.stack(fp.read_frame(fp.frames))\n  if flags.zoom:\n    frames = frames[:, flags.zoom[1]: flags.zoom[3],\n             flags.zoom[0]: flags.zoom[2]]\n  savedir = Path(flags.output)\n  if flags.compose_id is not None:\n    compose = frames[flags.compose_id]\n  else:\n    compose = None\n  if 0 <= flags.row < frames.shape[1]:\n    sliced = frames[:, flags.row]\n    if compose is not None:\n      sliced = np.concatenate([compose, sliced], axis=0)\n    if savedir.is_dir():\n      savedir.mkdir(exist_ok=True, parents=True)\n      savedir /= f\'{fp.name}_slice_row{flags.row}.png\'\n  elif 0 <= flags.col < frames.shape[2]:\n    sliced = frames[:, :, flags.col]\n    sliced = np.transpose(sliced, [1, 0, 2])\n    if compose:\n      sliced = np.concatenate([compose, sliced], axis=1)\n    if savedir.is_dir():\n      savedir.mkdir(exist_ok=True, parents=True)\n      savedir /= f\'{fp.name}_slice_col{flags.col}.png\'\n  Image.fromarray(sliced, \'RGB\').save(str(savedir))\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Tools/Vimeo.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/6 \xe4\xb8\x8b\xe5\x8d\x881:39\n\nimport argparse\nimport shutil\nfrom pathlib import Path\n\nimport h5py\nimport tqdm\nimport numpy as np\nfrom PIL import Image\n\nparser = argparse.ArgumentParser(description=""Vimeo dataset splitter"")\nparser.add_argument(""vimeo_dir"", help=""vimeo data path"")\nparser.add_argument(""--type"", help=""vimeo data type"", default=\'sep\')\nparser.add_argument(""--hdf"", action=\'store_true\', help=""export HDF5 dataset"")\n# To use vimeo HDF file, you need ""vimeo"" parser.\ngroup_hdf = parser.add_argument_group(""HDF5"")\ngroup_hdf.add_argument(""--compression"", type=int, default=None)\ngroup_hdf.add_argument(""--data_format"",\n                       choices=(\'channels_first\', \'channels_last\'),\n                       default=\'channels_first\',\n                       help=""data format (default: CHW)"")\nflags = parser.parse_args()\n\n\ndef split_sep(root: Path):\n  testlist = root / \'sep_testlist.txt\'\n  trainlist = root / \'sep_trainlist.txt\'\n  if not testlist.exists() or not trainlist.exists():\n    raise RuntimeError(""[!] Can\'t find separating text files!"")\n  # copy destination\n  (root / \'train\').mkdir(exist_ok=True, parents=True)\n  (root / \'test\').mkdir(exist_ok=True, parents=True)\n  with testlist.open(\'r\') as fd:\n    n = fd.readline()[:-1]\n    test_cnt = 0\n    while n:\n      src = root / \'sequences\' / n\n      dst = root / \'test\' / f\'{src.parent.stem}_{src.stem}\'\n      if not dst.exists():\n        shutil.copytree(src, dst)\n        print(f""{src.stem} --> {dst.stem}"")\n      n = fd.readline()[:-1]\n      test_cnt += 1\n  with trainlist.open(\'r\') as fd:\n    n = fd.readline()[:-1]\n    train_cnt = 0\n    while n:\n      src = root / \'sequences\' / n\n      dst = root / \'train\' / f\'{src.parent.stem}_{src.stem}\'\n      if not dst.exists():\n        shutil.copytree(src, dst)\n        print(f""{src.stem} --> {dst.stem}"")\n      n = fd.readline()[:-1]\n      train_cnt += 1\n  print(f""Total test videos: {test_cnt}"")\n  print(f""Total train videos: {train_cnt}"")\n\n\ndef make_hdf(root: Path):\n  test_dir = root / \'test\'\n  train_dir = root / \'train\'\n  test_shape = (7824, 7, 256, 448, 3)\n  train_shape = (64612, 7, 256, 448, 3)\n  if flags.data_format == \'channels_first\':\n    test_shape = (7824, 7, 3, 256, 448)\n    train_shape = (64612, 7, 3, 256, 448)\n  with h5py.File(str(root / \'test.hdf\'), \'w\') as hdf:\n    hdf.attrs[\'author\'] = \'LoSealL\'\n    hdf.attrs[\'email\'] = \'wenyi.tang@intel.com\'\n    hdf.attrs[\'data_format\'] = flags.data_format\n    data = hdf.create_dataset(\'seq_test\', dtype=\'uint8\', shape=test_shape,\n                              compression=flags.compression)\n    n = 0\n    for v in tqdm.tqdm(sorted(test_dir.iterdir()), desc=\'Test\'):\n      frames = sorted(v.glob(\'*\'))\n      cube = np.stack([Image.open(f) for f in frames])\n      if flags.data_format == \'channels_first\':\n        cube = cube.transpose([0, 3, 1, 2])\n      data[n] = cube\n      n += 1\n      del cube\n  with h5py.File(str(root / \'train.hdf\'), \'w\') as hdf:\n    hdf.attrs[\'author\'] = \'LoSealL\'\n    hdf.attrs[\'email\'] = \'wenyi.tang@intel.com\'\n    hdf.attrs[\'data_format\'] = flags.data_format\n    data = hdf.create_dataset(\'seq_train\', dtype=\'uint8\', shape=train_shape,\n                              compression=flags.compression)\n    n = 0\n    for v in tqdm.tqdm(sorted(train_dir.iterdir()), desc=\'Train\'):\n      frames = sorted(v.glob(\'*\'))\n      cube = np.stack([Image.open(f) for f in frames])\n      if flags.data_format == \'channels_first\':\n        cube = cube.transpose([0, 3, 1, 2])\n      data[n] = cube\n      n += 1\n      del cube\n\n\ndef main():\n  root = Path(flags.vimeo_dir)\n  if not root.exists():\n    raise RuntimeError(""[!] Vimeo directory doesn\'t exist."")\n  if flags.type == \'sep\':\n    split_sep(root)\n  if flags.hdf:\n    make_hdf(root)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Tools/YoukuPackage.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/22 \xe4\xb8\x8b\xe5\x8d\x881:25\n\nimport argparse\nimport os\nimport subprocess\nfrom pathlib import Path\n\nimport tqdm\n\nparser = argparse.ArgumentParser(description=""Youku VSR Packager"")\nparser.add_argument(""-i"", ""--input_dir"")\nparser.add_argument(""-o"", ""--output_dir"")\nparser.add_argument(""--full_percentage"", default=0.1)\nparser.add_argument(""--extract_interval"", default=25)\nparser.add_argument(""-v"", action=\'store_true\', help=""Show debug information"")\nFLAGS = parser.parse_args()\n\n\ndef _check_ffmpeg():\n  import shutil\n  if shutil.which(\'ffmpeg\') is None:\n    raise FileNotFoundError(""Couldn\'t find ffmpeg!"")\n\n\ndef gen_y4m_full(url, num):\n  output = f\'{FLAGS.output_dir}/Youku_{num}_h_Res.y4m\'\n  cmd = f\'ffmpeg -i {str(url)} -pix_fmt yuv420p -vsync 0 {str(output)} -y\'\n  if FLAGS.v:\n    print(cmd)\n  subprocess.call(cmd, stderr=subprocess.DEVNULL, shell=True)\n\n\ndef gen_y4m_part(url, num):\n  output = f\'{FLAGS.output_dir}/Youku_{num}_h_Sub25_Res.y4m\'\n  cmd = f\'ffmpeg -i {str(url)} -pix_fmt yuv420p \'\n  cmd += f""-vf select=\'not(mod(n\\\\,{FLAGS.extract_interval}))\' ""\n  cmd += f""-vsync 0 {str(output)} -y""\n  if FLAGS.v:\n    print(cmd)\n  subprocess.call(cmd, stderr=subprocess.DEVNULL, shell=True)\n\n\ndef parse_video_clips(path):\n  _path = Path(path)\n  if not _path.exists():\n    raise FileNotFoundError(f""{path} doesn\'t exist!!"")\n\n  files = _path.rglob(\'*\')\n  parents = set(f.parent for f in filter(lambda f: f.is_file(), files))\n  return sorted(parents)\n\n\ndef parse_url(path):\n  _path = Path(path)\n  if not _path.exists():\n    raise FileNotFoundError(f""{path} doesn\'t exist!!"")\n  files = filter(lambda f: f.is_file(), _path.glob(\'*\'))\n  files = sorted(files)\n  for i, fp in enumerate(files):\n    target = _path / f\'frames_{i:04d}{fp.suffix}\'\n    fp.rename(target)\n    assert target.exists()\n  return _path / f\'frames_%04d{fp.suffix}\'\n\n\ndef zip(url):\n  url = Path(url)\n  os.chdir(url)\n  cmd = \'zip youku_results.zip *.y4m\'\n  subprocess.call(cmd, shell=True)\n  subprocess.call(""rm *.y4m"", shell=True)\n\n\ndef main():\n  _check_ffmpeg()\n  videos = parse_video_clips(FLAGS.input_dir)\n  root = Path(FLAGS.output_dir)\n  root.mkdir(exist_ok=True, parents=True)\n  print(f"" [*] Total videos found: {len(videos)}."")\n  with tqdm.tqdm(videos, ascii=True, unit=\' video\') as r:\n    for i, fp in enumerate(r):\n      _, num, _ = fp.name.split(\'_\')\n      url = parse_url(fp)\n      if i < FLAGS.full_percentage * len(videos):\n        gen_y4m_full(url, num)\n      else:\n        gen_y4m_part(url, num)\n      r.set_postfix({""name"": fp.stem})\n  zip(root)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Train/check_dataset.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 4 - 17\n\nimport argparse\n\nfrom VSR.DataLoader import load_datasets\n\n\ndef main(entry: str, ddf_file):\n  entry = entry.upper()\n  all_data = load_datasets(ddf_file)\n  if entry not in all_data:\n    raise KeyError(f""The dataset `{entry}` not found in the DDF"")\n\n  data = all_data.get(entry)\n  print(f""Dataset: {data.name}"")\n\n  def _check(name: str):\n    print(f""\\n=========  CHECKING  {name}  =========\\n"")\n    if name in data and data[name] is not None:\n      print(f""Found `{name}` set in \\""{data.name}\\"":"")\n      _hr = data[name].hr\n      _lr = data[name].lr\n      video_type = _hr.as_video\n      if video_type:\n        print(f""\\""{data.name}\\"" is video data"")\n      if _hr is not None:\n        _hr = _hr.compile()\n        print(f""Found {len(_hr)} ground-truth {name} data"")\n      if _lr is not None:\n        _lr = _lr.compile()\n        print(f""Found {len(_lr)} custom degraded {name} data"")\n        if len(_hr) != len(_lr):\n          print(\n              f"" [E] Ground-truth data and degraded data quantity not matched!!"")\n        elif video_type:\n          for x, y in zip(_hr, _lr):\n            if x.frames != y.frames:\n              print(f"" [E] Video clip {x.name}|{y.name} quantity not matched!!"")\n    else:\n      print(f""{data.name} doesn\'t contain any {name} data."")\n\n  _check(\'train\')\n  _check(\'val\')\n  _check(\'test\')\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(\n      description=""Check the dataset and print out its content"")\n  parser.add_argument(""dataset"", type=str,\n                      help=""The name of the dataset, case insensitive."")\n  parser.add_argument(""--description-file"", default=""../Data/datasets.yaml"",\n                      help=""DDF file"")\n  flags = parser.parse_args()\n  main(flags.dataset, flags.description_file)\n'"
Train/eval.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 10\n\nimport argparse\nfrom pathlib import Path\n\nfrom VSR.Backend import BACKEND\nfrom VSR.DataLoader import Dataset, Loader, load_datasets\nfrom VSR.Model import get_model, list_supported_models\nfrom VSR.Util import (\n  Config, compat_param, save_inference_images, suppress_opt_by_args\n)\n\nparser = argparse.ArgumentParser(description=f\'VSR ({BACKEND}) Testing Tool v1.0\')\ng0 = parser.add_argument_group(""basic options"")\ng0.add_argument(""model"", choices=list_supported_models(), help=""specify the model name"")\ng0.add_argument(""-p"", ""--parameter"", help=""specify the model parameter file (*.yaml)"")\ng0.add_argument(""-t"", ""--test"", nargs=\'*\', help=""specify test dataset name or data path"")\ng0.add_argument(""--save_dir"", default=\'../Results\', help=""working directory"")\ng0.add_argument(""--data_config"", default=""../Data/datasets.yaml"", help=""specify dataset config file"")\ng1 = parser.add_argument_group(""evaluating options"")\ng1.add_argument(""--pretrain"", help=""specify the pre-trained model checkpoint or will search into `save_dir` if not specified"")\ng1.add_argument(""--ensemble"", action=""store_true"")\ng1.add_argument(""--video"", action=""store_true"", help=""notify load test data as video stream"")\ng2 = parser.add_argument_group(""device options"")\ng2.add_argument(""--cuda"", action=""store_true"", help=""using cuda gpu"")\ng2.add_argument(""--threads"", type=int, default=8, help=""specify loading threads number"")\ng3 = parser.add_argument_group(""advanced options"")\ng3.add_argument(""--output_index"", default=\'-1\', help=""specify access index of output array (slicable)"")\ng3.add_argument(""--export"", help=""export ONNX (torch backend) or protobuf (tf backend) (needs support from model)"")\ng3.add_argument(""--overwrite"", action=""store_true"", help=""overwrite the existing predicted output files"")\ng3.add_argument(""-c"", ""--comment"", default=None, help=""extend a comment string after saving folder"")\n\n\ndef str2boolean(s):\n  assert isinstance(s, str)\n  if s.lower() in (\'true\', \'yes\', \'1\'):\n    return True\n  else:\n    return False\n\n\ndef overwrite_from_env(flags):\n  import os\n  auto_rename = os.getenv(\'VSR_AUTO_RENAME\')\n  output_index = os.getenv(\'VSR_OUTPUT_INDEX\')\n\n  if auto_rename and auto_rename != \'\':\n    flags.auto_rename = str2boolean(auto_rename)\n  if output_index and output_index != \'\':\n    flags.output_index = output_index\n\n\ndef main():\n  flags, args = parser.parse_known_args()\n  opt = Config(depth=-1)\n  for pair in flags._get_kwargs():\n    opt.setdefault(*pair)\n  overwrite_from_env(opt)\n  data_config_file = Path(flags.data_config)\n  if not data_config_file.exists():\n    raise FileNotFoundError(""dataset config file doesn\'t exist!"")\n  for _ext in (\'json\', \'yaml\', \'yml\'):  # for compat\n    if opt.parameter:\n      model_config_file = Path(opt.parameter)\n    else:\n      model_config_file = Path(f\'par/{BACKEND}/{opt.model}.{_ext}\')\n    if model_config_file.exists():\n      opt.update(compat_param(Config(str(model_config_file))))\n  # get model parameters from pre-defined YAML file\n  model_params = opt.get(opt.model, {})\n  suppress_opt_by_args(model_params, *args)\n  opt.update(model_params)\n  # construct model\n  model = get_model(opt.model)(**model_params)\n  if opt.cuda:\n    model.cuda()\n  if opt.pretrain:\n    model.load(opt.pretrain)\n  root = f\'{opt.save_dir}/{opt.model}\'\n  if opt.comment:\n    root += \'_\' + opt.comment\n  root = Path(root)\n\n  datasets = load_datasets(data_config_file)\n  try:\n    test_datas = [datasets[t.upper()] for t in opt.test] if opt.test else []\n  except KeyError:\n    test_datas = [Config(test=Config(lr=Dataset(*opt.test)), name=\'infer\')]\n    if opt.video:\n      test_datas[0].test.lr.use_like_video_()\n  # enter model executor environment\n  with model.get_executor(root) as t:\n    for data in test_datas:\n      run_benchmark = False if data.test.hr is None else True\n      if run_benchmark:\n        ld = Loader(data.test.hr, data.test.lr, opt.scale,\n                    threads=opt.threads)\n      else:\n        ld = Loader(data.test.hr, data.test.lr, threads=opt.threads)\n      if opt.channel == 1:\n        # convert data color space to grayscale\n        ld.set_color_space(\'hr\', \'L\')\n        ld.set_color_space(\'lr\', \'L\')\n      config = t.query_config(opt)\n      config.inference_results_hooks = [save_inference_images(root / data.name, opt.output_index, not opt.overwrite)]\n      config.batch_shape = [1, opt.depth, -1, -1, -1]\n      config.traced_val = True\n      if run_benchmark:\n        t.benchmark(ld, config)\n      else:\n        t.infer(ld, config)\n    if opt.export:\n      t.export(opt.export)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
Train/train.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport argparse\nfrom pathlib import Path\n\nfrom VSR.Backend import BACKEND\nfrom VSR.DataLoader import CenterCrop, Loader, RandomCrop\nfrom VSR.DataLoader import load_datasets\nfrom VSR.Model import get_model, list_supported_models\nfrom VSR.Util import Config, lr_decay, suppress_opt_by_args, compat_param\n\nparser = argparse.ArgumentParser(description=f\'VSR ({BACKEND}) Training Tool v1.0\')\ng0 = parser.add_argument_group(""basic options"")\ng0.add_argument(""model"", choices=list_supported_models(), help=""specify the model name"")\ng0.add_argument(""-p"", ""--parameter"", help=""specify the model parameter file (*.yaml)"")\ng0.add_argument(""--save_dir"", default=\'../Results\', help=""working directory"")\ng0.add_argument(""--data_config"", default=""../Data/datasets.yaml"", help=""specify dataset config file"")\ng1 = parser.add_argument_group(""training options"")\ng1.add_argument(""--dataset"", default=\'none\', help=""specify a dataset alias for training"")\ng1.add_argument(""--epochs"", type=int, default=1, help=""specify total epochs to train"")\ng1.add_argument(""--steps"", type=int, default=200, help=""specify steps of iteration in every training epoch"")\ng1.add_argument(""--val_steps"", type=int, default=10, help=""steps of iteration of validations during training"")\ng2 = parser.add_argument_group(""device options"")\ng2.add_argument(""--cuda"", action=""store_true"", help=""using cuda gpu"")\ng2.add_argument(""--threads"", type=int, default=8, help=""specify loading threads number"")\ng2.add_argument(\'--memory_limit\', default=None, help=""limit the CPU memory usage. i.e. \'4GB\', \'1024MB\'"")\ng3 = parser.add_argument_group(""advanced options"")\ng3.add_argument(""--traced_val"", action=""store_true"")\ng3.add_argument(""--pretrain"", help=""specify the pre-trained model checkpoint or will search into `save_dir` if not specified"")\ng3.add_argument(""--export"", help=""export ONNX (torch backend) or protobuf (tf backend) (needs support from model)"")\ng3.add_argument(""-c"", ""--comment"", default=None, help=""extend a comment string after saving folder"")\n\n\ndef main():\n  flags, args = parser.parse_known_args()\n  opt = Config()  # An EasyDict object\n  # overwrite flag values into opt object\n  for pair in flags._get_kwargs():\n    opt.setdefault(*pair)\n  # fetch dataset descriptions\n  data_config_file = Path(opt.data_config)\n  if not data_config_file.exists():\n    raise FileNotFoundError(""dataset config file doesn\'t exist!"")\n  for _ext in (\'json\', \'yaml\', \'yml\'):  # for compat\n    if opt.parameter:\n      model_config_file = Path(opt.parameter)\n    else:\n      model_config_file = Path(f\'par/{BACKEND}/{opt.model}.{_ext}\')\n    if model_config_file.exists():\n      opt.update(compat_param(Config(str(model_config_file))))\n  # get model parameters from pre-defined YAML file\n  model_params = opt.get(opt.model, {})\n  suppress_opt_by_args(model_params, *args)\n  opt.update(model_params)\n  # construct model\n  model = get_model(opt.model)(**model_params)\n  if opt.cuda:\n    model.cuda()\n  if opt.pretrain:\n    model.load(opt.pretrain)\n  root = f\'{opt.save_dir}/{opt.model}\'\n  if opt.comment:\n    root += \'_\' + opt.comment\n\n  dataset = load_datasets(data_config_file, opt.dataset)\n  # construct data loader for training\n  lt = Loader(dataset.train.hr, dataset.train.lr, opt.scale, threads=opt.threads)\n  lt.image_augmentation()\n  # construct data loader for validating\n  lv = None\n  if dataset.val is not None:\n    lv = Loader(dataset.val.hr, dataset.val.lr, opt.scale, threads=opt.threads)\n  lt.cropper(RandomCrop(opt.scale))\n  if opt.traced_val and lv is not None:\n    lv.cropper(CenterCrop(opt.scale))\n  elif lv is not None:\n    lv.cropper(RandomCrop(opt.scale))\n  if opt.channel == 1:\n    # convert data color space to grayscale\n    lt.set_color_space(\'hr\', \'L\')\n    lt.set_color_space(\'lr\', \'L\')\n    if lv is not None:\n      lv.set_color_space(\'hr\', \'L\')\n      lv.set_color_space(\'lr\', \'L\')\n  # enter model executor environment\n  with model.get_executor(root) as t:\n    config = t.query_config(opt)\n    if opt.lr_decay:\n      config.lr_schedule = lr_decay(lr=opt.lr, **opt.lr_decay)\n    t.fit([lt, lv], config)\n    if opt.export:\n      t.export(opt.export)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
VSR/__init__.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n#  Collect and reproduce Image/Video Super-Resolution Algorithms.\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport VSR.Backend\nimport VSR.DataLoader\n\n__all__ = [\n  'Backend',\n  'DataLoader',\n  'Model',\n  'Util',\n]\n"""
VSR/Backend/__init__.py,2,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport os\nimport logging\nfrom importlib import import_module\nfrom pathlib import Path\n\nimport yaml\n\ntry:\n  from yaml import FullLoader as _Loader\nexcept ImportError:\n  # For older versions\n  from yaml import Loader as _Loader\n\nLOG = logging.getLogger(\'VSR\')\nHOME = os.environ.get(\'VSR_HOME\')\nif not HOME:\n  HOME = Path(\'~\').expanduser() / \'.vsr\'\nCONFIG = {\n  \'backend\': os.environ.get(\'VSR_BACKEND\', \'pytorch\'),\n  \'verbose\': os.environ.get(\'VSR_VERBOSE\', \'info\'),\n}\nif Path(HOME / \'config.yml\').exists():\n  with open(HOME / \'config.yml\', encoding=\'utf8\') as fd:\n    CONFIG = yaml.load(fd.read(), Loader=_Loader)\n\nLOG.setLevel(CONFIG[\'verbose\'].upper())\nhdl = logging.StreamHandler()\nhdl.setFormatter(logging.Formatter(""%(asctime)s %(levelname)s: %(message)s""))\nLOG.addHandler(hdl)\n\nBACKEND = CONFIG[\'backend\'].lower()\nif BACKEND == \'auto\':\n  BACKEND = \'tensorflow\'\nif BACKEND not in (\'tensorflow\', \'tensorflow2\', \'pytorch\'):\n  BACKEND = \'pytorch\'\n\nif BACKEND in (\'tensorflow\', \'tensorflow2\'):\n  try:\n    tf = import_module(\'tensorflow\')\n    CONFIG[\'data_format\'] = \'channels_last\'\n    if BACKEND == \'tensorflow2\' and tf.__version__.split(\'.\')[0] != \'2\':\n      LOG.warning(f""[!] Current tensorflow version is {tf.__version__}"")\n      LOG.info(""[*] Fallback to use tensorflow"")\n      BACKEND = \'tensorflow\'\n  except ImportError:\n    LOG.warning(""[!] Tensorflow package not found in your system."")\n    LOG.info(""[*] Fallback to use PyTorch..."")\n    BACKEND = \'pytorch\'\n\nif BACKEND == \'pytorch\':\n  try:\n    torch = import_module(\'torch\')\n    CONFIG[\'data_format\'] = \'channels_first\'\n    _ver = torch.__version__.split(\'.\')\n    if _ver[0] != \'1\' or _ver[1] <= \'1\':\n      LOG.warning(\n          f""[!] PyTorch version too low: {torch.__version__}, recommended 1.2.0"")\n  except ImportError:\n    LOG.fatal(""[!] PyTorch package not found in your system."")\n    raise ImportError(""Not an available backend found! Check your environment."")\n\nDATA_FORMAT = CONFIG[\'data_format\'].lower()\n'"
VSR/DataLoader/Crop.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport numpy as np\nfrom ..Backend import DATA_FORMAT\n\n\nclass Cropper(object):\n  def __init__(self, scale):\n    self.scale = scale\n\n  def __call__(self, img_pair: tuple, shape: list):\n    assert len(img_pair) >= 2, \\\n      f""Pair must contain more than 2 elements, which is {img_pair}""\n    for img in img_pair:\n      assert img.ndim == len(shape), \\\n        f""Shape mis-match: {img.ndim} != {len(shape)}""\n\n    return self.call(img_pair, shape)\n\n  def call(self, img: tuple, shape: (list, tuple)) -> tuple:\n    raise NotImplementedError\n\n\nclass RandomCrop(Cropper):\n  def call(self, img: tuple, shape: (list, tuple)) -> tuple:\n    hr, lr = img\n    if lr.shape[-2] < shape[-2]:\n      raise ValueError(""Batch shape is too large than data"")\n    ind = [np.random.randint(nd + 1) for nd in lr.shape - np.array(shape)]\n    slc1 = [slice(n, n + s) for n, s in zip(ind, shape)]\n    slc2 = slc1.copy()\n    if DATA_FORMAT == \'channels_last\':\n      slc2[-2] = slice(ind[-2] * self.scale,\n                       (ind[-2] + shape[-2]) * self.scale)\n      slc2[-3] = slice(ind[-3] * self.scale,\n                       (ind[-3] + shape[-3]) * self.scale)\n    else:\n      slc2[-1] = slice(ind[-1] * self.scale,\n                       (ind[-1] + shape[-1]) * self.scale)\n      slc2[-2] = slice(ind[-2] * self.scale,\n                       (ind[-2] + shape[-2]) * self.scale)\n    return hr[tuple(slc2)], lr[tuple(slc1)]\n\n\nclass CenterCrop(Cropper):\n  def call(self, img: tuple, shape: (list, tuple)) -> tuple:\n    hr, lr = img\n    ind = [nd // 2 for nd in hr.shape - np.array(shape)]\n    slc1 = [slice(n, n + s) for n, s in zip(ind, shape)]\n    slc2 = slc1.copy()\n    if DATA_FORMAT == \'channels_last\':\n      slc2[-2] = slice(ind[-2] * self.scale,\n                       (ind[-2] + shape[-2]) * self.scale)\n      slc2[-3] = slice(ind[-3] * self.scale,\n                       (ind[-3] + shape[-3]) * self.scale)\n    else:\n      slc2[-1] = slice(ind[-1] * self.scale,\n                       (ind[-1] + shape[-1]) * self.scale)\n      slc2[-2] = slice(ind[-2] * self.scale,\n                       (ind[-2] + shape[-2]) * self.scale)\n    return hr[tuple(slc2)], lr[tuple(slc1)]\n'"
VSR/DataLoader/Dataset.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport re\nfrom concurrent import futures\nfrom pathlib import Path\nimport copy\n\nimport yaml\n\nfrom .VirtualFile import ImageFile, RawFile\nfrom ..Util import Config, to_list\n\ntry:\n  from yaml import FullLoader as _Loader\nexcept ImportError:\n  from yaml import Loader as _Loader\n\nIMAGE_SUF = (\'PNG\', \'JPG\', \'JPEG\', \'BMP\', \'TIFF\', \'TIF\', \'GIF\')\nVIDEO_SUF = {\n  \'NV12\': \'NV12\',\n  \'YUV\': \'YV12\',\n  \'YV12\': \'YV12\',\n  \'NV21\': \'NV21\',\n  \'YV21\': \'YV21\',\n  \'RGB\': \'RGB\'\n}\n\n\ndef _supported_image(x: Path):\n  return x.suffix[1:].upper() in IMAGE_SUF\n\n\ndef _supported_video(x: Path):\n  return x.suffix[1:].upper() in VIDEO_SUF\n\n\ndef _supported_suffix(x: Path):\n  return _supported_image(x) or _supported_video(x)\n\n\nclass Dataset(object):\n  """""" Make a `dataset` object\n\n  """"""\n\n  def __init__(self, *folders):\n    self.dirs = list(map(Path, folders))\n    self.recursive = True\n    self.glob_patterns = (\'*\',)\n    self.inc_patterns = None\n    self.exc_patterns = None\n    self.as_video = False\n    self.compiled = None\n\n  def use_like_video_(self):\n    self.as_video = True\n\n  def use_like_video(self):\n    d = copy.copy(self)\n    d.compiled = None\n    d.use_like_video_()\n    return d\n\n  def include_(self, *pattern: str):\n    self.glob_patterns = list(pattern)\n    self.inc_patterns = None\n\n  def include(self, *pattern: str):\n    d = copy.copy(self)\n    d.compiled = None\n    d.include_(*pattern)\n    return d\n\n  def include_reg_(self, *reg: str):\n    self.inc_patterns = [re.compile(r) for r in reg]\n    self.glob_patterns = (\'*\',)\n\n  def include_reg(self, *reg: str):\n    d = copy.copy(self)\n    d.compiled = None\n    d.include_reg_(*reg)\n    return d\n\n  def exclude_(self, *reg: str):\n    self.exc_patterns = [re.compile(r) for r in reg]\n\n  def exclude(self, *reg: str):\n    d = copy.copy(self)\n    d.compiled = None\n    d.exclude_(*reg)\n    return d\n\n  def compile(self):\n    if self.compiled:\n      return self.compiled\n    files = []\n\n    def _exc(x: Path):\n      if self.exc_patterns:\n        for reg in self.exc_patterns:\n          if reg.search(str(x.absolute().as_posix())):\n            return False\n      return True\n\n    def _inc(x: Path):\n      if self.inc_patterns:\n        for reg in self.inc_patterns:\n          if reg.search(str(x.absolute().as_posix())):\n            return True\n      return False\n\n    for folder in self.dirs:\n      if not Path(folder).exists():\n        continue\n      nodes = []\n      if folder.is_file():\n        # if points to a file rather than a directory\n        nodes.append(folder)\n      fn_glob = Path.rglob if self.recursive else Path.glob\n      for pat in self.glob_patterns:\n        nodes += list(fn_glob(folder, pat))\n      if self.inc_patterns:\n        nodes = filter(_inc, nodes)\n      files += list(filter(_exc, filter(_supported_suffix, nodes)))\n    image_nodes = list(filter(_supported_image, files))\n    if not self.as_video:\n      self.compiled = Container(sorted(image_nodes), self.as_video)\n      return self.compiled\n    video_nodes = list(filter(_supported_video, files))\n    video_nodes += list(map(lambda x: x.parent, image_nodes))\n    video_nodes = list(set(video_nodes))  # remove duplicated nodes\n    self.compiled = Container(sorted(video_nodes), self.as_video)\n    return self.compiled\n\n\nclass Container(object):\n  """"""Frames container\n\n  """"""\n\n  def __init__(self, urls, is_video: bool):\n    assert isinstance(urls, (list, tuple))\n    pool = futures.ThreadPoolExecutor(4)\n    fs = []\n    self.nodes = []\n\n    def _parse_image_node(url: Path):\n      if url.is_dir():\n        for i in filter(_supported_image, url.glob(\'*\')):\n          self.nodes.append(ImageFile(i, rewind=True))\n      elif _supported_image(url):\n        self.nodes.append(ImageFile(url, rewind=True))\n\n    def _parse_video_node(url: Path):\n      if _supported_video(url):\n        size = re.findall(""\\\\d+x\\\\d+"", url.stem)\n        if size:\n          size = [int(x) for x in size[0].split(\'x\')]\n          self.nodes.append(\n              RawFile(url, VIDEO_SUF[url.suffix[1:].upper()], size,\n                      rewind=True))\n      elif url.is_dir():\n        self.nodes.append(ImageFile(url))\n\n    for j in urls:\n      if is_video:\n        fs.append(pool.submit(_parse_video_node, j))\n      else:\n        fs.append(pool.submit(_parse_image_node, j))\n    futures.as_completed(fs)\n    pool.shutdown()\n    self.nodes = sorted(self.nodes, key=lambda x: x.path)\n\n  def __getitem__(self, item):\n    return self.nodes[item]\n\n  def __len__(self):\n    return len(self.nodes)\n\n  @property\n  def capacity(self):\n    if not self.nodes:\n      return 0\n    pos = 0\n    max_sz = 0\n    total_frames = 0\n    for i, n in enumerate(self.nodes):\n      total_frames += n.frames\n      if n.size() > max_sz:\n        max_sz = n.size()\n        pos = i\n    shape = self.nodes[pos].shape\n    max_bpp = 3\n    return shape[0] * shape[1] * max_bpp * total_frames\n\n\ndef load_datasets(describe_file, key=\'\'):\n  """"""load dataset described in YAML file""""""\n\n  def _extend_pattern(url):\n    _url = root / Path(url)\n    url_p = _url\n\n    while True:\n      try:\n        if url_p.exists():\n          break\n      except OSError:\n        url_p = url_p.parent\n        continue\n      if url_p == url_p.parent:\n        break\n      url_p = url_p.parent\n    # retrieve glob pattern\n    url_r = str(_url.relative_to(url_p))\n    if url_r == \'.\' and url_p.is_dir():\n      return str(Path(url) / \'**/*\')\n    return url\n\n  def _get_dataset(desc, use_as_video=None, name=None):\n    dataset = Config(name=name)\n    for i in desc:\n      if i not in (\'train\', \'val\', \'test\'):\n        continue\n      if isinstance(desc[i], dict):\n        hr = to_list(desc[i].get(\'hr\'))\n        lr = to_list(desc[i].get(\'lr\'))\n      else:\n        hr = to_list(desc[i])\n        lr = []\n      if use_as_video:\n        hr_pattern = [\n          x if x not in all_path and x + \'[video]\' not in all_path else\n          all_path[x + \'[video]\'] for x in hr]\n        lr_pattern = [\n          x if x not in all_path and x + \'[video]\' not in all_path else\n          all_path[x + \'[video]\'] for x in lr]\n      else:\n        hr_pattern = [x if x not in all_path else all_path[x] for x in hr]\n        lr_pattern = [x if x not in all_path else all_path[x] for x in lr]\n      hr_data = Dataset(root).include(*(_extend_pattern(x) for x in hr_pattern))\n      lr_data = Dataset(root).include(\n          *(_extend_pattern(x) for x in lr_pattern)) if lr_pattern else None\n      hr_data.recursive = False\n      if lr_data is not None:\n        lr_data.recursive = False\n      if use_as_video:\n        hr_data.use_like_video_()\n        if lr_data is not None:\n          lr_data.use_like_video_()\n      setattr(dataset, i, Config(hr=hr_data, lr=lr_data))\n    return dataset\n\n  datasets = Config()\n  with open(describe_file, \'r\') as fd:\n    config = yaml.load(fd, Loader=_Loader)\n    root = Path(config[""Root""])\n    if not root.is_absolute():\n      # make `root` relative to the file\n      root = Path(describe_file).resolve().parent / root\n      root = root.resolve()\n    all_path = config[""Path""]\n    if key.upper() in config[""Dataset""]:\n      return _get_dataset(config[""Dataset""][key.upper()], name=key)\n    elif key.upper() + \'[video]\' in config[""Dataset""]:\n      return _get_dataset(config[""Dataset""][key.upper() + \'[video]\'], True,\n                          name=key)\n    elif key.upper() in all_path:\n      return _get_dataset(Config(test=all_path[key.upper()]), name=key)\n    elif key.upper() + \'[video]\' in all_path:\n      return _get_dataset(Config(test=all_path[key.upper() + \'[video]\']), True,\n                          name=key)\n    for name, value in config[""Dataset""].items():\n      if \'[video]\' in name:\n        name = name.replace(\'[video]\', \'\')\n        datasets[name] = _get_dataset(value, True, name=name)\n      else:\n        datasets[name] = _get_dataset(value, name=name)\n    for name in all_path:\n      if \'[video]\' in name:\n        _name = name.replace(\'[video]\', \'\')\n        datasets[_name] = _get_dataset(Config(test=all_path[name]), True,\n                                       name=_name)\n      else:\n        datasets[name] = _get_dataset(Config(test=all_path[name]), name=name)\n    return datasets\n'"
VSR/DataLoader/FloDecoder.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\nimport logging\nfrom importlib import import_module\n\nimport numpy as np\n\n\ndef open_flo(fn):\n  """""" Read .flo file in Middlebury format\n  # Code adapted from:\n  # http://stackoverflow.com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy\n\n  # WARNING: this will work on little-endian architectures (eg Intel x86) only!\n  # print \'fn = %s\'%(fn)\n  """"""\n  with open(fn, \'rb\') as f:\n    magic = np.fromfile(f, np.float32, count=1)\n    if 202021.25 != magic:\n      logging.error(\'Magic number incorrect. Invalid .flo file\')\n      return None\n    else:\n      w = np.fromfile(f, np.int32, count=1)[0]\n      h = np.fromfile(f, np.int32, count=1)[0]\n      # print \'Reading %d x %d flo file\\n\' % (w, h)\n      data = np.fromfile(f, np.float32, count=2 * w * h)\n      # Reshape data into 3D array (columns, rows, bands)\n      # The reshape here is for visualization, the original code is (w,h,2)\n      return np.resize(data, (int(h), int(w), 2))\n\n\ndef write_flo(filename, uv, v=None):\n  """""" Write optical flow to file.\n\n  Original code by Deqing Sun, adapted from Daniel Scharstein.\n  """"""\n  n_bands = 2\n  _TAG_CHAR = np.array([202021.25], np.float32)\n\n  if v is None:\n    u = uv[..., 0]\n    v = uv[..., 1]\n  else:\n    u = uv\n  height, width = u.shape\n  with open(filename, \'wb\') as f:\n    # write the header\n    f.write(_TAG_CHAR.tobytes())\n    np.array(width).astype(np.int32).tofile(f)\n    np.array(height).astype(np.int32).tofile(f)\n    # arrange into matrix form\n    tmp = np.zeros((height, width * n_bands))\n    tmp[:, np.arange(width) * 2] = u\n    tmp[:, np.arange(width) * 2 + 1] = v\n    tmp.astype(np.float32).tofile(f)\n\n\nclass KITTI:\n  @staticmethod\n  def open_png16(fn):\n    """"""Read 16bit png file""""""\n\n    png = import_module(\'png\')\n    reader = png.Reader(fn)\n    data = reader.asDirect()\n    pixels = []\n    for row in data[2]:\n      row = np.reshape(np.asarray(row), [-1, 3])\n      pixels += [row]\n    return np.stack(pixels, 0)\n\n  @staticmethod\n  def open_flow(fn):\n    flow = KITTI.open_png16(fn)\n    valid = flow[..., -1]\n    u = flow[..., 0].astype(\'float32\')\n    v = flow[..., 1].astype(\'float32\')\n    u = (u - 2 ** 15) / 64 * valid\n    v = (v - 2 ** 15) / 64 * valid\n    return np.stack([u, v], -1)\n'"
VSR/DataLoader/Loader.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport logging\nfrom concurrent import futures\n\nimport numpy as np\nfrom PIL import Image\nfrom psutil import virtual_memory\n\nfrom .Crop import RandomCrop\nfrom .Dataset import Container, Dataset\nfrom .Transform import Bicubic, Tidy\nfrom ..Backend import DATA_FORMAT\nfrom ..Util import Utility\nfrom ..Util.ImageProcess import img_to_array\n\nFREE_MEMORY = virtual_memory().available * 0.5\nLOG = logging.getLogger(\'VSR.Loader\')\n\n\ndef _augment(image, op):\n  """"""Image augmentation""""""\n  assert image.ndim == 4, f\'Dim of image must be 4, but is {image.ndim}\'\n  if op[0]:\n    if DATA_FORMAT == \'channels_last\':\n      image = np.rot90(image, 1, axes=(1, 2))\n    else:\n      image = np.rot90(image, 1, axes=(2, 3))\n  if op[1]:\n    if DATA_FORMAT == \'channels_last\':\n      image = image[:, :, ::-1]\n    else:\n      image = image[..., ::-1]\n  if op[2]:\n    if DATA_FORMAT == \'channels_last\':\n      image = image[:, ::-1]\n    else:\n      image = image[:, :, ::-1]\n  return image\n\n\nclass EpochIterator:\n  """"""An iterator for generating batch data in one epoch\n\n  Args:\n      loader: A `Loader` object to provide properties.\n      shape: The shape of the generated batch, 5-D requested [N, T, C, H, W].\n      steps: The number of batches to generate in one epoch.\n      shuffle: A boolean representing whether to shuffle the dataset.\n\n  Note:\n      The rules for -1 shape:\n      - If shape[1] is -1, which represents the temporal length,\n        will generate the entire video clips;\n      - if shape[2] is -1, will auto choose the channel number according to\n        color format;\n      - if shape[3] or shape[4] is -1, will deduce the frame height or width\n        to replace the value.\n\n      The rules for steps:\n      - If the `steps` is -1, will generate batches in sequential order;\n  """"""\n\n  def __init__(self, loader, shape, steps, shuffle=None):\n    self.loader = loader\n    self.shape = shape\n    self.depth = shape[1]\n    self.count = 0\n    t = len(self.loader.data[\'hr\'])\n    frame_nums = [len(i) for i in self.loader.data[\'hr\']]\n    temporal_padding = not shuffle\n    self.index = []\n    for i in range(t):\n      depth = self.depth if self.depth >= 0 else len(self.loader.data[\'hr\'][i])\n      idx_ = [(i, np.array([j + x for x in range(depth)])) for j in\n              range(-(depth // 2), frame_nums[i] - (depth // 2))]\n      d2_ = depth // 2\n      self.index += idx_ if temporal_padding or d2_ == 0 else idx_[d2_ : -d2_]\n    self.steps = steps if steps >= 0 else len(self.index) // shape[0]\n    while len(self.index) < self.steps * shape[0] and self.index:\n      self.index += self.index\n    if shuffle:\n      np.random.shuffle(self.index)\n\n  def __len__(self):\n    return self.steps\n\n  def __iter__(self):\n    return self\n\n  def __next__(self):\n    pack = {\'hr\': [], \'lr\': [], \'name\': []}\n    if self.count >= self.steps:\n      raise StopIteration(""All batch data generated."")\n\n    slc = slice(self.count * self.shape[0], (self.count + 1) * self.shape[0])\n    crop = self.loader.crop\n    cb_hr = (self.loader.hr[\'transform1\'], self.loader.hr[\'transform2\'])\n    cb_lr = (self.loader.lr[\'transform1\'], self.loader.lr[\'transform2\'])\n    for i, d in self.index[slc]:\n      if i >= len(self.loader.data[\'hr\']):\n        continue\n      hr = self.loader.data[\'hr\'][i]\n      lr = self.loader.data[\'lr\'][i]\n      # crop a video clip, clamp the depth index\n      d[d < 0] = 0\n      d[d >= len(hr)] = len(hr) - 1\n      name = self.loader.data[\'names\'][i]\n      hr2 = np.asarray(hr, dtype=object)[d]\n      for fn in cb_hr[0]:\n        hr2 = [fn(img) for img in hr2]\n      hr2 = [img.convert(self.loader.hr[\'color\']) for img in hr2]\n      lr2 = np.asarray(lr, dtype=object)[d]\n      for fn in cb_lr[0]:\n        lr2 = [fn(img) for img in lr2]\n      lr2 = [img.convert(self.loader.lr[\'color\']) for img in lr2]\n      hr3 = np.stack([img_to_array(img, DATA_FORMAT) for img in hr2])\n      lr3 = np.stack([img_to_array(img, DATA_FORMAT) for img in lr2])\n      del hr2, lr2\n      if hr3.shape[0] == 1 and lr3.shape[0] == 1:\n        hr3 = hr3.squeeze(0)\n        lr3 = lr3.squeeze(0)\n        hr4, lr4 = crop((hr3, lr3), shape=self.shape[2:]) if crop else (\n          hr3, lr3)\n      else:\n        hr4, lr4 = crop((hr3, lr3), shape=self.shape[1:]) if crop else (\n          hr3, lr3)\n      del hr3, lr3\n      hr4 = np.expand_dims(hr4, 0)  # 4-D or 5-D\n      lr4 = np.expand_dims(lr4, 0)  # [1, (T,) C, H, W]\n      for fn in cb_hr[1]:\n        hr4 = fn(hr4)\n      for fn in cb_lr[1]:\n        lr4 = fn(lr4)\n\n      if self.loader.aux[\'augmentation\']:\n        ops = np.random.randint(0, 2, [3])\n      else:\n        ops = [0, 0, 0]\n      _shape0 = hr4.shape\n      _shape1 = lr4.shape\n      hr5 = _augment(hr4.reshape([-1, *_shape0[-3:]]), ops)\n      lr5 = _augment(lr4.reshape([-1, *_shape1[-3:]]), ops)\n      del hr4, lr4\n      pack[\'hr\'].append(hr5.reshape(_shape0))\n      pack[\'lr\'].append(lr5.reshape(_shape1))\n      pack[\'name\'].append(name)\n\n    if pack[\'hr\']:\n      pack[\'hr\'] = np.concatenate(pack[\'hr\'])\n    if pack[\'lr\']:\n      pack[\'lr\'] = np.concatenate(pack[\'lr\'])\n    self.count += 1\n    return pack\n\n\nclass Loader(object):\n  """"""A parallel data loader that generates label and data batches each epoch.\n\n  Args:\n      hr_data: this is a data container from `Dataset` object, represents the\n               label (high-resolution) data.\n      lr_data: this is a data container from `Dataset` object, represents the\n               training (low-resolution) data.\n      scale: specify the scale factor for this model. If scale is not specified,\n             you must set ""lr_data"", and ""cropper"" explicitly.\n      extra_data: a dict object contains extra information. You won\'t use this.\n      threads: num of threads used to load the data.\n\n  Note:\n      A `Loader` object has several attributes to enhance the loader\'s ability.\n      See `Loader.add_data_transform`, `Loader.image_augmentation`,\n      `Loader.cropper`, `Loader.set_color_space` for details.\n  """"""\n\n  def __init__(self, hr_data, lr_data=None, scale=None, extra_data: dict = None,\n               threads=1):\n    # check type\n    if isinstance(hr_data, Dataset):\n      hr_data = hr_data.compile()\n      assert isinstance(hr_data, Container)\n    if isinstance(lr_data, Dataset):\n      lr_data = lr_data.compile()\n      assert isinstance(lr_data, Container)\n    if lr_data is not None and hr_data is not None:\n      assert len(hr_data) == len(lr_data), \\\n        f""Length of HR and LR data mis-match: {len(lr_data)} != {len(lr_data)}""\n    else:\n      hr_data = hr_data or lr_data\n      lr_data = lr_data or hr_data\n    if hr_data is None and lr_data is None:\n      hr_data = lr_data = Container([], False)\n    scale = scale or 1  # deduce to 1\n    if extra_data is not None:\n      assert isinstance(extra_data, dict)\n\n    # default params\n    self.hr = {\n      \'data\': hr_data,\n      \'transform1\': [],\n      \'transform2\': [],\n      \'color\': \'RGB\'\n    }\n    self.lr = {\n      \'data\': lr_data,\n      \'transform1\': [],\n      \'transform2\': [],\n      \'color\': \'RGB\'\n    }\n    self.aux = {\n      \'augmentation\': False,\n      \'scale\': scale,\n      \'fetchList\': list(np.arange(len(hr_data)))\n    }\n    self.data = {\n      \'hr\': [],\n      \'lr\': [],\n      \'names\': [],\n      \'extra\': []\n    }\n    self.cache = {\n      \'hr\': [],\n      \'lr\': [],\n      \'names\': [],\n      \'extra\': []\n    }\n    self.extra = extra_data or {}\n    self.crop = None\n    self.threads = threads\n    self.thp = futures.ThreadPoolExecutor(max_workers=threads)\n    self.fs = []\n    self.loaded = 0\n    if self.hr[\'data\'] is self.lr[\'data\']:\n      cap = self.hr[\'data\'].capacity\n    else:\n      cap = self.hr[\'data\'].capacity + self.lr[\'data\'].capacity\n    if self.extra and isinstance(self.extra[\'data\'], Container):\n      cap += self.extra[\'data\'].capacity\n    self.aux[\'cap\'] = cap  # estimated memory usage in bytes\n    if hr_data is lr_data and scale > 1:\n      self.add_data_transform(\'hr\', Tidy(scale))\n      self.add_data_transform(\'lr\', Tidy(scale), Bicubic(1 / scale))\n\n  def add_data_transform(self, target: str, *fn, dtype=\'pillow\'):\n    """"""Add data transform functions. Each function will be called before\n    generating batch data.\n\n    Args:\n        target: either ""hr"" or ""lr"", specify which data to apply.\n        *fn: functions with only one argument, the type of the argument will\n             be specified through `dtype`.\n        dtype: specify the type of the function\'s argument.\n\n    Note:\n        `dtype` supports `numpy.ndarray` and `PIL.Image.Image`\n    """"""\n    assert target.lower() in (\'hr\', \'lr\')\n    if isinstance(dtype, Image.Image):\n      dtype = \'pillow\'\n    elif isinstance(dtype, np.ndarray):\n      dtype = \'numpy\'\n    assert dtype.lower() in (\'pillow\', \'numpy\', \'pil\', \'np\')\n    fn = filter(callable, fn)\n    if dtype.lower() in (\'pillow\', \'pil\'):\n      getattr(self, target.lower())[\'transform1\'] += list(fn)\n    else:\n      getattr(self, target.lower())[\'transform2\'] += list(fn)\n\n  def image_augmentation(self):\n    """"""Enable data augmentation\n\n    The data augmentation for single image will randomly rotate and flip the\n    image.\n    The data augmentation for video will randomly rotate, flip and revert the\n    video.\n\n    TODO: revert is not implemented.\n    """"""\n    self.aux[\'augmentation\'] = True\n\n  def cropper(self, fn):\n    assert callable(fn)\n    self.crop = fn\n\n  def set_color_space(self, target: str, mode: str):\n    if not mode.upper() in (\'RGB\', \'L\', \'YCbCr\', \'Gray\'):\n      raise ValueError(f""Invalid mode: {mode}, must be RGB | L | YCbCr | Gray"")\n    assert target.lower() in (\'hr\', \'lr\')\n    getattr(self, target.lower()).update(color=mode)\n\n  def make_one_shot_iterator(self, batch_shape, steps, shuffle=None,\n                             memory_limit=None):\n    """"""Make an iterator object to generate batch data for models.\n\n    Args:\n        batch_shape: The shape of batch to generate.\n        steps: The number of batches to generate in one epoch.\n        shuffle: A boolean representing whether to shuffle the dataset.\n        memory_limit: the maximum system memory to use. (Not GPU memory!!)\n\n    Note:\n        The rules for -1 shape:\n        - If shape[1] is -1, which represents the temporal length,\n          will generate the entire video clips;\n        - if shape[2] is -1, will auto choose the channel number according to\n          color format;\n        - if shape[3] or shape[4] is -1, will deduce the frame height or width\n          to replace the value.\n\n        The rules for steps:\n        - If the `steps` is -1, will generate batches in sequential order;\n        - If the `steps` is an positive integer, the generated batches are\n          randomly shuffled.\n    """"""\n    shape = list(batch_shape)\n    if len(shape) == 4:\n      shape.insert(1, 1)\n    assert len(shape) is 5, f""Shape is not 5D, which is {len(shape)}""\n    if shape[-2] != -1 and self.crop is None:\n      self.cropper(RandomCrop(self.aux[\'scale\']))\n    if isinstance(memory_limit, str):\n      memory_limit = Utility.str_to_bytes(memory_limit)\n    self.prefetch(shuffle, memory_limit)\n    futures.as_completed(self.fs)\n    for fs in self.fs:\n      if fs.exception():\n        raise fs.exception()\n      assert fs.done(), ""Thread pool not finished!""\n    self.fs.clear()\n    if not (self.loaded & int(2 ** self.threads - 1)):\n      self.data, self.cache = self.cache, self.data\n      [self.cache[k].clear() for k in self.cache]\n      loaded = self.loaded >> (self.threads * 2)\n      if not shuffle:\n        loaded += 1  # move to next chunk\n        if loaded >= self.aux[\'cap\'] / memory_limit:\n          loaded = 0\n      self.loaded = loaded << (self.threads * 2)\n    return EpochIterator(self, shape, steps, shuffle)\n\n  def prefetch(self, shuffle=None, memory_usage=None):\n    # check memory usage\n    if isinstance(memory_usage, str):\n      memory_usage = Utility.str_to_bytes(memory_usage)\n    if not memory_usage:\n      memory_usage = FREE_MEMORY\n    available_mem = min([np.uint64(memory_usage), np.uint64(FREE_MEMORY)])\n    if not self.fs:\n      if shuffle:\n        self.aux[\'fetchList\'] = list(\n            np.random.permutation(self.aux[\'fetchList\']))\n      if available_mem > self.aux[\'cap\']:\n        LOG.debug(""Loading all data into memory."")\n        for i in range(self.threads):\n          if self.loaded & (1 << i):\n            continue\n          self.fs.append(self.thp.submit(self._prefetch_all, i))\n      else:\n        prop = memory_usage / self.aux[\'cap\'] / self.threads\n        # How many frames can be read into memory each thread each epoch\n        # Note: we assume each ""frame"" has a close size.\n        n = max(1, int(np.round(len(self.hr[\'data\']) * prop)))\n        LOG.debug(f""Loading {prop * self.threads * 100:.4f}% data."")\n        [self.fs.append(self.thp.submit(self._prefecth_chunk, n, i)) for i in\n         range(self.threads)]\n\n  def _prefetch_all(self, index):\n    length = len(self.hr[\'data\'])\n    # load all clips\n    interval = int(np.ceil(length / self.threads))\n    frames_hr = []\n    frames_lr = []\n    frames_extra = []\n    names = []\n    for img in self.hr[\'data\'][index * interval:(index + 1) * interval]:\n      frames_hr.append(img.read_frame(img.frames))\n      names.append(img.name)\n    if self.hr[\'data\'] is self.lr[\'data\']:\n      frames_lr = frames_hr\n    else:\n      for img in self.lr[\'data\'][index * interval:(index + 1) * interval]:\n        frames_lr.append(img.read_frame(img.frames))\n    if self.extra and isinstance(self.extra[\'data\'], Container):\n      for img in self.extra[\'data\'][index * interval:(index + 1) * interval]:\n        frames_extra.append(img.read_frame(img.frames))\n    self.data[\'hr\'] += frames_hr\n    self.data[\'lr\'] += frames_lr\n    self.data[\'names\'] += names\n    self.data[\'extra\'] += frames_extra\n    self.loaded |= (1 << index)\n\n  def _prefecth_chunk(self, chunk_size, index):\n    loaded = self.loaded >> (self.threads * 2)\n    n = chunk_size\n    st = n * self.threads * loaded  # start chunk\n    frames_hr = []\n    frames_lr = []\n    frames_extra = []\n    names = []\n    for i in self.aux[\'fetchList\'][st + n * index:st + n * (index + 1)]:\n      img = self.hr[\'data\'][i]\n      frames_hr.append(img.read_frame(img.frames))\n      img.reopen()\n      names.append(img.name)\n      if self.hr[\'data\'] is self.lr[\'data\']:\n        frames_lr.append(frames_hr[-1])\n      else:\n        img = self.lr[\'data\'][i]\n        frames_lr.append(img.read_frame(img.frames))\n        img.reopen()\n      if self.extra and isinstance(self.extra[\'data\'], Container):\n        img = self.extra[\'data\'][i]\n        frames_extra.append(img.read_frame(img.frames))\n        img.reopen()\n    self.cache[\'hr\'] += frames_hr\n    self.cache[\'lr\'] += frames_lr\n    self.cache[\'extra\'] += frames_extra\n    self.cache[\'names\'] += names\n    loaded <<= self.threads\n    loaded |= (1 << index)\n    self.loaded = loaded << self.threads\n'"
VSR/DataLoader/NVDecoder.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\n# Image customized decoder for NV12([Y][UV/4]), NV21([Y][VU/4])\n# NOTE: [Y] means Y channel is a planar channel, [UV] means UV\n#   channels together is planar, but U and V are packed. [UV/4]\n#   means U and V are sub-sampled by a factor of [2, 2]\n\nimport numpy as np\nfrom PIL import ImageFile\n\n\nclass NV12Decoder(ImageFile.PyDecoder):\n  """"""PIL.Image.DECODERS for NV12 format raw bytes\n\n  Registered in `Image.DECODERS`, don\'t use this class directly!\n  """"""\n\n  def __init__(self, mode, *args):\n    super(NV12Decoder, self).__init__(mode, *args)\n\n  def decode(self, buffer):\n    if self.mode == \'L\':\n      # discard UV channel\n      self.set_as_raw(buffer, \'L\')\n    else:\n      w, h = self.im.size\n      y = np.frombuffer(buffer, \'uint8\', count=w * h)\n      uv = np.frombuffer(buffer, \'uint8\', count=w * h // 2, offset=w * h)\n      y = np.reshape(y, [h, w, 1])\n      uv = np.reshape(uv, [h // 2, w // 2, 2])\n      uv = uv[np.arange(h) // 2][:, np.arange(w) // 2]\n      yuv = np.concatenate([y, uv], axis=-1)\n      self.set_as_raw(yuv.flatten().tobytes())\n    return -1, 0\n\n\nclass NV21Decoder(ImageFile.PyDecoder):\n  """"""PIL.Image.DECODERS for NV21 format raw bytes\n\n  Registered in `Image.DECODERS`, don\'t use this class directly!\n  """"""\n\n  def __init__(self, mode, *args):\n    super(NV21Decoder, self).__init__(mode, *args)\n\n  def decode(self, buffer):\n    if self.mode == \'L\':\n      # discard UV channel\n      self.set_as_raw(buffer, \'L\')\n    else:\n      w, h = self.im.size\n      y = np.frombuffer(buffer, \'uint8\', count=w * h)\n      vu = np.frombuffer(buffer, \'uint8\', count=w * h // 2, offset=w * h)\n      y = np.reshape(y, [h, w, 1])\n      vu = np.reshape(vu, [h // 2, w // 2, 2])\n      vu = vu[np.arange(h) // 2][:, np.arange(w) // 2]\n      uv = vu[:, :, ::-1]\n      yuv = np.concatenate([y, uv], axis=-1)\n      self.set_as_raw(yuv.flatten().tobytes())\n    return -1, 0\n'"
VSR/DataLoader/Transform.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nfrom PIL import Image, ImageFilter, ImageEnhance\nimport numpy as np\n\n\nclass Transformer(object):\n  """"""Image transformer.\n\n  Args:\n      value: the parameter for each transform function.\n      random: if specify \'uniform\', generate value sampled from 0 to `+value`;\n              if specify \'normal\', generate value N~(mean=0, std=value)\n  """"""\n\n  def __init__(self, value=1, random=None):\n    self._v = value\n    self._r = random\n\n  @property\n  def value(self):\n    if self._r == \'uniform\':\n      return np.random.uniform(0, self._v)\n    elif self._r == \'normal\':\n      return np.random.normal(0, self._v)\n    else:\n      return self._v\n\n\nclass _Transformer1(Transformer):\n  def __call__(self, img: Image.Image):\n    assert isinstance(img, Image.Image)\n    return self.call(img)\n\n  def call(self, img):\n    raise NotImplementedError\n\n\nclass Tidy(_Transformer1):\n  def call(self, img: Image.Image):\n    scale = self.value\n    shape = np.array((img.width, img.height))\n    shape -= shape % scale\n    return img.crop([0, 0, *shape.tolist()])\n\n\nclass Bicubic(_Transformer1):\n  def call(self, img: Image.Image):\n    scale = self.value\n    shape = np.array((img.width, img.height))\n    if scale < 1:\n      rscale = int(1 / scale)\n      if np.any(shape % rscale):\n        raise ValueError(f""Image size is not divisible by {rscale}."")\n      return img.resize(shape // rscale, resample=Image.BICUBIC)\n    else:\n      return img.resize((shape * scale).astype(\'int32\'), resample=Image.BICUBIC)\n\n\nclass Brightness(_Transformer1):\n  def call(self, img: Image.Image):\n    brightness = max(0, self.value)\n    return ImageEnhance.Brightness(img).enhance(brightness)\n\n\nclass Contrast(_Transformer1):\n  def call(self, img: Image.Image):\n    contrast = self.value\n    return ImageEnhance.Contrast(img).enhance(contrast)\n\n\nclass Sharpness(_Transformer1):\n  def call(self, img):\n    sharp = min(max(0, self.value), 2)\n    return ImageEnhance.Sharpness(img).enhance(sharp)\n\n\nclass GaussianBlur(_Transformer1):\n  def call(self, img):\n    radius = self.value\n    return ImageFilter.GaussianBlur(radius).filter(img)\n\n\nclass _Transformer2(Transformer):\n  def __call__(self, img: np.ndarray):\n    assert isinstance(img, np.ndarray)\n    return self.call(img)\n\n  def call(self, img):\n    raise NotImplementedError\n\n\nclass GaussianWhiteNoise(_Transformer2):\n  def call(self, img):\n    shape = img.shape\n    noise = np.random.normal(0, self.value, shape)\n    noise += img.astype(\'float32\')\n    return np.clip(np.round(noise), 0, 255).astype(\'uint8\')\n\n\nclass FixedVideoLengthBatch(_Transformer2):\n  def call(self, img):\n    assert img.ndim == 5, f""img is not 5D, which is {img.ndim}""\n    depth = int(self.value)\n    shape = img.shape\n    if shape[1] <= depth:\n      return img\n    ret = []\n    for i in range(shape[1] - depth + 1):\n      ret.append(img[:, i * depth: (i + 1) * depth])\n    return np.stack(ret, 1).reshape([-1, depth, *shape[-3:]])\n'"
VSR/DataLoader/VirtualFile.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nfrom io import BytesIO, SEEK_CUR, SEEK_END, SEEK_SET\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\n\nfrom . import NVDecoder, YVDecoder\nfrom .FloDecoder import open_flo, KITTI\n\nImage.register_decoder(\'NV12\', NVDecoder.NV12Decoder)\nImage.register_decoder(\'NV21\', NVDecoder.NV21Decoder)\nImage.register_decoder(\'YV12\', YVDecoder.YV12Decoder)\nImage.register_decoder(\'YV21\', YVDecoder.YV21Decoder)\n\n\nclass File:\n  """"""An abstract file object\n\n  NOTE: If `path` is a file, `File` opens it and calculates its length.\n  If `path` is a folder, `File` opens every file in the folder in abc order.\n\n  Args:\n       path: path to a **node**, where node can be a **file** or a **folder**\n         contains multiple files.\n       rewind: rewind the file automatically when reaches EOF.\n  """"""\n\n  def __init__(self, path, rewind=False):\n    self.path = Path(path)\n    self.file = []\n    self.length = dict()\n    self.name = self.path.stem\n    self.full_name = self.path.absolute().as_posix()\n    if self.path.is_file():\n      self.file = [self.path]\n      self.length[self.path.name] = self.path.stat().st_size\n    elif self.path.is_dir():\n      for _file in self.path.glob(\'*\'):\n        self.file.append(_file)\n        self.length[_file.name] = _file.stat().st_size\n      # sort the files by name, because they are unordered in UNIX\n      self.file.sort()\n    self.file_ = self.file.copy()\n    self.read_file = []\n    self.read_pointer = 0\n    self.end_pointer = sum(self.length.values())\n    self.cur_fd = None\n    self.rewind = rewind\n\n  def __len__(self):\n    """"""total size of file or files""""""\n    return self.end_pointer\n\n  def _seek(self, target):\n    """"""seek to a target position of `File`\n\n    Args:\n        target: an int representing byte position.\n    """"""\n    assert 0 <= target < self.end_pointer\n    if self.read_pointer == target:\n      # do not need operation\n      return\n    if not self.cur_fd and self.file:\n      # guarantee an opened fd, happens if `File` is just initialized.\n      self.cur_fd = self.file[0].open(\'rb\')\n      self.read_file.append(self.file.pop(0))\n    while self.read_pointer < target:\n      # move forward\n      reminder = self.length.get(self.read_file[-1].name) - \\\n                 self.cur_fd.tell()\n      if self.read_pointer + reminder >= target:\n        self.cur_fd.seek(target - self.read_pointer, SEEK_CUR)\n        self.read_pointer = target\n        return\n      else:\n        self.read_pointer += reminder\n        self.cur_fd.close()\n        self.cur_fd = self.file[0].open(\'rb\')\n        self.read_file.append(self.file.pop(0))\n    while self.read_pointer > target:\n      # move backward\n      reminder = self.cur_fd.tell()\n      if self.read_pointer - reminder <= target:\n        self.cur_fd.seek(target - self.read_pointer, SEEK_CUR)\n        self.read_pointer = target\n        return\n      else:\n        self.read_pointer -= reminder\n        self.cur_fd.close()\n        self.cur_fd = self.read_file[-1].open(\'rb\')\n        self.file.insert(0, self.read_file.pop())\n\n  def reopen(self):\n    """"""clear the current state and re-initialize read pointer""""""\n    self.file = self.file_.copy()\n    self.read_file.clear()\n    self.read_pointer = 0\n    self.cur_fd = None\n\n  def split(self, depth):\n    raise NotImplementedError\n\n  def read(self, count=None):\n    """"""Read `count` bytes\n\n    Args:\n        count: size of bytes to read, if None (default),\n          read all bytes of current file\n\n    Return:\n        bytes: bytes read\n    """"""\n    if count == 0:\n      return b\'\'\n    if not self.cur_fd and self.file:\n      self.cur_fd = self.file[0].open(\'rb\')\n      self.read_file.append(self.file.pop(0))\n    elif not self.cur_fd:\n      if self.rewind and self.read_file:\n        self.reopen()\n        return self.read(count)\n      else:\n        raise EOFError(f\'End of File! {self.name}\')\n    read_bytes = self.cur_fd.read(count)\n    if read_bytes:\n      self.read_pointer += len(read_bytes)\n      if count and count > len(read_bytes):\n        return read_bytes + self.read(count - len(read_bytes))\n      elif count:\n        return read_bytes\n      else:\n        # read entire file, close the file descriptor\n        self.cur_fd.close()\n        self.cur_fd = None\n        return read_bytes\n    else:\n      if self.file:\n        self.cur_fd.close()\n        self.cur_fd = self.file[0].open(\'rb\')\n        self.read_file.append(self.file.pop(0))\n        return self.read(count)\n      elif self.rewind and self.read_file:\n        self.reopen()\n        return self.read(count)\n      else:\n        raise EOFError(f\'End of File! {self.name}\')\n\n  def read_frame(self, frames=1, *args):\n    """"""An abstract interface""""""\n    raise NotImplementedError\n\n  def seek(self, offset, where):\n    """"""Seek the position by `offset` relative to `where`.\n\n    Args:\n         offset: move the read pointer by `offset` bytes.\n         where: same as io.SEEK_END, io.SEEK_CUR or io.SEEK_SET.\n    """"""\n    if where == SEEK_SET:\n      self._seek(offset)\n    if where == SEEK_CUR:\n      self._seek(self.read_pointer + offset)\n    if where == SEEK_END:\n      self._seek(self.end_pointer + offset)\n\n  def tell(self):\n    """"""Tell the current position of the read pointer.""""""\n    return self.read_pointer\n\n  def size(self, name=None):\n    """"""Get the length of the file named `name`\n\n    Args:\n        name: specify a named file.\n\n    Return:\n        int: length in bytes\n    """"""\n    if name is None:\n      return sum(self.length.values())\n    path = Path(name)\n    name = path.stem if path.exists() else name\n    return self.length.get(name)\n\n\n# Supported RAW format, see \'FOURCC\' standard\n_ALLOWED_RAW_FORMAT = [\n  \'YV12\',  # [Y][U][V]\n  \'YV21\',  # [Y][V][U]\n  \'NV12\',  # [Y][UV]\n  \'NV21\',  # [Y][VU]\n  \'RGB\',  # [RGB]\n  \'BGR\',  # [BGR]\n  \'RGBA\',  # [RGBA]\n  \'BGRA\',  # [BGRA]\n]\n\n\nclass RawFile(File):\n  """"""For reading raw files. The file is lazy loaded, which means\n  the file is opened but not loaded into memory at initialization.\n\n  Args:\n       path: a string representing `node` path.\n       mode: a string, since raw file has no headers, type must be\n         explicitly given, see `_ALLOWED_RAW_FORMAT`.\n       size: a tuple of int (width, height). If `path` is a folder,\n         all files in it must be the same shape.\n       rewind: rewind the file automatically when reaches EOF\n\n  Raise:\n      TypeError: if `mode` is not supported\n  """"""\n\n  def __init__(self, path, mode, size, rewind=False):\n\n    if not mode.upper() in _ALLOWED_RAW_FORMAT:\n      raise TypeError(\'unknown mode: \' + mode)\n    self.mode = mode.upper()\n    self._size = size\n    self.pitch, self.channel_pitch = self._get_frame_pitch()\n    super(RawFile, self).__init__(path, rewind)\n    self._pair = None\n\n  def _get_frame_pitch(self):\n    """"""Get bytes length of one frame.\n    For the detail of mode fourcc, please see https://www.fourcc.org/\n\n    NOTE: RGB, BGR, and UV channel of NV12, NV21 is packed, while YV12 and\n      YV21 is planar, hence we have:\n      - **channel0** of YV12, YV21, NV12, NV21 if Y\n      - **channel1** of YV12 is U, YV21 is V, NV12 is UV, NV21 is VU\n      - **channel2** of YV12 is V, YV21 is U\n    """"""\n    mode = self.mode\n    w, h = self._size\n    if mode in (\'YV12\', \'YV21\'):\n      return h * w * 3 // 2, [h * w, h * w // 4, h * w // 4]\n    if mode in (\'NV12\', \'NV21\'):\n      return h * w * 3 // 2, [h * w, h * w // 2]\n    if mode in (\'RGB\', \'BGR\'):\n      return h * w * 3, [h * w * 3]\n    if mode in (\'RGBA\', \'BGRA\'):\n      return h * w * 4, [h * w * 4]\n\n  def _get_frame_channel_shape(self):\n    """"""Get each channel\'s shape according to mode and frame length.\n    For the detail of mode fourcc, please see https://www.fourcc.org/\n    """"""\n    mode = self.mode\n    w, h = self._size\n    if mode in (\'YV12\', \'YV21\'):\n      return (np.array([1, h, w]),\n              np.array([1, h // 2, w // 2]),\n              np.array([1, h // 2, w // 2]))\n    if mode in (\'NV12\', \'NV21\'):\n      return np.array([1, h, w]), np.array([2, h // 2, w // 2])\n    if mode in (\'RGB\', \'BGR\'):\n      return np.array([h, w, 3])\n    if mode in (\'RGBA\', \'BGRA\'):\n      return np.array([h, w, 4])\n\n  def read_frame(self, frames=1, *args):\n    """"""read number of `frames` of the file. A frame is a single image\n\n    Args:\n        frames: number of frames to be loaded.\n    """"""\n    if self.mode in (\'YV12\', \'YV21\', \'NV12\', \'NV21\',):\n      ret = []\n      for _ in range(frames):\n        data = self.read(self.pitch)\n        ret.append(Image.frombytes(\'YCbCr\', self._size, data, self.mode))\n      return ret\n    elif self.mode in (\'RGB\', \'RGBA\'):\n      ret = []\n      for _ in range(frames):\n        data = self.read(self.pitch)\n        ret.append(Image.frombytes(self.mode, self._size, data))\n      return ret\n    elif self.mode in (\'BGR\',):\n      _image_mode = \'RGB\'\n      ret = []\n      for _ in range(frames):\n        data = b\'\'.join(\n            (self.read(3)[::-1] for _ in range(self.pitch // 3)))\n        ret.append(Image.frombytes(\'RGB\', self._size, data))\n      return ret\n    elif self.mode in (\'BGRA\',):\n      ret = []\n      for _ in range(frames):\n        buf = bytes()\n        for _ in range(self.pitch // 4):\n          c = self.read(4)\n          buf.join((c[2::-1], c[3:]))\n        ret.append(Image.frombytes(\'RGBA\', self._size, buf))\n      return ret\n\n  def seek(self, offset, where=SEEK_SET):\n    """"""Seek the position by `offset` relative to `where`.\n\n    Args:\n         offset: move the read pointer by `offset` bytes.\n         where: same as io.SEEK_END, io.SEEK_CUR or io.SEEK_SET.\n    """"""\n    if where == SEEK_SET:\n      super(RawFile, self).seek(offset * self.pitch, where)\n    if where == SEEK_CUR:\n      super(RawFile, self).seek(\n          offset * self.pitch - self.tell() % self.pitch, where)\n    if where == SEEK_END:\n      super(RawFile, self).seek(offset * self.pitch, where)\n\n  def pad(self, padding):\n    """"""RawFile doesn\'t support pad for now""""""\n\n    print("" [!] warning: pad is not supported in RawFile"")\n\n  def attach_pair(self, pair_file):\n    self._pair = RawFile(pair_file, self.mode, self._size, self.rewind)\n    return self\n\n  @property\n  def pair(self):\n    return self._pair\n\n  @property\n  def shape(self):\n    return self._size\n\n  @property\n  def frames(self):\n    """"""frames in `RawFile`""""""\n    return self.end_pointer // self.pitch\n\n\nclass ImageFile(File):\n  """"""Open image file or a sequence of image frames\n\n  Args:\n      path: a string representing `node` path.\n      rewind: rewind the file when reaches EOF.\n  """"""\n\n  def __init__(self, path, rewind=False):\n    super(ImageFile, self).__init__(path, rewind)\n    self._flow = None\n    self._pair = None\n\n  def read_frame(self, frames=1, *args):\n    """"""read number `frames` of the file.\n\n    Args:\n        frames: number of frames to be loaded\n    """"""\n    image_bytes = [BytesIO(self.read()) for _ in range(frames)]\n    return [Image.open(fp) for fp in image_bytes]\n\n  def read_frame2(self, frames=1, *args):\n    """"""new API, saving memory while loading frames. But will consume a lot of\n    file descriptors.\n\n    Args:\n        frames: number of frames to be loaded\n    """"""\n    imgs = []\n    if frames == 0:\n      return imgs\n    while True:\n      if len(self.file) > 0:\n        cur_fd = self.file.pop(0)\n        imgs.append(Image.open(cur_fd))\n        self.read_file.append(cur_fd)\n        with open(cur_fd, \'rb\') as fd:\n          fd.seek(0, SEEK_END)\n          self.read_pointer += fd.tell()\n      elif self.rewind:\n        self.reopen()\n      else:\n        raise EOFError(\'End of File!\')\n      if len(imgs) == frames:\n        break\n    return imgs\n\n  def seek(self, offset, where=SEEK_SET):\n    """"""Seek the position by `offset` relative to `where`.\n\n    Args:\n        offset: move the read pointer by `offset` bytes.\n        where: same as io.SEEK_END, io.SEEK_CUR or io.SEEK_SET.\n    """"""\n    if where == SEEK_CUR:\n      cur = len(self.read_file)\n      pos = cur + offset\n    elif where == SEEK_END:\n      pos = len(self.read_file) + len(self.file) + offset\n    else:\n      pos = offset\n    if pos < 0:\n      pos = 0\n    self.file = self.read_file + self.file\n    self.read_file = self.file[:pos]\n    self.file = self.file[pos:]\n    self.cur_fd = None\n\n  def pad(self, padding):\n    """"""Pad file(s) list in the head and tail.\n      Padded file is temperate and will be dropped after `reopen()`\n\n    Args:\n      padding: a integer or a list of 2 integers.\n    """"""\n\n    if not isinstance(padding, (list, tuple)):\n      padding = [padding, padding]\n    else:\n      assert len(padding) is 2, f""Invalid padding, {padding}""\n    if self.read_file:\n      raise RuntimeError(\n          ""pad must be called when reading cursor is at the beginning."")\n    for _ in range(padding[0]):\n      self.file.insert(0, self.file[0])\n    for _ in range(padding[1]):\n      self.file.append(self.file[-1])\n\n  def attach_flow(self, flow_file):\n    self._flow = flow_file\n    return self\n\n  @property\n  def flow(self):\n    fd = Path(self._flow)\n    assert fd.exists(), f""{str(fd)} doesn\'t exist.""\n    if fd.suffix == \'.flo\':\n      flow = open_flo(str(fd))\n    elif fd.suffix == \'.png\':\n      flow = KITTI.open_flow(str(fd))\n    else:\n      raise TypeError(\'unsupported flow format\', fd.suffix)\n    return flow\n\n  def attach_pair(self, pair_file):\n    self._pair = ImageFile(pair_file, self.rewind)\n    return self\n\n  @property\n  def pair(self):\n    return self._pair\n\n  @property\n  def shape(self):\n    if self.file:\n      file = self.file[0]\n    else:\n      file = self.read_file[0]\n    with Image.open(file) as img:\n      return img.width, img.height\n\n  @property\n  def frames(self):\n    return len(self.file) + len(self.read_file)\n'"
VSR/DataLoader/YVDecoder.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\n# Image customized decoder for YV12([Y][U/4][V/4]), YV21([Y][V/4][U/4])\n# NOTE: [Y][U][V] means Y/U/V channel is a planar channel, [U/4] means\n#   U channel is sub-sampled by a factor of [2, 2]\n\nimport numpy as np\nfrom PIL import ImageFile\n\n\nclass YV12Decoder(ImageFile.PyDecoder):\n  """"""PIL.Image.DECODERS for YV12 format raw bytes\n\n  Registered in `Image.DECODERS`, don\'t use this class directly!\n  """"""\n\n  def __init__(self, mode, *args):\n    super(YV12Decoder, self).__init__(mode, *args)\n\n  def decode(self, buffer):\n    if self.mode == \'L\':\n      # discard UV channel\n      self.set_as_raw(buffer, \'L\')\n    else:\n      w, h = self.im.size\n      y = np.frombuffer(buffer, \'uint8\', count=w * h)\n      u = np.frombuffer(buffer, \'uint8\', count=w * h // 4, offset=w * h)\n      v = np.frombuffer(\n        buffer, \'uint8\', count=w * h // 4, offset=w * h + w * h // 4)\n      y = np.reshape(y, [h, w])\n      u = np.reshape(u, [h // 2, w // 2])\n      v = np.reshape(v, [h // 2, w // 2])\n      u = u[np.arange(h) // 2][:, np.arange(w) // 2]\n      v = v[np.arange(h) // 2][:, np.arange(w) // 2]\n      yuv = np.stack([y, u, v], axis=-1)\n      self.set_as_raw(yuv.flatten().tobytes())\n    return -1, 0\n\n\nclass YV21Decoder(ImageFile.PyDecoder):\n  """"""PIL.Image.DECODERS for YV21 format raw bytes\n\n  Registered in `Image.DECODERS`, don\'t use this class directly!\n  """"""\n\n  def __init__(self, mode, *args):\n    super(YV21Decoder, self).__init__(mode, *args)\n\n  def decode(self, buffer):\n    if self.mode == \'L\':\n      # discard UV channel\n      self.set_as_raw(buffer, \'L\')\n    else:\n      w, h = self.im.size\n      y = np.frombuffer(buffer, \'uint8\', count=w * h)\n      v = np.frombuffer(buffer, \'uint8\', count=w * h // 4, offset=w * h)\n      u = np.frombuffer(\n        buffer, \'uint8\', count=w * h // 4, offset=w * h + w * h // 4)\n      y = np.reshape(y, [h, w])\n      u = np.reshape(u, [h // 2, w // 2])\n      v = np.reshape(v, [h // 2, w // 2])\n      u = u[np.arange(h) // 2][:, np.arange(w) // 2]\n      v = v[np.arange(h) // 2][:, np.arange(w) // 2]\n      yuv = np.stack([y, u, v], axis=-1)\n      self.set_as_raw(yuv.flatten().tobytes())\n    return -1, 0\n'"
VSR/DataLoader/__init__.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nfrom .Crop import CenterCrop, RandomCrop\nfrom .Dataset import Container, Dataset, load_datasets\nfrom .Loader import Loader\nfrom .Transform import (\n  Bicubic, Brightness, Contrast, FixedVideoLengthBatch, GaussianBlur,\n  GaussianWhiteNoise, Sharpness\n)\n\n__all__ = [\n  'load_datasets',\n  'Dataset',\n  'Loader',\n  'CenterCrop',\n  'RandomCrop',\n  'Bicubic',\n  'Brightness',\n  'Contrast',\n  'FixedVideoLengthBatch',\n  'GaussianWhiteNoise',\n  'GaussianBlur',\n  'Sharpness'\n]\n"""
VSR/Model/__init__.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nfrom importlib import import_module\n\nfrom ..Backend import BACKEND\n\n__all__ = [\n  \'get_model\',\n  \'list_supported_models\'\n]\n\n\ndef get_model(name: str):\n  name = name.lower()\n  try:\n    if BACKEND == \'pytorch\':\n      return import_module(\'.Models\', \'VSR.Backend.Torch\').get_model(name)\n    elif BACKEND == \'tensorflow\':\n      return import_module(\'.Models\', \'VSR.Backend.TF\').get_model(name)\n    elif BACKEND == \'tensorflow2\':\n      pass\n  except (KeyError, ImportError):\n    raise ImportError(f""Using {BACKEND}, can\'t find model {name}."")\n\n\ndef list_supported_models():\n  if BACKEND == \'pytorch\':\n    return import_module(\'.Models\', \'VSR.Backend.Torch\').list_supported_models()\n  elif BACKEND == \'tensorflow\':\n    return import_module(\'.Models\', \'VSR.Backend.TF\').list_supported_models()\n  elif BACKEND == \'tensorflow2\':\n    pass\n'"
VSR/Util/Config.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport easydict\nimport yaml\n\ntry:\n  from yaml import FullLoader as _Loader\nexcept ImportError:\n  from yaml import Loader as _Loader\n\n\nclass Config(easydict.EasyDict):\n  def __init__(self, obj=None, **kwargs):\n    super(Config, self).__init__(**kwargs)\n    if obj is not None:\n      assert isinstance(obj, (dict, str))\n      if isinstance(obj, str):\n        with open(obj, 'r') as fd:\n          obj = yaml.load(fd, Loader=_Loader)\n      self.update(**obj)\n\n  def __getattr__(self, item):\n    return self.get(item)\n"""
VSR/Util/GoogleDriveDownloader.py,0,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 21st 2018\n\nDownload binary files shared on google drive\n""""""\n\nimport io\nimport sys\nfrom pathlib import Path\n\ntry:\n  from googleapiclient.discovery import build\n  from googleapiclient.http import MediaIoBaseDownload\n  from httplib2 import Http\n  from oauth2client import file, client, tools\nexcept ImportError as ex:\n  raise ImportError(\n    ""To download shared google drive file via python,""\n    ""google-api-python-client, oauth2client is required.""\n    ""Please use pip install google-api-python-client oauth2client."")\n\nSCOPES = \'https://www.googleapis.com/auth/drive.readonly\'\nCREDENTIALS = \'./Data/credentials.json\'\n\n\ndef require_authorize(store, credentials, scope):\n  _argv = sys.argv.copy()\n  sys.argv = _argv[:1]\n  if \'--noauth_local_webserver\' in _argv:\n    sys.argv.append(\'--noauth_local_webserver\')\n  flow = client.flow_from_clientsecrets(credentials, scope)\n  creds = tools.run_flow(flow, store)\n  sys.argv = _argv\n  return creds\n\n\ndef drive_download(name, fileid, path):\n  store_path = Path(path) / name\n  if store_path.exists():\n    print(""{} exists, skip download."".format(name))\n    return store_path\n  # The file token.json stores the user\'s access and refresh tokens, and is\n  # created automatically when the authorization flow completes for the first\n  # time.\n  store = file.Storage(\'/tmp/token.json\')\n  creds = store.get()\n  if not creds or creds.invalid:\n    creds = require_authorize(store, CREDENTIALS, SCOPES)\n  service = build(\'drive\', \'v3\', http=creds.authorize(Http()))\n\n  request = service.files().get_media(fileId=fileid)\n\n  fh = io.FileIO(store_path.resolve(), \'wb\')\n  downloader = MediaIoBaseDownload(fh, request)\n  done = False\n  while not done:\n    status, done = downloader.next_chunk()\n    print(""\\rDownload {}%."".format(int(status.progress() * 100)))\n  print(\'\\n\', flush=True)\n  if done:\n    return store_path\n'"
VSR/Util/Hook.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nfrom functools import partial\nfrom pathlib import Path\n\nfrom .ImageProcess import array_to_img\n\n\ndef _str_to_slice(index: str):\n  index = index.split(\':\')\n  if len(index) == 1:\n    ind = int(index[0])\n    if ind < 0:\n      sl = slice(ind, None, None)\n    else:\n      sl = slice(ind, ind + 1)\n  else:\n    def _maybe_int(x):\n      try:\n        return int(x)\n      except ValueError:\n        return None\n\n    sl = slice(*(_maybe_int(i) for i in index))\n  return sl\n\n\ndef _save_model_predicted_images(output, names, save_dir, index, auto_rename):\n  assert len(names) == 1, f""Name list exceeds 1, which is {names}""\n  name = names[0]\n  for img in output[_str_to_slice(index)]:\n    shape = img.shape\n    path = Path(save_dir)\n    if shape[0] > 1 or auto_rename:\n      path /= name\n    path.mkdir(exist_ok=True, parents=True)\n    for i, n in enumerate(img):\n      rep = 0\n      if auto_rename:\n        while (path / f""{name}_id{i:04d}_{rep:04d}.png"").exists():\n          rep += 1\n      path /= f""{name}_id{i:04d}_{rep:04d}.png""\n      array_to_img(n).convert(\'RGB\').save(str(path))\n  return output\n\n\ndef save_inference_images(save_dir, multi_output_index=-1, auto_rename=None):\n  return partial(_save_model_predicted_images, save_dir=save_dir,\n                 index=multi_output_index, auto_rename=auto_rename)\n'"
VSR/Util/ImageProcess.py,1,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport numpy as np\nfrom PIL import Image\n\nfrom ..Backend import DATA_FORMAT\n\n\ndef array_to_img(x: np.ndarray, mode=\'RGB\', data_format=None, min_val=0,\n                 max_val=255):\n  """"""Convert an ndarray to PIL Image.""""""\n\n  x = np.squeeze(x).astype(\'float32\')\n  x = (x - min_val) / (max_val - min_val)\n  x = x.clip(0, 1) * 255\n  if data_format not in (\'channels_first\', \'channels_last\'):\n    data_format = DATA_FORMAT\n  if np.ndim(x) == 2:\n    return Image.fromarray(x.astype(\'uint8\'), mode=\'L\').convert(mode)\n  elif np.ndim(x) == 3:\n    if data_format == \'channels_first\':\n      x = x.transpose([1, 2, 0])\n    return Image.fromarray(x.astype(\'uint8\'), mode=mode)\n  elif np.ndim(x) == 4:\n    if data_format == \'channels_first\':\n      x = x.transpose([0, 2, 3, 1])\n    ret = [Image.fromarray(np.round(i).astype(\'uint8\'), mode=mode) for i in x]\n    return ret.pop() if len(ret) is 1 else ret\n  elif np.ndim(x) >= 5:\n    raise ValueError(f""Dimension of x must <= 4. Got {np.ndim(x)}."")\n\n\ndef img_to_array(img, data_format=None):\n  """"""Converts a PIL Image instance to a Numpy array.\n\n    !!Copy from Keras!!\n\n    Assure the array\'s ndim is 3\n  """"""\n  if not isinstance(img, Image.Image):\n    return img\n  if data_format is None:\n    data_format = DATA_FORMAT\n  if data_format not in {\'channels_first\', \'channels_last\'}:\n    raise ValueError(\'Unknown data_format: \', data_format)\n  # Numpy array x has format (height, width, channel)\n  # or (channel, height, width)\n  # but original PIL image1 has format (width, height, channel)\n  x = np.asarray(img, dtype=np.uint8)\n  if len(x.shape) == 3:\n    if data_format == \'channels_first\':\n      x = x.transpose([2, 0, 1])\n  elif len(x.shape) == 2:\n    if data_format == \'channels_first\':\n      x = x.reshape((1, x.shape[0], x.shape[1]))\n    else:\n      x = x.reshape((x.shape[0], x.shape[1], 1))\n  else:\n    raise ValueError(\'Unsupported image1 shape: \', x.shape)\n  return x\n\n\ndef imresize(image, scale, size=None, mode=None, resample=None):\n  """"""Image resize using simple cubic provided in PIL""""""\n\n  def _resample(name: str):\n    if \'cubic\' in name:\n      return Image.BICUBIC\n    if \'linear\' in name:\n      return Image.BILINEAR\n    if \'nearest\' in name:\n      return Image.NEAREST\n    return 0\n\n  dtype = Image.Image\n  if isinstance(image, np.ndarray):\n    dtype = np.ndarray\n    mode = \'RGB\' or mode\n    image = array_to_img(image, mode)\n  if size is None:\n    size = (np.array(image.size) * scale).astype(int)\n  if image.mode in (\'RGB\', \'BGR\'):\n    image = image.convert(\'YCbCr\')\n  mode = image.mode if not mode else mode\n  if isinstance(resample, str):\n    resample = _resample(resample)\n  if not resample:\n    resample = Image.BICUBIC\n  image = image.resize(size, resample=resample).convert(mode)\n  if dtype is np.ndarray:\n    return img_to_array(image, DATA_FORMAT)\n  return image\n\n\ndef imread(url, mode=\'RGB\'):\n  """"""Read image from file to ndarray""""""\n\n  img = Image.open(url)\n  return img_to_array(img.convert(mode))\n\n\n_Y601 = (0.299, 0.587, 0.114)\n_Y709 = (0.2126, 0.7152, 0.0722)\n_UMAX = 0.436\n_VMAX = 0.615\n_U601 = (_Y601[0] / (_Y601[2] - 1), _Y601[1] / (_Y601[2] - 1), 1.0)\n_V601 = (1.0, _Y601[1] / (_Y601[0] - 1), _Y601[2] / (_Y601[0] - 1))\n_Y601 = np.array(_Y601, dtype=np.float32)\n_U601 = np.array(_U601, dtype=np.float32) * _UMAX\n_V601 = np.array(_V601, dtype=np.float32) * _VMAX\n_U709 = (_Y709[0] / (_Y709[2] - 1), _Y709[1] / (_Y709[2] - 1), 1.0)\n_V709 = (1.0, _Y709[1] / (_Y709[0] - 1), _Y709[2] / (_Y709[0] - 1))\n_Y709 = np.array(_Y709, dtype=np.float32)\n_U709 = np.array(_U709, dtype=np.float32) * _UMAX\n_V709 = np.array(_V709, dtype=np.float32) * _VMAX\n_T601 = np.stack([_Y601, _U601, _V601])\n_T709 = np.stack([_Y709, _U709, _V709])\n\n\ndef rgb_to_yuv(img, max_val=1.0, standard=\'bt601\'):\n  """"""convert rgb to yuv\n\n  There are plenty of rgb2yuv functions in python modules, but here we want to\n  make things more clearly.\n\n  Usually there are two standards: BT.601 and BT.709. While bt601 is the most\n  widely used (PIL, opencv, matlab\'s rgb2gray and also in the lovely\n  tf.image), somehow bt709 is not found used in any libs.\n  However, matlab\'s rgb2ycbcr uses different weights, which come from\n  C.A. Poynton. Most SR papers use matlab\'s rgb2ycbcr in benchmark,\n  because it gets the highest PSNR :)\n\n  Args:\n       img: a 3-D numpy array. If `dtype=uint8`, it ranges from [0, 255], if\n         `dtype=float`, it ranges from [0, 1]\n       max_val: a scalar representing range of the image value\n       standard: a string, should be one of (\'bt601\', \'bt709\', \'matlab\')\n\n  Return:\n      yuv image\n  """"""\n  _standard = standard.lower()\n  if _standard not in (\'bt601\', \'bt709\', \'matlab\'):\n    raise ValueError(\'Not known standard:\', standard)\n  if DATA_FORMAT == \'channels_first\':\n    img = img.transpose([1, 2, 0])\n  if img.shape[-1] != 3:\n    return img\n  """""" matrix used in matlab\n    yuv = _trans * rgb + _bias\n  """"""\n  _trans = np.array([[65.481, 128.553, 24.966], [-37.797, -74.203, 112],\n                     [112, -93.786, -18.214]], dtype=np.float32)\n  _bias = np.array([16, 128, 128], dtype=np.float32)\n  _trans /= 255\n  _bias /= 255\n  _img = img.reshape([-1, 3]) / max_val\n  if _standard == \'bt601\':\n    _yuv = np.matmul(_T601, _img.transpose())\n    _yuv = _yuv.transpose() + np.array([0, 0.5, 0.5])\n    _yuv = _yuv.reshape(img.shape)\n  elif _standard == \'bt709\':\n    _yuv = np.matmul(_T709, _img.transpose())\n    _yuv = _yuv.transpose() + np.array([0, 0.5, 0.5])\n    _yuv = _yuv.reshape(img.shape)\n  else:\n    _yuv = np.matmul(_trans, _img.transpose())\n    _yuv = _yuv.transpose() + _bias\n    _yuv = _yuv.reshape(img.shape)\n  _yuv = np.clip(_yuv, 0, 1) * max_val\n  if DATA_FORMAT == \'channels_first\':\n    _yuv = _yuv.transpose([2, 0, 1])\n  return _yuv.astype(img.dtype)\n'"
VSR/Util/LearningRateScheduler.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\nfrom functools import partial\n\n\ndef _exponential_decay(start_lr, steps, decay_step, decay_rate, **kwargs):\n  return start_lr * decay_rate ** (steps / decay_step)\n\n\ndef _poly_decay(start_lr, end_lr, steps, decay_step, power, **kwargs):\n  return (start_lr - end_lr) * (1 - steps / decay_step) ** power + end_lr\n\n\ndef _stair_decay(start_lr, steps, decay_step, decay_rate, **kwargs):\n  return start_lr * decay_rate ** (steps // decay_step)\n\n\ndef _multistep_decay(start_lr, steps, decay_step, decay_rate, **kwargs):\n  if not decay_step:\n    return start_lr\n  for n, s in enumerate(decay_step):\n    if steps <= s:\n      return start_lr * (decay_rate ** n)\n  if steps > decay_step[-1]:\n    return start_lr * (decay_rate ** len(decay_step))\n\n\ndef lr_decay(method, lr, **kwargs):\n  if method == 'exp':\n    return partial(_exponential_decay, start_lr=lr, **kwargs)\n  elif method == 'poly':\n    return partial(_poly_decay, start_lr=lr, **kwargs)\n  elif method == 'stair':\n    return partial(_stair_decay, start_lr=lr, **kwargs)\n  elif method == 'multistep':\n    return partial(_multistep_decay, start_lr=lr, **kwargs)\n  else:\n    print('invalid decay method!')\n    return None\n"""
VSR/Util/Math.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 12\n\nimport numpy as np\n\nfrom .Utility import to_list\n\n\ndef gaussian_kernel(kernel_size: (int, tuple, list), width: float):\n  """"""generate a gaussian kernel\n\n  Args:\n      kernel_size: the size of generated gaussian kernel. If is a scalar, the\n                   kernel is a square matrix, or it\'s a kernel of HxW.\n      width: the standard deviation of gaussian kernel. If width is 0, the\n             kernel is identity, if width reaches to +inf, the kernel becomes\n             averaging kernel.\n  """"""\n\n  kernel_size = np.asarray(to_list(kernel_size, 2), np.float)\n  half_ksize = (kernel_size - 1) / 2.0\n  x, y = np.mgrid[-half_ksize[0]:half_ksize[0] + 1,\n         -half_ksize[1]:half_ksize[1] + 1]\n  kernel = np.exp(-(x ** 2 + y ** 2) / (2 * width ** 2))\n  return kernel / (kernel.sum() + 1e-8)\n\n\ndef anisotropic_gaussian_kernel(kernel_size: (int, tuple, list), theta: float,\n                                l1: float, l2: float):\n  """"""generate anisotropic gaussian kernel\n\n  Args:\n      kernel_size: the size of generated gaussian kernel. If is a scalar, the\n                   kernel is a square matrix, or it\'s a kernel of HxW.\n      theta: rotation angle (rad) of the kernel. [0, pi]\n      l1: scaling of eigen values on base 0. [0.1, 10]\n      l2: scaling of eigen values on base 1. [0.1, L1]\n  """"""\n\n  def gmdistribution(mu, sigma):\n    half_k = (kernel_size - 1) / 2.0\n    x, y = np.mgrid[-half_k[0]:half_k[0] + 1, -half_k[1]:half_k[1] + 1]\n    X = np.expand_dims(np.stack([y, x], axis=-1), axis=-2)\n    L = np.linalg.cholesky(sigma).transpose()\n    diag_l = np.diag(L)\n    log_det_sigma = 2 * np.log(diag_l).sum()\n    log_1h = np.sum(np.matmul((X - mu), np.linalg.inv(L)) ** 2, axis=(-1, -2))\n    log_1h = -0.5 * (log_1h + log_det_sigma)\n    log_1h -= np.log(2 * np.pi)\n    y = np.exp(log_1h)\n    return y\n\n  kernel_size = np.array(to_list(kernel_size, 2), np.int)\n  theta = np.clip(theta, 0, np.pi)\n  l1 = np.clip(l1, 0.1, 10)\n  l2 = np.clip(l2, 0.1, l1)\n  mat_v = np.array([\n    [np.cos(theta), np.sin(theta)],\n    [np.sin(theta), -np.cos(theta)]\n  ], np.float)\n  mat_d = np.array([[l1, 0], [0, l2]], np.float)\n  sigma = np.matmul(np.matmul(mat_v, mat_d), np.linalg.inv(mat_v))\n  kernel = gmdistribution(0, sigma)\n  return kernel / (kernel.sum() + 1e-8)\n\n\ndef list_rshift(l, s):\n  for _ in range(s):\n    l.insert(0, l.pop(-1))\n  return l\n\n\ndef bicubic_filter(x, a=-0.5):\n  # https://en.wikipedia.org/wiki/Bicubic_interpolation#Bicubic_convolution_algorithm\n  if x < 0:\n    x = -x\n  if x < 1:\n    return ((a + 2.0) * x - (a + 3.0)) * x * x + 1\n  if x < 2:\n    return (((x - 5) * x + 8) * x - 4) * a\n  return 0\n\n\ndef weights_downsample(scale_factor):\n  if scale_factor < 1:\n    ss = int(1 / scale_factor + 0.5)\n  else:\n    ss = int(scale_factor + 0.5)\n  support = 2 * ss\n  ksize = support * 2 + 1\n  weights = []\n  for lambd in range(ksize):\n    dist = -2 + (2 * lambd + 1) / support\n    weights.append(bicubic_filter(dist))\n  h = np.array([weights])\n  h /= h.sum()\n  v = h.transpose()\n  kernel = np.matmul(v, h)\n  assert kernel.shape == (ksize, ksize), f""{kernel.shape} != [{ksize}]""\n  return kernel, ss\n\n\ndef weights_upsample(scale_factor):\n  if scale_factor < 1:\n    ss = int(1 / scale_factor + 0.5)\n  else:\n    ss = int(scale_factor + 0.5)\n  support = 2\n  ksize = support * 2 + 1\n  weights = [[] for _ in range(ss)]\n  for i in range(ss):\n    for lambd in range(ksize):\n      dist = int((1 + ss + 2 * i) / 2 / ss) + lambd - 1.5 - (2 * i + 1) / 2 / ss\n      weights[i].append(bicubic_filter(dist))\n  w = [np.array([i]) / np.sum(i) for i in weights]\n  w = list_rshift(w, ss - ss // 2)\n  kernels = []\n  for i in range(len(w)):\n    for j in range(len(w)):\n      kernels.append(np.matmul(w[i].transpose(), w[j]))\n  return kernels, ss\n\n\ndef nd_meshgrid(*size, permute=None):\n  _error_msg = (""Permute index must match mesh dimensions, ""\n                ""should have {} indexes but got {}"")\n  size = np.array(size)\n  ranges = []\n  for x in size:\n    ranges.append(np.linspace(-1, 1, x))\n  mesh = np.stack(np.meshgrid(*ranges, indexing=\'ij\'))\n  if permute is not None:\n    if len(permute) != len(size):\n      raise ValueError(_error_msg.format(len(size), len(permute)))\n    mesh = mesh[permute]\n  return mesh.transpose(*range(1, mesh.ndim), 0)\n\n\ndef camera_response_function(inputs, crf_table, max_val=1):\n  """"""Estimated CRF, transform irradiance L to RGB image. If `crf_table` is\n    inverted, transform RGB image to irradiance L.\n\n  Args:\n      inputs: A 3-D or 4-D tensor, representing irradiance.\n      crf_table: CRF lookup table shape (1024,).\n      max_val: specify the range of inputs: in (0, max_val)\n  Return:\n      RGB images (or L) with the same shape as inputs, in range [0, max_val].\n  """"""\n\n  inputs_norm = np.clip(inputs / max_val, 0, 1)\n  quant = crf_table.shape[0] - 1\n  inputs_index = (inputs_norm * quant).astype(\'int32\')\n  ret = []\n  for i in inputs_index.flatten():\n    ret.append(crf_table[i])\n  return np.reshape(ret, inputs.shape)\n'"
VSR/Util/Utility.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport logging\nfrom typing import Generator\n\nfrom .Config import Config\n\nLOG = logging.getLogger(\'VSR.Util\')\n\n\ndef to_list(x, repeat=1):\n  """"""convert x to list object\n\n    Args:\n       x: any object to convert\n       repeat: if x is to make as [x], repeat `repeat` elements in the list\n  """"""\n  if isinstance(x, (Generator, tuple, set)):\n    return list(x)\n  elif isinstance(x, list):\n    return x\n  elif isinstance(x, dict):\n    return list(x.values())\n  elif x is not None:\n    return [x] * repeat\n  else:\n    return []\n\n\ndef str_to_bytes(s):\n  """"""convert string to byte unit. Case insensitive.\n\n  >>> str_to_bytes(\'2GB\')\n    2147483648\n  >>> str_to_bytes(\'1kb\')\n    1024\n  """"""\n  s = s.replace(\' \', \'\')\n  if s[-1].isalpha() and s[-2].isalpha():\n    _unit = s[-2:].upper()\n    _num = s[:-2]\n  elif s[-1].isalpha():\n    _unit = s[-1].upper()\n    _num = s[:-1]\n  else:\n    return float(s)\n  if not _unit in (\'B\', \'KB\', \'MB\', \'GB\', \'TB\', \'PB\', \'EB\', \'ZB\', \'YB\'):\n    raise ValueError(\'invalid unit\', _unit)\n  carry = {\n    \'B\': 1,\n    \'KB\': 1024,\n    \'MB\': 1024 ** 2,\n    \'GB\': 1024 ** 3,\n    \'TB\': 1024 ** 4,\n    \'PB\': 1024 ** 5,\n    \'EB\': 1024 ** 6,\n    \'ZB\': 1024 ** 7,\n    \'YB\': 1024 ** 8\n  }\n  return float(_num) * carry[_unit]\n\n\ndef cross_type_assign(value, dtype):\n  """"""Convert `value` to `dtype`.\n    Usually this can be done by simply `dtype(value)`, however, this ain\'t\n    correct for str -> bool conversion.\n  """"""\n\n  if dtype is bool and isinstance(value, str):\n    if value.lower() == \'false\':\n      return False\n    elif value.lower() == \'true\':\n      return True\n    else:\n      LOG.warning(\n          ""suspect wrong typo {}, do you mean true/false?"".format(value))\n      return True\n  return dtype(value)\n\n\ndef suppress_opt_by_args(opt, *args):\n  """"""Use cmdline arguments to overwrite tf declared in yaml file.\n    Account for safety, writing section not declared in yaml is not allowed.\n  """"""\n\n  def parse_args(argstr: str, prev_argstr: str):\n    if prev_argstr:\n      k, v = prev_argstr, argstr\n    elif argstr.startswith(\'--\'):\n      if \'=\' in argstr:\n        k, v = argstr[2:].split(\'=\')\n      else:\n        k = argstr[2:]\n        v = None\n    elif argstr.startswith(\'-\'):\n      if \'=\' in argstr:\n        k, v = argstr[1:].split(\'=\')\n      else:\n        k = argstr[1:]\n        v = None\n    else:\n      raise KeyError(""Unknown parameter: {}"".format(argstr))\n    return k, v\n\n  prev_arg = None\n  for arg in args:\n    key, value = parse_args(arg, prev_arg)\n    prev_arg = None  # clear after use\n    if key and value:\n      # dict support\n      keys = key.split(\'.\')\n      if keys[0] not in opt:\n        raise KeyError(""Parameter {} doesn\'t exist in model!"".format(key))\n      old_v = opt.get(keys[0])\n      if isinstance(old_v, (list, tuple)):\n        # list, tuple support\n        if not value.startswith(\'[\') and not value.startswith(\'(\'):\n          raise TypeError(""Invalid list syntax: {}"".format(value))\n        if not value.endswith(\']\') and not value.endswith(\')\'):\n          raise TypeError(""Invalid list syntax: {}"".format(value))\n        values = value[1:-1].split(\',\')\n        if len(values) == 1 and values[0] == \'\':\n          # empty list\n          values = []\n        new_v = [cross_type_assign(nv, type(ov)) for ov, nv in\n                 zip(old_v, values)]\n        opt[keys[0]] = new_v\n      elif isinstance(old_v, dict):\n        # dict support\n        try:\n          for k in keys[1:-1]:\n            old_v = old_v[k]\n          ref_v = old_v\n          old_v = old_v[keys[-1]]\n        except KeyError:\n          raise KeyError(""Parameter {} doesn\'t exist in model!"".format(key))\n        if isinstance(old_v, (list, tuple)):\n          raise NotImplementedError(""Don\'t support nested list type."")\n        new_v = cross_type_assign(value, type(old_v))\n        ref_v[keys[-1]] = new_v\n      else:\n        new_v = cross_type_assign(value, type(old_v))\n        opt[keys[0]] = new_v\n    elif key:\n      prev_arg = key\n\n  if prev_arg:\n    raise KeyError(""Parameter missing value: {}"".format(prev_arg))\n\n\ndef compat_param(par):\n  from ..Backend import DATA_FORMAT\n\n  _par = Config(par)\n  for i in par:\n    if isinstance(par[i], (dict, Config)):\n      _par.update(**par[i])\n  if \'batch_shape\' not in _par:\n    assert \'batch\' in _par and \'patch_size\' in _par and \'channel\' in _par\n    batch = _par.batch\n    channel = _par.channel\n    patch_size = _par.patch_size\n    patch_size = [x // _par.scale for x in to_list(patch_size, 2)]\n    batch_shape = [batch, *patch_size]\n    if DATA_FORMAT == \'channels_last\':\n      batch_shape.append(channel)\n    else:\n      batch_shape.insert(1, channel)\n    if \'depth\' in _par and _par.depth != 1:\n      batch_shape.insert(1, _par.depth)\n    par.batch_shape = batch_shape\n    delattr(par, \'batch\')\n    delattr(par, \'patch_size\')\n  return par\n'"
VSR/Util/VisualizeOpticalFlow.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport numpy as np\nfrom .ImageProcess import array_to_img\nfrom ..Backend import DATA_FORMAT\n\n\ndef _color_wheel():\n  red_yellow, yellow_green, green_cyan = 15, 6, 4\n  cyan_blue, blue_magenta, magenta_red = 11, 13, 6\n  colors = [red_yellow, yellow_green, green_cyan,\n            cyan_blue, blue_magenta, magenta_red]\n  color = np.zeros([np.sum(colors), 3])\n  for i in range(red_yellow):\n    color[i] = [255, 255 * i / red_yellow, 0]\n  for i in range(yellow_green):\n    color[i + np.sum(colors[:1])] = [255 - 255 * i / yellow_green, 255, 0]\n  for i in range(green_cyan):\n    color[i + np.sum(colors[:2])] = [0, 255, 255 * i / green_cyan]\n  for i in range(cyan_blue):\n    color[i + np.sum(colors[:3])] = [0, 255 - 255 * i / cyan_blue, 255]\n  for i in range(blue_magenta):\n    color[i + np.sum(colors[:4])] = [255 * i / blue_magenta, 0, 255]\n  for i in range(magenta_red):\n    color[i + np.sum(colors[:5])] = [255, 0, 255 - 255 * i / magenta_red]\n  return color / 255\n\n\ndef _viz_flow(u, v, logscale=True, scaledown=6):\n  """"""\n  Copied from @jswulff:\n    https://github.com/jswulff/pcaflow/blob/master/pcaflow/utils/viz_flow.py\n\n  top_left is zero, u is horizon, v is vertical\n  red is 3 o\'clock, yellow is 6, light blue is 9, blue/purple is 12\n  """"""\n  color_wheel = _color_wheel()\n  n_cols = color_wheel.shape[0]\n\n  radius = np.sqrt(u ** 2 + v ** 2)\n  if logscale:\n    radius = np.log(radius + 1)\n  radius = radius / scaledown\n  rot = np.arctan2(-v, -u) / np.pi\n\n  fk = (rot + 1) / 2 * (n_cols - 1)  # -1~1 mapped to 0~n_cols\n  k0 = fk.astype(np.uint8)  # 0, 1, 2, ..., n_cols\n\n  k1 = k0 + 1\n  k1[k1 == n_cols] = 0\n\n  f = fk - k0\n\n  n_colors = color_wheel.shape[1]\n  img = np.zeros(u.shape + (n_colors,))\n  for i in range(n_colors):\n    tmp = color_wheel[:, i]\n    col0 = tmp[k0]\n    col1 = tmp[k1]\n    col = (1 - f) * col0 + f * col1\n\n    idx = radius <= 1\n    # increase saturation with radius\n    col[idx] = 1 - radius[idx] * (1 - col[idx])\n    # out of range\n    col[~idx] *= 0.75\n    img[:, :, i] = np.floor(255 * col).astype(np.uint8)\n\n  return img.astype(np.uint8)\n\n\ndef visualize_flow(flow, v=None):\n  if DATA_FORMAT == \'channels_last\':\n    u = flow[..., 0] if v is None else flow\n    v = flow[..., 1] if v is None else v\n  else:\n    u = flow[0, :, :] if v is None else flow\n    v = flow[1, :, :] if v is None else v\n  viz = _viz_flow(u, v)\n  return array_to_img(viz, \'RGB\')\n'"
VSR/Util/__init__.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nfrom .Config import Config\nfrom .Hook import save_inference_images\nfrom .ImageProcess import (\n  array_to_img, img_to_array, imread, imresize, rgb_to_yuv\n)\nfrom .LearningRateScheduler import lr_decay\nfrom .Utility import (str_to_bytes, suppress_opt_by_args, to_list, compat_param)\n\n\n__all__ = [\n  'Config',\n  'lr_decay',\n  'str_to_bytes',\n  'suppress_opt_by_args',\n  'to_list',\n  'array_to_img',\n  'imresize',\n  'imread',\n  'img_to_array',\n  'rgb_to_yuv',\n  'save_inference_images',\n  'compat_param',\n]\n"""
VSR/Backend/TF/Util.py,131,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 8th 2018\nUpdated Date: May 8th 2018\n\nutility functions\n""""""\n\nimport logging\n\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom VSR.Util.Math import weights_downsample, weights_upsample\n\nLOG = logging.getLogger(\'VSR.TF.Util\')\n\n\ndef shrink_mod_scale(x, scale):\n  """"""clip each dim of x to multiple of scale""""""\n  return [_x - _x % _s for _x, _s in zip(x, to_list(scale, 2))]\n\n\ndef repeat(x, n):\n  """"""Repeats a 2D tensor. Copy from Keras\n\n  if `x` has shape (samples, dim) and `n` is `2`,\n  the output will have shape `(samples, 2, dim)`.\n\n    Args:\n      x: Tensor or variable.\n      n: Python integer, number of times to repeat.\n\n    Returns:\n      A tensor.\n  """"""\n  x = tf.expand_dims(x, 1)\n  pattern = tf.stack([1, n, 1])\n  return tf.tile(x, pattern)\n\n\ndef pixel_shift(image, scale, channel=1):\n  """"""Efficient Sub-pixel Convolution,\n    see paper: https://arxiv.org/abs/1609.05158\n\n    Args:\n        image: A 4-D tensor of [N, H, W, C*scale[0]*scale[1]]\n        scale: A scalar or 1-D tensor with 2 elements, the scale factor for\n          width and height respectively\n        channel: specify the channel number\n\n    Return:\n        A 4-D tensor of [N, H*scale[1], W*scale[0], C]\n  """"""\n\n  with tf.name_scope(\'PixelShift\'):\n    r = to_list(scale, 2)\n    shape = tf.shape(image)\n    h, w = shape[1], shape[2]\n    image = tf.reshape(image, [-1, h, w, r[1], r[0], channel])\n    image = tf.transpose(image, perm=[0, 1, 3, 2, 4, 5])  # B, H, r, W, r, C\n    image = tf.reshape(image, [-1, h * r[1], w * r[0], channel])\n    return image\n\n\ndef crop_to_batch(image, scale):\n  """"""Crop image into `scale[0]*scale[1]` parts, and concat into batch dimension\n\n    Args:\n        image: A 4-D tensor with [N, H, W, C]\n        scale: A 1-D tensor or scalar of scale factor for width and height\n  """"""\n\n  scale = to_list(scale, 2)\n  with tf.name_scope(\'BatchEnhance\'):\n    hs = tf.split(image, scale[1], axis=1)\n    image = tf.concat(hs, axis=0)\n    rs = tf.split(image, scale[0], axis=2)\n    return tf.concat(rs, axis=0)\n\n\ndef bicubic_rescale(img, scale):\n  """"""Resize image in tensorflow.\n\n  NOTE: tf.image.resize_bicubic uses different boundary to PIL.Image.resize,\n    try to use resize_area without aligned corners.\n  """"""\n  print(""bicubic_rescale is deprecated. Use bicubic_resize instead."")\n  with tf.name_scope(\'Bicubic\'):\n    shape = tf.shape(img)\n    scale = to_list(scale, 2)\n    shape_enlarge = tf.to_float(shape) * [1, *scale, 1]\n    shape_enlarge = tf.to_int32(shape_enlarge)\n    return tf.image.resize_bicubic(img, shape_enlarge[1:3], False)\n\n\ndef _push_shape_4d(x):\n  shape = tf.shape(x)\n  if shape.shape[0] == 2:\n    return tf.reshape(x, [1, shape[0], shape[1], 1]), 2\n  elif shape.shape[0] == 3:\n    return tf.expand_dims(x, 0), 3\n  elif shape.shape[0] == 4:\n    return x, 4\n  else:\n    raise ValueError(""Unsupported tensor! Must be 2D/3D/4D"")\n\n\ndef _pop_shape(x, shape):\n  if shape == 2:\n    return x[0, ..., 0]\n  elif shape == 3:\n    return x[0]\n  elif shape == 4:\n    return x\n  else:\n    raise ValueError(""Unsupported shape! Must be 2/3/4"")\n\n\ndef downsample(img, scale, border=\'REFLECT\'):\n  """"""Bicubical downsample via **CONV2D**. Using PIL\'s kernel.\n\n  Args:\n    img: a tf tensor of 2/3/4-D.\n    scale: n or 1/n. `n` must be integer >= 2.\n    border: padding mode. Recommend to \'REFLECT\'.\n  """"""\n  kernel, s = weights_downsample(scale)\n  if s == 1:\n    return img  # bypass\n  kernel = tf.convert_to_tensor(kernel, dtype=\'float32\')\n  p1 = int(s * 3 / 2)\n  p2 = 4 * s - int(s * 3 / 2)\n  img, shape = _push_shape_4d(img)\n  img_ex = tf.pad(img, [[0, 0], [p1, p2], [p1, p2], [0, 0]], border)\n  c = img_ex.shape[-1]\n  assert c is not None, ""img must define channel number""\n  c = int(c)\n  filters = tf.reshape(tf.eye(c, c), [c, c, 1, 1]) * kernel\n  filters = tf.transpose(filters, [2, 3, 0, 1])\n  img_s = tf.nn.conv2d(img_ex, filters, [1, s, s, 1], \'VALID\')\n  img_s = _pop_shape(img_s, shape)\n  return img_s\n\n\ndef upsample(img, scale, border=\'REFLECT\'):\n  """"""Bicubical upsample via **CONV2D**. Using PIL\'s kernel.\n\n  Args:\n    img: a tf tensor of 2/3/4-D.\n    scale: must be integer >= 2.\n    border: padding mode. Recommend to \'REFLECT\'.\n  """"""\n  kernels, s = weights_upsample(scale)\n  if s == 1:\n    return img  # bypass\n  kernels = [tf.convert_to_tensor(k, dtype=\'float32\') for k in kernels]\n  p1 = 1 + s // 2\n  p2 = 3\n  img, shape = _push_shape_4d(img)\n  img_ex = tf.pad(img, [[0, 0], [p1, p2], [p1, p2], [0, 0]], border)\n  c = img_ex.shape[-1]\n  assert c is not None, ""img must define channel number""\n  c = int(c)\n  filters = [tf.reshape(tf.eye(c, c), [c, c, 1, 1]) * k for k in kernels]\n  filters = [tf.transpose(f, [2, 3, 0, 1]) for f in filters]\n  weights = tf.concat(filters, axis=-1)\n  img_s = tf.nn.conv2d(img_ex, weights, [1, 1, 1, 1], \'VALID\')\n  img_s = tf.nn.depth_to_space(img_s, s)\n  more = s // 2 * s\n  crop = slice(more - s // 2, - (s // 2))\n  img_s = _pop_shape(img_s[:, crop, crop], shape)\n  return img_s\n\n\ndef bicubic_resize(img, scale, border=\'REFLECT\'):\n  with tf.name_scope(\'Bicubic\'):\n    if scale > 1:\n      return upsample(img, scale, border)\n    elif 0 < scale < 1:\n      return downsample(img, scale, border)\n    elif scale == 1:\n      return img\n    else:\n      raise ValueError(""Wrong scale factor!"")\n\n\ndef prelu(x, initialize=0, name=None, scope=\'PReLU\'):\n  """"""Parametric ReLU""""""\n  with tf.variable_scope(name, scope):\n    alphas = tf.get_variable(\n        \'Variable\', shape=(x.shape[-1],), dtype=\'float32\',\n        initializer=tf.initializers.constant(initialize))\n    return tf.nn.relu(x) + tf.multiply(alphas, (x - tf.abs(x))) * 0.5\n\n\ndef imfilter(image, kernel, name=None):\n  """"""Image filter""""""\n  with tf.variable_scope(\'imfilter\'):\n    _c = image.shape[-1]\n    _k = tf.expand_dims(kernel, -1)\n    _k = tf.expand_dims(_k, -1)\n    _p = tf.zeros_like(_k)\n    _m = []\n    # apply kernel to each channel separately\n    for i in range(_c):\n      t = [_p] * _c\n      t[i] = _k\n      _m.append(tf.concat(t, -1))\n    _k = tf.concat(_m, -2)\n    _k = tf.cast(_k, dtype=image.dtype)\n    return tf.nn.conv2d(image, _k, strides=[1, 1, 1, 1], padding=\'SAME\',\n                        name=name)\n\n\ndef pixel_norm(images, epsilon=1.0e-8, scale=1.0, bias=0):\n  """"""Pixel normalization.\n\n  For each pixel a[i,j,k] of image in HWC format, normalize its value to\n  b[i,j,k] = a[i,j,k] / SQRT(SUM_k(a[i,j,k]^2) / C + eps).\n\n  Args:\n    images: A 4D `Tensor` of NHWC format.\n    epsilon: A small positive number to avoid division by zero.\n    scale: scale the normalized value\n    bias: add bias to output\n\n  Returns:\n    A 4D `Tensor` with pixel-wise normalized channels.\n  """"""\n  return images * scale * tf.rsqrt(\n      tf.reduce_mean(tf.square(images), axis=3,\n                     keepdims=True) + epsilon) + bias\n\n\ndef color_consistency(feature, label, lambd=5):\n  """"""Color consistency regularization (from StackGAN++)\n\n  See: https://arxiv.org/abs/1710.10916\n  """"""\n\n  m1 = tf.reduce_mean(feature, [1, 2], True)\n  m2 = tf.reduce_mean(label, [1, 2], True)\n  shape = tf.shape(feature)\n  b = shape[0]\n  h, w = shape[1], shape[2]\n  c = shape[3]\n  f_hat = tf.reshape(feature - m1, [b, -1, c])\n  l_hat = tf.reshape(label - m2, [b, -1, c])\n  c1 = tf.matmul(f_hat, f_hat, True) / tf.cast(h * w, tf.float32)\n  c2 = tf.matmul(l_hat, l_hat, True) / tf.cast(h * w, tf.float32)\n  cc = tf.losses.mean_squared_error(m1, m2) + \\\n       tf.losses.mean_squared_error(c1, c2, lambd)\n  return cc\n\n\ndef pop_dict_wo_keyerror(d, key):\n  value = d.get(key)\n  if value is not None:\n    d.pop(key)\n  return value\n\n\ndef summary_tensor_image(x, name, reshape=None):\n  """"""summary a tensor\n\n  split each channel and reshape into a huge image\n\n  Args:\n      x: a 4-D tensor of any shape. Note the channel numbers have to be\n        divided by 2^n, or `reshape` must be set to inform how to divide it.\n      name: specify a name in summary.\n      reshape: a tuple of 2 integers, if not None, representing columns and\n        rows of feature maps.\n  """"""\n\n  shape = tf.shape(x)\n  channel = x.get_shape().as_list()[-1]\n  # tensor has to be 4-D\n  assert len(x.get_shape()) == 4, \\\n    f\'Dimension of x must be 4, but is {len(x.get_shape())}\'\n\n  def _placement(t):\n    if t <= 16:\n      return 4, t // 4\n    elif t <= 64:\n      return 8, t // 8\n    elif t <= 256:\n      return 16, t // 16\n    else:\n      return 32, t // 32\n\n  if reshape:\n    col, row = reshape\n  else:\n    col, row = _placement(channel)\n  x = tf.reshape(x, [shape[0], shape[1], shape[2], row, col])\n  x = tf.transpose(x, [0, 3, 1, 4, 2])\n  x = tf.reshape(x, [-1, 1, shape[1] * row, 1, shape[2] * col])\n  x = tf.squeeze(x, [1, 3])\n  x = tf.expand_dims(x, -1)\n\n  tf.summary.image(name, x)\n  return x\n\n\ndef _make_vector(x, patch=3, stride=1):\n  """"""[B, H, W, C]->[B, H, W, c*k1*k2]""""""\n  k1, k2 = to_list(patch, 2)\n  h, w = tf.shape(x)[1], tf.shape(x)[2]\n  padded_x = tf.pad(x, [[0, 0], [k1 // 2] * 2, [k2 // 2] * 2, [0, 0]])\n  vec = []\n  for i in range(k1):\n    for j in range(k2):\n      vec.append(padded_x[:, i:i + h:stride, j:j + w:stride, :])\n  return tf.concat(vec, axis=-1)\n\n\ndef _make_displacement(x, patch=3, max_dis=1, stride1=1, stride2=1):\n  """"""[B, H, W, C]->[B, H, W, V, d*d]""""""\n  k1, k2 = to_list(patch, 2)\n  d1, d2 = to_list(max_dis, 2)\n  h, w = tf.shape(x)[1], tf.shape(x)[2]\n  padding = [[0, 0], [k1 // 2 + d1] * 2, [k2 // 2 + d2] * 2, [0, 0]]\n  padded_x = tf.pad(x, padding)\n  disp = []\n  vec = []\n  for i in range(0, 2 * d1 + 1, stride2):\n    for j in range(0, 2 * d2 + 1, stride2):\n      for k in range(k1):\n        for l in range(k2):\n          vec.append(padded_x[\n                     :,\n                     i + k:i + k + h:stride1,\n                     j + l:j + l + w:stride1,\n                     :])\n      disp.append(tf.concat(vec, -1))\n      vec.clear()\n  return tf.stack(disp, axis=-1)\n\n\ndef correlation(f1, f2, patch, max_displacement, stride1=1, stride2=1):\n  """"""calculate correlation between feature map ""f1"" and ""f2"".\n  See ""FlowNet: Learning Optical Flow with Convolutional Networks"" for\n  details.\n\n  Args:\n      f1: a 4-D tensor with shape [B, H, W, C]\n      f2: a 4-D tensor with shape [B, H, W, C]\n      patch: an integer or a list like [k1, k2], window size for comparison\n      max_displacement: an integer, representing the max searching distance\n      stride1: stride for patch\n      stride2: stride for displacement\n\n  Returns:\n      a 4-D correlation tensor with shape [B, H, W, d*d]\n  """"""\n  channel = f1.shape[-1]\n  norm = tf.reduce_prod(to_list(patch, 2) + [channel])\n  v1 = _make_vector(f1, patch, stride1)\n  v1 = tf.expand_dims(v1, -2)\n  v2 = _make_displacement(f2, patch, max_displacement, stride1, stride2)\n  corr = tf.matmul(v1, v2) / tf.to_float(norm)\n  return tf.squeeze(corr, axis=-2)\n\n\ndef pad_if_divide(x, value=16, mode=\'CONSTANT\'):\n  """"""pad tensor if its width and height couldn\'t be divided by `value`.\n\n  Args:\n      x: a tensor at least has 3 dimensions.\n      value: value to divide width and height.\n      mode: a string, representing padding mode.\n  Return:\n      padded tensor.\n  """"""\n\n  shape = tf.shape(x)\n  h = shape[-3]\n  w = shape[-2]\n  h2 = tf.cond(tf.equal(tf.mod(h, value), 0),\n               lambda: h,\n               lambda: h + value - tf.mod(h, value))\n  w2 = tf.cond(tf.equal(tf.mod(w, value), 0),\n               lambda: w,\n               lambda: w + value - tf.mod(w, value))\n  pad = [[0, h2 - h], [0, w2 - w], [0, 0]]\n  pad = [[0, 0]] * (x.get_shape().ndims - 3) + pad\n  return tf.pad(x, pad, mode=mode)\n\n\ndef shave_if_divide(x, value=16):\n  """"""crop tensor if its width and height couldn\'t be divided by `value`.\n\n  Args:\n      x: a tensor at least has 3 dimensions.\n      value: value to divide width and height.\n  Return:\n      cropped tensor.\n  """"""\n\n  shape = tf.shape(x)\n  h = shape[-3]\n  w = shape[-2]\n  h2 = tf.cond(tf.equal(tf.mod(h, value), 0),\n               lambda: h,\n               lambda: h - tf.mod(h, value))\n  w2 = tf.cond(tf.equal(tf.mod(w, value), 0),\n               lambda: w,\n               lambda: w - tf.mod(w, value))\n  return x[..., :h2, :w2, :]\n\n\ndef clip_image(image, max_val=255):\n  image = tf.clip_by_value(image / max_val, 0, 1) * 255\n  return tf.cast(tf.round(image), tf.uint8)\n\n\nclass SpectralNorm(tf.keras.constraints.Constraint):\n  """"""Spectral normalization constraint.\n    Ref: https://arxiv.org/pdf/1802.05957\n\n    Args:\n        iteration: power iteration, default 1\n    Note:\n        use SN as a kernel constraint seems not work well\n        I now use like this:\n        >>> nn = tf.layers.Conv2D(...)\n        >>> nn.build(input_shape=x.shape.as_list())\n        >>> if use_sn: nn.kernel = SpectralNorm()(nn.kernel)\n        >>> y = nn(x)\n  """"""\n\n  def __init__(self, iteration=1):\n    self.pi = iteration\n\n  def __call__(self, w):\n    scope = w.op.name.replace(\':\', \'\') + \'/snorm\'\n    with tf.variable_scope(scope):\n      w_shape = w.shape.as_list()\n      w = tf.reshape(w, [-1, w_shape[-1]])\n      u = tf.get_variable(\n          \'u\',\n          shape=(w.shape[0], 1),\n          dtype=w.dtype,\n          collections=[tf.GraphKeys.MODEL_VARIABLES,\n                       tf.GraphKeys.GLOBAL_VARIABLES],\n          initializer=tf.random_normal_initializer(),\n          trainable=False)\n      u_hat = u\n      v_hat = None\n      for i in range(self.pi):\n        # power iteration\n        v_hat = tf.nn.l2_normalize(tf.matmul(tf.transpose(w), u_hat),\n                                   axis=None, epsilon=1e-12)\n        u_hat = tf.nn.l2_normalize(tf.matmul(w, v_hat),\n                                   axis=None, epsilon=1e-12)\n      u_hat = tf.stop_gradient(u_hat)\n      v_hat = tf.stop_gradient(v_hat)\n      sigma = tf.matmul(tf.matmul(tf.transpose(u_hat), w), v_hat)\n      w_norm = w / sigma\n      w_norm = tf.reshape(w_norm, w_shape)\n      return w_norm\n\n  def get_config(self):\n    return {""iteration"": self.pi}\n\n\nclass Vgg:\n  VGG16 = \'vgg16\'\n  VGG19 = \'vgg19\'\n  WEIGHTS_SITE = (\'https://github.com/fchollet/deep-learning-models/\'\n                  \'releases/download/v0.1/\')\n  WEIGHTS_HASH = {\n    \'vgg16_notop\': \'6d6bbae143d832006294945121d1f1fc\',\n    \'vgg16\': \'64373286793e3c8b2b4e3219cbf3544b\',\n    \'vgg19_notop\': \'253f8cb515780f3b799900260a226db6\',\n    \'vgg19\': \'cbe5617147190e668d6c5d5026f83318\'\n  }\n  """"""VGG forward inference, including VGG-16 and VGG-19.\n  Usage:\n      declare which model to use: either including full-connected layers or\n      not, 16 conv layers or 19 layers.\n      >>> `vgg = Vgg(False, \'vgg19\')`\n      get any output by calling with specified layer name.\n      >>> `b2c2 = vgg(x, \'block2_conv2\')`\n      if you are not familiar with VGG, use `dump_layer_names` to list all\n      output names.\n      >>> `vgg.dump_layer_names()`\n  """"""\n\n  def __init__(self, include_top=False, vgg=VGG16):\n    import h5py\n    kutil = tf.keras.utils\n    model_url = vgg + \'_weights_tf_dim_ordering_tf_kernels\'\n    self.vgg = vgg\n    self.include_top = include_top\n    if not include_top:\n      model_url += \'_notop\'\n      vgg += \'_notop\'\n    model_url += \'.h5\'\n    weights_path = kutil.get_file(\n        model_url,\n        self.WEIGHTS_SITE + model_url,\n        cache_subdir=\'models\',\n        file_hash=self.WEIGHTS_HASH[vgg])\n    self.weights = h5py.File(weights_path, \'r\')\n    self.outputs = {}\n    self.built = False\n\n  def __call__(self, inputs, output_layer=None):\n    if inputs.shape[-1] == 1:\n      inputs = tf.image.grayscale_to_rgb(inputs)\n    if self.include_top:\n      inputs = tf.image.resize_bicubic(inputs, (224, 224))\n    # normalize\n    inputs = tf.to_float(inputs)[..., ::-1] - [103.939, 116.779, 123.68]\n    self.build_graph(inputs)\n    if output_layer is None:\n      output_layer = \'final\'\n    return self.outputs[output_layer]\n\n  def build_graph(self, inputs):\n    def conv2d(x, f, k, name):\n      with tf.name_scope(name):\n        w = self.weights.get(name).get(name + \'_W_1:0\').value\n        bias = self.weights.get(name).get(name + \'_b_1:0\').value\n        x = tf.nn.conv2d(x, w, [1, 1, 1, 1], \'SAME\')\n        x = tf.nn.bias_add(x, bias)\n        x = tf.nn.relu(x)\n      self.outputs[name] = x\n      return x\n\n    def dense(x, unit, activation, name):\n      with tf.name_scope(name):\n        w = self.weights.get(name).get(name + \'_W_1:0\').value\n        bias = self.weights.get(name).get(name + \'_b_1:0\').value\n        x = tf.matmul(x, w)\n        tf.nn.bias_add(x, bias)\n        x = activation(x)\n      self.outputs[name] = x\n      return x\n\n    with tf.name_scope(self.vgg):\n      x = conv2d(inputs, 64, 3, name=\'block1_conv1\')\n      x = conv2d(x, 64, 3, name=\'block1_conv2\')\n      x = tf.layers.max_pooling2d(x, 2, 2, name=\'block1_pool\')\n      self.outputs[\'block1_pool\'] = x\n\n      x = conv2d(x, 128, 3, name=\'block2_conv1\')\n      x = conv2d(x, 128, 3, name=\'block2_conv2\')\n      x = tf.layers.max_pooling2d(x, 2, 2, name=\'block2_pool\')\n      self.outputs[\'block2_pool\'] = x\n\n      x = conv2d(x, 256, 3, name=\'block3_conv1\')\n      x = conv2d(x, 256, 3, name=\'block3_conv2\')\n      x = conv2d(x, 256, 3, name=\'block3_conv3\')\n      if self.vgg == self.VGG19:\n        x = conv2d(x, 256, 3, name=\'block3_conv4\')\n      x = tf.layers.max_pooling2d(x, 2, 2, name=\'block3_pool\')\n      self.outputs[\'block3_pool\'] = x\n\n      x = conv2d(x, 512, 3, name=\'block4_conv1\')\n      x = conv2d(x, 512, 3, name=\'block4_conv2\')\n      x = conv2d(x, 512, 3, name=\'block4_conv3\')\n      if self.vgg == self.VGG19:\n        x = conv2d(x, 512, 3, name=\'block4_conv4\')\n      x = tf.layers.max_pooling2d(x, 2, 2, name=\'block4_pool\')\n      self.outputs[\'block4_pool\'] = x\n\n      x = conv2d(x, 512, 3, name=\'block5_conv1\')\n      x = conv2d(x, 512, 3, name=\'block5_conv2\')\n      x = conv2d(x, 512, 3, name=\'block5_conv3\')\n      if self.vgg == self.VGG19:\n        x = conv2d(x, 512, 3, name=\'block5_conv4\')\n      x = tf.layers.max_pooling2d(x, 2, 2, name=\'block5_pool\')\n      self.outputs[\'block5_pool\'] = x\n\n      if self.include_top:\n        x = tf.layers.flatten(x, name=\'flatten\')\n        x = dense(x, 4096, tf.nn.relu, name=\'fc1\')\n        x = dense(x, 4096, tf.nn.relu, name=\'fc2\')\n        x = dense(x, 1024, tf.nn.softmax, name=\'predictions\')\n      else:\n        x = tf.reduce_mean(x, [1, 2, 3])\n\n    self.outputs[\'final\'] = x\n    self.built = True\n\n  def dump_layer_names(self):\n    if not self.built:\n      LOG.warning((\n        ""This VGG hasn\'t been built yet, ""\n        ""make inference on any tensor to build the model.""))\n\n    print(self.outputs.keys())\n\n\nclass TorchInitializer(tf.keras.initializers.Initializer):\n  """"""Default weight and bias initializer in PyTorch.\n    PyTorch (v1.0.1) uses a modified Kaiming (He) uniform distribution to\n    initialize both weight and bias.\n\n  Args:\n    fan_in: in order to init bias, give weight\'s fan_in here.\n    a: a scale factor in PyTorch, do not modify.\n    seed: A Python integer. Used to create random seeds. See\n      `tf.set_random_seed` for behavior.\n    dtype: Default data type, used if no `dtype` argument is provided when\n      calling the initializer. Only floating point types are supported.\n  """"""\n\n  def __init__(self, fan_in=None, a=5, seed=None, dtype=tf.float32):\n    self.fan_in = fan_in\n    self.a = a\n    self.seed = seed\n    self.dtype = tf.as_dtype(dtype)\n\n  def __call__(self, shape, dtype=None, partition_info=None):\n    if dtype is None:\n      dtype = self.dtype\n    scale_shape = shape\n    if partition_info is not None:\n      scale_shape = partition_info.full_shape\n    if self.fan_in is None:\n      self.fan_in, _ = self._compute_fans(scale_shape)\n    gain2 = 2.0 / (1 + self.a)  # 1/3\n    bound = tf.sqrt(3.0 * gain2 / int(self.fan_in))\n    return tf.random_uniform(shape, -bound, bound, dtype, self.seed)\n\n  def get_config(self):\n    return {\n      ""fan_in"": self.fan_in,\n      ""a"": self.a,\n      ""seed"": self.seed,\n      ""dtype"": self.dtype.name\n    }\n\n  @staticmethod\n  def _compute_fans(shape):\n    """"""Computes the number of input and output units for a weight shape.\n\n    Args:\n      shape: Integer shape tuple or TF tensor shape.\n\n    Returns:\n      A tuple of scalars (fan_in, fan_out).\n    """"""\n    if len(shape) < 1:  # Just to avoid errors for constants.\n      fan_in = fan_out = 1\n    elif len(shape) == 1:\n      fan_in = fan_out = shape[0]\n    elif len(shape) == 2:\n      fan_in = shape[0]\n      fan_out = shape[1]\n    else:\n      # Assuming convolution kernels (2D, 3D, or more).\n      # kernel shape: (..., input_depth, depth)\n      receptive_field_size = 1.\n      for dim in shape[:-2]:\n        receptive_field_size *= dim\n      fan_in = shape[-2] * receptive_field_size\n      fan_out = shape[-1] * receptive_field_size\n    return fan_in, fan_out\n'"
VSR/Backend/TF/Arch/Dense.py,2,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 17th 2018\n\nArchitectures of common dense blocks used in SR researches\n""""""\n\nimport tensorflow as tf\n\nfrom ..Framework.LayersHelper import Layers\n\n\ndef dense_block(layers: Layers, inputs, depth=8, rate=16, out_dims=128,\n                scope=None, reuse=None):\n  filters = out_dims - rate * depth\n  feat = [inputs]\n  with tf.variable_scope(scope, \'DenseBlock\', reuse=reuse):\n    for _ in range(depth):\n      filters += rate\n      x = layers.relu_conv2d(feat[-1], filters, 3)\n      feat.append(x)\n      feat[-1] = tf.concat(feat[1:], axis=-1)\n    return x\n'"
VSR/Backend/TF/Arch/Discriminator.py,24,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 4th 2018\n\nArchitectures of common discriminator\n""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..Framework.LayersHelper import Layers\n\n\ndef _view(inputs, input_shape):\n  """"""view inputs\' shape as `input_shape`\n\n  Args:\n      inputs: a tensor of any shape\n      input_shape: a list of 3 or 4 integers, specify the shape of the inputs.\n  Return:\n      identified tensor `x` and a boolean `has_shape` whether H and W is None.\n  """"""\n  input_shape = list(input_shape)\n  if len(input_shape) == 3:\n    input_shape.insert(0, -1)\n  if len(input_shape) != 4:\n    raise ValueError(\'input_shape must be a vector of 3 or 4 integers,\'\n                     \' representing [H W C] or [B H W C].\'\n                     \'input_shape: %s\' % str(input_shape))\n  if input_shape[1] and input_shape[2]:\n    if input_shape[0] is None:\n      input_shape[0] = -1\n    x = tf.reshape(inputs, input_shape)\n    has_shape = True\n  else:\n    has_shape = False\n    x = tf.identity(inputs)\n  return x, has_shape\n\n\ndef dcgan_d(layers: Layers, input_shape, filters_in=64, times_stride=3,\n            bias=True, leaky_alpha=0.2, norm=None, name_or_scope=None):\n  """"""Discriminator similar as DCGAN.\n  Ref: [1]\n       [2]\n       [3]\n\n  Args:\n      layers: a parent object (usually a derived SuperResolution class)\n      input_shape: a list of 3 or 4 integers, specify the shape of the inputs.\n      filters_in: number of filters of the 1st layer.\n      times_stride: number of stride (a.k.a downsample) times.\n      bias: to add bias before activation or not.\n      leaky_alpha: alpha value of `leaky_relu`.\n      norm: a string either \'sn\' or \'bn\' or None, representing spectral norm,\n        batch norm respectively.\n      name_or_scope: a string or tf.VariableScope.\n  """"""\n  if norm:\n    bn = np.any([word in norm for word in (\'bn\', \'batch\')])\n    sn = np.any([word in norm for word in (\'sn\', \'spectral\')])\n  else:\n    bn = sn = False\n\n  def critic(inputs, conditions=None):\n    with tf.variable_scope(name_or_scope, \'D\', reuse=tf.AUTO_REUSE):\n      x, has_shape = _view(inputs, input_shape)\n      kwargs = dict(use_sn=sn,\n                    use_batchnorm=bn,\n                    use_bias=bias,\n                    kernel_initializer=\'truncated_normal_0.02\')\n      ch = filters_in\n      for i in range(times_stride):\n        x = layers.conv2d(x, ch * (2 ** i), 3, **kwargs)\n        x = tf.nn.leaky_relu(x, leaky_alpha)\n        x = layers.conv2d(x, ch * (2 ** (i + 1)), 4, 2, **kwargs)\n        x = tf.nn.leaky_relu(x, leaky_alpha)\n      x = layers.conv2d(x, ch * 8, 3, **kwargs)\n      x = tf.nn.leaky_relu(x, leaky_alpha)\n      if has_shape:\n        x = tf.layers.flatten(x)\n      else:\n        x = tf.reduce_mean(x, [1, 2])\n      x = layers.dense(x, 1, use_sn=sn,\n                       kernel_initializer=\'random_normal_0.02\')\n      return x\n\n  return critic\n\n\ndef resnet_d(layers: Layers, input_shape, filters_in=64, times_pooling=5,\n             norm=None, bias=True, name_or_scope=None):\n  """"""Discriminator similar as SN paper table 4.\n  Ref: https://arxiv.org/pdf/1802.05957\n\n  Args:\n      layers: a parent object (usually a derived SuperResolution class)\n      input_shape: a list of 3 or 4 integers, specify the shape of the inputs.\n      filters_in: number of filters of the 1st layer.\n      times_pooling: number of pooling (a.k.a downsample) times.\n      bias: to add bias before activation or not.\n      norm: a string either \'sn\' or \'bn\' or None, representing spectral norm,\n        batch norm respectively.\n      name_or_scope: a string or tf.VariableScope.\n  """"""\n  if norm:\n    bn = np.any([word in norm for word in (\'bn\', \'batch\')])\n    sn = np.any([word in norm for word in (\'sn\', \'spectral\')])\n  else:\n    bn = sn = False\n\n  def critic(inputs, conditions=None):\n    with tf.variable_scope(name_or_scope, \'D\', reuse=tf.AUTO_REUSE):\n      x, has_shape = _view(inputs, input_shape)\n      kwargs = dict(use_sn=sn,\n                    use_batchnorm=bn,\n                    use_bias=bias,\n                    activation=\'relu\',\n                    placement=\'front\',\n                    kernel_initializer=\'truncated_normal_0.02\')\n      ch = filters_in\n      magic = (1, 2, 4, 4, 8, 8,) + (16,) * times_pooling\n      for i in range(times_pooling):\n        x = layers.resblock(x, ch * magic[i], 3, **kwargs)\n        x = tf.layers.average_pooling2d(x, 2, 2)\n      x = tf.nn.relu(x)\n      if has_shape:\n        x = tf.layers.flatten(x)\n      else:\n        x = tf.reduce_mean(x, [1, 2])\n      x = layers.dense(x, 1, use_sn=sn,\n                       kernel_initializer=\'random_normal_0.02\')\n      return x\n\n  return critic\n\n\ndef projection_d(layers: Layers, input_shape, filters_in=64, times_pooling=5,\n                 norm=None, bias=True, name_or_scope=None):\n  """"""Discriminator similar as projection GAN paper.\n  Ref: https://arxiv.org/abs/1802.05637\n\n  Args:\n      layers: a parent object (usually a derived SuperResolution class)\n      input_shape: a list of 3 or 4 integers, specify the shape of the inputs.\n      filters_in: number of filters of the 1st layer.\n      times_pooling: number of pooling (a.k.a downsample) times.\n      bias: to add bias before activation or not.\n      norm: a string either \'sn\' or \'bn\' or None, representing spectral norm,\n        batch norm respectively.\n      name_or_scope: a string or tf.VariableScope.\n  """"""\n  if norm:\n    bn = np.any([word in norm for word in (\'bn\', \'batch\')])\n    sn = np.any([word in norm for word in (\'sn\', \'spectral\')])\n  else:\n    bn = sn = False\n\n  def critic(inputs, conditions=None):\n    with tf.variable_scope(name_or_scope, \'D\', reuse=tf.AUTO_REUSE):\n      x, has_shape = _view(inputs, input_shape)\n      kwargs = dict(use_sn=sn,\n                    use_batchnorm=bn,\n                    use_bias=bias,\n                    activation=\'relu\',\n                    placement=\'front\',\n                    kernel_initializer=\'truncated_normal_0.02\')\n      ch = filters_in\n      magic = (1, 2, 2, 4, 8,) + (16,) * times_pooling\n      scale = layers.scale[0]  # determine position to embed condition\n      n_pooling = int(np.log2(scale)) - 1\n      for i in range(times_pooling):\n        x = layers.resblock(x, ch * magic[i], 3, **kwargs)\n        x = tf.layers.average_pooling2d(x, 2, 2)\n        if i == n_pooling and conditions is not None:\n          cond = layers.conv2d(x, layers.channel, 3, use_bias=bias,\n                               use_batchnorm=bn, use_sn=sn)\n          cond = tf.matmul(tf.layers.flatten(cond),\n                           tf.layers.flatten(conditions),\n                           transpose_b=True)\n        else:\n          cond = None\n      x = layers.resblock(x, ch * magic[-1], 3, **kwargs)\n      x = tf.nn.relu(x)\n      if has_shape:\n        x = tf.layers.flatten(x)\n      else:\n        x = tf.reduce_mean(x, [1, 2])\n      x = layers.dense(x, 1, use_sn=sn,\n                       kernel_initializer=\'random_normal_0.02\')\n      if cond is None:\n        cond = tf.zeros_like(x)\n      return x + cond\n\n  return critic\n'"
VSR/Backend/TF/Arch/Residual.py,33,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 17th 2018\n\nArchitectures of common residual blocks used in SR researches\n""""""\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom ..Framework.LayersHelper import Layers\n\n\ndef rdn(layers: Layers, inputs, depth, scaling=1.0,\n        scope=None, reuse=None, **kwargs):\n  """"""Residual Dense Block (CVPR18)""""""\n\n  k = kwargs.pop(\'kernel_size\', 3)\n  f = kwargs.pop(\'filters\', 64)\n  act = kwargs.pop(\'activation\', \'relu\')\n  kwargs.pop(\'name\', None)\n  with tf.variable_scope(scope, \'RDN\', reuse=reuse):\n    fl = [inputs]\n    for i in range(depth - 1):\n      x = layers.conv2d(tf.concat(fl, -1), f, k, activation=act, **kwargs)\n      fl.append(x)\n    x = layers.conv2d(tf.concat(fl, -1), f, k, **kwargs)\n    return x * scaling + inputs\n\n\ndef rcab(layers: Layers, inputs, ratio=16, scope=None, reuse=None, **kwargs):\n  """"""Residual channel attention block (ECCV18)\n\n  """"""\n\n  k = kwargs.pop(\'kernel_size\', 3)\n  f = kwargs.pop(\'filters\', 64)\n  act = kwargs.pop(\'activation\', \'relu\')\n  with tf.variable_scope(scope, \'RCAB\', reuse=reuse):\n    x = layers.conv2d(inputs, f, k, activation=act, **kwargs)\n    y = layers.conv2d(x, f, k, **kwargs)\n    x = tf.reduce_mean(y, axis=[1, 2], keepdims=True)\n    x = layers.conv2d(x, f // ratio, 1, activation=act, **kwargs)\n    x = layers.conv2d(x, f, 1, activation=tf.nn.sigmoid, **kwargs)\n    y *= x\n  return y + inputs\n\n\ndef msrb(layers: Layers, inputs, scope=None, reuse=None, **kwargs):\n  """"""Multi-scale residual block (ECCV18)\n\n  """"""\n\n  k1, k2 = kwargs.pop(\'kernel_size\', (3, 5))\n  f = kwargs.pop(\'filters\', 64)\n  act = kwargs.pop(\'activation\', \'relu\')\n  with tf.variable_scope(scope, \'MSRB\', reuse=reuse):\n    s1 = layers.conv2d(inputs, f, k1, activation=act)\n    p1 = layers.conv2d(inputs, f, k2, activation=act)\n\n    s2 = layers.conv2d(tf.concat([s1, p1], -1), f * 2, k1, activation=act)\n    p2 = layers.conv2d(tf.concat([p1, s1], -1), f * 2, k2, activation=act)\n\n    s = layers.conv2d(tf.concat([s2, p2], -1), f, 1)\n  return s + inputs\n\n\ndef cascade_block(layers: Layers, inputs, depth=4,\n                  scope=None, reuse=None, **kwargs):\n  """"""Cascading residual block (ECCV18)\n\n  """"""\n\n  k = kwargs.pop(\'kernel_size\', 3)\n  f = kwargs.pop(\'filters\', 64)\n  act = kwargs.pop(\'activation\', \'relu\')\n  with tf.variable_scope(scope, \'CARB\', reuse=reuse):\n    feat = [inputs]\n    for i in range(depth):\n      x = layers.resblock(inputs, f, k, activation=act)\n      feat.append(x)\n      inputs = layers.conv2d(\n        tf.concat(feat, axis=-1), f, 1,\n        kernel_initializer=\'he_uniform\')\n    inputs = layers.conv2d(inputs, f, k)\n    return inputs\n\n\ndef cascade_rdn(layers: Layers, inputs, depth, use_ca=False,\n                scope=None, reuse=None, **kwargs):\n  """"""Cascaded residual dense block.\n  Args:\n    layers: child class of Layers\n    inputs: input tensor\n    depth: an int or list of 2 ints, representing number of rdbs\n    use_ca: insert channel attention layer\n    scope: scope name\n    reuse: reuse variables\n  """"""\n\n  k = kwargs.pop(\'kernel_size\', 3)\n  f = kwargs.pop(\'filters\', 64)\n  act = kwargs.pop(\'activation\', \'relu\')\n  kwargs.pop(\'name\', None)\n  depth = to_list(depth, 2)\n  with tf.variable_scope(scope, \'CascadeRDN\', reuse=reuse):\n    fl = [inputs]\n    x = inputs\n    for i in range(depth[0]):\n      x = rdn(layers, x, depth[1], kernel_size=k, filters=f, activation=act,\n              **kwargs)\n      if use_ca:\n        x = rcab(layers, x, kernel_size=k, filters=f, activation=act, **kwargs)\n      fl.append(x)\n      x = tf.concat(fl, -1)\n      x = layers.conv2d(x, f, 1, **kwargs)\n    return x\n\n\ndef non_local(layers: Layers, inputs, filters=64, func=None, scaling=1,\n              pooling=None, use_bn=None, scope=None, reuse=None):\n  """"""Non-local neural networks (CVPR 18)\n\n  Note:\n      y_i = \\frac{1}{C(x)}\\Sigma_{j}(f(x_i, x_j)g(x_j))\n      A pairwise function f computes a scalar between i and all j. The unary\n      function g computes a representation of the input signal at the position\n      j. The response is normalized by a factor C(x).\n\n  Args:\n      layers: parent object.\n      inputs: input tensor.\n      filters: output filter numbers.\n      func: a string in (\'gaussian\', \'embedded\', \'dot\', \'concat\'),\n        representing the pairwise function f. Default \'embedded\'.\n      scaling: scaling channel numbers down.\n      pooling: sub-sampling x by max-pooling, `pooling` represents strides.\n      use_bn: if True, batch-normalizing last embedding output.\n      scope: variable scope for this block.\n      reuse: reuse flag.\n  """"""\n\n  def embedding(x: tf.Tensor, c, scale):\n    if len(x.shape) == 4:\n      return layers.conv2d(x, c // scale, 1)\n    elif len(x.shape) == 5:\n      return layers.conv3d(x, c // scale, 1)\n    else:\n      raise ValueError(\'input tensor dimensions must be 4 or 5.\')\n\n  def flatten(x: tf.Tensor):\n    c = x.shape[-1]\n    b = tf.shape(x)[0]\n    return tf.reshape(x, [b, -1, c])\n\n  def reduce(x: tf.Tensor):\n    if len(x.shape) == 4:\n      if pooling is not None:\n        return tf.layers.max_pooling2d(x, pooling, pooling)\n      else:\n        return x\n    elif len(x.shape) == 5:\n      if pooling is not None:\n        return tf.layers.max_pooling3d(x,\n                                       [1, pooling, pooling],\n                                       [1, pooling, pooling])\n      else:\n        return x\n    else:\n      raise ValueError(\'input tensor dimensions must be 4 or 5.\')\n\n  def gaussian(x: tf.Tensor):\n    x0 = flatten(x)\n    x1 = flatten(reduce(x))\n    return tf.nn.softmax(tf.matmul(x0, x1, transpose_b=True), -1)\n\n  def embedded(x: tf.Tensor):\n    theta = flatten(embedding(x, filters, scaling))\n    phi = flatten(embedding(reduce(x), filters, scaling))\n    return tf.nn.softmax(tf.matmul(theta, phi, transpose_b=True), -1)\n\n  def dot(x: tf.Tensor):\n    theta = flatten(embedding(x, filters, scaling))\n    phi = flatten(embedding(reduce(x), filters, scaling))\n    n = tf.shape(phi)[1]\n    return tf.truediv(tf.matmul(theta, phi, transpose_b=True), n)\n\n  def concat(x: tf.Tensor):\n    raise NotImplementedError\n\n  with tf.variable_scope(scope, \'NonLocal\', reuse=reuse):\n    channels = inputs.get_shape().as_list()[-1]\n    shape = tf.shape(inputs)\n    g = embedding(inputs, filters, scaling)\n    g = reduce(g)\n    if func == \'gaussian\':\n      f = gaussian\n    elif func == \'dot\':\n      f = dot\n    elif func == \'concat\':\n      f = concat\n    else:\n      f = embedded\n    corr = f(inputs)\n    y = tf.matmul(corr, flatten(g))\n    y = tf.reshape(y, [shape[0], shape[1], shape[2], channels // scaling])\n    y = embedding(y, channels, 1)\n    if use_bn:\n      y = layers.batch_norm(y, layers.training_phase)\n    return inputs + y\n'"
VSR/Backend/TF/Arch/__init__.py,0,b''
VSR/Backend/TF/Framework/GAN.py,39,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: July 20th 2018\n\nConventional Generator and Discriminator as well as objective function\nfor generative adversarial networks \n""""""\n\nfrom functools import partial\n\nimport numpy as np\nimport tensorflow as tf\n\n_INCEPTION_BATCH = 50\n_TFGAN = tf.contrib.gan.eval\n\n\ndef _preprocess_for_inception(images):\n  """"""Preprocess images for inception.\n\n  Args:\n    images: images minibatch. Shape [batch size, width, height,\n      channels]. Values are in [0..255].\n\n  Returns:\n    preprocessed_images\n  """"""\n\n  images = tf.cast(images, tf.float32)\n\n  # tfgan_eval.preprocess_image function takes values in [0, 255]\n  with tf.control_dependencies([tf.assert_greater_equal(images, 0.0),\n                                tf.assert_less_equal(images, 255.0)]):\n    images = tf.identity(images)\n\n  preprocessed_images = tf.map_fn(\n    fn=_TFGAN.preprocess_image,\n    elems=images,\n    back_prop=False)\n\n  return preprocessed_images\n\n\ndef _run_inception(images, layer_name, inception_graph):\n  preprocessed = _preprocess_for_inception(images)\n  return _TFGAN.run_inception(preprocessed,\n                              output_tensor=layer_name,\n                              graph_def=inception_graph)\n\n\ndef fid_score(real_image, gen_image, num_batches=None):\n  """"""FID function from tf.contrib\n\n  Args:\n      real_image: must be 4-D tensor, ranges from [0, 255]\n      gen_image: must be 4-D tensor, ranges from [0, 255]\n      num_batches: Number of batches to split `generated_images` in to in\n        order to efficiently run them through the classifier network.\n  """"""\n  batches = real_image.shape[0]\n  assert gen_image.shape[0] == batches, \\\n    f\'Batch mis-match: {batches} != {gen_image.shape[0]}\'\n  assert isinstance(real_image, np.ndarray)\n  assert isinstance(gen_image, np.ndarray)\n  if not num_batches:\n    num_batches = (batches + _INCEPTION_BATCH - 1) // _INCEPTION_BATCH\n  graph = _TFGAN.get_graph_def_from_url_tarball(\n    \'http://download.tensorflow.org/models/frozen_inception_v1_2015_12_05.tar.gz\',\n    \'inceptionv1_for_inception_score.pb\',\n    \'/tmp/frozen_inception_v1_2015_12_05.tar.gz\')\n  # make tensor batches\n  real_ph = tf.placeholder(tf.float32,\n                           [_INCEPTION_BATCH, *real_image.shape[1:]])\n  gen_ph = tf.placeholder(tf.float32,\n                          [_INCEPTION_BATCH, *gen_image.shape[1:]])\n  real_features = _run_inception(real_ph, \'pool_3:0\', graph)\n  gen_features = _run_inception(gen_ph, \'pool_3:0\', graph)\n  sess = tf.get_default_session()\n  real_feature_np = []\n  gen_feature_np = []\n  real_image = np.split(real_image, num_batches)\n  gen_image = np.split(gen_image, num_batches)\n  for i in range(num_batches):\n    r, g = sess.run(\n      [real_features, gen_features],\n      feed_dict={real_ph: real_image[i], gen_ph: gen_image[i]})\n    real_feature_np.append(r)\n    gen_feature_np.append(g)\n  real_feature_np = np.concatenate(real_feature_np)\n  gen_feature_np = np.concatenate(gen_feature_np)\n  fid_tensor = _TFGAN.frechet_classifier_distance(\n    classifier_fn=tf.identity,\n    real_images=real_feature_np,\n    generated_images=gen_feature_np,\n    num_batches=num_batches)\n  return fid_tensor\n\n\ndef inception_score(images, num_batches=None):\n  """"""IS function from tf.contrib\n\n  Args:\n      images: must be 4-D tensor, ranges from [0, 255]\n      num_batches: Number of batches to split `generated_images` in to in\n        order to efficiently run them through the classifier network.\n  """"""\n  batches = images.shape[0]\n  if not num_batches:\n    num_batches = (batches + _INCEPTION_BATCH - 1) // _INCEPTION_BATCH\n  graph = _TFGAN.get_graph_def_from_url_tarball(\n    \'http://download.tensorflow.org/models/frozen_inception_v1_2015_12_05.tar.gz\',\n    \'inceptionv1_for_inception_score.pb\',\n    \'/tmp/frozen_inception_v1_2015_12_05.tar.gz\')\n  return _TFGAN.classifier_score(\n    images=images,\n    classifier_fn=partial(_run_inception,\n                          layer_name=\'logits:0\',\n                          inception_graph=graph),\n    num_batches=num_batches)\n\n\ndef loss_bce_gan(y_real, y_fake):\n  """"""Original GAN loss with BCE""""""\n\n  d_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(y_real), y_real) + \\\n           tf.losses.sigmoid_cross_entropy(tf.zeros_like(y_fake), y_fake)\n\n  g_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(y_fake), y_fake)\n  return g_loss, d_loss\n\n\ndef loss_relative_bce_gan(y_real, y_fake, average=False):\n  """"""R(A)GAN""""""\n  bce = tf.losses.sigmoid_cross_entropy\n  if average:\n    d_loss = bce(tf.ones_like(y_real), y_real - tf.reduce_mean(y_fake)) + \\\n             bce(tf.zeros_like(y_fake), y_fake - tf.reduce_mean(y_real))\n\n    g_loss = bce(tf.ones_like(y_fake), y_fake - tf.reduce_mean(y_real)) + \\\n             bce(tf.zeros_like(y_real), y_real - tf.reduce_mean(y_fake))\n  else:\n    d_loss = bce(tf.ones_like(y_real), y_real - y_fake)\n\n    g_loss = bce(tf.ones_like(y_fake), y_fake - y_real)\n  return g_loss, d_loss\n\n\ndef loss_wgan(y_real, y_fake):\n  """"""W-GAN""""""\n\n  d_loss = tf.reduce_mean(y_fake - y_real)\n  g_loss = -tf.reduce_mean(y_fake)\n\n  return g_loss, d_loss\n\n\ndef gradient_penalty(y_true, y_pred, graph_fn, lamb=10):\n  """"""Gradient penalty""""""\n\n  if not callable(graph_fn):\n    raise TypeError(\'graph callee is not a callable!\')\n\n  diff = y_pred - y_true\n  alpha = tf.random_uniform(tf.shape(diff)[0:1], minval=0., maxval=1.)\n  alpha = tf.reshape(alpha, [-1, 1, 1, 1])\n  interp = y_true + alpha * diff\n  gradients = tf.gradients(graph_fn(interp), [interp])[0]\n  slopes = tf.sqrt(1e-4 + tf.reduce_sum(\n    tf.square(gradients), reduction_indices=[1, 2, 3]))\n  gp = tf.reduce_mean(tf.square(slopes - 1.))\n  return lamb * gp\n\n\ndef loss_lsgan(y_real, y_fake, a=0, b=1, c=1):\n  """"""Least-Square GAN.\n  There are many choice of (a, b, c). For those b - c==1 and b - a==2, the\n    objective function is equal to Pearson x^2 divergence. We choose default\n    value to be (0, 1, 1)\n  """"""\n\n  d_loss = tf.reduce_mean((y_real - b) ** 2) + \\\n           tf.reduce_mean((y_fake - a) ** 2)\n  g_loss = tf.reduce_mean((y_fake - c) ** 2)\n  return g_loss * 0.5, d_loss * 0.5\n\n\ndef loss_relative_lsgan(y_real, y_fake, average=False):\n  """"""R(A)LSGAN""""""\n\n  if average:\n    d_loss = tf.reduce_mean((y_real - tf.reduce_mean(y_fake) - 1) ** 2) + \\\n             tf.reduce_mean((y_fake - tf.reduce_mean(y_real) + 1) ** 2)\n\n    g_loss = tf.reduce_mean((y_real - tf.reduce_mean(y_fake) + 1) ** 2) + \\\n             tf.reduce_mean((y_fake - tf.reduce_mean(y_real) - 1) ** 2)\n  else:\n    d_loss = tf.reduce_mean((y_real - y_fake - 1) ** 2)\n    g_loss = tf.reduce_mean((y_fake - y_real - 1) ** 2)\n  return g_loss, d_loss\n\n\ndef loss_sensitive_gan(y_real, y_fake):\n  """"""Loss-sensitive GAN\n    http://arxiv.org/abs/1701.06264\n  """"""\n  del y_real, y_fake\n  raise NotImplementedError\n'"
VSR/Backend/TF/Framework/LayersHelper.py,43,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Sep 5th 2018\n\ncommonly used layers helper\n""""""\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom ..Util import (\n  SpectralNorm, TorchInitializer, pixel_shift, pop_dict_wo_keyerror, prelu\n)\n\n\nclass Layers(object):\n  def batch_norm(self, x, training, decay=0.9, epsilon=1e-5, name=None):\n    # interesting.\n    return tf.layers.batch_normalization(x,\n                                         momentum=decay,\n                                         training=training,\n                                         fused=False,\n                                         epsilon=epsilon,\n                                         name=name)\n\n  def instance_norm(self, x, trainable=True, name=None, reuse=None):\n    with tf.variable_scope(name, \'InstanceNorm\', reuse=reuse):\n      return tf.contrib.layers.instance_norm(\n          x,\n          trainable=trainable,\n          variables_collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n\n  def layer_norm(self, x, trainable=True, name=None, reuse=None):\n    with tf.variable_scope(name, \'LayerNorm\', reuse=reuse):\n      return tf.contrib.layers.layer_norm(\n          x,\n          trainable=trainable,\n          variables_collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n\n  def group_norm(self, x, group, axis, trainable=True, name=None, reuse=None):\n    with tf.variable_scope(name, \'GroupNorm\', reuse=reuse):\n      return tf.contrib.layers.group_norm(\n          x, group, axis,\n          trainable=trainable,\n          variables_collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n\n  def conv2d(self, x, filters, kernel_size, strides=(1, 1), padding=\'same\',\n             data_format=\'channels_last\', dilation_rate=(1, 1),\n             activation=None, use_bias=True, use_batchnorm=False,\n             use_sn=False, use_in=False, use_ln=False, use_gn=False,\n             kernel_initializer=\'he_normal\',\n             kernel_regularizer=\'l2\', **kwargs):\n    """"""wrap a convolution for common use case""""""\n\n    if kernel_initializer == \'torch\':\n      ki = TorchInitializer()\n      kr = None\n      if use_bias:\n        bi = TorchInitializer(kernel_size * kernel_size * x.shape[-1])\n      else:\n        bi = tf.zeros_initializer()\n    else:\n      ki, kr = self._kernel(kernel_initializer, kernel_regularizer)\n      bi = tf.zeros_initializer()\n    nn = tf.layers.Conv2D(filters, kernel_size, strides=strides,\n                          padding=padding, data_format=data_format,\n                          dilation_rate=dilation_rate, use_bias=use_bias,\n                          kernel_initializer=ki, kernel_regularizer=kr,\n                          bias_initializer=bi, **kwargs)\n    nn.build(x.shape.as_list())\n    if use_sn:\n      nn.kernel = SpectralNorm()(nn.kernel)\n    x = nn(x)\n    if use_batchnorm:\n      x = tf.layers.batch_normalization(x, training=self.training_phase)\n    if use_in:\n      x = self.instance_norm(x)\n    if use_ln:\n      x = self.layer_norm(x)\n    if use_gn:\n      x = self.group_norm(x, 32, -1)\n    activator = self._act(activation)\n    if activation:\n      x = activator(x)\n    return x\n\n  def conv3d(self, x, filters, kernel_size, strides=(1, 1, 1), padding=\'same\',\n             data_format=\'channels_last\', dilation_rate=(1, 1, 1),\n             activation=None, use_bias=True, use_batchnorm=False,\n             use_in=False, use_ln=False, use_gn=False,\n             kernel_initializer=\'he_normal\', kernel_regularizer=\'l2\',\n             **kwargs):\n\n    if kernel_initializer == \'torch\':\n      ki = TorchInitializer()\n      kr = None\n      if use_bias:\n        bi = TorchInitializer(kernel_size * kernel_size * x.shape[-1])\n      else:\n        bi = tf.zeros_initializer()\n    else:\n      ki, kr = self._kernel(kernel_initializer, kernel_regularizer)\n      bi = tf.zeros_initializer()\n    nn = tf.layers.Conv3D(filters, kernel_size, strides=strides,\n                          padding=padding, data_format=data_format,\n                          dilation_rate=dilation_rate, use_bias=use_bias,\n                          kernel_initializer=ki, kernel_regularizer=kr,\n                          bias_initializer=bi, **kwargs)\n    nn.build(x.shape.as_list())\n    x = nn(x)\n    if use_batchnorm:\n      x = tf.layers.batch_normalization(x, training=self.training_phase)\n    if use_in:\n      x = self.instance_norm(x)\n    if use_ln:\n      x = self.layer_norm(x)\n    if use_gn:\n      x = self.group_norm(x, 32, -1)\n    activator = self._act(activation)\n    if activation:\n      x = activator(x)\n    return x\n\n  def deconv2d(self, x,\n               filters,\n               kernel_size,\n               strides=(1, 1),\n               padding=\'same\',\n               data_format=\'channels_last\',\n               activation=None,\n               use_bias=True,\n               use_batchnorm=False,\n               use_sn=False,\n               use_in=False,\n               use_ln=False,\n               use_gn=False,\n               kernel_initializer=\'he_normal\',\n               kernel_regularizer=\'l2\',\n               **kwargs):\n    """"""warp a conv2d_transpose op for simplicity usage""""""\n\n    if kernel_initializer == \'torch\':\n      ki = TorchInitializer()\n      kr = None\n      if use_bias:\n        bi = TorchInitializer(kernel_size * kernel_size * x.shape[-1])\n      else:\n        bi = tf.zeros_initializer()\n    else:\n      ki, kr = self._kernel(kernel_initializer, kernel_regularizer)\n      bi = tf.zeros_initializer()\n    nn = tf.layers.Conv2DTranspose(filters, kernel_size, strides=strides,\n                                   padding=padding,\n                                   data_format=data_format,\n                                   use_bias=use_bias,\n                                   bias_initializer=bi,\n                                   kernel_initializer=ki,\n                                   kernel_regularizer=kr, **kwargs)\n    nn.build(x.shape.as_list())\n    if use_sn:\n      nn.kernel = SpectralNorm()(nn.kernel)\n    x = nn(x)\n    if use_batchnorm:\n      x = tf.layers.batch_normalization(x, training=self.training_phase)\n    if use_in:\n      x = self.instance_norm(x)\n    if use_ln:\n      x = self.layer_norm(x)\n    if use_gn:\n      x = self.group_norm(x, 32, -1)\n    activator = self._act(activation)\n    if activation:\n      x = activator(x)\n    return x\n\n  def deconv3d(self, x,\n               filters,\n               kernel_size,\n               strides=(1, 1, 1),\n               padding=\'same\',\n               data_format=\'channels_last\',\n               activation=None,\n               use_bias=True,\n               use_batchnorm=False,\n               use_in=False,\n               use_ln=False,\n               use_gn=False,\n               kernel_initializer=\'he_normal\',\n               kernel_regularizer=\'l2\',\n               **kwargs):\n\n    if kernel_initializer == \'torch\':\n      ki = TorchInitializer()\n      kr = None\n      if use_bias:\n        bi = TorchInitializer(kernel_size * kernel_size * x.shape[-1])\n      else:\n        bi = tf.zeros_initializer()\n    else:\n      ki, kr = self._kernel(kernel_initializer, kernel_regularizer)\n      bi = tf.zeros_initializer()\n    nn = tf.layers.Conv3DTranspose(filters, kernel_size, strides=strides,\n                                   padding=padding,\n                                   data_format=data_format,\n                                   use_bias=use_bias,\n                                   bias_initializer=bi,\n                                   kernel_initializer=ki,\n                                   kernel_regularizer=kr, **kwargs)\n    nn.build(x.shape.as_list())\n    x = nn(x)\n    if use_batchnorm:\n      x = tf.layers.batch_normalization(x, training=self.training_phase)\n    if use_in:\n      x = self.instance_norm(x)\n    if use_ln:\n      x = self.layer_norm(x)\n    if use_gn:\n      x = self.group_norm(x, 32, -1)\n    activator = self._act(activation)\n    if activation:\n      x = activator(x)\n    return x\n\n  def dense(self, x, units, activation=None, use_bias=True, use_sn=False,\n            kernel_initializer=\'he_normal\', kernel_regularizer=\'l2\',\n            **kwargs):\n    act = self._act(activation)\n    ki, kr = self._kernel(kernel_initializer, kernel_regularizer)\n    nn = tf.layers.Dense(units, use_bias=use_bias,\n                         kernel_initializer=ki,\n                         kernel_regularizer=kr, **kwargs)\n    nn.build(x.shape.as_list())\n    if use_sn:\n      nn.kernel = SpectralNorm()(nn.kernel)\n    x = nn(x)\n    if act:\n      x = act(x)\n    return x\n\n  linear = dense\n\n  @staticmethod\n  def _act(activation):\n    activator = None\n    if activation:\n      if isinstance(activation, str):\n        if activation == \'relu\':\n          activator = tf.nn.relu\n        elif activation == \'tanh\':\n          activator = tf.nn.tanh\n        elif activation == \'prelu\':\n          activator = prelu\n        elif activation == \'lrelu\':\n          activator = tf.nn.leaky_relu\n      elif callable(activation):\n        activator = activation\n      else:\n        raise ValueError(\'invalid activation!\')\n    return activator\n\n  def _kernel(self, kernel_initializer, kernel_regularizer):\n    ki = None\n    if isinstance(kernel_initializer, str):\n      if kernel_initializer == \'he_normal\':\n        ki = tf.keras.initializers.he_normal()\n      elif kernel_initializer == \'he_uniform\':\n        ki = tf.keras.initializers.he_uniform()\n      elif kernel_initializer == \'zeros\' or kernel_initializer == \'zero\':\n        ki = tf.keras.initializers.zeros()\n      elif \'truncated_normal\' in kernel_initializer:\n        stddev = float(kernel_initializer.split(\'_\')[-1])\n        ki = tf.truncated_normal_initializer(stddev=stddev)\n      elif \'random_normal\' in kernel_initializer:\n        stddev = float(kernel_initializer.split(\'_\')[-1])\n        ki = tf.random_normal_initializer(stddev=stddev)\n    elif callable(kernel_initializer):\n      ki = kernel_initializer\n    elif kernel_initializer:\n      raise ValueError(\'invalid kernel initializer!\')\n    kr = None\n    if isinstance(kernel_regularizer, str):\n      if kernel_regularizer == \'l1\':\n        kr = tf.keras.regularizers.l1(\n            self.weight_decay) if self.weight_decay else None\n      elif kernel_regularizer == \'l2\':\n        kr = tf.keras.regularizers.l2(\n            self.weight_decay) if self.weight_decay else None\n    elif callable(kernel_regularizer):\n      kr = kernel_regularizer\n    elif kernel_regularizer:\n      raise ValueError(\'invalid kernel regularizer!\')\n    return ki, kr\n\n  def upscale(self, image, method=\'espcn\', scale=None, direct_output=True,\n              **kwargs):\n    """"""Image up-scale layer\n\n    Upsample `image` width and height by scale factor `scale[0]` and\n    `scale[1]`. Perform upsample progressively: i.e. x12:= x2->x2->x3\n\n    Args:\n        image: tensors to upsample\n        method: method could be \'espcn\', \'nearest\' or \'deconv\'\n        scale: None or int or [int, int]. If None, `scale`=`self.scale`\n        direct_output: output channel is the desired RGB or Grayscale, if\n          False, keep the same channels as `image`\n    """"""\n    _allowed_method = (\'espcn\', \'nearest\', \'deconv\')\n    assert str(method).lower() in _allowed_method\n    method = str(method).lower()\n    act = kwargs.get(\'activator\')\n    ki = kwargs.get(\'kernel_initializer\', \'he_normal\')\n    kr = kwargs.get(\'kernel_regularizer\', \'l2\')\n    use_bias = kwargs.get(\'use_bias\', True)\n\n    scale_x, scale_y = to_list(scale, 2) or self.scale\n    features = self.channel if direct_output else image.shape.as_list()[-1]\n    while scale_x > 1 or scale_y > 1:\n      if scale_x % 2 == 1 or scale_y % 2 == 1:\n        if method == \'espcn\':\n          image = pixel_shift(self.conv2d(\n              image, features * scale_x * scale_y, 3,\n              use_bias=use_bias, kernel_initializer=ki, kernel_regularizer=kr),\n              [scale_x, scale_y], features)\n        elif method == \'nearest\':\n          image = pixel_shift(\n              tf.concat([image] * scale_x * scale_y, -1),\n              [scale_x, scale_y],\n              image.shape[-1])\n        elif method == \'deconv\':\n          image = self.deconv2d(image, features, 3,\n                                strides=[scale_y, scale_x],\n                                kernel_initializer=ki,\n                                kernel_regularizer=kr,\n                                use_bias=use_bias)\n        if act:\n          image = act(image)\n        break\n      else:\n        scale_x //= 2\n        scale_y //= 2\n        if method == \'espcn\':\n          image = pixel_shift(self.conv2d(\n              image, features * 4, 3, use_bias=use_bias,\n              kernel_initializer=ki, kernel_regularizer=kr), [2, 2], features)\n        elif method == \'nearest\':\n          image = pixel_shift(\n              tf.concat([image] * 4, -1),\n              [2, 2],\n              image.shape[-1])\n        elif method == \'deconv\':\n          image = self.deconv2d(image, features, 3, strides=2,\n                                use_bias=use_bias,\n                                kernel_initializer=ki, kernel_regularizer=kr)\n        if act:\n          image = act(image)\n    return image\n\n  def __getattr__(self, item):\n    from functools import partial as _p\n    """"""Make an alignment for easy calls. You can add more patterns as below.\n    \n    >>> Layers.relu_conv2d = Layers.conv2d(activation=\'relu\')\n    >>> Layers.bn_conv2d = Layers.conv2d(use_batchnorm=True)\n    >>> Layers.sn_leaky_conv2d = Layers.conv2d(use_sn=True, activation=\'lrelu\')\n    \n    NOTE: orders do not matter.\n    """"""\n    if \'conv2d\' in item:\n      items = item.split(\'_\')\n      kwargs = {\n        \'kernel_initializer\': \'he_normal\',\n        \'kernel_regularizer\': \'l2\',\n        \'use_batchnorm\': False,\n        \'use_sn\': False,\n      }\n      if \'bn\' in items or \'batchnorm\' in items:\n        kwargs.update(use_batchnorm=True)\n      if \'sn\' in items or \'spectralnorm\' in items:\n        kwargs.update(use_sn=True)\n      if \'relu\' in items:\n        kwargs.update(activation=\'relu\')\n      if \'leaky\' in items or \'lrelu\' in items or \'leakyrelu\' in items:\n        kwargs.update(activation=\'lrelu\')\n      if \'prelu\' in items:\n        kwargs.update(activation=\'prelu\')\n      if \'tanh\' in items:\n        kwargs.update(activation=\'tanh\')\n      return _p(self.conv2d, **kwargs)\n    elif \'conv3d\' in item:\n      items = item.split(\'_\')\n      kwargs = {\n        \'kernel_initializer\': \'he_normal\',\n        \'kernel_regularizer\': \'l2\',\n        \'use_batchnorm\': False,\n      }\n      if \'bn\' in items or \'batchnorm\' in items:\n        kwargs.update(use_batchnorm=True)\n      if \'relu\' in items:\n        kwargs.update(activation=\'relu\')\n      if \'leaky\' in items or \'lrelu\' in items or \'leakyrelu\' in items:\n        kwargs.update(activation=\'lrelu\')\n      if \'prelu\' in items:\n        kwargs.update(activation=\'prelu\')\n      if \'tanh\' in items:\n        kwargs.update(activation=\'tanh\')\n      return _p(self.conv3d, **kwargs)\n    elif \'dense\' in item or \'linear\' in item:\n      items = item.split(\'_\')\n      kwargs = {\n        \'kernel_initializer\': \'he_normal\',\n        \'kernel_regularizer\': \'l2\',\n        \'use_sn\': False,\n      }\n      if \'sn\' in items or \'spectralnorm\' in items:\n        kwargs.update(use_sn=True)\n      if \'relu\' in items:\n        kwargs.update(activation=\'relu\')\n      if \'leaky\' in items or \'lrelu\' in items or \'leakyrelu\' in items:\n        kwargs.update(activation=\'lrelu\')\n      if \'prelu\' in items:\n        kwargs.update(activation=\'prelu\')\n      if \'tanh\' in items:\n        kwargs.update(activation=\'tanh\')\n      return _p(self.dense, **kwargs)\n\n    return None\n\n  def resblock(self, x, filters, kernel_size, strides=(1, 1), padding=\'same\',\n               data_format=\'channels_last\', activation=None, use_bias=True,\n               use_batchnorm=False, use_sn=False,\n               kernel_initializer=\'he_normal\',\n               kernel_regularizer=\'l2\', placement=None, **kwargs):\n    """"""make a residual block\n\n    Args:\n        x:\n        filters:\n        kernel_size:\n        strides: if strides is more than 1, resblock downsample in the 2nd\n          conv, and the short cut 1x1 conv\n        padding:\n        data_format:\n        activation:\n        use_bias:\n        use_batchnorm:\n        use_sn:\n        kernel_initializer:\n        kernel_regularizer:\n        placement: \'front\' or \'behind\', use BN layer in front of or behind\n          after the 1st conv2d layer.\n    """"""\n\n    kwargs.update({\n      \'padding\': padding,\n      \'data_format\': data_format,\n      \'activation\': activation,\n      \'use_bias\': use_bias,\n      \'use_batchnorm\': use_batchnorm,\n      \'use_sn\': use_sn,\n      \'kernel_initializer\': kernel_initializer,\n      \'kernel_regularizer\': kernel_regularizer\n    })\n    if placement is None:\n      placement = \'behind\'\n    assert placement in (\'front\', \'behind\')\n    name = pop_dict_wo_keyerror(kwargs, \'name\')\n    reuse = pop_dict_wo_keyerror(kwargs, \'reuse\')\n    with tf.variable_scope(name, \'ResBlock\', reuse=reuse):\n      ori = x\n      if placement == \'front\':\n        act = self._act(activation)\n        if use_batchnorm:\n          x = tf.layers.batch_normalization(\n              x, training=self.training_phase)\n        if callable(act):\n          x = act(x)\n      x = self.conv2d(x, filters, kernel_size, **kwargs)\n      kwargs.pop(\'activation\')\n      if placement == \'front\':\n        kwargs.pop(\'use_batchnorm\')\n      strides = to_list(strides, 2)\n      x = self.conv2d(x, filters, kernel_size, strides=strides, **kwargs)\n      if ori.shape[-1] != x.shape[-1] or strides[0] > 1:\n        # short cut\n        ori = self.conv2d(ori, x.shape[-1], 1, strides=strides,\n                          kernel_initializer=kernel_initializer)\n      ori += x\n    return ori\n\n  def resblock3d(self, x, filters, kernel_size, strides=(1, 1, 1),\n                 padding=\'same\',\n                 data_format=\'channels_last\', activation=None, use_bias=True,\n                 use_batchnorm=False, kernel_initializer=\'he_normal\',\n                 kernel_regularizer=\'l2\', placement=None, **kwargs):\n    """"""make a residual block\n\n    Args:\n        x:\n        filters:\n        kernel_size:\n        strides:\n        padding:\n        data_format:\n        activation:\n        use_bias:\n        use_batchnorm:\n        kernel_initializer:\n        kernel_regularizer:\n        placement: \'front\' or \'behind\', use BN layer in front of or behind\n          after the 1st conv2d layer.\n    """"""\n\n    kwargs.update({\n      \'padding\': padding,\n      \'data_format\': data_format,\n      \'activation\': activation,\n      \'use_bias\': use_bias,\n      \'use_batchnorm\': use_batchnorm,\n      \'kernel_initializer\': kernel_initializer,\n      \'kernel_regularizer\': kernel_regularizer\n    })\n    if placement is None:\n      placement = \'behind\'\n    assert placement in (\'front\', \'behind\')\n    name = pop_dict_wo_keyerror(kwargs, \'name\')\n    reuse = pop_dict_wo_keyerror(kwargs, \'reuse\')\n    with tf.variable_scope(name, \'ResBlock\', reuse=reuse):\n      ori = x\n      if placement == \'front\':\n        act = self._act(activation)\n        if use_batchnorm:\n          x = tf.layers.batch_normalization(\n              x, training=self.training_phase)\n        if act:\n          x = act(x)\n      x = self.conv3d(x, filters, kernel_size, **kwargs)\n      kwargs.pop(\'activation\')\n      if placement == \'front\':\n        kwargs.pop(\'use_batchnorm\')\n      strides = to_list(strides, 3)\n      x = self.conv3d(x, filters, kernel_size, strides=strides, **kwargs)\n      if ori.shape[-1] != x.shape[-1] or strides[0] > 1:\n        # short cut\n        ori = self.conv3d(ori, x.shape[-1], 1, strides=strides,\n                          kernel_initializer=kernel_initializer)\n      ori += x\n    return ori\n'"
VSR/Backend/TF/Framework/Motion.py,77,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Aug 21st 2018\n\nUtility for motion compensation\n""""""\nimport numpy as np\nimport tensorflow as tf\n\n\ndef _grid_norm(width, height, bounds=(-1.0, 1.0)):\n  """"""generate a normalized mesh grid\n\n    Args:\n        width: width of the pixels(mesh)\n        height: height of the pixels\n        bounds: normalized lower and upper bounds\n    Return:\n        This should be equivalent to:\n        >>>  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n        >>>                         np.linspace(-1, 1, height))\n        >>>  ones = np.ones(np.prod(x_t.shape))\n        >>>  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n  """"""\n  x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),\n                  tf.transpose(tf.expand_dims(\n                      tf.linspace(*bounds, width), 1), [1, 0]))\n  y_t = tf.matmul(tf.expand_dims(tf.linspace(*bounds, height), 1),\n                  tf.ones(shape=tf.stack([1, width])))\n\n  grid = tf.stack([x_t, y_t], axis=-1)\n  return grid\n\n\ndef _grid(width, height, batch=1, dtype=None, with_batch=False):\n  """"""generate a mesh grid\n\n    Args:\n        batch: batch size\n        width: width of the pixels(mesh)\n        height: height of the pixels\n    Return:\n        A grid of shape [B, H, W, 3]\n  """"""\n  b = tf.range(0, batch)\n  h = tf.range(0, height)\n  w = tf.range(0, width)\n  grid = tf.meshgrid(w, h, b)\n  grid.reverse()\n  grid = tf.stack(grid, -1)\n  grid = tf.transpose(grid, [2, 0, 1, 3])\n  if dtype:\n    grid = tf.cast(grid, dtype)\n  if with_batch:\n    return grid\n  return grid[..., 1:]\n\n\ndef _sample(image, x, y):\n  """"""bilinear sample image at coordinate x, y\n\n    Args:\n        image: a 4-D tensor of shape [B H W C]\n        x, y: a 3-D tensor of shape [B H W], pixel index of image\n\n    Return:\n        sampled images, that\n        output[i, j] = image[y[i], x[j]]\n  """"""\n\n  shape = tf.shape(image)\n  batch = shape[0]\n  h = shape[1]\n  w = shape[2]\n\n  x = tf.to_float(x)\n  y = tf.to_float(y)\n  image = tf.to_float(image)\n  x0 = tf.to_int32(tf.floor(x))\n  y0 = tf.to_int32(tf.floor(y))\n  x1 = x0 + 1\n  y1 = y0 + 1\n\n  w00 = tf.expand_dims((tf.to_float(x1) - x) * (tf.to_float(y1) - y), -1)\n  w01 = tf.expand_dims((x - tf.to_float(x0)) * (tf.to_float(y1) - y), -1)\n  w10 = tf.expand_dims((tf.to_float(x1) - x) * (y - tf.to_float(y0)), -1)\n  w11 = tf.expand_dims((x - tf.to_float(x0)) * (y - tf.to_float(y0)), -1)\n\n  x0 = tf.clip_by_value(x0, 0, w - 1)\n  y0 = tf.clip_by_value(y0, 0, h - 1)\n  x1 = tf.clip_by_value(x1, 0, w - 1)\n  y1 = tf.clip_by_value(y1, 0, h - 1)\n\n  batch_idx = tf.reshape(tf.range(0, batch), [batch, 1, 1])\n  batch_idx = tf.tile(batch_idx, [1, h, w])\n  gather_00 = tf.stack([batch_idx, y0, x0], axis=-1)\n  gather_01 = tf.stack([batch_idx, y0, x1], axis=-1)\n  gather_10 = tf.stack([batch_idx, y1, x0], axis=-1)\n  gather_11 = tf.stack([batch_idx, y1, x1], axis=-1)\n\n  p00 = tf.gather_nd(image, gather_00) * w00\n  p01 = tf.gather_nd(image, gather_01) * w01\n  p10 = tf.gather_nd(image, gather_10) * w10\n  p11 = tf.gather_nd(image, gather_11) * w11\n\n  return tf.add_n([p00, p01, p10, p11])\n\n\ndef _move(image, x, y):\n  """"""move source image to target coordinate x, y""""""\n  shape = tf.shape(image)\n  batch = shape[0]\n  h = shape[1]\n  w = shape[2]\n\n  x = tf.to_float(x)\n  y = tf.to_float(y)\n  image = tf.to_float(image)\n  x0 = tf.to_int32(tf.floor(x))\n  y0 = tf.to_int32(tf.floor(y))\n  x1 = x0 + 1\n  y1 = y0 + 1\n\n  w00 = tf.expand_dims((tf.to_float(x1) - x) * (tf.to_float(y1) - y), -1)\n  w01 = tf.expand_dims((x - tf.to_float(x0)) * (tf.to_float(y1) - y), -1)\n  w10 = tf.expand_dims((tf.to_float(x1) - x) * (y - tf.to_float(y0)), -1)\n  w11 = tf.expand_dims((x - tf.to_float(x0)) * (y - tf.to_float(y0)), -1)\n\n  x0 = tf.clip_by_value(x0, 0, w - 1)\n  y0 = tf.clip_by_value(y0, 0, h - 1)\n  x1 = tf.clip_by_value(x1, 0, w - 1)\n  y1 = tf.clip_by_value(y1, 0, h - 1)\n\n  batch_idx = tf.reshape(tf.range(0, batch), [batch, 1, 1])\n  batch_idx = tf.tile(batch_idx, [1, h, w])\n  scatter_00 = tf.stack([batch_idx, y0, x0], axis=-1)\n  scatter_01 = tf.stack([batch_idx, y0, x1], axis=-1)\n  scatter_10 = tf.stack([batch_idx, y1, x0], axis=-1)\n  scatter_11 = tf.stack([batch_idx, y1, x1], axis=-1)\n\n  p00 = tf.scatter_nd(scatter_00, image * w00, shape)\n  p01 = tf.scatter_nd(scatter_01, image * w01, shape)\n  p10 = tf.scatter_nd(scatter_10, image * w10, shape)\n  p11 = tf.scatter_nd(scatter_11, image * w11, shape)\n\n  return tf.add_n([p00, p01, p10, p11])\n\n\ndef warp(image, u, v, additive_warp=True, normalized=False):\n  """"""warp the image with flow(u, v)\n\n  If flow=[u, v], representing motion from img1 to img2\n  then `warp(img2, u, v)->img1~`\n\n  Args:\n       image: a 4-D tensor [B, H, W, C], images to warp\n       u: horizontal motion vectors of optical flow\n       v: vertical motion vectors of optical flow\n       additive_warp: a boolean, if False, regard [u, v]\n         as destination coordinate rather than motion\n         vectors.\n       normalized: a boolean, if True, regard [u, v] as\n       [-1, 1] and scaled to [-W, W], [-H, H] respectively.\n\n  Note: usually nobody uses a normalized optical flow...\n  """"""\n  shape = tf.shape(image)\n  b, h, w = shape[0], shape[1], shape[2]\n\n  if normalized:\n    if not additive_warp:\n      u = (u + 1) * 0.5\n      v = (v + 1) * 0.5\n    u *= tf.to_float(w)\n    v *= tf.to_float(h)\n\n  if additive_warp:\n    grids = _grid(w, h, dtype=tf.float32)\n    u += grids[..., 1]\n    v += grids[..., 0]\n\n  return _sample(image, u, v)\n\n\ndef epe(label, predict):\n  """"""End-point error of optical flow""""""\n  ux, vx = predict[..., 0], predict[..., 1]\n  uy, vy = label[..., 0], label[..., 1]\n  diff = tf.squared_difference(ux, uy) + tf.squared_difference(vx, vy)\n  return tf.sqrt(diff, name=\'EPE\')\n\n\ndef viz_flow(flow):\n  """"""Visualize optical flow in TF""""""\n  from VSR.Util.VisualizeOpticalFlow import _color_wheel\n  with tf.name_scope(\'VizFlow\'):\n    color_wheel = _color_wheel().astype(\'float32\')\n    n_cols = color_wheel.shape[0]\n    u, v = flow[..., 0], flow[..., 1]\n    rot = tf.atan2(-v, -u) / np.pi\n    fk = (rot + 1) / 2 * (n_cols - 1)  # -1~1 mapped to 0~n_cols\n    k0 = tf.to_int32(fk)  # 0, 1, 2, ..., n_cols\n    k1 = tf.mod(k0 + 1, n_cols)\n    f = fk - tf.to_float(k0)\n    f = tf.expand_dims(f, -1)\n    col0 = tf.gather_nd(color_wheel, tf.expand_dims(k0, -1))\n    col1 = tf.gather_nd(color_wheel, tf.expand_dims(k1, -1))\n    col = (1 - f) * col0 + f * col1\n  return col\n'"
VSR/Backend/TF/Framework/Noise.py,13,"b'""""""\nCopyright: Wenyi Tang 2017-2019\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 25th 2018\n\nUtility for complicated noise model\nRef:\n[1] https://arxiv.org/abs/1807.04686\n""""""\n\nimport tensorflow as tf\n\n\ndef tf_camera_response_function(inputs, crf_table, max_val=1):\n  """"""TF implementation of CRF.""""""\n\n  with tf.name_scope(\'CRF\'):\n    inputs_norm = tf.clip_by_value(inputs / max_val, 0, 1)\n    quant = int(crf_table.shape[0] - 1)\n    inputs_index = tf.to_int32(inputs_norm * quant)\n    return tf.nn.embedding_lookup(crf_table, inputs_index)\n\n\ndef tf_poisson_noise(inputs, stddev=None, sigma_max=0.16):\n  with tf.name_scope(\'PoissonNoise\'):\n    if stddev is None:\n      stddev = tf.random_uniform(inputs.shape[-1:], maxval=sigma_max)\n    stddev = tf.reshape(stddev, [1, 1, 1, -1])\n    sigma_map = (1 - inputs) * stddev\n    return tf.random_normal(tf.shape(inputs)) * sigma_map\n\n\ndef tf_gaussian_noise(inputs, stddev=None, sigma_max=0.06, channel_wise=True):\n  with tf.name_scope(\'GaussianNoise\'):\n    channel = inputs.shape[-1:] if channel_wise else [1]\n    if stddev is None:\n      stddev = tf.random_uniform(channel, maxval=sigma_max)\n    stddev = tf.reshape(stddev, [1, 1, 1, -1])\n    noise_map = tf.random_normal(tf.shape(inputs)) * stddev\n    return noise_map\n\n\ndef tf_gaussian_poisson_noise(inputs, stddev_s=None, stddev_c=None,\n                              max_s=0.16, max_c=0.06):\n  with tf.name_scope(\'PoissonGaussianNoise\'):\n    ns = tf_poisson_noise(inputs, stddev_s, max_s)\n    nc = tf_gaussian_noise(inputs, stddev_c, max_c)\n    return ns + nc\n'"
VSR/Backend/TF/Framework/SuperResolution.py,32,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 9th 2018\nUpdated Date: June 15th 2018\n\nFramework for network model (tensorflow)\n""""""\nimport logging\nfrom pathlib import Path\n\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom .LayersHelper import Layers\nfrom .Trainer import VSR\n\nLOG = logging.getLogger(\'VSR.Framework.TF\')\n\nclass SuperResolution(Layers):\n  """"""A utility class helps for building SR architectures easily\n\n  Usage:\n      Inherit from `SuperResolution` and implement:\n        >>> build_graph()\n        >>> build_loss()\n        >>> build_summary()\n        >>> build_saver()\n      If you want to export gragh as a protobuf (say model.pb), implement:\n        >>> export_freeze_model()\n      and call its super method at the end\n  """"""\n\n  def __init__(self, scale, channel, weight_decay=0, **kwargs):\n    """"""Common initialize parameters\n\n    Args:\n        scale: the scale factor, can be a list of 2 integer to specify\n          different stretch in width and height\n        channel: input color channel\n        weight_decay: decay of L2 regularization on trainable weights\n    """"""\n\n    self.scale = to_list(scale, repeat=2)\n    self.channel = channel\n    self.weight_decay = weight_decay  # weights regularization\n    self.rgba = False  # deprecated\n    self._trainer = VSR  # default trainer\n\n    self.inputs = []  # hold placeholder for model inputs\n    # hold some image procession for inputs (i.e. RGB->YUV, if you need)\n    self.inputs_preproc = []\n    self.label = []  # hold placeholder for model labels\n    self.outputs = []  # hold output tensors\n    self.loss = []  # this is the optimize op\n    self.train_metric = {}  # metrics show at training phase\n    self.metrics = {}  # metrics record in tf.summary and show at benchmark\n    self.feed_dict = {}\n    self.savers = {}\n    self.global_steps = None\n    self.training_phase = None  # only useful for bn\n    self.learning_rate = None\n    self.summary_op = None\n    self.summary_writer = None\n    self.compiled = False\n    self.pre_ckpt = None\n    self.unknown_args = kwargs\n\n  def __getattr__(self, item):\n    """"""return extra initialized parameters""""""\n    if item in self.unknown_args:\n      return self.unknown_args.get(item)\n    return super(SuperResolution, self).__getattr__(item)\n\n  @property\n  def executor(self):\n    return self.get_executor(None)\n\n  def get_executor(self, root):\n    if issubclass(self._trainer.__class__, type):\n      self._trainer = self._trainer(self, root)\n      return self._trainer\n    else:\n      return self._trainer\n\n  def cuda(self):\n    pass\n\n  def load(self, ckpt):\n    self.pre_ckpt = ckpt\n\n  def compile(self):\n    """"""build entire graph and training ops""""""\n\n    self.global_steps = tf.Variable(0, trainable=False, name=\'global_step\')\n    self.training_phase = tf.placeholder(tf.bool, name=\'is_training\')\n    self.learning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n    self.build_graph()\n    self.build_loss()\n    self.build_summary()\n    self.summary_op = tf.summary.merge_all()\n    self.build_saver()\n    self.compiled = True\n    return self\n\n  def display(self):\n    """"""print model information""""""\n\n    pass\n\n  def build_saver(self):\n    """"""Build variable savers.\n\n    By default, I build a saver to save all variables. In case you need to\n    recover a part of variables, you can inherit this method and create\n    multiple savers for different variables. All savers should arrange in\n    a dict which maps saver and its saving name\n    """"""\n\n    default_saver = tf.train.Saver(max_to_keep=3, allow_empty=True)\n    self.savers = {self.name: default_saver}\n\n  def build_graph(self):\n    """"""this super method create input and label placeholder\n\n    Note: You can also suppress this method and create your own inputs from\n      scratch\n    """"""\n    self.inputs.append(\n        tf.placeholder(tf.uint8, shape=[None, None, None, None],\n                       name=\'input/lr\'))\n    inputs_f = tf.to_float(self.inputs[0])\n    # separate additional channels (e.g. alpha channel)\n    self.inputs_preproc.append(inputs_f[..., self.channel:])\n    self.inputs_preproc.append(inputs_f[..., :self.channel])\n    self.inputs_preproc[-1].set_shape([None, None, None, self.channel])\n    self.label.append(\n        tf.placeholder(tf.float32, shape=[None, None, None, self.channel],\n                       name=\'label/hr\'))\n\n  def build_loss(self):\n    """"""help to build mse loss via `self.label[-1]` and `self.outputs[-1]`\n    for simplicity.\n\n    Note: You can also suppress this method and build your own loss\n      function from scratch.\n    """"""\n\n    opt = tf.train.AdamOptimizer(self.learning_rate)\n    mse = tf.losses.mean_squared_error(self.label[-1], self.outputs[-1])\n    loss = tf.losses.get_total_loss()\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      self.loss.append(opt.minimize(loss, self.global_steps))\n\n    return mse, loss\n\n  def build_summary(self):\n    """"""summary scalars in metrics""""""\n    for k, v in self.metrics.items():\n      tf.summary.scalar(k, v)\n\n  def train_batch(self, feature, label, learning_rate=1e-4, **kwargs):\n    """"""training one batch one step.\n\n    Args:\n        feature: input tensors, LR image1 for SR use case\n        label: labels, HR image1 for SR use case\n        learning_rate: update step size in current calculation\n        kwargs: for future use\n\n    Return:\n        the results of ops in `self.loss`\n    """"""\n\n    feature = to_list(feature)\n    label = to_list(label)\n    self.feed_dict.update(\n        {self.training_phase: True, self.learning_rate: learning_rate})\n    for i in range(len(self.inputs)):\n      self.feed_dict[self.inputs[i]] = feature[i]\n    for i in range(len(self.label)):\n      self.feed_dict[self.label[i]] = label[i]\n    loss = kwargs.get(\'loss\') or self.loss\n    loss = to_list(loss)\n    loss = tf.get_default_session().run(\n        list(self.train_metric.values()) + loss, feed_dict=self.feed_dict)\n    ret = {}\n    for k, v in zip(self.train_metric, loss):\n      ret[k] = v\n    return ret\n\n  def test_batch(self, inputs, label=None, **kwargs):\n    """"""test one batch\n\n    Args:\n        inputs: LR images\n        label: if None, return only predicted outputs;\n          else return outputs along with metrics\n        kwargs: for future use\n\n    Return:\n        predicted outputs, metrics if `label` is not None\n    """"""\n\n    feature = to_list(inputs)\n    label = to_list(label)\n    self.feed_dict.update({self.training_phase: False})\n    for i in range(len(self.inputs)):\n      self.feed_dict[self.inputs[i]] = feature[i]\n    if label:\n      for i in range(len(self.label)):\n        self.feed_dict[self.label[i]] = label[i]\n      results = tf.get_default_session().run(\n          self.outputs + list(self.metrics.values()),\n          feed_dict=self.feed_dict)\n      outputs, metrics = results[:len(self.outputs)], results[\n                                                      len(self.outputs):]\n    else:\n      results = tf.get_default_session().run(self.outputs,\n                                             feed_dict=self.feed_dict)\n      outputs, metrics = results, []\n    ret = {}\n    for k, v in zip(self.metrics, metrics):\n      ret[k] = v\n    return outputs, ret\n\n  def summary(self):\n    return tf.get_default_session().run(self.summary_op,\n                                        feed_dict=self.feed_dict)\n\n  def export_freeze_model(self, export_dir=\'.\', version=1):\n    """"""export model as a constant protobuf.\n\n    Unlike saved model, this one is not trainable\n\n    Args:\n        export_dir: directory to save the exported model\n        version: version of the exported model\n    """"""\n\n    self.outputs = tf.identity_n(self.outputs, name=\'output/hr\')\n    sess = tf.get_default_session()\n    export_path = Path(export_dir) / str(version)\n    while export_path.exists():\n      version += 1  # step ahead 1 version\n      export_path = Path(export_dir) / str(version)\n    export_path = str(export_path)\n    graph = sess.graph.as_graph_def()\n    graph = tf.graph_util.remove_training_nodes(graph)\n    graph = tf.graph_util.convert_variables_to_constants(\n        sess, graph, [outp.name.split(\':\')[0] for outp in self.outputs])\n    tf.train.write_graph(graph, export_path, self.name, as_text=False)\n    LOG.info(""Model exported to {}/{}."".format(export_path, self.name))\n\n  def export_saved_model(self, export_dir=\'.\', version=1):\n    """"""export a saved model\n\n    Args:\n        export_dir: directory to save the saved model\n        version: version of the exported model\n    """"""\n\n    sess = tf.get_default_session()\n    export_path = Path(export_dir) / str(version)\n    while export_path.exists():\n      version += 1  # step ahead 1 version\n      export_path = Path(export_dir) / str(version)\n    export_path = str(export_path)\n    LOG.debug(""exporting to {}"".format(export_path))\n    builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n    # build the signature_def_map\n    inputs, outputs = {}, {}\n    for n, inp in enumerate(self.inputs):\n      tag = \'input_\' + str(n)\n      inputs[tag] = tf.saved_model.utils.build_tensor_info(inp)\n    for n, outp in enumerate(self.outputs):\n      tag = \'output_\' + str(n)\n      outputs[tag] = tf.saved_model.utils.build_tensor_info(outp)\n    sig = tf.saved_model.signature_def_utils.build_signature_def(\n        inputs=inputs, outputs=outputs,\n        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n\n    builder.add_meta_graph_and_variables(\n        sess, [tf.saved_model.tag_constants.SERVING],\n        signature_def_map={\n          tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: sig\n        },\n        strip_default_attrs=True)\n    builder.save()\n'"
VSR/Backend/TF/Framework/Trainer.py,14,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Oct 15th 2018\n\nExtend the pre-Environment module, provide different and extensible\ntraining methodology for SISR, VSR or other image tasks.\n""""""\n\nimport logging\nimport time\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\nimport tqdm\n\nfrom VSR.Util import Config, to_list\n\nLOG = logging.getLogger(\'VSR.Framework\')\n\n\ndef _make_ckpt_name(name, scale, step):\n  return \'{}-sc{}-ep{:04d}.ckpt\'.format(name, scale, step)\n\n\ndef _parse_ckpt_name(name):\n  # sample name: {model}-sc{scale}-ep{epoch}.ckpt(.index)\n  if not name:\n    return 0\n  model_name, scale, epochs = Path(name).stem.split(\'.\')[0].split(\'-\')\n  return int(epochs[2:])\n\n\ndef _ensemble_expand(feature):\n  r0 = feature\n  r1 = np.rot90(feature, 1, axes=[-3, -2])\n  r2 = np.rot90(feature, 2, axes=[-3, -2])\n  r3 = np.rot90(feature, 3, axes=[-3, -2])\n  r4 = np.flip(feature, axis=-2)\n  r5 = np.rot90(r4, 1, axes=[-3, -2])\n  r6 = np.rot90(r4, 2, axes=[-3, -2])\n  r7 = np.rot90(r4, 3, axes=[-3, -2])\n  return r0, r1, r2, r3, r4, r5, r6, r7\n\n\ndef _ensemble_reduce_mean(outputs):\n  results = []\n  for i in outputs:\n    outputs_ensemble = [\n      i[0],\n      np.rot90(i[1], 3, axes=[-3, -2]),\n      np.rot90(i[2], 2, axes=[-3, -2]),\n      np.rot90(i[3], 1, axes=[-3, -2]),\n      np.flip(i[4], axis=-2),\n      np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2),\n      np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2),\n      np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2),\n    ]\n    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))\n  return results\n\n\nclass Trainer:\n  """"""A pure interface trainer.\n\n     A trainer provides following APIs:\n       >>> Trainer.fit\n       >>> Trainer.infer\n       >>> Trainer.benchmark\n       >>> Trainer.export\n\n     Args:\n         model: the SR model object. @see SuperResolution\n         work_dir: the dir to save training checkpoints and logs\n         verbose: tf logging level\n     """"""\n\n  def __init__(self, model, work_dir):\n    self._m = model\n    self._saved = None\n    self._logd = None\n    if work_dir is not None:\n      self._saved = Path(work_dir) / \'save\'\n      self._logd = Path(work_dir) / \'log\'\n    self._restored = False\n\n  def _startup(self):\n    if isinstance(self._saved, Path):\n      self._saved.mkdir(parents=True, exist_ok=True)\n    if isinstance(self._logd, Path):\n      self._logd.mkdir(parents=True, exist_ok=True)\n      if LOG.isEnabledFor(logging.DEBUG):\n        hdl = logging.FileHandler(self._logd / \'training.txt\')\n        LOG.addHandler(hdl)\n    if self.model.compiled:\n      self.graph = tf.get_default_graph()\n    else:\n      with tf.Graph().as_default() as g:\n        self.model.compile()\n        self.graph = g\n\n  def __enter__(self):\n    """"""Create session of tensorflow and build model graph""""""\n\n    self._startup()\n    conf = tf.ConfigProto(\n        allow_soft_placement=True,\n        gpu_options=tf.GPUOptions(allow_growth=True))\n    sess = tf.Session(graph=self.graph, config=conf)\n    sess.__enter__()\n    self.model.display()\n    self.savers = self.model.savers\n    sess.run(tf.global_variables_initializer())\n    return self\n\n  def __exit__(self, exc_type, exc_val, exc_tb):\n    """"""Close session""""""\n\n    sess = tf.get_default_session()\n    sess.__exit__(exc_type, exc_val, exc_tb)\n\n  def _find_last_ckpt(self):\n    # restore the latest checkpoint in save dir\n    if self._saved is not None:\n      ckpt = tf.train.get_checkpoint_state(self._saved)\n      if ckpt and ckpt.model_checkpoint_path:\n        return tf.train.latest_checkpoint(self._saved)\n      # try another way\n      ckpt = to_list(self._saved.glob(\'*.ckpt.index\'))\n      # sort as modification time\n      ckpt = sorted(ckpt, key=lambda x: x.stat().st_mtime_ns)\n      return self._saved / ckpt[-1].stem if ckpt else None\n\n  def _restore_model(self, sess):\n    last_checkpoint_step = 0\n    if self.model.pre_ckpt is not None:\n      _saved = Path(self.model.pre_ckpt)\n    else:\n      _saved = self._saved\n    if _saved is None:\n      return last_checkpoint_step\n    for name in self.savers:\n      saver = self.savers.get(name)\n      ckpt = to_list(_saved.glob(\'{}*.index\'.format(name)))\n      if ckpt:\n        ckpt = sorted(ckpt, key=lambda x: x.stat().st_mtime_ns)\n        ckpt = _saved / ckpt[-1].stem\n        try:\n          saver.restore(sess, str(ckpt))\n        except tf.errors.NotFoundError:\n          LOG.warning(\n              \'{} of model {} could not be restored\'.format(\n                  name, self.model.name))\n        last_checkpoint_step = _parse_ckpt_name(ckpt)\n    return last_checkpoint_step\n\n  def _save_model(self, sess, step):\n    if self._saved is None:\n      return\n    for name in self.savers:\n      saver = self.savers.get(name)\n      file = self._saved / _make_ckpt_name(name, self.model.scale[0], step)\n      saver.save(sess, str(file))\n\n  def _restore(self):\n    # restore graph\n    sess = tf.get_default_session()\n    if sess is None:\n      raise RuntimeError(\'No session initialized\')\n    if self._restored:\n      return sess\n    self.last_epoch = self._restore_model(sess)\n    self._restored = True\n    return sess\n\n  def export(self, export_dir=\'.\', freeze_model=False):\n    """"""Export model as protobuf\n\n    Args:\n        export_dir: directory to save the exported model\n        freeze_model: freeze all trainable variables\n    """"""\n\n    self._restore()\n    if freeze_model:\n      self.model.export_freeze_model(export_dir)\n    else:\n      self.model.export_saved_model(export_dir)\n\n  def set_seed(self, seed):\n    np.random.seed(seed)\n    tf.set_random_seed(seed)\n\n  def fit(self, *args, **kwargs):\n    raise NotImplementedError\n\n  def infer(self, *args, **kwargs):\n    raise NotImplementedError\n\n  def benchmark(self, *args, **kwargs):\n    raise NotImplementedError\n\n  @property\n  def model(self):\n    return self._m\n\n\nclass VSR(Trainer):\n  """"""Default trainer for task SISR or VSR""""""\n  v = Config()  # local variables\n  """"""=======================================\n      components, sub-functions, helpers\n     =======================================\n  """"""\n\n  def query_config(self, config, **kwargs) -> Config:\n    config = Config(config or {})\n    config.update(kwargs)  # override parameters\n    self.v.epoch = config.epoch  # current epoch\n    self.v.epochs = config.epochs or 1  # total epochs\n    self.v.lr = config.lr or 1e-4  # learning rate\n    self.v.batch_shape = config.batch_shape or [1, -1, -1, -1]\n    self.v.steps = config.steps or 200\n    self.v.val_steps = config.val_steps or -1\n    self.v.lr_schedule = config.lr_schedule\n    self.v.memory_limit = config.memory_limit\n    self.v.inference_results_hooks = config.inference_results_hooks or []\n    self.v.validate_every_n_epoch = config.validate_every_n_epoch or 1\n    self.v.traced_val = config.traced_val\n    self.v.ensemble = config.ensemble\n    self.v.cuda = config.cuda\n    return self.v\n\n  def fit_init(self) -> bool:\n    v = self.v\n    v.sess = self._restore()\n    if self.last_epoch >= v.epochs:\n      LOG.info(f\'Found pre-trained epoch {v.epoch}>=target {v.epochs},\'\n               \' quit fitting.\')\n      return False\n    LOG.info(\'Fitting: {}\'.format(self.model.name.upper()))\n    v.summary_writer = tf.summary.FileWriter(\n        str(self._logd), graph=tf.get_default_graph())\n    v.global_step = self.model.global_steps.eval()\n    return True\n\n  def fit_close(self):\n    # flush all pending summaries to disk\n    if self.v.summary_writer:\n      self.v.summary_writer.close()\n    LOG.info(f\'Training {self.model.name.upper()} finished.\')\n\n  def fn_train_each_epoch(self):\n    v = self.v\n    mem = v.memory_limit\n    train_iter = v.train_loader.make_one_shot_iterator(v.batch_shape,\n                                                       v.steps,\n                                                       shuffle=True,\n                                                       memory_limit=mem)\n    v.train_loader.prefetch(v.memory_limit)\n    date = time.strftime(\'%Y-%m-%d %T\', time.localtime())\n    v.avg_meas = {}\n    if v.lr_schedule and callable(v.lr_schedule):\n      v.lr = v.lr_schedule(steps=v.global_step)\n    print(\'| {} | Epoch: {}/{} | LR: {:.2g} |\'.format(\n        date, v.epoch, v.epochs, v.lr))\n    with tqdm.tqdm(train_iter, unit=\'batch\', ascii=True) as r:\n      for items in r:\n        self.fn_train_each_step(items)\n        r.set_postfix(v.loss)\n    for _k, _v in v.avg_meas.items():\n      print(\'| Epoch average {} = {:.6f} |\'.format(_k, np.mean(_v)))\n    if v.epoch % v.validate_every_n_epoch == 0 and v.val_loader:\n      self.benchmark(v.val_loader, v, epoch=v.epoch, memory_limit=\'1GB\')\n      v.summary_writer.add_summary(self.model.summary(), v.global_step)\n    self._save_model(v.sess, v.epoch)\n\n  def fn_train_each_step(self, pack):\n    v = self.v\n    loss = self.model.train_batch(pack[\'lr\'], pack[\'hr\'], learning_rate=v.lr,\n                                  epochs=v.epoch)\n    v.global_step = self.model.global_steps.eval()\n    for _k, _v in loss.items():\n      v.avg_meas[_k] = \\\n        v.avg_meas[_k] + [_v] if v.avg_meas.get(_k) else [_v]\n      loss[_k] = \'{:08.5f}\'.format(_v)\n    v.loss = loss\n\n  def fn_infer_each_step(self, pack):\n    v = self.v\n    if v.ensemble:\n      # add self-ensemble boosting metric score\n      feature_ensemble = _ensemble_expand(pack[\'lr\'])\n      outputs_ensemble = []\n      for f in feature_ensemble:\n        y, _ = self.model.test_batch(f, None)\n        outputs_ensemble.append(y)\n      outputs = []\n      for i in range(len(outputs_ensemble[0])):\n        outputs.append([j[i] for j in outputs_ensemble])\n      outputs = _ensemble_reduce_mean(outputs)\n    else:\n      outputs, _ = self.model.test_batch(pack[\'lr\'], None)\n    for fn in v.inference_results_hooks:\n      outputs = fn(outputs, names=pack[\'name\'])\n      if outputs is None:\n        break\n\n  def fn_benchmark_each_step(self, pack):\n    v = self.v\n    outputs, metrics = self.model.test_batch(pack[\'lr\'], pack[\'hr\'],\n                                             epochs=v.epoch)\n    for _k, _v in metrics.items():\n      if _k not in v.mean_metrics:\n        v.mean_metrics[_k] = []\n      v.mean_metrics[_k] += [_v]\n    for fn in v.inference_results_hooks:\n      outputs = fn(outputs, names=pack[\'name\'])\n      if outputs is None:\n        break\n\n  def fn_benchmark_body(self):\n    v = self.v\n    it = v.loader.make_one_shot_iterator(v.batch_shape, v.val_steps,\n                                         shuffle=not v.traced_val,\n                                         memory_limit=v.memory_limit)\n    for items in tqdm.tqdm(it, \'Test\', ascii=True):\n      self.fn_benchmark_each_step(items)\n\n  """"""=======================================\n      Interface: fit, benchmark, infer\n     =======================================\n  """"""\n\n  def fit(self, loaders, config, **kwargs):\n    """"""Fit the model.\n\n    Args:\n        loaders: a tuple of 2 loaders, the 1st one is used for training,\n          and the 2nd one is used for validating.\n        config: fitting configuration, an instance of `Util.Config.Config`\n        kwargs: additional arguments to override the same ones in config.\n    """"""\n    v = self.query_config(config, **kwargs)\n    v.train_loader, v.val_loader = loaders\n    if not self.fit_init():\n      return\n    for epoch in range(self.last_epoch + 1, v.epochs + 1):\n      v.epoch = epoch\n      self.fn_train_each_epoch()\n    self.fit_close()\n\n  def infer(self, loader, config, **kwargs):\n    """"""Infer SR images.\n\n    Args:\n        loader: a loader for enumerating LR images\n        config: inferring configuration, an instance of `Util.Config.Config`\n        kwargs: additional arguments to override the same ones in config.\n    """"""\n    v = self.query_config(config, **kwargs)\n    self._restore()\n    it = loader.make_one_shot_iterator([1, -1, -1, -1], -1)\n    if hasattr(it, \'__len__\'):\n      if len(it):\n        LOG.info(\'Inferring {} at epoch {}\'.format(\n            self.model.name, self.last_epoch))\n      else:\n        return\n    # use original images in inferring\n    for items in tqdm.tqdm(it, \'Infer\', ascii=True):\n      self.fn_infer_each_step(items)\n\n  def benchmark(self, loader, config, **kwargs):\n    """"""Benchmark/validate the model.\n\n    Args:\n        loader: a loader for enumerating LR images\n        config: benchmark configuration, an instance of `Util.Config.Config`\n        kwargs: additional arguments to override the same ones in config.\n    """"""\n    v = self.query_config(config, **kwargs)\n    self._restore()\n    v.mean_metrics = {}\n    v.loader = loader\n    self.fn_benchmark_body()\n    for _k, _v in v.mean_metrics.items():\n      print(\'{}: {:.6f}\'.format(_k, np.mean(_v)), end=\', \')\n    print(\'\')\n'"
VSR/Backend/TF/Framework/__init__.py,0,b''
VSR/Backend/TF/Models/Carn.py,25,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Sep 11th 2018\n\nCascading Residual Network (ECCV 2018)\nSee https://arxiv.org/abs/1803.08664\n""""""\n\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass CARN(SuperResolution):\n  """"""Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual\n     Network\n\n  Args:\n      recursive: A boolean, specifies whether use shared Residual-E weights\n      groups: number of groups in group-convolution\n      n_residual: number of Residual-E layers in a cascading block\n      n_block: number of cascading blocks\n      clip: value of gradient normal clipping\n  """"""\n\n  def __init__(self, name=\'carn\', recursive=False, groups=1, n_residual=3,\n               n_blocks=3, filters=64, clip=10, **kwargs):\n    super(CARN, self).__init__(**kwargs)\n    self.name = name\n    self.recur = recursive\n    self.groups = groups\n    self.n_residual = n_residual\n    self.n_blocks = n_blocks\n    self.clip = clip\n    self.F = filters\n\n  def _residual_e(self, inputs, reuse=False, **kwargs):\n    if reuse: reuse = tf.AUTO_REUSE\n    with tf.variable_scope(kwargs.get(\'name\'), \'Residual-E\', reuse=reuse):\n      """"""Fake code since there is no efficient group conv2d in TF!\n      see https://github.com/tensorflow/tensorflow/pull/10482\n      and https://github.com/tensorflow/tensorflow/issues/3332\n        >>> x = group_conv2d(inputs, self.F, 3, self.groups, activation=\'relu\',\n                             kernel_initializer=\'torch\')\n        >>> x = group_conv2d(x, self.F, 3, self.groups, activation=\'relu\',\n                             kernel_initializer=\'torch\')\n        >>> x = conv2d(x, inputs.shape[-1], 1,\n                       kernel_initializer=\'torch\')\n      """"""\n      # use a normal residual instead\n      x = self.conv2d(inputs, self.F, 3, activation=\'relu\',\n                      kernel_initializer=\'torch\')\n      x = self.conv2d(x, inputs.shape[-1], 3, kernel_initializer=\'torch\')\n      inputs += x\n      return tf.nn.relu(inputs)\n\n  def _cascading(self, inputs, **kwargs):\n    with tf.variable_scope(kwargs.get(\'name\'), \'CascadingBlock\'):\n      feat = [inputs]\n      outp_1x1 = inputs\n      name = \'SharedResE\' if self.recur else None\n      F = inputs.shape[-1]\n      for i in range(self.n_residual):\n        x = self._residual_e(outp_1x1, reuse=self.recur, name=name)\n        feat.append(x)\n        x = tf.concat(feat, axis=-1)\n        outp_1x1 = self.conv2d(x, F, 1, kernel_initializer=\'torch\')\n      return outp_1x1\n\n  def build_graph(self):\n    super(CARN, self).build_graph()\n    with tf.variable_scope(self.name):\n      x = self.inputs_preproc[-1] / 255\n      outp_1x1 = self.conv2d(x, self.F, 3, kernel_initializer=\'torch\')\n      feat = [x]\n      F = outp_1x1.shape[-1]\n      for i in range(self.n_blocks):\n        x = self._cascading(outp_1x1)\n        feat.append(x)\n        x = tf.concat(feat, axis=-1)\n        outp_1x1 = self.conv2d(x, F, 1, kernel_initializer=\'torch\')\n      sr = self.upscale(outp_1x1, direct_output=False,\n                        kernel_initializer=\'torch\')\n      sr = self.conv2d(sr, self.channel, 3, kernel_initializer=\'torch\')\n      self.outputs.append(sr * 255)\n\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      l1_loss = tf.losses.absolute_difference(self.label[0], self.outputs[0])\n      re_loss = tf.losses.get_regularization_losses()\n      mse = tf.losses.mean_squared_error(self.label[0], self.outputs[0])\n      loss = tf.add_n(re_loss + [l1_loss], name=\'Loss\')\n\n      update_op = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_op):\n        opt = tf.train.AdamOptimizer(self.learning_rate)\n        var_n_grad = opt.compute_gradients(loss)\n        grad_clip = tf.contrib.training.clip_gradient_norms(\n          var_n_grad, self.clip)\n        opt = opt.apply_gradients(grad_clip, self.global_steps)\n        self.loss.append(opt)\n\n    # tensorboard\n    self.train_metric[\'loss\'] = loss\n    self.train_metric[\'l1\'] = l1_loss\n    self.metrics[\'mse\'] = mse\n    self.metrics[\'psnr\'] = tf.reduce_mean(\n      tf.image.psnr(self.label[0], self.outputs[0], 255))\n    self.metrics[\'ssim\'] = tf.reduce_mean(\n      tf.image.ssim(self.label[0], self.outputs[0], 255))\n\n  def build_summary(self):\n    super(CARN, self).build_summary()\n    tf.summary.image(\'SR\', self.outputs[0], 1)\n\n  def build_saver(self):\n    var_g = tf.global_variables(self.name)\n    misc = tf.global_variables(\'Loss\') + [self.global_steps]\n    self.savers[self.name] = tf.train.Saver(var_g, max_to_keep=1)\n    self.savers[\'misc\'] = tf.train.Saver(misc, max_to_keep=1)\n'"
VSR/Backend/TF/Models/Crdn.py,19,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Mar. 20th 2019\n\nCascaded Residual Dense Network (NTIRE 2019)\n""""""\n\n#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x888:28\n\nimport tensorflow as tf\n\nfrom ..Arch.Residual import cascade_rdn\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import clip_image\n\n\ndef _denormalize(inputs):\n  return (inputs + 0) * 255\n\n\ndef _normalize(inputs):\n  return (inputs - 0) / 255\n\n\nclass CRDN(SuperResolution):\n  """"""A Cascaded Residual Dense Network""""""\n\n  def __init__(self, name=\'crdn\', **kwargs):\n    super(CRDN, self).__init__(**kwargs)\n    self.name = name\n    self.n_cb = 6\n\n  def upsample(self, inputs, skips, method=\'nearest\', scale=2):\n    with tf.variable_scope(None, f\'UpX{scale}\'):\n      c = int(inputs.shape[-1])\n      up0 = self.upscale(inputs, method, scale, direct_output=False)\n      up1 = self.conv2d(up0, c, 3)\n      fs0 = tf.concat([up1, skips], axis=-1)\n      fs1 = self.conv2d(fs0, c // 2, 3)\n      return fs1\n\n  def build_graph(self):\n    super(CRDN, self).build_graph()\n    depth = self.n_cb\n    filters = 32\n    inputs = _normalize(self.inputs_preproc[-1])\n    with tf.variable_scope(self.name):\n      x = self.conv2d(inputs, filters, 7)\n      entry = self.conv2d(x, filters, 5)\n      x_list = [entry]\n      f = filters\n      for i in range(depth // 2):\n        x = cascade_rdn(self, x, depth=3, use_ca=True, filters=f)\n        x_list.append(x)\n        f *= 2\n        x = self.conv2d(x, f, 3, strides=2, name=\'Down%d\' % i)\n      x = cascade_rdn(self, x, depth=3, use_ca=True, filters=f)\n      x = cascade_rdn(self, x, depth=3, use_ca=True, filters=f)\n      for i in range(depth // 2):\n        f //= 2\n        x = self.upsample(x, x_list.pop())\n        x = cascade_rdn(self, x, depth=3, use_ca=True, filters=f)\n\n      assert len(x_list) == 1, f\'length of x_list is not 1: {len(x_list)}\'\n      assert f == filters\n      x += x_list.pop()\n      sr = self.conv2d(x, filters, 3)\n      sr = self.conv2d(sr, self.channel, 3)\n\n    self.outputs.append(_denormalize(sr))\n\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      l1 = tf.losses.absolute_difference(self.label[-1], self.outputs[-1])\n      op = tf.train.AdamOptimizer(self.learning_rate)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        opt = op.minimize(l1, self.global_steps)\n        self.loss.append(opt)\n\n      self.train_metric[\'l1\'] = l1\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(self.label[-1], self.outputs[-1], max_val=255))\n\n  def build_summary(self):\n    super(CRDN, self).build_summary()\n    tf.summary.image(\'sr\', clip_image(self.outputs[-1]))\n    tf.summary.image(\'lr\', clip_image(self.inputs_preproc[-1]))\n    tf.summary.image(\'hq\', clip_image(self.label[-1]))\n\n  def build_saver(self):\n    var_g = tf.global_variables(self.name)\n    misc = tf.global_variables(\'Loss\') + [self.global_steps]\n    self.savers[\'misc\'] = tf.train.Saver(misc, max_to_keep=1)\n    self.savers[self.name] = tf.train.Saver(var_g, max_to_keep=1)\n'"
VSR/Backend/TF/Models/Dbpn.py,24,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: June 15th 2018\nUpdated Date: Sep 11th 2018\n\nDeep Back-Projection Networks For Super-Resolution (CVPR 2018)\nSee https://arxiv.org/abs/1803.02735\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass DBPN(SuperResolution):\n  """"""Deep Back-Projection Networks For Super-Resolution\n\n  Args:\n      bp_layers: number of back-projection stages\n      use_dense: use dense unit or not\n      filters: number of filters in primary conv2d(s)\n  """"""\n\n  def __init__(self, bp_layers=7, use_dense=True, filters=64, name=\'dbpn\',\n               **kwargs):\n    super(DBPN, self).__init__(**kwargs)\n    self.bp = bp_layers\n    self.dense = use_dense\n    self.filter = filters\n    self.name = name\n    s0, s1 = self.scale\n    assert s0 == s1\n    if s0 == 3:\n      self.kernel_size = 7\n    elif s0 == 2 or s0 == 4 or s0 == 8:\n      self.kernel_size = int(4 + 2 * np.log2(s0))\n\n  def _up_projection(self, inputs, dense=True, **kwargs):\n    with tf.variable_scope(kwargs.get(\'name\'), \'UpProjection\'):\n      if dense:\n        l_pre = self.conv2d(inputs, self.filter, 1, activation=\'prelu\')\n      else:\n        l_pre = inputs\n      h0 = self.deconv2d(l_pre, self.filter, self.kernel_size, self.scale,\n                         activation=\'prelu\')\n      l0 = self.conv2d(h0, self.filter, self.kernel_size, self.scale,\n                       activation=\'prelu\')\n      res_cur = l0 - l_pre\n      h1 = self.deconv2d(res_cur, self.filter, self.kernel_size, self.scale,\n                         activation=\'prelu\')\n      h_cur = h0 + h1\n      return h_cur\n\n  def _down_projection(self, inputs, dense=True, **kwargs):\n    with tf.variable_scope(kwargs.get(\'name\'), \'DownProjection\'):\n      if dense:\n        h_pre = self.conv2d(inputs, self.filter, 1, activation=\'prelu\')\n      else:\n        h_pre = inputs\n      l0 = self.conv2d(h_pre, self.filter, self.kernel_size, self.scale,\n                       activation=\'prelu\')\n      h0 = self.deconv2d(l0, self.filter, self.kernel_size, strides=self.scale,\n                         activation=\'prelu\')\n      res_cur = h0 - h_pre\n      l1 = self.conv2d(res_cur, self.filter, self.kernel_size, self.scale,\n                       activation=\'prelu\')\n      l_cur = l0 + l1\n      return l_cur\n\n  def build_graph(self):\n    super(DBPN, self).build_graph()\n    with tf.variable_scope(self.name):\n      with tf.variable_scope(\'FE-Net\'):\n        x = self.conv2d(self.inputs_preproc[-1], 256, 3, activation=\'prelu\')\n        x = self.conv2d(x, self.filter, 1, activation=\'prelu\')\n      with tf.variable_scope(\'BP-Net\'):\n        l, h = [x], []\n        for _ in range(1, self.bp):\n          t = tf.concat(l, axis=-1) if self.dense else l[-1]\n          h.append(self._up_projection(t, self.dense))\n          t = tf.concat(h, axis=-1) if self.dense else h[-1]\n          l.append(self._down_projection(t, self.dense))\n        t = tf.concat(l, axis=-1) if self.dense else l[-1]\n        h.append(self._up_projection(t, self.dense))\n      x = tf.concat(h, axis=-1)\n      with tf.variable_scope(\'ReconNet\'):\n        x = self.conv2d(x, self.channel, 3)\n      self.outputs.append(x)\n\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      l1_loss = tf.losses.absolute_difference(self.label[0], self.outputs[0])\n      re_loss = tf.losses.get_regularization_losses()\n      mse = tf.losses.mean_squared_error(self.label[0], self.outputs[0])\n      loss = tf.add_n(re_loss + [l1_loss], name=\'Loss\')\n\n      update_op = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_op):\n        opt = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          loss, self.global_steps)\n        self.loss.append(opt)\n\n    # tensorboard\n    self.train_metric[\'loss\'] = loss\n    self.train_metric[\'l1\'] = l1_loss\n    self.metrics[\'mse\'] = mse\n    self.metrics[\'psnr\'] = tf.reduce_mean(\n      tf.image.psnr(self.label[0], self.outputs[0], 255))\n    self.metrics[\'ssim\'] = tf.reduce_mean(\n      tf.image.ssim(self.label[0], self.outputs[0], 255))\n\n  def build_summary(self):\n    super(DBPN, self).build_summary()\n    tf.summary.image(\'SR\', self.outputs[0], 1)\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)\n'"
VSR/Backend/TF/Models/Dcscn.py,16,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 23rd 2018\nUpdated Date: June 15th 2018\n\nImplementing Fast and Accurate Image Super Resolution by\nDeep CNN with Skip Connection and Network in Network\nSee https://arxiv.org/abs/1707.05425\n""""""\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass DCSCN(SuperResolution):\n  """"""Fast and Accurate Image Super Resolution by Deep CNN\n     with Skip Connection and Network in Network\n\n  Args:\n      layers: total layers of feature extraction net\n      reconstruction_layers: number of layers in upscale net\n      filters: maximum filters in feature extraction net\n      min_filters: number of filters in the 1st layer of feature extraction net\n      nin_filter: a tuple of 2 integer, representing filters in NIN\n      reconst_filter: number of filters in reconstruction net\n      filters_decay_gamma: ...\n      drop_out: drop out rate\n  """"""\n\n  def __init__(self,\n               layers=8,\n               reconstruction_layers=1,\n               filters=196,\n               min_filters=48,\n               nin_filter=(64, 32),\n               reconst_filter=32,\n               filters_decay_gamma=1.5,\n               drop_out=0.8,\n               name=\'dcscn\',\n               **kwargs):\n    super(DCSCN, self).__init__(**kwargs)\n    self.layers = layers\n    self.reconstruction_layers = reconstruction_layers\n    self.filters = filters\n    self.min_filters = min_filters\n    self.nin_filter = nin_filter\n    self.reconst_filter = reconst_filter\n    self.filters_decay_gamma = filters_decay_gamma\n    self.drop_out = drop_out\n    self.name = name\n\n  def build_graph(self):\n    super(DCSCN, self).build_graph()\n    with tf.variable_scope(self.name):\n      shape_enlarged = tf.shape(self.inputs_preproc[-1])[1:3]\n      shape_enlarged = shape_enlarged * self.scale\n      bic = tf.image.resize_bicubic(self.inputs_preproc[-1],\n                                    shape_enlarged)\n      x = [self.inputs_preproc[-1]]\n      drop_out = tf.cond(self.training_phase, lambda: self.drop_out,\n                         lambda: 1.0)\n      for i in range(self.layers):\n        if self.min_filters != 0 and i > 0:\n          x1 = i / float(self.layers - 1)\n          y1 = pow(x1, 1.0 / self.filters_decay_gamma)\n          output_feature_num = int(\n            (self.filters - self.min_filters) * (\n                1 - y1) + self.min_filters)\n          nn = self.bn_relu_conv2d(x[-1], output_feature_num, 3)\n          x.append(tf.nn.dropout(nn, drop_out))\n      concat_x = tf.concat(x, axis=-1)\n      with tf.variable_scope(\'NIN\'):\n        a1 = self.bn_relu_conv2d(concat_x, self.nin_filter[0], 1)\n        b1 = self.bn_relu_conv2d(concat_x, self.nin_filter[1], 1)\n        b2 = self.bn_relu_conv2d(b1, self.nin_filter[1], 3)\n      concat_nin = tf.concat([a1, b2], axis=-1)\n      ps = self.upscale(concat_nin)\n      with tf.variable_scope(\'Reconstruction\'):\n        for i in range(self.reconstruction_layers - 1):\n          ps = self.relu_conv2d(ps, self.reconst_filter, 3)\n          ps = tf.nn.dropout(ps, drop_out)\n        outputs = self.conv2d(ps, self.channel, 3)\n      self.outputs.append(outputs + bic)\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      mse, loss = super(DCSCN, self).build_loss()\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mse\'] = mse\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(self.label[-1], self.outputs[-1], max_val=255))\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)\n'"
VSR/Backend/TF/Models/DnCnn.py,4,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 23rd 2018\nUpdated Date: May 23rd 2018\n\nImplementing Feed-forward Denoising Convolutional Neural Network\nSee http://ieeexplore.ieee.org/document/7839189/\n""""""\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass DnCNN(SuperResolution):\n  """"""Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for\n    Image Denoising\n\n  Args:\n      layers: number of layers used\n  """"""\n\n  def __init__(self, layers=20, name=\'dncnn\', **kwargs):\n    self.name = name\n    self.layers = layers\n    if \'scale\' in kwargs:\n      kwargs.pop(\'scale\')\n    super(DnCNN, self).__init__(scale=1, **kwargs)\n\n  def build_graph(self):\n    super(DnCNN, self).build_graph()  # build inputs placeholder\n    with tf.variable_scope(self.name):\n      # build layers\n      x = self.inputs_preproc[-1] / 255  # use channel Y only\n      x = self.relu_conv2d(x, 64, 3)\n      for i in range(1, self.layers - 1):\n        x = self.bn_relu_conv2d(x, 64, 3, use_bias=False)\n      # the last layer w/o BN and ReLU\n      x = self.conv2d(x, self.channel, 3)\n      # residual training\n      outputs = self.inputs_preproc[-1] / 255 + x\n      self.outputs.append(outputs * 255)\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      mse, loss = super(DnCNN, self).build_loss()\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mse\'] = mse\n      self.metrics[\'psnr\'] = tf.reduce_mean(tf.image.psnr(\n        self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(tf.image.ssim(\n        self.label[-1], self.outputs[-1], max_val=255))\n'"
VSR/Backend/TF/Models/Drcn.py,17,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: June 5th 2018\nUpdated Date: June 5th 2018\n\nDeeply-Recursive Convolutional Network for Image Super-Resolution (CVPR 2016)\nSee https://arxiv.org/abs/1511.04491\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import bicubic_rescale\n\n\nclass DRCN(SuperResolution):\n  """"""Deeply-Recursive Convolutional Network for Image Super-Resolution\n\n  Args:\n      recur: number of recursions\n      filters: number of filters in conv2d(s)\n      custom_upsample: use --add_custom_callbacks=upsample during fitting, or\n        use `bicubic_rescale`. TODO: REMOVE IN FUTURE.\n  """"""\n\n  def __init__(self, recur=16, filters=256, custom_upsample=False,\n               name=\'drcn\', **kwargs):\n    self.recur = recur\n    self.filters = filters\n    self.do_up = not custom_upsample\n    self.name = name\n    super(DRCN, self).__init__(**kwargs)\n\n  def _embedding(self, inputs):\n    with tf.variable_scope(\'Embedding\'):\n      x = self.relu_conv2d(inputs, self.filters, 3)\n      x = self.relu_conv2d(x, self.filters, 3)\n      return x\n\n  def _inference(self, inputs):\n    with tf.variable_scope(\'Inference\', reuse=tf.AUTO_REUSE):\n      x = self.relu_conv2d(inputs, self.filters, 3)\n      return x\n\n  def _reconstruction(self, inputs):\n    with tf.variable_scope(\'Reconstruct\', reuse=tf.AUTO_REUSE):\n      x = self.relu_conv2d(inputs, self.filters, 3)\n      x = self.conv2d(x, self.channel, 3)\n      return x\n\n  def build_graph(self):\n    with tf.variable_scope(self.name):\n      super(DRCN, self).build_graph()\n      # bicubic upscale\n      x = self.inputs_preproc[-1]\n      if self.do_up:\n        x = bicubic_rescale(self.inputs_preproc[-1], self.scale)\n      bic = x\n      x = self._embedding(bic)\n      y = [bic]\n      for _ in range(self.recur):\n        x = self._inference(x)\n        y += [self._reconstruction(x)]\n      self.outputs = y\n      layer_weights = tf.Variable(np.ones_like(y, \'float\') / len(y),\n                                  name=""LayerWeights"", dtype=tf.float32)\n      output = 0\n      for i in range(len(y)):\n        output += y[i] * layer_weights[i]\n      self.outputs.insert(0, output / tf.reduce_sum(layer_weights))\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      y_true = self.label[-1]\n      mse_n = []\n      for y_pred in self.outputs[1:]:\n        mse_n.append(tf.losses.mean_squared_error(y_true, y_pred))\n      loss1 = tf.reduce_mean(mse_n)\n      loss2 = tf.losses.mean_squared_error(y_true, self.outputs[0])\n      regularization = tf.add_n(tf.losses.get_regularization_losses())\n      alpha = tf.placeholder(tf.float32, name=\'alpha\')\n      loss = alpha * loss1 + (1.0 - alpha) * loss2 + regularization\n      optimizer = tf.train.AdamOptimizer(self.learning_rate)\n      self.loss.append(optimizer.minimize(loss, self.global_steps))\n\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'local_mse\'] = loss1\n      self.metrics[\'final_mse\'] = loss2\n      self.metrics[\'regularization\'] = regularization\n      self.metrics[\'psnr\'] = tf.image.psnr(y_true, self.outputs[-1],\n                                           max_val=255)\n      self.metrics[\'ssim\'] = tf.image.ssim(y_true, self.outputs[-1],\n                                           max_val=255)\n\n  def train_batch(self, feature, label, learning_rate=1e-4, **kwargs):\n    epoch = kwargs.get(\'epochs\')\n    if epoch < 50:\n      self.feed_dict.update({\'loss/alpha:0\': 1.0})\n    elif epoch < 100:\n      self.feed_dict.update({\'loss/alpha:0\': 1 - (epoch - 50) / 50})\n    else:\n      self.feed_dict.update({\'loss/alpha:0\': 0})\n    return super(DRCN, self).train_batch(feature, label, learning_rate,\n                                         **kwargs)\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)\n'"
VSR/Backend/TF/Models/Drrn.py,12,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: June 8th 2018\nUpdated Date: June 8th 2018\n\nImage Super-Resolution via Deep Recursive Residual Network (CVPR 2017)\nSee http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf\n""""""\n\nimport logging\n\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import bicubic_rescale\n\nLOG = logging.getLogger(\'VSR.Model.DRRN\')\n\n\nclass DRRN(SuperResolution):\n  """"""Image Super-Resolution via Deep Recursive Residual Network\n\n  Args:\n      residual_unit: number of residual blocks in one recursion\n      recursive_block: number of recursions\n      grad_clip: gradient clip ratio according to the paper\n      custom_upsample: use --add_custom_callbacks=upsample during fitting, or\n        use `bicubic_rescale`. TODO: REMOVE IN FUTURE.\n  """"""\n\n  def __init__(self, residual_unit=3, recursive_block=3,\n               custom_upsample=False,\n               grad_clip=0.01, name=\'drrn\', **kwargs):\n    self.ru = residual_unit\n    self.rb = recursive_block\n    self.grad_clip = grad_clip\n    self.do_up = not custom_upsample\n    self.name = name\n    super(DRRN, self).__init__(**kwargs)\n\n  def display(self):\n    super(DRRN, self).display()\n    LOG.info(\'Recursive Blocks: %d\' % self.rb)\n    LOG.info(\'Residual Units: %d\' % self.ru)\n\n  def _shared_resblock(self, inputs, **kwargs):\n    x = self.relu_conv2d(inputs, 128, 3)\n    for _ in range(self.ru):\n      x = self.resblock(x, 128, 3, reuse=tf.AUTO_REUSE, name=\'Res\')\n    return x\n\n  def build_graph(self):\n    super(DRRN, self).build_graph()\n    with tf.variable_scope(self.name):\n      x = self.inputs_preproc[-1]\n      if self.do_up:\n        x = bicubic_rescale(self.inputs_preproc[-1], self.scale)\n      bic = x\n      for _ in range(self.rb):\n        x = self._shared_resblock(x)\n      x = self.conv2d(x, self.channel, 3)\n      self.outputs.append(x + bic)\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      y_true = self.label[-1]\n      y_pred = self.outputs[-1]\n      mse = tf.losses.mean_squared_error(y_true, y_pred)\n      loss = tf.add_n([mse] + tf.losses.get_regularization_losses())\n      opt = tf.train.AdamOptimizer(self.learning_rate)\n      if self.grad_clip > 0:\n        grads_and_vars = []\n        for grad, var in opt.compute_gradients(loss):\n          grads_and_vars.append((\n            tf.clip_by_value(\n                grad,\n                -self.grad_clip / self.learning_rate,\n                self.grad_clip / self.learning_rate),\n            var))\n        op = opt.apply_gradients(grads_and_vars, self.global_steps)\n      else:\n        op = opt.minimize(loss, self.global_steps)\n      self.loss.append(op)\n\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mse\'] = mse\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n          tf.image.psnr(y_true, y_pred, 255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n          tf.image.ssim(y_true, y_pred, 255))\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)\n'"
VSR/Backend/TF/Models/Drsr.py,71,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 1 - 11\n#  Degradation-restore Super-resolution Network\n\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom VSR.Util import Config\nfrom ..Framework import Noise, Trainer\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import summary_tensor_image\n\n_MEAN_GT = [84.1148, 68.3644, 64.8452]\n_MEAN_SR = [85.6586, 68.7887, 66.5135]\n_MEAN = np.array(_MEAN_SR, \'float32\')\nLOG = logging.getLogger(\'VSR.Model.DRSR\')\n\n\ndef _denormalize(inputs):\n  return (inputs + 0) * 255\n\n\ndef _normalize(inputs):\n  return inputs / 255\n\n\ndef _clip(image):\n  return tf.cast(tf.clip_by_value(image, 0, 255), \'uint8\')\n\n\nclass DRSR(SuperResolution):\n  def __init__(self, name=\'drsr\', n_cb=4, n_crb=4,\n               noise_config=None,\n               weights=(1, 0.5, 0.05, 1e-3),\n               finetune=2000,\n               mean_shift=False,\n               **kwargs):\n    super(DRSR, self).__init__(**kwargs)\n    self.name = name\n    self.n_cb = n_cb\n    self.n_crb = n_crb\n    self.weights = weights\n    self.finetune = finetune\n    self.mean_shift = mean_shift\n    self.noise = Config(scale=0, offset=0, penalty=0.7, max=0.2, layers=7)\n    if isinstance(noise_config, (dict, Config)):\n      self.noise.update(**noise_config)\n      if self.noise.type == \'crf\':\n        self.noise.crf = np.load(self.noise.crf)\n      self.noise.offset /= 255\n      self.noise.max /= 255\n    if \'tfrecords\' in kwargs:\n      self.tfr = kwargs[\'tfrecords\']\n      self._trainer = DrTrainer\n\n  def display(self):\n    # stats = tf.profiler.profile()\n    # LOG.info(""Total parameters: {}"".format(stats.total_parameters))\n    LOG.info(""Noisy scaling {}, bias sigma {}"".format(\n      self.noise.scale, self.noise.offset))\n    LOG.info(""Using {}"".format(self.trainer))\n\n  def _dncnn(self, inputs):\n    n = self.noise\n    with tf.variable_scope(\'Dncnn\'):\n      x = inputs\n      for _ in range(6):\n        x = self.bn_relu_conv2d(x, 64, 3)\n      x = self.conv2d(x, self.channel, 3)\n    return x\n\n  def cascade_block(self, inputs, noise, filters=64, depth=4, scope=None,\n                    reuse=None):\n    def _noise_condition(nc_inputs, layers=2):\n      with tf.variable_scope(None, \'NCL\'):\n        t = noise\n        for _ in range(layers - 1):\n          t = self.relu_conv2d(t, 64, 3)\n        t = self.conv2d(t, 64, 3)\n        gamma = tf.reduce_mean(t, [1, 2], keepdims=True)\n        t = noise\n        for _ in range(layers - 1):\n          t = self.relu_conv2d(t, 64, 3)\n        beta = self.conv2d(t, 64, 3)\n      return nc_inputs * gamma + beta\n\n    def _cond_resblock(cr_inputs, kernel_size):\n      with tf.variable_scope(None, \'CRB\'):\n        pre_inputs = cr_inputs\n        cr_inputs = self.relu_conv2d(cr_inputs, filters, kernel_size)\n        cr_inputs = _noise_condition(cr_inputs)\n        cr_inputs = self.relu_conv2d(cr_inputs, filters, kernel_size)\n        cr_inputs = _noise_condition(cr_inputs)\n        return pre_inputs + cr_inputs\n\n    with tf.variable_scope(scope, \'CB\', reuse=reuse):\n      feat = [inputs]\n      for i in range(depth):\n        x = _cond_resblock(inputs, 3)\n        feat.append(x)\n        inputs = self.conv2d(tf.concat(feat, axis=-1), filters, 1,\n                             kernel_initializer=\'he_uniform\')\n      # inputs = self.conv2d(inputs, filters, 3)\n      return inputs\n\n  def _upsample(self, inputs, noise):\n    x = [self.conv2d(inputs, 64, 7)]\n    for i in range(self.n_cb):\n      x += [self.cascade_block(x[i], noise, depth=self.n_crb)]\n    # bottleneck\n    df = [self.conv2d(n, 32, 1, kernel_initializer=\'he_uniform\')\n          for n in x[:-1]]\n    df.append(x[-1])\n    summary_tensor_image(x[-1], \'last_before_bn\')\n    bottleneck = tf.concat(df, axis=-1, name=\'bottleneck\')\n    sr = self.upscale(bottleneck, direct_output=False)\n    summary_tensor_image(sr, \'after_bn\')\n    sr = self.conv2d(sr, self.channel, 3)\n    return sr, x\n\n  def _unet(self, inputs, noise):\n    with tf.variable_scope(\'Unet\'):\n      x0 = self.conv2d(inputs, 64, 7)\n      x1 = self.cascade_block(x0, noise, depth=self.n_crb)\n      x1s = tf.layers.average_pooling2d(x1, 2, 2)\n      n1s = tf.layers.average_pooling2d(noise, 2, 2)\n      x2 = self.cascade_block(x1s, n1s, depth=self.n_crb)\n      x2s = tf.layers.average_pooling2d(x2, 2, 2)\n      n2s = tf.layers.average_pooling2d(noise, 4, 4)\n      x3 = self.cascade_block(x2s, n2s, depth=self.n_crb)\n      x3u = self.deconv2d(x3, 64, 3, strides=2)\n      x3u1 = tf.concat([x3u, x1s], -1)\n      x3u2 = self.conv2d(x3u1, 64, 3)\n      x4 = self.cascade_block(x3u2, n1s, depth=self.n_crb)\n      x4u = self.deconv2d(x4, 64, 3, strides=2)\n      x4u1 = tf.concat([x4u, x0], -1)\n      x4u2 = self.conv2d(x4u1, 64, 3)\n      x5 = self.conv2d(x4u2, self.channel, 3)\n    return x5, None\n\n  def _get_noise(self, inputs):\n    n = self.noise\n    if n.type == \'gaussian\':\n      sigma = tf.random_uniform([], maxval=n.max)\n      noise = tf.random_normal(tf.shape(inputs), stddev=sigma)\n      img = inputs + noise\n      return img, noise\n    elif n.type == \'crf\':\n      crf = tf.convert_to_tensor(n.crf[\'crf\'])\n      icrf = tf.convert_to_tensor(n.crf[\'icrf\'])\n      i = tf.random_uniform([], 0, crf.shape[0], dtype=tf.int32)\n      irr = Noise.tf_camera_response_function(inputs, icrf[i], max_val=1)\n      noise = Noise.tf_gaussian_poisson_noise(irr, max_c=n.max)\n      img = Noise.tf_camera_response_function(irr + noise, crf[i], max_val=1)\n      return img, img - inputs\n    else:\n      raise TypeError(n.type)\n\n  def build_graph(self):\n    super(DRSR, self).build_graph()\n    inputs_norm = _normalize(self.inputs_preproc[-1])\n    labels_norm = _normalize(self.label[-1])\n    if self.mean_shift:\n      inputs_norm -= _MEAN / 255\n      labels_norm -= _MEAN / 255\n    n = self.noise\n    inputs_noise, noise = self._get_noise(inputs_norm)\n    nn = self._upsample\n    with tf.variable_scope(\'Offset\'):\n      x = inputs_norm\n      for _ in range(n.layers):\n        x = self.relu_conv2d(x, 64, 3,\n                             kernel_initializer=tf.initializers.random_normal(\n                               stddev=0.01))\n      offset = self.conv2d(x, self.channel, 3,\n                           kernel_initializer=tf.initializers.random_normal(\n                             stddev=0.01))\n      offset *= Noise.tf_gaussian_noise(offset, n.offset2)\n\n    with tf.variable_scope(self.name):\n      zero = self._dncnn(inputs_norm)\n      zero_shift = zero + offset * n.scale + \\\n                   Noise.tf_gaussian_noise(zero, n.offset)\n      clean = nn(inputs_norm, zero_shift)\n    with tf.variable_scope(self.name, reuse=True):\n      noisy = self._dncnn(inputs_noise)\n      dirty = nn(inputs_noise, noisy)\n    if self.finetune == -1:\n      with tf.variable_scope(self.name, reuse=True):\n        s = 2\n        inputs_s2 = tf.layers.average_pooling2d(inputs_norm, s, s)\n        zero_s2 = self._dncnn(inputs_s2)\n        zero_shift_s2 = zero_s2 + Noise.tf_gaussian_noise(zero_s2, n.offset)\n        clean_s2 = nn(inputs_s2, zero_shift_s2)\n        noise_s2 = inputs_norm - clean_s2[0]\n      with tf.variable_scope(\'Fine\'):\n        x = self.conv2d(inputs_norm, 64, 3)\n        x = self.cascade_block(x, noise_s2, depth=6)\n        x = self.conv2d(x, self.channel, 3)\n        clean_fine = [x, x]\n      self.outputs.append(_denormalize(clean_s2[0]))\n      self.outputs.append(_denormalize(clean_fine[0]))\n    else:\n      self.outputs.append(_denormalize(tf.abs(zero)))\n      self.outputs.append(_denormalize(clean[0]))\n\n    if self.mean_shift:\n      self.outputs = [x + _MEAN for x in self.outputs]\n\n    def loss1():\n      l1_with_noise = tf.losses.absolute_difference(dirty[0], labels_norm)\n      l1_fine_tune = tf.losses.absolute_difference(clean[0], labels_norm)\n      penalty = tf.clip_by_value(2 * tf.ceil(tf.nn.relu(noisy - noise)),\n                                 0, 1)\n      penalty = tf.abs(self.noise.penalty - penalty)\n      noise_identity = penalty * tf.squared_difference(noisy, noise)\n      noise_identity = tf.reduce_mean(noise_identity)\n      noise_tv = tf.reduce_mean(tf.image.total_variation(noisy))\n      # tv clamp\n      l_tv_max = tf.nn.relu(noise_tv - 1000) ** 2\n      l_tv_min = tf.nn.relu(100 - noise_tv) ** 2\n      noise_tv += l_tv_max + l_tv_min\n      loss = tf.stack([l1_with_noise, noise_identity, noise_tv])\n      loss *= self.weights[:-1]\n      loss = tf.reduce_sum(loss)\n      self.train_metric[\'l1/noisy\'] = l1_with_noise\n      self.train_metric[\'l1/finet\'] = l1_fine_tune\n      self.train_metric[\'ni\'] = noise_identity\n      self.train_metric[\'nt\'] = noise_tv\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      var_g = tf.trainable_variables(self.name)\n      var_o = tf.trainable_variables(\'Offset\')\n      with tf.control_dependencies(update_ops):\n        op = tf.train.AdamOptimizer(self.learning_rate, 0.9)\n        op = op.minimize(loss, self.global_steps, var_list=var_g)\n        self.loss.append(op)\n        op = tf.train.AdamOptimizer(self.learning_rate, 0.9)\n        op = op.minimize(l1_fine_tune, self.global_steps, var_list=var_o)\n        self.loss.append(op)\n\n    def loss2():\n      l1_clean = tf.losses.mean_squared_error(clean[0], labels_norm)\n      var_g = tf.trainable_variables(self.name)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        op = tf.train.AdamOptimizer(self.learning_rate, 0.9)\n        op = op.minimize(l1_clean, self.global_steps, var_list=var_g)\n        self.loss += [op, op]\n      self.train_metric[\'l1/tune\'] = l1_clean\n\n    def loss3():\n      l1_clean = tf.losses.mean_squared_error(clean_fine[0], labels_norm)\n      var_f = tf.trainable_variables(\'Fine\')\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        op = tf.train.AdamOptimizer(self.learning_rate, 0.9)\n        op = op.minimize(l1_clean, self.global_steps, var_list=var_f)\n        self.loss += [op, op]\n      self.train_metric[\'l1/tune\'] = l1_clean\n      tf.summary.image(\'hr/coarse\', _clip(self.outputs[-2]))\n\n    with tf.name_scope(\'Loss\'):\n      if self.finetune == -1:\n        loss3()\n      elif \'DrTrainer\' in str(self.trainer):\n        loss2()\n      else:\n        loss1()\n    self.metrics[\'psnr1\'] = tf.reduce_mean(\n      tf.image.psnr(self.label[-1], self.outputs[-1], max_val=255))\n    tf.summary.image(\'noisy/zero\', zero)\n\n  def build_loss(self):\n    pass\n\n  def build_summary(self):\n    super(DRSR, self).build_summary()\n    tf.summary.image(\'lr/input\', self.inputs[-1])\n    tf.summary.image(\'hr/fine\', _clip(self.outputs[-1]))\n    tf.summary.image(\'hr/label\', _clip(self.label[0]))\n\n  def build_saver(self):\n    var_g = tf.global_variables(self.name)\n    steps = [self.global_steps]\n    loss = tf.global_variables(\'Loss\')\n    self.savers.update(drsr_g=tf.train.Saver(var_g, max_to_keep=1),\n                       misc=tf.train.Saver(steps + loss, max_to_keep=1))\n    if self.finetune == -1:\n      var_f = tf.global_variables(\'Fine\')\n      self.savers.update(drsr_f=tf.train.Saver(var_f, max_to_keep=1))\n\n  def train_batch(self, feature, label, learning_rate=1e-4, **kwargs):\n    epochs = kwargs.get(\'epochs\')\n    if epochs < self.finetune:\n      loss = self.loss[0]\n    else:\n      loss = self.loss[1]\n    return super(DRSR, self).train_batch(feature, label, learning_rate,\n                                         loss=loss)\n\n\nclass DrTrainer(Trainer.VSR):\n  def fn_train_each_step(self, label=None, feature=None, name=None,\n                         post=None):\n    if not self.model.tfr:\n      return super(DrTrainer, self).fn_train_each_step(\n        label, feature, name, post)\n    v = self.v\n    for fn in v.feature_callbacks:\n      feature = fn(feature, name=name)\n      post = fn(post, name=name)\n    for fn in v.label_callbacks:\n      label = fn(label, name=name)\n    loss = self._m.train_batch(post, label, learning_rate=v.lr,\n                               epochs=v.epoch)\n    v.global_step = self._m.global_steps.eval()\n    for _k, _v in loss.items():\n      v.avg_meas[_k] = \\\n        v.avg_meas[_k] + [_v] if v.avg_meas.get(_k) else [_v]\n      loss[_k] = \'{:08.5f}\'.format(_v)\n    v.loss = loss\n\n  def fn_benchmark_each_step(self, label=None, feature=None, name=None,\n                             post=None):\n    if not self.model.tfr:\n      return super(DrTrainer, self).fn_benchmark_each_step(\n        label, feature, name, post)\n    v = self.v\n    origin_feat = feature\n    for fn in v.feature_callbacks:\n      feature = fn(feature, name=name)\n      # post = fn(post, name=name)\n    for fn in v.label_callbacks:\n      label = fn(label, name=name)\n    outputs, metrics = self._m.test_batch(post, label, epochs=v.epoch)\n    for _k, _v in metrics.items():\n      if _k not in v.mean_metrics:\n        v.mean_metrics[_k] = []\n      v.mean_metrics[_k] += [_v]\n    for fn in v.output_callbacks:\n      outputs = fn(outputs, input=origin_feat, label=label, name=name,\n                   mode=v.color_format, subdir=v.subdir)\n'"
VSR/Backend/TF/Models/Drsr_v2.py,60,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 2 - 28\n\nfrom functools import partial\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom VSR.Util import Config, to_list\nfrom ..Arch.Residual import cascade_rdn\nfrom ..Framework import Noise\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import clip_image\n\n_MEAN_GT = [84.1148, 68.3644, 64.8452]\n_MEAN_SR = [85.6586, 68.7887, 66.5135]\nLOG = logging.getLogger(\'VSR.Model.DRSRv2\')\n\n\ndef _denormalize(inputs, shift):\n  return inputs * 255 + shift\n\n\ndef _normalize(inputs, shift):\n  return (inputs - shift) / 255\n\n\nclass DRSR(SuperResolution):\n  def __init__(self, name=\'drsr_v2\', noise_config=None, weights=(1, 10, 1e-5),\n               level=1, mean_shift=(0, 0, 0), arch=None, auto_shift=None,\n               **kwargs):\n    super(DRSR, self).__init__(**kwargs)\n    self.name = name\n    self.noise = Config(scale=0, offset=0, penalty=0.5, max=0, layers=7)\n    if isinstance(noise_config, (dict, Config)):\n      self.noise.update(**noise_config)\n      self.noise.crf = np.load(self.noise.crf)\n      self.noise.offset = to_list(self.noise.offset, 4)\n      self.noise.offset = [x / 255 for x in self.noise.offset]\n      self.noise.max /= 255\n    self.weights = weights\n    self.level = level\n    if mean_shift is not None:\n      self.norm = partial(_normalize, shift=mean_shift)\n      self.denorm = partial(_denormalize, shift=mean_shift)\n    self.arch = arch\n    self.auto = auto_shift\n    self.to_sum = []\n\n  def display(self):\n    LOG.info(str(self.noise))\n\n  def noise_cond(self, inputs, noise, layers, scope=\'NCL\'):\n    with tf.variable_scope(None, scope):\n      x = noise\n      c = inputs.shape[-1]\n      for _ in range(layers - 1):\n        x = self.prelu_conv2d(x, 64, 3)\n      x = self.conv2d(x, c, 3)\n      gamma = tf.nn.sigmoid(x)\n      x = noise\n      for _ in range(layers - 1):\n        x = self.prelu_conv2d(x, 64, 3)\n      beta = self.conv2d(x, c, 3)\n      return inputs * gamma + beta\n\n  def cond_rb(self, inputs, noise, scope=\'CRB\'):\n    with tf.variable_scope(None, scope):\n      x = self.prelu_conv2d(inputs, 64, 3)\n      x = self.conv2d(x, 64, 3)\n      x = self.noise_cond(x, noise, 3)\n      if inputs.shape[-1] != x.shape[-1]:\n        sc = self.conv2d(inputs, x.shape[-1], 1,\n                         kernel_initializer=\'he_uniform\')\n      else:\n        sc = inputs\n      return sc + x\n\n  def cond_rdb(self, inputs, noise, scope=\'CRDB\'):\n    with tf.variable_scope(None, scope):\n      x0 = self.prelu_conv2d(inputs, 64, 3)\n      x1 = self.prelu_conv2d(tf.concat([inputs, x0], -1), 64, 3)\n      x2 = self.conv2d(tf.concat([inputs, x0, x1], -1), 64, 3)\n      x = self.noise_cond(x2, noise, 3)\n      if inputs.shape[-1] != x.shape[-1]:\n        sc = self.conv2d(inputs, x.shape[-1], 1,\n                         kernel_initializer=\'he_uniform\')\n      else:\n        sc = inputs\n      return sc + x\n\n  def noise_estimate(self, inputs, scope=\'NoiseEstimator\', reuse=None):\n    n = self.noise\n    with tf.variable_scope(None, scope, reuse=reuse):\n      x = inputs\n      for _ in range(n.layers):\n        x = self.leaky_conv2d(x, 64, 3)\n      x = self.conv2d(x, self.channel, 3)\n      return x\n\n  def noise_shift(self, inputs, layers, scope=\'NoiseShift\', reuse=None):\n    n = self.noise\n    with tf.variable_scope(None, scope, reuse=reuse):\n      x = inputs\n      for _ in range(layers):\n        x = self.leaky_conv2d(x, 64, 3)\n      x = self.conv2d(x, self.channel, 3, activation=tf.nn.sigmoid)\n      return x * Noise.tf_gaussian_noise(inputs, n.max)\n\n  def local_net(self, inputs, noise, depth=4, scope=\'LC\'):\n    with tf.variable_scope(None, scope):\n      fl = [inputs]\n      x = inputs\n      for i in range(depth):\n        x = self.cond_rb(x, noise)\n        fl.append(x)\n        x = tf.concat(fl, axis=-1)\n        x = self.conv2d(x, 64, 1, kernel_initializer=\'he_uniform\')\n      return x\n\n  def local_net2(self, inputs, noise, depth=4, scope=\'LC\'):\n    with tf.variable_scope(None, scope):\n      fl = [inputs]\n      x = inputs\n      for i in range(depth):\n        x = self.cond_rdb(x, noise)\n        fl.append(x)\n        x = tf.concat(fl, axis=-1)\n        x = self.conv2d(x, 64, 1, kernel_initializer=\'he_uniform\')\n      return x\n\n  def global_net(self, inputs, noise, depth=4, scope=\'GC\', reuse=None):\n    with tf.variable_scope(None, scope, reuse=reuse):\n      fl = [inputs]\n      x = inputs\n      for i in range(depth):\n        if self.arch == \'concat\':\n          x = cascade_rdn(self, x, depth=3, use_ca=True)\n        elif self.arch == \'crb\':\n          x = self.local_net(x, noise[i], 4)\n        else:\n          x = self.local_net2(x, noise[i], 3)\n        if self.arch != \'crdb\':\n          fl.append(x)\n          x = tf.concat(fl, axis=-1)\n          x = self.conv2d(x, 64, 1, kernel_initializer=\'he_uniform\')\n      self.to_sum += fl\n      if self.arch == \'crdb\':\n        x += inputs\n      if self.auto:\n        sr = self.upscale(x, direct_output=False, scale=4)\n      else:\n        sr = self.upscale(x, direct_output=False)\n      sr = self.conv2d(sr, self.channel, 3)\n      return sr, x\n\n  def gen_noise(self, inputs, ntype, max1=0.06, max2=0.16):\n    with tf.name_scope(\'GenNoise\'):\n      n = self.noise\n      if ntype == \'gaussian\':\n        noise = Noise.tf_gaussian_noise(inputs, sigma_max=max1,\n                                        channel_wise=False)\n        return noise\n      elif ntype == \'crf\':\n        crf = tf.convert_to_tensor(n.crf[\'crf\'])\n        icrf = tf.convert_to_tensor(n.crf[\'icrf\'])\n        i = tf.random_uniform([], 0, crf.shape[0], dtype=tf.int32)\n        irr = Noise.tf_camera_response_function(inputs, icrf[i], max_val=1)\n        noise = Noise.tf_gaussian_poisson_noise(irr, max_c=max1, max_s=max2)\n        img = Noise.tf_camera_response_function(irr + noise, crf[i], max_val=1)\n        return img - inputs\n      else:\n        raise TypeError(ntype)\n\n  def net(self, inputs, level, scale=1, shift=(0, 0, 0, 0), reuse=None):\n    with tf.variable_scope(self.name, reuse=reuse):\n      level_outputs = []\n      level_noise = []\n      level_inputs = []\n      for i in range(1, level + 1):\n        with tf.variable_scope(f\'Level{i:1d}\'):\n          noise_hyp = self.noise_estimate(inputs) * scale + \\\n                      Noise.tf_gaussian_noise(inputs, self.noise.offset[0])\n          level_noise.append(noise_hyp)\n          noise_hyp = [noise_hyp + shift[0],\n                       noise_hyp + shift[1],\n                       noise_hyp + shift[2],\n                       noise_hyp + shift[3]]\n          if i == 1:\n            if self.arch == \'concat\':\n              inputs = tf.concat([inputs, noise_hyp[0]], axis=-1)\n            entry = self.conv2d(inputs, 64, 3)\n            entry = self.conv2d(entry, 64, 3)\n            level_inputs.append(entry)\n          y = self.global_net(level_inputs[-1], noise_hyp, 4)\n          level_outputs.append(y[0])\n          level_inputs.append(y[1])\n      return level_noise, level_outputs\n\n  def build_graph(self):\n    super(DRSR, self).build_graph()\n    inputs_norm = self.norm(self.inputs_preproc[-1])\n    labels_norm = self.norm(self.label[-1])\n    n = self.noise\n    if n.valid:\n      LOG.info(""adding noise"")\n      awgn = self.gen_noise(inputs_norm, \'gaussian\', n.max)\n      gp = self.gen_noise(inputs_norm, \'crf\', 5 / 255, n.max)\n    else:\n      awgn = gp = tf.zeros_like(inputs_norm)\n\n    if self.level == 1:\n      noise = awgn\n    elif self.level == 2:\n      noise = gp\n    else:\n      raise NotImplementedError(""Unknown level!"")\n    with tf.variable_scope(\'Offset\'):\n      shift = []\n      if not self.auto:\n        for i in range(4):\n          shift.append(Noise.tf_gaussian_noise(inputs_norm, n.offset[i]))\n        var_shift = []\n      else:\n        for i in range(4):\n          shift.append(self.noise_shift(inputs_norm, 8, f\'NoiseShift_{i}\'))\n        var_shift = tf.trainable_variables(\'Offset\')\n\n    noise_hyp, outputs = self.net(inputs_norm + noise, 1, n.scale, shift)\n    self.outputs += [tf.abs(x * 255) for x in noise_hyp + shift]\n    self.outputs += [self.denorm(x) for x in outputs]\n\n    l1_image = tf.losses.absolute_difference(outputs[-1], labels_norm)\n    noise_abs_diff = tf.abs(noise_hyp[-1]) - tf.abs(noise)\n    # 1: over estimated; 0: under estimated\n    penalty = tf.ceil(tf.clip_by_value(noise_abs_diff, 0, 1))\n    # 1 - n: over estimated; n: under estimated\n    penalty = tf.abs(n.penalty - penalty)\n    noise_error = penalty * tf.squared_difference(noise_hyp[-1], noise)\n    l2_noise = tf.reduce_mean(noise_error)\n\n    # tv clamp\n    tv = tf.reduce_mean(tf.image.total_variation(noise_hyp[-1]))\n    l_tv_max = tf.nn.relu(tv - 1000) ** 2\n    l_tv_min = tf.nn.relu(200 - tv) ** 2\n    tv = tv + l_tv_max + l_tv_min\n\n    def loss_fn1():\n      w = self.weights\n      loss = l1_image * w[0] + l2_noise * w[1] + tv * w[2]\n      var_to_opt = tf.trainable_variables(self.name + f""/Level1"")\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        op = tf.train.AdamOptimizer(self.learning_rate, 0.9)\n        op = op.minimize(loss, self.global_steps, var_list=var_to_opt)\n        self.loss.append(op)\n\n      self.train_metric[\'mae\'] = l1_image\n      self.train_metric[\'noise_error\'] = l2_noise\n      self.train_metric[\'tv\'] = tv\n      self.to_sum += noise_hyp\n\n    def loss_fn2():\n      w = self.weights\n      tv_noise = [tf.reduce_mean(tf.image.total_variation(x)) for x in shift]\n      tv_noise = tf.add_n(tv_noise) / 4\n      tv_max = tf.nn.relu(tv_noise - 1000) ** 2\n      tv_min = tf.nn.relu(200 - tv_noise) ** 2\n      tv_noise += tv_max + tv_min\n      loss = l1_image * w[0] + tv_noise * w[2]\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        op = tf.train.AdamOptimizer(self.learning_rate, 0.9)\n        op = op.minimize(loss, self.global_steps, var_list=var_shift)\n        self.loss.append(op)\n      self.train_metric[\'mae\'] = l1_image\n      self.train_metric[\'tv\'] = tv_noise\n\n    with tf.name_scope(\'Loss\'):\n      if not self.auto:\n        loss_fn1()\n      else:\n        loss_fn2()\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(self.label[-1], self.outputs[-1], max_val=255))\n\n  def build_loss(self):\n    pass\n\n  def build_summary(self):\n    super(DRSR, self).build_summary()\n    # tf.summary.image(\'lr/input\', self.inputs[-1])\n    tf.summary.image(f\'hr/fine_1\', clip_image(self.outputs[-1]))\n    tf.summary.image(\'hr/label\', clip_image(self.label[0]))\n\n  def build_saver(self):\n    var_misc = tf.global_variables(\'Loss\') + [self.global_steps]\n    self.savers.update(misc=tf.train.Saver(var_misc, max_to_keep=1))\n    var_g = tf.global_variables(self.name + f""/Level1"")\n    self.savers.update({\n      f""level_1"": tf.train.Saver(var_g, max_to_keep=1)\n    })\n    if self.auto:\n      self.savers.update(shift=tf.train.Saver(\n        tf.global_variables(\'Offset\'),\n        max_to_keep=1))\n'"
VSR/Backend/TF/Models/Duf.py,35,"b'""""""\nCopyright: Intel Corp. 2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Oct 9th 2018\n\nDynamic Upsampling Filters (CVPR 2018)\nSee http://openaccess.thecvf.com/content_cvpr_2018/papers/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.pdf\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import pixel_shift\n\n\nclass DUF(SuperResolution):\n  """"""Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation\n\n  """"""\n\n  STP = [[0, 0], [1, 1], [1, 1], [1, 1], [0, 0]]\n  SP = [[0, 0], [0, 0], [1, 1], [1, 1], [0, 0]]\n\n  def __init__(self, layers=16, filter_size=(5, 5), depth=7, name=\'duf\',\n               **kwargs):\n    super(DUF, self).__init__(**kwargs)\n    self.layers = layers\n    self.filter_size = to_list(filter_size, 2)\n    self.depth = depth\n    self.name = name\n\n  @staticmethod\n  def _normalize(x):\n    return x / 255\n\n  @staticmethod\n  def _denormalize(x):\n    return x * 255\n\n  def _dyn_filter3d(self, x, F):\n    """"""\n    3D Dynamic filtering\n    input x: (b, t, h, w)\n          F: (b, h, w, tower_depth, output_depth)\n          filter_shape (ft, fh, fw)\n    """"""\n    # make tower\n    size = np.prod(self.filter_size)\n    filter_localexpand_np = np.eye(size)\n    filter_localexpand_np = filter_localexpand_np.reshape(\n      [*self.filter_size, 1, size])\n    x = tf.expand_dims(x, axis=-1)\n    x_localexpand = tf.nn.conv2d(x, filter_localexpand_np, [1, 1, 1, 1],\n                                 \'SAME\')  # b, h, w, 1*5*5\n    x_localexpand = tf.expand_dims(x_localexpand, axis=3)  # b, h, w, 1, 1*5*5\n    x = tf.matmul(x_localexpand, F)  # b, h, w, 1, R*R\n    x = tf.squeeze(x, axis=3)  # b, h, w, R*R\n\n    return x\n\n  def build_graph(self):\n    self.inputs.append(\n      tf.placeholder(tf.float32, [None, None, None, None, self.channel],\n                     name=\'input/lr\'))\n    self.label.append(\n      tf.placeholder(tf.float32, [None, None, None, None, self.channel],\n                     name=\'label/hr\'))\n\n    input_norm = self._normalize(self.inputs[-1])\n    with tf.variable_scope(self.name):\n      F, G = 64, 32\n      t = tf.pad(input_norm, self.SP)\n      t = self.conv3d(t, F, (1, 3, 3), padding=\'valid\')\n      for i in range(self.layers - 3):\n        x = tf.layers.batch_normalization(t, training=self.training_phase)\n        x = tf.nn.relu(x)\n        x = self.conv3d(x, F, 1, activation=\'relu\', use_batchnorm=True)\n        x = self.conv3d(x, G, 3)\n        t = tf.concat([t, x], axis=-1)\n        F += G\n      for i in range(3):\n        x = tf.layers.batch_normalization(t, training=self.training_phase)\n        x = tf.nn.relu(x)\n        x = self.conv3d(x, F, 1, activation=\'relu\', use_batchnorm=True)\n        x = tf.pad(x, self.SP)\n        x = self.conv3d(x, G, 3, padding=\'valid\')\n        t = tf.concat([t[:, 1:-1], x], axis=-1)\n        F += G\n      t = tf.layers.batch_normalization(t, training=self.training_phase)\n      t = tf.nn.relu(t)\n      t = self.conv3d(t, 256, (1, 3, 3), activation=\'relu\')\n\n      r = self.relu_conv3d(t, 256, 1)\n      r = self.relu_conv3d(r, np.prod([self.channel, *self.scale]), 1)\n      r = r[:, 0]\n      r = pixel_shift(r, self.scale, self.channel)\n\n      f = self.relu_conv3d(t, 512, 1)\n      f = self.conv3d(f, np.prod(self.filter_size) * np.prod(self.scale), 1)\n      ds_f = tf.shape(f)\n      f = tf.reshape(f, [ds_f[0], ds_f[1], ds_f[2], ds_f[3],\n                         np.prod(self.filter_size), np.prod(self.scale)])\n      f = tf.nn.softmax(f, axis=4)\n\n      sr = []\n      for i in range(self.channel):\n        x = self._dyn_filter3d(input_norm[:, self.depth // 2, ..., i], f[:, 0])\n        sr.append(pixel_shift(x, self.scale))\n      sr = tf.concat(sr, axis=-1)\n      sr += r\n      self.outputs.append(self._denormalize(sr))\n\n  def build_loss(self):\n    sr_norm = self._normalize(self.outputs[-1])\n    label_norm = self._normalize(self.label[-1])\n    label_c = label_norm[:, self.depth // 2]\n\n    with tf.name_scope(\'Loss\'):\n      huber = tf.losses.huber_loss(label_c, sr_norm, delta=0.01)\n      mse = tf.losses.mean_squared_error(label_c, sr_norm)\n      reg = tf.losses.get_regularization_losses()\n\n      loss = tf.add_n([huber] + reg)\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        opt = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          loss, self.global_steps)\n        self.loss.append(opt)\n\n    self.train_metric[\'loss\'] = loss\n    self.metrics[\'mse\'] = mse\n    self.metrics[\'psnr\'] = tf.reduce_mean(tf.image.psnr(label_c, sr_norm, 1))\n\n  def build_summary(self):\n    tf.summary.scalar(\'loss\', self.train_metric[\'loss\'])\n    tf.summary.scalar(\'mse\', self.metrics[\'mse\'])\n    tf.summary.scalar(\'psnr\', self.metrics[\'psnr\'])\n    tf.summary.image(\'SR\', self.outputs[-1], 1)\n'"
VSR/Backend/TF/Models/Edsr.py,15,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: June 15th 2018\nUpdated Date: June 15th 2018\n\nEnhanced Deep Residual Networks for Single Image Super-Resolution (CVPR 2017)\nSee https://arxiv.org/abs/1707.02921\n""""""\n\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass EDSR(SuperResolution):\n  """"""Enhanced Deep Residual Networks for Single Image Super-Resolution\n\n  Args:\n      layers: number of residual blocks\n      filters: number of filters in each conv2d\n      clip: feature value clip ratio in each residual block\n  """"""\n\n  def __init__(self, layers=32, filters=256, clip=0.1, name=\'edsr\', **kwargs):\n    self.layers = layers\n    self.filters = filters\n    self.clip = clip\n    self.name = name\n    super(EDSR, self).__init__(**kwargs)\n\n  def build_graph(self):\n    super(EDSR, self).build_graph()\n    with tf.variable_scope(self.name):\n      fe = self.conv2d(self.inputs_preproc[-1], self.filters, 3)\n      x = fe\n      for _ in range(self.layers):\n        with tf.variable_scope(None, \'ResBlock\'):\n          x_old = x\n          x = self.relu_conv2d(x, self.filters, 3)\n          x = self.conv2d(x, self.filters, 3)\n          x = x * self.clip + x_old\n      x = self.conv2d(x, self.filters, 3)\n      x += fe\n      x = self.upscale(x, direct_output=False)\n      x = self.conv2d(x, self.channel, 3)\n      self.outputs.append(x)\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      opt = tf.train.AdamOptimizer(self.learning_rate)\n      mse = tf.losses.mean_squared_error(self.label[-1], self.outputs[-1])\n      mae = tf.losses.absolute_difference(self.label[-1], self.outputs[-1])\n      loss = tf.add_n([mae] + tf.losses.get_regularization_losses(),\n                      name=\'total_loss\')\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        self.loss.append(opt.minimize(loss, self.global_steps))\n\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mse\'] = mse\n      self.metrics[\'mae\'] = mae\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(self.label[-1], self.outputs[-1], max_val=255))\n\n  def build_summary(self):\n    super(EDSR, self).build_summary()\n    tf.summary.image(\'SR\', self.outputs[-1], 1)\n\n  def build_saver(self):\n    self.savers.update({\n      self.name: tf.train.Saver(tf.trainable_variables(self.name),\n                                max_to_keep=1)\n    })\n'"
VSR/Backend/TF/Models/Espcn.py,6,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 12th 2018\nUpdated Date: May 25th 2018\n\nEfficient Sub-Pixel Convolutional Neural Network\nRef https://arxiv.org/abs/1609.05158\n""""""\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom ..Framework.SuperResolution import SuperResolution\n\n\ndef _normalize(x):\n  return x / 127.5 - 1\n\n\ndef _denormalize(x):\n  return (x + 1) * 127.5\n\n\nclass ESPCN(SuperResolution):\n  """"""Efficient Sub-Pixel Convolutional Neural Network.\n\n  Args:\n      layers: layer number of the network\n      filters: a tuple of integer, representing each layer\'s filters\n      kernel: a tuple of integer, representing each layer\'s kernel size\n  """"""\n\n  def __init__(self, layers=3, filters=(64, 32), kernel=(5, 3, 3),\n               name=\'espcn\', **kwargs):\n    super(ESPCN, self).__init__(**kwargs)\n    self.name = name\n    self.layers = layers\n    self.filters = to_list(filters, layers - 1)\n    self.kernel_size = to_list(kernel, layers)\n    if len(self.kernel_size) < self.layers:\n      self.kernel_size += to_list(\n        kernel[-1], self.layers - len(self.kernel_size))\n\n  def build_graph(self):\n    super(ESPCN, self).build_graph()\n    with tf.variable_scope(self.name):\n      x = _normalize(self.inputs_preproc[-1])\n      for f, k in zip(self.filters, self.kernel_size):\n        x = self.tanh_conv2d(x, f, k, kernel_initializer=\'torch\')\n      x = self.upscale(x, \'espcn\', direct_output=True,\n                       kernel_initializer=\'torch\')\n      self.outputs.append(_denormalize(x))\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      mse, loss = super(ESPCN, self).build_loss()\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mse\'] = mse\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(self.label[-1], self.outputs[-1], max_val=255))\n'"
VSR/Backend/TF/Models/FFDNet.py,10,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/25 \xe4\xb8\x8b\xe5\x8d\x882:13\n\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass FFDNet(SuperResolution):\n  """"""FFDNet: Toward a Fast and Flexible Solution for CNN-Based Image Denoising\n  By Kai Zhang. (IEEE TIP 2018)\n\n  Args:\n    sigma: in training phase, this is the max sigma level added to clean images,\n      in testing phase, this is input noise level, correspond to pixel [0, 255].\n    space_down: block size for space-to-depth (default 2, same as paper\'s).\n    layers: convolutional layers used in the network.\n    training: set to false when evaluating.\n  """"""\n\n  def __init__(self, sigma, space_down=2, layers=15, training=True,\n               name=\'ffdnet\', **kwargs):\n    self.name = name\n    self.sigma = sigma\n    self.space_down = space_down\n    self.layers = layers\n    self.training = training\n    if \'scale\' in kwargs:\n      kwargs.pop(\'scale\')\n    super(FFDNet, self).__init__(scale=1, **kwargs)\n\n  def build_graph(self):\n    super(FFDNet, self).build_graph()  # build inputs placeholder\n    with tf.variable_scope(self.name):\n      # build layers\n      inputs = self.inputs_preproc[-1] / 255\n      if self.training:\n        sigma = tf.random_uniform((), maxval=self.sigma / 255)\n        inputs += tf.random_normal(tf.shape(inputs)) * sigma\n      else:\n        sigma = self.sigma / 255\n      inputs = tf.space_to_depth(inputs, block_size=self.space_down)\n      noise_map = tf.ones_like(inputs)[..., 0:1] * sigma\n      x = tf.concat([inputs, noise_map], axis=-1)\n      x = self.relu_conv2d(x, 64, 3)\n      for i in range(1, self.layers - 1):\n        x = self.bn_relu_conv2d(x, 64, 3, use_bias=False)\n      # the last layer w/o BN and ReLU\n      x = self.conv2d(x, self.channel * self.space_down ** 2, 3)\n      denoised = tf.depth_to_space(x, block_size=self.space_down)\n      self.outputs.append(denoised * 255)\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      mse, loss = super(FFDNet, self).build_loss()\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mse\'] = mse\n      self.metrics[\'psnr\'] = tf.reduce_mean(tf.image.psnr(\n        self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(tf.image.ssim(\n        self.label[-1], self.outputs[-1], max_val=255))\n'"
VSR/Backend/TF/Models/Gan.py,54,"b'""""""\nCopyright: Intel Corp. 2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Oct 19th 2018\n\nFor ICLR 2019 Reproducibility Challenge\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nimport tqdm\n\nfrom VSR.Util import Config, to_list\nfrom ..Arch import Discriminator\nfrom ..Framework.GAN import (\n  gradient_penalty, inception_score, loss_bce_gan,\n  loss_lsgan, loss_relative_bce_gan,\n  loss_relative_lsgan, loss_wgan\n)\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Framework.Trainer import VSR\n\n\nclass GAN(SuperResolution):\n  """"""Base class of GAN.\n  Args:\n      name: model name.\n      patch_size: generated image size.\n      z_dim: latent space dimension.\n      init_filter: filter size. (ideally, 512 for 32x32, 1024 for 64x64).\n      linear: boolean, toggle FC layer after random vector.\n      norm_g: normalization of G.\n      norm_d: normalization of D.\n      use_bias: boolean, use bias variables.\n      optimizer: str: \'adam\', \'rmsprop\', \'momentum\', \'sgd\'.\n      arch: G/D architecture: \'dcgan\' or \'resnet\'.\n      nd_iter: number of D updates for each G update.\n  """"""\n\n  def __init__(self, name=\'gan\', patch_size=32, z_dim=128, init_filter=512,\n               linear=False, norm_g=None, norm_d=None, use_bias=False,\n               optimizer=None, arch=None, nd_iter=1, **kwargs):\n    super(GAN, self).__init__(**kwargs)\n    self.name = name\n    self._trainer = GanTrainer\n    self.output_size = patch_size\n    self.z_dim = z_dim\n    self.init_filter = init_filter\n    self.linear = linear\n    self.bias = use_bias\n    self.nd_iter = nd_iter\n    if isinstance(norm_g, str):\n      self.bn = np.any([word in norm_g for word in (\'bn\', \'batch\')])\n      self.sn = np.any([word in norm_g for word in (\'sn\', \'spectral\')])\n    self.d_outputs = []  # (real, fake)\n    self.g_outputs = []  # (real, fake)\n    # monitor probability of being real and fake\n    self.p_fake = None\n    self.p_real = None\n    self.opt = optimizer\n    if self.opt is None:\n      self.opt = Config(name=\'adam\')\n    if arch is None or arch == \'dcgan\':\n      self.G = self.dcgan_g\n      self.D = Discriminator.dcgan_d(\n        self, [patch_size, patch_size, self.channel],\n        norm=norm_d, name_or_scope=\'D\')\n    elif arch == \'resnet\':\n      self.G = self.resnet_g\n      self.D = Discriminator.resnet_d(\n        self, [patch_size, patch_size, self.channel], times_pooling=4,\n        norm=norm_d, name_or_scope=\'D\')\n\n  @staticmethod\n  def _normalize(x):\n    return x / 127.5 - 1\n\n  @staticmethod\n  def _denormalize(x):\n    return (x + 1) * 127.5\n\n  def dcgan_g(self, inputs):\n    with tf.variable_scope(\'G\', reuse=tf.AUTO_REUSE):\n      f = self.init_filter\n      size = 4\n      n_up = int(np.log2(self.output_size // size)) + 1\n      kwargs = dict(use_sn=self.sn,\n                    kernel_initializer=\'random_normal_0.02\')\n      x = self.dense(inputs, f * size * size, use_sn=self.sn,\n                     kernel_initializer=\'random_normal_0.02\')\n      if self.bn:\n        x = self.batch_norm(x, self.training_phase, epsilon=2e-5)\n      x = tf.nn.relu(x)\n      x = tf.reshape(x, [-1, size, size, f])\n      for i in range(1, n_up):\n        x = self.deconv2d(x, f // 2 ** i, 4, 2, **kwargs)\n        if self.bn:\n          x = self.batch_norm(x, self.training_phase, epsilon=2e-5)\n        x = tf.nn.relu(x)\n      x = tf.nn.relu(x)\n      x = self.deconv2d(x, self.channel, 3, 1, **kwargs)\n      x = tf.nn.tanh(x)\n    return x\n\n  def resnet_g(self, inputs):\n    with tf.variable_scope(\'G\', reuse=tf.AUTO_REUSE):\n      f = self.init_filter // 2\n      size = 4\n      n_up = int(np.log2(self.output_size // size))\n      x = self.dense(inputs, f * size * size, use_sn=self.sn,\n                     kernel_initializer=\'random_normal_0.02\')\n      x = tf.reshape(x, [-1, size, size, f])\n      for _ in range(n_up):\n        # up\n        x = self.upscale(x, \'nearest\', 2)\n        x = self.resblock(x, 256, 3, activation=\'relu\',\n                          use_batchnorm=self.bn,\n                          use_sn=self.sn, use_bias=self.bias)\n      x = self.batch_norm(x, self.training_phase)\n      x = tf.nn.relu(x)\n      x = self.tanh_conv2d(x, self.channel, 3)\n      return x\n\n  def build_graph(self):\n    self.inputs.append(\n      tf.placeholder(\'float32\', (None, self.z_dim,), name=\'input/noise\'))\n    self.label.append(tf.placeholder(\n      \'float32\', [None, self.output_size, self.output_size, self.channel],\n      name=\'label/image\'))\n    with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n      fake_image = self.G(self.inputs[0])\n      real_image = self._normalize(self.label[0])\n\n      real_disc = self.D(real_image)\n      fake_disc = self.D(fake_image)\n      self.outputs.append(self._denormalize(fake_image))\n      self.d_outputs = (real_disc, fake_disc)\n      self.g_outputs = (real_image, fake_image)\n\n    self.p_fake = tf.reduce_mean(tf.sigmoid(fake_disc))\n    self.p_real = tf.reduce_mean(tf.sigmoid(real_disc))\n\n  def _build_loss(self, g_loss, d_loss):\n    # used in sub-class\n    var_d = tf.trainable_variables(self.name + \'/D\')\n    var_g = tf.trainable_variables(self.name + \'/G\')\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      opt_d = self.get_optimizer(self.opt)\n      opt_g = self.get_optimizer(self.opt)\n      op_d = opt_d.minimize(d_loss, self.global_steps, var_list=var_d)\n      op_g = opt_g.minimize(g_loss, var_list=var_g)\n      self.loss = [op_g, op_d]\n\n    self.train_metric = {\'gloss\': g_loss, \'d_loss\': d_loss,\n                         \'p_real\': self.p_real, \'p_fake\': self.p_fake}\n    self.metrics[\'inception-score\'] = inception_score(self.outputs[0], 1)\n    self.metrics[\'p_real\'] = self.p_real\n    self.metrics[\'p_fake\'] = self.p_fake\n\n  def build_summary(self):\n    tf.summary.scalar(\'FakeD\', self.p_fake)\n    tf.summary.scalar(\'RealD\', self.p_real)\n    tf.summary.scalar(\'Inception_Score\', self.metrics[\'inception-score\'])\n\n  def build_saver(self):\n    var_g = tf.global_variables(self.name + \'/G\')\n    var_d = tf.global_variables(self.name + \'/D\')\n    var_loss = tf.global_variables(\'Loss\')\n    steps = [self.global_steps]\n    self.savers[\'gen\'] = tf.train.Saver(var_g + steps, max_to_keep=1)\n    self.savers[\'disc\'] = tf.train.Saver(var_d, max_to_keep=1)\n    self.savers[\'loss\'] = tf.train.Saver(var_loss, max_to_keep=1)\n\n  def get_optimizer(self, config=None):\n    if config is None:\n      config = self.opt\n    name = config.name\n    if name == \'adam\':\n      return tf.train.AdamOptimizer(self.learning_rate, **config)\n    elif name == \'rmsprop\':\n      return tf.train.RMSPropOptimizer(self.learning_rate, **config)\n    elif name == \'momentum\':\n      return tf.train.MomentumOptimizer(self.learning_rate, **config)\n    elif name == \'sgd\':\n      return tf.train.GradientDescentOptimizer(\n        self.learning_rate, **config)\n    return None\n\n  def train_batch(self, feature, label, learning_rate=1e-4, **kwargs):\n    feature = to_list(feature)\n    label = to_list(label)\n    self.feed_dict.update(\n      {self.training_phase: True, self.learning_rate: learning_rate})\n    for i in range(len(self.inputs)):\n      self.feed_dict[self.inputs[i]] = feature[i]\n    for i in range(len(self.label)):\n      self.feed_dict[self.label[i]] = label[i]\n    loss = kwargs.get(\'loss\') or self.loss\n    loss = to_list(loss)\n    step = kwargs[\'steps\']\n    sess = tf.get_default_session()\n    if step % self.nd_iter == 0:\n      # update G-net\n      sess.run(loss[0], feed_dict=self.feed_dict)\n    # update D-net\n    sess.run(loss[1:], feed_dict=self.feed_dict)\n    loss = sess.run(list(self.train_metric.values()),\n                    feed_dict=self.feed_dict)\n    ret = {}\n    for k, v in zip(self.train_metric, loss):\n      ret[k] = v\n    return ret\n\n\nclass SGAN(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_bce_gan(*self.d_outputs)\n      self._build_loss(g_loss, d_loss)\n\n\nclass SGANGP(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_bce_gan(*self.d_outputs)\n      with tf.variable_scope(self.name, reuse=True):\n        gp = gradient_penalty(*self.g_outputs, graph_fn=self.D, lamb=10)\n      d_loss += gp\n      self._build_loss(g_loss, d_loss)\n\n\nclass LSGAN(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_lsgan(*self.d_outputs)\n      self._build_loss(g_loss, d_loss)\n\n\nclass WGAN(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_wgan(*self.d_outputs)\n      self._build_loss(g_loss, d_loss)\n      # weights clip\n      var_d = tf.trainable_variables(self.name + \'/D\')\n      clip_bounds = [-.01, .01]\n      clip_ops = [tf.assign(var, tf.clip_by_value(var, *clip_bounds)) for\n                  var in var_d]\n      clip_disc_weights = tf.group(*clip_ops)\n      self.loss.append(clip_disc_weights)\n\n  def build_saver(self):\n    var_g = tf.global_variables(self.name + \'/G\')\n    var_d = tf.global_variables(self.name + \'/D\')\n    steps = [self.global_steps]\n    self.savers[\'gen\'] = tf.train.Saver(var_g + steps, max_to_keep=1)\n    self.savers[\'disc\'] = tf.train.Saver(var_d, max_to_keep=1)\n\n\nclass WGANGP(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_wgan(*self.d_outputs)\n      with tf.variable_scope(self.name, reuse=True):\n        gp = gradient_penalty(*self.g_outputs, graph_fn=self.D, lamb=10)\n      d_loss += gp\n      self._build_loss(g_loss, d_loss)\n\n\nclass RGAN(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_relative_bce_gan(*self.d_outputs, average=False)\n      self._build_loss(g_loss, d_loss)\n\n\nclass RGANGP(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_relative_bce_gan(*self.d_outputs, average=False)\n      with tf.variable_scope(self.name, reuse=True):\n        gp = gradient_penalty(*self.g_outputs, graph_fn=self.D, lamb=10)\n      d_loss += gp\n      self._build_loss(g_loss, d_loss)\n\n\nclass RaGAN(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_relative_bce_gan(*self.d_outputs, average=True)\n      self._build_loss(g_loss, d_loss)\n\n\nclass RaGANGP(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_relative_bce_gan(*self.d_outputs, average=True)\n      with tf.variable_scope(self.name, reuse=True):\n        gp = gradient_penalty(*self.g_outputs, graph_fn=self.D, lamb=10)\n      d_loss += gp\n      self._build_loss(g_loss, d_loss)\n\n\nclass RLSGAN(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_relative_lsgan(*self.d_outputs, average=False)\n      self._build_loss(g_loss, d_loss)\n\n\nclass RaLSGAN(GAN):\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      g_loss, d_loss = loss_relative_lsgan(*self.d_outputs, average=True)\n      self._build_loss(g_loss, d_loss)\n\n\nclass GanTrainer(VSR):\n  def query_config(self, config, **kwargs):\n    # add [batch, patch_size] to collector `self.v`\n    self.v.batch = config.batch\n    self.v.patch_size = config.patch_size\n    return super(GanTrainer, self).query_config(config, **kwargs)\n\n  def fit_init(self):\n    # disable data augmentation of GAN training\n    self.v.train_loader.aug = False\n    return super(GanTrainer, self).fit_init()\n\n  def fn_train_each_step(self, label=None, feature=None, name=None,\n                         post=None):\n    """"""override this method for:\n      - sample feature from random noise (uniform distributed from [-1,1]).\n      - pass step number to `train_batch` call.\n    """"""\n    v = self.v\n    feature = np.random.uniform(-1, 1, [v.batch, self.model.z_dim])\n    for fn in v.label_callbacks:\n      label = fn(label, name=name)\n    loss = self.model.train_batch(feature, label, learning_rate=v.lr,\n                                  epochs=v.epoch, steps=v.global_step)\n    v.global_step = self.model.global_steps.eval()\n    # uncomment this if you want to record everything into tensorboard.\n    # v.summary_writer.add_summary(self.model.summary(), v.global_step)\n    for _k, _v in loss.items():\n      v.avg_meas[_k] = \\\n        v.avg_meas[_k] + [_v] if v.avg_meas.get(_k) else [_v]\n      loss[_k] = \'{:08.5f}\'.format(_v)\n    v.loss = loss\n\n  def fn_benchmark_body(self):\n    """"""override this method for:\n      - sample feature from random noise (uniform distributed from [-1,1]).\n      - save an image grid during validation ([8] x [batch / 8])\n    """"""\n    v = self.v\n    it = v.loader.make_one_shot_iterator(v.memory_limit, shuffle=True)\n    for label, _, name, _ in tqdm.tqdm(it, \'Test\', ascii=True):\n      feature = np.random.uniform(-1, 1, [v.batch, self.model.z_dim])\n      self.fn_benchmark_each_step(label, feature, name)\n    if v.loader.method == \'val\':\n      v.output_callbacks.pop(-1)\n\n  def infer(self, loader, config, **kwargs):\n    """"""there\'s nothing to infer from GAN""""""\n    pass\n'"
VSR/Backend/TF/Models/Idn.py,22,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 24th 2018\nUpdated Date: June 5th 2018\n\nArchitecture of Information Distillation Network (CVPR 2018)\nSee https://arxiv.org/abs/1803.09454\n""""""\n\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass InformationDistillationNetwork(SuperResolution):\n  """"""Fast and Accurate Single Image Super-Resolution via Information Distillation Network\n\n  Args:\n      blocks: number of distillation blocks\n      filters: number of filters in distillation blocks\n      delta: according to paper, D = D3 - D1 = D1 - D2 = D6 - D4 = D4 - D5,\n        where D3=D4, D_{i} is the filters of i-th conv2d\n      slice_factor: the number of channels sliced out from the 3rd conv2d\n      leaky_slope: slope used in leaky relu activators\n      fine_tune_epoch: epoch number beyond which use L1 loss to replace L2 loss\n  """"""\n\n  def __init__(self, blocks=4, filters=64, delta=16, slice_factor=4,\n               leaky_slope=0.05, fine_tune_epoch=200, name=\'idn\', **kwargs):\n    super(InformationDistillationNetwork, self).__init__(**kwargs)\n    self.blocks = blocks\n    self.F = filters\n    self.d = delta\n    self.s = slice_factor\n    self.leaky_slope = leaky_slope\n    self.fine_tune = fine_tune_epoch\n    self.name = name\n\n  def _idn(self, inputs, D3=64, d=16, s=4, **kwargs):\n    """""" the information distillation block contains:\n            - enhancement unit\n            - compression unit\n\n        Args:\n            inputs: input feature maps\n            D3: filters of the 3rd conv2d\n            d: according to paper, D = D3 - D1 = D1 - D2 = D6 - D4 = D4 - D5,\n               where D3=D4, D_{i} is the filters of i-th conv2d\n            s: s is the number of channels sliced out from the 3rd conv2d\n    """"""\n    D1 = D3 - d\n    D2 = D1 - d\n    D4 = D3\n    D5 = D4 - d\n    D6 = D4 + d\n    D = [D1, D2, D3, D4, D5, D6]\n    with tf.variable_scope(kwargs.get(\'name\'), \'Enhancement\'):\n      x = inputs\n      for _d in D[:3]:\n        x = self.conv2d(x, _d, 3)\n        x = tf.nn.leaky_relu(x, self.leaky_slope)\n      R, P2 = x[..., :D3 // s], x[..., D3 // s:]\n      x = P2\n      for _d in D[3:]:\n        x = self.conv2d(x, _d, 3)\n        x = tf.nn.leaky_relu(x, self.leaky_slope)\n      x += tf.concat([inputs, R], axis=-1)\n    with tf.variable_scope(kwargs.get(\'name\'), \'Compression\'):\n      outputs = self.conv2d(x, D3, 1)\n    return outputs\n\n  def _mse_weight_decay_fn(self, step):\n    if step < self.fine_tune:\n      return 1.0\n    else:\n      return 0\n\n  def build_graph(self):\n    super(InformationDistillationNetwork, self).build_graph()\n    with tf.variable_scope(self.name):\n      x = self.inputs_preproc[-1] / 255\n      with tf.variable_scope(\'Features\'):\n        x = self.conv2d(x, self.F, 3)\n        x = tf.nn.leaky_relu(x, self.leaky_slope)\n        x = self.conv2d(x, self.F, 3)\n        x = tf.nn.leaky_relu(x, self.leaky_slope)\n      with tf.variable_scope(\'Distillation\'):\n        for _ in range(self.blocks):\n          x = self._idn(x, self.F, self.d, self.s)\n      with tf.variable_scope(\'Reconstruction\'):\n        x = self.deconv2d(x, self.channel, 17, strides=self.scale)\n      self.outputs.append(x * 255)\n\n  def build_loss(self):\n    """"""The paper first use MSE to train network, then use MAE to fine-tune it\n\n    """"""\n    w = tf.placeholder(tf.float32, name=\'mse_weight\')\n    with tf.name_scope(\'loss\'):\n      y_true = self.label[-1]\n      y_pred = self.outputs[-1]\n      mse = tf.losses.mean_squared_error(y_true, y_pred, weights=w)\n      mae = tf.losses.absolute_difference(y_true, y_pred, weights=(1 - w))\n      loss = tf.add_n([mse, mae] + tf.losses.get_regularization_losses())\n      optimizer = tf.train.AdamOptimizer(self.learning_rate)\n      self.loss.append(optimizer.minimize(loss, self.global_steps))\n      self.train_metric[\'loss\'] = loss\n      self.train_metric[\'mse\'] = mse\n      self.train_metric[\'mae\'] = mae\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(y_true, y_pred, max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(y_true, y_pred, max_val=255))\n\n  def train_batch(self, feature, label, learning_rate=1e-4, **kwargs):\n    epoch = kwargs.get(\'epochs\')\n    self.feed_dict.update({\'mse_weight:0\': self._mse_weight_decay_fn(epoch)})\n    return super(InformationDistillationNetwork, self).train_batch(\n      feature, label, learning_rate, **kwargs)\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)\n'"
VSR/Backend/TF/Models/LapSrn.py,14,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 12th 2018\nUpdated Date: May 25th 2018\n\nDeep Laplacian Pyramid Networks\nRef http://vllab.ucmerced.edu/wlai24/LapSRN\n""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import bicubic_rescale\n\n\nclass LapSRN(SuperResolution):\n  """"""Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution\n\n  Args:\n      layers: number of layers in each pyramid level\n      epsilon: used in charbonnier loss function\n  """"""\n\n  def __init__(self, layers, epsilon=1e-3, name=\'lapsrn\', **kwargs):\n    super(LapSRN, self).__init__(**kwargs)\n    self.epsilon2 = epsilon ** 2\n    self.name = name\n    s0, s1 = self.scale\n    if np.any(np.log2([s0, s1]) != np.round(np.log2([s0, s1]))):\n      raise ValueError(\'Scale factor must be pow of 2.\'\n                       \'Error: scale={},{}\'.format(s0, s1))\n    assert s0 == s1\n    self.level = int(np.log2(s0))\n    self.layers = to_list(layers, self.level)\n\n  def build_graph(self):\n    super(LapSRN, self).build_graph()\n    with tf.variable_scope(self.name):\n      x = self.inputs_preproc[-1]\n      residual = []\n      with tf.variable_scope(\'FeatureExtraction\'):\n        for lv in range(self.level):\n          for _ in range(self.layers[lv] - 1):\n            x = self.leaky_conv2d(x, 64, 3)\n          x = self.deconv2d(x, 64, 4, 2, activation=\'lrelu\')\n          x = self.conv2d(x, self.channel, 3)\n          residual.append(x)\n      with tf.name_scope(\'Reconstruction\'):\n        y = self.inputs_preproc[-1]\n        _s = 2\n        for res in residual:\n          sr = bicubic_rescale(y, _s) + res\n          _s *= 2\n          self.outputs.append(sr)\n      self.outputs.reverse()\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      y_true = [self.label[-1]]\n      for _ in range(1, self.level):\n        y_true.append(bicubic_rescale(y_true[-1], 0.5))\n      charbonnier = []\n      mse = []\n      for x, y in zip(self.outputs, y_true):\n        charbonnier.append(\n          tf.reduce_mean(tf.sqrt(tf.square(x - y) + self.epsilon2)))\n        mse.append(tf.reduce_mean(tf.squared_difference(y, x)))\n      charbonnier_loss = tf.reduce_mean(charbonnier)\n      loss = tf.add_n(\n        [charbonnier_loss] + tf.losses.get_regularization_losses())\n      opt = tf.train.AdamOptimizer(self.learning_rate)\n      self.loss.append(opt.minimize(loss, self.global_steps))\n\n      self.train_metric[\'loss\'] = loss\n      self.train_metric[\'charbonnier_loss\'] = charbonnier_loss\n      for i in range(len(mse)):\n        self.metrics[\'mse_x{}\'.format(2 ** (i + 1))] = mse[i]\n        self.metrics[\'psnr_x{}\'.format(2 ** (i + 1))] = 10 * tf.log(\n          255 ** 2 / mse[i]) / tf.log(10.0)\n\n  def build_summary(self):\n    super(LapSRN, self).build_summary()\n    tf.summary.image(\'SR\', self.outputs[-1], 1)\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)'"
VSR/Backend/TF/Models/MemNet.py,24,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Sep 11th 2018\n\nMemNet (ICCV 2017)\nSee https://arxiv.org/abs/1708.02209\n""""""\n\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import bicubic_rescale\n\n\nclass MEMNET(SuperResolution):\n  """"""MemNet: A Persistent Memory Network for Image Restoration\n\n  Args:\n      n_memblock: number of memory blocks\n      n_recur: number of recursive unit in each memory block\n  """"""\n\n  def __init__(self, name=\'memnet\', n_memblock=6, n_recur=6, filters=64,\n               **kwargs):\n    super(MEMNET, self).__init__(**kwargs)\n    self.name = name\n    self.recur = n_recur\n    self.n_memblock = n_memblock\n    self.F = filters\n\n  def _recursive_unit(self, inputs, **kwargs):\n    with tf.variable_scope(kwargs.get(\'name\'), \'RecursiveUnit\'):\n      R = [inputs]\n      for _ in range(self.recur):\n        R.append(self.resblock(R[-1], self.F, 3, activation=\'relu\'))\n      b_short = tf.concat(R[1:], axis=-1, name=\'short_memory\')\n      return b_short\n\n  def _gate_unit(self, short, long, **kwargs):\n    with tf.variable_scope(kwargs.get(\'name\'), \'GateUnit\'):\n      long = to_list(long)\n      b_gate = tf.concat(long + [short], axis=-1)\n      return self.relu_conv2d(b_gate, self.F, 1)\n\n  def _memory_block(self, short, long, **kwargs):\n    with tf.variable_scope(kwargs.get(\'name\'), \'MemoryBlock\'):\n      long = to_list(long)\n      short = self._recursive_unit(short)\n      return self._gate_unit(short, long)\n\n  def _reconstruct(self, x, inputs, **kwargs):\n    with tf.variable_scope(\'Reconstruct\', reuse=tf.AUTO_REUSE):\n      inputs = self.relu_conv2d(inputs, self.F, 3)\n      sr = self.conv2d(inputs, self.channel, 3)\n      return sr + x\n\n  def build_graph(self):\n    super(MEMNET, self).build_graph()\n    with tf.variable_scope(self.name):\n      input_norm = self.inputs_preproc[-1] / 255\n      input_norm = bicubic_rescale(input_norm, self.scale)\n      sf = self.conv2d(input_norm, self.F, 3)\n      feat = [sf]\n      for i in range(self.n_memblock):\n        feat.append(self._memory_block(feat[-1], feat))\n      for fm in feat[1:]:\n        srm = self._reconstruct(input_norm, fm)\n        self.outputs.append(srm * 255)\n      weights = tf.Variable([1.0 / self.n_memblock] * self.n_memblock,\n                            dtype=tf.float32)\n      final = tf.add_n(\n        [self.outputs[i] * weights[i] for i in range(self.n_memblock)])\n      self.outputs.append(final)\n\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      alpha = 1 / (1 + self.n_memblock)\n      mse_losses = [\n        tf.losses.mean_squared_error(self.label[-1], self.outputs[-1],\n                                     weights=alpha / 2)]\n      mse_losses += [\n        tf.losses.mean_squared_error(self.label[0], o, weights=(1 - alpha) / 2)\n        for o in\n        self.outputs[:-1]]\n      re_loss = tf.losses.get_regularization_losses()\n      loss = tf.add_n(re_loss + mse_losses, name=\'Loss\')\n\n      update_op = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_op):\n        opt = tf.train.AdamOptimizer(self.learning_rate).minimize(loss,\n                                                                  self.global_steps)\n        self.loss.append(opt)\n\n    # tensorboard\n    self.train_metric[\'loss\'] = loss\n    self.metrics[\'mse\'] = mse_losses[0] * 2 / alpha\n    self.metrics[\'psnr\'] = tf.reduce_mean(\n      tf.image.psnr(self.label[-1], self.outputs[-1], 255))\n    self.metrics[\'ssim\'] = tf.reduce_mean(\n      tf.image.ssim(self.label[-1], self.outputs[-1], 255))\n\n  def build_summary(self):\n    super(MEMNET, self).build_summary()\n    tf.summary.image(\'SR\', self.outputs[-1], 1)\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)\n'"
VSR/Backend/TF/Models/Msrn.py,10,"b'""""""\nCopyright: Intel Corp. 2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 14th 2018\n\nMulti-scale Residual Network for Image Super-Resolution\nSee http://openaccess.thecvf.com/content_ECCV_2018/papers/Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper.pdf\n""""""\n\nimport tensorflow as tf\n\nfrom ..Arch.Residual import msrb\nfrom ..Framework.SuperResolution import SuperResolution\n\n\ndef _normalize(inputs):\n  rgb_mean = (0.4488, 0.4371, 0.4040)\n  return inputs / 255 - rgb_mean\n\n\ndef _denormalize(inputs):\n  rgb_mean = (0.4488, 0.4371, 0.4040)\n  return (inputs + rgb_mean) * 255\n\n\nclass MSRN(SuperResolution):\n  """"""Multi-scale Residual Network for Image Super-Resolution\n\n  Args:\n      n_blocks: number of MSRB blocks.\n  """"""\n\n  def __init__(self, n_blocks=8, name=\'msrn\', **kwargs):\n    super(MSRN, self).__init__(**kwargs)\n    self.name = name\n    self.blocks = n_blocks\n\n  def build_graph(self):\n    super(MSRN, self).build_graph()\n    inputs_norm = _normalize(self.inputs_preproc[-1])\n    with tf.variable_scope(self.name):\n      features = [self.conv2d(inputs_norm, 64, 3)]\n      for _ in range(self.blocks):\n        x = features[-1]\n        features.append(msrb(self, x))\n      x = self.conv2d(tf.concat(features, -1), 64, 1)\n      x = self.upscale(x, direct_output=False)\n      sr = self.conv2d(x, self.channel, 3)\n    self.outputs.append(_denormalize(sr))\n\n  def build_loss(self):\n    label_norm = _normalize(self.label[-1])\n    sr = _normalize(self.outputs[-1])\n    with tf.name_scope(\'Loss\'):\n      l1 = tf.losses.absolute_difference(label_norm, sr)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        op = tf.train.AdamOptimizer(self.learning_rate)\n        op = op.minimize(l1, self.global_steps)\n        self.loss.append(op)\n\n    self.train_metric[\'l1\'] = l1\n    self.metrics[\'psnr\'] = tf.reduce_mean(tf.image.psnr(\n      self.label[-1], self.outputs[-1], max_val=255))\n    self.metrics[\'ssim\'] = tf.reduce_mean(tf.image.ssim(\n      self.label[-1], self.outputs[-1], max_val=255))\n\n  def build_summary(self):\n    super(MSRN, self).build_summary()\n    tf.summary.image(\'sr\', self.outputs[0])\n'"
VSR/Backend/TF/Models/Nlrn.py,15,"b'""""""\nCopyright: Intel Corp. 2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 19th 2018\n\nNon-Local Recurrent Network for Image Restoration\nSee https://arxiv.org/abs/1806.02919\n""""""\n\nimport logging\n\nimport tensorflow as tf\n\nfrom ..Arch.Residual import non_local\nfrom ..Framework.SuperResolution import SuperResolution\n\nLOG = logging.getLogger(\'VSR.Model.NLRN\')\n\n\ndef _denormalize(inputs):\n  return (inputs + 0) * 255\n\n\ndef _normalize(inputs):\n  return inputs / 255\n\n\nclass NLRN(SuperResolution):\n  """"""Non-Local Recurrent Network for Image Restoration (NIPS 2018)\n\n  """"""\n\n  def __init__(self, recurrents=12, clip=2.5, name=\'nlrn\', **kwargs):\n    super(NLRN, self).__init__(**kwargs)\n    self.name = name\n    self.recurrents = recurrents\n    self.clip = clip\n    self.filters = kwargs.get(\'filters\', 128)\n\n  def display(self):\n    LOG.info(f""Recurrents: {self.recurrents}"")\n\n  def rnn(self, x, y):\n    with tf.variable_scope(\'RNN\', reuse=tf.AUTO_REUSE):\n      x = self.batch_norm(x, self.training_phase)\n      x = tf.nn.relu(x)\n      x = non_local(self, x, self.filters, scaling=2)\n\n      x = self.batch_norm(x, self.training_phase)\n      x = self.bn_relu_conv2d(x, self.filters, 3)\n      x = self.conv2d(x, self.filters, 3)\n      return x + y\n\n  def build_graph(self):\n    super(NLRN, self).build_graph()\n    with tf.variable_scope(self.name):\n      inputs_norm = _normalize(self.inputs_preproc[-1])\n      init_feat = self.batch_norm(inputs_norm, self.training_phase)\n      x = init_feat = self.conv2d(init_feat, self.filters, 3)\n      for _ in range(self.recurrents):\n        x = self.rnn(x, init_feat)\n      sr = self.batch_norm(x, self.training_phase)\n      sr = tf.nn.relu(sr)\n      sr = self.conv2d(sr, self.channel, 3)\n      sr += inputs_norm\n\n      self.outputs.append(_denormalize(sr))\n\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      mse = tf.losses.mean_squared_error(self.outputs[-1], self.label[-1])\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        opt = tf.train.AdadeltaOptimizer(self.learning_rate)\n        grad = opt.compute_gradients(mse)\n        grad_clip = tf.contrib.training.clip_gradient_norms(\n          grad, self.clip)\n        op = opt.apply_gradients(grad_clip, self.global_steps)\n        self.loss.append(op)\n\n      self.train_metric[\'mse\'] = mse\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(self.label[0], self.outputs[0], 255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(self.label[0], self.outputs[0], 255))\n\n  def build_summary(self):\n    super(NLRN, self).build_summary()\n    tf.summary.image(\'SR\', self.outputs[-1], 1)\n'"
VSR/Backend/TF/Models/Rcan.py,20,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Sep 11th 2018\n\nResidual Channel Attention Networks (ECCV 2018)\nSee https://arxiv.org/abs/1807.02758\n""""""\n\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass RCAN(SuperResolution):\n  """"""Image Super-Resolution Using Very Deep Residual Channel Attention Networks\n\n  Args:\n      channel_downscaling: channel downscaling ratio as `r` in the paper\n      n_rcab: number of RCABs in each RG(Residual Group)\n      n_rg: number of RGs in RIR(Residual in Residual)\n      filters: number of filters in primary conv2d(s)\n  """"""\n\n  def __init__(self, name=\'rcan\', channel_downscaling=16, n_rcab=20, n_rg=10,\n               filters=64, **kwargs):\n    super(RCAN, self).__init__(**kwargs)\n    self.name = name\n    self.R = channel_downscaling\n    self.n_rcab = n_rcab\n    self.n_rg = n_rg\n    self.F = filters\n\n  def _rir(self, inputs, **kwargs):\n    """"""Residual in residual block""""""\n    with tf.variable_scope(kwargs.get(\'name\'), \'RIR\'):\n      x = inputs\n      for _ in range(self.n_rg):\n        x = self._rg(x)\n      x = self.conv2d(x, self.F, 3)\n      # LCC\n      return inputs + x\n\n  def _rg(self, inputs, **kwargs):\n    """"""Residual group""""""\n    with tf.variable_scope(kwargs.get(\'name\'), \'RG\'):\n      x = inputs\n      for _ in range(self.n_rcab):\n        x = self._rcab(x)\n      x = self.conv2d(x, self.F, 3)\n      # SCC\n      return inputs + x\n\n  def _rcab(self, inputs, **kwargs):\n    """"""Residual channel attention block""""""\n    with tf.variable_scope(kwargs.get(\'name\'), \'RCAB\'):\n      x = self.relu_conv2d(inputs, self.F, 3)\n      y = self.conv2d(x, self.F, 3)\n      x = tf.reduce_mean(y, axis=[1, 2], keepdims=True)\n      x = self.relu_conv2d(x, self.F // self.R, 1)\n      x = self.conv2d(x, self.F, 1, activation=tf.nn.sigmoid)\n      y *= x\n      return inputs + y\n\n  def build_graph(self):\n    super(RCAN, self).build_graph()\n    with tf.variable_scope(self.name):\n      x = self.inputs_preproc[-1] / 255\n      sf = self.conv2d(x, self.F, 3)\n      df = self._rir(sf)\n      sr = self.upscale(df, direct_output=False)\n      sr = self.conv2d(sr, self.channel, 3)\n      self.outputs.append(sr * 255)\n\n  def build_loss(self):\n    with tf.name_scope(\'Loss\'):\n      l1_loss = tf.losses.absolute_difference(self.label[0], self.outputs[0])\n      re_loss = tf.losses.get_regularization_losses()\n      mse = tf.losses.mean_squared_error(self.label[0], self.outputs[0])\n      loss = tf.add_n(re_loss + [l1_loss], name=\'Loss\')\n\n      update_op = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_op):\n        opt = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          loss, self.global_steps)\n        self.loss.append(opt)\n\n    # tensorboard\n    self.train_metric[\'loss\'] = loss\n    self.train_metric[\'l1\'] = l1_loss\n    self.metrics[\'mse\'] = mse\n    self.metrics[\'psnr\'] = tf.reduce_mean(\n      tf.image.psnr(self.label[0], self.outputs[0], 255))\n    self.metrics[\'ssim\'] = tf.reduce_mean(\n      tf.image.ssim(self.label[0], self.outputs[0], 255))\n\n  def build_summary(self):\n    super(RCAN, self).build_summary()\n    tf.summary.image(\'SR\', self.outputs[0], 1)\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)\n'"
VSR/Backend/TF/Models/Rdn.py,18,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 24th 2018\nUpdated Date: May 25th 2018\n\nArchitecture of Residual Dense Network (CVPR 2018)\nSee https://arxiv.org/abs/1802.08797\n""""""\n\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\n\n\nclass ResidualDenseNetwork(SuperResolution):\n  """"""Residual Dense Network for Image Super-Resolution\n\n  Args:\n      global_filters: filters used in shallow feature extraction and global fusion\n      rdb_blocks: number of residual dense blocks\n      rdb_conv: number of conv2d layers in each RDB\n      rdb_filters: number of filters in RDB conv2d\n\n  NOTE: total conv2d layers := `rdb_blocks` * `rdb_conv` + 5 + Upscale\n  """"""\n\n  def __init__(self, global_filters=64, rdb_blocks=10, rdb_conv=6,\n               rdb_filters=64, name=\'rdn\', **kwargs):\n    super(ResidualDenseNetwork, self).__init__(**kwargs)\n    self.name = name\n    self.gfilter = global_filters\n    self.block = rdb_blocks\n    self.conv = rdb_conv\n    self.growth_rate = rdb_filters\n\n  def _rdb(self, inputs, **kwargs):\n    """"""Make Residual Dense Block\n\n    Args:\n        inputs: input features\n    """"""\n    with tf.variable_scope(kwargs.get(\'name\'), \'ResDenseBlock\'):\n      filters, conv = self.growth_rate, self.conv\n      x = [inputs]\n      x += [self.relu_conv2d(x[-1], filters, 3)]\n      for i in range(1, conv):\n        x += [self.relu_conv2d(tf.concat(x, axis=-1), filters, 3)]\n      # 1x1 conv\n      local_fusion = self.conv2d(tf.concat(x, axis=-1), filters, 1)\n      # local residual learning\n      outputs = inputs + local_fusion\n      return outputs\n\n  def build_graph(self):\n    super(ResidualDenseNetwork, self).build_graph()\n    with tf.variable_scope(self.name):\n      x = self.inputs_preproc[-1]\n      # shallow feature extraction\n      # NOTE: no activation\n      with tf.variable_scope(\'ShallowFeature\'):\n        sf0 = self.conv2d(x, self.gfilter, 3)\n        sf1 = self.conv2d(sf0, self.gfilter, 3)\n      with tf.variable_scope(\'ResBlocks\'):\n        F = [sf1]\n        for i in range(self.block):\n          F += [self._rdb(F[-1])]\n      with tf.variable_scope(\'GlobalFusion\'):\n        gf0 = self.conv2d(tf.concat(F[1:], axis=-1), self.gfilter, 1)\n        gf1 = self.conv2d(gf0, self.gfilter, 3)\n      dense_feature = sf0 + gf1\n      # use pixel shift in ESPCN to upscale\n      upscaled = self.upscale(dense_feature, direct_output=False)\n      hr = self.conv2d(upscaled, self.channel, 3)\n      self.outputs.append(hr)\n\n  def build_loss(self):\n    """"""In paper, authors use L1 loss instead of MSE error. Claimed a better perf.""""""\n    with tf.name_scope(\'loss\'):\n      y_true = self.label[-1]\n      y_pred = self.outputs[-1]\n      mae = tf.losses.absolute_difference(y_true, y_pred)\n      mse = tf.losses.mean_squared_error(y_true, y_pred)\n      opt = tf.train.AdamOptimizer(self.learning_rate)\n      loss = tf.add_n([mae] + tf.losses.get_regularization_losses())\n      self.loss.append(opt.minimize(loss, self.global_steps))\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mse\'] = mse\n      self.metrics[\'mae\'] = mae\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(y_true, self.outputs[-1], 255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(y_true, self.outputs[-1], 255))\n\n  def build_saver(self):\n    self.savers[self.name] = tf.train.Saver(tf.global_variables(self.name),\n                                            max_to_keep=1)\n'"
VSR/Backend/TF/Models/SRDenseNet.py,5,"b'""""""\nCopyright: Intel Corp. 2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 19th 2018\n\nImage Super-Resolution Using Dense Skip Connections\nSee http://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf\n""""""\n\nimport tensorflow as tf\n\nfrom ..Arch import Dense\nfrom ..Framework.SuperResolution import SuperResolution\n\n\ndef _denormalize(inputs):\n  return (inputs + 0) * 255\n\n\ndef _normalize(inputs):\n  return inputs / 255\n\n\nclass SRDenseNet(SuperResolution):\n  """"""Image Super-Resolution Using Dense Skip Connections.\n  Args:\n      n_blocks: number of dense blocks.\n  """"""\n\n  def __init__(self, name=\'srdensenet\', n_blocks=8, **kwargs):\n    super(SRDenseNet, self).__init__(**kwargs)\n    self.name = name\n    self.n_blocks = n_blocks\n\n  def build_graph(self):\n    super(SRDenseNet, self).build_graph()\n    with tf.variable_scope(self.name):\n      inputs_norm = _normalize(self.inputs_preproc[-1])\n      feat = [self.conv2d(inputs_norm, 64, 3)]\n      for i in range(self.n_blocks):\n        feat.append(Dense.dense_block(self, feat[-1]))\n      bottleneck = self.conv2d(tf.concat(feat, -1), 256, 1)\n      sr = self.upscale(bottleneck, \'deconv\', direct_output=False)\n      sr = self.conv2d(sr, self.channel, 3)\n      self.outputs.append(_denormalize(sr))\n\n  def build_loss(self):\n    mse, loss = super(SRDenseNet, self).build_loss()\n    self.train_metric[\'mse\'] = mse\n    self.train_metric[\'loss\'] = loss\n    self.metrics[\'psnr\'] = tf.reduce_mean(tf.image.psnr(\n      self.label[-1], self.outputs[-1], max_val=255))\n    self.metrics[\'ssim\'] = tf.reduce_mean(tf.image.ssim(\n      self.label[-1], self.outputs[-1], max_val=255))\n\n  def build_summary(self):\n    super(SRDenseNet, self).build_summary()\n    tf.summary.image(\'sr\', self.outputs[-1])\n'"
VSR/Backend/TF/Models/SRFeat.py,27,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Dec 20th 2018\n\nSingle Image Super-Resolution with Feature Discrimination (ECCV 2018)\nSee http://openaccess.thecvf.com/content_ECCV_2018/papers/Seong-Jin_Park_SRFeat_Single_Image_ECCV_2018_paper.pdf\n""""""\nimport tensorflow as tf\n\nfrom ..Arch import Discriminator\nfrom ..Framework.GAN import loss_bce_gan\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import Vgg, prelu\n\n\ndef _normalize(x):\n  return x / 127.5 - 1\n\n\ndef _denormalize(x):\n  return (x + 1) * 127.5\n\n\ndef _clip(image):\n  return tf.cast(tf.clip_by_value(image, 0, 255), \'uint8\')\n\n\nclass SRFEAT(SuperResolution):\n  """"""SRFeat\n\n  Args:\n      glayers: number of layers in generator.\n      dlayers: number of layers in discriminator.\n      vgg_layer: vgg feature layer name for perceptual loss.\n      init_epoch: number of initializing epochs.\n  """"""\n\n  def __init__(self, glayers=16, dlayers=4, vgg_layer=\'block5_conv4\',\n               init_epoch=100, gan_weight=1e-3, vgg_weight=0.1569,\n               name=\'srfeat\', **kwargs):\n    super(SRFEAT, self).__init__(**kwargs)\n    self.name = name\n    self.g_layers = glayers\n    self.init_epoch = init_epoch\n    self.gan_weight = gan_weight\n    self.vgg_weight = vgg_weight\n    self.vgg_layer = vgg_layer\n    self.vgg = Vgg(False, Vgg.VGG19)\n    self.F = 64\n    self.D = Discriminator.dcgan_d(self, [None, None, self.channel], 64,\n                                   times_stride=dlayers, norm=\'bn\',\n                                   name_or_scope=\'Critic\')\n    self.DF = Discriminator.dcgan_d(self, [None, None, self.channel], 64,\n                                    times_stride=dlayers, norm=\'bn\',\n                                    name_or_scope=\'DF\')\n\n  def build_graph(self):\n    super(SRFEAT, self).build_graph()\n    inputs_norm = _normalize(self.inputs_preproc[-1])\n    label_norm = _normalize(self.label[-1])\n    with tf.variable_scope(self.name):\n      shallow_feature = self.prelu_conv2d(inputs_norm, self.F, 9)\n      x = [shallow_feature]\n      for _ in range(self.g_layers):\n        x.append(self.resblock(x[-1], self.F, 3, activation=\'prelu\',\n                               use_batchnorm=True))\n      bottleneck = x[-1]\n      for t in x[1:-1]:\n        bottleneck += self.conv2d(t, self.F, 1)\n      sr = self.upscale(bottleneck, direct_output=False, activator=prelu)\n      sr = self.tanh_conv2d(sr, self.channel, 9)\n      self.outputs.append(_denormalize(sr))\n\n    disc_real = self.D(label_norm)\n    disc_fake = self.D(sr)\n    vgg_features = [self.vgg(self.outputs[0], self.vgg_layer)]\n    vgg_features += [self.vgg(self.label[0], self.vgg_layer)]\n    vgg_fake = self.DF(vgg_features[0])\n    vgg_real = self.DF(vgg_features[1])\n\n    with tf.name_scope(\'Loss\'):\n      loss_gen, loss_disc = loss_bce_gan(disc_real, disc_fake)\n      vgg_loss_g, vgg_loss_d = loss_bce_gan(vgg_real, vgg_fake)\n      mse = tf.losses.mean_squared_error(label_norm, sr)\n      loss_d = loss_disc + vgg_loss_d\n      loss_g = loss_gen + vgg_loss_g\n      loss_vgg = tf.losses.mean_squared_error(*vgg_features)\n      loss = tf.stack([loss_g, loss_vgg])\n      loss = tf.reduce_sum(loss * [self.gan_weight, self.vgg_weight])\n\n      var_g = tf.trainable_variables(self.name)\n      var_d = tf.trainable_variables(\'Critic\')\n      var_df = tf.trainable_variables(\'DF\')\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        opt_i = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          mse, self.global_steps, var_list=var_g)\n        opt_g = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          loss, self.global_steps, var_list=var_g)\n        opt_d = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          loss_d, var_list=var_d + var_df)\n        self.loss = [opt_i, opt_d, opt_g]\n\n    self.train_metric[\'g_loss\'] = loss_g\n    self.train_metric[\'d_loss\'] = loss_d\n    self.train_metric[\'vgg_loss\'] = loss_vgg\n    self.train_metric[\'loss\'] = loss\n    self.metrics[\'psnr\'] = tf.reduce_mean(\n      tf.image.psnr(self.label[-1], self.outputs[-1], 255))\n    self.metrics[\'ssim\'] = tf.reduce_mean(\n      tf.image.ssim(self.label[-1], self.outputs[-1], 255))\n\n  def build_loss(self):\n    pass\n\n  def build_summary(self):\n    super(SRFEAT, self).build_summary()\n    tf.summary.image(\'SR\', _clip(self.outputs[-1]))\n\n  def build_saver(self):\n    var_d = tf.global_variables(\'Critic\')\n    var_df = tf.global_variables(\'DF\')\n    var_g = tf.global_variables(self.name)\n    loss = tf.global_variables(\'Loss\')\n    steps = [self.global_steps]\n    self.savers.update({\n      \'Critic\': tf.train.Saver(var_d + var_df, max_to_keep=1),\n      \'Gen\': tf.train.Saver(var_g, max_to_keep=1),\n      \'Misc\': tf.train.Saver(loss + steps, max_to_keep=1),\n    })\n\n  def train_batch(self, feature, label, learning_rate=1e-4, **kwargs):\n    epoch = kwargs.get(\'epochs\')\n    if epoch <= self.init_epoch:\n      loss = self.loss[0]\n    else:\n      loss = self.loss[1:]\n    return super(SRFEAT, self).train_batch(feature, label, learning_rate,\n                                           loss=loss)\n'"
VSR/Backend/TF/Models/SrGan.py,25,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 17th 2018\nUpdated Date: May 25th 2018\n\nSRGAN implementation (CVPR 2017)\nSee https://arxiv.org/abs/1609.04802\n""""""\nimport tensorflow as tf\n\nfrom ..Arch import Discriminator\nfrom ..Framework.GAN import loss_bce_gan\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import Vgg, prelu\n\n\ndef _normalize(x):\n  return x / 127.5 - 1\n\n\ndef _denormalize(x):\n  return (x + 1) * 127.5\n\n\ndef _clip(image):\n  return tf.cast(tf.clip_by_value(image, 0, 255), \'uint8\')\n\n\nclass SRGAN(SuperResolution):\n  """"""Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n\n  Args:\n      glayers: number of layers in generator.\n      dlayers: number of layers in discriminator.\n      vgg_layer: vgg feature layer name for perceptual loss.\n      init_epoch: number of initializing epochs.\n      mse_weight:\n      gan_weight:\n      vgg_weight:\n  """"""\n\n  def __init__(self, glayers=16, dlayers=4, vgg_layer=\'block2_conv2\',\n               init_epoch=100, mse_weight=1, gan_weight=1e-3,\n               use_vgg=False, vgg_weight=2e-6, name=\'srgan\', **kwargs):\n    super(SRGAN, self).__init__(**kwargs)\n    self.name = name\n    self.g_layers = glayers\n    self.init_epoch = init_epoch\n    self.mse_weight = mse_weight\n    self.gan_weight = gan_weight\n    self.vgg_weight = vgg_weight\n    self.vgg_layer = vgg_layer\n    self.use_vgg = use_vgg\n    self.vgg = None\n    if self.use_vgg:\n      self.vgg = Vgg(False, \'vgg19\')\n    self.D = Discriminator.dcgan_d(self, [None, None, self.channel], 64,\n                                   times_stride=dlayers, norm=\'bn\',\n                                   name_or_scope=\'Critic\')\n\n  def build_graph(self):\n    super(SRGAN, self).build_graph()\n    inputs_norm = _normalize(self.inputs_preproc[-1])\n    label_norm = _normalize(self.label[-1])\n    with tf.variable_scope(self.name):\n      shallow_feature = self.prelu_conv2d(inputs_norm, 64, 9)\n      x = shallow_feature\n      for _ in range(self.g_layers):\n        x = self.resblock(x, 64, 3, activation=\'prelu\',\n                          use_batchnorm=True)\n      x = self.bn_conv2d(x, 64, 3)\n      x += shallow_feature\n      x = self.conv2d(x, 256, 3)\n      sr = self.upscale(x, direct_output=False, activator=prelu)\n      sr = self.tanh_conv2d(sr, self.channel, 9)\n      self.outputs.append(_denormalize(sr))\n\n    disc_real = self.D(label_norm)\n    disc_fake = self.D(sr)\n\n    with tf.name_scope(\'Loss\'):\n      loss_gen, loss_disc = loss_bce_gan(disc_real, disc_fake)\n      mse = tf.losses.mean_squared_error(label_norm, sr)\n      reg = tf.losses.get_regularization_losses()\n\n      loss = tf.add_n(\n        [mse * self.mse_weight, loss_gen * self.gan_weight] + reg)\n      if self.use_vgg:\n        vgg_real = self.vgg(self.label[-1], self.vgg_layer)\n        vgg_fake = self.vgg(self.outputs[-1], self.vgg_layer)\n        loss_vgg = tf.losses.mean_squared_error(\n          vgg_real, vgg_fake, self.vgg_weight)\n        loss += loss_vgg\n\n      var_g = tf.trainable_variables(self.name)\n      var_d = tf.trainable_variables(\'Critic\')\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        opt_i = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          mse, self.global_steps, var_list=var_g)\n        opt_g = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          loss, self.global_steps, var_list=var_g)\n        opt_d = tf.train.AdamOptimizer(self.learning_rate).minimize(\n          loss_disc, var_list=var_d)\n        self.loss = [opt_i, opt_d, opt_g]\n\n    self.train_metric[\'g_loss\'] = loss_gen\n    self.train_metric[\'d_loss\'] = loss_disc\n    self.train_metric[\'loss\'] = loss\n    self.metrics[\'psnr\'] = tf.reduce_mean(\n      tf.image.psnr(self.label[-1], self.outputs[-1], 255))\n    self.metrics[\'ssim\'] = tf.reduce_mean(\n      tf.image.ssim(self.label[-1], self.outputs[-1], 255))\n\n  def build_loss(self):\n    pass\n\n  def build_summary(self):\n    super(SRGAN, self).build_summary()\n    tf.summary.image(\'SR\', _clip(self.outputs[-1]))\n\n  def build_saver(self):\n    var_d = tf.global_variables(\'Critic\')\n    var_g = tf.global_variables(self.name)\n    loss = tf.global_variables(\'Loss\')\n    steps = [self.global_steps]\n    self.savers.update({\n      \'Critic\': tf.train.Saver(var_d, max_to_keep=1),\n      \'Gen\': tf.train.Saver(var_g, max_to_keep=1),\n      \'Misc\': tf.train.Saver(loss + steps, max_to_keep=1),\n    })\n\n  def train_batch(self, feature, label, learning_rate=1e-4, **kwargs):\n    epoch = kwargs.get(\'epochs\')\n    if epoch <= self.init_epoch:\n      loss = self.loss[0]\n    else:\n      loss = self.loss[1:]\n    return super(SRGAN, self).train_batch(feature, label, learning_rate,\n                                          loss=loss)\n'"
VSR/Backend/TF/Models/Srcnn.py,12,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: May 8th 2018\nUpdated Date: May 25th 2018\n\nSRCNN mainly for framework tests (ECCV 2014)\nRef https://arxiv.org/abs/1501.00092\n""""""\nimport tensorflow as tf\n\nfrom VSR.Util import to_list\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import bicubic_rescale\n\n\nclass SRCNN(SuperResolution):\n  """"""Image Super-Resolution Using Deep Convolutional Networks\n\n  Args:\n      layers: number layers to use\n      filters: number of filters of conv2d(s)\n      kernel: a tuple of integer, representing kernel size of each layer,\n        can also be one integer to specify the same size\n      custom_upsample: use --add_custom_callbacks=upsample during fitting, or\n        use `bicubic_rescale`. TODO: REMOVE IN FUTURE.\n  """"""\n\n  def __init__(self, layers=3, filters=64, kernel=(9, 5, 5),\n               custom_upsample=False,\n               name=\'srcnn\', **kwargs):\n    super(SRCNN, self).__init__(**kwargs)\n    self.name = name\n    self.do_up = not custom_upsample\n    self.layers = layers\n    self.filters = filters\n    self.kernel_size = to_list(kernel)\n    if len(self.kernel_size) < self.layers:\n      self.kernel_size += to_list(kernel[-1],\n                                  self.layers - len(self.kernel_size))\n\n  def build_graph(self):\n    super(SRCNN, self).build_graph()\n    with tf.variable_scope(self.name):\n      x = self.inputs_preproc[-1]\n      if self.do_up:\n        x = bicubic_rescale(x, self.scale)\n      f = self.filters\n      ks = self.kernel_size\n      x = self.relu_conv2d(x, f, ks[0])\n      for i in range(1, self.layers - 1):\n        x = self.relu_conv2d(x, f, ks[i])\n      x = self.conv2d(x, self.channel, ks[-1])\n      self.outputs.append(x)\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      y_pred = self.outputs[-1]\n      y_true = self.label[-1]\n      mse = tf.losses.mean_squared_error(y_true, y_pred)\n      loss = tf.losses.get_total_loss()\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        opt = tf.train.AdamOptimizer(self.learning_rate)\n        self.loss.append(opt.minimize(loss, self.global_steps))\n\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mse\'] = mse\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(self.label[-1], self.outputs[-1], max_val=255))\n\n  def build_summary(self):\n    super(SRCNN, self).build_summary()\n    tf.summary.image(\'SR\', self.outputs[-1], 1)\n'"
VSR/Backend/TF/Models/Vdsr.py,11,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: June 5th 2018\nUpdated Date: June 15th 2018\n\nAccurate Image Super-Resolution Using Very Deep Convolutional Networks\nSee https://arxiv.org/abs/1511.04587\n""""""\n\nimport tensorflow as tf\n\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import bicubic_rescale\n\n\nclass VDSR(SuperResolution):\n  """"""Accurate Image Super-Resolution Using Very Deep Convolutional Networks\n\n  Args:\n      layers: number of conv2d layers\n      filters: number of filters in conv2d(s)\n      custom_upsample: use --add_custom_callbacks=upsample during fitting, or\n        use `bicubic_rescale`. TODO: REMOVE IN FUTURE.\n  """"""\n\n  def __init__(self, layers=20, filters=64, custom_upsample=False,\n               name=\'vdsr\', **kwargs):\n    self.layers = layers\n    self.filters = filters\n    self.do_up = not custom_upsample\n    self.name = name\n    super(VDSR, self).__init__(**kwargs)\n\n  def build_graph(self):\n    super(VDSR, self).build_graph()\n    with tf.variable_scope(self.name):\n      # bicubic upscale\n      x = bic = self.inputs_preproc[-1]\n      if self.do_up:\n        x = bic = bicubic_rescale(self.inputs_preproc[-1], self.scale)\n      for _ in range(self.layers - 1):\n        x = self.relu_conv2d(x, self.filters, 3)\n      x = self.conv2d(x, self.channel, 3)\n      self.outputs.append(x + bic)\n\n  def build_loss(self):\n    with tf.name_scope(\'loss\'):\n      mae = tf.losses.absolute_difference(self.label[-1], self.outputs[-1])\n      loss = tf.losses.get_total_loss()\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        opt = tf.train.AdamOptimizer(self.learning_rate)\n        self.loss.append(opt.minimize(loss, self.global_steps))\n\n      self.train_metric[\'loss\'] = loss\n      self.metrics[\'mae\'] = mae\n      self.metrics[\'psnr\'] = tf.reduce_mean(\n        tf.image.psnr(self.label[-1], self.outputs[-1], max_val=255))\n      self.metrics[\'ssim\'] = tf.reduce_mean(\n        tf.image.ssim(self.label[-1], self.outputs[-1], max_val=255))\n'"
VSR/Backend/TF/Models/Vespcn.py,32,"b'""""""\nCopyright: Wenyi Tang 2017-2018\nAuthor: Wenyi Tang\nEmail: wenyi.tang@intel.com\nCreated Date: Sep 12th 2018\n\nReal-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation (CVPR 2017)\nSee https://arxiv.org/abs/1611.05250\n""""""\n\nimport tensorflow as tf\n\nfrom ..Framework.Motion import viz_flow, warp\nfrom ..Framework.SuperResolution import SuperResolution\nfrom ..Util import pixel_shift, pad_if_divide\n\n\nclass VESPCN(SuperResolution):\n  """"""Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation\n\n  Args:\n      depth: the sequential depth of a clip.\n      beta: the loss weight\n      gamma: the loss weight\n  """"""\n\n  def __init__(self, name=\'vespcn\', depth=3, beta=1, gamma=0.01, **kwargs):\n    super(VESPCN, self).__init__(**kwargs)\n    self.name = name\n    self.depth = depth\n    self.beta = beta\n    self.gamma = gamma\n\n  def _flow_coarse(self, x0, x1, **kwargs):\n    with tf.variable_scope(\'Flow/Coarse\', reuse=tf.AUTO_REUSE):\n      x = tf.concat([x0, x1], axis=-1)\n      x = self.relu_conv2d(x, 24, 5, strides=2)\n      x = self.relu_conv2d(x, 24, 3, strides=1)\n      x = self.relu_conv2d(x, 24, 5, strides=2)\n      x = self.relu_conv2d(x, 24, 3, strides=1)\n      x = self.tanh_conv2d(x, 32, 3, strides=1)\n      flow = pixel_shift(x, 4, 2)\n      return flow\n\n  def _flow_fine(self, x0, x1, flow, w, **kwargs):\n    with tf.variable_scope(\'Flow/Fine\', reuse=tf.AUTO_REUSE):\n      x = tf.concat([x0, x1, flow, w], axis=-1)\n      x = self.relu_conv2d(x, 24, 5, strides=2)\n      x = self.relu_conv2d(x, 24, 3)\n      x = self.relu_conv2d(x, 24, 3)\n      x = self.relu_conv2d(x, 24, 3)\n      x = self.tanh_conv2d(x, 8, 3)\n      x = pixel_shift(x, 2, 2)\n      return x\n\n  def _me(self, x0, x1, **kwargs):\n    with tf.variable_scope(\'MotionEstimation\', reuse=tf.AUTO_REUSE):\n      flow0 = self._flow_coarse(x0, x1)\n      u0, v0 = flow0[..., 0], flow0[..., 1]\n      w0 = warp(x1, u0, v0, True)\n      flow_d = self._flow_fine(x0, x1, flow0, w0)\n      flow1 = flow0 + flow_d\n      u1, v1 = flow1[..., 0], flow1[..., 1]\n      w1 = warp(x1, u1, v1, True)\n      return w1, flow1\n\n  def _stn(self, inputs, **kwargs):\n    with tf.variable_scope(kwargs.get(\'name\'), \'SpatialTemporalNet\'):\n      x = self.conv2d(inputs, 64, 3)\n      sf = self.conv2d(x, 64, 3)\n      x = self.resblock(sf, 64, 3, activation=\'relu\', placement=\'front\')\n      x = self.resblock(x, 64, 3, activation=\'relu\', placement=\'front\')\n      x = self.resblock(x, 64, 3, activation=\'relu\', placement=\'front\')\n      sr = self.upscale(x + sf, direct_output=False)\n      sr = self.conv2d(sr, self.channel, 3)\n      return sr\n\n  def _strange_huber_loss(self, inputs, epsilon=0.01):\n    """"""The ""Huber loss"" used in paper.""""""\n    diff_x = inputs[:, 1:, :, :] - inputs[:, :-1, :, :]\n    diff_y = inputs[:, :, 1:, :] - inputs[:, :, :-1, :]\n    diff_x2 = diff_x ** 2\n    diff_y2 = diff_y ** 2\n    loss = tf.reduce_sum(diff_x2, axis=[1, 2, 3]) + \\\n           tf.reduce_sum(diff_y2, axis=[1, 2, 3]) + epsilon\n    return tf.reduce_mean(tf.sqrt(loss))\n\n  def build_graph(self):\n    self.inputs.append(tf.placeholder(\n      tf.float32, [None, self.depth, None, None, self.channel],\n      name=\'input/lr\'))\n    self.label.append(tf.placeholder(\n      tf.float32, [None, self.depth, None, None, self.channel],\n      name=\'label\'))\n    inputs = self.inputs[0]\n    inputs = pad_if_divide(inputs, 4)\n    labels = self.label[0]\n    labels = pad_if_divide(labels, 4 * self.scale[0])\n    center = (self.depth - 1) // 2\n    input_center = inputs[:, center, ...]\n    label_center = labels[:, center, ...]\n    with tf.variable_scope(self.name):\n      frames = tf.split(inputs, self.depth, axis=1)\n      frames = [tf.squeeze(f, axis=1) for f in frames]\n      warps = []\n      flows = []\n      for i in range(self.depth):\n        if i == center:\n          continue\n        w, f = self._me(input_center, frames[i])\n        warps.append(w)\n        flows.append(f)\n      ef = tf.concat(warps + [input_center], axis=-1)\n      sr = self._stn(ef)\n      self.outputs = [*flows, *warps, *frames, input_center, sr]\n      self.WARP = warps\n\n    with tf.name_scope(\'Loss\'):\n      loss_l2 = tf.losses.mean_squared_error(label_center, sr)\n      loss_re = tf.losses.get_regularization_losses()\n      loss_warps = [tf.losses.mean_squared_error(input_center, w) for w in\n                    warps]\n      loss_flows = [self._strange_huber_loss(f) for f in flows]\n      loss_me = tf.add_n([w * self.beta + f * self.gamma for w, f in\n                          zip(loss_warps, loss_flows)])\n      loss = tf.add_n([loss_l2, loss_me] + loss_re)\n\n      update_op = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_op):\n        opt = tf.train.AdamOptimizer(self.learning_rate)\n        opt = opt.minimize(loss, self.global_steps)\n        self.loss.append(opt)\n\n    self.train_metric[\'l2\'] = loss_l2\n    self.train_metric[\'loss\'] = loss\n    self.train_metric[\'me\'] = loss_me\n    self.metrics[\'mse\'] = loss_l2\n    self.metrics[\'psnr\'] = tf.reduce_mean(\n      tf.image.psnr(label_center, sr, 255))\n\n  def build_loss(self):\n    pass\n\n  def build_summary(self):\n    super(VESPCN, self).build_summary()\n    tf.summary.image(\'flow\', viz_flow(self.outputs[0]), 1)\n    tf.summary.image(\'SR\', self.outputs[-1], 1)\n    tf.summary.image(\'WARP/0\', self.WARP[0], 1)\n    tf.summary.image(\'WARP/1\', self.WARP[1], 1)\n'"
VSR/Backend/TF/Models/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2018 - 8 - 1\n\nimport importlib\nimport re\nfrom pathlib import Path\n\n__all__ = [\'get_model\', \'list_supported_models\']\n\n\ndef auto_search(root):\n  def _parse_class(file):\n    obj = []\n    key_to_remove = set()\n    file = Path(file)\n    with file.open(\'r\', encoding=\'utf-8\') as fd:\n      line = fd.readline()\n      while line:\n        if line.startswith(\'class\'):\n          if \'(SuperResolution)\' in line:\n            try:\n              classname = re.compile(""(?<=class\\s)\\w+\\\\b"").findall(line)[0]\n              obj.append(classname)\n            except IndexError:\n              print("" [!] class: "" + line)\n          else:\n            for cls in obj:\n              if f\'({cls})\' in line:\n                try:\n                  classname = re.compile(""(?<=class\\s)\\w+\\\\b"").findall(line)[0]\n                  obj.append(classname)\n                  key_to_remove.add(cls)\n                except IndexError:\n                  print("" [!] class: "" + line)\n        line = fd.readline()\n    for key in key_to_remove:\n      obj.remove(key)\n    return {file.stem: obj}\n\n  mods = sorted(filter(\n    lambda x: x.is_file() and not x.stem.startswith(\'__\'),\n    Path(root).glob(\'*.py\')))\n  for _m in mods:\n    cls = _parse_class(_m)\n    for k in cls:\n      if k.lower() in models:\n        print("" [!] duplicated model names found: "" + k)\n        continue\n      if len(cls[k]) == 1:\n        models[k.lower()] = (k, cls[k][0])\n      elif len(cls[k]) > 1:\n        for i in cls[k]:\n          models[f\'{k.lower()}.{i.lower()}\'] = (k, i)\n\n\nmodels = {}\nauto_search(Path(__file__).parent)\n\n\ndef get_model(name):\n  module = f\'.Backend.TF.Models.{models[name][0]}\'\n  package = \'VSR\'\n  m = importlib.import_module(module, package)\n  return m.__dict__[models[name][1]]\n\n\ndef list_supported_models():\n  return models.keys()\n'"
VSR/Backend/Torch/Framework/Environment.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport logging\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nLOG = logging.getLogger(\'VSR.Framework\')\n\n\ndef _make_ckpt_name(name, step):\n  return \'{}_ep{:04d}.pth\'.format(name, step)\n\n\ndef _parse_ckpt_name(name):\n  if not name:\n    return 0\n  model_name, epochs = Path(name).stem.split(\'.\')[0].split(\'_\')\n  return int(epochs[2:])\n\n\nclass Env:\n  """"""Pytorch model runtime Env-ironment.\n\n  Args:\n    model: a Model object (note it\'s NOT nn.Module), representing a container\n      of multiple nn.Module objects. See `VSRTorch.Models.Model` for details.\n    work_dir: a folder path, working directory of this environment.\n\n  Usage:\n    Use `with` syntax to enter the Env:\n\n    >>> with Env(...) as e: ...\n  """"""\n\n  def __init__(self, model, work_dir=None):\n    self._m = model\n    self._saved = None\n    self._logd = None\n    if work_dir is not None:\n      self._saved = Path(work_dir) / \'save\'\n      self._logd = Path(work_dir) / \'log\'\n    self._restored = False\n\n  def _startup(self):\n    if isinstance(self._saved, Path):\n      self._saved.mkdir(parents=True, exist_ok=True)\n    if isinstance(self._logd, Path):\n      self._logd.mkdir(parents=True, exist_ok=True)\n      if LOG.isEnabledFor(logging.DEBUG):\n        hdl = logging.FileHandler(self._logd / \'training.txt\')\n        LOG.addHandler(hdl)\n\n  def _close(self):\n    """"""TODO anything to close?""""""\n    pass\n\n  def __enter__(self):\n    """"""Create session of tensorflow and build model graph""""""\n\n    self._startup()\n    self.model.display()\n    return self\n\n  def __exit__(self, exc_type, exc_val, exc_tb):\n    """"""Close session""""""\n\n    self._close()\n\n  @property\n  def model(self):\n    return self._m\n\n  def _find_last_ckpt(self, pattern):\n    # restore the latest checkpoint in save dir\n    # sort as modification time\n    if not isinstance(self._saved, Path): return\n    ckpt = sorted(self._saved.glob(pattern), key=lambda x: x.stat().st_mtime_ns)\n    return ckpt[-1].resolve() if ckpt else None\n\n  def _restore_model(self, epoch=None, pth=None, map_location=None):\n    last_epoch = 0\n    for key, model in self.model.modules.items():\n      if pth is None:\n        if epoch is None:\n          ckpt = f\'*{key}*.pth\'\n        else:\n          ckpt = _make_ckpt_name(key, epoch)\n        fp = self._find_last_ckpt(ckpt)\n      else:\n        fp = pth\n      if fp:\n        LOG.info(f""Restoring params for {key} from {fp}."")\n        try:\n          last_epoch = max(_parse_ckpt_name(str(fp)), last_epoch)\n        except ValueError:\n          last_epoch = 0\n        try:\n          model.load_state_dict(torch.load(str(fp), map_location=map_location))\n        except RuntimeError as ex:\n          print(ex)\n          LOG.warning(f""Couldn\'t restore state for {key} from {fp}."")\n    if pth is None and isinstance(self._saved, Path):\n      for key, opt in self.model.opts.items():\n        fp = self._saved / f\'{key}.pth\'\n        try:\n          opt.load_state_dict(torch.load(str(fp)))\n        except (ValueError, FileNotFoundError):\n          LOG.warning(f""trying to restore state for optimizer {key}, ""\n                      ""but failed."")\n    return last_epoch\n\n  def _save_model(self, step):\n    if not isinstance(self._saved, Path): return\n    for key, model in self.model.modules.items():\n      fp = self._saved / _make_ckpt_name(key, step)\n      torch.save(model.state_dict(), str(fp))\n    for key, opt in self.model.opts.items():\n      fp = self._saved / f\'{key}.pth\'\n      torch.save(opt.state_dict(), str(fp))\n\n  def _restore(self, epoch=None, map_location=None):\n    # restore graph\n    if self._restored:\n      return self.last_epoch\n    self.last_epoch = self._restore_model(epoch, map_location=map_location)\n    self._restored = True\n    return self.last_epoch\n\n  def set_seed(self, seed):\n    """"""set a seed for RNG\n\n    Note: RNG in torch and numpy is different.\n    """"""\n\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n  def export(self, export_dir=\'.\', version=1):\n    """"""export ONNX model.\n\n    Args:\n      export_dir: path to save onnx files.\n      version: (optional) a child-folder to control output versions.\n    """"""\n\n    export_path = Path(export_dir) / str(version)\n    while export_path.exists():\n      version += 1  # step ahead 1 version\n      export_path = Path(export_dir) / str(version)\n    export_path.mkdir(exist_ok=False, parents=True)\n    self.model.export(export_path)\n    LOG.info(f""Export ONNX to {str(export_path)}"")\n'"
VSR/Backend/Torch/Framework/Summary.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\ntry:\n  # torch >= 1.1.0\n  from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n  from tensorboardX import SummaryWriter\n\n_writer_container = {}\n\n\nclass Summarizer:\n  def __init__(self, log_path, key=None):\n    if key is not None:\n      self.key = hash(key)\n    else:\n      self.key = hash(str(log_path))\n    self._logd = log_path\n    self.writer = SummaryWriter(str(log_path))\n    _writer_container[self.key] = self\n\n  def close(self):\n    self.writer.close()\n\n  def scalar(self, name, x, step=None, collection=None):\n    if collection is not None:\n      name = f\'{collection}/{name}\'\n    self.writer.add_scalar(name, x, step)\n\n  def image(self, name, image, max=3, step=None, collection=None):\n    if image.ndimension() == 4:\n      images = image.split(1, dim=0)[:max]\n    else:\n      assert image.ndimension() == 3, \\\n        f\'Dim of image is not 3, which is {image.ndimension()}\'\n      images = [image]\n    if collection is not None:\n      name = f\'{collection}/{name}\'\n    for i, img in enumerate(images):\n      self.writer.add_image(f\'{name}_{i}\', img.squeeze(0), step)\n\n  def tensor(self, name, tensor, max=3, step=None, reshape=None):\n    assert tensor.ndimension() == 4, \\\n      f""Support 4-D tensor only! {tensor.ndimension()}""\n    shape = tensor.shape\n\n    def _placement(t):\n      if t <= 16:\n        return 4, t // 4\n      elif t <= 64:\n        return 8, t // 8\n      elif t <= 256:\n        return 16, t // 16\n      else:\n        return 32, t // 32\n\n    if reshape:\n      col, row = reshape\n    else:\n      col, row = _placement(shape[1])\n    tensor = tensor.view([shape[0], row, col, shape[2], shape[3]])\n    tensor = tensor.transpose(2, 3)\n    tensor = tensor.view([shape[0], row * shape[2], 1, col * shape[3], 1])\n    tensor = tensor.squeeze([2, 4])\n    tensor = tensor.unsqueeze(1)\n    self.image(name, tensor, step, max, collection=\'features\')\n\n  def graph(self, model, *args, **kwargs):\n    self.writer.add_graph(model, args, **kwargs)\n\n\ndef get_writer(key) -> Summarizer:\n  return _writer_container.get(hash(key))\n'"
VSR/Backend/Torch/Framework/Trainer.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport logging\nimport time\n\nimport numpy as np\nimport torch\nimport tqdm\n\nfrom VSR.Util.Config import Config\nfrom .Environment import Env\nfrom .Summary import Summarizer\n\nLOG = logging.getLogger(\'VSR.Framework\')\n\n\ndef _ensemble_expand(feature):\n  r0 = feature\n  r1 = np.rot90(feature, 1, axes=[-3, -2])\n  r2 = np.rot90(feature, 2, axes=[-3, -2])\n  r3 = np.rot90(feature, 3, axes=[-3, -2])\n  r4 = np.flip(feature, axis=-2)\n  r5 = np.rot90(r4, 1, axes=[-3, -2])\n  r6 = np.rot90(r4, 2, axes=[-3, -2])\n  r7 = np.rot90(r4, 3, axes=[-3, -2])\n  return r0, r1, r2, r3, r4, r5, r6, r7\n\n\ndef _ensemble_reduce_mean(outputs):\n  results = []\n  for i in outputs:\n    outputs_ensemble = [\n      i[0],\n      np.rot90(i[1], 3, axes=[-3, -2]),\n      np.rot90(i[2], 2, axes=[-3, -2]),\n      np.rot90(i[3], 1, axes=[-3, -2]),\n      np.flip(i[4], axis=-2),\n      np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2),\n      np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2),\n      np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2),\n    ]\n    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))\n  return results\n\n\ndef to_tensor(x, cuda=False):\n  x = torch.as_tensor(x / 255.0, dtype=torch.float32)\n  if cuda and torch.cuda.is_available():\n    x = x.cuda()\n  return x\n\n\ndef from_tensor(x):\n  return x * 255\n\n\nclass SRTrainer(Env):\n  v = Config()\n\n  def query_config(self, config, **kwargs):\n    config = Config(config or {})\n    config.update(kwargs)\n    self.v.epochs = config.epochs or 1  # total epochs\n    self.v.batch_shape = config.batch_shape or [1, -1, -1, -1]\n    self.v.steps = config.steps or 200\n    self.v.val_steps = config.val_steps or -1\n    self.v.lr = config.lr or 1e-4  # learning rate\n    self.v.lr_schedule = config.lr_schedule\n    self.v.memory_limit = config.memory_limit\n    self.v.inference_results_hooks = config.inference_results_hooks or []\n    self.v.validate_every_n_epoch = config.validate_every_n_epoch or 1\n    self.v.traced_val = config.traced_val\n    self.v.ensemble = config.ensemble\n    self.v.cuda = config.cuda\n    self.v.map_location = \'cuda:0\' if config.cuda and torch.cuda.is_available() else \'cpu\'\n    return self.v\n\n  def fit_init(self) -> bool:\n    v = self.v\n    v.epoch = self._restore()\n    if v.epoch >= v.epochs:\n      LOG.info(f\'Found pre-trained epoch {v.epoch}>=target {v.epochs},\'\n               \' quit fitting.\')\n      return False\n    LOG.info(\'Fitting: {}\'.format(self.model.name.upper()))\n    if self._logd:\n      v.writer = Summarizer(str(self._logd), self.model.name)\n    return True\n\n  def fit_close(self):\n    # flush all pending summaries to disk\n    if isinstance(self.v.writer, Summarizer):\n      self.v.writer.close()\n    LOG.info(f\'Training {self.model.name.upper()} finished.\')\n\n  def fit(self, loaders, config, **kwargs):\n    v = self.query_config(config, **kwargs)\n    v.train_loader, v.val_loader = loaders\n    if not self.fit_init():\n      return\n    mem = v.memory_limit\n    for epoch in range(self.last_epoch + 1, v.epochs + 1):\n      v.epoch = epoch\n      train_iter = v.train_loader.make_one_shot_iterator(v.batch_shape,\n                                                         v.steps,\n                                                         shuffle=True,\n                                                         memory_limit=mem)\n      v.train_loader.prefetch(shuffle=True, memory_usage=mem)\n      date = time.strftime(\'%Y-%m-%d %T\', time.localtime())\n      v.avg_meas = {}\n      if v.lr_schedule and callable(v.lr_schedule):\n        v.lr = v.lr_schedule(steps=v.epoch)\n      print(\'| {} | Epoch: {}/{} | LR: {:.2g} |\'.format(\n        date, v.epoch, v.epochs, v.lr))\n      with tqdm.tqdm(train_iter, unit=\'batch\', ascii=True) as r:\n        self.model.to_train()\n        for items in r:\n          self.fn_train_each_step(items)\n          r.set_postfix(v.loss)\n      for _k, _v in v.avg_meas.items():\n        _v = np.mean(_v)\n        if isinstance(self.v.writer, Summarizer):\n          v.writer.scalar(_k, _v, step=v.epoch, collection=\'train\')\n        print(\'| Epoch average {} = {:.6f} |\'.format(_k, _v))\n      if v.epoch % v.validate_every_n_epoch == 0 and v.val_loader:\n        # Hard-coded memory limitation for validating\n        self.benchmark(v.val_loader, v, memory_limit=\'1GB\')\n      self._save_model(v.epoch)\n    self.fit_close()\n\n  def fn_train_each_step(self, pack):\n    v = self.v\n    feature = to_tensor(pack[\'lr\'], v.cuda)\n    label = to_tensor(pack[\'hr\'], v.cuda)\n    loss = self.model.train([feature], [label], v.lr)\n    for _k, _v in loss.items():\n      v.avg_meas[_k] = \\\n        v.avg_meas[_k] + [_v] if v.avg_meas.get(_k) else [_v]\n      loss[_k] = \'{:08.5f}\'.format(_v)\n    v.loss = loss\n\n  def benchmark(self, loader, config, **kwargs):\n    """"""Benchmark/validate the model.\n\n    Args:\n        loader: a loader for enumerating LR images\n        config: benchmark configuration, an instance of `Util.Config.Config`\n        kwargs: additional arguments to override the same ones in config.\n    """"""\n    v = self.query_config(config, **kwargs)\n    self._restore(config.epoch, v.map_location)\n    v.mean_metrics = {}\n    v.loader = loader\n    it = v.loader.make_one_shot_iterator(v.batch_shape, v.val_steps,\n                                         shuffle=not v.traced_val,\n                                         memory_limit=v.memory_limit)\n    self.model.to_eval()\n    for items in tqdm.tqdm(it, \'Test\', ascii=True):\n      with torch.no_grad():\n        self.fn_benchmark_each_step(items)\n    for _k, _v in v.mean_metrics.items():\n      _v = np.mean(_v)\n      if isinstance(self.v.writer, Summarizer):\n        v.writer.scalar(_k, _v, step=v.epoch, collection=\'eval\')\n      print(\'{}: {:.6f}\'.format(_k, _v), end=\', \')\n    print(\'\')\n\n  def fn_benchmark_each_step(self, pack):\n    v = self.v\n    feature = to_tensor(pack[\'lr\'], v.cuda)\n    label = to_tensor(pack[\'hr\'], v.cuda)\n    with torch.set_grad_enabled(False):\n      outputs, metrics = self.model.eval([feature], [label], epoch=v.epoch)\n    for _k, _v in metrics.items():\n      if _k not in v.mean_metrics:\n        v.mean_metrics[_k] = []\n      v.mean_metrics[_k] += [_v]\n    outputs = [from_tensor(x) for x in outputs]\n    for fn in v.inference_results_hooks:\n      outputs = fn(outputs, names=pack[\'name\'])\n      if outputs is None: break\n\n  def infer(self, loader, config, **kwargs):\n    """"""Infer SR images.\n\n    Args:\n        loader: a loader for enumerating LR images\n        config: inferring configuration, an instance of `Util.Config.Config`\n        kwargs: additional arguments to override the same ones in config.\n    """"""\n    v = self.query_config(config, **kwargs)\n    self._restore(config.epoch, v.map_location)\n    it = loader.make_one_shot_iterator([1, -1, -1, -1], -1)\n    if hasattr(it, \'__len__\'):\n      if len(it):\n        LOG.info(\'Inferring {} at epoch {}\'.format(\n          self.model.name, self.last_epoch))\n      else:\n        return\n    # use original images in inferring\n    self.model.to_eval()\n    for items in tqdm.tqdm(it, \'Infer\', ascii=True):\n      with torch.no_grad():\n        self.fn_infer_each_step(items)\n\n  def fn_infer_each_step(self, pack):\n    v = self.v\n    with torch.set_grad_enabled(False):\n      if v.ensemble:\n        # add self-ensemble boosting metric score\n        feature_ensemble = _ensemble_expand(pack[\'lr\'])\n        outputs_ensemble = []\n        for f in feature_ensemble:\n          f = to_tensor(f, v.cuda)\n          y, _ = self.model.eval([f])\n          y = [from_tensor(x) for x in y]\n          outputs_ensemble.append(y)\n        outputs = []\n        for i in range(len(outputs_ensemble[0])):\n          outputs.append([j[i] for j in outputs_ensemble])\n        outputs = _ensemble_reduce_mean(outputs)\n      else:\n        feature = to_tensor(pack[\'lr\'], v.cuda)\n        outputs, _ = self.model.eval([feature])\n        outputs = [from_tensor(x) for x in outputs]\n    for fn in v.inference_results_hooks:\n      outputs = fn(outputs, names=pack[\'name\'])\n      if outputs is None: break\n'"
VSR/Backend/Torch/Framework/__init__.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\n__all__ = [\n  'Environment',\n  'Summary',\n  'Trainer'\n]\n"""
VSR/Backend/Torch/Models/Arch.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:10\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom VSR.Util.Utility import to_list\n\n\nclass EasyConv2d(nn.Module):\n  def __init__(self, in_channels, out_channels, kernel_size,\n               stride=1, padding=\'same\', dilation=1, groups=1,\n               activation=None, use_bias=True, use_bn=False, use_sn=False):\n    super(EasyConv2d, self).__init__()\n    assert padding.lower() in (\'same\', \'valid\')\n    if padding == \'same\':\n      padding_ = (kernel_size - 1) // 2\n    else:\n      padding_ = 0\n    net = [nn.Conv2d(in_channels, out_channels, kernel_size, stride,\n                     padding_, dilation, groups, use_bias)]\n    if use_sn:\n      net[0] = nn.utils.spectral_norm(net[0])\n    if use_bn:\n      net += [nn.BatchNorm2d(out_channels)]\n    if activation:\n      net += [Activation(activation, in_place=True)]\n    self.body = nn.Sequential(*net)\n\n  def forward(self, x):\n    return self.body(x)\n\n  def initialize_(self, kernel, bias=None):\n    """"""initialize the convolutional weights from external sources\n\n    Args:\n        kernel: kernel weight. Shape=[OUT, IN, K, K]\n        bias: bias weight. Shape=[OUT]\n    """"""\n\n    dtype = self.body[0].weight.dtype\n    device = self.body[0].weight.device\n    kernel = torch.tensor(kernel, dtype=dtype, device=device,\n                          requires_grad=True)\n    assert kernel.shape == self.body[0].weight.shape, ""Wrong kernel shape!""\n    if bias is not None:\n      bias = torch.tensor(bias, dtype=dtype, device=device, requires_grad=True)\n      assert bias.shape == self.body[0].bias.shape, ""Wrong bias shape!""\n    self.body[0].weight.data.copy_(kernel)\n    self.body[0].bias.data.copy_(bias)\n\n\nclass RB(nn.Module):\n  def __init__(self, channels, kernel_size, activation=None, use_bias=True,\n               use_bn=False, use_sn=False, act_first=None):\n    super(RB, self).__init__()\n    in_c, out_c = to_list(channels, 2)\n    conv1 = nn.Conv2d(\n        in_c, out_c, kernel_size, 1, kernel_size // 2, bias=use_bias)\n    conv2 = nn.Conv2d(\n        out_c, out_c, kernel_size, 1, kernel_size // 2, bias=use_bias)\n    if use_sn:\n      conv1 = nn.utils.spectral_norm(conv1)\n      conv2 = nn.utils.spectral_norm(conv2)\n    net = [conv1, Activation(activation, in_place=True), conv2]\n    if use_bn:\n      net.insert(1, nn.BatchNorm2d(out_c))\n      if act_first:\n        net = [nn.BatchNorm2d(in_c), Activation(activation, in_place=True)] + \\\n              net\n      else:\n        net.append(nn.BatchNorm2d(out_c))\n    self.body = nn.Sequential(*net)\n    if in_c != out_c:\n      self.shortcut = nn.Conv2d(in_c, out_c, 1)\n\n  def forward(self, x):\n    out = self.body(x)\n    if hasattr(self, \'shortcut\'):\n      sc = self.shortcut(x)\n      return out + sc\n    return out + x\n\n\nclass Rdb(nn.Module):\n  def __init__(self, channels, depth=3, scaling=1.0, name=\'Rdb\', **kwargs):\n    super(Rdb, self).__init__()\n    self.name = name\n    self.depth = depth\n    self.scaling = scaling\n    in_c, out_c = to_list(channels, 2)\n    ks = kwargs.get(\'kernel_size\', 3)\n    stride = kwargs.get(\'stride\', 1)\n    padding = kwargs.get(\'padding\', ks // 2)\n    dilation = kwargs.get(\'dilation\', 1)\n    group = kwargs.get(\'group\', 1)\n    bias = kwargs.get(\'bias\', True)\n    act = kwargs.get(\'activation\', \'relu\')\n    for i in range(depth):\n      conv = nn.Conv2d(\n          in_c + out_c * i, out_c, ks, stride, padding, dilation, group, bias)\n      if i < depth - 1:  # no activation after last layer\n        conv = nn.Sequential(conv, Activation(act))\n      setattr(self, f\'conv_{i}\', conv)\n\n  def forward(self, inputs):\n    fl = [inputs]\n    for i in range(self.depth):\n      conv = getattr(self, f\'conv_{i}\')\n      fl.append(conv(torch.cat(fl, dim=1)))\n    return fl[-1] * self.scaling + inputs\n\n  def extra_repr(self):\n    return f""{self.name}: depth={self.depth}, scaling={self.scaling}""\n\n\nclass Rcab(nn.Module):\n  def __init__(self, channels, ratio=16, name=\'RCAB\', **kwargs):\n    super(Rcab, self).__init__()\n    self.name = name\n    self.ratio = ratio\n    in_c, out_c = to_list(channels, 2)\n    ks = kwargs.get(\'kernel_size\', 3)\n    padding = kwargs.get(\'padding\', ks // 2)\n    group = kwargs.get(\'group\', 1)\n    bias = kwargs.get(\'bias\', True)\n    self.c1 = nn.Sequential(\n        nn.Conv2d(in_c, out_c, ks, 1, padding, 1, group, bias),\n        nn.ReLU(True))\n    self.c2 = nn.Conv2d(out_c, out_c, ks, 1, padding, 1, group, bias)\n    self.c3 = nn.Sequential(\n        nn.Conv2d(out_c, out_c // ratio, 1, groups=group, bias=bias),\n        nn.ReLU(True))\n    self.c4 = nn.Sequential(\n        nn.Conv2d(out_c // ratio, in_c, 1, groups=group, bias=bias),\n        nn.Sigmoid())\n    self.pooling = nn.AdaptiveAvgPool2d(1)\n\n  def forward(self, inputs):\n    x = self.c1(inputs)\n    y = self.c2(x)\n    x = self.pooling(y)\n    x = self.c3(x)\n    x = self.c4(x)\n    y = x * y\n    return inputs + y\n\n  def extra_repr(self):\n    return f""{self.name}: ratio={self.ratio}""\n\n\nclass CascadeRdn(nn.Module):\n  def __init__(self, channels, depth=3, use_ca=False, name=\'CascadeRdn\',\n               **kwargs):\n    super(CascadeRdn, self).__init__()\n    self.name = name\n    self.depth = to_list(depth, 2)\n    self.ca = use_ca\n    in_c, out_c = to_list(channels, 2)\n    for i in range(self.depth[0]):\n      setattr(self, f\'conv11_{i}\', nn.Conv2d(in_c + out_c * (i + 1), out_c, 1))\n      setattr(self, f\'rdn_{i}\', Rdb(channels, self.depth[1], **kwargs))\n      if use_ca:\n        setattr(self, f\'rcab_{i}\', Rcab(channels))\n\n  def forward(self, inputs):\n    fl = [inputs]\n    x = inputs\n    for i in range(self.depth[0]):\n      rdn = getattr(self, f\'rdn_{i}\')\n      x = rdn(x)\n      if self.ca:\n        rcab = getattr(self, f\'rcab_{i}\')\n        x = rcab(x)\n      fl.append(x)\n      c11 = getattr(self, f\'conv11_{i}\')\n      x = c11(torch.cat(fl, dim=1))\n\n    return x\n\n  def extra_repr(self):\n    return f""{self.name}: depth={self.depth}, ca={self.ca}""\n\n\nclass Activation(nn.Module):\n  def __init__(self, name, *args, **kwargs):\n    super(Activation, self).__init__()\n    if name is None:\n      self.f = lambda t: t\n    self.name = name.lower()\n    in_place = kwargs.get(\'in_place\', True)\n    if self.name == \'relu\':\n      self.f = nn.ReLU(in_place)\n    elif self.name == \'prelu\':\n      self.f = nn.PReLU()\n    elif self.name in (\'lrelu\', \'leaky\', \'leakyrelu\'):\n      self.f = nn.LeakyReLU(*args, inplace=in_place)\n    elif self.name == \'tanh\':\n      self.f = nn.Tanh()\n    elif self.name == \'sigmoid\':\n      self.f = nn.Sigmoid()\n\n  def forward(self, x):\n    return self.f(x)\n\n\nclass _UpsampleNearest(nn.Module):\n  def __init__(self, scale):\n    super(_UpsampleNearest, self).__init__()\n    self.scale = scale\n\n  def forward(self, x, scale=None):\n    scale = scale or self.scale\n    return F.interpolate(x, scale_factor=scale)\n\n\nclass _UpsampleLinear(nn.Module):\n  def __init__(self, scale):\n    super(_UpsampleLinear, self).__init__()\n    self._mode = (\'linear\', \'bilinear\', \'trilinear\')\n    self.scale = scale\n\n  def forward(self, x, scale=None):\n    scale = scale or self.scale\n    mode = self._mode[x.dim() - 3]\n    return F.interpolate(x, scale_factor=scale, mode=mode, align_corners=False)\n\n\nclass Upsample(nn.Module):\n  def __init__(self, channel, scale, method=\'ps\', name=\'Upsample\', **kwargs):\n    super(Upsample, self).__init__()\n    self.name = name\n    self.channel = channel\n    self.scale = scale\n    self.method = method.lower()\n    self.kernel_size = kwargs.get(\'kernel_size\', 3)\n\n    _allowed_methods = (\'ps\', \'nearest\', \'deconv\', \'linear\')\n    assert self.method in _allowed_methods\n    act = kwargs.get(\'activation\')\n\n    samplers = []\n    while scale > 1:\n      if scale % 2 == 1 or scale == 2:\n        samplers.append(self.upsampler(self.method, scale))\n        break\n      else:\n        samplers.append(self.upsampler(self.method, 2, act))\n        scale //= 2\n    self.body = nn.Sequential(*samplers)\n\n  def upsampler(self, method, scale, activation=None):\n    body = []\n    k = self.kernel_size\n    if method == \'ps\':\n      p = k // 2  # padding\n      s = 1  # strides\n      body = [nn.Conv2d(self.channel, self.channel * scale * scale, k, s, p),\n              nn.PixelShuffle(scale)]\n      if activation:\n        body.insert(1, Activation(activation))\n    if method == \'deconv\':\n      q = k % 2  # output padding\n      p = (k + q) // 2 - 1  # padding\n      s = scale  # strides\n      body = [nn.ConvTranspose2d(self.channel, self.channel, k, s, p, q)]\n      if activation:\n        body.insert(1, Activation(activation))\n    if method == \'nearest\':\n      body = [_UpsampleNearest(scale),\n              EasyConv2d(self.channel, self.channel, k, activation=activation)]\n    if method == \'linear\':\n      body = [_UpsampleLinear(scale),\n              EasyConv2d(self.channel, self.channel, k, activation=activation)]\n    return nn.Sequential(*body)\n\n  def forward(self, inputs):\n    return self.body(inputs)\n\n  def extra_repr(self):\n    return f""{self.name}: scale={self.scale}""\n\n\nclass SpaceToDim(nn.Module):\n  def __init__(self, scale_factor, dims=(-2, -1), dim=0):\n    super(SpaceToDim, self).__init__()\n    self.scale_factor = scale_factor\n    self.dims = dims\n    self.dim = dim\n\n  def forward(self, x):\n    _shape = list(x.shape)\n    shape = _shape.copy()\n    dims = [x.dim() + self.dims[0] if self.dims[0] < 0 else self.dims[0],\n            x.dim() + self.dims[1] if self.dims[1] < 0 else self.dims[1]]\n    dims = [max(abs(dims[0]), abs(dims[1])),\n            min(abs(dims[0]), abs(dims[1]))]\n    if self.dim in dims:\n      raise RuntimeError(""Integrate dimension can\'t be space dimension!"")\n    shape[dims[0]] //= self.scale_factor\n    shape[dims[1]] //= self.scale_factor\n    shape.insert(dims[0] + 1, self.scale_factor)\n    shape.insert(dims[1] + 1, self.scale_factor)\n    dim = self.dim if self.dim < dims[1] else self.dim + 1\n    dim = dim if dim <= dims[0] else dim + 1\n    x = x.reshape(*shape)\n    perm = [dim, dims[1] + 1, dims[0] + 2]\n    perm = [i for i in range(min(perm))] + perm\n    perm.extend((i for i in range(x.dim()) if i not in perm))\n    x = x.permute(*perm)\n    shape = _shape\n    shape[self.dim] *= self.scale_factor ** 2\n    shape[self.dims[0]] //= self.scale_factor\n    shape[self.dims[1]] //= self.scale_factor\n    return x.reshape(*shape)\n\n  def extra_repr(self):\n    return f\'scale_factor={self.scale_factor}\'\n\n\nclass SpaceToDepth(nn.Module):\n  def __init__(self, block_size):\n    super(SpaceToDepth, self).__init__()\n    self.body = SpaceToDim(block_size, dim=1)\n\n  def forward(self, x):\n    return self.body(x)\n\n\nclass SpaceToBatch(nn.Module):\n  def __init__(self, block_size):\n    super(SpaceToBatch, self).__init__()\n    self.body = SpaceToDim(block_size, dim=0)\n\n  def forward(self, x):\n    return self.body(x)\n\n\nclass CBAM(nn.Module):\n  """"""Convolutional Block Attention Module (ECCV 18)\n  - CA: channel attention module\n  - SA: spatial attention module\n\n  Args:\n    channels: input channel of tensors\n    channel_reduction: reduction ratio in `CA`\n    spatial_first: put SA ahead of CA (default: CA->SA)\n  """"""\n\n  class CA(nn.Module):\n    def __init__(self, channels, ratio=16):\n      super(CBAM.CA, self).__init__()\n      self.max_pool = nn.AdaptiveMaxPool2d(1)\n      self.avg_pool = nn.AdaptiveAvgPool2d(1)\n      self.mlp = nn.Sequential(\n          nn.Conv2d(channels, channels // ratio, 1),\n          nn.ReLU(),\n          nn.Conv2d(channels // ratio, channels, 1))\n\n    def forward(self, x):\n      maxpool = self.max_pool(x)\n      avgpool = self.avg_pool(x)\n      att = F.sigmoid(self.mlp(maxpool) + self.mlp(avgpool))\n      return att * x\n\n  class SA(nn.Module):\n    def __init__(self, kernel_size=7):\n      super(CBAM.SA, self).__init__()\n      self.conv = nn.Conv2d(2, 1, kernel_size, 1, kernel_size // 2)\n\n    def forward(self, x):\n      max_c_pool = x.max(dim=1, keepdim=True)\n      avg_c_pool = x.mean(dim=1, keepdim=True)\n      y = torch.cat([max_c_pool, avg_c_pool], dim=1)\n      att = F.sigmoid(self.conv(y))\n      return att * x\n\n  def __init__(self, channels, channel_reduction=16, spatial_first=None):\n    super(CBAM, self).__init__()\n    self.channel_attention = CBAM.CA(channels, ratio=channel_reduction)\n    self.spatial_attention = CBAM.SA(7)\n    self.spatial_first = spatial_first\n\n  def forward(self, inputs):\n    if self.spatial_first:\n      x = self.spatial_attention(inputs)\n      return self.channel_attention(x)\n    else:\n      x = self.channel_attention(inputs)\n      return self.spatial_attention(x)\n'"
VSR/Backend/Torch/Models/Carn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 13\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .Model import SuperResolution\nfrom .carn import carn, carn_m\nfrom ..Util import Metrics\n\n\nclass CARN(SuperResolution):\n  def __init__(self, scale, channel, **kwargs):\n    super(CARN, self).__init__(scale, channel, **kwargs)\n    group = kwargs.get(\'group\', 1)\n    ms = kwargs.get(\'multi_scale\', 0)\n    self.clip = kwargs.get(\'clip\', 10)\n    if group > 1:\n      self.carn = carn_m.Net(group=group, scale=scale, multi_scale=ms)\n    else:\n      self.carn = carn.Net(scale=scale, multi_scale=ms)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.carn(inputs[0], self.scale)\n    loss = F.l1_loss(sr, labels[0])\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(self.carn.parameters(), self.clip)\n    self.opt.step()\n    return {\'l1\': loss.detach().cpu().numpy()}\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.carn(inputs[0], self.scale).cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n    return [sr.numpy()], metrics\n\n  def export(self, export_dir):\n    """"""An example of how to export ONNX format""""""\n\n    # ONNX needs input placeholder to export model!\n    # Sounds stupid to set a 48x48 inputs.\n\n    device = list(self.carn.parameters())[0].device\n    inputs = torch.randn(1, self.channel, 144, 128, device=device)\n    scale = torch.tensor(self.scale, device=device)\n    torch.onnx.export(self.carn, (inputs, scale), export_dir / \'carn.onnx\')\n'"
VSR/Backend/Torch/Models/Classic.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 21\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .Arch import EasyConv2d, RB\nfrom .Loss import VggFeatureLoss\nfrom .Model import SuperResolution\nfrom ..Util import Metrics\nfrom ..Util.Utility import upsample\nfrom ..Framework.Summary import get_writer\n\n\nclass Espcn(nn.Module):\n  def __init__(self, channel, scale):\n    super(Espcn, self).__init__()\n    conv1 = nn.Conv2d(channel, 64, 5, 1, 2)\n    conv2 = nn.Conv2d(64, 32, 3, 1, 1)\n    conv3 = nn.Conv2d(32, channel * scale * scale, 3, 1, 1)\n    ps = nn.PixelShuffle(scale)\n    self.body = nn.Sequential(conv1, nn.Tanh(),\n                              conv2, nn.Tanh(),\n                              conv3, nn.Tanh(), ps)\n\n  def forward(self, x):\n    return self.body(x)\n\n\nclass Srcnn(nn.Module):\n  def __init__(self, channel, filters=(9, 5, 5)):\n    super(Srcnn, self).__init__()\n    self.net = nn.Sequential(\n      EasyConv2d(channel, 64, filters[0], activation=\'relu\'),\n      EasyConv2d(64, 32, filters[1], activation=\'relu\'),\n      EasyConv2d(32, channel, filters[2], activation=None))\n\n  def forward(self, x):\n    return self.net(x)\n\n\nclass Vdsr(nn.Module):\n  def __init__(self, channel, layers=20):\n    super(Vdsr, self).__init__()\n    net = [EasyConv2d(channel, 64, 3, activation=\'relu\')]\n    for i in range(1, layers - 1):\n      net.append(EasyConv2d(64, 64, 3, activation=\'relu\'))\n    net.append(EasyConv2d(64, channel, 3))\n    self.net = nn.Sequential(*net)\n\n  def forward(self, x):\n    return self.net(x) + x\n\n\nclass DnCnn(nn.Module):\n  def __init__(self, channel, layers, bn):\n    super(DnCnn, self).__init__()\n    net = [EasyConv2d(channel, 64, 3, activation=\'relu\', use_bn=bn)]\n    for i in range(1, layers - 1):\n      net.append(EasyConv2d(64, 64, 3, activation=\'relu\', use_bn=bn))\n    net.append(EasyConv2d(64, channel, 3))\n    self.net = nn.Sequential(*net)\n\n  def forward(self, x):\n    return self.net(x) + x\n\n\nclass Drcn(nn.Module):\n  def __init__(self, scale, channel, n_recur, filters):\n    from torch.nn import Parameter\n\n    super(Drcn, self).__init__()\n    self.entry = nn.Sequential(\n      EasyConv2d(channel, filters, 3, activation=\'relu\'),\n      EasyConv2d(filters, filters, 3, activation=\'relu\'))\n    self.exit = nn.Sequential(\n      EasyConv2d(filters, filters, 3, activation=\'relu\'),\n      EasyConv2d(filters, channel, 3))\n    self.conv = EasyConv2d(filters, filters, 3, activation=\'relu\')\n    self.output_weights = Parameter(torch.empty(n_recur + 1))\n    torch.nn.init.uniform_(self.output_weights, 0, 1)\n    self.n_recur = n_recur\n    self.scale = scale\n\n  def forward(self, x):\n    bic = upsample(x, self.scale)\n    y = [self.entry(bic)]\n    for i in range(self.n_recur):\n      y.append(self.conv(y[-1]))\n    sr = [self.exit(i) for i in y[1:]]\n    final = bic * self.output_weights[0]\n    for i in range(len(sr)):\n      final = final + self.output_weights[i + 1] * sr[i]\n    return final\n\n\nclass Drrn(nn.Module):\n  def __init__(self, channel, n_ru, n_rb, filters):\n    super(Drrn, self).__init__()\n    self.entry0 = EasyConv2d(channel, filters, 3, activation=\'relu\')\n    for i in range(1, n_rb):\n      setattr(self, f\'entry{i}\',\n              EasyConv2d(filters, filters, 3, activation=\'relu\'))\n    self.n_rb = n_rb\n    self.rb = RB(filters, 3, activation=\'relu\')\n    self.n_ru = n_ru\n    self.exit = EasyConv2d(filters, channel, 3)\n\n  def forward(self, x):\n    for i in range(self.n_rb):\n      entry = getattr(self, f\'entry{i}\')\n      y = entry(x)\n      for j in range(self.n_ru):\n        y = self.rb(y)\n      x = y\n    return self.exit(x)\n\n\nclass PerceptualOptimizer(SuperResolution):\n  def __init__(self, scale, channel, image_weight=1, feature_weight=0,\n               **kwargs):\n    super(PerceptualOptimizer, self).__init__(scale, channel, **kwargs)\n    if feature_weight > 0:\n      # tricks: do not save weights of vgg\n      self.feature = [VggFeatureLoss([\'block3_conv4\'], True)]\n    self.w = [image_weight, feature_weight]\n    self.clip = kwargs.get(\'clip\')\n    self.opt_config = kwargs.get(\'opt\')\n\n  def get_opt(self, params, lr):\n    if self.opt_config is None:\n      return torch.optim.Adam(params, lr=lr)\n    if self.opt_config.get(\'name\') == \'Adadelta\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.Adadelta(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'Adagrad\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.Adagrad(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'Adam\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.Adam(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'SparseAdam\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.SparseAdam(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'Adamax\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.Adamax(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'ASGD\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.ASGD(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'SGD\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.SGD(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'LBFGS\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.LBFGS(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'Rprop\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.Rprop(params, lr=lr, **kwargs)\n    elif self.opt_config.get(\'name\') == \'RMSprop\':\n      kwargs = self.opt_config\n      kwargs.pop(\'name\')\n      return torch.optim.RMSprop(params, lr=lr, **kwargs)\n\n  def cuda(self):\n    super(PerceptualOptimizer, self).cuda()\n    if self.w[1] > 0:\n      self.feature[0].cuda()\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.fn(inputs[0])\n    image_loss = F.mse_loss(sr, labels[0])\n    loss = image_loss * self.w[0]\n    if self.w[1] > 0:\n      self.feature[0].eval()\n      # sr = self.fn(inputs[0])\n      feat_fake = self.feature[0](sr)[0]\n      feat_real = self.feature[0](labels[0])[0]\n      feature_loss = F.mse_loss(feat_fake, feat_real)\n      loss += feature_loss * self.w[1]\n    opt = list(self.opts.values())[0]\n    if learning_rate:\n      for param_group in opt.param_groups:\n        param_group[""lr""] = learning_rate\n    opt.zero_grad()\n    loss.backward()\n    if self.clip:\n      clip = self.clip / learning_rate\n      nn.utils.clip_grad_norm_(self.trainable_variables(), clip)\n    opt.step()\n    return {\n      \'loss\': loss.detach().cpu().numpy(),\n      \'image\': image_loss.detach().cpu().numpy(),\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.fn(inputs[0]).detach().cpu()\n    bi = upsample(inputs[0], self.scale).detach().cpu()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs.get(\'epoch\')\n        writer.image(\'sr\', sr.clamp(0, 1), max=1, step=step)\n        writer.image(\'bicubic\', bi.clamp(0, 1), max=1, step=step)\n        writer.image(\'gt\', labels[0], max=1, step=step)\n    return [sr.numpy()], metrics\n\n  def fn(self, tensor):\n    raise NotImplementedError\n\n\nclass ESPCN(PerceptualOptimizer):\n  def __init__(self, scale, channel, **kwargs):\n    super(ESPCN, self).__init__(scale, channel, **kwargs)\n    self.espcn = Espcn(channel, scale)\n    self.opt = self.get_opt(self.trainable_variables(), 1e-4)\n\n  def fn(self, tensor):\n    return self.espcn(tensor * 2 - 1) / 2 + 0.5\n\n\nclass SRCNN(PerceptualOptimizer):\n  def __init__(self, scale, channel, **kwargs):\n    super(SRCNN, self).__init__(scale, channel, **kwargs)\n    filters = kwargs.get(\'filters\', (9, 5, 5))\n    self.srcnn = Srcnn(channel, filters)\n    self.opt = self.get_opt(self.trainable_variables(), 1e-4)\n\n  def fn(self, tensor):\n    x = upsample(tensor, self.scale)\n    return self.srcnn(x)\n\n\nclass VDSR(PerceptualOptimizer):\n  def __init__(self, scale, channel, **kwargs):\n    super(VDSR, self).__init__(scale, channel, **kwargs)\n    layers = kwargs.get(\'layers\', 20)\n    self.vdsr = Vdsr(channel, layers)\n    self.opt = self.get_opt(self.trainable_variables(), 1e-4)\n\n  def fn(self, tensor):\n    x = upsample(tensor, self.scale)\n    return self.vdsr(x)\n\n\nclass DNCNN(PerceptualOptimizer):\n  def __init__(self, channel, scale, noise, **kwargs):\n    super(DNCNN, self).__init__(1, channel, **kwargs)\n    layers = kwargs.get(\'layers\', 15)\n    bn = kwargs.get(\'bn\', True)\n    self.dncnn = DnCnn(channel, layers, bn)\n    self.opt = self.get_opt(self.trainable_variables(), 1e-4)\n    self.noise = noise / 255\n    self.norm = torch.distributions.normal.Normal(0, self.noise)\n\n  def fn(self, tensor):\n    if self.noise > 0:\n      device = tensor.device\n      noise = self.norm.sample(tensor.shape)\n      tensor += noise.to(device)\n    return self.dncnn(tensor)\n\n\nclass DRCN(PerceptualOptimizer):\n  def __init__(self, scale, channel, n_recur, **kwargs):\n    super(DRCN, self).__init__(scale, channel, **kwargs)\n    self.drcn = Drcn(scale, channel, n_recur, 128)\n    self.opt = self.get_opt(self.trainable_variables(), 1e-4)\n\n  def fn(self, tensor):\n    return self.drcn(tensor)\n\n\nclass DRRN(PerceptualOptimizer):\n  def __init__(self, scale, channel, n_rb, n_ru, **kwargs):\n    super(DRRN, self).__init__(scale, channel, **kwargs)\n    self.drrn = Drrn(channel, n_ru, n_rb, 128)\n    self.opt = self.get_opt(self.trainable_variables(), 1e-4)\n\n  def fn(self, tensor):\n    x = upsample(tensor, self.scale)\n    return self.drrn(x)\n'"
VSR/Backend/Torch/Models/Crdn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 13\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom VSR.Util.Utility import to_list\nfrom . import Model\nfrom .Arch import CascadeRdn\nfrom ..Framework.Summary import get_writer\nfrom ..Util import Metrics\n\n\nclass Upsample(nn.Module):\n  def __init__(self, channels):\n    super(Upsample, self).__init__()\n    in_c, out_c = to_list(channels, 2)\n    self.c1 = nn.Conv2d(in_c, out_c, 3, 1, 1)\n    self.c2 = nn.Conv2d(in_c, out_c, 3, 1, 1)\n\n  def forward(self, inputs, skips, scale=2):\n    up = F.interpolate(inputs, scale_factor=scale)\n    up = self.c1(up)\n    con = torch.cat([up, skips], dim=1)\n    return self.c2(con)\n\n\nclass Crdn(nn.Module):\n  def __init__(self, blocks=(4, 4), **kwargs):\n    super(Crdn, self).__init__()\n    self.blocks = to_list(blocks, 2)\n\n    self.entry = nn.Sequential(\n      nn.Conv2d(3, 32, 7, 1, 3),\n      nn.Conv2d(32, 32, 5, 1, 2))\n    self.exit = nn.Sequential(\n      nn.Conv2d(32, 32, 3, 1, 1),\n      nn.Conv2d(32, 3, 3, 1, 1))\n    self.down1 = nn.Conv2d(32, 64, 3, 2, 1)\n    self.down2 = nn.Conv2d(64, 128, 3, 2, 1)\n    self.up1 = Upsample([128, 64])\n    self.up2 = Upsample([64, 32])\n    self.cb1 = CascadeRdn(32, 3, True)\n    self.cb2 = CascadeRdn(64, 3, True)\n    self.cb3 = CascadeRdn(128, 3, True)\n    self.cb4 = CascadeRdn(128, 3, True)\n    self.cb5 = CascadeRdn(64, 3, True)\n    self.cb6 = CascadeRdn(32, 3, True)\n\n  def forward(self, inputs):\n    entry = self.entry(inputs)\n    x1 = self.cb1(entry)\n    x = self.down1(x1)\n    x2 = self.cb2(x)\n    x = self.down2(x2)\n    x = self.cb3(x)\n    x = self.cb4(x)\n    x = self.up1(x, x2)\n    x = self.cb5(x)\n    x = self.up2(x, x1)\n    x = self.cb6(x)\n    x += entry\n    out = self.exit(x)\n    return out\n\n\nclass CRDN(Model.SuperResolution):\n  def __init__(self, **kwargs):\n    super(CRDN, self).__init__(scale=1, channel=3)\n    self.rsr = Crdn()\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.rsr(inputs[0])\n    loss = F.l1_loss(sr, labels[0])\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\'l1\': loss.detach().cpu().numpy()}\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.rsr(inputs[0]).cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n      writer = get_writer(self.name)\n      if writer is not None:\n        writer.image(\'clean\', sr)\n    return [sr.numpy()], metrics\n'"
VSR/Backend/Torch/Models/Dbpn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .Model import SuperResolution\nfrom .dbpn import dbpn, dbpn_v1, dbpns\nfrom ..Util import Metrics\n\n\nclass DBPNMaker(torch.nn.Module):\n  def __init__(self, mode=\'dbpn\', **kwargs):\n    super(DBPNMaker, self).__init__()\n    _allowed_mode = (\'dbpn\', \'dbpnll\', \'dbpns\')\n    mode = mode.lower()\n    assert mode in _allowed_mode, ""mode must in (\'DBPN\', \'DBPNLL\', \'DBPNS).""\n    if mode == \'dbpn\':\n      self.module = dbpn.Net(**kwargs)\n    elif mode == \'dbpnll\':\n      self.module = dbpn_v1.Net(**kwargs)\n    elif mode == \'dbpns\':\n      self.module = dbpns.Net(**kwargs)\n    else:\n      raise NotImplemented\n\n  def forward(self, x):\n    return self.module(x)\n\n\nclass DBPN(SuperResolution):\n\n  def __init__(self, scale, mode=\'dbpn\', **kwargs):\n    super(DBPN, self).__init__(scale, 3)\n    self.body = DBPNMaker(mode, scale_factor=scale, **kwargs)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.body(inputs[0])\n    loss = F.l1_loss(sr, labels[0])\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\'l1\': loss.detach().cpu().numpy()}\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.body(inputs[0]).cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n    return [sr.numpy()], metrics\n\n  def export(self, export_dir):\n    device = list(self.body.parameters())[0].device\n    inputs = torch.randn(1, self.channel, 144, 128, device=device)\n    torch.onnx.export(self.body, (inputs,), export_dir / \'dbpn.onnx\')\n'"
VSR/Backend/Torch/Models/Discriminator.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/25 \xe4\xb8\x8b\xe5\x8d\x884:08\n\nfrom torch import nn\nfrom .Arch import EasyConv2d, RB, Activation\n\n\ndef _pull_conv_args(**kwargs):\n  f = kwargs.get(\'filters\', 64)\n  ks = kwargs.get(\'kernel_size\', 3)\n  activation = kwargs.get(\'activation\', \'leaky\')\n  bias = kwargs.get(\'bias\', True)\n  norm = kwargs.get(\'norm\', \'\')\n  bn = norm.lower() in (\'bn\', \'batch\')\n  sn = norm.lower() in (\'sn\', \'spectral\')\n  return f, ks, activation, bias, bn, sn\n\n\nclass DCGAN(nn.Module):\n  """"""DCGAN-like discriminator:\n    stack of conv2d layers, stride down to 4x4\n\n  Args:\n    channel: input tensor channel\n    num_layers: number of total cnn layers\n    norm: could be ""None"", ""SN/Spectral"" or ""BN/Batch""\n    leaky: leaky slope\n    favor: some pre-defined topology:\n      \'A\': s1 s2 s1 s2 ...\n      \'B\': s1 s2 s2 s2 ...\n    kwargs: additional options to common CNN\n\n  Note: Since the input before FC layer is B*N*4*4, the input shape can be\n    derived as 4 * (2 ** n_strided), where $n_{strided}=num_layers / 2$ in\n    favor \'A\' and $n_{strided}=num_layers - 1$ in favor \'B\'.\n  """"""\n\n  def __init__(self, channel, num_layers, norm=None, favor=\'A\', **kwargs):\n    super(DCGAN, self).__init__()\n    f, ks, act, bias, bn, sn = _pull_conv_args(norm=norm, **kwargs)\n    net = [EasyConv2d(channel, f, ks, activation=act, use_bn=bn, use_sn=sn,\n                      use_bias=bias)]\n    self.n_strided = 0\n    counter = 1\n    assert favor in (\'A\', \'B\', \'C\'), ""favor must be A | B | C""\n    while True:\n      f *= 2\n      net.append(EasyConv2d(\n        f // 2, f, ks + 1, 2, activation=act, use_bias=bias, use_bn=bn,\n        use_sn=sn))\n      self.n_strided += 1\n      counter += 1\n      if counter >= num_layers:\n        break\n      if favor in (\'A\', \'C\'):\n        net.append(EasyConv2d(\n          f, f, ks, 1, activation=act, use_bias=bias, use_bn=bn,\n          use_sn=sn))\n        counter += 1\n        if counter >= num_layers:\n          break\n    if favor == \'C\':\n      self.body = nn.Sequential(*net, nn.AdaptiveAvgPool2d(1))\n      linear = [nn.Linear(f, 100, bias),\n                Activation(act, in_place=True),\n                nn.Linear(100, 1, bias)]\n    else:\n      self.body = nn.Sequential(*net)\n      linear = [nn.Linear(f * 4 * 4, 100, bias),\n                Activation(act, in_place=True),\n                nn.Linear(100, 1, bias)]\n    if sn:\n      linear[0] = nn.utils.spectral_norm(linear[0])\n      linear[2] = nn.utils.spectral_norm(linear[2])\n    self.linear = nn.Sequential(*linear)\n\n  def forward(self, x):\n    # assert x.size(2) == x.size(3) == 4 * 2 ** self.n_strided\n    y = self.body(x).flatten(1)\n    return self.linear(y)\n\n\nclass Residual(nn.Module):\n  """"""Resnet-like discriminator.\n    Stack of residual block, avg_pooling down to 4x4.\n\n  Args:\n    channel: input tensor channel\n    num_residual: number of total cnn layers\n    norm: could be ""None"", ""SN/Spectral"" or ""BN/Batch""\n    leaky: leaky slope\n    favor: some pre-defined topology:\n      \'A\': norm before 1st conv in residual\n      \'B\': norm after 2nd conv in residual\n    kwargs: additional options to common CNN\n\n  Note: there is always activation and norm after 1st conv; if channel mis-\n    matches, a 1x1 conv is used for shortcut\n  """"""\n\n  def __init__(self, channel, num_residual, norm=None, favor=\'A\', **kwargs):\n    super(Residual, self).__init__()\n    f, ks, act, bias, bn, sn = _pull_conv_args(norm=norm, **kwargs)\n    net = [EasyConv2d(channel, f, ks, activation=act, use_bn=bn, use_sn=sn,\n                      use_bias=bias)]\n    for i in range(num_residual):\n      net.append(RB(f, ks, act, bias, bn, sn, favor == \'A\'))\n      net.append(nn.AvgPool2d(2))\n    net.append(Activation(act, in_place=True))\n    self.body = nn.Sequential(*net)\n    linear = [nn.Linear(f * 4 * 4, 100, bias),\n              Activation(act, in_place=True),\n              nn.Linear(100, 1, bias)]\n    if sn:\n      linear[0] = nn.utils.spectral_norm(linear[0])\n      linear[2] = nn.utils.spectral_norm(linear[2])\n    self.linear = nn.Sequential(*linear)\n    self.n_strided = num_residual\n\n  def forward(self, x):\n    assert x.size(2) == x.size(3) == 4 * 2 ** self.n_strided\n    y = self.body(x).flatten(1)\n    return self.linear(y)\n'"
VSR/Backend/Torch/Models/Drn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/21 \xe4\xb8\x8b\xe5\x8d\x884:56\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .Arch import Upsample, EasyConv2d\nfrom .Model import SuperResolution\nfrom ..Framework.Summary import get_writer\nfrom ..Util import Metrics, Utility\nfrom .Loss import total_variance\n\n\nclass NoiseExtractor(nn.Module):\n  def __init__(self, channel=3, layers=8, bn=False, **kwargs):\n    super(NoiseExtractor, self).__init__()\n    f = kwargs.get(\'filters\', 32)\n    ks = kwargs.get(\'kernel_size\', 3)\n    convs = [EasyConv2d(channel, f, ks, use_bn=bn, activation=\'lrelu\')]\n    for i in range(1, layers - 1):\n      convs += [EasyConv2d(f, f, ks, use_bn=bn, activation=\'lrelu\')]\n    convs += [EasyConv2d(f, channel, ks)]\n    self.body = nn.Sequential(*convs)\n\n  def forward(self, x):\n    return self.body(x)\n\n\nclass NoiseShifter(nn.Module):\n  def __init__(self, channel=3, layers=8, bn=False, **kwargs):\n    super(NoiseShifter, self).__init__()\n    f = kwargs.get(\'filters\', 32)\n    ks = kwargs.get(\'kernel_size\', 3)\n    convs = [EasyConv2d(channel, f, ks, use_bn=bn, activation=\'lrelu\')]\n    for i in range(1, layers - 1):\n      convs += [EasyConv2d(f, f, ks, use_bn=bn, activation=\'lrelu\')]\n    convs += [EasyConv2d(f, channel, ks, activation=\'sigmoid\')]\n    self.body = nn.Sequential(*convs)\n\n  def forward(self, x):\n    return self.body(x)\n\n\nclass NCL(nn.Module):\n  def __init__(self, channels, filters=32, layers=3, **kwargs):\n    super(NCL, self).__init__()\n    ks = kwargs.get(\'kernel_size\', 3)\n    c = channels\n    f = filters\n    conv = []\n    for i in range(1, layers):\n      if i == 1:\n        conv.append(EasyConv2d(3, f, ks, activation=\'lrelu\'))\n      else:\n        conv.append(EasyConv2d(f, f, ks, activation=\'lrelu\'))\n    self.gamma = nn.Sequential(\n      *conv, EasyConv2d(f, c, ks, activation=\'sigmoid\'))\n    self.beta = nn.Sequential(\n      *conv.copy(), EasyConv2d(f, c, ks))\n\n  def forward(self, x, noise=None):\n    if noise is None:\n      return x\n    return x * self.gamma(noise) + self.beta(noise)\n\n\nclass CRDB(nn.Module):\n  def __init__(self, channels, depth=3, scaling=1.0, name=\'Rdb\', **kwargs):\n    super(CRDB, self).__init__()\n    self.name = name\n    self.depth = depth\n    self.scaling = scaling\n    ks = kwargs.get(\'kernel_size\', 3)\n    stride = kwargs.get(\'stride\', 1)\n    padding = kwargs.get(\'padding\', ks // 2)\n    dilation = kwargs.get(\'dilation\', 1)\n    group = kwargs.get(\'group\', 1)\n    bias = kwargs.get(\'bias\', True)\n    c = channels\n    for i in range(depth):\n      conv = nn.Conv2d(\n        c + c * i, c, ks, stride, padding, dilation, group, bias)\n      if i < depth - 1:  # no activation after last layer\n        conv = nn.Sequential(conv, nn.ReLU(True))\n      setattr(self, f\'conv_{i}\', conv)\n    self.ncl = NCL(c)\n\n  def forward(self, inputs, noise):\n    fl = [inputs]\n    for i in range(self.depth):\n      conv = getattr(self, f\'conv_{i}\')\n      fl.append(conv(torch.cat(fl, dim=1)))\n    y = fl[-1] * self.scaling + inputs\n    return self.ncl(y, noise)\n\n\nclass CascadeRdn(nn.Module):\n  def __init__(self, channels, depth=(3, 3), name=\'CascadeRdn\', **kwargs):\n    super(CascadeRdn, self).__init__()\n    self.name = name\n    self.depth = depth\n    c = channels\n    for i in range(self.depth[0]):\n      setattr(self, f\'conv11_{i}\', nn.Conv2d(c + c * (i + 1), c, 1))\n      setattr(self, f\'rdn_{i}\', CRDB(c, self.depth[1], **kwargs))\n\n  def forward(self, inputs, noise):\n    fl = [inputs]\n    x = inputs\n    for i in range(self.depth[0]):\n      rdn = getattr(self, f\'rdn_{i}\')\n      x = rdn(x, noise)\n      fl.append(x)\n      c11 = getattr(self, f\'conv11_{i}\')\n      x = c11(torch.cat(fl, dim=1))\n\n    return x\n\n\nclass Drn(nn.Module):\n  def __init__(self, channel, scale, n_cb, **kwargs):\n    super(Drn, self).__init__()\n    f = kwargs.get(\'filters\', 64)\n    self.entry = nn.Sequential(\n      nn.Conv2d(channel, f, 3, 1, 1), nn.Conv2d(f, f, 3, 1, 1))\n    for i in range(n_cb):\n      setattr(self, f\'cb{i}\', CascadeRdn(f))\n    self.n_cb = n_cb\n    self.tail = nn.Sequential(\n      Upsample(f, scale), nn.Conv2d(f, channel, 3, 1, 1))\n\n  def forward(self, x, noise=None):\n    x0 = self.entry(x)\n    x = x0\n    for i in range(self.n_cb):\n      cb = getattr(self, f\'cb{i}\')\n      x = cb(x, noise)\n    x += x0\n    return self.tail(x)\n\n\nclass DRN(SuperResolution):\n  def __init__(self, channel, scale, n_cb, noise, offset=0):\n    super(DRN, self).__init__(channel=channel, scale=scale)\n    self.noise = noise\n    self.drn = Drn(channel, scale, n_cb)\n    self.ne = NoiseExtractor(channel, bn=False)\n    self.ns = NoiseShifter(channel, bn=False)\n    p1 = self.trainable_variables(\'drn\') + self.trainable_variables(\'ne\')\n    p2 = self.trainable_variables(\'ns\')\n    self.offset = offset\n    if self.noise < 0:\n      self.opt = torch.optim.Adam(p2, 1e-4)\n    else:\n      self.opt = torch.optim.Adam(p1, 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    x0 = inputs[0]\n    metrics = {}\n    if self.noise > 0:\n      stddev = torch.rand(1) * self.noise / 255\n      stddev = stddev.reshape([1, 1, 1, 1])\n      noise_map = torch.randn(*x0.shape) * stddev\n      noise_map = noise_map.to(x0.device)\n      x0 = (x0 + noise_map).clamp(0, 1)\n      noise = self.ne(x0)\n      l2_noise = F.mse_loss(noise, noise_map)\n      metrics[\'noise\'] = l2_noise.detach().cpu().numpy()\n    elif self.noise < 0:\n      stddev = self.ns(x0)\n      noise_map = torch.randn(*x0.shape, device=x0.device) * stddev\n      noise = self.ne(x0) + noise_map\n      l2_noise = 0\n    else:\n      noise = None\n      l2_noise = 0\n\n    y = self.drn(x0, noise)\n    l1_image = F.l1_loss(y, labels[0])\n    loss = l1_image + 10 * l2_noise\n    if self.noise != 0:\n      tv = total_variance(noise)\n      loss += tv * 1.0e-3\n      metrics[\'tv\'] = tv.detach().cpu().numpy()\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    metrics[\'loss\'] = loss.detach().cpu().numpy()\n    metrics[\'image\'] = l1_image.detach().cpu().numpy()\n    return metrics\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    x0 = inputs[0]\n    if self.noise > 0:\n      stddev = torch.rand(1) * self.noise / 255\n      stddev = stddev.reshape([1, 1, 1, 1])\n      noise_map = torch.randn(*x0.shape) * stddev\n      noise_map = noise_map.to(x0.device)\n      x0 = (x0 + noise_map).clamp(0, 1)\n      noise = self.ne(x0)\n    elif self.offset > 0:\n      noise = self.ne(x0)\n      stddev = torch.ones(3, dtype=torch.float32) * self.offset / 255\n      stddev = stddev.reshape([1, 3, 1, 1])\n      noise_map = torch.randn(*x0.shape) * stddev\n      noise_map = noise_map.to(x0.device)\n      noise += noise_map\n    elif self.noise < 0:\n      stddev = self.ns(x0)\n      noise_map = torch.randn(*x0.shape, device=x0.device) * stddev\n      noise = self.ne(x0) + noise_map\n    else:\n      noise = None\n\n    y = self.drn(x0, noise)\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(y, labels[0])\n      writer = get_writer(self.name)\n      step = kwargs[\'epoch\']\n      if writer is not None:\n        writer.image(\'sr\', y.clamp(0, 1), step=step)\n        writer.image(\'hr\', labels[0], step=step)\n        writer.image(\'lr\', x0, step=step)\n    return [y.detach().cpu().numpy()], metrics\n'"
VSR/Backend/Torch/Models/Edsr.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .Model import SuperResolution\nfrom .edsr import edsr, mdsr\nfrom ..Util import Metrics\nfrom VSR.Util.Config import Config\n\n\nclass EDSR(SuperResolution):\n\n  def __init__(self, scale, **kwargs):\n    super(EDSR, self).__init__(scale, 3)\n    args = Config(kwargs)\n    args.scale = [scale]\n    self.rgb_range = args.rgb_range\n    self.edsr = edsr.EDSR(args)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.edsr(inputs[0] * self.rgb_range) / self.rgb_range\n    loss = F.l1_loss(sr, labels[0])\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\'l1\': loss.detach().cpu().numpy()}\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.edsr(inputs[0] * self.rgb_range) / self.rgb_range\n    sr = sr.cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n    return [sr.numpy()], metrics\n\n\nclass MSDR(SuperResolution):\n\n  def __init__(self, scale, **kwargs):\n    super(MSDR, self).__init__(scale, 3)\n    args = Config(kwargs)\n    args.scale = [2, 3, 4]\n    self.scales = args.scale\n    self.rgb_range = args.rgb_range\n    self.edsr = mdsr.MDSR(args)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    # TODO\n    self.edsr.set_scale(2)\n    sr = self.edsr(inputs[0] * self.rgb_range) / self.rgb_range\n    loss = F.l1_loss(sr, labels[0])\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\'l1\': loss.detach().cpu().numpy()}\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    self.edsr.set_scale(self.scales.index(self.scale))\n    sr = self.edsr(inputs[0] * self.rgb_range) / self.rgb_range\n    sr = sr.cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n    return [sr.numpy()], metrics\n'"
VSR/Backend/Torch/Models/Esrgan.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom . import Discriminator as disc\nfrom .Model import SuperResolution\nfrom .Loss import gan_bce_loss, VggFeatureLoss\nfrom .esrgan import architecture as arch\nfrom ..Util import Metrics\nfrom ..Framework.Summary import get_writer\n\n\nclass ESRGAN(SuperResolution):\n  def __init__(self, scale, patch_size=128, weights=(0.01, 1, 5e-3), **kwargs):\n    super(ESRGAN, self).__init__(scale, 3)\n    self.use_vgg = weights[1] > 0\n    self.use_gan = weights[2] > 0\n    if self.use_gan:\n      self.dnet = disc.DCGAN(3, np.log2(patch_size // 4) * 2, \'bn\')\n      self.optd = torch.optim.Adam(self.trainable_variables(\'dnet\'), 1e-4)\n    self.rrdb = arch.RRDB_Net(upscale=scale, **kwargs)\n    self.optg = torch.optim.Adam(self.trainable_variables(\'rrdb\'), 1e-4)\n    if self.use_vgg:\n      self.vgg = [VggFeatureLoss([\'block5_conv4\'], True)]\n    # image, vgg, gan\n    self.w = weights\n\n  def cuda(self):\n    super(ESRGAN, self).cuda()\n    if self.use_vgg:\n      self.vgg[0].cuda()\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.rrdb(inputs[0])\n    for opt in self.opts.values():\n      if learning_rate:\n        for param_group in opt.param_groups:\n          param_group[""lr""] = learning_rate\n    image_loss = F.l1_loss(sr, labels[0])\n    loss = image_loss * self.w[0]\n    if self.use_vgg:\n      feature_loss = F.l1_loss(self.vgg[0](sr)[0], self.vgg[0](labels[0])[0])\n      loss += feature_loss * self.w[1]\n    if self.use_gan:\n      # update G\n      self.optg.zero_grad()\n      fake = self.dnet(sr)\n      gan_loss_g = gan_bce_loss(fake, True)\n      loss += gan_loss_g * self.w[2]\n      loss.backward()\n      self.optg.step()\n      # update D\n      self.optd.zero_grad()\n      real = self.dnet(labels[0])\n      fake = self.dnet(sr.detach())\n      loss_d = gan_bce_loss(real, True) + gan_bce_loss(fake, False)\n      loss_d.backward()\n      self.optd.step()\n      return {\n        \'loss\': loss.detach().cpu().numpy(),\n        \'image\': image_loss.detach().cpu().numpy(),\n        \'loss_g\': gan_loss_g.detach().cpu().numpy(),\n        \'loss_d\': loss_d.detach().cpu().numpy()\n      }\n    else:\n      self.optg.zero_grad()\n      loss.backward()\n      self.optg.step()\n      return {\n        \'loss\': loss.detach().cpu().numpy(),\n        \'image\': image_loss.detach().cpu().numpy()\n      }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.rrdb(inputs[0]).cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs.get(\'epoch\')\n        writer.image(\'sr\', sr.clamp(0, 1), step=step)\n        writer.image(\'lr\', inputs[0], step=step)\n        writer.image(\'hr\', labels[0], step=step)\n    return [sr.numpy()], metrics\n\n  def export(self, export_dir):\n    device = list(self.rrdb.parameters())[0].device\n    inputs = torch.randn(1, self.channel, 144, 128, device=device)\n    torch.onnx.export(self.rrdb, (inputs,), export_dir / \'rrdb.onnx\')\n'"
VSR/Backend/Torch/Models/Ffdnet.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/27 \xe4\xb8\x8b\xe5\x8d\x885:22\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .Arch import EasyConv2d, SpaceToDepth, Upsample\nfrom .Model import SuperResolution\nfrom ..Framework.Summary import get_writer\nfrom ..Util import Metrics\n\n\nclass Net(nn.Module):\n  def __init__(self, channel, layers, bn, filters=64):\n    super(Net, self).__init__()\n    self.spd = SpaceToDepth(2)\n    body = [EasyConv2d(channel * 4 + 1, filters, 3, activation=\'relu\')]\n    for i in range(1, layers):\n      body.append(EasyConv2d(filters, filters, 3, activation=\'relu\', use_bn=bn))\n    body += [\n      Upsample(filters, 2),\n      EasyConv2d(filters, channel, 3)\n    ]\n    self.body = nn.Sequential(*body)\n\n  def forward(self, x, sigma):\n    x = self.spd(x)\n    sig = torch.ones_like(x)[:, 0:1] * sigma\n    return self.body(torch.cat((x, sig), dim=1))\n\n\nclass FFDNET(SuperResolution):\n  def __init__(self, scale, channel, n_layers, level, training, **kwargs):\n    super(FFDNET, self).__init__(scale, channel)\n    self.ffdnet = Net(channel, n_layers, True)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n    self.level = level / 255\n    self.is_training = training\n\n  def train(self, inputs, labels, learning_rate=None):\n    for opt in self.opts.values():\n      if learning_rate:\n        for param_group in opt.param_groups:\n          param_group[""lr""] = learning_rate\n    lr = inputs[0]\n    sigma = torch.rand(1, device=lr.device) * 75 / 255\n    noise = torch.randn_like(lr) * sigma\n    hr = self.ffdnet((lr + noise).clamp(0, 1), sigma)\n    loss = F.l1_loss(hr, labels[0])\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\n      \'loss\': loss.detach().cpu().numpy()\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    lr = inputs[0]\n    if self.is_training:\n      sigma = torch.rand(1, device=lr.device) * 75 / 255\n      noise = torch.randn_like(lr) * sigma\n    else:\n      sigma = self.level\n      noise = torch.zeros_like(lr)\n    hr = self.ffdnet((lr + noise).clamp(0, 1), sigma).detach().cpu()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(hr, labels[0])\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs.get(\'epoch\')\n        writer.image(\'gt\', labels[0], step=step)\n        writer.image(\'clean\', hr.clamp(0, 1), step=step)\n    return [hr.numpy()], metrics\n'"
VSR/Backend/Torch/Models/Frvsr.py,0,"b'#   Copyright (c): Wenyi Tang 2017-2019.\n#   Author: Wenyi Tang\n#   Email: wenyi.tang@intel.com\n#   Update Date: 4/1/19, 7:13 PM\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .Arch import SpaceToDepth\nfrom .Loss import total_variance\nfrom .Model import SuperResolution\nfrom .frvsr.ops import FNet, SRNet\nfrom .video.motion import STN\nfrom ..Framework.Summary import get_writer\nfrom ..Util import Metrics\nfrom ..Util.Utility import pad_if_divide, upsample\n\n\nclass FRNet(nn.Module):\n  def __init__(self, channel, scale, n_rb):\n    super(FRNet, self).__init__()\n    self.fnet = FNet(channel, gain=32)\n    self.warp = STN(padding_mode=\'border\')\n    self.snet = SRNet(channel, scale, n_rb)\n    self.space_to_depth = SpaceToDepth(scale)\n    self.scale = scale\n\n  def forward(self, lr, last_lr, last_sr):\n    flow = self.fnet(lr, last_lr)\n    flow2 = self.scale * upsample(flow, self.scale)\n    hw = self.warp(last_sr, flow2[:, 0], flow2[:, 1])\n    lw = self.warp(last_lr, flow[:, 0], flow[:, 1])\n    hws = self.space_to_depth(hw)\n    y = self.snet(hws, lr)\n    return y, hw, lw, flow2\n\n\nclass FRVSR(SuperResolution):\n  def __init__(self, scale, channel, **kwargs):\n    super(FRVSR, self).__init__(scale, channel, **kwargs)\n    self.frvsr = FRNet(channel, scale, kwargs.get(\'n_rb\', 10))\n    self.adam = torch.optim.Adam(self.trainable_variables(), 1e-4)\n    self.w = kwargs.get(\'weights\', [1, 1, 1e-3])\n\n  def train(self, inputs, labels, learning_rate=None):\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    labels = [x.squeeze(1) for x in labels[0].split(1, dim=1)]\n    if learning_rate:\n      for param_group in self.adam.param_groups:\n        param_group[""lr""] = learning_rate\n    total_loss = 0\n    flow_loss = 0\n    image_loss = 0\n    last_lr = frames[0]\n    last_sr = upsample(last_lr, self.scale)\n    for lr, hr in zip(frames, labels):\n      sr, hrw, lrw, flow = self.frvsr(lr, last_lr, last_sr.detach())\n      last_lr = lr\n      last_sr = sr\n      l2_image = F.mse_loss(sr, hr)\n      l2_warp = F.mse_loss(lrw, lr)\n      tv_flow = total_variance(flow)\n      loss = l2_image * self.w[0] + l2_warp * self.w[1] + tv_flow * self.w[2]\n      self.adam.zero_grad()\n      loss.backward()\n      self.adam.step()\n      total_loss += loss.detach()\n      image_loss += l2_image.detach()\n      flow_loss += l2_warp.detach()\n    return {\n      \'total_loss\': total_loss.cpu().numpy() / len(frames),\n      \'image_loss\': image_loss.cpu().numpy() / len(frames),\n      \'flow_loss\': flow_loss.cpu().numpy() / len(frames),\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    predicts = []\n    last_lr = pad_if_divide(frames[0], 8, \'reflect\')\n    a = (last_lr.size(2) - frames[0].size(2)) * self.scale\n    b = (last_lr.size(3) - frames[0].size(3)) * self.scale\n    slice_h = slice(None) if a == 0 else slice(a // 2, -a // 2)\n    slice_w = slice(None) if b == 0 else slice(b // 2, -b // 2)\n    last_sr = upsample(last_lr, self.scale)\n    for lr in frames:\n      lr = pad_if_divide(lr, 8, \'reflect\')\n      sr, _, _, _ = self.frvsr(lr, last_lr, last_sr)\n      last_lr = lr.detach()\n      last_sr = sr.detach()\n      sr = sr[..., slice_h, slice_w]\n      predicts.append(sr.cpu().detach().numpy())\n    if labels is not None:\n      labels = [x.squeeze(1) for x in labels[0].split(1, dim=1)]\n      psnr = [Metrics.psnr(x, y) for x, y in zip(predicts, labels)]\n      metrics[\'psnr\'] = np.mean(psnr)\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs[\'epoch\']\n        writer.image(\'clean\', sr.clamp(0, 1), step=step)\n        writer.image(\'label\', labels[-1], step=step)\n    return predicts, metrics\n'"
VSR/Backend/Torch/Models/Loss.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/9 \xe4\xb8\x8b\xe5\x8d\x882:41\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef total_variance(x, dims=(2, 3), reduction=\'mean\'):\n  tot_var = 0\n  reduce = 1\n  for dim in dims:\n    row = x.split(1, dim=dim)\n    reduce *= x.shape[dim]\n    for i in range(len(row) - 1):\n      tot_var += torch.abs(row[i] - row[i + 1]).sum()\n  if reduction != \'mean\':\n    reduce = 1\n  return tot_var / reduce\n\n\ndef focal_l1_loss(x, y, a=2, b=0, c=1, y_is_label=True, focal=\'edge\'):\n  if not y_is_label:\n    x, y = y, x\n  absolute_diff = torch.abs(x - y)\n  if focal == \'edge\':\n    focal = F.pad(y[..., :-1, :-1] - y[..., 1:, 1:], [0, 1, 0, 1])\n  elif focal == \'focal\':\n    focal = absolute_diff\n  focal = torch.abs((focal - focal.mean()) / focal.std())\n  focal = (focal + 1) / 2\n  tuned_diff = torch.pow(focal, a) * absolute_diff\n  loss = b * absolute_diff + c * tuned_diff\n  return loss.mean()\n\n\ndef gan_bce_loss(x, as_real: bool):\n  """"""vanilla GAN binary cross entropy loss""""""\n  if as_real:\n    return F.binary_cross_entropy_with_logits(x, torch.ones_like(x))\n  else:\n    return F.binary_cross_entropy_with_logits(x, torch.zeros_like(x))\n\n\ndef rgan_bce_loss(x, y, x_real_than_y: bool = True):\n  """"""relativistic GAN loss""""""\n  if x_real_than_y:\n    return F.binary_cross_entropy_with_logits(x - y, torch.ones_like(x))\n  else:\n    return F.binary_cross_entropy_with_logits(y - x, torch.ones_like(x))\n\n\ndef ragan_bce_loss(x, y, x_real_than_y: bool = True):\n  """"""relativistic average GAN loss""""""\n  if x_real_than_y:\n    return F.binary_cross_entropy_with_logits(x - y.mean(1, keepdim=True),\n                                              torch.ones_like(x)) + \\\n           F.binary_cross_entropy_with_logits(y - x.mean(1, keepdim=True),\n                                              torch.zeros_like(y))\n  else:\n    return F.binary_cross_entropy_with_logits(y - x.mean(1, keepdim=True),\n                                              torch.ones_like(x)) + \\\n           F.binary_cross_entropy_with_logits(x - y.mean(1, keepdim=True),\n                                              torch.zeros_like(y))\n\n\nclass VggFeatureLoss(nn.Module):\n  # layer name stick to keras model\n  _LAYER_NAME = {\n    \'block1_conv1\': 1,\n    \'block1_conv2\': 3,\n    \'block2_conv1\': 6,\n    \'block2_conv2\': 8,\n    \'block3_conv1\': 11,\n    \'block3_conv2\': 13,\n    \'block3_conv3\': 15,\n    \'block3_conv4\': 17,\n    \'block4_conv1\': 20,\n    \'block4_conv2\': 22,\n    \'block4_conv3\': 24,\n    \'block4_conv4\': 26,\n    \'block5_conv1\': 29,\n    \'block5_conv2\': 31,\n    \'block5_conv3\': 33,\n    \'block5_conv4\': 35,\n  }\n  """"""VGG19 based perceptual loss from ECCV 2016.\n  \n  Args:\n    layer_names: a list of `_LAYER_NAME` strings, specify features to forward.\n    before_relu: forward features before ReLU activation.\n    external_weights: a path to an external vgg weights file, default download\n      from model zoo.\n  """"""\n\n  def __init__(self, layer_names, before_relu=False, external_weights=None):\n    super(VggFeatureLoss, self).__init__()\n    if not external_weights:\n      net = torchvision.models.vgg19(pretrained=True)\n    else:\n      net = torchvision.models.vgg19()\n      # TODO map_location=?\n      net.load_state_dict(torch.load(external_weights))\n    for p in net.parameters():\n      p.requires_grad = False\n    self.childs = nn.Sequential(*net.features.children())\n    self.eval()\n    self.exit_id = [self._LAYER_NAME[n] - int(before_relu) for n in layer_names]\n\n  def normalize(self, x, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n    # see https://pytorch.org/docs/master/torchvision/models.html for details\n    assert x.size(1) == 3, ""wrong channel! must be 3!!""\n    mean = torch.tensor(mean, dtype=torch.float32).view(1, 3, 1, 1)\n    std = torch.tensor(std, dtype=torch.float32).view(1, 3, 1, 1)\n    mean = mean.to(x.device)\n    std = std.to(x.device)\n    return (x - mean) / std\n\n  def forward(self, x):\n    exits = []\n    x = self.normalize(x)\n    for i, fn in enumerate(self.childs.children()):\n      x = fn(x)\n      if i in self.exit_id:\n        exits.append(x)\n      if i >= max(self.exit_id):\n        break\n    return exits\n'"
VSR/Backend/Torch/Models/Mldn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 14\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .Arch import CascadeRdn, Upsample\nfrom .Model import SuperResolution\nfrom ..Framework.Summary import get_writer\nfrom ..Util import Metrics, Utility\n\n\nclass NoiseExtractor(nn.Module):\n  def __init__(self, channel=32, layers=7, bn=False, **kwargs):\n    super(NoiseExtractor, self).__init__()\n    convs = [nn.Conv2d(3, channel, 3, 1, 1), nn.ReLU(True)]\n    if bn:\n      convs.insert(-1, nn.BatchNorm2d(channel))\n    for i in range(1, layers - 1):\n      convs += [nn.Conv2d(channel, channel, 3, 1, 1), nn.ReLU(True)]\n      if bn:\n        convs.insert(-1, nn.BatchNorm2d(channel))\n    convs += [nn.Conv2d(channel, 3, 3, 1, 1)]\n    self.body = nn.Sequential(*convs)\n\n  def forward(self, x):\n    return self.body(x)\n\n\nclass NoiseRemover(nn.Module):\n  def __init__(self, in_channel, up, **kwargs):\n    super(NoiseRemover, self).__init__()\n    entry = nn.Conv2d(in_channel, 64, 3, 1, 1)\n    rdn1 = CascadeRdn(64, 3, True)\n    rdn2 = CascadeRdn(64, 3, True)\n    exits = nn.Conv2d(64, 3, 3, 1, 1)\n    if up:\n      up = Upsample(64, 2)\n      self.body = nn.Sequential(entry, rdn1, rdn2, up, exits)\n    else:\n      self.body = nn.Sequential(entry, rdn1, rdn2, exits)\n\n  def forward(self, x, noise=None):\n    if noise is not None:\n      x = torch.cat([x, noise], dim=1)\n    x = self.body(x)\n    return x\n\n\nclass Mldn(nn.Module):\n  def __init__(self):\n    super(Mldn, self).__init__()\n    self.ne = NoiseExtractor(bn=True)\n    self.sub_x8 = NoiseRemover(6, True)\n    self.sub_x4 = NoiseRemover(6, True)\n    self.sub_x2 = NoiseRemover(6, True)\n    self.main = NoiseRemover(6, False)\n\n  def forward(self, x, x2, x4, x8):\n    noise = self.ne(x8)\n    up4 = self.sub_x8(x8, noise)\n    up2 = self.sub_x4(x4, up4)\n    up1 = self.sub_x2(x2, up2)\n    clean = self.main(x, up1)\n    return clean, up1, up2, up4, noise\n\n\nclass MLDN(SuperResolution):\n\n  def __init__(self, finetune=False, **kwargs):\n    super(MLDN, self).__init__(scale=1, channel=3)\n    self.drn = Mldn()\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n    self.finetune = finetune\n\n  def train(self, inputs, labels, learning_rate=None):\n    x0 = inputs[0]\n    x0 = Utility.pad_if_divide(x0, 8, \'reflect\')\n    if not self.finetune:\n      stddev = torch.rand(3) * 75 / 255\n      stddev = stddev.reshape([1, 3, 1, 1])\n      noise_map = torch.randn(*x0.shape) * stddev\n      noise_map = noise_map.to(x0.device)\n      x0 += noise_map\n      x0 = torch.clamp(x0, 0, 1)\n    x8 = F.interpolate(x0, scale_factor=1 / 8, mode=\'bilinear\')\n    x4 = F.interpolate(x0, scale_factor=1 / 4, mode=\'bilinear\')\n    x2 = F.interpolate(x0, scale_factor=1 / 2, mode=\'bilinear\')\n    label0 = labels[0]\n    label2 = F.interpolate(label0, scale_factor=1 / 2, mode=\'bilinear\')\n    label4 = F.interpolate(label0, scale_factor=1 / 4, mode=\'bilinear\')\n    label8 = F.interpolate(label0, scale_factor=1 / 8, mode=\'bilinear\')\n\n    clean, sub1, sub2, sub4, noise = self.drn(x0, x2, x4, x8)\n\n    l1_clean = F.l1_loss(clean, label0)\n    l1_sub1 = F.l1_loss(sub1, label0)\n    l1_sub2 = F.l1_loss(sub2, label2)\n    l1_sub4 = F.l1_loss(sub4, label4)\n    l2_noise = F.mse_loss(noise, x8 - label8)\n\n    loss = l1_sub4 + l2_noise + l1_sub2 + l1_sub1 + l1_clean\n\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\n      \'noise\': l2_noise.detach().cpu().numpy(),\n      \'x8\': l1_sub4.detach().cpu().numpy(),\n      \'x4\': l1_sub2.detach().cpu().numpy(),\n      \'x2\': l1_sub1.detach().cpu().numpy(),\n      \'x0\': l1_clean.detach().cpu().numpy(),\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    x0 = inputs[0]\n    x0 = Utility.pad_if_divide(x0, 8, \'reflect\')\n    x8 = F.interpolate(x0, scale_factor=1 / 8, mode=\'bilinear\')\n    x4 = F.interpolate(x0, scale_factor=1 / 4, mode=\'bilinear\')\n    x2 = F.interpolate(x0, scale_factor=1 / 2, mode=\'bilinear\')\n    if labels is not None:\n      label0 = labels[0]\n      label2 = F.interpolate(label0, scale_factor=1 / 2, mode=\'bilinear\')\n      label4 = F.interpolate(label0, scale_factor=1 / 4, mode=\'bilinear\')\n      label8 = F.interpolate(label0, scale_factor=1 / 8, mode=\'bilinear\')\n    outputs = self.drn(x0, x2, x4, x8)\n    clean = outputs[0].detach().cpu()\n    sub1 = outputs[1].detach().cpu()\n    sub2 = outputs[2].detach().cpu()\n    sub4 = outputs[3].detach().cpu()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(clean.numpy(), label0.cpu().numpy())\n      writer = get_writer(self.name)\n      if writer is not None:\n        writer.image(\'clean\', clean)\n        writer.image(\'up2\', sub1)\n        writer.image(\'up4\', sub2)\n        writer.image(\'up8\', sub4)\n    return [clean.numpy(), sub1.numpy(), sub2.numpy(), sub4.numpy()], metrics\n'"
VSR/Backend/Torch/Models/Model.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:10\n\nimport torch\nimport logging\n\nfrom ..Framework.Trainer import SRTrainer\n\n\nclass BasicModel:\n  """"""Trainable model wrapper for PyTorch nn.Module objects\n\n  There are 2 built-in attributes:\n    - modules: contains a K-V pair of `str: nn.Module`. It will be automatically\n      appended if a derived object assign any attribute with `nn.Module` object.\n    - opts: contains a K-V pair of `str: optim.Optimizer`. Will be automatically\n      appended if a derived object assign any attribute with `optim.Optimizer`.\n  """"""\n\n  def __init__(self, **kwargs):\n    self.modules = {}\n    self.opts = {}\n    self.name = \'\'\n    self._trainer = None\n\n  def __setattr__(self, key, value):\n    if key in (\'modules\', \'opts\',):\n      if hasattr(self, key):\n        raise ValueError(f""Can\'t overwrite built-in \'{key}\' of BasicModel"")\n    if isinstance(value, torch.nn.Module):\n      if key in self.modules:\n        if self.modules[key] is value:\n          return\n        else:\n          # TODO: why assign twice??\n          raise NotImplementedError\n      else:\n        self.modules[key] = value\n        self.name += f\'[{key}]\'\n    if isinstance(value, torch.optim.Optimizer):\n      if key in self.opts:\n        if self.opts[key] is value:\n          return\n        else:\n          raise NotImplementedError\n      else:\n        self.opts[key] = value\n\n    return super(BasicModel, self).__setattr__(key, value)\n\n  def trainable_variables(self, name=None):\n    """"""Return variables who require gradients.\n\n    Args:\n      name: module name. Will return all trainable variables if no name given.\n    """"""\n\n    _m = [self.modules.get(name)] if name else self.modules.values()\n    _var = []\n    for i in _m:\n      _var += filter(lambda p: p.requires_grad, i.parameters())\n    return _var\n\n  def to_train(self):\n    """"""Change modules to train mode.""""""\n    for _m in self.modules.values():\n      _m.train()\n\n  def train(self, *args, **kwargs):\n    """"""Forward and backward data path.\n      The trainer knows data pipeline through this callback.""""""\n    raise NotImplementedError\n\n  def to_eval(self):\n    """"""Change modules to evaluate mode.""""""\n    for _m in self.modules.values():\n      _m.eval()\n\n  def eval(self, *args, **kwargs):\n    """"""Forward data path. No backward needed for this is only for testing.""""""\n    raise NotImplementedError\n\n  def display(self):\n    """"""Show model info""""""\n    num_params = 0\n    for m in self.modules.values():\n      for p in m.parameters():\n        num_params += p.nelement()\n    logging.getLogger(\'VSR\').info(f""Total params: {num_params}"")\n\n  def cuda(self):\n    """"""Move model to cuda device.""""""\n    for i in self.modules:\n      if torch.cuda.is_available():\n        self.modules[i] = self.modules[i].cuda()\n\n  def export(self, export_dir):\n    """"""export ONNX model.\n\n    Args:\n      export_dir: path to save onnx files.\n    """"""\n\n    raise NotImplementedError(""Should implement in specific model!"")\n\n  @property\n  def executor(self):\n    """"""Return the trainer class type for this model.""""""\n    return self.get_executor(None)\n\n  def get_executor(self, root):\n    if issubclass(self._trainer.__class__, type):\n      self._trainer = self._trainer(self, root)\n      return self._trainer\n    else:\n      return self._trainer\n\n  def load(self, pth, map_location=None):\n    for key, model in self.modules.items():\n      if not isinstance(pth, dict):\n        model.load_state_dict(torch.load(str(pth), map_location=map_location))\n        break\n      model.load_state_dict(\n          torch.load(str(pth[key]), map_location=map_location))\n    for key, opt in self.opts.items():\n      if isinstance(pth, dict):\n        opt.load_state_dict(\n            torch.load(str(pth[key]), map_location=map_location))\n\n\nclass SuperResolution(BasicModel):\n  """"""A default model for (video) super-resolution""""""\n\n  def __init__(self, scale, channel, **kwargs):\n    super(SuperResolution, self).__init__(**kwargs)\n    self.scale = scale\n    self.channel = channel\n    # Default SR trainer\n    self._trainer = SRTrainer\n'"
VSR/Backend/Torch/Models/Msrn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .Model import SuperResolution\nfrom .msrn import msrn\nfrom ..Util import Metrics\nfrom VSR.Util.Config import Config\n\n\nclass MSRN(SuperResolution):\n\n  def __init__(self, scale, **kwargs):\n    super(MSRN, self).__init__(scale, 3)\n    args = Config(kwargs)\n    args.scale = [scale]\n    self.rgb_range = args.rgb_range\n    self.msrn = msrn.MSRN(args)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.msrn(inputs[0] * self.rgb_range) / self.rgb_range\n    loss = F.l1_loss(sr, labels[0])\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\'l1\': loss.detach().cpu().numpy()}\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.msrn(inputs[0] * self.rgb_range) / self.rgb_range\n    sr = sr.cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n    return [sr.numpy()], metrics\n'"
VSR/Backend/Torch/Models/NTIRE19.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/16\n\nimport torch\nimport torch.nn.functional as F\n\nfrom VSR.Util.Config import Config\nfrom .Model import SuperResolution\nfrom .ntire19 import denoise, edrn, frn, ran2\nfrom ..Util import Metrics, Utility\n\n\nclass L1Optimizer(SuperResolution):\n  def __init__(self, channel, scale=1):\n    super(L1Optimizer, self).__init__(scale, channel)\n\n  def fn(self, x):\n    raise NotImplementedError\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.fn(inputs[0])\n    loss = F.l1_loss(sr, labels[0])\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\'l1\': loss.detach().cpu().numpy()}\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    _lr = inputs[0]\n    lr = Utility.pad_if_divide(_lr, 32)\n    a = lr.size(2) - _lr.size(2)\n    b = lr.size(3) - _lr.size(3)\n    slice_h = slice(None) if a == 0 else slice(a // 2, -a // 2)\n    slice_w = slice(None) if b == 0 else slice(b // 2, -b // 2)\n    sr = self.fn(lr)[..., slice_h, slice_w]\n    sr = sr.cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n    return [sr.numpy()], metrics\n\n\nclass EDRN(L1Optimizer):\n  """"""EDRN is one candidate of NTIRE19 RSR""""""\n\n  def __init__(self, scale, channel, **kwargs):\n    super(EDRN, self).__init__(channel=channel, scale=scale)\n    args = Config(kwargs)\n    args.scale = [scale]\n    args.n_colors = channel\n    self.rgb_range = args.rgb_range\n    self.edrn = edrn.EDRN(args)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def fn(self, x):\n    return self.edrn(x * self.rgb_range) / self.rgb_range\n\n\nclass FRN(L1Optimizer):\n  def __init__(self, scale, channel, **kwargs):\n    super(FRN, self).__init__(channel=channel, scale=scale)\n    args = Config(kwargs)\n    args.scale = [scale]\n    args.n_colors = channel\n    self.rgb_range = args.rgb_range\n    self.frn = frn.FRN_UPDOWN(args)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def fn(self, x):\n    return self.frn(x * self.rgb_range) / self.rgb_range\n\n\nclass RAN(L1Optimizer):\n  def __init__(self, scale, channel, **kwargs):\n    super(RAN, self).__init__(channel=channel, scale=scale)\n    args = Config(kwargs)\n    args.scale = [scale]\n    args.n_colors = channel\n    self.rgb_range = args.rgb_range\n    self.ran = ran2.RAN(args)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def fn(self, x):\n    return self.ran(x * self.rgb_range) / self.rgb_range\n\n\nclass DIDN(L1Optimizer):\n  def __init__(self, channel, filters, umodule, **kwargs):\n    super(DIDN, self).__init__(channel=channel)\n    self.didn = denoise.EraserTeam.DIDN(channel, filters, umodule)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def fn(self, x):\n    return self.didn(x)\n\n\nclass DHDN(L1Optimizer):\n  def __init__(self, channel, filters, **kwargs):\n    super(DHDN, self).__init__(channel=channel)\n    self.dhdn = denoise.EraserTeam.DHDN(channel, filters)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def fn(self, x):\n    return self.dhdn(x)\n\n\nclass GRDN(L1Optimizer):\n  def __init__(self, channel, filters, grdb, rdb, **kwargs):\n    super(GRDN, self).__init__(channel=channel)\n    self.grdn = denoise.DGUTeam.GRDN(channel, filters, grdb, rdb)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def fn(self, x):\n    return self.grdn(x)\n\n\nclass ResUNet(L1Optimizer):\n  def __init__(self, channel, filters, rb, **kwargs):\n    super(ResUNet, self).__init__(channel=channel)\n    self.resunet = denoise.HITVPCTeam.ResUNet(channel, filters, rb)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def fn(self, x):\n    return self.resunet(x)\n'"
VSR/Backend/Torch/Models/NTIRE20.py,0,b''
VSR/Backend/Torch/Models/Qprn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/8 \xe4\xb8\x8b\xe5\x8d\x882:43\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nfrom torch import nn\n\nfrom .Arch import CascadeRdn, Rdb, SpaceToDepth, Upsample\nfrom .Crdn import Upsample as RsrUp\nfrom .Discriminator import DCGAN\nfrom .Loss import gan_bce_loss, total_variance\nfrom .Model import SuperResolution\nfrom .video.motion import STTN\nfrom ..Framework.Summary import get_writer\nfrom ..Framework.Trainer import SRTrainer, from_tensor, to_tensor\nfrom ..Util import Metrics\nfrom ..Util.Utility import pad_if_divide\n\n\nclass Fnet(nn.Module):\n  def __init__(self, channel, L=2, gain=64):\n    super(Fnet, self).__init__()\n    self.lq_entry = nn.Sequential(\n      nn.Conv2d(channel * (L + 1), 16, 3, 1, 1),\n      SpaceToDepth(4),\n      nn.Conv2d(256, 64, 1, 1, 0),\n      Rdb(64), Rdb(64))\n    self.hq_entry = nn.Sequential(\n      nn.Conv2d(channel * L, 16, 3, 1, 1),\n      SpaceToDepth(4),\n      nn.Conv2d(256, 64, 1, 1, 0),\n      Rdb(64), Rdb(64))\n    self.flownet = nn.Sequential(\n      nn.Conv2d(128, 64, 1, 1, 0),\n      Rdb(64), Rdb(64), Upsample(64, 4),\n      nn.Conv2d(64, 3, 3, 1, 1), nn.Tanh())\n    gain = torch.as_tensor([L, gain, gain], dtype=torch.float32)\n    self.gain = gain.reshape(1, 3, 1, 1)\n\n  def forward(self, lq, hq):\n    x = torch.cat(lq, dim=1)\n    y = torch.cat(hq, dim=1)\n    x = self.lq_entry(x)\n    y = self.hq_entry(y)\n    z = torch.cat([x, y], dim=1)\n    flow = self.flownet(z)\n    gain = self.gain.to(flow.device)\n    return flow * gain\n\n\nclass Unet(nn.Module):\n  def __init__(self, channel, N=2):\n    super(Unet, self).__init__()\n    self.entry = nn.Sequential(\n      nn.Conv2d(channel * N, 32, 3, 1, 1),\n      SpaceToDepth(2),\n      nn.Conv2d(128, 32, 1, 1, 0))\n    self.exit = nn.Sequential(\n      Upsample(32, 2), nn.Conv2d(32, channel, 3, 1, 1))\n    self.down1 = nn.Conv2d(32, 64, 3, 2, 1)\n    self.up1 = RsrUp([64, 32])\n    self.cb = CascadeRdn(64, 3, True)\n\n  def forward(self, *inputs):\n    inp = torch.cat(inputs, dim=1)  # w\n    c0 = self.entry(inp)  # w / 2\n    c1 = self.down1(c0)  # w / 4\n    x = self.cb(c1)  # w / 4\n    c2 = self.up1(x, c0)\n    out = self.exit(c2)\n    return out\n\n\nclass Composer(nn.Module):\n  def __init__(self, channel, L=2, gain=64):\n    super(Composer, self).__init__()\n    self.flownet = Fnet(channel, L, gain=gain)\n    self.refiner = Unet(channel, N=2)\n    self.warpper = STTN((0, 2, 1, 3, 4), padding_mode=\'border\')\n    self.L = L\n\n  def forward(self, lq, hq):\n    """"""\n    :param lq: [lq_{i+1}, lq_i, lq_{i-1}]\n    :param hq: [hq_i, hq_{i-1}]\n    :return: hq_{i+1}\n    """"""\n    assert isinstance(lq, list) and isinstance(hq, list)\n    assert len(lq) == self.L + 1 and len(hq) == self.L\n    flow = self.flownet(lq, hq)\n    d, u, v = [t.squeeze(1) for t in flow.split(1, dim=1)]\n    warp = self.warpper(torch.stack(hq, dim=1), d, u, v)\n    warp = warp.squeeze(1)\n    y = self.refiner(warp, lq[0])\n    return y, warp, (d, u, v)\n\n\nclass QPRN(SuperResolution):\n  """"""QP-based video restoration network""""""\n\n  def __init__(self, gain, scale, channel, **kwargs):\n    super(QPRN, self).__init__(scale, channel, **kwargs)\n    self.debug = kwargs.get(\'debug\', {})\n    # image, flow, tv, history, gan\n    self.w = kwargs.get(\'weights\', [1, 10, 1.0e-4, 0.1, 5e-3])\n    self.qprn = Composer(channel, L=2, gain=gain)\n    self.adam = torch.optim.Adam(self.trainable_variables(\'qprn\'), 1e-4)\n    if self.debug.gan:\n      self.dnet = DCGAN(channel * 4, 9, \'bn\', \'A\')\n      self.adam_d = torch.optim.Adam(self.trainable_variables(\'dnet\'), 1e-4)\n    self._trainer = _Trainer\n\n  def train(self, inputs, labels, learning_rate=None):\n    metrics = {}\n    frames = list(torch.split(inputs[0], 1, dim=1))\n    labels = list(torch.split(labels[0], 1, dim=1))\n    total_loss = 0\n    flow_loss = 0\n    image_loss = 0\n    his_loss = 0\n    for opt in self.opts.values():\n      if learning_rate:\n        for param_group in opt.param_groups:\n          param_group[""lr""] = learning_rate\n    self.adam.zero_grad()\n    if self.debug.reverse:\n      frames.reverse()\n      labels.reverse()\n    # # time extension\n    # frames_rev, labels_rev = frames.copy(), labels.copy()\n    # frames_rev.reverse()\n    # labels_rev.reverse()\n    # rev_windows = {\'lq\': [], \'hq\': [], \'label\': []}\n    # warps = []\n    # for lq, label in zip(frames_rev, labels_rev):\n    #   lq = lq.squeeze(1)\n    #   label = label.squeeze(1)\n    #   if not rev_windows[\'lq\']:\n    #     rev_windows[\'lq\'] = [lq.detach(), lq.detach(), lq.detach()]\n    #     rev_windows[\'hq\'] = [lq.detach(), lq.detach()]\n    #     rev_windows[\'label\'] = [label.detach(), label.detach(), label.detach()]\n    #   rev_windows[\'lq\'].pop(-1)\n    #   rev_windows[\'lq\'].insert(0, lq.detach())\n    #   rev_windows[\'label\'].pop(-1)\n    #   rev_windows[\'label\'].insert(0, label.detach())\n    #   hq, hq_warp, _ = self.qprn(rev_windows[\'lq\'], rev_windows[\'hq\'])\n    #   warps.append(hq_warp)\n    #   rev_windows[\'hq\'].pop(-1)\n    #   rev_windows[\'hq\'].insert(0, hq.detach())\n    # warps.reverse()\n    idr_lq = frames[0].squeeze(1)\n    idr_hq = labels[0].squeeze(1)\n    idr = self.qprn.refiner(idr_lq, idr_lq)\n    length = self.qprn.L + 1\n    windows = {\n      \'lq\': [idr_lq.detach() for _ in range(length)],\n      \'hq\': [idr.detach() for _ in range(1, length)],\n      \'label\': [idr_hq.detach() for _ in range(length)]\n    }\n    history = [idr.detach()]\n    history_hq = [idr_hq.detach()]\n    for lq, label in zip(frames[1:], labels[1:]):\n      lq = lq.squeeze(1)\n      label = label.squeeze(1)\n      windows[\'lq\'].pop(-1)\n      windows[\'lq\'].insert(0, lq.detach())\n      windows[\'label\'].pop(-1)\n      windows[\'label\'].insert(0, label.detach())\n      hq, hq_warp, flow = self.qprn(windows[\'lq\'], windows[\'hq\'])\n      lq_to_warp = windows[\'lq\'][1:]\n      lb_to_warp = windows[\'label\'][1:]\n      windows[\'hq\'].pop(-1)\n      windows[\'hq\'].insert(0, hq.detach())\n      lq_warp = self.qprn.warpper(torch.stack(lq_to_warp, dim=1), *flow)\n      lq_warp = lq_warp.squeeze(1)\n      lb_warp = self.qprn.warpper(torch.stack(lb_to_warp, dim=1), *flow)\n      lb_warp = lb_warp.squeeze(1)\n\n      history.append(hq.detach())\n      history_hq.append(label.detach())\n      l2_his = 0\n      for his, his_hq in zip(history[:-1], history_hq[:-1]):\n        his_to_warp = [his.detach() for _ in range(1, length)]\n        his_flow = self.qprn.flownet(his_to_warp + [hq.detach()], his_to_warp)\n        his_flow = [t.squeeze(1) for t in his_flow.split(1, dim=1)]\n        his_warp = self.qprn.warpper(torch.stack(his_to_warp, dim=1), *his_flow)\n        his_warp = his_warp.squeeze(1)\n        his_lb_to_warp = [his_hq.detach() for _ in range(1, length)]\n        his_lb_warp = self.qprn.warpper(torch.stack(his_lb_to_warp, dim=1),\n                                        *his_flow).squeeze(1)\n        his_mask = torch.exp(-50.0 * (his_lb_warp - label) ** 2)\n        l2_his += torch.mean(his_mask * (his_warp - hq) ** 2)\n\n      l1_image = F.l1_loss(hq, label)\n      l2_warp = 0.8 * F.mse_loss(lb_warp, label) + 0.2 * F.mse_loss(lq_warp, lq)\n      tv_flow = total_variance(torch.stack(flow, dim=1), dims=(2, 3))\n      loss = self.w[0] * l1_image + self.w[1] * l2_warp + self.w[2] * tv_flow\n      loss += self.w[3] * l2_his\n      if self.debug.gan:\n        fake = self.dnet(torch.cat((hq, hq_warp.detach(),\n                                    lq, lq_warp.detach()), dim=1))\n        gloss = gan_bce_loss(fake, True)\n        loss += gloss * self.w[4]\n        metrics[\'g\'] = gloss.detach().cpu().numpy()\n      self.adam.zero_grad()\n      loss.backward()\n      self.adam.step()\n      total_loss += loss.detach()\n      image_loss += l1_image.detach()\n      flow_loss += l2_warp.detach()\n      his_loss += l2_his.detach()\n      if self.debug.gan:\n        fake = self.dnet(torch.cat((hq.detach(), hq_warp.detach(),\n                                    lq, lq_warp.detach()), dim=1))\n        real = self.dnet(torch.cat((label, lb_warp.detach(),\n                                    lq, lq_warp.detach()), dim=1))\n        d_loss = gan_bce_loss(fake, False) + gan_bce_loss(real, True)\n        self.adam_d.zero_grad()\n        d_loss.backward()\n        self.adam_d.step()\n        metrics[\'d\'] = d_loss.detach().cpu().numpy()\n    metrics.update({\n      \'total_loss\': total_loss.detach().cpu().numpy() / len(frames),\n      \'image_loss\': image_loss.detach().cpu().numpy() / len(frames),\n      \'flow_loss\': flow_loss.detach().cpu().numpy() / len(frames),\n      \'his_loss\': his_loss.detach().cpu().numpy() / len(frames),\n    })\n    return metrics\n\n  def eval(self, inputs, labels=None, **kwargs):\n    frames = torch.split(inputs[0], 1, dim=1)\n    metrics = {}\n    idr_lq_ = frames[0].squeeze(1)\n    idr_lq = pad_if_divide(idr_lq_, 4, \'reflect\')\n    a = idr_lq.shape[-2] - idr_lq_.shape[-2]\n    c = idr_lq.shape[-1] - idr_lq_.shape[-1]\n    a, b = a // 2, -a // 2\n    c, d = c // 2, -c // 2\n    if a == 0: a = b = None\n    if c == 0: c = d = None\n    idr = self.qprn.refiner(idr_lq, idr_lq)\n    length = self.qprn.L + 1\n    windows = {\n      \'lq\': [idr_lq.detach() for _ in range(length)],\n      \'hq\': [idr.detach() for _ in range(1, length)],\n      \'predict\': [idr.detach().cpu().numpy()[..., a:b, c:d]]\n    }\n    time_loss = 0\n    for lq_ in frames[1:]:\n      lq_ = lq_.squeeze(1)\n      lq = pad_if_divide(lq_, 4, \'reflect\')\n      windows[\'lq\'].pop(-1)\n      windows[\'lq\'].insert(0, lq.detach())\n      hq, hq_warp, flow = self.qprn(windows[\'lq\'], windows[\'hq\'])\n      windows[\'hq\'].pop(-1)\n      windows[\'hq\'].insert(0, hq.detach())\n      lq_to_warp = windows[\'lq\'][1:]\n      lq_warp = self.qprn.warpper(torch.stack(lq_to_warp, dim=1), *flow)\n      lq_warp = lq_warp.squeeze(1)\n      if self.debug.get(\'see_warp\'):\n        windows[\'predict\'].append(hq_warp.detach().cpu().numpy()[..., a:b, c:d])\n      elif self.debug.get(\'see_flow\'):\n        windows[\'predict\'].append(torch.stack(\n          flow[1:], dim=1).detach().cpu().numpy()[..., a:b, c:d])\n      else:\n        windows[\'predict\'].append(hq.detach().cpu().numpy()[..., a:b, c:d])\n      time_loss += F.mse_loss(hq, hq_warp).detach()\n    if labels is not None:\n      targets = torch.split(labels[0], 1, dim=1)\n      targets = [t.squeeze(1) for t in targets]\n      psnr = [Metrics.psnr(x, y) for x, y in zip(windows[\'predict\'], targets)]\n      metrics[\'psnr\'] = np.mean(psnr)\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs[\'epoch\']\n        writer.image(\'hq\', hq.clamp(0, 1), step=step)\n        writer.image(\'lq\', lq.clamp(0, 1), step=step)\n        writer.image(\'idr\', idr.clamp(0, 1), step=step)\n        writer.image(\'warp\', lq_warp.clamp(0, 1), step=step)\n    metrics[\'time_loss\'] = time_loss.detach().cpu().numpy() / len(frames)\n    return windows[\'predict\'], metrics\n\n\nclass _Trainer(SRTrainer):\n  jitter = torchvision.transforms.Compose([\n    torchvision.transforms.ToPILImage(\'RGB\'),\n    torchvision.transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n    torchvision.transforms.ToTensor()\n  ])\n  trans = torchvision.transforms.Compose([\n    torchvision.transforms.ToPILImage(\'RGB\'),\n    torchvision.transforms.RandomAffine(0, [0.02, 0.02]),\n    torchvision.transforms.ToTensor()\n  ])\n\n  def random_apply_fn(self, tensor, fn, prob=0.5):\n    assert isinstance(tensor, torch.Tensor) and callable(fn)\n    tensor_ = []\n    for t in tensor.cpu():\n      # enum batch\n      p = np.random.rand()\n      if p < prob:\n        t = [fn(x.squeeze(0)) for x in t.split(1, dim=0)]\n        t = torch.stack(t, dim=0)\n      tensor_.append(t)\n    tensor_ = torch.stack(tensor_, dim=0)\n    if self.v.cuda and torch.cuda.is_available():\n      tensor_ = tensor_.cuda()\n    assert tensor_.shape == tensor.shape\n    return tensor_\n\n  def fn_train_each_step(self, label=None, feature=None, name=None, post=None):\n    v = self.v\n    for fn in v.feature_callbacks:\n      feature = fn(feature, name=name)\n    for fn in v.label_callbacks:\n      label = fn(label, name=name)\n    feature = to_tensor(feature, v.cuda)\n    # Inline data augmentation\n    # feature = self.random_apply_fn(feature, self.jitter, 0.3)\n    # feature = self.random_apply_fn(feature, self.trans, 0.3)\n    label = to_tensor(label, v.cuda)\n    loss = self.model.train([feature], [label], v.lr)\n    for _k, _v in loss.items():\n      v.avg_meas[_k] = \\\n        v.avg_meas[_k] + [_v] if v.avg_meas.get(_k) else [_v]\n      loss[_k] = \'{:08.5f}\'.format(_v)\n    v.loss = loss\n\n  def fn_benchmark_each_step(self, label=None, feature=None, name=None,\n                             post=None):\n    v = self.v\n    origin_feat = feature\n    for fn in v.feature_callbacks:\n      feature = fn(feature, name=name)\n    for fn in v.label_callbacks:\n      label = fn(label, name=name)\n    feature = to_tensor(feature, v.cuda)\n    label = to_tensor(label, v.cuda)\n    with torch.set_grad_enabled(False):\n      outputs, metrics = self.model.eval([feature], [label], epoch=v.epoch)\n    for _k, _v in metrics.items():\n      if _k not in v.mean_metrics:\n        v.mean_metrics[_k] = []\n      v.mean_metrics[_k] += [_v]\n    outputs = [from_tensor(x) for x in outputs]\n    for fn in v.output_callbacks:\n      outputs = fn(outputs, input=origin_feat, label=label, name=name,\n                   mode=v.color_format, subdir=v.subdir)\n'"
VSR/Backend/Torch/Models/Rbpn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/26 \xe4\xb8\x8b\xe5\x8d\x883:24\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .Loss import total_variance\nfrom .Model import SuperResolution\nfrom .frvsr.ops import FNet\nfrom .rbpn.ops import Rbpn\nfrom .video.motion import STN\nfrom ..Framework.Summary import get_writer\nfrom ..Util.Metrics import psnr\nfrom ..Util.Utility import pad_if_divide, upsample\n\n\nclass Composer(nn.Module):\n  def __init__(self, **kwargs):\n    super(Composer, self).__init__()\n    self.module = Rbpn(**kwargs)\n    self.fnet = FNet(kwargs[\'num_channels\'])\n    self.warper = STN(padding_mode=\'border\')\n\n  def forward(self, target, neighbors):\n    flows = []\n    warps = []\n    for i in neighbors:\n      flow = self.fnet(target, i)\n      warp = self.warper(i, flow[:, 0], flow[:, 1])\n      flows.append(flow)\n      warps.append(warp)\n    sr = self.module(target, neighbors, flows)\n    return sr, flows, warps\n\n\nclass RBPN(SuperResolution):\n  def __init__(self, scale, channel, depth, residual, **kwargs):\n    super(RBPN, self).__init__(scale, channel, **kwargs)\n    self.depth = depth\n    self.res = residual\n    self.w = kwargs.get(\'weights\', [1, 1e-4])\n    ops = {\n      \'num_channels\': channel,\n      \'scale_factor\': scale,\n      \'base_filter\': kwargs.get(\'base_filter\', 256),\n      \'feat\': kwargs.get(\'feat\', 64),\n      \'num_stages\': kwargs.get(\'num_stages\', 3),\n      \'n_resblock\': kwargs.get(\'n_resblock\', 5),\n      \'nFrames\': depth\n    }\n    self.rbpn = Composer(**ops)\n    self.adam = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    labels = [x.squeeze(1) for x in labels[0].split(1, dim=1)]\n    for opt in self.opts.values():\n      if learning_rate:\n        for param_group in opt.param_groups:\n          param_group[""lr""] = learning_rate\n    target = frames.pop(self.depth // 2)\n    neighbors = frames\n    sr, flows, warps = self.rbpn(target, neighbors)\n    if self.res:\n      sr = sr + upsample(target, self.scale)\n\n    image_loss = F.l1_loss(sr, labels[self.depth // 2])\n    warp_loss = [F.l1_loss(w, target) for w in warps]\n    tv_loss = [total_variance(f) for f in flows]\n    flow_loss = torch.stack(warp_loss).sum() * self.w[0] + \\\n                torch.stack(tv_loss).sum() * self.w[1]\n    loss = image_loss + flow_loss\n    self.adam.zero_grad()\n    loss.backward()\n    self.adam.step()\n    return {\n      \'flow\': flow_loss.detach().cpu().numpy(),\n      \'image\': image_loss.detach().cpu().numpy(),\n      \'total\': loss.detach().cpu().numpy()\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    _frames = [pad_if_divide(x, 8, \'reflect\') for x in frames]\n    target = _frames.pop(self.depth // 2)\n    neighbors = _frames\n    a = (target.size(2) - frames[0].size(2)) * self.scale\n    b = (target.size(3) - frames[0].size(3)) * self.scale\n    slice_h = slice(None) if a == 0 else slice(a // 2, -a // 2)\n    slice_w = slice(None) if b == 0 else slice(b // 2, -b // 2)\n    sr, _, warps = self.rbpn(target, neighbors)\n    if self.res:\n      sr = sr + upsample(target, self.scale)\n    sr = sr[..., slice_h, slice_w].detach().cpu()\n    if labels is not None:\n      labels = [x.squeeze(1) for x in labels[0].split(1, dim=1)]\n      gt = labels[self.depth // 2]\n      metrics[\'psnr\'] = psnr(sr, gt)\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs[\'epoch\']\n        writer.image(\'hr\', gt, step=step)\n        writer.image(\'sr\', sr.clamp(0, 1), step=step)\n        writer.image(\'warp/0\', warps[0].clamp(0, 1), step=step)\n        writer.image(\'warp/1\', warps[1].clamp(0, 1), step=step)\n    return [sr.numpy()], metrics\n'"
VSR/Backend/Torch/Models/Rcan.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .Model import SuperResolution\nfrom .rcan import rcan\nfrom ..Util import Metrics\nfrom VSR.Util.Config import Config\n\n\nclass RCAN(SuperResolution):\n\n  def __init__(self, scale, **kwargs):\n    super(RCAN, self).__init__(scale, 3)\n    args = Config(kwargs)\n    args.scale = [scale]\n    self.rgb_range = args.rgb_range\n    self.rcan = rcan.RCAN(args)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n\n  def train(self, inputs, labels, learning_rate=None):\n    sr = self.rcan(inputs[0] * self.rgb_range) / self.rgb_range\n    loss = F.l1_loss(sr, labels[0])\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\'l1\': loss.detach().cpu().numpy()}\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.rcan(inputs[0] * self.rgb_range) / self.rgb_range\n    sr = sr.cpu().detach()\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n    return [sr.numpy()], metrics\n'"
VSR/Backend/Torch/Models/SRFeat.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/27 \xe4\xb8\x8b\xe5\x8d\x8811:26\n\n#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom . import Discriminator as disc\nfrom .Loss import VggFeatureLoss, gan_bce_loss\nfrom .Model import SuperResolution\nfrom .srfeat import ops\nfrom ..Framework.Summary import get_writer\nfrom ..Util import Metrics\n\n\nclass SRFEAT(SuperResolution):\n  def __init__(self, channel, scale, patch_size=64, weights=(1, 0.01, 0.01),\n               **kwargs):\n    super(SRFEAT, self).__init__(scale, channel)\n    n_rb = kwargs.get(\'num_residualblocks\', 16)\n    f = kwargs.get(\'filters\', 64)\n    self.use_gan = weights[1] > 0\n    self.use_feat_gan = weights[2] > 0\n    self.srfeat = ops.Generator(channel, scale, f, n_rb)\n    self.gopt = torch.optim.Adam(self.trainable_variables(\'srfeat\'), 1e-4)\n    if self.use_gan:\n      # vanilla image\n      self.dnet1 = disc.DCGAN(channel, np.log2(patch_size // 4) * 2, \'bn\')\n      self.dopt1 = torch.optim.Adam(self.trainable_variables(\'dnet1\'), 1e-4)\n    if self.use_feat_gan:\n      # vgg feature\n      self.dnet2 = disc.DCGAN(256, np.log2(patch_size // 16) * 2, \'bn\')\n      self.dopt2 = torch.optim.Adam(self.trainable_variables(\'dnet2\'), 1e-4)\n    self.vgg = [VggFeatureLoss([\'block3_conv1\'], True)]\n    self.w = weights\n\n  def cuda(self):\n    super(SRFEAT, self).cuda()\n    self.vgg[0].cuda()\n\n  def train(self, inputs, labels, learning_rate=None):\n    metrics = {}\n    sr = self.srfeat(self.norm(inputs[0]))\n    for opt in self.opts.values():\n      if learning_rate:\n        for param_group in opt.param_groups:\n          param_group[""lr""] = learning_rate\n    fake_feature = self.vgg[0](self.denorm(sr))[0]\n    real_feature = self.vgg[0](labels[0])[0]\n    loss_p = F.mse_loss(fake_feature, real_feature)\n    loss = loss_p * self.w[0]\n    if self.use_gan:\n      fake_prob_image = self.dnet1(sr)\n      loss_g_image = gan_bce_loss(fake_prob_image, True)\n      loss += loss_g_image * self.w[1]\n    if self.use_feat_gan:\n      fake_prob_feat = self.dnet2(fake_feature)\n      loss_g_feat = gan_bce_loss(fake_prob_feat, True)\n      loss += loss_g_feat * self.w[2]\n    # update G\n    self.gopt.zero_grad()\n    loss.backward()\n    self.gopt.step()\n    metrics[\'vgg\'] = loss_p.detach().cpu().numpy()\n    metrics[\'g_loss\'] = loss.detach().cpu().numpy()\n    if self.use_gan:\n      # update D\n      real_prob_image = self.dnet1(self.norm(labels[0]))\n      fake_prob_image = self.dnet1(sr.detach())\n      loss_d_image = gan_bce_loss(real_prob_image, True) + \\\n                     gan_bce_loss(fake_prob_image, False)\n      self.dopt1.zero_grad()\n      loss_d_image.backward()\n      self.dopt1.step()\n      metrics[\'d_image\'] = loss_d_image.detach().cpu().numpy()\n    if self.use_feat_gan:\n      real_prob_feat = self.dnet2(real_feature.detach())\n      fake_prob_feat = self.dnet2(fake_feature.detach())\n      loss_d_feat = gan_bce_loss(real_prob_feat, True) + \\\n                    gan_bce_loss(fake_prob_feat, False)\n      self.dopt2.zero_grad()\n      loss_d_feat.backward()\n      self.dopt2.step()\n      metrics[\'d_feat\'] = loss_d_feat.detach().cpu().numpy()\n    return metrics\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    sr = self.srfeat(self.norm(inputs[0])).cpu().detach()\n    sr = self.denorm(sr)\n    if labels is not None:\n      metrics[\'psnr\'] = Metrics.psnr(sr.numpy(), labels[0].cpu().numpy())\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs.get(\'epoch\')\n        writer.image(\'sr\', sr, step=step)\n    return [sr.numpy()], metrics\n\n  @staticmethod\n  def norm(x):\n    return x * 2 - 1\n\n  @staticmethod\n  def denorm(x):\n    return x / 2 + 0.5\n'"
VSR/Backend/Torch/Models/Sofvsr.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/2 \xe4\xb8\x8a\xe5\x8d\x8810:54\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .Model import SuperResolution\nfrom .sof.modules import SOFVSR as _SOFVSR\nfrom .sof.modules import optical_flow_warp\nfrom ..Util import Metrics\nfrom ..Util.Metrics import total_variance\n\n\nclass SOFVSR(SuperResolution):\n  """"""Note: SOF is Y-channel SR with depth=3""""""\n\n  def __init__(self, scale, channel, depth=3, **kwargs):\n    super(SOFVSR, self).__init__(scale, channel, **kwargs)\n    self.sof = _SOFVSR(scale, channel, depth)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n    assert depth == 3\n    self.center = depth // 2\n\n  def train(self, inputs, labels, learning_rate=None):\n    pre, cur, nxt = torch.split(inputs[0], 1, dim=1)\n    pre = torch.squeeze(pre, dim=1)\n    cur = torch.squeeze(cur, dim=1)\n    nxt = torch.squeeze(nxt, dim=1)\n    low_res = torch.cat([pre, cur, nxt], dim=1)\n    sr, flow01, flow21 = self.sof(low_res)\n    hrp, hr, hrn = torch.split(labels[0], 1, dim=1)\n    hrp = torch.squeeze(hrp, dim=1)\n    hr = torch.squeeze(hr, dim=1)\n    hrn = torch.squeeze(hrn, dim=1)\n    loss_sr = F.mse_loss(sr, hr)\n    pre_d = F.avg_pool2d(pre, 2)\n    cur_d = F.avg_pool2d(cur, 2)\n    nxt_d = F.avg_pool2d(nxt, 2)\n\n    pre_d_warp = optical_flow_warp(pre_d, flow01[2])\n    pre_warp = optical_flow_warp(pre, flow01[1])\n    hrp_warp = optical_flow_warp(hrp, flow01[0])\n    nxt_d_warp = optical_flow_warp(nxt_d, flow21[2])\n    nxt_warp = optical_flow_warp(nxt, flow21[1])\n    hrn_warp = optical_flow_warp(hrn, flow21[0])\n\n    loss_lvl1 = F.mse_loss(pre_d_warp, cur_d) + F.mse_loss(nxt_d_warp, cur_d) + \\\n                0.01 * (total_variance(flow01[2]) + total_variance(flow21[2]))\n    loss_lvl2 = F.mse_loss(pre_warp, cur) + F.mse_loss(nxt_warp, cur) + \\\n                0.01 * (total_variance(flow01[1]) + total_variance(flow21[1]))\n    loss_lvl3 = F.mse_loss(hrp_warp, hr) + F.mse_loss(hrn_warp, hr) + \\\n                0.01 * (total_variance(flow01[0]) + total_variance(flow21[0]))\n    loss = loss_sr + 0.01 * (loss_lvl3 + 0.25 * loss_lvl2 + 0.125 * loss_lvl1)\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\n      \'image\': loss_sr.detach().cpu().numpy(),\n      \'flow/lvl1\': loss_lvl1.detach().cpu().numpy(),\n      \'flow/lvl2\': loss_lvl2.detach().cpu().numpy(),\n      \'flow/lvl3\': loss_lvl3.detach().cpu().numpy(),\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    pre, cur, nxt = torch.split(inputs[0], 1, dim=1)\n    low_res = torch.cat([pre, cur, nxt], dim=2)\n    low_res = torch.squeeze(low_res, dim=1)\n    sr, _, _ = self.sof(low_res)\n    sr = sr.cpu().detach()\n    if labels is not None:\n      hr = labels[0][:, self.center]\n      metrics[\'psnr\'] = Metrics.psnr(sr, hr)\n    return [sr.numpy()], metrics\n'"
VSR/Backend/Torch/Models/Spmc.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/26 \xe4\xb8\x8b\xe5\x8d\x8812:49\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom .Loss import total_variance\nfrom .Model import SuperResolution\nfrom .spmc.ops import DetailRevealer\nfrom ..Framework.Summary import get_writer\nfrom ..Util.Metrics import psnr\nfrom ..Util.Utility import pad_if_divide, upsample\n\n\nclass SPMC(SuperResolution):\n  def __init__(self, scale, channel, stage, lambda1, lambda2, residual,\n               **kwargs):\n    super(SPMC, self).__init__(scale, channel)\n    self.spmc = DetailRevealer(scale, channel, **kwargs)\n    self.adam = torch.optim.Adam(self.trainable_variables(), 1e-4)\n    self.stage = stage\n    self.lambda1 = lambda1\n    self.lambda2 = lambda2\n    self.residual = residual\n\n  def train(self, inputs, labels, learning_rate=None):\n    self.spmc.reset()\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    labels = [x.squeeze(1) for x in labels[0].split(1, dim=1)]\n    for opt in self.opts.values():\n      if learning_rate:\n        for param_group in opt.param_groups:\n          param_group[""lr""] = learning_rate\n    srs = []\n    warps = []\n    flows = []\n    center = len(frames) // 2\n    target = frames[center]\n    gt = labels[center]\n    for ref in frames:\n      sr, flow = self.spmc(target, ref)\n      if self.residual:\n        sr = sr + upsample(target, self.scale)\n      warp = self.spmc.me.warper(ref, flow[:, 0], flow[:, 1])\n      srs.append(sr)\n      warps.append(warp)\n      flows.append(flow)\n    losses = [F.mse_loss(x, gt) for x in srs]\n    image_loss = torch.stack(losses).sum()\n    losses = []\n    for w, f in zip(warps, flows):\n      losses.append(F.l1_loss(w, target) + total_variance(f) * self.lambda1)\n    me_loss = torch.stack(losses).sum()\n    if self.stage == 1:\n      loss = me_loss\n    elif self.stage == 2:\n      loss = image_loss\n    else:\n      loss = image_loss + me_loss * self.lambda2\n    self.adam.zero_grad()\n    loss.backward()\n    self.adam.step()\n    return {\n      \'me\': me_loss.detach().cpu().numpy(),\n      \'image\': image_loss.detach().cpu().numpy(),\n      \'total\': loss.detach().cpu().numpy()\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    self.spmc.reset()\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    center = len(frames) // 2\n    _frames = [pad_if_divide(x, 8, \'reflect\') for x in frames]\n    target = _frames[center]\n    a = (target.size(2) - frames[0].size(2)) * self.scale\n    b = (target.size(3) - frames[0].size(3)) * self.scale\n    slice_h = slice(None) if a == 0 else slice(a // 2, -a // 2)\n    slice_w = slice(None) if b == 0 else slice(b // 2, -b // 2)\n    srs = []\n    for ref in _frames:\n      sr, _ = self.spmc(target, ref)\n      if self.residual:\n        sr = sr + upsample(target, self.scale)\n      srs.append(sr[..., slice_h, slice_w].detach().cpu().numpy())\n    if labels is not None:\n      labels = [x.squeeze(1) for x in labels[0].split(1, dim=1)]\n      gt = labels[center]\n      for i, v in enumerate(psnr(x, gt) for x in srs):\n        metrics[f\'psnr{i}\'] = v\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs[\'epoch\']\n        writer.image(\'hr\', gt, step=step)\n        writer.image(\'sr\', sr.clamp(0, 1), step=step)\n    return srs, metrics\n'"
VSR/Backend/Torch/Models/Srmd.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 11\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom VSR.Util.Math import gaussian_kernel, anisotropic_gaussian_kernel\nfrom .Model import SuperResolution\nfrom .srmd import ops, pca\nfrom ..Framework.Summary import get_writer\nfrom ..Util.Metrics import psnr\nfrom ..Util.Utility import imfilter\n\n\nclass SRMD(SuperResolution):\n  def __init__(self, scale, channel, degradation=None, **kwargs):\n    super(SRMD, self).__init__(scale, channel)\n    self.srmd = ops.Net(scale=scale, channels=channel, **kwargs)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n    degradation = degradation or {}\n    noise = degradation.get(\'noise\', 0)\n    if noise > 1:\n      noise /= 255\n    assert 0 <= noise <= 1\n    self.pca_dim = kwargs.get(\'pca_dim\', pca._PCA.shape[0])\n    self.kernel_size = degradation.get(\'kernel_size\', 15)\n    self.ktype = degradation.get(\'kernel_type\', \'isotropic\')\n    self.l1 = degradation.get(\'l1\', 0.1)\n    self.l2 = degradation.get(\'l2\', 0.1)\n    self.theta = degradation.get(\'theta\', 0.1)\n    self.noise = noise\n\n  def gen_kernel(self, ktype, ksize, l1, l2=None, theta=0):\n    if ktype == \'isotropic\':\n      kernel = gaussian_kernel(ksize, l1)\n    elif ktype == \'anisotropic\':\n      kernel = anisotropic_gaussian_kernel(ksize, theta, l1, l2 or l1)\n    else:\n      # TODO(wenyi) this is ""directKernel""\n      raise NotImplementedError(""DirectKernel not implemented."")\n    return kernel\n\n  def gen_random_kernel(self):\n    theta = np.random.uniform(0, np.pi)\n    l1 = np.random.uniform(0.1, 10)\n    l2 = np.random.uniform(0.1, l1)\n    return self.gen_kernel(\'anisotropic\', self.kernel_size, l1, l2, theta)\n\n  def gen_random_noise(self, shape):\n    stddev = np.random.uniform(0, 75 / 255, size=[shape[0]])\n    noise = np.random.normal(size=shape) * stddev\n    return noise, stddev\n\n  def train(self, inputs, labels, learning_rate=None):\n    for opt in self.opts.values():\n      if learning_rate:\n        for param_group in opt.param_groups:\n          param_group[""lr""] = learning_rate\n    lr = inputs[0]\n    batch = lr.shape[0]\n    noise, stddev = self.gen_random_noise(lr.shape)\n    kernel = [self.gen_random_kernel() for _ in range(batch)]\n    degpar = torch.tensor([pca.get_degradation(k) for k in kernel],\n                          dtype=lr.dtype, device=lr.device)\n    kernel = torch.tensor(kernel, dtype=lr.dtype, device=lr.device)\n    noise = torch.tensor(noise, dtype=lr.dtype, device=lr.device)\n    stddev = torch.tensor(stddev, dtype=lr.dtype, device=lr.device)\n    lr = imfilter(lr, kernel) + noise\n    sr = self.srmd(lr, degpar, stddev)\n    loss = F.l1_loss(sr, labels[0])\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\n      \'loss\': loss.detach().cpu().numpy()\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    lr = inputs[0]\n    batch = lr.shape[0]\n    degpar = torch.tensor(\n        [\n          pca.get_degradation(self.gen_kernel(self.ktype,\n                                              self.kernel_size,\n                                              self.l1,\n                                              self.l2,\n                                              self.theta))\n        ] * batch,\n        dtype=lr.dtype,\n        device=lr.device)\n    stddev = torch.tensor(\n        [self.noise] * batch,\n        dtype=lr.dtype,\n        device=lr.device)\n    sr = self.srmd(lr, degpar, stddev).detach().cpu()\n    if labels is not None:\n      metrics[\'psnr\'] = psnr(sr, labels[0])\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs.get(\'epoch\', 0)\n        writer.image(\'gt\', labels[0], step=step)\n        writer.image(\'clean\', sr.clamp(0, 1), step=step)\n    return [sr.numpy()], metrics\n\n  def export(self, export_dir):\n    """"""An example of how to export ONNX format""""""\n\n    # ONNX needs input placeholder to export model!\n    # Sounds stupid to set a 48x48 inputs.\n\n    device = list(self.srmd.parameters())[0].device\n    inputs = torch.randn(1, self.channel, 144, 128, device=device)\n    pca = torch.randn(1, self.pca_dim, 1, device=device)\n    noise = torch.randn(1, 1, device=device)\n    torch.onnx.export(self.srmd, (inputs, pca, noise), export_dir / \'srmd.onnx\')\n'"
VSR/Backend/Torch/Models/TecoGAN.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/7 \xe4\xb8\x8b\xe5\x8d\x885:21\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .Arch import SpaceToDepth\nfrom .Loss import VggFeatureLoss, gan_bce_loss, ragan_bce_loss\nfrom .Model import SuperResolution\nfrom .frvsr.ops import FNet\nfrom .teco.ops import TecoDiscriminator, TecoGenerator\nfrom .video.motion import STN\nfrom ..Framework.Summary import get_writer\nfrom ..Util import Metrics\nfrom ..Util.Utility import pad_if_divide, upsample\n\n\nclass Composer(nn.Module):\n  def __init__(self, scale, channel, gain=24, filters=64, n_rb=16):\n    super(Composer, self).__init__()\n    self.fnet = FNet(channel, gain=gain)\n    self.gnet = TecoGenerator(channel, scale, filters, n_rb)\n    self.warpper = STN(padding_mode=\'border\')\n    self.spd = SpaceToDepth(scale)\n    self.scale = scale\n\n  def forward(self, lr, lr_pre, sr_pre, detach_fnet=None):\n    """"""\n    Args:\n       lr: t_1 lr frame\n       lr_pre: t_0 lr frame\n       sr_pre: t_0 sr frame\n       detach_fnet: detach BP to fnet\n    """"""\n    flow = self.fnet(lr, lr_pre)\n    flow_up = self.scale * upsample(flow, self.scale)\n    u, v = [x.squeeze(1) for x in flow_up.split(1, dim=1)]\n    sr_warp = self.warpper(sr_pre, u, v)\n    bi = upsample(lr, self.scale)\n    if detach_fnet:\n      sr = self.gnet(lr, self.spd(sr_warp.detach()), bi)\n    else:\n      sr = self.gnet(lr, self.spd(sr_warp), bi)\n    return sr, sr_warp, flow, flow_up\n\n\nclass TeCoGAN(SuperResolution):\n  """"""Temporally Coherent GANs for Video Super-Resolution.\n\n  WARNING: Training now is experimental.\n\n  Args:\n    scale: scale factor\n    channel: input channel number\n    weights: a list of 4 integers representing weights of\n      [Image, Flow, Ping-Pong, GAN]\n    vgg_layers: a list of string representing VGG layers to extract\n    vgg_layer_weights: a list of integers the same number as `vgg_layers`,\n      weights for each VGG layers.\n    gan_layer_weights: a list of 4 integers representing weights for each layer\n      in the discriminator.\n  """"""\n\n  def __init__(self, scale, channel, weights, vgg_layers, vgg_layer_weights,\n               gan_layer_weights, patch_size, **kwargs):\n    super(TeCoGAN, self).__init__(scale, channel, **kwargs)\n    filters = kwargs.get(\'filters\', 64)  # default filter number\n    gain = kwargs.get(\'max_displacement\', 24)  # max movement of optical flow\n    n_rb = kwargs.get(\'num_residualblocks\', 16)  # default RB numbers\n    self.use_vgg = vgg_layers != []\n    self.use_gan = weights[3] > 0\n    self.debug = kwargs.get(\'debug\', {})\n    self.gnet = Composer(scale, channel, gain, filters, n_rb)\n    self.gopt = torch.optim.Adam(self.trainable_variables(\'gnet\'), 5e-5)\n    if self.use_vgg:\n      # put into list to avoid saving VGG weights\n      self.vgg = [VggFeatureLoss(vgg_layers, True)]\n      self.vgg_weights = vgg_layer_weights\n    if self.use_gan:\n      self.dnet = TecoDiscriminator(channel, filters, patch_size)\n      self.dopt = torch.optim.Adam(self.trainable_variables(\'dnet\'), 5e-5)\n      self.gan_weights = gan_layer_weights\n    self.weights = weights  # [L2, flow, ping-pong, gan]\n\n  def cuda(self):\n    super(TeCoGAN, self).cuda()\n    if self.use_vgg:\n      self.vgg[0].cuda()\n\n  @staticmethod\n  def shave_border_pixel(x, border=16):\n    x = x[..., border:-border, border:-border]\n    return F.pad(x, [border, border, border, border])\n\n  def gen_sr_clips(self, frames):\n    """"""generate a video clip""""""\n    last_lr = frames[0]\n    sr = [upsample(last_lr, self.scale)]\n    flow = []\n    for lr in frames:\n      _sr, _, _f, _ = self.gnet(lr, last_lr, sr[-1].detach())\n      sr.append(_sr)\n      flow.append(_f)\n    return sr[1:], flow\n\n  def prepare_dnet_input(self, frames):\n    # For now inputs don\'t take LR into account\n    cube = []\n    for i in range(1, len(frames) - 1):\n      pre, cur, nex = frames[i - 1:i + 2]\n      with torch.no_grad():\n        ff = self.gnet.fnet(cur, pre)\n        bf = self.gnet.fnet(cur, nex)\n        warpf = self.gnet.warpper(pre, ff[:, 0], ff[:, 1])\n        warpb = self.gnet.warpper(nex, bf[:, 0], bf[:, 1])\n        cube.append(torch.cat((pre, cur, nex, warpf, cur, warpb), dim=1))\n    return cube\n\n  def train(self, inputs, labels, learning_rate=None):\n    metrics = {}\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    labels = [x.squeeze(1) for x in labels[0].split(1, dim=1)]\n    for opt in self.opts.values():\n      if learning_rate:\n        for param_group in opt.param_groups:\n          param_group[""lr""] = learning_rate\n    # For ping-pong loss\n    frames_rev = frames.copy()\n    frames_rev.reverse()\n    back_sr, _ = self.gen_sr_clips(frames_rev)\n    back_sr.reverse()\n    sr, ff = self.gen_sr_clips(frames)\n    # Generator loss\n    # 1. Image MSE\n    loss_image_mse = torch.stack([F.mse_loss(x, y) for x, y in zip(sr, labels)])\n    # 2. Ping-pong loss\n    loss_pp = torch.stack([F.mse_loss(x, y) for x, y in zip(sr, back_sr)])\n    # 3. FlowNet loss\n    loss_flow = []\n    for i in range(len(sr)):\n      last_lr = frames[i] if i == 0 else frames[i - 1]\n      flow = ff[i]\n      lr_warp = self.gnet.warpper(last_lr, flow[:, 0], flow[:, 1])\n      loss_flow.append(F.mse_loss(frames[i], lr_warp))\n    loss_flow = torch.stack(loss_flow)\n    w = self.weights\n    loss_image_mse = loss_image_mse.mean() * w[0]\n    loss_flow = loss_flow.mean() * w[1]\n    loss_pp = loss_pp.mean() * w[2]\n    loss = loss_image_mse + loss_flow + loss_pp\n    # recording\n    metrics[\'image\'] = loss_image_mse.detach().cpu().numpy()\n    metrics[\'flow\'] = loss_flow.detach().cpu().numpy()\n    metrics[\'pp\'] = loss_pp.detach().cpu().numpy()\n    # 4. Vgg feature loss\n    if self.use_vgg:\n      loss_vgg = []\n      for x, y in zip(sr, labels):\n        real_feature = self.vgg[0](y)\n        fake_feature = self.vgg[0](x)\n        loss_vgg += [F.mse_loss(x, y) * w for x, y, w in\n                     zip(real_feature, fake_feature, self.vgg_weights)]\n      loss_vgg = torch.stack(loss_vgg).mean()\n      loss += loss_vgg\n      metrics[\'vgg\'] = loss_vgg.detach().cpu().numpy()\n    # 5. GAN loss g\n    if self.use_gan:\n      fake_inputs = self.prepare_dnet_input(sr)\n      real_inputs = self.prepare_dnet_input(labels)\n      loss_d_feature = []\n      loss_g = []\n      for x, y in zip(fake_inputs, real_inputs):\n        fake, fake_d_feats = self.dnet(self.norm(x))\n        real, real_d_feats = self.dnet(self.norm(y.detach()))\n        loss_d_feature += [F.mse_loss(x, y) * w for x, y, w in\n                           zip(real_d_feats, fake_d_feats, self.gan_weights)]\n        loss_g += [ragan_bce_loss(fake, real)]\n      loss_d_feature = torch.stack(loss_d_feature).mean()\n      loss_g = torch.stack(loss_g).mean() * w[3]\n      loss += loss_d_feature\n      metrics[\'df\'] = loss_d_feature.detach().cpu().numpy()\n      metrics[\'g\'] = loss_g.detach().cpu().numpy()\n    # Discriminator loss\n    if self.use_gan:\n      loss_d = []\n      for x, y in zip(fake_inputs, real_inputs):\n        fake, _ = self.dnet(self.norm(x.detach()))\n        real, _ = self.dnet(self.norm(y.detach()))\n        loss_d.append(ragan_bce_loss(real, fake))\n      loss_d = torch.stack(loss_d).mean()\n      metrics[\'d\'] = loss_d.detach().cpu().numpy()\n    metrics[\'total\'] = loss.detach().cpu().numpy()\n    # Optimize\n    self.gopt.zero_grad()\n    if self.use_gan:\n      loss_g.backward(retain_graph=True)\n      self.gnet.fnet.zero_grad()\n    loss.backward()\n    self.gopt.step()\n    if self.use_gan:\n      self.dopt.zero_grad()\n      loss_d.backward()\n      self.dopt.step()\n    return metrics\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    predicts = []\n    last_lr = pad_if_divide(frames[0], 8, \'reflect\')\n    a = (last_lr.size(2) - frames[0].size(2)) * self.scale\n    b = (last_lr.size(3) - frames[0].size(3)) * self.scale\n    slice_h = slice(None) if a == 0 else slice(a // 2, -a // 2)\n    slice_w = slice(None) if b == 0 else slice(b // 2, -b // 2)\n    last_sr = upsample(last_lr, self.scale)\n    for lr in frames:\n      lr = pad_if_divide(lr, 8, \'reflect\')\n      sr, warp, _, _ = self.gnet(lr, last_lr, last_sr)\n      last_lr = lr.detach()\n      last_sr = sr.detach()\n      sr = sr[..., slice_h, slice_w]\n      warp = warp[..., slice_h, slice_w]\n      predicts.append(sr.cpu().detach().numpy())\n    if labels is not None:\n      labels = [x.squeeze(1) for x in labels[0].split(1, dim=1)]\n      psnr = [Metrics.psnr(x, y) for x, y in zip(predicts, labels)]\n      metrics[\'psnr\'] = np.mean(psnr)\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs[\'epoch\']\n        writer.image(\'hr\', labels[-1], step=step)\n        writer.image(\'sr\', sr.clamp(0, 1), step=step)\n        writer.image(\'warp\', warp.clamp(0, 1), step=step)\n    return predicts, metrics\n\n  @staticmethod\n  def norm(x):\n    return x * 2.0 - 1\n\n  @staticmethod\n  def denorm(x):\n    return x / 2.0 + 0.5\n'"
VSR/Backend/Torch/Models/Vespcn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:10\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .Model import SuperResolution\nfrom .vespcn import ops\nfrom ..Framework.Summary import get_writer\nfrom ..Util import Metrics\nfrom ..Util.Utility import pad_if_divide\n\n\nclass VESPCN(SuperResolution):\n  def __init__(self, scale, channel, depth=3, **kwargs):\n    super(VESPCN, self).__init__(scale, channel, **kwargs)\n    self.vespcn = ops.VESPCN(scale, channel, depth)\n    self.opt = torch.optim.Adam(self.trainable_variables(), 1e-4)\n    self.depth = depth\n\n  def train(self, inputs, labels, learning_rate=None):\n    frames = torch.split(inputs[0], 1, dim=1)\n    frames = [f.squeeze(1) for f in frames]\n    sr, warps, flows = self.vespcn(*frames)\n    targets = torch.split(labels[0], 1, dim=1)\n    targets = [t.squeeze(1) for t in targets]\n    target = targets[self.depth // 2]\n    ref = frames[self.depth // 2]\n\n    loss_content = F.mse_loss(sr, target)\n    loss_flow = torch.sum(torch.stack([F.mse_loss(ref, w) for w in warps]))\n    loss_tv = torch.sum(torch.stack([Metrics.total_variance(f) for f in flows]))\n\n    loss = loss_content + loss_flow + 0.01 * loss_tv\n    if learning_rate:\n      for param_group in self.opt.param_groups:\n        param_group[""lr""] = learning_rate\n    self.opt.zero_grad()\n    loss.backward()\n    self.opt.step()\n    return {\n      \'image\': loss_content.detach().cpu().numpy(),\n      \'flow\': loss_flow.detach().cpu().numpy(),\n      \'tv\': loss_tv.detach().cpu().numpy(),\n    }\n\n  def eval(self, inputs, labels=None, **kwargs):\n    metrics = {}\n    frames = [x.squeeze(1) for x in inputs[0].split(1, dim=1)]\n    _frames = [pad_if_divide(x, 4, \'reflect\') for x in frames]\n    a = (_frames[0].size(2) - frames[0].size(2)) * self.scale\n    b = (_frames[0].size(3) - frames[0].size(3)) * self.scale\n    slice_h = slice(None) if a == 0 else slice(a // 2, -a // 2)\n    slice_w = slice(None) if b == 0 else slice(b // 2, -b // 2)\n    sr, warps, flows = self.vespcn(*_frames)\n    sr = sr[..., slice_h, slice_w].cpu().detach()\n    if labels is not None:\n      targets = torch.split(labels[0], 1, dim=1)\n      targets = [t.squeeze(1) for t in targets]\n      hr = targets[self.depth // 2]\n      metrics[\'psnr\'] = Metrics.psnr(sr, hr)\n      writer = get_writer(self.name)\n      if writer is not None:\n        step = kwargs[\'epoch\']\n        writer.image(\'clean\', sr.clamp(0, 1), step=step)\n        writer.image(\'warp/0\', warps[0].clamp(0, 1), step=step)\n        writer.image(\'warp/1\', warps[-1].clamp(0, 1), step=step)\n    return [sr.numpy()], metrics\n'"
VSR/Backend/Torch/Models/__init__.py,0,"b""#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:10\n\nimport importlib\n\n__all__ = ['get_model', 'list_supported_models']\n\nmodels = {\n  # alias: (file, class)\n  'espcn': ('Classic', 'ESPCN'),\n  'srcnn': ('Classic', 'SRCNN'),\n  'vdsr': ('Classic', 'VDSR'),\n  'dncnn': ('Classic', 'DNCNN'),\n  'drcn': ('Classic', 'DRCN'),\n  'drrn': ('Classic', 'DRRN'),\n  'ffdnet': ('Ffdnet', 'FFDNET'),\n  'edsr': ('Edsr', 'EDSR'),\n  'carn': ('Carn', 'CARN'),\n  'dbpn': ('Dbpn', 'DBPN'),\n  'rcan': ('Rcan', 'RCAN'),\n  'srfeat': ('SRFeat', 'SRFEAT'),\n  'esrgan': ('Esrgan', 'ESRGAN'),\n  'msrn': ('Msrn', 'MSRN'),\n  'crdn': ('Crdn', 'CRDN'),\n  'mldn': ('Mldn', 'MLDN'),\n  'drn': ('Drn', 'DRN'),\n  'sofvsr': ('Sofvsr', 'SOFVSR'),\n  'vespcn': ('Vespcn', 'VESPCN'),\n  'frvsr': ('Frvsr', 'FRVSR'),\n  'qprn': ('Qprn', 'QPRN'),\n  'ufvsr': ('Ufvsr', 'UFVSR'),\n  'yovsr': ('Yovsr', 'YOVSR'),\n  'tecogan': ('TecoGAN', 'TeCoGAN'),\n  'spmc': ('Spmc', 'SPMC'),\n  'rbpn': ('Rbpn', 'RBPN'),\n  'srmd': ('Srmd', 'SRMD'),\n  # NTIRE 2019 Collections\n  'didn': ('NTIRE19', 'DIDN'),\n  'dhdn': ('NTIRE19', 'DHDN'),\n  'grdn': ('NTIRE19', 'GRDN'),\n  'resunet': ('NTIRE19', 'ResUNet'),\n  'edrn': ('NTIRE19', 'EDRN'),\n  'frn': ('NTIRE19', 'FRN'),\n  'ran': ('NTIRE19', 'RAN'),\n}\n\n\ndef get_model(name):\n  module = f'.Backend.Torch.Models.{models[name][0]}'\n  package = 'VSR'\n  m = importlib.import_module(module, package)\n  return m.__dict__[models[name][1]]\n\n\ndef list_supported_models():\n  return models.keys()\n"""
VSR/Backend/Torch/Util/Metrics.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport numpy as np\nimport torch\n\n\ndef psnr(x, y, max_val=1.0):\n  if isinstance(x, torch.Tensor):\n    x = x.detach().cpu().numpy()\n  if isinstance(y, torch.Tensor):\n    y = y.detach().cpu().numpy()\n  mse = np.square(x - y).mean()\n  return 10 * np.log10(max_val ** 2 / mse)\n\n\ndef total_variance(x, reduction='mean'):\n  hor = x[..., :-1, :] - x[..., 1:, :]\n  ver = x[..., :-1] - x[..., 1:]\n  if isinstance(x, torch.Tensor):\n    tot_var = torch.sum(torch.abs(hor)) + torch.sum(torch.abs(ver))\n  elif isinstance(x, np.ndarray):\n    tot_var = np.abs(hor).sum() + np.abs(ver).sum()\n  if reduction == 'mean':\n    reduce = x.shape[-1] * x.shape[-2]\n  else:\n    reduce = 1\n  return tot_var / reduce\n"""
VSR/Backend/Torch/Util/Utility.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\nimport torch\nimport torch.nn.functional as F\n\nfrom VSR.Util.Math import weights_downsample, weights_upsample\n\n\ndef pad_if_divide(x: torch.Tensor, value, mode=\'constant\'):\n  """"""pad tensor if its width and height couldn\'t be divided by `value`.\n\n  Args:\n      x: a tensor at least has 3 dimensions.\n      value: value to divide width and height.\n      mode: a string, representing padding mode.\n  Return:\n      padded tensor.\n  """"""\n\n  shape = x.shape\n  assert 3 <= x.dim() <= 4, f""Dim of x is not 3 or 4, which is {x.dim()}""\n  h = shape[-2]\n  w = shape[-1]\n  dh = h + (value - h % value) % value - h\n  dw = w + (value - w % value) % value - w\n  pad = [dw // 2, dw - dw // 2, dh // 2, dh - dh // 2]\n  return F.pad(x, pad, mode)\n\n\ndef shave_if_divide(x: torch.Tensor, value):\n  """"""crop tensor if its width and height couldn\'t be divided by `value`.\n\n  Args:\n      x: a tensor at least has 3 dimensions.\n      value: value to divide width and height.\n  Return:\n      cropped tensor.\n  """"""\n\n  shape = x.shape\n  h = shape[-2]\n  w = shape[-1]\n  dh = h % value\n  dw = w % value\n  return x[..., dh // 2:h - dh // 2, dw // 2:w - dw // 2]\n\n\ndef transpose(x: torch.Tensor, dims):\n  """"""transpose like numpy and tensorflow""""""\n  _dims = list(dims)\n  for i in range(len(_dims)):\n    if _dims[i] != i:\n      x = x.transpose(i, _dims[i])\n      j = _dims.index(i)\n      _dims[i], _dims[j] = i, _dims[i]\n  return x\n\n\ndef irtranspose(x: torch.Tensor, dims):\n  """"""back transpose.\n    `x = irtranspose(transpose(x, d), d)`\n  """"""\n\n  _dims = list(dims)\n  _ir_dims = [_dims.index(i) for i in range(len(_dims))]\n  return transpose(x, _ir_dims)\n\n\ndef _push_shape_4d(x):\n  dim = x.dim()\n  if dim == 2:\n    return x.unsqueeze(0).unsqueeze(1), 2\n  elif dim == 3:\n    return x.unsqueeze(0), 3\n  elif dim == 4:\n    return x, 4\n  else:\n    raise ValueError(""Unsupported tensor! Must be 2D/3D/4D"")\n\n\ndef _pop_shape(x, shape):\n  if shape == 2:\n    return x[0, ..., 0]\n  elif shape == 3:\n    return x[0]\n  elif shape == 4:\n    return x\n  else:\n    raise ValueError(""Unsupported shape! Must be 2/3/4"")\n\n\ndef downsample(img, scale, border=\'reflect\'):\n  """"""Bicubical downsample via **CONV2D**. Using PIL\'s kernel.\n\n  Args:\n    img: a tf tensor of 2/3/4-D.\n    scale: n or 1/n. `n` must be integer >= 2.\n    border: padding mode. Recommend to \'REFLECT\'.\n  """"""\n  device = img.device\n  kernel, s = weights_downsample(scale)\n  if s == 1:\n    return img  # bypass\n  kernel = kernel.astype(\'float32\')\n  kernel = torch.from_numpy(kernel)\n  p1 = int(s * 3 / 2)\n  p2 = 4 * s - int(s * 3 / 2)\n  img, shape = _push_shape_4d(img)\n  img_ex = F.pad(img, [p1, p2, p1, p2], mode=border)\n  c = img_ex.shape[1]\n  assert c is not None, ""img must define channel number""\n  c = int(c)\n  filters = torch.reshape(torch.eye(c, c), [c, c, 1, 1]) * kernel\n  img_s = F.conv2d(img_ex, filters.to(device), stride=s)\n  img_s = _pop_shape(img_s, shape)\n  return img_s\n\n\ndef upsample(img, scale, border=\'reflect\'):\n  """"""Bicubical upsample via **CONV2D**. Using PIL\'s kernel.\n\n  Args:\n    img: a tf tensor of 2/3/4-D.\n    scale: must be integer >= 2.\n    border: padding mode. Recommend to \'REFLECT\'.\n  """"""\n  device = img.device\n  kernels, s = weights_upsample(scale)\n  if s == 1:\n    return img  # bypass\n  kernels = [k.astype(\'float32\') for k in kernels]\n  kernels = [torch.from_numpy(k) for k in kernels]\n  p1 = 1 + s // 2\n  p2 = 3\n  img, shape = _push_shape_4d(img)\n  img_ex = F.pad(img, [p1, p2, p1, p2], mode=border)\n  c = img_ex.shape[1]\n  assert c is not None, ""img must define channel number""\n  c = int(c)\n  filters = [torch.reshape(torch.eye(c, c), [c, c, 1, 1]) * k for k in kernels]\n  weights = torch.stack(filters, dim=0).transpose(0, 1).reshape([-1, c, 5, 5])\n  img_s = F.conv2d(img_ex, weights.to(device))\n  img_s = F.pixel_shuffle(img_s, s)\n  more = s // 2 * s\n  crop = slice(more - s // 2, - (s // 2))\n  img_s = _pop_shape(img_s[..., crop, crop], shape)\n  return img_s\n\n\ndef bicubic_resize(img, scale, border=\'reflect\'):\n  if scale > 1:\n    return upsample(img, scale, border)\n  elif 0 < scale < 1:\n    return downsample(img, scale, border)\n  elif scale == 1:\n    return img  # bypass\n  else:\n    raise ValueError(""Wrong scale factor!"")\n\n\ndef imfilter(image: torch.Tensor, kernel: torch.Tensor):\n  with torch.no_grad():\n    if image.dim() == 3:\n      image = image.unsqueeze(0)\n    assert image.dim() == 4, f""Dim of image must be 4, but is {image.dim()}""\n    if kernel.dim() == 2:\n      kernel = kernel.unsqueeze(0)\n      kernel = torch.cat([kernel] * image.shape[0])\n    assert kernel.dim() == 3, f""Dim of kernel must be 3, but is {kernel.dim()}""\n\n    ret = []\n    for i, k in zip(image.split(1), kernel.split(1)):\n      _c = i.shape[1]\n      _k = k.unsqueeze(0)\n      _p = torch.zeros_like(_k)\n      _m = []\n      for j in range(_c):\n        t = [_p] * _c\n        t[j] = _k\n        _m.append(torch.cat(t, dim=1))\n      _k = torch.cat(_m, dim=0)\n      ret.append(F.conv2d(i, _k, padding=[x // 2 for x in kernel.shape[1:]]))\n    return torch.cat(ret)\n\n\ndef poisson_noise(inputs, stddev=None, sigma_max=0.16):\n  """"""Add poisson noise to inputs.""""""\n\n  if stddev is None:\n    stddev = np.random.rand(inputs.shape[-1]) * sigma_max\n  stddev = np.reshape(stddev, [1] * (inputs.ndim - 1) + [-1])\n  sigma_map = (1 - inputs) * stddev\n  return np.random.randn(*inputs.shape) * sigma_map\n\n\ndef gaussian_noise(inputs, stddev=None, sigma_max=0.06, channel_wise=True):\n  """"""Add channel wise gaussian noise.""""""\n\n  channel = inputs.shape[-1] if channel_wise else 1\n  if stddev is None:\n    stddev = np.random.rand(channel) * sigma_max\n  stddev = np.reshape(stddev, [1] * (inputs.ndim - 1) + [-1])\n  noise_map = np.random.randn(*inputs.shape) * stddev\n  return noise_map\n\n\ndef gaussian_poisson_noise(inputs, stddev_s=None, stddev_c=None,\n                           max_s=0.16, max_c=0.06):\n  noise = poisson_noise(inputs, stddev_s, max_s)\n  return noise + gaussian_noise(inputs, stddev_c, max_c)\n'"
VSR/Backend/Torch/Util/__init__.py,0,b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 7\n\n'
VSR/Backend/Torch/Models/carn/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 14\n\nimport logging\n_logger = logging.getLogger(""VSR.CARN"")\n_logger.info(""LICENSE: CARN is implemented by Namhyuk Ahn. ""\n             ""@nmhkahn https://github.com/nmhkahn/CARN-pytorch"")\n'"
VSR/Backend/Torch/Models/carn/carn.py,0,"b'import torch\nimport torch.nn as nn\n\nfrom . import ops\n\n\nclass Block(nn.Module):\n  def __init__(self,\n               in_channels, out_channels,\n               group=1):\n    super(Block, self).__init__()\n\n    self.b1 = ops.ResidualBlock(64, 64)\n    self.b2 = ops.ResidualBlock(64, 64)\n    self.b3 = ops.ResidualBlock(64, 64)\n    self.c1 = ops.BasicBlock(64 * 2, 64, 1, 1, 0)\n    self.c2 = ops.BasicBlock(64 * 3, 64, 1, 1, 0)\n    self.c3 = ops.BasicBlock(64 * 4, 64, 1, 1, 0)\n\n  def forward(self, x):\n    c0 = o0 = x\n\n    b1 = self.b1(o0)\n    c1 = torch.cat([c0, b1], dim=1)\n    o1 = self.c1(c1)\n\n    b2 = self.b2(o1)\n    c2 = torch.cat([c1, b2], dim=1)\n    o2 = self.c2(c2)\n\n    b3 = self.b3(o2)\n    c3 = torch.cat([c2, b3], dim=1)\n    o3 = self.c3(c3)\n\n    return o3\n\n\nclass Net(nn.Module):\n  def __init__(self, **kwargs):\n    super(Net, self).__init__()\n\n    scale = kwargs.get(""scale"")\n    multi_scale = kwargs.get(""multi_scale"")\n    group = kwargs.get(""group"", 1)\n\n    self.sub_mean = ops.MeanShift((0.4488, 0.4371, 0.4040), sub=True)\n    self.add_mean = ops.MeanShift((0.4488, 0.4371, 0.4040), sub=False)\n\n    self.entry = nn.Conv2d(3, 64, 3, 1, 1)\n\n    self.b1 = Block(64, 64)\n    self.b2 = Block(64, 64)\n    self.b3 = Block(64, 64)\n    self.c1 = ops.BasicBlock(64 * 2, 64, 1, 1, 0)\n    self.c2 = ops.BasicBlock(64 * 3, 64, 1, 1, 0)\n    self.c3 = ops.BasicBlock(64 * 4, 64, 1, 1, 0)\n\n    self.upsample = ops.UpsampleBlock(64, scale=scale,\n                                      multi_scale=multi_scale,\n                                      group=group)\n    self.exit = nn.Conv2d(64, 3, 3, 1, 1)\n\n  def forward(self, x, scale=None):\n    x = self.sub_mean(x)\n    x = self.entry(x)\n    c0 = o0 = x\n\n    b1 = self.b1(o0)\n    c1 = torch.cat([c0, b1], dim=1)\n    o1 = self.c1(c1)\n\n    b2 = self.b2(o1)\n    c2 = torch.cat([c1, b2], dim=1)\n    o2 = self.c2(c2)\n\n    b3 = self.b3(o2)\n    c3 = torch.cat([c2, b3], dim=1)\n    o3 = self.c3(c3)\n\n    out = self.upsample(o3, scale=scale)\n\n    out = self.exit(out)\n    out = self.add_mean(out)\n\n    return out\n'"
VSR/Backend/Torch/Models/carn/carn_m.py,0,"b'import torch\nimport torch.nn as nn\n\nfrom . import ops\n\n\nclass Block(nn.Module):\n  def __init__(self,\n               in_channels, out_channels,\n               group=1):\n    super(Block, self).__init__()\n\n    self.b1 = ops.EResidualBlock(64, 64, group=group)\n    self.c1 = ops.BasicBlock(64 * 2, 64, 1, 1, 0)\n    self.c2 = ops.BasicBlock(64 * 3, 64, 1, 1, 0)\n    self.c3 = ops.BasicBlock(64 * 4, 64, 1, 1, 0)\n\n  def forward(self, x):\n    c0 = o0 = x\n\n    b1 = self.b1(o0)\n    c1 = torch.cat([c0, b1], dim=1)\n    o1 = self.c1(c1)\n\n    b2 = self.b1(o1)\n    c2 = torch.cat([c1, b2], dim=1)\n    o2 = self.c2(c2)\n\n    b3 = self.b1(o2)\n    c3 = torch.cat([c2, b3], dim=1)\n    o3 = self.c3(c3)\n\n    return o3\n\n\nclass Net(nn.Module):\n  def __init__(self, **kwargs):\n    super(Net, self).__init__()\n\n    scale = kwargs.get(""scale"")\n    multi_scale = kwargs.get(""multi_scale"")\n    group = kwargs.get(""group"", 1)\n\n    self.sub_mean = ops.MeanShift((0.4488, 0.4371, 0.4040), sub=True)\n    self.add_mean = ops.MeanShift((0.4488, 0.4371, 0.4040), sub=False)\n\n    self.entry = nn.Conv2d(3, 64, 3, 1, 1)\n\n    self.b1 = Block(64, 64, group=group)\n    self.b2 = Block(64, 64, group=group)\n    self.b3 = Block(64, 64, group=group)\n    self.c1 = ops.BasicBlock(64 * 2, 64, 1, 1, 0)\n    self.c2 = ops.BasicBlock(64 * 3, 64, 1, 1, 0)\n    self.c3 = ops.BasicBlock(64 * 4, 64, 1, 1, 0)\n\n    self.upsample = ops.UpsampleBlock(64, scale=scale,\n                                      multi_scale=multi_scale,\n                                      group=group)\n    self.exit = nn.Conv2d(64, 3, 3, 1, 1)\n\n  def forward(self, x, scale):\n    x = self.sub_mean(x)\n    x = self.entry(x)\n    c0 = o0 = x\n\n    b1 = self.b1(o0)\n    c1 = torch.cat([c0, b1], dim=1)\n    o1 = self.c1(c1)\n\n    b2 = self.b2(o1)\n    c2 = torch.cat([c1, b2], dim=1)\n    o2 = self.c2(c2)\n\n    b3 = self.b3(o2)\n    c3 = torch.cat([c2, b3], dim=1)\n    o3 = self.c3(c3)\n\n    out = self.upsample(o3, scale=scale)\n\n    out = self.exit(out)\n    out = self.add_mean(out)\n\n    return out\n'"
VSR/Backend/Torch/Models/carn/ops.py,0,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef init_weights(modules):\n  pass\n\n\nclass MeanShift(nn.Module):\n  def __init__(self, mean_rgb, sub):\n    super(MeanShift, self).__init__()\n\n    sign = -1 if sub else 1\n    r = mean_rgb[0] * sign\n    g = mean_rgb[1] * sign\n    b = mean_rgb[2] * sign\n\n    self.shifter = nn.Conv2d(3, 3, 1, 1, 0)\n    self.shifter.weight.data = torch.eye(3).view(3, 3, 1, 1)\n    self.shifter.bias.data = torch.Tensor([r, g, b])\n\n    # Freeze the mean shift layer\n    for params in self.shifter.parameters():\n      params.requires_grad = False\n\n  def forward(self, x):\n    x = self.shifter(x)\n    return x\n\n\nclass BasicBlock(nn.Module):\n  def __init__(self,\n               in_channels, out_channels,\n               ksize=3, stride=1, pad=1):\n    super(BasicBlock, self).__init__()\n\n    self.body = nn.Sequential(\n      nn.Conv2d(in_channels, out_channels, ksize, stride, pad),\n      nn.ReLU(inplace=True)\n    )\n\n    init_weights(self.modules)\n\n  def forward(self, x):\n    out = self.body(x)\n    return out\n\n\nclass ResidualBlock(nn.Module):\n  def __init__(self,\n               in_channels, out_channels):\n    super(ResidualBlock, self).__init__()\n\n    self.body = nn.Sequential(\n      nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n      nn.ReLU(inplace=True),\n      nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n    )\n\n    init_weights(self.modules)\n\n  def forward(self, x):\n    out = self.body(x)\n    out = F.relu(out + x)\n    return out\n\n\nclass EResidualBlock(nn.Module):\n  def __init__(self,\n               in_channels, out_channels,\n               group=1):\n    super(EResidualBlock, self).__init__()\n\n    self.body = nn.Sequential(\n      nn.Conv2d(in_channels, out_channels, 3, 1, 1, groups=group),\n      nn.ReLU(inplace=True),\n      nn.Conv2d(out_channels, out_channels, 3, 1, 1, groups=group),\n      nn.ReLU(inplace=True),\n      nn.Conv2d(out_channels, out_channels, 1, 1, 0),\n    )\n\n    init_weights(self.modules)\n\n  def forward(self, x):\n    out = self.body(x)\n    out = F.relu(out + x)\n    return out\n\n\nclass UpsampleBlock(nn.Module):\n  def __init__(self,\n               n_channels, scale, multi_scale,\n               group=1):\n    super(UpsampleBlock, self).__init__()\n\n    if multi_scale:\n      self.up2 = _UpsampleBlock(n_channels, scale=2, group=group)\n      self.up3 = _UpsampleBlock(n_channels, scale=3, group=group)\n      self.up4 = _UpsampleBlock(n_channels, scale=4, group=group)\n    else:\n      self.up = _UpsampleBlock(n_channels, scale=scale, group=group)\n\n    self.multi_scale = multi_scale\n\n  def forward(self, x, scale=None):\n    if self.multi_scale:\n      if scale == 2:\n        return self.up2(x)\n      elif scale == 3:\n        return self.up3(x)\n      elif scale == 4:\n        return self.up4(x)\n    else:\n      return self.up(x)\n\n\nclass _UpsampleBlock(nn.Module):\n  def __init__(self,\n               n_channels, scale,\n               group=1):\n    super(_UpsampleBlock, self).__init__()\n\n    modules = []\n    if scale == 2 or scale == 4 or scale == 8:\n      for _ in range(int(math.log(scale, 2))):\n        modules += [\n          nn.Conv2d(n_channels, 4 * n_channels, 3, 1, 1, groups=group),\n          nn.ReLU(inplace=True)]\n        modules += [nn.PixelShuffle(2)]\n    elif scale == 3:\n      modules += [nn.Conv2d(n_channels, 9 * n_channels, 3, 1, 1, groups=group),\n                  nn.ReLU(inplace=True)]\n      modules += [nn.PixelShuffle(3)]\n\n    self.body = nn.Sequential(*modules)\n    init_weights(self.modules)\n\n  def forward(self, x):\n    out = self.body(x)\n    return out\n'"
VSR/Backend/Torch/Models/dbpn/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport logging\n_logger = logging.getLogger(""VSR.DBPN"")\n_logger.info(""LICENSE: DBPN is implemented by Haris. ""\n             ""@alterzero https://github.com/alterzero/DBPN-Pytorch"")\n'"
VSR/Backend/Torch/Models/dbpn/base_networks.py,0,"b""import torch\nimport math\n\n\nclass DenseBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, bias=True, activation='relu',\n               norm='batch'):\n    super(DenseBlock, self).__init__()\n    self.fc = torch.nn.Linear(input_size, output_size, bias=bias)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm1d(output_size)\n    elif self.norm == 'instance':\n      self.bn = torch.nn.InstanceNorm1d(output_size)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    if self.norm is not None:\n      out = self.bn(self.fc(x))\n    else:\n      out = self.fc(x)\n\n    if self.activation is not None:\n      return self.act(out)\n    else:\n      return out\n\n\nclass ConvBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, kernel_size=3, stride=1,\n               padding=1, bias=True, activation='prelu', norm=None):\n    super(ConvBlock, self).__init__()\n    self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size, stride,\n                                padding, bias=bias)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm2d(output_size)\n    elif self.norm == 'instance':\n      self.bn = torch.nn.InstanceNorm2d(output_size)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    if self.norm is not None:\n      out = self.bn(self.conv(x))\n    else:\n      out = self.conv(x)\n\n    if self.activation is not None:\n      return self.act(out)\n    else:\n      return out\n\n\nclass DeconvBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, kernel_size=4, stride=2,\n               padding=1, bias=True, activation='prelu', norm=None):\n    super(DeconvBlock, self).__init__()\n    self.deconv = torch.nn.ConvTranspose2d(input_size, output_size, kernel_size,\n                                           stride, padding, bias=bias)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm2d(output_size)\n    elif self.norm == 'instance':\n      self.bn = torch.nn.InstanceNorm2d(output_size)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    if self.norm is not None:\n      out = self.bn(self.deconv(x))\n    else:\n      out = self.deconv(x)\n\n    if self.activation is not None:\n      return self.act(out)\n    else:\n      return out\n\n\nclass ResnetBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=3, stride=1, padding=1, bias=True,\n               activation='prelu', norm='batch'):\n    super(ResnetBlock, self).__init__()\n    self.conv1 = torch.nn.Conv2d(num_filter, num_filter, kernel_size, stride,\n                                 padding, bias=bias)\n    self.conv2 = torch.nn.Conv2d(num_filter, num_filter, kernel_size, stride,\n                                 padding, bias=bias)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm2d(num_filter)\n    elif norm == 'instance':\n      self.bn = torch.nn.InstanceNorm2d(num_filter)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    residual = x\n    if self.norm is not None:\n      out = self.bn(self.conv1(x))\n    else:\n      out = self.conv1(x)\n\n    if self.activation is not None:\n      out = self.act(out)\n\n    if self.norm is not None:\n      out = self.bn(self.conv2(out))\n    else:\n      out = self.conv2(out)\n\n    out = torch.add(out, residual)\n    return out\n\n\nclass UpBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, bias=True,\n               activation='prelu', norm=None):\n    super(UpBlock, self).__init__()\n    self.up_conv1 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                              padding, activation, norm=None)\n    self.up_conv3 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    h0 = self.up_conv1(x)\n    l0 = self.up_conv2(h0)\n    h1 = self.up_conv3(l0 - x)\n    return h1 + h0\n\n\nclass UpBlockPix(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, scale=4,\n               bias=True, activation='prelu', norm=None):\n    super(UpBlockPix, self).__init__()\n    self.up_conv1 = Upsampler(scale, num_filter)\n    self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                              padding, activation, norm=None)\n    self.up_conv3 = Upsampler(scale, num_filter)\n\n  def forward(self, x):\n    h0 = self.up_conv1(x)\n    l0 = self.up_conv2(h0)\n    h1 = self.up_conv3(l0 - x)\n    return h1 + h0\n\n\nclass D_UpBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2,\n               num_stages=1, bias=True, activation='prelu', norm=None):\n    super(D_UpBlock, self).__init__()\n    self.conv = ConvBlock(num_filter * num_stages, num_filter, 1, 1, 0,\n                          activation, norm=None)\n    self.up_conv1 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                              padding, activation, norm=None)\n    self.up_conv3 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    x = self.conv(x)\n    h0 = self.up_conv1(x)\n    l0 = self.up_conv2(h0)\n    h1 = self.up_conv3(l0 - x)\n    return h1 + h0\n\n\nclass D_UpBlockPix(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2,\n               num_stages=1, scale=4, bias=True, activation='prelu', norm=None):\n    super(D_UpBlockPix, self).__init__()\n    self.conv = ConvBlock(num_filter * num_stages, num_filter, 1, 1, 0,\n                          activation, norm=None)\n    self.up_conv1 = Upsampler(scale, num_filter)\n    self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                              padding, activation, norm=None)\n    self.up_conv3 = Upsampler(scale, num_filter)\n\n  def forward(self, x):\n    x = self.conv(x)\n    h0 = self.up_conv1(x)\n    l0 = self.up_conv2(h0)\n    h1 = self.up_conv3(l0 - x)\n    return h1 + h0\n\n\nclass DownBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, bias=True,\n               activation='prelu', norm=None):\n    super(DownBlock, self).__init__()\n    self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.down_conv2 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                  padding, activation, norm=None)\n    self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    l0 = self.down_conv1(x)\n    h0 = self.down_conv2(l0)\n    l1 = self.down_conv3(h0 - x)\n    return l1 + l0\n\n\nclass DownBlockPix(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, scale=4,\n               bias=True, activation='prelu', norm=None):\n    super(DownBlockPix, self).__init__()\n    self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.down_conv2 = Upsampler(scale, num_filter)\n    self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    l0 = self.down_conv1(x)\n    h0 = self.down_conv2(l0)\n    l1 = self.down_conv3(h0 - x)\n    return l1 + l0\n\n\nclass D_DownBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2,\n               num_stages=1, bias=True, activation='prelu', norm=None):\n    super(D_DownBlock, self).__init__()\n    self.conv = ConvBlock(num_filter * num_stages, num_filter, 1, 1, 0,\n                          activation, norm=None)\n    self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.down_conv2 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                  padding, activation, norm=None)\n    self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    x = self.conv(x)\n    l0 = self.down_conv1(x)\n    h0 = self.down_conv2(l0)\n    l1 = self.down_conv3(h0 - x)\n    return l1 + l0\n\n\nclass D_DownBlockPix(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2,\n               num_stages=1, scale=4, bias=True, activation='prelu', norm=None):\n    super(D_DownBlockPix, self).__init__()\n    self.conv = ConvBlock(num_filter * num_stages, num_filter, 1, 1, 0,\n                          activation, norm=None)\n    self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.down_conv2 = Upsampler(scale, num_filter)\n    self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    x = self.conv(x)\n    l0 = self.down_conv1(x)\n    h0 = self.down_conv2(l0)\n    l1 = self.down_conv3(h0 - x)\n    return l1 + l0\n\n\nclass PSBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, scale_factor, kernel_size=3,\n               stride=1, padding=1, bias=True, activation='prelu',\n               norm='batch'):\n    super(PSBlock, self).__init__()\n    self.conv = torch.nn.Conv2d(input_size, output_size * scale_factor ** 2,\n                                kernel_size, stride, padding, bias=bias)\n    self.ps = torch.nn.PixelShuffle(scale_factor)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm2d(output_size)\n    elif norm == 'instance':\n      self.bn = torch.nn.InstanceNorm2d(output_size)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    if self.norm is not None:\n      out = self.bn(self.ps(self.conv(x)))\n    else:\n      out = self.ps(self.conv(x))\n\n    if self.activation is not None:\n      out = self.act(out)\n    return out\n\n\nclass Upsampler(torch.nn.Module):\n  def __init__(self, scale, n_feat, bn=False, act='prelu', bias=True):\n    super(Upsampler, self).__init__()\n    modules = []\n    for _ in range(int(math.log(scale, 2))):\n      modules.append(\n        ConvBlock(n_feat, 4 * n_feat, 3, 1, 1, bias, activation=None,\n                  norm=None))\n      modules.append(torch.nn.PixelShuffle(2))\n      if bn: modules.append(torch.nn.BatchNorm2d(n_feat))\n      # modules.append(torch.nn.PReLU())\n    self.up = torch.nn.Sequential(*modules)\n\n    self.activation = act\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    out = self.up(x)\n    if self.activation is not None:\n      out = self.act(out)\n    return out\n\n\nclass Upsample2xBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, bias=True, upsample='deconv',\n               activation='relu', norm='batch'):\n    super(Upsample2xBlock, self).__init__()\n    scale_factor = 2\n    # 1. Deconvolution (Transposed convolution)\n    if upsample == 'deconv':\n      self.upsample = DeconvBlock(input_size, output_size,\n                                  kernel_size=4, stride=2, padding=1,\n                                  bias=bias, activation=activation, norm=norm)\n\n    # 2. Sub-pixel convolution (Pixel shuffler)\n    elif upsample == 'ps':\n      self.upsample = PSBlock(input_size, output_size,\n                              scale_factor=scale_factor,\n                              bias=bias, activation=activation, norm=norm)\n\n    # 3. Resize and Convolution\n    elif upsample == 'rnc':\n      self.upsample = torch.nn.Sequential(\n        torch.nn.Upsample(scale_factor=scale_factor, mode='nearest'),\n        ConvBlock(input_size, output_size,\n                  kernel_size=3, stride=1, padding=1,\n                  bias=bias, activation=activation, norm=norm)\n      )\n\n  def forward(self, x):\n    out = self.upsample(x)\n    return out\n\n\nclass CascadedBlock(torch.nn.Module):\n  def __init__(self, padding=1, **kwargs):\n    super(CascadedBlock, self).__init__()\n    self.rb1 = ResnetBlock(64, padding=padding)\n    self.rb2 = ResnetBlock(64, padding=padding)\n    self.rb3 = ResnetBlock(64, padding=padding)\n    self.rb4 = ResnetBlock(64, padding=padding)\n\n    self.cb1 = torch.nn.Conv2d(64 * 2, 64, 1, padding=0)\n    self.cb2 = torch.nn.Conv2d(64 * 3, 64, 1, padding=0)\n    self.cb3 = torch.nn.Conv2d(64 * 4, 64, 1, padding=0)\n    self.cb4 = torch.nn.Conv2d(64 * 5, 64, 1, padding=0)\n\n  def forward(self, x):\n    x1 = self.rb1(x)\n    x1_c = torch.cat([x, x1], 1)\n    x1_s = self.cb1(x1_c)\n    x2 = self.rb2(x1_s)\n    x2_c = torch.cat([x, x1, x2], 1)\n    x2_s = self.cb2(x2_c)\n    x3 = self.rb3(x2_s)\n    x3_c = torch.cat([x, x1, x2, x3], 1)\n    x3_s = self.cb3(x3_c)\n    x4 = self.rb4(x3_s)\n    x4_c = torch.cat([x, x1, x2, x3, x4], 1)\n    x4_s = self.cb4(x4_c)\n    return x4_s\n"""
VSR/Backend/Torch/Models/dbpn/dbpn.py,0,"b""import torch.nn as nn\n\nfrom .base_networks import *\n\n\nclass Net(nn.Module):\n  def __init__(self, num_channels, base_filter, feat, num_stages, scale_factor):\n    super(Net, self).__init__()\n\n    if scale_factor == 2:\n      kernel = 6\n      stride = 2\n      padding = 2\n    elif scale_factor == 4:\n      kernel = 8\n      stride = 4\n      padding = 2\n    elif scale_factor == 8:\n      kernel = 12\n      stride = 8\n      padding = 2\n\n    # Initial Feature Extraction\n    self.feat0 = ConvBlock(num_channels, feat, 3, 1, 1, activation='prelu',\n                           norm=None)\n    self.feat1 = ConvBlock(feat, base_filter, 1, 1, 0, activation='prelu',\n                           norm=None)\n    # Back-projection stages\n    self.up1 = UpBlock(base_filter, kernel, stride, padding)\n    self.down1 = DownBlock(base_filter, kernel, stride, padding)\n    self.up2 = UpBlock(base_filter, kernel, stride, padding)\n    self.down2 = D_DownBlock(base_filter, kernel, stride, padding, 2)\n    self.up3 = D_UpBlock(base_filter, kernel, stride, padding, 2)\n    self.down3 = D_DownBlock(base_filter, kernel, stride, padding, 3)\n    self.up4 = D_UpBlock(base_filter, kernel, stride, padding, 3)\n    self.down4 = D_DownBlock(base_filter, kernel, stride, padding, 4)\n    self.up5 = D_UpBlock(base_filter, kernel, stride, padding, 4)\n    self.down5 = D_DownBlock(base_filter, kernel, stride, padding, 5)\n    self.up6 = D_UpBlock(base_filter, kernel, stride, padding, 5)\n    self.down6 = D_DownBlock(base_filter, kernel, stride, padding, 6)\n    self.up7 = D_UpBlock(base_filter, kernel, stride, padding, 6)\n    # Reconstruction\n    self.output_conv = ConvBlock(num_stages * base_filter, num_channels, 3, 1,\n                                 1, activation=None, norm=None)\n\n    for m in self.modules():\n      classname = m.__class__.__name__\n      if classname.find('Conv2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n      elif classname.find('ConvTranspose2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n  def forward(self, x):\n    x = self.feat0(x)\n    x = self.feat1(x)\n\n    h1 = self.up1(x)\n    l1 = self.down1(h1)\n    h2 = self.up2(l1)\n\n    concat_h = torch.cat((h2, h1), 1)\n    l = self.down2(concat_h)\n\n    concat_l = torch.cat((l, l1), 1)\n    h = self.up3(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down3(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up4(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down4(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up5(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down5(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up6(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down6(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up7(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    x = self.output_conv(concat_h)\n\n    return x\n"""
VSR/Backend/Torch/Models/dbpn/dbpn_v1.py,0,"b""import torch.nn as nn\n\nfrom .base_networks import *\n\n\nclass Net(nn.Module):\n  def __init__(self, num_channels, base_filter, feat, num_stages, scale_factor):\n    super(Net, self).__init__()\n\n    if scale_factor == 2:\n      kernel = 6\n      stride = 2\n      padding = 2\n    elif scale_factor == 4:\n      kernel = 8\n      stride = 4\n      padding = 2\n    elif scale_factor == 8:\n      kernel = 12\n      stride = 8\n      padding = 2\n\n    # Initial Feature Extraction\n    self.feat0 = ConvBlock(num_channels, feat, 3, 1, 1, activation='prelu',\n                           norm=None)\n    self.feat1 = ConvBlock(feat, base_filter, 1, 1, 0, activation='prelu',\n                           norm=None)\n    # Back-projection stages\n    self.up1 = UpBlock(base_filter, kernel, stride, padding)\n    self.down1 = DownBlock(base_filter, kernel, stride, padding)\n    self.up2 = UpBlock(base_filter, kernel, stride, padding)\n    self.down2 = D_DownBlock(base_filter, kernel, stride, padding, 2)\n    self.up3 = D_UpBlock(base_filter, kernel, stride, padding, 2)\n    self.down3 = D_DownBlock(base_filter, kernel, stride, padding, 3)\n    self.up4 = D_UpBlock(base_filter, kernel, stride, padding, 3)\n    self.down4 = D_DownBlock(base_filter, kernel, stride, padding, 4)\n    self.up5 = D_UpBlock(base_filter, kernel, stride, padding, 4)\n    self.down5 = D_DownBlock(base_filter, kernel, stride, padding, 5)\n    self.up6 = D_UpBlock(base_filter, kernel, stride, padding, 5)\n    self.down6 = D_DownBlock(base_filter, kernel, stride, padding, 6)\n    self.up7 = D_UpBlock(base_filter, kernel, stride, padding, 6)\n    self.down7 = D_DownBlock(base_filter, kernel, stride, padding, 7)\n    self.up8 = D_UpBlock(base_filter, kernel, stride, padding, 7)\n    self.down8 = D_DownBlock(base_filter, kernel, stride, padding, 8)\n    self.up9 = D_UpBlock(base_filter, kernel, stride, padding, 8)\n    self.down9 = D_DownBlock(base_filter, kernel, stride, padding, 9)\n    self.up10 = D_UpBlock(base_filter, kernel, stride, padding, 9)\n    # Reconstruction\n    self.output_conv = ConvBlock(num_stages * base_filter, num_channels, 3, 1,\n                                 1, activation=None, norm=None)\n\n    for m in self.modules():\n      classname = m.__class__.__name__\n      if classname.find('Conv2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n      elif classname.find('ConvTranspose2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n  def forward(self, x):\n    x = self.feat0(x)\n    x = self.feat1(x)\n\n    h1 = self.up1(x)\n    l1 = self.down1(h1)\n    h2 = self.up2(l1)\n\n    concat_h = torch.cat((h2, h1), 1)\n    l = self.down2(concat_h)\n\n    concat_l = torch.cat((l, l1), 1)\n    h = self.up3(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down3(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up4(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down4(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up5(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down5(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up6(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down6(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up7(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down7(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up8(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down8(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up9(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    l = self.down9(concat_h)\n\n    concat_l = torch.cat((l, concat_l), 1)\n    h = self.up10(concat_l)\n\n    concat_h = torch.cat((h, concat_h), 1)\n    x = self.output_conv(concat_h)\n\n    return x\n"""
VSR/Backend/Torch/Models/dbpn/dbpns.py,0,"b""import torch.nn as nn\n\nfrom .base_networks import *\n\n\nclass Net(nn.Module):\n  def __init__(self, num_channels, base_filter, feat, num_stages, scale_factor):\n    super(Net, self).__init__()\n\n    if scale_factor == 2:\n      kernel = 6\n      stride = 2\n      padding = 2\n    elif scale_factor == 4:\n      kernel = 8\n      stride = 4\n      padding = 2\n    elif scale_factor == 8:\n      kernel = 12\n      stride = 8\n      padding = 2\n\n    # Initial Feature Extraction\n    self.feat0 = ConvBlock(num_channels, feat, 3, 1, 1, activation='prelu',\n                           norm=None)\n    self.feat1 = ConvBlock(feat, base_filter, 1, 1, 0, activation='prelu',\n                           norm=None)\n    # Back-projection stages\n    self.up1 = UpBlock(base_filter, kernel, stride, padding)\n    self.down1 = DownBlock(base_filter, kernel, stride, padding)\n    self.up2 = UpBlock(base_filter, kernel, stride, padding)\n    # Reconstruction\n    self.output_conv = ConvBlock(num_stages * base_filter, num_channels, 3, 1,\n                                 1, activation=None, norm=None)\n\n    for m in self.modules():\n      classname = m.__class__.__name__\n      if classname.find('Conv2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n      elif classname.find('ConvTranspose2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n  def forward(self, x):\n    x = self.feat0(x)\n    x = self.feat1(x)\n\n    h1 = self.up1(x)\n    h2 = self.up2(self.down1(h1))\n\n    x = self.output_conv(torch.cat((h2, h1), 1))\n\n    return x\n"""
VSR/Backend/Torch/Models/edsr/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport logging\n_logger = logging.getLogger(""VSR.EDSR"")\n_logger.info(""LICENSE: EDSR is implemented by Bee Lim. ""\n             ""@thstkdgus35 https://github.com/thstkdgus35/EDSR-PyTorch"")\n'"
VSR/Backend/Torch/Models/edsr/common.py,0,"b""import math\n\nimport torch\nimport torch.nn as nn\n\n\ndef default_conv(in_channels, out_channels, kernel_size, bias=True):\n  return nn.Conv2d(\n    in_channels, out_channels, kernel_size,\n    padding=(kernel_size // 2), bias=bias)\n\n\nclass MeanShift(nn.Conv2d):\n  def __init__(\n      self, rgb_range,\n      rgb_mean=(0.4488, 0.4371, 0.4040), rgb_std=(1.0, 1.0, 1.0), sign=-1):\n    super(MeanShift, self).__init__(3, 3, kernel_size=1)\n    std = torch.Tensor(rgb_std)\n    self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n    self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean) / std\n    for p in self.parameters():\n      p.requires_grad = False\n\n\nclass BasicBlock(nn.Sequential):\n  def __init__(\n      self, conv, in_channels, out_channels, kernel_size, stride=1, bias=False,\n      bn=True, act=nn.ReLU(True)):\n\n    m = [conv(in_channels, out_channels, kernel_size, bias=bias)]\n    if bn:\n      m.append(nn.BatchNorm2d(out_channels))\n    if act is not None:\n      m.append(act)\n\n    super(BasicBlock, self).__init__(*m)\n\n\nclass ResBlock(nn.Module):\n  def __init__(\n      self, conv, n_feats, kernel_size,\n      bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n    super(ResBlock, self).__init__()\n    m = []\n    for i in range(2):\n      m.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n      if bn:\n        m.append(nn.BatchNorm2d(n_feats))\n      if i == 0:\n        m.append(act)\n\n    self.body = nn.Sequential(*m)\n    self.res_scale = res_scale\n\n  def forward(self, x):\n    res = self.body(x).mul(self.res_scale)\n    res += x\n\n    return res\n\n\nclass Upsampler(nn.Sequential):\n  def __init__(self, conv, scale, n_feats, bn=False, act=False, bias=True):\n\n    m = []\n    if (scale & (scale - 1)) == 0:  # Is scale = 2^n?\n      for _ in range(int(math.log(scale, 2))):\n        m.append(conv(n_feats, 4 * n_feats, 3, bias))\n        m.append(nn.PixelShuffle(2))\n        if bn:\n          m.append(nn.BatchNorm2d(n_feats))\n        if act == 'relu':\n          m.append(nn.ReLU(True))\n        elif act == 'prelu':\n          m.append(nn.PReLU(n_feats))\n\n    elif scale == 3:\n      m.append(conv(n_feats, 9 * n_feats, 3, bias))\n      m.append(nn.PixelShuffle(3))\n      if bn:\n        m.append(nn.BatchNorm2d(n_feats))\n      if act == 'relu':\n        m.append(nn.ReLU(True))\n      elif act == 'prelu':\n        m.append(nn.PReLU(n_feats))\n    else:\n      raise NotImplementedError\n\n    super(Upsampler, self).__init__(*m)\n"""
VSR/Backend/Torch/Models/edsr/edsr.py,0,"b'import torch.nn as nn\n\nfrom . import common\n\nurl = {\n  \'r16f64x2\': \'https://cv.snu.ac.kr/research/EDSR/models/edsr_baseline_x2-1bc95232.pt\',\n  \'r16f64x3\': \'https://cv.snu.ac.kr/research/EDSR/models/edsr_baseline_x3-abf2a44e.pt\',\n  \'r16f64x4\': \'https://cv.snu.ac.kr/research/EDSR/models/edsr_baseline_x4-6b446fab.pt\',\n  \'r32f256x2\': \'https://cv.snu.ac.kr/research/EDSR/models/edsr_x2-0edfb8a3.pt\',\n  \'r32f256x3\': \'https://cv.snu.ac.kr/research/EDSR/models/edsr_x3-ea3ef2c6.pt\',\n  \'r32f256x4\': \'https://cv.snu.ac.kr/research/EDSR/models/edsr_x4-4f62e9ef.pt\'\n}\n\n\ndef make_model(args, parent=False):\n  return EDSR(args)\n\n\nclass EDSR(nn.Module):\n  def __init__(self, args, conv=common.default_conv):\n    super(EDSR, self).__init__()\n\n    n_resblocks = args.n_resblocks\n    n_feats = args.n_feats\n    kernel_size = 3\n    scale = args.scale[0]\n    act = nn.ReLU(True)\n    self.url = url[\'r{}f{}x{}\'.format(n_resblocks, n_feats, scale)]\n    self.sub_mean = common.MeanShift(args.rgb_range)\n    self.add_mean = common.MeanShift(args.rgb_range, sign=1)\n\n    # define head module\n    m_head = [conv(args.n_colors, n_feats, kernel_size)]\n\n    # define body module\n    m_body = [\n      common.ResBlock(\n        conv, n_feats, kernel_size, act=act, res_scale=args.res_scale\n      ) for _ in range(n_resblocks)\n    ]\n    m_body.append(conv(n_feats, n_feats, kernel_size))\n\n    # define tail module\n    m_tail = [\n      common.Upsampler(conv, scale, n_feats, act=False),\n      conv(n_feats, args.n_colors, kernel_size)\n    ]\n\n    self.head = nn.Sequential(*m_head)\n    self.body = nn.Sequential(*m_body)\n    self.tail = nn.Sequential(*m_tail)\n\n  def forward(self, x):\n    x = self.sub_mean(x)\n    x = self.head(x)\n\n    res = self.body(x)\n    res += x\n\n    x = self.tail(res)\n    x = self.add_mean(x)\n\n    return x\n\n  def load_state_dict(self, state_dict, strict=True):\n    own_state = self.state_dict()\n    for name, param in state_dict.items():\n      if name in own_state:\n        if isinstance(param, nn.Parameter):\n          param = param.data\n        try:\n          own_state[name].copy_(param)\n        except Exception:\n          if name.find(\'tail\') == -1:\n            raise RuntimeError(\'While copying the parameter named {}, \'\n                               \'whose dimensions in the model are {} and \'\n                               \'whose dimensions in the checkpoint are {}.\'\n                               .format(name, own_state[name].size(),\n                                       param.size()))\n      elif strict:\n        if name.find(\'tail\') == -1:\n          raise KeyError(\'unexpected key ""{}"" in state_dict\'\n                         .format(name))\n'"
VSR/Backend/Torch/Models/edsr/mdsr.py,0,"b""import torch.nn as nn\nfrom . import common\n\nurl = {\n  'r16f64': 'https://cv.snu.ac.kr/research/EDSR/models/mdsr_baseline-a00cab12.pt',\n  'r80f64': 'https://cv.snu.ac.kr/research/EDSR/models/mdsr-4a78bedf.pt'\n}\n\n\ndef make_model(args, parent=False):\n  return MDSR(args)\n\n\nclass MDSR(nn.Module):\n  def __init__(self, args, conv=common.default_conv):\n    super(MDSR, self).__init__()\n    n_resblocks = args.n_resblocks\n    n_feats = args.n_feats\n    kernel_size = 3\n    act = nn.ReLU(True)\n    self.scale_idx = 0\n    self.url = url['r{}f{}'.format(n_resblocks, n_feats)]\n    self.sub_mean = common.MeanShift(args.rgb_range)\n    self.add_mean = common.MeanShift(args.rgb_range, sign=1)\n\n    m_head = [conv(args.n_colors, n_feats, kernel_size)]\n\n    self.pre_process = nn.ModuleList([\n      nn.Sequential(\n        common.ResBlock(conv, n_feats, 5, act=act),\n        common.ResBlock(conv, n_feats, 5, act=act)\n      ) for _ in args.scale\n    ])\n\n    m_body = [\n      common.ResBlock(\n        conv, n_feats, kernel_size, act=act\n      ) for _ in range(n_resblocks)\n    ]\n    m_body.append(conv(n_feats, n_feats, kernel_size))\n\n    self.upsample = nn.ModuleList([\n      common.Upsampler(conv, s, n_feats, act=False) for s in args.scale\n    ])\n\n    m_tail = [conv(n_feats, args.n_colors, kernel_size)]\n\n    self.head = nn.Sequential(*m_head)\n    self.body = nn.Sequential(*m_body)\n    self.tail = nn.Sequential(*m_tail)\n\n  def forward(self, x):\n    x = self.sub_mean(x)\n    x = self.head(x)\n    x = self.pre_process[self.scale_idx](x)\n\n    res = self.body(x)\n    res += x\n\n    x = self.upsample[self.scale_idx](res)\n    x = self.tail(x)\n    x = self.add_mean(x)\n\n    return x\n\n  def set_scale(self, scale_idx):\n    self.scale_idx = scale_idx\n"""
VSR/Backend/Torch/Models/esrgan/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport logging\n_logger = logging.getLogger(""VSR.ESRGAN"")\n_logger.info(""LICENSE: ESRGAN is implemented by Xintao Wang. ""\n             ""@xinntao https://github.com/xinntao/ESRGAN"")\n'"
VSR/Backend/Torch/Models/esrgan/architecture.py,0,"b""import math\n\nimport torch.nn as nn\n\nfrom . import block as B\n\n\nclass RRDB_Net(nn.Module):\n  def __init__(self, in_nc, out_nc, nf, nb, gc=32, upscale=4, norm_type=None,\n               act_type='leakyrelu', mode='CNA', res_scale=1,\n               upsample_mode='upconv'):\n    super(RRDB_Net, self).__init__()\n    n_upscale = int(math.log(upscale, 2))\n    if upscale == 3:\n      n_upscale = 1\n\n    fea_conv = B.conv_block(in_nc, nf, kernel_size=3, norm_type=None,\n                            act_type=None)\n    rb_blocks = [\n      B.RRDB(nf, kernel_size=3, gc=32, stride=1, bias=True, pad_type='zero',\n             norm_type=norm_type, act_type=act_type, mode='CNA') for _ in\n      range(nb)]\n    LR_conv = B.conv_block(nf, nf, kernel_size=3, norm_type=norm_type,\n                           act_type=None, mode=mode)\n\n    if upsample_mode == 'upconv':\n      upsample_block = B.upconv_blcok\n    elif upsample_mode == 'pixelshuffle':\n      upsample_block = B.pixelshuffle_block\n    else:\n      raise NotImplementedError(\n        'upsample mode [%s] is not found' % upsample_mode)\n    if upscale == 3:\n      upsampler = upsample_block(nf, nf, 3, act_type=act_type)\n    else:\n      upsampler = [upsample_block(nf, nf, act_type=act_type) for _ in\n                   range(n_upscale)]\n    HR_conv0 = B.conv_block(nf, nf, kernel_size=3, norm_type=None,\n                            act_type=act_type)\n    HR_conv1 = B.conv_block(nf, out_nc, kernel_size=3, norm_type=None,\n                            act_type=None)\n\n    self.model = B.sequential(fea_conv, B.ShortcutBlock(\n      B.sequential(*rb_blocks, LR_conv)), *upsampler, HR_conv0, HR_conv1)\n\n  def forward(self, x):\n    x = self.model(x)\n    return x\n"""
VSR/Backend/Torch/Models/esrgan/block.py,0,"b'from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\n\n####################\n# Basic blocks\n####################\n\n\ndef act(act_type, inplace=True, neg_slope=0.2, n_prelu=1):\n  # helper selecting activation\n  # neg_slope: for leakyrelu and init of prelu\n  # n_prelu: for p_relu num_parameters\n  act_type = act_type.lower()\n  if act_type == \'relu\':\n    layer = nn.ReLU(inplace)\n  elif act_type == \'leakyrelu\':\n    layer = nn.LeakyReLU(neg_slope, inplace)\n  elif act_type == \'prelu\':\n    layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n  else:\n    raise NotImplementedError(\'activation layer [%s] is not found\' % act_type)\n  return layer\n\n\ndef norm(norm_type, nc):\n  # helper selecting normalization layer\n  norm_type = norm_type.lower()\n  if norm_type == \'batch\':\n    layer = nn.BatchNorm2d(nc, affine=True)\n  elif norm_type == \'instance\':\n    layer = nn.InstanceNorm2d(nc, affine=False)\n  else:\n    raise NotImplementedError(\n      \'normalization layer [%s] is not found\' % norm_type)\n  return layer\n\n\ndef pad(pad_type, padding):\n  # helper selecting padding layer\n  # if padding is \'zero\', do by conv layers\n  pad_type = pad_type.lower()\n  if padding == 0:\n    return None\n  if pad_type == \'reflect\':\n    layer = nn.ReflectionPad2d(padding)\n  elif pad_type == \'replicate\':\n    layer = nn.ReplicationPad2d(padding)\n  else:\n    raise NotImplementedError(\n      \'padding layer [%s] is not implemented\' % pad_type)\n  return layer\n\n\ndef get_valid_padding(kernel_size, dilation):\n  kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n  padding = (kernel_size - 1) // 2\n  return padding\n\n\nclass ConcatBlock(nn.Module):\n  # Concat the output of a submodule to its input\n  def __init__(self, submodule):\n    super(ConcatBlock, self).__init__()\n    self.sub = submodule\n\n  def forward(self, x):\n    output = torch.cat((x, self.sub(x)), dim=1)\n    return output\n\n  def __repr__(self):\n    tmpstr = \'Identity .. \\n|\'\n    modstr = self.sub.__repr__().replace(\'\\n\', \'\\n|\')\n    tmpstr = tmpstr + modstr\n    return tmpstr\n\n\nclass ShortcutBlock(nn.Module):\n  # Elementwise sum the output of a submodule to its input\n  def __init__(self, submodule):\n    super(ShortcutBlock, self).__init__()\n    self.sub = submodule\n\n  def forward(self, x):\n    output = x + self.sub(x)\n    return output\n\n  def __repr__(self):\n    tmpstr = \'Identity + \\n|\'\n    modstr = self.sub.__repr__().replace(\'\\n\', \'\\n|\')\n    tmpstr = tmpstr + modstr\n    return tmpstr\n\n\ndef sequential(*args):\n  # Flatten Sequential. It unwraps nn.Sequential.\n  if len(args) == 1:\n    if isinstance(args[0], OrderedDict):\n      raise NotImplementedError(\n        \'sequential does not support OrderedDict input.\')\n    return args[0]  # No sequential is needed.\n  modules = []\n  for module in args:\n    if isinstance(module, nn.Sequential):\n      for submodule in module.children():\n        modules.append(submodule)\n    elif isinstance(module, nn.Module):\n      modules.append(module)\n  return nn.Sequential(*modules)\n\n\ndef conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1,\n               bias=True,\n               pad_type=\'zero\', norm_type=None, act_type=\'relu\', mode=\'CNA\'):\n  """"""\n  Conv layer with padding, normalization, activation\n  mode: CNA --> Conv -> Norm -> Act\n      NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)\n  """"""\n  assert mode in [\'CNA\', \'NAC\', \'CNAC\'], \'Wong conv mode [%s]\' % mode\n  padding = get_valid_padding(kernel_size, dilation)\n  p = pad(pad_type, padding) if pad_type and pad_type != \'zero\' else None\n  padding = padding if pad_type == \'zero\' else 0\n\n  c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride,\n                padding=padding, \\\n                dilation=dilation, bias=bias, groups=groups)\n  a = act(act_type) if act_type else None\n  if \'CNA\' in mode:\n    n = norm(norm_type, out_nc) if norm_type else None\n    return sequential(p, c, n, a)\n  elif mode == \'NAC\':\n    if norm_type is None and act_type is not None:\n      a = act(act_type, inplace=False)\n      # Important!\n      # input----ReLU(inplace)----Conv--+----output\n      #        |________________________|\n      # inplace ReLU will modify the input, therefore wrong output\n    n = norm(norm_type, in_nc) if norm_type else None\n    return sequential(n, a, p, c)\n\n\n####################\n# Useful blocks\n####################\n\n\nclass ResNetBlock(nn.Module):\n  """"""\n  ResNet Block, 3-3 style\n  with extra residual scaling used in EDSR\n  (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW 17)\n  """"""\n\n  def __init__(self, in_nc, mid_nc, out_nc, kernel_size=3, stride=1, dilation=1,\n               groups=1, \\\n               bias=True, pad_type=\'zero\', norm_type=None, act_type=\'relu\',\n               mode=\'CNA\', res_scale=1):\n    super(ResNetBlock, self).__init__()\n    conv0 = conv_block(in_nc, mid_nc, kernel_size, stride, dilation, groups,\n                       bias, pad_type, \\\n                       norm_type, act_type, mode)\n    if mode == \'CNA\':\n      act_type = None\n    if mode == \'CNAC\':  # Residual path: |-CNAC-|\n      act_type = None\n      norm_type = None\n    conv1 = conv_block(mid_nc, out_nc, kernel_size, stride, dilation, groups,\n                       bias, pad_type, \\\n                       norm_type, act_type, mode)\n    # if in_nc != out_nc:\n    #     self.project = conv_block(in_nc, out_nc, 1, stride, dilation, 1, bias, pad_type, \\\n    #         None, None)\n    #     print(\'Need a projecter in ResNetBlock.\')\n    # else:\n    #     self.project = lambda x:x\n    self.res = sequential(conv0, conv1)\n    self.res_scale = res_scale\n\n  def forward(self, x):\n    res = self.res(x).mul(self.res_scale)\n    return x + res\n\n\nclass ResidualDenseBlock_5C(nn.Module):\n  """"""\n  Residual Dense Block\n  style: 5 convs\n  The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n  """"""\n\n  def __init__(self, nc, kernel_size=3, gc=32, stride=1, bias=True,\n               pad_type=\'zero\', \\\n               norm_type=None, act_type=\'leakyrelu\', mode=\'CNA\'):\n    super(ResidualDenseBlock_5C, self).__init__()\n    # gc: growth channel, i.e. intermediate channels\n    self.conv1 = conv_block(nc, gc, kernel_size, stride, bias=bias,\n                            pad_type=pad_type, \\\n                            norm_type=norm_type, act_type=act_type, mode=mode)\n    self.conv2 = conv_block(nc + gc, gc, kernel_size, stride, bias=bias,\n                            pad_type=pad_type, \\\n                            norm_type=norm_type, act_type=act_type, mode=mode)\n    self.conv3 = conv_block(nc + 2 * gc, gc, kernel_size, stride, bias=bias,\n                            pad_type=pad_type, \\\n                            norm_type=norm_type, act_type=act_type, mode=mode)\n    self.conv4 = conv_block(nc + 3 * gc, gc, kernel_size, stride, bias=bias,\n                            pad_type=pad_type, \\\n                            norm_type=norm_type, act_type=act_type, mode=mode)\n    if mode == \'CNA\':\n      last_act = None\n    else:\n      last_act = act_type\n    self.conv5 = conv_block(nc + 4 * gc, nc, 3, stride, bias=bias,\n                            pad_type=pad_type, \\\n                            norm_type=norm_type, act_type=last_act, mode=mode)\n\n  def forward(self, x):\n    x1 = self.conv1(x)\n    x2 = self.conv2(torch.cat((x, x1), 1))\n    x3 = self.conv3(torch.cat((x, x1, x2), 1))\n    x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n    x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n    return x5.mul(0.2) + x\n\n\nclass RRDB(nn.Module):\n  """"""\n  Residual in Residual Dense Block\n  """"""\n\n  def __init__(self, nc, kernel_size=3, gc=32, stride=1, bias=True,\n               pad_type=\'zero\', \\\n               norm_type=None, act_type=\'leakyrelu\', mode=\'CNA\'):\n    super(RRDB, self).__init__()\n    self.RDB1 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias,\n                                      pad_type, \\\n                                      norm_type, act_type, mode)\n    self.RDB2 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias,\n                                      pad_type, \\\n                                      norm_type, act_type, mode)\n    self.RDB3 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias,\n                                      pad_type, \\\n                                      norm_type, act_type, mode)\n\n  def forward(self, x):\n    out = self.RDB1(x)\n    out = self.RDB2(out)\n    out = self.RDB3(out)\n    return out.mul(0.2) + x\n\n\n####################\n# Upsampler\n####################\n\n\ndef pixelshuffle_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1,\n                       bias=True,\n                       pad_type=\'zero\', norm_type=None, act_type=\'relu\'):\n  """"""\n  Pixel shuffle layer\n  (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional\n  Neural Network, CVPR17)\n  """"""\n  conv = conv_block(in_nc, out_nc * (upscale_factor ** 2), kernel_size, stride,\n                    bias=bias,\n                    pad_type=pad_type, norm_type=None, act_type=None)\n  pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n  n = norm(norm_type, out_nc) if norm_type else None\n  a = act(act_type) if act_type else None\n  return sequential(conv, pixel_shuffle, n, a)\n\n\ndef upconv_blcok(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1,\n                 bias=True,\n                 pad_type=\'zero\', norm_type=None, act_type=\'relu\',\n                 mode=\'nearest\'):\n  # Up conv\n  # described in https://distill.pub/2016/deconv-checkerboard/\n  upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)\n  conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias,\n                    pad_type=pad_type, norm_type=norm_type, act_type=act_type)\n  return sequential(upsample, conv)\n'"
VSR/Backend/Torch/Models/frvsr/__init__.py,0,b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/4 \xe4\xb8\x8b\xe5\x8d\x888:51\n'
VSR/Backend/Torch/Models/frvsr/ops.py,0,"b""#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/4 \xe4\xb8\x8b\xe5\x8d\x888:51\n\nimport torch\nfrom torch import nn\nfrom torch.nn.functional import interpolate\nfrom ..Arch import Upsample\n\n\nclass BilinerUp(nn.Module):\n  def __init__(self, scale_factor):\n    super(BilinerUp, self).__init__()\n    self.scale = scale_factor\n\n  def forward(self, x):\n    return interpolate(x, scale_factor=self.scale,\n                       mode='bilinear', align_corners=False)\n\n\nclass FNet(nn.Module):\n  def __init__(self, channel, gain=32):\n    super(FNet, self).__init__()\n    f = 32\n    layers = []\n    in_c = channel * 2\n    for i in range(3):\n      layers += [nn.Conv2d(in_c, f, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True)]\n      layers += [nn.Conv2d(f, f, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True)]\n      layers += [nn.MaxPool2d(2)]\n      in_c = f\n      f *= 2\n    for i in range(3):\n      layers += [nn.Conv2d(in_c, f, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True)]\n      layers += [nn.Conv2d(f, f, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True)]\n      layers += [BilinerUp(2)]\n      in_c = f\n      f //= 2\n    layers += [nn.Conv2d(in_c, f, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True)]\n    layers += [nn.Conv2d(f, 2, 3, 1, 1), nn.Tanh()]\n    self.body = nn.Sequential(*layers)\n    self.gain = gain\n\n  def forward(self, *inputs):\n    x = torch.cat(inputs, dim=1)\n    return self.body(x) * self.gain\n\n\nclass RB(nn.Module):\n  def __init__(self, channel):\n    super(RB, self).__init__()\n    conv1 = nn.Conv2d(channel, channel, 3, 1, 1)\n    conv2 = nn.Conv2d(channel, channel, 3, 1, 1)\n    self.body = nn.Sequential(conv1, nn.ReLU(True), conv2)\n\n  def forward(self, x):\n    return x + self.body(x)\n\n\nclass SRNet(nn.Module):\n  def __init__(self, channel, scale, n_rb=10):\n    super(SRNet, self).__init__()\n    rbs = [RB(64) for _ in range(n_rb)]\n    entry = [nn.Conv2d(channel * (scale ** 2 + 1), 64, 3, 1, 1), nn.ReLU(True)]\n    up = Upsample(64, scale, method='ps')\n    out = nn.Conv2d(64, channel, 3, 1, 1)\n    self.body = nn.Sequential(*entry, *rbs, up, out)\n\n  def forward(self, *inputs):\n    x = torch.cat(inputs, dim=1)\n    return self.body(x)\n"""
VSR/Backend/Torch/Models/msrn/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport logging\n_logger = logging.getLogger(""VSR.MSRN"")\n_logger.info(""LICENSE: MSRN is implemented by Juncheng Li. ""\n             ""@MIVRC https://github.com/MIVRC/MSRN-PyTorch"")\n'"
VSR/Backend/Torch/Models/msrn/msrn.py,0,"b'import torch\nimport torch.nn as nn\n\nfrom ..rcan import common\n\n\ndef make_model(args, parent=False):\n  return MSRN(args)\n\n\nclass MSRB(nn.Module):\n  def __init__(self, conv=common.default_conv, n_feats=64):\n    super(MSRB, self).__init__()\n\n    kernel_size_1 = 3\n    kernel_size_2 = 5\n\n    self.conv_3_1 = conv(n_feats, n_feats, kernel_size_1)\n    self.conv_3_2 = conv(n_feats * 2, n_feats * 2, kernel_size_1)\n    self.conv_5_1 = conv(n_feats, n_feats, kernel_size_2)\n    self.conv_5_2 = conv(n_feats * 2, n_feats * 2, kernel_size_2)\n    self.confusion = nn.Conv2d(n_feats * 4, n_feats, 1, padding=0, stride=1)\n    self.relu = nn.ReLU(inplace=True)\n\n  def forward(self, x):\n    input_1 = x\n    output_3_1 = self.relu(self.conv_3_1(input_1))\n    output_5_1 = self.relu(self.conv_5_1(input_1))\n    input_2 = torch.cat([output_3_1, output_5_1], 1)\n    output_3_2 = self.relu(self.conv_3_2(input_2))\n    output_5_2 = self.relu(self.conv_5_2(input_2))\n    input_3 = torch.cat([output_3_2, output_5_2], 1)\n    output = self.confusion(input_3)\n    output += x\n    return output\n\n\nclass MSRN(nn.Module):\n  def __init__(self, args, conv=common.default_conv):\n    super(MSRN, self).__init__()\n\n    n_feats = 64\n    n_blocks = 8\n    kernel_size = 3\n    scale = args.scale[0]\n    act = nn.ReLU(True)\n\n    self.n_blocks = n_blocks\n\n    # RGB mean for DIV2K\n    rgb_mean = (0.4488, 0.4371, 0.4040)\n    rgb_std = (1.0, 1.0, 1.0)\n    self.sub_mean = common.MeanShift(args.rgb_range, rgb_mean, rgb_std)\n\n    # define head module\n    modules_head = [conv(args.n_colors, n_feats, kernel_size)]\n\n    # define body module\n    modules_body = nn.ModuleList()\n    for i in range(n_blocks):\n      modules_body.append(\n        MSRB(n_feats=n_feats))\n\n    # define tail module\n    modules_tail = [\n      nn.Conv2d(n_feats * (self.n_blocks + 1), n_feats, 1, padding=0, stride=1),\n      conv(n_feats, n_feats, kernel_size),\n      common.Upsampler(conv, scale, n_feats, act=False),\n      conv(n_feats, args.n_colors, kernel_size)]\n\n    self.add_mean = common.MeanShift(args.rgb_range, rgb_mean, rgb_std, 1)\n\n    self.head = nn.Sequential(*modules_head)\n    self.body = nn.Sequential(*modules_body)\n    self.tail = nn.Sequential(*modules_tail)\n\n  def forward(self, x):\n    x = self.sub_mean(x)\n    x = self.head(x)\n    res = x\n\n    MSRB_out = []\n    for i in range(self.n_blocks):\n      x = self.body[i](x)\n      MSRB_out.append(x)\n    MSRB_out.append(res)\n\n    res = torch.cat(MSRB_out, 1)\n    x = self.tail(res)\n    x = self.add_mean(x)\n    return x\n\n  def load_state_dict(self, state_dict, strict=False):\n    own_state = self.state_dict()\n    for name, param in state_dict.items():\n      if name in own_state:\n        if isinstance(param, nn.Parameter):\n          param = param.data\n        try:\n          own_state[name].copy_(param)\n        except Exception:\n          if name.find(\'tail\') >= 0:\n            print(\'Replace pre-trained upsampler to new one...\')\n          else:\n            raise RuntimeError(\'While copying the parameter named {}, \'\n                               \'whose dimensions in the model are {} and \'\n                               \'whose dimensions in the checkpoint are {}.\'\n                               .format(name, own_state[name].size(),\n                                       param.size()))\n      elif strict:\n        if name.find(\'tail\') == -1:\n          raise KeyError(\'unexpected key ""{}"" in state_dict\'\n                         .format(name))\n\n    if strict:\n      missing = set(own_state.keys()) - set(state_dict.keys())\n      if len(missing) > 0:\n        raise KeyError(\'missing keys in state_dict: ""{}""\'.format(missing))\n'"
VSR/Backend/Torch/Models/ntire19/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/20 \xe4\xb8\x8a\xe5\x8d\x8811:47\n\nimport logging\n_logger = logging.getLogger(""VSR.NTIRE2019"")\n_logger.info(""Top rank models in NTIRE 2019.""\n             ""Denoise: sRGB and Real Super-Resolution"")\n'"
VSR/Backend/Torch/Models/ntire19/denoise.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/20 \xe4\xb8\x8a\xe5\x8d\x8811:47\n\nimport logging\n\nimport torch\nimport torch.nn as nn\n\nfrom ..Arch import Activation, Rdb, SpaceToDepth, CBAM\n\n_logger = logging.getLogger(""VSR.NTIRE2019.Denoise"")\n\n\nclass EraserTeam:\n  class URB(nn.Module):\n    def __init__(self, filters, depth, act=None):\n      super(EraserTeam.URB, self).__init__()\n      convs = []\n      for i in range(depth):\n        convs.append(nn.Conv2d(filters, filters, 3, 1, 1))\n        if act:\n          convs.append(Activation(act))\n      self.body = nn.Sequential(*convs)\n\n    def forward(self, x):\n      return x + self.body(x)\n\n  class UModule(nn.Module):\n    def __init__(self, filters):\n      super(EraserTeam.UModule, self).__init__()\n      self.head = EraserTeam.URB(filters, 2, \'PReLU\')\n      self.down = nn.Sequential(\n        nn.Conv2d(filters, filters * 2, 3, 2, 1),\n        EraserTeam.URB(filters * 2, 1, \'PReLU\'))\n      self.downup = nn.Sequential(\n        nn.Conv2d(filters * 2, filters * 4, 3, 2, 1),\n        EraserTeam.URB(filters * 4, 1, \'PReLU\'),\n        nn.Conv2d(filters * 4, filters * 8, 1), nn.PixelShuffle(2))\n      self.up = nn.Sequential(\n        nn.Conv2d(filters * 4, filters * 2, 1),\n        EraserTeam.URB(filters * 2, 1, \'PReLU\'),\n        nn.Conv2d(filters * 2, filters * 4, 1), nn.PixelShuffle(2))\n      self.tail = nn.Sequential(\n        nn.Conv2d(filters * 2, filters, 1),\n        EraserTeam.URB(filters, 2, \'PReLU\'),\n        nn.Conv2d(filters, filters, 3, 1, 1), nn.PReLU())\n\n    def forward(self, inputs):\n      c0 = inputs\n      c1 = self.head(inputs)\n      c2 = self.down(c1)\n      c3 = self.downup(c2)\n      c4 = self.up(torch.cat([c3, c2], dim=1))\n      c5 = self.tail(torch.cat([c4, c1], dim=1))\n      return c5 + c0\n\n  class DIDN(nn.Module):\n    def __init__(self, channels=3, filters=32, n_modules=8):\n      _logger.info(""DIDN was introduced by Songhyun Yu @Hanyang University, ""\n                   ""Implemented by @LoSealL"")\n      super(EraserTeam.DIDN, self).__init__()\n      self.head = nn.Sequential(\n        nn.Conv2d(channels, filters, 3, 1, 1), nn.PReLU(),\n        nn.Conv2d(filters, filters * 2, 3, 2, 1))\n      self.tail = nn.Sequential(\n        nn.Conv2d(filters * n_modules, filters * 4, 1),\n        EraserTeam.URB(filters * 4, 1, \'PReLU\'),\n        nn.PixelShuffle(2),\n        nn.Conv2d(filters, channels, 3, 1, 1), nn.PReLU())\n      self.recon = nn.Sequential(\n        nn.Conv2d(filters * 2, filters, 3, 1, 1), nn.PReLU())\n      for i in range(n_modules):\n        setattr(self, f\'umod_{i:02d}\', EraserTeam.UModule(filters * 2))\n      self.nu = n_modules\n\n    def forward(self, inputs):\n      c0 = inputs\n      x = self.head(c0)\n      u_out = []\n      for i in range(self.nu):\n        x = getattr(self, f\'umod_{i:02d}\')(x)\n        u_out.append(x)\n      u_out = [self.recon(t) for t in u_out]\n      x = self.tail(torch.cat(u_out, dim=1))\n      return c0 + x\n\n  class DCR(nn.Module):\n    def __init__(self, filters):\n      super(EraserTeam.DCR, self).__init__()\n      self.conv1 = nn.Sequential(\n        nn.Conv2d(filters, filters // 2, 3, 1, 1), nn.PReLU())\n      self.conv2 = nn.Sequential(\n        nn.Conv2d(filters * 3 // 2, filters // 2, 3, 1, 1), nn.PReLU())\n      self.conv3 = nn.Sequential(\n        nn.Conv2d(filters * 2, filters, 3, 1, 1), nn.PReLU())\n\n    def forward(self, x):\n      c0 = x\n      c1 = self.conv1(c0)\n      c2 = self.conv2(torch.cat([c0, c1], dim=1))\n      c3 = self.conv3(torch.cat([c0, c1, c2], dim=1))\n      return c3 + c0\n\n  class DHDN(nn.Module):\n    def __init__(self, channels=3, filters=128):\n      _logger.info(""DHDN was introduced by Songhyun Yu @Hanyang University, ""\n                   ""Implemented by @LoSealL"")\n      super(EraserTeam.DHDN, self).__init__()\n      self.entry = nn.Sequential(\n        nn.Conv2d(channels, filters, 1), nn.PReLU(),\n        EraserTeam.DCR(filters), EraserTeam.DCR(filters))\n      self.exit = nn.Sequential(\n        EraserTeam.DCR(filters * 2), EraserTeam.DCR(filters * 2),\n        nn.Conv2d(filters * 2, channels, 1), nn.PReLU())\n      self.down1 = nn.Sequential(\n        nn.Conv2d(filters, filters * 2, 3, 2, 1),\n        EraserTeam.DCR(filters * 2), EraserTeam.DCR(filters * 2))\n      self.down2 = nn.Sequential(\n        nn.Conv2d(filters * 2, filters * 4, 3, 2, 1),\n        EraserTeam.DCR(filters * 4), EraserTeam.DCR(filters * 4))\n      self.downup = nn.Sequential(\n        nn.Conv2d(filters * 4, filters * 8, 3, 2, 1),\n        EraserTeam.DCR(filters * 8), EraserTeam.DCR(filters * 8),\n        nn.ConvTranspose2d(filters * 8, filters * 4, 3, 2, 1, 1))\n      self.up1 = nn.Sequential(\n        EraserTeam.DCR(filters * 8), EraserTeam.DCR(filters * 8),\n        nn.ConvTranspose2d(filters * 8, filters * 2, 3, 2, 1, 1))\n      self.up2 = nn.Sequential(\n        EraserTeam.DCR(filters * 4), EraserTeam.DCR(filters * 4),\n        nn.ConvTranspose2d(filters * 4, filters, 3, 2, 1, 1))\n\n    def forward(self, inputs):\n      c0 = inputs\n      c1 = self.entry(c0)\n      c2 = self.down1(c1)\n      c3 = self.down2(c2)\n      c4 = self.downup(c3)\n      c5 = self.up1(torch.cat([c4, c3], dim=1))\n      c6 = self.up2(torch.cat([c5, c2], dim=1))\n      c7 = self.exit(torch.cat([c6, c1], dim=1))\n      return c7 + c0\n\n\nclass DGUTeam:\n  class GRDB(nn.Module):\n    def __init__(self, n_rdb, filters):\n      super(DGUTeam.GRDB, self).__init__()\n      for i in range(n_rdb):\n        setattr(self, f\'rdb_{i:02d}\', Rdb(filters))\n      self.n_rdb = n_rdb\n      self.conv = nn.Conv2d(filters * n_rdb, filters, 1)\n\n    def forward(self, x):\n      rdb_o = [x]\n      for i in range(self.n_rdb):\n        rdb_o.append(getattr(self, f\'rdb_{i:02d}\')(rdb_o[-1]))\n      return x + self.conv(torch.cat(rdb_o[1:], dim=1))\n\n  class GRDN(nn.Module):\n    def __init__(self, channels, filters=64, n_grdb=10, n_rdb=4):\n      _logger.info(""GRDN was introduced by Seung-Won Jung @Dongguk University, ""\n                   ""Implemented by @LoSealL"")\n      super(DGUTeam.GRDN, self).__init__()\n      nets = [nn.Conv2d(channels, filters, 3, 1, 1)]\n      for _ in range(n_grdb):\n        nets.append(DGUTeam.GRDB(n_rdb, filters))\n      nets.append(nn.ConvTranspose2d(filters, filters, 3, 2, 1, 1))\n      self.head = nn.Sequential(*nets)\n      self.cbam = CBAM(filters)\n      self.tail = nn.Conv2d(filters, channels, 3, 1, 1)\n\n    def forward(self, inputs):\n      c0 = inputs\n      x = self.head(c0)\n      x = self.cbam(x)\n      x = self.tail(x)\n      return x + c0\n\n\nclass HITVPCTeam:\n  class RB(nn.Module):\n    def __init__(self, filters):\n      super(HITVPCTeam.RB, self).__init__()\n      self.conv1 = nn.Conv2d(filters, filters, 3, 1, 1)\n      self.act = nn.ReLU(True)\n      self.conv2 = nn.Conv2d(filters, filters, 3, 1, 1)\n\n    def forward(self, x):\n      c0 = x\n      x = self.conv1(x)\n      x = self.act(x)\n      x = self.conv2(x)\n      return x + c0\n\n  class NRB(nn.Module):\n    def __init__(self, n, f):\n      super(HITVPCTeam.NRB, self).__init__()\n      nets = []\n      for i in range(n):\n        nets.append(HITVPCTeam.RB(f))\n      self.body = nn.Sequential(*nets)\n\n    def forward(self, x):\n      return self.body(x)\n\n  class ResUNet(nn.Module):\n    def __init__(self, channels, filters=128, n_rb=10):\n      _logger.info(""ResUNet was introduced by Kai Zhang @HIT, ""\n                   ""Implemented by @LoSealL"")\n      super(HITVPCTeam.ResUNet, self).__init__()\n      self.head = nn.Sequential(\n        SpaceToDepth(2),\n        nn.Conv2d(channels * 4, filters, 3, 1, 1),\n        nn.ReLU(True))\n      self.down1 = nn.Sequential(\n        HITVPCTeam.NRB(n_rb, filters),\n        nn.Conv2d(filters, filters, 3, 2, 1), nn.ReLU(True))\n      self.down2 = nn.Sequential(\n        HITVPCTeam.NRB(n_rb, filters),\n        nn.Conv2d(filters, filters, 3, 2, 1), nn.ReLU(True))\n      self.down3 = nn.Sequential(\n        HITVPCTeam.NRB(n_rb, filters),\n        nn.Conv2d(filters, filters, 3, 2, 1), nn.ReLU(True))\n      self.middle = HITVPCTeam.NRB(n_rb, filters)\n      self.up1 = nn.Sequential(\n        nn.ConvTranspose2d(filters, filters, 3, 2, 1, 1), nn.ReLU(True),\n        HITVPCTeam.NRB(n_rb, filters))\n      self.up2 = nn.Sequential(\n        nn.ConvTranspose2d(filters, filters, 3, 2, 1, 1), nn.ReLU(True),\n        HITVPCTeam.NRB(n_rb, filters))\n      self.up3 = nn.Sequential(\n        nn.ConvTranspose2d(filters, filters, 3, 2, 1, 1), nn.ReLU(True),\n        HITVPCTeam.NRB(n_rb, filters))\n      self.tail = nn.Sequential(\n        nn.Conv2d(filters, channels * 4, 3, 1, 1),\n        nn.PixelShuffle(2))\n\n    def forward(self, inputs):\n      c0 = inputs  # 512\n      c1 = self.head(c0)  # 256\n      c2 = self.down1(c1)  # 128\n      c3 = self.down2(c2)  # 64\n      c4 = self.down3(c3)  # 32\n      m = self.middle(c4) + c4\n      c5 = self.up1(m) + c3\n      c6 = self.up2(c5) + c2\n      c7 = self.up3(c6) + c1\n      return self.tail(c7) + c0\n'"
VSR/Backend/Torch/Models/ntire19/edrn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/11 \xe4\xb8\x8b\xe5\x8d\x887:35\n\nimport logging\n\nimport torch\nimport torch.nn as nn\n\n_logger = logging.getLogger(""VSR.EDRN"")\n\n\nclass CALayer(nn.Module):\n  def __init__(self, channel, wn, reduction=16):\n    super(CALayer, self).__init__()\n    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n    self.conv_du = nn.Sequential(\n      wn(nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True)),\n      nn.ReLU(inplace=True),\n      wn(nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True)),\n      nn.Sigmoid()\n    )\n\n  def forward(self, x):\n    y = self.avg_pool(x)\n    y = self.conv_du(y)\n    return x * y\n\n\nclass MeanShift(nn.Conv2d):\n  def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n    super(MeanShift, self).__init__(3, 3, kernel_size=1)\n    std = torch.Tensor(rgb_std)\n    self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n    self.weight.data.div_(std.view(3, 1, 1, 1))\n    self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n    self.bias.data.div_(std)\n    self.requires_grad = False\n    del std\n\n\nclass Residual_Block(nn.Module):\n  def __init__(self, inChannels, growRate0, wn, kSize=3, stride=1):\n    super(Residual_Block, self).__init__()\n    Cin = inChannels\n    G0 = growRate0\n    self.conv = nn.Sequential(*[\n      wn(nn.Conv2d(Cin, G0, kSize, padding=(kSize - 1) // 2, stride=stride)),\n      nn.ReLU(inplace=True),\n      wn(nn.Conv2d(G0, G0, kSize, padding=(kSize - 1) // 2, stride=stride)),\n      CALayer(Cin, wn, 16)\n    ])\n\n  def forward(self, x):\n    out = self.conv(x)\n    out += x\n    return out\n\n\nclass RG(nn.Module):\n  def __init__(self, growRate0, nConvLayers, wn, kSize=3):\n    super(RG, self).__init__()\n    G0 = growRate0\n    C = nConvLayers\n    convs_residual = []\n    for c in range(C):\n      convs_residual.append(Residual_Block(G0, G0, wn))\n    self.convs_residual = nn.Sequential(*convs_residual)\n    self.last_conv = wn(\n      nn.Conv2d(G0, G0, kSize, padding=(kSize - 1) // 2, stride=1))\n\n  def forward(self, x):\n    x = self.last_conv(self.convs_residual(x)) + x\n    return x\n\n\nclass EDRN(nn.Module):\n  def __init__(self, args):\n    _logger.info(""LICENSE: EDRN is implemented by IVIP-Lab. ""\n                 ""@yyknight https://github.com/yyknight/NTIRE2019_EDRN"")\n    super(EDRN, self).__init__()\n    r = args.scale[0]\n    G0 = args.G0\n    kSize = args.EDRNkSize\n    rgb_mean = (0.4313, 0.4162, 0.3861)\n    rgb_std = (1.0, 1.0, 1.0)\n    self.sub_mean = MeanShift(args.rgb_range, rgb_mean, rgb_std)\n    wn = lambda x: torch.nn.utils.weight_norm(x)\n    # number of RG blocks, conv layers, out channels\n    self.D, C, G = {\n      \'B\': (4, 10, 16),\n    }[args.EDRNconfig]\n\n    self.SFENet1 = wn(\n      nn.Conv2d(args.n_colors, G0, kSize, padding=(kSize - 1) // 2, stride=1))\n    self.encoder1 = nn.Sequential(*[\n      wn(nn.Conv2d(G0, 2 * G0, kSize, padding=(kSize - 1) // 2, stride=2)),\n      nn.BatchNorm2d(2 * G0),\n      nn.ReLU(inplace=True)])\n    self.encoder2 = nn.Sequential(*[\n      wn(nn.Conv2d(2 * G0, 4 * G0, kSize, padding=(kSize - 1) // 2, stride=2)),\n      nn.BatchNorm2d(4 * G0),\n      nn.ReLU(inplace=True)\n    ])\n    self.decoder1 = nn.Sequential(*[\n      wn(nn.ConvTranspose2d(4 * G0, 2 * G0, 3, padding=1, output_padding=1,\n                            stride=2)),\n      nn.BatchNorm2d(2 * G0),\n      nn.ReLU(inplace=True)])\n    self.decoder2 = nn.Sequential(*[\n      wn(nn.ConvTranspose2d(2 * G0, G0, 3, padding=1, output_padding=1,\n                            stride=2)),\n      nn.BatchNorm2d(G0),\n      nn.ReLU()\n    ])\n    RGs0 = [\n      RG(growRate0=4 * G0, nConvLayers=C, wn=wn) \\\n      for _ in range(self.D)]\n    RGs0.append(\n      wn(nn.Conv2d(4 * G0, 4 * G0, kSize, padding=(kSize - 1) // 2, stride=1)))\n    RGs1 = [\n      RG(growRate0=2 * G0, nConvLayers=C, wn=wn) \\\n      for _ in range(self.D // 2)]\n    RGs1.append(\n      wn(nn.Conv2d(2 * G0, 2 * G0, kSize, padding=(kSize - 1) // 2, stride=1)))\n    RGs2 = [\n      RG(growRate0=G0, nConvLayers=C, wn=wn) \\\n      for _ in range(self.D // 4)]\n    RGs2.append(\n      wn(nn.Conv2d(G0, G0, kSize, padding=(kSize - 1) // 2, stride=1)))\n    self.RGs0 = nn.Sequential(*RGs0)\n    self.RGs1 = nn.Sequential(*RGs1)\n    self.RGs2 = nn.Sequential(*RGs2)\n    self.restoration = wn(\n      nn.Conv2d(G0, args.n_colors, kSize, padding=(kSize - 1) // 2, stride=1))\n    self.add_mean = MeanShift(args.rgb_range, rgb_mean, rgb_std, 1)\n\n  def forward(self, x):\n    x = self.sub_mean(x)\n    f__1 = self.SFENet1(x)\n    f__2 = self.encoder1(f__1)\n    f__3 = self.encoder2(f__2)\n    x = f__3\n    x = self.decoder1(self.RGs0(x) + f__3)\n    x = self.decoder2(self.RGs1(x) + f__2)\n    x = self.RGs2(x) + f__1\n    x = self.restoration(x)\n    x = self.add_mean(x)\n\n    return x\n'"
VSR/Backend/Torch/Models/ntire19/frn.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 4 - 16\n\nimport torch.nn as nn\n\nfrom ..edsr import common\n\n\n## Channel Attention (CA) Layer\nclass CALayer(nn.Module):\n  def __init__(self, channel, reduction=16):\n    super(CALayer, self).__init__()\n    # global average pooling: feature --> point\n    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n    # feature channel downscale and upscale --> channel weight\n    self.conv_du = nn.Sequential(\n      nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n      nn.ReLU(inplace=True),\n      nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n    )\n\n    self.sigmoid = nn.Sigmoid()\n    self.ch = channel\n\n  def forward(self, x):\n    y = self.avg_pool(x)\n    y = self.conv_du(y)\n    y = self.sigmoid(y)\n\n    return x * y\n  ## Residual Channel Attention Block (RCAB)\n\n\nclass RCAB(nn.Module):\n  def __init__(\n      self, conv, n_feat, kernel_size, reduction,\n      bias=True, bn=False, act=nn.LeakyReLU(0.2, True), res_scale=1, px=1):\n\n    super(RCAB, self).__init__()\n    modules_body = []\n    # modules_body.append(common.invPixelShuffle(2))\n    modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n    modules_body.append(act)\n    modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n\n    if px != 1:\n      modules_body.append(common.invPixelShuffle(px))\n    modules_body.append(CALayer(n_feat * px ** 2, reduction))\n    if px != 1:\n      modules_body.append(nn.PixelShuffle(px))\n    self.body = nn.Sequential(*modules_body)\n    self.res_scale = res_scale\n\n  def forward(self, x):\n    res = self.body(x)\n    # res = self.body(x).mul(self.res_scale)\n    res += x\n    return res\n\n\n## Residual Group (RG)\nclass ResidualGroup(nn.Module):\n  def __init__(self, conv, n_feat, kernel_size, reduction, act, res_scale,\n               n_resblocks, n_resgroups, px):\n    super(ResidualGroup, self).__init__()\n    modules_body = []\n    if len(n_resgroups) == 0:\n      modules_body = [\n        RCAB(\n          conv, n_feat, kernel_size, reduction, bias=True, bn=False,\n          act=nn.ReLU(True), res_scale=1, px=px) \\\n        for _ in range(n_resblocks)]\n    else:\n      modules_body = [\n        ResidualGroup(conv, n_feat, kernel_size, reduction, act, res_scale,\n                      n_resblocks, n_resgroups[1:], px=px) \\\n        for _ in range(n_resgroups[0])]\n\n    modules_body.append(conv(n_feat, n_feat, kernel_size))\n    self.body = nn.Sequential(*modules_body)\n\n  def forward(self, x):\n    res = self.body(x)\n    res += x\n    return res\n\n\n## Residual Channel Attention Network (RCAN)\nclass FRN_UPDOWN(nn.Module):\n  def __init__(self, args, conv=common.default_conv):\n    super(FRN_UPDOWN, self).__init__()\n\n    n_resgroups = args.n_resgroups\n    # n_resgroups_ = args.n_resgroups2\n    # n_resgroups__ = args.n_resgroups3\n\n    n_resblocks = args.n_resblocks\n    n_feats = args.n_feats\n    kernel_size = 3\n    reduction = args.reduction\n    scale = args.scale[0]\n    act = nn.ReLU(True)\n\n    # RGB mean for DIV2K\n    rgb_mean = (0.4488, 0.4371, 0.4040)\n    rgb_std = (1.0, 1.0, 1.0)\n    self.sub_mean = common.MeanShift(args.rgb_range, rgb_mean, rgb_std)\n\n    # define head module\n    modules_head = [conv(args.n_colors, n_feats, kernel_size)]\n\n    # define body module\n    modules_body = [\n      ResidualGroup(\n        conv, n_feats, kernel_size, reduction, act=act,\n        res_scale=args.res_scale, n_resblocks=n_resblocks,\n        n_resgroups=n_resgroups[1:], px=args.px) \\\n      for _ in range(n_resgroups[0])]\n\n    modules_body.append(conv(n_feats, n_feats, kernel_size))\n    # define tail module\n    modules_tail = [\n      common.Upsampler(conv, scale, n_feats, act=False),\n      conv(n_feats, args.n_colors, kernel_size)]\n\n    self.add_mean = common.MeanShift(args.rgb_range, rgb_mean, rgb_std, 1)\n    m_down = [\n      common.invUpsampler(conv, scale, n_feats, act=False),\n    ]\n\n    self.down = nn.Sequential(*m_down)\n\n    # self.head = nn.Sequential(*modules_head)\n    self.new_head = nn.Sequential(*modules_head)\n    self.body = nn.Sequential(*modules_body)\n    # self.tail = nn.Sequential(*modules_tail)\n    self.new_tail = nn.Sequential(*modules_tail)\n\n    for m in self.body.modules():\n      if isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(0, 0.01)\n        m.bias.data.fill_(0)\n\n  def forward(self, x):\n    x = self.sub_mean(x)\n    x = self.new_head(x)\n    x = self.down(x)\n\n    res = self.body(x)\n    res += x\n\n    x = self.new_tail(res)\n    x = self.add_mean(x)\n\n    return x\n\n  def forward_(self, x):\n\n    x = self.sub_mean(x)\n    x = self.new_head(x)\n    x = self.down(x)\n\n    x = self.new_tail(x)\n    x = self.add_mean(x)\n\n    return x\n\n  def load_state_dict(self, state_dict, strict=False):\n    own_state = self.state_dict()\n    for name, param in state_dict.items():\n      if name in own_state:\n        if isinstance(param, nn.Parameter):\n          param = param.data\n        try:\n          own_state[name].copy_(param)\n        except Exception:\n          if name.find(\'tail\') >= 0:\n            print(\'Replace pre-trained upsampler to new one...\')\n          else:\n            raise RuntimeError(\'While copying the parameter named {}, \'\n                               \'whose dimensions in the model are {} and \'\n                               \'whose dimensions in the checkpoint are {}.\'\n                               .format(name, own_state[name].size(),\n                                       param.size()))\n      elif strict:\n        if name.find(\'tail\') == -1:\n          raise KeyError(\'unexpected key ""{}"" in state_dict\'\n                         .format(name))\n\n    if strict:\n      missing = set(own_state.keys()) - set(state_dict.keys())\n      if len(missing) > 0:\n        raise KeyError(\'missing keys in state_dict: ""{}""\'.format(missing))\n'"
VSR/Backend/Torch/Models/ntire19/ran2.py,0,"b""#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 4 - 16\n\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..edsr.common import MeanShift\n\n\nclass sub_pixel(nn.Module):\n  def __init__(self, scale, act=False):\n    super(sub_pixel, self).__init__()\n    modules = []\n    modules.append(nn.PixelShuffle(scale))\n    self.body = nn.Sequential(*modules)  # nn.Sequential(*modules)\n\n  def forward(self, x):\n    x = self.body(x)\n    return x\n\n\nclass RAN(nn.Module):\n  def __init__(self, args):\n    super(RAN, self).__init__()\n    nChannel = args.n_colors\n    nFeat = args.n_feats\n    scale = args.scale[0]\n    self.args = args\n\n    self.plot_cnt = 0\n    self.args = args\n    self.id = [None] * 200\n    # self.vis2 = visdom.Visdom(env='att_main')\n    self.idx = 0\n\n    rgb_mean = (0.45738, 0.43637, 0.40293)\n    rgb_std = (1.0, 1.0, 1.0)\n    self.sub_mean = MeanShift(255, rgb_mean, rgb_std, -1)\n    self.add_mean = MeanShift(255, rgb_mean, rgb_std, 1)\n\n    self.conv1 = nn.Conv2d(nChannel, nFeat, kernel_size=3, padding=1,\n                           bias=True)\n    self.conv2 = nn.Conv2d(nFeat, nFeat, kernel_size=3, padding=1,\n                           bias=True)\n    self.Mod = torch.nn.ModuleList(\n      [ST_Block5(args.n_resblocks, nFeat, i) for i in\n       range(args.n_resgroups)])  # nn.ModuleList()\n    self.spatial_att = Spatial_att_score2(nFeat, args.n_resgroups)\n    # for i in range(10):\n    #    self.Mod.append(ST_Block2(1,nFeat))\n    # self.Mod = nn.Sequential(*self.Mod)\n    # self.conv3 = nn.Conv2d(nFeat*10*2, nFeat, kernel_size=1, padding=0, bias=True, groups=2)\n    self.G_F1 = nn.Conv2d(nFeat * args.n_resgroups, nFeat, kernel_size=1,\n                          padding=0, bias=True)\n    self.G_F33 = nn.Conv2d(nFeat, nFeat, kernel_size=3, padding=1,\n                           bias=True)\n    self.G_F3 = nn.Conv2d(nFeat, nFeat, kernel_size=3, padding=1, bias=True)\n    self.GFF_3x3 = nn.Conv2d(nFeat * 2, nFeat, kernel_size=1, padding=0,\n                             bias=True)\n    self.GFF_3x3_2 = nn.Conv2d(nFeat, nFeat, kernel_size=3, padding=1,\n                               bias=True)\n    self.G_F2 = nn.Conv2d(nFeat, nFeat, kernel_size=3, padding=1, bias=True)\n\n    self.Dblock = Dilated_block(nFeat)\n\n    if scale == 4:\n      self.up = nn.Sequential(\n        nn.Conv2d(nFeat, nFeat * scale, kernel_size=3, padding=1,\n                  bias=True),\n        sub_pixel(2),\n        nn.Conv2d(nFeat, nFeat * scale, kernel_size=3, padding=1,\n                  bias=True),\n        sub_pixel(2))\n    else:\n      self.up = nn.Sequential(\n        nn.Conv2d(nFeat, nFeat * scale * scale, kernel_size=3,\n                  padding=1, bias=True),\n        sub_pixel(2))\n\n    self.HR = nn.Sequential(\n      nn.Conv2d(nFeat, nFeat, kernel_size=3, padding=1, bias=True),\n      nn.LeakyReLU(0.1),\n      nn.Conv2d(nFeat, 3, kernel_size=3, padding=1, bias=True))\n\n  def forward(self, x):\n\n    self.plot_cnt += 1\n\n    i = x\n\n    x_list = []\n    out_list = []\n    x = self.conv1(x)\n\n    x = (self.conv2(x))  # ,5e-2)\n\n    x_list.append(x)\n\n    out_list.append(x)\n\n    io = x\n    # out_list.append(x)\n    for ii, submodel in enumerate(self.Mod):\n      if ii == 3:\n        inp = x.detach()\n      res, x, scale = submodel(x, x_list, out_list)\n      if False:  # (ii+1)%4==0:\n        x += io\n        io = x\n      out_list.append(res)\n      x_list.append(x)\n\n      ##for visdom\n      if ii == 3:\n        s = scale\n    s = None\n    # plot scale ------------------------------------------------------------------\n    if self.plot_cnt == 200 and s is not None:\n      self.plot_cnt = 0\n      for idx, sle in enumerate(s):\n        self.id[idx] = self.vis2.images(\n          sle[0, :, :, :].unsqueeze(dim=1), nrow=6, win=self.id[idx],\n          opts={'title': 'BLOCK-15'})\n\n    x = self.G_F1(torch.cat(out_list[1:], dim=1))\n    x = self.G_F33(x)\n\n    if not self.args.nvis:\n      self.id[self.idx] = self.vis2.heatmap(mask[0, 0, :, :],\n                                            win=self.id[self.idx],\n                                            opts={'title': 'SA'})\n    self.idx += 1\n\n    if self.idx == 20:\n      self.idx = 0\n\n    x = self.HR(x) + i\n\n    return x\n\n\nclass Dilated_block(nn.Module):\n  def __init__(self, inChannel):\n    super(Dilated_block, self).__init__()\n\n    self.conv11 = nn.Conv2d(inChannel, inChannel, kernel_size=3, padding=1)\n    self.conv12 = nn.Conv2d(inChannel, inChannel, kernel_size=3, padding=2,\n                            dilation=2)\n\n    self.conv21 = nn.Conv2d(inChannel, inChannel, kernel_size=5, padding=2)\n    self.conv22 = nn.Conv2d(inChannel, inChannel, kernel_size=3, padding=3,\n                            dilation=3)\n\n    self.conv31 = nn.Conv2d(inChannel, inChannel, kernel_size=7, padding=3)\n    self.conv32 = nn.Conv2d(inChannel, inChannel, kernel_size=3, padding=3,\n                            dilation=3)\n\n    self.Fuse = nn.Conv2d(inChannel * 3, inChannel, kernel_size=1,\n                          padding=0)\n\n  def forward(self, x):\n    i = x\n    x1 = self.conv12(F.leaky_relu(self.conv11(x), 0.1))\n    x2 = self.conv22(F.leaky_relu(self.conv21(x), 0.1))\n    x3 = self.conv32(F.leaky_relu(self.conv31(x), 0.1))\n\n    x = self.Fuse(torch.cat([x1, x2, x3], dim=1))\n\n    return x + i\n\n\nclass Spatial_att_score2(nn.Module):\n  def __init__(self, inChannel, num_block):\n    super(Spatial_att_score2, self).__init__()\n\n    self.conv1 = nn.Conv2d(inChannel * num_block, inChannel, kernel_size=1,\n                           padding=0)\n    self.conv2 = nn.Conv2d(inChannel * 2, inChannel, kernel_size=3,\n                           padding=1)\n    self.conv3 = nn.Conv2d(inChannel, inChannel, kernel_size=3, padding=1)\n    self.conv4 = nn.Conv2d(inChannel, inChannel, kernel_size=3, padding=1)\n    self.conv5 = nn.Conv2d(inChannel, 1, kernel_size=3, padding=1)\n\n    self.alpha = nn.Parameter(torch.FloatTensor(1).zero_())\n    self.beta = nn.Parameter(torch.FloatTensor(1).zero_())\n\n    nn.init.constant_(self.alpha, 1)\n\n  def forward(self, x, res_list):\n    res = torch.cat(res_list[1:], dim=1)\n    score = self.conv1(res)\n    # score = self.conv2(F.leaky_relu(score,0.1))\n    score = self.conv2(torch.cat([score, res_list[0]], dim=1))\n    i = score\n    # score = self.conv3(F.leaky_relu(score,0.1))\n    # score = self.conv4(F.leaky_relu(score,0.1))\n    # score = self.conv5(F.leaky_relu(score+i,0.1))\n    score = self.conv3(torch.tanh(score))\n    score = self.conv4(torch.tanh(score))\n    score = self.conv5(torch.tanh(score + i))\n    # score = self.conv5(score)\n    # in\n    '''\n    t = score\n    m = t.mean(dim=3, keepdim=True).mean(dim=2, keepdim=True)\n    std = ((t - m) ** 2).mean(dim=3, keepdim=True).mean(dim=2, keepdim=True)\n    t = (t - m) / (std.sqrt()) * self.alpha + self.beta\n    score = t\n    '''\n    #\n    mask = F.sigmoid(score)\n\n    return mask\n\n\nclass Channel_att_score3(nn.Module):\n  def __init__(self, inChannel, num_prev, r=8):\n    super(Channel_att_score3, self).__init__()\n    self.num_prev = num_prev\n\n    # self.conv = nn.Conv2d(num_prev*inChannel,inChannel,kernel_size=1)\n    # self.conv2 = nn.Conv2d(inChannel,inChannel,kernel_size=3,padding=1)\n    # self.conv3 = nn.Conv2d(inChannel,1,kernel_size=3,padding=1)\n    # self.fc1_1 = nn.Conv2d(1,4,kernel_size=[64,num_prev+1],padding=0,bias=True)\n    self.fc1_1 = nn.Conv2d(inChannel, inChannel // r, kernel_size=1,\n                           bias=False)\n    self.fc1_2 = nn.Conv2d(inChannel // r, inChannel, kernel_size=1,\n                           bias=False)\n    if self.num_prev > 0:\n      self.fc1_3 = nn.Conv2d(inChannel, inChannel // r,\n                             kernel_size=[num_prev + 1, 1], padding=0,\n                             bias=False)\n      self.fc1_4 = nn.Conv2d(inChannel // r, inChannel, kernel_size=1,\n                             bias=False)\n      # self.fc1_3 = nn.Conv2d(inChannel, inChannel // r, kernel_size=[num_prev, 1], padding=0, bias=True)\n\n    # self.conv4  =nn.Conv2d(2,1,kernel_size=7,padding=3)\n    self.alpha = nn.Parameter(torch.FloatTensor(1).zero_())\n    self.beta = nn.Parameter(torch.FloatTensor(1).zero_())\n    self.register_parameter('norm_alpha', self.alpha)\n    self.register_parameter('norm_beta', self.beta)\n\n    self.alpha1 = nn.Parameter(torch.FloatTensor(1).zero_())\n    self.beta1 = nn.Parameter(torch.FloatTensor(1).zero_())\n    self.register_parameter('norm_alpha1', self.alpha1)\n    self.register_parameter('norm_beta1', self.beta1)\n\n    nn.init.constant_(self.alpha, 1)\n    nn.init.constant_(self.alpha1, 1)\n\n    self.activ1 = nn.ReLU()\n    self.activ2 = nn.Sigmoid()\n\n  def forward(self, x, MP_list, GP_list):\n\n    MP = F.max_pool2d(x, kernel_size=x.size()[2:])\n    GP = F.avg_pool2d(x, kernel_size=x.size()[2:])\n\n    MP_list_ = copy.copy(MP_list)\n    GP_list_ = copy.copy(GP_list)  # shallow copy\n    # MP_list_.append(MP.squeeze(3).squeeze(2))\n    # GP_list_.append(GP.squeeze(3).squeeze(2))\n    GP_list_.append(GP)\n\n    t2 = GP\n\n    m = t2.mean(dim=1, keepdim=True)\n    std = ((t2 - m) ** 2).mean(dim=1, keepdim=True)\n    t2 = (t2 - m) / (std.sqrt()) * self.alpha + self.beta\n\n    t2 = self.fc1_1(t2)\n\n    t = (self.fc1_2(self.activ1(t2)))\n    output = 1 + torch.tanh(t)\n\n    if self.num_prev == 0:\n\n      # return output,(output)*x\n      return output.squeeze(3).squeeze(2), output * x\n\n\n    else:\n      # x2 = torch.cat(GP_list_,dim=2)\n      x2 = torch.cat(GP_list_, dim=2)\n\n      m = x2.mean(dim=1, keepdim=True)\n      std = ((x2 - m) ** 2).mean(dim=1, keepdim=True)\n      x2 = (x2 - m) / (std.sqrt()) * self.alpha1 + self.beta1\n\n      x2 = self.fc1_3(x2)\n\n      # print(x2)\n      x2 = self.fc1_4(self.activ1(x2))  # + t\n      # print(F.tanh(t))\n      output = 1 + torch.tanh(x2)\n      # output = (self.activ2(x2))+(self.activ2(t))\n      # print(output.size())\n      # print(output)\n      # output = (output.unsqueeze(2).unsqueeze(3))*x\n      return output.squeeze(3).squeeze(2), output * x\n\n\nclass Global_att(nn.Module):\n  def __init__(self, inChannel, num_unit, prev):\n    super(Global_att, self).__init__()\n    self.inChannel = inChannel\n    self.num_unit = num_unit\n    self.Fuse = nn.Conv2d(inChannel * num_unit, inChannel, 1)\n    # self.Fuse = nn.Conv2d(inChannel,inChannel,3,padding=1,groups=num_unit)\n    self.Trans = nn.Conv2d(inChannel, inChannel, 3, padding=1)\n    # self.Mask = nn.Conv2d(inChannel,1,7,padding=3)\n    self.alpha = nn.Parameter(torch.Tensor([1]))\n    self.cnt = 0\n\n    nn.init.constant_(self.alpha, 0)\n\n  def forward(self, x_ori, x, res_list, temp_list):\n    x_ = torch.cat(res_list, dim=1)\n    '''\n    x_ = x_.view(-1, self.num_unit, self.inChannel, *x.size()[2:])\n    x_ = torch.transpose(x_,1,2).contiguous()\n    x_ = x_.view(-1, self.num_unit*self.inChannel, *x.size()[2:])\n    '''\n    mask = (self.Trans(F.relu(self.Fuse(x_))))\n    x = (mask) + x_ori\n\n    # score = self.Mask(F.relu(x))\n    # mask = self.Fuse(x_)\n    # self.cnt+=1\n    # if self.cnt==1600:\n    #     print(self.alpha)\n    #     self.cnt = 0\n    return x\n\n\nclass ST_Unit5(nn.Module):\n  def __init__(self, nChannel, prev=0, bias=True, dilation=1, conv1=None,\n               conv2=None):  # ch = 128  nch = 64 (split)\n    super(ST_Unit5, self).__init__()\n    self.nChannel = nChannel\n    self.prev = prev\n\n    self.att_c = Channel_att_score3(nChannel, self.prev)\n    '''\n    self.Conv = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=1,bias=bias)\n    self.Linear = nn.Conv2d(nChannel, nChannel*9, kernel_size=1, padding=0,bias=bias)\n    self.Subpixel = nn.PixelShuffle(3)\n    #self.att_s = Spatial_att_score2(prev)\n\n    var = np.sqrt(6 / (64 * 3 * 3 + 64 * 3 * 3))\n    self.filters = nn.Parameter(torch.FloatTensor(64,64,3,3).uniform_(-var,var))\n    self.register_parameter('conv1x1',self.filters)\n    '''\n    # self.Spatial_att = Spatial_att()\n    # self.conv3x3 = nn.Conv2d(nChannel*(prev+1), nChannel, kernel_size=3, padding=1, bias=bias)\n    # self.Conv_1x1 = torch.\n    self.Conv_1x1 = nn.Conv2d(nChannel, nChannel, kernel_size=3,\n                              padding=dilation, dilation=dilation,\n                              bias=bias) if conv1 is None else conv1\n    self.DWconv_3x3 = nn.Conv2d(nChannel, nChannel, kernel_size=3,\n                                padding=1,\n                                bias=bias) if conv2 is None else conv2\n\n    # self.Conv_1x1_2 = nn.Conv2d(nChannel*4, nChannel, kernel_size=1, padding=0, bias=bias)\n    # self.Channel_att = Channel_att(nChannel,)\n    # self.Compress = nn.Conv2d(nChannel*4, nChannel, kernel_size=1, padding=0, bias=bias)\n    # self.Fuse = nn.Conv2d(nChannel*2, nChannel, kernel_size=1, padding=0, bias=bias)\n\n    '''\n    self.conv3x3_1 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=1, bias=bias)\n    self.conv3x3_2 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=2, dilation=2, bias=bias)\n    self.conv1x1 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=3, dilation=3, bias=bias)\n    self.conv1x1_2 = nn.Conv2d(nChannel*4, nChannel, kernel_size=1, padding=0, bias=bias)\n    '''\n    # var = np.sqrt(6 / (64 * 3 * 3 + 64 * 3 * 3))\n    # self.filters = nn.Parameter(torch.FloatTensor(64,64,3,3).uniform_(-var,var))\n    # self.register_parameter('shared_conv',self.filters)\n    # self.conv3x3_2 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=2, dilation=2,bias=bias)\n    # self.conv3x3_3 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=3, dilation=3,bias=bias)\n    # var = np.sqrt(6 / (64 * 3 * 3 + 64 * 3 * 3))\n    # self.filters = nn.Parameter(torch.FloatTensor(64,64,3,3).uniform_(-var,var))\n    # self.register_parameter('shared_conv',self.filters)\n    # self.Fuse = nn.Conv2d(nChannel*2, nChannel, kernel_size=1, padding=0,bias=False)\n    # if self.prev>0:\n    # self.conv1 = nn.Conv2d(nChannel*(self.prev+1), nChannel, kernel_size=1, padding=0, bias=bias)\n    # self.conv2 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=1, bias=bias)\n\n  def forward(self, x, pre_res, MP_list, GP_list):\n    ori = x[:, -self.nChannel:, :, :]\n\n    scale = None\n    '''\n    ka = F.avg_pool2d(self.Conv(x),kernel_size=x.size()[2:])\n    ka = self.Linear(F.relu(ka))\n    ka = self.Subpixel(ka)\n    ka_ = ka.view(-1,self.nChannel,9,1)\n    ka_ = F.softmax(ka_/8.,dim=2)*9\n    ka = ka_.view(-1,self.nChannel,3,3)\n    ka = torch.mean(ka,dim=0,keepdim=False).unsqueeze(0)\n\n    x = F.conv2d(x,self.filters*ka,padding=1)\n    '''\n\n    x1 = self.DWconv_3x3(F.relu(self.Conv_1x1(x)))  # not use relu\n    # x1 = F.relu((self.Conv_1x1(x))) #not use relu\n    # scale,x1 = self.Channel_att(x1)\n    # scale = F.avg_pool2d(x1,kernel_size=x1.size()[2:])\n\n    i = x1\n\n    # x1 = F.conv2d(F.relu(x1),self.filters,bias=None,padding=2,dilation=2)\n    # x1 = F.conv2d(F.relu(x1),self.filters,bias=None,padding=3,dilation=3)\n    # x1 = self.Spatial_att(x1)\n    # x1 = self.Spatial_conv2(F.relu(x1))\n    # x1 = self.Spatial_att(x1)\n    # x1 = self.att_c(x1, pre_res)\n\n    # x1 = self.Fuse(torch.cat([i,x1],dim=1)) #not use relu\n    # x1 = self.att_c(x1,pre_res)\n    # x1 = ((self.conv3x3_2(F.relu(self.conv3x3_1(x)))))\n    # x1 = F.conv2d(x,self.filters,bias=None,padding=1,dilation=1)\n    # x1 = (F.conv2d(F.relu(x1),self.filters,bias=None,padding=2,dilation=2))\n    # x1 = (F.conv2d(F.relu(x1),self.filters,bias=None,padding=3,dilation=3))\n\n    # x1 = ((self.conv1x1(F.relu(x1)))) # self.conv1x1(x1)#\n    # x1 = F.conv2d(x,self.filters,bias=None,padding=1)\n    # x1 = F.conv2d(F.relu(x1),self.filters,bias=None,padding=2,dilation=2)\n\n    # x1 = self.conv3x3_3(F.relu(self.conv3x3_2(x1)))\n\n    scale, x1 = self.att_c(x1, MP_list, GP_list)\n    # print(len(MP_list))\n    #\n    # x1 = self.att_s(x1, pre_res)\n\n    # x1 = x1+i\n    # if not self.prev:\n    #    return x1,x1 + ori\n    # else:\n    # x1 = self.Fuse(torch.cat([i,x1],dim=1))\n    # score = F.tanh(self.conv2(F.leaky_relu(self.conv1(torch.cat(pre_list,dim=1)),0.1)))\n    # bias = F.tanh(self.conv4(torch.cat([F.leaky_relu(self.conv3(torch.cat(pre_res,dim=1)),0.1),F.leaky_relu(x1,0.1)],dim=1)))\n    # bias = (self.conv4(torch.cat([F.leaky_relu(self.conv3(torch.cat(pre_res,dim=1)),0.1),F.relu(x1)],dim=1)))\n    # x1 = (score)*x1\n    return x1, x1 + ori, scale\n\n\nclass ST_Block5(nn.Module):\n  def __init__(self, num_unit, nChannel, prev=0,\n               bias=True):  # ch = 128  nch = 64 (split)\n    super(ST_Block5, self).__init__()\n    self.nChannel = nChannel\n    self.num_unit = num_unit\n    self.Conv_1x1 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=1,\n                              dilation=1, bias=bias)\n    self.DWconv_3x3 = nn.Conv2d(nChannel, nChannel, kernel_size=3,\n                                padding=1, bias=bias)\n    self.prev = prev\n    # self.att_s = Spatial_att_score3(nChannel,prev)#prev)\n    # self.att_c = Channel_att_score3(nChannel, self.prev, nChannel)\n    # self.Conv1 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=1, bias=bias)\n    # self.att_c = Channel_att_score2(nChannel,self.num_unit,nChannel)\n\n    self.Spatial_att = Global_att(nChannel, num_unit, prev)\n    self.unit = torch.nn.ModuleList(\n      [ST_Unit5(64, i, dilation=1, conv1=None, conv2=None) for i in\n       range(num_unit)])\n    self.Fuse = nn.Conv2d(nChannel * 2, nChannel, kernel_size=1, padding=0,\n                          bias=bias)  # *(num_unit+1)\n    self.Infuse = nn.Conv2d(nChannel * (prev + 1), nChannel, kernel_size=1,\n                            padding=0, bias=bias)\n    if prev > 0: self.Conv = nn.Conv2d(nChannel * (2), nChannel,\n                                       kernel_size=1, padding=0, bias=bias)\n    # self.lastFuse = nn.Conv2d(nChannel * (prev + 1), nChannel, kernel_size=1, padding=0, bias=bias)\n    # self.Mask1 = nn.Conv2d(nChannel*(num_unit), nChannel, kernel_size=1, padding=0, bias=bias)\n    # self.Mask2 = nn.Conv2d(nChannel, nChannel, kernel_size=3, padding=1, bias=bias)\n\n  def forward(self, x, res_list, temp_list):\n\n    i = x[:, -self.nChannel:, :, :]\n    x = i\n\n    if self.prev == 0:\n      x = self.Infuse(x)\n    else:\n      r = torch.cat(res_list, dim=1)\n      # x = torch.cat([x, r], dim=1)\n      x = self.Infuse(r)\n\n    ori = x\n    # x = self.Conv1(x)#fore or after?\n    out_list = []\n    x_list = []\n    MP_list = []\n    GP_list = []\n    for i, model in enumerate(self.unit):\n      res, x, scale = model(x, out_list, MP_list, GP_list)\n      if i != self.num_unit - 1:  # if False:\n        # pass\n        # MP_list.append(F.max_pool2d(res, kernel_size=x.size()[2:]))\n        GP_list.append(F.avg_pool2d(res, kernel_size=x.size()[2:]))\n      out_list.append(res)\n      x_list.append(x)\n    # x = self.Spatial_att(x,out_list)\n    # res_x = torch.cat(out_list,dim=1)\n    # mask = F.sigmoid(self.Mask2(F.relu(self.Mask1(res_x))))\n\n    # x = self.Fuse(x)\n    # x = self.att_c(x,res_list)\n    # x = self.Channel_att(x)\n    x_ = self.Spatial_att(ori, x, out_list, temp_list)\n    x = self.Fuse(torch.cat([x_, x], dim=1))\n    # if self.prev > 0:\n    #            x = self.Conv(torch.cat([x, temp_list[-1]], dim=1))\n    '''\n    if self.prev==0:x = self.lastFuse(x)\n    else:\n        r = torch.cat(temp_list,dim=1)\n        x = torch.cat([x,r],dim=1)\n        x = self.lastFuse(x)\n    '''\n    return x, x + i, x_list  # x+i\n"""
VSR/Backend/Torch/Models/rbpn/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/25 \xe4\xb8\x8b\xe5\x8d\x884:38\n\nimport logging\n\n_logger = logging.getLogger(""VSR.RBPN"")\n_logger.info(""LICENSE: RBPN is implemented by M. Haris, et. al. @alterzero"")\n_logger.warning(\n  ""I use unsupervised flownet to estimate optical flow, rather than pyflow module."")\n'"
VSR/Backend/Torch/Models/rbpn/base_network.py,0,"b""import math\nimport torch\n\n\nclass DenseBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, bias=True, activation='relu',\n               norm='batch'):\n    super(DenseBlock, self).__init__()\n    self.fc = torch.nn.Linear(input_size, output_size, bias=bias)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm1d(output_size)\n    elif self.norm == 'instance':\n      self.bn = torch.nn.InstanceNorm1d(output_size)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    if self.norm is not None:\n      out = self.bn(self.fc(x))\n    else:\n      out = self.fc(x)\n\n    if self.activation is not None:\n      return self.act(out)\n    else:\n      return out\n\n\nclass ConvBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, kernel_size=3, stride=1,\n               padding=1, bias=True, activation='prelu', norm=None):\n    super(ConvBlock, self).__init__()\n    self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size, stride,\n                                padding, bias=bias)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm2d(output_size)\n    elif self.norm == 'instance':\n      self.bn = torch.nn.InstanceNorm2d(output_size)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    if self.norm is not None:\n      out = self.bn(self.conv(x))\n    else:\n      out = self.conv(x)\n\n    if self.activation is not None:\n      return self.act(out)\n    else:\n      return out\n\n\nclass DeconvBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, kernel_size=4, stride=2,\n               padding=1, bias=True, activation='prelu', norm=None):\n    super(DeconvBlock, self).__init__()\n    self.deconv = torch.nn.ConvTranspose2d(input_size, output_size, kernel_size,\n                                           stride, padding, bias=bias)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm2d(output_size)\n    elif self.norm == 'instance':\n      self.bn = torch.nn.InstanceNorm2d(output_size)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    if self.norm is not None:\n      out = self.bn(self.deconv(x))\n    else:\n      out = self.deconv(x)\n\n    if self.activation is not None:\n      return self.act(out)\n    else:\n      return out\n\n\nclass ResnetBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=3, stride=1, padding=1, bias=True,\n               activation='prelu', norm='batch'):\n    super(ResnetBlock, self).__init__()\n    self.conv1 = torch.nn.Conv2d(num_filter, num_filter, kernel_size, stride,\n                                 padding, bias=bias)\n    self.conv2 = torch.nn.Conv2d(num_filter, num_filter, kernel_size, stride,\n                                 padding, bias=bias)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm2d(num_filter)\n    elif norm == 'instance':\n      self.bn = torch.nn.InstanceNorm2d(num_filter)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    residual = x\n    if self.norm is not None:\n      out = self.bn(self.conv1(x))\n    else:\n      out = self.conv1(x)\n\n    if self.activation is not None:\n      out = self.act(out)\n\n    if self.norm is not None:\n      out = self.bn(self.conv2(out))\n    else:\n      out = self.conv2(out)\n\n    out = torch.add(out, residual)\n\n    if self.activation is not None:\n      out = self.act(out)\n\n    return out\n\n\nclass UpBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, bias=True,\n               activation='prelu', norm=None):\n    super(UpBlock, self).__init__()\n    self.up_conv1 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                              padding, activation, norm=None)\n    self.up_conv3 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    h0 = self.up_conv1(x)\n    l0 = self.up_conv2(h0)\n    h1 = self.up_conv3(l0 - x)\n    return h1 + h0\n\n\nclass UpBlockPix(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, scale=4,\n               bias=True, activation='prelu', norm=None):\n    super(UpBlockPix, self).__init__()\n    self.up_conv1 = Upsampler(scale, num_filter)\n    self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                              padding, activation, norm=None)\n    self.up_conv3 = Upsampler(scale, num_filter)\n\n  def forward(self, x):\n    h0 = self.up_conv1(x)\n    l0 = self.up_conv2(h0)\n    h1 = self.up_conv3(l0 - x)\n    return h1 + h0\n\n\nclass D_UpBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2,\n               num_stages=1, bias=True, activation='prelu', norm=None):\n    super(D_UpBlock, self).__init__()\n    self.conv = ConvBlock(num_filter * num_stages, num_filter, 1, 1, 0,\n                          activation, norm=None)\n    self.up_conv1 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                              padding, activation, norm=None)\n    self.up_conv3 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    x = self.conv(x)\n    h0 = self.up_conv1(x)\n    l0 = self.up_conv2(h0)\n    h1 = self.up_conv3(l0 - x)\n    return h1 + h0\n\n\nclass D_UpBlockPix(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2,\n               num_stages=1, scale=4, bias=True, activation='prelu', norm=None):\n    super(D_UpBlockPix, self).__init__()\n    self.conv = ConvBlock(num_filter * num_stages, num_filter, 1, 1, 0,\n                          activation, norm=None)\n    self.up_conv1 = Upsampler(scale, num_filter)\n    self.up_conv2 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                              padding, activation, norm=None)\n    self.up_conv3 = Upsampler(scale, num_filter)\n\n  def forward(self, x):\n    x = self.conv(x)\n    h0 = self.up_conv1(x)\n    l0 = self.up_conv2(h0)\n    h1 = self.up_conv3(l0 - x)\n    return h1 + h0\n\n\nclass DownBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, bias=True,\n               activation='prelu', norm=None):\n    super(DownBlock, self).__init__()\n    self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.down_conv2 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                  padding, activation, norm=None)\n    self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    l0 = self.down_conv1(x)\n    h0 = self.down_conv2(l0)\n    l1 = self.down_conv3(h0 - x)\n    return l1 + l0\n\n\nclass DownBlockPix(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, scale=4,\n               bias=True, activation='prelu', norm=None):\n    super(DownBlockPix, self).__init__()\n    self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.down_conv2 = Upsampler(scale, num_filter)\n    self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    l0 = self.down_conv1(x)\n    h0 = self.down_conv2(l0)\n    l1 = self.down_conv3(h0 - x)\n    return l1 + l0\n\n\nclass D_DownBlock(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2,\n               num_stages=1, bias=True, activation='prelu', norm=None):\n    super(D_DownBlock, self).__init__()\n    self.conv = ConvBlock(num_filter * num_stages, num_filter, 1, 1, 0,\n                          activation, norm=None)\n    self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.down_conv2 = DeconvBlock(num_filter, num_filter, kernel_size, stride,\n                                  padding, activation, norm=None)\n    self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    x = self.conv(x)\n    l0 = self.down_conv1(x)\n    h0 = self.down_conv2(l0)\n    l1 = self.down_conv3(h0 - x)\n    return l1 + l0\n\n\nclass D_DownBlockPix(torch.nn.Module):\n  def __init__(self, num_filter, kernel_size=8, stride=4, padding=2,\n               num_stages=1, scale=4, bias=True, activation='prelu', norm=None):\n    super(D_DownBlockPix, self).__init__()\n    self.conv = ConvBlock(num_filter * num_stages, num_filter, 1, 1, 0,\n                          activation, norm=None)\n    self.down_conv1 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n    self.down_conv2 = Upsampler(scale, num_filter)\n    self.down_conv3 = ConvBlock(num_filter, num_filter, kernel_size, stride,\n                                padding, activation, norm=None)\n\n  def forward(self, x):\n    x = self.conv(x)\n    l0 = self.down_conv1(x)\n    h0 = self.down_conv2(l0)\n    l1 = self.down_conv3(h0 - x)\n    return l1 + l0\n\n\nclass PSBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, scale_factor, kernel_size=3,\n               stride=1, padding=1, bias=True, activation='prelu',\n               norm='batch'):\n    super(PSBlock, self).__init__()\n    self.conv = torch.nn.Conv2d(input_size, output_size * scale_factor ** 2,\n                                kernel_size, stride, padding, bias=bias)\n    self.ps = torch.nn.PixelShuffle(scale_factor)\n\n    self.norm = norm\n    if self.norm == 'batch':\n      self.bn = torch.nn.BatchNorm2d(output_size)\n    elif norm == 'instance':\n      self.bn = torch.nn.InstanceNorm2d(output_size)\n\n    self.activation = activation\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    if self.norm is not None:\n      out = self.bn(self.ps(self.conv(x)))\n    else:\n      out = self.ps(self.conv(x))\n\n    if self.activation is not None:\n      out = self.act(out)\n    return out\n\n\nclass Upsampler(torch.nn.Module):\n  def __init__(self, scale, n_feat, bn=False, act='prelu', bias=True):\n    super(Upsampler, self).__init__()\n    modules = []\n    for _ in range(int(math.log(scale, 2))):\n      modules.append(\n        ConvBlock(n_feat, 4 * n_feat, 3, 1, 1, bias, activation=None,\n                  norm=None))\n      modules.append(torch.nn.PixelShuffle(2))\n      if bn: modules.append(torch.nn.BatchNorm2d(n_feat))\n      # modules.append(torch.nn.PReLU())\n    self.up = torch.nn.Sequential(*modules)\n\n    self.activation = act\n    if self.activation == 'relu':\n      self.act = torch.nn.ReLU(True)\n    elif self.activation == 'prelu':\n      self.act = torch.nn.PReLU()\n    elif self.activation == 'lrelu':\n      self.act = torch.nn.LeakyReLU(0.2, True)\n    elif self.activation == 'tanh':\n      self.act = torch.nn.Tanh()\n    elif self.activation == 'sigmoid':\n      self.act = torch.nn.Sigmoid()\n\n  def forward(self, x):\n    out = self.up(x)\n    if self.activation is not None:\n      out = self.act(out)\n    return out\n\n\nclass Upsample2xBlock(torch.nn.Module):\n  def __init__(self, input_size, output_size, bias=True, upsample='deconv',\n               activation='relu', norm='batch'):\n    super(Upsample2xBlock, self).__init__()\n    scale_factor = 2\n    # 1. Deconvolution (Transposed convolution)\n    if upsample == 'deconv':\n      self.upsample = DeconvBlock(input_size, output_size,\n                                  kernel_size=4, stride=2, padding=1,\n                                  bias=bias, activation=activation, norm=norm)\n\n    # 2. Sub-pixel convolution (Pixel shuffler)\n    elif upsample == 'ps':\n      self.upsample = PSBlock(input_size, output_size,\n                              scale_factor=scale_factor,\n                              bias=bias, activation=activation, norm=norm)\n\n    # 3. Resize and Convolution\n    elif upsample == 'rnc':\n      self.upsample = torch.nn.Sequential(\n        torch.nn.Upsample(scale_factor=scale_factor, mode='nearest'),\n        ConvBlock(input_size, output_size,\n                  kernel_size=3, stride=1, padding=1,\n                  bias=bias, activation=activation, norm=norm)\n      )\n\n  def forward(self, x):\n    out = self.upsample(x)\n    return out\n"""
VSR/Backend/Torch/Models/rbpn/ops.py,0,"b""import torch.nn as nn\n\nfrom .base_network import *\n\n\nclass Dbpns(nn.Module):\n  def __init__(self, base_filter, feat, num_stages, scale_factor):\n    super(Dbpns, self).__init__()\n\n    if scale_factor == 2:\n      kernel = 6\n      stride = 2\n      padding = 2\n    elif scale_factor == 4:\n      kernel = 8\n      stride = 4\n      padding = 2\n    elif scale_factor == 8:\n      kernel = 12\n      stride = 8\n      padding = 2\n\n    # Initial Feature Extraction\n    # self.feat0 = ConvBlock(num_channels, feat, 3, 1, 1, activation='prelu', norm=None)\n    self.feat1 = ConvBlock(base_filter, feat, 1, 1, 0, activation='prelu',\n                           norm=None)\n    # Back-projection stages\n    self.up1 = UpBlock(feat, kernel, stride, padding)\n    self.down1 = DownBlock(feat, kernel, stride, padding)\n    self.up2 = UpBlock(feat, kernel, stride, padding)\n    self.down2 = DownBlock(feat, kernel, stride, padding)\n    self.up3 = UpBlock(feat, kernel, stride, padding)\n    # Reconstruction\n    self.output = ConvBlock(num_stages * feat, feat, 1, 1, 0, activation=None,\n                            norm=None)\n\n    for m in self.modules():\n      classname = m.__class__.__name__\n      if classname.find('Conv2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n      elif classname.find('ConvTranspose2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n  def forward(self, x):\n    # x = self.feat0(x)\n    x = self.feat1(x)\n\n    h1 = self.up1(x)\n    h2 = self.up2(self.down1(h1))\n    h3 = self.up3(self.down2(h2))\n\n    x = self.output(torch.cat((h3, h2, h1), 1))\n\n    return x\n\n\nclass Rbpn(nn.Module):\n  def __init__(self, num_channels, base_filter, feat, num_stages, n_resblock,\n               nFrames, scale_factor):\n    super(Rbpn, self).__init__()\n    # base_filter=256\n    # feat=64\n    self.nFrames = nFrames\n\n    if scale_factor == 2:\n      kernel = 6\n      stride = 2\n      padding = 2\n    elif scale_factor == 4:\n      kernel = 8\n      stride = 4\n      padding = 2\n    elif scale_factor == 8:\n      kernel = 12\n      stride = 8\n      padding = 2\n\n    # Initial Feature Extraction\n    self.feat0 = ConvBlock(num_channels, base_filter, 3, 1, 1,\n                           activation='prelu', norm=None)\n    self.feat1 = ConvBlock(8, base_filter, 3, 1, 1, activation='prelu',\n                           norm=None)\n\n    ###DBPNS\n    self.DBPN = Dbpns(base_filter, feat, num_stages, scale_factor)\n\n    # Res-Block1\n    modules_body1 = [\n      ResnetBlock(base_filter, kernel_size=3, stride=1, padding=1, bias=True,\n                  activation='prelu', norm=None) \\\n      for _ in range(n_resblock)]\n    modules_body1.append(\n      DeconvBlock(base_filter, feat, kernel, stride, padding,\n                  activation='prelu', norm=None))\n    self.res_feat1 = nn.Sequential(*modules_body1)\n\n    # Res-Block2\n    modules_body2 = [\n      ResnetBlock(feat, kernel_size=3, stride=1, padding=1, bias=True,\n                  activation='prelu', norm=None) \\\n      for _ in range(n_resblock)]\n    modules_body2.append(\n      ConvBlock(feat, feat, 3, 1, 1, activation='prelu', norm=None))\n    self.res_feat2 = nn.Sequential(*modules_body2)\n\n    # Res-Block3\n    modules_body3 = [\n      ResnetBlock(feat, kernel_size=3, stride=1, padding=1, bias=True,\n                  activation='prelu', norm=None) \\\n      for _ in range(n_resblock)]\n    modules_body3.append(ConvBlock(feat, base_filter, kernel, stride, padding,\n                                   activation='prelu', norm=None))\n    self.res_feat3 = nn.Sequential(*modules_body3)\n\n    # Reconstruction\n    self.output = ConvBlock((nFrames - 1) * feat, num_channels, 3, 1, 1,\n                            activation=None, norm=None)\n\n    for m in self.modules():\n      classname = m.__class__.__name__\n      if classname.find('Conv2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n      elif classname.find('ConvTranspose2d') != -1:\n        torch.nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n  def forward(self, x, neigbor, flow):\n    ### initial feature extraction\n    feat_input = self.feat0(x)\n    feat_frame = []\n    for j in range(len(neigbor)):\n      feat_frame.append(self.feat1(torch.cat((x, neigbor[j], flow[j]), 1)))\n\n    ####Projection\n    Ht = []\n    for j in range(len(neigbor)):\n      h0 = self.DBPN(feat_input)\n      h1 = self.res_feat1(feat_frame[j])\n\n      e = h0 - h1\n      e = self.res_feat2(e)\n      h = h0 + e\n      Ht.append(h)\n      feat_input = self.res_feat3(h)\n\n    ####Reconstruction\n    out = torch.cat(Ht, 1)\n    output = self.output(out)\n\n    return output\n"""
VSR/Backend/Torch/Models/rcan/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 15\n\nimport logging\n_logger = logging.getLogger(""VSR.RCAN"")\n_logger.info(""LICENSE: RCAN is implemented by Yulun Zhang. ""\n             ""@yulunzhang https://github.com/yulunzhang/RCAN."")\n'"
VSR/Backend/Torch/Models/rcan/common.py,0,"b'import math\n\nimport torch\nimport torch.nn as nn\n\n\ndef default_conv(in_channels, out_channels, kernel_size, bias=True):\n  return nn.Conv2d(\n    in_channels, out_channels, kernel_size,\n    padding=(kernel_size // 2), bias=bias)\n\n\nclass MeanShift(nn.Conv2d):\n  def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n    super(MeanShift, self).__init__(3, 3, kernel_size=1)\n    std = torch.Tensor(rgb_std)\n    self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n    self.weight.data.div_(std.view(3, 1, 1, 1))\n    self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n    self.bias.data.div_(std)\n    self.requires_grad = False\n\n\nclass BasicBlock(nn.Sequential):\n  def __init__(\n      self, in_channels, out_channels, kernel_size, stride=1, bias=False,\n      bn=True, act=nn.ReLU(True)):\n\n    m = [nn.Conv2d(\n      in_channels, out_channels, kernel_size,\n      padding=(kernel_size // 2), stride=stride, bias=bias)\n    ]\n    if bn: m.append(nn.BatchNorm2d(out_channels))\n    if act is not None: m.append(act)\n    super(BasicBlock, self).__init__(*m)\n\n\nclass ResBlock(nn.Module):\n  def __init__(\n      self, conv, n_feat, kernel_size,\n      bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n    super(ResBlock, self).__init__()\n    m = []\n    for i in range(2):\n      m.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n      if bn: m.append(nn.BatchNorm2d(n_feat))\n      if i == 0: m.append(act)\n\n    self.body = nn.Sequential(*m)\n    self.res_scale = res_scale\n\n  def forward(self, x):\n    res = self.body(x).mul(self.res_scale)\n    res += x\n\n    return res\n\n\nclass Upsampler(nn.Sequential):\n  def __init__(self, conv, scale, n_feat, bn=False, act=False, bias=True):\n\n    m = []\n    if (scale & (scale - 1)) == 0:  # Is scale = 2^n?\n      for _ in range(int(math.log(scale, 2))):\n        m.append(conv(n_feat, 4 * n_feat, 3, bias))\n        m.append(nn.PixelShuffle(2))\n        if bn: m.append(nn.BatchNorm2d(n_feat))\n        if act: m.append(act())\n    elif scale == 3:\n      m.append(conv(n_feat, 9 * n_feat, 3, bias))\n      m.append(nn.PixelShuffle(3))\n      if bn: m.append(nn.BatchNorm2d(n_feat))\n      if act: m.append(act())\n    else:\n      raise NotImplementedError\n\n    super(Upsampler, self).__init__(*m)\n'"
VSR/Backend/Torch/Models/rcan/rcan.py,0,"b'import torch.nn as nn\n\nfrom . import common\n\n\ndef make_model(args, parent=False):\n  return RCAN(args)\n\n\n## Channel Attention (CA) Layer\nclass CALayer(nn.Module):\n  def __init__(self, channel, reduction=16):\n    super(CALayer, self).__init__()\n    # global average pooling: feature --> point\n    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n    # feature channel downscale and upscale --> channel weight\n    self.conv_du = nn.Sequential(\n      nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n      nn.ReLU(inplace=True),\n      nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n      nn.Sigmoid()\n    )\n\n  def forward(self, x):\n    y = self.avg_pool(x)\n    y = self.conv_du(y)\n    return x * y\n\n\n## Residual Channel Attention Block (RCAB)\nclass RCAB(nn.Module):\n  def __init__(\n      self, conv, n_feat, kernel_size, reduction,\n      bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n    super(RCAB, self).__init__()\n    modules_body = []\n    for i in range(2):\n      modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n      if bn: modules_body.append(nn.BatchNorm2d(n_feat))\n      if i == 0: modules_body.append(act)\n    modules_body.append(CALayer(n_feat, reduction))\n    self.body = nn.Sequential(*modules_body)\n    self.res_scale = res_scale\n\n  def forward(self, x):\n    res = self.body(x)\n    # res = self.body(x).mul(self.res_scale)\n    res += x\n    return res\n\n\n## Residual Group (RG)\nclass ResidualGroup(nn.Module):\n  def __init__(self, conv, n_feat, kernel_size, reduction, act, res_scale,\n               n_resblocks):\n    super(ResidualGroup, self).__init__()\n    modules_body = []\n    modules_body = [\n      RCAB(\n        conv, n_feat, kernel_size, reduction, bias=True, bn=False,\n        act=nn.ReLU(True), res_scale=1) \\\n      for _ in range(n_resblocks)]\n    modules_body.append(conv(n_feat, n_feat, kernel_size))\n    self.body = nn.Sequential(*modules_body)\n\n  def forward(self, x):\n    res = self.body(x)\n    res += x\n    return res\n\n\n## Residual Channel Attention Network (RCAN)\nclass RCAN(nn.Module):\n  def __init__(self, args, conv=common.default_conv):\n    super(RCAN, self).__init__()\n\n    n_resgroups = args.n_resgroups\n    n_resblocks = args.n_resblocks\n    n_feats = args.n_feats\n    kernel_size = 3\n    reduction = args.reduction\n    scale = args.scale[0]\n    act = nn.ReLU(True)\n\n    # RGB mean for DIV2K\n    rgb_mean = (0.4488, 0.4371, 0.4040)\n    rgb_std = (1.0, 1.0, 1.0)\n    self.sub_mean = common.MeanShift(args.rgb_range, rgb_mean, rgb_std)\n\n    # define head module\n    modules_head = [conv(args.n_colors, n_feats, kernel_size)]\n\n    # define body module\n    modules_body = [\n      ResidualGroup(\n        conv, n_feats, kernel_size, reduction, act=act,\n        res_scale=args.res_scale, n_resblocks=n_resblocks) \\\n      for _ in range(n_resgroups)]\n\n    modules_body.append(conv(n_feats, n_feats, kernel_size))\n\n    # define tail module\n    modules_tail = [\n      common.Upsampler(conv, scale, n_feats, act=False),\n      conv(n_feats, args.n_colors, kernel_size)]\n\n    self.add_mean = common.MeanShift(args.rgb_range, rgb_mean, rgb_std, 1)\n\n    self.head = nn.Sequential(*modules_head)\n    self.body = nn.Sequential(*modules_body)\n    self.tail = nn.Sequential(*modules_tail)\n\n  def forward(self, x):\n    x = self.sub_mean(x)\n    x = self.head(x)\n\n    res = self.body(x)\n    res += x\n\n    x = self.tail(res)\n    x = self.add_mean(x)\n\n    return x\n\n  def load_state_dict(self, state_dict, strict=False):\n    own_state = self.state_dict()\n    for name, param in state_dict.items():\n      if name in own_state:\n        if isinstance(param, nn.Parameter):\n          param = param.data\n        try:\n          own_state[name].copy_(param)\n        except Exception:\n          if name.find(\'tail\') >= 0:\n            print(\'Replace pre-trained upsampler to new one...\')\n          else:\n            raise RuntimeError(\'While copying the parameter named {}, \'\n                               \'whose dimensions in the model are {} and \'\n                               \'whose dimensions in the checkpoint are {}.\'\n                               .format(name, own_state[name].size(),\n                                       param.size()))\n      elif strict:\n        if name.find(\'tail\') == -1:\n          raise KeyError(\'unexpected key ""{}"" in state_dict\'\n                         .format(name))\n\n    if strict:\n      missing = set(own_state.keys()) - set(state_dict.keys())\n      if len(missing) > 0:\n        raise KeyError(\'missing keys in state_dict: ""{}""\'.format(missing))\n'"
VSR/Backend/Torch/Models/sof/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019 - 3 - 22\n\nimport logging\n_logger = logging.getLogger(""VSR.SOF"")\n_logger.info(""LICENSE: SOF-VSR is implemented by Longguan Wang. ""\n             ""@LongguanWang https://github.com/LongguangWang/SOF-VSR."")\n'"
VSR/Backend/Torch/Models/sof/modules.py,0,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef optical_flow_warp(image, image_optical_flow):\n  """"""\n  Arguments\n      image_ref: reference images tensor, (b, c, h, w)\n      image_optical_flow: optical flow to image_ref (b, 2, h, w)\n  """"""\n  b, _, h, w = image.size()\n  grid = np.meshgrid(range(w), range(h))\n  grid = np.stack(grid, axis=-1).astype(np.float64)\n  grid[:, :, 0] = grid[:, :, 0] * 2 / (w - 1) - 1\n  grid[:, :, 1] = grid[:, :, 1] * 2 / (h - 1) - 1\n  grid = grid.transpose(2, 0, 1)\n  grid = np.tile(grid, (b, 1, 1, 1))\n  grid = Variable(torch.Tensor(grid))\n  if image_optical_flow.is_cuda == True:\n    grid = grid.cuda()\n\n  flow_0 = torch.unsqueeze(image_optical_flow[:, 0, :, :] * 31 / (w - 1), dim=1)\n  flow_1 = torch.unsqueeze(image_optical_flow[:, 1, :, :] * 31 / (h - 1), dim=1)\n  grid = grid + torch.cat((flow_0, flow_1), 1)\n  grid = grid.transpose(1, 2)\n  grid = grid.transpose(3, 2)\n  output = F.grid_sample(image, grid, padding_mode=\'border\')\n  return output\n\n\nclass make_dense(nn.Module):\n  def __init__(self, channels_in, channels_out, kernel_size=3):\n    super(make_dense, self).__init__()\n    self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n    self.conv = nn.Conv2d(channels_in, channels_out, kernel_size=kernel_size,\n                          padding=(kernel_size - 1) // 2,\n                          bias=False)\n\n  def forward(self, x):\n    out = self.leaky_relu(self.conv(x))\n    out = torch.cat((x, out), 1)\n    return out\n\n\nclass RDB(nn.Module):\n  def __init__(self, nDenselayer, channels, growth):\n    super(RDB, self).__init__()\n    modules = []\n    channels_buffer = channels\n    for i in range(nDenselayer):\n      modules.append(make_dense(channels_buffer, growth))\n      channels_buffer += growth\n    self.dense_layers = nn.Sequential(*modules)\n    self.conv_1x1 = nn.Conv2d(channels_buffer, channels, kernel_size=1,\n                              padding=0, bias=False)\n\n  def forward(self, x):\n    out = self.dense_layers(x)\n    out = self.conv_1x1(out)\n    out = out + x\n    return out\n\n\nclass OFRnet(nn.Module):\n  def __init__(self, upscale_factor):\n    super(OFRnet, self).__init__()\n    self.pool = nn.AvgPool2d(kernel_size=2)\n    self.upsample = nn.Upsample(scale_factor=2, mode=\'bilinear\',\n                                align_corners=False)\n    self.final_upsample = nn.Upsample(scale_factor=upscale_factor,\n                                      mode=\'bilinear\', align_corners=False)\n    self.shuffle = nn.PixelShuffle(upscale_factor)\n    self.upscale_factor = upscale_factor\n    # Level 1\n    self.conv_L1_1 = nn.Conv2d(2, 32, 3, 1, 1, bias=False)\n    self.RDB1_1 = RDB(4, 32, 32)\n    self.RDB1_2 = RDB(4, 32, 32)\n    self.bottleneck_L1 = nn.Conv2d(64, 2, 3, 1, 1, bias=False)\n    self.conv_L1_2 = nn.Conv2d(2, 2, 3, 1, 1, bias=True)\n    # Level 2\n    self.conv_L2_1 = nn.Conv2d(6, 32, 3, 1, 1, bias=False)\n    self.RDB2_1 = RDB(4, 32, 32)\n    self.RDB2_2 = RDB(4, 32, 32)\n    self.bottleneck_L2 = nn.Conv2d(64, 2, 3, 1, 1, bias=False)\n    self.conv_L2_2 = nn.Conv2d(2, 2, 3, 1, 1, bias=True)\n    # Level 3\n    self.conv_L3_1 = nn.Conv2d(6, 32, 3, 1, 1, bias=False)\n    self.RDB3_1 = RDB(4, 32, 32)\n    self.RDB3_2 = RDB(4, 32, 32)\n    self.bottleneck_L3 = nn.Conv2d(64, 2 * upscale_factor ** 2, 3, 1, 1,\n                                   bias=False)\n    self.conv_L3_2 = nn.Conv2d(2 * upscale_factor ** 2, 2 * upscale_factor ** 2,\n                               3, 1, 1, bias=True)\n\n  def forward(self, x):\n    # Level 1\n    x_L1 = self.pool(x)\n    _, _, h, w = x_L1.size()\n    input_L1 = self.conv_L1_1(x_L1)\n    buffer_1 = self.RDB1_1(input_L1)\n    buffer_2 = self.RDB1_2(buffer_1)\n    buffer = torch.cat((buffer_1, buffer_2), 1)\n    optical_flow_L1 = self.bottleneck_L1(buffer)\n    optical_flow_L1 = self.conv_L1_2(optical_flow_L1)\n    optical_flow_L1_upscaled = self.upsample(optical_flow_L1)  # *2\n    # x_L1_res = optical_flow_warp(torch.unsqueeze(x_L1[:, 0, :, :], dim=1), optical_flow_L1) - torch.unsqueeze(x_L1[:, 1, :, :], dim=1)\n    # Level 2\n    x_L2 = optical_flow_warp(torch.unsqueeze(x[:, 0, :, :], dim=1),\n                             optical_flow_L1_upscaled)\n    x_L2_res = torch.unsqueeze(x[:, 1, :, :], dim=1) - x_L2\n    x_L2 = torch.cat((x, x_L2, x_L2_res, optical_flow_L1_upscaled), 1)\n    input_L2 = self.conv_L2_1(x_L2)\n    buffer_1 = self.RDB2_1(input_L2)\n    buffer_2 = self.RDB2_2(buffer_1)\n    buffer = torch.cat((buffer_1, buffer_2), 1)\n    optical_flow_L2 = self.bottleneck_L2(buffer)\n    optical_flow_L2 = self.conv_L2_2(optical_flow_L2)\n    optical_flow_L2 = optical_flow_L2 + optical_flow_L1_upscaled\n    # x_L2_res = optical_flow_warp(torch.unsqueeze(x_L2[:, 0, :, :], dim=1), optical_flow_L2) - torch.unsqueeze(x_L2[:, 1, :, :], dim=1)\n    # Level 3\n    x_L3 = optical_flow_warp(torch.unsqueeze(x[:, 0, :, :], dim=1),\n                             optical_flow_L2)\n    x_L3_res = torch.unsqueeze(x[:, 1, :, :], dim=1) - x_L3\n    x_L3 = torch.cat((x, x_L3, x_L3_res, optical_flow_L2), 1)\n    input_L3 = self.conv_L3_1(x_L3)\n    buffer_1 = self.RDB3_1(input_L3)\n    buffer_2 = self.RDB3_2(buffer_1)\n    buffer = torch.cat((buffer_1, buffer_2), 1)\n    optical_flow_L3 = self.bottleneck_L3(buffer)\n    optical_flow_L3 = self.conv_L3_2(optical_flow_L3)\n    optical_flow_L3 = self.shuffle(optical_flow_L3) + self.final_upsample(\n      optical_flow_L2)  # *4\n\n    return optical_flow_L3, optical_flow_L2, optical_flow_L1\n\n\nclass SRnet(nn.Module):\n  def __init__(self, s, c, d):\n    """"""\n    Args:\n      s: scale factor\n      c: channel numbers\n      d: video sequence number\n    """"""\n    super(SRnet, self).__init__()\n    self.conv = nn.Conv2d(c * (2 * s ** 2 + d), 64, 3, 1, 1, bias=False)\n    self.RDB_1 = RDB(5, 64, 32)\n    self.RDB_2 = RDB(5, 64, 32)\n    self.RDB_3 = RDB(5, 64, 32)\n    self.RDB_4 = RDB(5, 64, 32)\n    self.RDB_5 = RDB(5, 64, 32)\n    self.bottleneck = nn.Conv2d(384, c * s ** 2, 1, 1, 0, bias=False)\n    self.conv_2 = nn.Conv2d(c * s ** 2, c * s ** 2, 3, 1, 1, bias=True)\n    self.shuffle = nn.PixelShuffle(upscale_factor=s)\n\n  def forward(self, x):\n    input = self.conv(x)\n    buffer_1 = self.RDB_1(input)\n    buffer_2 = self.RDB_2(buffer_1)\n    buffer_3 = self.RDB_3(buffer_2)\n    buffer_4 = self.RDB_4(buffer_3)\n    buffer_5 = self.RDB_5(buffer_4)\n    output = torch.cat(\n      (buffer_1, buffer_2, buffer_3, buffer_4, buffer_5, input), 1)\n    output = self.bottleneck(output)\n    output = self.conv_2(output)\n    output = self.shuffle(output)\n    return output\n\n\nclass SOFVSR(nn.Module):\n  def __init__(self, scale, channel, depth):\n    super(SOFVSR, self).__init__()\n    self.upscale_factor = scale\n    self.c = channel\n    self.OFRnet = OFRnet(upscale_factor=scale)\n    self.SRnet = SRnet(scale, channel, depth)\n\n  def forward(self, x):\n    input_01 = torch.cat((torch.unsqueeze(x[:, 0, :, :], dim=1),\n                          torch.unsqueeze(x[:, 1, :, :], dim=1)), 1)\n    input_21 = torch.cat((torch.unsqueeze(x[:, 2, :, :], dim=1),\n                          torch.unsqueeze(x[:, 1, :, :], dim=1)), 1)\n    flow_01_L3, flow_01_L2, flow_01_L1 = self.OFRnet(input_01)\n    flow_21_L3, flow_21_L2, flow_21_L1 = self.OFRnet(input_21)\n    draft_cube = x\n    for i in range(self.upscale_factor):\n      for j in range(self.upscale_factor):\n        draft_01 = optical_flow_warp(x[:, :self.c, :, :],\n                                     flow_01_L3[:, :, i::self.upscale_factor,\n                                     j::self.upscale_factor] / self.upscale_factor)\n        draft_21 = optical_flow_warp(x[:, self.c * 2:, :, :],\n                                     flow_21_L3[:, :, i::self.upscale_factor,\n                                     j::self.upscale_factor] / self.upscale_factor)\n        draft_cube = torch.cat((draft_cube, draft_01, draft_21), 1)\n    output = self.SRnet(draft_cube)\n    return output, (flow_01_L3, flow_01_L2, flow_01_L1), (\n      flow_21_L3, flow_21_L2, flow_21_L1)\n'"
VSR/Backend/Torch/Models/spmc/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/25 \xe4\xb8\x8b\xe5\x8d\x884:38\n\nimport logging\n\n_logger = logging.getLogger(""VSR.SPMC"")\n_logger.info(""LICENSE: SPMC is proposed by X. Tao, et. al. ""\n             ""Implemented via PyTorch by @LoSealL."")\n_logger.info(""LICENSE: ConvLSTM is implemented by @Kaixhin."")\n'"
VSR/Backend/Torch/Models/spmc/ops.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/5/26 \xe4\xb8\x8a\xe5\x8d\x8811:39\n\nimport math\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.nn.modules.utils import _pair\n\nfrom ..Arch import EasyConv2d\nfrom ..video.motion import STN\nfrom ...Util.Utility import upsample\n\n\nclass Conv2dLSTMCell(nn.Module):\n  """"""ConvLSTM cell.\n  Copied from https://gist.github.com/Kaixhin/57901e91e5c5a8bac3eb0cbbdd3aba81\n  Special thanks to @Kaixhin\n  """"""\n\n  def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n               padding=0, dilation=1, groups=1, bias=True):\n\n    super(Conv2dLSTMCell, self).__init__()\n    if in_channels % groups != 0:\n      raise ValueError(\'in_channels must be divisible by groups\')\n    if out_channels % groups != 0:\n      raise ValueError(\'out_channels must be divisible by groups\')\n    kernel_size = _pair(kernel_size)\n    stride = _pair(stride)\n    padding = _pair(padding)\n    dilation = _pair(dilation)\n    self.in_channels = in_channels\n    self.out_channels = out_channels\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.padding = padding\n    self.padding_h = tuple(\n      k // 2 for k, s, p, d in zip(kernel_size, stride, padding, dilation))\n    self.dilation = dilation\n    self.groups = groups\n    self.weight_ih = Parameter(\n      torch.Tensor(4 * out_channels, in_channels // groups, *kernel_size))\n    self.weight_hh = Parameter(\n      torch.Tensor(4 * out_channels, out_channels // groups, *kernel_size))\n    self.weight_ch = Parameter(\n      torch.Tensor(3 * out_channels, out_channels // groups, *kernel_size))\n    if bias:\n      self.bias_ih = Parameter(torch.Tensor(4 * out_channels))\n      self.bias_hh = Parameter(torch.Tensor(4 * out_channels))\n      self.bias_ch = Parameter(torch.Tensor(3 * out_channels))\n    else:\n      self.register_parameter(\'bias_ih\', None)\n      self.register_parameter(\'bias_hh\', None)\n      self.register_parameter(\'bias_ch\', None)\n    self.register_buffer(\'wc_blank\', torch.zeros(out_channels))\n    self.reset_parameters()\n\n  def reset_parameters(self):\n    n = 4 * self.in_channels\n    for k in self.kernel_size:\n      n *= k\n    stdv = 1. / math.sqrt(n)\n    self.weight_ih.data.uniform_(-stdv, stdv)\n    self.weight_hh.data.uniform_(-stdv, stdv)\n    self.weight_ch.data.uniform_(-stdv, stdv)\n    if self.bias_ih is not None:\n      self.bias_ih.data.uniform_(-stdv, stdv)\n      self.bias_hh.data.uniform_(-stdv, stdv)\n      self.bias_ch.data.uniform_(-stdv, stdv)\n\n  def forward(self, input, hx):\n    h_0, c_0 = hx\n\n    wx = F.conv2d(input, self.weight_ih, self.bias_ih, self.stride,\n                  self.padding, self.dilation, self.groups)\n    wh = F.conv2d(h_0, self.weight_hh, self.bias_hh, self.stride,\n                  self.padding_h, self.dilation, self.groups)\n    # Cell uses a Hadamard product instead of a convolution?\n    wc = F.conv2d(c_0, self.weight_ch, self.bias_ch, self.stride,\n                  self.padding_h, self.dilation, self.groups)\n    v = Variable(self.wc_blank).reshape((1, -1, 1, 1))\n    wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels],\n                                v.expand(wc.size(0), wc.size(1) // 3,\n                                         wc.size(2), wc.size(3)),\n                                wc[:, 2 * self.out_channels:]), 1)\n\n    i = torch.sigmoid(wxhc[:, :self.out_channels])\n    f = torch.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n    g = torch.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n    o = torch.sigmoid(wxhc[:, 3 * self.out_channels:])\n\n    c_1 = f * c_0 + i * g\n    h_1 = o * torch.tanh(c_1)\n    return h_1, (h_1, c_1)\n\n\nclass ZeroUpsample(nn.Module):\n  def __init__(self, scale_factor):\n    super(ZeroUpsample, self).__init__()\n    self.ps = nn.PixelShuffle(scale_factor)\n    self.scale = scale_factor\n\n  def forward(self, x):\n    z = torch.zeros_like(x).repeat_interleave(self.scale ** 2 - 1, dim=1)\n    x = torch.cat((x, z), dim=1)\n    return self.ps(x)\n\n\nclass SPMC(nn.Module):\n  def __init__(self, scale):\n    super(SPMC, self).__init__()\n    self.zero_up = ZeroUpsample(scale)\n    self.warper = STN()\n    self.scale = scale\n\n  def forward(self, x, u=0, v=0, flow=None):\n    if flow is not None:\n      u = flow[:, 0]\n      v = flow[:, 1]\n    x2 = self.zero_up(x)\n    u2 = self.zero_up(u.unsqueeze(1)) * self.scale\n    v2 = self.zero_up(v.unsqueeze(1)) * self.scale\n    return self.warper(x2, u2.squeeze(1), v2.squeeze(1))\n\n\nclass MotionEstimation(nn.Module):\n  def __init__(self, channel, gain=32):\n    super(MotionEstimation, self).__init__()\n    self.gain = gain\n    in_c = channel * 2\n    # Coarse Flow\n    conv1 = nn.Sequential(nn.Conv2d(in_c, 24, 5, 2, 2), nn.ReLU(True))\n    conv2 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv3 = nn.Sequential(nn.Conv2d(24, 24, 5, 2, 2), nn.ReLU(True))\n    conv4 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv5 = nn.Sequential(nn.Conv2d(24, 32, 3, 1, 1), nn.Tanh())\n    up1 = nn.PixelShuffle(4)\n    self.coarse_flow = nn.Sequential(conv1, conv2, conv3, conv4, conv5, up1)\n    # Fine Flow\n    in_c = channel * 3 + 2\n    conv1 = nn.Sequential(nn.Conv2d(in_c, 24, 5, 2, 2), nn.ReLU(True))\n    conv2 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv3 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv4 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv5 = nn.Sequential(nn.Conv2d(24, 8, 3, 1, 1), nn.Tanh())\n    up2 = nn.PixelShuffle(2)\n    self.fine_flow = nn.Sequential(conv1, conv2, conv3, conv4, conv5, up2)\n    self.warper = STN(padding_mode=\'border\')\n\n  def forward(self, target, ref, to_tuple=None):\n    flow0 = self.coarse_flow(torch.cat((ref, target), dim=1))\n    w0 = self.warper(ref, flow0[:, 0], flow0[:, 1])\n    flow_res = self.fine_flow(torch.cat((ref, target, flow0, w0), dim=1))\n    flow1 = (flow_res + flow0) * self.gain\n    if to_tuple:\n      return flow1[:, 0], flow1[:, 1]\n    return flow1\n\n\nclass DetailFusion(nn.Module):\n  def __init__(self, channel, base_filter):\n    super(DetailFusion, self).__init__()\n    f = base_filter\n    self.enc1 = EasyConv2d(channel, f, 5, activation=\'relu\')\n    self.enc2 = nn.Sequential(\n      EasyConv2d(f, f * 2, 3, 2, activation=\'relu\'),\n      EasyConv2d(f * 2, f * 2, 3, activation=\'relu\'))\n    self.enc3 = EasyConv2d(f * 2, f * 4, 3, 2, activation=\'relu\')\n    self.lstm = Conv2dLSTMCell(f * 4, f * 4, 3, 1, 1)\n    self.dec1 = nn.Sequential(\n      EasyConv2d(f * 4, f * 4, 3, activation=\'relu\'),\n      nn.ConvTranspose2d(f * 4, f * 2, 4, 2, 1),\n      nn.ReLU(True))\n    self.dec2 = nn.Sequential(\n      EasyConv2d(f * 2, f * 2, 3, activation=\'relu\'),\n      nn.ConvTranspose2d(f * 2, f, 4, 2, 1),\n      nn.ReLU(True))\n    self.dec3 = nn.Sequential(\n      EasyConv2d(f, f, 3, activation=\'relu\'),\n      EasyConv2d(f, channel, 5))\n\n  def forward(self, x, hx):\n    add1 = self.enc1(x)\n    add2 = self.enc2(add1)\n    h0 = self.enc3(add2)\n    x, hx = self.lstm(h0, hx)\n    x = self.dec1(x)\n    x = self.dec2(x + add2)\n    x = self.dec3(x + add1)\n    return x, hx\n\n\nclass DetailRevealer(nn.Module):\n  def __init__(self, scale, channel, **kwargs):\n    super(DetailRevealer, self).__init__()\n    self.base_filter = kwargs.get(\'base_filter\', 32)\n    self.me = MotionEstimation(channel, gain=kwargs.get(\'gain\', 32))\n    self.spmc = SPMC(scale)\n    self.vsr = DetailFusion(channel, self.base_filter)\n    self.scale = scale\n    self.hidden_state = None\n\n  def reset(self):\n    self.hidden_state = None\n\n  def forward(self, target, ref):\n    flow = self.me(target, ref)\n    hr_ref = self.spmc(ref, flow=flow)\n    hr_target = upsample(target, self.scale)\n    if self.hidden_state is None:\n      batch, _, height, width = hr_ref.shape\n      hidden_shape = (batch, self.base_filter * 4, height // 4, width // 4)\n      hx = (torch.zeros(hidden_shape, device=ref.device),\n            torch.zeros(hidden_shape, device=ref.device))\n    else:\n      hx = self.hidden_state\n    res, hx = self.vsr(hr_ref, hx)\n    sr = hr_target + res\n    self.hidden_state = hx\n    return sr, flow\n'"
VSR/Backend/Torch/Models/srfeat/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/27 \xe4\xb8\x8b\xe5\x8d\x8811:06\n\nimport logging\n\n_logger = logging.getLogger(""VSR.SRFEAT"")\n_logger.info(""LICENSE: SRFeat is proposed by S. Park, et. al. ""\n             ""Implemented via PyTorch by @LoSealL."")\n'"
VSR/Backend/Torch/Models/srfeat/ops.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/27 \xe4\xb8\x8b\xe5\x8d\x8811:06\n\nimport torch\nimport torch.nn as nn\n\nfrom ..Arch import RB, Upsample\n\n\nclass Generator(nn.Module):\n  """"""Generator for SRFeat: Single Image Super-Resolution with Feature Discrimination (ECCV 2018)\n\n  """"""\n\n  def __init__(self, channel, scale, filters, num_rb):\n    super(Generator, self).__init__()\n    self.head = nn.Conv2d(channel, filters, 9, 1, 4)\n    for i in range(num_rb):\n      setattr(self, f\'rb_{i:02d}\', RB(filters, 3, \'lrelu\', use_bn=True))\n      setattr(self, f\'merge_{i:02d}\', nn.Conv2d(filters, filters, 1))\n    self.tail = nn.Sequential(\n      Upsample(filters, scale),\n      nn.Conv2d(filters, channel, 3, 1, 1))\n    self.num_rb = num_rb\n\n  def forward(self, inputs):\n    x = self.head(inputs)\n    feat = []\n    for i in range(self.num_rb):\n      x = getattr(self, f\'rb_{i:02d}\')(x)\n      feat.append(getattr(self, f\'merge_{i:02d}\')(x))\n    x = self.tail(x + torch.stack(feat, dim=0).sum(0).squeeze(0))\n    return x\n'"
VSR/Backend/Torch/Models/srmd/__init__.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 11\n\nimport logging\n\nlogging.getLogger(""VSR.SRFEAT"").info(\n    ""LICENSE: SRMD is proposed by Kai Zhang, et. al. ""\n    ""Implemented via PyTorch by @LoSealL."")\n'"
VSR/Backend/Torch/Models/srmd/ops.py,0,"b'#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 11\n\nimport torch\nimport torch.nn as nn\n\nfrom ..Arch import EasyConv2d\n\n\nclass Net(nn.Module):\n  """"""\n  SRMD CNN network. 12 conv layers\n  """"""\n\n  def __init__(self, scale=4, channels=3, layers=12, filters=128,\n               pca_length=15):\n    super(Net, self).__init__()\n    self.pca_length = pca_length\n    net = [EasyConv2d(channels + pca_length + 1, filters, 3, activation=\'relu\')]\n    net += [EasyConv2d(filters, filters, 3, activation=\'relu\') for _ in\n            range(layers - 2)]\n    net += [EasyConv2d(filters, channels * scale ** 2, 3),\n            nn.PixelShuffle(scale)]\n    self.body = nn.Sequential(*net)\n\n  def forward(self, x, kernel=None, noise=None):\n    if kernel is None and noise is None:\n      kernel = torch.zeros(x.shape[0], 15, 1, device=x.device, dtype=x.dtype)\n      noise = torch.zeros(x.shape[0], 1, 1, device=x.device, dtype=x.dtype)\n    # degradation parameter\n    degpar = torch.cat([kernel, noise.reshape([-1, 1, 1])], dim=1)\n    degpar = degpar.reshape([-1, 1 + self.pca_length, 1, 1])\n    degpar = torch.ones_like(x)[:, 0:1] * degpar\n    _x = torch.cat([x, degpar], dim=1)\n    return self.body(_x)\n'"
VSR/Backend/Torch/Models/srmd/pca.py,0,"b""#  Copyright (c) 2017-2020 Wenyi Tang.\n#  Author: Wenyi Tang\n#  Email: wenyitang@outlook.com\n#  Update: 2020 - 2 - 12\n\nimport numpy as np\nfrom VSR.Util.Math import anisotropic_gaussian_kernel\n\n\n# Pre-calculated PCA array\n_PCA = np.array(\n    [[-0.0020399868953973055,\n      -0.0029279158916324377,\n      -0.0017012208700180054,\n      0.0030665011145174503,\n      0.013115793466567993,\n      0.011058831587433815,\n      0.003089039819315076,\n      5.052147145079289e-08,\n      -0.0030889532063156366,\n      -0.011058780364692211,\n      -0.013115735724568367,\n      -0.0030665346421301365,\n      0.0017011883901432157,\n      0.002927895402535796,\n      0.002039877697825432,\n      -0.002927914261817932,\n      -0.008832859806716442,\n      -0.011305442079901695,\n      -0.0028356409166008234,\n      0.021720848977565765,\n      0.033999282866716385,\n      0.012819912284612656,\n      -1.0304020037210648e-07,\n      -0.012820076197385788,\n      -0.03399936482310295,\n      -0.021720796823501587,\n      0.0028356490656733513,\n      0.011305538937449455,\n      0.008832959458231926,\n      0.0029278909787535667,\n      -0.0017012213356792927,\n      -0.011305435560643673,\n      -0.031703539192676544,\n      -0.034133315086364746,\n      0.011720086447894573,\n      0.07361755520105362,\n      0.0450340211391449,\n      -2.1959802154469799e-07,\n      -0.045034054666757584,\n      -0.07361731678247452,\n      -0.011720090173184872,\n      0.034133270382881165,\n      0.031703609973192215,\n      0.011305524967610836,\n      0.0017011829186230898,\n      0.0030664836522191763,\n      -0.0028356106486171484,\n      -0.034133315086364746,\n      -0.09118887037038803,\n      -0.06951935589313507,\n      0.08591154962778091,\n      0.1186351627111435,\n      -6.440835420562507e-08,\n      -0.11863492429256439,\n      -0.08591113984584808,\n      0.0695190504193306,\n      0.09118873625993729,\n      0.0341331884264946,\n      0.002835656516253948,\n      -0.0030665751546621323,\n      0.013115775771439075,\n      0.02172083966434002,\n      0.011719997972249985,\n      -0.0695192962884903,\n      -0.19212305545806885,\n      -0.05470500886440277,\n      0.18521863222122192,\n      -1.3679414223588537e-07,\n      -0.18521904945373535,\n      0.05470552295446396,\n      0.19212308526039124,\n      0.06951904296875,\n      -0.0117203863337636,\n      -0.021720917895436287,\n      -0.013115822337567806,\n      0.011058809235692024,\n      0.033999279141426086,\n      0.07361724972724915,\n      0.08591150492429733,\n      -0.05470556393265724,\n      -0.2146897315979004,\n      0.06187356263399124,\n      -1.6864385088410927e-07,\n      -0.061873096972703934,\n      0.214689701795578,\n      0.0547051876783371,\n      -0.08591204881668091,\n      -0.0736178532242775,\n      -0.0339994952082634,\n      -0.011058837175369263,\n      0.003089070087298751,\n      0.012820051982998848,\n      0.045034006237983704,\n      0.11863528937101364,\n      0.18521881103515625,\n      0.0618734136223793,\n      -0.02754642628133297,\n      5.75851686335227e-07,\n      0.027546515688300133,\n      -0.06187382712960243,\n      -0.18521854281425476,\n      -0.11863555759191513,\n      -0.04503418505191803,\n      -0.01281999796628952,\n      -0.003088951576501131,\n      1.0044065135161873e-07,\n      2.7556211534829345e-08,\n      1.4632682621140702e-07,\n      4.4122322151451954e-07,\n      2.4363518491554714e-07,\n      -5.78519632199459e-07,\n      -8.986797972454497e-09,\n      -1.1236095609490349e-08,\n      3.5835955713992007e-07,\n      -2.790251301121316e-07,\n      4.284285353151063e-07,\n      5.408565471043403e-07,\n      1.1483560768965617e-07,\n      4.331122127609888e-08,\n      1.0811985617920072e-07,\n      -0.0030889571644365788,\n      -0.012820024974644184,\n      -0.0450342632830143,\n      -0.11863547563552856,\n      -0.1852184236049652,\n      -0.06187338009476662,\n      0.027546415105462074,\n      -6.283627271841397e-07,\n      -0.027546605095267296,\n      0.061873529106378555,\n      0.1852189600467682,\n      0.11863535642623901,\n      0.04503398388624191,\n      0.012820008210837841,\n      0.0030890691559761763,\n      -0.011058833450078964,\n      -0.0339994914829731,\n      -0.07361790537834167,\n      -0.08591198921203613,\n      0.05470512434840202,\n      0.21468974649906158,\n      -0.06187373399734497,\n      -7.18920674103174e-08,\n      0.06187371164560318,\n      -0.21468962728977203,\n      -0.054705359041690826,\n      0.08591153472661972,\n      0.07361729443073273,\n      0.033999279141426086,\n      0.011058841831982136,\n      -0.01311581488698721,\n      -0.021720897406339645,\n      -0.011720368638634682,\n      0.06951906532049179,\n      0.19212308526039124,\n      0.05470569431781769,\n      -0.1852189004421234,\n      -7.051387740375503e-08,\n      0.1852189302444458,\n      -0.0547051727771759,\n      -0.1921229362487793,\n      -0.06951925158500671,\n      0.011720013804733753,\n      0.02172081731259823,\n      0.013115763664245605,\n      -0.00306657119654119,\n      0.002835631836205721,\n      0.0341331921517849,\n      0.09118872880935669,\n      0.06951911747455597,\n      -0.08591118454933167,\n      -0.11863486468791962,\n      -1.2696521878297062e-07,\n      0.11863543838262558,\n      0.08591163903474808,\n      -0.06951933354139328,\n      -0.09118882566690445,\n      -0.03413332253694534,\n      -0.002835612278431654,\n      0.003066489240154624,\n      0.0017011859454214573,\n      0.011305529624223709,\n      0.03170362859964371,\n      0.03413325920701027,\n      -0.011720077134668827,\n      -0.07361733168363571,\n      -0.04503408446907997,\n      -2.28148394398886e-07,\n      0.04503396525979042,\n      0.07361747324466705,\n      0.011720106936991215,\n      -0.03413333743810654,\n      -0.03170353174209595,\n      -0.01130544114857912,\n      -0.0017012176103889942,\n      0.0029278926085680723,\n      0.008832959458231926,\n      0.01130553986877203,\n      0.0028356541879475117,\n      -0.02172081544995308,\n      -0.033999305218458176,\n      -0.012820043601095676,\n      -8.881274737859712e-08,\n      0.012819955125451088,\n      0.03399932011961937,\n      0.02172083593904972,\n      -0.0028356376569718122,\n      -0.011305447667837143,\n      -0.008832859806716442,\n      -0.002927917055785656,\n      0.0020398807246237993,\n      0.002927894238382578,\n      0.001701189437881112,\n      -0.0030665304511785507,\n      -0.013115731999278069,\n      -0.01105878408998251,\n      -0.0030889457557350397,\n      4.149776700046459e-08,\n      0.003089060541242361,\n      0.011058846488595009,\n      0.013115780428051949,\n      0.0030664969235658646,\n      -0.0017012200551107526,\n      -0.00292791286483407,\n      -0.002039985964074731],\n     [-1.785886283300897e-08,\n      -0.00013300833234097809,\n      -0.0004993926850147545,\n      -0.003832455025985837,\n      -0.011758697219192982,\n      -0.007401719223707914,\n      0.01311352290213108,\n      0.033717263489961624,\n      0.013113564811646938,\n      -0.007401665206998587,\n      -0.011758641339838505,\n      -0.003832402639091015,\n      -0.0004993624752387404,\n      -0.00013298772682901472,\n      -5.527549973294299e-09,\n      0.00013296937686391175,\n      -4.02503310681368e-08,\n      -0.0004475064342841506,\n      -0.004601374734193087,\n      -0.022123944014310837,\n      -0.02648961916565895,\n      0.022006889805197716,\n      0.07165329903364182,\n      0.022007053717970848,\n      -0.02648947574198246,\n      -0.022123822942376137,\n      -0.004601260181516409,\n      -0.00044741216697730124,\n      3.6732699104646827e-09,\n      0.00013298960402607918,\n      0.0004993485054001212,\n      0.00044737462303601205,\n      -1.149224786445302e-07,\n      -0.004314514808356762,\n      -0.03044459968805313,\n      -0.06543167680501938,\n      0.02145235426723957,\n      0.13307125866413116,\n      0.02145262248814106,\n      -0.06543131172657013,\n      -0.030444243922829628,\n      -0.004314222373068333,\n      7.714147898241208e-08,\n      0.0004474669985938817,\n      0.0004993818583898246,\n      0.0038323933258652687,\n      0.004601231310516596,\n      0.004314157646149397,\n      -2.931185747456766e-07,\n      -0.028864746913313866,\n      -0.11178688704967499,\n      -0.014350054785609245,\n      0.20262902975082397,\n      -0.014350170269608498,\n      -0.11178651452064514,\n      -0.02886408008635044,\n      2.1570045305452368e-07,\n      0.004314441699534655,\n      0.00460134306922555,\n      0.003832445712760091,\n      0.01175867859274149,\n      0.022123783826828003,\n      0.030444186180830002,\n      0.028864068910479546,\n      -7.564701718365541e-07,\n      -0.12210830301046371,\n      -0.09767520427703857,\n      0.21406705677509308,\n      -0.09767533838748932,\n      -0.12210797518491745,\n      3.3430754342589353e-07,\n      0.02886463701725006,\n      0.030444487929344177,\n      0.022123903036117554,\n      0.01175873726606369,\n      0.007401596754789352,\n      0.02648937702178955,\n      0.06543111801147461,\n      0.11178652197122574,\n      0.12210772186517715,\n      1.4430897010697663e-07,\n      -0.14049440622329712,\n      0.08753777295351028,\n      -0.1404944658279419,\n      3.54689660753138e-07,\n      0.12210837751626968,\n      0.11178674548864365,\n      0.06543141603469849,\n      0.02648944966495037,\n      0.007401660084724426,\n      -0.013113537803292274,\n      -0.02200710028409958,\n      -0.02145247533917427,\n      0.01435024756938219,\n      0.09767529368400574,\n      0.1404954344034195,\n      4.724861355498433e-07,\n      -0.022305143997073174,\n      -2.576222755124036e-07,\n      0.14049473404884338,\n      0.09767493605613708,\n      0.01435023546218872,\n      -0.021452344954013824,\n      -0.02200699783861637,\n      -0.013113507069647312,\n      -0.03371721878647804,\n      -0.0716533362865448,\n      -0.13307133316993713,\n      -0.20262885093688965,\n      -0.21406696736812592,\n      -0.08753791451454163,\n      0.022305799648165703,\n      -1.3689915867587388e-08,\n      0.022305713966488838,\n      -0.08753779530525208,\n      -0.2140669971704483,\n      -0.2026287317276001,\n      -0.13307125866413116,\n      -0.071653351187706,\n      -0.03371717408299446,\n      -0.013113508932292461,\n      -0.02200699783861637,\n      -0.021452324464917183,\n      0.014350245706737041,\n      0.09767499566078186,\n      0.14049474895000458,\n      -9.606310413801111e-07,\n      -0.02230602689087391,\n      1.141126588777297e-07,\n      0.14049482345581055,\n      0.0976756364107132,\n      0.01435024756938219,\n      -0.02145250141620636,\n      -0.022007111459970474,\n      -0.013113553635776043,\n      0.007401646114885807,\n      0.026489445939660072,\n      0.06543140113353729,\n      0.1117868572473526,\n      0.12210838496685028,\n      2.4310926960424695e-07,\n      -0.1404953896999359,\n      0.0875372365117073,\n      -0.1404944211244583,\n      -4.937534754390072e-07,\n      0.1221076175570488,\n      0.11178657412528992,\n      0.065431147813797,\n      0.02648935839533806,\n      0.007401612121611834,\n      0.011758746579289436,\n      0.02212391421198845,\n      0.030444519594311714,\n      0.028864704072475433,\n      3.591519259771303e-07,\n      -0.12210814654827118,\n      -0.09767503291368484,\n      0.21406719088554382,\n      -0.09767501801252365,\n      -0.12210841476917267,\n      -5.331362444849219e-07,\n      0.028864117339253426,\n      0.03044421598315239,\n      0.022123785689473152,\n      0.011758696287870407,\n      0.003832452464848757,\n      0.0046013519167900085,\n      0.004314439371228218,\n      1.8067181883907324e-07,\n      -0.028864163905382156,\n      -0.11178643256425858,\n      -0.014350185170769691,\n      0.20262876152992249,\n      -0.014349855482578278,\n      -0.11178691685199738,\n      -0.028864651918411255,\n      -2.788786730434367e-07,\n      0.004314155783504248,\n      0.004601235501468182,\n      0.003832389134913683,\n      0.0004993830807507038,\n      0.0004474768356885761,\n      7.894903575333956e-08,\n      -0.004314210265874863,\n      -0.03044423833489418,\n      -0.06543134897947311,\n      0.021452607586979866,\n      0.13307124376296997,\n      0.0214524045586586,\n      -0.06543165445327759,\n      -0.030444616451859474,\n      -0.004314525984227657,\n      -1.209232038945629e-07,\n      0.0004473769513424486,\n      0.000499350018799305,\n      0.00013298801786731929,\n      7.641165922223081e-09,\n      -0.00044741187593899667,\n      -0.004601269494742155,\n      -0.02212381549179554,\n      -0.02648947760462761,\n      0.022007010877132416,\n      0.0716533437371254,\n      0.022006910294294357,\n      -0.026489602401852608,\n      -0.022123944014310837,\n      -0.0046013882383704185,\n      -0.0004474912420846522,\n      -5.004401515407153e-08,\n      0.00013296403631102294,\n      -3.074265064739734e-09,\n      -0.00013298804697114974,\n      -0.0004993571783415973,\n      -0.0038323921617120504,\n      -0.011758641339838505,\n      -0.007401661481708288,\n      0.013113586232066154,\n      0.03371726721525192,\n      0.0131135368719697,\n      -0.007401711773127317,\n      -0.01175870094448328,\n      -0.0038324459455907345,\n      -0.0004993872717022896,\n      -0.00013301048602443188,\n      -1.6635754107596767e-08],\n     [0.03521299734711647,\n      0.012940133921802044,\n      0.00939028151333332,\n      0.012768430635333061,\n      -0.01344638504087925,\n      0.020487667992711067,\n      -0.01742815598845482,\n      -0.02881019189953804,\n      -0.01742815598845482,\n      0.020487643778324127,\n      -0.01344640739262104,\n      0.012768428772687912,\n      0.009390292689204216,\n      0.012940139509737492,\n      0.03521300107240677,\n      0.01294013299047947,\n      -0.020423773676156998,\n      -0.022682899609208107,\n      0.0011815642938017845,\n      0.01722952537238598,\n      0.0898461565375328,\n      0.07852420955896378,\n      0.07030210644006729,\n      0.07852418720722198,\n      0.08984605967998505,\n      0.017229482531547546,\n      0.0011815686011686921,\n      -0.022682886570692062,\n      -0.02042376808822155,\n      0.01294014137238264,\n      0.009390286169946194,\n      -0.022682907059788704,\n      -0.014690889045596123,\n      0.03323446959257126,\n      0.060243505984544754,\n      0.13395652174949646,\n      0.11259789764881134,\n      0.10686244815587997,\n      0.11259773373603821,\n      0.13395634293556213,\n      0.060243356972932816,\n      0.03323452174663544,\n      -0.014690839685499668,\n      -0.02268286794424057,\n      0.009390296414494514,\n      0.012768441811203957,\n      0.0011815644102171063,\n      0.033234477043151855,\n      0.05330490320920944,\n      0.03822152316570282,\n      0.05871599167585373,\n      0.006588819436728954,\n      -0.009692247025668621,\n      0.006588302552700043,\n      0.05871523916721344,\n      0.038221683353185654,\n      0.053304996341466904,\n      0.033234596252441406,\n      0.0011815858306363225,\n      0.012768451124429703,\n      -0.013446364551782608,\n      0.017229564487934113,\n      0.060243427753448486,\n      0.03822162747383118,\n      -0.07998383790254593,\n      -0.09419766068458557,\n      -0.11733363568782806,\n      -0.11724746227264404,\n      -0.1173342615365982,\n      -0.09419847279787064,\n      -0.07998335361480713,\n      0.038221776485443115,\n      0.0602436438202858,\n      0.017229586839675903,\n      -0.01344633661210537,\n      0.020487697795033455,\n      0.0898461788892746,\n      0.1339566558599472,\n      0.05871579051017761,\n      -0.09419870376586914,\n      -0.0729561597108841,\n      0.03610151633620262,\n      0.10392013192176819,\n      0.036102328449487686,\n      -0.07295569777488708,\n      -0.09419742971658707,\n      0.05871598795056343,\n      0.13395676016807556,\n      0.08984627574682236,\n      0.020487727597355843,\n      -0.017428100109100342,\n      0.07852433621883392,\n      0.11259805411100388,\n      0.006588860880583525,\n      -0.11733333021402359,\n      0.036101970821619034,\n      0.13480354845523834,\n      -0.11055655032396317,\n      0.13480378687381744,\n      0.03610088676214218,\n      -0.11733349412679672,\n      0.006589041557163,\n      0.11259815096855164,\n      0.0785244032740593,\n      -0.017428087070584297,\n      -0.028810150921344757,\n      0.07030234485864639,\n      0.10686270147562027,\n      -0.00969215203076601,\n      -0.11724764853715897,\n      0.10391868650913239,\n      -0.1105557456612587,\n      0.01856379583477974,\n      -0.11055633425712585,\n      0.10391891747713089,\n      -0.11724734306335449,\n      -0.009692330844700336,\n      0.10686274617910385,\n      0.07030226290225983,\n      -0.028810113668441772,\n      -0.017428083345294,\n      0.0785243883728981,\n      0.11259815841913223,\n      0.006589066237211227,\n      -0.11733344942331314,\n      0.03610144928097725,\n      0.13480377197265625,\n      -0.11055692285299301,\n      0.1348048448562622,\n      0.03610245883464813,\n      -0.11733357608318329,\n      0.0065888697281479836,\n      0.11259809136390686,\n      0.07852433621883392,\n      -0.01742810197174549,\n      0.020487729460000992,\n      0.08984627574682236,\n      0.13395676016807556,\n      0.058715999126434326,\n      -0.09419745951890945,\n      -0.07295580208301544,\n      0.036102816462516785,\n      0.10391999036073685,\n      0.03610139340162277,\n      -0.07295607775449753,\n      -0.09419882297515869,\n      0.058715641498565674,\n      0.133956640958786,\n      0.089846171438694,\n      0.020487705245614052,\n      -0.013446340337395668,\n      0.017229579389095306,\n      0.0602436400949955,\n      0.038221731781959534,\n      -0.07998345792293549,\n      -0.09419877827167511,\n      -0.11733411997556686,\n      -0.11724769324064255,\n      -0.1173338070511818,\n      -0.09419768303632736,\n      -0.07998393476009369,\n      0.03822164237499237,\n      0.060243453830480576,\n      0.017229558899998665,\n      -0.013446372002363205,\n      0.012768462300300598,\n      0.0011815970065072179,\n      0.03323456272482872,\n      0.05330503359436989,\n      0.038221634924411774,\n      0.05871530994772911,\n      0.006588457617908716,\n      -0.009692209772765636,\n      0.006588910240679979,\n      0.058715980499982834,\n      0.03822164237499237,\n      0.05330491438508034,\n      0.03323447331786156,\n      0.0011815688339993358,\n      0.012768427841365337,\n      0.009390300139784813,\n      -0.02268286980688572,\n      -0.014690830372273922,\n      0.03323452174663544,\n      0.0602433942258358,\n      0.13395632803440094,\n      0.11259782314300537,\n      0.10686249285936356,\n      0.11259788274765015,\n      0.13395649194717407,\n      0.060243524610996246,\n      0.03323449566960335,\n      -0.01469089649617672,\n      -0.02268289029598236,\n      0.009390288032591343,\n      0.01294014323502779,\n      -0.020423758774995804,\n      -0.022682880982756615,\n      0.0011815679026767612,\n      0.017229489982128143,\n      0.08984608203172684,\n      0.078524149954319,\n      0.0703020915389061,\n      0.07852422446012497,\n      0.0898461639881134,\n      0.017229510471224785,\n      0.0011815563775599003,\n      -0.022682905197143555,\n      -0.0204237699508667,\n      0.012940132059156895,\n      0.03521299734711647,\n      0.012940138578414917,\n      0.009390290826559067,\n      0.01276842225342989,\n      -0.013446406461298466,\n      0.020487647503614426,\n      -0.017428135499358177,\n      -0.02881019376218319,\n      -0.017428141087293625,\n      0.02048766054213047,\n      -0.013446381315588951,\n      0.01276843249797821,\n      0.009390282444655895,\n      0.012940134853124619,\n      0.035213008522987366],\n     [-0.0013359708245843649,\n      -0.0026988994795829058,\n      -0.0048589385114610195,\n      -0.007664563599973917,\n      -0.009886864572763443,\n      -0.009911679662764072,\n      -0.006209696643054485,\n      1.5192249236406496e-08,\n      0.006209497340023518,\n      0.009911473840475082,\n      0.009886911138892174,\n      0.007664642762392759,\n      0.0048589701764285564,\n      0.0026988587342202663,\n      0.0013359837466850877,\n      -0.0026988990139216185,\n      -0.00585686881095171,\n      -0.01114058867096901,\n      -0.018340637907385826,\n      -0.024141598492860794,\n      -0.024410078302025795,\n      -0.01639947108924389,\n      -1.7273440278131602e-07,\n      0.016398988664150238,\n      0.02440960891544819,\n      0.02414141409099102,\n      0.018340561538934708,\n      0.011140490882098675,\n      0.005856736097484827,\n      0.0026988585013896227,\n      -0.004858935717493296,\n      -0.011140581220388412,\n      -0.021679053083062172,\n      -0.037398286163806915,\n      -0.0528230257332325,\n      -0.053412843495607376,\n      -0.0367395281791687,\n      -3.101567358498869e-07,\n      0.036738861352205276,\n      0.05341247469186783,\n      0.05282294377684593,\n      0.03739825263619423,\n      0.02167901024222374,\n      0.011140496470034122,\n      0.004858973436057568,\n      -0.007664554752409458,\n      -0.01834063045680523,\n      -0.03739825263619423,\n      -0.0637119933962822,\n      -0.09429735690355301,\n      -0.10054066777229309,\n      -0.06629036366939545,\n      -3.6722695995194954e-07,\n      0.06628987193107605,\n      0.1005406528711319,\n      0.09429752826690674,\n      0.06371209025382996,\n      0.03739826753735542,\n      0.0183405801653862,\n      0.0076646567322313786,\n      -0.009886856190860271,\n      -0.024141527712345123,\n      -0.05282297357916832,\n      -0.09429741650819778,\n      -0.13087347149848938,\n      -0.135506272315979,\n      -0.07614006847143173,\n      -4.741361578908254e-07,\n      0.07613987475633621,\n      0.13550640642642975,\n      0.13087359070777893,\n      0.0942976102232933,\n      0.052822988480329514,\n      0.024141473695635796,\n      0.009886943735182285,\n      -0.009911663830280304,\n      -0.024409998208284378,\n      -0.05341275408864021,\n      -0.10054095089435577,\n      -0.1355065107345581,\n      -0.08011966943740845,\n      0.05064868554472923,\n      2.651312627222069e-07,\n      -0.05064672976732254,\n      0.08011965453624725,\n      0.13550642132759094,\n      0.1005406379699707,\n      0.05341256037354469,\n      0.02440972626209259,\n      0.009911504574120045,\n      -0.006209601182490587,\n      -0.01639927737414837,\n      -0.03673914447426796,\n      -0.06629011034965515,\n      -0.0761399194598198,\n      0.050647493451833725,\n      0.3168618977069855,\n      -3.5289872357679997e-08,\n      -0.31686174869537354,\n      -0.05064843222498894,\n      0.07613983005285263,\n      0.06629034131765366,\n      0.036739282310009,\n      0.016399234533309937,\n      0.006209604442119598,\n      1.8208685048648476e-07,\n      1.5955039600612508e-07,\n      2.2561117418717913e-07,\n      3.0976809739513556e-07,\n      4.089551453034801e-07,\n      5.634558988276694e-07,\n      -7.344357300098636e-07,\n      2.8606688573518113e-08,\n      -3.790785640944705e-08,\n      -3.103232018020208e-07,\n      4.920307787870115e-07,\n      3.659076526218996e-07,\n      1.5229602468025405e-07,\n      1.4282367999385315e-07,\n      1.781378244913867e-07,\n      0.006209607236087322,\n      0.016399195417761803,\n      0.03673926740884781,\n      0.06629037111997604,\n      0.07614005357027054,\n      -0.05064815282821655,\n      -0.3168615698814392,\n      2.860222423350933e-07,\n      0.31686171889305115,\n      0.050647586584091187,\n      -0.07614016532897949,\n      -0.06629014015197754,\n      -0.03673918917775154,\n      -0.016399236395955086,\n      -0.006209596525877714,\n      0.009911502711474895,\n      0.024409713223576546,\n      0.0534125454723835,\n      0.1005406603217125,\n      0.1355062574148178,\n      0.08011948317289352,\n      -0.05064709857106209,\n      -1.0982291769323638e-06,\n      0.05064868554472923,\n      -0.08011959493160248,\n      -0.13550642132759094,\n      -0.10054095834493637,\n      -0.053412776440382004,\n      -0.02440999262034893,\n      -0.00991166103631258,\n      0.009886938147246838,\n      0.024141479283571243,\n      0.05282299593091011,\n      0.09429755806922913,\n      0.1308736652135849,\n      0.13550643622875214,\n      0.07613995671272278,\n      -6.022973479957727e-07,\n      -0.07613992691040039,\n      -0.13550619781017303,\n      -0.130873441696167,\n      -0.09429740160703659,\n      -0.05282297357916832,\n      -0.024141525849699974,\n      -0.009886850602924824,\n      0.007664655335247517,\n      0.0183405838906765,\n      0.03739827871322632,\n      0.06371211260557175,\n      0.09429750591516495,\n      0.1005406305193901,\n      0.06628976762294769,\n      -2.2430778301441023e-07,\n      -0.06629037857055664,\n      -0.10054057836532593,\n      -0.09429740905761719,\n      -0.06371200084686279,\n      -0.03739824891090393,\n      -0.01834063045680523,\n      -0.007664552889764309,\n      0.004858975764364004,\n      0.011140494607388973,\n      0.02167901210486889,\n      0.03739824518561363,\n      0.05282295495271683,\n      0.05341247096657753,\n      0.03673882782459259,\n      -3.0643261084151163e-07,\n      -0.03673955798149109,\n      -0.05341286584734917,\n      -0.05282299593091011,\n      -0.03739825636148453,\n      -0.02167906053364277,\n      -0.011140582151710987,\n      -0.004858936183154583,\n      0.0026988587342202663,\n      0.005856737494468689,\n      0.011140488088130951,\n      0.01834055967628956,\n      0.024141421541571617,\n      0.02440960891544819,\n      0.016398999840021133,\n      -1.393536450677857e-07,\n      -0.016399459913372993,\n      -0.024410076439380646,\n      -0.024141596630215645,\n      -0.018340641632676125,\n      -0.011140589602291584,\n      -0.0058568683452904224,\n      -0.0026988997124135494,\n      0.0013359823497012258,\n      0.002698858268558979,\n      0.004858972504734993,\n      0.007664648350328207,\n      0.009886914864182472,\n      0.009911470115184784,\n      0.006209504324942827,\n      2.7790639123281835e-08,\n      -0.0062097045592963696,\n      -0.009911677800118923,\n      -0.009886865504086018,\n      -0.007664554286748171,\n      -0.004858941305428743,\n      -0.0026988997124135494,\n      -0.0013359696604311466],\n     [0.013316198252141476,\n      -0.011782802641391754,\n      0.01628103293478489,\n      0.038854118436574936,\n      0.031071539968252182,\n      -0.06372655183076859,\n      -0.035409264266490936,\n      0.05535779520869255,\n      -0.03540927544236183,\n      -0.06372657418251038,\n      0.031071508303284645,\n      0.038854096084833145,\n      0.016281025484204292,\n      -0.011782807298004627,\n      0.013316194526851177,\n      -0.01178279984742403,\n      -0.05292234197258949,\n      -0.03734873980283737,\n      -0.02047141268849373,\n      -0.028468573465943336,\n      -0.12135148048400879,\n      -0.07727882266044617,\n      0.007375901564955711,\n      -0.07727886736392975,\n      -0.12135159969329834,\n      -0.028468633070588112,\n      -0.020471464842557907,\n      -0.037348758429288864,\n      -0.05292234942317009,\n      -0.011782806366682053,\n      0.016281042248010635,\n      -0.037348728626966476,\n      -0.018176548182964325,\n      0.018888698890805244,\n      0.012998627498745918,\n      -0.0896860808134079,\n      -0.06187131628394127,\n      0.00982462428510189,\n      -0.061871547251939774,\n      -0.0896863043308258,\n      0.01299842819571495,\n      0.018888618797063828,\n      -0.018176589161157608,\n      -0.03734875097870827,\n      0.01628103293478489,\n      0.038854144513607025,\n      -0.02047138288617134,\n      0.018888749182224274,\n      0.0762600228190422,\n      0.08247682452201843,\n      -0.058689992874860764,\n      -0.07725539803504944,\n      -0.039816875010728836,\n      -0.07725587487220764,\n      -0.05869064852595329,\n      0.08247668296098709,\n      0.07625988125801086,\n      0.0188886858522892,\n      -0.020471416413784027,\n      0.03885412961244583,\n      0.03107159212231636,\n      -0.028468457981944084,\n      0.01299870852380991,\n      0.08247707039117813,\n      0.11352851241827011,\n      0.0010277852416038513,\n      -0.06868176907300949,\n      -0.08498808741569519,\n      -0.06868232041597366,\n      0.0010267785983160138,\n      0.1135282889008522,\n      0.08247681707143784,\n      0.012998693622648716,\n      -0.02846851758658886,\n      0.031071586534380913,\n      -0.06372644752264023,\n      -0.12135130912065506,\n      -0.08968576043844223,\n      -0.05868978425860405,\n      0.001027388614602387,\n      0.05954010784626007,\n      0.08437780290842056,\n      0.008008844219148159,\n      0.08437839150428772,\n      0.059540025889873505,\n      0.0010277658002451062,\n      -0.0586899034678936,\n      -0.08968580514192581,\n      -0.12135132402181625,\n      -0.06372646242380142,\n      -0.03540913015604019,\n      -0.07727852463722229,\n      -0.06187082454562187,\n      -0.0772547572851181,\n      -0.06868112087249756,\n      0.08437781780958176,\n      0.19360902905464172,\n      -0.1750839203596115,\n      0.19360968470573425,\n      0.084377720952034,\n      -0.0686812773346901,\n      -0.07725469768047333,\n      -0.061870794743299484,\n      -0.07727852463722229,\n      -0.03540913388133049,\n      0.05535795912146568,\n      0.007376275956630707,\n      0.009825218468904495,\n      -0.03981610760092735,\n      -0.08498761802911758,\n      0.008007449097931385,\n      -0.17508500814437866,\n      0.03352929651737213,\n      -0.1750851571559906,\n      0.008007279597222805,\n      -0.08498747646808624,\n      -0.03981611505150795,\n      0.009825282730162144,\n      0.007376221474260092,\n      0.05535796657204628,\n      -0.03540913015604019,\n      -0.07727852463722229,\n      -0.06187080964446068,\n      -0.07725471258163452,\n      -0.06868118047714233,\n      0.08437798917293549,\n      0.19360977411270142,\n      -0.17508384585380554,\n      0.19360947608947754,\n      0.08437813073396683,\n      -0.06868132203817368,\n      -0.07725474238395691,\n      -0.06187079846858978,\n      -0.07727852463722229,\n      -0.03540912643074989,\n      -0.06372646242380142,\n      -0.12135131657123566,\n      -0.089685820043087,\n      -0.05868985131382942,\n      0.0010276935063302517,\n      0.059540100395679474,\n      0.08437870442867279,\n      0.008008881472051144,\n      0.08437781780958176,\n      0.059540241956710815,\n      0.0010272585786879063,\n      -0.058689821511507034,\n      -0.08968577533960342,\n      -0.12135130912065506,\n      -0.06372644752264023,\n      0.031071588397026062,\n      -0.02846851758658886,\n      0.012998702935874462,\n      0.08247679471969604,\n      0.11352823674678802,\n      0.001026524812914431,\n      -0.06868226081132889,\n      -0.08498834818601608,\n      -0.0686817318201065,\n      0.0010278706904500723,\n      0.11352843046188354,\n      0.08247707784175873,\n      0.012998729012906551,\n      -0.028468459844589233,\n      0.031071588397026062,\n      0.03885413333773613,\n      -0.02047141082584858,\n      0.018888670951128006,\n      0.07625988870859146,\n      0.0824766531586647,\n      -0.0586906373500824,\n      -0.07725579291582108,\n      -0.039816729724407196,\n      -0.07725536078214645,\n      -0.05869003012776375,\n      0.08247688412666321,\n      0.0762600302696228,\n      0.018888743594288826,\n      -0.02047138288617134,\n      0.03885414078831673,\n      0.01628103479743004,\n      -0.03734875097870827,\n      -0.01817658357322216,\n      0.018888607621192932,\n      0.01299844216555357,\n      -0.0896863341331482,\n      -0.06187150999903679,\n      0.009824685752391815,\n      -0.06187131628394127,\n      -0.08968609571456909,\n      0.012998650781810284,\n      0.018888704478740692,\n      -0.018176546320319176,\n      -0.03734872117638588,\n      0.016281042248010635,\n      -0.011782805435359478,\n      -0.05292234197258949,\n      -0.037348758429288864,\n      -0.020471470430493355,\n      -0.02846863679587841,\n      -0.12135157734155655,\n      -0.07727888226509094,\n      0.0073758806101977825,\n      -0.07727880775928497,\n      -0.12135148048400879,\n      -0.028468577191233635,\n      -0.020471414551138878,\n      -0.03734873980283737,\n      -0.052922338247299194,\n      -0.01178279984742403,\n      0.013316191732883453,\n      -0.011782807298004627,\n      0.016281023621559143,\n      0.038854096084833145,\n      0.031071512028574944,\n      -0.06372657418251038,\n      -0.03540927171707153,\n      0.05535779520869255,\n      -0.03540926054120064,\n      -0.06372655183076859,\n      0.03107154741883278,\n      0.038854118436574936,\n      0.01628103479743004,\n      -0.01178280171006918,\n      0.013316205702722073],\n     [5.118743118259772e-08,\n      0.00032708022627048194,\n      0.001341885537840426,\n      0.004019209649413824,\n      0.008604618720710278,\n      0.015011267736554146,\n      0.021038133651018143,\n      0.022693639621138573,\n      0.021038150414824486,\n      0.015011324547231197,\n      0.008604660630226135,\n      0.0040192436426877975,\n      0.0013418952003121376,\n      0.00032708392245694995,\n      5.242130640681353e-08,\n      -0.0003271230962127447,\n      -1.361901809104893e-07,\n      0.0016038764733821154,\n      0.006635135505348444,\n      0.016525285318493843,\n      0.031343650072813034,\n      0.045768022537231445,\n      0.04878775402903557,\n      0.04576811566948891,\n      0.031343746930360794,\n      0.016525371000170708,\n      0.006635169964283705,\n      0.0016038933536037803,\n      -1.308879120642814e-07,\n      -0.00032711916719563305,\n      -0.0013417622540146112,\n      -0.0016040721675381064,\n      -1.924819947873857e-08,\n      0.007583654019981623,\n      0.025714760646224022,\n      0.054716408252716064,\n      0.08485884219408035,\n      0.090099036693573,\n      0.08485902100801468,\n      0.054716646671295166,\n      0.025714926421642303,\n      0.0075837355107069016,\n      1.2846355978979318e-08,\n      -0.0016040605260059237,\n      -0.0013417588779702783,\n      -0.004018954001367092,\n      -0.006635183934122324,\n      -0.007583423051983118,\n      3.3686123401821533e-07,\n      0.026822539046406746,\n      0.07421347498893738,\n      0.1231953427195549,\n      0.12988121807575226,\n      0.12319565564393997,\n      0.07421407103538513,\n      0.026822824031114578,\n      4.446185641882039e-07,\n      -0.007583359256386757,\n      -0.006635183934122324,\n      -0.004018951673060656,\n      -0.008604466915130615,\n      -0.0165253896266222,\n      -0.025714563205838203,\n      -0.026821833103895187,\n      4.4509900476441544e-07,\n      0.06150265783071518,\n      0.10759247094392776,\n      0.09571222960948944,\n      0.10759340971708298,\n      0.0615030974149704,\n      6.499175242424826e-07,\n      -0.026821734383702278,\n      -0.025714563205838203,\n      -0.01652543619275093,\n      -0.008604481816291809,\n      -0.015011557377874851,\n      -0.031344130635261536,\n      -0.05471666529774666,\n      -0.07421354949474335,\n      -0.06150268390774727,\n      4.650003404549352e-07,\n      -0.02082240581512451,\n      -0.1409217268228531,\n      -0.020821960642933846,\n      4.317786306273774e-07,\n      -0.061502642929553986,\n      -0.07421379536390305,\n      -0.05471675470471382,\n      -0.0313442125916481,\n      -0.015011562034487724,\n      -0.0210383590310812,\n      -0.04576844722032547,\n      -0.08485910296440125,\n      -0.12319585680961609,\n      -0.10759326815605164,\n      0.020822638645768166,\n      1.2661884056797135e-06,\n      -0.29759618639945984,\n      4.2579583237056795e-07,\n      0.020822452381253242,\n      -0.10759385675191879,\n      -0.12319619953632355,\n      -0.0848592072725296,\n      -0.04576842859387398,\n      -0.021038368344306946,\n      -0.02269311621785164,\n      -0.04878754913806915,\n      -0.0900988057255745,\n      -0.12988132238388062,\n      -0.0957135260105133,\n      0.14092113077640533,\n      0.29759499430656433,\n      8.372729354277908e-08,\n      0.2975955903530121,\n      0.14092102646827698,\n      -0.095713771879673,\n      -0.12988139688968658,\n      -0.09009873121976852,\n      -0.048787545412778854,\n      -0.022693101316690445,\n      -0.021038368344306946,\n      -0.04576842486858368,\n      -0.08485922962427139,\n      -0.1231960877776146,\n      -0.10759394615888596,\n      0.02082275040447712,\n      -4.298336975239181e-08,\n      -0.2975960671901703,\n      8.069438308666577e-07,\n      0.020822813734412193,\n      -0.10759362578392029,\n      -0.12319593131542206,\n      -0.08485911786556244,\n      -0.04576843976974487,\n      -0.02103835716843605,\n      -0.015011563897132874,\n      -0.0313442088663578,\n      -0.05471673607826233,\n      -0.07421376556158066,\n      -0.061502620577812195,\n      2.3038744245695852e-07,\n      -0.02082223631441593,\n      -0.1409212201833725,\n      -0.02082245610654354,\n      3.6793565527659666e-07,\n      -0.06150263547897339,\n      -0.07421357184648514,\n      -0.05471663549542427,\n      -0.031344134360551834,\n      -0.015011566691100597,\n      -0.008604485541582108,\n      -0.016525432467460632,\n      -0.025714585557579994,\n      -0.02682163566350937,\n      6.691942644465598e-07,\n      0.06150335073471069,\n      0.10759323090314865,\n      0.09571252018213272,\n      0.10759242624044418,\n      0.06150241941213608,\n      3.6075863363294047e-07,\n      -0.026821836829185486,\n      -0.025714566931128502,\n      -0.016525395214557648,\n      -0.00860446598380804,\n      -0.004018949810415506,\n      -0.0066351802088320255,\n      -0.0075833676382899284,\n      4.666668189656775e-07,\n      0.02682286500930786,\n      0.07421398162841797,\n      0.1231955960392952,\n      0.12988123297691345,\n      0.12319530546665192,\n      0.07421346008777618,\n      0.026822492480278015,\n      3.548265965491737e-07,\n      -0.007583414204418659,\n      -0.006635183468461037,\n      -0.004018950741738081,\n      -0.0013417643494904041,\n      -0.001604056335054338,\n      1.1151483292337616e-08,\n      0.007583740632981062,\n      0.025714941322803497,\n      0.054716676473617554,\n      0.08485900610685349,\n      0.09009899199008942,\n      0.08485881984233856,\n      0.05471641942858696,\n      0.025714758783578873,\n      0.007583653088659048,\n      -2.2750068495724918e-08,\n      -0.0016040671616792679,\n      -0.0013417648151516914,\n      -0.000327121902955696,\n      -1.374076674665048e-07,\n      0.0016038959147408605,\n      0.006635171361267567,\n      0.01652536168694496,\n      0.0313437357544899,\n      0.04576811194419861,\n      0.048787832260131836,\n      0.04576800763607025,\n      0.031343650072813034,\n      0.016525279730558395,\n      0.006635134574025869,\n      0.0016038757748901844,\n      -1.4425009453589155e-07,\n      -0.00032712522079236805,\n      5.231037647490666e-08,\n      0.00032708284561522305,\n      0.001341895549558103,\n      0.004019242245703936,\n      0.008604662492871284,\n      0.015011323615908623,\n      0.021038135513663292,\n      0.022693663835525513,\n      0.021038126200437546,\n      0.015011264011263847,\n      0.008604616858065128,\n      0.004019223153591156,\n      0.001341881463304162,\n      0.0003270788583904505,\n      5.273190240018266e-08],\n     [1.3253848019090242e-09,\n      -0.00010008723620558158,\n      -0.0006708684959448874,\n      -0.003617051988840103,\n      -0.010159855708479881,\n      -0.008945534005761147,\n      -0.0032064998522400856,\n      5.507429250428686e-07,\n      0.0032061890233308077,\n      0.008945845998823643,\n      0.010159569792449474,\n      0.003617228241637349,\n      0.0006709573208354414,\n      0.0001001430646283552,\n      1.2578001973295727e-09,\n      0.00010014238068833947,\n      6.099361371525447e-08,\n      -0.0007405743235722184,\n      -0.004809883888810873,\n      -0.019387587904930115,\n      -0.026821477338671684,\n      -0.011723972856998444,\n      3.763403810808086e-07,\n      0.011723521165549755,\n      0.026822015643119812,\n      0.01938757672905922,\n      0.004810212180018425,\n      0.0007407670491375029,\n      5.99790510591447e-08,\n      -0.00010008564277086407,\n      0.000670960231218487,\n      0.0007407666998915374,\n      2.083290127075088e-07,\n      -0.00477775651961565,\n      -0.02756843902170658,\n      -0.061322204768657684,\n      -0.03802096098661423,\n      2.3841003837787866e-07,\n      0.038020502775907516,\n      0.06132308021187782,\n      0.027568822726607323,\n      0.004778489004820585,\n      2.0328899097421527e-07,\n      -0.0007405726355500519,\n      -0.0006708648870699108,\n      0.003617234295234084,\n      0.00481022521853447,\n      0.004778505302965641,\n      6.727694312758103e-07,\n      -0.027145110070705414,\n      -0.10218176990747452,\n      -0.10018506646156311,\n      -2.9345693519644556e-07,\n      0.100184366106987,\n      0.10218323022127151,\n      0.0271463543176651,\n      6.652346655755537e-07,\n      -0.004777750000357628,\n      -0.0048098633997142315,\n      -0.0036170417442917824,\n      0.010159586556255817,\n      0.019387613981962204,\n      0.027568861842155457,\n      0.02714642696082592,\n      8.922801271182834e-07,\n      -0.1107553243637085,\n      -0.1935751885175705,\n      -1.4809364756729337e-06,\n      0.19357380270957947,\n      0.11075703054666519,\n      8.421120014645567e-07,\n      -0.027145039290189743,\n      -0.027568383142352104,\n      -0.01938754692673683,\n      -0.010159837082028389,\n      0.008945874869823456,\n      0.026822084560990334,\n      0.061323150992393494,\n      0.10218333452939987,\n      0.11075711995363235,\n      1.5752044646433205e-06,\n      -0.21585801243782043,\n      -1.1882445960509358e-06,\n      0.2158571481704712,\n      1.453843083254469e-06,\n      -0.11075516045093536,\n      -0.10218161344528198,\n      -0.06132202595472336,\n      -0.026821380481123924,\n      -0.008945493958890438,\n      0.003206237917765975,\n      0.011723638512194157,\n      0.03802068531513214,\n      0.10018458962440491,\n      0.19357390701770782,\n      0.21585723757743835,\n      -2.7992996365355793e-07,\n      1.0772312180051813e-06,\n      -4.94854930366273e-07,\n      -0.21585789322853088,\n      -0.19357480108737946,\n      -0.10018480569124222,\n      -0.038020722568035126,\n      -0.011723874136805534,\n      -0.0032064535189419985,\n      5.999108338983206e-07,\n      4.865184450864035e-07,\n      4.3023669604735915e-07,\n      8.303728549208245e-08,\n      -1.1377394457667833e-06,\n      -1.790937858459074e-06,\n      4.4793659981223755e-07,\n      -2.5442400897190964e-07,\n      6.094190325711679e-07,\n      -1.4468677136392216e-06,\n      -1.1444445817687665e-06,\n      3.251399505188601e-08,\n      4.205539596568997e-07,\n      4.806882998309447e-07,\n      6.020985097165976e-07,\n      -0.0032064516562968493,\n      -0.011723869480192661,\n      -0.03802075609564781,\n      -0.10018477588891983,\n      -0.19357480108737946,\n      -0.21585798263549805,\n      -4.711796819378833e-08,\n      1.4482243386737537e-06,\n      -9.79336718387458e-08,\n      0.2158571183681488,\n      0.1935739368200302,\n      0.10018463432788849,\n      0.038020651787519455,\n      0.011723627336323261,\n      0.003206241177394986,\n      -0.008945494890213013,\n      -0.026821395382285118,\n      -0.061322007328271866,\n      -0.10218161344528198,\n      -0.11075516045093536,\n      1.38820519168803e-06,\n      0.2158573418855667,\n      -1.2709498378171702e-06,\n      -0.21585796773433685,\n      1.6045296433730982e-06,\n      0.11075718700885773,\n      0.10218336433172226,\n      0.0613231398165226,\n      0.026822073385119438,\n      0.008945880457758904,\n      -0.010159836150705814,\n      -0.019387537613511086,\n      -0.027568381279706955,\n      -0.02714504301548004,\n      8.713619195077627e-07,\n      0.11075711995363235,\n      0.19357378780841827,\n      -1.4534521142195445e-06,\n      -0.19357523322105408,\n      -0.11075528711080551,\n      9.168066981146694e-07,\n      0.02714642509818077,\n      0.027568869292736053,\n      0.019387606531381607,\n      0.010159589350223541,\n      -0.0036170422099530697,\n      -0.004809863865375519,\n      -0.004777747206389904,\n      6.71909504035284e-07,\n      0.027146397158503532,\n      0.10218319296836853,\n      0.10018431395292282,\n      -2.5894675559356983e-07,\n      -0.10018514096736908,\n      -0.10218183696269989,\n      -0.027145108208060265,\n      6.700911399093457e-07,\n      0.004778503905981779,\n      0.0048102280125021935,\n      0.003617234528064728,\n      -0.0006708650034852326,\n      -0.0007405767682939768,\n      2.078256216009322e-07,\n      0.004778482485562563,\n      0.02756882831454277,\n      0.06132304295897484,\n      0.038020484149456024,\n      2.1880022416098655e-07,\n      -0.03802094981074333,\n      -0.061322178691625595,\n      -0.027568450197577477,\n      -0.0047777616418898106,\n      2.038237170154389e-07,\n      0.0007407697266899049,\n      0.0006709599983878434,\n      -0.00010008572280639783,\n      5.9144138475630825e-08,\n      0.0007407653611153364,\n      0.004810213577002287,\n      0.019387582316994667,\n      0.026822013780474663,\n      0.011723507195711136,\n      3.6985673546041653e-07,\n      -0.011723979376256466,\n      -0.026821482926607132,\n      -0.019387587904930115,\n      -0.004809880629181862,\n      -0.0007405764772556722,\n      6.356141568630846e-08,\n      0.00010014267172664404,\n      1.0552173579370105e-09,\n      0.00010014232975663617,\n      0.0006709576700814068,\n      0.0036172273103147745,\n      0.010159569792449474,\n      0.008945845998823643,\n      0.003206182038411498,\n      5.434420131678053e-07,\n      -0.00320649822242558,\n      -0.00894553679972887,\n      -0.01015985943377018,\n      -0.003617051290348172,\n      -0.0006708684959448874,\n      -0.00010008677054429427,\n      1.283019135378538e-09],\n     [-0.0016856914153322577,\n      0.004300857428461313,\n      0.004031796474009752,\n      0.00793745182454586,\n      -0.020424734801054,\n      0.02698400430381298,\n      -0.01454427931457758,\n      0.07053769379854202,\n      -0.014544321224093437,\n      0.02698386460542679,\n      -0.02042488008737564,\n      0.007937398739159107,\n      0.004031785763800144,\n      0.004300856962800026,\n      -0.0016856909496709704,\n      0.004300856962800026,\n      0.012609228491783142,\n      0.01569530740380287,\n      0.023244522511959076,\n      0.0024409492034465075,\n      0.046662136912345886,\n      -0.018830647692084312,\n      0.05259333550930023,\n      -0.018830815330147743,\n      0.04666175693273544,\n      0.0024406728334724903,\n      0.023244459182024002,\n      0.015695296227931976,\n      0.012609231285750866,\n      0.0043008578941226006,\n      0.00403178995475173,\n      0.015695301815867424,\n      0.02736622653901577,\n      0.04477749019861221,\n      0.026290787383913994,\n      0.06926127523183823,\n      -0.015542100183665752,\n      0.03897054120898247,\n      -0.01554261427372694,\n      0.06926038861274719,\n      0.02629040740430355,\n      0.04477742686867714,\n      0.027366233989596367,\n      0.01569531299173832,\n      0.004031802527606487,\n      0.007937404327094555,\n      0.023244474083185196,\n      0.04477744176983833,\n      0.08066029101610184,\n      0.07450882345438004,\n      0.09925948828458786,\n      -0.024624498561024666,\n      -0.008782543241977692,\n      -0.024625947698950768,\n      0.09925808012485504,\n      0.07450844347476959,\n      0.08066030591726303,\n      0.044777512550354004,\n      0.023244550451636314,\n      0.007937463000416756,\n      -0.02042486146092415,\n      0.00244070403277874,\n      0.02629046142101288,\n      0.07450848817825317,\n      0.10634501278400421,\n      0.1270095854997635,\n      -0.08043473213911057,\n      -0.13884484767913818,\n      -0.08043766766786575,\n      0.12700821459293365,\n      0.1063450425863266,\n      0.0745089128613472,\n      0.02629086747765541,\n      0.0024409962352365255,\n      -0.02042471244931221,\n      0.02698390930891037,\n      0.04666184261441231,\n      0.06926052272319794,\n      0.0992581769824028,\n      0.12700842320919037,\n      0.1986091583967209,\n      -0.04890034720301628,\n      -0.19762162864208221,\n      -0.04890385642647743,\n      0.1986090987920761,\n      0.12700989842414856,\n      0.09925972670316696,\n      0.06926143914461136,\n      0.04666224122047424,\n      0.026984049007296562,\n      -0.01454426534473896,\n      -0.018830688670277596,\n      -0.015542442910373211,\n      -0.024625612422823906,\n      -0.08043748140335083,\n      -0.04890364781022072,\n      -0.040910374373197556,\n      0.12329334020614624,\n      -0.04091046005487442,\n      -0.048900056630373,\n      -0.08043431490659714,\n      -0.02462414652109146,\n      -0.015541866421699524,\n      -0.01883050426840782,\n      -0.014544217847287655,\n      0.0705377608537674,\n      0.052593447268009186,\n      0.038970787078142166,\n      -0.00878224615007639,\n      -0.13884446024894714,\n      -0.19762197136878967,\n      0.12329192459583282,\n      -0.03162349760532379,\n      0.12329219281673431,\n      -0.1976221352815628,\n      -0.13884469866752625,\n      -0.008782126940786839,\n      0.03897080570459366,\n      0.05259346589446068,\n      0.0705377385020256,\n      -0.014544217847287655,\n      -0.018830515444278717,\n      -0.015541885048151016,\n      -0.02462419681251049,\n      -0.08043438196182251,\n      -0.048900291323661804,\n      -0.04091060161590576,\n      0.12329314649105072,\n      -0.04091070592403412,\n      -0.048903778195381165,\n      -0.08043736219406128,\n      -0.024625662714242935,\n      -0.015542468056082726,\n      -0.0188306774944067,\n      -0.014544262550771236,\n      0.026984047144651413,\n      0.04666224494576454,\n      0.06926143169403076,\n      0.09925975650548935,\n      0.12700988352298737,\n      0.19860908389091492,\n      -0.048904091119766235,\n      -0.197621688246727,\n      -0.048900339752435684,\n      0.19860905408859253,\n      0.12700846791267395,\n      0.09925823658704758,\n      0.06926051527261734,\n      0.046661846339702606,\n      0.02698390744626522,\n      -0.02042470872402191,\n      0.0024409994948655367,\n      0.02629087306559086,\n      0.07450892776250839,\n      0.1063450425863266,\n      0.12700827419757843,\n      -0.08043773472309113,\n      -0.1388448029756546,\n      -0.08043467253446579,\n      0.12700963020324707,\n      0.1063450500369072,\n      0.07450847327709198,\n      0.026290448382496834,\n      0.002440700540319085,\n      -0.020424865186214447,\n      0.007937461137771606,\n      0.023244546726346016,\n      0.0447775162756443,\n      0.08066029846668243,\n      0.074508436024189,\n      0.09925804287195206,\n      -0.024625957012176514,\n      -0.008782577700912952,\n      -0.024624567478895187,\n      0.09925947338342667,\n      0.07450879365205765,\n      0.08066029101610184,\n      0.04477744922041893,\n      0.023244470357894897,\n      0.007937412708997726,\n      0.004031799733638763,\n      0.01569531299173832,\n      0.027366233989596367,\n      0.04477742686867714,\n      0.02629038877785206,\n      0.06926039606332779,\n      -0.015542645007371902,\n      0.038970548659563065,\n      -0.015542104840278625,\n      0.06926129758358002,\n      0.026290778070688248,\n      0.044777486473321915,\n      0.02736623026430607,\n      0.015695298090577126,\n      0.004031790420413017,\n      0.0043008592911064625,\n      0.01260922197252512,\n      0.015695296227931976,\n      0.02324446104466915,\n      0.0024406646843999624,\n      0.04666176065802574,\n      -0.018830807879567146,\n      0.052593328058719635,\n      -0.018830643966794014,\n      0.04666214436292648,\n      0.0024409526959061623,\n      0.023244528099894524,\n      0.01569530740380287,\n      0.012609225697815418,\n      0.004300856497138739,\n      -0.0016856908332556486,\n      0.004300856031477451,\n      0.004031785763800144,\n      0.007937402464449406,\n      -0.02042488195002079,\n      0.02698386274278164,\n      -0.014544324018061161,\n      0.07053770124912262,\n      -0.014544284902513027,\n      0.02698400802910328,\n      -0.02042473666369915,\n      0.007937449961900711,\n      0.004031797405332327,\n      0.004300857428461313,\n      -0.0016856922302395105],\n     [0.019821863621473312,\n      -0.004529736004769802,\n      0.018569931387901306,\n      0.03503548353910446,\n      0.08314484357833862,\n      -0.0444437637925148,\n      0.032572630792856216,\n      -0.054199252277612686,\n      0.032572630792856216,\n      -0.0444437637925148,\n      0.08314484357833862,\n      0.03503548353910446,\n      0.018569931387901306,\n      -0.004529736004769802,\n      0.019821863621473312,\n      -0.004529736004769802,\n      -0.03978470340371132,\n      -0.024260487407445908,\n      -0.009309295564889908,\n      0.03950991481542587,\n      -0.06959977746009827,\n      0.055790845304727554,\n      -0.01890655979514122,\n      0.055790845304727554,\n      -0.06959978491067886,\n      0.039509911090135574,\n      -0.009309293702244759,\n      -0.024260487407445908,\n      -0.03978470340371132,\n      -0.004529735539108515,\n      0.018569931387901306,\n      -0.024260487407445908,\n      -0.005061911419034004,\n      0.0243797916918993,\n      0.08074557781219482,\n      -0.026447463780641556,\n      0.10488977283239365,\n      0.03135961666703224,\n      0.10488977283239365,\n      -0.02644747868180275,\n      0.08074558526277542,\n      0.02437978982925415,\n      -0.00506191048771143,\n      -0.02426048554480076,\n      0.018569931387901306,\n      0.03503547981381416,\n      -0.009309292770922184,\n      0.02437978982925415,\n      0.06501778215169907,\n      0.11978775262832642,\n      -0.006251721642911434,\n      0.10861879587173462,\n      0.029965706169605255,\n      0.108618825674057,\n      -0.0062517146579921246,\n      0.11978779733181,\n      0.06501780450344086,\n      0.02437979355454445,\n      -0.00930928997695446,\n      0.03503548353910446,\n      0.08314484357833862,\n      0.039509911090135574,\n      0.08074558526277542,\n      0.1197877824306488,\n      0.14500337839126587,\n      -0.002813167404383421,\n      0.09176062047481537,\n      0.00651169940829277,\n      0.0917605459690094,\n      -0.002813151804730296,\n      0.14500342309474945,\n      0.1197877749800682,\n      0.08074560016393661,\n      0.03950991854071617,\n      0.08314485102891922,\n      -0.0444437637925148,\n      -0.06959977746009827,\n      -0.026447471231222153,\n      -0.006251724436879158,\n      -0.002813153900206089,\n      -0.17572978138923645,\n      -0.0774039775133133,\n      -0.14122775197029114,\n      -0.07740404456853867,\n      -0.17572978138923645,\n      -0.002813096158206463,\n      -0.006251683924347162,\n      -0.026447458192706108,\n      -0.06959977000951767,\n      -0.0444437600672245,\n      0.032572634518146515,\n      0.05579085275530815,\n      0.10488978773355484,\n      0.10861886292695999,\n      0.09176050126552582,\n      -0.07740403711795807,\n      0.04249430075287819,\n      0.05036263167858124,\n      0.042494334280490875,\n      -0.0774039626121521,\n      0.09176070243120193,\n      0.1086188554763794,\n      0.10488979518413544,\n      0.05579084903001785,\n      0.032572634518146515,\n      -0.05419924110174179,\n      -0.018906541168689728,\n      0.03135964646935463,\n      0.029965749010443687,\n      0.0065118237398564816,\n      -0.14122772216796875,\n      0.05036222189664841,\n      -0.013516925275325775,\n      0.05036244913935661,\n      -0.14122773706912994,\n      0.006511738523840904,\n      0.02996574528515339,\n      0.03135966137051582,\n      -0.01890653371810913,\n      -0.05419924855232239,\n      0.03257263824343681,\n      0.05579085275530815,\n      0.10488980263471603,\n      0.1086188405752182,\n      0.09176063537597656,\n      -0.07740401476621628,\n      0.04249425604939461,\n      0.05036252364516258,\n      0.04249424859881401,\n      -0.07740401476621628,\n      0.09176052361726761,\n      0.10861887782812119,\n      0.10488978028297424,\n      0.05579085275530815,\n      0.032572634518146515,\n      -0.0444437600672245,\n      -0.06959977000951767,\n      -0.026447443291544914,\n      -0.006251680664718151,\n      -0.002813107566908002,\n      -0.17572976648807526,\n      -0.07740408927202225,\n      -0.14122778177261353,\n      -0.07740399241447449,\n      -0.17572976648807526,\n      -0.002813127823174,\n      -0.006251716986298561,\n      -0.026447463780641556,\n      -0.06959977746009827,\n      -0.0444437600672245,\n      0.08314485102891922,\n      0.03950992226600647,\n      0.08074560016393661,\n      0.1197877824306488,\n      0.14500342309474945,\n      -0.0028131171129643917,\n      0.09176050126552582,\n      0.006511724554002285,\n      0.09176057577133179,\n      -0.0028131285216659307,\n      0.14500342309474945,\n      0.11978776752948761,\n      0.08074557781219482,\n      0.039509907364845276,\n      0.08314484357833862,\n      0.03503548353910446,\n      -0.009309290908277035,\n      0.024379797279834747,\n      0.06501779705286026,\n      0.1197877898812294,\n      -0.006251731421798468,\n      0.10861881077289581,\n      0.02996569499373436,\n      0.10861878842115402,\n      -0.006251712795346975,\n      0.11978775262832642,\n      0.06501779705286026,\n      0.024379786103963852,\n      -0.009309293702244759,\n      0.03503548353910446,\n      0.018569931387901306,\n      -0.02426048554480076,\n      -0.0050619118846952915,\n      0.02437978982925415,\n      0.08074558526277542,\n      -0.026447473093867302,\n      0.10488977283239365,\n      0.03135961666703224,\n      0.10488976538181305,\n      -0.026447469368577003,\n      0.08074557781219482,\n      0.024379787966609,\n      -0.005061910953372717,\n      -0.02426048554480076,\n      0.018569931387901306,\n      -0.0045297350734472275,\n      -0.03978470340371132,\n      -0.024260487407445908,\n      -0.009309293702244759,\n      0.039509911090135574,\n      -0.06959978491067886,\n      0.05579085275530815,\n      -0.018906552344560623,\n      0.05579084903001785,\n      -0.06959977000951767,\n      0.03950991481542587,\n      -0.009309294633567333,\n      -0.024260489270091057,\n      -0.03978470340371132,\n      -0.004529736004769802,\n      0.019821863621473312,\n      -0.004529736004769802,\n      0.018569931387901306,\n      0.03503548726439476,\n      0.08314484357833862,\n      -0.0444437637925148,\n      0.032572630792856216,\n      -0.05419924855232239,\n      0.032572630792856216,\n      -0.0444437637925148,\n      0.08314484357833862,\n      0.03503548353910446,\n      0.018569931387901306,\n      -0.0045297350734472275,\n      0.019821863621473312],\n     [-0.00045146088814362884,\n      -0.0007810047827661037,\n      -0.0010759624419733882,\n      -0.001902921125292778,\n      -0.0034958464093506336,\n      -0.003166845766827464,\n      -0.0014316432643681765,\n      -2.013494260211246e-08,\n      0.001431652926839888,\n      0.003166850423440337,\n      0.0034958506003022194,\n      0.001902912394143641,\n      0.0010759582510218024,\n      0.0007810070528648794,\n      0.00045146141201257706,\n      -0.0007810047827661037,\n      -0.0020626760087907314,\n      -0.0033838520757853985,\n      -0.004685735329985619,\n      -0.008072400465607643,\n      -0.009446810930967331,\n      -0.004797111265361309,\n      -5.098757416277522e-09,\n      0.004797141067683697,\n      0.009446833282709122,\n      0.00807241816073656,\n      0.004685733001679182,\n      0.0033838548697531223,\n      0.002062681131064892,\n      0.000781006645411253,\n      -0.0010759624419733882,\n      -0.0033838534727692604,\n      -0.00814371183514595,\n      -0.012640673667192459,\n      -0.017783796414732933,\n      -0.023729242384433746,\n      -0.014763019047677517,\n      2.5464741426617366e-09,\n      0.014763053506612778,\n      0.02372925542294979,\n      0.017783796414732933,\n      0.012640666216611862,\n      0.008143709972500801,\n      0.0033838546369224787,\n      0.0010759583674371243,\n      -0.0019029221730306745,\n      -0.004685737192630768,\n      -0.012640678323805332,\n      -0.027485469356179237,\n      -0.04095461592078209,\n      -0.052629344165325165,\n      -0.040185462683439255,\n      1.8905470966501525e-08,\n      0.040185511112213135,\n      0.052629344165325165,\n      0.040954601019620895,\n      0.027485450729727745,\n      0.012640665285289288,\n      0.004685731139034033,\n      0.001902910997159779,\n      -0.003495849436149001,\n      -0.008072414435446262,\n      -0.01778380386531353,\n      -0.040954627096652985,\n      -0.07958409935235977,\n      -0.11050509661436081,\n      -0.09574064612388611,\n      8.010911045630564e-08,\n      0.09574069827795029,\n      0.1105051040649414,\n      0.07958407700061798,\n      0.04095458984375,\n      0.017783788964152336,\n      0.008072406984865665,\n      0.0034958466421812773,\n      -0.0031668490264564753,\n      -0.009446827694773674,\n      -0.023729268461465836,\n      -0.052629366517066956,\n      -0.1105051189661026,\n      -0.19115696847438812,\n      -0.19846485555171967,\n      8.297711673321828e-08,\n      0.19846491515636444,\n      0.19115698337554932,\n      0.11050505191087723,\n      0.05262930691242218,\n      0.02372923120856285,\n      0.009446818381547928,\n      0.003166846465319395,\n      -0.0014316487358883023,\n      -0.0047971284948289394,\n      -0.014763055369257927,\n      -0.040185533463954926,\n      -0.09574073553085327,\n      -0.19846495985984802,\n      -0.26390910148620605,\n      1.1263771426683888e-07,\n      0.2639091908931732,\n      0.19846487045288086,\n      0.09574062377214432,\n      0.04018542543053627,\n      0.014763014391064644,\n      0.004797118715941906,\n      0.0014316446613520384,\n      -3.253846614370559e-08,\n      -3.861439168417746e-08,\n      -6.384954787108654e-08,\n      -1.0774846259664628e-07,\n      -1.629337305075751e-07,\n      -1.704345748976266e-07,\n      7.88160825493378e-09,\n      -1.1046608960896265e-08,\n      -1.700234548707158e-07,\n      -9.809913592562225e-08,\n      -1.9141745610795624e-07,\n      -1.0379150694461714e-07,\n      -5.6049376695455067e-08,\n      -3.877086740544655e-08,\n      -3.308684526359684e-08,\n      0.0014316446613520384,\n      0.004797121975570917,\n      0.014763019047677517,\n      0.04018542170524597,\n      0.09574063122272491,\n      0.19846488535404205,\n      0.2639091908931732,\n      9.578682380606551e-08,\n      -0.26390907168388367,\n      -0.1984649896621704,\n      -0.09574071317911148,\n      -0.04018552973866463,\n      -0.014763051643967628,\n      -0.004797130823135376,\n      -0.0014316483866423368,\n      0.0031668469309806824,\n      0.009446818381547928,\n      0.0237292293459177,\n      0.05262930318713188,\n      0.11050505936145782,\n      0.19115698337554932,\n      0.19846493005752563,\n      2.2141638567063637e-07,\n      -0.19846487045288086,\n      -0.19115696847438812,\n      -0.1105051264166832,\n      -0.05262937396764755,\n      -0.023729266598820686,\n      -0.0094468267634511,\n      -0.00316684995777905,\n      0.0034958473406732082,\n      0.00807240605354309,\n      0.017783790826797485,\n      0.04095458984375,\n      0.07958407700061798,\n      0.11050508916378021,\n      0.09574071317911148,\n      8.107257798428691e-08,\n      -0.0957406610250473,\n      -0.11050508916378021,\n      -0.07958408445119858,\n      -0.040954627096652985,\n      -0.01778380572795868,\n      -0.008072412572801113,\n      -0.003495850134640932,\n      0.001902911113575101,\n      0.0046857306733727455,\n      0.012640664353966713,\n      0.027485447004437447,\n      0.040954604744911194,\n      0.05262934789061546,\n      0.04018549993634224,\n      6.561189369591602e-09,\n      -0.04018547013401985,\n      -0.05262935906648636,\n      -0.04095461219549179,\n      -0.027485469356179237,\n      -0.012640679255127907,\n      -0.004685737192630768,\n      -0.0019029219402000308,\n      0.0010759583674371243,\n      0.0033838539384305477,\n      0.008143709972500801,\n      0.012640665285289288,\n      0.017783796414732933,\n      0.023729251697659492,\n      0.014763053506612778,\n      1.173008246091456e-09,\n      -0.014763018116354942,\n      -0.023729242384433746,\n      -0.017783796414732933,\n      -0.012640675529837608,\n      -0.008143710903823376,\n      -0.003383853705599904,\n      -0.0010759623255580664,\n      0.0007810067618265748,\n      0.002062681131064892,\n      0.0033838548697531223,\n      0.004685733467340469,\n      0.008072417229413986,\n      0.009446831420063972,\n      0.004797142464667559,\n      -7.070287910693196e-09,\n      -0.0047971089370548725,\n      -0.009446813724935055,\n      -0.008072400465607643,\n      -0.004685735795646906,\n      -0.0033838527742773294,\n      -0.0020626753102988005,\n      -0.0007810050155967474,\n      0.0004514615866355598,\n      0.0007810071110725403,\n      0.0010759582510218024,\n      0.0019029119284823537,\n      0.0034958499018102884,\n      0.003166851121932268,\n      0.001431652926839888,\n      -2.1080293066688682e-08,\n      -0.0014316417509689927,\n      -0.0031668462324887514,\n      -0.0034958466421812773,\n      -0.001902921823784709,\n      -0.0010759622091427445,\n      -0.0007810048991814256,\n      -0.0004514609172474593],\n     [0.0001720193977234885,\n      -0.007465417962521315,\n      0.012030628509819508,\n      0.03202327340841293,\n      -0.006051389966160059,\n      -0.01112467423081398,\n      -0.024710148572921753,\n      0.16366539895534515,\n      -0.024710148572921753,\n      -0.011124672368168831,\n      -0.006051389500498772,\n      0.03202327713370323,\n      0.012030629441142082,\n      -0.007465417962521315,\n      0.00017201952869072556,\n      -0.007465417962521315,\n      -0.02529807575047016,\n      -0.013269967399537563,\n      0.006218539085239172,\n      -0.02525206468999386,\n      -0.03355620801448822,\n      -0.05472126975655556,\n      0.1309676170349121,\n      -0.05472126975655556,\n      -0.03355620428919792,\n      -0.02525206282734871,\n      0.006218540016561747,\n      -0.013269967399537563,\n      -0.02529807575047016,\n      -0.007465417496860027,\n      0.012030628509819508,\n      -0.013269967399537563,\n      -0.002977611729875207,\n      0.02994649112224579,\n      0.009154504165053368,\n      0.008419334888458252,\n      -0.01250527799129486,\n      0.1808798760175705,\n      -0.012505270540714264,\n      0.008419342339038849,\n      0.009154511615633965,\n      0.029946496710181236,\n      -0.0029776094015687704,\n      -0.013269966468214989,\n      0.012030628509819508,\n      0.03202327713370323,\n      0.006218539085239172,\n      0.029946492984890938,\n      0.06339194625616074,\n      0.05246751382946968,\n      0.04810662567615509,\n      0.026925180107355118,\n      0.2209254801273346,\n      0.026925206184387207,\n      0.048106659203767776,\n      0.05246752128005028,\n      0.06339195370674133,\n      0.029946496710181236,\n      0.006218541879206896,\n      0.03202327713370323,\n      -0.006051388569176197,\n      -0.02525206282734871,\n      0.009154509752988815,\n      0.05246751010417938,\n      0.024875838309526443,\n      0.016924895346164703,\n      -0.005886283703148365,\n      0.18731629848480225,\n      -0.005886218510568142,\n      0.016924945637583733,\n      0.024875856935977936,\n      0.05246753618121147,\n      0.00915451068431139,\n      -0.02525206096470356,\n      -0.0060513876378536224,\n      -0.011124671436846256,\n      -0.03355620056390762,\n      0.008419345133006573,\n      0.04810665175318718,\n      0.016924921423196793,\n      -0.003936604596674442,\n      -0.03634336218237877,\n      0.15299972891807556,\n      -0.0363432876765728,\n      -0.003936551045626402,\n      0.016924940049648285,\n      0.048106662929058075,\n      0.00841935072094202,\n      -0.03355620056390762,\n      -0.011124671436846256,\n      -0.024710146710276604,\n      -0.05472126603126526,\n      -0.012505261227488518,\n      0.026925209909677505,\n      -0.005886204075068235,\n      -0.03634333610534668,\n      -0.15422511100769043,\n      -0.07320302724838257,\n      -0.15422500669956207,\n      -0.03634326905012131,\n      -0.0058862012811005116,\n      0.026925217360258102,\n      -0.012505259364843369,\n      -0.05472126603126526,\n      -0.024710144847631454,\n      0.16366539895534515,\n      0.1309676170349121,\n      0.18087990581989288,\n      0.22092552483081818,\n      0.18731634318828583,\n      0.1529998481273651,\n      -0.07320293039083481,\n      0.03847537934780121,\n      -0.07320297509431839,\n      0.1529998481273651,\n      0.18731637299060822,\n      0.22092550992965698,\n      0.1808798909187317,\n      0.1309676170349121,\n      0.16366539895534515,\n      -0.024710144847631454,\n      -0.05472126603126526,\n      -0.012505260296165943,\n      0.0269252210855484,\n      -0.005886191036552191,\n      -0.036343250423669815,\n      -0.15422499179840088,\n      -0.07320302724838257,\n      -0.15422511100769043,\n      -0.03634334355592728,\n      -0.005886194296181202,\n      0.026925208047032356,\n      -0.012505260296165943,\n      -0.05472126603126526,\n      -0.024710146710276604,\n      -0.011124671436846256,\n      -0.03355620056390762,\n      0.00841935072094202,\n      0.048106662929058075,\n      0.016924945637583733,\n      -0.003936550579965115,\n      -0.036343298852443695,\n      0.15299974381923676,\n      -0.03634336218237877,\n      -0.003936606924980879,\n      0.01692492701113224,\n      0.04810664802789688,\n      0.008419346064329147,\n      -0.03355620056390762,\n      -0.011124671436846256,\n      -0.006051388569176197,\n      -0.02525206096470356,\n      0.00915451068431139,\n      0.05246753245592117,\n      0.024875856935977936,\n      0.016924945637583733,\n      -0.005886207800358534,\n      0.18731629848480225,\n      -0.005886274855583906,\n      0.016924887895584106,\n      0.024875832721590996,\n      0.05246751382946968,\n      0.009154509752988815,\n      -0.02525206282734871,\n      -0.006051388569176197,\n      0.03202327713370323,\n      0.0062185414135456085,\n      0.029946496710181236,\n      0.06339195370674133,\n      0.05246752128005028,\n      0.048106659203767776,\n      0.026925204321742058,\n      0.2209254652261734,\n      0.026925181970000267,\n      0.04810662567615509,\n      0.05246751755475998,\n      0.06339193880558014,\n      0.029946494847536087,\n      0.006218539085239172,\n      0.03202327340841293,\n      0.012030629441142082,\n      -0.013269965536892414,\n      -0.0029776094015687704,\n      0.029946496710181236,\n      0.00915451254695654,\n      0.008419346064329147,\n      -0.01250526960939169,\n      0.1808798760175705,\n      -0.012505274266004562,\n      0.008419334888458252,\n      0.009154503233730793,\n      0.02994649112224579,\n      -0.0029776114970445633,\n      -0.013269967399537563,\n      0.012030629441142082,\n      -0.007465417496860027,\n      -0.02529807575047016,\n      -0.013269966468214989,\n      0.006218539550900459,\n      -0.02525206282734871,\n      -0.03355620428919792,\n      -0.05472126975655556,\n      0.1309676170349121,\n      -0.054721273481845856,\n      -0.03355620428919792,\n      -0.02525206468999386,\n      0.006218539085239172,\n      -0.013269967399537563,\n      -0.02529807575047016,\n      -0.007465417962521315,\n      0.00017201951413881034,\n      -0.007465417496860027,\n      0.012030629441142082,\n      0.03202327713370323,\n      -0.006051389034837484,\n      -0.011124672368168831,\n      -0.024710148572921753,\n      0.16366539895534515,\n      -0.024710148572921753,\n      -0.01112467423081398,\n      -0.006051389966160059,\n      0.03202327713370323,\n      0.012030628509819508,\n      -0.007465417962521315,\n      0.0001720194995868951],\n     [-7.150240399766972e-10,\n      -5.12256046931725e-05,\n      -0.0002653774281498045,\n      -0.0010907184332609177,\n      -0.0028760540299117565,\n      -0.0040710787288844585,\n      -0.006553263403475285,\n      -0.009860539808869362,\n      -0.006553262937813997,\n      -0.004071075469255447,\n      -0.0028760514687746763,\n      -0.0010907164542004466,\n      -0.0002653765259310603,\n      -5.122521542944014e-05,\n      -4.4370990215369943e-10,\n      5.122907168697566e-05,\n      7.2315904375841455e-09,\n      -0.00031582461087964475,\n      -0.0016488498076796532,\n      -0.0056711710058152676,\n      -0.010280588641762733,\n      -0.015908587723970413,\n      -0.023182153701782227,\n      -0.015908578410744667,\n      -0.010280583053827286,\n      -0.005671167280524969,\n      -0.0016488461988046765,\n      -0.0003158218169119209,\n      8.145100593992538e-09,\n      5.1229551900178194e-05,\n      0.00026537166559137404,\n      0.00031583014060743153,\n      -1.3209321414464625e-09,\n      -0.001825270359404385,\n      -0.008736914023756981,\n      -0.021745285019278526,\n      -0.03576071187853813,\n      -0.05028252303600311,\n      -0.035760704427957535,\n      -0.021745268255472183,\n      -0.008736900053918362,\n      -0.0018252608133479953,\n      6.189771362130614e-09,\n      0.00031583241070620716,\n      0.0002653722476679832,\n      0.001090701320208609,\n      0.001648845849558711,\n      0.0018252483569085598,\n      -2.6131177222055157e-08,\n      -0.009414266794919968,\n      -0.036150820553302765,\n      -0.07229059189558029,\n      -0.10040708631277084,\n      -0.0722905769944191,\n      -0.03615079075098038,\n      -0.009414230473339558,\n      -4.825346788805973e-09,\n      0.0018252595327794552,\n      0.0016488489927724004,\n      0.0010907029500231147,\n      0.0028760533314198256,\n      0.005671182181686163,\n      0.008736895397305489,\n      0.009414218366146088,\n      -4.0412153623492486e-08,\n      -0.040932368487119675,\n      -0.12249457836151123,\n      -0.18106228113174438,\n      -0.12249448150396347,\n      -0.040932297706604004,\n      3.099443190990314e-08,\n      0.0094142509624362,\n      0.008736911229789257,\n      0.005671186372637749,\n      0.0028760568238794804,\n      0.004071084316819906,\n      0.010280606336891651,\n      0.021745264530181885,\n      0.03615079075098038,\n      0.04093228280544281,\n      -5.048662998774489e-08,\n      -0.14337705075740814,\n      -0.26843392848968506,\n      -0.14337687194347382,\n      9.386075561224061e-08,\n      0.04093239828944206,\n      0.03615081310272217,\n      0.021745286881923676,\n      0.010280612856149673,\n      0.004071088507771492,\n      0.006553278770297766,\n      0.015908611938357353,\n      0.03576071932911873,\n      0.0722905695438385,\n      0.12249450385570526,\n      0.1433769017457962,\n      -1.3513435703771393e-07,\n      -0.19902093708515167,\n      2.381788988259359e-07,\n      0.14337708055973053,\n      0.12249460071325302,\n      0.07229059934616089,\n      0.035760726779699326,\n      0.0159086175262928,\n      0.006553277838975191,\n      0.00986047089099884,\n      0.0231820959597826,\n      0.05028245225548744,\n      0.10040701180696487,\n      0.18106219172477722,\n      0.26843389868736267,\n      0.1990208774805069,\n      3.794657388311862e-08,\n      0.1990208476781845,\n      0.2684338688850403,\n      0.18106217682361603,\n      0.10040701925754547,\n      0.05028245598077774,\n      0.0231820959597826,\n      0.009860469959676266,\n      0.006553277838975191,\n      0.01590861938893795,\n      0.035760726779699326,\n      0.07229060679674149,\n      0.12249459326267242,\n      0.14337711036205292,\n      1.701879739357537e-07,\n      -0.19902093708515167,\n      -1.0625962687527135e-07,\n      0.1433769166469574,\n      0.12249448150396347,\n      0.0722905695438385,\n      0.03576071932911873,\n      0.015908611938357353,\n      0.006553278304636478,\n      0.004071088042110205,\n      0.010280612856149673,\n      0.021745288744568825,\n      0.03615081310272217,\n      0.04093238711357117,\n      6.97350657219431e-08,\n      -0.1433769166469574,\n      -0.26843389868736267,\n      -0.14337703585624695,\n      -6.268393804020889e-08,\n      0.04093227908015251,\n      0.036150794476270676,\n      0.021745266392827034,\n      0.010280607268214226,\n      0.004071083851158619,\n      0.0028760561253875494,\n      0.005671185441315174,\n      0.008736911229789257,\n      0.009414262138307095,\n      3.707896567561875e-08,\n      -0.04093227908015251,\n      -0.12249448895454407,\n      -0.1810622662305832,\n      -0.12249458581209183,\n      -0.040932390838861465,\n      -4.719005985975855e-08,\n      0.009414222091436386,\n      0.008736898191273212,\n      0.005671182181686163,\n      0.002876053797081113,\n      0.0010907029500231147,\n      0.0016488488763570786,\n      0.0018252587178722024,\n      -1.2408987171141916e-09,\n      -0.009414230473339558,\n      -0.036150794476270676,\n      -0.0722905695438385,\n      -0.10040708631277084,\n      -0.07229059934616089,\n      -0.036150816828012466,\n      -0.009414270520210266,\n      -2.5519632629311673e-08,\n      0.0018252487061545253,\n      0.0016488460823893547,\n      0.0010907019022852182,\n      0.0002653716946952045,\n      0.00031583308009430766,\n      6.164498245198047e-09,\n      -0.0018252615118399262,\n      -0.008736897259950638,\n      -0.021745262667536736,\n      -0.035760704427957535,\n      -0.05028252676129341,\n      -0.03576071932911873,\n      -0.02174527943134308,\n      -0.008736916817724705,\n      -0.0018252691952511668,\n      -1.1238899810805947e-09,\n      0.0003158301115036011,\n      0.0002653714327607304,\n      5.122969378135167e-05,\n      7.609355812121521e-09,\n      -0.0003158216713927686,\n      -0.0016488461988046765,\n      -0.005671168211847544,\n      -0.01028058398514986,\n      -0.015908578410744667,\n      -0.023182149976491928,\n      -0.015908589586615562,\n      -0.010280587710440159,\n      -0.005671171937137842,\n      -0.0016488493420183659,\n      -0.00031582399969920516,\n      6.708952060563433e-09,\n      5.122912989463657e-05,\n      -4.487734628355611e-10,\n      -5.122508082422428e-05,\n      -0.0002653768169693649,\n      -0.001090716803446412,\n      -0.002876051003113389,\n      -0.00407107500359416,\n      -0.006553263869136572,\n      -0.009860539808869362,\n      -0.006553263869136572,\n      -0.0040710787288844585,\n      -0.0028760540299117565,\n      -0.0010907177347689867,\n      -0.00026537803933024406,\n      -5.1225622883066535e-05,\n      -5.620129917005556e-10],\n     [0.0032727173529565334,\n      -0.0051739402115345,\n      0.005555334966629744,\n      0.015670644119381905,\n      -0.003220251062884927,\n      -0.019391393288969994,\n      -0.02338237129151821,\n      0.06399203091859818,\n      -0.02338237129151821,\n      -0.019391395151615143,\n      -0.003220250830054283,\n      0.015670644119381905,\n      0.005555334500968456,\n      -0.0051739402115345,\n      0.0032727173529565334,\n      -0.0051739402115345,\n      -0.021371910348534584,\n      -0.0166278425604105,\n      -0.0081947585567832,\n      -0.024266919121146202,\n      -0.04140797629952431,\n      -0.04601521044969559,\n      0.04024716094136238,\n      -0.04601521044969559,\n      -0.04140797629952431,\n      -0.02426692098379135,\n      -0.0081947585567832,\n      -0.0166278425604105,\n      -0.021371910348534584,\n      -0.0051739402115345,\n      0.005555334966629744,\n      -0.0166278425604105,\n      -0.014186307787895203,\n      -8.940850966610014e-05,\n      -0.015302857384085655,\n      -0.0342729426920414,\n      -0.04397919774055481,\n      0.04464908689260483,\n      -0.04397919028997421,\n      -0.034272946417331696,\n      -0.015302862040698528,\n      -8.94118202268146e-05,\n      -0.014186309650540352,\n      -0.0166278425604105,\n      0.005555334966629744,\n      0.015670644119381905,\n      -0.008194757625460625,\n      -8.940964471548796e-05,\n      0.008245466277003288,\n      -0.012721703387796879,\n      -0.04759246110916138,\n      -0.06694036722183228,\n      0.01871740072965622,\n      -0.06694038212299347,\n      -0.04759247601032257,\n      -0.0127217136323452,\n      0.008245464414358139,\n      -8.941075066104531e-05,\n      -0.0081947585567832,\n      0.015670644119381905,\n      -0.0032202505972236395,\n      -0.02426692098379135,\n      -0.01530285645276308,\n      -0.012721709907054901,\n      -0.05811602994799614,\n      -0.10436159372329712,\n      -0.12368221580982208,\n      -0.03514440357685089,\n      -0.12368225306272507,\n      -0.10436158627271652,\n      -0.058116037398576736,\n      -0.012721708975732327,\n      -0.015302857384085655,\n      -0.024266919121146202,\n      -0.003220251528546214,\n      -0.019391395151615143,\n      -0.04140797629952431,\n      -0.0342729426920414,\n      -0.047592468559741974,\n      -0.10436157137155533,\n      -0.14155295491218567,\n      -0.11076636612415314,\n      0.018799707293510437,\n      -0.1107664629817009,\n      -0.14155296981334686,\n      -0.10436159372329712,\n      -0.04759245365858078,\n      -0.0342729426920414,\n      -0.04140797629952431,\n      -0.019391395151615143,\n      -0.023382369428873062,\n      -0.04601520672440529,\n      -0.04397919401526451,\n      -0.06694035977125168,\n      -0.12368223816156387,\n      -0.11076637357473373,\n      0.060233376920223236,\n      0.2969141900539398,\n      0.06023338809609413,\n      -0.11076635867357254,\n      -0.12368220835924149,\n      -0.06694037467241287,\n      -0.04397919774055481,\n      -0.04601520672440529,\n      -0.023382369428873062,\n      0.06399203091859818,\n      0.04024716466665268,\n      0.04464909806847572,\n      0.018717411905527115,\n      -0.03514434024691582,\n      0.01879974640905857,\n      0.29691416025161743,\n      -0.24491675198078156,\n      0.2969141900539398,\n      0.018799826502799988,\n      -0.0351443774998188,\n      0.018717417493462563,\n      0.04464909806847572,\n      0.04024716839194298,\n      0.06399202346801758,\n      -0.023382369428873062,\n      -0.04601520672440529,\n      -0.04397919774055481,\n      -0.06694036722183228,\n      -0.12368223071098328,\n      -0.11076638102531433,\n      0.060233376920223236,\n      0.29691410064697266,\n      0.06023341044783592,\n      -0.11076638847589493,\n      -0.12368223071098328,\n      -0.06694036722183228,\n      -0.04397919401526451,\n      -0.04601520672440529,\n      -0.023382369428873062,\n      -0.019391395151615143,\n      -0.04140797629952431,\n      -0.0342729426920414,\n      -0.04759245738387108,\n      -0.10436160117387772,\n      -0.14155296981334686,\n      -0.11076648533344269,\n      0.01879969798028469,\n      -0.11076635122299194,\n      -0.14155296981334686,\n      -0.10436155647039413,\n      -0.047592464834451675,\n      -0.0342729426920414,\n      -0.04140797629952431,\n      -0.019391395151615143,\n      -0.0032202512957155704,\n      -0.024266919121146202,\n      -0.01530285831540823,\n      -0.012721704319119453,\n      -0.058116037398576736,\n      -0.10436157137155533,\n      -0.12368225306272507,\n      -0.0351443812251091,\n      -0.12368223816156387,\n      -0.10436160862445831,\n      -0.05811602249741554,\n      -0.012721708975732327,\n      -0.01530285831540823,\n      -0.02426692098379135,\n      -0.003220250830054283,\n      0.015670644119381905,\n      -0.0081947585567832,\n      -8.941073610913008e-05,\n      0.008245467208325863,\n      -0.012721710838377476,\n      -0.04759247228503227,\n      -0.06694038212299347,\n      0.018717383965849876,\n      -0.06694037467241287,\n      -0.04759246110916138,\n      -0.012721705250442028,\n      0.008245467208325863,\n      -8.940922998590395e-05,\n      -0.008194757625460625,\n      0.015670645982027054,\n      0.005555334500968456,\n      -0.0166278425604105,\n      -0.014186309650540352,\n      -8.941026317188516e-05,\n      -0.015302861109375954,\n      -0.034272946417331696,\n      -0.04397919401526451,\n      0.04464908316731453,\n      -0.04397920146584511,\n      -0.0342729426920414,\n      -0.01530285831540823,\n      -8.940886618802324e-05,\n      -0.014186308719217777,\n      -0.0166278425604105,\n      0.005555334966629744,\n      -0.0051739402115345,\n      -0.021371912211179733,\n      -0.0166278425604105,\n      -0.008194757625460625,\n      -0.0242669228464365,\n      -0.04140797629952431,\n      -0.04601521044969559,\n      0.04024716094136238,\n      -0.04601520672440529,\n      -0.04140797629952431,\n      -0.024266919121146202,\n      -0.008194757625460625,\n      -0.0166278425604105,\n      -0.021371910348534584,\n      -0.0051739402115345,\n      0.0032727178186178207,\n      -0.0051739402115345,\n      0.005555334500968456,\n      0.015670645982027054,\n      -0.0032202519942075014,\n      -0.019391395151615143,\n      -0.02338237129151821,\n      0.06399203091859818,\n      -0.02338237129151821,\n      -0.019391395151615143,\n      -0.003220251528546214,\n      0.015670644119381905,\n      0.005555334966629744,\n      -0.0051739402115345,\n      0.0032727171201258898],\n     [-0.00025740134879015386,\n      0.0008770020795054734,\n      -0.0002822779642883688,\n      -0.0012495031114667654,\n      0.0016539569478482008,\n      0.004080437123775482,\n      0.005178987048566341,\n      -0.00595001270994544,\n      0.005178987514227629,\n      0.0040804375894367695,\n      0.0016539570642635226,\n      -0.0012495031114667654,\n      -0.0002822779642883688,\n      0.0008770020795054734,\n      -0.00025740134879015386,\n      0.0008770020795054734,\n      0.003134208731353283,\n      0.003011508844792843,\n      0.0027187704108655453,\n      0.005964358802884817,\n      0.009380005300045013,\n      0.011369556188583374,\n      0.0007804449996910989,\n      0.0113695552572608,\n      0.009380006231367588,\n      0.005964358802884817,\n      0.0027187701780349016,\n      0.003011508844792843,\n      0.003134208731353283,\n      0.0008770021377131343,\n      -0.0002822779642883688,\n      0.0030115090776234865,\n      0.0037625757977366447,\n      0.0037280095275491476,\n      0.00829201191663742,\n      0.013809982687234879,\n      0.018193520605564117,\n      0.008142407052218914,\n      0.018193522468209267,\n      0.013809983618557453,\n      0.008292010053992271,\n      0.0037280090618878603,\n      0.0037625757977366447,\n      0.003011508844792843,\n      -0.00028227793518453836,\n      -0.0012495031114667654,\n      0.0027187701780349016,\n      0.0037280095275491476,\n      0.006246168632060289,\n      0.014383704401552677,\n      0.0257856547832489,\n      0.035342078655958176,\n      0.027512693777680397,\n      0.03534207493066788,\n      0.0257856547832489,\n      0.014383703470230103,\n      0.006246168632060289,\n      0.0037280090618878603,\n      0.0027187701780349016,\n      -0.001249503344297409,\n      0.0016539570642635226,\n      0.005964358802884817,\n      0.008292010985314846,\n      0.014383703470230103,\n      0.03072439879179001,\n      0.051105912774801254,\n      0.06893555074930191,\n      0.06472045183181763,\n      0.06893555074930191,\n      0.05110591650009155,\n      0.03072439879179001,\n      0.014383704401552677,\n      0.00829201191663742,\n      0.005964358802884817,\n      0.0016539569478482008,\n      0.004080437123775482,\n      0.009380005300045013,\n      0.013809983618557453,\n      0.025785652920603752,\n      0.051105912774801254,\n      0.08278024941682816,\n      0.10860900580883026,\n      0.10583575069904327,\n      0.10860900580883026,\n      0.08278024941682816,\n      0.051105912774801254,\n      0.0257856585085392,\n      0.013809982687234879,\n      0.009380006231367588,\n      0.004080437123775482,\n      0.005178987514227629,\n      0.011369556188583374,\n      0.018193522468209267,\n      0.03534207493066788,\n      0.06893555074930191,\n      0.10860899835824966,\n      0.12369456142187119,\n      0.07278922200202942,\n      0.12369458377361298,\n      0.10860902070999146,\n      0.06893555074930191,\n      0.035342078655958176,\n      0.018193522468209267,\n      0.011369557119905949,\n      0.005178987048566341,\n      -0.00595001270994544,\n      0.000780444242991507,\n      0.008142407052218914,\n      0.027512693777680397,\n      0.06472045928239822,\n      0.10583575069904327,\n      0.07278922200202942,\n      -0.8022134900093079,\n      0.07278922200202942,\n      0.10583578795194626,\n      0.06472045183181763,\n      0.027512693777680397,\n      0.008142407052218914,\n      0.0007804464548826218,\n      -0.005950015038251877,\n      0.005178987514227629,\n      0.011369557119905949,\n      0.018193520605564117,\n      0.035342078655958176,\n      0.06893554329872131,\n      0.10860901325941086,\n      0.12369457632303238,\n      0.07278918474912643,\n      0.12369459867477417,\n      0.10860899835824966,\n      0.06893555074930191,\n      0.035342078655958176,\n      0.018193520605564117,\n      0.011369557119905949,\n      0.005178987048566341,\n      0.004080437123775482,\n      0.009380006231367588,\n      0.013809982687234879,\n      0.02578565664589405,\n      0.051105909049510956,\n      0.08278025686740875,\n      0.10860899835824966,\n      0.10583575069904327,\n      0.10860901325941086,\n      0.08278024941682816,\n      0.051105912774801254,\n      0.0257856547832489,\n      0.013809983618557453,\n      0.009380005300045013,\n      0.004080437123775482,\n      0.0016539569478482008,\n      0.005964358802884817,\n      0.00829201191663742,\n      0.014383704401552677,\n      0.03072439879179001,\n      0.051105909049510956,\n      0.06893555074930191,\n      0.06472045183181763,\n      0.06893555074930191,\n      0.051105912774801254,\n      0.03072439692914486,\n      0.014383704401552677,\n      0.008292010985314846,\n      0.005964358802884817,\n      0.0016539570642635226,\n      -0.0012495027622208,\n      0.0027187701780349016,\n      0.003728008596226573,\n      0.006246169097721577,\n      0.014383704401552677,\n      0.0257856547832489,\n      0.03534207493066788,\n      0.027512693777680397,\n      0.035342078655958176,\n      0.02578565664589405,\n      0.014383704401552677,\n      0.006246168632060289,\n      0.0037280088290572166,\n      0.0027187701780349016,\n      -0.0012495031114667654,\n      -0.0002822779642883688,\n      0.003011508844792843,\n      0.0037625757977366447,\n      0.0037280095275491476,\n      0.008292010985314846,\n      0.013809982687234879,\n      0.018193522468209267,\n      0.00814240612089634,\n      0.018193520605564117,\n      0.013809982687234879,\n      0.00829201191663742,\n      0.0037280090618878603,\n      0.0037625757977366447,\n      0.003011508844792843,\n      -0.0002822781971190125,\n      0.0008770020795054734,\n      0.0031342084985226393,\n      0.0030115090776234865,\n      0.0027187701780349016,\n      0.005964358802884817,\n      0.009380006231367588,\n      0.0113695552572608,\n      0.0007804438937455416,\n      0.011369556188583374,\n      0.009380005300045013,\n      0.005964358802884817,\n      0.0027187704108655453,\n      0.003011508844792843,\n      0.0031342084985226393,\n      0.0008770020795054734,\n      -0.00025740129058249295,\n      0.0008770020795054734,\n      -0.0002822779933921993,\n      -0.0012495031114667654,\n      0.0016539570642635226,\n      0.004080437123775482,\n      0.005178987048566341,\n      -0.00595001270994544,\n      0.005178987048566341,\n      0.004080437123775482,\n      0.0016539569478482008,\n      -0.0012495027622208,\n      -0.00028227793518453836,\n      0.0008770020212978125,\n      -0.00025740134879015386],\n     [-0.0003610880521591753,\n      9.011493239086121e-05,\n      -0.0008604656322859228,\n      -0.0019334083190187812,\n      -0.001077542663551867,\n      -0.0006488121580332518,\n      -0.0007724234601482749,\n      -0.007340127602219582,\n      -0.0007724234601482749,\n      -0.0006488120998255908,\n      -0.001077542663551867,\n      -0.0019334083190187812,\n      -0.0008604656322859228,\n      9.011493239086121e-05,\n      -0.0003610880521591753,\n      9.011493239086121e-05,\n      0.0008889992604963481,\n      2.516521817597095e-05,\n      -0.0014261560281738639,\n      -0.0015848824987187982,\n      -0.0020498293451964855,\n      -0.002824551658704877,\n      -0.00973751861602068,\n      -0.0028245514258742332,\n      -0.0020498293451964855,\n      -0.0015848827315494418,\n      -0.0014261561445891857,\n      2.5165190891129896e-05,\n      0.0008889992022886872,\n      9.011493966681883e-05,\n      -0.0008604656322859228,\n      2.516522908990737e-05,\n      -0.0013923387741670012,\n      -0.0043399943970143795,\n      -0.006103459279984236,\n      -0.008659210987389088,\n      -0.010917608626186848,\n      -0.018898136913776398,\n      -0.010917608626186848,\n      -0.008659210987389088,\n      -0.006103459745645523,\n      -0.0043399943970143795,\n      -0.0013923386577516794,\n      2.5165241822833195e-05,\n      -0.0008604656322859228,\n      -0.001933408435434103,\n      -0.0014261561445891857,\n      -0.0043399943970143795,\n      -0.009113889187574387,\n      -0.013694832101464272,\n      -0.01949489116668701,\n      -0.02556983381509781,\n      -0.035283055156469345,\n      -0.02556983381509781,\n      -0.01949489116668701,\n      -0.013694831170141697,\n      -0.009113889187574387,\n      -0.0043399943970143795,\n      -0.0014261561445891857,\n      -0.001933408435434103,\n      -0.001077542663551867,\n      -0.00158488261513412,\n      -0.006103459745645523,\n      -0.013694832101464272,\n      -0.02276057004928589,\n      -0.036399129778146744,\n      -0.052227847278118134,\n      -0.06678059697151184,\n      -0.052227847278118134,\n      -0.036399129778146744,\n      -0.02276056818664074,\n      -0.013694831170141697,\n      -0.006103459279984236,\n      -0.00158488261513412,\n      -0.001077542663551867,\n      -0.0006488121580332518,\n      -0.0020498293451964855,\n      -0.008659210987389088,\n      -0.01949489116668701,\n      -0.036399129778146744,\n      -0.06629747152328491,\n      -0.10626056045293808,\n      -0.13546210527420044,\n      -0.10626056045293808,\n      -0.06629747152328491,\n      -0.036399129778146744,\n      -0.01949489116668701,\n      -0.008659210987389088,\n      -0.0020498293451964855,\n      -0.0006488122162409127,\n      -0.0007724235765635967,\n      -0.002824551658704877,\n      -0.010917607694864273,\n      -0.02556983381509781,\n      -0.052227847278118134,\n      -0.10626056790351868,\n      -0.19707798957824707,\n      -0.27363118529319763,\n      -0.19707797467708588,\n      -0.10626056045293808,\n      -0.052227847278118134,\n      -0.02556983381509781,\n      -0.010917607694864273,\n      -0.002824551658704877,\n      -0.0007724235183559358,\n      -0.007340127602219582,\n      -0.009737519547343254,\n      -0.018898136913776398,\n      -0.035283055156469345,\n      -0.06678059697151184,\n      -0.13546210527420044,\n      -0.27363118529319763,\n      -0.5405497550964355,\n      -0.27363118529319763,\n      -0.13546210527420044,\n      -0.06678059697151184,\n      -0.035283055156469345,\n      -0.018898136913776398,\n      -0.00973751861602068,\n      -0.007340128067880869,\n      -0.0007724235183559358,\n      -0.002824551658704877,\n      -0.010917608626186848,\n      -0.025569835677742958,\n      -0.052227847278118134,\n      -0.10626056790351868,\n      -0.19707797467708588,\n      -0.27363118529319763,\n      -0.19707797467708588,\n      -0.10626056790351868,\n      -0.052227847278118134,\n      -0.02556983381509781,\n      -0.010917608626186848,\n      -0.002824551658704877,\n      -0.0007724235765635967,\n      -0.0006488122162409127,\n      -0.0020498293451964855,\n      -0.008659210987389088,\n      -0.01949489116668701,\n      -0.036399129778146744,\n      -0.06629747152328491,\n      -0.10626056790351868,\n      -0.13546210527420044,\n      -0.10626056045293808,\n      -0.06629747152328491,\n      -0.036399129778146744,\n      -0.01949489116668701,\n      -0.008659210987389088,\n      -0.0020498293451964855,\n      -0.0006488120998255908,\n      -0.001077542663551867,\n      -0.00158488261513412,\n      -0.006103459279984236,\n      -0.013694831170141697,\n      -0.02276057004928589,\n      -0.036399129778146744,\n      -0.052227847278118134,\n      -0.06678059697151184,\n      -0.052227847278118134,\n      -0.036399129778146744,\n      -0.02276057004928589,\n      -0.013694831170141697,\n      -0.006103459745645523,\n      -0.0015848827315494418,\n      -0.001077542663551867,\n      -0.0019334083190187812,\n      -0.0014261561445891857,\n      -0.004339994862675667,\n      -0.009113889187574387,\n      -0.013694831170141697,\n      -0.01949489116668701,\n      -0.02556983381509781,\n      -0.035283055156469345,\n      -0.025569835677742958,\n      -0.01949489116668701,\n      -0.013694831170141697,\n      -0.009113889187574387,\n      -0.0043399943970143795,\n      -0.0014261561445891857,\n      -0.0019334083190187812,\n      -0.0008604656322859228,\n      2.5165236365864985e-05,\n      -0.0013923386577516794,\n      -0.0043399943970143795,\n      -0.006103459745645523,\n      -0.008659210987389088,\n      -0.010917608626186848,\n      -0.018898136913776398,\n      -0.010917608626186848,\n      -0.008659210987389088,\n      -0.006103459279984236,\n      -0.0043399943970143795,\n      -0.0013923387741670012,\n      2.5165210900013335e-05,\n      -0.0008604657487012446,\n      9.011491056298837e-05,\n      0.0008889993187040091,\n      2.516520908102393e-05,\n      -0.0014261561445891857,\n      -0.00158488261513412,\n      -0.0020498293451964855,\n      -0.0028245514258742332,\n      -0.009737519547343254,\n      -0.002824551658704877,\n      -0.0020498293451964855,\n      -0.00158488261513412,\n      -0.0014261560281738639,\n      2.5165205443045124e-05,\n      0.0008889992604963481,\n      9.011493239086121e-05,\n      -0.0003610880521591753,\n      9.011493966681883e-05,\n      -0.0008604656322859228,\n      -0.001933408435434103,\n      -0.001077542663551867,\n      -0.0006488121580332518,\n      -0.0007724234601482749,\n      -0.007340127602219582,\n      -0.000772423401940614,\n      -0.0006488121580332518,\n      -0.001077542663551867,\n      -0.0019334083190187812,\n      -0.0008604656904935837,\n      9.011491783894598e-05,\n      -0.0003610880521591753]],\n    dtype=np.float\n)\n\ndef gen_pca_mat(dim=15,kernel_size=15, samples=10000):\n  kernels = []\n  for i in range(samples):\n    theta = np.random.uniform(0, np.pi)\n    l1 = np.random.uniform(0.1, 10)\n    l2 = np.random.uniform(0.1, l1)\n    kernels.append(anisotropic_gaussian_kernel(kernel_size, theta, l1, l2))\n  kernels = np.stack(kernels).reshape([samples, -1]).transpose()\n  mat_c = np.matmul(kernels, kernels.transpose())\n  _, mat_v = np.linalg.eigh(mat_c, 'U')\n  return mat_v[..., -dim:].transpose()\n\n\ndef get_degradation(kernel: np.ndarray):\n  ret = np.matmul(_PCA, kernel.reshape([-1, 1]))\n  return ret\n"""
VSR/Backend/Torch/Models/teco/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/27 \xe4\xb8\x8b\xe5\x8d\x882:37\n\nimport logging\n\n_logger = logging.getLogger(""VSR.TecoGAN"")\n_logger.info(""LICENSE: TecoGAN is implemented by Mengyu Chu, et. al. ""\n             ""@rachelchu https://github.com/rachelchu/TecoGAN"")\n_logger.warning(""Training of TecoGAN hasn\'t been verified!!"")\n'"
VSR/Backend/Torch/Models/teco/ops.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/27 \xe4\xb8\x8b\xe5\x8d\x882:37\n\nimport torch\nfrom torch import nn\n\nfrom ..Arch import EasyConv2d, RB, Upsample\n\n\nclass TecoGenerator(nn.Module):\n  """"""Generator in TecoGAN.\n\n  Note: the flow estimation net `Fnet` shares with FRVSR.\n\n  Args:\n    filters: basic filter numbers [default: 64]\n    num_rb: number of residual blocks [default: 16]\n  """"""\n\n  def __init__(self, channel, scale, filters, num_rb):\n    super(TecoGenerator, self).__init__()\n    rbs = []\n    for i in range(num_rb):\n      rbs.append(RB(filters, 3, \'relu\'))\n    self.body = nn.Sequential(\n      EasyConv2d(channel * (1 + scale ** 2), filters, 3, activation=\'relu\'),\n      *rbs,\n      Upsample(filters, scale, \'nearest\', activation=\'relu\'),\n      EasyConv2d(filters, channel, 3))\n\n  def forward(self, x, prev, residual=None):\n    """"""`residual` is the bicubically upsampled HR images""""""\n    sr = self.body(torch.cat((x, prev), dim=1))\n    if residual is not None:\n      sr += residual\n    return sr\n\n\nclass TecoDiscriminator(nn.Module):\n  def __init__(self, channel, filters, patch_size):\n    super(TecoDiscriminator, self).__init__()\n    f = filters\n    self.conv0 = EasyConv2d(channel * 6, f, 3, activation=\'leaky\')\n    self.conv1 = EasyConv2d(f, f, 4, 2, activation=\'leaky\', use_bn=True)\n    self.conv2 = EasyConv2d(f, f, 4, 2, activation=\'leaky\', use_bn=True)\n    self.conv3 = EasyConv2d(f, f * 2, 4, 2, activation=\'leaky\', use_bn=True)\n    self.conv4 = EasyConv2d(f * 2, f * 4, 4, 2, activation=\'leaky\', use_bn=True)\n    # self.pool = nn.AdaptiveAvgPool2d(1)\n    self.linear = nn.Linear(f * 4 * (patch_size // 16) ** 2, 1)\n\n  def forward(self, x):\n    """"""The inputs `x` is the concat of 8 tensors.\n      Note that we remove the duplicated gt/yt in paper (9 - 1 = 8).\n    """"""\n    l0 = self.conv0(x)\n    l1 = self.conv1(l0)\n    l2 = self.conv2(l1)\n    l3 = self.conv3(l2)\n    l4 = self.conv4(l3)\n    # y = self.pool(l4)\n    y = self.linear(l4.flatten(1))\n    return y, (l1, l2, l3, l4)\n'"
VSR/Backend/Torch/Models/vespcn/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:10\n\nimport logging\n\n_logger = logging.getLogger(""VSR.VESPCN"")\n_logger.info(""LICENSE: VESPCN is proposed at CVPR2017 by Twitter. ""\n             ""Implemented by myself @LoSealL."")\n'"
VSR/Backend/Torch/Models/vespcn/ops.py,0,"b""#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:10\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom ..video.motion import STN\n\n\nclass RB(nn.Module):\n  def __init__(self, inchannels, outchannels):\n    super(RB, self).__init__()\n    self.conv1 = nn.Conv2d(inchannels, 64, 3, 1, 1)\n    self.conv2 = nn.Conv2d(64, outchannels, 3, 1, 1)\n    if inchannels != outchannels:\n      self.sc = nn.Conv2d(inchannels, outchannels, 1)\n\n  def forward(self, inputs):\n    x = F.relu(inputs)\n    x = self.conv1(x)\n    x = F.relu(x)\n    x = self.conv2(x)\n    if hasattr(self, 'sc'):\n      sc = self.sc(inputs)\n    else:\n      sc = inputs\n    return x + sc\n\n\nclass MotionCompensation(nn.Module):\n  def __init__(self, channel, gain=32):\n    super(MotionCompensation, self).__init__()\n    self.gain = gain\n    in_c = channel * 2\n    # Coarse Flow\n    conv1 = nn.Sequential(nn.Conv2d(in_c, 24, 5, 2, 2), nn.ReLU(True))\n    conv2 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv3 = nn.Sequential(nn.Conv2d(24, 24, 5, 2, 2), nn.ReLU(True))\n    conv4 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv5 = nn.Sequential(nn.Conv2d(24, 32, 3, 1, 1), nn.Tanh())\n    up1 = nn.PixelShuffle(4)\n    self.coarse_flow = nn.Sequential(conv1, conv2, conv3, conv4, conv5, up1)\n    # Fine Flow\n    in_c = channel * 3 + 2\n    conv1 = nn.Sequential(nn.Conv2d(in_c, 24, 5, 2, 2), nn.ReLU(True))\n    conv2 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv3 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv4 = nn.Sequential(nn.Conv2d(24, 24, 3, 1, 1), nn.ReLU(True))\n    conv5 = nn.Sequential(nn.Conv2d(24, 8, 3, 1, 1), nn.Tanh())\n    up2 = nn.PixelShuffle(2)\n    self.fine_flow = nn.Sequential(conv1, conv2, conv3, conv4, conv5, up2)\n    self.warp1 = STN(padding_mode='border')\n    self.warp2 = STN(padding_mode='border')\n\n  def forward(self, target, ref):\n    flow0 = self.coarse_flow(torch.cat([ref, target], 1))\n    flow0 *= self.gain\n    w0 = self.warp1(ref, flow0[:, 0], flow0[:, 1])\n    flow1 = self.fine_flow(torch.cat([ref, target, flow0, w0], 1))\n    flow1 *= self.gain\n    flow1 += flow0\n    w1 = self.warp2(ref, flow1[:, 0], flow1[:, 1])\n    return w1, flow1\n\n\nclass SRNet(nn.Module):\n  def __init__(self, scale, channel, depth):\n    super(SRNet, self).__init__()\n    self.entry = nn.Conv2d(channel * depth, 64, 3, 1, 1)\n    self.exit = nn.Conv2d(64, channel, 3, 1, 1)\n    self.body = nn.Sequential(RB(64, 64), RB(64, 64), RB(64, 64), nn.ReLU(True))\n    self.conv = nn.Conv2d(64, 64 * scale ** 2, 3, 1, 1)\n    self.up = nn.PixelShuffle(scale)\n\n  def forward(self, inputs):\n    x = self.entry(inputs)\n    y = self.body(x) + x\n    y = self.conv(y)\n    y = self.up(y)\n    y = self.exit(y)\n    return y\n\n\nclass VESPCN(nn.Module):\n  def __init__(self, scale, channel, depth):\n    super(VESPCN, self).__init__()\n    self.sr = SRNet(scale, channel, depth)\n    self.mc = MotionCompensation(channel)\n    self.depth = depth\n\n  def forward(self, *inputs):\n    center = self.depth // 2\n    target = inputs[center]\n    refs = inputs[:center] + inputs[center + 1:]\n    warps = []\n    flows = []\n    for r in refs:\n      warp, flow = self.mc(target, r)\n      warps.append(warp)\n      flows.append(flow)\n    warps.append(target)\n    x = torch.cat(warps, 1)\n    sr = self.sr(x)\n    return sr, warps[:-1], flows\n"""
VSR/Backend/Torch/Models/video/__init__.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:10\n\nimport logging\n\n_logger = logging.getLogger(""VSR.VIDEO"")\n'"
VSR/Backend/Torch/Models/video/motion.py,0,"b'#  Copyright (c): Wenyi Tang 2017-2019.\n#  Author: Wenyi Tang\n#  Email: wenyi.tang@intel.com\n#  Update Date: 2019/4/3 \xe4\xb8\x8b\xe5\x8d\x885:10\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom VSR.Util.Math import nd_meshgrid\nfrom ...Util.Utility import irtranspose, transpose\n\n\nclass STN(nn.Module):\n  """"""Spatial transformer network.\n    For optical flow based frame warping.\n\n  Args:\n    mode: sampling interpolation mode of `grid_sample`\n    padding_mode: can be `zeros` | `borders`\n    normalized: flow value is normalized to [-1, 1] or absolute value\n  """"""\n\n  def __init__(self, mode=\'bilinear\', padding_mode=\'zeros\', normalize=False):\n    super(STN, self).__init__()\n    self.mode = mode\n    self.padding_mode = padding_mode\n    self.norm = normalize\n\n  def forward(self, inputs, u, v):\n    batch = inputs.size(0)\n    device = inputs.device\n    mesh = nd_meshgrid(*inputs.shape[-2:], permute=[1, 0])\n    mesh = torch.tensor(mesh, dtype=torch.float32, device=device)\n    mesh = mesh.unsqueeze(0).repeat_interleave(batch, dim=0)\n    # add flow to mesh\n    _u, _v = u, v\n    if not self.norm:\n      # flow needs to normalize to [-1, 1]\n      h, w = inputs.shape[-2:]\n      _u = u / w * 2\n      _v = v / h * 2\n    flow = torch.stack([_u, _v], dim=-1)\n    assert flow.shape == mesh.shape, \\\n      f""Shape mis-match: {flow.shape} != {mesh.shape}""\n    mesh = mesh + flow\n    return F.grid_sample(inputs, mesh,\n                         mode=self.mode, padding_mode=self.padding_mode)\n\n\nclass STTN(nn.Module):\n  """"""Spatio-temporal transformer network. (ECCV 2018)\n\n  Args:\n    transpose_ncthw: how input tensor be transposed to format NCTHW\n    mode: sampling interpolation mode of `grid_sample`\n    padding_mode: can be `zeros` | `borders`\n    normalize: flow value is normalized to [-1, 1] or absolute value\n  """"""\n\n  def __init__(self, transpose_ncthw=(0, 1, 2, 3, 4),\n               normalize=False, mode=\'bilinear\', padding_mode=\'zeros\'):\n    super(STTN, self).__init__()\n    self.normalized = normalize\n    self.mode = mode\n    self.padding_mode = padding_mode\n    self.t = transpose_ncthw\n\n  def forward(self, inputs, d, u, v):\n    _error_msg = ""STTN only works for 5D tensor but got {}D input!""\n    if inputs.dim() != 5:\n      raise ValueError(_error_msg.format(inputs.dim()))\n    device = inputs.device\n    batch, channel, t, h, w = (inputs.shape[i] for i in self.t)\n    mesh = nd_meshgrid(t, h, w, permute=[2, 1, 0])\n    mesh = torch.tensor(mesh, dtype=torch.float32, device=device)\n    mesh = mesh.unsqueeze(0).repeat_interleave(batch, dim=0)\n    _d, _u, _v = d, u, v\n    if not self.normalized:\n      _d = d / t * 2\n      _u = u / w * 2\n      _v = v / h * 2\n    st_flow = torch.stack([_u, _v, _d], dim=-1)\n    st_flow = st_flow.unsqueeze(1).repeat_interleave(t, dim=1)\n    assert st_flow.shape == mesh.shape, \\\n      f""Shape mis-match: {st_flow.shape} != {mesh.shape}""\n    mesh = mesh + st_flow\n    inputs = transpose(inputs, self.t)\n    warp = F.grid_sample(inputs, mesh, mode=self.mode,\n                         padding_mode=self.padding_mode)\n    # STTN warps into a single frame\n    warp = warp[:, :, 0:1]\n    return irtranspose(warp, self.t)\n'"
