file_path,api_count,code
data.py,10,"b'import numpy as np\nimport tensorflow as tf\nimport tf2lib as tl\n\n\ndef make_dataset(img_paths, batch_size, load_size, crop_size, training, drop_remainder=True, shuffle=True, repeat=1):\n    if training:\n        @tf.function\n        def _map_fn(img):  # preprocessing\n            img = tf.image.random_flip_left_right(img)\n            img = tf.image.resize(img, [load_size, load_size])\n            img = tf.image.random_crop(img, [crop_size, crop_size, tf.shape(img)[-1]])\n            img = tf.clip_by_value(img, 0, 255) / 255.0  # or img = tl.minmax_norm(img)\n            img = img * 2 - 1\n            return img\n    else:\n        @tf.function\n        def _map_fn(img):  # preprocessing\n            img = tf.image.resize(img, [crop_size, crop_size])  # or img = tf.image.resize(img, [load_size, load_size]); img = tl.center_crop(img, crop_size)\n            img = tf.clip_by_value(img, 0, 255) / 255.0  # or img = tl.minmax_norm(img)\n            img = img * 2 - 1\n            return img\n\n    return tl.disk_image_batch_dataset(img_paths,\n                                       batch_size,\n                                       drop_remainder=drop_remainder,\n                                       map_fn=_map_fn,\n                                       shuffle=shuffle,\n                                       repeat=repeat)\n\n\ndef make_zip_dataset(A_img_paths, B_img_paths, batch_size, load_size, crop_size, training, shuffle=True, repeat=False):\n    # zip two datasets aligned by the longer one\n    if repeat:\n        A_repeat = B_repeat = None  # cycle both\n    else:\n        if len(A_img_paths) >= len(B_img_paths):\n            A_repeat = 1\n            B_repeat = None  # cycle the shorter one\n        else:\n            A_repeat = None  # cycle the shorter one\n            B_repeat = 1\n\n    A_dataset = make_dataset(A_img_paths, batch_size, load_size, crop_size, training, drop_remainder=True, shuffle=shuffle, repeat=A_repeat)\n    B_dataset = make_dataset(B_img_paths, batch_size, load_size, crop_size, training, drop_remainder=True, shuffle=shuffle, repeat=B_repeat)\n\n    A_B_dataset = tf.data.Dataset.zip((A_dataset, B_dataset))\n    len_dataset = max(len(A_img_paths), len(B_img_paths)) // batch_size\n\n    return A_B_dataset, len_dataset\n\n\nclass ItemPool:\n\n    def __init__(self, pool_size=50):\n        self.pool_size = pool_size\n        self.items = []\n\n    def __call__(self, in_items):\n        # `in_items` should be a batch tensor\n\n        if self.pool_size == 0:\n            return in_items\n\n        out_items = []\n        for in_item in in_items:\n            if len(self.items) < self.pool_size:\n                self.items.append(in_item)\n                out_items.append(in_item)\n            else:\n                if np.random.rand() > 0.5:\n                    idx = np.random.randint(0, len(self.items))\n                    out_item, self.items[idx] = self.items[idx], in_item\n                    out_items.append(out_item)\n                else:\n                    out_items.append(in_item)\n        return tf.stack(out_items, axis=0)\n'"
module.py,14,"b""import tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras as keras\n\n\n# ==============================================================================\n# =                                  networks                                  =\n# ==============================================================================\n\ndef _get_norm_layer(norm):\n    if norm == 'none':\n        return lambda: lambda x: x\n    elif norm == 'batch_norm':\n        return keras.layers.BatchNormalization\n    elif norm == 'instance_norm':\n        return tfa.layers.InstanceNormalization\n    elif norm == 'layer_norm':\n        return keras.layers.LayerNormalization\n\n\ndef ResnetGenerator(input_shape=(256, 256, 3),\n                    output_channels=3,\n                    dim=64,\n                    n_downsamplings=2,\n                    n_blocks=9,\n                    norm='instance_norm'):\n    Norm = _get_norm_layer(norm)\n\n    def _residual_block(x):\n        dim = x.shape[-1]\n        h = x\n\n        h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n        h = keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(h)\n        h = Norm()(h)\n        h = tf.nn.relu(h)\n\n        h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n        h = keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(h)\n        h = Norm()(h)\n\n        return keras.layers.add([x, h])\n\n    # 0\n    h = inputs = keras.Input(shape=input_shape)\n\n    # 1\n    h = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n    h = keras.layers.Conv2D(dim, 7, padding='valid', use_bias=False)(h)\n    h = Norm()(h)\n    h = tf.nn.relu(h)\n\n    # 2\n    for _ in range(n_downsamplings):\n        dim *= 2\n        h = keras.layers.Conv2D(dim, 3, strides=2, padding='same', use_bias=False)(h)\n        h = Norm()(h)\n        h = tf.nn.relu(h)\n\n    # 3\n    for _ in range(n_blocks):\n        h = _residual_block(h)\n\n    # 4\n    for _ in range(n_downsamplings):\n        dim //= 2\n        h = keras.layers.Conv2DTranspose(dim, 3, strides=2, padding='same', use_bias=False)(h)\n        h = Norm()(h)\n        h = tf.nn.relu(h)\n\n    # 5\n    h = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n    h = keras.layers.Conv2D(output_channels, 7, padding='valid')(h)\n    h = tf.tanh(h)\n\n    return keras.Model(inputs=inputs, outputs=h)\n\n\ndef ConvDiscriminator(input_shape=(256, 256, 3),\n                      dim=64,\n                      n_downsamplings=3,\n                      norm='instance_norm'):\n    dim_ = dim\n    Norm = _get_norm_layer(norm)\n\n    # 0\n    h = inputs = keras.Input(shape=input_shape)\n\n    # 1\n    h = keras.layers.Conv2D(dim, 4, strides=2, padding='same')(h)\n    h = tf.nn.leaky_relu(h, alpha=0.2)\n\n    for _ in range(n_downsamplings - 1):\n        dim = min(dim * 2, dim_ * 8)\n        h = keras.layers.Conv2D(dim, 4, strides=2, padding='same', use_bias=False)(h)\n        h = Norm()(h)\n        h = tf.nn.leaky_relu(h, alpha=0.2)\n\n    # 2\n    dim = min(dim * 2, dim_ * 8)\n    h = keras.layers.Conv2D(dim, 4, strides=1, padding='same', use_bias=False)(h)\n    h = Norm()(h)\n    h = tf.nn.leaky_relu(h, alpha=0.2)\n\n    # 3\n    h = keras.layers.Conv2D(1, 4, strides=1, padding='same')(h)\n\n    return keras.Model(inputs=inputs, outputs=h)\n\n\n# ==============================================================================\n# =                          learning rate scheduler                           =\n# ==============================================================================\n\nclass LinearDecay(keras.optimizers.schedules.LearningRateSchedule):\n    # if `step` < `step_decay`: use fixed learning rate\n    # else: linearly decay the learning rate to zero\n\n    def __init__(self, initial_learning_rate, total_steps, step_decay):\n        super(LinearDecay, self).__init__()\n        self._initial_learning_rate = initial_learning_rate\n        self._steps = total_steps\n        self._step_decay = step_decay\n        self.current_learning_rate = tf.Variable(initial_value=initial_learning_rate, trainable=False, dtype=tf.float32)\n\n    def __call__(self, step):\n        self.current_learning_rate.assign(tf.cond(\n            step >= self._step_decay,\n            true_fn=lambda: self._initial_learning_rate * (1 - 1 / (self._steps - self._step_decay) * (step - self._step_decay)),\n            false_fn=lambda: self._initial_learning_rate\n        ))\n        return self.current_learning_rate\n"""
test.py,2,"b""import imlib as im\nimport numpy as np\nimport pylib as py\nimport tensorflow as tf\nimport tf2lib as tl\n\nimport data\nimport module\n\n# ==============================================================================\n# =                                   param                                    =\n# ==============================================================================\n\npy.arg('--experiment_dir')\npy.arg('--batch_size', type=int, default=32)\ntest_args = py.args()\nargs = py.args_from_yaml(py.join(test_args.experiment_dir, 'settings.yml'))\nargs.__dict__.update(test_args.__dict__)\n\n\n# ==============================================================================\n# =                                    test                                    =\n# ==============================================================================\n\n# data\nA_img_paths_test = py.glob(py.join(args.datasets_dir, args.dataset, 'testA'), '*.jpg')\nB_img_paths_test = py.glob(py.join(args.datasets_dir, args.dataset, 'testB'), '*.jpg')\nA_dataset_test = data.make_dataset(A_img_paths_test, args.batch_size, args.load_size, args.crop_size,\n                                   training=False, drop_remainder=False, shuffle=False, repeat=1)\nB_dataset_test = data.make_dataset(B_img_paths_test, args.batch_size, args.load_size, args.crop_size,\n                                   training=False, drop_remainder=False, shuffle=False, repeat=1)\n\n# model\nG_A2B = module.ResnetGenerator(input_shape=(args.crop_size, args.crop_size, 3))\nG_B2A = module.ResnetGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n\n# resotre\ntl.Checkpoint(dict(G_A2B=G_A2B, G_B2A=G_B2A), py.join(args.experiment_dir, 'checkpoints')).restore()\n\n\n@tf.function\ndef sample_A2B(A):\n    A2B = G_A2B(A, training=False)\n    A2B2A = G_B2A(A2B, training=False)\n    return A2B, A2B2A\n\n\n@tf.function\ndef sample_B2A(B):\n    B2A = G_B2A(B, training=False)\n    B2A2B = G_A2B(B2A, training=False)\n    return B2A, B2A2B\n\n\n# run\nsave_dir = py.join(args.experiment_dir, 'samples_testing', 'A2B')\npy.mkdir(save_dir)\ni = 0\nfor A in A_dataset_test:\n    A2B, A2B2A = sample_A2B(A)\n    for A_i, A2B_i, A2B2A_i in zip(A, A2B, A2B2A):\n        img = np.concatenate([A_i.numpy(), A2B_i.numpy(), A2B2A_i.numpy()], axis=1)\n        im.imwrite(img, py.join(save_dir, py.name_ext(A_img_paths_test[i])))\n        i += 1\n\nsave_dir = py.join(args.experiment_dir, 'samples_testing', 'B2A')\npy.mkdir(save_dir)\ni = 0\nfor B in B_dataset_test:\n    B2A, B2A2B = sample_B2A(B)\n    for B_i, B2A_i, B2A2B_i in zip(B, B2A, B2A2B):\n        img = np.concatenate([B_i.numpy(), B2A_i.numpy(), B2A2B_i.numpy()], axis=1)\n        im.imwrite(img, py.join(save_dir, py.name_ext(B_img_paths_test[i])))\n        i += 1\n"""
train.py,9,"b""import functools\n\nimport imlib as im\nimport numpy as np\nimport pylib as py\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tf2lib as tl\nimport tf2gan as gan\nimport tqdm\n\nimport data\nimport module\n\n\n# ==============================================================================\n# =                                   param                                    =\n# ==============================================================================\n\npy.arg('--dataset', default='horse2zebra')\npy.arg('--datasets_dir', default='datasets')\npy.arg('--load_size', type=int, default=286)  # load image to this size\npy.arg('--crop_size', type=int, default=256)  # then crop to this size\npy.arg('--batch_size', type=int, default=1)\npy.arg('--epochs', type=int, default=200)\npy.arg('--epoch_decay', type=int, default=100)  # epoch to start decaying learning rate\npy.arg('--lr', type=float, default=0.0002)\npy.arg('--beta_1', type=float, default=0.5)\npy.arg('--adversarial_loss_mode', default='lsgan', choices=['gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan'])\npy.arg('--gradient_penalty_mode', default='none', choices=['none', 'dragan', 'wgan-gp'])\npy.arg('--gradient_penalty_weight', type=float, default=10.0)\npy.arg('--cycle_loss_weight', type=float, default=10.0)\npy.arg('--identity_loss_weight', type=float, default=0.0)\npy.arg('--pool_size', type=int, default=50)  # pool size to store fake samples\nargs = py.args()\n\n# output_dir\noutput_dir = py.join('output', args.dataset)\npy.mkdir(output_dir)\n\n# save settings\npy.args_to_yaml(py.join(output_dir, 'settings.yml'), args)\n\n\n# ==============================================================================\n# =                                    data                                    =\n# ==============================================================================\n\nA_img_paths = py.glob(py.join(args.datasets_dir, args.dataset, 'trainA'), '*.jpg')\nB_img_paths = py.glob(py.join(args.datasets_dir, args.dataset, 'trainB'), '*.jpg')\nA_B_dataset, len_dataset = data.make_zip_dataset(A_img_paths, B_img_paths, args.batch_size, args.load_size, args.crop_size, training=True, repeat=False)\n\nA2B_pool = data.ItemPool(args.pool_size)\nB2A_pool = data.ItemPool(args.pool_size)\n\nA_img_paths_test = py.glob(py.join(args.datasets_dir, args.dataset, 'testA'), '*.jpg')\nB_img_paths_test = py.glob(py.join(args.datasets_dir, args.dataset, 'testB'), '*.jpg')\nA_B_dataset_test, _ = data.make_zip_dataset(A_img_paths_test, B_img_paths_test, args.batch_size, args.load_size, args.crop_size, training=False, repeat=True)\n\n\n# ==============================================================================\n# =                                   models                                   =\n# ==============================================================================\n\nG_A2B = module.ResnetGenerator(input_shape=(args.crop_size, args.crop_size, 3))\nG_B2A = module.ResnetGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n\nD_A = module.ConvDiscriminator(input_shape=(args.crop_size, args.crop_size, 3))\nD_B = module.ConvDiscriminator(input_shape=(args.crop_size, args.crop_size, 3))\n\nd_loss_fn, g_loss_fn = gan.get_adversarial_losses_fn(args.adversarial_loss_mode)\ncycle_loss_fn = tf.losses.MeanAbsoluteError()\nidentity_loss_fn = tf.losses.MeanAbsoluteError()\n\nG_lr_scheduler = module.LinearDecay(args.lr, args.epochs * len_dataset, args.epoch_decay * len_dataset)\nD_lr_scheduler = module.LinearDecay(args.lr, args.epochs * len_dataset, args.epoch_decay * len_dataset)\nG_optimizer = keras.optimizers.Adam(learning_rate=G_lr_scheduler, beta_1=args.beta_1)\nD_optimizer = keras.optimizers.Adam(learning_rate=D_lr_scheduler, beta_1=args.beta_1)\n\n\n# ==============================================================================\n# =                                 train step                                 =\n# ==============================================================================\n\n@tf.function\ndef train_G(A, B):\n    with tf.GradientTape() as t:\n        A2B = G_A2B(A, training=True)\n        B2A = G_B2A(B, training=True)\n        A2B2A = G_B2A(A2B, training=True)\n        B2A2B = G_A2B(B2A, training=True)\n        A2A = G_B2A(A, training=True)\n        B2B = G_A2B(B, training=True)\n\n        A2B_d_logits = D_B(A2B, training=True)\n        B2A_d_logits = D_A(B2A, training=True)\n\n        A2B_g_loss = g_loss_fn(A2B_d_logits)\n        B2A_g_loss = g_loss_fn(B2A_d_logits)\n        A2B2A_cycle_loss = cycle_loss_fn(A, A2B2A)\n        B2A2B_cycle_loss = cycle_loss_fn(B, B2A2B)\n        A2A_id_loss = identity_loss_fn(A, A2A)\n        B2B_id_loss = identity_loss_fn(B, B2B)\n\n        G_loss = (A2B_g_loss + B2A_g_loss) + (A2B2A_cycle_loss + B2A2B_cycle_loss) * args.cycle_loss_weight + (A2A_id_loss + B2B_id_loss) * args.identity_loss_weight\n\n    G_grad = t.gradient(G_loss, G_A2B.trainable_variables + G_B2A.trainable_variables)\n    G_optimizer.apply_gradients(zip(G_grad, G_A2B.trainable_variables + G_B2A.trainable_variables))\n\n    return A2B, B2A, {'A2B_g_loss': A2B_g_loss,\n                      'B2A_g_loss': B2A_g_loss,\n                      'A2B2A_cycle_loss': A2B2A_cycle_loss,\n                      'B2A2B_cycle_loss': B2A2B_cycle_loss,\n                      'A2A_id_loss': A2A_id_loss,\n                      'B2B_id_loss': B2B_id_loss}\n\n\n@tf.function\ndef train_D(A, B, A2B, B2A):\n    with tf.GradientTape() as t:\n        A_d_logits = D_A(A, training=True)\n        B2A_d_logits = D_A(B2A, training=True)\n        B_d_logits = D_B(B, training=True)\n        A2B_d_logits = D_B(A2B, training=True)\n\n        A_d_loss, B2A_d_loss = d_loss_fn(A_d_logits, B2A_d_logits)\n        B_d_loss, A2B_d_loss = d_loss_fn(B_d_logits, A2B_d_logits)\n        D_A_gp = gan.gradient_penalty(functools.partial(D_A, training=True), A, B2A, mode=args.gradient_penalty_mode)\n        D_B_gp = gan.gradient_penalty(functools.partial(D_B, training=True), B, A2B, mode=args.gradient_penalty_mode)\n\n        D_loss = (A_d_loss + B2A_d_loss) + (B_d_loss + A2B_d_loss) + (D_A_gp + D_B_gp) * args.gradient_penalty_weight\n\n    D_grad = t.gradient(D_loss, D_A.trainable_variables + D_B.trainable_variables)\n    D_optimizer.apply_gradients(zip(D_grad, D_A.trainable_variables + D_B.trainable_variables))\n\n    return {'A_d_loss': A_d_loss + B2A_d_loss,\n            'B_d_loss': B_d_loss + A2B_d_loss,\n            'D_A_gp': D_A_gp,\n            'D_B_gp': D_B_gp}\n\n\ndef train_step(A, B):\n    A2B, B2A, G_loss_dict = train_G(A, B)\n\n    # cannot autograph `A2B_pool`\n    A2B = A2B_pool(A2B)  # or A2B = A2B_pool(A2B.numpy()), but it is much slower\n    B2A = B2A_pool(B2A)  # because of the communication between CPU and GPU\n\n    D_loss_dict = train_D(A, B, A2B, B2A)\n\n    return G_loss_dict, D_loss_dict\n\n\n@tf.function\ndef sample(A, B):\n    A2B = G_A2B(A, training=False)\n    B2A = G_B2A(B, training=False)\n    A2B2A = G_B2A(A2B, training=False)\n    B2A2B = G_A2B(B2A, training=False)\n    return A2B, B2A, A2B2A, B2A2B\n\n\n# ==============================================================================\n# =                                    run                                     =\n# ==============================================================================\n\n# epoch counter\nep_cnt = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)\n\n# checkpoint\ncheckpoint = tl.Checkpoint(dict(G_A2B=G_A2B,\n                                G_B2A=G_B2A,\n                                D_A=D_A,\n                                D_B=D_B,\n                                G_optimizer=G_optimizer,\n                                D_optimizer=D_optimizer,\n                                ep_cnt=ep_cnt),\n                           py.join(output_dir, 'checkpoints'),\n                           max_to_keep=5)\ntry:  # restore checkpoint including the epoch counter\n    checkpoint.restore().assert_existing_objects_matched()\nexcept Exception as e:\n    print(e)\n\n# summary\ntrain_summary_writer = tf.summary.create_file_writer(py.join(output_dir, 'summaries', 'train'))\n\n# sample\ntest_iter = iter(A_B_dataset_test)\nsample_dir = py.join(output_dir, 'samples_training')\npy.mkdir(sample_dir)\n\n# main loop\nwith train_summary_writer.as_default():\n    for ep in tqdm.trange(args.epochs, desc='Epoch Loop'):\n        if ep < ep_cnt:\n            continue\n\n        # update epoch counter\n        ep_cnt.assign_add(1)\n\n        # train for an epoch\n        for A, B in tqdm.tqdm(A_B_dataset, desc='Inner Epoch Loop', total=len_dataset):\n            G_loss_dict, D_loss_dict = train_step(A, B)\n\n            # # summary\n            tl.summary(G_loss_dict, step=G_optimizer.iterations, name='G_losses')\n            tl.summary(D_loss_dict, step=G_optimizer.iterations, name='D_losses')\n            tl.summary({'learning rate': G_lr_scheduler.current_learning_rate}, step=G_optimizer.iterations, name='learning rate')\n\n            # sample\n            if G_optimizer.iterations.numpy() % 100 == 0:\n                A, B = next(test_iter)\n                A2B, B2A, A2B2A, B2A2B = sample(A, B)\n                img = im.immerge(np.concatenate([A, A2B, A2B2A, B, B2A, B2A2B], axis=0), n_rows=2)\n                im.imwrite(img, py.join(sample_dir, 'iter-%09d.jpg' % G_optimizer.iterations.numpy()))\n\n        # save checkpoint\n        checkpoint.save(ep)\n"""
imlib/__init__.py,0,b'from imlib.basic import *\nfrom imlib.dtype import *\nfrom imlib.transform import *\n'
imlib/basic.py,0,"b'import numpy as np\nimport skimage.io as iio\n\nfrom imlib import dtype\n\n\ndef imread(path, as_gray=False, **kwargs):\n    """"""Return a float64 image in [-1.0, 1.0].""""""\n    image = iio.imread(path, as_gray, **kwargs)\n    if image.dtype == np.uint8:\n        image = image / 127.5 - 1\n    elif image.dtype == np.uint16:\n        image = image / 32767.5 - 1\n    elif image.dtype in [np.float32, np.float64]:\n        image = image * 2 - 1.0\n    else:\n        raise Exception(""Inavailable image dtype: %s!"" % image.dtype)\n    return image\n\n\ndef imwrite(image, path, quality=95, **plugin_args):\n    """"""Save a [-1.0, 1.0] image.""""""\n    iio.imsave(path, dtype.im2uint(image), quality=quality, **plugin_args)\n\n\ndef imshow(image):\n    """"""Show a [-1.0, 1.0] image.""""""\n    iio.imshow(dtype.im2uint(image))\n\n\nshow = iio.show\n'"
imlib/dtype.py,0,"b'import numpy as np\n\n\ndef _check(images, dtypes, min_value=-np.inf, max_value=np.inf):\n    # check type\n    assert isinstance(images, np.ndarray), \'`images` should be np.ndarray!\'\n\n    # check dtype\n    dtypes = dtypes if isinstance(dtypes, (list, tuple)) else [dtypes]\n    assert images.dtype in dtypes, \'dtype of `images` shoud be one of %s!\' % dtypes\n\n    # check nan and inf\n    assert np.all(np.isfinite(images)), \'`images` contains NaN or Inf!\'\n\n    # check value\n    if min_value not in [None, -np.inf]:\n        l = \'[\' + str(min_value)\n    else:\n        l = \'(-inf\'\n        min_value = -np.inf\n    if max_value not in [None, np.inf]:\n        r = str(max_value) + \']\'\n    else:\n        r = \'inf)\'\n        max_value = np.inf\n    assert np.min(images) >= min_value and np.max(images) <= max_value, \\\n        \'`images` should be in the range of %s!\' % (l + \',\' + r)\n\n\ndef to_range(images, min_value=0.0, max_value=1.0, dtype=None):\n    """"""Transform images from [-1.0, 1.0] to [min_value, max_value] of dtype.""""""\n    _check(images, [np.float32, np.float64], -1.0, 1.0)\n    dtype = dtype if dtype else images.dtype\n    return ((images + 1.) / 2. * (max_value - min_value) + min_value).astype(dtype)\n\n\ndef float2im(images):\n    """"""Transform images from [0, 1.0] to [-1.0, 1.0].""""""\n    _check(images, [np.float32, np.float64], 0.0, 1.0)\n    return images * 2 - 1.0\n\n\ndef float2uint(images):\n    """"""Transform images from [0, 1.0] to uint8.""""""\n    _check(images, [np.float32, np.float64], -0.0, 1.0)\n    return (images * 255).astype(np.uint8)\n\n\ndef im2uint(images):\n    """"""Transform images from [-1.0, 1.0] to uint8.""""""\n    return to_range(images, 0, 255, np.uint8)\n\n\ndef im2float(images):\n    """"""Transform images from [-1.0, 1.0] to [0.0, 1.0].""""""\n    return to_range(images, 0.0, 1.0)\n\n\ndef uint2im(images):\n    """"""Transform images from uint8 to [-1.0, 1.0] of float64.""""""\n    _check(images, np.uint8)\n    return images / 127.5 - 1.0\n\n\ndef uint2float(images):\n    """"""Transform images from uint8 to [0.0, 1.0] of float64.""""""\n    _check(images, np.uint8)\n    return images / 255.0\n\n\ndef cv2im(images):\n    """"""Transform opencv images to [-1.0, 1.0].""""""\n    images = uint2im(images)\n    return images[..., ::-1]\n\n\ndef im2cv(images):\n    """"""Transform images from [-1.0, 1.0] to opencv images.""""""\n    images = im2uint(images)\n    return images[..., ::-1]\n'"
imlib/transform.py,0,"b'import numpy as np\nimport skimage.color as color\nimport skimage.transform as transform\n\n\nrgb2gray = color.rgb2gray\ngray2rgb = color.gray2rgb\n\nimresize = transform.resize\nimrescale = transform.rescale\n\n\ndef immerge(images, n_rows=None, n_cols=None, padding=0, pad_value=0):\n    """"""Merge images to an image with (n_rows * h) * (n_cols * w).\n\n    Parameters\n    ----------\n    images : numpy.array or object which can be converted to numpy.array\n        Images in shape of N * H * W(* C=1 or 3).\n\n    """"""\n    images = np.array(images)\n    n = images.shape[0]\n    if n_rows:\n        n_rows = max(min(n_rows, n), 1)\n        n_cols = int(n - 0.5) // n_rows + 1\n    elif n_cols:\n        n_cols = max(min(n_cols, n), 1)\n        n_rows = int(n - 0.5) // n_cols + 1\n    else:\n        n_rows = int(n ** 0.5)\n        n_cols = int(n - 0.5) // n_rows + 1\n\n    h, w = images.shape[1], images.shape[2]\n    shape = (h * n_rows + padding * (n_rows - 1),\n             w * n_cols + padding * (n_cols - 1))\n    if images.ndim == 4:\n        shape += (images.shape[3],)\n    img = np.full(shape, pad_value, dtype=images.dtype)\n\n    for idx, image in enumerate(images):\n        i = idx % n_cols\n        j = idx // n_cols\n        img[j * (h + padding):j * (h + padding) + h,\n            i * (w + padding):i * (w + padding) + w, ...] = image\n\n    return img\n'"
pylib/__init__.py,0,b'from pylib.argument import *\nfrom pylib.processing import *\nfrom pylib.path import *\nfrom pylib.serialization import *\nfrom pylib.timer import *\n\nimport pprint\n\npp = pprint.pprint\n'
pylib/argument.py,0,"b'import argparse\nimport functools\nimport json\n\nfrom pylib import serialization\n\n\nGLOBAL_COMMAND_PARSER = argparse.ArgumentParser()\n\n\ndef _serialization_wrapper(func):\n    @functools.wraps(func)\n    def _wrapper(*args, **kwargs):\n        to_json = kwargs.pop(""to_json"", None)\n        to_yaml = kwargs.pop(""to_yaml"", None)\n        namespace = func(*args, **kwargs)\n        if to_json:\n            args_to_json(to_json, namespace)\n        if to_yaml:\n            args_to_yaml(to_yaml, namespace)\n        return namespace\n    return _wrapper\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected!\')\n\n\ndef argument(*args, **kwargs):\n    """"""Wrap argparse.add_argument.""""""\n    if \'type\'in kwargs:\n        if issubclass(kwargs[\'type\'], bool):\n            kwargs[\'type\'] = str2bool\n        elif issubclass(kwargs[\'type\'], dict):\n            kwargs[\'type\'] = json.loads\n    return GLOBAL_COMMAND_PARSER.add_argument(*args, **kwargs)\n\n\narg = argument\n\n\n@_serialization_wrapper\ndef args(args=None, namespace=None):\n    """"""Parse args using the global parser.""""""\n    namespace = GLOBAL_COMMAND_PARSER.parse_args(args=args, namespace=namespace)\n    return namespace\n\n\n@_serialization_wrapper\ndef args_from_xxx(obj, parser, check=True):\n    """"""Load args from xxx ignoring type and choices with default still valid.\n\n    Parameters\n    ----------\n    parser: function\n        Should return a dict.\n\n    """"""\n    dict_ = parser(obj)\n    namespace = argparse.ArgumentParser().parse_args(args=\'\')  # \'\' for not to accept command line args\n    for k, v in dict_.items():\n        namespace.__setattr__(k, v)\n    return namespace\n\n\nargs_from_dict = functools.partial(args_from_xxx, parser=lambda x: x)\nargs_from_json = functools.partial(args_from_xxx, parser=serialization.load_json)\nargs_from_yaml = functools.partial(args_from_xxx, parser=serialization.load_yaml)\n\n\ndef args_to_json(path, namespace, **kwagrs):\n    serialization.save_json(path, vars(namespace), **kwagrs)\n\n\ndef args_to_yaml(path, namespace, **kwagrs):\n    serialization.save_yaml(path, vars(namespace), **kwagrs)\n'"
pylib/path.py,0,"b'import datetime\nimport fnmatch\nimport os\nimport glob as _glob\nimport sys\n\n\ndef add_path(paths):\n    if not isinstance(paths, (list, tuple)):\n        paths = [paths]\n    for path in paths:\n        if path not in sys.path:\n            sys.path.insert(0, path)\n\n\ndef mkdir(paths):\n    if not isinstance(paths, (list, tuple)):\n        paths = [paths]\n    for path in paths:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n\ndef split(path):\n    """"""Return dir, name, ext.""""""\n    dir, name_ext = os.path.split(path)\n    name, ext = os.path.splitext(name_ext)\n    return dir, name, ext\n\n\ndef directory(path):\n    return split(path)[0]\n\n\ndef name(path):\n    return split(path)[1]\n\n\ndef ext(path):\n    return split(path)[2]\n\n\ndef name_ext(path):\n    return \'\'.join(split(path)[1:])\n\n\ndef change_ext(path, ext):\n    if ext[0] == \'.\':\n        ext = ext[1:]\n    return os.path.splitext(path)[0] + \'.\' + ext\n\n\nasbpath = os.path.abspath\n\n\njoin = os.path.join\n\n\ndef prefix(path, prefixes, sep=\'-\'):\n    prefixes = prefixes if isinstance(prefixes, (list, tuple)) else [prefixes]\n    dir, name, ext = split(path)\n    return join(dir, sep.join(prefixes) + sep + name + ext)\n\n\ndef suffix(path, suffixes, sep=\'-\'):\n    suffixes = suffixes if isinstance(suffixes, (list, tuple)) else [suffixes]\n    dir, name, ext = split(path)\n    return join(dir, name + sep + sep.join(suffixes) + ext)\n\n\ndef prefix_now(path, fmt=""%Y-%m-%d-%H:%M:%S"", sep=\'-\'):\n    return prefix(path, prefixes=datetime.datetime.now().strftime(fmt), sep=sep)\n\n\ndef suffix_now(path, fmt=""%Y-%m-%d-%H:%M:%S"", sep=\'-\'):\n    return suffix(path, suffixes=datetime.datetime.now().strftime(fmt), sep=sep)\n\n\ndef glob(dir, pats, recursive=False):  # faster than match, python3 only\n    pats = pats if isinstance(pats, (list, tuple)) else [pats]\n    matches = []\n    for pat in pats:\n        matches += _glob.glob(os.path.join(dir, pat), recursive=recursive)\n    return matches\n\n\ndef match(dir, pats, recursive=False):  # slow\n    pats = pats if isinstance(pats, (list, tuple)) else [pats]\n\n    iterator = list(os.walk(dir))\n    if not recursive:\n        iterator = iterator[0:1]\n\n    matches = []\n    for pat in pats:\n        for root, _, file_names in iterator:\n            for file_name in fnmatch.filter(file_names, pat):\n                matches.append(os.path.join(root, file_name))\n\n    return matches\n'"
pylib/processing.py,0,"b""import concurrent.futures\nimport functools\nimport multiprocessing\n\n\ndef run_parallels(work_fn, iterable, max_workers=None, chunksize=1, processing_bar=True, backend_executor=multiprocessing.Pool, debug=False):\n    if not debug:\n        with backend_executor(max_workers) as executor:\n            try:\n                works = executor.imap(work_fn, iterable, chunksize=chunksize)  # for multiprocessing.Pool\n            except:\n                works = executor.map(work_fn, iterable, chunksize=chunksize)\n\n            if processing_bar:\n                try:\n                    import tqdm\n                    try:\n                        total = len(iterable)\n                    except:\n                        total = None\n                    works = tqdm.tqdm(works, total=total)\n                except ImportError:\n                    print('`import tqdm` fails! Run without processing bar!')\n\n            results = list(works)\n    else:\n        results = [work_fn(i) for i in iterable]\n    return results\n\nrun_parallels_mp = run_parallels\nrun_parallels_cfprocess = functools.partial(run_parallels, backend_executor=concurrent.futures.ProcessPoolExecutor)\nrun_parallels_cfthread = functools.partial(run_parallels, backend_executor=concurrent.futures.ThreadPoolExecutor)\n\n\nif __name__ == '__main__':\n    import time\n\n    def work(i):\n        time.sleep(0.0001)\n        i**i\n        return i\n\n    t = time.time()\n    results = run_parallels_mp(work, range(10000), max_workers=2, chunksize=1, processing_bar=True, debug=False)\n    for i in results:\n        print(i)\n    print(time.time() - t)\n"""
pylib/serialization.py,0,"b""import json\nimport os\nimport pickle\n\n\ndef _check_ext(path, default_ext):\n    name, ext = os.path.splitext(path)\n    if ext == '':\n        if default_ext[0] == '.':\n            default_ext = default_ext[1:]\n        path = name + '.' + default_ext\n    return path\n\n\ndef save_json(path, obj, **kwargs):\n    # default\n    if 'indent' not in kwargs:\n        kwargs['indent'] = 4\n    if 'separators' not in kwargs:\n        kwargs['separators'] = (',', ': ')\n\n    path = _check_ext(path, 'json')\n\n    # wrap json.dump\n    with open(path, 'w') as f:\n        json.dump(obj, f, **kwargs)\n\n\ndef load_json(path, **kwargs):\n    # wrap json.load\n    with open(path) as f:\n        return json.load(f, **kwargs)\n\n\ndef save_yaml(path, data, **kwargs):\n    import oyaml as yaml\n\n    path = _check_ext(path, 'yml')\n\n    with open(path, 'w') as f:\n        yaml.dump(data, f, **kwargs)\n\n\ndef load_yaml(path, **kwargs):\n    import oyaml as yaml\n    with open(path) as f:\n        return yaml.load(f, **kwargs)\n\n\ndef save_pickle(path, obj, **kwargs):\n\n    path = _check_ext(path, 'pkl')\n\n    # wrap pickle.dump\n    with open(path, 'wb') as f:\n        pickle.dump(obj, f, **kwargs)\n\n\ndef load_pickle(path, **kwargs):\n    # wrap pickle.load\n    with open(path, 'rb') as f:\n        return pickle.load(f, **kwargs)\n"""
pylib/timer.py,0,"b'import datetime\nimport timeit\n\n\nclass Timer:  # deprecated, use tqdm instead\n    """"""A timer as a context manager.\n\n    Wraps around a timer. A custom timer can be passed\n    to the constructor. The default timer is timeit.default_timer.\n\n    Note that the latter measures wall clock time, not CPU time!\n    On Unix systems, it corresponds to time.time.\n    On Windows systems, it corresponds to time.clock.\n\n    Parameters\n    ----------\n    print_at_exit : boolean\n        If True, print when exiting context.\n    format : str\n        `ms`, `s` or `datetime`.\n\n    References\n    ----------\n    - https://github.com/brouberol/contexttimer/blob/master/contexttimer/__init__.py.\n\n\n    """"""\n\n    def __init__(self, fmt=\'s\', print_at_exit=True, timer=timeit.default_timer):\n        assert fmt in [\'ms\', \'s\', \'datetime\'], ""`fmt` should be \'ms\', \'s\' or \'datetime\'!""\n        self._fmt = fmt\n        self._print_at_exit = print_at_exit\n        self._timer = timer\n        self.start()\n\n    def __enter__(self):\n        """"""Start the timer in the context manager scope.""""""\n        self.restart()\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        """"""Print the end time.""""""\n        if self._print_at_exit:\n            print(str(self))\n\n    def __str__(self):\n        return self.fmt(self.elapsed)[1]\n\n    def start(self):\n        self.start_time = self._timer()\n\n    restart = start\n\n    @property\n    def elapsed(self):\n        """"""Return the current elapsed time since last (re)start.""""""\n        return self._timer() - self.start_time\n\n    def fmt(self, second):\n        if self._fmt == \'ms\':\n            time_fmt = second * 1000\n            time_str = \'%s %s\' % (time_fmt, self._fmt)\n        elif self._fmt == \'s\':\n            time_fmt = second\n            time_str = \'%s %s\' % (time_fmt, self._fmt)\n        elif self._fmt == \'datetime\':\n            time_fmt = datetime.timedelta(seconds=second)\n            time_str = str(time_fmt)\n        return time_fmt, time_str\n\n\ndef timeit(run_times=1, **timer_kwargs):\n    """"""Function decorator displaying the function execution time.\n\n    All kwargs are the arguments taken by the Timer class constructor.\n\n    """"""\n    # store Timer kwargs in local variable so the namespace isn\'t polluted\n    # by different level args and kwargs\n\n    def decorator(f):\n        def wrapper(*args, **kwargs):\n            timer_kwargs.update(print_at_exit=False)\n            with Timer(**timer_kwargs) as t:\n                for _ in range(run_times):\n                    out = f(*args, **kwargs)\n            fmt = \'[*] Execution time of function ""%(function_name)s"" for %(run_times)d runs is %(execution_time)s = %(execution_time_each)s * %(run_times)d [*]\'\n            context = {\'function_name\': f.__name__, \'run_times\': run_times, \'execution_time\': t, \'execution_time_each\': t.fmt(t.elapsed / run_times)[1]}\n            print(fmt % context)\n            return out\n        return wrapper\n\n    return decorator\n\n\nif __name__ == ""__main__"":\n    import time\n\n    # 1\n    print(1)\n    with Timer() as t:\n        time.sleep(1)\n        print(t)\n        time.sleep(1)\n\n    with Timer(fmt=\'datetime\') as t:\n        time.sleep(1)\n\n    # 2\n    print(2)\n    t = Timer(fmt=\'ms\')\n    time.sleep(2)\n    print(t)\n\n    t = Timer(fmt=\'datetime\')\n    time.sleep(1)\n    print(t)\n\n    # 3\n    print(3)\n\n    @timeit(run_times=5, fmt=\'s\')\n    def blah():\n        time.sleep(2)\n\n    blah()\n'"
tf2gan/__init__.py,0,b'from tf2gan.loss import *\n'
tf2gan/loss.py,25,"b""import tensorflow as tf\n\n\ndef get_gan_losses_fn():\n    bce = tf.losses.BinaryCrossentropy(from_logits=True)\n\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = bce(tf.ones_like(r_logit), r_logit)\n        f_loss = bce(tf.zeros_like(f_logit), f_logit)\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = bce(tf.ones_like(f_logit), f_logit)\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_hinge_v1_losses_fn():\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = tf.reduce_mean(tf.maximum(1 - r_logit, 0))\n        f_loss = tf.reduce_mean(tf.maximum(1 + f_logit, 0))\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = tf.reduce_mean(tf.maximum(1 - f_logit, 0))\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_hinge_v2_losses_fn():\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = tf.reduce_mean(tf.maximum(1 - r_logit, 0))\n        f_loss = tf.reduce_mean(tf.maximum(1 + f_logit, 0))\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = tf.reduce_mean(- f_logit)\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_lsgan_losses_fn():\n    mse = tf.losses.MeanSquaredError()\n\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = mse(tf.ones_like(r_logit), r_logit)\n        f_loss = mse(tf.zeros_like(f_logit), f_logit)\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = mse(tf.ones_like(f_logit), f_logit)\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_wgan_losses_fn():\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = - tf.reduce_mean(r_logit)\n        f_loss = tf.reduce_mean(f_logit)\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = - tf.reduce_mean(f_logit)\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_adversarial_losses_fn(mode):\n    if mode == 'gan':\n        return get_gan_losses_fn()\n    elif mode == 'hinge_v1':\n        return get_hinge_v1_losses_fn()\n    elif mode == 'hinge_v2':\n        return get_hinge_v2_losses_fn()\n    elif mode == 'lsgan':\n        return get_lsgan_losses_fn()\n    elif mode == 'wgan':\n        return get_wgan_losses_fn()\n\n\ndef gradient_penalty(f, real, fake, mode):\n    def _gradient_penalty(f, real, fake=None):\n        def _interpolate(a, b=None):\n            if b is None:   # interpolation in DRAGAN\n                beta = tf.random.uniform(shape=tf.shape(a), minval=0., maxval=1.)\n                b = a + 0.5 * tf.math.reduce_std(a) * beta\n            shape = [tf.shape(a)[0]] + [1] * (a.shape.ndims - 1)\n            alpha = tf.random.uniform(shape=shape, minval=0., maxval=1.)\n            inter = a + alpha * (b - a)\n            inter.set_shape(a.shape)\n            return inter\n\n        x = _interpolate(real, fake)\n        with tf.GradientTape() as t:\n            t.watch(x)\n            pred = f(x)\n        grad = t.gradient(pred, x)\n        norm = tf.norm(tf.reshape(grad, [tf.shape(grad)[0], -1]), axis=1)\n        gp = tf.reduce_mean((norm - 1.)**2)\n\n        return gp\n\n    if mode == 'none':\n        gp = tf.constant(0, dtype=real.dtype)\n    elif mode == 'dragan':\n        gp = _gradient_penalty(f, real)\n    elif mode == 'wgan-gp':\n        gp = _gradient_penalty(f, real, fake)\n\n    return gp\n"""
tf2lib/__init__.py,2,"b""import tensorflow as tf\n\nfrom tf2lib.data import *\nfrom tf2lib.image import *\nfrom tf2lib.ops import *\nfrom tf2lib.utils import *\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nfor d in physical_devices:\n    tf.config.experimental.set_memory_growth(d, True)\n"""
tf2lib/data/__init__.py,0,b'from tf2lib.data.dataset import *\n'
tf2lib/data/dataset.py,3,"b'import multiprocessing\n\nimport tensorflow as tf\n\n\ndef batch_dataset(dataset,\n                  batch_size,\n                  drop_remainder=True,\n                  n_prefetch_batch=1,\n                  filter_fn=None,\n                  map_fn=None,\n                  n_map_threads=None,\n                  filter_after_map=False,\n                  shuffle=True,\n                  shuffle_buffer_size=None,\n                  repeat=None):\n    # set defaults\n    if n_map_threads is None:\n        n_map_threads = multiprocessing.cpu_count()\n    if shuffle and shuffle_buffer_size is None:\n        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n\n    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n    if shuffle:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n\n    if not filter_after_map:\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n        if map_fn:\n            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n\n    else:  # [*] this is slower\n        if map_fn:\n            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n\n    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n\n    return dataset\n\n\ndef memory_data_batch_dataset(memory_data,\n                              batch_size,\n                              drop_remainder=True,\n                              n_prefetch_batch=1,\n                              filter_fn=None,\n                              map_fn=None,\n                              n_map_threads=None,\n                              filter_after_map=False,\n                              shuffle=True,\n                              shuffle_buffer_size=None,\n                              repeat=None):\n    """"""Batch dataset of memory data.\n\n    Parameters\n    ----------\n    memory_data : nested structure of tensors/ndarrays/lists\n\n    """"""\n    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n    dataset = batch_dataset(dataset,\n                            batch_size,\n                            drop_remainder=drop_remainder,\n                            n_prefetch_batch=n_prefetch_batch,\n                            filter_fn=filter_fn,\n                            map_fn=map_fn,\n                            n_map_threads=n_map_threads,\n                            filter_after_map=filter_after_map,\n                            shuffle=shuffle,\n                            shuffle_buffer_size=shuffle_buffer_size,\n                            repeat=repeat)\n    return dataset\n\n\ndef disk_image_batch_dataset(img_paths,\n                             batch_size,\n                             labels=None,\n                             drop_remainder=True,\n                             n_prefetch_batch=1,\n                             filter_fn=None,\n                             map_fn=None,\n                             n_map_threads=None,\n                             filter_after_map=False,\n                             shuffle=True,\n                             shuffle_buffer_size=None,\n                             repeat=None):\n    """"""Batch dataset of disk image for PNG and JPEG.\n\n    Parameters\n    ----------\n    img_paths : 1d-tensor/ndarray/list of str\n    labels : nested structure of tensors/ndarrays/lists\n\n    """"""\n    if labels is None:\n        memory_data = img_paths\n    else:\n        memory_data = (img_paths, labels)\n\n    def parse_fn(path, *label):\n        img = tf.io.read_file(path)\n        img = tf.image.decode_png(img, 3)  # fix channels to 3\n        return (img,) + label\n\n    if map_fn:  # fuse `map_fn` and `parse_fn`\n        def map_fn_(*args):\n            return map_fn(*parse_fn(*args))\n    else:\n        map_fn_ = parse_fn\n\n    dataset = memory_data_batch_dataset(memory_data,\n                                        batch_size,\n                                        drop_remainder=drop_remainder,\n                                        n_prefetch_batch=n_prefetch_batch,\n                                        filter_fn=filter_fn,\n                                        map_fn=map_fn_,\n                                        n_map_threads=n_map_threads,\n                                        filter_after_map=filter_after_map,\n                                        shuffle=shuffle,\n                                        shuffle_buffer_size=shuffle_buffer_size,\n                                        repeat=repeat)\n\n    return dataset\n'"
tf2lib/image/__init__.py,0,b'from tf2lib.image.image import *\n'
tf2lib/image/image.py,15,"b'import functools\nimport math\nimport random\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n\n@tf.function\ndef center_crop(image, size):\n    # for image of shape [batch, height, width, channels] or [height, width, channels]\n    if not isinstance(size, (tuple, list)):\n        size = [size, size]\n    offset_height = (tf.shape(image)[-3] - size[0]) // 2\n    offset_width = (tf.shape(image)[-2] - size[1]) // 2\n    return tf.image.crop_to_bounding_box(image, offset_height, offset_width, size[0], size[1])\n\n\n@tf.function\ndef color_jitter(image, brightness=0, contrast=0, saturation=0, hue=0):\n    """"""Color jitter.\n\n    Examples\n    --------\n    >>> color_jitter(img, 25, 0.2, 0.2, 0.1)\n\n    """"""\n    tforms = []\n    if brightness > 0:\n        tforms.append(functools.partial(tf.image.random_brightness, max_delta=brightness))\n    if contrast > 0:\n        tforms.append(functools.partial(tf.image.random_contrast, lower=max(0, 1 - contrast), upper=1 + contrast))\n    if saturation > 0:\n        tforms.append(functools.partial(tf.image.random_saturation, lower=max(0, 1 - saturation), upper=1 + saturation))\n    if hue > 0:\n        tforms.append(functools.partial(tf.image.random_hue, max_delta=hue))\n\n    random.shuffle(tforms)\n    for tform in tforms:\n        image = tform(image)\n\n    return image\n\n\n@tf.function\ndef random_grayscale(image, p=0.1):\n    return tf.cond(pred=tf.random.uniform(()) < p,\n                   true_fn=lambda: tf.image.adjust_saturation(image, 0),\n                   false_fn=lambda: image)\n\n\n@tf.function\ndef random_rotate(images, max_degrees, interpolation=\'BILINEAR\'):\n    # Randomly rotate image(s) counterclockwise by the angle(s) uniformly chosen from [-max_degree(s), max_degree(s)].\n    max_degrees = tf.convert_to_tensor(max_degrees, dtype=tf.float32)\n    angles = tf.random.uniform(tf.shape(max_degrees), minval=-1.0, maxval=1.0) * max_degrees / 180.0 * math.pi\n    return tfa.image.rotate(images, angles, interpolation=interpolation)\n'"
tf2lib/ops/__init__.py,0,b'from tf2lib.ops.ops import *\n'
tf2lib/ops/ops.py,9,"b'import tensorflow as tf\n\n\n@tf.function\ndef minmax_norm(x, epsilon=1e-12):\n    x = tf.cast(x, tf.float32)\n    min_val = tf.reduce_min(x)\n    max_val = tf.reduce_max(x)\n    norm_x = (x - min_val) / tf.maximum((max_val - min_val), epsilon)\n    return norm_x\n\n\n@tf.function\ndef reshape(x, shape):\n    x = tf.convert_to_tensor(x)\n    shape = [x.shape[i] if shape[i] == 0 else shape[i] for i in range(len(shape))]  # TODO(Lynn): is it slow here?\n    shape = [tf.shape(x)[i] if shape[i] is None else shape[i] for i in range(len(shape))]\n    return tf.reshape(x, shape)\n'"
tf2lib/utils/__init__.py,0,b'from tf2lib.utils.utils import *\n'
tf2lib/utils/utils.py,13,"b'import tensorflow as tf\n\n\nclass Checkpoint:\n    """"""Enhanced ""tf.train.Checkpoint"".""""""\n\n    def __init__(self,\n                 checkpoint_kwargs,  # for ""tf.train.Checkpoint""\n                 directory,  # for ""tf.train.CheckpointManager""\n                 max_to_keep=5,\n                 keep_checkpoint_every_n_hours=None):\n        self.checkpoint = tf.train.Checkpoint(**checkpoint_kwargs)\n        self.manager = tf.train.CheckpointManager(self.checkpoint, directory, max_to_keep, keep_checkpoint_every_n_hours)\n\n    def restore(self, save_path=None):\n        save_path = self.manager.latest_checkpoint if save_path is None else save_path\n        return self.checkpoint.restore(save_path)\n\n    def save(self, file_prefix_or_checkpoint_number=None, session=None):\n        if isinstance(file_prefix_or_checkpoint_number, str):\n            return self.checkpoint.save(file_prefix_or_checkpoint_number, session=session)\n        else:\n            return self.manager.save(checkpoint_number=file_prefix_or_checkpoint_number)\n\n    def __getattr__(self, attr):\n        if hasattr(self.checkpoint, attr):\n            return getattr(self.checkpoint, attr)\n        elif hasattr(self.manager, attr):\n            return getattr(self.manager, attr)\n        else:\n            self.__getattribute__(attr)  # this will raise an exception\n\n\ndef summary(name_data_dict,\n            step=None,\n            types=[\'mean\', \'std\', \'max\', \'min\', \'sparsity\', \'histogram\'],\n            historgram_buckets=None,\n            name=\'summary\'):\n    """"""Summary.\n\n    Examples\n    --------\n    >>> summary({\'a\': data_a, \'b\': data_b})\n\n    """"""\n    def _summary(name, data):\n        if data.shape == ():\n            tf.summary.scalar(name, data, step=step)\n        else:\n            if \'mean\' in types:\n                tf.summary.scalar(name + \'-mean\', tf.math.reduce_mean(data), step=step)\n            if \'std\' in types:\n                tf.summary.scalar(name + \'-std\', tf.math.reduce_std(data), step=step)\n            if \'max\' in types:\n                tf.summary.scalar(name + \'-max\', tf.math.reduce_max(data), step=step)\n            if \'min\' in types:\n                tf.summary.scalar(name + \'-min\', tf.math.reduce_min(data), step=step)\n            if \'sparsity\' in types:\n                tf.summary.scalar(name + \'-sparsity\', tf.math.zero_fraction(data), step=step)\n            if \'histogram\' in types:\n                tf.summary.histogram(name, data, step=step, buckets=historgram_buckets)\n\n    with tf.name_scope(name):\n        for name, data in name_data_dict.items():\n            _summary(name, data)\n'"
