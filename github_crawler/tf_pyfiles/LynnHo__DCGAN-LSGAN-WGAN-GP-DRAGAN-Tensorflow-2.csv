file_path,api_count,code
data.py,16,"b""import tensorflow as tf\nimport tf2lib as tl\n\n\n# ==============================================================================\n# =                                  datasets                                  =\n# ==============================================================================\n\ndef make_32x32_dataset(dataset, batch_size, drop_remainder=True, shuffle=True, repeat=1):\n    if dataset == 'mnist':\n        (train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n        train_images.shape = train_images.shape + (1,)\n    elif dataset == 'fashion_mnist':\n        (train_images, _), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\n        train_images.shape = train_images.shape + (1,)\n    elif dataset == 'cifar10':\n        (train_images, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n    else:\n        raise NotImplementedError\n\n    @tf.function\n    def _map_fn(img):\n        img = tf.image.resize(img, [32, 32])\n        img = tf.clip_by_value(img, 0, 255)\n        img = img / 127.5 - 1\n        return img\n\n    dataset = tl.memory_data_batch_dataset(train_images,\n                                           batch_size,\n                                           drop_remainder=drop_remainder,\n                                           map_fn=_map_fn,\n                                           shuffle=shuffle,\n                                           repeat=repeat)\n    img_shape = (32, 32, train_images.shape[-1])\n    len_dataset = len(train_images) // batch_size\n\n    return dataset, img_shape, len_dataset\n\n\ndef make_celeba_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n    @tf.function\n    def _map_fn(img):\n        crop_size = 108\n        img = tf.image.crop_to_bounding_box(img, (218 - crop_size) // 2, (178 - crop_size) // 2, crop_size, crop_size)\n        img = tf.image.resize(img, [resize, resize])\n        img = tf.clip_by_value(img, 0, 255)\n        img = img / 127.5 - 1\n        return img\n\n    dataset = tl.disk_image_batch_dataset(img_paths,\n                                          batch_size,\n                                          drop_remainder=drop_remainder,\n                                          map_fn=_map_fn,\n                                          shuffle=shuffle,\n                                          repeat=repeat)\n    img_shape = (resize, resize, 3)\n    len_dataset = len(img_paths) // batch_size\n\n    return dataset, img_shape, len_dataset\n\n\ndef make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n    @tf.function\n    def _map_fn(img):\n        img = tf.image.resize(img, [resize, resize])\n        img = tf.clip_by_value(img, 0, 255)\n        img = img / 127.5 - 1\n        return img\n\n    dataset = tl.disk_image_batch_dataset(img_paths,\n                                          batch_size,\n                                          drop_remainder=drop_remainder,\n                                          map_fn=_map_fn,\n                                          shuffle=shuffle,\n                                          repeat=repeat)\n    img_shape = (resize, resize, 3)\n    len_dataset = len(img_paths) // batch_size\n\n    return dataset, img_shape, len_dataset\n\n\n# ==============================================================================\n# =                               custom dataset                               =\n# ==============================================================================\n\ndef make_custom_datset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n    @tf.function\n    def _map_fn(img):\n        # ======================================\n        # =               custom               =\n        # ======================================\n        img = ...  # custom preprocessings, should output img in [0.0, 255.0]\n        # ======================================\n        # =               custom               =\n        # ======================================\n        img = tf.image.resize(img, [resize, resize])\n        img = tf.clip_by_value(img, 0, 255)\n        img = img / 127.5 - 1\n        return img\n\n    dataset = tl.disk_image_batch_dataset(img_paths,\n                                          batch_size,\n                                          drop_remainder=drop_remainder,\n                                          map_fn=_map_fn,\n                                          shuffle=shuffle,\n                                          repeat=repeat)\n    img_shape = (resize, resize, 3)\n    len_dataset = len(img_paths) // batch_size\n\n    return dataset, img_shape, len_dataset\n"""
make_gif.py,0,"b""import imageio\nimport pylib as py\n\n\n# ==============================================================================\n# =                                   param                                    =\n# ==============================================================================\n\npy.arg('--save_path', default='pics/celeba_dragan.gif')\npy.arg('--img_dir', default='output/celeba_gan_dragan/samples_training')\npy.arg('--max_frames', type=int, default=0)\nargs = py.args()\n\npy.mkdir(py.directory(args.save_path))\n\n\n# ==============================================================================\n# =                                  make gif                                  =\n# ==============================================================================\n# modified from https://www.tensorflow.org/alpha/tutorials/generative/dcgan\n\nwith imageio.get_writer(args.save_path, mode='I', fps=8) as writer:\n    filenames = sorted(py.glob(args.img_dir, '*.jpg'))\n    if args.max_frames:\n        step = len(filenames) // args.max_frames\n    else:\n        step = 1\n    last = -1\n    for i, filename in enumerate(filenames[::step]):\n        frame = 2 * (i**0.3)\n        if round(frame) > round(last):\n            last = frame\n        else:\n            continue\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)\n"""
module.py,5,"b""import tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras as keras\n\n\n# ==============================================================================\n# =                                  networks                                  =\n# ==============================================================================\n\ndef _get_norm_layer(norm):\n    if norm == 'none':\n        return lambda: lambda x: x\n    elif norm == 'batch_norm':\n        return keras.layers.BatchNormalization\n    elif norm == 'instance_norm':\n        return tfa.layers.InstanceNormalization\n    elif norm == 'layer_norm':\n        return keras.layers.LayerNormalization\n\n\ndef ConvGenerator(input_shape=(1, 1, 128),\n                  output_channels=3,\n                  dim=64,\n                  n_upsamplings=4,\n                  norm='batch_norm',\n                  name='ConvGenerator'):\n    Norm = _get_norm_layer(norm)\n\n    # 0\n    h = inputs = keras.Input(shape=input_shape)\n\n    # 1: 1x1 -> 4x4\n    d = min(dim * 2 ** (n_upsamplings - 1), dim * 8)\n    h = keras.layers.Conv2DTranspose(d, 4, strides=1, padding='valid', use_bias=False)(h)\n    h = Norm()(h)\n    h = tf.nn.relu(h)  # or h = keras.layers.ReLU()(h)\n\n    # 2: upsamplings, 4x4 -> 8x8 -> 16x16 -> ...\n    for i in range(n_upsamplings - 1):\n        d = min(dim * 2 ** (n_upsamplings - 2 - i), dim * 8)\n        h = keras.layers.Conv2DTranspose(d, 4, strides=2, padding='same', use_bias=False)(h)\n        h = Norm()(h)\n        h = tf.nn.relu(h)  # or h = keras.layers.ReLU()(h)\n\n    h = keras.layers.Conv2DTranspose(output_channels, 4, strides=2, padding='same')(h)\n    h = tf.tanh(h)  # or h = keras.layers.Activation('tanh')(h)\n\n    return keras.Model(inputs=inputs, outputs=h, name=name)\n\n\ndef ConvDiscriminator(input_shape=(64, 64, 3),\n                      dim=64,\n                      n_downsamplings=4,\n                      norm='batch_norm',\n                      name='ConvDiscriminator'):\n    Norm = _get_norm_layer(norm)\n\n    # 0\n    h = inputs = keras.Input(shape=input_shape)\n\n    # 1: downsamplings, ... -> 16x16 -> 8x8 -> 4x4\n    h = keras.layers.Conv2D(dim, 4, strides=2, padding='same')(h)\n    h = tf.nn.leaky_relu(h, alpha=0.2)  # or keras.layers.LeakyReLU(alpha=0.2)(h)\n\n    for i in range(n_downsamplings - 1):\n        d = min(dim * 2 ** (i + 1), dim * 8)\n        h = keras.layers.Conv2D(d, 4, strides=2, padding='same', use_bias=False)(h)\n        h = Norm()(h)\n        h = tf.nn.leaky_relu(h, alpha=0.2)  # or h = keras.layers.LeakyReLU(alpha=0.2)(h)\n\n    # 2: logit\n    h = keras.layers.Conv2D(1, 4, strides=1, padding='valid')(h)\n\n    return keras.Model(inputs=inputs, outputs=h, name=name)\n"""
train.py,10,"b""import functools\n\nimport imlib as im\nimport pylib as py\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tf2lib as tl\nimport tf2gan as gan\nimport tqdm\n\nimport data\nimport module\n\n\n# ==============================================================================\n# =                                   param                                    =\n# ==============================================================================\n\n# command line\npy.arg('--dataset', default='fashion_mnist', choices=['cifar10', 'fashion_mnist', 'mnist', 'celeba', 'anime', 'custom'])\npy.arg('--batch_size', type=int, default=64)\npy.arg('--epochs', type=int, default=25)\npy.arg('--lr', type=float, default=0.0002)\npy.arg('--beta_1', type=float, default=0.5)\npy.arg('--n_d', type=int, default=1)  # # d updates per g update\npy.arg('--z_dim', type=int, default=128)\npy.arg('--adversarial_loss_mode', default='gan', choices=['gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan'])\npy.arg('--gradient_penalty_mode', default='none', choices=['none', 'dragan', 'wgan-gp'])\npy.arg('--gradient_penalty_weight', type=float, default=10.0)\npy.arg('--experiment_name', default='none')\nargs = py.args()\n\n# output_dir\nif args.experiment_name == 'none':\n    args.experiment_name = '%s_%s' % (args.dataset, args.adversarial_loss_mode)\n    if args.gradient_penalty_mode != 'none':\n        args.experiment_name += '_%s' % args.gradient_penalty_mode\noutput_dir = py.join('output', args.experiment_name)\npy.mkdir(output_dir)\n\n# save settings\npy.args_to_yaml(py.join(output_dir, 'settings.yml'), args)\n\n\n# ==============================================================================\n# =                                    data                                    =\n# ==============================================================================\n\n# setup dataset\nif args.dataset in ['cifar10', 'fashion_mnist', 'mnist']:  # 32x32\n    dataset, shape, len_dataset = data.make_32x32_dataset(args.dataset, args.batch_size)\n    n_G_upsamplings = n_D_downsamplings = 3\n\nelif args.dataset == 'celeba':  # 64x64\n    img_paths = py.glob('data/img_align_celeba', '*.jpg')\n    dataset, shape, len_dataset = data.make_celeba_dataset(img_paths, args.batch_size)\n    n_G_upsamplings = n_D_downsamplings = 4\n\nelif args.dataset == 'anime':  # 64x64\n    img_paths = py.glob('data/faces', '*.jpg')\n    dataset, shape, len_dataset = data.make_anime_dataset(img_paths, args.batch_size)\n    n_G_upsamplings = n_D_downsamplings = 4\n\nelif args.dataset == 'custom':\n    # ======================================\n    # =               custom               =\n    # ======================================\n    img_paths = ...  # image paths of custom dataset\n    dataset, shape, len_dataset = data.make_custom_dataset(img_paths, args.batch_size)\n    n_G_upsamplings = n_D_downsamplings = ...  # 3 for 32x32 and 4 for 64x64\n    # ======================================\n    # =               custom               =\n    # ======================================\n\n\n# ==============================================================================\n# =                                   model                                    =\n# ==============================================================================\n\n# setup the normalization function for discriminator\nif args.gradient_penalty_mode == 'none':\n    d_norm = 'batch_norm'\nelif args.gradient_penalty_mode in ['dragan', 'wgan-gp']:  # cannot use batch normalization with gradient penalty\n    # TODO(Lynn)\n    # Layer normalization is more stable than instance normalization here,\n    # but instance normalization works in other implementations.\n    # Please tell me if you find out the cause.\n    d_norm = 'layer_norm'\n\n# networks\nG = module.ConvGenerator(input_shape=(1, 1, args.z_dim), output_channels=shape[-1], n_upsamplings=n_G_upsamplings, name='G_%s' % args.dataset)\nD = module.ConvDiscriminator(input_shape=shape, n_downsamplings=n_D_downsamplings, norm=d_norm, name='D_%s' % args.dataset)\nG.summary()\nD.summary()\n\n# adversarial_loss_functions\nd_loss_fn, g_loss_fn = gan.get_adversarial_losses_fn(args.adversarial_loss_mode)\n\nG_optimizer = keras.optimizers.Adam(learning_rate=args.lr, beta_1=args.beta_1)\nD_optimizer = keras.optimizers.Adam(learning_rate=args.lr, beta_1=args.beta_1)\n\n\n# ==============================================================================\n# =                                 train step                                 =\n# ==============================================================================\n\n@tf.function\ndef train_G():\n    with tf.GradientTape() as t:\n        z = tf.random.normal(shape=(args.batch_size, 1, 1, args.z_dim))\n        x_fake = G(z, training=True)\n        x_fake_d_logit = D(x_fake, training=True)\n        G_loss = g_loss_fn(x_fake_d_logit)\n\n    G_grad = t.gradient(G_loss, G.trainable_variables)\n    G_optimizer.apply_gradients(zip(G_grad, G.trainable_variables))\n\n    return {'g_loss': G_loss}\n\n\n@tf.function\ndef train_D(x_real):\n    with tf.GradientTape() as t:\n        z = tf.random.normal(shape=(args.batch_size, 1, 1, args.z_dim))\n        x_fake = G(z, training=True)\n\n        x_real_d_logit = D(x_real, training=True)\n        x_fake_d_logit = D(x_fake, training=True)\n\n        x_real_d_loss, x_fake_d_loss = d_loss_fn(x_real_d_logit, x_fake_d_logit)\n        gp = gan.gradient_penalty(functools.partial(D, training=True), x_real, x_fake, mode=args.gradient_penalty_mode)\n\n        D_loss = (x_real_d_loss + x_fake_d_loss) + gp * args.gradient_penalty_weight\n\n    D_grad = t.gradient(D_loss, D.trainable_variables)\n    D_optimizer.apply_gradients(zip(D_grad, D.trainable_variables))\n\n    return {'d_loss': x_real_d_loss + x_fake_d_loss, 'gp': gp}\n\n\n@tf.function\ndef sample(z):\n    return G(z, training=False)\n\n\n# ==============================================================================\n# =                                    run                                     =\n# ==============================================================================\n\n# epoch counter\nep_cnt = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)\n\n# checkpoint\ncheckpoint = tl.Checkpoint(dict(G=G,\n                                D=D,\n                                G_optimizer=G_optimizer,\n                                D_optimizer=D_optimizer,\n                                ep_cnt=ep_cnt),\n                           py.join(output_dir, 'checkpoints'),\n                           max_to_keep=5)\ntry:  # restore checkpoint including the epoch counter\n    checkpoint.restore().assert_existing_objects_matched()\nexcept Exception as e:\n    print(e)\n\n# summary\ntrain_summary_writer = tf.summary.create_file_writer(py.join(output_dir, 'summaries', 'train'))\n\n# sample\nsample_dir = py.join(output_dir, 'samples_training')\npy.mkdir(sample_dir)\n\n# main loop\nz = tf.random.normal((100, 1, 1, args.z_dim))  # a fixed noise for sampling\nwith train_summary_writer.as_default():\n    for ep in tqdm.trange(args.epochs, desc='Epoch Loop'):\n        if ep < ep_cnt:\n            continue\n\n        # update epoch counter\n        ep_cnt.assign_add(1)\n\n        # train for an epoch\n        for x_real in tqdm.tqdm(dataset, desc='Inner Epoch Loop', total=len_dataset):\n            D_loss_dict = train_D(x_real)\n            tl.summary(D_loss_dict, step=D_optimizer.iterations, name='D_losses')\n\n            if D_optimizer.iterations.numpy() % args.n_d == 0:\n                G_loss_dict = train_G()\n                tl.summary(G_loss_dict, step=G_optimizer.iterations, name='G_losses')\n\n            # sample\n            if G_optimizer.iterations.numpy() % 100 == 0:\n                x_fake = sample(z)\n                img = im.immerge(x_fake, n_rows=10).squeeze()\n                im.imwrite(img, py.join(sample_dir, 'iter-%09d.jpg' % G_optimizer.iterations.numpy()))\n\n        # save checkpoint\n        checkpoint.save(ep)\n"""
imlib/__init__.py,0,b'from imlib.basic import *\nfrom imlib.dtype import *\nfrom imlib.transform import *\n'
imlib/basic.py,0,"b'import numpy as np\nimport skimage.io as iio\n\nfrom imlib import dtype\n\n\ndef imread(path, as_gray=False, **kwargs):\n    """"""Return a float64 image in [-1.0, 1.0].""""""\n    image = iio.imread(path, as_gray, **kwargs)\n    if image.dtype == np.uint8:\n        image = image / 127.5 - 1\n    elif image.dtype == np.uint16:\n        image = image / 32767.5 - 1\n    elif image.dtype in [np.float32, np.float64]:\n        image = image * 2 - 1.0\n    else:\n        raise Exception(""Inavailable image dtype: %s!"" % image.dtype)\n    return image\n\n\ndef imwrite(image, path, quality=95, **plugin_args):\n    """"""Save a [-1.0, 1.0] image.""""""\n    iio.imsave(path, dtype.im2uint(image), quality=quality, **plugin_args)\n\n\ndef imshow(image):\n    """"""Show a [-1.0, 1.0] image.""""""\n    iio.imshow(dtype.im2uint(image))\n\n\nshow = iio.show\n'"
imlib/dtype.py,0,"b'import numpy as np\n\n\ndef _check(images, dtypes, min_value=-np.inf, max_value=np.inf):\n    # check type\n    assert isinstance(images, np.ndarray), \'`images` should be np.ndarray!\'\n\n    # check dtype\n    dtypes = dtypes if isinstance(dtypes, (list, tuple)) else [dtypes]\n    assert images.dtype in dtypes, \'dtype of `images` shoud be one of %s!\' % dtypes\n\n    # check nan and inf\n    assert np.all(np.isfinite(images)), \'`images` contains NaN or Inf!\'\n\n    # check value\n    if min_value not in [None, -np.inf]:\n        l = \'[\' + str(min_value)\n    else:\n        l = \'(-inf\'\n        min_value = -np.inf\n    if max_value not in [None, np.inf]:\n        r = str(max_value) + \']\'\n    else:\n        r = \'inf)\'\n        max_value = np.inf\n    assert np.min(images) >= min_value and np.max(images) <= max_value, \\\n        \'`images` should be in the range of %s!\' % (l + \',\' + r)\n\n\ndef to_range(images, min_value=0.0, max_value=1.0, dtype=None):\n    """"""Transform images from [-1.0, 1.0] to [min_value, max_value] of dtype.""""""\n    _check(images, [np.float32, np.float64], -1.0, 1.0)\n    dtype = dtype if dtype else images.dtype\n    return ((images + 1.) / 2. * (max_value - min_value) + min_value).astype(dtype)\n\n\ndef float2im(images):\n    """"""Transform images from [0, 1.0] to [-1.0, 1.0].""""""\n    _check(images, [np.float32, np.float64], 0.0, 1.0)\n    return images * 2 - 1.0\n\n\ndef float2uint(images):\n    """"""Transform images from [0, 1.0] to uint8.""""""\n    _check(images, [np.float32, np.float64], -0.0, 1.0)\n    return (images * 255).astype(np.uint8)\n\n\ndef im2uint(images):\n    """"""Transform images from [-1.0, 1.0] to uint8.""""""\n    return to_range(images, 0, 255, np.uint8)\n\n\ndef im2float(images):\n    """"""Transform images from [-1.0, 1.0] to [0.0, 1.0].""""""\n    return to_range(images, 0.0, 1.0)\n\n\ndef uint2im(images):\n    """"""Transform images from uint8 to [-1.0, 1.0] of float64.""""""\n    _check(images, np.uint8)\n    return images / 127.5 - 1.0\n\n\ndef uint2float(images):\n    """"""Transform images from uint8 to [0.0, 1.0] of float64.""""""\n    _check(images, np.uint8)\n    return images / 255.0\n\n\ndef cv2im(images):\n    """"""Transform opencv images to [-1.0, 1.0].""""""\n    images = uint2im(images)\n    return images[..., ::-1]\n\n\ndef im2cv(images):\n    """"""Transform images from [-1.0, 1.0] to opencv images.""""""\n    images = im2uint(images)\n    return images[..., ::-1]\n'"
imlib/transform.py,0,"b'import numpy as np\nimport skimage.color as color\nimport skimage.transform as transform\n\n\nrgb2gray = color.rgb2gray\ngray2rgb = color.gray2rgb\n\nimresize = transform.resize\nimrescale = transform.rescale\n\n\ndef immerge(images, n_rows=None, n_cols=None, padding=0, pad_value=0):\n    """"""Merge images to an image with (n_rows * h) * (n_cols * w).\n\n    Parameters\n    ----------\n    images : numpy.array or object which can be converted to numpy.array\n        Images in shape of N * H * W(* C=1 or 3).\n\n    """"""\n    images = np.array(images)\n    n = images.shape[0]\n    if n_rows:\n        n_rows = max(min(n_rows, n), 1)\n        n_cols = int(n - 0.5) // n_rows + 1\n    elif n_cols:\n        n_cols = max(min(n_cols, n), 1)\n        n_rows = int(n - 0.5) // n_cols + 1\n    else:\n        n_rows = int(n ** 0.5)\n        n_cols = int(n - 0.5) // n_rows + 1\n\n    h, w = images.shape[1], images.shape[2]\n    shape = (h * n_rows + padding * (n_rows - 1),\n             w * n_cols + padding * (n_cols - 1))\n    if images.ndim == 4:\n        shape += (images.shape[3],)\n    img = np.full(shape, pad_value, dtype=images.dtype)\n\n    for idx, image in enumerate(images):\n        i = idx % n_cols\n        j = idx // n_cols\n        img[j * (h + padding):j * (h + padding) + h,\n            i * (w + padding):i * (w + padding) + w, ...] = image\n\n    return img\n'"
pylib/__init__.py,0,b'from pylib.argument import *\nfrom pylib.processing import *\nfrom pylib.path import *\nfrom pylib.serialization import *\nfrom pylib.timer import *\n\nimport pprint\n\npp = pprint.pprint\n'
pylib/argument.py,0,"b'import argparse\nimport functools\nimport json\n\nfrom pylib import serialization\n\n\nGLOBAL_COMMAND_PARSER = argparse.ArgumentParser()\n\n\ndef _serialization_wrapper(func):\n    @functools.wraps(func)\n    def _wrapper(*args, **kwargs):\n        to_json = kwargs.pop(""to_json"", None)\n        to_yaml = kwargs.pop(""to_yaml"", None)\n        namespace = func(*args, **kwargs)\n        if to_json:\n            args_to_json(to_json, namespace)\n        if to_yaml:\n            args_to_yaml(to_yaml, namespace)\n        return namespace\n    return _wrapper\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected!\')\n\n\ndef argument(*args, **kwargs):\n    """"""Wrap argparse.add_argument.""""""\n    if \'type\'in kwargs:\n        if issubclass(kwargs[\'type\'], bool):\n            kwargs[\'type\'] = str2bool\n        elif issubclass(kwargs[\'type\'], dict):\n            kwargs[\'type\'] = json.loads\n    return GLOBAL_COMMAND_PARSER.add_argument(*args, **kwargs)\n\n\narg = argument\n\n\n@_serialization_wrapper\ndef args(args=None, namespace=None):\n    """"""Parse args using the global parser.""""""\n    namespace = GLOBAL_COMMAND_PARSER.parse_args(args=args, namespace=namespace)\n    return namespace\n\n\n@_serialization_wrapper\ndef args_from_xxx(obj, parser, check=True):\n    """"""Load args from xxx ignoring type and choices with default still valid.\n\n    Parameters\n    ----------\n    parser: function\n        Should return a dict.\n\n    """"""\n    dict_ = parser(obj)\n    namespace = argparse.ArgumentParser().parse_args(args=\'\')  # \'\' for not to accept command line args\n    for k, v in dict_.items():\n        namespace.__setattr__(k, v)\n    return namespace\n\n\nargs_from_dict = functools.partial(args_from_xxx, parser=lambda x: x)\nargs_from_json = functools.partial(args_from_xxx, parser=serialization.load_json)\nargs_from_yaml = functools.partial(args_from_xxx, parser=serialization.load_yaml)\n\n\ndef args_to_json(path, namespace, **kwagrs):\n    serialization.save_json(path, vars(namespace), **kwagrs)\n\n\ndef args_to_yaml(path, namespace, **kwagrs):\n    serialization.save_yaml(path, vars(namespace), **kwagrs)\n'"
pylib/path.py,0,"b'import datetime\nimport fnmatch\nimport os\nimport glob as _glob\nimport sys\n\n\ndef add_path(paths):\n    if not isinstance(paths, (list, tuple)):\n        paths = [paths]\n    for path in paths:\n        if path not in sys.path:\n            sys.path.insert(0, path)\n\n\ndef mkdir(paths):\n    if not isinstance(paths, (list, tuple)):\n        paths = [paths]\n    for path in paths:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n\ndef split(path):\n    """"""Return dir, name, ext.""""""\n    dir, name_ext = os.path.split(path)\n    name, ext = os.path.splitext(name_ext)\n    return dir, name, ext\n\n\ndef directory(path):\n    return split(path)[0]\n\n\ndef name(path):\n    return split(path)[1]\n\n\ndef ext(path):\n    return split(path)[2]\n\n\ndef name_ext(path):\n    return \'\'.join(split(path)[1:])\n\n\ndef change_ext(path, ext):\n    if ext[0] == \'.\':\n        ext = ext[1:]\n    return os.path.splitext(path)[0] + \'.\' + ext\n\n\nasbpath = os.path.abspath\n\n\njoin = os.path.join\n\n\ndef prefix(path, prefixes, sep=\'-\'):\n    prefixes = prefixes if isinstance(prefixes, (list, tuple)) else [prefixes]\n    dir, name, ext = split(path)\n    return join(dir, sep.join(prefixes) + sep + name + ext)\n\n\ndef suffix(path, suffixes, sep=\'-\'):\n    suffixes = suffixes if isinstance(suffixes, (list, tuple)) else [suffixes]\n    dir, name, ext = split(path)\n    return join(dir, name + sep + sep.join(suffixes) + ext)\n\n\ndef prefix_now(path, fmt=""%Y-%m-%d-%H:%M:%S"", sep=\'-\'):\n    return prefix(path, prefixes=datetime.datetime.now().strftime(fmt), sep=sep)\n\n\ndef suffix_now(path, fmt=""%Y-%m-%d-%H:%M:%S"", sep=\'-\'):\n    return suffix(path, suffixes=datetime.datetime.now().strftime(fmt), sep=sep)\n\n\ndef glob(dir, pats, recursive=False):  # faster than match, python3 only\n    pats = pats if isinstance(pats, (list, tuple)) else [pats]\n    matches = []\n    for pat in pats:\n        matches += _glob.glob(os.path.join(dir, pat), recursive=recursive)\n    return matches\n\n\ndef match(dir, pats, recursive=False):  # slow\n    pats = pats if isinstance(pats, (list, tuple)) else [pats]\n\n    iterator = list(os.walk(dir))\n    if not recursive:\n        iterator = iterator[0:1]\n\n    matches = []\n    for pat in pats:\n        for root, _, file_names in iterator:\n            for file_name in fnmatch.filter(file_names, pat):\n                matches.append(os.path.join(root, file_name))\n\n    return matches\n'"
pylib/processing.py,0,"b""import concurrent.futures\nimport functools\nimport multiprocessing\n\n\ndef run_parallels(work_fn, iterable, max_workers=None, chunksize=1, processing_bar=True, backend_executor=multiprocessing.Pool, debug=False):\n    if not debug:\n        with backend_executor(max_workers) as executor:\n            try:\n                works = executor.imap(work_fn, iterable, chunksize=chunksize)  # for multiprocessing.Pool\n            except:\n                works = executor.map(work_fn, iterable, chunksize=chunksize)\n\n            if processing_bar:\n                try:\n                    import tqdm\n                    try:\n                        total = len(iterable)\n                    except:\n                        total = None\n                    works = tqdm.tqdm(works, total=total)\n                except ImportError:\n                    print('`import tqdm` fails! Run without processing bar!')\n\n            results = list(works)\n    else:\n        results = [work_fn(i) for i in iterable]\n    return results\n\nrun_parallels_mp = run_parallels\nrun_parallels_cfprocess = functools.partial(run_parallels, backend_executor=concurrent.futures.ProcessPoolExecutor)\nrun_parallels_cfthread = functools.partial(run_parallels, backend_executor=concurrent.futures.ThreadPoolExecutor)\n\n\nif __name__ == '__main__':\n    import time\n\n    def work(i):\n        time.sleep(0.0001)\n        i**i\n        return i\n\n    t = time.time()\n    results = run_parallels_mp(work, range(10000), max_workers=2, chunksize=1, processing_bar=True, debug=False)\n    for i in results:\n        print(i)\n    print(time.time() - t)\n"""
pylib/serialization.py,0,"b""import json\nimport os\nimport pickle\n\n\ndef _check_ext(path, default_ext):\n    name, ext = os.path.splitext(path)\n    if ext == '':\n        if default_ext[0] == '.':\n            default_ext = default_ext[1:]\n        path = name + '.' + default_ext\n    return path\n\n\ndef save_json(path, obj, **kwargs):\n    # default\n    if 'indent' not in kwargs:\n        kwargs['indent'] = 4\n    if 'separators' not in kwargs:\n        kwargs['separators'] = (',', ': ')\n\n    path = _check_ext(path, 'json')\n\n    # wrap json.dump\n    with open(path, 'w') as f:\n        json.dump(obj, f, **kwargs)\n\n\ndef load_json(path, **kwargs):\n    # wrap json.load\n    with open(path) as f:\n        return json.load(f, **kwargs)\n\n\ndef save_yaml(path, data, **kwargs):\n    import oyaml as yaml\n\n    path = _check_ext(path, 'yml')\n\n    with open(path, 'w') as f:\n        yaml.dump(data, f, **kwargs)\n\n\ndef load_yaml(path, **kwargs):\n    import oyaml as yaml\n    with open(path) as f:\n        return yaml.load(f, **kwargs)\n\n\ndef save_pickle(path, obj, **kwargs):\n\n    path = _check_ext(path, 'pkl')\n\n    # wrap pickle.dump\n    with open(path, 'wb') as f:\n        pickle.dump(obj, f, **kwargs)\n\n\ndef load_pickle(path, **kwargs):\n    # wrap pickle.load\n    with open(path, 'rb') as f:\n        return pickle.load(f, **kwargs)\n"""
pylib/timer.py,0,"b'import datetime\nimport timeit\n\n\nclass Timer:  # deprecated, use tqdm instead\n    """"""A timer as a context manager.\n\n    Wraps around a timer. A custom timer can be passed\n    to the constructor. The default timer is timeit.default_timer.\n\n    Note that the latter measures wall clock time, not CPU time!\n    On Unix systems, it corresponds to time.time.\n    On Windows systems, it corresponds to time.clock.\n\n    Parameters\n    ----------\n    print_at_exit : boolean\n        If True, print when exiting context.\n    format : str\n        `ms`, `s` or `datetime`.\n\n    References\n    ----------\n    - https://github.com/brouberol/contexttimer/blob/master/contexttimer/__init__.py.\n\n\n    """"""\n\n    def __init__(self, fmt=\'s\', print_at_exit=True, timer=timeit.default_timer):\n        assert fmt in [\'ms\', \'s\', \'datetime\'], ""`fmt` should be \'ms\', \'s\' or \'datetime\'!""\n        self._fmt = fmt\n        self._print_at_exit = print_at_exit\n        self._timer = timer\n        self.start()\n\n    def __enter__(self):\n        """"""Start the timer in the context manager scope.""""""\n        self.restart()\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        """"""Print the end time.""""""\n        if self._print_at_exit:\n            print(str(self))\n\n    def __str__(self):\n        return self.fmt(self.elapsed)[1]\n\n    def start(self):\n        self.start_time = self._timer()\n\n    restart = start\n\n    @property\n    def elapsed(self):\n        """"""Return the current elapsed time since last (re)start.""""""\n        return self._timer() - self.start_time\n\n    def fmt(self, second):\n        if self._fmt == \'ms\':\n            time_fmt = second * 1000\n            time_str = \'%s %s\' % (time_fmt, self._fmt)\n        elif self._fmt == \'s\':\n            time_fmt = second\n            time_str = \'%s %s\' % (time_fmt, self._fmt)\n        elif self._fmt == \'datetime\':\n            time_fmt = datetime.timedelta(seconds=second)\n            time_str = str(time_fmt)\n        return time_fmt, time_str\n\n\ndef timeit(run_times=1, **timer_kwargs):\n    """"""Function decorator displaying the function execution time.\n\n    All kwargs are the arguments taken by the Timer class constructor.\n\n    """"""\n    # store Timer kwargs in local variable so the namespace isn\'t polluted\n    # by different level args and kwargs\n\n    def decorator(f):\n        def wrapper(*args, **kwargs):\n            timer_kwargs.update(print_at_exit=False)\n            with Timer(**timer_kwargs) as t:\n                for _ in range(run_times):\n                    out = f(*args, **kwargs)\n            fmt = \'[*] Execution time of function ""%(function_name)s"" for %(run_times)d runs is %(execution_time)s = %(execution_time_each)s * %(run_times)d [*]\'\n            context = {\'function_name\': f.__name__, \'run_times\': run_times, \'execution_time\': t, \'execution_time_each\': t.fmt(t.elapsed / run_times)[1]}\n            print(fmt % context)\n            return out\n        return wrapper\n\n    return decorator\n\n\nif __name__ == ""__main__"":\n    import time\n\n    # 1\n    print(1)\n    with Timer() as t:\n        time.sleep(1)\n        print(t)\n        time.sleep(1)\n\n    with Timer(fmt=\'datetime\') as t:\n        time.sleep(1)\n\n    # 2\n    print(2)\n    t = Timer(fmt=\'ms\')\n    time.sleep(2)\n    print(t)\n\n    t = Timer(fmt=\'datetime\')\n    time.sleep(1)\n    print(t)\n\n    # 3\n    print(3)\n\n    @timeit(run_times=5, fmt=\'s\')\n    def blah():\n        time.sleep(2)\n\n    blah()\n'"
tf2gan/__init__.py,0,b'from tf2gan.loss import *\n'
tf2gan/loss.py,25,"b""import tensorflow as tf\n\n\ndef get_gan_losses_fn():\n    bce = tf.losses.BinaryCrossentropy(from_logits=True)\n\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = bce(tf.ones_like(r_logit), r_logit)\n        f_loss = bce(tf.zeros_like(f_logit), f_logit)\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = bce(tf.ones_like(f_logit), f_logit)\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_hinge_v1_losses_fn():\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = tf.reduce_mean(tf.maximum(1 - r_logit, 0))\n        f_loss = tf.reduce_mean(tf.maximum(1 + f_logit, 0))\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = tf.reduce_mean(tf.maximum(1 - f_logit, 0))\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_hinge_v2_losses_fn():\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = tf.reduce_mean(tf.maximum(1 - r_logit, 0))\n        f_loss = tf.reduce_mean(tf.maximum(1 + f_logit, 0))\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = tf.reduce_mean(- f_logit)\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_lsgan_losses_fn():\n    mse = tf.losses.MeanSquaredError()\n\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = mse(tf.ones_like(r_logit), r_logit)\n        f_loss = mse(tf.zeros_like(f_logit), f_logit)\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = mse(tf.ones_like(f_logit), f_logit)\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_wgan_losses_fn():\n    def d_loss_fn(r_logit, f_logit):\n        r_loss = - tf.reduce_mean(r_logit)\n        f_loss = tf.reduce_mean(f_logit)\n        return r_loss, f_loss\n\n    def g_loss_fn(f_logit):\n        f_loss = - tf.reduce_mean(f_logit)\n        return f_loss\n\n    return d_loss_fn, g_loss_fn\n\n\ndef get_adversarial_losses_fn(mode):\n    if mode == 'gan':\n        return get_gan_losses_fn()\n    elif mode == 'hinge_v1':\n        return get_hinge_v1_losses_fn()\n    elif mode == 'hinge_v2':\n        return get_hinge_v2_losses_fn()\n    elif mode == 'lsgan':\n        return get_lsgan_losses_fn()\n    elif mode == 'wgan':\n        return get_wgan_losses_fn()\n\n\ndef gradient_penalty(f, real, fake, mode):\n    def _gradient_penalty(f, real, fake=None):\n        def _interpolate(a, b=None):\n            if b is None:   # interpolation in DRAGAN\n                beta = tf.random.uniform(shape=tf.shape(a), minval=0., maxval=1.)\n                b = a + 0.5 * tf.math.reduce_std(a) * beta\n            shape = [tf.shape(a)[0]] + [1] * (a.shape.ndims - 1)\n            alpha = tf.random.uniform(shape=shape, minval=0., maxval=1.)\n            inter = a + alpha * (b - a)\n            inter.set_shape(a.shape)\n            return inter\n\n        x = _interpolate(real, fake)\n        with tf.GradientTape() as t:\n            t.watch(x)\n            pred = f(x)\n        grad = t.gradient(pred, x)\n        norm = tf.norm(tf.reshape(grad, [tf.shape(grad)[0], -1]), axis=1)\n        gp = tf.reduce_mean((norm - 1.)**2)\n\n        return gp\n\n    if mode == 'none':\n        gp = tf.constant(0, dtype=real.dtype)\n    elif mode == 'dragan':\n        gp = _gradient_penalty(f, real)\n    elif mode == 'wgan-gp':\n        gp = _gradient_penalty(f, real, fake)\n\n    return gp\n"""
tf2lib/__init__.py,2,"b""import tensorflow as tf\n\nfrom tf2lib.data import *\nfrom tf2lib.image import *\nfrom tf2lib.ops import *\nfrom tf2lib.utils import *\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nfor d in physical_devices:\n    tf.config.experimental.set_memory_growth(d, True)\n"""
v1/data_mnist.py,0,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport os\nimport gzip\nimport struct\nimport subprocess\nimport numpy as np\n\n\ndef unzip_gz(file_name):\n    unzip_name = file_name.replace(\'.gz\', \'\')\n    gz_file = gzip.GzipFile(file_name)\n    open(unzip_name, \'w+\').write(gz_file.read())\n    gz_file.close()\n\n\ndef mnist_load(data_dir, dataset=\'train\'):\n    """"""\n    modified from https://gist.github.com/akesling/5358964\n\n    return:\n    1. [-1.0, 1.0] float64 images of shape (N * H * W)\n    2. int labels of shape (N,)\n    3. # of datas\n    """"""\n\n    if dataset is \'train\':\n        fname_img = os.path.join(data_dir, \'train-images-idx3-ubyte\')\n        fname_lbl = os.path.join(data_dir, \'train-labels-idx1-ubyte\')\n    elif dataset is \'test\':\n        fname_img = os.path.join(data_dir, \'t10k-images-idx3-ubyte\')\n        fname_lbl = os.path.join(data_dir, \'t10k-labels-idx1-ubyte\')\n    else:\n        raise ValueError(""dataset must be \'test\' or \'train\'"")\n\n    if not os.path.exists(fname_img):\n        unzip_gz(fname_img + \'.gz\')\n    if not os.path.exists(fname_lbl):\n        unzip_gz(fname_lbl + \'.gz\')\n\n    # Load everything in some numpy arrays\n    with open(fname_lbl, \'rb\') as flbl:\n        struct.unpack(\'>II\', flbl.read(8))\n        lbls = np.fromfile(flbl, dtype=np.int8)\n\n    with open(fname_img, \'rb\') as fimg:\n        _, _, rows, cols = struct.unpack(\'>IIII\', fimg.read(16))\n        imgs = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbls), rows, cols) / 127.5 - 1\n\n    return imgs, lbls, len(lbls)\n\n\ndef mnist_download(download_dir):\n    url_base = \'http://yann.lecun.com/exdb/mnist/\'\n    file_names = [\'train-images-idx3-ubyte.gz\',\n                  \'train-labels-idx1-ubyte.gz\',\n                  \'t10k-images-idx3-ubyte.gz\',\n                  \'t10k-labels-idx1-ubyte.gz\']\n    for file_name in file_names:\n        url = url_base + file_name\n        save_path = os.path.join(download_dir, file_name)\n        cmd = [\'curl\', url, \'-o\', save_path]\n        print(\'Downloading \', file_name)\n        if not os.path.exists(save_path):\n            subprocess.call(cmd)\n        else:\n            print(\'%s exists, skip!\' % file_name)\n'"
v1/models_64x64.py,9,"b""from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport ops\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom functools import partial\n\nconv = partial(slim.conv2d, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=0.02))\ndconv = partial(slim.conv2d_transpose, activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.02))\nfc = partial(ops.flatten_fully_connected, activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.02))\nrelu = tf.nn.relu\nlrelu = partial(ops.leak_relu, leak=0.2)\nbatch_norm = partial(slim.batch_norm, decay=0.9, scale=True, epsilon=1e-5, updates_collections=None)\nln = slim.layer_norm\n\n\ndef generator(z, dim=64, reuse=True, training=True):\n    bn = partial(batch_norm, is_training=training)\n    dconv_bn_relu = partial(dconv, normalizer_fn=bn, activation_fn=relu, biases_initializer=None)\n    fc_bn_relu = partial(fc, normalizer_fn=bn, activation_fn=relu, biases_initializer=None)\n\n    with tf.variable_scope('generator', reuse=reuse):\n        y = fc_bn_relu(z, 4 * 4 * dim * 8)\n        y = tf.reshape(y, [-1, 4, 4, dim * 8])\n        y = dconv_bn_relu(y, dim * 4, 5, 2)\n        y = dconv_bn_relu(y, dim * 2, 5, 2)\n        y = dconv_bn_relu(y, dim * 1, 5, 2)\n        img = tf.tanh(dconv(y, 3, 5, 2))\n        return img\n\n\ndef discriminator(img, dim=64, reuse=True, training=True):\n    bn = partial(batch_norm, is_training=training)\n    conv_bn_lrelu = partial(conv, normalizer_fn=bn, activation_fn=lrelu, biases_initializer=None)\n\n    with tf.variable_scope('discriminator', reuse=reuse):\n        y = lrelu(conv(img, dim, 5, 2))\n        y = conv_bn_lrelu(y, dim * 2, 5, 2)\n        y = conv_bn_lrelu(y, dim * 4, 5, 2)\n        y = conv_bn_lrelu(y, dim * 8, 5, 2)\n        logit = fc(y, 1)\n        return logit\n\n\ndef discriminator_wgan_gp(img, dim=64, reuse=True, training=True):\n    conv_ln_lrelu = partial(conv, normalizer_fn=ln, activation_fn=lrelu, biases_initializer=None)\n\n    with tf.variable_scope('discriminator', reuse=reuse):\n        y = lrelu(conv(img, dim, 5, 2))\n        y = conv_ln_lrelu(y, dim * 2, 5, 2)\n        y = conv_ln_lrelu(y, dim * 4, 5, 2)\n        y = conv_ln_lrelu(y, dim * 8, 5, 2)\n        logit = fc(y, 1)\n        return logit\n"""
v1/models_mnist.py,9,"b""from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport ops\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom functools import partial\n\nconv = partial(slim.conv2d, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=0.02))\ndconv = partial(slim.conv2d_transpose, activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.02))\nfc = partial(ops.flatten_fully_connected, activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.02))\nrelu = tf.nn.relu\nlrelu = partial(ops.leak_relu, leak=0.2)\nbatch_norm = partial(slim.batch_norm, decay=0.9, scale=True, epsilon=1e-5, updates_collections=None)\nln = slim.layer_norm\n\n\ndef generator(z, dim=64, reuse=True, training=True):\n    bn = partial(batch_norm, is_training=training)\n    dconv_bn_relu = partial(dconv, normalizer_fn=bn, activation_fn=relu, biases_initializer=None)\n    fc_bn_relu = partial(fc, normalizer_fn=bn, activation_fn=relu, biases_initializer=None)\n\n    with tf.variable_scope('generator', reuse=reuse):\n        y = fc_bn_relu(z, 1024)\n        y = fc_bn_relu(y, 7 * 7 * dim * 2)\n        y = tf.reshape(y, [-1, 7, 7, dim * 2])\n        y = dconv_bn_relu(y, dim * 2, 5, 2)\n        img = tf.tanh(dconv(y, 1, 5, 2))\n        return img\n\n\ndef discriminator(img, dim=64, reuse=True, training=True):\n    bn = partial(batch_norm, is_training=training)\n    conv_bn_lrelu = partial(conv, normalizer_fn=bn, activation_fn=lrelu, biases_initializer=None)\n    fc_bn_lrelu = partial(fc, normalizer_fn=bn, activation_fn=lrelu, biases_initializer=None)\n\n    with tf.variable_scope('discriminator', reuse=reuse):\n        y = lrelu(conv(img, 1, 5, 2))\n        y = conv_bn_lrelu(y, dim, 5, 2)\n        y = fc_bn_lrelu(y, 1024)\n        logit = fc(y, 1)\n        return logit\n\n\ndef discriminator_wgan_gp(img, dim=64, reuse=True, training=True):\n    conv_ln_lrelu = partial(conv, normalizer_fn=ln, activation_fn=lrelu, biases_initializer=None)\n    fc_ln_lrelu = partial(fc, normalizer_fn=ln, activation_fn=lrelu, biases_initializer=None)\n\n    with tf.variable_scope('discriminator', reuse=reuse):\n        y = lrelu(conv(img, 1, 5, 2))\n        y = conv_ln_lrelu(y, dim, 5, 2)\n        y = fc_ln_lrelu(y, 1024)\n        logit = fc(y, 1)\n        return logit\n"""
v1/ops.py,6,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\ndef flatten_fully_connected(inputs,\n                            num_outputs,\n                            activation_fn=tf.nn.relu,\n                            normalizer_fn=None,\n                            normalizer_params=None,\n                            weights_initializer=slim.xavier_initializer(),\n                            weights_regularizer=None,\n                            biases_initializer=tf.zeros_initializer(),\n                            biases_regularizer=None,\n                            reuse=None,\n                            variables_collections=None,\n                            outputs_collections=None,\n                            trainable=True,\n                            scope=None):\n    with tf.variable_scope(scope, 'flatten_fully_connected', [inputs]):\n        if inputs.shape.ndims > 2:\n            inputs = slim.flatten(inputs)\n        return slim.fully_connected(inputs,\n                                    num_outputs,\n                                    activation_fn,\n                                    normalizer_fn,\n                                    normalizer_params,\n                                    weights_initializer,\n                                    weights_regularizer,\n                                    biases_initializer,\n                                    biases_regularizer,\n                                    reuse,\n                                    variables_collections,\n                                    outputs_collections,\n                                    trainable,\n                                    scope)\n\n\ndef leak_relu(x, leak, scope=None):\n    with tf.name_scope(scope, 'leak_relu', [x, leak]):\n        if leak < 1:\n            y = tf.maximum(x, leak * x)\n        else:\n            y = tf.minimum(x, leak * x)\n        return y\n"""
v1/train_cartoon_dcgan.py,12,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 25\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/faces\n# cartoon faces original size is [96, 96, 3]\n\n\ndef preprocess_fn(img):\n    re_size = 64\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/faces/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[96, 96, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    d_r_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(r_logit), r_logit)\n    d_f_loss = tf.losses.sigmoid_cross_entropy(tf.zeros_like(f_logit), f_logit)\n    d_loss = (d_r_loss + d_f_loss) / 2.0\n    g_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(f_logit), f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.3).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.3).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({d_loss: \'d_loss\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/cartoon_dcgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/cartoon_dcgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # batch data\n        real_ipt = data_pool.batch()\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n\n        # train D\n        d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        sess.run([g_step], feed_dict={z: z_ipt})\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/cartoon_dcgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_cartoon_dragan.py,20,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 100\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/faces\n# cartoon faces original size is [96, 96, 3]\n\n\ndef preprocess_fn(img):\n    re_size = 64\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/faces/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[96, 96, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator_wgan_gp\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    def gradient_penalty(real, f):\n        def interpolate(a):\n            beta = tf.random_uniform(shape=tf.shape(a), minval=0., maxval=1.)\n            _, variance = tf.nn.moments(a, range(a.shape.ndims))\n            b = a + 0.5 * tf.sqrt(variance) * beta\n\n            shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)\n            alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n            inter = a + alpha * (b - a)\n            inter.set_shape(a.get_shape().as_list())\n\n            return inter\n\n        x = interpolate(real)\n        pred = f(x)\n        gradients = tf.gradients(pred, x)[0]\n        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=range(1, x.shape.ndims)))\n        gp = tf.reduce_mean((slopes - 1.)**2)\n        return gp\n\n    # losses\n    d_r_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(r_logit), r_logit)\n    d_f_loss = tf.losses.sigmoid_cross_entropy(tf.zeros_like(f_logit), f_logit)\n    gp = gradient_penalty(real, discriminator)\n\n    d_loss = d_r_loss + d_f_loss + 10.0 * gp\n    g_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(f_logit), f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({d_r_loss: \'d_r_loss\', d_f_loss: \'d_f_loss\', gp: \'gp\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/cartoon_dragan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/cartoon_dragan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // batch_size\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # batch data\n        real_ipt = data_pool.batch()\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n\n        # train D\n        d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/cartoon_dragan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_cartoon_lsgan.py,12,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 25\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/faces\n# cartoon faces original size is [96, 96, 3]\n\n\ndef preprocess_fn(img):\n    re_size = 64\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/faces/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[96, 96, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    d_r_loss = tf.losses.mean_squared_error(tf.ones_like(r_logit), r_logit)\n    d_f_loss = tf.losses.mean_squared_error(tf.zeros_like(f_logit), f_logit)\n    d_loss = (d_r_loss + d_f_loss) / 2.0\n    g_loss = tf.losses.mean_squared_error(tf.ones_like(f_logit), f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({d_loss: \'d_loss\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/cartoon_lsgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/cartoon_lsgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # batch data\n        real_ipt = data_pool.batch()\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n\n        # train D\n        d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        sess.run([g_step], feed_dict={z: z_ipt})\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/cartoon_lsgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_cartoon_wgan.py,13,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 100\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\nclip = 0.01\nn_critic = 5\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/faces\n# cartoon faces original size is [96, 96, 3]\n\n\ndef preprocess_fn(img):\n    re_size = 64\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/faces/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[96, 96, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    wd = tf.reduce_mean(r_logit) - tf.reduce_mean(f_logit)\n    d_loss = -wd\n    g_loss = -tf.reduce_mean(f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step_ = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(d_loss, var_list=d_var)\n    with tf.control_dependencies([d_step_]):\n        d_step = tf.group(*(tf.assign(var, tf.clip_by_value(var, -clip, clip)) for var in d_var))\n    g_step = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({wd: \'wd\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/cartoon_wgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/cartoon_wgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size * n_critic)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # train D\n        if it < 25:\n            c_iter = 100\n        else:\n            c_iter = n_critic\n        for i in range(c_iter):\n            # batch data\n            real_ipt = data_pool.batch()\n            z_ipt = np.random.normal(size=[batch_size, z_dim])\n            d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/cartoon_wgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_cartoon_wgan_gp.py,16,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 100\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\nn_critic = 5\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/faces\n# cartoon faces original size is [96, 96, 3]\n\n\ndef preprocess_fn(img):\n    re_size = 64\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/faces/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[96, 96, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator_wgan_gp\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    def gradient_penalty(real, fake, f):\n        def interpolate(a, b):\n            shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)\n            alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n            inter = a + alpha * (b - a)\n            inter.set_shape(a.get_shape().as_list())\n            return inter\n\n        x = interpolate(real, fake)\n        pred = f(x)\n        gradients = tf.gradients(pred, x)[0]\n        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=range(1, x.shape.ndims)))\n        gp = tf.reduce_mean((slopes - 1.)**2)\n        return gp\n\n    wd = tf.reduce_mean(r_logit) - tf.reduce_mean(f_logit)\n    gp = gradient_penalty(real, fake, discriminator)\n    d_loss = -wd + gp * 10.0\n    g_loss = -tf.reduce_mean(f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({wd: \'wd\', gp: \'gp\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/cartoon_wgan_gp\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/cartoon_wgan_gp\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size * n_critic)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # train D\n        for i in range(n_critic):\n            # batch data\n            real_ipt = data_pool.batch()\n            z_ipt = np.random.normal(size=[batch_size, z_dim])\n            d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/cartoon_wgan_gp\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_celeba_dcgan.py,13,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/img_align_celeba\n# celeba original size is [218, 178, 3]\n\n\ndef preprocess_fn(img):\n    crop_size = 108\n    re_size = 64\n    img = tf.image.crop_to_bounding_box(img, (218 - crop_size) // 2, (178 - crop_size) // 2, crop_size, crop_size)\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/img_align_celeba/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[218, 178, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    d_r_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(r_logit), r_logit)\n    d_f_loss = tf.losses.sigmoid_cross_entropy(tf.zeros_like(f_logit), f_logit)\n    d_loss = (d_r_loss + d_f_loss) / 2.0\n    g_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(f_logit), f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({d_loss: \'d_loss\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/celeba_dcgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/celeba_dcgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # batch data\n        real_ipt = data_pool.batch()\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n\n        # train D\n        d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        sess.run([g_step], feed_dict={z: z_ipt})\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/celeba_dcgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_celeba_dragan.py,21,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/img_align_celeba\n# celeba original size is [218, 178, 3]\n\n\ndef preprocess_fn(img):\n    crop_size = 108\n    re_size = 64\n    img = tf.image.crop_to_bounding_box(img, (218 - crop_size) // 2, (178 - crop_size) // 2, crop_size, crop_size)\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/img_align_celeba/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[218, 178, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator_wgan_gp\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    def gradient_penalty(real, f):\n        def interpolate(a):\n            beta = tf.random_uniform(shape=tf.shape(a), minval=0., maxval=1.)\n            _, variance = tf.nn.moments(a, range(a.shape.ndims))\n            b = a + 0.5 * tf.sqrt(variance) * beta\n\n            shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)\n            alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n            inter = a + alpha * (b - a)\n            inter.set_shape(a.get_shape().as_list())\n\n            return inter\n\n        x = interpolate(real)\n        pred = f(x)\n        gradients = tf.gradients(pred, x)[0]\n        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=range(1, x.shape.ndims)))\n        gp = tf.reduce_mean((slopes - 1.)**2)\n        return gp\n\n    # losses\n    d_r_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(r_logit), r_logit)\n    d_f_loss = tf.losses.sigmoid_cross_entropy(tf.zeros_like(f_logit), f_logit)\n    gp = gradient_penalty(real, discriminator)\n\n    d_loss = d_r_loss + d_f_loss + 10.0 * gp\n    g_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(f_logit), f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({d_r_loss: \'d_r_loss\', d_f_loss: \'d_f_loss\', gp: \'gp\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/celeba_dragan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/celeba_dragan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // batch_size\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # batch data\n        real_ipt = data_pool.batch()\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n\n        # train D\n        d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/celeba_dragan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_celeba_lsgan.py,13,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/img_align_celeba\n# celeba original size is [218, 178, 3]\n\n\ndef preprocess_fn(img):\n    crop_size = 108\n    re_size = 64\n    img = tf.image.crop_to_bounding_box(img, (218 - crop_size) // 2, (178 - crop_size) // 2, crop_size, crop_size)\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/img_align_celeba/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[218, 178, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    d_r_loss = tf.losses.mean_squared_error(tf.ones_like(r_logit), r_logit)\n    d_f_loss = tf.losses.mean_squared_error(tf.zeros_like(f_logit), f_logit)\n    d_loss = (d_r_loss + d_f_loss) / 2.0\n    g_loss = tf.losses.mean_squared_error(tf.ones_like(f_logit), f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({d_loss: \'d_loss\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/celeba_lsgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/celeba_lsgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # batch data\n        real_ipt = data_pool.batch()\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n\n        # train D\n        d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        sess.run([g_step], feed_dict={z: z_ipt})\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/celeba_lsgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_celeba_wgan.py,14,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\nclip = 0.01\nn_critic = 5\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/img_align_celeba\n# celeba original size is [218, 178, 3]\n\n\ndef preprocess_fn(img):\n    crop_size = 108\n    re_size = 64\n    img = tf.image.crop_to_bounding_box(img, (218 - crop_size) // 2, (178 - crop_size) // 2, crop_size, crop_size)\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/img_align_celeba/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[218, 178, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    wd = tf.reduce_mean(r_logit) - tf.reduce_mean(f_logit)\n    d_loss = -wd\n    g_loss = -tf.reduce_mean(f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step_ = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(d_loss, var_list=d_var)\n    with tf.control_dependencies([d_step_]):\n        d_step = tf.group(*(tf.assign(var, tf.clip_by_value(var, -clip, clip)) for var in d_var))\n    g_step = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({wd: \'wd\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/celeba_wgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/celeba_wgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size * n_critic)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # train D\n        if it < 25:\n            c_iter = 100\n        else:\n            c_iter = n_critic\n        for i in range(c_iter):\n            # batch data\n            real_ipt = data_pool.batch()\n            z_ipt = np.random.normal(size=[batch_size, z_dim])\n            d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/celeba_wgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_celeba_wgan_gp.py,17,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport glob\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport models_64x64 as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\nn_critic = 5\ngpu_id = 0\n\n\'\'\' data \'\'\'\n# you should prepare your own data in ./data/img_align_celeba\n# celeba original size is [218, 178, 3]\n\n\ndef preprocess_fn(img):\n    crop_size = 108\n    re_size = 64\n    img = tf.image.crop_to_bounding_box(img, (218 - crop_size) // 2, (178 - crop_size) // 2, crop_size, crop_size)\n    img = tf.to_float(tf.image.resize_images(img, [re_size, re_size], method=tf.image.ResizeMethod.BICUBIC)) / 127.5 - 1\n    return img\n\nimg_paths = glob.glob(\'./data/img_align_celeba/*.jpg\')\ndata_pool = utils.DiskImageData(img_paths, batch_size, shape=[218, 178, 3], preprocess_fn=preprocess_fn)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator_wgan_gp\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    def gradient_penalty(real, fake, f):\n        def interpolate(a, b):\n            shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)\n            alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n            inter = a + alpha * (b - a)\n            inter.set_shape(a.get_shape().as_list())\n            return inter\n\n        x = interpolate(real, fake)\n        pred = f(x)\n        gradients = tf.gradients(pred, x)[0]\n        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=range(1, x.shape.ndims)))\n        gp = tf.reduce_mean((slopes - 1.)**2)\n        return gp\n\n    wd = tf.reduce_mean(r_logit) - tf.reduce_mean(f_logit)\n    gp = gradient_penalty(real, fake, discriminator)\n    d_loss = -wd + gp * 10.0\n    g_loss = -tf.reduce_mean(f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({wd: \'wd\', gp: \'gp\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/celeba_wgan_gp\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/celeba_wgan_gp\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size * n_critic)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # train D\n        for i in range(n_critic):\n            # batch data\n            real_ipt = data_pool.batch()\n            z_ipt = np.random.normal(size=[batch_size, z_dim])\n            d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/celeba_wgan_gp\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_mnist_dcgan.py,11,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport data_mnist as data\nimport models_mnist as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\ngpu_id = 0\n\n\'\'\' data \'\'\'\nutils.mkdir(\'./data/mnist/\')\ndata.mnist_download(\'./data/mnist\')\nimgs, _, _ = data.mnist_load(\'./data/mnist\')\nimgs.shape = imgs.shape + (1,)\ndata_pool = utils.MemoryData({\'img\': imgs}, batch_size)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    d_r_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(r_logit), r_logit)\n    d_f_loss = tf.losses.sigmoid_cross_entropy(tf.zeros_like(f_logit), f_logit)\n    d_loss = (d_r_loss + d_f_loss) / 2.0\n    g_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(f_logit), f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({d_loss: \'d_loss\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/mnist_dcgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/mnist_dcgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # batch data\n        real_ipt = data_pool.batch(\'img\')\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n\n        # train D\n        d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        sess.run([g_step], feed_dict={z: z_ipt})\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 1000 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/mnist_dcgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_mnist_lsgan.py,11,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport data_mnist as data\nimport models_mnist as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\ngpu_id = 0\n\n\'\'\' data \'\'\'\nutils.mkdir(\'./data/mnist/\')\ndata.mnist_download(\'./data/mnist\')\nimgs, _, _ = data.mnist_load(\'./data/mnist\')\nimgs.shape = imgs.shape + (1,)\ndata_pool = utils.MemoryData({\'img\': imgs}, batch_size)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    d_r_loss = tf.losses.mean_squared_error(tf.ones_like(r_logit), r_logit)\n    d_f_loss = tf.losses.mean_squared_error(tf.zeros_like(f_logit), f_logit)\n    d_loss = (d_r_loss + d_f_loss) / 2.0\n    g_loss = tf.losses.mean_squared_error(tf.ones_like(f_logit), f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({d_loss: \'d_loss\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/mnist_lsgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/mnist_lsgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # batch data\n        real_ipt = data_pool.batch(\'img\')\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n\n        # train D\n        d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        sess.run([g_step], feed_dict={z: z_ipt})\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 1000 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/mnist_lsgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_mnist_wgan.py,12,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport data_mnist as data\nimport models_mnist as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\nclip = 0.01\nn_critic = 5\ngpu_id = 0\n\n\'\'\' data \'\'\'\nutils.mkdir(\'./data/mnist/\')\ndata.mnist_download(\'./data/mnist\')\nimgs, _, _ = data.mnist_load(\'./data/mnist\')\nimgs.shape = imgs.shape + (1,)\ndata_pool = utils.MemoryData({\'img\': imgs}, batch_size)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    wd = tf.reduce_mean(r_logit) - tf.reduce_mean(f_logit)\n    d_loss = -wd\n    g_loss = -tf.reduce_mean(f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step_ = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(d_loss, var_list=d_var)\n    with tf.control_dependencies([d_step_]):\n        d_step = tf.group(*(tf.assign(var, tf.clip_by_value(var, -clip, clip)) for var in d_var))\n    g_step = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({wd: \'wd\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/mnist_wgan\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/mnist_wgan\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size * n_critic)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # train D\n        if it < 25:\n            c_iter = 100\n        else:\n            c_iter = n_critic\n        for i in range(c_iter):\n            # batch data\n            real_ipt = data_pool.batch(\'img\')\n            z_ipt = np.random.normal(size=[batch_size, z_dim])\n            d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/mnist_wgan\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/train_mnist_wgan_gp.py,15,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport utils\nimport traceback\nimport numpy as np\nimport tensorflow as tf\nimport data_mnist as data\nimport models_mnist as models\n\n\n"""""" param """"""\nepoch = 50\nbatch_size = 64\nlr = 0.0002\nz_dim = 100\nn_critic = 5\ngpu_id = 0\n\n\'\'\' data \'\'\'\nutils.mkdir(\'./data/mnist/\')\ndata.mnist_download(\'./data/mnist\')\nimgs, _, _ = data.mnist_load(\'./data/mnist\')\nimgs.shape = imgs.shape + (1,)\ndata_pool = utils.MemoryData({\'img\': imgs}, batch_size)\n\n\n"""""" graphs """"""\nwith tf.device(\'/gpu:%d\' % gpu_id):\n    \'\'\' models \'\'\'\n    generator = models.generator\n    discriminator = models.discriminator_wgan_gp\n\n    \'\'\' graph \'\'\'\n    # inputs\n    real = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n    z = tf.placeholder(tf.float32, shape=[None, z_dim])\n\n    # generate\n    fake = generator(z, reuse=False)\n\n    # dicriminate\n    r_logit = discriminator(real, reuse=False)\n    f_logit = discriminator(fake)\n\n    # losses\n    def gradient_penalty(real, fake, f):\n        def interpolate(a, b):\n            shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)\n            alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n            inter = a + alpha * (b - a)\n            inter.set_shape(a.get_shape().as_list())\n            return inter\n\n        x = interpolate(real, fake)\n        pred = f(x)\n        gradients = tf.gradients(pred, x)[0]\n        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=range(1, x.shape.ndims)))\n        gp = tf.reduce_mean((slopes - 1.)**2)\n        return gp\n\n    wd = tf.reduce_mean(r_logit) - tf.reduce_mean(f_logit)\n    gp = gradient_penalty(real, fake, discriminator)\n    d_loss = -wd + gp * 10.0\n    g_loss = -tf.reduce_mean(f_logit)\n\n    # otpims\n    d_var = utils.trainable_variables(\'discriminator\')\n    g_var = utils.trainable_variables(\'generator\')\n    d_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(d_loss, var_list=d_var)\n    g_step = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.5).minimize(g_loss, var_list=g_var)\n\n    # summaries\n    d_summary = utils.summary({wd: \'wd\', gp: \'gp\'})\n    g_summary = utils.summary({g_loss: \'g_loss\'})\n\n    # sample\n    f_sample = generator(z, training=False)\n\n\n"""""" train """"""\n\'\'\' init \'\'\'\n# session\nsess = utils.session()\n# iteration counter\nit_cnt, update_cnt = utils.counter()\n# saver\nsaver = tf.train.Saver(max_to_keep=5)\n# summary writer\nsummary_writer = tf.summary.FileWriter(\'./summaries/mnist_wgan_gp\', sess.graph)\n\n\'\'\' initialization \'\'\'\nckpt_dir = \'./checkpoints/mnist_wgan_gp\'\nutils.mkdir(ckpt_dir + \'/\')\nif not utils.load_checkpoint(ckpt_dir, sess):\n    sess.run(tf.global_variables_initializer())\n\n\'\'\' train \'\'\'\ntry:\n    z_ipt_sample = np.random.normal(size=[100, z_dim])\n\n    batch_epoch = len(data_pool) // (batch_size * n_critic)\n    max_it = epoch * batch_epoch\n    for it in range(sess.run(it_cnt), max_it):\n        sess.run(update_cnt)\n\n        # which epoch\n        epoch = it // batch_epoch\n        it_epoch = it % batch_epoch + 1\n\n        # train D\n        for i in range(n_critic):\n            # batch data\n            real_ipt = data_pool.batch(\'img\')\n            z_ipt = np.random.normal(size=[batch_size, z_dim])\n            d_summary_opt, _ = sess.run([d_summary, d_step], feed_dict={real: real_ipt, z: z_ipt})\n        summary_writer.add_summary(d_summary_opt, it)\n\n        # train G\n        z_ipt = np.random.normal(size=[batch_size, z_dim])\n        g_summary_opt, _ = sess.run([g_summary, g_step], feed_dict={z: z_ipt})\n        summary_writer.add_summary(g_summary_opt, it)\n\n        # display\n        if it % 1 == 0:\n            print(""Epoch: (%3d) (%5d/%5d)"" % (epoch, it_epoch, batch_epoch))\n\n        # save\n        if (it + 1) % 1000 == 0:\n            save_path = saver.save(sess, \'%s/Epoch_(%d)_(%dof%d).ckpt\' % (ckpt_dir, epoch, it_epoch, batch_epoch))\n            print(\'Model saved in file: % s\' % save_path)\n\n        # sample\n        if (it + 1) % 100 == 0:\n            f_sample_opt = sess.run(f_sample, feed_dict={z: z_ipt_sample})\n\n            save_dir = \'./sample_images_while_training/mnist_wgan_gp\'\n            utils.mkdir(save_dir + \'/\')\n            utils.imwrite(utils.immerge(f_sample_opt, 10, 10), \'%s/Epoch_(%d)_(%dof%d).jpg\' % (save_dir, epoch, it_epoch, batch_epoch))\n\nexcept Exception, e:\n    traceback.print_exc()\nfinally:\n    print("" [*] Close main session!"")\n    sess.close()\n'"
v1/utils.py,40,"b'from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport os\nimport re\nimport scipy\nimport numpy as np\nimport tensorflow as tf\n\nfrom collections import OrderedDict\n\n\ndef mkdir(paths):\n    if not isinstance(paths, (list, tuple)):\n        paths = [paths]\n    for path in paths:\n        path_dir, _ = os.path.split(path)\n        if not os.path.isdir(path_dir):\n            os.makedirs(path_dir)\n\n\ndef session(graph=None, allow_soft_placement=True,\n            log_device_placement=False, allow_growth=True):\n    """""" return a Session with simple config """"""\n\n    config = tf.ConfigProto(allow_soft_placement=allow_soft_placement,\n                            log_device_placement=log_device_placement)\n    config.gpu_options.allow_growth = allow_growth\n    return tf.Session(graph=graph, config=config)\n\n\ndef tensors_filter(tensors, filters, combine_type=\'or\'):\n    assert isinstance(tensors, (list, tuple)), \'`tensors` shoule be a list or tuple!\'\n    assert isinstance(filters, (str, list, tuple)), \\\n        \'`filters` should be a string or a list(tuple) of strings!\'\n    assert combine_type == \'or\' or combine_type == \'and\', ""`combine_type` should be \'or\' or \'and\'!""\n\n    if isinstance(filters, str):\n        filters = [filters]\n\n    f_tens = []\n    for ten in tensors:\n        if combine_type == \'or\':\n            for filt in filters:\n                if filt in ten.name:\n                    f_tens.append(ten)\n                    break\n        elif combine_type == \'and\':\n            all_pass = True\n            for filt in filters:\n                if filt not in ten.name:\n                    all_pass = False\n                    break\n            if all_pass:\n                f_tens.append(ten)\n    return f_tens\n\n\ndef trainable_variables(filters=None, combine_type=\'or\'):\n    t_var = tf.trainable_variables()\n    if filters is None:\n        return t_var\n    else:\n        return tensors_filter(t_var, filters, combine_type)\n\n\ndef summary(tensor_collection, summary_type=[\'mean\', \'stddev\', \'max\', \'min\', \'sparsity\', \'histogram\']):\n    """"""\n    usage:\n\n    1. summary(tensor)\n\n    2. summary([tensor_a, tensor_b])\n\n    3. summary({tensor_a: \'a\', tensor_b: \'b})\n    """"""\n\n    def _summary(tensor, name, summary_type=[\'mean\', \'stddev\', \'max\', \'min\', \'sparsity\', \'histogram\']):\n        """""" Attach a lot of summaries to a Tensor. """"""\n\n        if name is None:\n            # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n            # session. This helps the clarity of presentation on tensorboard.\n            name = re.sub(\'%s_[0-9]*/\' % \'tower\', \'\', tensor.name)\n            name = re.sub(\':\', \'-\', name)\n\n        with tf.name_scope(\'summary_\' + name):\n            summaries = []\n            if len(tensor._shape) == 0:\n                summaries.append(tf.summary.scalar(name, tensor))\n            else:\n                if \'mean\' in summary_type:\n                    mean = tf.reduce_mean(tensor)\n                    summaries.append(tf.summary.scalar(name + \'/mean\', mean))\n                if \'stddev\' in summary_type:\n                    mean = tf.reduce_mean(tensor)\n                    stddev = tf.sqrt(tf.reduce_mean(tf.square(tensor - mean)))\n                    summaries.append(tf.summary.scalar(name + \'/stddev\', stddev))\n                if \'max\' in summary_type:\n                    summaries.append(tf.summary.scalar(name + \'/max\', tf.reduce_max(tensor)))\n                if \'min\' in summary_type:\n                    summaries.append(tf.summary.scalar(name + \'/min\', tf.reduce_min(tensor)))\n                if \'sparsity\' in summary_type:\n                    summaries.append(tf.summary.scalar(name + \'/sparsity\', tf.nn.zero_fraction(tensor)))\n                if \'histogram\' in summary_type:\n                    summaries.append(tf.summary.histogram(name, tensor))\n            return tf.summary.merge(summaries)\n\n    if not isinstance(tensor_collection, (list, tuple, dict)):\n        tensor_collection = [tensor_collection]\n    with tf.name_scope(\'summaries\'):\n        summaries = []\n        if isinstance(tensor_collection, (list, tuple)):\n            for tensor in tensor_collection:\n                summaries.append(_summary(tensor, None, summary_type))\n        else:\n            for tensor, name in tensor_collection.items():\n                summaries.append(_summary(tensor, name, summary_type))\n        return tf.summary.merge(summaries)\n\n\ndef counter(scope=\'counter\'):\n    with tf.variable_scope(scope):\n        counter = tf.Variable(0, dtype=tf.int32, name=\'counter\')\n        update_cnt = tf.assign(counter, tf.add(counter, 1))\n        return counter, update_cnt\n\n\ndef load_checkpoint(checkpoint_dir, session, var_list=None):\n    print(\' [*] Loading checkpoint...\')\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n        ckpt_path = os.path.join(checkpoint_dir, ckpt_name)\n    try:\n        restorer = tf.train.Saver(var_list)\n        restorer.restore(session, ckpt_path)\n        print(\' [*] Loading successful! Copy variables from % s\' % ckpt_path)\n        return True\n    except:\n        print(\' [*] No suitable checkpoint!\')\n        return False\n\n\ndef memory_data_batch(memory_data_dict, batch_size, preprocess_fns={}, shuffle=True, num_threads=16,\n                      min_after_dequeue=5000, allow_smaller_final_batch=False, scope=None):\n    """"""\n    memory_data_dict:\n        for example\n        {\'img\': img_ndarray, \'point\': point_ndarray} or\n        {\'img\': img_tensor, \'point\': point_tensor}\n        the value of each item of `memory_data_dict` is in shape of (N, ...)\n\n    preprocess_fns:\n        for example\n        {\'img\': img_preprocess_fn, \'point\': point_preprocess_fn}\n    """"""\n\n    with tf.name_scope(scope, \'memory_data_batch\'):\n        fields = []\n        tensor_dict = OrderedDict()\n        for k in memory_data_dict:\n            fields.append(k)\n            tensor_dict[k] = tf.convert_to_tensor(memory_data_dict[k])  # the same dtype of the input data\n        data_num = tensor_dict[k].get_shape().as_list()[0]\n\n        # slice to single example, and since it\'s memory data, the `capacity` is set as data_num\n        data_values = tf.train.slice_input_producer(list(tensor_dict.values()), shuffle=shuffle, capacity=data_num)\n        data_keys = list(tensor_dict.keys())\n        data_dict = {}\n        for k, v in zip(data_keys, data_values):\n            if k in preprocess_fns:\n                data_dict[k] = preprocess_fns[k](v)\n            else:\n                data_dict[k] = v\n\n        # batch datas\n        if shuffle:\n            capacity = min_after_dequeue + (num_threads + 1) * batch_size\n            data_batch = tf.train.shuffle_batch(data_dict,\n                                                batch_size=batch_size,\n                                                capacity=capacity,\n                                                min_after_dequeue=min_after_dequeue,\n                                                num_threads=num_threads,\n                                                allow_smaller_final_batch=allow_smaller_final_batch)\n        else:\n            data_batch = tf.train.batch(data_dict,\n                                        batch_size=batch_size,\n                                        allow_smaller_final_batch=allow_smaller_final_batch)\n\n        return data_batch, data_num, fields\n\n\nclass MemoryData:\n\n    def __init__(self, memory_data_dict, batch_size, preprocess_fns={}, shuffle=True, num_threads=16,\n                 min_after_dequeue=5000, allow_smaller_final_batch=False, scope=None):\n        """"""\n        memory_data_dict:\n            for example\n            {\'img\': img_ndarray, \'point\': point_ndarray} or\n            {\'img\': img_tensor, \'point\': point_tensor}\n            the value of each item of `memory_data_dict` is in shape of (N, ...)\n\n        preprocess_fns:\n            for example\n            {\'img\': img_preprocess_fn, \'point\': point_preprocess_fn}\n        """"""\n\n        self.graph = tf.Graph()  # declare ops in a separated graph\n        with self.graph.as_default():\n            # @TODO\n            # There are some strange errors if the gpu device is the\n            # same with the main graph, but cpu device is ok. I don\'t know why...\n            with tf.device(\'/cpu:0\'):\n                self._batch_ops, self._data_num, self._fields = memory_data_batch(memory_data_dict, batch_size, preprocess_fns, shuffle, num_threads,\n                                                                                  min_after_dequeue, allow_smaller_final_batch, scope)\n\n        print(\' [*] MemoryData: create session!\')\n        self.sess = session(graph=self.graph)\n        self.coord = tf.train.Coordinator()\n        self.threads = tf.train.start_queue_runners(sess=self.sess, coord=self.coord)\n\n    def __len__(self):\n        return self._data_num\n\n    def batch(self, fields=None):\n        batch_data = self.sess.run(self._batch_ops)\n        if fields is None:\n            fields = self._fields\n        if isinstance(fields, (list, tuple)):\n            return [batch_data[field] for field in fields]\n        else:\n            return batch_data[fields]\n\n    def fields(self):\n        return self._fields\n\n    def __del__(self):\n        print(\' [*] MemoryData: stop threads and close session!\')\n        self.coord.request_stop()\n        self.coord.join(self.threads)\n        self.sess.close()\n\n\ndef disk_image_batch(image_paths, batch_size, shape, preprocess_fn=None, shuffle=True, num_threads=16,\n                     min_after_dequeue=100, allow_smaller_final_batch=False, scope=None):\n    """"""\n    This function is suitable for bmp, jpg, png and gif files\n\n    image_paths: string list or 1-D tensor, each of which is an iamge path\n    preprocess_fn: single image preprocessing function\n    """"""\n\n    with tf.name_scope(scope, \'disk_image_batch\'):\n        data_num = len(image_paths)\n\n        # dequeue a single image path and read the image bytes; enqueue the whole file list\n        _, img = tf.WholeFileReader().read(tf.train.string_input_producer(image_paths, shuffle=shuffle, capacity=data_num))\n        img = tf.image.decode_image(img)\n\n        # preprocessing\n        img.set_shape(shape)\n        if preprocess_fn is not None:\n            img = preprocess_fn(img)\n\n        # batch datas\n        if shuffle:\n            capacity = min_after_dequeue + (num_threads + 1) * batch_size\n            img_batch = tf.train.shuffle_batch([img],\n                                               batch_size=batch_size,\n                                               capacity=capacity,\n                                               min_after_dequeue=min_after_dequeue,\n                                               num_threads=num_threads,\n                                               allow_smaller_final_batch=allow_smaller_final_batch)\n        else:\n            img_batch = tf.train.batch([img],\n                                       batch_size=batch_size,\n                                       allow_smaller_final_batch=allow_smaller_final_batch)\n\n        return img_batch, data_num\n\n\nclass DiskImageData:\n\n    def __init__(self, image_paths, batch_size, shape, preprocess_fn=None, shuffle=True, num_threads=16,\n                 min_after_dequeue=100, allow_smaller_final_batch=False, scope=None):\n        """"""\n        This function is suitable for bmp, jpg, png and gif files\n\n        image_paths: string list or 1-D tensor, each of which is an iamge path\n        preprocess_fn: single image preprocessing function\n        """"""\n\n        self.graph = tf.Graph()  # declare ops in a separated graph\n        with self.graph.as_default():\n            # @TODO\n            # There are some strange errors if the gpu device is the\n            # same with the main graph, but cpu device is ok. I don\'t know why...\n            with tf.device(\'/cpu:0\'):\n                self._batch_ops, self._data_num = disk_image_batch(image_paths, batch_size, shape, preprocess_fn, shuffle, num_threads,\n                                                                   min_after_dequeue, allow_smaller_final_batch, scope)\n\n        print(\' [*] DiskImageData: create session!\')\n        self.sess = session(graph=self.graph)\n        self.coord = tf.train.Coordinator()\n        self.threads = tf.train.start_queue_runners(sess=self.sess, coord=self.coord)\n\n    def __len__(self):\n        return self._data_num\n\n    def batch(self):\n        return self.sess.run(self._batch_ops)\n\n    def __del__(self):\n        print(\' [*] DiskImageData: stop threads and close session!\')\n        self.coord.request_stop()\n        self.coord.join(self.threads)\n        self.sess.close()\n\n\ndef to_range(images, min_value=0.0, max_value=1.0, dtype=None):\n    """"""\n    transform images from [-1.0, 1.0] to [min_value, max_value] of dtype\n    """"""\n    assert \\\n        np.min(images) >= -1.0 - 1e-5 and np.max(images) <= 1.0 + 1e-5 \\\n        and (images.dtype == np.float32 or images.dtype == np.float64), \\\n        \'The input images should be float64(32) and in the range of [-1.0, 1.0]!\'\n    if dtype is None:\n        dtype = images.dtype\n    return ((images + 1.) / 2. * (max_value - min_value) + min_value).astype(dtype)\n\n\ndef imwrite(image, path):\n    """""" save an [-1.0, 1.0] image """"""\n\n    if image.ndim == 3 and image.shape[2] == 1:  # for gray image\n        image = np.array(image, copy=True)\n        image.shape = image.shape[0:2]\n    return scipy.misc.imsave(path, to_range(image, 0, 255, np.uint8))\n\n\ndef immerge(images, row, col):\n    """"""\n    merge images into an image with (row * h) * (col * w)\n\n    `images` is in shape of N * H * W(* C=1 or 3)\n    """"""\n\n    h, w = images.shape[1], images.shape[2]\n    if images.ndim == 4:\n        img = np.zeros((h * row, w * col, images.shape[3]))\n    elif images.ndim == 3:\n        img = np.zeros((h * row, w * col))\n    for idx, image in enumerate(images):\n        i = idx % col\n        j = idx // col\n        img[j * h:j * h + h, i * w:i * w + w, ...] = image\n\n    return img\n'"
tf2lib/data/__init__.py,0,b'from tf2lib.data.dataset import *\n'
tf2lib/data/dataset.py,3,"b'import multiprocessing\n\nimport tensorflow as tf\n\n\ndef batch_dataset(dataset,\n                  batch_size,\n                  drop_remainder=True,\n                  n_prefetch_batch=1,\n                  filter_fn=None,\n                  map_fn=None,\n                  n_map_threads=None,\n                  filter_after_map=False,\n                  shuffle=True,\n                  shuffle_buffer_size=None,\n                  repeat=None):\n    # set defaults\n    if n_map_threads is None:\n        n_map_threads = multiprocessing.cpu_count()\n    if shuffle and shuffle_buffer_size is None:\n        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n\n    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n    if shuffle:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n\n    if not filter_after_map:\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n        if map_fn:\n            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n\n    else:  # [*] this is slower\n        if map_fn:\n            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n\n    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n\n    return dataset\n\n\ndef memory_data_batch_dataset(memory_data,\n                              batch_size,\n                              drop_remainder=True,\n                              n_prefetch_batch=1,\n                              filter_fn=None,\n                              map_fn=None,\n                              n_map_threads=None,\n                              filter_after_map=False,\n                              shuffle=True,\n                              shuffle_buffer_size=None,\n                              repeat=None):\n    """"""Batch dataset of memory data.\n\n    Parameters\n    ----------\n    memory_data : nested structure of tensors/ndarrays/lists\n\n    """"""\n    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n    dataset = batch_dataset(dataset,\n                            batch_size,\n                            drop_remainder=drop_remainder,\n                            n_prefetch_batch=n_prefetch_batch,\n                            filter_fn=filter_fn,\n                            map_fn=map_fn,\n                            n_map_threads=n_map_threads,\n                            filter_after_map=filter_after_map,\n                            shuffle=shuffle,\n                            shuffle_buffer_size=shuffle_buffer_size,\n                            repeat=repeat)\n    return dataset\n\n\ndef disk_image_batch_dataset(img_paths,\n                             batch_size,\n                             labels=None,\n                             drop_remainder=True,\n                             n_prefetch_batch=1,\n                             filter_fn=None,\n                             map_fn=None,\n                             n_map_threads=None,\n                             filter_after_map=False,\n                             shuffle=True,\n                             shuffle_buffer_size=None,\n                             repeat=None):\n    """"""Batch dataset of disk image for PNG and JPEG.\n\n    Parameters\n    ----------\n    img_paths : 1d-tensor/ndarray/list of str\n    labels : nested structure of tensors/ndarrays/lists\n\n    """"""\n    if labels is None:\n        memory_data = img_paths\n    else:\n        memory_data = (img_paths, labels)\n\n    def parse_fn(path, *label):\n        img = tf.io.read_file(path)\n        img = tf.image.decode_png(img, 3)  # fix channels to 3\n        return (img,) + label\n\n    if map_fn:  # fuse `map_fn` and `parse_fn`\n        def map_fn_(*args):\n            return map_fn(*parse_fn(*args))\n    else:\n        map_fn_ = parse_fn\n\n    dataset = memory_data_batch_dataset(memory_data,\n                                        batch_size,\n                                        drop_remainder=drop_remainder,\n                                        n_prefetch_batch=n_prefetch_batch,\n                                        filter_fn=filter_fn,\n                                        map_fn=map_fn_,\n                                        n_map_threads=n_map_threads,\n                                        filter_after_map=filter_after_map,\n                                        shuffle=shuffle,\n                                        shuffle_buffer_size=shuffle_buffer_size,\n                                        repeat=repeat)\n\n    return dataset\n'"
tf2lib/image/__init__.py,0,b'from tf2lib.image.image import *\n'
tf2lib/image/image.py,15,"b'import functools\nimport math\nimport random\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n\n@tf.function\ndef center_crop(image, size):\n    # for image of shape [batch, height, width, channels] or [height, width, channels]\n    if not isinstance(size, (tuple, list)):\n        size = [size, size]\n    offset_height = (tf.shape(image)[-3] - size[0]) // 2\n    offset_width = (tf.shape(image)[-2] - size[1]) // 2\n    return tf.image.crop_to_bounding_box(image, offset_height, offset_width, size[0], size[1])\n\n\n@tf.function\ndef color_jitter(image, brightness=0, contrast=0, saturation=0, hue=0):\n    """"""Color jitter.\n\n    Examples\n    --------\n    >>> color_jitter(img, 25, 0.2, 0.2, 0.1)\n\n    """"""\n    tforms = []\n    if brightness > 0:\n        tforms.append(functools.partial(tf.image.random_brightness, max_delta=brightness))\n    if contrast > 0:\n        tforms.append(functools.partial(tf.image.random_contrast, lower=max(0, 1 - contrast), upper=1 + contrast))\n    if saturation > 0:\n        tforms.append(functools.partial(tf.image.random_saturation, lower=max(0, 1 - saturation), upper=1 + saturation))\n    if hue > 0:\n        tforms.append(functools.partial(tf.image.random_hue, max_delta=hue))\n\n    random.shuffle(tforms)\n    for tform in tforms:\n        image = tform(image)\n\n    return image\n\n\n@tf.function\ndef random_grayscale(image, p=0.1):\n    return tf.cond(pred=tf.random.uniform(()) < p,\n                   true_fn=lambda: tf.image.adjust_saturation(image, 0),\n                   false_fn=lambda: image)\n\n\n@tf.function\ndef random_rotate(images, max_degrees, interpolation=\'BILINEAR\'):\n    # Randomly rotate image(s) counterclockwise by the angle(s) uniformly chosen from [-max_degree(s), max_degree(s)].\n    max_degrees = tf.convert_to_tensor(max_degrees, dtype=tf.float32)\n    angles = tf.random.uniform(tf.shape(max_degrees), minval=-1.0, maxval=1.0) * max_degrees / 180.0 * math.pi\n    return tfa.image.rotate(images, angles, interpolation=interpolation)\n'"
tf2lib/ops/__init__.py,0,b'from tf2lib.ops.ops import *\n'
tf2lib/ops/ops.py,9,"b'import tensorflow as tf\n\n\n@tf.function\ndef minmax_norm(x, epsilon=1e-12):\n    x = tf.cast(x, tf.float32)\n    min_val = tf.reduce_min(x)\n    max_val = tf.reduce_max(x)\n    norm_x = (x - min_val) / tf.maximum((max_val - min_val), epsilon)\n    return norm_x\n\n\n@tf.function\ndef reshape(x, shape):\n    x = tf.convert_to_tensor(x)\n    shape = [x.shape[i] if shape[i] == 0 else shape[i] for i in range(len(shape))]  # TODO(Lynn): is it slow here?\n    shape = [tf.shape(x)[i] if shape[i] is None else shape[i] for i in range(len(shape))]\n    return tf.reshape(x, shape)\n'"
tf2lib/utils/__init__.py,0,b'from tf2lib.utils.utils import *\n'
tf2lib/utils/utils.py,13,"b'import tensorflow as tf\n\n\nclass Checkpoint:\n    """"""Enhanced ""tf.train.Checkpoint"".""""""\n\n    def __init__(self,\n                 checkpoint_kwargs,  # for ""tf.train.Checkpoint""\n                 directory,  # for ""tf.train.CheckpointManager""\n                 max_to_keep=5,\n                 keep_checkpoint_every_n_hours=None):\n        self.checkpoint = tf.train.Checkpoint(**checkpoint_kwargs)\n        self.manager = tf.train.CheckpointManager(self.checkpoint, directory, max_to_keep, keep_checkpoint_every_n_hours)\n\n    def restore(self, save_path=None):\n        save_path = self.manager.latest_checkpoint if save_path is None else save_path\n        return self.checkpoint.restore(save_path)\n\n    def save(self, file_prefix_or_checkpoint_number=None, session=None):\n        if isinstance(file_prefix_or_checkpoint_number, str):\n            return self.checkpoint.save(file_prefix_or_checkpoint_number, session=session)\n        else:\n            return self.manager.save(checkpoint_number=file_prefix_or_checkpoint_number)\n\n    def __getattr__(self, attr):\n        if hasattr(self.checkpoint, attr):\n            return getattr(self.checkpoint, attr)\n        elif hasattr(self.manager, attr):\n            return getattr(self.manager, attr)\n        else:\n            self.__getattribute__(attr)  # this will raise an exception\n\n\ndef summary(name_data_dict,\n            step=None,\n            types=[\'mean\', \'std\', \'max\', \'min\', \'sparsity\', \'histogram\'],\n            historgram_buckets=None,\n            name=\'summary\'):\n    """"""Summary.\n\n    Examples\n    --------\n    >>> summary({\'a\': data_a, \'b\': data_b})\n\n    """"""\n    def _summary(name, data):\n        if data.shape == ():\n            tf.summary.scalar(name, data, step=step)\n        else:\n            if \'mean\' in types:\n                tf.summary.scalar(name + \'-mean\', tf.math.reduce_mean(data), step=step)\n            if \'std\' in types:\n                tf.summary.scalar(name + \'-std\', tf.math.reduce_std(data), step=step)\n            if \'max\' in types:\n                tf.summary.scalar(name + \'-max\', tf.math.reduce_max(data), step=step)\n            if \'min\' in types:\n                tf.summary.scalar(name + \'-min\', tf.math.reduce_min(data), step=step)\n            if \'sparsity\' in types:\n                tf.summary.scalar(name + \'-sparsity\', tf.math.zero_fraction(data), step=step)\n            if \'histogram\' in types:\n                tf.summary.histogram(name, data, step=step, buckets=historgram_buckets)\n\n    with tf.name_scope(name):\n        for name, data in name_data_dict.items():\n            _summary(name, data)\n'"
