file_path,api_count,code
data_loader.py,0,"b'import numpy as np\n\n\nclass DataLoader:\n    """"""Data Loader class. As a simple case, the model is tried on TinyImageNet. For larger datasets,\n    you may need to adapt this class to use the Tensorflow Dataset API""""""\n\n    def __init__(self, batch_size, shuffle=False):\n        self.X_train = None\n        self.X_mean = None\n        self.y_train = None\n        self.train_data_len = 0\n\n        self.X_val = None\n        self.y_val = None\n        self.val_data_len = 0\n\n        self.X_test = None\n        self.y_test = None\n        self.test_data_len = 0\n\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n\n    def load_data(self):\n        # This method is an example of loading a dataset. Change it to suit your needs..\n        import matplotlib.pyplot as plt\n        # For going in the same experiment as the paper. Resizing the input image data to 224x224 is done.\n        train_data = np.array([plt.imread(\'./data/0.jpg\')], dtype=np.float32)\n        self.X_train = train_data\n        self.y_train = np.array([283], dtype=np.int32)\n\n        val_data = np.array([plt.imread(\'./data/0.jpg\')], dtype=np.float32)\n        self.X_val = val_data\n        self.y_val = np.array([283])\n\n        self.train_data_len = self.X_train.shape[0]\n        self.val_data_len = self.X_val.shape[0]\n        img_height = 224\n        img_width = 224\n        num_channels = 3\n        return img_height, img_width, num_channels, self.train_data_len, self.val_data_len\n\n    def generate_batch(self, type=\'train\'):\n        """"""Generate batch from X_train/X_test and y_train/y_test using a python DataGenerator""""""\n        if type == \'train\':\n            # Training time!\n            new_epoch = True\n            start_idx = 0\n            mask = None\n            while True:\n                if new_epoch:\n                    start_idx = 0\n                    if self.shuffle:\n                        mask = np.random.choice(self.train_data_len, self.train_data_len, replace=False)\n                    else:\n                        mask = np.arange(self.train_data_len)\n                    new_epoch = False\n\n                # Batch mask selection\n                X_batch = self.X_train[mask[start_idx:start_idx + self.batch_size]]\n                y_batch = self.y_train[mask[start_idx:start_idx + self.batch_size]]\n                start_idx += self.batch_size\n\n                # Reset everything after the end of an epoch\n                if start_idx >= self.train_data_len:\n                    new_epoch = True\n                    mask = None\n                yield X_batch, y_batch\n        elif type == \'test\':\n            # Testing time!\n            start_idx = 0\n            while True:\n                # Batch mask selection\n                X_batch = self.X_test[start_idx:start_idx + self.batch_size]\n                y_batch = self.y_test[start_idx:start_idx + self.batch_size]\n                start_idx += self.batch_size\n\n                # Reset everything\n                if start_idx >= self.test_data_len:\n                    start_idx = 0\n                yield X_batch, y_batch\n        elif type == \'val\':\n            # Testing time!\n            start_idx = 0\n            while True:\n                # Batch mask selection\n                X_batch = self.X_val[start_idx:start_idx + self.batch_size]\n                y_batch = self.y_val[start_idx:start_idx + self.batch_size]\n                start_idx += self.batch_size\n\n                # Reset everything\n                if start_idx >= self.val_data_len:\n                    start_idx = 0\n                yield X_batch, y_batch\n        else:\n            raise ValueError(""Please select a type from \\\'train\\\', \\\'val\\\', or \\\'test\\\'"")\n'"
layers.py,82,"b'import tensorflow as tf\nimport numpy as np\n\n\n############################################################################################################\n# Convolution layer Methods\ndef __conv2d_p(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding=\'SAME\', stride=(1, 1),\n               initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0):\n    """"""\n    Convolution 2D Wrapper\n    :param name: (string) The name scope provided by the upper tf.name_scope(\'name\') as scope.\n    :param x: (tf.tensor) The input to the layer (N, H, W, C).\n    :param w: (tf.tensor) pretrained weights (if None, it means no pretrained weights)\n    :param num_filters: (integer) No. of filters (This is the output depth)\n    :param kernel_size: (integer tuple) The size of the convolving kernel.\n    :param padding: (string) The amount of padding required.\n    :param stride: (integer tuple) The stride required.\n    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)\n    :return out: The output of the layer. (N, H\', W\', num_filters)\n    """"""\n    with tf.variable_scope(name):\n        stride = [1, stride[0], stride[1], 1]\n        kernel_shape = [kernel_size[0], kernel_size[1], x.shape[-1], num_filters]\n\n        with tf.name_scope(\'layer_weights\'):\n            if w == None:\n                w = __variable_with_weight_decay(kernel_shape, initializer, l2_strength)\n            __variable_summaries(w)\n        with tf.name_scope(\'layer_biases\'):\n            if isinstance(bias, float):\n                bias = tf.get_variable(\'biases\', [num_filters], initializer=tf.constant_initializer(bias))\n            __variable_summaries(bias)\n        with tf.name_scope(\'layer_conv2d\'):\n            conv = tf.nn.conv2d(x, w, stride, padding)\n            out = tf.nn.bias_add(conv, bias)\n\n    return out\n\n\ndef conv2d(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding=\'SAME\', stride=(1, 1),\n           initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0,\n           activation=None, batchnorm_enabled=False, max_pool_enabled=False, dropout_keep_prob=-1,\n           is_training=True):\n    """"""\n    This block is responsible for a convolution 2D layer followed by optional (non-linearity, dropout, max-pooling).\n    Note that: ""is_training"" should be passed by a correct value based on being in either training or testing.\n    :param name: (string) The name scope provided by the upper tf.name_scope(\'name\') as scope.\n    :param x: (tf.tensor) The input to the layer (N, H, W, C).\n    :param num_filters: (integer) No. of filters (This is the output depth)\n    :param kernel_size: (integer tuple) The size of the convolving kernel.\n    :param padding: (string) The amount of padding required.\n    :param stride: (integer tuple) The stride required.\n    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n    :param bias: (float) Amount of bias.\n    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.\n    :param batchnorm_enabled: (boolean) for enabling batch normalization.\n    :param max_pool_enabled:  (boolean) for enabling max-pooling 2x2 to decrease width and height by a factor of 2.\n    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout\n    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)\n    :return: The output tensor of the layer (N, H\', W\', C\').\n    """"""\n    with tf.variable_scope(name) as scope:\n        conv_o_b = __conv2d_p(\'conv\', x=x, w=w, num_filters=num_filters, kernel_size=kernel_size, stride=stride,\n                              padding=padding,\n                              initializer=initializer, l2_strength=l2_strength, bias=bias)\n\n        if batchnorm_enabled:\n            conv_o_bn = tf.layers.batch_normalization(conv_o_b, training=is_training, epsilon=1e-5)\n            if not activation:\n                conv_a = conv_o_bn\n            else:\n                conv_a = activation(conv_o_bn)\n        else:\n            if not activation:\n                conv_a = conv_o_b\n            else:\n                conv_a = activation(conv_o_b)\n\n        def dropout_with_keep():\n            return tf.nn.dropout(conv_a, dropout_keep_prob)\n\n        def dropout_no_keep():\n            return tf.nn.dropout(conv_a, 1.0)\n\n        if dropout_keep_prob != -1:\n            conv_o_dr = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\n        else:\n            conv_o_dr = conv_a\n\n        conv_o = conv_o_dr\n        if max_pool_enabled:\n            conv_o = max_pool_2d(conv_o_dr)\n\n    return conv_o\n\n\ndef grouped_conv2d(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding=\'SAME\', stride=(1, 1),\n                   initializer=tf.contrib.layers.xavier_initializer(), num_groups=1, l2_strength=0.0, bias=0.0,\n                   activation=None, batchnorm_enabled=False, dropout_keep_prob=-1,\n                   is_training=True):\n    with tf.variable_scope(name) as scope:\n        sz = x.get_shape()[3].value // num_groups\n        conv_side_layers = [\n            conv2d(name + ""_"" + str(i), x[:, :, :, i * sz:i * sz + sz], w, num_filters // num_groups, kernel_size,\n                   padding,\n                   stride,\n                   initializer,\n                   l2_strength, bias, activation=None,\n                   batchnorm_enabled=False, max_pool_enabled=False, dropout_keep_prob=dropout_keep_prob,\n                   is_training=is_training) for i in\n            range(num_groups)]\n        conv_g = tf.concat(conv_side_layers, axis=-1)\n\n        if batchnorm_enabled:\n            conv_o_bn = tf.layers.batch_normalization(conv_g, training=is_training, epsilon=1e-5)\n            if not activation:\n                conv_a = conv_o_bn\n            else:\n                conv_a = activation(conv_o_bn)\n        else:\n            if not activation:\n                conv_a = conv_g\n            else:\n                conv_a = activation(conv_g)\n\n        return conv_a\n\n\ndef __depthwise_conv2d_p(name, x, w=None, kernel_size=(3, 3), padding=\'SAME\', stride=(1, 1),\n                         initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0):\n    with tf.variable_scope(name):\n        stride = [1, stride[0], stride[1], 1]\n        kernel_shape = [kernel_size[0], kernel_size[1], x.shape[-1], 1]\n\n        with tf.name_scope(\'layer_weights\'):\n            if w is None:\n                w = __variable_with_weight_decay(kernel_shape, initializer, l2_strength)\n            __variable_summaries(w)\n        with tf.name_scope(\'layer_biases\'):\n            if isinstance(bias, float):\n                bias = tf.get_variable(\'biases\', [x.shape[-1]], initializer=tf.constant_initializer(bias))\n            __variable_summaries(bias)\n        with tf.name_scope(\'layer_conv2d\'):\n            conv = tf.nn.depthwise_conv2d(x, w, stride, padding)\n            out = tf.nn.bias_add(conv, bias)\n\n    return out\n\n\ndef depthwise_conv2d(name, x, w=None, kernel_size=(3, 3), padding=\'SAME\', stride=(1, 1),\n                     initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0, activation=None,\n                     batchnorm_enabled=False, is_training=True):\n    with tf.variable_scope(name) as scope:\n        conv_o_b = __depthwise_conv2d_p(name=\'conv\', x=x, w=w, kernel_size=kernel_size, padding=padding,\n                                        stride=stride, initializer=initializer, l2_strength=l2_strength, bias=bias)\n\n        if batchnorm_enabled:\n            conv_o_bn = tf.layers.batch_normalization(conv_o_b, training=is_training, epsilon=1e-5)\n            if not activation:\n                conv_a = conv_o_bn\n            else:\n                conv_a = activation(conv_o_bn)\n        else:\n            if not activation:\n                conv_a = conv_o_b\n            else:\n                conv_a = activation(conv_o_b)\n    return conv_a\n\n\n############################################################################################################\n# ShuffleNet unit methods\n\ndef shufflenet_unit(name, x, w=None, num_groups=1, group_conv_bottleneck=True, num_filters=16, stride=(1, 1),\n                    l2_strength=0.0, bias=0.0, batchnorm_enabled=True, is_training=True, fusion=\'add\'):\n    # Paper parameters. If you want to change them feel free to pass them as method parameters.\n    activation = tf.nn.relu\n\n    with tf.variable_scope(name) as scope:\n        residual = x\n        bottleneck_filters = (num_filters // 4) if fusion == \'add\' else (num_filters - residual.get_shape()[\n            3].value) // 4\n\n        if group_conv_bottleneck:\n            bottleneck = grouped_conv2d(\'Gbottleneck\', x=x, w=None, num_filters=bottleneck_filters, kernel_size=(1, 1),\n                                        padding=\'VALID\',\n                                        num_groups=num_groups, l2_strength=l2_strength, bias=bias,\n                                        activation=activation,\n                                        batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n            shuffled = channel_shuffle(\'channel_shuffle\', bottleneck, num_groups)\n        else:\n            bottleneck = conv2d(\'bottleneck\', x=x, w=None, num_filters=bottleneck_filters, kernel_size=(1, 1),\n                                padding=\'VALID\', l2_strength=l2_strength, bias=bias, activation=activation,\n                                batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n            shuffled = bottleneck\n        padded = tf.pad(shuffled, [[0, 0], [1, 1], [1, 1], [0, 0]], ""CONSTANT"")\n        depthwise = depthwise_conv2d(\'depthwise\', x=padded, w=None, stride=stride, l2_strength=l2_strength,\n                                     padding=\'VALID\', bias=bias,\n                                     activation=None, batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n        if stride == (2, 2):\n            residual_pooled = avg_pool_2d(residual, size=(3, 3), stride=stride, padding=\'SAME\')\n        else:\n            residual_pooled = residual\n\n        if fusion == \'concat\':\n            group_conv1x1 = grouped_conv2d(\'Gconv1x1\', x=depthwise, w=None,\n                                           num_filters=num_filters - residual.get_shape()[3].value,\n                                           kernel_size=(1, 1),\n                                           padding=\'VALID\',\n                                           num_groups=num_groups, l2_strength=l2_strength, bias=bias,\n                                           activation=None,\n                                           batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n            return activation(tf.concat([residual_pooled, group_conv1x1], axis=-1))\n        elif fusion == \'add\':\n            group_conv1x1 = grouped_conv2d(\'Gconv1x1\', x=depthwise, w=None,\n                                           num_filters=num_filters,\n                                           kernel_size=(1, 1),\n                                           padding=\'VALID\',\n                                           num_groups=num_groups, l2_strength=l2_strength, bias=bias,\n                                           activation=None,\n                                           batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n            residual_match = residual_pooled\n            # This is used if the number of filters of the residual block is different from that\n            # of the group convolution.\n            if num_filters != residual_pooled.get_shape()[3].value:\n                residual_match = conv2d(\'residual_match\', x=residual_pooled, w=None, num_filters=num_filters,\n                                        kernel_size=(1, 1),\n                                        padding=\'VALID\', l2_strength=l2_strength, bias=bias, activation=None,\n                                        batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n            return activation(group_conv1x1 + residual_match)\n        else:\n            raise ValueError(""Specify whether the fusion is \\\'concat\\\' or \\\'add\\\'"")\n\n\ndef channel_shuffle(name, x, num_groups):\n    with tf.variable_scope(name) as scope:\n        n, h, w, c = x.shape.as_list()\n        x_reshaped = tf.reshape(x, [-1, h, w, num_groups, c // num_groups])\n        x_transposed = tf.transpose(x_reshaped, [0, 1, 2, 4, 3])\n        output = tf.reshape(x_transposed, [-1, h, w, c])\n        return output\n\n\n############################################################################################################\n# Fully Connected layer Methods\n\ndef __dense_p(name, x, w=None, output_dim=128, initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0,\n              bias=0.0):\n    """"""\n    Fully connected layer\n    :param name: (string) The name scope provided by the upper tf.name_scope(\'name\') as scope.\n    :param x: (tf.tensor) The input to the layer (N, D).\n    :param output_dim: (integer) It specifies H, the output second dimension of the fully connected layer [ie:(N, H)]\n    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)\n    :return out: The output of the layer. (N, H)\n    """"""\n    n_in = x.get_shape()[-1].value\n    with tf.variable_scope(name):\n        if w == None:\n            w = __variable_with_weight_decay([n_in, output_dim], initializer, l2_strength)\n        __variable_summaries(w)\n        if isinstance(bias, float):\n            bias = tf.get_variable(""layer_biases"", [output_dim], tf.float32, tf.constant_initializer(bias))\n        __variable_summaries(bias)\n        output = tf.nn.bias_add(tf.matmul(x, w), bias)\n        return output\n\n\ndef dense(name, x, w=None, output_dim=128, initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0,\n          bias=0.0,\n          activation=None, batchnorm_enabled=False, dropout_keep_prob=-1,\n          is_training=True\n          ):\n    """"""\n    This block is responsible for a fully connected followed by optional (non-linearity, dropout, max-pooling).\n    Note that: ""is_training"" should be passed by a correct value based on being in either training or testing.\n    :param name: (string) The name scope provided by the upper tf.name_scope(\'name\') as scope.\n    :param x: (tf.tensor) The input to the layer (N, D).\n    :param output_dim: (integer) It specifies H, the output second dimension of the fully connected layer [ie:(N, H)]\n    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n    :param bias: (float) Amount of bias.\n    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.\n    :param batchnorm_enabled: (boolean) for enabling batch normalization.\n    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout\n    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)\n    :return out: The output of the layer. (N, H)\n    """"""\n    with tf.variable_scope(name) as scope:\n        dense_o_b = __dense_p(name=\'dense\', x=x, w=w, output_dim=output_dim, initializer=initializer,\n                              l2_strength=l2_strength,\n                              bias=bias)\n\n        if batchnorm_enabled:\n            dense_o_bn = tf.layers.batch_normalization(dense_o_b, training=is_training, epsilon=1e-5)\n            if not activation:\n                dense_a = dense_o_bn\n            else:\n                dense_a = activation(dense_o_bn)\n        else:\n            if not activation:\n                dense_a = dense_o_b\n            else:\n                dense_a = activation(dense_o_b)\n\n        def dropout_with_keep():\n            return tf.nn.dropout(dense_a, dropout_keep_prob)\n\n        def dropout_no_keep():\n            return tf.nn.dropout(dense_a, 1.0)\n\n        if dropout_keep_prob != -1:\n            dense_o_dr = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\n        else:\n            dense_o_dr = dense_a\n\n        dense_o = dense_o_dr\n    return dense_o\n\n\ndef flatten(x):\n    """"""\n    Flatten a (N,H,W,C) input into (N,D) output. Used for fully connected layers after conolution layers\n    :param x: (tf.tensor) representing input\n    :return: flattened output\n    """"""\n    all_dims_exc_first = np.prod([v.value for v in x.get_shape()[1:]])\n    o = tf.reshape(x, [-1, all_dims_exc_first])\n    return o\n\n\n############################################################################################################\n# Pooling Methods\n\ndef max_pool_2d(x, size=(2, 2), stride=(2, 2), name=\'pooling\'):\n    """"""\n    Max pooling 2D Wrapper\n    :param x: (tf.tensor) The input to the layer (N,H,W,C).\n    :param size: (tuple) This specifies the size of the filter as well as the stride.\n    :param name: (string) Scope name.\n    :return: The output is the same input but halfed in both width and height (N,H/2,W/2,C).\n    """"""\n    size_x, size_y = size\n    stride_x, stride_y = stride\n    return tf.nn.max_pool(x, ksize=[1, size_x, size_y, 1], strides=[1, stride_x, stride_y, 1], padding=\'VALID\',\n                          name=name)\n\n\ndef avg_pool_2d(x, size=(2, 2), stride=(2, 2), name=\'avg_pooling\', padding=\'VALID\'):\n    """"""\n        Average pooling 2D Wrapper\n        :param x: (tf.tensor) The input to the layer (N,H,W,C).\n        :param size: (tuple) This specifies the size of the filter as well as the stride.\n        :param name: (string) Scope name.\n        :return: The output is the same input but halfed in both width and height (N,H/2,W/2,C).\n    """"""\n    size_x, size_y = size\n    stride_x, stride_y = stride\n    return tf.nn.avg_pool(x, ksize=[1, size_x, size_y, 1], strides=[1, stride_x, stride_y, 1], padding=padding,\n                          name=name)\n\n\n############################################################################################################\n# Utilities for layers\n\ndef __variable_with_weight_decay(kernel_shape, initializer, wd):\n    """"""\n    Create a variable with L2 Regularization (Weight Decay)\n    :param kernel_shape: the size of the convolving weight kernel.\n    :param initializer: The initialization scheme, He et al. normal or Xavier normal are recommended.\n    :param wd:(weight decay) L2 regularization parameter.\n    :return: The weights of the kernel initialized. The L2 loss is added to the loss collection.\n    """"""\n    w = tf.get_variable(\'weights\', kernel_shape, tf.float32, initializer=initializer)\n\n    collection_name = tf.GraphKeys.REGULARIZATION_LOSSES\n    if wd and (not tf.get_variable_scope().reuse):\n        weight_decay = tf.multiply(tf.nn.l2_loss(w), wd, name=\'w_loss\')\n        tf.add_to_collection(collection_name, weight_decay)\n    return w\n\n\n# Summaries for variables\ndef __variable_summaries(var):\n    """"""\n    Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n    :param var: variable to be summarized\n    :return: None\n    """"""\n    with tf.name_scope(\'summaries\'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar(\'mean\', mean)\n        with tf.name_scope(\'stddev\'):\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar(\'stddev\', stddev)\n        tf.summary.scalar(\'max\', tf.reduce_max(var))\n        tf.summary.scalar(\'min\', tf.reduce_min(var))\n        tf.summary.histogram(\'histogram\', var)\n'"
main.py,3,"b'from utils import parse_args, create_experiment_dirs, calculate_flops, show_parameters\nfrom model import ShuffleNet\nfrom train import Train\nfrom data_loader import DataLoader\nfrom summarizer import Summarizer\nimport tensorflow as tf\n\n\ndef main():\n    # Parse the JSON arguments\n    config_args = parse_args()\n\n    # Create the experiment directories\n    _, config_args.summary_dir, config_args.checkpoint_dir = create_experiment_dirs(config_args.experiment_dir)\n\n    # Reset the default Tensorflow graph\n    tf.reset_default_graph()\n\n    # Tensorflow specific configuration\n    config = tf.ConfigProto(allow_soft_placement=True)\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n\n    # Data loading\n    # The batch size is equal to 1 when testing to simulate the real experiment.\n    data_batch_size = config_args.batch_size if config_args.train_or_test == ""train"" else 1\n    data = DataLoader(data_batch_size, config_args.shuffle)\n    print(""Loading Data..."")\n    config_args.img_height, config_args.img_width, config_args.num_channels, \\\n    config_args.train_data_size, config_args.test_data_size = data.load_data()\n    print(""Data loaded\\n\\n"")\n\n    # Model creation\n    print(""Building the model..."")\n    model = ShuffleNet(config_args)\n    print(""Model is built successfully\\n\\n"")\n\n    # Parameters visualization\n    show_parameters()\n\n    # Summarizer creation\n    summarizer = Summarizer(sess, config_args.summary_dir)\n    # Train class\n    trainer = Train(sess, model, data, summarizer)\n\n    if config_args.train_or_test == \'train\':\n        try:\n            # print(""FLOPs for batch size = "" + str(config_args.batch_size) + ""\\n"")\n            # calculate_flops()\n            print(""Training..."")\n            trainer.train()\n            print(""Training Finished\\n\\n"")\n        except KeyboardInterrupt:\n            trainer.save_model()\n\n    elif config_args.train_or_test == \'test\':\n        # print(""FLOPs for single inference \\n"")\n        # calculate_flops()\n        # This can be \'val\' or \'test\' or even \'train\' according to the needs.\n        print(""Testing..."")\n        trainer.test(\'val\')\n        print(""Testing Finished\\n\\n"")\n\n    else:\n        raise ValueError(""Train or Test options only are allowed"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
model.py,35,"b'import tensorflow as tf\nfrom layers import shufflenet_unit, conv2d, max_pool_2d, avg_pool_2d, dense, flatten\n\n\nclass ShuffleNet:\n    """"""ShuffleNet is implemented here!""""""\n    MEAN = [103.94, 116.78, 123.68]\n    NORMALIZER = 0.017\n\n    def __init__(self, args):\n        self.args = args\n        self.X = None\n        self.y = None\n        self.logits = None\n        self.is_training = None\n        self.loss = None\n        self.regularization_loss = None\n        self.cross_entropy_loss = None\n        self.train_op = None\n        self.accuracy = None\n        self.y_out_argmax = None\n        self.summaries_merged = None\n\n        # A number stands for the num_groups\n        # Output channels for conv1 layer\n        self.output_channels = {\'1\': [144, 288, 576], \'2\': [200, 400, 800], \'3\': [240, 480, 960], \'4\': [272, 544, 1088],\n                                \'8\': [384, 768, 1536], \'conv1\': 24}\n\n        self.__build()\n\n    def __init_input(self):\n        batch_size = self.args.batch_size if self.args.train_or_test == \'train\' else 1\n        with tf.variable_scope(\'input\'):\n            # Input images\n            self.X = tf.placeholder(tf.float32,\n                                    [batch_size, self.args.img_height, self.args.img_width,\n                                     self.args.num_channels])\n            # Classification supervision, it\'s an argmax. Feel free to change it to one-hot,\n            # but don\'t forget to change the loss from sparse as well\n            self.y = tf.placeholder(tf.int32, [batch_size])\n            # is_training is for batch normalization and dropout, if they exist\n            self.is_training = tf.placeholder(tf.bool)\n\n    def __resize(self, x):\n        return tf.image.resize_bicubic(x, [224, 224])\n\n    def __stage(self, x, stage=2, repeat=3):\n        if 2 <= stage <= 4:\n            stage_layer = shufflenet_unit(\'stage\' + str(stage) + \'_0\', x=x, w=None,\n                                          num_groups=self.args.num_groups,\n                                          group_conv_bottleneck=not (stage == 2),\n                                          num_filters=\n                                          self.output_channels[str(self.args.num_groups)][\n                                              stage - 2],\n                                          stride=(2, 2),\n                                          fusion=\'concat\', l2_strength=self.args.l2_strength,\n                                          bias=self.args.bias,\n                                          batchnorm_enabled=self.args.batchnorm_enabled,\n                                          is_training=self.is_training)\n            for i in range(1, repeat + 1):\n                stage_layer = shufflenet_unit(\'stage\' + str(stage) + \'_\' + str(i),\n                                              x=stage_layer, w=None,\n                                              num_groups=self.args.num_groups,\n                                              group_conv_bottleneck=True,\n                                              num_filters=self.output_channels[\n                                                  str(self.args.num_groups)][stage - 2],\n                                              stride=(1, 1),\n                                              fusion=\'add\',\n                                              l2_strength=self.args.l2_strength,\n                                              bias=self.args.bias,\n                                              batchnorm_enabled=self.args.batchnorm_enabled,\n                                              is_training=self.is_training)\n            return stage_layer\n        else:\n            raise ValueError(""Stage should be from 2 -> 4"")\n\n    def __init_output(self):\n        with tf.variable_scope(\'output\'):\n            # Losses\n            self.regularization_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n            self.cross_entropy_loss = tf.reduce_mean(\n                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y, name=\'loss\'))\n            self.loss = self.regularization_loss + self.cross_entropy_loss\n\n            # Optimizer\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.args.learning_rate)\n                self.train_op = self.optimizer.minimize(self.loss)\n                # This is for debugging NaNs. Check TensorFlow documentation.\n                self.check_op = tf.add_check_numerics_ops()\n\n            # Output and Metrics\n            self.y_out_softmax = tf.nn.softmax(self.logits)\n            self.y_out_argmax = tf.argmax(self.y_out_softmax, axis=-1, output_type=tf.int32)\n            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.y, self.y_out_argmax), tf.float32))\n\n        with tf.name_scope(\'train-summary-per-iteration\'):\n            tf.summary.scalar(\'loss\', self.loss)\n            tf.summary.scalar(\'acc\', self.accuracy)\n            self.summaries_merged = tf.summary.merge_all()\n\n    def __build(self):\n        self.__init_global_epoch()\n        self.__init_global_step()\n        self.__init_input()\n\n        with tf.name_scope(\'Preprocessing\'):\n            red, green, blue = tf.split(self.X, num_or_size_splits=3, axis=3)\n            preprocessed_input = tf.concat([\n                tf.subtract(blue, ShuffleNet.MEAN[0]) * ShuffleNet.NORMALIZER,\n                tf.subtract(green, ShuffleNet.MEAN[1]) * ShuffleNet.NORMALIZER,\n                tf.subtract(red, ShuffleNet.MEAN[2]) * ShuffleNet.NORMALIZER,\n            ], 3)\n        x_padded = tf.pad(preprocessed_input, [[0, 0], [1, 1], [1, 1], [0, 0]], ""CONSTANT"")\n        conv1 = conv2d(\'conv1\', x=x_padded, w=None, num_filters=self.output_channels[\'conv1\'], kernel_size=(3, 3),\n                       stride=(2, 2), l2_strength=self.args.l2_strength, bias=self.args.bias,\n                       batchnorm_enabled=self.args.batchnorm_enabled, is_training=self.is_training,\n                       activation=tf.nn.relu, padding=\'VALID\')\n        padded = tf.pad(conv1, [[0, 0], [0, 1], [0, 1], [0, 0]], ""CONSTANT"")\n        max_pool = max_pool_2d(padded, size=(3, 3), stride=(2, 2), name=\'max_pool\')\n        stage2 = self.__stage(max_pool, stage=2, repeat=3)\n        stage3 = self.__stage(stage2, stage=3, repeat=7)\n        stage4 = self.__stage(stage3, stage=4, repeat=3)\n        global_pool = avg_pool_2d(stage4, size=(7, 7), stride=(1, 1), name=\'global_pool\', padding=\'VALID\')\n\n        logits_unflattened = conv2d(\'fc\', global_pool, w=None, num_filters=self.args.num_classes,\n                                    kernel_size=(1, 1),\n                                    l2_strength=self.args.l2_strength,\n                                    bias=self.args.bias,\n                                    is_training=self.is_training)\n        self.logits = flatten(logits_unflattened)\n\n        self.__init_output()\n\n    def __init_global_epoch(self):\n        """"""\n        Create a global epoch tensor to totally save the process of the training\n        :return:\n        """"""\n        with tf.variable_scope(\'global_epoch\'):\n            self.global_epoch_tensor = tf.Variable(-1, trainable=False, name=\'global_epoch\')\n            self.global_epoch_input = tf.placeholder(\'int32\', None, name=\'global_epoch_input\')\n            self.global_epoch_assign_op = self.global_epoch_tensor.assign(self.global_epoch_input)\n\n    def __init_global_step(self):\n        """"""\n        Create a global step variable to be a reference to the number of iterations\n        :return:\n        """"""\n        with tf.variable_scope(\'global_step\'):\n            self.global_step_tensor = tf.Variable(0, trainable=False, name=\'global_step\')\n            self.global_step_input = tf.placeholder(\'int32\', None, name=\'global_step_input\')\n            self.global_step_assign_op = self.global_step_tensor.assign(self.global_step_input)\n'"
summarizer.py,4,"b'import tensorflow as tf\n\n\nclass Summarizer:\n    """"""The class responsible for Tensorboard summaries such as loss, and classification accuracy""""""\n\n    def __init__(self, sess, summary_dir):\n        # Summaries\n        self.sess = sess\n        self.scalar_summary_tags = [\'loss\', \'acc\', \'test-loss\', \'test-acc\']\n        self.summary_tags = []\n        self.summary_placeholders = {}\n        self.summary_ops = {}\n        self.summary_writer = tf.summary.FileWriter(summary_dir, self.sess.graph)\n        self.__init_summaries()\n\n    ############################################################################################################\n    # Summaries methods\n    def __init_summaries(self):\n        """"""\n        Create the summary part of the graph\n        :return:\n        """"""\n        with tf.variable_scope(\'train-summary-per-epoch\'):\n            for tag in self.scalar_summary_tags:\n                self.summary_tags += tag\n                self.summary_placeholders[tag] = tf.placeholder(\'float32\', None, name=tag)\n                self.summary_ops[tag] = tf.summary.scalar(tag, self.summary_placeholders[tag])\n\n    def add_summary(self, step, summaries_dict=None, summaries_merged=None):\n        """"""\n        Add the summaries to tensorboard\n        :param step:\n        :param summaries_dict:\n        :param summaries_merged:\n        :return:\n        """"""\n        if summaries_dict is not None:\n            summary_list = self.sess.run([self.summary_ops[tag] for tag in summaries_dict.keys()],\n                                         {self.summary_placeholders[tag]: value for tag, value in\n                                          summaries_dict.items()})\n            for summary in summary_list:\n                self.summary_writer.add_summary(summary, step)\n        if summaries_merged is not None:\n            self.summary_writer.add_summary(summaries_merged, step)\n'"
train.py,5,"b'import tensorflow as tf\nfrom tqdm import tqdm\nimport numpy as np\nfrom utils import load_obj\n\n\nclass Train:\n    """"""Trainer class for the CNN.\n    It\'s also responsible for loading/saving the model checkpoints from/to experiments/experiment_name/checkpoint_dir""""""\n\n    def __init__(self, sess, model, data, summarizer):\n        self.sess = sess\n        self.model = model\n        self.args = self.model.args\n        self.saver = tf.train.Saver(max_to_keep=self.args.max_to_keep,\n                                    keep_checkpoint_every_n_hours=10,\n                                    save_relative_paths=True)\n        # Summarizer references\n        self.data = data\n        self.summarizer = summarizer\n\n        # Initializing the model\n        self.init = None\n        self.__init_model()\n\n        # Loading the model checkpoint if exists\n        self.__load_imagenet_weights()\n        self.__load_model()\n\n    ############################################################################################################\n    # Model related methods\n    def __init_model(self):\n        print(""Initializing the model..."")\n        self.init = tf.group(tf.global_variables_initializer())\n        self.sess.run(self.init)\n        print(""Model initialized\\n\\n"")\n\n    def save_model(self):\n        """"""\n        Save Model Checkpoint\n        :return:\n        """"""\n        print(""Saving a checkpoint"")\n        self.saver.save(self.sess, self.args.checkpoint_dir, self.model.global_step_tensor)\n        print(""Checkpoint Saved\\n\\n"")\n\n    def __load_model(self):\n        latest_checkpoint = tf.train.latest_checkpoint(self.args.checkpoint_dir)\n        if latest_checkpoint:\n            print(""Loading model checkpoint {} ...\\n"".format(latest_checkpoint))\n            self.saver.restore(self.sess, latest_checkpoint)\n            print(""Checkpoint loaded\\n\\n"")\n        else:\n            print(""First time to train!\\n\\n"")\n\n    def __load_imagenet_weights(self):\n        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n        try:\n            print(""Loading ImageNet pretrained weights..."")\n            dict = load_obj(self.args.pretrained_path)\n            run_list = []\n            for variable in variables:\n                for key, value in dict.items():\n                    # Adding \':\' means that we are interested in the variable itself and not the variable parameters\n                    # that are used in adaptive optimizers\n                    if key + "":"" in variable.name:\n                        run_list.append(tf.assign(variable, value))\n            self.sess.run(run_list)\n            print(""Weights loaded\\n\\n"")\n        except KeyboardInterrupt:\n            print(""No pretrained ImageNet weights exist. Skipping...\\n\\n"")\n\n    ############################################################################################################\n    # Train and Test methods\n    def train(self):\n        for cur_epoch in range(self.model.global_epoch_tensor.eval(self.sess) + 1, self.args.num_epochs + 1, 1):\n\n            # Initialize tqdm\n            num_iterations = self.args.train_data_size // self.args.batch_size\n            tqdm_batch = tqdm(self.data.generate_batch(type=\'train\'), total=num_iterations,\n                              desc=""Epoch-"" + str(cur_epoch) + ""-"")\n\n            # Initialize the current iterations\n            cur_iteration = 0\n\n            # Initialize classification accuracy and loss lists\n            loss_list = []\n            acc_list = []\n\n            # Loop by the number of iterations\n            for X_batch, y_batch in tqdm_batch:\n                # Get the current iteration for summarizing it\n                cur_step = self.model.global_step_tensor.eval(self.sess)\n\n                # Feed this variables to the network\n                feed_dict = {self.model.X: X_batch,\n                             self.model.y: y_batch,\n                             self.model.is_training: True\n                             }\n                # Run the feed_forward\n                _, loss, acc, summaries_merged = self.sess.run(\n                    [self.model.train_op, self.model.loss, self.model.accuracy, self.model.summaries_merged],\n                    feed_dict=feed_dict)\n                # Append loss and accuracy\n                loss_list += [loss]\n                acc_list += [acc]\n\n                # Update the Global step\n                self.model.global_step_assign_op.eval(session=self.sess,\n                                                      feed_dict={self.model.global_step_input: cur_step + 1})\n\n                self.summarizer.add_summary(cur_step, summaries_merged=summaries_merged)\n\n                if cur_iteration >= num_iterations - 1:\n                    avg_loss = np.mean(loss_list)\n                    avg_acc = np.mean(acc_list)\n                    # summarize\n                    summaries_dict = dict()\n                    summaries_dict[\'loss\'] = avg_loss\n                    summaries_dict[\'acc\'] = avg_acc\n\n                    # summarize\n                    self.summarizer.add_summary(cur_step, summaries_dict=summaries_dict)\n\n                    # Update the Current Epoch tensor\n                    self.model.global_epoch_assign_op.eval(session=self.sess,\n                                                           feed_dict={self.model.global_epoch_input: cur_epoch + 1})\n\n                    # Print in console\n                    tqdm_batch.close()\n                    print(""Epoch-"" + str(cur_epoch) + "" | "" + ""loss: "" + str(avg_loss) + "" -"" + "" acc: "" + str(\n                        avg_acc)[\n                                                                                                           :7])\n                    # Break the loop to finalize this epoch\n                    break\n\n                # Update the current iteration\n                cur_iteration += 1\n\n            # Save the current checkpoint\n            if cur_epoch % self.args.save_model_every == 0 and cur_epoch != 0:\n                self.save_model()\n\n            # Test the model on validation or test data\n            if cur_epoch % self.args.test_every == 0:\n                self.test(\'val\')\n\n    def test(self, test_type=\'val\'):\n        num_iterations = self.args.test_data_size // self.args.batch_size\n        tqdm_batch = tqdm(self.data.generate_batch(type=test_type), total=num_iterations,\n                          desc=\'Testing\')\n        # Initialize classification accuracy and loss lists\n        loss_list = []\n        acc_list = []\n        cur_iteration = 0\n\n        for X_batch, y_batch in tqdm_batch:\n            # Feed this variables to the network\n            feed_dict = {self.model.X: X_batch,\n                         self.model.y: y_batch,\n                         self.model.is_training: False\n                         }\n            # Run the feed_forward\n            loss, acc = self.sess.run(\n                [self.model.loss, self.model.accuracy],\n                feed_dict=feed_dict)\n\n            # Append loss and accuracy\n            loss_list += [loss]\n            acc_list += [acc]\n\n            if cur_iteration >= num_iterations - 1:\n                avg_loss = np.mean(loss_list)\n                avg_acc = np.mean(acc_list)\n                print(\'Test results | test_loss: \' + str(avg_loss) + \' - test_acc: \' + str(avg_acc)[:7])\n                break\n\n            cur_iteration += 1\n'"
utils.py,6,"b'from easydict import EasyDict as edict\nimport json\nimport argparse\nimport os\nimport tensorflow as tf\nfrom pprint import pprint\nimport sys\n\n\ndef parse_args():\n    """"""\n    Parse the arguments of the program\n    :return: (config_args)\n    :rtype: tuple\n    """"""\n    # Create a parser\n    parser = argparse.ArgumentParser(description=""ShuffleNet TensorFlow Implementation"")\n    parser.add_argument(\'--version\', action=\'version\', version=\'%(prog)s 1.0.0\')\n    parser.add_argument(\'--config\', default=None, type=str, help=\'Configuration file\')\n\n    # Parse the arguments\n    args = parser.parse_args()\n\n    # Parse the configurations from the config json file provided\n    try:\n        if args.config is not None:\n            with open(args.config, \'r\') as config_file:\n                config_args_dict = json.load(config_file)\n        else:\n            print(""Add a config file using \\\'--config file_name.json\\\'"", file=sys.stderr)\n            exit(1)\n\n    except FileNotFoundError:\n        print(""ERROR: Config file not found: {}"".format(args.config), file=sys.stderr)\n        exit(1)\n    except json.decoder.JSONDecodeError:\n        print(""ERROR: Config file is not a proper JSON file!"", file=sys.stderr)\n        exit(1)\n\n    config_args = edict(config_args_dict)\n\n    pprint(config_args)\n    print(""\\n"")\n\n    return config_args\n\n\ndef create_experiment_dirs(exp_dir):\n    """"""\n    Create Directories of a regular tensorflow experiment directory\n    :param exp_dir:\n    :return summary_dir, checkpoint_dir:\n    """"""\n    experiment_dir = os.path.realpath(os.path.join(os.path.dirname(__file__))) + ""/experiments/"" + exp_dir + ""/""\n    summary_dir = experiment_dir + \'summaries/\'\n    checkpoint_dir = experiment_dir + \'checkpoints/\'\n    # output_dir = experiment_dir + \'output/\'\n    # test_dir = experiment_dir + \'test/\'\n    # dirs = [summary_dir, checkpoint_dir, output_dir, test_dir]\n    dirs = [summary_dir, checkpoint_dir]\n    try:\n        for dir_ in dirs:\n            if not os.path.exists(dir_):\n                os.makedirs(dir_)\n        print(""Experiment directories created!"")\n        # return experiment_dir, summary_dir, checkpoint_dir, output_dir, test_dir\n        return experiment_dir, summary_dir, checkpoint_dir\n    except Exception as err:\n        print(""Creating directories error: {0}"".format(err))\n        exit(-1)\n\n\ndef calculate_flops():\n    # Print to stdout an analysis of the number of floating point operations in the\n    # model broken down by individual operations.\n    tf.profiler.profile(\n        tf.get_default_graph(),\n        options=tf.profiler.ProfileOptionBuilder.float_operation(), cmd=\'scope\')\n\n\ndef show_parameters():\n    tf.profiler.profile(\n        tf.get_default_graph(),\n        options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter(), cmd=\'scope\')\n\n\ndef load_obj(filename):\n    import pickle\n    with open(filename, \'rb\') as file:\n        return pickle.load(file)\n'"
