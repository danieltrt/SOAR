file_path,api_count,code
demo.py,7,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Author: Marvin Teichmann\n\n\n""""""\nDetects Cars in an image using KittiBox.\n\nInput: Image\nOutput: Image (with Cars plotted in Green)\n\nUtilizes: Trained KittiBox weights. If no logdir is given,\npretrained weights will be downloaded and used.\n\nUsage:\npython demo.py --input_image data/demo.png [--output_image output_image]\n                [--logdir /path/to/weights] [--gpus 0]\n\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nimport os\nimport sys\n\nimport collections\n\n# configure logging\n\nlogging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                    level=logging.INFO,\n                    stream=sys.stdout)\n\n# https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-220820070\nimport numpy as np\nimport scipy as scp\nimport scipy.misc\nimport tensorflow as tf\n\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nsys.path.insert(1, \'incl\')\nfrom utils import train_utils as kittibox_utils\n\ntry:\n    # Check whether setup was done correctly\n    import tensorvision.utils as tv_utils\n    import tensorvision.core as core\nexcept ImportError:\n    # You forgot to initialize submodules\n    logging.error(""Could not import the submodules."")\n    logging.error(""Please execute:""\n                  ""\'git submodule update --init --recursive\'"")\n    exit(1)\n\n\nflags.DEFINE_string(\'logdir\', None,\n                    \'Path to logdir.\')\nflags.DEFINE_string(\'input_image\', None,\n                    \'Image to apply KittiBox.\')\nflags.DEFINE_string(\'output_image\', None,\n                    \'Image to apply KittiBox.\')\n\n\ndefault_run = \'KittiBox_pretrained\'\nweights_url = (""ftp://mi.eng.cam.ac.uk/""\n               ""pub/mttt2/models/KittiBox_pretrained.zip"")\n\n\ndef maybe_download_and_extract(runs_dir):\n    logdir = os.path.join(runs_dir, default_run)\n\n    if os.path.exists(logdir):\n        # weights are downloaded. Nothing to do\n        return\n\n    if not os.path.exists(runs_dir):\n        os.makedirs(runs_dir)\n\n    import zipfile\n    download_name = tv_utils.download(weights_url, runs_dir)\n\n    logging.info(""Extracting KittiBox_pretrained.zip"")\n\n    zipfile.ZipFile(download_name, \'r\').extractall(runs_dir)\n\n    return\n\n\ndef main(_):\n    tv_utils.set_gpus_to_use()\n\n    if FLAGS.input_image is None:\n        logging.error(""No input_image was given."")\n        logging.info(\n            ""Usage: python demo.py --input_image data/test.png ""\n            ""[--output_image output_image] [--logdir /path/to/weights] ""\n            ""[--gpus GPUs_to_use] "")\n        exit(1)\n\n    if FLAGS.logdir is None:\n        # Download and use weights from the MultiNet Paper\n        if \'TV_DIR_RUNS\' in os.environ:\n            runs_dir = os.path.join(os.environ[\'TV_DIR_RUNS\'],\n                                    \'KittiBox\')\n        else:\n            runs_dir = \'RUNS\'\n        maybe_download_and_extract(runs_dir)\n        logdir = os.path.join(runs_dir, default_run)\n    else:\n        logging.info(""Using weights found in {}"".format(FLAGS.logdir))\n        logdir = FLAGS.logdir\n\n    # Loading hyperparameters from logdir\n    hypes = tv_utils.load_hypes_from_logdir(logdir, base_path=\'hypes\')\n\n    logging.info(""Hypes loaded successfully."")\n\n    # Loading tv modules (encoder.py, decoder.py, eval.py) from logdir\n    modules = tv_utils.load_modules_from_logdir(logdir)\n    logging.info(""Modules loaded successfully. Starting to build tf graph."")\n\n    # Create tf graph and build module.\n    with tf.Graph().as_default():\n        # Create placeholder for input\n        image_pl = tf.placeholder(tf.float32)\n        image = tf.expand_dims(image_pl, 0)\n\n        # build Tensorflow graph using the model from logdir\n        prediction = core.build_inference_graph(hypes, modules,\n                                                image=image)\n\n        logging.info(""Graph build successfully."")\n\n        # Create a session for running Ops on the Graph.\n        sess = tf.Session()\n        saver = tf.train.Saver()\n\n        # Load weights from logdir\n        core.load_weights(logdir, sess, saver)\n\n        logging.info(""Weights loaded successfully."")\n\n    input_image = FLAGS.input_image\n    logging.info(""Starting inference using {} as input"".format(input_image))\n\n    # Load and resize input image\n    image = scp.misc.imread(input_image)\n    image = scp.misc.imresize(image, (hypes[""image_height""],\n                                      hypes[""image_width""]),\n                              interp=\'cubic\')\n    feed = {image_pl: image}\n\n    # Run KittiBox model on image\n    pred_boxes = prediction[\'pred_boxes_new\']\n    pred_confidences = prediction[\'pred_confidences\']\n\n    (np_pred_boxes, np_pred_confidences) = sess.run([pred_boxes,\n                                                     pred_confidences],\n                                                    feed_dict=feed)\n\n    # Apply non-maximal suppression\n    # and draw predictions on the image\n    output_image, rectangles = kittibox_utils.add_rectangles(\n        hypes, [image], np_pred_confidences,\n        np_pred_boxes, show_removed=False,\n        use_stitching=True, rnn_len=1,\n        min_conf=0.50, tau=hypes[\'tau\'], color_acc=(0, 255, 0))\n\n    threshold = 0.5\n    accepted_predictions = []\n    # removing predictions <= threshold\n    for rect in rectangles:\n        if rect.score >= threshold:\n            accepted_predictions.append(rect)\n\n    print(\'\')\n    logging.info(""{} Cars detected"".format(len(accepted_predictions)))\n\n    # Printing coordinates of predicted rects.\n    for i, rect in enumerate(accepted_predictions):\n        logging.info("""")\n        logging.info(""Coordinates of Box {}"".format(i))\n        logging.info(""    x1: {}"".format(rect.x1))\n        logging.info(""    x2: {}"".format(rect.x2))\n        logging.info(""    y1: {}"".format(rect.y1))\n        logging.info(""    y2: {}"".format(rect.y2))\n        logging.info(""    Confidence: {}"".format(rect.score))\n\n    # save Image\n    if FLAGS.output_image is None:\n        output_name = input_image.split(\'.\')[0] + \'_rects.png\'\n    else:\n        output_name = FLAGS.output_image\n\n    scp.misc.imsave(output_name, output_image)\n    logging.info("""")\n    logging.info(""Output image saved to {}"".format(output_name))\n\n    logging.info("""")\n    logging.warning(""Do NOT use this Code to evaluate multiple images."")\n\n    logging.warning(""Demo.py is **very slow** and designed ""\n                    ""to be a tutorial to show how the KittiBox works."")\n    logging.warning("""")\n    logging.warning(""Please see this comment, if you like to apply demo.py to""\n                    ""multiple images see:"")\n    logging.warning(""https://github.com/MarvinTeichmann/KittiBox/""\n                    ""issues/15#issuecomment-301800058"")\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
download_data.py,0,"b'""""""Download data relevant to train the KittiSeg model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport sys\nimport os\nimport subprocess\n\nimport zipfile\n\n\nfrom six.moves import urllib\nfrom shutil import copy2\n\nimport argparse\n\nlogging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                    level=logging.INFO,\n                    stream=sys.stdout)\n\nsys.path.insert(1, \'incl\')\n\n# Please set kitti_data_url to the download link for the Kitti DATA.\n#\n# You can obtain by going to this website:\n# http://www.cvlibs.net/download.php?file=data_road.zip\n#\n# Replace \'http://kitti.is.tue.mpg.de/kitti/?????????.???\' by the\n# correct URL.\n\n\nvgg_url = \'ftp://mi.eng.cam.ac.uk/pub/mttt2/models/vgg16.npy\'\n\n\ndef get_pathes():\n    """"""\n    Get location of `data_dir` and `run_dir\'.\n\n    Defaut is ./DATA and ./RUNS.\n    Alternativly they can be set by the environoment variabels\n    \'TV_DIR_DATA\' and \'TV_DIR_RUNS\'.\n    """"""\n\n    if \'TV_DIR_DATA\' in os.environ:\n        data_dir = os.path.join([\'hypes\'], os.environ[\'TV_DIR_DATA\'])\n    else:\n        data_dir = ""DATA""\n\n    if \'TV_DIR_RUNS\' in os.environ:\n        run_dir = os.path.join([\'hypes\'], os.environ[\'TV_DIR_DATA\'])\n    else:\n        run_dir = ""RUNS""\n\n    return data_dir, run_dir\n\n\ndef download(url, dest_directory):\n    filename = url.split(\'/\')[-1]\n    filepath = os.path.join(dest_directory, filename)\n\n    logging.info(""Download URL: {}"".format(url))\n    logging.info(""Download DIR: {}"".format(dest_directory))\n\n    def _progress(count, block_size, total_size):\n                prog = float(count * block_size) / float(total_size) * 100.0\n                sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' %\n                                 (filename, prog))\n                sys.stdout.flush()\n\n    filepath, _ = urllib.request.urlretrieve(url, filepath,\n                                             reporthook=_progress)\n    print()\n    return filepath\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--kitti_url\', default=\'\', type=str)\n    args = parser.parse_args()\n\n    kitti_data_url = args.kitti_url\n\n    data_dir, run_dir = get_pathes()\n\n    vgg_weights = os.path.join(data_dir, \'vgg16.npy\')\n\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    kitti_dec_dir = os.path.join(data_dir, \'KittiBox\')\n\n    if not os.path.exists(kitti_dec_dir):\n        os.makedirs(kitti_dec_dir)\n\n    # Download VGG DATA\n    if not os.path.exists(vgg_weights):\n        download_command = ""wget {} -P {}"".format(vgg_url, data_dir)\n        logging.info(""Downloading VGG weights."")\n        download(vgg_url, data_dir)\n    else:\n        logging.warning(""File: {} exists."".format(vgg_weights))\n        logging.warning(""Please delete to redownload VGG weights."")\n\n    kitti_dec_zip = os.path.join(kitti_dec_dir, \'data_object_image_2.zip\')\n    kitti_label_zip = os.path.join(kitti_dec_dir, ""data_object_label_2.zip"")\n\n    # Download KITTI DATA\n    if not os.path.exists(kitti_dec_zip):\n        if kitti_data_url == \'\':\n            logging.error(""Data URL for Kitti Data not provided."")\n            url = ""http://www.cvlibs.net/download.php?file=data_object_image_2.zip""\n            logging.error(""Please visit: {}"".format(url))\n            logging.error(""and request Kitti Download link."")\n            logging.error(""Rerun scipt using""\n                          ""\'python download_data.py --kitti_url [url]\'"")\n            exit(1)\n        if not kitti_data_url[-29:] == \'kitti/data_object_image_2.zip\':\n            logging.error(""Wrong url."")\n            url = ""http://www.cvlibs.net/download.php?file=data_object_image_2.zip""\n            logging.error(""Please visit: {}"".format(url))\n            logging.error(""and request Kitti Download link."")\n            logging.error(""Rerun scipt using""\n                          ""\'python download_data.py --kitti_url [url]\'"")\n            exit(1)\n        else:\n            logging.info(""Downloading Kitti Road Data."")\n            download(kitti_data_url, kitti_dec_dir)\n\n    if not os.path.exists(kitti_label_zip):\n        logging.info(""Downloading Kitti Label Data."")\n        kitti_main = os.path.dirname(kitti_data_url)\n        kitti_label_url = os.path.join(kitti_main, kitti_label_zip)\n        kitti_label_url = os.path.join(kitti_main,\n                                       os.path.basename(kitti_label_zip))\n        download(kitti_label_url, kitti_dec_dir)\n\n    # Extract and prepare KITTI DATA\n    logging.info(""Extracting kitti_road data."")\n    zipfile.ZipFile(kitti_dec_zip, \'r\').extractall(kitti_dec_dir)\n    zipfile.ZipFile(kitti_label_zip, \'r\').extractall(kitti_dec_dir)\n\n    logging.info(""Preparing kitti_road data."")\n\n    copyfiles = [""train_2.idl"", ""train_3.idl"", ""train_4.idl"",\n                 ""val_2.idl"", ""val_3.idl"", ""val_4.idl"", ""train.txt"", ""val.txt""]\n\n    for file in copyfiles:\n        filename = os.path.join(\'data\', file)\n        copy2(filename, kitti_dec_dir)\n\n    logging.info(""All data have been downloaded successful."")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
evaluate.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Trains, evaluates and saves the KittiSeg model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nimport os\nimport sys\n\nimport collections\n\n# configure logging\nif \'TV_IS_DEV\' in os.environ and os.environ[\'TV_IS_DEV\']:\n    logging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                        level=logging.INFO,\n                        stream=sys.stdout)\nelse:\n    logging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                        level=logging.INFO,\n                        stream=sys.stdout)\n\n# https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-220820070\nimport numpy as np\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nsys.path.insert(1, \'incl\')\n\nimport tensorvision.train as train\nimport tensorvision.analyze as ana\nimport tensorvision.utils as utils\n\nflags.DEFINE_string(\'RUN\', \'KittiBox_pretrained\',\n                    \'Modifier for model parameters.\')\nflags.DEFINE_string(\'hypes\', \'hypes/kittiBox.json\',\n                    \'File storing model parameters.\')\nflags.DEFINE_string(\'name\', None,\n                    \'Append a name Tag to run.\')\n\nflags.DEFINE_string(\'project\', None,\n                    \'Append a name Tag to run.\')\n\nif \'TV_SAVE\' in os.environ and os.environ[\'TV_SAVE\']:\n    tf.app.flags.DEFINE_boolean(\n        \'save\', True, (\'Whether to save the run. In case --nosave (default) \'\n                       \'output will be saved to the folder TV_DIR_RUNS/debug, \'\n                       \'hence it will get overwritten by further runs.\'))\nelse:\n    tf.app.flags.DEFINE_boolean(\n        \'save\', True, (\'Whether to save the run. In case --nosave (default) \'\n                       \'output will be saved to the folder TV_DIR_RUNS/debug \'\n                       \'hence it will get overwritten by further runs.\'))\n\n\nweights_url = (""ftp://mi.eng.cam.ac.uk/""\n               ""pub/mttt2/models/KittiBox_pretrained.zip"")\n\n\ndef maybe_download_and_extract(runs_dir):\n    logdir = os.path.join(runs_dir, FLAGS.RUN)\n\n    if os.path.exists(logdir):\n        # weights are downloaded. Nothing to do\n        return\n\n    if not FLAGS.RUN == \'KittiBox_pretrained\':\n        return\n\n    import zipfile\n    download_name = utils.download(weights_url, runs_dir)\n\n    logging.info(""Extracting KittiBox_pretrained.zip"")\n\n    zipfile.ZipFile(download_name, \'r\').extractall(runs_dir)\n\n    return\n\n\ndef main(_):\n    utils.set_gpus_to_use()\n\n    try:\n        import tensorvision.train\n        import tensorflow_fcn.utils\n    except ImportError:\n        logging.error(""Could not import the submodules."")\n        logging.error(""Please execute:""\n                      ""\'git submodule update --init --recursive\'"")\n        exit(1)\n\n    with open(tf.app.flags.FLAGS.hypes, \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = json.load(f)\n    utils.load_plugins()\n\n    if \'TV_DIR_RUNS\' in os.environ:\n        runs_dir = os.path.join(os.environ[\'TV_DIR_RUNS\'],\n                                \'KittiBox\')\n    else:\n        runs_dir = \'RUNS\'\n\n    utils.set_dirs(hypes, tf.app.flags.FLAGS.hypes)\n\n    utils._add_paths_to_sys(hypes)\n\n    train.maybe_download_and_extract(hypes)\n\n    maybe_download_and_extract(runs_dir)\n    logging.info(""Evaluating on Validation data."")\n    logdir = os.path.join(runs_dir, FLAGS.RUN)\n    # logging.info(""Output images will be saved to {}"".format)\n    ana.do_analyze(logdir, base_path=\'hypes\')\n\n    logging.info(""Analysis for pretrained model complete."")\n    logging.info(""For evaluating your own models I recommend using:""\n                 ""`tv-analyze --logdir /path/to/run`."")\n    logging.info("""")\n    logging.info(""Output images can be found in {}/analyse/images."".format(\n        logdir))\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
train.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Trains, evaluates and saves the TensorDetect model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nimport os\nimport sys\n\n# configure logging\nif \'TV_IS_DEV\' in os.environ and os.environ[\'TV_IS_DEV\']:\n    logging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                        level=logging.INFO,\n                        stream=sys.stdout)\nelse:\n    logging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                        level=logging.INFO,\n                        stream=sys.stdout)\n\n# https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-220820070\nimport numpy as np\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nsys.path.insert(1, \'incl\')\n\nimport tensorvision.train as train\nimport tensorvision.utils as utils\n\nflags.DEFINE_string(\'name\', None,\n                    \'Append a name Tag to run.\')\n\nflags.DEFINE_string(\'project\', None,\n                    \'Append a name Tag to run.\')\n\nflags.DEFINE_string(\'hypes\', \'hypes/kittiBox.json\',\n                    \'File storing model parameters.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'save\', True, (\'Whether to save the run. In case --nosave (default) \'\n                   \'output will be saved to the folder TV_DIR_RUNS/debug, \'\n                   \'hence it will get overwritten by further runs.\'))\n\n\ndef main(_):\n    utils.set_gpus_to_use()\n\n    try:\n        import tensorvision.train\n    except ImportError:\n        logging.error(""Could not import the submodules."")\n        logging.error(""Please execute:""\n                      ""\'git submodule update --init --recursive\'"")\n        exit(1)\n\n    with open(tf.app.flags.FLAGS.hypes, \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = json.load(f)\n    utils.load_plugins()\n\n    if \'TV_DIR_RUNS\' in os.environ:\n        os.environ[\'TV_DIR_RUNS\'] = os.path.join(os.environ[\'TV_DIR_RUNS\'],\n                                                 \'KittiBox\')\n    utils.set_dirs(hypes, tf.app.flags.FLAGS.hypes)\n\n    utils._add_paths_to_sys(hypes)\n\n    logging.info(""Initialize training folder"")\n    train.initialize_training_folder(hypes)\n    train.maybe_download_and_extract(hypes)\n    logging.info(""Start training"")\n    train.do_training(hypes)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
decoder/fastBox.py,81,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Create the fastbox decoder. For a detailed description see:\nhttps://arxiv.org/abs/1612.07695 .""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport scipy as scp\nimport random\n\nfrom utils import train_utils\nfrom utils import data_utils\n\n\nimport tensorflow as tf\n\n\ndef _rezoom(hyp, pred_boxes, early_feat, early_feat_channels,\n            w_offsets, h_offsets):\n    \'\'\'\n    Rezoom into a feature map at multiple interpolation points\n    in a grid.\n\n    If the predicted object center is at X, len(w_offsets) == 3,\n    and len(h_offsets) == 5,\n    the rezoom grid will look as follows:\n\n    [o o o]\n    [o o o]\n    [o X o]\n    [o o o]\n    [o o o]\n\n    Where each letter indexes into the feature map with bilinear interpolation\n    \'\'\'\n    with tf.name_scope(\'rezoom\'):\n        grid_size = hyp[\'grid_width\'] * hyp[\'grid_height\']\n        outer_size = grid_size * hyp[\'batch_size\']\n        indices = []\n        for w_offset in w_offsets:\n            for h_offset in h_offsets:\n                indices.append(train_utils.bilinear_select(hyp,\n                                                           pred_boxes,\n                                                           early_feat,\n                                                           early_feat_channels,\n                                                           w_offset, h_offset))\n        interp_indices = tf.concat(axis=0, values=indices)\n        rezoom_features = train_utils.interp(early_feat,\n                                             interp_indices,\n                                             early_feat_channels)\n        rezoom_features_r = tf.reshape(rezoom_features,\n                                       [len(w_offsets) * len(h_offsets),\n                                        outer_size,\n                                        hyp[\'rnn_len\'],\n                                        early_feat_channels])\n        rezoom_features_t = tf.transpose(rezoom_features_r, [1, 2, 0, 3])\n        return tf.reshape(rezoom_features_t,\n                          [outer_size,\n                           hyp[\'rnn_len\'],\n                           len(w_offsets) * len(h_offsets) * early_feat_channels])\n\n\ndef _build_inner_layer(hyp, encoded_features, train):\n    \'\'\'\n    Apply an 1x1 convolutions to compute inner features\n    The layer consists of 1x1 convolutions implemented as\n    matrix multiplication. This makes the layer very fast.\n    The layer has ""hyp[\'num_inner_channel\']"" channels\n    \'\'\'\n    grid_size = hyp[\'grid_width\'] * hyp[\'grid_height\']\n    outer_size = grid_size * hyp[\'batch_size\']\n\n    num_ex = hyp[\'batch_size\'] * hyp[\'grid_width\'] * hyp[\'grid_height\']\n\n    channels = int(encoded_features.shape[-1])\n    hyp[\'cnn_channels\'] = channels\n    hidden_input = tf.reshape(encoded_features, [num_ex, channels])\n\n    scale_down = hyp[\'scale_down\']\n\n    hidden_input = tf.reshape(\n        hidden_input * scale_down, (hyp[\'batch_size\'] * grid_size, channels))\n\n    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n    with tf.variable_scope(\'Overfeat\', initializer=initializer):\n        w = tf.get_variable(\'ip\', shape=[hyp[\'cnn_channels\'],\n                                         hyp[\'num_inner_channel\']])\n        output = tf.matmul(hidden_input, w)\n\n    if train:\n        # Adding dropout during training\n        output = tf.nn.dropout(output, 0.5)\n    return output\n\n\ndef _build_output_layer(hyp, hidden_output):\n    \'\'\'\n    Build an 1x1 conv layer.\n    The layer consists of 1x1 convolutions implemented as\n    matrix multiplication. This makes the layer very fast.\n    The layer has ""hyp[\'num_inner_channel\']"" channels\n    \'\'\'\n\n    grid_size = hyp[\'grid_width\'] * hyp[\'grid_height\']\n    outer_size = grid_size * hyp[\'batch_size\']\n\n    box_weights = tf.get_variable(\'box_out\',\n                                  shape=(hyp[\'num_inner_channel\'], 4))\n    conf_weights = tf.get_variable(\'confs_out\',\n                                   shape=(hyp[\'num_inner_channel\'],\n                                          hyp[\'num_classes\']))\n\n    pred_boxes = tf.reshape(tf.matmul(hidden_output, box_weights) * 50,\n                            [outer_size, 1, 4])\n\n    # hyp[\'rnn_len\']\n    pred_logits = tf.reshape(tf.matmul(hidden_output, conf_weights),\n                             [outer_size, 1, hyp[\'num_classes\']])\n\n    pred_logits_squash = tf.reshape(pred_logits,\n                                    [outer_size,\n                                     hyp[\'num_classes\']])\n\n    pred_confidences_squash = tf.nn.softmax(pred_logits_squash)\n    pred_confidences = tf.reshape(pred_confidences_squash,\n                                  [outer_size, hyp[\'rnn_len\'],\n                                   hyp[\'num_classes\']])\n    return pred_boxes, pred_logits, pred_confidences\n\n\ndef _build_rezoom_layer(hyp, rezoom_input, train):\n    with tf.name_scope(\'rezoom_layer\'):\n        grid_size = hyp[\'grid_width\'] * hyp[\'grid_height\']\n        outer_size = grid_size * hyp[\'batch_size\']\n\n        pred_boxes, pred_logits, pred_confidences, early_feat, \\\n            hidden_output = rezoom_input\n\n        early_feat_channels = hyp[\'early_feat_channels\']\n        early_feat = early_feat[:, :, :, :early_feat_channels]\n\n        w_offsets = hyp[\'rezoom_w_coords\']\n        h_offsets = hyp[\'rezoom_h_coords\']\n        num_offsets = len(w_offsets) * len(h_offsets)\n        rezoom_features = _rezoom(\n            hyp, pred_boxes, early_feat, early_feat_channels,\n            w_offsets, h_offsets)\n        if train:\n            rezoom_features = tf.nn.dropout(rezoom_features, 0.5)\n\n        delta_features = tf.concat(\n            axis=1,\n            values=[hidden_output,\n                    rezoom_features[:, 0, :] / 1000.])\n        dim = 128\n        shape = [hyp[\'num_inner_channel\'] +\n                 early_feat_channels * num_offsets,\n                 dim]\n        delta_weights1 = tf.get_variable(\'delta1\',\n                                         shape=shape)\n        # TODO: maybe adding dropout here?\n        ip1 = tf.nn.relu(tf.matmul(delta_features, delta_weights1))\n        if train:\n            ip1 = tf.nn.dropout(ip1, 0.5)\n        delta_confs_weights = tf.get_variable(\n            \'delta2\', shape=[dim, hyp[\'num_classes\']])\n        delta_boxes_weights = tf.get_variable(\'delta_boxes\', shape=[dim, 4])\n\n        rere_feature = tf.matmul(ip1, delta_boxes_weights) * 5\n        pred_boxes_delta = (tf.reshape(rere_feature, [outer_size, 1, 4]))\n\n        scale = hyp.get(\'rezoom_conf_scale\', 50)\n        feature2 = tf.matmul(ip1, delta_confs_weights) * scale\n        pred_confs_delta = tf.reshape(feature2, [outer_size, 1,\n                                      hyp[\'num_classes\']])\n\n        pred_confs_delta = tf.reshape(pred_confs_delta,\n                                      [outer_size, hyp[\'num_classes\']])\n\n        pred_confidences_squash = tf.nn.softmax(pred_confs_delta)\n        pred_confidences = tf.reshape(pred_confidences_squash,\n                                      [outer_size, hyp[\'rnn_len\'],\n                                       hyp[\'num_classes\']])\n\n        return pred_boxes, pred_logits, pred_confidences, \\\n            pred_confs_delta, pred_boxes_delta\n\n\ndef decoder(hyp, logits, train):\n    """"""Apply decoder to the logits.\n\n    Computation which decode CNN boxes.\n    The output can be interpreted as bounding Boxes.\n\n\n    Args:\n      logits: Logits tensor, output von encoder\n\n    Return:\n      decoded_logits: values which can be interpreted as bounding boxes\n    """"""\n    hyp[\'rnn_len\'] = 1\n    encoded_features = logits[\'deep_feat\']\n\n    batch_size = hyp[\'batch_size\']\n    hyp[\'solver\'][\'batch_size\'] = batch_size\n    if not train:\n        hyp[\'batch_size\'] = 1\n\n    early_feat = logits[\'early_feat\']\n\n    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n\n    with tf.variable_scope(\'decoder\', initializer=initializer):\n        with tf.name_scope(\'inner_layer\'):\n            # Build inner layer.\n            # See https://arxiv.org/abs/1612.07695 fig. 2 for details\n            hidden_output = _build_inner_layer(hyp, encoded_features, train)\n\n        with tf.name_scope(\'output_layer\'):\n            # Build output layer\n            # See https://arxiv.org/abs/1612.07695 fig. 2 for details\n            pred_boxes, pred_logits, pred_confidences = _build_output_layer(\n                hyp, hidden_output)\n\n        # Dictionary filled with return values\n        dlogits = {}\n\n        if hyp[\'use_rezoom\']:\n            rezoom_input = pred_boxes, pred_logits, pred_confidences, \\\n                early_feat, hidden_output\n            # Build rezoom layer\n            # See https://arxiv.org/abs/1612.07695 fig. 2 for details\n            rezoom_output = _build_rezoom_layer(hyp, rezoom_input, train)\n\n            pred_boxes, pred_logits, pred_confidences, \\\n                pred_confs_deltas, pred_boxes_deltas = rezoom_output\n\n            dlogits[\'pred_confs_deltas\'] = pred_confs_deltas\n            dlogits[\'pred_boxes_deltas\'] = pred_boxes_deltas\n\n            dlogits[\'pred_boxes_new\'] = pred_boxes + pred_boxes_deltas\n\n    # Fill dict with return values\n    dlogits[\'pred_boxes\'] = pred_boxes\n    dlogits[\'pred_logits\'] = pred_logits\n    dlogits[\'pred_confidences\'] = pred_confidences\n\n    hyp[\'batch_size\'] = batch_size\n\n    return dlogits\n\n\ndef _add_rezoom_loss_histograms(hypes, pred_boxes_deltas):\n    """"""\n    Add some histograms to tensorboard.\n    """"""\n    tf.summary.histogram(\n        \'/delta_hist0_x\', pred_boxes_deltas[:, 0, 0])\n    tf.summary.histogram(\n        \'/delta_hist0_y\', pred_boxes_deltas[:, 0, 1])\n    tf.summary.histogram(\n        \'/delta_hist0_w\', pred_boxes_deltas[:, 0, 2])\n    tf.summary.histogram(\n        \'/delta_hist0_h\', pred_boxes_deltas[:, 0, 3])\n\n\ndef _compute_rezoom_loss(hypes, rezoom_loss_input):\n    """"""\n    Computes loss for delta output. Only relevant\n    if rezoom layers are used.\n    """"""\n    grid_size = hypes[\'grid_width\'] * hypes[\'grid_height\']\n    outer_size = grid_size * hypes[\'batch_size\']\n    head = hypes[\'solver\'][\'head_weights\']\n\n    perm_truth, pred_boxes, classes, pred_mask, \\\n        pred_confs_deltas, pred_boxes_deltas, mask_r = rezoom_loss_input\n    if hypes[\'rezoom_change_loss\'] == \'center\':\n        error = (perm_truth[:, :, 0:2] - pred_boxes[:, :, 0:2]) \\\n            / tf.maximum(perm_truth[:, :, 2:4], 1.)\n        square_error = tf.reduce_sum(tf.square(error), 2)\n        inside = tf.reshape(tf.to_int64(\n            tf.logical_and(tf.less(square_error, 0.2**2),\n                           tf.greater(classes, 0))), [-1])\n    elif hypes[\'rezoom_change_loss\'] == \'iou\':\n        pred_boxes_flat = tf.reshape(pred_boxes, [-1, 4])\n        perm_truth_flat = tf.reshape(perm_truth, [-1, 4])\n        iou = train_utils.iou(train_utils.to_x1y1x2y2(pred_boxes_flat),\n                              train_utils.to_x1y1x2y2(perm_truth_flat))\n        inside = tf.reshape(tf.to_int64(tf.greater(iou, 0.5)), [-1])\n    else:\n        assert not hypes[\'rezoom_change_loss\']\n        inside = tf.reshape(tf.to_int64((tf.greater(classes, 0))), [-1])\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=pred_confs_deltas, labels=inside)\n\n    delta_confs_loss = tf.reduce_sum(cross_entropy*mask_r) \\\n        / outer_size * hypes[\'solver\'][\'head_weights\'][0] * 0.1\n\n    delta_unshaped = perm_truth - (pred_boxes + pred_boxes_deltas)\n\n    delta_residual = tf.reshape(delta_unshaped * pred_mask,\n                                [outer_size, hypes[\'rnn_len\'], 4])\n    sqrt_delta = tf.minimum(tf.square(delta_residual), 10. ** 2)\n    delta_boxes_loss = (tf.reduce_sum(sqrt_delta) /\n                        outer_size * head[1] * 0.03)\n\n    return delta_confs_loss, delta_boxes_loss\n\n\ndef loss(hypes, decoded_logits, labels):\n    """"""Calculate the loss from the logits and the labels.\n\n    Args:\n      decoded_logits: output of decoder\n      labels: Labels tensor; Output from data_input\n\n      flags: 0 if object is present 1 otherwise\n      confidences: ??\n      boxes: encoding of bounding box location\n\n    Returns:\n      loss: Loss tensor of type float.\n    """"""\n\n    confidences, boxes, mask = labels\n\n    pred_boxes = decoded_logits[\'pred_boxes\']\n    pred_logits = decoded_logits[\'pred_logits\']\n    pred_confidences = decoded_logits[\'pred_confidences\']\n\n    pred_confs_deltas = decoded_logits[\'pred_confs_deltas\']\n    pred_boxes_deltas = decoded_logits[\'pred_boxes_deltas\']\n\n    grid_size = hypes[\'grid_width\'] * hypes[\'grid_height\']\n    outer_size = grid_size * hypes[\'batch_size\']\n\n    head = hypes[\'solver\'][\'head_weights\']\n\n    # Compute confidence loss\n    confidences = tf.reshape(confidences, (outer_size, 1))\n    true_classes = tf.reshape(tf.cast(tf.greater(confidences, 0), \'int64\'),\n                              [outer_size])\n\n    pred_classes = tf.reshape(pred_logits, [outer_size, hypes[\'num_classes\']])\n    mask_r = tf.reshape(mask, [outer_size])\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=pred_classes, labels=true_classes)\n\n    # ignore don\'t care areas\n    cross_entropy_sum = (tf.reduce_sum(mask_r*cross_entropy))\n    confidences_loss = cross_entropy_sum / outer_size * head[0]\n\n    true_boxes = tf.reshape(boxes, (outer_size, hypes[\'rnn_len\'], 4))\n\n    # box loss for background prediction needs to be zerod out\n    boxes_mask = tf.reshape(\n        tf.cast(tf.greater(confidences, 0), \'float32\'), (outer_size, 1, 1))\n\n    # danger zone\n    residual = (true_boxes - pred_boxes) * boxes_mask\n\n    boxes_loss = tf.reduce_sum(tf.abs(residual)) / outer_size * head[1]\n\n    if hypes[\'use_rezoom\']:\n        # add rezoom loss\n        rezoom_loss_input = true_boxes, pred_boxes, confidences, boxes_mask, \\\n            pred_confs_deltas, pred_boxes_deltas, mask_r\n\n        delta_confs_loss, delta_boxes_loss = _compute_rezoom_loss(\n            hypes, rezoom_loss_input)\n\n        _add_rezoom_loss_histograms(hypes, pred_boxes_deltas)\n\n        loss = confidences_loss + boxes_loss + delta_boxes_loss \\\n            + delta_confs_loss\n    else:\n        loss = confidences_loss + boxes_loss\n\n    tf.add_to_collection(\'total_losses\', loss)\n\n    reg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\n\n    weight_loss = tf.add_n(tf.get_collection(reg_loss_col),\n                           name=\'reg_loss\')\n\n    total_loss = weight_loss + loss\n\n    losses = {}\n    losses[\'total_loss\'] = total_loss\n    losses[\'loss\'] = loss\n    losses[\'confidences_loss\'] = confidences_loss\n    losses[\'boxes_loss\'] = boxes_loss\n    losses[\'weight_loss\'] = weight_loss\n    if hypes[\'use_rezoom\']:\n        losses[\'delta_boxes_loss\'] = delta_boxes_loss\n        losses[\'delta_confs_loss\'] = delta_confs_loss\n\n    return losses\n\n\ndef evaluation(hyp, images, labels, decoded_logits, losses, global_step):\n    """"""\n    Compute summary metrics for tensorboard\n    """"""\n\n    pred_confidences = decoded_logits[\'pred_confidences\']\n    pred_boxes = decoded_logits[\'pred_boxes\']\n    # Estimating Accuracy\n    grid_size = hyp[\'grid_width\'] * hyp[\'grid_height\']\n    confidences, boxes, mask = labels\n\n    new_shape = [hyp[\'batch_size\'], hyp[\'grid_height\'],\n                 hyp[\'grid_width\'], hyp[\'num_classes\']]\n    pred_confidences_r = tf.reshape(pred_confidences, new_shape)\n    # Set up summary operations for tensorboard\n    a = tf.equal(tf.cast(confidences, \'int64\'),\n                 tf.argmax(pred_confidences_r, 3))\n\n    accuracy = tf.reduce_mean(tf.cast(a, \'float32\'), name=\'/accuracy\')\n\n    eval_list = []\n    eval_list.append((\'Acc.\', accuracy))\n    eval_list.append((\'Conf\', losses[\'confidences_loss\']))\n    eval_list.append((\'Box\', losses[\'boxes_loss\']))\n    eval_list.append((\'Weight\', losses[\'weight_loss\']))\n    if hyp[\'use_rezoom\']:\n        eval_list.append((\'Delta\', losses[\'delta_boxes_loss\'] +\n                          losses[\'delta_confs_loss\']))\n\n    # Log Images\n    # show ground truth to verify labels are correct\n    pred_confidences_r = tf.reshape(\n        pred_confidences,\n        [hyp[\'batch_size\'], grid_size, hyp[\'rnn_len\'], hyp[\'num_classes\']])\n\n    # show predictions to visualize training progress\n    pred_boxes_r = tf.reshape(\n        pred_boxes, [hyp[\'batch_size\'], grid_size, hyp[\'rnn_len\'],\n                     4])\n    test_pred_confidences = pred_confidences_r[0, :, :, :]\n    test_pred_boxes = pred_boxes_r[0, :, :, :]\n\n    def log_image(np_img, np_confidences, np_boxes, np_global_step,\n                  pred_or_true):\n\n        if pred_or_true == \'pred\':\n            plot_image = train_utils.add_rectangles(\n                hyp, np_img, np_confidences, np_boxes, use_stitching=True,\n                rnn_len=hyp[\'rnn_len\'])[0]\n        else:\n            np_mask = np_boxes\n            plot_image = data_utils.draw_encoded(\n                np_img[0], np_confidences[0], mask=np_mask[0], cell_size=32)\n\n        num_images = 10\n\n        filename = \'%s_%s.jpg\' % \\\n            ((np_global_step // hyp[\'logging\'][\'write_iter\'])\n                % num_images, pred_or_true)\n        img_path = os.path.join(hyp[\'dirs\'][\'output_dir\'], filename)\n\n        scp.misc.imsave(img_path, plot_image)\n        return plot_image\n\n    pred_log_img = tf.py_func(log_image,\n                              [images, test_pred_confidences,\n                               test_pred_boxes, global_step, \'pred\'],\n                              [tf.float32])\n\n    true_log_img = tf.py_func(log_image,\n                              [images, confidences,\n                               mask, global_step, \'true\'],\n                              [tf.uint8])\n    tf.summary.image(\'/pred_boxes\', tf.stack(pred_log_img))\n    tf.summary.image(\'/true_boxes\', tf.stack(true_log_img))\n    return eval_list\n'"
encoder/googleNet.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport os\n\nfrom utils import googlenet_load\n\nencoder_net = []\n\n\ndef inference(hypes, images, phase):\n    # Load googlenet and returns the cnn_codes\n\n    if phase == 'train':\n        encoder_net.append(googlenet_load.init(hypes))\n\n    input_mean = 117.\n    images -= input_mean\n    cnn, early_feat, _ = googlenet_load.model(images, encoder_net[0], hypes)\n\n    return cnn, early_feat, _\n"""
encoder/resnet.py,41,"b'""""""\nThe MIT License (MIT)\n\nOriginal Work: Copyright (c) 2016 Ryan Dahl\n(See: https://github.com/ry/tensorflow-resnet)\n\nModified Work: Copyright (c) 2017 Marvin Teichmann\n\nFor details see \'licenses/RESNET_LICENSE.txt\'\n""""""\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.training import moving_averages\n\nimport datetime\nimport numpy as np\nimport os\nimport time\n\nimport logging\n\nMOVING_AVERAGE_DECAY = 0.998\nBN_DECAY = MOVING_AVERAGE_DECAY\nBN_EPSILON = 0.001\nCONV_WEIGHT_DECAY = 0.00004\nCONV_WEIGHT_STDDEV = 0.1\nFC_WEIGHT_DECAY = 0.00004\nFC_WEIGHT_STDDEV = 0.01\nMOMENTUM = 0.9\nRESNET_VARIABLES = \'resnet_variables\'\nUPDATE_OPS_COLLECTION = tf.GraphKeys.UPDATE_OPS\n# must be grouped with training op\nIMAGENET_MEAN_BGR = [103.062623801, 115.902882574, 123.151630838, ]\n\n\nnetwork_url = ""Not yet uploaded.""\n\n\ndef checkpoint_fn(layers):\n    return \'ResNet-L%d.ckpt\' % layers\n\n\ndef inference(hypes, images, train=True,\n              num_classes=1000,\n              num_blocks=[3, 4, 6, 3],  # defaults to 50-layer network\n              preprocess=True,\n              bottleneck=True):\n    # if preprocess is True, input should be RGB [0,1], otherwise BGR with mean\n    # subtracted\n\n    layers = hypes[\'arch\'][\'layers\']\n\n    if layers == 50:\n        num_blocks = [3, 4, 6, 3]\n    elif layers == 101:\n        num_blocks = [3, 4, 23, 3]\n    elif layers == 152:\n        num_blocks = [3, 8, 36, 3]\n    else:\n        assert()\n\n    if preprocess:\n        x = _imagenet_preprocess(images)\n\n    is_training = tf.convert_to_tensor(train,\n                                       dtype=\'bool\',\n                                       name=\'is_training\')\n\n    logits = {}\n\n    with tf.variable_scope(\'scale1\'):\n        x = _conv(x, 64, ksize=7, stride=2)\n        x = _bn(x, is_training, hypes)\n        x = _relu(x)\n        scale1 = x\n\n    with tf.variable_scope(\'scale2\'):\n        x = _max_pool(x, ksize=3, stride=2)\n        x = stack(x, num_blocks[0], 64, bottleneck, is_training, stride=1,\n                  hypes=hypes)\n        scale2 = x\n\n    with tf.variable_scope(\'scale3\'):\n        x = stack(x, num_blocks[1], 128, bottleneck, is_training, stride=2,\n                  hypes=hypes)\n        scale3 = x\n\n    with tf.variable_scope(\'scale4\'):\n        x = stack(x, num_blocks[2], 256, bottleneck, is_training, stride=2,\n                  hypes=hypes)\n        scale4 = x\n\n    with tf.variable_scope(\'scale5\'):\n        x = stack(x, num_blocks[3], 512, bottleneck, is_training, stride=2,\n                  hypes=hypes)\n        scale5 = x\n\n    logits[\'deep_feat\'] = scale5\n\n    logits[\'early_feat\'] = scale3\n\n    if train:\n        restore = tf.global_variables()\n        hypes[\'init_function\'] = _initalize_variables\n        hypes[\'restore\'] = restore\n\n    return logits\n\n\ndef _initalize_variables(hypes):\n    if hypes[\'load_pretrained\']:\n        logging.info(""Pretrained weights are loaded."")\n        logging.info(""The model is fine-tuned from previous training."")\n        restore = hypes[\'restore\']\n        init = tf.global_variables_initializer()\n        sess = tf.get_default_session()\n        sess.run(init)\n\n        saver = tf.train.Saver(var_list=restore)\n\n        layers = hypes[\'arch\'][\'layers\']\n\n        assert layers in [50, 101, 152]\n\n        filename = checkpoint_fn(layers)\n\n        if \'TV_DIR_DATA\' in os.environ:\n            filename = os.path.join(os.environ[\'TV_DIR_DATA\'], \'weights\',\n                                    ""tensorflow_resnet"", filename)\n        else:\n            filename = os.path.join(\'DATA\', \'weights\', ""tensorflow_resnet"",\n                                    filename)\n\n        if not os.path.exists(filename):\n            logging.error(""File not found: {}"".format(filename))\n            logging.error(""Please download weights from here: {}""\n                          .format(\'network_url\'))\n            exit(1)\n\n        logging.info(""Loading weights from disk."")\n        saver.restore(sess, filename)\n    else:\n        logging.info(""Random initialization performed."")\n        sess = tf.get_default_session()\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n\ndef _imagenet_preprocess(rgb):\n    """"""Changes RGB [0,1] valued image to BGR [0,255] with mean subtracted.""""""\n    red, green, blue = tf.split(\n        axis=3, num_or_size_splits=3, value=rgb * 255.0)\n    bgr = tf.concat(axis=3, values=[blue, green, red])\n    bgr -= IMAGENET_MEAN_BGR\n    return bgr\n\n\ndef stack(x, num_blocks, filters_internal, bottleneck, is_training, stride,\n          hypes):\n    for n in range(num_blocks):\n        s = stride if n == 0 else 1\n        with tf.variable_scope(\'block%d\' % (n + 1)):\n            x = block(x,\n                      filters_internal,\n                      bottleneck=bottleneck,\n                      is_training=is_training,\n                      stride=s,\n                      hypes=hypes)\n    return x\n\n\ndef block(x, filters_internal, is_training, stride, bottleneck, hypes):\n    filters_in = x.get_shape()[-1]\n\n    # Note: filters_out isn\'t how many filters are outputed.\n    # That is the case when bottleneck=False but when bottleneck is\n    # True, filters_internal*4 filters are outputted. filters_internal\n    # is how many filters\n    # the 3x3 convs output internally.\n    if bottleneck:\n        filters_out = 4 * filters_internal\n    else:\n        filters_out = filters_internal\n\n    shortcut = x  # branch 1\n\n    if bottleneck:\n        with tf.variable_scope(\'a\'):\n            x = _conv(x, filters_internal, ksize=1, stride=stride)\n            x = _bn(x, is_training, hypes)\n            x = _relu(x)\n\n        with tf.variable_scope(\'b\'):\n            x = _conv(x, filters_internal, ksize=3, stride=1)\n            x = _bn(x, is_training, hypes)\n            x = _relu(x)\n\n        with tf.variable_scope(\'c\'):\n            x = _conv(x, filters_out, ksize=1, stride=1)\n            x = _bn(x, is_training, hypes)\n    else:\n        with tf.variable_scope(\'A\'):\n            x = _conv(x, filters_internal, ksize=3, stride=stride)\n            x = _bn(x, is_training, hypes)\n            x = _relu(x)\n\n        with tf.variable_scope(\'B\'):\n            x = _conv(x, filters_out, ksize=3, stride=1)\n            x = _bn(x, is_training, hypes)\n\n    with tf.variable_scope(\'shortcut\'):\n        if filters_out != filters_in or stride != 1:\n            shortcut = _conv(shortcut, filters_out, ksize=1, stride=stride)\n            shortcut = _bn(shortcut, is_training, hypes)\n\n    return _relu(x + shortcut)\n\n\ndef _relu(x):\n    return tf.nn.relu(x)\n\n\ndef _bn(x, is_training, hypes):\n    x_shape = x.get_shape()\n    params_shape = x_shape[-1:]\n    axis = list(range(len(x_shape) - 1))\n\n    beta = _get_variable(\'beta\',\n                         params_shape,\n                         initializer=tf.zeros_initializer())\n    gamma = _get_variable(\'gamma\',\n                          params_shape,\n                          initializer=tf.ones_initializer())\n\n    moving_mean = _get_variable(\'moving_mean\',\n                                params_shape,\n                                initializer=tf.zeros_initializer(),\n                                trainable=False)\n    moving_variance = _get_variable(\'moving_variance\',\n                                    params_shape,\n                                    initializer=tf.ones_initializer(),\n                                    trainable=False)\n\n    # These ops will only be preformed when training.\n    mean, variance = tf.nn.moments(x, axis)\n\n    update_moving_mean = moving_averages.assign_moving_average(moving_mean,\n                                                               mean,\n                                                               BN_DECAY)\n    update_moving_variance = moving_averages.assign_moving_average(\n        moving_variance, variance, BN_DECAY)\n    if hypes[\'use_moving_average_bn\']:\n        tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_mean)\n        tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_variance)\n\n        mean, variance = control_flow_ops.cond(\n            is_training, lambda: (mean, variance),\n            lambda: (moving_mean, moving_variance))\n    else:\n        mean, variance = mean, variance\n\n    x = tf.nn.batch_normalization(x, mean, variance, beta, gamma, BN_EPSILON)\n    # x.set_shape(inputs.get_shape()) ??\n\n    return x\n\n\ndef _fc(x, num_units_out):\n    num_units_in = x.get_shape()[1]\n    weights_initializer = tf.truncated_normal_initializer(\n        stddev=FC_WEIGHT_STDDEV)\n\n    weights = _get_variable(\'weights\',\n                            shape=[num_units_in, num_units_out],\n                            initializer=weights_initializer,\n                            weight_decay=FC_WEIGHT_STDDEV)\n    biases = _get_variable(\'biases\',\n                           shape=[num_units_out],\n                           initializer=tf.zeros_initializer())\n    x = tf.nn.xw_plus_b(x, weights, biases)\n    return x\n\n\ndef _get_variable(name,\n                  shape,\n                  initializer,\n                  weight_decay=0.0,\n                  dtype=\'float\',\n                  trainable=True):\n    ""A little wrapper around tf.get_variable to do weight decay and add to""\n    ""resnet collection""\n    if weight_decay > 0:\n        regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n    else:\n        regularizer = None\n    collections = [tf.GraphKeys.GLOBAL_VARIABLES, RESNET_VARIABLES]\n    return tf.get_variable(name,\n                           shape=shape,\n                           initializer=initializer,\n                           dtype=dtype,\n                           regularizer=regularizer,\n                           collections=collections,\n                           trainable=trainable)\n\n\ndef _conv(x, filters_out, ksize=3, stride=1):\n    filters_in = x.get_shape()[-1]\n    shape = [ksize, ksize, filters_in, filters_out]\n    initializer = tf.truncated_normal_initializer(stddev=CONV_WEIGHT_STDDEV)\n    weights = _get_variable(\'weights\',\n                            shape=shape,\n                            dtype=\'float\',\n                            initializer=initializer,\n                            weight_decay=CONV_WEIGHT_DECAY)\n    return tf.nn.conv2d(x, weights, [1, stride, stride, 1], padding=\'SAME\')\n\n\ndef _max_pool(x, ksize=3, stride=2):\n    return tf.nn.max_pool(x,\n                          ksize=[1, ksize, ksize, 1],\n                          strides=[1, stride, stride, 1],\n                          padding=\'SAME\')\n'"
encoder/vgg.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow_fcn import fcn8_vgg\n\nimport tensorflow as tf\nimport os\n\n\ndef inference(hypes, images, train=True):\n    """"""Build the MNIST model up to where it may be used for inference.\n\n    Args:\n      images: Images placeholder, from inputs().\n      train: whether the network is used for train of inference\n\n    Returns:\n      softmax_linear: Output tensor with the computed logits.\n    """"""\n    vgg16_npy_path = os.path.join(hypes[\'dirs\'][\'data_dir\'], ""vgg16.npy"")\n    vgg_fcn = fcn8_vgg.FCN8VGG(vgg16_npy_path=vgg16_npy_path)\n\n    \'\'\'\n    num_classes does not influence training when KittiBox is used alone.\n    However, if you wish to use MultiNet with custom submodules, \n    e.g., if KittiSeg is customized for != 2 classes, this value must\n    reflect num_classes in KittiSeg, since MultiNet will try to share \n    variable score_fr/weights\n    \'\'\' \n    num_classes = 2  \n    vgg_fcn.wd = hypes[\'wd\']\n\n    vgg_fcn.build(images, train=train, num_classes=num_classes,\n                  random_init_fc8=True)\n\n    if hypes[\'arch\'][\'deep_feat\'] == ""pool5"":\n        deep_feat = vgg_fcn.pool5\n    elif hypes[\'arch\'][\'deep_feat\'] == ""fc7"":\n        deep_feat = vgg_fcn.fc7\n    else:\n        raise NotImplementedError\n\n    vgg_dict = {\'deep_feat\': deep_feat,\n                \'early_feat\': vgg_fcn.conv4_3}\n\n    return vgg_dict\n'"
evals/kitti_eval.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Trains, evaluates and saves the MediSeg model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport subprocess\n\nimport scipy as scp\nimport scipy.misc\n\nimport numpy as np\n\nimport tensorflow as tf\n\nimport utils.train_utils\nimport time\n\nimport random\n\nfrom utils.annolist import AnnotationLib as AnnLib\n\nimport logging\n\n\ndef make_val_dir(hypes, validation=True):\n    if validation:\n        val_dir = os.path.join(hypes[\'dirs\'][\'output_dir\'], \'val_out\')\n    else:\n        val_dir = os.path.join(hypes[\'dirs\'][\'output_dir\'], \'train_out\')\n    if not os.path.exists(val_dir):\n        os.mkdir(val_dir)\n    return val_dir\n\n\ndef make_img_dir(hypes):\n    val_dir = os.path.join(hypes[\'dirs\'][\'output_dir\'], \'val_images\')\n    if not os.path.exists(val_dir):\n        os.mkdir(val_dir)\n    return val_dir\n\n\ndef write_rects(rects, filename):\n    with open(filename, \'w\') as f:\n        for rect in rects:\n            string = ""Car 0 1 0 %f %f %f %f 0 0 0 0 0 0 0 %f"" % \\\n                (rect.x1, rect.y1, rect.x2, rect.y2, rect.score)\n            print(string, file=f)\n\n\ndef evaluate(hypes, sess, image_pl, softmax):\n    pred_annolist, image_list, dt, dt2 = get_results(\n        hypes, sess, image_pl, softmax, True)\n\n    val_path = make_val_dir(hypes)\n\n    eval_list = []\n\n    eval_cmd = os.path.join(hypes[\'dirs\'][\'base_path\'],\n                            hypes[\'data\'][\'eval_cmd\'])\n\n    label_dir = os.path.join(hypes[\'dirs\'][\'data_dir\'],\n                             hypes[\'data\'][\'label_dir\'])\n\n    try:\n        subprocess.check_call([eval_cmd, val_path, label_dir])\n    except OSError as error:\n        logging.warning(""Failed to run run kitti evaluation code."")\n        logging.warning(""Please run: `cd submodules/KittiObjective2/ && make`"")\n        logging.warning(""For more information see:""\n                        ""`submodules/KittiObjective2/README.md`"")\n        exit(1)\n        img_dir = make_img_dir(hypes)\n        logging.info(""Output images have been written to {}."".format(img_dir))\n        eval_list.append((\'Speed (msec)\', 1000*dt))\n        eval_list.append((\'Speed (fps)\', 1/dt))\n        eval_list.append((\'Post (msec)\', 1000*dt2))\n        return eval_list, image_list\n\n    res_file = os.path.join(val_path, ""stats_car_detection.txt"")\n\n    with open(res_file) as f:\n        for mode in [\'easy\', \'medium\', \'hard\']:\n            line = f.readline()\n            result = np.array(line.rstrip().split("" "")).astype(float)\n            mean = np.mean(result)\n            eval_list.append((""val   "" + mode, mean))\n\n    pred_annolist, image_list2, dt, dt2 = get_results(\n        hypes, sess, image_pl, softmax, False)\n\n    val_path = make_val_dir(hypes, False)\n    subprocess.check_call([eval_cmd, val_path, label_dir])\n    res_file = os.path.join(val_path, ""stats_car_detection.txt"")\n\n    with open(res_file) as f:\n        for mode in [\'easy\', \'medium\', \'hard\']:\n            line = f.readline()\n            result = np.array(line.rstrip().split("" "")).astype(float)\n            mean = np.mean(result)\n            eval_list.append((""train   "" + mode, mean))\n\n    eval_list.append((\'Speed (msec)\', 1000*dt))\n    eval_list.append((\'Speed (fps)\', 1/dt))\n    eval_list.append((\'Post (msec)\', 1000*dt2))\n\n    return eval_list, image_list\n\n\ndef get_results(hypes, sess, image_pl, decoded_logits, validation=True):\n\n    if hypes[\'use_rezoom\']:\n        pred_boxes = decoded_logits[\'pred_boxes_new\']\n    else:\n        pred_boxes = decoded_logits[\'pred_boxes\']\n    pred_confidences = decoded_logits[\'pred_confidences\']\n\n    # Build Placeholder\n    shape = [hypes[\'image_height\'], hypes[\'image_width\'], 3]\n\n    if validation:\n        kitti_txt = os.path.join(hypes[\'dirs\'][\'data_dir\'],\n                                 hypes[\'data\'][\'val_file\'])\n    else:\n        kitti_txt = os.path.join(hypes[\'dirs\'][\'data_dir\'],\n                                 hypes[\'data\'][\'train_file\'])\n    # true_annolist = AnnLib.parse(test_idl)\n\n    val_dir = make_val_dir(hypes, validation)\n    img_dir = make_img_dir(hypes)\n\n    image_list = []\n\n    pred_annolist = AnnLib.AnnoList()\n\n    files = [line.rstrip() for line in open(kitti_txt)]\n    base_path = os.path.realpath(os.path.dirname(kitti_txt))\n\n    for i, file in enumerate(files):\n        image_file = file.split("" "")[0]\n        if not validation and random.random() > 0.2:\n            continue\n        image_file = os.path.join(base_path, image_file)\n        orig_img = scp.misc.imread(image_file)[:, :, :3]\n        img = scp.misc.imresize(orig_img, (hypes[""image_height""],\n                                           hypes[""image_width""]),\n                                interp=\'cubic\')\n        feed = {image_pl: img}\n        (np_pred_boxes, np_pred_confidences) = sess.run([pred_boxes,\n                                                         pred_confidences],\n                                                        feed_dict=feed)\n        pred_anno = AnnLib.Annotation()\n        pred_anno.imageName = image_file\n        new_img, rects = utils.train_utils.add_rectangles(\n            hypes, [img], np_pred_confidences,\n            np_pred_boxes, show_removed=False,\n            use_stitching=True, rnn_len=hypes[\'rnn_len\'],\n            min_conf=0.50, tau=hypes[\'tau\'], color_acc=(0, 255, 0))\n\n        if validation and i % 15 == 0:\n            image_name = os.path.basename(pred_anno.imageName)\n            image_name = os.path.join(img_dir, image_name)\n            scp.misc.imsave(image_name, new_img)\n\n        if validation:\n            image_name = os.path.basename(pred_anno.imageName)\n            image_list.append((image_name, new_img))\n        # get name of file to write to\n        image_name = os.path.basename(image_file)\n        val_file_name = image_name.split(\'.\')[0] + \'.txt\'\n        val_file = os.path.join(val_dir, val_file_name)\n\n        # write rects to file\n\n        pred_anno.rects = rects\n        pred_anno = utils.train_utils.rescale_boxes((\n            hypes[""image_height""],\n            hypes[""image_width""]),\n            pred_anno, orig_img.shape[0],\n            orig_img.shape[1])\n\n        write_rects(rects, val_file)\n\n        pred_annolist.append(pred_anno)\n\n    start_time = time.time()\n    for i in xrange(100):\n        (np_pred_boxes, np_pred_confidences) = sess.run([pred_boxes,\n                                                         pred_confidences],\n                                                        feed_dict=feed)\n    dt = (time.time() - start_time)/100\n\n    start_time = time.time()\n    for i in xrange(100):\n        utils.train_utils.compute_rectangels(\n            hypes, np_pred_confidences,\n            np_pred_boxes, show_removed=False,\n            use_stitching=True, rnn_len=hypes[\'rnn_len\'],\n            min_conf=0.001, tau=hypes[\'tau\'])\n    dt2 = (time.time() - start_time)/100\n\n    return pred_annolist, image_list, dt, dt2\n'"
inputs/kitti_input.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport json\nimport logging\nimport os\nimport sys\nimport random\nfrom random import shuffle\n\nimport numpy as np\n\nimport scipy as scp\nimport scipy.misc\n\nfrom scipy.misc import imread, imresize\n\nimport tensorflow as tf\n\nfrom utils.data_utils import (annotation_jitter, annotation_to_h5)\nfrom utils.annolist import AnnotationLib as AnnoLib\nfrom utils.rect import Rect\n\nimport threading\n\nfrom collections import namedtuple\nfake_anno = namedtuple(\'fake_anno_object\', [\'rects\'])\n\n\ndef read_kitti_anno(label_file, detect_truck):\n    """""" Reads a kitti annotation file.\n\n    Args:\n    label_file: Path to file\n\n    Returns:\n      Lists of rectangels: Cars and don\'t care area.\n    """"""\n    labels = [line.rstrip().split(\' \') for line in open(label_file)]\n    rect_list = []\n    for label in labels:\n        if not (label[0] == \'Car\' or label[0] == \'Van\' or\n                label[0] == \'Truck\' or label[0] == \'DontCare\'):\n            continue\n        notruck = not detect_truck\n        if notruck and label[0] == \'Truck\':\n            continue\n        if label[0] == \'DontCare\':\n            class_id = -1\n        else:\n            class_id = 1\n        object_rect = AnnoLib.AnnoRect(\n            x1=float(label[4]), y1=float(label[5]),\n            x2=float(label[6]), y2=float(label[7]))\n        assert object_rect.x1 < object_rect.x2\n        assert object_rect.y1 < object_rect.y2\n        object_rect.classID = class_id\n        rect_list.append(object_rect)\n\n    return rect_list\n\n\ndef _rescale_boxes(current_shape, anno, target_height, target_width):\n    x_scale = target_width / float(current_shape[1])\n    y_scale = target_height / float(current_shape[0])\n    for r in anno.rects:\n        assert r.x1 < r.x2\n        r.x1 *= x_scale\n        r.x2 *= x_scale\n        assert r.x1 < r.x2\n        r.y1 *= y_scale\n        r.y2 *= y_scale\n    return anno\n\n\ndef _generate_mask(hypes, ignore_rects):\n\n    width = hypes[""image_width""]\n    height = hypes[""image_height""]\n    grid_width = hypes[""grid_width""]\n    grid_height = hypes[""grid_height""]\n\n    mask = np.ones([grid_height, grid_width])\n\n    if not hypes[\'use_mask\']:\n        return mask\n\n    for rect in ignore_rects:\n        left = int((rect.x1+2)/width*grid_width)\n        right = int((rect.x2-2)/width*grid_width)\n        top = int((rect.y1+2)/height*grid_height)\n        bottom = int((rect.y2-2)/height*grid_height)\n        for x in range(left, right+1):\n            for y in range(top, bottom+1):\n                mask[y, x] = 0\n\n    return mask\n\n\ndef _load_kitti_txt(kitti_txt, hypes, jitter=False, random_shuffel=True):\n    """"""Take the txt file and net configuration and create a generator\n    that outputs a jittered version of a random image from the annolist\n    that is mean corrected.""""""\n\n    base_path = os.path.realpath(os.path.dirname(kitti_txt))\n    files = [line.rstrip() for line in open(kitti_txt)]\n    if hypes[\'data\'][\'truncate_data\']:\n        files = files[:10]\n        random.seed(0)\n    for epoch in itertools.count():\n        if random_shuffel:\n            random.shuffle(files)\n        for file in files:\n            image_file, gt_image_file = file.split("" "")\n            image_file = os.path.join(base_path, image_file)\n            assert os.path.exists(image_file), \\\n                ""File does not exist: %s"" % image_file\n            gt_image_file = os.path.join(base_path, gt_image_file)\n            assert os.path.exists(gt_image_file), \\\n                ""File does not exist: %s"" % gt_image_file\n\n            rect_list = read_kitti_anno(gt_image_file,\n                                        detect_truck=hypes[\'detect_truck\'])\n\n            anno = AnnoLib.Annotation()\n            anno.rects = rect_list\n\n            im = scp.misc.imread(image_file)\n            if im.shape[2] == 4:\n                im = im[:, :, :3]\n            if im.shape[0] != hypes[""image_height""] or \\\n               im.shape[1] != hypes[""image_width""]:\n                if True:\n                    anno = _rescale_boxes(im.shape, anno,\n                                          hypes[""image_height""],\n                                          hypes[""image_width""])\n                im = imresize(\n                    im, (hypes[""image_height""], hypes[""image_width""]),\n                    interp=\'cubic\')\n            if jitter:\n                jitter_scale_min = 0.9\n                jitter_scale_max = 1.1\n                jitter_offset = 16\n                im, anno = annotation_jitter(\n                    im, anno, target_width=hypes[""image_width""],\n                    target_height=hypes[""image_height""],\n                    jitter_scale_min=jitter_scale_min,\n                    jitter_scale_max=jitter_scale_max,\n                    jitter_offset=jitter_offset)\n\n            pos_list = [rect for rect in anno.rects if rect.classID == 1]\n            pos_anno = fake_anno(pos_list)\n\n            boxes, confs = annotation_to_h5(hypes,\n                                            pos_anno,\n                                            hypes[""grid_width""],\n                                            hypes[""grid_height""],\n                                            hypes[""rnn_len""])\n\n            mask_list = [rect for rect in anno.rects if rect.classID == -1]\n            mask = _generate_mask(hypes, mask_list)\n\n            boxes = boxes.reshape([hypes[""grid_height""],\n                                   hypes[""grid_width""], 4])\n            confs = confs.reshape(hypes[""grid_height""], hypes[""grid_width""])\n\n            yield {""image"": im, ""boxes"": boxes, ""confs"": confs,\n                   ""rects"": pos_list, ""mask"": mask}\n\n\ndef _make_sparse(n, d):\n    v = np.zeros((d,), dtype=np.float32)\n    v[n] = 1.\n    return v\n\n\ndef create_queues(hypes, phase):\n    """"""Create Queues.""""""\n    hypes[""rnn_len""] = 1\n    dtypes = [tf.float32, tf.float32, tf.float32, tf.float32]\n    grid_size = hypes[\'grid_width\'] * hypes[\'grid_height\']\n    shapes = ([hypes[\'image_height\'], hypes[\'image_width\'], 3],\n              [hypes[\'grid_height\'], hypes[\'grid_width\']],\n              [hypes[\'grid_height\'], hypes[\'grid_width\'], 4],\n              [hypes[\'grid_height\'], hypes[\'grid_width\']])\n    capacity = 30\n    q = tf.FIFOQueue(capacity=capacity, dtypes=dtypes, shapes=shapes)\n    return q\n\n\ndef _processe_image(hypes, image):\n    # Because these operations are not commutative, consider randomizing\n    # randomize the order their operation.\n    augment_level = hypes[\'augment_level\']\n    if augment_level > 0:\n        image = tf.image.random_brightness(image, max_delta=30)\n        image = tf.image.random_contrast(image, lower=0.75, upper=1.25)\n    if augment_level > 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.6)\n        image = tf.image.random_hue(image, max_delta=0.15)\n\n    image = tf.minimum(image, 255.0)\n    image = tf.maximum(image, 0)\n\n    return image\n\n\ndef start_enqueuing_threads(hypes, q, phase, sess):\n    """"""Start enqueuing threads.""""""\n\n    # Creating Placeholder for the Queue\n    x_in = tf.placeholder(tf.float32)\n    confs_in = tf.placeholder(tf.float32)\n    boxes_in = tf.placeholder(tf.float32)\n    mask_in = tf.placeholder(tf.float32)\n\n    # Creating Enqueue OP\n    enqueue_op = q.enqueue((x_in, confs_in, boxes_in, mask_in))\n\n    def make_feed(data):\n        return {x_in: data[\'image\'],\n                confs_in: data[\'confs\'],\n                boxes_in: data[\'boxes\'],\n                mask_in: data[\'mask\']}\n\n    def thread_loop(sess, enqueue_op, gen):\n        for d in gen:\n            sess.run(enqueue_op, feed_dict=make_feed(d))\n\n    data_file = hypes[""data""][\'%s_file\' % phase]\n    data_dir = hypes[\'dirs\'][\'data_dir\']\n    data_file = os.path.join(data_dir, data_file)\n\n    gen = _load_kitti_txt(data_file, hypes,\n                          jitter={\'train\': hypes[\'solver\'][\'use_jitter\'],\n                                  \'val\': False}[phase])\n\n    data = gen.next()\n    sess.run(enqueue_op, feed_dict=make_feed(data))\n    t = threading.Thread(target=thread_loop,\n                         args=(sess, enqueue_op, gen))\n    t.daemon = True\n    t.start()\n\n\ndef test_new_kitti():\n    idlfile = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train_3.idl""\n    kitti_txt = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train.txt""\n\n    with open(\'hypes/kittiBox.json\', \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = json.load(f)\n\n    hypes[""rnn_len""] = 1\n    hypes[""image_height""] = 200\n    hypes[""image_width""] = 800\n\n    gen1 = _load_kitti_txt(kitti_txt, hypes, random_shuffel=False)\n    gen2 = _load_kitti_txt(idlfile, hypes, random_shuffel=False)\n\n    print(\'testing generators\')\n\n    for i in range(20):\n        data1 = gen1.next()\n        data2 = gen2.next()\n        rects1 = data1[\'rects\']\n        rects2 = data2[\'rects\']\n\n        assert len(rects1) <= len(rects2)\n\n        if not len(rects1) == len(rects2):\n            print(\'ignoring flags\')\n            continue\n        else:\n            print(\'comparing flags\')\n            assert(np.all(data1[\'image\'] == data2[\'image\']))\n            # assert(np.all(data1[\'boxes\'] == data2[\'boxes\']))\n            if np.all(data1[\'flags\'] == data2[\'flags\']):\n                print(\'same\')\n            else:\n                print(\'diff\')\n\n\ndef inputs(hypes, q, phase):\n\n    if phase == \'val\':\n        image, confidences, boxes, mask = q.dequeue()\n        image = tf.expand_dims(image, 0)\n        confidences = tf.expand_dims(confidences, 0)\n        boxes = tf.expand_dims(boxes, 0)\n        mask = tf.expand_dims(mask, 0)\n        return image, (confidences, boxes, mask)\n    elif phase == \'train\':\n        image, confidences, boxes, mask = q.dequeue_many(hypes[\'batch_size\'])\n        image = _processe_image(hypes, image)\n        return image, (confidences, boxes, mask)\n    else:\n        assert(""Bad phase: {}"".format(phase))\n'"
optimizer/generic_optimizer.py,7,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport sys\nimport tensorflow as tf\n\n\ndef get_learning_rate(hypes, step):\n    if ""learning_rates"" not in hypes[\'solver\']:\n        lr = hypes[\'solver\'][\'learning_rate\']\n        lr_step = hypes[\'solver\'][\'learning_rate_step\']\n        if lr_step is not None:\n            adjusted_lr = (lr * 0.5 ** max(0, (step / lr_step) - 2))\n            return adjusted_lr\n        else:\n            return lr\n\n    for i, num in enumerate(hypes[\'solver\'][\'steps\']):\n        if step < num:\n            return hypes[\'solver\'][\'learning_rates\'][i]\n\n\ndef training(hypes, loss, global_step, learning_rate):\n    """"""Sets up the training Ops.\n\n    Creates a summarizer to track the loss over time in TensorBoard.\n\n    Creates an optimizer and applies the gradients to all trainable variables.\n\n    The Op returned by this function is what must be passed to the\n    `sess.run()` call to cause the model to train.\n\n    Args:\n      loss: Loss tensor, from loss().\n      global_step: Integer Variable counting the number of training steps\n        processed.\n      learning_rate: The learning rate to use for gradient descent.\n\n    Returns:\n      train_op: The Op for training.\n    """"""\n    # Add a scalar summary for the snapshot loss.\'\'\n    sol = hypes[""solver""]\n    hypes[\'tensors\'] = {}\n    hypes[\'tensors\'][\'global_step\'] = global_step\n    total_loss = loss[\'total_loss\']\n    with tf.name_scope(\'training\'):\n\n        if sol[\'opt\'] == \'RMS\':\n            opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n                                            decay=0.9, epsilon=sol[\'epsilon\'])\n        elif sol[\'opt\'] == \'Adam\':\n            opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\n                                         epsilon=sol[\'epsilon\'])\n        elif sol[\'opt\'] == \'SGD\':\n            lr = learning_rate\n            opt = tf.train.GradientDescentOptimizer(learning_rate=lr)\n        else:\n            raise ValueError(\'Unrecognized opt type\')\n\n        grads_and_vars = opt.compute_gradients(total_loss)\n\n        if hypes[\'clip_norm\'] > 0:\n            grads, tvars = zip(*grads_and_vars)\n            clip_norm = hypes[""clip_norm""]\n            clipped_grads, norm = tf.clip_by_global_norm(grads, clip_norm)\n            grads_and_vars = zip(clipped_grads, tvars)\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n        with tf.control_dependencies(update_ops):\n            train_op = opt.apply_gradients(grads_and_vars,\n                                           global_step=global_step)\n\n    return train_op\n'"
tests/test_anno_load.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport json\nimport logging\nimport os\nimport sys\nimport random\nfrom random import shuffle\n\nimport numpy as np\n\nimport scipy as scp\nimport scipy.misc\n\nsys.path.insert(1, \'../incl\')\nfrom scipy.misc import imread, imresize\n\nfrom utils.data_utils import (annotation_jitter, annotation_to_h5)\nfrom utils.annolist import AnnotationLib as AnnoLib\n\nimport threading\n\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n\ntf.app.flags.DEFINE_boolean(\n    \'save\', False, (\'Whether to save the run. In case --nosave (default) \'\n                    \'output will be saved to the folder TV_DIR_RUNS/debug, \'\n                    \'hence it will get overwritten by further runs.\'))\n\nflags.DEFINE_string(\'name\', None,\n                    \'Append a name Tag to run.\')\n\n\nfake_anno = namedtuple(\'fake_anno_object\', [\'rects\'])\n\n\nfrom PIL import Image, ImageDraw\n\nrect = namedtuple(\'Rectangel\', [\'left\', \'top\', \'right\', \'bottom\'])\n\n\ndef _get_ignore_rect(x, y, cell_size):\n    left = x*cell_size\n    right = (x+1)*cell_size\n    top = y*cell_size\n    bottom = (y+1)*cell_size\n\n    return rect(left, top, right, bottom)\n\n\ndef _rescale_boxes(current_shape, anno, target_height, target_width):\n    x_scale = target_width / float(current_shape[1])\n    y_scale = target_height / float(current_shape[0])\n    for r in anno.rects:\n        assert r.x1 < r.x2\n        r.x1 *= x_scale\n        r.x2 *= x_scale\n        assert r.y1 < r.y2\n        r.y1 *= y_scale\n        r.y2 *= y_scale\n    return anno\n\n\ndef read_kitti_anno(label_file):\n    """""" Reads a kitti annotation file.\n\n    Args:\n    label_file: Path to file\n\n    Returns:\n      Lists of rectangels: Cars and don\'t care area.\n    """"""\n    labels = [line.rstrip().split(\' \') for line in open(label_file)]\n    rect_list = []\n    for label in labels:\n        if not (label[0] == \'Car\' or label[0] == \'Van\' or\n                label[0] == \'DontCare\'):\n            continue\n        if label[0] == \'DontCare\':\n            class_id = -1\n        else:\n            class_id = 1\n        object_rect = AnnoLib.AnnoRect(\n            x1=float(label[4]), y1=float(label[5]),\n            x2=float(label[6]), y2=float(label[7]))\n        assert object_rect.x1 < object_rect.x2\n        assert object_rect.y1 < object_rect.y2\n        object_rect.classID = class_id\n        rect_list.append(object_rect)\n\n    return rect_list\n\n\ndef _load_idl_tf(idlfile, hypes, jitter=False, random_shuffel=True):\n    """"""Take the idlfile and net configuration and create a generator\n    that outputs a jittered version of a random image from the annolist\n    that is mean corrected.""""""\n\n    annolist = AnnoLib.parse(idlfile)\n    annos = []\n    for anno in annolist:\n        anno.imageName = os.path.join(\n            os.path.dirname(os.path.realpath(idlfile)), anno.imageName)\n        annos.append(anno)\n    random.seed(0)\n    if hypes[\'data\'][\'truncate_data\']:\n        annos = annos[:10]\n    for epoch in itertools.count():\n        if random_shuffel:\n            random.shuffle(annos)\n        for anno in annos:\n            im = imread(anno.imageName)\n            if im.shape[2] == 4:\n                im = im[:, :, :3]\n            if im.shape[0] != hypes[""image_height""] or \\\n               im.shape[1] != hypes[""image_width""]:\n                if epoch == 0:\n                    anno = _rescale_boxes(im.shape, anno,\n                                          hypes[""image_height""],\n                                          hypes[""image_width""])\n                im = imresize(\n                    im, (hypes[""image_height""], hypes[""image_width""]),\n                    interp=\'cubic\')\n            if jitter:\n                jitter_scale_min = 0.9\n                jitter_scale_max = 1.1\n                jitter_offset = 16\n                im, anno = annotation_jitter(\n                    im, anno, target_width=hypes[""image_width""],\n                    target_height=hypes[""image_height""],\n                    jitter_scale_min=jitter_scale_min,\n                    jitter_scale_max=jitter_scale_max,\n                    jitter_offset=jitter_offset)\n\n            boxes, flags = annotation_to_h5(hypes,\n                                            anno,\n                                            hypes[""grid_width""],\n                                            hypes[""grid_height""],\n                                            hypes[""rnn_len""])\n\n            boxes = boxes.reshape([hypes[""grid_height""],\n                                   hypes[""grid_width""], 4])\n            flags = flags.reshape(hypes[""grid_height""], hypes[""grid_width""])\n\n            yield {""image"": im, ""boxes"": boxes, ""flags"": flags,\n                   ""rects"": anno.rects, ""anno"": anno}\n\n\ndef _generate_mask(hypes, ignore_rects):\n\n    width = hypes[""image_width""]\n    height = hypes[""image_height""]\n    grid_width = hypes[""grid_width""]\n    grid_height = hypes[""grid_height""]\n\n    mask = np.ones([grid_height, grid_width])\n\n    for rect in ignore_rects:\n        left = int(rect.x1/width*grid_width)\n        right = int(rect.x2/width*grid_width)\n        top = int(rect.y1/height*grid_height)\n        bottom = int(rect.y2/height*grid_height)\n        for x in range(left, right+1):\n            for y in range(top, bottom+1):\n                mask[y, x] = 0\n\n    return mask\n\n\ndef _load_kitti_txt(kitti_txt, hypes, jitter=False, random_shuffel=True):\n    """"""Take the txt file and net configuration and create a generator\n    that outputs a jittered version of a random image from the annolist\n    that is mean corrected.""""""\n\n    base_path = os.path.realpath(os.path.dirname(kitti_txt))\n    files = [line.rstrip() for line in open(kitti_txt)]\n    if hypes[\'data\'][\'truncate_data\']:\n        files = files[:10]\n        random.seed(0)\n    for epoch in itertools.count():\n        if random_shuffel:\n            random.shuffle(files)\n        for file in files:\n            image_file, gt_image_file = file.split("" "")\n            image_file = os.path.join(base_path, image_file)\n            assert os.path.exists(image_file), \\\n                ""File does not exist: %s"" % image_file\n            gt_image_file = os.path.join(base_path, gt_image_file)\n            assert os.path.exists(gt_image_file), \\\n                ""File does not exist: %s"" % gt_image_file\n\n            rect_list = read_kitti_anno(gt_image_file)\n\n            anno = fake_anno(rect_list)\n\n            im = scp.misc.imread(image_file)\n            if im.shape[2] == 4:\n                im = im[:, :, :3]\n            if im.shape[0] != hypes[""image_height""] or \\\n               im.shape[1] != hypes[""image_width""]:\n                if epoch == 0:\n                    anno = _rescale_boxes(im.shape, anno,\n                                          hypes[""image_height""],\n                                          hypes[""image_width""])\n                im = imresize(\n                    im, (hypes[""image_height""], hypes[""image_width""]),\n                    interp=\'cubic\')\n            if jitter:\n                jitter_scale_min = 0.9\n                jitter_scale_max = 1.1\n                jitter_offset = 16\n                im, anno = annotation_jitter(\n                    im, anno, target_width=hypes[""image_width""],\n                    target_height=hypes[""image_height""],\n                    jitter_scale_min=jitter_scale_min,\n                    jitter_scale_max=jitter_scale_max,\n                    jitter_offset=jitter_offset)\n\n            pos_list = [rect for rect in anno.rects if rect.classID == 1]\n            pos_anno = fake_anno(pos_list)\n\n            boxes, confs = annotation_to_h5(hypes,\n                                            pos_anno,\n                                            hypes[""grid_width""],\n                                            hypes[""grid_height""],\n                                            hypes[""rnn_len""])\n\n            mask_list = [rect for rect in anno.rects if rect.classID == -1]\n            mask = _generate_mask(hypes, mask_list)\n\n            boxes = boxes.reshape([hypes[""grid_height""],\n                                   hypes[""grid_width""], 4])\n            confs = confs.reshape(hypes[""grid_height""], hypes[""grid_width""])\n\n            yield {""image"": im, ""boxes"": boxes, ""confs"": confs,\n                   ""rects"": pos_list, ""mask"": mask}\n\n\ndef _make_sparse(n, d):\n    v = np.zeros((d,), dtype=np.float32)\n    v[n] = 1.\n    return v\n\n\ndef _load_data_gen(hypes, phase, jitter):\n    grid_size = hypes[\'grid_width\'] * hypes[\'grid_height\']\n\n    data_file = hypes[""data""][\'%s_idl\' % phase]\n    data_dir = hypes[\'dirs\'][\'data_dir\']\n    data_file = os.path.join(data_dir, data_file)\n\n    data = _load_idl_tf(data_file, hypes,\n                        jitter={\'train\': jitter, \'val\': False}[phase])\n\n    for d in data:\n        output = {}\n        rnn_len = hypes[""rnn_len""]\n        flags = d[\'flags\'][0, :, 0, 0:rnn_len, 0]\n        boxes = np.transpose(d[\'boxes\'][0, :, :, 0:rnn_len, 0], (0, 2, 1))\n        assert(flags.shape == (grid_size, rnn_len))\n        assert(boxes.shape == (grid_size, rnn_len, 4))\n\n        output[\'image\'] = d[\'image\']\n        confs = [[_make_sparse(int(detection), d=hypes[\'num_classes\'])\n                  for detection in cell] for cell in flags]\n        output[\'confs\'] = np.array(confs)\n        output[\'boxes\'] = boxes\n        output[\'flags\'] = flags\n\n        yield output\n\n\ndef test_new_kitti():\n    idlfile = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train_3.idl""\n    kitti_txt = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train.txt""\n\n    with open(\'../hypes/kittiBox.json\', \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = json.load(f)\n\n    hypes[""rnn_len""] = 1\n    hypes[""image_height""] = 200\n    hypes[""image_width""] = 800\n\n    gen1 = _load_kitti_txt(kitti_txt, hypes, random_shuffel=False)\n    gen2 = _load_idl_tf(idlfile, hypes, random_shuffel=False)\n\n    print(\'testing generators\')\n\n    for i in range(20):\n        data1 = gen1.next()\n        data2 = gen2.next()\n        rects1 = data1[\'rects\']\n        rects2 = data2[\'rects\']\n\n        assert len(rects1) <= len(rects2)\n\n        if not len(rects1) == len(rects2):\n            print(\'ignoring flags\')\n            continue\n        else:\n            print(\'comparing flags\')\n            assert(np.all(data1[\'image\'] == data2[\'image\']))\n            # assert(np.all(data1[\'boxes\'] == data2[\'boxes\']))\n            if np.all(data1[\'flags\'] == data2[\'flags\']):\n                print(\'same\')\n            else:\n                print(\'diff\')\n\n\ndef draw_rect(draw, rect, color):\n    rect_cords = ((rect.left, rect.top), (rect.left, rect.bottom),\n                  (rect.right, rect.bottom), (rect.right, rect.top),\n                  (rect.left, rect.top))\n    draw.line(rect_cords, fill=color, width=2)\n\n\ndef draw_encoded(image, confs, mask=None, rects=None, cell_size=32):\n    image = image.astype(\'uint8\')\n    im = Image.fromarray(image)\n\n    shape = confs.shape\n    if mask is None:\n        mask = np.ones(shape)\n\n    # overimage = mycm(confs_pred, bytes=True)\n\n    poly = Image.new(\'RGBA\', im.size)\n    pdraw = ImageDraw.Draw(poly)\n\n    for y in range(shape[0]):\n        for x in range(shape[1]):\n            outline = (0, 0, 0, 255)\n            if confs[y, x]:\n                fill = (0, 255, 0, 100)\n            else:\n                fill = (0, 0, 0, 0)\n            rect = _get_ignore_rect(x, y, cell_size)\n            pdraw.rectangle(rect, fill=fill,\n                            outline=fill)\n            if not mask[y, x]:\n                pdraw.line(((rect.left, rect.bottom), (rect.right, rect.top)),\n                           fill=(0, 0, 0, 255), width=2)\n                pdraw.line(((rect.left, rect.top), (rect.right, rect.bottom)),\n                           fill=(0, 0, 0, 255), width=2)\n\n    color = (0, 0, 255)\n    for rect in rects:\n        rect_cords = ((rect.x1, rect.y1), (rect.x1, rect.y2),\n                      (rect.x2, rect.y2), (rect.x2, rect.y1),\n                      (rect.x1, rect.y1))\n        pdraw.line(rect_cords, fill=color, width=2)\n\n    im.paste(poly, mask=poly)\n\n    return np.array(im)\n\n\ndef draw_kitti_jitter():\n    idlfile = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train_3.idl""\n    kitti_txt = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train.txt""\n\n    with open(\'../hypes/kittiBox.json\', \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = json.load(f)\n\n    hypes[""rnn_len""] = 1\n    gen = _load_kitti_txt(kitti_txt, hypes, random_shuffel=False)\n\n    data = gen.next()\n    for i in range(20):\n        data = gen.next()\n        image = draw_encoded(image=data[\'image\'], confs=data[\'confs\'],\n                             rects=data[\'rects\'], mask=data[\'mask\'])\n\n        scp.misc.imshow(image)\n        scp.misc.imshow(data[\'mask\'])\n\n\ndef draw_idl():\n    idlfile = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train_3.idl""\n    kitti_txt = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train.txt""\n\n    with open(\'../hypes/kittiBox.json\', \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = json.load(f)\n\n    hypes[""rnn_len""] = 1\n\n    gen = _load_idl_tf(idlfile, hypes, random_shuffel=False)\n\n    data = gen.next()\n    for i in range(20):\n        data = gen.next()\n        image = draw_encoded(image=data[\'image\'], confs=data[\'flags\'],\n                             rects=data[\'rects\'])\n\n        scp.misc.imshow(image)\n\n\ndef draw_both():\n    idlfile = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train_3.idl""\n    kitti_txt = ""/home/mifs/mttt2/cvfs/DATA/KittiBox/train.txt""\n\n    with open(\'../hypes/kittiBox.json\', \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = json.load(f)\n\n    hypes[""rnn_len""] = 1\n\n    gen1 = _load_idl_tf(idlfile, hypes, random_shuffel=False)\n    gen2 = _load_kitti_txt(kitti_txt, hypes, random_shuffel=False)\n\n    data1 = gen1.next()\n    data2 = gen2.next()\n    for i in range(20):\n        data1 = gen1.next()\n        data2 = gen2.next()\n        image1 = draw_encoded(image=data1[\'image\'], confs=data1[\'flags\'],\n                              rects=data1[\'rects\'])\n        image2 = draw_encoded(image=data2[\'image\'], confs=data2[\'confs\'],\n                              rects=data2[\'rects\'], mask=data2[\'mask\'])\n\n        scp.misc.imshow(image1)\n\n        scp.misc.imshow(image2)\n\n\nif __name__ == \'__main__\':\n\n    draw_both()\n'"
submodules/utils/__init__.py,0,b''
submodules/utils/data_utils.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\n\nimport os\nimport re\nimport sys\nimport argparse\nimport numpy as np\nimport copy\nimport utils.annolist.AnnotationLib as al\n\nimport scipy as scp\nimport scipy.misc\n\nimport random\n\nfrom collections import namedtuple\n\n\ndef annotation_to_h5(H, a, cell_width, cell_height, max_len):\n    region_size = H[\'region_size\']\n    # assert H[\'region_size\'] == H[\'image_height\'] / H[\'grid_height\']\n    # assert H[\'region_size\'] == H[\'image_width\'] / H[\'grid_width\']\n    cell_regions = get_cell_grid(cell_width, cell_height, region_size)\n\n    cells_per_image = len(cell_regions)\n\n    box_list = [[] for idx in range(cells_per_image)]\n\n    for cidx, c in enumerate(cell_regions):\n        box_list[cidx] = [r for r in a.rects if all(r.intersection(c))]\n\n    boxes = np.zeros((1, cells_per_image, 4, max_len, 1), dtype=np.float)\n    box_flags = np.zeros((1, cells_per_image, 1, max_len, 1), dtype=np.float)\n\n    for cidx in xrange(cells_per_image):\n        # assert(cur_num_boxes <= max_len)\n\n        cell_ox = 0.5 * (cell_regions[cidx].x1 + cell_regions[cidx].x2)\n        cell_oy = 0.5 * (cell_regions[cidx].y1 + cell_regions[cidx].y2)\n\n        unsorted_boxes = []\n        for bidx in xrange(min(len(box_list[cidx]), max_len)):\n\n            # relative box position with respect to cell\n            ox = 0.5 * \\\n                (box_list[cidx][bidx].x1 + box_list[cidx][bidx].x2) - cell_ox\n            oy = 0.5 * \\\n                (box_list[cidx][bidx].y1 + box_list[cidx][bidx].y2) - cell_oy\n\n            width = abs(box_list[cidx][bidx].x2 - box_list[cidx][bidx].x1)\n            height = abs(box_list[cidx][bidx].y2 - box_list[cidx][bidx].y1)\n\n            if (abs(ox) < H[\'focus_size\'] * region_size and abs(oy) < H[\'focus_size\'] * region_size and\n                    width < H[\'biggest_box_px\'] and height < H[\'biggest_box_px\']):\n                unsorted_boxes.append(\n                    np.array([ox, oy, width, height], dtype=np.float))\n\n        for bidx, box in enumerate(sorted(unsorted_boxes, key=lambda x: x[0]**2 + x[1]**2)):\n            boxes[0, cidx, :, bidx, 0] = box\n            box_flags[0, cidx, 0, bidx, 0] = max(\n                box_list[cidx][bidx].silhouetteID, 1)\n\n    return boxes, box_flags\n\n\ndef get_cell_grid(cell_width, cell_height, region_size):\n\n    cell_regions = []\n    for iy in xrange(cell_height):\n        for ix in xrange(cell_width):\n            cidx = iy * cell_width + ix\n            ox = (ix + 0.5) * region_size\n            oy = (iy + 0.5) * region_size\n\n            r = al.AnnoRect(ox - 0.5 * region_size, oy - 0.5 * region_size,\n                            ox + 0.5 * region_size, oy + 0.5 * region_size)\n            r.track_id = cidx\n\n            cell_regions.append(r)\n\n    return cell_regions\n\n\ndef annotation_jitter(I, a_in, min_box_width=20, jitter_scale_min=0.9, jitter_scale_max=1.1, jitter_offset=16, target_width=640, target_height=480):\n    a = copy.deepcopy(a_in)\n\n    # MA: sanity check\n    new_rects = []\n    for i in range(len(a.rects)):\n        r = a.rects[i]\n        try:\n            assert(r.x1 < r.x2 and r.y1 < r.y2)\n            new_rects.append(r)\n        except:\n            print(\'bad rectangle\')\n    a.rects = new_rects\n\n    if a.rects:\n        cur_min_box_width = min([r.width() for r in a.rects])\n    else:\n        cur_min_box_width = min_box_width / jitter_scale_min\n\n    # don\'t downscale below min_box_width\n    jitter_scale_min = max(\n        jitter_scale_min, float(min_box_width) / cur_min_box_width)\n\n    # it\'s always ok to upscale\n    jitter_scale_min = min(jitter_scale_min, 1.0)\n\n    jitter_scale_max = jitter_scale_max\n\n    jitter_scale = np.random.uniform(jitter_scale_min, jitter_scale_max)\n\n    jitter_flip = np.random.random_integers(0, 1)\n\n    if jitter_flip == 1:\n        I = np.fliplr(I)\n\n        for r in a:\n            r.x1 = I.shape[1] - r.x1\n            r.x2 = I.shape[1] - r.x2\n            r.x1, r.x2 = r.x2, r.x1\n\n            for p in r.point:\n                p.x = I.shape[1] - p.x\n\n    if random.random > 0.8:\n        return I, a\n\n    I1 = scp.misc.imresize(I, jitter_scale, interp=\'cubic\')\n\n    print(""Resized image with scale {}"".format(jitter_scale))\n\n    jitter_offset_x = np.random.random_integers(-jitter_offset, jitter_offset)\n    jitter_offset_y = np.random.random_integers(-jitter_offset, jitter_offset)\n\n    rescaled_width = I1.shape[1]\n    rescaled_height = I1.shape[0]\n\n\n\n    px = round(0.5*(target_width)) - \\\n        round(0.5*(rescaled_width)) + jitter_offset_x\n    py = round(0.5*(target_height)) - \\\n        round(0.5*(rescaled_height)) + jitter_offset_y\n\n    I2 = np.zeros((target_height, target_width, 3), dtype=I1.dtype)\n\n    x1 = int(max(0, px))\n    y1 = int(max(0, py))\n    x2 = min(rescaled_width, target_width - x1)\n    y2 = min(rescaled_height, target_height - y1)\n\n    I2[0:(y2 - y1), 0:(x2 - x1), :] = I1[y1:y2, x1:x2, :]\n\n    ox1 = round(0.5*rescaled_width) + jitter_offset_x\n    oy1 = round(0.5*rescaled_height) + jitter_offset_y\n\n    ox2 = round(0.5*target_width)\n    oy2 = round(0.5*target_height)\n\n    for r in a:\n        r.x1 = round(jitter_scale*r.x1 - x1)\n        r.x2 = round(jitter_scale*r.x2 - x1)\n\n        r.y1 = round(jitter_scale*r.y1 - y1)\n        r.y2 = round(jitter_scale*r.y2 - y1)\n\n        if r.x1 < 0:\n            r.x1 = 0\n\n        if r.y1 < 0:\n            r.y1 = 0\n\n        if r.x2 >= I2.shape[1]:\n            r.x2 = I2.shape[1] - 1\n\n        if r.y2 >= I2.shape[0]:\n            r.y2 = I2.shape[0] - 1\n\n        for p in r.point:\n            p.x = round(jitter_scale*p.x - x1)\n            p.y = round(jitter_scale*p.y - y1)\n\n        # MA: make sure all points are inside the image\n        r.point = [p for p in r.point if p.x >= 0 and p.y >=\n                   0 and p.x < I2.shape[1] and p.y < I2.shape[0]]\n\n    new_rects = []\n    for r in a.rects:\n        if r.x1 <= r.x2 and r.y1 <= r.y2:\n            new_rects.append(r)\n        else:\n            pass\n\n    a.rects = new_rects\n\n    return I2, a\n\n\nfrom PIL import Image, ImageDraw\n\nrect = namedtuple(\'Rectangel\', [\'left\', \'top\', \'right\', \'bottom\'])\n\n\ndef _get_ignore_rect(x, y, cell_size):\n    left = x*cell_size\n    right = (x+1)*cell_size\n    top = y*cell_size\n    bottom = (y+1)*cell_size\n\n    return rect(left, top, right, bottom)\n\n\ndef draw_rect(draw, rect, color):\n    rect_cords = ((rect.left, rect.top), (rect.left, rect.bottom),\n                  (rect.right, rect.bottom), (rect.right, rect.top),\n                  (rect.left, rect.top))\n    draw.line(rect_cords, fill=color, width=2)\n\n\ndef draw_encoded(image, confs, mask=None, rects=None, cell_size=32):\n    image = image.astype(\'uint8\')\n    im = Image.fromarray(image)\n\n    shape = confs.shape\n    if mask is None:\n        mask = np.ones(shape)\n\n    # overimage = mycm(confs_pred, bytes=True)\n\n    poly = Image.new(\'RGBA\', im.size)\n    pdraw = ImageDraw.Draw(poly)\n\n    for y in range(shape[0]):\n        for x in range(shape[1]):\n            outline = (0, 0, 0, 255)\n            if confs[y, x]:\n                fill = (0, 255, 0, 100)\n            else:\n                fill = (0, 0, 0, 0)\n            rect = _get_ignore_rect(x, y, cell_size)\n            pdraw.rectangle(rect, fill=fill,\n                            outline=fill)\n            if not mask[y, x]:\n                pdraw.line(((rect.left, rect.bottom), (rect.right, rect.top)),\n                           fill=(0, 0, 0, 255), width=2)\n                pdraw.line(((rect.left, rect.top), (rect.right, rect.bottom)),\n                           fill=(0, 0, 0, 255), width=2)\n\n    color = (0, 0, 255)\n    if rects is not None:\n        for rect in rects:\n            rect_cords = ((rect.x1, rect.y1), (rect.x1, rect.y2),\n                          (rect.x2, rect.y2), (rect.x2, rect.y1),\n                          (rect.x1, rect.y1))\n            pdraw.line(rect_cords, fill=color, width=2)\n\n    im.paste(poly, mask=poly)\n\n    return np.array(im)\n'"
submodules/utils/googlenet_load.py,17,"b'import tensorflow as tf\nfrom kaffe import mynet\nimport os\nimport numpy as np\n\ndef init(H, config=None):\n    if config is None:\n        gpu_options = tf.GPUOptions()\n        config = tf.ConfigProto(gpu_options=gpu_options)\n\n    k = H[\'num_classes\']\n    features_dim = 1024\n    input_layer = \'input\'\n\n    features_layers = [\'output/confidences\', \'output/boxes\']\n\n    data_dir = H[\'dirs\'][\'data_dir\']\n    google_file = \'googlenet.pb\'\n\n    graph_def_orig_file = os.path.join(data_dir, google_file)\n\n    dense_layer_num_output = [k, 4]\n\n    googlenet_graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    tf.set_random_seed(0)\n    with open(graph_def_orig_file) as f:\n        tf.set_random_seed(0)\n        graph_def.MergeFromString(f.read())\n\n    with googlenet_graph.as_default():\n        tf.import_graph_def(graph_def, name=\'\')\n\n    input_op = googlenet_graph.get_operation_by_name(input_layer)\n\n    weights_ops = [\n        op for op in googlenet_graph.get_operations() \n        if any(op.name.endswith(x) for x in [ \'_w\', \'_b\'])\n        and op.type == \'Const\'\n    ]\n\n    reuse_ops = [\n        op for op in googlenet_graph.get_operations() \n        if op not in weights_ops + [input_op]\n        and op.name != \'output\'\n    ]\n\n    with tf.Session(graph=googlenet_graph, config=config):\n        weights_orig = {\n            op.name: op.outputs[0].eval()\n            for op in weights_ops\n        }\n\n    def weight_init(num_output):\n        return 0.001 * np.random.randn(features_dim, num_output).astype(np.float32)\n\n    def bias_init(num_output):\n        return 0.001 * np.random.randn(num_output).astype(np.float32)\n\n\n    W = [\n        tf.Variable(weight_init(dense_layer_num_output[i]), \n                    name=\'softmax/weights_{}\'.format(i)) \n        for i in range(len(features_layers))\n    ]\n\n    B = [\n        tf.Variable(bias_init(dense_layer_num_output[i]),\n                    name=\'softmax/biases_{}\'.format(i)) \n        for i in range(len(features_layers))\n    ]\n\n    weight_vars = {\n        name: tf.Variable(weight, name=name)\n        for name, weight in weights_orig.iteritems()\n    }\n\n    weight_tensors = {\n        name: tf.convert_to_tensor(weight)\n        for name, weight in weight_vars.iteritems()\n    }\n\n    W_norm = [tf.nn.l2_loss(weight) for weight in weight_vars.values() + W]\n    W_norm = tf.reduce_sum(tf.pack(W_norm), name=\'weights_norm\')\n    tf.scalar_summary(W_norm.op.name, W_norm)\n\n    googlenet = {\n        ""W"": W,\n        ""B"": B,\n        ""weight_tensors"": weight_tensors,\n        ""reuse_ops"": reuse_ops,\n        ""input_op"": input_op,\n        ""W_norm"": W_norm,\n        }\n    return googlenet\n\ndef model(x, googlenet, H):\n    weight_tensors = googlenet[""weight_tensors""]\n    input_op = googlenet[""input_op""]\n    reuse_ops = googlenet[""reuse_ops""]\n    def is_early_loss(name):\n        early_loss_layers = [\'head0\', \'nn0\', \'softmax0\', \'head1\', \'nn1\', \'softmax1\', \'output1\']\n        return any(name.startswith(prefix) for prefix in early_loss_layers)\n\n    T = weight_tensors\n    T[input_op.name] = x\n\n    for op in reuse_ops:\n        if is_early_loss(op.name):\n            continue\n        elif op.name == \'avgpool0\':\n            pool_op = tf.nn.avg_pool(T[\'mixed5b\'], ksize=[1,H[\'grid_height\'],H[\'grid_width\'],1], strides=[1,1,1,1], padding=\'VALID\', name=op.name)\n            T[op.name] = pool_op\n\n        else:\n            copied_op = x.graph.create_op(\n                op_type = op.type, \n                inputs = [T[t.op.name] for t in list(op.inputs)], \n                dtypes = [o.dtype for o in op.outputs], \n                name = op.name, \n                attrs =  op.node_def.attr\n            )\n\n            T[op.name] = copied_op.outputs[0]\n            #T[op.name] = tf.Print(copied_op.outputs[0], [tf.shape(copied_op.outputs[0]), tf.constant(op.name)], summarize=4)\n    \n\n    coarse_feat = T[\'mixed5b\']\n\n    # fine feat can be used to reinspect input\n    attention_lname = H.get(\'attention_lname\', \'mixed3b\')\n    early_feat = T[attention_lname]\n    early_feat_channels = 480\n\n    return coarse_feat, early_feat, early_feat_channels\n'"
submodules/utils/rect.py,0,"b'class Rect(object):\n    def __init__(self, cx, cy, width, height, confidence):\n        self.cx = cx\n        self.cy = cy\n        self.width = width\n        self.height = height\n        self.confidence = confidence\n        self.true_confidence = confidence\n    def overlaps(self, other):\n        if abs(self.cx - other.cx) > (self.width + other.width) / 1.5:\n            return False\n        elif abs(self.cy - other.cy) > (self.height + other.height) / 2.0:\n            return False\n        else:\n            return True\n    def distance(self, other):\n        return sum(map(abs, [self.cx - other.cx, self.cy - other.cy,\n                       self.width - other.width, self.height - other.height]))\n    def intersection(self, other):\n        left = max(self.cx - self.width/2., other.cx - other.width/2.)\n        right = min(self.cx + self.width/2., other.cx + other.width/2.)\n        width = max(right - left, 0)\n        top = max(self.cy - self.height/2., other.cy - other.height/2.)\n        bottom = min(self.cy + self.height/2., other.cy + other.height/2.)\n        height = max(bottom - top, 0)\n        return width * height\n    def area(self):\n        return self.height * self.width\n    def union(self, other):\n        return self.area() + other.area() - self.intersection(other)\n    def iou(self, other):\n        return self.intersection(other) / self.union(other)\n    def __eq__(self, other):\n        return (self.cx == other.cx and \n            self.cy == other.cy and\n            self.width == other.width and\n            self.height == other.height and\n            self.confidence == other.confidence)\n'"
submodules/utils/stitch_wrapper.py,0,"b'print("""")\nprint(\'ERROR: stitck_wrapper not yet compiled. Please run:\')\nprint(\'cd KittiBox/submodules/utils && make\')\n'"
submodules/utils/train_utils.py,33,"b'import numpy as np\nimport random\nimport json\nimport os\nimport itertools\nfrom scipy.misc import imread, imresize\nimport tensorflow as tf\n\nfrom PIL import Image, ImageDraw\n\nfrom utils.data_utils import (annotation_jitter, annotation_to_h5)\nfrom utils.annolist import AnnotationLib as al\nfrom utils.rect import Rect\n\n\ndef rescale_boxes(current_shape, anno, target_height, target_width):\n    x_scale = target_width / float(current_shape[1])\n    y_scale = target_height / float(current_shape[0])\n    for r in anno.rects:\n        # assert r.x1 < r.x2\n        r.x1 *= x_scale\n        r.x2 *= x_scale\n        # assert r.x1 < r.x2\n        r.y1 *= y_scale\n        r.y2 *= y_scale\n    return anno\n\n\ndef _draw_rect(draw, rect, color):\n    left = rect.cx-int(rect.width/2)\n    bottom = rect.cy+int(rect.height/2)\n    right = rect.cx+int(rect.width/2)\n    top = rect.cy-int(rect.height/2)\n    rect_cords = ((left, top), (left, bottom),\n                  (right, bottom), (right, top),\n                  (left, top))\n    draw.line(rect_cords, fill=color, width=2)\n\n\ndef compute_rectangels(H, confidences, boxes, use_stitching=False, rnn_len=1, min_conf=0.1, show_removed=True, tau=0.25):\n    num_cells = H[""grid_height""] * H[""grid_width""]\n    boxes_r = np.reshape(boxes, (-1,\n                                 H[""grid_height""],\n                                 H[""grid_width""],\n                                 rnn_len,\n                                 4))\n    confidences_r = np.reshape(confidences, (-1,\n                                             H[""grid_height""],\n                                             H[""grid_width""],\n                                             rnn_len,\n                                             H[\'num_classes\']))\n    cell_pix_size = H[\'region_size\']\n    all_rects = [[[] for _ in range(H[""grid_width""])] for _ in range(H[""grid_height""])]\n    for n in range(rnn_len):\n        for y in range(H[""grid_height""]):\n            for x in range(H[""grid_width""]):\n                bbox = boxes_r[0, y, x, n, :]\n                abs_cx = int(bbox[0]) + cell_pix_size/2 + cell_pix_size * x\n                abs_cy = int(bbox[1]) + cell_pix_size/2 + cell_pix_size * y\n                w = bbox[2]\n                h = bbox[3]\n                conf = np.max(confidences_r[0, y, x, n, 1:])\n                all_rects[y][x].append(Rect(abs_cx,abs_cy,w,h,conf))\n\n    all_rects_r = [r for row in all_rects for cell in row for r in cell]\n    if use_stitching:\n        from stitch_wrapper import stitch_rects\n        acc_rects = stitch_rects(all_rects, tau)\n    else:\n        acc_rects = all_rects_r\n\n\n\ndef add_rectangles(H, orig_image, confidences, boxes, use_stitching=False, rnn_len=1, min_conf=0.1, show_removed=True, tau=0.25,\n    color_removed=(0, 0, 255), color_acc=(0, 0, 255)):\n    image = np.copy(orig_image[0])\n    num_cells = H[""grid_height""] * H[""grid_width""]\n    boxes_r = np.reshape(boxes, (-1,\n                                 H[""grid_height""],\n                                 H[""grid_width""],\n                                 rnn_len,\n                                 4))\n    confidences_r = np.reshape(confidences, (-1,\n                                             H[""grid_height""],\n                                             H[""grid_width""],\n                                             rnn_len,\n                                             H[\'num_classes\']))\n    cell_pix_size = H[\'region_size\']\n    all_rects = [[[] for _ in range(H[""grid_width""])] for _ in range(H[""grid_height""])]\n    for n in range(rnn_len):\n        for y in range(H[""grid_height""]):\n            for x in range(H[""grid_width""]):\n                bbox = boxes_r[0, y, x, n, :]\n                abs_cx = int(bbox[0]) + cell_pix_size/2 + cell_pix_size * x\n                abs_cy = int(bbox[1]) + cell_pix_size/2 + cell_pix_size * y\n                w = bbox[2]\n                h = bbox[3]\n                conf = np.max(confidences_r[0, y, x, n, 1:])\n                all_rects[y][x].append(Rect(abs_cx,abs_cy,w,h,conf))\n\n    all_rects_r = [r for row in all_rects for cell in row for r in cell]\n    if use_stitching:\n        from utils.stitch_wrapper import stitch_rects\n        acc_rects = stitch_rects(all_rects, tau)\n    else:\n        acc_rects = all_rects_r\n\n    if not show_removed:\n        all_rects_r = []\n\n    pairs = [(all_rects_r, color_removed), (acc_rects, color_acc)]\n    im = Image.fromarray(image.astype(\'uint8\'))\n    draw = ImageDraw.Draw(im)\n    for rect_set, color in pairs:\n        for rect in rect_set:\n            if rect.confidence > min_conf:\n                _draw_rect(draw, rect, color)\n\n    image = np.array(im).astype(\'float32\')\n\n    rects = []\n    for rect in acc_rects:\n        r = al.AnnoRect()\n        r.x1 = rect.cx - rect.width/2.\n        r.x2 = rect.cx + rect.width/2.\n        r.y1 = rect.cy - rect.height/2.\n        r.y2 = rect.cy + rect.height/2.\n        r.score = rect.true_confidence\n        rects.append(r)\n\n    return image, rects\n\ndef to_x1y1x2y2(box):\n    w = tf.maximum(box[:, 2:3], 1)\n    h = tf.maximum(box[:, 3:4], 1)\n    x1 = box[:, 0:1] - w / 2\n    x2 = box[:, 0:1] + w / 2\n    y1 = box[:, 1:2] - h / 2\n    y2 = box[:, 1:2] + h / 2\n    return tf.concat(axis=1, values=[x1, y1, x2, y2])\n\ndef intersection(box1, box2):\n    x1_max = tf.maximum(box1[:, 0], box2[:, 0])\n    y1_max = tf.maximum(box1[:, 1], box2[:, 1])\n    x2_min = tf.minimum(box1[:, 2], box2[:, 2])\n    y2_min = tf.minimum(box1[:, 3], box2[:, 3])\n\n    x_diff = tf.maximum(x2_min - x1_max, 0)\n    y_diff = tf.maximum(y2_min - y1_max, 0)\n\n    return x_diff * y_diff\n\ndef area(box):\n    x_diff = tf.maximum(box[:, 2] - box[:, 0], 0)\n    y_diff = tf.maximum(box[:, 3] - box[:, 1], 0)\n    return x_diff * y_diff\n\ndef union(box1, box2):\n    return area(box1) + area(box2) - intersection(box1, box2)\n\ndef iou(box1, box2):\n    return intersection(box1, box2) / union(box1, box2)\n\ndef to_idx(vec, w_shape):\n    \'\'\'\n    vec = (idn, idh, idw)\n    w_shape = [n, h, w, c]\n    \'\'\'\n    return vec[:, 2] + w_shape[2] * (vec[:, 1] + w_shape[1] * vec[:, 0])\n\ndef interp(w, i, channel_dim):\n    \'\'\'\n    Input:\n        w: A 4D block tensor of shape (n, h, w, c)\n        i: A list of 3-tuples [(x_1, y_1, z_1), (x_2, y_2, z_2), ...],\n            each having type (int, float, float)\n\n        The 4D block represents a batch of 3D image feature volumes with c channels.\n        The input i is a list of points  to index into w via interpolation. Direct\n        indexing is not possible due to y_1 and z_1 being float values.\n    Output:\n        A list of the values: [\n            w[x_1, y_1, z_1, :]\n            w[x_2, y_2, z_2, :]\n            ...\n            w[x_k, y_k, z_k, :]\n        ]\n        of the same length == len(i)\n    \'\'\'\n    w_as_vector = tf.reshape(w, [-1, channel_dim]) # gather expects w to be 1-d\n    upper_l = tf.to_int32(tf.concat(axis=1, values=[i[:, 0:1], tf.floor(i[:, 1:2]), tf.floor(i[:, 2:3])]))\n    upper_r = tf.to_int32(tf.concat(axis=1, values=[i[:, 0:1], tf.floor(i[:, 1:2]), tf.ceil(i[:, 2:3])]))\n    lower_l = tf.to_int32(tf.concat(axis=1, values=[i[:, 0:1], tf.ceil(i[:, 1:2]), tf.floor(i[:, 2:3])]))\n    lower_r = tf.to_int32(tf.concat(axis=1, values=[i[:, 0:1], tf.ceil(i[:, 1:2]), tf.ceil(i[:, 2:3])]))\n\n    upper_l_idx = to_idx(upper_l, tf.shape(w))\n    upper_r_idx = to_idx(upper_r, tf.shape(w))\n    lower_l_idx = to_idx(lower_l, tf.shape(w))\n    lower_r_idx = to_idx(lower_r, tf.shape(w))\n\n    upper_l_value = tf.gather(w_as_vector, upper_l_idx)\n    upper_r_value = tf.gather(w_as_vector, upper_r_idx)\n    lower_l_value = tf.gather(w_as_vector, lower_l_idx)\n    lower_r_value = tf.gather(w_as_vector, lower_r_idx)\n\n    alpha_lr = tf.expand_dims(i[:, 2] - tf.floor(i[:, 2]), 1)\n    alpha_ud = tf.expand_dims(i[:, 1] - tf.floor(i[:, 1]), 1)\n\n    upper_value = (1 - alpha_lr) * upper_l_value + (alpha_lr) * upper_r_value\n    lower_value = (1 - alpha_lr) * lower_l_value + (alpha_lr) * lower_r_value\n    value = (1 - alpha_ud) * upper_value + (alpha_ud) * lower_value\n    return value\n\ndef bilinear_select(H, pred_boxes, early_feat, early_feat_channels, w_offset, h_offset):\n    \'\'\'\n    Function used for rezooming high level feature maps. Uses bilinear interpolation\n    to select all channels at index (x, y) for a high level feature map, where x and y are floats.\n    \'\'\'\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n\n    fine_stride = 8. # pixels per 60x80 grid cell in 480x640 image\n    coarse_stride = H[\'region_size\'] # pixels per 15x20 grid cell in 480x640 image\n    batch_ids = []\n    x_offsets = []\n    y_offsets = []\n    for n in range(H[\'batch_size\']):\n        for i in range(H[\'grid_height\']):\n            for j in range(H[\'grid_width\']):\n                for k in range(H[\'rnn_len\']):\n                    batch_ids.append([n])\n                    x_offsets.append([coarse_stride / 2. + coarse_stride * j])\n                    y_offsets.append([coarse_stride / 2. + coarse_stride * i])\n\n    batch_ids = tf.constant(batch_ids)\n    x_offsets = tf.constant(x_offsets)\n    y_offsets = tf.constant(y_offsets)\n\n    pred_boxes_r = tf.reshape(pred_boxes, [outer_size * H[\'rnn_len\'], 4])\n    scale_factor = coarse_stride / fine_stride # scale difference between 15x20 and 60x80 features\n\n    pred_x_center = (pred_boxes_r[:, 0:1] + w_offset * pred_boxes_r[:, 2:3] + x_offsets) / fine_stride\n    pred_x_center_clip = tf.clip_by_value(pred_x_center,\n                                     0,\n                                     scale_factor * H[\'grid_width\'] - 1)\n    pred_y_center = (pred_boxes_r[:, 1:2] + h_offset * pred_boxes_r[:, 3:4] + y_offsets) / fine_stride\n    pred_y_center_clip = tf.clip_by_value(pred_y_center,\n                                          0,\n                                          scale_factor * H[\'grid_height\'] - 1)\n\n    interp_indices = tf.concat(axis=1, values=[tf.to_float(batch_ids), pred_y_center_clip, pred_x_center_clip])\n    return interp_indices\n'"
submodules/utils/annolist/AnnoList_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: AnnoList.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'AnnoList.proto\',\n  package=\'protobuf_annolist\',\n  serialized_pb=_b(\'\\n\\x0e\\x41nnoList.proto\\x12\\x11protobuf_annolist\\""B\\n\\tAttribute\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\x05\\x12\\x0b\\n\\x03val\\x18\\x02 \\x01(\\x05\\x12\\x0c\\n\\x04\\x66val\\x18\\x03 \\x01(\\x02\\x12\\x0e\\n\\x06strval\\x18\\x04 \\x01(\\t\\""\\""\\n\\tIdStrPair\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\x05\\x12\\t\\n\\x01s\\x18\\x02 \\x01(\\t\\""j\\n\\rAttributeDesc\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05\\x64type\\x18\\x03 \\x01(\\x05\\x12\\x30\\n\\nval_to_str\\x18\\x04 \\x03(\\x0b\\x32\\x1c.protobuf_annolist.IdStrPair\\"".\\n\\x11\\x41nnoRectAttribute\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0b\\n\\x03val\\x18\\x02 \\x01(\\t\\""\\x98\\x01\\n\\x08\\x41nnoRect\\x12\\n\\n\\x02x1\\x18\\x01 \\x01(\\x02\\x12\\n\\n\\x02y1\\x18\\x02 \\x01(\\x02\\x12\\n\\n\\x02x2\\x18\\x03 \\x01(\\x02\\x12\\n\\n\\x02y2\\x18\\x04 \\x01(\\x02\\x12\\r\\n\\x05score\\x18\\x05 \\x01(\\x02\\x12\\n\\n\\x02id\\x18\\x06 \\x01(\\x05\\x12\\x10\\n\\x08track_id\\x18\\x0b \\x01(\\x05\\x12/\\n\\tattribute\\x18\\x0c \\x03(\\x0b\\x32\\x1c.protobuf_annolist.Attribute\\""o\\n\\nAnnotation\\x12\\x11\\n\\timageName\\x18\\x01 \\x01(\\t\\x12)\\n\\x04rect\\x18\\x02 \\x03(\\x0b\\x32\\x1b.protobuf_annolist.AnnoRect\\x12\\x10\\n\\x08imgWidth\\x18\\x03 \\x01(\\x05\\x12\\x11\\n\\timgHeight\\x18\\x04 \\x01(\\x05\\""w\\n\\x08\\x41nnoList\\x12\\x31\\n\\nannotation\\x18\\x01 \\x03(\\x0b\\x32\\x1d.protobuf_annolist.Annotation\\x12\\x38\\n\\x0e\\x61ttribute_desc\\x18\\x02 \\x03(\\x0b\\x32 .protobuf_annolist.AttributeDescB\\x0c\\x42\\nAnnoListPb\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_ATTRIBUTE = _descriptor.Descriptor(\n  name=\'Attribute\',\n  full_name=\'protobuf_annolist.Attribute\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.Attribute.id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val\', full_name=\'protobuf_annolist.Attribute.val\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fval\', full_name=\'protobuf_annolist.Attribute.fval\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'strval\', full_name=\'protobuf_annolist.Attribute.strval\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=37,\n  serialized_end=103,\n)\n\n\n_IDSTRPAIR = _descriptor.Descriptor(\n  name=\'IdStrPair\',\n  full_name=\'protobuf_annolist.IdStrPair\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.IdStrPair.id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'s\', full_name=\'protobuf_annolist.IdStrPair.s\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=105,\n  serialized_end=139,\n)\n\n\n_ATTRIBUTEDESC = _descriptor.Descriptor(\n  name=\'AttributeDesc\',\n  full_name=\'protobuf_annolist.AttributeDesc\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'protobuf_annolist.AttributeDesc.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.AttributeDesc.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'protobuf_annolist.AttributeDesc.dtype\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val_to_str\', full_name=\'protobuf_annolist.AttributeDesc.val_to_str\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=141,\n  serialized_end=247,\n)\n\n\n_ANNORECTATTRIBUTE = _descriptor.Descriptor(\n  name=\'AnnoRectAttribute\',\n  full_name=\'protobuf_annolist.AnnoRectAttribute\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'protobuf_annolist.AnnoRectAttribute.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val\', full_name=\'protobuf_annolist.AnnoRectAttribute.val\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=249,\n  serialized_end=295,\n)\n\n\n_ANNORECT = _descriptor.Descriptor(\n  name=\'AnnoRect\',\n  full_name=\'protobuf_annolist.AnnoRect\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'x1\', full_name=\'protobuf_annolist.AnnoRect.x1\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y1\', full_name=\'protobuf_annolist.AnnoRect.y1\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x2\', full_name=\'protobuf_annolist.AnnoRect.x2\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y2\', full_name=\'protobuf_annolist.AnnoRect.y2\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'protobuf_annolist.AnnoRect.score\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.AnnoRect.id\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'track_id\', full_name=\'protobuf_annolist.AnnoRect.track_id\', index=6,\n      number=11, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attribute\', full_name=\'protobuf_annolist.AnnoRect.attribute\', index=7,\n      number=12, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=298,\n  serialized_end=450,\n)\n\n\n_ANNOTATION = _descriptor.Descriptor(\n  name=\'Annotation\',\n  full_name=\'protobuf_annolist.Annotation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'imageName\', full_name=\'protobuf_annolist.Annotation.imageName\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rect\', full_name=\'protobuf_annolist.Annotation.rect\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'imgWidth\', full_name=\'protobuf_annolist.Annotation.imgWidth\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'imgHeight\', full_name=\'protobuf_annolist.Annotation.imgHeight\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=452,\n  serialized_end=563,\n)\n\n\n_ANNOLIST = _descriptor.Descriptor(\n  name=\'AnnoList\',\n  full_name=\'protobuf_annolist.AnnoList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'annotation\', full_name=\'protobuf_annolist.AnnoList.annotation\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attribute_desc\', full_name=\'protobuf_annolist.AnnoList.attribute_desc\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=565,\n  serialized_end=684,\n)\n\n_ATTRIBUTEDESC.fields_by_name[\'val_to_str\'].message_type = _IDSTRPAIR\n_ANNORECT.fields_by_name[\'attribute\'].message_type = _ATTRIBUTE\n_ANNOTATION.fields_by_name[\'rect\'].message_type = _ANNORECT\n_ANNOLIST.fields_by_name[\'annotation\'].message_type = _ANNOTATION\n_ANNOLIST.fields_by_name[\'attribute_desc\'].message_type = _ATTRIBUTEDESC\nDESCRIPTOR.message_types_by_name[\'Attribute\'] = _ATTRIBUTE\nDESCRIPTOR.message_types_by_name[\'IdStrPair\'] = _IDSTRPAIR\nDESCRIPTOR.message_types_by_name[\'AttributeDesc\'] = _ATTRIBUTEDESC\nDESCRIPTOR.message_types_by_name[\'AnnoRectAttribute\'] = _ANNORECTATTRIBUTE\nDESCRIPTOR.message_types_by_name[\'AnnoRect\'] = _ANNORECT\nDESCRIPTOR.message_types_by_name[\'Annotation\'] = _ANNOTATION\nDESCRIPTOR.message_types_by_name[\'AnnoList\'] = _ANNOLIST\n\nAttribute = _reflection.GeneratedProtocolMessageType(\'Attribute\', (_message.Message,), dict(\n  DESCRIPTOR = _ATTRIBUTE,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.Attribute)\n  ))\n_sym_db.RegisterMessage(Attribute)\n\nIdStrPair = _reflection.GeneratedProtocolMessageType(\'IdStrPair\', (_message.Message,), dict(\n  DESCRIPTOR = _IDSTRPAIR,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.IdStrPair)\n  ))\n_sym_db.RegisterMessage(IdStrPair)\n\nAttributeDesc = _reflection.GeneratedProtocolMessageType(\'AttributeDesc\', (_message.Message,), dict(\n  DESCRIPTOR = _ATTRIBUTEDESC,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AttributeDesc)\n  ))\n_sym_db.RegisterMessage(AttributeDesc)\n\nAnnoRectAttribute = _reflection.GeneratedProtocolMessageType(\'AnnoRectAttribute\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNORECTATTRIBUTE,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoRectAttribute)\n  ))\n_sym_db.RegisterMessage(AnnoRectAttribute)\n\nAnnoRect = _reflection.GeneratedProtocolMessageType(\'AnnoRect\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNORECT,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoRect)\n  ))\n_sym_db.RegisterMessage(AnnoRect)\n\nAnnotation = _reflection.GeneratedProtocolMessageType(\'Annotation\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNOTATION,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.Annotation)\n  ))\n_sym_db.RegisterMessage(Annotation)\n\nAnnoList = _reflection.GeneratedProtocolMessageType(\'AnnoList\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNOLIST,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoList)\n  ))\n_sym_db.RegisterMessage(AnnoList)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'B\\nAnnoListPb\'))\n# @@protoc_insertion_point(module_scope)\n'"
submodules/utils/annolist/AnnotationLib.py,0,"b'import os\r\n\r\nfrom math import sqrt\r\n\r\nimport gzip\r\nimport bz2\r\nimport sys\r\nimport numpy as np;\r\n\r\nfrom collections import MutableSequence\r\n\r\n#import AnnoList_pb2\r\nimport  utils.annolist.PalLib;\r\n\r\nimport xml.dom.minidom\r\nfrom xml.dom.minidom import Node\r\nxml_dom_ext_available=False\r\ntry:\r\n        import xml.dom.ext\r\n        xml_dom_ext_available=True\r\nexcept ImportError:\r\n        pass\r\n\r\n\r\n################################################\r\n#\r\n#  TODO: check distance function\r\n#\r\n################################################\r\n\r\n\r\ndef cmpAnnRectsByScore(r1, r2):\r\n        return cmp(r1.score, r2.score)\r\n\r\ndef cmpAnnoRectsByScoreDescending(r1, r2):\r\n        return (-1)*cmp(r1.score, r2.score)\r\n\r\ndef cmpDetAnnoRectsByScore(r1, r2):\r\n        return cmp(r1.rect.score, r2.rect.score);\r\n\r\n\r\ndef suffixMatch(fn1, fn2):\r\n        l1 = len(fn1);\r\n        l2 = len(fn2);\r\n\r\n        if fn1[-l2:] == fn2:\r\n                return True\r\n\r\n        if fn2[-l1:] == fn1:\r\n                return True\r\n\r\n        return False\r\n\r\nclass AnnoList(MutableSequence):\r\n        """"""Define a list format, which I can customize""""""\r\n        TYPE_INT32 = 5;\r\n        TYPE_FLOAT = 2;\r\n        TYPE_STRING = 9;\r\n\r\n        def __init__(self, data=None):\r\n                super(AnnoList, self).__init__()\r\n\r\n                self.attribute_desc = {};\r\n                self.attribute_val_to_str = {};\r\n\r\n                if not (data is None):\r\n                        self._list = list(data)\r\n                else:\r\n                        self._list = list()\r\n\r\n        def add_attribute(self, name, dtype):\r\n                _adesc = AnnoList_pb2.AttributeDesc();\r\n                _adesc.name = name;\r\n                if self.attribute_desc:\r\n                        _adesc.id = max((self.attribute_desc[d].id for d in self.attribute_desc)) + 1;\r\n                else:\r\n                        _adesc.id = 0;\r\n\r\n                if dtype == int:\r\n                        _adesc.dtype = AnnoList.TYPE_INT32;\r\n                elif dtype == float or dtype == np.float32:\r\n                        _adesc.dtype = AnnoList.TYPE_FLOAT;\r\n                elif dtype == str:\r\n                        _adesc.dtype = AnnoList.TYPE_STRING;\r\n                else:\r\n                        print (""unknown attribute type: "", dtype)\r\n                        assert(False);\r\n\r\n                #print ""adding attribute: {}, id: {}, type: {}"".format(_adesc.name, _adesc.id, _adesc.dtype);\r\n                self.attribute_desc[name] = _adesc;\r\n\r\n        def add_attribute_val(self, aname, vname, val):\r\n                # add attribute before adding string corresponding to integer value\r\n                assert(aname in self.attribute_desc);\r\n\r\n                # check and add if new\r\n                if all((val_desc.id != val for val_desc in self.attribute_desc[aname].val_to_str)):\r\n                        val_desc = self.attribute_desc[aname].val_to_str.add()\r\n                        val_desc.id = val;\r\n                        val_desc.s = vname;\r\n\r\n                # also add to map for quick access\r\n                if not aname in self.attribute_val_to_str:\r\n                        self.attribute_val_to_str[aname] = {};\r\n\r\n                assert(not val in self.attribute_val_to_str[aname]);\r\n                self.attribute_val_to_str[aname][val] = vname;\r\n\r\n\r\n        def attribute_get_value_str(self, aname, val):\r\n                if aname in self.attribute_val_to_str and val in self.attribute_val_to_str[aname]:\r\n                        return self.attribute_val_to_str[aname][val];\r\n                else:\r\n                        return str(val);\r\n\r\n        def save(self, fname):\r\n                save(fname, self);\r\n\r\n        #MA: list interface\r\n        def __len__(self):\r\n                return len(self._list)\r\n\r\n        def __getitem__(self, ii):\r\n                if isinstance(ii, slice):\r\n                        res = AnnoList();\r\n                        res.attribute_desc = self.attribute_desc;\r\n                        res._list = self._list[ii]\r\n                        return res;\r\n                else:\r\n                        return self._list[ii]\r\n\r\n        def __delitem__(self, ii):\r\n                del self._list[ii]\r\n\r\n        def __setitem__(self, ii, val):\r\n                return self._list[ii]\r\n\r\n        def __str__(self):\r\n                return self.__repr__()\r\n\r\n        def __repr__(self):\r\n                return """"""<AnnoList %s>"""""" % self._list\r\n\r\n        def insert(self, ii, val):\r\n                self._list.insert(ii, val)\r\n\r\n        def append(self, val):\r\n                list_idx = len(self._list)\r\n                self.insert(list_idx, val)\r\n\r\n\r\ndef is_compatible_attr_type(protobuf_type, attr_type):\r\n        if protobuf_type == AnnoList.TYPE_INT32:\r\n                return (attr_type == int);\r\n        elif protobuf_type == AnnoList.TYPE_FLOAT:\r\n                return (attr_type == float or attr_type == np.float32);\r\n        elif protobuf_type == AnnoList.TYPE_STRING:\r\n                return (attr_type == str);\r\n        else:\r\n                assert(false);\r\n\r\n\r\ndef protobuf_type_to_python(protobuf_type):\r\n        if protobuf_type == AnnoList.TYPE_INT32:\r\n                return int;\r\n        elif protobuf_type == AnnoList.TYPE_FLOAT:\r\n                return float;\r\n        elif protobuf_type == AnnoList.TYPE_STRING:\r\n                return str;\r\n        else:\r\n                assert(false);\r\n\r\n\r\nclass AnnoPoint(object):\r\n        def __init__(self, x=None, y=None, id=None):\r\n                self.x = x;\r\n                self.y = y;\r\n                self.id = id;\r\n\r\nclass AnnoRect(object):\r\n        def __init__(self, x1=-1, y1=-1, x2=-1, y2=-1):\r\n\r\n                self.x1 = x1\r\n                self.y1 = y1\r\n                self.x2 = x2\r\n                self.y2 = y2\r\n\r\n                self.score = -1.0\r\n                self.scale = -1.0\r\n                self.articulations =[]\r\n                self.viewpoints =[]\r\n                self.d3 = []\r\n\r\n                self.silhouetteID = -1\r\n                self.classID = -1\r\n                self.track_id = -1\r\n\r\n                self.point = [];\r\n                self.at = {};\r\n\r\n        def __str__(self):\r\n                return ""<AnnoRect x1: {}, x2: {}, y1: {}, y2: {}, class: {}>"".format(\r\n                        self.x1, self.x2, self.y1, self.y2, self.classID)\r\n\r\n        def __repr__(self):\r\n                return ""<AnnoRect x1: {}, x2: {}, y1: {}, y2: {}, class: {}>"".format(\r\n                        self.x1, self.x2, self.y1, self.y2, self.classID)\r\n\r\n        def width(self):\r\n                return abs(self.x2-self.x1)\r\n\r\n        def height(self):\r\n                return abs(self.y2-self.y1)\r\n\r\n        def centerX(self):\r\n                return (self.x1+self.x2)/2.0\r\n\r\n        def centerY(self):\r\n                return (self.y1+self.y2)/2.0\r\n\r\n        def left(self):\r\n                return min(self.x1, self.x2)\r\n\r\n        def right(self):\r\n                return max(self.x1, self.x2)\r\n\r\n        def top(self):\r\n                return min(self.y1, self.y2)\r\n\r\n        def bottom(self):\r\n                return max(self.y1, self.y2)\r\n\r\n        def forceAspectRatio(self, ratio, KeepHeight = False, KeepWidth = False):\r\n                """"""force the Aspect ratio""""""\r\n                if KeepWidth or ((not KeepHeight) and self.width() * 1.0 / self.height() > ratio):\r\n                        # extend height\r\n                        newHeight = self.width() * 1.0 / ratio\r\n                        self.y1 = (self.centerY() - newHeight / 2.0)\r\n                        self.y2 = (self.y1 + newHeight)\r\n                else:\r\n                        # extend width\r\n                        newWidth = self.height() * ratio\r\n                        self.x1 = (self.centerX() - newWidth / 2.0)\r\n                        self.x2 = (self.x1 + newWidth)\r\n\r\n        def clipToImage(self, min_x, max_x, min_y, max_y):\r\n                        self.x1 = max(min_x, self.x1)\r\n                        self.x2 = max(min_x, self.x2)\r\n                        self.y1 = max(min_y, self.y1)\r\n                        self.y2 = max(min_y, self.y2)\r\n                        self.x1 = min(max_x, self.x1)\r\n                        self.x2 = min(max_x, self.x2)\r\n                        self.y1 = min(max_y, self.y1)\r\n                        self.y2 = min(max_y, self.y2)\r\n\r\n        def printContent(self):\r\n                print (""Coords: "", self.x1, self.y1, self.x2, self.y2)\r\n                print (""Score: "", self.score)\r\n                print (""Articulations: "", self.articulations)\r\n                print (""Viewpoints: "", self.viewpoints)\r\n                print (""Silhouette: "", self.silhouetteID)\r\n\r\n        def ascii(self):\r\n                r = ""(""+str(self.x1)+"", ""+str(self.y1)+"", ""+str(self.x2)+"", ""+str(self.y2)+"")""\r\n                if (self.score!=-1):\r\n                        r = r + "":""+str(self.score)\r\n                if (self.silhouetteID !=-1):\r\n                        ri = r + ""/""+str(self.silhouetteID)\r\n                return r\r\n\r\n        def writeIDL(self, file):\r\n                file.write("" (""+str(self.x1)+"", ""+str(self.y1)+"", ""+str(self.x2)+"", ""+str(self.y2)+"")"")\r\n                if (self.score!=-1):\r\n                        file.write("":""+str(self.score))\r\n                if (self.silhouetteID !=-1):\r\n                        file.write(""/""+str(self.silhouetteID))\r\n\r\n        def sortCoords(self):\r\n                if (self.x1>self.x2):\r\n                        self.x1, self.x2 = self.x2, self.x1\r\n                if (self.y1>self.y2):\r\n                        self.y1, self.y2 = self.y2, self.y1\r\n\r\n        def rescale(self, factor):\r\n                self.x1=(self.x1*float(factor))\r\n                self.y1=(self.y1*float(factor))\r\n                self.x2=(self.x2*float(factor))\r\n                self.y2=(self.y2*float(factor))\r\n\r\n        def resize(self, factor, factor_y = None):\r\n                w = self.width()\r\n                h = self.height()\r\n                if factor_y is None:\r\n                        factor_y = factor\r\n                centerX = float(self.x1+self.x2)/2.0\r\n                centerY = float(self.y1+self.y2)/2.0\r\n                self.x1 = (centerX - (w/2.0)*factor)\r\n                self.y1 = (centerY - (h/2.0)*factor_y)\r\n                self.x2 = (centerX + (w/2.0)*factor)\r\n                self.y2 = (centerY + (h/2.0)*factor_y)\r\n\r\n\r\n        def intersection(self, other):\r\n                self.sortCoords()\r\n                other.sortCoords()\r\n\r\n                if(self.x1 >= other.x2):\r\n                        return (0, 0)\r\n                if(self.x2 <= other.x1):\r\n                        return (0, 0)\r\n                if(self.y1 >= other.y2):\r\n                        return (0, 0)\r\n                if(self.y2 <= other.y1):\r\n                        return (0, 0)\r\n\r\n                l = max(self.x1, other.x1);\r\n                t = max(self.y1, other.y1);\r\n                r = min(self.x2, other.x2);\r\n                b = min(self.y2, other.y2);\r\n                return (r - l, b - t)\r\n\r\n                #Alternate implementation\r\n                #nWidth  = self.x2 - self.x1\r\n                #nHeight = self.y2 - self.y1\r\n                #iWidth  = max(0,min(max(0,other.x2-self.x1),nWidth )-max(0,other.x1-self.x1))\r\n                #iHeight = max(0,min(max(0,other.y2-self.y1),nHeight)-max(0,other.y1-self.y1))\r\n                #return (iWidth, iHeight)\r\n\r\n        def cover(self, other):\r\n                nWidth = self.width()\r\n                nHeight = self.height()\r\n                iWidth, iHeight = self.intersection(other)\r\n                return float(iWidth * iHeight) / float(nWidth * nHeight)\r\n\r\n        def overlap_pascal(self, other):\r\n                self.sortCoords()\r\n                other.sortCoords()\r\n\r\n                nWidth  = self.x2 - self.x1\r\n                nHeight = self.y2 - self.y1\r\n                iWidth, iHeight = self.intersection(other)\r\n                interSection = iWidth * iHeight\r\n\r\n                union = self.width() * self.height() + other.width() * other.height() - interSection\r\n\r\n                overlap = interSection * 1.0 / union\r\n                return overlap\r\n\r\n        def isMatchingPascal(self, other, minOverlap):\r\n                overlap = self.overlap_pascal(other)\r\n                if (overlap >= minOverlap and (self.classID == -1 or other.classID == -1 or self.classID == other.classID)):\r\n                        return 1\r\n                else:\r\n                        return 0\r\n\r\n        def distance(self, other, aspectRatio=-1, fixWH=\'fixheight\'):\r\n                if (aspectRatio!=-1):\r\n                        if (fixWH==\'fixwidth\'):\r\n                                dWidth  = float(self.x2 - self.x1)\r\n                                dHeight = dWidth / aspectRatio\r\n                        elif (fixWH==\'fixheight\'):\r\n                                dHeight = float(self.y2 - self.y1)\r\n                                dWidth  = dHeight * aspectRatio\r\n                else:\r\n                        dWidth  = float(self.x2 - self.x1)\r\n                        dHeight = float(self.y2 - self.y1)\r\n\r\n                xdist   = (self.x1 + self.x2 - other.x1 - other.x2) / dWidth\r\n                ydist   = (self.y1 + self.y2 - other.y1 - other.y2) / dHeight\r\n\r\n                return sqrt(xdist*xdist + ydist*ydist)\r\n\r\n        def isMatchingStd(self, other, coverThresh, overlapThresh, distThresh, aspectRatio=-1, fixWH=-1):\r\n                cover = other.cover(self)\r\n                overlap = self.cover(other)\r\n                dist = self.distance(other, aspectRatio, fixWH)\r\n\r\n                #if(self.width() == 24 ):\r\n                #print cover, "" "", overlap, "" "", dist\r\n                #print coverThresh, overlapThresh, distThresh\r\n                #print (cover>=coverThresh and overlap>=overlapThresh and dist<=distThresh)\r\n\r\n                if (cover>=coverThresh and overlap>=overlapThresh and dist<=distThresh and self.classID == other.classID):\r\n                        return 1\r\n                else:\r\n                        return 0\r\n\r\n        def isMatching(self, other, style, coverThresh, overlapThresh, distThresh, minOverlap, aspectRatio=-1, fixWH=-1):\r\n                #choose matching style\r\n                if (style == 0):\r\n                        return self.isMatchingStd(other, coverThresh, overlapThresh, distThresh, aspectRatio=-1, fixWH=-1)\r\n\r\n                if (style == 1):\r\n                        return self.isMatchingPascal(other, minOverlap)\r\n\r\n        def addToXML(self, node, doc): # no Silhouette yet\r\n                rect_el = doc.createElement(""annorect"")\r\n                for item in ""x1 y1 x2 y2 score scale track_id"".split():\r\n                        coord_el = doc.createElement(item)\r\n                        coord_val = doc.createTextNode(str(self.__getattribute__(item)))\r\n                        coord_el.appendChild(coord_val)\r\n                        rect_el.appendChild(coord_el)\r\n\r\n                articulation_el = doc.createElement(""articulation"")\r\n                for articulation in self.articulations:\r\n                        id_el = doc.createElement(""id"")\r\n                        id_val = doc.createTextNode(str(articulation))\r\n                        id_el.appendChild(id_val)\r\n                        articulation_el.appendChild(id_el)\r\n                if(len(self.articulations) > 0):\r\n                        rect_el.appendChild(articulation_el)\r\n\r\n                viewpoint_el    = doc.createElement(""viewpoint"")\r\n                for viewpoint in self.viewpoints:\r\n                        id_el = doc.createElement(""id"")\r\n                        id_val = doc.createTextNode(str(viewpoint))\r\n                        id_el.appendChild(id_val)\r\n                        viewpoint_el.appendChild(id_el)\r\n                if(len(self.viewpoints) > 0):\r\n                        rect_el.appendChild(viewpoint_el)\r\n\r\n                d3_el    = doc.createElement(""D3"")\r\n                for d in self.d3:\r\n                        id_el = doc.createElement(""id"")\r\n                        id_val = doc.createTextNode(str(d))\r\n                        id_el.appendChild(id_val)\r\n                        d3_el.appendChild(id_el)\r\n                if(len(self.d3) > 0):\r\n                        rect_el.appendChild(d3_el)\r\n\r\n                if self.silhouetteID != -1:\r\n                        silhouette_el    = doc.createElement(""silhouette"")\r\n                        id_el = doc.createElement(""id"")\r\n                        id_val = doc.createTextNode(str(self.silhouetteID))\r\n                        id_el.appendChild(id_val)\r\n                        silhouette_el.appendChild(id_el)\r\n                        rect_el.appendChild(silhouette_el)\r\n\r\n                if self.classID != -1:\r\n                        class_el    = doc.createElement(""classID"")\r\n                        class_val = doc.createTextNode(str(self.classID))\r\n                        class_el.appendChild(class_val)\r\n                        rect_el.appendChild(class_el)\r\n\r\n                if len(self.point) > 0:\r\n                        annopoints_el = doc.createElement(""annopoints"")\r\n\r\n                        for p in self.point:\r\n                                point_el = doc.createElement(""point"");\r\n\r\n                                point_id_el = doc.createElement(""id"");\r\n                                point_id_val = doc.createTextNode(str(p.id));\r\n                                point_id_el.appendChild(point_id_val);\r\n                                point_el.appendChild(point_id_el);\r\n\r\n                                point_x_el = doc.createElement(""x"");\r\n                                point_x_val = doc.createTextNode(str(p.x));\r\n                                point_x_el.appendChild(point_x_val);\r\n                                point_el.appendChild(point_x_el);\r\n\r\n                                point_y_el = doc.createElement(""y"");\r\n                                point_y_val = doc.createTextNode(str(p.y));\r\n                                point_y_el.appendChild(point_y_val);\r\n                                point_el.appendChild(point_y_el);\r\n\r\n                                annopoints_el.appendChild(point_el);\r\n\r\n                        rect_el.appendChild(annopoints_el);\r\n\r\n                node.appendChild(rect_el)\r\n\r\n\r\n\r\nclass Annotation(object):\r\n\r\n        def __init__(self):\r\n                self.imageName = """"\r\n                self.imagePath = """"\r\n                self.rects =[]\r\n                self.frameNr = -1\r\n\r\n        def clone_empty(self):\r\n                new = Annotation()\r\n                new.imageName = self.imageName\r\n                new.imagePath = self.imagePath\r\n                new.frameNr   = self.frameNr\r\n                new.rects     = []\r\n                return new\r\n\r\n        def filename(self):\r\n                return os.path.join(self.imagePath, self.imageName)\r\n\r\n        def printContent(self):\r\n                print (""Name: "", self.imageName)\r\n                for rect in self.rects:\r\n                        rect.printContent()\r\n\r\n        def writeIDL(self, file):\r\n                if (self.frameNr == -1):\r\n                        file.write(""\\""""+os.path.join(self.imagePath, self.imageName)+""\\"""")\r\n                else:\r\n                        file.write(""\\""""+os.path.join(self.imagePath, self.imageName)+""@%d\\"""" % self.frameNr)\r\n\r\n                if (len(self.rects)>0):\r\n                        file.write("":"")\r\n                i=0\r\n                for rect in self.rects:\r\n                        rect.writeIDL(file)\r\n                        if (i+1<len(self.rects)):\r\n                                file.write("","")\r\n                        i+=1\r\n\r\n        def addToXML(self, node, doc): # no frame# yet\r\n                annotation_el = doc.createElement(""annotation"")\r\n                img_el = doc.createElement(""image"")\r\n                name_el = doc.createElement(""name"")\r\n                name_val = doc.createTextNode(os.path.join(self.imagePath, self.imageName))\r\n                name_el.appendChild(name_val)\r\n                img_el.appendChild(name_el)\r\n\r\n                if(self.frameNr != -1):\r\n                        frame_el = doc.createElement(""frameNr"")\r\n                        frame_val = doc.createTextNode(str(self.frameNr))\r\n                        frame_el.appendChild(frame_val)\r\n                        img_el.appendChild(frame_el)\r\n\r\n                annotation_el.appendChild(img_el)\r\n                for rect in self.rects:\r\n                        rect.addToXML(annotation_el, doc)\r\n                node.appendChild(annotation_el)\r\n\r\n\r\n        def sortByScore(self, dir=""ascending""):\r\n                if (dir==""descending""):\r\n                        self.rects.sort(cmpAnnoRectsByScoreDescending)\r\n                else:\r\n                        self.rects.sort(cmpAnnoRectsByScore)\r\n\r\n        def __getitem__(self, index):\r\n                return self.rects[index]\r\n\r\nclass detAnnoRect:\r\n        def __init(self):\r\n                self.imageName = """"\r\n                self.frameNr = -1\r\n                self.rect = AnnoRect()\r\n                self.imageIndex = -1\r\n                self.boxIndex = -1\r\n\r\n#####################################################################\r\n### Parsing\r\n\r\ndef parseTii(filename):\r\n\r\n        # MA: this must be some really old code\r\n        assert(False);\r\n        annotations = []\r\n\r\n        #--- parse xml ---#\r\n        doc = xml.dom.minidom.parse(filename)\r\n\r\n        #--- get tags ---#\r\n        for file in doc.getElementsByTagName(""file""):\r\n\r\n                anno = Annotation()\r\n\r\n                for filename in file.getElementsByTagName(""filename""):\r\n                        aNode = filename.getAttributeNode(""Src"")\r\n                        anno.imageName = aNode.firstChild.data[:-4]+"".png""\r\n\r\n                for objects in file.getElementsByTagName(""objects""):\r\n\r\n                        for vehicle in objects.getElementsByTagName(""vehicle""):\r\n\r\n                                aNode = vehicle.getAttributeNode(""Type"")\r\n                                type = aNode.firstChild.data\r\n\r\n                                if (type==""pedestrian""):\r\n\r\n                                        rect = AnnoRect()\r\n                                        aNode = vehicle.getAttributeNode(""FR"")\r\n                                        frontrear = aNode.firstChild.data\r\n                                        aNode = vehicle.getAttributeNode(""SD"")\r\n                                        side = aNode.firstChild.data\r\n                                        if (frontrear == ""1""):\r\n                                                orientation=""FR""\r\n                                        elif (side == ""1""):\r\n                                                orientation=""SD""\r\n                                        aNode = vehicle.getAttributeNode( orientation+""_TopLeft_X"")\r\n                                        rect.x1 = float(aNode.firstChild.data)\r\n                                        aNode = vehicle.getAttributeNode( orientation+""_TopLeft_Y"")\r\n                                        rect.y1 = float(aNode.firstChild.data)\r\n                                        aNode = vehicle.getAttributeNode( orientation+""_BottomRight_X"")\r\n                                        rect.x2 = float(aNode.firstChild.data)\r\n                                        aNode = vehicle.getAttributeNode( orientation+""_BottomRight_Y"")\r\n                                        rect.y2 = float(aNode.firstChild.data)\r\n                                        print (""pedestrian:"", anno.imageName, rect.x1, rect.y1, rect.x2, rect.y2)\r\n                                        anno.rects.append(rect)\r\n\r\n                annotations.append(anno)\r\n\r\n        return annotations\r\n\r\ndef parseXML(filename):\r\n        filename = os.path.realpath(filename)\r\n\r\n        name, ext = os.path.splitext(filename)\r\n\r\n        annotations = AnnoList([])\r\n\r\n        if(ext == "".al""):\r\n                file = open(filename,\'r\')\r\n                lines = file.read()\r\n                file.close()\r\n\r\n        if(ext == "".gz""):\r\n                zfile = gzip.GzipFile(filename)\r\n                lines = zfile.read()\r\n                zfile.close()\r\n\r\n        if(ext == "".bz2""):\r\n                bfile = bz2.BZ2File(filename)\r\n                lines = bfile.read()\r\n                bfile.close()\r\n\r\n        #--- parse xml ---#\r\n        doc = xml.dom.minidom.parseString(lines)\r\n\r\n        #--- get tags ---#\r\n        for annotation in doc.getElementsByTagName(""annotation""):\r\n                anno = Annotation()\r\n                for image in annotation.getElementsByTagName(""image""):\r\n                        for name in image.getElementsByTagName(""name""):\r\n                                anno.imageName = name.firstChild.data\r\n\r\n                        for fn in image.getElementsByTagName(""frameNr""):\r\n                                anno.frameNr = int(fn.firstChild.data)\r\n\r\n                rects = []\r\n                for annoRect in annotation.getElementsByTagName(""annorect""):\r\n                        rect = AnnoRect()\r\n\r\n                        for x1 in annoRect.getElementsByTagName(""x1""):\r\n                                rect.x1 = float(x1.firstChild.data)\r\n\r\n                        for y1 in annoRect.getElementsByTagName(""y1""):\r\n                                rect.y1 = float(y1.firstChild.data)\r\n\r\n                        for x2 in annoRect.getElementsByTagName(""x2""):\r\n                                rect.x2 = float(x2.firstChild.data)\r\n\r\n                        for y2 in annoRect.getElementsByTagName(""y2""):\r\n                                rect.y2 = float(y2.firstChild.data)\r\n\r\n                        for scale in annoRect.getElementsByTagName(""scale""):\r\n                                rect.scale = float(scale.firstChild.data)\r\n\r\n                        for score in annoRect.getElementsByTagName(""score""):\r\n                                rect.score = float(score.firstChild.data)\r\n\r\n                        for classID in annoRect.getElementsByTagName(""classID""):\r\n                                rect.classID = int(classID.firstChild.data)\r\n\r\n                        for track_id in annoRect.getElementsByTagName(""track_id""):\r\n                                rect.track_id = int(track_id.firstChild.data)\r\n\r\n                        for articulation in annoRect.getElementsByTagName(""articulation""):\r\n                                for id in articulation.getElementsByTagName(""id""):\r\n                                        rect.articulations.append(int(id.firstChild.data))\r\n                                #print ""Articulations: "", rect.articulations\r\n\r\n                        for viewpoint in annoRect.getElementsByTagName(""viewpoint""):\r\n                                for id in viewpoint.getElementsByTagName(""id""):\r\n                                        rect.viewpoints.append(int(id.firstChild.data))\r\n                                        #print ""Viewpoints: "", rect.viewpoints\r\n\r\n                        for d in annoRect.getElementsByTagName(""D3""):\r\n                                for id in d.getElementsByTagName(""id""):\r\n                                        rect.d3.append(float(id.firstChild.data))\r\n\r\n                        for silhouette in annoRect.getElementsByTagName(""silhouette""):\r\n                                for id in silhouette.getElementsByTagName(""id""):\r\n                                        rect.silhouetteID = int(id.firstChild.data)\r\n                                #print ""SilhouetteID: "", rect.silhouetteID\r\n\r\n                        for annoPoints in annoRect.getElementsByTagName(""annopoints""):\r\n                                for annoPoint in annoPoints.getElementsByTagName(""point""):\r\n\r\n                                        p = AnnoPoint();\r\n                                        for annoPointX in annoPoint.getElementsByTagName(""x""):\r\n                                                p.x = int(float(annoPointX.firstChild.data));\r\n\r\n                                        for annoPointY in annoPoint.getElementsByTagName(""y""):\r\n                                                p.y = int(float(annoPointY.firstChild.data));\r\n\r\n                                        for annoPointId in annoPoint.getElementsByTagName(""id""):\r\n                                                p.id = int(annoPointId.firstChild.data);\r\n\r\n                                        assert(p.x != None and p.y != None and p.id != None);\r\n                                        rect.point.append(p);\r\n\r\n                        rects.append(rect)\r\n\r\n                anno.rects = rects\r\n                annotations.append(anno)\r\n\r\n        return annotations\r\n\r\n\r\ndef parse(filename, abs_path=False):\r\n        #print ""Parsing: "", filename\r\n        name, ext = os.path.splitext(filename)\r\n\r\n        if (ext == "".gz"" or ext == "".bz2""):\r\n                name, ext = os.path.splitext(name)\r\n\r\n        if(ext == "".idl""):\r\n                annolist = parseIDL(filename)\r\n        elif(ext == "".al""):\r\n                annolist = parseXML(filename)\r\n        elif(ext == "".pal""):\r\n                annolist = PalLib.pal2al(PalLib.loadPal(filename));\r\n        else:\r\n                annolist = AnnoList([]);\r\n\r\n        if abs_path:\r\n                basedir = os.path.dirname(os.path.abspath(filename))\r\n                for a in annolist:\r\n                        a.imageName = basedir + ""/"" + os.path.basename(a.imageName)\r\n\r\n        return annolist\r\n\r\n\r\ndef parseIDL(filename):\r\n        filename = os.path.realpath(filename)\r\n\r\n        name, ext = os.path.splitext(filename)\r\n\r\n        lines = []\r\n        if(ext == "".idl""):\r\n                file = open(filename,\'r\')\r\n                lines = file.readlines()\r\n                file.close()\r\n\r\n        if(ext == "".gz""):\r\n                zfile = gzip.GzipFile(filename)\r\n                lines = zfile.readlines()\r\n                zfile.close()\r\n\r\n        if(ext == "".bz2""):\r\n                bfile = bz2.BZ2File(filename)\r\n                lines = bfile.readlines()\r\n                bfile.close()\r\n\r\n        annotations = AnnoList([])\r\n\r\n        for line in lines:\r\n                anno = Annotation()\r\n\r\n                ### remove line break\r\n                if (line[-1]==\'\\n\'):\r\n                        line = line[:-1]; # remove \'\\n\'\r\n                lineLen = len(line)\r\n                #print line\r\n\r\n                ### get image name\r\n                posImageEnd = line.find(\'\\"":\')\r\n                if (posImageEnd==-1):\r\n                        posImageEnd = line.rfind(""\\"""")\r\n                anno.imageName = line[1:posImageEnd]\r\n                #print anno.imageName\r\n\r\n                pos = anno.imageName.rfind(""@"")\r\n                if (pos >= 0):\r\n                        anno.frameNr = int(anno.imageName[pos+1:])\r\n                        anno.imageName = anno.imageName[:pos]\r\n                        if anno.imageName[-1] == ""/"":\r\n                                anno.imageName = anno.imageName[:-1]\r\n                else:\r\n                        anno.frameNr = -1\r\n\r\n                ### get rect list\r\n                # we split by \',\'. there are 3 commas for each rect and 1 comma seperating the rects\r\n                rectSegs=[]\r\n                if (posImageEnd!=-1 and posImageEnd+4<lineLen):\r\n\r\n                        line = line[posImageEnd+3:-1]; # remove ; or .\r\n\r\n                        segments = line.split(\',\')\r\n                        if (len(segments)%4!=0):\r\n                                print (""Parse Errror"")\r\n                        else:\r\n                                for i in range(0,len(segments),4):\r\n                                        rectSeg = segments[i]+"",""+segments[i+1]+"",""+segments[i+2]+"",""+segments[i+3]\r\n                                        rectSegs.append(rectSeg)\r\n                                        #print rectSegs\r\n\r\n                        ## parse rect segments\r\n                        for rectSeg in rectSegs:\r\n                                #print ""RectSeg: "", rectSeg\r\n                                rect = AnnoRect()\r\n                                posBracket1 = rectSeg.find(\'(\')\r\n                                posBracket2 = rectSeg.find(\')\')\r\n                                coordinates = rectSeg[posBracket1+1:posBracket2].split(\',\')\r\n                                #print coordinates\r\n                                #print ""Coordinates: "",coordinates\r\n                                rect.x1 = float(round(float(coordinates[0].strip())))\r\n                                rect.y1 = float(round(float(coordinates[1].strip())))\r\n                                rect.x2 = float(round(float(coordinates[2].strip())))\r\n                                rect.y2 = float(round(float(coordinates[3].strip())))\r\n                                posColon = rectSeg.find(\':\')\r\n                                posSlash = rectSeg.find(\'/\')\r\n                                if (posSlash!=-1):\r\n                                        rect.silhouetteID = int(rectSeg[posSlash+1:])\r\n                                else:\r\n                                        rectSeg+=""\\n""\r\n                                if (posColon!=-1):\r\n                                        #print rectSeg[posColon+1:posSlash]\r\n                                        rect.score = float(rectSeg[posColon+1:posSlash])\r\n                                anno.rects.append(rect)\r\n\r\n                annotations.append(anno)\r\n\r\n        return annotations\r\n\r\n\r\n\r\n\r\n\r\n#####################################################################\r\n### Saving\r\n\r\ndef save(filename, annotations):\r\n        print (""saving: "", filename)\r\n\r\n        name, ext = os.path.splitext(filename)\r\n\r\n        if (ext == "".gz"" or ext == "".bz2""):\r\n                name, ext = os.path.splitext(name)\r\n\r\n        if(ext == "".idl""):\r\n                return saveIDL(filename, annotations)\r\n\r\n        elif(ext == "".al""):\r\n                return saveXML(filename, annotations)\r\n\r\n        elif(ext == "".pal""):\r\n                return PalLib.savePal(filename, PalLib.al2pal(annotations));\r\n\r\n        else:\r\n                assert(False);\r\n                return False;\r\n\r\ndef saveIDL(filename, annotations):\r\n        [name, ext] = os.path.splitext(filename)\r\n\r\n        if(ext == "".idl""):\r\n                file = open(filename,\'w\')\r\n\r\n        if(ext == "".gz""):\r\n                file = gzip.GzipFile(filename, \'w\')\r\n\r\n        if(ext == "".bz2""):\r\n                file = bz2.BZ2File(filename, \'w\')\r\n\r\n        i=0\r\n        for annotation in annotations:\r\n                annotation.writeIDL(file)\r\n                if (i+1<len(annotations)):\r\n                        file.write("";\\n"")\r\n                else:\r\n                        file.write("".\\n"")\r\n                i+=1\r\n\r\n        file.close()\r\n\r\ndef idlBase(filename):\r\n        if (filename.rfind("".pal"") == len(filename) - 4):\r\n                return (filename[:-4], "".pal"")\r\n\r\n        if (filename.rfind("".idl"") == len(filename) - 4):\r\n                return (filename[:-4], "".idl"")\r\n\r\n        if (filename.rfind("".al"") == len(filename) - 3):\r\n                return (filename[:-3], "".al"")\r\n\r\n        if (filename.rfind("".idl.gz"") == len(filename) - 7):\r\n                return (filename[:-7], "".idl.gz"")\r\n\r\n        if (filename.rfind("".idl.bz2"") == len(filename) - 8):\r\n                return (filename[:-8], "".idl.bz2"")\r\n\r\n        if (filename.rfind("".al.gz"") == len(filename) - 6):\r\n                return (filename[:-6], "".al.gz"")\r\n\r\n        if (filename.rfind("".al.bz2"") == len(filename) - 7):\r\n                return (filename[:-7], "".al.bz2"")\r\n\r\ndef saveXML(filename, annotations):\r\n        document = xml.dom.minidom.Document()\r\n        rootnode = document.createElement(""annotationlist"")\r\n        for anno in annotations:\r\n                anno.addToXML(rootnode, document)\r\n        document.appendChild(rootnode)\r\n        [name, ext] = os.path.splitext(filename)\r\n        if(ext == "".al""):\r\n                writer = open(filename,\'w\')\r\n        elif(ext == "".gz""):\r\n                writer = gzip.GzipFile(filename, \'w\')\r\n        elif(ext == "".bz2""):\r\n                writer = bz2.BZ2File(filename, \'w\')\r\n        else:\r\n                print (""invalid filename - .al(.gz|.bz2) is accepted"")\r\n                return\r\n\r\n\r\n        if xml_dom_ext_available:\r\n                xml.dom.ext.PrettyPrint(document, writer)\r\n        else:\r\n                # MA: skip header (currently Matlab\'s loadannotations can\'t deal with the header)\r\n                document.documentElement.writexml(writer);\r\n\r\n                #document.writexml(writer)\r\n\r\n        document.unlink()\r\n\r\n\r\n\r\n\r\n\r\n#####################################################################\r\n### Statistics\r\n\r\ndef getStats(annotations):\r\n        no = 0\r\n        noTiny =0\r\n        noSmall =0\r\n        heights = []\r\n        widths =[]\r\n\r\n        ###--- get all rects ---###\r\n        for anno in annotations:\r\n                no = no + len(anno.rects)\r\n                for rect in anno.rects:\r\n                        if (rect.height()<36):\r\n                                noTiny=noTiny+1\r\n                        if (rect.height()<128):\r\n                                noSmall=noSmall+1\r\n                        heights.append(rect.height())\r\n                        if (rect.width()==0):\r\n                                print (""Warning: width=0 in image "", anno.imageName)\r\n                                widths.append(1)\r\n                        else:\r\n                                widths.append(rect.width())\r\n                                if (float(rect.height())/float(rect.width())<1.5):\r\n                                        print (""Degenerated pedestrian annotation: "", anno.imageName)\r\n\r\n        ###--- compute average height and variance ---###\r\n        avgHeight = 0\r\n        varHeight = 0\r\n\r\n\r\n        minHeight = 0\r\n        maxHeight = 0\r\n        if len(heights) > 0:\r\n                minHeight = heights[0]\r\n                maxHeight = heights[0]\r\n\r\n        for height in heights:\r\n                avgHeight = avgHeight+height\r\n                if (height > maxHeight):\r\n                        maxHeight = height\r\n                if (height < minHeight):\r\n                        minHeight = height\r\n\r\n        if (no>0):\r\n                avgHeight = avgHeight/no\r\n        for height in heights:\r\n                varHeight += (height-avgHeight)*(height-avgHeight)\r\n        if (no>1):\r\n                varHeight=float(varHeight)/float(no-1)\r\n\r\n        ###--- compute average width and variance ---###\r\n        avgWidth = 0\r\n        varWidth = 0\r\n        for width in widths:\r\n                avgWidth = avgWidth+width\r\n        if (no>0):\r\n                avgWidth = avgWidth/no\r\n        for width in widths:\r\n                varWidth += (width-avgWidth)*(width-avgWidth)\r\n\r\n        if (no>1):\r\n                varWidth=float(varWidth)/float(no-1)\r\n\r\n        ###--- write statistics ---###\r\n        print(""  Total # rects:"", no)\r\n        print(""     avg. Width:"", avgWidth, "" ("", sqrt(varWidth), ""standard deviation )"")\r\n        print(""    avg. Height:"", avgHeight, "" ("", sqrt(varHeight), ""standard deviation )"")\r\n        print(""     tiny rects:"", noTiny, "" (< 36 pixels)"")\r\n        print(""    small rects:"", noSmall, "" (< 128 pixels)"")\r\n        print(""    minimum height:"", minHeight)\r\n        print(""    maximum height:"", maxHeight)\r\n\r\n        ###--- return ---###\r\n        return [widths, heights]\r\n\r\n############################################################\r\n##\r\n##  IDL merging\r\n##\r\n\r\ndef mergeIDL(detIDL, det2IDL, detectionFuse= True, minOverlap = 0.5):\r\n        mergedIDL = []\r\n\r\n        for i,anno in enumerate(detIDL):\r\n                mergedAnno = Annotation()\r\n                mergedAnno.imageName = anno.imageName\r\n                mergedAnno.frameNr = anno.frameNr\r\n                mergedAnno.rects = anno.rects\r\n\r\n                imageFound = False\r\n                filterIndex = -1\r\n                for i,filterAnno in enumerate(det2IDL):\r\n                                if (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\r\n                                        filterIndex = i\r\n                                        imageFound = True\r\n                                        break\r\n\r\n                if(not imageFound):\r\n                        mergedIDL.append(mergedAnno)\r\n                        continue\r\n\r\n                for rect in det2IDL[filterIndex].rects:\r\n                        matches = False\r\n\r\n                        for frect in anno.rects:\r\n                                if rect.overlap_pascal(frect) > minOverlap:\r\n                                        matches = True\r\n                                        break\r\n\r\n                        if (not matches or detectionFuse == False):\r\n                                mergedAnno.rects.append(rect)\r\n\r\n                mergedIDL.append(mergedAnno)\r\n\r\n        return mergedIDL\r\n\r\n\r\n############################################################################33\r\n#\r\n# Function to force the aspect ratio of annotations to ratio = width / height\r\n#\r\n#\r\ndef forceAspectRatio(annotations, ratio, KeepHeight = False, KeepWidth = False):\r\n        for anno in annotations:\r\n                for rect in anno.rects:\r\n                        rect.forceAspectRatio(ratio, KeepHeight, KeepWidth)\r\n                        #Determine which side needs to be extended\r\n#                       if (rect.width() * 1.0 / rect.height() > ratio):\r\n#\r\n#                               #Too wide -> extend height\r\n#                               newHeight = rect.width() * 1.0 / ratio\r\n#                               rect.y1 = int(rect.centerY() - newHeight / 2.0)\r\n#                               rect.y2 = int(rect.y1 + newHeight)\r\n#\r\n#                       else:\r\n#                               #Too short -> extend width\r\n#                               newWidth = rect.height() * ratio\r\n#                               rect.x1 = int(rect.centerX() - newWidth / 2.0)\r\n#                               rect.x2 = int(rect.x1 + newWidth)\r\n\r\n\r\n###################################################################\r\n# Function to greedyly remove subset detIDL from gtIDL\r\n#\r\n# returns two sets\r\n#\r\n# [filteredIDL, missingRecallIDL]\r\n#\r\n# filteredIDL == Rects that were present in both sets\r\n# missingRecallIDL == Rects that were only present in set gtIDL\r\n#\r\n###################################################################\r\ndef extractSubSet(gtIDL, detIDL):\r\n        filteredIDL = []\r\n        missingRecallIDL = []\r\n\r\n        for i,gtAnno in enumerate(gtIDL):\r\n                filteredAnno = Annotation()\r\n                filteredAnno.imageName = gtAnno.imageName\r\n                filteredAnno.frameNr = gtAnno.frameNr\r\n\r\n                missingRecallAnno = Annotation()\r\n                missingRecallAnno.imageName = gtAnno.imageName\r\n                missingRecallAnno.frameNr = gtAnno.frameNr\r\n\r\n                imageFound = False\r\n                filterIndex = -1\r\n                for i,anno in enumerate(detIDL):\r\n                                if (suffixMatch(anno.imageName, gtAnno.imageName) and anno.frameNr == gtAnno.frameNr):\r\n                                        filterIndex = i\r\n                                        imageFound = True\r\n                                        break\r\n\r\n                if(not imageFound):\r\n                        print(""Image not found "" + gtAnno.imageName + "" !"")\r\n                        missingRecallIDL.append(gtAnno)\r\n                        filteredIDL.append(filteredAnno)\r\n                        continue\r\n\r\n                matched = [-1] * len(detIDL[filterIndex].rects)\r\n                for j, rect in enumerate(gtAnno.rects):\r\n                        matches = False\r\n\r\n                        matchingID = -1\r\n                        minCenterPointDist = -1\r\n                        for k,frect in enumerate(detIDL[filterIndex].rects):\r\n                                minCover = 0.5\r\n                                minOverlap = 0.5\r\n                                maxDist = 0.5\r\n\r\n                                if rect.isMatchingStd(frect, minCover,minOverlap, maxDist):\r\n                                        if (matchingID == -1 or rect.distance(frect) < minCenterPointDist):\r\n                                                matchingID = k\r\n                                                minCenterPointDist = rect.distance(frect)\r\n                                                matches = True\r\n\r\n                        if (matches):\r\n                                #Already matched once check if you are the better match\r\n                                if(matched[matchingID] >= 0):\r\n                                        #Take the match with the smaller center point distance\r\n                                        if(gtAnno.rects[matched[matchingID]].distance(frect) > rect.distance(frect)):\r\n                                                missingRecallAnno.rects.append(gtAnno.rects[matched[matchingID]])\r\n                                                filteredAnno.rects.remove(gtAnno.rects[matched[matchingID]])\r\n                                                filteredAnno.rects.append(rect)\r\n                                                matched[matchingID] = j\r\n                                        else:\r\n                                                missingRecallAnno.rects.append(rect)\r\n                                else:\r\n                                        #Not matched before.. go on and add the match\r\n                                        filteredAnno.rects.append(rect)\r\n                                        matched[matchingID] = j\r\n                        else:\r\n                                missingRecallAnno.rects.append(rect)\r\n\r\n                filteredIDL.append(filteredAnno)\r\n                missingRecallIDL.append(missingRecallAnno)\r\n\r\n        return (filteredIDL     , missingRecallIDL)\r\n\r\n###########################################################\r\n#\r\n#  Function to remove all detections with a too low score\r\n#\r\n#\r\ndef filterMinScore(detections, minScore):\r\n        newDetections = []\r\n        for anno in detections:\r\n                newAnno = Annotation()\r\n                newAnno.frameNr = anno.frameNr\r\n                newAnno.imageName = anno.imageName\r\n                newAnno.imagePath = anno.imagePath\r\n                newAnno.rects = []\r\n\r\n                for rect in anno.rects:\r\n                        if(rect.score >= minScore):\r\n                                newAnno.rects.append(rect)\r\n\r\n                newDetections.append(newAnno)\r\n        return newDetections\r\n\r\n# foo.idl -> foo-suffix.idl, foo.idl.gz -> foo-suffix.idl.gz etc\r\ndef suffixIdlFileName(filename, suffix):\r\n        exts = ["".idl"", "".idl.gz"", "".idl.bz2""]\r\n        for ext in exts:\r\n                if filename.endswith(ext):\r\n                        return filename[0:-len(ext)] + ""-"" + suffix + ext\r\n        raise ValueError(""this does not seem to be a valid filename for an idl-file"")\r\n\r\nif __name__ == ""__main__"":\r\n# test output\r\n        idl = parseIDL(""/tmp/asdf.idl"")\r\n        idl[0].rects[0].articulations = [4,2]\r\n        idl[0].rects[0].viewpoints = [2,3]\r\n        saveXML("""", idl)\r\n\r\n\r\ndef annoAnalyze(detIDL):\r\n        allRects = []\r\n\r\n        for i,anno in enumerate(detIDL):\r\n                for j in anno.rects:\r\n                        newRect = detAnnoRect()\r\n                        newRect.imageName = anno.imageName\r\n                        newRect.frameNr = anno.frameNr\r\n                        newRect.rect = j\r\n                        allRects.append(newRect)\r\n\r\n        allRects.sort(cmpDetAnnoRectsByScore)\r\n\r\n        filteredIDL = AnnoList([])\r\n        for i in allRects:\r\n                a = Annotation()\r\n                a.imageName = i.imageName\r\n                a.frameNr = i.frameNr\r\n                a.rects = []\r\n                a.rects.append(i.rect)\r\n                filteredIDL.append(a)\r\n\r\n        return filteredIDL\r\n\r\n\r\n'"
submodules/utils/annolist/MatPlotter.py,0,"b'import os\nimport sys\nimport string\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom pylab import *\nimport numpy as np\n\nclass MatPlotter:\n\tfontsize=15\n\tcolor=0\n\tcolors=[""r-"", ""b-"", ""k-"", ""c-"", ""m-"", ""y-""]\n\tcolors+=[x + ""-"" for x in colors]\n\tcolors+=[""g-"", ""g--""]\n\tcurFigure=[]\n\tlegendNames=[]\n\tfontsizeLegend=14\n\tlegendPlace=\'lower right\'\n\tlegendborderpad = None\n\tlegendlabelsep = None\n\t\t\n\n\tdef __init__(self, fontsize=15):\n\t\t# self.newFigure()\n\t\tself.fontsize=fontsize\n\t\tself.fontsizeLegend=fontsize - 1\n\t\tpass\n\t\t\t\t\t\n\tdef formatLegend(self, newFontSize = 14, newPlace = \'lower right\', borderpad = None, labelsep = None):\n\t\tself.fontsizeLegend=newFontSize\n\t\tself.legendPlace=newPlace\n\t\tself.legendborderpad = borderpad\n\t\tself.legendlabelsep = labelsep\n\t\t\n\tdef newFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\treturn self.newRPCFigure(plotTitle, fsize)\n\n\tdef newRPCFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\t#subplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n\t\tsubplots_adjust(right=0.975, top=0.975)\n\n\t\t#axis(\'equal\')\n\t\taxis([0, 1, 0, 1])\n\t\txticklocs, xticklabels = xticks(arange(0, 1.01, 0.1))\n\t\tsetp(xticklabels, size=self.fontsize)\n\t\tyticklocs, yticklabels = yticks(arange(0, 1.01, 0.1))\n\t\tsetp(yticklabels, size=self.fontsize)\n\t\tself.xlabel = xlabel(""1-precision"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""recall"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\tgrid()\n\t\thold(True)\n\t\t\n\tdef newFPPIFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\tsubplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n\n\t\t#axis(\'equal\')\n\t\taxis([0, 100, 0, 1])\n\t\txticklocs, xticklabels = xticks(arange(0, 100.01, 0.5))\n\t\tsetp(xticklabels, size=self.fontsize)\n\t\tyticklocs, yticklabels = yticks(arange(0, 1.01, 0.1))\n\t\tsetp(yticklabels, size=self.fontsize)\n\t\tself.xlabel = xlabel(""false positives per image"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""recall"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\tgrid()\n\t\thold(True)\n\n\n\tdef newFreqFigure(self, plotTitle="""", maxX = 10, maxY = 10,fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\tsubplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.1)\n\t\t#axis(\'equal\')\n\n\t\taxis([0, maxX, 0, maxY])\n\t\txticklocs, xticklabels = xticks(arange(0, maxX + 0.01, maxX * 1.0/ 10))\n\t\tsetp(xticklabels, size=self.fontsize)\n\t\tyticklocs, yticklabels = yticks(arange(0, maxY + 0.01, maxY * 1.0/ 10))\n\t\tsetp(yticklabels, size=self.fontsize)\n\t\tself.xlabel = xlabel(""False positive / ground truth rect"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""True positives / ground truth rect"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\tgrid()\n\t\thold(True)\n\n\tdef newFPPWFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\tsubplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n\n\t\tself.xlabel = xlabel(""false positive per windows (FPPW)"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""miss rate"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\t\n\t\tgrid()\n\t\thold(True)\n\n\tdef newLogFPPIFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\tsubplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.1)\n\n\t\t#axis(\'equal\')\n\n\t\tself.xlabel = xlabel(""false positives per image"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""miss rate"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\tgrid()\n\t\thold(True)\n\n\tdef loadRPCData(self, fname):\n\t\tself.filename = fname\n\t\tself.prec=[]\n\t\tself.rec=[]\n\t\tself.score=[]\n\t\tself.fppi=[]\n\t\tfile = open(fname)\n\n\t\tprecScores = []\n\t\tfor i in range(1,10,1):\n\t\t\tprecScores.append(100 - i * 10)\n\t\t\t\n\t\tfppiScores=[]\n\t\tfor i in range(0, 500, 5):\n\t\t\tfppiScores.append(i * 1.0 / 100.0)\n\t\t\t\n\t\n\n\t\tprecinfo = []\n\t\tfppiinfo = []\n\t\teerinfo = []\n\t\tlogAvInfo = []\n\t\t\n\t\tlogAvMR= []\n\t\tself.lamr = 0;\n\t\tself.eer = None;\n\t\tfirstLine = True\n\t\tleadingZeroCount = 0\n\n\t\tfor line in file.readlines():\n\t\t\tvals = line.split()\n\t\t\t#vals=line.split("" "")\n\t\t\t#for val in vals:\n\t\t\t#\tif val=="""":\n\t\t\t#\t\tvals.remove(val)\n\t\t\tself.prec.append(1-float(vals[0]))\n\t\t\tself.rec.append(float(vals[1]))\n\t\t\tself.score.append(float(vals[2]))\n\n\t\t\tif(len(vals)>3):\n\t\t\t\tself.fppi.append(float(vals[3]))\n\t\t\t\tif firstLine and not float(vals[3]) == 0:\n\t\t\t\t\tfirstLine = False\n\t\t\t\t\t\n\t\t\t\t\tlamrcount = 1\n\t\t\t\t\tself.lamr = 1 - float(vals[1])\n\t\t\t\t\t\n\t\t\t\t\tlowest_fppi = math.ceil( math.log(float(vals[3]))/ math.log(10) * 10 )  \n\t\t\t\t\tprint ""lowest_fppi: "",lowest_fppi;\n\n\t\t\t\t\t# MA: temporarily commented out \n\t\t\t\t\t# for i in range(lowest_fppi, 1, 1):\t\t\t\t\t\t\n\t\t\t\t\t# \tlogAvMR.append(10** (i * 1.0 / 10))\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t#self.score.append(float(vals[2][:-1]))\n\t\t\t#print 1-self.prec[-1], self.rec[-1], self.score[-1]\n\t\t\tif (len(self.prec)>1):\n\t\t\t\tdiff = (1-self.prec[-1]-self.rec[-1]) * (1-self.prec[-2]-self.rec[-2])\n\t\t\t\tif ( diff <0):\n\t\t\t\t\teerinfo.append( ""EER between: %.03f and %.03f\\tScore:%f"" % (self.rec[-1], self.rec[-2], self.score[-1]))\n\t\t\t\t\tself.eer = (self.rec[-1]+self.rec[-2]) * 0.5\n\t\t\t\tif ( diff == 0 and 1-self.prec[-1]-self.rec[-1]==0):\n\t\t\t\t\teerinfo.append( ""EER: %.03f\\tScore:%f"" % (self.rec[-1], self.score[-1]))\n\t\t\t\t\tself.eer = self.rec[-1]\n\n\t\t\t#Remove already passed precision\n\t\t\tif (len(precScores) > 0 and (float(vals[0])) < precScores[0] / 100.0):\n\t\t\t\tprecinfo.append(""%d percent precision score: %f, recall: %.03f"" % (precScores[0], float(vals[2]), float(vals[1])))\n\t\t\t\twhile(len(precScores) > 0 and precScores[0]/100.0 > float(vals[0])):\n\t\t\t\t\tprecScores.pop(0)\n\t\t\t\t\t\n\t\t\t#Remove already passed precision\n\t\t\tif(len(vals) > 3):\n\t\t\t\tif (len(fppiScores) > 0 and (float(vals[3])) > fppiScores[0]):\n\t\t\t\t\tfppiinfo.append(""%f fppi score: %f, recall: %.03f"" % (fppiScores[0], float(vals[2]), float(vals[1])))\n\t\t\t\t\twhile(len(fppiScores) > 0 and fppiScores[0] < float(vals[3])):\n\t\t\t\t\t\tfppiScores.pop(0)\n\t\t\t\t\n\t\t\t\tif (len(logAvMR) > 0 and (float(vals[3])) > logAvMR[0]):\n\t\t\t\t\twhile(len(logAvMR) > 0 and logAvMR[0] < float(vals[3])):\n\t\t\t\t\t\tlogAvInfo.append(""%f fppi, miss rate: %.03f, score: %f"" % (logAvMR[0], 1-float(vals[1]), float(vals[2])) )\n\t\t\t\t\t\tself.lamr += 1-float(vals[1])\n\t\t\t\t\t\tlamrcount += 1\t\t\t\t\t\t\n\t\t\t\t\t\tlogAvMR.pop(0)\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tlastMR = 1-float(vals[1])\n\t\t\t\t\n\t\t\n\t\tif(len(vals)>3):\n\t\t\tfor i in logAvMR:\n\t\t\t\tlogAvInfo.append(""%f fppi, miss rate: %.03f, extended"" % (i, lastMR) )\n\t\t\t\tself.lamr += lastMR\n\t\t\t\tlamrcount += 1\n\t\t\n\t\tfor i in precinfo:\n\t\t\tprint i;\n\t\tprint;\n\t\tfor i in fppiinfo:\n\t\t\tprint i;\n\t\tprint\n\t\tfor i in eerinfo:\n\t\t\tprint i;\n\t\tprint\n\t\tprint ""Recall at first false positive: %.03f"" % self.rec[0]\n\t\tif(len(vals)>3):\n\t\t\tprint\n\t\t\tfor i in logAvInfo:\n\t\t\t\tprint i;\n\t\t\tself.lamr =  self.lamr * 1.0 / lamrcount\n\t\t\tprint ""Log average miss rate in [10^%.01f, 10^0]: %.03f"" % (lowest_fppi / 10.0, self.lamr )\n\t\n\t\t\n\t\t\n\t\tprint; print\n\t\tfile.close()\n\n\tdef loadFreqData(self, fname):\n\t\tself.filename = fname\n\t\tself.prec=[]\n\t\tself.rec=[]\n\t\tself.score=[]\n\t\tfile = open(fname)\n\n\t\tfor line in file.readlines():\n\t\t\tvals = line.split()\n\n\t\t\tself.prec.append(float(vals[0]))\n\t\t\tself.rec.append(float(vals[1]))\n\t\t\tself.score.append(float(vals[2]))\n\n\t\tfile.close()\n\n\tdef loadFPPWData(self, fname):\n\t\tself.loadFreqData(fname)\n\n\tdef finishPlot(self, axlimits = [0,1.0,0,1.0]):\n\t\t# MA:\n\t\t#self.legend = legend(self.legendNames, self.legendPlace, pad = self.legendborderpad, labelsep = self.legendlabelsep)\n\t\tself.legend = legend(self.legendNames, self.legendPlace)\n\t\t\n\t\tlstrings = self.legend.get_texts()\n\t\tsetp(lstrings, fontsize=self.fontsizeLegend)\t\t\n\t\t#line= plot( [1 - axlimits[0], 0], [axlimits[3], 1 - axlimits[3] ] , \'k\')\n\t\tline= plot( [1, 0], [0, 1] , \'k\')\n\n\tdef finishFreqPlot(self):\n\t\tself.legend = legend(self.legendNames, self.legendPlace, pad = self.legendborderpad, labelsep = self.legendlabelsep)\n\t\tlstrings = self.legend.get_texts()\t\t\n\t\tsetp(lstrings, fontsize=self.fontsizeLegend)\n\t\t\n\n\tdef show(self, plotEER = True, axlimits = [0,1.0,0,1.0]):\n\t\tif (plotEER):\t\t\t\n\t\t\tself.finishPlot(axlimits)\n\t\t\taxis(axlimits)\n\t\telse:\n\t\t\tself.finishFreqPlot()\n\n\t\tshow()\n\n\tdef saveCurrentFigure(self, plotEER, filename, axlimits = [0,1.0,0,1.0]):\n\t\tif (plotEER):\n\t\t\tself.finishPlot(axlimits)\n\t\t\taxis(axlimits)\n\t\telse:\n\t\t\tself.finishFreqPlot()\t\t\t\n\t\t\n\t\tprint ""Saving: "" + filename\t\t\n\t\tsavefig(filename)\n\n\tdef plotRFP(self, numImages, fname, line=""r-""):\n\t\tprint \'NOT YET IMPLEMENTED\'\n\n\tdef plotRPC(self, fname, descr=""line"", style=""-1"", axlimits = [0,1.0,0,1.0], linewidth = 2, dashstyle = [], addEER = False ):\n\t\tself.loadRPCData(fname)\n\t\t\n\t\t#axis(axlimits);\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.prec, self.rec, self.colors[self.color])\n\t\t\tself.color=self.color+1\n\t\t\tself.color=self.color % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.prec, self.rec, style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.prec, self.rec, style)\n\t\t\n\t\taxis(axlimits)\n\t\t\n\t\tif addEER and self.eer != None:\n\t\t\tdescr += "" (%.01f%%)"" % (self.eer * 100)\n\t\t\t\t\t\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\tself.legendNames= self.legendNames+[descr]\n\n\tdef plotFPPI(self, fname, descr=""line"", style=""-1"", axlimits = [0,2,0,1], linewidth = 2, dashstyle = []):\n\t\tself.loadRPCData(fname)\n\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.fppi, self.rec, self.colors[self.color], dashes = dashstyle)\t\t\t\t\n\t\t\telse:\n\t\t\t\tline = plot(self.fppi, self.rec, self.colors[self.color])\t\t\t\t\n\t\t\tself.color=self.color+1\n\t\t\tself.color=self.color % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.fppi, self.rec, style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.fppi, self.rec, style)\n\t\t\n\t\taxis(axlimits);\n\t\t\t\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\tself.legendNames= self.legendNames+[descr]\n\t\t\t\n\t\t\n\tdef plotFreq(self, fname, descr=""line"", style=""-1"", linewidth = 2, dashstyle = []):\n\t\tself.loadFreqData(fname)\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.prec, self.rec, self.colors[self.color])\n\t\t\tself.color=self.color+1\n\t\t\tself.color=self.color % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.prec, self.rec, style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.prec, self.rec, style)\n\n\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\tself.legendNames= self.legendNames+[descr]\n\n\tdef plotFPPW(self, fname, descr=""line"", style=""-1"", axlimits = [5e-6, 1e0, 1e-2, 0.5], linewidth = 2, dashstyle = []):\n\t\tself.loadFPPWData(fname)\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = loglog(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = loglog(self.prec, self.rec, self.colors[self.color])\n\t\t\tself.color=self.color+1\n\t\t\tself.color=self.color % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = loglog(self.prec, self.rec, style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = loglog(self.prec, self.rec, style)\n\t\t\n \t\txticklocs, xticklabels = xticks([1e-5, 1e-4,1e-3, 1e-2, 1e-1, 1e0])\n \t\tsetp(xticklabels, size=self.fontsize)\n \t\tyticklocs, yticklabels = yticks(array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5]), \n\t\t\t\t\t\t\t\t\t\t(""0.01"", ""0.02"", ""0.03"", ""0.04"", ""0.05"", ""0.06"", ""0.07"", ""0.08"",""0.09"", ""0.1"", ""0.2"", ""0.3"", ""0.4"", ""0.5""))\n \t\tsetp(yticklabels, size=self.fontsize)\n \t\t\n \t\taxis(axlimits)\n\n\t\tgca().yaxis.grid(True, \'minor\')\t\t\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\t\t\t\n\t\tself.legendNames= self.legendNames+[descr]\n\n\tdef plotLogFPPI(self, fname, descr=""line"", style=""-1"", axlimits = [5e-3, 1e1, 1e-1, 1], linewidth = 2, dashstyle = [], addlamr = False):\n\t\tself.loadRPCData(fname)\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = loglog(self.fppi, [1 - x for x in self.rec], self.colors[self.color], dashes = dashstyle)\t\t\t\t\n\t\t\telse:\n\t\t\t\tline = loglog(self.fppi, [1 - x for x in self.rec], self.colors[self.color])\t\t\t\t\n\n\t\t\tself.color=(self.color+1) % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = loglog(self.fppi, [1 - x for x in self.rec], style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = loglog(self.fppi, [1 - x for x in self.rec], style)\n\t\t\t\t\n\t\tgca().yaxis.grid(True, \'minor\')\n\t\t\t\t\n\t\tm = min(self.fppi)\n\t\tlax = axlimits[0]\n\t\tfor i in self.fppi:\n\t\t\tif(i != m):\t\t\t\t\n\t\t\t\tlax = math.floor(log(i)/math.log(10))\n\t\t\t\tleftlabel = math.pow(10, lax)\t\t\t\t \n\t\t\t\tbreak\n\t\t\t\n\t\tm = max(self.fppi)\n\t\trightlabel = math.pow(10, math.ceil(log(m)/math.log(10))) + 0.01\t\t\t\t \n\t\t\t\t\n\t\tk = leftlabel\n\t\tticks = [k]\n\t\twhile k < rightlabel:\n\t\t\tk = k * 10\n\t\t\tticks.append(k)\n\t\t\t\t\n \t\txticklocs, xticklabels = xticks(ticks)\n \t\tsetp(xticklabels, size=self.fontsize)\n \t\tyticklocs, yticklabels = yticks(arange(0.1, 1.01, 0.1), (""0.1"", ""0.2"", ""0.3"", ""0.4"", ""0.5"", ""0.6"", ""0.7"", ""0.8"", ""0.9"", ""1.0""))\n \t\tsetp(yticklabels, size=self.fontsize)\n \t\t\n \t\taxlimits[0] = lax\t\t\n \t\taxis(axlimits)\t\t\n\t\t\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\t\n\t\tif addlamr:\n\t\t\tdescr += "" (%.01f%%)"" % (self.lamr * 100)\n\t\t\t\n\t\tself.legendNames= self.legendNames+[descr]\n'"
submodules/utils/annolist/PalLib.py,0,"b'import sys\n#import AnnoList_pb2\nimport utils.annolist.AnnotationLib\n\nfrom utils.annolist.ma_utils import is_number;\n\ndef loadPal(filename):\n    _annolist = AnnoList_pb2.AnnoList();\n\n    f = open(filename, ""rb"");\n    _annolist.ParseFromString(f.read());\n    f.close();\n\n    return _annolist;\n\ndef savePal(filename, _annolist):\n    f = open(filename, ""wb"");\n    f.write(_annolist.SerializeToString());\n    f.close();\n\ndef al2pal(annotations):\n    _annolist = AnnoList_pb2.AnnoList();\n\n    #assert(isinstance(annotations, AnnotationLib.AnnoList));\n\n    # check type of attributes, add missing attributes\n    for a in annotations:\n        for r in a.rects:\n            for k, v in r.at.iteritems():\n                if not k in annotations.attribute_desc:\n                    annotations.add_attribute(k, type(v));\n                else:\n                    assert(AnnotationLib.is_compatible_attr_type(annotations.attribute_desc[k].dtype, type(v)));\n\n    # check attributes values\n    for a in annotations:\n        for r in a.rects:\n            for k, v in r.at.iteritems():\n                if k in annotations.attribute_val_to_str:\n                    # don\'t allow undefined values\n                    if not v in annotations.attribute_val_to_str[k]:\n                        print(""attribute: {}, undefined value: {}"".format(k, v))\n                        assert(False);\n\n    # store attribute descriptions in pal structure\n    for aname, adesc in annotations.attribute_desc.iteritems():\n        _annolist.attribute_desc.extend([adesc]);\n\n    for a in annotations:\n        _a = _annolist.annotation.add();\n        _a.imageName = a.imageName;\n\n        for r in a.rects:\n            _r = _a.rect.add();\n\n            _r.x1 = r.x1;\n            _r.y1 = r.y1;\n            _r.x2 = r.x2;\n            _r.y2 = r.y2;\n\n            _r.score = float(r.score);\n\n            if hasattr(r, \'id\'):\n                _r.id = r.id;\n\n            if hasattr(r, \'track_id\'):\n                _r.track_id = r.track_id;\n\n            if hasattr(r, \'at\'):\n                for k, v in r.at.items():\n                    _at = _r.attribute.add();\n\n                    _at.id = annotations.attribute_desc[k].id;\n\n                    if annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_INT32:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_INT32, type(v)));\n                        _at.val = int(v);\n                    elif annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_FLOAT:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_FLOAT, type(v)));\n                        _at.fval = float(v);\n                    elif annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_STRING:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_STRING, type(v)));\n                        _at.strval = str(v);\n                    else:\n                        assert(false);\n\n    return _annolist;\n\ndef pal2al(_annolist):\n    #annotations = [];\n    annotations = AnnotationLib.AnnoList();\n\n    for adesc in _annolist.attribute_desc:\n        annotations.attribute_desc[adesc.name] = adesc;\n        print(""attribute: "", adesc.name, adesc.id)\n\n        for valdesc in adesc.val_to_str:\n            annotations.add_attribute_val(adesc.name, valdesc.s, valdesc.id);\n\n    attribute_name_from_id = {adesc.id: aname for aname, adesc in annotations.attribute_desc.iteritems()}\n    attribute_dtype_from_id = {adesc.id: adesc.dtype for aname, adesc in annotations.attribute_desc.iteritems()}\n\n    for _a in _annolist.annotation:\n        anno = AnnotationLib.Annotation()\n\n        anno.imageName = _a.imageName;\n\n        anno.rects = [];\n\n        for _r in _a.rect:\n            rect = AnnotationLib.AnnoRect()\n\n            rect.x1 = _r.x1;\n            rect.x2 = _r.x2;\n            rect.y1 = _r.y1;\n            rect.y2 = _r.y2;\n\n            if _r.HasField(""id""):\n                rect.id = _r.id;\n\n            if _r.HasField(""track_id""):\n                rect.track_id = _r.track_id;\n\n            if _r.HasField(""score""):\n                rect.score = _r.score;\n\n            for _at in _r.attribute:\n                try:\n                    cur_aname = attribute_name_from_id[_at.id];\n                    cur_dtype = attribute_dtype_from_id[_at.id];\n                except KeyError as e:\n                    print(""attribute: "", _at.id)\n                    print(e)\n                    assert(False);\n\n                if cur_dtype == AnnotationLib.AnnoList.TYPE_INT32:\n                    rect.at[cur_aname] = _at.val;\n                elif cur_dtype == AnnotationLib.AnnoList.TYPE_FLOAT:\n                    rect.at[cur_aname] = _at.fval;\n                elif cur_dtype == AnnotationLib.AnnoList.TYPE_STRING:\n                    rect.at[cur_aname] = _at.strval;\n                else:\n                    assert(False);\n\n            anno.rects.append(rect);\n\n        annotations.append(anno);\n\n    return annotations;\n'"
submodules/utils/annolist/__init__.py,0,b''
submodules/utils/annolist/doRPC.py,0,"b'#!/usr/bin/env python\n\nimport os, sys\nfrom AnnotationLib import *\nfrom optparse import OptionParser\nimport copy\nimport math\n\n# BASED ON WIKIPEDIA VERSION\n# n - number of nodes\n# C - capacity matrix\n# F - flow matrix\n# s - source\n# t - sink\n# sumC - sum over rows of C (too speed up computation)\n\ndef edmonds_karp(n, C, s, t, sumC):\n\n\t# Residual capacity from u to v is C[u][v] - F[u][v]\n\tF = [[0] * n for i in xrange(n)]\n\twhile True:\n\t\tP = [-1] * n # Parent table\n\t\tP[s] = s\n\t\tM = [0] * n  # Capacity of path to node\n\t\tM[s] = float(\'infinity\')\n\t\tQ = [s]      # BFS queue\n\t\twhile Q:\n\t\t\tu = Q.pop(0)\t\t\t\n\t\t\tfor v in xrange(n):\n\t\t\t\t# There is available capacity, \n               \t# and v is not seen before in search\n\t\t\t\tif C[u][v] - F[u][v] > 0 and P[v] == -1:\n\t\t\t\t\tP[v] = u\n\t\t\t\t\tM[v] = min(M[u], C[u][v] - F[u][v])\n\t\t\t\t\tif v != t:\n\t\t\t\t\t\tif(sumC[u] > 0):\n\t\t\t\t\t\t\tQ.append(v)\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Backtrack search, and write flow\n\t\t\t\t\t\twhile P[v] != v:\n\t\t\t\t\t\t\tu = P[v]\n\t\t\t\t\t\t\tF[u][v] += M[t]\n\t\t\t\t\t\t\tF[v][u] -= M[t]\n\t\t\t\t\t\t\tv = u\n\t\t\t\t\t\tQ = None\n\t\t\t\t\t\tbreak\n\t\tif P[t] == -1: # We did not find a path to t\t\t\t\n\t\t\treturn (F)\n\nclass AnnoGraph:\n\n\tdef __init__(self, anno, det, ignore, style, minCover, minOverlap, maxDistance, ignoreOverlap):\n\t\t\n\t\t# setting rects\n\t\t#print anno.imageName\n\t\tself.anno = anno\n\t\tself.det = det\n\t\tself.det.sortByScore(""descending"")\n\t\t\n\t\t# generate initial graph\n\t\tself.n = len(det.rects)\n\t\tself.m = len(anno.rects)\n\t\t\n\t\t# Number of nodes = number of detections + number of GT + source + sink\n\t\tself.a = self.n + self.m + 2\n\t\n\t\t# Flow matrix\n\t\tself.F = [[0] * self.a for i in xrange(self.a)]\n\t\t# Capacity matrix\n\t\tself.C = [[0] * self.a for i in xrange(self.a)]\n\t\t\n\t\t# Connect source to all detections\n\t\tfor i in range(1, self.n + 1):\n\t\t\tself.C[0][i] = 1\n\t\t\tself.C[i][0] = 1\n\t\t\t\n\t\t# Connect sink to all GT\t\n\t\tfor i in range(self.n + 1, self.a - 1):\n\t\t\tself.C[i][self.a - 1] = 1\n\t\t\tself.C[self.a - 1][i] = 1\n\t\t\n\t\t# Overall flow\n\t\tself.full_flow = 0\n\t\tself.ignore_flow = 0\n\t\n\t\t# match rects / Adjacency matrix\n\t\tself.M = [[] for i in xrange(self.n)]\n\t\tself.match(style, minCover, minOverlap, maxDistance)\n\t\tself.nextN = 0\n\t\t\n\t\t# Deactivate All Non Matching detections\n\t\t# Save row sums for capacity matrix\n\t\tself.sumC = []\n\t\tself.sumC.append(self.n)\n\t\tfor q in [len(self.M[j]) for j in xrange(len(self.M))]: \n\t\t\tself.sumC.append(q)\n\t\tfor q in [1] * self.m:\n\t\t\tself.sumC.append(q)\n\t\t\n\t\t# Initially no links are active\n\t\tself.sumC_active = []\n\t\tself.sumC_active.append(self.n)\t\n\t\tfor q in [len(self.M[j]) for j in xrange(len(self.M))]: \n\t\t\tself.sumC_active.append(0)\t\n\t\tfor q in [1] * self.m:\n\t\t\tself.sumC_active.append(q)\n\t\t\t\n\t\t#\n\t\tself.ignore = [ 0 ] * self.m\n\t\tfor ig in ignore.rects:\n\t\t\tfor i, r in enumerate(anno.rects):\n\t\t\t\tif(ig.overlap_pascal(r) > ignoreOverlap):\n\t\t\t\t\tself.ignore[i] = 1\n\t\t\t\t \n\n\tdef match(self, style, minCover, minOverlap, maxDistance):\n\t\tfor i in xrange(self.n):\n\t\t\tdetRect = self.det.rects[i]\n\t\t\tfor j in xrange(self.m):\n\t\t\t\tannoRect = self.anno.rects[j]\n\t\t\t\t\n\t\t\t\t# Bastian Leibe\'s matching style\t\t\t\t\n\t\t\t\tif(style == 0):\t\t\t\t\t\n\t\t\t\t\tassert False;\n\t\t\t\t\tif detRect.isMatchingStd(annoRect, minCover, minOverlap, maxDistance):\n\t\t\t\t\t\tself.M[i].append(self.n + 1 + j)\n\t\t\t\t\t\n\t\t\t\t# Pascal Matching style\t\n\t\t\t\tif(style == 1):\t\n\t\t\t\t\tif (detRect.isMatchingPascal(annoRect, minOverlap)):\n\t\t\t\t\t\tself.M[i].append(self.n + 1 + j)\n\t\t\t\t\t\n\tdef decreaseScore(self, score):\n\t\tcapacity_change = False\n\t\tfor i in xrange(self.nextN, self.n):\n\t\t\tif (self.det.rects[i].score >= score):\t\t\t\t\n\t\t\t\tcapacity_change = self.insertIntoC(i + 1) or capacity_change\n\t\t\t\tself.nextN += 1\n\t\t\telse:\n\t\t\t\tbreak\n\t\t\t\n\t\tif capacity_change:\n\t\t\tself.F = edmonds_karp(self.a, self.C, 0, self.a - 1, self.sumC_active)\n\t\t\tself.full_flow = sum([self.F[0][i] for i in xrange(self.a)])\n\t\t\tself.ignore_flow = sum([self.F[i][self.a - 1] * self.ignore[i - 1 - self.n] for i in range(1 + self.n, 1 + self.n + self.m )])\n\t\t\t\t\t\n\t\treturn capacity_change\n\t\n\tdef addBB(self, rect):\n\t\tself.nextN += 1\n\t\t\n\t\tcapacity_change = self.insertIntoC(rect.boxIndex + 1)\n\t\t\t\n\t\tif capacity_change:\n\t\t\tself.F = edmonds_karp(self.a, self.C, 0, self.a - 1, self.sumC_active)\n\t\t\tself.full_flow = sum([self.F[0][i] for i in xrange(self.a)])\n\t\t\tself.ignore_flow = sum([self.F[i][self.a - 1] * self.ignore[i - 1 - self.n] for i in range(1 + self.n, 1 + self.n + self.m )])\n\t\t\t\n\t\treturn capacity_change\n\n\tdef\tinsertIntoC(self, i):\n\t\t#print ""Inserting node"", i, self.det.rects[i-1].score, ""of image"", self.anno.imageName\n\t\t\t\n\t\tfor match in self.M[i - 1]:\n\t\t\t#print ""  match: "", match\n\t\t\tself.C[i][match] = 1\n\t\t\tself.C[match][i] = 1\n\t\t\t\t\t\t\t\t\n\t\tself.sumC_active[i] = self.sumC[i]\n\t\t\n\t\treturn self.sumC[i] > 0\t\t\n\n\tdef maxflow(self):\n\t\treturn self.full_flow - self.ignore_flow\n\t\n\tdef consideredDets(self):\t\t\t\t\n\t\treturn self.nextN - self.ignore_flow\n\t\n\tdef ignoredFlow(self):\n\t\treturn self.ignore_flow\n\t\t\n\tdef getTruePositives(self):\n\t\tret = copy.copy(self.anno)\n\t\tret.rects = []\n\t\t#iterate over GT\n\t\tfor i in xrange(self.n + 1, self.a - 1):\n\t\t\t#Flow to sink > 0\n\t\t\tif(self.F[i][self.a - 1] > 0 and self.ignore[i - self.n - 1] == 0):\n\t\t\t\t#Find associated det\n\t\t\t\tfor j in xrange(1, self.n + 1):\n\t\t\t\t\tif(self.F[j][i] > 0):\n\t\t\t\t\t\tret.rects.append(self.det[j - 1])\n\t\t\t\t\t\tbreak\t\n\t\treturn ret\n\t\n\tdef getIgnoredTruePositives(self):\n\t\tret = copy.copy(self.anno)\n\t\tret.rects = []\n\t\t#iterate over GT\n\t\tfor i in xrange(self.n + 1, self.a - 1):\n\t\t\t#Flow to sink > 0\n\t\t\tif(self.F[i][self.a - 1] > 0 and self.ignore[i - self.n - 1] == 1):\n\t\t\t\t#Find associated det\n\t\t\t\tfor j in xrange(1, self.n + 1):\n\t\t\t\t\tif(self.F[j][i] > 0):\n\t\t\t\t\t\tret.rects.append(self.det[j - 1])\n\t\t\t\t\t\tbreak\t\n\t\treturn ret\n\t\t\n\tdef getMissingRecall(self):\n\t\tret = copy.copy(self.anno)\n\t\tret.rects = []\n\t\tfor i in xrange(self.n + 1, self.a - 1):\n\t\t\tif(self.F[i][self.a - 1] == 0 and self.ignore[i - self.n - 1] == 0):\n\t\t\t\tret.rects.append(self.anno.rects[i - self.n - 1])\n\t\treturn ret\n\t\t\n\tdef getFalsePositives(self):\t\t\t\t\n\t\tret = copy.copy(self.det)\n\t\tret.rects = []\n\t\tfor i in xrange(1, self.n + 1):\n\t\t\tif(self.F[0][i] == 0):\t\t\t\t\t\n\t\t\t\tret.rects.append(self.det[i - 1])\t\t\t\n\t\treturn ret\n\t\ndef asort(idlGT, idlDet, minWidth, minHeight, style, minCover, minOverlap, maxDistance, maxWidth=float(\'inf\'), maxHeight=float(\'inf\')):\n\t#Asort too small object\tin ground truth\n\t\t\n\tfor x,anno in enumerate(idlGT):\n\t\t\t\t\n\t\timageFound = False\n\t\tfilterIndex = -1\n\t\tfor i,filterAnno in enumerate(idlDet):\n\t\t\t\tif (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\n\t\t\t\t\tfilterIndex = i\n\t\t\t\t\timageFound = True\n\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\n\t\tif(not imageFound):\n\t\t\tcontinue\n\t\t\t\t\n\t\tvalidGTRects = []\t\t\n\t\tfor j in anno.rects:\n\t\t\tif (j.width() >= minWidth) and (j.height() >= minHeight) and (j.width() <= maxWidth) and (j.height() <= maxHeight):\n\t\t\t\tvalidGTRects.append(j)\n\t\t\telse:\n\t\t\t\t# Sort out detections that would have matched\t\t\t\t\n\t\t\t\tmatchingIndexes = []\n\t\t\t\t\t\t\t\t\t\n\t\t\t\tfor m,frect in enumerate(idlDet[filterIndex].rects):\n\t\t\t\t\tif(style == 0):\n\t\t\t\t\t\tif (j.isMatchingStd(frect, minCover,minOverlap, maxDistance)):\n\t\t\t\t\t\t\toverlap = j.overlap_pascal(frect)\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tmatchingIndexes.append((m,overlap))\n\t\t\t\t\t\t\t\n\t\t\t\t\tif(style == 1):\n\t\t\t\t\t\tif(j.isMatchingPascal(frect, minOverlap)):\n\t\t\t\t\t\t\toverlap = j.overlap_pascal(frect)\n\t\t\t\t\t\t\tmatchingIndexes.append((m, overlap))\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tfor m in xrange(len(matchingIndexes) - 1, -1, -1):\n\t\t\t\t\tmatching_rect = idlDet[filterIndex].rects[matchingIndexes[m][0]]\n\t\t\t\t\tmatching_overlap = matchingIndexes[m][1]\n\t\t\t\t\tbetter_overlap_found = False\n\t\t\t\t\tfor l in anno.rects:\n\t\t\t\t\t\tif l.overlap_pascal(matching_rect) > matching_overlap:\n\t\t\t\t\t\t\tbetter_overlap_found = True\n\t\t\t\t\t\t\t\n\t\t\t\t\tif better_overlap_found:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\tdel idlDet[filterIndex].rects[matchingIndexes[m][0]]\n\t\t\t\t\t\t\t\n\t\tidlGT[x].rects = validGTRects\n\t\n\t#Sort out too small false positives\t\n\tfor x,anno in enumerate(idlDet):\n\t\t\t\t\t\t\n\t\timageFound = False\n\t\tfilterIndex = -1\n\t\tfor i,filterAnno in enumerate(idlGT):\n\t\t\t\tif (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\n\t\t\t\t\tfilterIndex = i\n\t\t\t\t\timageFound = True\n\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\n\t\tif(not imageFound):\n\t\t\tcontinue\n\t\t\n\t\tvalidDetRects = []\t\t\n\t\tfor j in anno.rects:\n\t\t\tif (j.width() >= minWidth) and (j.height() >= minHeight) and (j.width() <= maxWidth) and (j.height() <= maxHeight):\n\t\t\t\tvalidDetRects.append(j)\n\t\t\telse:\t\t\t\t\n\t\t\t\tfor frect in idlGT[filterIndex].rects:\n\t\t\t\t\t\n\t\t\t\t\tif(style == 0):\n\t\t\t\t\t\tif j.isMatchingStd(frect, minCover,minOverlap, maxDistance):\n\t\t\t\t\t\t\tvalidDetRects.append(j)\n\t\t\t\t\t\t\n\t\t\t\t\tif(style == 1):\t\t\n\t\t\t\t\t\tif(j.isMatchingPascal(frect, minOverlap)):\n\t\t\t\t\t\t\tvalidDetRects.append(j)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tidlDet[x].rects = validDetRects\n\n\n# MA: simplified version that does Pascal style matching with one parameter controlling ""intersection-over-union"" matching threshold \ndef comp_prec_recall(annoIDL, detIDL, minOverlap):\n        ignoreIDL = copy.deepcopy(annoIDL)\n        for anno in ignoreIDL:\n                anno.rects = []\n\n        precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minOverlap=minOverlap);\n        return precs, recalls, scores, fppi\n\ndef comp_prec_recall_graphs(annoIDL, detIDL, minOverlap):\n        ignoreIDL = copy.deepcopy(annoIDL)\n        for anno in ignoreIDL:\n                anno.rects = []\n\n        precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minOverlap=minOverlap);\n        return graphs\n\n\n# MA: full version\ndef comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minWidth=0, minHeight=0, maxWidth=float(\'inf\'), maxHeight=float(\'inf\'), \n                                matchingStyle=1, minCover=0.5, minOverlap=0.5, maxDistance=0.5, ignoreOverlap=0.9, verbose=False):\n\n\t# Asort detections which are too small/too big\n        if verbose: \n                print ""Asorting too large/ too small detections""\n                print ""minWidth:"", minWidth\n                print ""minHeight:"", minHeight\n                print ""maxWidth: "", maxWidth\n                print ""maxHeight: "", maxHeight\n\n\tasort(annoIDL, detIDL, minWidth, minHeight, matchingStyle, minCover, minOverlap, maxDistance, maxWidth, maxHeight)\n\t\n\t#Debugging asort\n\t#saveIDL(""testGT.idl"", annoIDL)\n\t#saveIDL(""testDET.idl"", detIDL)\n\t\n\n\n\tnoAnnotations = 0\n\tfor anno in annoIDL:\n\t\tfor j,detAnno in enumerate(detIDL):\n\t\t\t\tif (suffixMatch(anno.imageName, detIDL[j].imageName) and anno.frameNr == detIDL[j].frameNr):\n\t\t\t\t\tnoAnnotations = noAnnotations + len(anno.rects)\n\t\t\t\t\tbreak\n\n        if verbose:\n                print ""#Annotations:"", noAnnotations\n\t\n                ###--- set up graphs ---###\n                print ""Setting up graphs ...""\n\n\tgraphs = []\n\tallRects = []\n\tmissingFrames = 0\n\tfor i in xrange(len(annoIDL)):\n\t\t\t\t\n\t\timageFound = False\n\t\tfilterIndex = -1\n\n                for j, detAnno in enumerate(detIDL):\t\t\t\n                        if (suffixMatch(annoIDL[i].imageName, detIDL[j].imageName) and annoIDL[i].frameNr == detIDL[j].frameNr):\n                                filterIndex = j\n                                imageFound = True\n                                break\n\t\t\t\t\t\t\t\n\t\tif(not imageFound):\n\t\t\tprint ""No annotation/detection pair found for: "" + annoIDL[i].imageName + "" frame: "" + str(annoIDL[i].frameNr)\n\t\t\tmissingFrames += 1\n\t\t\tcontinue;\n\t\t\t\t\n\t\tgraphs.append(AnnoGraph(annoIDL[i], detIDL[filterIndex], ignoreIDL[i], matchingStyle, minCover, minOverlap, maxDistance, ignoreOverlap))\n\t\t\t\t\t\n\t\tfor j,rect in enumerate(detIDL[filterIndex]):\n\t\t\tnewRect = detAnnoRect()\n\t\t\tnewRect.imageName = anno.imageName\n\t\t\tnewRect.frameNr = anno.frameNr\n\t\t\tnewRect.rect = rect\n\t\t\tnewRect.imageIndex = i - missingFrames\n\t\t\tnewRect.boxIndex = j\n\t\t\tallRects.append(newRect)\t\n\t\n        if verbose: \n                print ""missingFrames: "", missingFrames\n                print ""Number of detections on annotated frames: "" , len(allRects)\n\t\n                ###--- get scores from all rects ---###\n                print ""Sorting scores ...""\n\n\tallRects.sort(cmpDetAnnoRectsByScore)\n\tallRects.reverse()\n\n\t###--- gradually decrease score ---###\n        if verbose: \n                print ""Gradually decrease score ...""\n\n\tlastScore = float(\'infinity\')\n\t\n\tprecs = [1.0] \n\trecalls = [0.0] \n\t#fppi = [ 10**(math.floor(math.log(1.0 / float(len(annoIDL)))/math.log(10) * 10.0) / 10.0) ]\n\tfppi = [ 1.0 / float(len(annoIDL)) ]\n\tscores = [lastScore]  \n\t\n\tnumDet = len(allRects)\n\tsf = lastsf = 0\n\tcd = lastcd = 0\t\n\tiflow = lastiflow = 0\n\t\n\tchanged = False\t\n\tfirstFP = True\n\tfor i,nextrect in enumerate(allRects):\n\t\tscore = nextrect.rect.score;\n\t\t\n\t\t# updating true and false positive counts\t\t\n\t\tsf = sf - graphs[nextrect.imageIndex].maxflow()\n\t\tcd = cd - graphs[nextrect.imageIndex].consideredDets()\n\t\tiflow = iflow -\tgraphs[nextrect.imageIndex].ignoredFlow()\t\t\n\t\t\n\t\t#changed = changed or graphs[nextrect.imageIndex].decreaseScore(score)\n\t\tchanged = graphs[nextrect.imageIndex].addBB(nextrect) or changed\t\t\t\t\t\n\t\tsf = sf + graphs[nextrect.imageIndex].maxflow()\n\t\tcd = cd + graphs[nextrect.imageIndex].consideredDets()\n\t\tiflow = iflow + graphs[nextrect.imageIndex].ignoredFlow()\n\t\t\n\t\tif(firstFP and cd - sf != 0):\n\t\t\tfirstFP = False\n\t\t\tchanged = True\n\t\t\t\t\t\t\n\t\tif (i == numDet - 1 or score != allRects[i + 1].rect.score or firstFP or i == len(allRects)):\t\n\t\t\tif(changed or i == numDet - 1 or i == len(allRects)):\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tif(lastcd > 0):\n\t\t\t\t\tscores.append(lastScore)\n\t\t\t\t\trecalls.append(float(lastsf) / float(noAnnotations - lastiflow))\n\t\t\t\t\tprecs.append(float(lastsf) / float(lastcd))\n\t\t\t\t\tfppi.append(float(lastcd - lastsf) / float(len(annoIDL)))\n\t\t\t\t\t\t\t\t\n\t\t\t\tif (cd > 0):\n\t\t\t\t\tscores.append(score)\n\t\t\t\t\trecalls.append(float(sf) / float(noAnnotations - iflow))\t\t\t\t\t\t\t\t\n\t\t\t\t\tprecs.append(float(sf) / float(cd))\t\t\t\t\t\n\t\t\t\t\tfppi.append(float(cd - sf) / float(len(annoIDL)))\n\t\t\t\t\t\n\t\t\t\t\n\t\t\tchanged = False\n\t\t\n\t\tlastScore = score\n\t\tlastsf = sf\n\t\tlastcd = cd\n\t\tlastiflow = iflow\n        \n        return precs, recalls, scores, fppi, graphs\n\ndef main():\n\t\n\tparser = OptionParser(usage=""usage: %prog [options] <groundTruthIdl> <detectionIdl>"")\n\t\t             \n\tparser.add_option(""-o"", ""--outFile"",\n                  action=""store"", type=""string"", dest=""outFile"")\n\tparser.add_option(""-a"", ""--analysisFiles"",\n                  action=""store"", type=""string"", dest=""analysisFile"")\n\t\n\tparser.add_option(""-s"", ""--minScore"",\n                  action=""store"", type=""float"", dest=""minScore"")\n                  \n\tparser.add_option(""-w"", ""--minWidth"",\n\t\t\t\taction=""store"", type=""int"", dest=""minWidth"", default=0)\n\t\t\t\t  \n\tparser.add_option(""-u"", ""--minHeight"",\n\t\t\t\taction=""store"", type=""int"", dest=""minHeight"",default=0)\n\tparser.add_option(""--maxWidth"", action=""store"", type=""float"", dest=""maxWidth"", default=float(\'inf\'))\n\tparser.add_option(""--maxHeight"", action=""store"", type=""float"", dest=""maxHeight"", default=float(\'inf\'))\n\n\tparser.add_option(""-r"", ""--fixAspectRatio"",\n\t\t\taction=""store"", type=""float"", dest=""aspectRatio"")\n\n\tparser.add_option(""-p"", ""--Pascal-Style"", action=""store_true"", dest=""pascalStyle"")\n\tparser.add_option(""-l"", ""--Leibe-Seemann-Matching-Style"", action=""store_true"", dest=""leibeStyle"")\n\t\n\tparser.add_option(""--minCover"", action=""store"", type=""float"", dest=""minCover"", default=0.5)\n\tparser.add_option(""--maxDistance"", action=""store"", type=""float"", dest=""maxDistance"", default=0.5)\n\tparser.add_option(""--minOverlap"", action=""store"", type=""float"", dest=""minOverlap"", default=0.5)\n\t\n\t\n\tparser.add_option(""--clipToImageWidth"", action=""store"", type=""float"", dest=""clipWidth"", default= None)\n\tparser.add_option(""--clipToImageHeight"", action=""store"", type=""float"", dest=""clipHeight"", default= None)\n\t\t\n\tparser.add_option(""-d"", ""--dropFirst"", action=""store_true"", dest=""dropFirst"")\n\t\n\t#parser.add_option(""-c"", ""--class"", action=""store"", type=""int"", dest=""classID"", default=-1)\n\tparser.add_option(""-c"", ""--class"", action=""store"", type=""int"", dest=""classID"", default = None)\n\t\n\tparser.add_option(""-i"", ""--ignore"", action=""store"", type=""string"", dest=""ignoreFile"")\n\tparser.add_option(""--ignoreOverlap"", action=""store"", type=""float"", dest=""ignoreOverlap"", default = 0.9)\n\t\n\t(options, args) = parser.parse_args()\n\t\n\tif (len(args) < 2):\n\t\tprint ""Please specify annotation and detection as arguments!""\n\t\tparser.print_help()\n\t\tsys.exit(1)\n\t\n\tannoFile = args[0]\n\t\t\n\t# First figure out the minimum height and width we are dealing with\n\tminWidth =  options.minWidth\n\tminHeight = options.minHeight\n\tmaxWidth =  options.maxWidth\n\tmaxHeight = options.maxHeight\n\t\t\n\tprint ""Minimum width: %d height: %d"" % (minWidth, minHeight)\n\t\n\t# Load files\t\n\tannoIDL = parse(annoFile)\t\n\tdetIDL = []\n\tfor dets in args[1:]:\n\t\tdetIDL += parse(dets)\n\n\t\n\tif options.ignoreFile != None:\n\t\tignoreIDL = parse(options.ignoreFile)\n\telse:\n\t\tignoreIDL = copy.deepcopy(annoIDL)\n\t\tfor anno in ignoreIDL:\n\t\t\tanno.rects = []\n\t\t\t\n\tif(options.classID is not None):\n\t\tfor anno in annoIDL:\n\t\t\tanno.rects = [rect for rect in anno.rects if (rect.classID == options.classID  or rect.classID == -1)]\n\t\tfor anno in detIDL:\n\t\t\tanno.rects = [rect for rect in anno.rects if (rect.classID == options.classID or rect.classID == -1)]\n\t\tfor anno in ignoreIDL:\n\t\t\tanno.rects = [rect for rect in anno.rects if (rect.classID == options.classID or rect.classID == -1)]\n\n\t# prevent division by zero when fixing aspect ratio\n\tfor anno in annoIDL:\n\t\tanno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n\tfor anno in detIDL:\n\t\tanno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n\tfor anno in ignoreIDL:\n\t\tanno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n\n\t\n\t# Fix aspect ratio \n\tif (not options.aspectRatio == None):\n\t\tforceAspectRatio(annoIDL, options.aspectRatio)\n\t\tforceAspectRatio(detIDL, options.aspectRatio)\n\t\tforceAspectRatio(ignoreIDL, options.aspectRatio)\n\t\t\t\n\t# Deselect detections with too low score\n\tif (not options.minScore == None):\n\t\tfor i,anno in enumerate(detIDL):\n\t\t\tvalidRects = []\n\t\t\tfor rect in anno.rects:\t\t\t\t\n\t\t\t\tif (rect.score >= options.minScore):\n\t\t\t\t\tvalidRects.append(rect)\n\t\t\tanno.rects = validRects\n\t\n\t# Clip detections to the image dimensions\n\tif(options.clipWidth != None or options.clipHeight != None):\n\t\tmin_x = -float(\'inf\')\n\t\tmin_y = -float(\'inf\')\n\t\tmax_x = float(\'inf\')\n\t\tmax_y = float(\'inf\')\n\t\t\n\t\tif(options.clipWidth != None):\n\t\t\tmin_x = 0\n\t\t\tmax_x = options.clipWidth\n\t\tif(options.clipHeight != None):\n\t\t\tmin_y = 0\n\t\t\tmax_y = options.clipHeight\n\t\t\t\n\t\tprint ""Clipping width: (%.02f-%.02f); clipping height: (%.02f-%.02f)"" % (min_x, max_x, min_y, max_y)\n\t\tfor anno in annoIDL:\n\t\t\tfor rect in anno:\n\t\t\t\trect.clipToImage(min_x, max_x, min_y, max_y)\n\t\tfor anno in detIDL:\n\t\t\tfor rect in anno:\n\t\t\t\trect.clipToImage(min_x, max_x, min_y, max_y)\t\t\n\t\n\t# Setup matching style; standard is Pascal\n\t# style\n\tmatchingStyle = 1\n\t\n\t# Pascal style\n\tif (options.pascalStyle == True):\n\t\tmatchingStyle = 1\n\t\t\n\tif (options.leibeStyle == True):\n\t\tmatchingStyle = 0\n\t\t\n\tif (options.pascalStyle and options.leibeStyle):\n\t\tprint ""Conflicting matching styles!""\n\t\tsys.exit(1)\n\t\t\n\tif (options.dropFirst == True):\n\t\tprint ""Drop first frame of each sequence...""\n\t\tnewIDL = []\n\t\tfor i, anno in enumerate(detIDL):\n\t\t\tif (i > 1 and detIDL[i].frameNr == detIDL[i-1].frameNr + 1 and detIDL[i].frameNr == detIDL[i-2].frameNr + 2 and  detIDL[i].frameNr == detIDL[i-3].frameNr + 3  and detIDL[i].frameNr == detIDL[i-4].frameNr + 4):\n\t\t\t\tnewIDL.append(anno)\n\t\tdetIDL = newIDL\n        \n        verbose = True;\n        precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL,\n                                                                   minWidth=options.minWidth, minHeight=options.minHeight, \n                                                                   maxWidth=options.maxWidth, maxHeight=options.maxHeight,\n                                                                   matchingStyle=matchingStyle, \n                                                                   minCover=options.minCover, minOverlap=options.minOverlap, \n                                                                   maxDistance=options.maxDistance, ignoreOverlap=options.ignoreOverlap, \n                                                                   verbose=verbose);\n\n\t###--- output to file ---###\n\toutfilename = options.outFile\n\tif outfilename is None:\n\t\toutputDir = os.path.dirname(os.path.abspath(args[1]))\n\t\toutputFile = os.path.basename(os.path.abspath(args[1]))\n\t\t[base, ext] = idlBase(outputFile)\n\t\t#outfilename = outputDir + ""/rpc-"" + base + "".txt""\n\n                outfilename = outputDir + ""/rpc-"" + base + ""_overlap"" + str(options.minOverlap) + "".txt""\n\n\tprint ""saving:\\n"" + outfilename;\n\n\tfile = open(outfilename, \'w\')\n\tfor i in xrange(len(precs)):\n\t\tfile.write(str(precs[i])+"" ""+str(recalls[i])+"" ""+str(scores[i])+ "" "" + str(fppi[i])+ ""\\n"")\n\tfile.close()\n\t\n\t# Extracting failure cases\n\tif(options.analysisFile != None):\n\t\t\t\t\n\t\tanaPrefix = options.analysisFile\n\t\t\t\n\t\tfalsePositives = AnnoList([])\n\t\ttruePositives = AnnoList([])\n\t\tmissingRecall = AnnoList([])\n\t\tignoredTruePositives = AnnoList([])\n\n\t\tfor i in xrange(len(graphs)):\t\t\t\t\n\t\t\tfalsePositives.append(graphs[i].getFalsePositives())\n\t\t\ttruePositives.append(graphs[i].getTruePositives())\n\t\t\ttruePositives[-1].imageName = falsePositives[-1].imageName\n\t\t\ttruePositives[-1].imagePath = falsePositives[-1].imagePath\n\t\t\tmissingRecall.append(graphs[i].getMissingRecall())\n\t\t\tmissingRecall[-1].imageName = falsePositives[-1].imageName\n\t\t\tmissingRecall[-1].imagePath = falsePositives[-1].imagePath\n\t\t\tif options.ignoreFile != None:\n\t\t\t\tignoredTruePositives.append(graphs[i].getIgnoredTruePositives())\n\t\t\n\t\t#saveIDL(anaPrefix + ""-falsePositives.idl.gz"", falsePositives);\n                falsePositives.save(anaPrefix + ""-falsePositives.pal"")\n\t\n\t\tsortedFP = annoAnalyze(falsePositives);\n\t\t#saveIDL(anaPrefix + ""-falsePositives-sortedByScore.idl.gz"", sortedFP);\n\t\t#saveIDL(anaPrefix + ""-truePositives.idl.gz"", truePositives);\n\n\t\t# saveIDL(anaPrefix + ""-falsePositives-sortedByScore.idl"", sortedFP);\n\t\t# saveIDL(anaPrefix + ""-truePositives.idl"", truePositives);\n\n                sortedFP.save(anaPrefix + ""-falsePositives-sortedByScore.pal"")\n                truePositives.save(anaPrefix + ""-truePositives.pal"")\n\n\t\tsortedFP = annoAnalyze(truePositives);\n\t\t#saveIDL(anaPrefix + ""-truePositives-sortedByScore.idl.gz"", sortedFP);\n                #saveIDL(anaPrefix + ""-truePositives-sortedByScore.idl"", sortedFP);\n                sortedFP.save(anaPrefix + ""-truePositives-sortedByScore.pal"")\n\n\t\tif options.ignoreFile != None:\n\t\t\t#saveIDL(anaPrefix + ""-ignoredTruePositives.idl.gz"", ignoredTruePositives)\n                        #saveIDL(anaPrefix + ""-ignoredTruePositives.idl"", ignoredTruePositives)\n                        ignoredTruePositives.save(anaPrefix + ""-ignoredTruePositives.pal"")\n\t\t\n\t\t#saveIDL(anaPrefix + ""-missingRecall.idl.gz"", missingRecall);\n                #saveIDL(anaPrefix + ""-missingRecall.idl"", missingRecall);\n                missingRecall.save(anaPrefix + ""-missingRecall.pal"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n\n\n'"
submodules/utils/annolist/ma_utils.py,0,b'def is_number(s):\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n'
submodules/utils/annolist/plotSimple.py,0,"b'#!/usr/bin/env python\n\nimport sys\nimport os\nimport random\nimport re\nfrom AnnotationLib import *\nfrom MatPlotter import *\nfrom optparse import OptionParser\nfrom copy import deepcopy\nfrom math import sqrt\n\n\ndef main(argv):\n\tparser = OptionParser(usage=""usage: %prog [options] <datafile> [...]"")\n\tparser.add_option(""-o"", ""--output-file"",        action=""store"",\n\t\t\tdest=""output"", type=""str"", help=""outfile. mandatory"")\n\tparser.add_option(""--fppw"", action=""store_true"", dest=""fppw"", help=""False Positives Per Window"")\n\tparser.add_option(""--colors"", action=""store"", dest=""colors"", help=""colors"")\n\tparser.add_option(""--fppi"", action=""store_true"", dest=""fppi"", help=""False Positives Per Image"")\n\tparser.add_option(""--lfppi"", action=""store_true"", dest=""lfppi"", help=""False Positives Per Image(log)"")\n\tparser.add_option(""-c"", ""--components"", action=""store"", dest=""ncomponents"", type=""int"", help=""show n trailing components of the part"", default=3)\n\tparser.add_option(""--cut-trailing"", action=""store"", dest=""cutcomponents"", type=""int"", help=""cut n trailing components of the part (applied after --components)"", default=-1)\n\tparser.add_option(""-t"", ""--title"", action=""store"", dest=""title"", type=""str"", default="""")\n\tparser.add_option(""-f"", ""--fontsize"", action=""store"", dest=""fontsize"", type=""int"", default=12)\n\tparser.add_option(""-l"", ""--legend\'"", action=""store"", dest=""legend"", type=""string"", default=""lr"")\n\t(options, args) = parser.parse_args()\n\tplotter = MatPlotter(options.fontsize)\n\t\n\tposition = ""lower right""\n\tif(options.legend == ""ur""):\n\t\tposition = ""upper right""\n\tif(options.legend == ""ul""):\n\t\tposition = ""upper left""\n\tif(options.legend == ""ll""):\n\t\tposition = ""lower left""\t\n\tplotter.formatLegend(options.fontsize, newPlace = position)\n\t\n\ttitle = options.title\n\tcolors = None\n\tif (options.colors):\n\t\tcolors = options.colors.split()\n\tif (options.fppw):\n\t\tplotter.newFPPWFigure(title)\n\telif (options.lfppi):\n\t\tplotter.newLogFPPIFigure(title)\n\telif (options.fppi):\n\t\tplotter.newFPPIFigure(title)\n\telse:\n\t\tplotter.newFigure(title)\t\t\n\t\t\n\tfor i, filename in enumerate(args):\n\t\tif (os.path.isdir(filename)):\n\t\t\tfilename = os.path.join(filename, ""rpc"", ""result-minh-48"")\n\t\tdisplayname = filename\n\t\tif (options.ncomponents > 0):\n\t\t\tsuffix = None\n\t\t\tfor idx in xrange(options.ncomponents):\n\t\t\t\tdisplayname, last = os.path.split(displayname)\n\t\t\t\tif (suffix):\n\t\t\t\t\tsuffix = os.path.join(last, suffix)\n\t\t\t\telse:\n\t\t\t\t\tsuffix = last\n\t\t\tdisplayname = suffix\n\t\tif (options.cutcomponents > 0):\n\t\t\tfor idx in xrange(options.cutcomponents):\n\t\t\t\tdisplayname, last = os.path.split(displayname)\n#\t\tplusidx = displayname.index(""+"")\n#\t\tdisplayname = displayname[plusidx:]\n\t\tprint ""Plotting: ""+displayname\n\t\tif (options.fppw):\n\t\t\tplotter.plotFPPW(filename, displayname)\n\t\telif (options.lfppi):\n\t\t\tif colors:\n\t\t\t\tplotter.plotLogFPPI(filename, displayname, colors[i])\n\t\t\telse:\n\t\t\t\tplotter.plotLogFPPI(filename, displayname)\n\t\telif (options.fppi):\n\t\t\tplotter.plotFPPI(filename, displayname)\n\t\telse:\t\t\n\t\t\tplotter.plotRPC(filename, displayname)\n\t\n\tplotLine = not (options.fppw or options.lfppi or options.fppi);\t\t\n\t\n\tif (options.output is None):\n\t\tplotter.show(plotLine)\n\telse:\n\t\tplotter.saveCurrentFigure(plotLine, options.output)\n\treturn 0\n\nif __name__ == ""__main__"":\n\tsys.exit(main(sys.argv))\n'"
submodules/utils/kaffe/__init__.py,0,b''
submodules/utils/kaffe/mynet.py,0,"b""from network import Network\n\nclass VGG(Network):\n    def setup(self):\n        (self.feed('data')\n             .conv(3, 3, 64, 1, 1, name='conv1_1')\n             .conv(3, 3, 64, 1, 1, name='conv1_2')\n             .max_pool(2, 2, 2, 2, name='pool1')\n             .conv(3, 3, 128, 1, 1, name='conv2_1')\n             .conv(3, 3, 128, 1, 1, name='conv2_2')\n             .max_pool(2, 2, 2, 2, name='pool2')\n             .conv(3, 3, 256, 1, 1, name='conv3_1')\n             .conv(3, 3, 256, 1, 1, name='conv3_2')\n             .conv(3, 3, 256, 1, 1, name='conv3_3')\n             .max_pool(2, 2, 2, 2, name='pool3')\n             .conv(3, 3, 512, 1, 1, name='conv4_1')\n             .conv(3, 3, 512, 1, 1, name='conv4_2')\n             .conv(3, 3, 512, 1, 1, name='conv4_3')\n             .max_pool(2, 2, 2, 2, name='pool4')\n             .conv(3, 3, 512, 1, 1, name='conv5_1')\n             .conv(3, 3, 512, 1, 1, name='conv5_2')\n             .conv(3, 3, 512, 1, 1, name='conv5_3')\n             .max_pool(2, 2, 2, 2, name='pool5')\n             )\n             #.fc(4096, name='fc6')\n             #.fc(4096, name='fc7')\n             #.fc(1000, relu=False, name='fc8')\n             #.softmax(name='prob'))\n"""
submodules/utils/kaffe/network.py,21,"b""import numpy as np\nimport tensorflow as tf\n\nDEFAULT_PADDING = 'SAME'\n\ndef layer(op):\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.inputs)==0:\n            raise RuntimeError('No input variables found for layer %s.'%name)\n        elif len(self.inputs)==1:\n            layer_input = self.inputs[0]\n        else:\n            layer_input = list(self.inputs)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n    return layer_decorated\n\nclass Network(object):\n    def __init__(self, inputs, trainable=True):\n        self.inputs = []\n        self.layers = dict(inputs)\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        raise NotImplementedError('Must be subclassed.')\n\n    def load(self, data_path, session, ignore_missing=False):\n        data_dict = np.load(data_path).item()\n        for key in data_dict:\n            with tf.variable_scope(key, reuse=True):\n                for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n                    try:\n                        var = tf.get_variable(subkey)\n                        session.run(var.assign(data))\n                    except ValueError:\n                        if not ignore_missing:\n                            raise\n\n    def feed(self, *args):\n        assert len(args)!=0\n        self.inputs = []\n        for layer in args:\n            if isinstance(layer, basestring):\n                try:\n                    layer = self.layers[layer]\n                except KeyError:\n                    print self.layers.keys()\n                    raise KeyError('Unknown layer name fed: %s'%layer)\n            self.inputs.append(layer)\n        return self\n\n    def get_output(self):\n        return self.inputs[-1]\n\n    def get_unique_name(self, prefix):\n        id = sum(t.startswith(prefix) for t,_ in self.layers.items())+1\n        return '%s_%d'%(prefix, id)\n\n    def make_var(self, name, shape):\n        return tf.get_variable(name, shape, trainable=self.trainable)\n\n    def validate_padding(self, padding):\n        assert padding in ('SAME', 'VALID')\n\n    @layer\n    def conv(self, input, k_h, k_w, c_o, s_h, s_w, name, relu=True, padding=DEFAULT_PADDING, group=1):\n        self.validate_padding(padding)\n        c_i = input.get_shape()[-1]\n        assert c_i%group==0\n        assert c_o%group==0\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n            kernel = self.make_var('weights', shape=[k_h, k_w, c_i/group, c_o])\n            biases = self.make_var('biases', [c_o])\n            if group==1:\n                conv = convolve(input, kernel)\n            else:\n                input_groups = tf.split(3, group, input)\n                kernel_groups = tf.split(3, group, kernel)\n                output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n                conv = tf.concat(3, output_groups)\n            if relu:\n                bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list())\n                return tf.nn.relu(bias, name=scope.name)\n            return tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list(), name=scope.name)\n\n    @layer\n    def relu(self, input, name):\n        return tf.nn.relu(input, name=name)\n\n    @layer\n    def max_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def avg_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.avg_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def lrn(self, input, radius, alpha, beta, name, bias=1.0):\n        return tf.nn.local_response_normalization(input,\n                                                  depth_radius=radius,\n                                                  alpha=alpha,\n                                                  beta=beta,\n                                                  bias=bias,\n                                                  name=name)\n\n    @layer\n    def concat(self, inputs, axis, name):\n        return tf.concat(concat_dim=axis, values=inputs, name=name)\n\n    @layer\n    def fc(self, input, num_out, name, relu=True):\n        with tf.variable_scope(name) as scope:\n            input_shape = input.get_shape()\n            if input_shape.ndims==4:\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= d\n                feed_in = tf.reshape(input, [int(input_shape[0]), dim])\n            else:\n                feed_in, dim = (input, int(input_shape[-1]))\n            weights = self.make_var('weights', shape=[dim, num_out])\n            biases = self.make_var('biases', [num_out])\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=scope.name)\n            return fc\n\n    @layer\n    def softmax(self, input, name):\n        return tf.nn.softmax(input, name)\n\n    @layer\n    def dropout(self, input, keep_prob, name):\n        return tf.nn.dropout(input, keep_prob, name=name)\n"""
