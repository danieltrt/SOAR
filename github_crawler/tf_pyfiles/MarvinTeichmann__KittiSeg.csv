file_path,api_count,code
demo.py,7,"b'""""""\nDetects Cars in an image using KittiSeg.\n\nInput: Image\nOutput: Image (with Cars plotted in Green)\n\nUtilizes: Trained KittiSeg weights. If no logdir is given,\npretrained weights will be downloaded and used.\n\nUsage:\npython demo.py --input_image data/demo.png [--output_image output_image]\n                [--logdir /path/to/weights] [--gpus 0]\n\n--------------------------------------------------------------------------------\n\nThe MIT License (MIT)\n\nCopyright (c) 2017 Marvin Teichmann\n\nDetails: https://github.com/MarvinTeichmann/KittiSeg/blob/master/LICENSE\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nimport os\nimport sys\n\nimport collections\n\n# configure logging\n\nlogging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                    level=logging.INFO,\n                    stream=sys.stdout)\n\n# https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-220820070\nimport numpy as np\nimport scipy as scp\nimport scipy.misc\nimport tensorflow as tf\n\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nsys.path.insert(1, \'incl\')\n\nfrom seg_utils import seg_utils as seg\n\ntry:\n    # Check whether setup was done correctly\n\n    import tensorvision.utils as tv_utils\n    import tensorvision.core as core\nexcept ImportError:\n    # You forgot to initialize submodules\n    logging.error(""Could not import the submodules."")\n    logging.error(""Please execute:""\n                  ""\'git submodule update --init --recursive\'"")\n    exit(1)\n\n\nflags.DEFINE_string(\'logdir\', None,\n                    \'Path to logdir.\')\nflags.DEFINE_string(\'input_image\', None,\n                    \'Image to apply KittiSeg.\')\nflags.DEFINE_string(\'output_image\', None,\n                    \'Image to apply KittiSeg.\')\n\n\ndefault_run = \'KittiSeg_pretrained\'\nweights_url = (""ftp://mi.eng.cam.ac.uk/""\n               ""pub/mttt2/models/KittiSeg_pretrained.zip"")\n\n\ndef maybe_download_and_extract(runs_dir):\n    logdir = os.path.join(runs_dir, default_run)\n\n    if os.path.exists(logdir):\n        # weights are downloaded. Nothing to do\n        return\n      \n    if not os.path.exists(runs_dir):\n        os.makedirs(runs_dir)\n    download_name = tv_utils.download(weights_url, runs_dir)\n    logging.info(""Extracting KittiSeg_pretrained.zip"")\n\n    import zipfile\n    zipfile.ZipFile(download_name, \'r\').extractall(runs_dir)\n\n    return\n\n\ndef resize_label_image(image, gt_image, image_height, image_width):\n    image = scp.misc.imresize(image, size=(image_height, image_width),\n                              interp=\'cubic\')\n    shape = gt_image.shape\n    gt_image = scp.misc.imresize(gt_image, size=(image_height, image_width),\n                                 interp=\'nearest\')\n\n    return image, gt_image\n\n\ndef main(_):\n    tv_utils.set_gpus_to_use()\n\n    if FLAGS.input_image is None:\n        logging.error(""No input_image was given."")\n        logging.info(\n            ""Usage: python demo.py --input_image data/test.png ""\n            ""[--output_image output_image] [--logdir /path/to/weights] ""\n            ""[--gpus GPUs_to_use] "")\n        exit(1)\n\n    if FLAGS.logdir is None:\n        # Download and use weights from the MultiNet Paper\n        if \'TV_DIR_RUNS\' in os.environ:\n            runs_dir = os.path.join(os.environ[\'TV_DIR_RUNS\'],\n                                    \'KittiSeg\')\n        else:\n            runs_dir = \'RUNS\'\n        maybe_download_and_extract(runs_dir)\n        logdir = os.path.join(runs_dir, default_run)\n    else:\n        logging.info(""Using weights found in {}"".format(FLAGS.logdir))\n        logdir = FLAGS.logdir\n\n    # Loading hyperparameters from logdir\n    hypes = tv_utils.load_hypes_from_logdir(logdir, base_path=\'hypes\')\n\n    logging.info(""Hypes loaded successfully."")\n\n    # Loading tv modules (encoder.py, decoder.py, eval.py) from logdir\n    modules = tv_utils.load_modules_from_logdir(logdir)\n    logging.info(""Modules loaded successfully. Starting to build tf graph."")\n\n    # Create tf graph and build module.\n    with tf.Graph().as_default():\n        # Create placeholder for input\n        image_pl = tf.placeholder(tf.float32)\n        image = tf.expand_dims(image_pl, 0)\n\n        # build Tensorflow graph using the model from logdir\n        prediction = core.build_inference_graph(hypes, modules,\n                                                image=image)\n\n        logging.info(""Graph build successfully."")\n\n        # Create a session for running Ops on the Graph.\n        sess = tf.Session()\n        saver = tf.train.Saver()\n\n        # Load weights from logdir\n        core.load_weights(logdir, sess, saver)\n\n        logging.info(""Weights loaded successfully."")\n\n    input_image = FLAGS.input_image\n    logging.info(""Starting inference using {} as input"".format(input_image))\n\n    # Load and resize input image\n    image = scp.misc.imread(input_image)\n    if hypes[\'jitter\'][\'reseize_image\']:\n        # Resize input only, if specified in hypes\n        image_height = hypes[\'jitter\'][\'image_height\']\n        image_width = hypes[\'jitter\'][\'image_width\']\n        image = scp.misc.imresize(image, size=(image_height, image_width),\n                                  interp=\'cubic\')\n\n    # Run KittiSeg model on image\n    feed = {image_pl: image}\n    softmax = prediction[\'softmax\']\n    output = sess.run([softmax], feed_dict=feed)\n\n    # Reshape output from flat vector to 2D Image\n    shape = image.shape\n    output_image = output[0][:, 1].reshape(shape[0], shape[1])\n\n    # Plot confidences as red-blue overlay\n    rb_image = seg.make_overlay(image, output_image)\n\n    # Accept all pixel with conf >= 0.5 as positive prediction\n    # This creates a `hard` prediction result for class street\n    threshold = 0.5\n    street_prediction = output_image > threshold\n\n    # Plot the hard prediction as green overlay\n    green_image = tv_utils.fast_overlay(image, street_prediction)\n\n    # Save output images to disk.\n    if FLAGS.output_image is None:\n        output_base_name = input_image\n    else:\n        output_base_name = FLAGS.output_image\n\n    raw_image_name = output_base_name.split(\'.\')[0] + \'_raw.png\'\n    rb_image_name = output_base_name.split(\'.\')[0] + \'_rb.png\'\n    green_image_name = output_base_name.split(\'.\')[0] + \'_green.png\'\n\n    scp.misc.imsave(raw_image_name, output_image)\n    scp.misc.imsave(rb_image_name, rb_image)\n    scp.misc.imsave(green_image_name, green_image)\n\n    logging.info("""")\n    logging.info(""Raw output image has been saved to: {}"".format(\n        os.path.realpath(raw_image_name)))\n    logging.info(""Red-Blue overlay of confs have been saved to: {}"".format(\n        os.path.realpath(rb_image_name)))\n    logging.info(""Green plot of predictions have been saved to: {}"".format(\n        os.path.realpath(green_image_name)))\n\n    logging.info("""")\n    logging.warning(""Do NOT use this Code to evaluate multiple images."")\n\n    logging.warning(""Demo.py is **very slow** and designed ""\n                    ""to be a tutorial to show how the KittiSeg works."")\n    logging.warning("""")\n    logging.warning(""Please see this comment, if you like to apply demo.py to""\n                    ""multiple images see:"")\n    logging.warning(""https://github.com/MarvinTeichmann/KittiBox/""\n                    ""issues/15#issuecomment-301800058"")\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
download_data.py,0,"b'""""""Download data relevant to train the KittiSeg model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport sys\nimport os\nimport subprocess\n\nimport zipfile\n\n\nfrom six.moves import urllib\nfrom shutil import copy2\n\nimport argparse\n\nlogging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                    level=logging.INFO,\n                    stream=sys.stdout)\n\nsys.path.insert(1, \'incl\')\n\n# Please set kitti_data_url to the download link for the Kitti DATA.\n#\n# You can obtain by going to this website:\n# http://www.cvlibs.net/download.php?file=data_road.zip\n#\n# Replace \'http://kitti.is.tue.mpg.de/kitti/?????????.???\' by the\n# correct URL.\n\n\nvgg_url = \'ftp://mi.eng.cam.ac.uk/pub/mttt2/models/vgg16.npy\'\n\n\ndef get_pathes():\n    """"""\n    Get location of `data_dir` and `run_dir\'.\n\n    Defaut is ./DATA and ./RUNS.\n    Alternativly they can be set by the environoment variabels\n    \'TV_DIR_DATA\' and \'TV_DIR_RUNS\'.\n    """"""\n\n    if \'TV_DIR_DATA\' in os.environ:\n        data_dir = os.path.join([\'hypes\'], os.environ[\'TV_DIR_DATA\'])\n    else:\n        data_dir = ""DATA""\n\n    if \'TV_DIR_RUNS\' in os.environ:\n        run_dir = os.path.join([\'hypes\'], os.environ[\'TV_DIR_DATA\'])\n    else:\n        run_dir = ""RUNS""\n\n    return data_dir, run_dir\n\n\ndef download(url, dest_directory):\n    filename = url.split(\'/\')[-1]\n    filepath = os.path.join(dest_directory, filename)\n\n    logging.info(""Download URL: {}"".format(url))\n    logging.info(""Download DIR: {}"".format(dest_directory))\n\n    def _progress(count, block_size, total_size):\n                prog = float(count * block_size) / float(total_size) * 100.0\n                sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' %\n                                 (filename, prog))\n                sys.stdout.flush()\n\n    filepath, _ = urllib.request.urlretrieve(url, filepath,\n                                             reporthook=_progress)\n    print()\n    return filepath\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--kitti_url\', default=\'\', type=str)\n    args = parser.parse_args()\n\n    kitti_data_url = args.kitti_url\n\n    data_dir, run_dir = get_pathes()\n\n    vgg_weights = os.path.join(data_dir, \'weights\', \'vgg16.npy\')\n\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # Download VGG DATA\n    if not os.path.exists(vgg_weights):\n        download_command = ""wget {} -P {}"".format(vgg_url, data_dir)\n        logging.info(""Downloading VGG weights."")\n        download(vgg_url, data_dir)\n    else:\n        logging.warning(""File: {} exists."".format(vgg_weights))\n        logging.warning(""Please delete to redownload VGG weights."")\n\n    data_road_zip = os.path.join(data_dir, \'data_road.zip\')\n\n    # Download KITTI DATA\n    if not os.path.exists(data_road_zip):\n        if kitti_data_url == \'\':\n            logging.error(""Data URL for Kitti Data not provided."")\n            url = ""http://www.cvlibs.net/download.php?file=data_road.zip""\n            logging.error(""Please visit: {}"".format(url))\n            logging.error(""and request Kitti Download link."")\n            logging.error(""Rerun scipt using""\n                          ""\'python download_data.py\' --kitti_url [url]"")\n            exit(1)\n        if not kitti_data_url[-19:] == \'kitti/data_road.zip\':\n            logging.error(""Wrong url."")\n            url = ""http://www.cvlibs.net/download.php?file=data_road.zip""\n            logging.error(""Please visit: {}"".format(url))\n            logging.error(""and request Kitti Download link."")\n            logging.error(""Rerun scipt using""\n                          ""\'python download_data.py\' --kitti_url [url]"")\n            exit(1)\n        else:\n            logging.info(""Downloading Kitti Road Data."")\n            download(kitti_data_url, data_dir)\n\n    # Extract and prepare KITTI DATA\n    logging.info(""Extracting kitti_road data."")\n    zipfile.ZipFile(data_road_zip, \'r\').extractall(data_dir)\n    kitti_road_dir = os.path.join(data_dir, \'data_road/\')\n\n    logging.info(""Preparing kitti_road data."")\n\n    train_txt = ""data/train3.txt""\n    val_txt = ""data/val3.txt""\n    testing_txt = ""data/testing.txt""\n    copy2(train_txt, kitti_road_dir)\n    copy2(val_txt, kitti_road_dir)\n    copy2(testing_txt, kitti_road_dir)\n\n    logging.info(""All data have been downloaded successful."")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
evaluate.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Trains, evaluates and saves the KittiSeg model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nimport os\nimport sys\n\nimport collections\n\n# https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-220820070\nimport numpy as np\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nsys.path.insert(1, \'incl\')\n\nimport tensorvision.train as train\nimport tensorvision.analyze as ana\nimport tensorvision.utils as utils\n\nfrom evaluation import kitti_test\n\nflags.DEFINE_string(\'RUN\', \'KittiSeg_pretrained\',\n                    \'Modifier for model parameters.\')\nflags.DEFINE_string(\'hypes\', \'hypes/KittiSeg.json\',\n                    \'File storing model parameters.\')\nflags.DEFINE_string(\'name\', None,\n                    \'Append a name Tag to run.\')\n\nflags.DEFINE_string(\'project\', None,\n                    \'Append a name Tag to run.\')\n\nif \'TV_SAVE\' in os.environ and os.environ[\'TV_SAVE\']:\n    tf.app.flags.DEFINE_boolean(\n        \'save\', True, (\'Whether to save the run. In case --nosave (default) \'\n                       \'output will be saved to the folder TV_DIR_RUNS/debug, \'\n                       \'hence it will get overwritten by further runs.\'))\nelse:\n    tf.app.flags.DEFINE_boolean(\n        \'save\', True, (\'Whether to save the run. In case --nosave (default) \'\n                       \'output will be saved to the folder TV_DIR_RUNS/debug \'\n                       \'hence it will get overwritten by further runs.\'))\n\n\nsegmentation_weights_url = (""ftp://mi.eng.cam.ac.uk/""\n                            ""pub/mttt2/models/KittiSeg_pretrained.zip"")\n\n\ndef maybe_download_and_extract(runs_dir):\n    logdir = os.path.join(runs_dir, FLAGS.RUN)\n\n    if os.path.exists(logdir):\n        # weights are downloaded. Nothing to do\n        return\n\n    if not FLAGS.RUN == \'KittiSeg_pretrained\':\n        return\n\n    import zipfile\n    download_name = utils.download(segmentation_weights_url, runs_dir)\n\n    logging.info(""Extracting KittiSeg_pretrained.zip"")\n\n    zipfile.ZipFile(download_name, \'r\').extractall(runs_dir)\n\n    return\n\n\ndef main(_):\n    utils.set_gpus_to_use()\n\n    try:\n        import tensorvision.train\n        import tensorflow_fcn.utils\n    except ImportError:\n        logging.error(""Could not import the submodules."")\n        logging.error(""Please execute:""\n                      ""\'git submodule update --init --recursive\'"")\n        exit(1)\n\n    with open(tf.app.flags.FLAGS.hypes, \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = json.load(f)\n    utils.load_plugins()\n\n    if \'TV_DIR_RUNS\' in os.environ:\n        runs_dir = os.path.join(os.environ[\'TV_DIR_RUNS\'],\n                                \'KittiSeg\')\n    else:\n        runs_dir = \'RUNS\'\n\n    utils.set_dirs(hypes, tf.app.flags.FLAGS.hypes)\n\n    utils._add_paths_to_sys(hypes)\n\n    train.maybe_download_and_extract(hypes)\n\n    maybe_download_and_extract(runs_dir)\n    logging.info(""Evaluating on Validation data."")\n    logdir = os.path.join(runs_dir, FLAGS.RUN)\n    # logging.info(""Output images will be saved to {}"".format)\n    ana.do_analyze(logdir)\n\n    logging.info(""Creating output on test data."")\n    kitti_test.do_inference(logdir)\n\n    logging.info(""Analysis for pretrained model complete."")\n    logging.info(""For evaluating your own models I recommend using:""\n                 ""`tv-analyze --logdir /path/to/run`."")\n    logging.info(""tv-analysis has a much cleaner interface."")\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
train.py,9,"b'""""""\nTrains, evaluates and saves the KittiSeg model.\n\n-------------------------------------------------\n\nThe MIT License (MIT)\n\nCopyright (c) 2017 Marvin Teichmann\n\nMore details: https://github.com/MarvinTeichmann/KittiSeg/blob/master/LICENSE\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport commentjson\nimport logging\nimport os\nimport sys\n\nimport collections\n\n\ndef dict_merge(dct, merge_dct):\n    """""" Recursive dict merge. Inspired by :meth:``dict.update()``, instead of\n    updating only top-level keys, dict_merge recurses down into dicts nested\n    to an arbitrary depth, updating keys. The ``merge_dct`` is merged into\n    ``dct``.\n    :param dct: dict onto which the merge is executed\n    :param merge_dct: dct merged into dct\n    :return: None\n    """"""\n    for k, v in merge_dct.iteritems():\n        if (k in dct and isinstance(dct[k], dict) and\n                isinstance(merge_dct[k], collections.Mapping)):\n            dict_merge(dct[k], merge_dct[k])\n        else:\n            dct[k] = merge_dct[k]\n\n\n# configure logging\nif \'TV_IS_DEV\' in os.environ and os.environ[\'TV_IS_DEV\']:\n    logging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                        level=logging.INFO,\n                        stream=sys.stdout)\nelse:\n    logging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                        level=logging.INFO,\n                        stream=sys.stdout)\n\n# https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-220820070\nimport numpy as np\n\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nsys.path.insert(1, \'incl\')\n\nimport tensorvision.train as train\nimport tensorvision.utils as utils\n\nflags.DEFINE_string(\'name\', None,\n                    \'Append a name Tag to run.\')\n\nflags.DEFINE_string(\'project\', None,\n                    \'Append a name Tag to run.\')\n\nflags.DEFINE_string(\'hypes\', None,\n                    \'File storing model parameters.\')\n\nflags.DEFINE_string(\'mod\', None,\n                    \'Modifier for model parameters.\')\n\nif \'TV_SAVE\' in os.environ and os.environ[\'TV_SAVE\']:\n    tf.app.flags.DEFINE_boolean(\n        \'save\', True, (\'Whether to save the run. In case --nosave (default) \'\n                       \'output will be saved to the folder TV_DIR_RUNS/debug, \'\n                       \'hence it will get overwritten by further runs.\'))\nelse:\n    tf.app.flags.DEFINE_boolean(\n        \'save\', True, (\'Whether to save the run. In case --nosave (default) \'\n                       \'output will be saved to the folder TV_DIR_RUNS/debug \'\n                       \'hence it will get overwritten by further runs.\'))\n\n\ndef main(_):\n    utils.set_gpus_to_use()\n\n    try:\n        import tensorvision.train\n        import tensorflow_fcn.utils\n    except ImportError:\n        logging.error(""Could not import the submodules."")\n        logging.error(""Please execute:""\n                      ""\'git submodule update --init --recursive\'"")\n        exit(1)\n\n    if tf.app.flags.FLAGS.hypes is None:\n        logging.error(""No hype file is given."")\n        logging.info(""Usage: python train.py --hypes hypes/KittiClass.json"")\n        exit(1)\n\n    with open(tf.app.flags.FLAGS.hypes, \'r\') as f:\n        logging.info(""f: %s"", f)\n        hypes = commentjson.load(f)\n    utils.load_plugins()\n\n    if tf.app.flags.FLAGS.mod is not None:\n        import ast\n        mod_dict = ast.literal_eval(tf.app.flags.FLAGS.mod)\n        dict_merge(hypes, mod_dict)\n\n    if \'TV_DIR_RUNS\' in os.environ:\n        os.environ[\'TV_DIR_RUNS\'] = os.path.join(os.environ[\'TV_DIR_RUNS\'],\n                                                 \'KittiSeg\')\n    utils.set_dirs(hypes, tf.app.flags.FLAGS.hypes)\n\n    utils._add_paths_to_sys(hypes)\n\n    train.maybe_download_and_extract(hypes)\n    logging.info(""Initialize training folder"")\n    train.initialize_training_folder(hypes)\n    logging.info(""Start training"")\n    train.do_training(hypes)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
data/prepare_data.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Dec 17 11:50:47 2015\n\n@author: teichman\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\nimport time\nimport numpy as np\nimport scipy as scp\nimport scipy.misc\nimport sys\nfrom random import shuffle\nimport zipfile\n\n\nimport logging\nimport utils\nreload(logging)\n\nlogging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                    level=logging.DEBUG,\n                    stream=sys.stdout)\n\n\ndef make_val_split(data_folder):\n    """"""\n    Splits the Images in train and test.\n    Assumes a File all.txt in data_folder.\n    """"""\n\n    all_file = ""all.txt""\n    train_file = ""train.txt""\n    test_file = ""val.txt""\n    test_num = 20\n\n    filename = os.path.join(data_folder, all_file)\n    assert os.path.exists(filename), (""File not Found %s""\n                                      % filename)\n\n    files = [line for line in open(filename)]\n\n    shuffle(files)\n\n    train = files[:-test_num]\n    test = files[-test_num:]\n\n    train_file = os.path.join(data_folder, train_file)\n    test_file = os.path.join(data_folder, test_file)\n\n    with open(train_file, \'w\') as file:\n        for label in train:\n            file.write(label)\n\n    with open(test_file, \'w\') as file:\n        for label in test:\n            file.write(label)\n\n\ndef main():\n    data_dir = utils.cfg.data_dir\n    zip_file = ""data_road.zip""\n    zip_file = os.path.join(data_dir, zip_file)\n    if not os.path.exists(zip_file):\n        logging.error(""File not found: %s"", zip_file)\n        exit(1)\n    zipfile.ZipFile(zip_file, \'r\').extractall(data_dir)\n\n    make_val_split(data_dir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
decoder/fcn.py,50,"b'""""""\nAn implementation of FCN in tensorflow.\n------------------------\n\nThe MIT License (MIT)\n\nCopyright (c) 2017 Marvin Teichmann\n\nDetails: https://github.com/MarvinTeichmann/KittiSeg/blob/master/LICENSE\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport scipy as scp\nimport random\nfrom seg_utils import seg_utils as seg\n\nfrom tensorflow.python.framework import dtypes\nfrom math import ceil\n\n\nimport tensorflow as tf\n\n\ndef _add_softmax(hypes, logits):\n    num_classes = hypes[\'arch\'][\'num_classes\']\n    with tf.name_scope(\'decoder\'):\n        logits = tf.reshape(logits, (-1, num_classes))\n        epsilon = tf.constant(value=hypes[\'solver\'][\'epsilon\'])\n        # logits = logits + epsilon\n\n        softmax = tf.nn.softmax(logits)\n\n    return softmax\n\n\ndef decoder(hypes, logits, train=True, skip=True, debug=False):\n    fcn_in = logits[\'fcn_in\']\n    num_classes = hypes[\'arch\'][\'num_classes\']\n\n    fcn_in = tf.Print(fcn_in, [tf.shape(fcn_in)],\n                      message=\'Shape of %s\' % fcn_in.name,\n                      summarize=4, first_n=1)\n\n    if \'scale_down\' in hypes:\n        sd = hypes[\'scale_down\']\n    else:\n        sd = 1\n\n    he_init = tf.contrib.layers.variance_scaling_initializer()\n\n    l2_regularizer = tf.contrib.layers.l2_regularizer(hypes[\'wd\'])\n\n    # Build score_fr layer\n    score_fr = tf.layers.conv2d(\n        fcn_in, kernel_size=[1, 1], filters=num_classes, padding=\'SAME\',\n        name=\'score_fr\', kernel_initializer=he_init,\n        kernel_regularizer=l2_regularizer)\n\n    _activation_summary(score_fr)\n\n    # Do first upsampling\n    upscore2 = _upscore_layer(\n        score_fr, upshape=tf.shape(logits[\'feed2\']),\n        num_classes=num_classes, name=\'upscore2\', ksize=4, stride=2)\n\n    he_init2 = tf.contrib.layers.variance_scaling_initializer(factor=2.0*sd)\n    # Score feed2\n    score_feed2 = tf.layers.conv2d(\n        logits[\'feed2\'], kernel_size=[1, 1], filters=num_classes,\n        padding=\'SAME\', name=\'score_feed2\', kernel_initializer=he_init2,\n        kernel_regularizer=l2_regularizer)\n\n    _activation_summary(score_feed2)\n\n    if skip:\n        # Create skip connection\n        fuse_feed2 = tf.add(upscore2, score_feed2)\n    else:\n        fuse_feed2 = upscore2\n        fuse_feed2.set_shape(score_feed2.shape)\n\n    # Do second upsampling\n    upscore4 = _upscore_layer(\n        fuse_feed2, upshape=tf.shape(logits[\'feed4\']),\n        num_classes=num_classes, name=\'upscore4\', ksize=4, stride=2)\n\n    he_init4 = tf.contrib.layers.variance_scaling_initializer(factor=2.0*sd*sd)\n    # Score feed4\n    score_feed4 = tf.layers.conv2d(\n        logits[\'feed4\'], kernel_size=[1, 1], filters=num_classes,\n        padding=\'SAME\', name=\'score_feed4\', kernel_initializer=he_init4,\n        kernel_regularizer=l2_regularizer)\n\n    _activation_summary(score_feed4)\n\n    if skip:\n        # Create second skip connection\n        fuse_pool3 = tf.add(upscore4, score_feed4)\n    else:\n        fuse_pool3 = upscore4\n        fuse_pool3.set_shape(score_feed4.shape)\n\n    # Do final upsampling\n    upscore32 = _upscore_layer(\n        fuse_pool3, upshape=tf.shape(logits[\'images\']),\n        num_classes=num_classes, name=\'upscore32\', ksize=16, stride=8)\n\n    decoded_logits = {}\n    decoded_logits[\'logits\'] = upscore32\n    decoded_logits[\'softmax\'] = _add_softmax(hypes, upscore32)\n\n    return decoded_logits\n\n\ndef _upscore_layer(bottom, upshape,\n                   num_classes, name,\n                   ksize=4, stride=2):\n    strides = [1, stride, stride, 1]\n    with tf.variable_scope(name):\n        in_features = bottom.get_shape()[3].value\n\n        new_shape = [upshape[0], upshape[1], upshape[2], num_classes]\n        output_shape = tf.stack(new_shape)\n\n        f_shape = [ksize, ksize, num_classes, in_features]\n\n        up_init = upsample_initilizer()\n\n        weights = tf.get_variable(name=""weights"", initializer=up_init,\n                                  shape=f_shape)\n\n        tf.add_to_collection(tf.GraphKeys.WEIGHTS, weights)\n\n        deconv = tf.nn.conv2d_transpose(bottom, weights, output_shape,\n                                        strides=strides, padding=\'SAME\')\n\n        deconv = tf.Print(deconv, [tf.shape(deconv)],\n                          message=\'Shape of %s\' % name,\n                          summarize=4, first_n=1)\n\n        _activation_summary(deconv)\n\n    return deconv\n\n\ndef upsample_initilizer(dtype=dtypes.float32):\n    """"""Returns an initializer that creates filter for bilinear upsampling.\n\n    Use a transposed convolution layer with ksize = 2n and stride = n to\n    perform upsampling by a factor of n.\n    """"""\n    if not dtype.is_floating:\n        raise TypeError(\'Cannot create initializer for non-float point type.\')\n\n    def _initializer(shape, dtype=dtype, partition_info=None):\n        """"""Initializer function.""""""\n        if not dtype.is_floating:\n            raise TypeError(\'Cannot create initializer for non-floating type.\')\n\n        width = shape[0]\n        heigh = shape[0]\n        f = ceil(width/2.0)\n        c = (2 * f - 1 - f % 2) / (2.0 * f)\n        bilinear = np.zeros([shape[0], shape[1]])\n        for x in range(width):\n            for y in range(heigh):\n                value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n                bilinear[x, y] = value\n        weights = np.zeros(shape)\n        for i in range(shape[2]):\n            \'\'\'\n            the next line of code is correct as given\n            [several issues were opened ...]\n            we only want to scale each feature,\n            so there is no interaction between channels,\n            that is why only the diagonal i, i is initialized\n            \'\'\'\n            weights[:, :, i, i] = bilinear\n\n        return weights\n\n    return _initializer\n\n\ndef _activation_summary(x):\n    """"""Helper to create summaries for activations.\n\n    Creates a summary that provides a histogram of activations.\n    Creates a summary that measure the sparsity of activations.\n\n    Args:\n      x: Tensor\n    Returns:\n      nothing\n    """"""\n    # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on tensorboard.\n    tensor_name = x.op.name\n    # tensor_name = re.sub(\'%s_[0-9]*/\' % TOWER_NAME, \'\', x.op.name)\n    tf.summary.histogram(tensor_name + \'/activations\', x)\n    tf.summary.scalar(tensor_name + \'/sparsity\', tf.nn.zero_fraction(x))\n\n\ndef loss(hypes, decoded_logits, labels):\n    """"""Calculate the loss from the logits and the labels.\n\n    Args:\n      logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n      labels: Labels tensor, int32 - [batch_size].\n\n    Returns:\n      loss: Loss tensor of type float.\n    """"""\n    logits = decoded_logits[\'logits\']\n    with tf.name_scope(\'loss\'):\n\n        logits = tf.reshape(logits, (-1, 2))\n        shape = [logits.get_shape()[0], 2]\n        epsilon = tf.constant(value=hypes[\'solver\'][\'epsilon\'])\n        # logits = logits + epsilon\n        labels = tf.to_float(tf.reshape(labels, (-1, 2)))\n\n        softmax = tf.nn.softmax(logits) + epsilon\n\n        if hypes[\'loss\'] == \'xentropy\':\n            cross_entropy_mean = _compute_cross_entropy_mean(hypes, labels,\n                                                             softmax)\n        elif hypes[\'loss\'] == \'softF1\':\n            cross_entropy_mean = _compute_f1(hypes, labels, softmax, epsilon)\n\n        elif hypes[\'loss\'] == \'softIU\':\n            cross_entropy_mean = _compute_soft_ui(hypes, labels, softmax,\n                                                  epsilon)\n\n        reg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\n\n        weight_loss = tf.add_n(tf.get_collection(reg_loss_col),\n                               name=\'reg_loss\')\n\n        total_loss = cross_entropy_mean + weight_loss\n\n        losses = {}\n        losses[\'total_loss\'] = total_loss\n        losses[\'xentropy\'] = cross_entropy_mean\n        losses[\'weight_loss\'] = weight_loss\n\n    return losses\n\n\ndef _compute_cross_entropy_mean(hypes, labels, softmax):\n    head = hypes[\'arch\'][\'weight\']\n    cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax), head),\n                                   reduction_indices=[1])\n\n    cross_entropy_mean = tf.reduce_mean(cross_entropy,\n                                        name=\'xentropy_mean\')\n    return cross_entropy_mean\n\n\ndef _compute_f1(hypes, labels, softmax, epsilon):\n    labels = tf.to_float(tf.reshape(labels, (-1, 2)))[:, 1]\n    logits = softmax[:, 1]\n    true_positive = tf.reduce_sum(labels*logits)\n    false_positive = tf.reduce_sum((1-labels)*logits)\n\n    recall = true_positive / tf.reduce_sum(labels)\n    precision = true_positive / (true_positive + false_positive + epsilon)\n\n    score = 2*recall * precision / (precision + recall)\n    f1_score = 1 - 2*recall * precision / (precision + recall)\n\n    return f1_score\n\n\ndef _compute_soft_ui(hypes, labels, softmax, epsilon):\n    intersection = tf.reduce_sum(labels*softmax, reduction_indices=0)\n    union = tf.reduce_sum(labels+softmax, reduction_indices=0) \\\n        - intersection+epsilon\n\n    mean_iou = 1-tf.reduce_mean(intersection/union, name=\'mean_iou\')\n\n    return mean_iou\n\n\ndef evaluation(hyp, images, labels, decoded_logits, losses, global_step):\n    """"""Evaluate the quality of the logits at predicting the label.\n\n    Args:\n      logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n      labels: Labels tensor, int32 - [batch_size], with values in the\n        range [0, NUM_CLASSES).\n\n    Returns:\n      A scalar int32 tensor with the number of examples (out of batch_size)\n      that were predicted correctly.\n    """"""\n    # For a classifier model, we can use the in_top_k Op.\n    # It returns a bool tensor with shape [batch_size] that is true for\n    # the examples where the label\'s is was in the top k (here k=1)\n    # of all logits for that example.\n    eval_list = []\n    logits = tf.reshape(decoded_logits[\'logits\'], (-1, 2))\n    labels = tf.reshape(labels, (-1, 2))\n\n    pred = tf.argmax(logits, dimension=1)\n\n    negativ = tf.to_int32(tf.equal(pred, 0))\n    tn = tf.reduce_sum(negativ*labels[:, 0])\n    fn = tf.reduce_sum(negativ*labels[:, 1])\n\n    positive = tf.to_int32(tf.equal(pred, 1))\n    tp = tf.reduce_sum(positive*labels[:, 1])\n    fp = tf.reduce_sum(positive*labels[:, 0])\n\n    eval_list.append((\'Acc. \', (tn+tp)/(tn + fn + tp + fp)))\n    eval_list.append((\'xentropy\', losses[\'xentropy\']))\n    eval_list.append((\'weight_loss\', losses[\'weight_loss\']))\n\n    # eval_list.append((\'Precision\', tp/(tp + fp)))\n    # eval_list.append((\'True BG\', tn/(tn + fp)))\n    # eval_list.append((\'True Street [Recall]\', tp/(tp + fn)))\n\n    return eval_list\n'"
decoder/kitti_multiloss.py,29,"b'""""""\nAn implementation of FCN in tensorflow.\n------------------------\n\nThe MIT License (MIT)\n\nCopyright (c) 2016 Marvin Teichmann\n\nDetails: https://github.com/MarvinTeichmann/KittiSeg/blob/master/LICENSE\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport scipy as scp\nimport random\nfrom seg_utils import seg_utils as seg\n\n\nimport tensorflow as tf\n\n\ndef _add_softmax(hypes, logits):\n    num_classes = hypes[\'arch\'][\'num_classes\']\n    with tf.name_scope(\'decoder\'):\n        logits = tf.reshape(logits, (-1, num_classes))\n        epsilon = tf.constant(value=hypes[\'solver\'][\'epsilon\'])\n        # logits = logits + epsilon\n\n        softmax = tf.nn.softmax(logits)\n\n    return softmax\n\n\ndef decoder(hypes, logits, train):\n    """"""Apply decoder to the logits.\n\n    Args:\n      logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n\n    Return:\n      logits: the logits are already decoded.\n    """"""\n    decoded_logits = {}\n    decoded_logits[\'logits\'] = logits[\'fcn_logits\']\n    decoded_logits[\'softmax\'] = _add_softmax(hypes, logits[\'fcn_logits\'])\n    return decoded_logits\n\n\ndef loss(hypes, decoded_logits, labels):\n    """"""Calculate the loss from the logits and the labels.\n\n    Args:\n      logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n      labels: Labels tensor, int32 - [batch_size].\n\n    Returns:\n      loss: Loss tensor of type float.\n    """"""\n    logits = decoded_logits[\'logits\']\n    with tf.name_scope(\'loss\'):\n        logits = tf.reshape(logits, (-1, 2))\n        shape = [logits.get_shape()[0], 2]\n        epsilon = tf.constant(value=hypes[\'solver\'][\'epsilon\'])\n        # logits = logits + epsilon\n        labels = tf.to_float(tf.reshape(labels, (-1, 2)))\n\n        softmax = tf.nn.softmax(logits) + epsilon\n\n        if hypes[\'loss\'] == \'xentropy\':\n            cross_entropy_mean = _compute_cross_entropy_mean(hypes, labels,\n                                                             softmax)\n        elif hypes[\'loss\'] == \'softF1\':\n            cross_entropy_mean = _compute_f1(hypes, labels, softmax, epsilon)\n\n        elif hypes[\'loss\'] == \'softIU\':\n            cross_entropy_mean = _compute_soft_ui(hypes, labels, softmax,\n                                                  epsilon)\n\n        reg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\n\n        weight_loss = tf.add_n(tf.get_collection(reg_loss_col),\n                               name=\'reg_loss\')\n\n        total_loss = cross_entropy_mean + weight_loss\n\n        losses = {}\n        losses[\'total_loss\'] = total_loss\n        losses[\'xentropy\'] = cross_entropy_mean\n        losses[\'weight_loss\'] = weight_loss\n\n    return losses\n\n\ndef _compute_cross_entropy_mean(hypes, labels, softmax):\n    head = hypes[\'arch\'][\'weight\']\n    cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax), head),\n                                   reduction_indices=[1])\n\n    cross_entropy_mean = tf.reduce_mean(cross_entropy,\n                                        name=\'xentropy_mean\')\n    return cross_entropy_mean\n\n\ndef _compute_f1(hypes, labels, softmax, epsilon):\n    labels = tf.to_float(tf.reshape(labels, (-1, 2)))[:, 1]\n    logits = softmax[:, 1]\n    true_positive = tf.reduce_sum(labels*logits)\n    false_positive = tf.reduce_sum((1-labels)*logits)\n\n    recall = true_positive / tf.reduce_sum(labels)\n    precision = true_positive / (true_positive + false_positive + epsilon)\n\n    score = 2*recall * precision / (precision + recall)\n    f1_score = 1 - 2*recall * precision / (precision + recall)\n\n    return f1_score\n\n\ndef _compute_soft_ui(hypes, labels, softmax, epsilon):\n    intersection = tf.reduce_sum(labels*softmax, reduction_indices=0)\n    union = tf.reduce_sum(labels+softmax, reduction_indices=0) \\\n        - intersection+epsilon\n\n    mean_iou = 1-tf.reduce_mean(intersection/union, name=\'mean_iou\')\n\n    return mean_iou\n\n\ndef evaluation(hyp, images, labels, decoded_logits, losses, global_step):\n    """"""Evaluate the quality of the logits at predicting the label.\n\n    Args:\n      logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n      labels: Labels tensor, int32 - [batch_size], with values in the\n        range [0, NUM_CLASSES).\n\n    Returns:\n      A scalar int32 tensor with the number of examples (out of batch_size)\n      that were predicted correctly.\n    """"""\n    # For a classifier model, we can use the in_top_k Op.\n    # It returns a bool tensor with shape [batch_size] that is true for\n    # the examples where the label\'s is was in the top k (here k=1)\n    # of all logits for that example.\n    eval_list = []\n    logits = tf.reshape(decoded_logits[\'logits\'], (-1, 2))\n    labels = tf.reshape(labels, (-1, 2))\n\n    pred = tf.argmax(logits, dimension=1)\n\n    negativ = tf.to_int32(tf.equal(pred, 0))\n    tn = tf.reduce_sum(negativ*labels[:, 0])\n    fn = tf.reduce_sum(negativ*labels[:, 1])\n\n    positive = tf.to_int32(tf.equal(pred, 1))\n    tp = tf.reduce_sum(positive*labels[:, 1])\n    fp = tf.reduce_sum(positive*labels[:, 0])\n\n    eval_list.append((\'Acc. \', (tn+tp)/(tn + fn + tp + fp)))\n    eval_list.append((\'xentropy\', losses[\'xentropy\']))\n    eval_list.append((\'weight_loss\', losses[\'weight_loss\']))\n\n    # eval_list.append((\'Precision\', tp/(tp + fp)))\n    # eval_list.append((\'True BG\', tn/(tn + fp)))\n    # eval_list.append((\'True Street [Recall]\', tp/(tp + fn)))\n\n    return eval_list\n'"
encoder/fcn8_vgg.py,0,"b'""""""\nUtilize vgg_fcn8 as encoder.\n------------------------\n\nThe MIT License (MIT)\n\nCopyright (c) 2017 Marvin Teichmann\n\nDetails: https://github.com/MarvinTeichmann/KittiSeg/blob/master/LICENSE\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow_fcn import fcn8_vgg\n\nimport tensorflow as tf\n\nimport os\n\n\ndef inference(hypes, images, train=True):\n    """"""Build the MNIST model up to where it may be used for inference.\n\n    Args:\n      images: Images placeholder, from inputs().\n      train: whether the network is used for train of inference\n\n    Returns:\n      softmax_linear: Output tensor with the computed logits.\n    """"""\n    vgg16_npy_path = os.path.join(hypes[\'dirs\'][\'data_dir\'], \'weights\',\n                                  ""vgg16.npy"")\n    vgg_fcn = fcn8_vgg.FCN8VGG(vgg16_npy_path=vgg16_npy_path)\n\n    vgg_fcn.wd = hypes[\'wd\']\n\n    vgg_fcn.build(images, train=train, num_classes=2, random_init_fc8=True)\n\n    logits = {}\n\n    logits[\'images\'] = images\n\n    if hypes[\'arch\'][\'fcn_in\'] == \'pool5\':\n        logits[\'fcn_in\'] = vgg_fcn.pool5\n    elif hypes[\'arch\'][\'fcn_in\'] == \'fc7\':\n        logits[\'fcn_in\'] = vgg_fcn.fc7\n    else:\n        raise NotImplementedError\n\n    logits[\'feed2\'] = vgg_fcn.pool4\n    logits[\'feed4\'] = vgg_fcn.pool3\n\n    logits[\'fcn_logits\'] = vgg_fcn.upscore32\n\n    logits[\'deep_feat\'] = vgg_fcn.pool5\n    logits[\'early_feat\'] = vgg_fcn.conv4_3\n\n    return logits\n'"
encoder/resnet.py,41,"b'""""""\nThe MIT License (MIT)\n\nOriginal Work: Copyright (c) 2016 Ryan Dahl\n(See: https://github.com/ry/tensorflow-resnet)\n\nModified Work: Copyright (c) 2017 Marvin Teichmann\n\nFor details see \'licenses/RESNET_LICENSE.txt\'\n""""""\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.training import moving_averages\n\nimport datetime\nimport numpy as np\nimport os\nimport time\n\nimport logging\n\nMOVING_AVERAGE_DECAY = 0.998\nBN_DECAY = MOVING_AVERAGE_DECAY\nBN_EPSILON = 0.001\nCONV_WEIGHT_DECAY = 0.00004\nCONV_WEIGHT_STDDEV = 0.1\nFC_WEIGHT_DECAY = 0.00004\nFC_WEIGHT_STDDEV = 0.01\nMOMENTUM = 0.9\nRESNET_VARIABLES = \'resnet_variables\'\nUPDATE_OPS_COLLECTION = tf.GraphKeys.UPDATE_OPS\n# must be grouped with training op\nIMAGENET_MEAN_BGR = [103.062623801, 115.902882574, 123.151630838, ]\n\n\nnetwork_url = ""Not yet uploaded.""\n\n\ndef checkpoint_fn(layers):\n    return \'ResNet-L%d.ckpt\' % layers\n\n\ndef inference(hypes, images, train=True,\n              num_classes=1000,\n              num_blocks=[3, 4, 6, 3],  # defaults to 50-layer network\n              preprocess=True,\n              bottleneck=True):\n    # if preprocess is True, input should be RGB [0,1], otherwise BGR with mean\n    # subtracted\n\n    layers = hypes[\'arch\'][\'layers\']\n\n    if layers == 50:\n        num_blocks = [3, 4, 6, 3]\n    elif layers == 101:\n        num_blocks = [3, 4, 23, 3]\n    elif layers == 152:\n        num_blocks = [3, 8, 36, 3]\n    else:\n        assert()\n\n    if preprocess:\n        x = _imagenet_preprocess(images)\n\n    is_training = tf.convert_to_tensor(train,\n                                       dtype=\'bool\',\n                                       name=\'is_training\')\n\n    logits = {}\n\n    with tf.variable_scope(\'scale1\'):\n        x = _conv(x, 64, ksize=7, stride=2)\n        x = _bn(x, is_training, hypes)\n        x = _relu(x)\n        scale1 = x\n\n    with tf.variable_scope(\'scale2\'):\n        x = _max_pool(x, ksize=3, stride=2)\n        x = stack(x, num_blocks[0], 64, bottleneck, is_training, stride=1,\n                  hypes=hypes)\n        scale2 = x\n\n    with tf.variable_scope(\'scale3\'):\n        x = stack(x, num_blocks[1], 128, bottleneck, is_training, stride=2,\n                  hypes=hypes)\n        scale3 = x\n\n    with tf.variable_scope(\'scale4\'):\n        x = stack(x, num_blocks[2], 256, bottleneck, is_training, stride=2,\n                  hypes=hypes)\n        scale4 = x\n\n    with tf.variable_scope(\'scale5\'):\n        x = stack(x, num_blocks[3], 512, bottleneck, is_training, stride=2,\n                  hypes=hypes)\n        scale5 = x\n\n    logits[\'images\'] = images\n\n    logits[\'fcn_in\'] = scale5\n    logits[\'feed2\'] = scale4\n    logits[\'feed4\'] = scale3\n\n    logits[\'early_feat\'] = scale3\n    logits[\'deep_feat\'] = scale5\n\n    if train:\n        restore = tf.global_variables()\n        hypes[\'init_function\'] = _initalize_variables\n        hypes[\'restore\'] = restore\n\n    return logits\n\n\ndef _initalize_variables(hypes):\n    if hypes[\'load_pretrained\']:\n        logging.info(""Pretrained weights are loaded."")\n        logging.info(""The model is fine-tuned from previous training."")\n        restore = hypes[\'restore\']\n        init = tf.global_variables_initializer()\n        sess = tf.get_default_session()\n        sess.run(init)\n\n        saver = tf.train.Saver(var_list=restore)\n\n        layers = hypes[\'arch\'][\'layers\']\n\n        assert layers in [50, 101, 152]\n\n        filename = checkpoint_fn(layers)\n\n        if \'TV_DIR_DATA\' in os.environ:\n            filename = os.path.join(os.environ[\'TV_DIR_DATA\'], \'weights\',\n                                    ""tensorflow_resnet"", filename)\n        else:\n            filename = os.path.join(\'DATA\', \'weights\', ""tensorflow_resnet"",\n                                    filename)\n\n        if not os.path.exists(filename):\n            logging.error(""File not found: {}"".format(filename))\n            logging.error(""Please download weights from here: {}""\n                          .format(\'network_url\'))\n            exit(1)\n\n        logging.info(""Loading weights from disk."")\n        saver.restore(sess, filename)\n    else:\n        logging.info(""Random initialization performed."")\n        sess = tf.get_default_session()\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n\ndef _imagenet_preprocess(rgb):\n    """"""Changes RGB [0,1] valued image to BGR [0,255] with mean subtracted.""""""\n    red, green, blue = tf.split(\n        axis=3, num_or_size_splits=3, value=rgb * 255.0)\n    bgr = tf.concat(axis=3, values=[blue, green, red])\n    bgr -= IMAGENET_MEAN_BGR\n    return bgr\n\n\ndef stack(x, num_blocks, filters_internal, bottleneck, is_training, stride,\n          hypes):\n    for n in range(num_blocks):\n        s = stride if n == 0 else 1\n        with tf.variable_scope(\'block%d\' % (n + 1)):\n            x = block(x,\n                      filters_internal,\n                      bottleneck=bottleneck,\n                      is_training=is_training,\n                      stride=s,\n                      hypes=hypes)\n    return x\n\n\ndef block(x, filters_internal, is_training, stride, bottleneck, hypes):\n    filters_in = x.get_shape()[-1]\n\n    # Note: filters_out isn\'t how many filters are outputed.\n    # That is the case when bottleneck=False but when bottleneck is\n    # True, filters_internal*4 filters are outputted. filters_internal\n    # is how many filters\n    # the 3x3 convs output internally.\n    if bottleneck:\n        filters_out = 4 * filters_internal\n    else:\n        filters_out = filters_internal\n\n    shortcut = x  # branch 1\n\n    if bottleneck:\n        with tf.variable_scope(\'a\'):\n            x = _conv(x, filters_internal, ksize=1, stride=stride)\n            x = _bn(x, is_training, hypes)\n            x = _relu(x)\n\n        with tf.variable_scope(\'b\'):\n            x = _conv(x, filters_internal, ksize=3, stride=1)\n            x = _bn(x, is_training, hypes)\n            x = _relu(x)\n\n        with tf.variable_scope(\'c\'):\n            x = _conv(x, filters_out, ksize=1, stride=1)\n            x = _bn(x, is_training, hypes)\n    else:\n        with tf.variable_scope(\'A\'):\n            x = _conv(x, filters_internal, ksize=3, stride=stride)\n            x = _bn(x, is_training, hypes)\n            x = _relu(x)\n\n        with tf.variable_scope(\'B\'):\n            x = _conv(x, filters_out, ksize=3, stride=1)\n            x = _bn(x, is_training, hypes)\n\n    with tf.variable_scope(\'shortcut\'):\n        if filters_out != filters_in or stride != 1:\n            shortcut = _conv(shortcut, filters_out, ksize=1, stride=stride)\n            shortcut = _bn(shortcut, is_training, hypes)\n\n    return _relu(x + shortcut)\n\n\ndef _relu(x):\n    return tf.nn.relu(x)\n\n\ndef _bn(x, is_training, hypes):\n    x_shape = x.get_shape()\n    params_shape = x_shape[-1:]\n    axis = list(range(len(x_shape) - 1))\n\n    beta = _get_variable(\'beta\',\n                         params_shape,\n                         initializer=tf.zeros_initializer())\n    gamma = _get_variable(\'gamma\',\n                          params_shape,\n                          initializer=tf.ones_initializer())\n\n    moving_mean = _get_variable(\'moving_mean\',\n                                params_shape,\n                                initializer=tf.zeros_initializer(),\n                                trainable=False)\n    moving_variance = _get_variable(\'moving_variance\',\n                                    params_shape,\n                                    initializer=tf.ones_initializer(),\n                                    trainable=False)\n\n    # These ops will only be preformed when training.\n    mean, variance = tf.nn.moments(x, axis)\n\n    update_moving_mean = moving_averages.assign_moving_average(moving_mean,\n                                                               mean,\n                                                               BN_DECAY)\n    update_moving_variance = moving_averages.assign_moving_average(\n        moving_variance, variance, BN_DECAY)\n    if hypes[\'use_moving_average_bn\']:\n        tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_mean)\n        tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_variance)\n\n        mean, variance = control_flow_ops.cond(\n            is_training, lambda: (mean, variance),\n            lambda: (moving_mean, moving_variance))\n    else:\n        mean, variance = mean, variance\n\n    x = tf.nn.batch_normalization(x, mean, variance, beta, gamma, BN_EPSILON)\n    # x.set_shape(inputs.get_shape()) ??\n\n    return x\n\n\ndef _fc(x, num_units_out):\n    num_units_in = x.get_shape()[1]\n    weights_initializer = tf.truncated_normal_initializer(\n        stddev=FC_WEIGHT_STDDEV)\n\n    weights = _get_variable(\'weights\',\n                            shape=[num_units_in, num_units_out],\n                            initializer=weights_initializer,\n                            weight_decay=FC_WEIGHT_STDDEV)\n    biases = _get_variable(\'biases\',\n                           shape=[num_units_out],\n                           initializer=tf.zeros_initializer())\n    x = tf.nn.xw_plus_b(x, weights, biases)\n    return x\n\n\ndef _get_variable(name,\n                  shape,\n                  initializer,\n                  weight_decay=0.0,\n                  dtype=\'float\',\n                  trainable=True):\n    ""A little wrapper around tf.get_variable to do weight decay and add to""\n    ""resnet collection""\n    if weight_decay > 0:\n        regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n    else:\n        regularizer = None\n    collections = [tf.GraphKeys.GLOBAL_VARIABLES, RESNET_VARIABLES]\n    return tf.get_variable(name,\n                           shape=shape,\n                           initializer=initializer,\n                           dtype=dtype,\n                           regularizer=regularizer,\n                           collections=collections,\n                           trainable=trainable)\n\n\ndef _conv(x, filters_out, ksize=3, stride=1):\n    filters_in = x.get_shape()[-1]\n    shape = [ksize, ksize, filters_in, filters_out]\n    initializer = tf.truncated_normal_initializer(stddev=CONV_WEIGHT_STDDEV)\n    weights = _get_variable(\'weights\',\n                            shape=shape,\n                            dtype=\'float\',\n                            initializer=initializer,\n                            weight_decay=CONV_WEIGHT_DECAY)\n    return tf.nn.conv2d(x, weights, [1, stride, stride, 1], padding=\'SAME\')\n\n\ndef _max_pool(x, ksize=3, stride=2):\n    return tf.nn.max_pool(x,\n                          ksize=[1, ksize, ksize, 1],\n                          strides=[1, stride, stride, 1],\n                          padding=\'SAME\')\n'"
evals/kitti_eval.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Trains, evaluates and saves the model network using a queue.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport scipy as scp\nimport random\nfrom seg_utils import seg_utils as seg\n\nimport tensorflow as tf\nimport time\n\nimport tensorvision\nimport tensorvision.utils as utils\n\n\ndef eval_image(hypes, gt_image, cnn_image):\n    """""".""""""\n    thresh = np.array(range(0, 256))/255.0\n\n    road_color = np.array(hypes[\'data\'][\'road_color\'])\n    background_color = np.array(hypes[\'data\'][\'background_color\'])\n    gt_road = np.all(gt_image == road_color, axis=2)\n    gt_bg = np.all(gt_image == background_color, axis=2)\n    valid_gt = gt_road + gt_bg\n\n    FN, FP, posNum, negNum = seg.evalExp(gt_road, cnn_image,\n                                         thresh, validMap=None,\n                                         validArea=valid_gt)\n\n    return FN, FP, posNum, negNum\n\n\ndef resize_label_image(image, gt_image, image_height, image_width):\n    image = scp.misc.imresize(image, size=(image_height, image_width),\n                              interp=\'cubic\')\n    shape = gt_image.shape\n    gt_image = scp.misc.imresize(gt_image, size=(image_height, image_width),\n                                 interp=\'nearest\')\n\n    return image, gt_image\n\n\ndef evaluate(hypes, sess, image_pl, inf_out):\n\n    softmax = inf_out[\'softmax\']\n    data_dir = hypes[\'dirs\'][\'data_dir\']\n\n    eval_dict = {}\n    for phase in [\'train\', \'val\']:\n        data_file = hypes[\'data\'][\'{}_file\'.format(phase)]\n        data_file = os.path.join(data_dir, data_file)\n        image_dir = os.path.dirname(data_file)\n\n        thresh = np.array(range(0, 256))/255.0\n        total_fp = np.zeros(thresh.shape)\n        total_fn = np.zeros(thresh.shape)\n        total_posnum = 0\n        total_negnum = 0\n\n        image_list = []\n\n        with open(data_file) as file:\n            for i, datum in enumerate(file):\n                    datum = datum.rstrip()\n                    image_file, gt_file = datum.split("" "")\n                    image_file = os.path.join(image_dir, image_file)\n                    gt_file = os.path.join(image_dir, gt_file)\n\n                    image = scp.misc.imread(image_file, mode=\'RGB\')\n                    gt_image = scp.misc.imread(gt_file, mode=\'RGB\')\n\n                    if hypes[\'jitter\'][\'fix_shape\']:\n                        shape = image.shape\n                        image_height = hypes[\'jitter\'][\'image_height\']\n                        image_width = hypes[\'jitter\'][\'image_width\']\n                        assert(image_height >= shape[0])\n                        assert(image_width >= shape[1])\n\n                        offset_x = (image_height - shape[0])//2\n                        offset_y = (image_width - shape[1])//2\n                        new_image = np.zeros([image_height, image_width, 3])\n                        new_image[offset_x:offset_x+shape[0],\n                                  offset_y:offset_y+shape[1]] = image\n                        input_image = new_image\n                    elif hypes[\'jitter\'][\'reseize_image\']:\n                        image_height = hypes[\'jitter\'][\'image_height\']\n                        image_width = hypes[\'jitter\'][\'image_width\']\n                        gt_image_old = gt_image\n                        image, gt_image = resize_label_image(image, gt_image,\n                                                             image_height,\n                                                             image_width)\n                        input_image = image\n                    else:\n                        input_image = image\n\n                    shape = input_image.shape\n\n                    feed_dict = {image_pl: input_image}\n\n                    output = sess.run([softmax], feed_dict=feed_dict)\n                    output_im = output[0][:, 1].reshape(shape[0], shape[1])\n\n                    if hypes[\'jitter\'][\'fix_shape\']:\n                        gt_shape = gt_image.shape\n                        output_im = output_im[offset_x:offset_x+gt_shape[0],\n                                              offset_y:offset_y+gt_shape[1]]\n\n                    if phase == \'val\':\n                        # Saving RB Plot\n                        ov_image = seg.make_overlay(image, output_im)\n                        name = os.path.basename(image_file)\n                        image_list.append((name, ov_image))\n\n                        name2 = name.split(\'.\')[0] + \'_green.png\'\n\n                        hard = output_im > 0.5\n                        green_image = utils.fast_overlay(image, hard)\n                        image_list.append((name2, green_image))\n\n                    FN, FP, posNum, negNum = eval_image(hypes,\n                                                        gt_image, output_im)\n\n                    total_fp += FP\n                    total_fn += FN\n                    total_posnum += posNum\n                    total_negnum += negNum\n\n        eval_dict[phase] = seg.pxEval_maximizeFMeasure(\n            total_posnum, total_negnum, total_fn, total_fp, thresh=thresh)\n\n        if phase == \'val\':\n            start_time = time.time()\n            for i in xrange(10):\n                sess.run([softmax], feed_dict=feed_dict)\n            dt = (time.time() - start_time)/10\n\n    eval_list = []\n\n    for phase in [\'train\', \'val\']:\n        eval_list.append((\'[{}] MaxF1\'.format(phase),\n                          100*eval_dict[phase][\'MaxF\']))\n        eval_list.append((\'[{}] BestThresh\'.format(phase),\n                          100*eval_dict[phase][\'BestThresh\']))\n        eval_list.append((\'[{}] Average Precision\'.format(phase),\n                          100*eval_dict[phase][\'AvgPrec\']))\n    eval_list.append((\'Speed (msec)\', 1000*dt))\n    eval_list.append((\'Speed (fps)\', 1/dt))\n\n    return eval_list, image_list\n'"
inputs/kitti_seg_input.py,29,"b'""""""\nLoad Kitti Segmentation Input\n-------------------------------\n\nThe MIT License (MIT)\n\nCopyright (c) 2017 Marvin Teichmann\n\nDetails: https://github.com/MarvinTeichmann/KittiSeg/blob/master/LICENSE\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport json\nimport logging\nimport os\nimport sys\nimport random\nfrom random import shuffle\n\nimport numpy as np\n\nimport scipy as scp\nimport scipy.misc\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.training import queue_runner\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.framework import dtypes\n\nimport threading\n\n\nlogging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                    level=logging.INFO,\n                    stream=sys.stdout)\n\n\ndef maybe_download_and_extract(hypes):\n    """""" Downloads, extracts and prepairs data.\n\n    """"""\n\n    data_dir = hypes[\'dirs\'][\'data_dir\']\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    data_road_zip = os.path.join(data_dir, \'data_road.zip\')\n    vgg_weights = os.path.join(data_dir, \'vgg16.npy\')\n    kitti_road_dir = os.path.join(data_dir, \'data_road/\')\n\n    if os.path.exists(vgg_weights) and os.path.exists(kitti_road_dir):\n        return\n\n    import tensorvision.utils as utils\n    import zipfile\n    from shutil import copy2\n\n    # Download KITTI DATA\n    kitti_data_url = hypes[\'data\'][\'kitti_url\']\n\n    if kitti_data_url == \'\':\n        logging.error(""Data URL for Kitti Data not provided."")\n        url = ""http://www.cvlibs.net/download.php?file=data_road.zip""\n        logging.error(""Please visit: {}"".format(url))\n        logging.error(""and request Kitti Download link."")\n        logging.error(""Enter URL in hypes/kittiSeg.json"")\n        exit(1)\n    if not kitti_data_url[-19:] == \'kitti/data_road.zip\':\n        logging.error(""Wrong url."")\n        url = ""http://www.cvlibs.net/download.php?file=data_road.zip""\n        logging.error(""Please visit: {}"".format(url))\n        logging.error(""and request Kitti Download link."")\n        logging.error(""Enter URL in hypes/kittiSeg.json"")\n        exit(1)\n\n    logging.info(""Downloading Kitti Road Data."")\n    utils.download(kitti_data_url, data_dir)\n    # Extract and prepare KITTI DATA\n    logging.info(""Extracting kitti_road data."")\n    zipfile.ZipFile(data_road_zip, \'r\').extractall(data_dir)\n    kitti_road_dir = os.path.join(data_dir, \'data_road/\')\n\n    logging.info(""Preparing kitti_road data."")\n\n    train_txt = ""data/train3.txt""\n    val_txt = ""data/val3.txt""\n    copy2(train_txt, kitti_road_dir)\n    copy2(val_txt, kitti_road_dir)\n\n    vgg_url = kitti_data_url = hypes[\'data\'][\'vgg_url\']\n    # Download VGG DATA\n    download_command = ""wget {} -P {}"".format(vgg_url, data_dir)\n    logging.info(""Downloading VGG weights."")\n    utils.download(vgg_url, data_dir)\n    return\n\n\ndef _load_gt_file(hypes, data_file=None):\n    """"""Take the data_file and hypes and create a generator.\n\n    The generator outputs the image and the gt_image.\n    """"""\n    base_path = os.path.realpath(os.path.dirname(data_file))\n    files = [line.rstrip() for line in open(data_file)]\n\n    for epoche in itertools.count():\n        shuffle(files)\n        for file in files:\n            image_file, gt_image_file = file.split("" "")\n            image_file = os.path.join(base_path, image_file)\n            assert os.path.exists(image_file), \\\n                ""File does not exist: %s"" % image_file\n            gt_image_file = os.path.join(base_path, gt_image_file)\n            assert os.path.exists(gt_image_file), \\\n                ""File does not exist: %s"" % gt_image_file\n            image = scipy.misc.imread(image_file, mode=\'RGB\')\n            # Please update Scipy, if mode=\'RGB\' is not avaible\n            gt_image = scp.misc.imread(gt_image_file, mode=\'RGB\')\n\n            yield image, gt_image\n\n\ndef _make_data_gen(hypes, phase, data_dir):\n    """"""Return a data generator that outputs image samples.\n\n    @ Returns\n    image: integer array of shape [height, width, 3].\n    Representing RGB value of each pixel.\n    gt_image: boolean array of shape [height, width, num_classes].\n    Set `gt_image[i,j,k] == 1` if and only if pixel i,j\n    is assigned class k. `gt_image[i,j,k] == 0` otherwise.\n\n    [Alternativly make gt_image[i,j,*] a valid propability\n    distribution.]\n    """"""\n    if phase == \'train\':\n        data_file = hypes[\'data\'][""train_file""]\n    elif phase == \'val\':\n        data_file = hypes[\'data\'][""val_file""]\n    else:\n        assert False, ""Unknown Phase %s"" % phase\n\n    data_file = os.path.join(data_dir, data_file)\n\n    road_color = np.array(hypes[\'data\'][\'road_color\'])\n    background_color = np.array(hypes[\'data\'][\'background_color\'])\n\n    data = _load_gt_file(hypes, data_file)\n\n    for image, gt_image in data:\n\n        gt_bg = np.all(gt_image == background_color, axis=2)\n        gt_road = np.all(gt_image == road_color, axis=2)\n\n        assert(gt_road.shape == gt_bg.shape)\n        shape = gt_bg.shape\n        gt_bg = gt_bg.reshape(shape[0], shape[1], 1)\n        gt_road = gt_road.reshape(shape[0], shape[1], 1)\n\n        gt_image = np.concatenate((gt_bg, gt_road), axis=2)\n\n        if phase == \'val\':\n            yield image, gt_image\n        elif phase == \'train\':\n\n            yield jitter_input(hypes, image, gt_image)\n\n            yield jitter_input(hypes, np.fliplr(image), np.fliplr(gt_image))\n\n\ndef jitter_input(hypes, image, gt_image):\n\n    jitter = hypes[\'jitter\']\n    res_chance = jitter[\'res_chance\']\n    crop_chance = jitter[\'crop_chance\']\n\n    if jitter[\'random_resize\'] and res_chance > random.random():\n        lower_size = jitter[\'lower_size\']\n        upper_size = jitter[\'upper_size\']\n        sig = jitter[\'sig\']\n        image, gt_image = random_resize(image, gt_image, lower_size,\n                                        upper_size, sig)\n        image, gt_image = crop_to_size(hypes, image, gt_image)\n\n    if jitter[\'random_crop\'] and crop_chance > random.random():\n        max_crop = jitter[\'max_crop\']\n        crop_chance = jitter[\'crop_chance\']\n        image, gt_image = random_crop_soft(image, gt_image, max_crop)\n\n    if jitter[\'reseize_image\']:\n        image_height = jitter[\'image_height\']\n        image_width = jitter[\'image_width\']\n        image, gt_image = resize_label_image(image, gt_image,\n                                             image_height,\n                                             image_width)\n\n    if jitter[\'crop_patch\']:\n        patch_height = jitter[\'patch_height\']\n        patch_width = jitter[\'patch_width\']\n        image, gt_image = random_crop(image, gt_image,\n                                      patch_height, patch_width)\n\n    assert(image.shape[:-1] == gt_image.shape[:-1])\n    return image, gt_image\n\n\ndef random_crop(image, gt_image, height, width):\n    old_width = image.shape[1]\n    old_height = image.shape[0]\n    assert(old_width >= width)\n    assert(old_height >= height)\n    max_x = max(old_height-height, 0)\n    max_y = max(old_width-width, 0)\n    offset_x = random.randint(0, max_x)\n    offset_y = random.randint(0, max_y)\n    image = image[offset_x:offset_x+height, offset_y:offset_y+width]\n    gt_image = gt_image[offset_x:offset_x+height, offset_y:offset_y+width]\n\n    assert(image.shape[0] == height)\n    assert(image.shape[1] == width)\n\n    return image, gt_image\n\n\ndef random_crop_soft(image, gt_image, max_crop):\n    offset_x = random.randint(1, max_crop)\n    offset_y = random.randint(1, max_crop)\n\n    if random.random() > 0.5:\n        image = image[offset_x:, offset_y:, :]\n        gt_image = gt_image[offset_x:, offset_y:, :]\n    else:\n        image = image[:-offset_x, :-offset_y, :]\n        gt_image = gt_image[:-offset_x, :-offset_y, :]\n\n    return image, gt_image\n\n\ndef resize_label_image_with_pad(image, label, image_height, image_width):\n    shape = image.shape\n    assert(image_height >= shape[0])\n    assert(image_width >= shape[1])\n\n    pad_height = image_height - shape[0]\n    pad_width = image_width - shape[1]\n    offset_x = random.randint(0, pad_height)\n    offset_y = random.randint(0, pad_width)\n\n    new_image = np.zeros([image_height, image_width, 3])\n    new_image[offset_x:offset_x+shape[0], offset_y:offset_y+shape[1]] = image\n\n    new_label = np.zeros([image_height, image_width, 2])\n    new_label[offset_x:offset_x+shape[0], offset_y:offset_y+shape[1]] = label\n\n    return new_image, new_label\n\n\ndef resize_label_image(image, gt_image, image_height, image_width):\n    image = scipy.misc.imresize(image, size=(image_height, image_width),\n                                interp=\'cubic\')\n    shape = gt_image.shape\n    gt_zero = np.zeros([shape[0], shape[1], 1])\n    gt_image = np.concatenate((gt_image, gt_zero), axis=2)\n    gt_image = scipy.misc.imresize(gt_image, size=(image_height, image_width),\n                                   interp=\'nearest\')\n    gt_image = gt_image[:, :, 0:2]/255\n\n    return image, gt_image\n\n\ndef random_resize(image, gt_image, lower_size, upper_size, sig):\n    factor = random.normalvariate(1, sig)\n    if factor < lower_size:\n        factor = lower_size\n    if factor > upper_size:\n        factor = upper_size\n    image = scipy.misc.imresize(image, factor)\n    shape = gt_image.shape\n    gt_zero = np.zeros([shape[0], shape[1], 1])\n    gt_image = np.concatenate((gt_image, gt_zero), axis=2)\n    gt_image = scipy.misc.imresize(gt_image, factor, interp=\'nearest\')\n    gt_image = gt_image[:, :, 0:2]/255\n    return image, gt_image\n\n\ndef crop_to_size(hypes, image, gt_image):\n    new_width = image.shape[1]\n    new_height = image.shape[0]\n    width = hypes[\'arch\'][\'image_width\']\n    height = hypes[\'arch\'][\'image_height\']\n    if new_width > width:\n        max_x = max(new_height-height, 0)\n        max_y = new_width-width\n        offset_x = random.randint(0, max_x)\n        offset_y = random.randint(0, max_y)\n        image = image[offset_x:offset_x+height, offset_y:offset_y+width]\n        gt_image = gt_image[offset_x:offset_x+height, offset_y:offset_y+width]\n\n    return image, gt_image\n\n\ndef create_queues(hypes, phase):\n    """"""Create Queues.""""""\n    arch = hypes[\'arch\']\n    dtypes = [tf.float32, tf.int32]\n\n    shape_known = hypes[\'jitter\'][\'reseize_image\'] \\\n        or hypes[\'jitter\'][\'crop_patch\']\n\n    if shape_known:\n        if hypes[\'jitter\'][\'crop_patch\']:\n            height = hypes[\'jitter\'][\'patch_height\']\n            width = hypes[\'jitter\'][\'patch_width\']\n        else:\n            height = hypes[\'jitter\'][\'image_height\']\n            width = hypes[\'jitter\'][\'image_width\']\n        channel = hypes[\'arch\'][\'num_channels\']\n        num_classes = hypes[\'arch\'][\'num_classes\']\n        shapes = [[height, width, channel],\n                  [height, width, num_classes]]\n    else:\n        shapes = None\n\n    capacity = 50\n    q = tf.FIFOQueue(capacity=50, dtypes=dtypes, shapes=shapes)\n    tf.summary.scalar(""queue/%s/fraction_of_%d_full"" %\n                      (q.name + ""_"" + phase, capacity),\n                      math_ops.cast(q.size(), tf.float32) * (1. / capacity))\n\n    return q\n\n\ndef start_enqueuing_threads(hypes, q, phase, sess):\n    """"""Start enqueuing threads.""""""\n    image_pl = tf.placeholder(tf.float32)\n    label_pl = tf.placeholder(tf.int32)\n    data_dir = hypes[\'dirs\'][\'data_dir\']\n\n    def make_feed(data):\n        image, label = data\n        return {image_pl: image, label_pl: label}\n\n    def enqueue_loop(sess, enqueue_op, phase, gen):\n        # infinity loop enqueueing data\n        for d in gen:\n            sess.run(enqueue_op, feed_dict=make_feed(d))\n\n    enqueue_op = q.enqueue((image_pl, label_pl))\n    gen = _make_data_gen(hypes, phase, data_dir)\n    gen.next()\n    # sess.run(enqueue_op, feed_dict=make_feed(data))\n    if phase == \'val\':\n        num_threads = 1\n    else:\n        num_threads = 1\n    for i in range(num_threads):\n        t = threading.Thread(target=enqueue_loop,\n                             args=(sess, enqueue_op,\n                                   phase, gen))\n        t.daemon = True\n        t.start()\n\n\ndef _read_processed_image(hypes, q, phase):\n    image, label = q.dequeue()\n    jitter = hypes[\'jitter\']\n    if phase == \'train\':\n        # Because these operations are not commutative, consider randomizing\n        # randomize the order their operation.\n        augment_level = jitter[\'augment_level\']\n        if augment_level > 0:\n            image = tf.image.random_brightness(image, max_delta=30)\n            image = tf.image.random_contrast(image, lower=0.75, upper=1.25)\n        if augment_level > 1:\n            image = tf.image.random_hue(image, max_delta=0.15)\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.6)\n\n    if \'whitening\' not in hypes[\'arch\'] or \\\n            hypes[\'arch\'][\'whitening\']:\n        image = tf.image.per_image_whitening(image)\n        logging.info(\'Whitening is enabled.\')\n    else:\n        logging.info(\'Whitening is disabled.\')\n\n    image = tf.expand_dims(image, 0)\n    label = tf.expand_dims(label, 0)\n\n    return image, label\n\n\ndef _dtypes(tensor_list_list):\n    all_types = [[t.dtype for t in tl] for tl in tensor_list_list]\n    types = all_types[0]\n    for other_types in all_types[1:]:\n        if other_types != types:\n            raise TypeError(""Expected types to be consistent: %s vs. %s."" %\n                            ("", "".join(x.name for x in types),\n                             "", "".join(x.name for x in other_types)))\n    return types\n\n\ndef _enqueue_join(queue, tensor_list_list):\n    enqueue_ops = [queue.enqueue(tl) for tl in tensor_list_list]\n    queue_runner.add_queue_runner(queue_runner.QueueRunner(queue, enqueue_ops))\n\n\ndef shuffle_join(tensor_list_list, capacity,\n                 min_ad, phase):\n    name = \'shuffel_input\'\n    types = _dtypes(tensor_list_list)\n    queue = data_flow_ops.RandomShuffleQueue(\n        capacity=capacity, min_after_dequeue=min_ad,\n        dtypes=types)\n\n    # Build enque Operations\n    _enqueue_join(queue, tensor_list_list)\n\n    full = (math_ops.cast(math_ops.maximum(0, queue.size() - min_ad),\n                          dtypes.float32) * (1. / (capacity - min_ad)))\n    # Note that name contains a \'/\' at the end so we intentionally do not place\n    # a \'/\' after %s below.\n    summary_name = (\n        ""queue/%s/fraction_over_%d_of_%d_full"" %\n        (name + \'_\' + phase, min_ad, capacity - min_ad))\n    tf.summary.scalar(summary_name, full)\n\n    dequeued = queue.dequeue(name=\'shuffel_deqeue\')\n    # dequeued = _deserialize_sparse_tensors(dequeued, sparse_info)\n    return dequeued\n\n\ndef _processe_image(hypes, image):\n    # Because these operations are not commutative, consider randomizing\n    # randomize the order their operation.\n    augment_level = hypes[\'jitter\'][\'augment_level\']\n    if augment_level > 0:\n        image = tf.image.random_brightness(image, max_delta=30)\n        image = tf.image.random_contrast(image, lower=0.75, upper=1.25)\n    if augment_level > 1:\n        image = tf.image.random_hue(image, max_delta=0.15)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.6)\n\n    return image\n\n\ndef inputs(hypes, q, phase):\n    """"""Generate Inputs images.""""""\n    if phase == \'val\':\n        image, label = q.dequeue()\n        image = tf.expand_dims(image, 0)\n        label = tf.expand_dims(label, 0)\n        return image, label\n\n    shape_known = hypes[\'jitter\'][\'reseize_image\'] \\\n        or hypes[\'jitter\'][\'crop_patch\']\n\n    if not shape_known:\n        image, label = q.dequeue()\n        nc = hypes[""arch""][""num_classes""]\n        label.set_shape([None, None, nc])\n        image.set_shape([None, None, 3])\n        image = tf.expand_dims(image, 0)\n        label = tf.expand_dims(label, 0)\n        if hypes[\'solver\'][\'batch_size\'] > 1:\n            logging.error(""Using a batch_size of {} with unknown shape.""\n                          .format(hypes[\'solver\'][\'batch_size\']))\n            logging.error(""Set batch_size to 1 or use `reseize_image` ""\n                          ""or `crop_patch` to obtain a defined shape"")\n            raise ValueError\n    else:\n        image, label = q.dequeue_many(hypes[\'solver\'][\'batch_size\'])\n\n    image = _processe_image(hypes, image)\n\n    # Display the training images in the visualizer.\n    tensor_name = image.op.name\n    tf.summary.image(tensor_name + \'/image\', image)\n\n    road = tf.expand_dims(tf.to_float(label[:, :, :, 0]), 3)\n    tf.summary.image(tensor_name + \'/gt_image\', road)\n\n    return image, label\n\n\ndef main():\n    """"""main.""""""\n    with open(\'../hypes/kitti_seg.json\', \'r\') as f:\n        hypes = json.load(f)\n\n    q = {}\n    q[\'train\'] = create_queues(hypes, \'train\')\n    q[\'val\'] = create_queues(hypes, \'val\')\n    data_dir = ""../DATA""\n\n    _make_data_gen(hypes, \'train\', data_dir)\n\n    image_batch, label_batch = inputs(hypes, q, \'train\', data_dir)\n\n    logging.info(""Start running"")\n\n    with tf.Session() as sess:\n        # Run the Op to initialize the variables.\n        init = tf.initialize_all_variables()\n        sess.run(init)\n        coord = tf.train.Coordinator()\n        start_enqueuing_threads(hypes, q, sess, data_dir)\n\n        logging.info(""Start running"")\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        for i in itertools.count():\n            image = image_batch.eval()\n            gt = label_batch.eval()\n            scp.misc.imshow(image[0])\n            gt_bg = gt[0, :, :, 0]\n            gt_road = gt[0, :, :, 1]\n            scp.misc.imshow(gt_bg)\n            scp.misc.imshow(gt_road)\n\n        coord.request_stop()\n        coord.join(threads)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
optimizer/generic_optimizer.py,7,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport sys\nimport tensorflow as tf\n\n\n# def get_learning_rate(hypes, step):\n#    lr = hypes[\'solver\'][\'learning_rate\']\n#    lr_step = hypes[\'solver\'][\'learning_rate_step\']\n#    if lr_step is not None:\n#      adjusted_lr = (lr * 0.5 ** max(0, (step / lr_step) - 2))\n#        return adjusted_lr\n#    else:\n#        return lr\n\ndef get_learning_rate(hypes, step):\n    if ""learning_rates"" not in hypes[\'solver\']:\n        lr = hypes[\'solver\'][\'learning_rate\']\n        lr_step = hypes[\'solver\'][\'learning_rate_step\']\n        if lr_step is not None:\n            adjusted_lr = (lr * 0.5 ** max(0, (step / lr_step) - 2))\n            return adjusted_lr\n        else:\n            return lr\n\n    for i, num in enumerate(hypes[\'solver\'][\'steps\']):\n        if step < num:\n            return hypes[\'solver\'][\'learning_rates\'][i]\n\n\ndef training(hypes, loss, global_step, learning_rate, opt=None):\n    """"""Sets up the training Ops.\n\n    Creates a summarizer to track the loss over time in TensorBoard.\n\n    Creates an optimizer and applies the gradients to all trainable variables.\n\n    The Op returned by this function is what must be passed to the\n    `sess.run()` call to cause the model to train.\n\n    Args:\n      loss: Loss tensor, from loss().\n      global_step: Integer Variable counting the number of training steps\n        processed.\n      learning_rate: The learning rate to use for gradient descent.\n\n    Returns:\n      train_op: The Op for training.\n    """"""\n    # Add a scalar summary for the snapshot loss.\'\'\n    sol = hypes[""solver""]\n    hypes[\'tensors\'] = {}\n    hypes[\'tensors\'][\'global_step\'] = global_step\n    total_loss = loss[\'total_loss\']\n    with tf.name_scope(\'training\'):\n\n        if opt is None:\n\n            if sol[\'opt\'] == \'RMS\':\n                opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n                                                decay=0.9,\n                                                epsilon=sol[\'epsilon\'])\n            elif sol[\'opt\'] == \'Adam\':\n                opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\n                                             epsilon=sol[\'adam_eps\'])\n            elif sol[\'opt\'] == \'SGD\':\n                lr = learning_rate\n                opt = tf.train.GradientDescentOptimizer(learning_rate=lr)\n            else:\n                raise ValueError(\'Unrecognized opt type\')\n\n        hypes[\'opt\'] = opt\n\n        grads_and_vars = opt.compute_gradients(total_loss)\n\n        if hypes[\'clip_norm\'] > 0:\n            grads, tvars = zip(*grads_and_vars)\n            clip_norm = hypes[""clip_norm""]\n            clipped_grads, norm = tf.clip_by_global_norm(grads, clip_norm)\n            grads_and_vars = zip(clipped_grads, tvars)\n\n        train_op = opt.apply_gradients(grads_and_vars, global_step=global_step)\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n        with tf.control_dependencies(update_ops):\n            train_op = opt.apply_gradients(grads_and_vars,\n                                           global_step=global_step)\n\n    return train_op\n'"
submodules/evaluation/__init__.py,0,b''
submodules/evaluation/kitti_test.py,9,"b'#!/usr/bin/env python\n# pylint: disable=missing-docstring\n# -*- coding: utf-8 -*-\n\n""""""Trains, evaluates and saves the model network using a queue.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport imp\nimport json\nimport logging\nimport numpy as np\nimport os.path\nimport sys\n\nimport scipy as scp\nimport scipy.misc\n\n\nsys.path.insert(1, \'../../incl\')\n\nimport tensorflow as tf\n\nimport tensorvision.utils as utils\nimport tensorvision.core as core\nimport tensorvision.analyze as ana\n\nfrom seg_utils import seg_utils as seg\n\n# configure logging\nif \'TV_IS_DEV\' in os.environ and os.environ[\'TV_IS_DEV\']:\n    logging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                        level=logging.INFO,\n                        stream=sys.stdout)\nelse:\n    logging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                        level=logging.INFO,\n                        stream=sys.stdout)\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\ntest_file = \'data_road/testing.txt\'\n\n\ndef create_test_output(hypes, sess, image_pl, softmax):\n    data_dir = hypes[\'dirs\'][\'data_dir\']\n    data_file = os.path.join(data_dir, test_file)\n    image_dir = os.path.dirname(data_file)\n\n    logdir = ""test_images/""\n    logdir_rb = ""test_images_rb/""\n    logdir_green = ""test_images_green/""\n\n    logging.info(""Images will be written to {}/test_images_{{green, rg}}""\n                 .format(logdir))\n\n    logdir = os.path.join(hypes[\'dirs\'][\'output_dir\'], logdir)\n    logdir_rb = os.path.join(hypes[\'dirs\'][\'output_dir\'], logdir_rb)\n    logdir_green = os.path.join(hypes[\'dirs\'][\'output_dir\'], logdir_green)\n\n    if not os.path.exists(logdir):\n        os.mkdir(logdir)\n\n    if not os.path.exists(logdir_rb):\n        os.mkdir(logdir_rb)\n\n    if not os.path.exists(logdir_green):\n        os.mkdir(logdir_green)\n\n    image_list = []\n\n    with open(data_file) as file:\n        for i, image_file in enumerate(file):\n                image_file = image_file.rstrip()\n                image_file = os.path.join(image_dir, image_file)\n                image = scp.misc.imread(image_file)\n                shape = image.shape\n\n                feed_dict = {image_pl: image}\n\n                output = sess.run([softmax[\'softmax\']], feed_dict=feed_dict)\n                output_im = output[0][:, 1].reshape(shape[0], shape[1])\n\n                ov_image = seg.make_overlay(image, output_im)\n                hard = output_im > 0.5\n                green_image = utils.fast_overlay(image, hard)\n\n                name = os.path.basename(image_file)\n                new_name = name.split(\'_\')[0] + ""_road_"" + name.split(\'_\')[1]\n\n                save_file = os.path.join(logdir, new_name)\n                logging.info(""Writing file: %s"", save_file)\n                scp.misc.imsave(save_file, output_im)\n                save_file = os.path.join(logdir_rb, new_name)\n                scp.misc.imsave(save_file, ov_image)\n                save_file = os.path.join(logdir_green, new_name)\n                scp.misc.imsave(save_file, green_image)\n\n\ndef _create_input_placeholder():\n    image_pl = tf.placeholder(tf.float32)\n    label_pl = tf.placeholder(tf.float32)\n    return image_pl, label_pl\n\n\ndef do_inference(logdir):\n    """"""\n    Analyze a trained model.\n\n    This will load model files and weights found in logdir and run a basic\n    analysis.\n\n    Parameters\n    ----------\n    logdir : string\n        Directory with logs.\n    """"""\n    hypes = utils.load_hypes_from_logdir(logdir)\n    modules = utils.load_modules_from_logdir(logdir)\n\n    # Tell TensorFlow that the model will be built into the default Graph.\n    with tf.Graph().as_default():\n\n        # prepaire the tv session\n\n        with tf.name_scope(\'Validation\'):\n            image_pl, label_pl = _create_input_placeholder()\n            image = tf.expand_dims(image_pl, 0)\n            softmax = core.build_inference_graph(hypes, modules,\n                                                 image=image)\n\n        sess = tf.Session()\n        saver = tf.train.Saver()\n\n        core.load_weights(logdir, sess, saver)\n\n        create_test_output(hypes, sess, image_pl, softmax)\n    return\n\n\ndef main(_):\n    """"""Run main function.""""""\n    if FLAGS.logdir is None:\n        logging.error(""No logdir are given."")\n        logging.error(""Usage: tv-analyze --logdir dir"")\n        exit(1)\n\n    if FLAGS.gpus is None:\n        if \'TV_USE_GPUS\' in os.environ:\n            if os.environ[\'TV_USE_GPUS\'] == \'force\':\n                logging.error(\'Please specify a GPU.\')\n                logging.error(\'Usage tv-train --gpus <ids>\')\n                exit(1)\n            else:\n                gpus = os.environ[\'TV_USE_GPUS\']\n                logging.info(""GPUs are set to: %s"", gpus)\n                os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpus\n    else:\n        logging.info(""GPUs are set to: %s"", FLAGS.gpus)\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpus\n\n    utils.load_plugins()\n\n    logdir = os.path.realpath(FLAGS.logdir)\n\n    logging.info(""Starting to analyze Model in: %s"", logdir)\n    do_inference(logdir)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
submodules/evaluation/overlay_utils.py,0,"b'#!/usr/bin/env python\n\n""""""Utility functions for segmentation tasks.""""""\n\nfrom PIL import Image\nimport scipy.ndimage\nimport numpy as np\n\n\ndef replace_colors(segmentation, color_changes):\n    """"""\n    Replace the values in segmentation to the values defined in color_changes.\n\n    Parameters\n    ----------\n    segmentation : numpy array\n        Two dimensional\n    color_changes : dict\n        The key is the original color, the value is the color to change to.\n        The key \'default\' is used when the color is not in the dict.\n        If default is not defined, no replacement is done.\n        Each color has to be a tuple (r, g, b) with r, g, b in {0, 1, ..., 255}\n    Returns\n    -------\n    np.array\n        The new colored segmentation\n    """"""\n    width, height = segmentation.shape\n    output = scipy.misc.toimage(segmentation)\n    output = output.convert(\'RGBA\')\n    for x in range(0, width):\n        for y in range(0, height):\n            if segmentation[x, y] in color_changes:\n                output.putpixel((y, x), color_changes[segmentation[x, y]])\n            elif \'default\' in color_changes:\n                output.putpixel((y, x), color_changes[\'default\'])\n    return output\n\n\ndef overlay_segmentation(image, segmentation, color_dict):\n    """"""\n    Overlay original_image with segmentation_image.\n\n    Parameters\n    ----------\n    """"""\n    width, height = segmentation.shape\n    output = scipy.misc.toimage(segmentation)\n    output = output.convert(\'RGBA\')\n    for x in range(0, width):\n        for y in range(0, height):\n            if segmentation[x, y] in color_dict:\n                output.putpixel((y, x), color_dict[segmentation[x, y]])\n            elif \'default\' in color_dict:\n                output.putpixel((y, x), color_dict[\'default\'])\n\n    background = scipy.misc.toimage(image)\n    background.paste(output, box=None, mask=output)\n\n    return np.array(background)\n'"
submodules/evaluation/kitti_devkit/__init__.py,0,b''
submodules/evaluation/kitti_devkit/helper.py,0,"b'#!/usr/bin/env python\n#\n#  THE KITTI VISION BENCHMARK SUITE: ROAD BENCHMARK\n#\n#  Copyright (C) 2013\n#  Honda Research Institute Europe GmbH\n#  Carl-Legien-Str. 30\n#  63073 Offenbach/Main\n#  Germany\n#\n#  UNPUBLISHED PROPRIETARY MATERIAL.\n#  ALL RIGHTS RESERVED.\n#\n#  Authors: Tobias Kuehnl <tkuehnl@cor-lab.uni-bielefeld.de>\n#           Jannik Fritsch <jannik.fritsch@honda-ri.de>\n#\n\nimport numpy as np\nimport pylab\nimport os\nimport cv2\n\ndef getGroundTruth(fileNameGT):\n    \'\'\'\n    Returns the ground truth maps for roadArea and the validArea \n    :param fileNameGT:\n    \'\'\'\n    # Read GT\n    assert os.path.isfile(fileNameGT), \'Cannot find: %s\' % fileNameGT\n    full_gt = cv2.imread(fileNameGT, cv2.CV_LOAD_IMAGE_UNCHANGED)\n    #attention: OpenCV reads in as BGR, so first channel has Blue / road GT\n    roadArea =  full_gt[:,:,0] > 0\n    validArea = full_gt[:,:,2] > 0\n\n    return roadArea, validArea\n\n\ndef overlayImageWithConfidence(in_image, conf, vis_channel = 1, threshold = 0.5):\n    \'\'\'\n    \n    :param in_image:\n    :param conf:\n    :param vis_channel:\n    :param threshold:\n    \'\'\'\n    if in_image.dtype == \'uint8\':\n        visImage = in_image.copy().astype(\'f4\')/255\n    else:\n        visImage = in_image.copy()\n    \n    channelPart = visImage[:, :, vis_channel] * (conf > threshold) - conf\n    channelPart[channelPart < 0] = 0\n    visImage[:, :, vis_channel] = visImage[:, :, vis_channel] * (conf <= threshold) + (conf > threshold) * conf + channelPart\n    return visImage\n\ndef evalExp(gtBin, cur_prob, thres, validMap = None, validArea=None):\n    \'\'\'\n    Does the basic pixel based evaluation!\n    :param gtBin:\n    :param cur_prob:\n    :param thres:\n    :param validMap:\n    \'\'\'\n\n    assert len(cur_prob.shape) == 2, \'Wrong size of input prob map\'\n    assert len(gtBin.shape) == 2, \'Wrong size of input prob map\'\n    thresInf = np.concatenate(([-np.Inf], thres, [np.Inf]))\n    \n    #Merge validMap with validArea\n    if validMap!=None:\n        if validArea!=None:\n            validMap = (validMap == True) & (validArea == True)\n    elif validArea!=None:\n        validMap=validArea\n\n    # histogram of false negatives\n    if validMap!=None:\n        fnArray = cur_prob[(gtBin == True) & (validMap == True)]\n    else:\n        fnArray = cur_prob[(gtBin == True)]\n    fnHist = np.histogram(fnArray,bins=thresInf)[0]\n    fnCum = np.cumsum(fnHist)\n    FN = fnCum[0:0+len(thres)];\n    \n    if validMap!=None:\n        fpArray = cur_prob[(gtBin == False) & (validMap == True)]\n    else:\n        fpArray = cur_prob[(gtBin == False)]\n    \n    fpHist  = np.histogram(fpArray, bins=thresInf)[0]\n    fpCum = np.flipud(np.cumsum(np.flipud(fpHist)))\n    FP = fpCum[1:1+len(thres)]\n\n    # count labels and protos\n    #posNum = fnArray.shape[0]\n    #negNum = fpArray.shape[0]\n    if validMap!=None:\n        posNum = np.sum((gtBin == True) & (validMap == True))\n        negNum = np.sum((gtBin == False) & (validMap == True))\n    else:\n        posNum = np.sum(gtBin == True)\n        negNum = np.sum(gtBin == False)\n    return FN, FP, posNum, negNum\n\ndef pxEval_maximizeFMeasure(totalPosNum, totalNegNum, totalFN, totalFP, thresh = None):\n    \'\'\'\n\n    @param totalPosNum: scalar\n    @param totalNegNum: scalar\n    @param totalFN: vector\n    @param totalFP: vector\n    @param thresh: vector\n    \'\'\'\n\n    #Calc missing stuff\n    totalTP = totalPosNum - totalFN\n    totalTN = totalNegNum - totalFP\n\n\n    valid = (totalTP>=0) & (totalTN>=0)\n    assert valid.all(), \'Detected invalid elements in eval\'\n\n    recall = totalTP / float( totalPosNum )\n    precision =  totalTP / (totalTP + totalFP + 1e-10)\n    \n    selector_invalid = (recall==0) & (precision==0)\n    recall = recall[~selector_invalid]\n    precision = precision[~selector_invalid]\n        \n    maxValidIndex = len(precision)\n    \n    #Pascal VOC average precision\n    AvgPrec = 0\n    counter = 0\n    for i in np.arange(0,1.1,0.1):\n        ind = np.where(recall>=i)\n        if ind == None:\n            continue\n        pmax = max(precision[ind])\n        AvgPrec += pmax\n        counter += 1\n    AvgPrec = AvgPrec/counter\n    \n    \n    # F-measure operation point\n    beta = 1.0\n    betasq = beta**2\n    F = (1 + betasq) * (precision * recall)/((betasq * precision) + recall + 1e-10)\n    index = F.argmax()\n    MaxF= F[index]\n    \n    recall_bst = recall[index]\n    precision_bst =  precision[index]\n\n    TP = totalTP[index]\n    TN = totalTN[index]\n    FP = totalFP[index]\n    FN = totalFN[index]\n    valuesMaxF = np.zeros((1,4),\'u4\')\n    valuesMaxF[0,0] = TP\n    valuesMaxF[0,1] = TN\n    valuesMaxF[0,2] = FP\n    valuesMaxF[0,3] = FN\n\n    #ACC = (totalTP+ totalTN)/(totalPosNum+totalNegNum)\n    import ipdb\n    ipdb.set_trace()\n    prob_eval_scores  = calcEvalMeasures(valuesMaxF)\n    prob_eval_scores[\'AvgPrec\'] = AvgPrec\n    prob_eval_scores[\'MaxF\'] = MaxF\n\n    #prob_eval_scores[\'totalFN\'] = totalFN\n    #prob_eval_scores[\'totalFP\'] = totalFP\n    prob_eval_scores[\'totalPosNum\'] = totalPosNum\n    prob_eval_scores[\'totalNegNum\'] = totalNegNum\n\n    prob_eval_scores[\'precision\'] = precisioncalc\n    prob_eval_scores[\'recall\'] = recall\n    #prob_eval_scores[\'precision_bst\'] = precision_bst\n    #prob_eval_scores[\'recall_bst\'] = recall_bst\n    prob_eval_scores[\'thresh\'] = thresh\n    if thresh != None:\n        BestThresh= thresh[index]\n        prob_eval_scores[\'BestThresh\'] = BestThresh\n\n    #return a dict\n    return prob_eval_scores\n\n\n\ndef calcEvalMeasures(evalDict, tag  = \'_wp\'):\n    \'\'\'\n    \n    :param evalDict:\n    :param tag:\n    \'\'\'\n    # array mode!\n    TP = evalDict[:,0].astype(\'f4\')\n    TN = evalDict[:,1].astype(\'f4\')\n    FP = evalDict[:,2].astype(\'f4\')\n    FN = evalDict[:,3].astype(\'f4\')\n    Q = TP / (TP + FP + FN)\n    P = TP + FN\n    N = TN + FP\n    TPR = TP / P\n    FPR = FP / N\n    FNR = FN / P\n    TNR = TN / N\n    A = (TP + TN) / (P + N)\n    precision = TP / (TP + FP)\n    recall = TP / P\n    #numSamples = TP + TN + FP + FN\n    correct_rate = A\n\n    # F-measure\n    #beta = 1.0\n    #betasq = beta**2\n    #F_max = (1 + betasq) * (precision * recall)/((betasq * precision) + recall + 1e-10)\n    \n    \n    outDict =dict()\n\n    outDict[\'TP\'+ tag] = TP\n    outDict[\'FP\'+ tag] = FP\n    outDict[\'FN\'+ tag] = FN\n    outDict[\'TN\'+ tag] = TN\n    outDict[\'Q\'+ tag] = Q\n    outDict[\'A\'+ tag] = A\n    outDict[\'TPR\'+ tag] = TPR\n    outDict[\'FPR\'+ tag] = FPR\n    outDict[\'FNR\'+ tag] = FNR\n    outDict[\'PRE\'+ tag] = precision\n    outDict[\'REC\'+ tag] = recall\n    outDict[\'correct_rate\'+ tag] = correct_rate\n    return outDict\n\ndef setFigLinesBW(fig):\n    """"""\n    Take each axes in the figure, and for each line in the axes, make the\n    line viewable in black and white.\n    """"""\n    for ax in fig.get_axes():\n        setAxLinesBW(ax)\n        \ndef setAxLinesBW(ax):\n    """"""\n    Take each Line2D in the axes, ax, and convert the line style to be\n    suitable for black and white viewing.\n    """"""\n    MARKERSIZE = 3\n\n#     COLORMAP = {\n#         \'r\': {\'marker\': None, \'dash\': (None,None)},\n#         \'g\': {\'marker\': None, \'dash\': [5,2]},\n#         \'m\': {\'marker\': None, \'dash\': [11,3]},\n#         \'b\': {\'marker\': None, \'dash\': [6,3,2,3]},\n#         \'c\': {\'marker\': None, \'dash\': [1,3]},\n#         \'y\': {\'marker\': None, \'dash\': [5,3,1,2,1,10]},\n#         \'k\': {\'marker\': \'o\', \'dash\': (None,None)} #[1,2,1,10]}\n#         }\n    COLORMAP = {\n        \'r\': {\'marker\': ""None"", \'dash\': (""None"",""None"")},\n        \'g\': {\'marker\': ""None"", \'dash\': [5,2]},\n        \'m\': {\'marker\': ""None"", \'dash\': [11,3]},\n        \'b\': {\'marker\': ""None"", \'dash\': [6,3,2,3]},\n        \'c\': {\'marker\': ""None"", \'dash\': [1,3]},\n        \'y\': {\'marker\': ""None"", \'dash\': [5,3,1,2,1,10]},\n        \'k\': {\'marker\': \'o\', \'dash\': (""None"",""None"")} #[1,2,1,10]}\n        }\n\n    for line in ax.get_lines():\n        origColor = line.get_color()\n        #line.set_color(\'black\')\n        line.set_dashes(COLORMAP[origColor][\'dash\'])\n        line.set_marker(COLORMAP[origColor][\'marker\'])\n        line.set_markersize(MARKERSIZE)\n        \ndef plotPrecisionRecall(precision, recall, outFileName, Fig=None, drawCol=1, textLabel = None, title = None, fontsize1 = 24, fontsize2 = 20, linewidth = 3):\n    \'\'\'\n    \n    :param precision:\n    :param recall:\n    :param outFileName:\n    :param Fig:\n    :param drawCol:\n    :param textLabel:\n    :param fontsize1:\n    :param fontsize2:\n    :param linewidth:\n    \'\'\'\n                      \n    clearFig = False  \n           \n    if Fig == None:\n        Fig = pylab.figure()\n        clearFig = True\n        \n    #tableString = \'Algo avgprec Fmax prec recall accuracy fpr Q(TonITS)\\n\'\n    linecol = [\'g\',\'m\',\'b\',\'c\']\n    #if we are evaluating SP, then BL is available\n    #sectionName = \'Evaluation_\'+tag+\'PxProb\'\n    #fullEvalFile = os.path.join(eval_dir,evalName)\n    #Precision,Recall,evalString = readEvaluation(fullEvalFile,sectionName,AlgoLabel)\n\n    pylab.plot(100*recall, 100*precision, linewidth=linewidth, color=linecol[drawCol], label=textLabel)\n\n\n    #writing out PrecRecall curves as graphic\n    setFigLinesBW(Fig)\n    if textLabel!= None:\n        pylab.legend(loc=\'lower left\',prop={\'size\':fontsize2})\n    \n    if title!= None:\n        pylab.title(title, fontsize=fontsize1)\n        \n    #pylab.title(title,fontsize=24)\n    pylab.ylabel(\'PRECISION [%]\',fontsize=fontsize1)\n    pylab.xlabel(\'RECALL [%]\',fontsize=fontsize1)\n    \n    pylab.xlim(0,100)\n    pylab.xticks( [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n                      (\'0\',\'\',\'20\',\'\',\'40\',\'\',\'60\',\'\',\'80\',\'\',\'100\'), fontsize=fontsize2 )\n    pylab.ylim(0,100)\n    pylab.yticks( [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n                      (\'0\',\'\',\'20\',\'\',\'40\',\'\',\'60\',\'\',\'80\',\'\',\'100\'), fontsize=fontsize2 )\n    pylab.grid(True)\n   \n    # \n    if type(outFileName) != list:\n        pylab.savefig( outFileName )\n    else:\n        for outFn in outFileName:\n            pylab.savefig( outFn )\n    if clearFig:\n        pylab.close()\n        Fig.clear()\n   \n\n\ndef saveBEVImageWithAxes(data, outputname, cmap = None, xlabel = \'x [m]\', ylabel = \'z [m]\', rangeX = [-10, 10], rangeXpx = None, numDeltaX = 5, rangeZ = [7, 62], rangeZpx = None, numDeltaZ = 5, fontSize = 16):\n    \'\'\'\n    \n    :param data:\n    :param outputname:\n    :param cmap:\n    \'\'\'\n    aspect_ratio = float(data.shape[1])/data.shape[0]\n    fig = pylab.figure()\n    Scale = 8\n    # add +1 to get axis text\n    fig.set_size_inches(Scale*aspect_ratio+1,Scale*1)\n    ax = pylab.gca()\n    #ax.set_axis_off()\n    #fig.add_axes(ax)\n    if cmap != None:\n        pylab.set_cmap(cmap)\n    \n    #ax.imshow(data, interpolation=\'nearest\', aspect = \'normal\')\n    ax.imshow(data, interpolation=\'nearest\')\n    \n    if rangeXpx == None:\n        rangeXpx = (0, data.shape[1])\n    \n    if rangeZpx == None:\n        rangeZpx = (0, data.shape[0])\n        \n    modBev_plot(ax, rangeX, rangeXpx, numDeltaX, rangeZ, rangeZpx, numDeltaZ, fontSize, xlabel = xlabel, ylabel = ylabel)\n    #plt.savefig(outputname, bbox_inches=\'tight\', dpi = dpi)\n    pylab.savefig(outputname, dpi = data.shape[0]/Scale)\n    pylab.close()\n    fig.clear()\n    \ndef modBev_plot(ax, rangeX = [-10, 10 ], rangeXpx= [0, 400], numDeltaX = 5, rangeZ= [8,48 ], rangeZpx= [0, 800], numDeltaZ = 9, fontSize = None, xlabel = \'x [m]\', ylabel = \'z [m]\'):\n    \'\'\'\n\n    @param ax:\n    \'\'\'\n    #TODO: Configureabiltiy would be nice!\n    if fontSize==None:\n        fontSize = 8\n \n    ax.set_xlabel(xlabel, fontsize=fontSize)\n    ax.set_ylabel(ylabel, fontsize=fontSize)\n        \n    zTicksLabels_val = np.linspace(rangeZpx[0], rangeZpx[1], numDeltaZ)\n    ax.set_yticks(zTicksLabels_val)\n    #ax.set_yticks([0, 100, 200, 300, 400, 500, 600, 700, 800])\n    xTicksLabels_val = np.linspace(rangeXpx[0], rangeXpx[1], numDeltaX)\n    ax.set_xticks(xTicksLabels_val)\n    xTicksLabels_val = np.linspace(rangeX[0], rangeX[1], numDeltaX)\n    zTicksLabels = map(lambda x: str(int(x)), xTicksLabels_val)\n    ax.set_xticklabels(zTicksLabels,fontsize=fontSize)\n    zTicksLabels_val = np.linspace(rangeZ[1],rangeZ[0], numDeltaZ)\n    zTicksLabels = map(lambda x: str(int(x)), zTicksLabels_val)\n    ax.set_yticklabels(zTicksLabels,fontsize=fontSize)\n    \n '"
submodules/evaluation/kitti_devkit/seg_utils.py,0,"b'#!/usr/bin/env python\n#\n#  THE KITTI VISION BENCHMARK SUITE: ROAD BENCHMARK\n#\n#  Copyright (C) 2013\n#  Honda Research Institute Europe GmbH\n#  Carl-Legien-Str. 30\n#  63073 Offenbach/Main\n#  Germany\n#\n#  UNPUBLISHED PROPRIETARY MATERIAL.\n#  ALL RIGHTS RESERVED.\n#\n#  Authors: Tobias Kuehnl <tkuehnl@cor-lab.uni-bielefeld.de>\n#           Jannik Fritsch <jannik.fritsch@honda-ri.de>\n#\n\nimport numpy as np\n# import pylab\nimport matplotlib.cm as cm\nimport os\n# import cv2\n\ndef make_overlay(image, gt_prob):\n\n    mycm = cm.get_cmap(\'bwr\')\n\n    overimage = mycm(gt_prob, bytes=True)\n    output = 0.4*overimage[:,:,0:3] + 0.6*image\n\n    return output\n\n\ndef overlayImageWithConfidence(in_image, conf, vis_channel = 1, threshold = 0.5):\n    \'\'\'\n    \n    :param in_image:\n    :param conf:\n    :param vis_channel:\n    :param threshold:\n    \'\'\'\n    if in_image.dtype == \'uint8\':\n        visImage = in_image.copy().astype(\'f4\')/255\n    else:\n        visImage = in_image.copy()\n    \n    channelPart = visImage[:, :, vis_channel] * (conf > threshold) - conf\n    channelPart[channelPart < 0] = 0\n    visImage[:, :, vis_channel] = 0.5*visImage[:, :, vis_channel] + 255*conf\n    return visImage\n\ndef evalExp(gtBin, cur_prob, thres, validMap = None, validArea=None):\n    \'\'\'\n    Does the basic pixel based evaluation!\n    :param gtBin:\n    :param cur_prob:\n    :param thres:\n    :param validMap:\n    \'\'\'\n\n    assert len(cur_prob.shape) == 2, \'Wrong size of input prob map\'\n    assert len(gtBin.shape) == 2, \'Wrong size of input prob map\'\n    thresInf = np.concatenate(([-np.Inf], thres, [np.Inf]))\n    \n    #Merge validMap with validArea\n    if validMap is not None:\n        if validArea is not None:\n            validMap = (validMap == True) & (validArea == True)\n    elif validArea is not None:\n        validMap=validArea\n\n    # histogram of false negatives\n    if validMap is not None:\n        fnArray = cur_prob[(gtBin == True) & (validMap == True)]\n    else:\n        fnArray = cur_prob[(gtBin == True)]\n    fnHist = np.histogram(fnArray,bins=thresInf)[0]\n    fnCum = np.cumsum(fnHist)\n    FN = fnCum[0:0+len(thres)];\n    \n    if validMap is not None:\n        fpArray = cur_prob[(gtBin == False) & (validMap == True)]\n    else:\n        fpArray = cur_prob[(gtBin == False)]\n    \n    fpHist  = np.histogram(fpArray, bins=thresInf)[0]\n    fpCum = np.flipud(np.cumsum(np.flipud(fpHist)))\n    FP = fpCum[1:1+len(thres)]\n\n    # count labels and protos\n    #posNum = fnArray.shape[0]\n    #negNum = fpArray.shape[0]\n    if validMap is not None:\n        posNum = np.sum((gtBin == True) & (validMap == True))\n        negNum = np.sum((gtBin == False) & (validMap == True))\n    else:\n        posNum = np.sum(gtBin == True)\n        negNum = np.sum(gtBin == False)\n    return FN, FP, posNum, negNum\n\ndef pxEval_maximizeFMeasure(totalPosNum, totalNegNum, totalFN, totalFP, thresh = None):\n    \'\'\'\n\n    @param totalPosNum: scalar\n    @param totalNegNum: scalar\n    @param totalFN: vector\n    @param totalFP: vector\n    @param thresh: vector\n    \'\'\'\n\n    #Calc missing stuff\n    totalTP = totalPosNum - totalFN\n    totalTN = totalNegNum - totalFP\n\n\n    valid = (totalTP>=0) & (totalTN>=0)\n    assert valid.all(), \'Detected invalid elements in eval\'\n\n    recall = totalTP / float( totalPosNum )\n    TNR    = totalTN / float( totalNegNum )\n    precision =  totalTP / (totalTP + totalFP + 1e-10)\n\n    accuracy = (totalTP + totalTN) / (float( totalPosNum ) + float( totalNegNum ))\n    \n    selector_invalid = (recall==0) & (precision==0)\n    recall = recall[~selector_invalid]\n    precision = precision[~selector_invalid]\n        \n    maxValidIndex = len(precision)\n    \n    #Pascal VOC average precision\n    AvgPrec = 0\n    counter = 0\n    for i in np.arange(0,1.1,0.1):\n        ind = np.where(recall>=i)\n        if ind == None:\n            continue\n        pmax = max(precision[ind])\n        AvgPrec += pmax\n        counter += 1\n    AvgPrec = AvgPrec/counter\n    \n    \n    # F-measure operation point\n    beta = 1.0\n    betasq = beta**2\n    F = (1 + betasq) * (precision * recall)/((betasq * precision) + recall + 1e-10)\n    index = F.argmax()\n    MaxF= F[index]\n    \n    recall_bst = recall[index]\n    precision_bst =  precision[index]\n\n    TP = totalTP[index]\n    TN = totalTN[index]\n    FP = totalFP[index]\n    FN = totalFN[index]\n    valuesMaxF = np.zeros((1,4),\'u4\')\n    valuesMaxF[0,0] = TP\n    valuesMaxF[0,1] = TN\n    valuesMaxF[0,2] = FP\n    valuesMaxF[0,3] = FN\n\n    #ACC = (totalTP+ totalTN)/(totalPosNum+totalNegNum)\n    prob_eval_scores  = calcEvalMeasures(valuesMaxF)\n    prob_eval_scores[\'AvgPrec\'] = AvgPrec\n    prob_eval_scores[\'MaxF\'] = MaxF\n    prob_eval_scores[\'accuracy\'] = accuracy\n\n    #prob_eval_scores[\'totalFN\'] = totalFN\n    #prob_eval_scores[\'totalFP\'] = totalFP\n    prob_eval_scores[\'totalPosNum\'] = totalPosNum\n    prob_eval_scores[\'totalNegNum\'] = totalNegNum\n\n    prob_eval_scores[\'precision\'] = precision\n    prob_eval_scores[\'recall\'] = recall\n    prob_eval_scores[\'TNR\'] = TNR\n    #prob_eval_scores[\'precision_bst\'] = precision_bst\n    #prob_eval_scores[\'recall_bst\'] = recall_bst\n    prob_eval_scores[\'thresh\'] = thresh\n    if thresh is not None:\n        BestThresh= thresh[index]\n        prob_eval_scores[\'BestThresh\'] = BestThresh\n\n    #return a dict\n    return prob_eval_scores\n\n\n\ndef calcEvalMeasures(evalDict, tag  = \'_wp\'):\n    \'\'\'\n    \n    :param evalDict:\n    :param tag:\n    \'\'\'\n    # array mode!\n    TP = evalDict[:,0].astype(\'f4\')\n    TN = evalDict[:,1].astype(\'f4\')\n    FP = evalDict[:,2].astype(\'f4\')\n    FN = evalDict[:,3].astype(\'f4\')\n    Q = TP / (TP + FP + FN)\n    P = TP + FN\n    N = TN + FP\n    TPR = TP / P\n    FPR = FP / N\n    FNR = FN / P\n    TNR = TN / N\n    A = (TP + TN) / (P + N)\n    precision = TP / (TP + FP)\n    recall = TP / P\n    #numSamples = TP + TN + FP + FN\n    correct_rate = A\n\n    # F-measure\n    #beta = 1.0\n    #betasq = beta**2\n    #F_max = (1 + betasq) * (precision * recall)/((betasq * precision) + recall + 1e-10)\n    \n    \n    outDict =dict()\n\n    outDict[\'TP\'+ tag] = TP\n    outDict[\'FP\'+ tag] = FP\n    outDict[\'FN\'+ tag] = FN\n    outDict[\'TN\'+ tag] = TN\n    outDict[\'Q\'+ tag] = Q\n    outDict[\'A\'+ tag] = A\n    outDict[\'TPR\'+ tag] = TPR\n    outDict[\'FPR\'+ tag] = FPR\n    outDict[\'FNR\'+ tag] = FNR\n    outDict[\'PRE\'+ tag] = precision\n    outDict[\'REC\'+ tag] = recall\n    outDict[\'correct_rate\'+ tag] = correct_rate\n    return outDict\n\ndef setFigLinesBW(fig):\n    """"""\n    Take each axes in the figure, and for each line in the axes, make the\n    line viewable in black and white.\n    """"""\n    for ax in fig.get_axes():\n        setAxLinesBW(ax)\n        \ndef setAxLinesBW(ax):\n    """"""\n    Take each Line2D in the axes, ax, and convert the line style to be\n    suitable for black and white viewing.\n    """"""\n    MARKERSIZE = 3\n\n#     COLORMAP = {\n#         \'r\': {\'marker\': None, \'dash\': (None,None)},\n#         \'g\': {\'marker\': None, \'dash\': [5,2]},\n#         \'m\': {\'marker\': None, \'dash\': [11,3]},\n#         \'b\': {\'marker\': None, \'dash\': [6,3,2,3]},\n#         \'c\': {\'marker\': None, \'dash\': [1,3]},\n#         \'y\': {\'marker\': None, \'dash\': [5,3,1,2,1,10]},\n#         \'k\': {\'marker\': \'o\', \'dash\': (None,None)} #[1,2,1,10]}\n#         }\n    COLORMAP = {\n        \'r\': {\'marker\': ""None"", \'dash\': (""None"",""None"")},\n        \'g\': {\'marker\': ""None"", \'dash\': [5,2]},\n        \'m\': {\'marker\': ""None"", \'dash\': [11,3]},\n        \'b\': {\'marker\': ""None"", \'dash\': [6,3,2,3]},\n        \'c\': {\'marker\': ""None"", \'dash\': [1,3]},\n        \'y\': {\'marker\': ""None"", \'dash\': [5,3,1,2,1,10]},\n        \'k\': {\'marker\': \'o\', \'dash\': (""None"",""None"")} #[1,2,1,10]}\n        }\n\n    for line in ax.get_lines():\n        origColor = line.get_color()\n        #line.set_color(\'black\')\n        line.set_dashes(COLORMAP[origColor][\'dash\'])\n        line.set_marker(COLORMAP[origColor][\'marker\'])\n        line.set_markersize(MARKERSIZE)\n        \ndef plotPrecisionRecall(precision, recall, outFileName, Fig=None, drawCol=1, textLabel = None, title = None, fontsize1 = 24, fontsize2 = 20, linewidth = 3):\n    \'\'\'\n    \n    :param precision:\n    :param recall:\n    :param outFileName:\n    :param Fig:\n    :param drawCol:\n    :param textLabel:\n    :param fontsize1:\n    :param fontsize2:\n    :param linewidth:\n    \'\'\'\n                      \n    clearFig = False  \n           \n    if Fig == None:\n        Fig = pylab.figure()\n        clearFig = True\n        \n    #tableString = \'Algo avgprec Fmax prec recall accuracy fpr Q(TonITS)\\n\'\n    linecol = [\'g\',\'m\',\'b\',\'c\']\n    #if we are evaluating SP, then BL is available\n    #sectionName = \'Evaluation_\'+tag+\'PxProb\'\n    #fullEvalFile = os.path.join(eval_dir,evalName)\n    #Precision,Recall,evalString = readEvaluation(fullEvalFile,sectionName,AlgoLabel)\n\n    pylab.plot(100*recall, 100*precision, linewidth=linewidth, color=linecol[drawCol], label=textLabel)\n\n\n    #writing out PrecRecall curves as graphic\n    setFigLinesBW(Fig)\n    if textLabel!= None:\n        pylab.legend(loc=\'lower left\',prop={\'size\':fontsize2})\n    \n    if title!= None:\n        pylab.title(title, fontsize=fontsize1)\n        \n    #pylab.title(title,fontsize=24)\n    pylab.ylabel(\'PRECISION [%]\',fontsize=fontsize1)\n    pylab.xlabel(\'RECALL [%]\',fontsize=fontsize1)\n    \n    pylab.xlim(0,100)\n    pylab.xticks( [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n                      (\'0\',\'\',\'20\',\'\',\'40\',\'\',\'60\',\'\',\'80\',\'\',\'100\'), fontsize=fontsize2 )\n    pylab.ylim(0,100)\n    pylab.yticks( [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n                      (\'0\',\'\',\'20\',\'\',\'40\',\'\',\'60\',\'\',\'80\',\'\',\'100\'), fontsize=fontsize2 )\n    pylab.grid(True)\n   \n    # \n    if type(outFileName) != list:\n        pylab.savefig( outFileName )\n    else:\n        for outFn in outFileName:\n            pylab.savefig( outFn )\n    if clearFig:\n        pylab.close()\n        Fig.clear()\n   \n\n\ndef saveBEVImageWithAxes(data, outputname, cmap = None, xlabel = \'x [m]\', ylabel = \'z [m]\', rangeX = [-10, 10], rangeXpx = None, numDeltaX = 5, rangeZ = [7, 62], rangeZpx = None, numDeltaZ = 5, fontSize = 16):\n    \'\'\'\n    \n    :param data:\n    :param outputname:\n    :param cmap:\n    \'\'\'\n    aspect_ratio = float(data.shape[1])/data.shape[0]\n    fig = pylab.figure()\n    Scale = 8\n    # add +1 to get axis text\n    fig.set_size_inches(Scale*aspect_ratio+1,Scale*1)\n    ax = pylab.gca()\n    #ax.set_axis_off()\n    #fig.add_axes(ax)\n    if cmap != None:\n        pylab.set_cmap(cmap)\n    \n    #ax.imshow(data, interpolation=\'nearest\', aspect = \'normal\')\n    ax.imshow(data, interpolation=\'nearest\')\n    \n    if rangeXpx == None:\n        rangeXpx = (0, data.shape[1])\n    \n    if rangeZpx == None:\n        rangeZpx = (0, data.shape[0])\n        \n    modBev_plot(ax, rangeX, rangeXpx, numDeltaX, rangeZ, rangeZpx, numDeltaZ, fontSize, xlabel = xlabel, ylabel = ylabel)\n    #plt.savefig(outputname, bbox_inches=\'tight\', dpi = dpi)\n    pylab.savefig(outputname, dpi = data.shape[0]/Scale)\n    pylab.close()\n    fig.clear()\n    \ndef modBev_plot(ax, rangeX = [-10, 10 ], rangeXpx= [0, 400], numDeltaX = 5, rangeZ= [8,48 ], rangeZpx= [0, 800], numDeltaZ = 9, fontSize = None, xlabel = \'x [m]\', ylabel = \'z [m]\'):\n    \'\'\'\n\n    @param ax:\n    \'\'\'\n    #TODO: Configureabiltiy would be nice!\n    if fontSize==None:\n        fontSize = 8\n \n    ax.set_xlabel(xlabel, fontsize=fontSize)\n    ax.set_ylabel(ylabel, fontsize=fontSize)\n        \n    zTicksLabels_val = np.linspace(rangeZpx[0], rangeZpx[1], numDeltaZ)\n    ax.set_yticks(zTicksLabels_val)\n    #ax.set_yticks([0, 100, 200, 300, 400, 500, 600, 700, 800])\n    xTicksLabels_val = np.linspace(rangeXpx[0], rangeXpx[1], numDeltaX)\n    ax.set_xticks(xTicksLabels_val)\n    xTicksLabels_val = np.linspace(rangeX[0], rangeX[1], numDeltaX)\n    zTicksLabels = map(lambda x: str(int(x)), xTicksLabels_val)\n    ax.set_xticklabels(zTicksLabels,fontsize=fontSize)\n    zTicksLabels_val = np.linspace(rangeZ[1],rangeZ[0], numDeltaZ)\n    zTicksLabels = map(lambda x: str(int(x)), zTicksLabels_val)\n    ax.set_yticklabels(zTicksLabels,fontsize=fontSize)\n    \n '"
