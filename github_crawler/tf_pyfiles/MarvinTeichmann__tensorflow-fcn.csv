file_path,api_count,code
__init__.py,0,b''
fcn16_vgg.py,62,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nfrom math import ceil\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nVGG_MEAN = [103.939, 116.779, 123.68]\n\n\nclass FCN16VGG:\n\n    def __init__(self, vgg16_npy_path=None):\n        if vgg16_npy_path is None:\n            path = sys.modules[self.__class__.__module__].__file__\n            # print path\n            path = os.path.abspath(os.path.join(path, os.pardir))\n            # print path\n            path = os.path.join(path, ""vgg16.npy"")\n            vgg16_npy_path = path\n            logging.info(""Load npy file from \'%s\'."", vgg16_npy_path)\n        if not os.path.isfile(vgg16_npy_path):\n            logging.error((""File \'%s\' not found. Download it from ""\n                           ""ftp://mi.eng.cam.ac.uk/pub/mttt2/""\n                           ""models/vgg16.npy""), vgg16_npy_path)\n            sys.exit(1)\n\n        self.data_dict = np.load(vgg16_npy_path, encoding=\'latin1\').item()\n        self.wd = 5e-4\n        print(""npy file loaded"")\n\n    def build(self, rgb, train=False, num_classes=20, random_init_fc8=False,\n              debug=False):\n        """"""\n        Build the VGG model using loaded weights\n        Parameters\n        ----------\n        rgb: image batch tensor\n            Image in rgb shap. Scaled to Intervall [0, 255]\n        train: bool\n            Whether to build train or inference graph\n        num_classes: int\n            How many classes should be predicted (by fc8)\n        random_init_fc8 : bool\n            Whether to initialize fc8 layer randomly.\n            Finetuning is required in this case.\n        debug: bool\n            Whether to print additional Debug Information.\n        """"""\n        # Convert RGB to BGR\n\n        with tf.name_scope(\'Processing\'):\n            # rgb = tf.image.convert_image_dtype(rgb, tf.float32)\n            red, green, blue = tf.split(rgb, 3, 3)\n            # assert red.get_shape().as_list()[1:] == [224, 224, 1]\n            # assert green.get_shape().as_list()[1:] == [224, 224, 1]\n            # assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n            bgr = tf.concat([\n                blue - VGG_MEAN[0],\n                green - VGG_MEAN[1],\n                red - VGG_MEAN[2]], axis=3)\n\n            if debug:\n                bgr = tf.Print(bgr, [tf.shape(bgr)],\n                               message=\'Shape of input image: \',\n                               summarize=4, first_n=1)\n\n        self.conv1_1 = self._conv_layer(bgr, ""conv1_1"")\n        self.conv1_2 = self._conv_layer(self.conv1_1, ""conv1_2"")\n        self.pool1 = self._max_pool(self.conv1_2, \'pool1\', debug)\n\n        self.conv2_1 = self._conv_layer(self.pool1, ""conv2_1"")\n        self.conv2_2 = self._conv_layer(self.conv2_1, ""conv2_2"")\n        self.pool2 = self._max_pool(self.conv2_2, \'pool2\', debug)\n\n        self.conv3_1 = self._conv_layer(self.pool2, ""conv3_1"")\n        self.conv3_2 = self._conv_layer(self.conv3_1, ""conv3_2"")\n        self.conv3_3 = self._conv_layer(self.conv3_2, ""conv3_3"")\n        self.pool3 = self._max_pool(self.conv3_3, \'pool3\', debug)\n\n        self.conv4_1 = self._conv_layer(self.pool3, ""conv4_1"")\n        self.conv4_2 = self._conv_layer(self.conv4_1, ""conv4_2"")\n        self.conv4_3 = self._conv_layer(self.conv4_2, ""conv4_3"")\n        self.pool4 = self._max_pool(self.conv4_3, \'pool4\', debug)\n\n        self.conv5_1 = self._conv_layer(self.pool4, ""conv5_1"")\n        self.conv5_2 = self._conv_layer(self.conv5_1, ""conv5_2"")\n\n        self.conv5_3 = self._conv_layer(self.conv5_2, ""conv5_3"")\n        self.pool5 = self._max_pool(self.conv5_3, \'pool5\', debug)\n\n        self.fc6 = self._fc_layer(self.pool5, ""fc6"")\n\n        if train:\n            self.fc6 = tf.nn.dropout(self.fc6, 0.5)\n\n        self.fc7 = self._fc_layer(self.fc6, ""fc7"")\n        if train:\n            self.fc7 = tf.nn.dropout(self.fc7, 0.5)\n\n        if random_init_fc8:\n            self.score_fr = self._score_layer(self.fc7, ""score_fr"",\n                                              num_classes)\n        else:\n            self.score_fr = self._fc_layer(self.fc7, ""score_fr"",\n                                           num_classes=num_classes,\n                                           relu=False)\n\n        self.pred = tf.argmax(self.score_fr, dimension=3)\n\n        self.upscore2 = self._upscore_layer(self.score_fr,\n                                            shape=tf.shape(self.pool4),\n                                            num_classes=num_classes,\n                                            debug=debug, name=\'upscore2\',\n                                            ksize=4, stride=2)\n\n        self.score_pool4 = self._score_layer(self.pool4, ""score_pool4"",\n                                             num_classes=num_classes)\n\n        self.fuse_pool4 = tf.add(self.upscore2, self.score_pool4)\n\n        self.upscore32 = self._upscore_layer(self.fuse_pool4,\n                                             shape=tf.shape(bgr),\n                                             num_classes=num_classes,\n                                             debug=debug, name=\'upscore32\',\n                                             ksize=32, stride=16)\n\n        self.pred_up = tf.argmax(self.upscore32, dimension=3)\n\n    def _max_pool(self, bottom, name, debug):\n        pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                              padding=\'SAME\', name=name)\n\n        if debug:\n            pool = tf.Print(pool, [tf.shape(pool)],\n                            message=\'Shape of %s\' % name,\n                            summarize=4, first_n=1)\n        return pool\n\n    def _conv_layer(self, bottom, name):\n        with tf.variable_scope(name) as scope:\n            filt = self.get_conv_filter(name)\n            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=\'SAME\')\n\n            conv_biases = self.get_bias(name)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            relu = tf.nn.relu(bias)\n            # Add summary to Tensorboard\n            _activation_summary(relu)\n            return relu\n\n    def _fc_layer(self, bottom, name, num_classes=None,\n                  relu=True, debug=False):\n        with tf.variable_scope(name) as scope:\n            shape = bottom.get_shape().as_list()\n\n            if name == \'fc6\':\n                filt = self.get_fc_weight_reshape(name, [7, 7, 512, 4096])\n            elif name == \'score_fr\':\n                name = \'fc8\'  # Name of score_fr layer in VGG Model\n                filt = self.get_fc_weight_reshape(name, [1, 1, 4096, 1000],\n                                                  num_classes=num_classes)\n            else:\n                filt = self.get_fc_weight_reshape(name, [1, 1, 4096, 4096])\n            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=\'SAME\')\n            conv_biases = self.get_bias(name, num_classes=num_classes)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            if relu:\n                bias = tf.nn.relu(bias)\n            _activation_summary(bias)\n\n            if debug:\n                bias = tf.Print(bias, [tf.shape(bias)],\n                                message=\'Shape of %s\' % name,\n                                summarize=4, first_n=1)\n            return bias\n\n    def _score_layer(self, bottom, name, num_classes):\n        with tf.variable_scope(name) as scope:\n            # get number of input channels\n            in_features = bottom.get_shape()[3].value\n            shape = [1, 1, in_features, num_classes]\n            # He initialization Sheme\n            if name == ""score_fr"":\n                num_input = in_features\n                stddev = (2 / num_input)**0.5\n            elif name == ""score_pool4"":\n                stddev = 0.001\n            # Apply convolution\n            w_decay = self.wd\n            weights = self._variable_with_weight_decay(shape, stddev, w_decay)\n            conv = tf.nn.conv2d(bottom, weights, [1, 1, 1, 1], padding=\'SAME\')\n            # Apply bias\n            conv_biases = self._bias_variable([num_classes], constant=0.0)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            _activation_summary(bias)\n\n            return bias\n\n    def _upscore_layer(self, bottom, shape,\n                       num_classes, name, debug,\n                       ksize=4, stride=2):\n        strides = [1, stride, stride, 1]\n        with tf.variable_scope(name):\n            in_features = bottom.get_shape()[3].value\n\n            if shape is None:\n                # Compute shape out of Bottom\n                in_shape = tf.shape(bottom)\n\n                h = ((in_shape[1] - 1) * stride) + 1\n                w = ((in_shape[2] - 1) * stride) + 1\n                new_shape = [in_shape[0], h, w, num_classes]\n            else:\n                new_shape = [shape[0], shape[1], shape[2], num_classes]\n            output_shape = tf.stack(new_shape)\n\n            logging.debug(""Layer: %s, Fan-in: %d"" % (name, in_features))\n            f_shape = [ksize, ksize, num_classes, in_features]\n\n            # create\n            num_input = ksize * ksize * in_features / stride\n            stddev = (2 / num_input)**0.5\n\n            weights = self.get_deconv_filter(f_shape)\n            deconv = tf.nn.conv2d_transpose(bottom, weights, output_shape,\n                                            strides=strides, padding=\'SAME\')\n\n            if debug:\n                deconv = tf.Print(deconv, [tf.shape(deconv)],\n                                  message=\'Shape of %s\' % name,\n                                  summarize=4, first_n=1)\n\n        _activation_summary(deconv)\n        return deconv\n\n    def get_deconv_filter(self, f_shape):\n        width = f_shape[0]\n        height = f_shape[1]\n        f = ceil(width/2.0)\n        c = (2 * f - 1 - f % 2) / (2.0 * f)\n        bilinear = np.zeros([f_shape[0], f_shape[1]])\n        for x in range(width):\n            for y in range(height):\n                value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n                bilinear[x, y] = value\n        weights = np.zeros(f_shape)\n        for i in range(f_shape[2]):\n            weights[:, :, i, i] = bilinear\n\n        init = tf.constant_initializer(value=weights,\n                                       dtype=tf.float32)\n        return tf.get_variable(name=""up_filter"", initializer=init,\n                               shape=weights.shape)\n\n    def get_conv_filter(self, name):\n        init = tf.constant_initializer(value=self.data_dict[name][0],\n                                       dtype=tf.float32)\n        shape = self.data_dict[name][0].shape\n        print(\'Layer name: %s\' % name)\n        print(\'Layer shape: %s\' % str(shape))\n        var = tf.get_variable(name=""filter"", initializer=init, shape=shape)\n        if not tf.get_variable_scope().reuse:\n            weight_decay = tf.multiply(tf.nn.l2_loss(var), self.wd,\n                                       name=\'weight_loss\')\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 weight_decay)\n        return var\n\n    def get_bias(self, name, num_classes=None):\n        bias_wights = self.data_dict[name][1]\n        shape = self.data_dict[name][1].shape\n        if name == \'fc8\':\n            bias_wights = self._bias_reshape(bias_wights, shape[0],\n                                             num_classes)\n            shape = [num_classes]\n        init = tf.constant_initializer(value=bias_wights,\n                                       dtype=tf.float32)\n        return tf.get_variable(name=""biases"", initializer=init, shape=shape)\n\n    def get_fc_weight(self, name):\n        init = tf.constant_initializer(value=self.data_dict[name][0],\n                                       dtype=tf.float32)\n        shape = self.data_dict[name][0].shape\n        var = tf.get_variable(name=""weights"", initializer=init, shape=shape)\n        if not tf.get_variable_scope().reuse:\n            weight_decay = tf.multiply(tf.nn.l2_loss(var), self.wd,\n                                       name=\'weight_loss\')\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 weight_decay)\n        return var\n\n    def _bias_reshape(self, bweight, num_orig, num_new):\n        """""" Build bias weights for filter produces with `_summary_reshape`\n\n        """"""\n        n_averaged_elements = num_orig//num_new\n        avg_bweight = np.zeros(num_new)\n        for i in range(0, num_orig, n_averaged_elements):\n            start_idx = i\n            end_idx = start_idx + n_averaged_elements\n            avg_idx = start_idx//n_averaged_elements\n            if avg_idx == num_new:\n                break\n            avg_bweight[avg_idx] = np.mean(bweight[start_idx:end_idx])\n        return avg_bweight\n\n    def _summary_reshape(self, fweight, shape, num_new):\n        """""" Produce weights for a reduced fully-connected layer.\n\n        FC8 of VGG produces 1000 classes. Most semantic segmentation\n        task require much less classes. This reshapes the original weights\n        to be used in a fully-convolutional layer which produces num_new\n        classes. To archive this the average (mean) of n adjanced classes is\n        taken.\n\n        Consider reordering fweight, to perserve semantic meaning of the\n        weights.\n\n        Args:\n          fweight: original weights\n          shape: shape of the desired fully-convolutional layer\n          num_new: number of new classes\n\n\n        Returns:\n          Filter weights for `num_new` classes.\n        """"""\n        num_orig = shape[3]\n        shape[3] = num_new\n        assert(num_new < num_orig)\n        n_averaged_elements = num_orig//num_new\n        avg_fweight = np.zeros(shape)\n        for i in range(0, num_orig, n_averaged_elements):\n            start_idx = i\n            end_idx = start_idx + n_averaged_elements\n            avg_idx = start_idx//n_averaged_elements\n            if avg_idx == num_new:\n                break\n            avg_fweight[:, :, :, avg_idx] = np.mean(\n                fweight[:, :, :, start_idx:end_idx], axis=3)\n        return avg_fweight\n\n    def _variable_with_weight_decay(self, shape, stddev, wd):\n        """"""Helper to create an initialized Variable with weight decay.\n\n        Note that the Variable is initialized with a truncated normal\n        distribution.\n        A weight decay is added only if one is specified.\n\n        Args:\n          name: name of the variable\n          shape: list of ints\n          stddev: standard deviation of a truncated Gaussian\n          wd: add L2Loss weight decay multiplied by this float. If None, weight\n              decay is not added for this Variable.\n\n        Returns:\n          Variable Tensor\n        """"""\n\n        initializer = tf.truncated_normal_initializer(stddev=stddev)\n        var = tf.get_variable(\'weights\', shape=shape,\n                              initializer=initializer)\n\n        if wd and (not tf.get_variable_scope().reuse):\n            weight_decay = tf.multiply(\n                tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 weight_decay)\n        return var\n\n    def _bias_variable(self, shape, constant=0.0):\n        initializer = tf.constant_initializer(constant)\n        return tf.get_variable(name=\'biases\', shape=shape,\n                               initializer=initializer)\n\n    def get_fc_weight_reshape(self, name, shape, num_classes=None):\n        print(\'Layer name: %s\' % name)\n        print(\'Layer shape: %s\' % shape)\n        weights = self.data_dict[name][0]\n        weights = weights.reshape(shape)\n        if num_classes is not None:\n            weights = self._summary_reshape(weights, shape,\n                                            num_new=num_classes)\n        init = tf.constant_initializer(value=weights,\n                                       dtype=tf.float32)\n        return tf.get_variable(name=""weights"", initializer=init, shape=shape)\n\n\ndef _activation_summary(x):\n    """"""Helper to create summaries for activations.\n\n    Creates a summary that provides a histogram of activations.\n    Creates a summary that measure the sparsity of activations.\n\n    Args:\n      x: Tensor\n    Returns:\n      nothing\n    """"""\n    # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on tensorboard.\n    tensor_name = x.op.name\n    # tensor_name = re.sub(\'%s_[0-9]*/\' % TOWER_NAME, \'\', x.op.name)\n    tf.summary.histogram(tensor_name + \'/activations\', x)\n    tf.summary.scalar(tensor_name + \'/sparsity\', tf.nn.zero_fraction(x))\n'"
fcn32_vgg.py,59,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nfrom math import ceil\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nVGG_MEAN = [103.939, 116.779, 123.68]\n\n\nclass FCN32VGG:\n\n    def __init__(self, vgg16_npy_path=None):\n        if vgg16_npy_path is None:\n            path = sys.modules[self.__class__.__module__].__file__\n            # print path\n            path = os.path.abspath(os.path.join(path, os.pardir))\n            # print path\n            path = os.path.join(path, ""vgg16.npy"")\n            vgg16_npy_path = path\n            logging.info(""Load npy file from \'%s\'."", vgg16_npy_path)\n        if not os.path.isfile(vgg16_npy_path):\n            logging.error((""File \'%s\' not found. Download it from ""\n                           ""ftp://mi.eng.cam.ac.uk/pub/mttt2/""\n                           ""models/vgg16.npy""), vgg16_npy_path)\n            sys.exit(1)\n\n        self.data_dict = np.load(vgg16_npy_path, encoding=\'latin1\').item()\n        self.wd = 5e-4\n        print(""npy file loaded"")\n\n    def build(self, rgb, train=False, num_classes=20, random_init_fc8=False,\n              debug=False):\n        """"""\n        Build the VGG model using loaded weights\n        Parameters\n        ----------\n        rgb: image batch tensor\n            Image in rgb shap. Scaled to Intervall [0, 255]\n        train: bool\n            Whether to build train or inference graph\n        num_classes: int\n            How many classes should be predicted (by fc8)\n        random_init_fc8 : bool\n            Whether to initialize fc8 layer randomly.\n            Finetuning is required in this case.\n        debug: bool\n            Whether to print additional Debug Information.\n        """"""\n        # Convert RGB to BGR\n\n        with tf.name_scope(\'Processing\'):\n\n            red, green, blue = tf.split(rgb, 3, 3)\n            # assert red.get_shape().as_list()[1:] == [224, 224, 1]\n            # assert green.get_shape().as_list()[1:] == [224, 224, 1]\n            # assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n            bgr = tf.concat([\n                blue - VGG_MEAN[0],\n                green - VGG_MEAN[1],\n                red - VGG_MEAN[2],\n            ], 3)\n\n            if debug:\n                bgr = tf.Print(bgr, [tf.shape(bgr)],\n                               message=\'Shape of input image: \',\n                               summarize=4, first_n=1)\n\n        self.conv1_1 = self._conv_layer(bgr, ""conv1_1"")\n        self.conv1_2 = self._conv_layer(self.conv1_1, ""conv1_2"")\n        self.pool1 = self._max_pool(self.conv1_2, \'pool1\', debug)\n\n        self.conv2_1 = self._conv_layer(self.pool1, ""conv2_1"")\n        self.conv2_2 = self._conv_layer(self.conv2_1, ""conv2_2"")\n        self.pool2 = self._max_pool(self.conv2_2, \'pool2\', debug)\n\n        self.conv3_1 = self._conv_layer(self.pool2, ""conv3_1"")\n        self.conv3_2 = self._conv_layer(self.conv3_1, ""conv3_2"")\n        self.conv3_3 = self._conv_layer(self.conv3_2, ""conv3_3"")\n        self.pool3 = self._max_pool(self.conv3_3, \'pool3\', debug)\n\n        self.conv4_1 = self._conv_layer(self.pool3, ""conv4_1"")\n        self.conv4_2 = self._conv_layer(self.conv4_1, ""conv4_2"")\n        self.conv4_3 = self._conv_layer(self.conv4_2, ""conv4_3"")\n        self.pool4 = self._max_pool(self.conv4_3, \'pool4\', debug)\n\n        self.conv5_1 = self._conv_layer(self.pool4, ""conv5_1"")\n        self.conv5_2 = self._conv_layer(self.conv5_1, ""conv5_2"")\n        self.conv5_3 = self._conv_layer(self.conv5_2, ""conv5_3"")\n        self.pool5 = self._max_pool(self.conv5_3, \'pool5\', debug)\n\n        self.fc6 = self._fc_layer(self.pool5, ""fc6"")\n\n        if train:\n            self.fc6 = tf.nn.dropout(self.fc6, 0.5)\n\n        self.fc7 = self._fc_layer(self.fc6, ""fc7"")\n        if train:\n            self.fc7 = tf.nn.dropout(self.fc7, 0.5)\n\n        if random_init_fc8:\n            self.score_fr = self._score_layer(self.fc7, ""score_fr"",\n                                              num_classes)\n        else:\n            self.score_fr = self._fc_layer(self.fc7, ""score_fr"",\n                                           num_classes=num_classes,\n                                           relu=False)\n\n        self.pred = tf.argmax(self.score_fr, dimension=3)\n\n        self.upscore = self._upscore_layer(self.score_fr, shape=tf.shape(bgr),\n                                           num_classes=num_classes,\n                                           debug=debug,\n                                           name=\'up\', ksize=64, stride=32)\n\n        self.pred_up = tf.argmax(self.upscore, dimension=3)\n\n    def _max_pool(self, bottom, name, debug):\n        pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                              padding=\'SAME\', name=name)\n\n        if debug:\n            pool = tf.Print(pool, [tf.shape(pool)],\n                            message=\'Shape of %s\' % name,\n                            summarize=4, first_n=1)\n        return pool\n\n    def _conv_layer(self, bottom, name):\n        with tf.variable_scope(name) as scope:\n            filt = self.get_conv_filter(name)\n            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=\'SAME\')\n\n            conv_biases = self.get_bias(name)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            relu = tf.nn.relu(bias)\n            # Add summary to Tensorboard\n            _activation_summary(relu)\n            return relu\n\n    def _fc_layer(self, bottom, name, num_classes=None,\n                  relu=True, debug=False):\n        with tf.variable_scope(name) as scope:\n            shape = bottom.get_shape().as_list()\n\n            if name == \'fc6\':\n                filt = self.get_fc_weight_reshape(name, [7, 7, 512, 4096])\n            elif name == \'score_fr\':\n                name = \'fc8\'  # Name of score_fr layer in VGG Model\n                filt = self.get_fc_weight_reshape(name, [1, 1, 4096, 1000],\n                                                  num_classes=num_classes)\n            else:\n                filt = self.get_fc_weight_reshape(name, [1, 1, 4096, 4096])\n            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=\'SAME\')\n            conv_biases = self.get_bias(name, num_classes=num_classes)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            if relu:\n                bias = tf.nn.relu(bias)\n            _activation_summary(bias)\n\n            if debug:\n                bias = tf.Print(bias, [tf.shape(bias)],\n                                message=\'Shape of %s\' % name,\n                                summarize=4, first_n=1)\n            return bias\n\n    def _score_layer(self, bottom, name, num_classes):\n        with tf.variable_scope(name) as scope:\n            # get number of input channels\n            in_features = bottom.get_shape()[3].value\n            shape = [1, 1, in_features, num_classes]\n            # He initialization Sheme\n            num_input = in_features\n            stddev = (2 / num_input)**0.5\n            # Apply convolution\n            w_decay = self.wd\n            weights = self._variable_with_weight_decay(shape, stddev, w_decay)\n            conv = tf.nn.conv2d(bottom, weights, [1, 1, 1, 1], padding=\'SAME\')\n            # Apply bias\n            conv_biases = self._bias_variable([num_classes], constant=0.0)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            _activation_summary(bias)\n\n            return bias\n\n    def _upscore_layer(self, bottom, shape,\n                       num_classes, name, debug,\n                       ksize=4, stride=2):\n        strides = [1, stride, stride, 1]\n        with tf.variable_scope(name):\n            in_features = bottom.get_shape()[3].value\n\n            if shape is None:\n                # Compute shape out of Bottom\n                in_shape = tf.shape(bottom)\n\n                h = ((in_shape[1] - 1) * stride) + 1\n                w = ((in_shape[2] - 1) * stride) + 1\n                new_shape = [in_shape[0], h, w, num_classes]\n            else:\n                new_shape = [shape[0], shape[1], shape[2], num_classes]\n            output_shape = tf.stack(new_shape)\n\n            logging.debug(""Layer: %s, Fan-in: %d"" % (name, in_features))\n            f_shape = [ksize, ksize, num_classes, in_features]\n\n            # create\n            num_input = ksize * ksize * in_features / stride\n            stddev = (2 / num_input)**0.5\n\n            weights = self.get_deconv_filter(f_shape)\n            deconv = tf.nn.conv2d_transpose(bottom, weights, output_shape,\n                                            strides=strides, padding=\'SAME\')\n\n            if debug:\n                deconv = tf.Print(deconv, [tf.shape(deconv)],\n                                  message=\'Shape of %s\' % name,\n                                  summarize=4, first_n=1)\n\n        _activation_summary(deconv)\n        return deconv\n\n    def get_deconv_filter(self, f_shape):\n        width = f_shape[0]\n        height = f_shape[1]\n        f = ceil(width/2.0)\n        c = (2 * f - 1 - f % 2) / (2.0 * f)\n        bilinear = np.zeros([f_shape[0], f_shape[1]])\n        for x in range(width):\n            for y in range(height):\n                value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n                bilinear[x, y] = value\n        weights = np.zeros(f_shape)\n        for i in range(f_shape[2]):\n            weights[:, :, i, i] = bilinear\n\n        init = tf.constant_initializer(value=weights,\n                                       dtype=tf.float32)\n        return tf.get_variable(name=""up_filter"", initializer=init,\n                               shape=weights.shape)\n\n    def get_conv_filter(self, name):\n        init = tf.constant_initializer(value=self.data_dict[name][0],\n                                       dtype=tf.float32)\n        shape = self.data_dict[name][0].shape\n        print(\'Layer name: %s\' % name)\n        print(\'Layer shape: %s\' % str(shape))\n        var = tf.get_variable(name=""filter"", initializer=init, shape=shape)\n        if not tf.get_variable_scope().reuse:\n            weight_decay = tf.multiply(tf.nn.l2_loss(var), self.wd,\n                                       name=\'weight_loss\')\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 weight_decay)\n        return var\n\n    def get_bias(self, name, num_classes=None):\n        bias_wights = self.data_dict[name][1]\n        shape = self.data_dict[name][1].shape\n        if name == \'fc8\':\n            bias_wights = self._bias_reshape(bias_wights, shape[0],\n                                             num_classes)\n            shape = [num_classes]\n        init = tf.constant_initializer(value=bias_wights,\n                                       dtype=tf.float32)\n        return tf.get_variable(name=""biases"", initializer=init, shape=shape)\n\n    def get_fc_weight(self, name):\n        init = tf.constant_initializer(value=self.data_dict[name][0],\n                                       dtype=tf.float32)\n        shape = self.data_dict[name][0].shape\n        var = tf.get_variable(name=""weights"", initializer=init, shape=shape)\n        if not tf.get_variable_scope().reuse:\n            weight_decay = tf.multiply(tf.nn.l2_loss(var), self.wd,\n                                       name=\'weight_loss\')\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 weight_decay)\n        return var\n\n    def _bias_reshape(self, bweight, num_orig, num_new):\n        """""" Build bias weights for filter produces with `_summary_reshape`\n\n        """"""\n        n_averaged_elements = num_orig//num_new\n        avg_bweight = np.zeros(num_new)\n        for i in range(0, num_orig, n_averaged_elements):\n            start_idx = i\n            end_idx = start_idx + n_averaged_elements\n            avg_idx = start_idx//n_averaged_elements\n            if avg_idx == num_new:\n                break\n            avg_bweight[avg_idx] = np.mean(bweight[start_idx:end_idx])\n        return avg_bweight\n\n    def _summary_reshape(self, fweight, shape, num_new):\n        """""" Produce weights for a reduced fully-connected layer.\n\n        FC8 of VGG produces 1000 classes. Most semantic segmentation\n        task require much less classes. This reshapes the original weights\n        to be used in a fully-convolutional layer which produces num_new\n        classes. To archive this the average (mean) of n adjanced classes is\n        taken.\n\n        Consider reordering fweight, to perserve semantic meaning of the\n        weights.\n\n        Args:\n          fweight: original weights\n          shape: shape of the desired fully-convolutional layer\n          num_new: number of new classes\n\n\n        Returns:\n          Filter weights for `num_new` classes.\n        """"""\n        num_orig = shape[3]\n        shape[3] = num_new\n        assert(num_new < num_orig)\n        n_averaged_elements = num_orig//num_new\n        avg_fweight = np.zeros(shape)\n        for i in range(0, num_orig, n_averaged_elements):\n            start_idx = i\n            end_idx = start_idx + n_averaged_elements\n            avg_idx = start_idx//n_averaged_elements\n            if avg_idx == num_new:\n                break\n            avg_fweight[:, :, :, avg_idx] = np.mean(\n                fweight[:, :, :, start_idx:end_idx], axis=3)\n        return avg_fweight\n\n    def _variable_with_weight_decay(self, shape, stddev, wd):\n        """"""Helper to create an initialized Variable with weight decay.\n\n        Note that the Variable is initialized with a truncated normal\n        distribution.\n        A weight decay is added only if one is specified.\n\n        Args:\n          name: name of the variable\n          shape: list of ints\n          stddev: standard deviation of a truncated Gaussian\n          wd: add L2Loss weight decay multiplied by this float. If None, weight\n              decay is not added for this Variable.\n\n        Returns:\n          Variable Tensor\n        """"""\n\n        initializer = tf.truncated_normal_initializer(stddev=stddev)\n        var = tf.get_variable(\'weights\', shape=shape,\n                              initializer=initializer)\n\n        if wd and (not tf.get_variable_scope().reuse):\n            weight_decay = tf.multiply(\n                tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 weight_decay)\n        return var\n\n    def _bias_variable(self, shape, constant=0.0):\n        initializer = tf.constant_initializer(constant)\n        return tf.get_variable(name=\'biases\', shape=shape,\n                               initializer=initializer)\n\n    def get_fc_weight_reshape(self, name, shape, num_classes=None):\n        print(\'Layer name: %s\' % name)\n        print(\'Layer shape: %s\' % shape)\n        weights = self.data_dict[name][0]\n        weights = weights.reshape(shape)\n        if num_classes is not None:\n            weights = self._summary_reshape(weights, shape,\n                                            num_new=num_classes)\n        init = tf.constant_initializer(value=weights,\n                                       dtype=tf.float32)\n        return tf.get_variable(name=""weights"", initializer=init, shape=shape)\n\n\ndef _activation_summary(x):\n    """"""Helper to create summaries for activations.\n\n    Creates a summary that provides a histogram of activations.\n    Creates a summary that measure the sparsity of activations.\n\n    Args:\n      x: Tensor\n    Returns:\n      nothing\n    """"""\n    # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on tensorboard.\n    tensor_name = x.op.name\n    # tensor_name = re.sub(\'%s_[0-9]*/\' % TOWER_NAME, \'\', x.op.name)\n    tf.summary.histogram(tensor_name + \'/activations\', x)\n    tf.summary.scalar(tensor_name + \'/sparsity\', tf.nn.zero_fraction(x))\n'"
fcn8_vgg.py,87,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nfrom math import ceil\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nVGG_MEAN = [103.939, 116.779, 123.68]\n\n\nclass FCN8VGG:\n\n    def __init__(self, vgg16_npy_path=None):\n        if vgg16_npy_path is None:\n            path = sys.modules[self.__class__.__module__].__file__\n            # print path\n            path = os.path.abspath(os.path.join(path, os.pardir))\n            # print path\n            path = os.path.join(path, ""vgg16.npy"")\n            vgg16_npy_path = path\n            logging.info(""Load npy file from \'%s\'."", vgg16_npy_path)\n        if not os.path.isfile(vgg16_npy_path):\n            logging.error((""File \'%s\' not found. Download it from ""\n                           ""ftp://mi.eng.cam.ac.uk/pub/mttt2/""\n                           ""models/vgg16.npy""), vgg16_npy_path)\n            sys.exit(1)\n\n        self.data_dict = np.load(vgg16_npy_path, encoding=\'latin1\').item()\n        self.wd = 5e-4\n        print(""npy file loaded"")\n\n    def build(self, rgb, train=False, num_classes=20, random_init_fc8=False,\n              debug=False, use_dilated=False):\n        """"""\n        Build the VGG model using loaded weights\n        Parameters\n        ----------\n        rgb: image batch tensor\n            Image in rgb shap. Scaled to Intervall [0, 255]\n        train: bool\n            Whether to build train or inference graph\n        num_classes: int\n            How many classes should be predicted (by fc8)\n        random_init_fc8 : bool\n            Whether to initialize fc8 layer randomly.\n            Finetuning is required in this case.\n        debug: bool\n            Whether to print additional Debug Information.\n        """"""\n        # Convert RGB to BGR\n\n        with tf.name_scope(\'Processing\'):\n\n            red, green, blue = tf.split(rgb, 3, 3)\n            # assert red.get_shape().as_list()[1:] == [224, 224, 1]\n            # assert green.get_shape().as_list()[1:] == [224, 224, 1]\n            # assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n            bgr = tf.concat([\n                blue - VGG_MEAN[0],\n                green - VGG_MEAN[1],\n                red - VGG_MEAN[2],\n            ], 3)\n\n            if debug:\n                bgr = tf.Print(bgr, [tf.shape(bgr)],\n                               message=\'Shape of input image: \',\n                               summarize=4, first_n=1)\n\n        self.conv1_1 = self._conv_layer(bgr, ""conv1_1"")\n        self.conv1_2 = self._conv_layer(self.conv1_1, ""conv1_2"")\n        self.pool1 = self._max_pool(self.conv1_2, \'pool1\', debug)\n\n        self.conv2_1 = self._conv_layer(self.pool1, ""conv2_1"")\n        self.conv2_2 = self._conv_layer(self.conv2_1, ""conv2_2"")\n        self.pool2 = self._max_pool(self.conv2_2, \'pool2\', debug)\n\n        self.conv3_1 = self._conv_layer(self.pool2, ""conv3_1"")\n        self.conv3_2 = self._conv_layer(self.conv3_1, ""conv3_2"")\n        self.conv3_3 = self._conv_layer(self.conv3_2, ""conv3_3"")\n        self.pool3 = self._max_pool(self.conv3_3, \'pool3\', debug)\n\n        self.conv4_1 = self._conv_layer(self.pool3, ""conv4_1"")\n        self.conv4_2 = self._conv_layer(self.conv4_1, ""conv4_2"")\n        self.conv4_3 = self._conv_layer(self.conv4_2, ""conv4_3"")\n\n        if use_dilated:\n            pad = [[0, 0], [0, 0]]\n            self.pool4 = tf.nn.max_pool(self.conv4_3, ksize=[1, 2, 2, 1],\n                                        strides=[1, 1, 1, 1],\n                                        padding=\'SAME\', name=\'pool4\')\n            self.pool4 = tf.space_to_batch(self.pool4,\n                                           paddings=pad, block_size=2)\n        else:\n            self.pool4 = self._max_pool(self.conv4_3, \'pool4\', debug)\n\n        self.conv5_1 = self._conv_layer(self.pool4, ""conv5_1"")\n        self.conv5_2 = self._conv_layer(self.conv5_1, ""conv5_2"")\n        self.conv5_3 = self._conv_layer(self.conv5_2, ""conv5_3"")\n        if use_dilated:\n            pad = [[0, 0], [0, 0]]\n            self.pool5 = tf.nn.max_pool(self.conv5_3, ksize=[1, 2, 2, 1],\n                                        strides=[1, 1, 1, 1],\n                                        padding=\'SAME\', name=\'pool5\')\n            self.pool5 = tf.space_to_batch(self.pool5,\n                                           paddings=pad, block_size=2)\n        else:\n            self.pool5 = self._max_pool(self.conv5_3, \'pool5\', debug)\n\n        self.fc6 = self._fc_layer(self.pool5, ""fc6"")\n\n        if train:\n            self.fc6 = tf.nn.dropout(self.fc6, 0.5)\n\n        self.fc7 = self._fc_layer(self.fc6, ""fc7"")\n        if train:\n            self.fc7 = tf.nn.dropout(self.fc7, 0.5)\n\n        if use_dilated:\n            self.pool5 = tf.batch_to_space(self.pool5, crops=pad, block_size=2)\n            self.pool5 = tf.batch_to_space(self.pool5, crops=pad, block_size=2)\n            self.fc7 = tf.batch_to_space(self.fc7, crops=pad, block_size=2)\n            self.fc7 = tf.batch_to_space(self.fc7, crops=pad, block_size=2)\n            return\n\n        if random_init_fc8:\n            self.score_fr = self._score_layer(self.fc7, ""score_fr"",\n                                              num_classes)\n        else:\n            self.score_fr = self._fc_layer(self.fc7, ""score_fr"",\n                                           num_classes=num_classes,\n                                           relu=False)\n\n        self.pred = tf.argmax(self.score_fr, dimension=3)\n\n        self.upscore2 = self._upscore_layer(self.score_fr,\n                                            shape=tf.shape(self.pool4),\n                                            num_classes=num_classes,\n                                            debug=debug, name=\'upscore2\',\n                                            ksize=4, stride=2)\n        self.score_pool4 = self._score_layer(self.pool4, ""score_pool4"",\n                                             num_classes=num_classes)\n        self.fuse_pool4 = tf.add(self.upscore2, self.score_pool4)\n\n        self.upscore4 = self._upscore_layer(self.fuse_pool4,\n                                            shape=tf.shape(self.pool3),\n                                            num_classes=num_classes,\n                                            debug=debug, name=\'upscore4\',\n                                            ksize=4, stride=2)\n        self.score_pool3 = self._score_layer(self.pool3, ""score_pool3"",\n                                             num_classes=num_classes)\n        self.fuse_pool3 = tf.add(self.upscore4, self.score_pool3)\n\n        self.upscore32 = self._upscore_layer(self.fuse_pool3,\n                                             shape=tf.shape(bgr),\n                                             num_classes=num_classes,\n                                             debug=debug, name=\'upscore32\',\n                                             ksize=16, stride=8)\n\n        self.pred_up = tf.argmax(self.upscore32, dimension=3)\n\n    def _max_pool(self, bottom, name, debug):\n        pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                              padding=\'SAME\', name=name)\n\n        if debug:\n            pool = tf.Print(pool, [tf.shape(pool)],\n                            message=\'Shape of %s\' % name,\n                            summarize=4, first_n=1)\n        return pool\n\n    def _conv_layer(self, bottom, name):\n        with tf.variable_scope(name) as scope:\n            filt = self.get_conv_filter(name)\n            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=\'SAME\')\n\n            conv_biases = self.get_bias(name)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            relu = tf.nn.relu(bias)\n            # Add summary to Tensorboard\n            _activation_summary(relu)\n            return relu\n\n    def _fc_layer(self, bottom, name, num_classes=None,\n                  relu=True, debug=False):\n        with tf.variable_scope(name) as scope:\n            shape = bottom.get_shape().as_list()\n\n            if name == \'fc6\':\n                filt = self.get_fc_weight_reshape(name, [7, 7, 512, 4096])\n            elif name == \'score_fr\':\n                name = \'fc8\'  # Name of score_fr layer in VGG Model\n                filt = self.get_fc_weight_reshape(name, [1, 1, 4096, 1000],\n                                                  num_classes=num_classes)\n            else:\n                filt = self.get_fc_weight_reshape(name, [1, 1, 4096, 4096])\n\n            self._add_wd_and_summary(filt, self.wd, ""fc_wlosses"")\n\n            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding=\'SAME\')\n            conv_biases = self.get_bias(name, num_classes=num_classes)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            if relu:\n                bias = tf.nn.relu(bias)\n            _activation_summary(bias)\n\n            if debug:\n                bias = tf.Print(bias, [tf.shape(bias)],\n                                message=\'Shape of %s\' % name,\n                                summarize=4, first_n=1)\n            return bias\n\n    def _score_layer(self, bottom, name, num_classes):\n        with tf.variable_scope(name) as scope:\n            # get number of input channels\n            in_features = bottom.get_shape()[3].value\n            shape = [1, 1, in_features, num_classes]\n            # He initialization Sheme\n            if name == ""score_fr"":\n                num_input = in_features\n                stddev = (2 / num_input)**0.5\n            elif name == ""score_pool4"":\n                stddev = 0.001\n            elif name == ""score_pool3"":\n                stddev = 0.0001\n            # Apply convolution\n            w_decay = self.wd\n\n            weights = self._variable_with_weight_decay(shape, stddev, w_decay,\n                                                       decoder=True)\n            conv = tf.nn.conv2d(bottom, weights, [1, 1, 1, 1], padding=\'SAME\')\n            # Apply bias\n            conv_biases = self._bias_variable([num_classes], constant=0.0)\n            bias = tf.nn.bias_add(conv, conv_biases)\n\n            _activation_summary(bias)\n\n            return bias\n\n    def _upscore_layer(self, bottom, shape,\n                       num_classes, name, debug,\n                       ksize=4, stride=2):\n        strides = [1, stride, stride, 1]\n        with tf.variable_scope(name):\n            in_features = bottom.get_shape()[3].value\n\n            if shape is None:\n                # Compute shape out of Bottom\n                in_shape = tf.shape(bottom)\n\n                h = ((in_shape[1] - 1) * stride) + 1\n                w = ((in_shape[2] - 1) * stride) + 1\n                new_shape = [in_shape[0], h, w, num_classes]\n            else:\n                new_shape = [shape[0], shape[1], shape[2], num_classes]\n            output_shape = tf.stack(new_shape)\n\n            logging.debug(""Layer: %s, Fan-in: %d"" % (name, in_features))\n            f_shape = [ksize, ksize, num_classes, in_features]\n\n            # create\n            num_input = ksize * ksize * in_features / stride\n            stddev = (2 / num_input)**0.5\n\n            weights = self.get_deconv_filter(f_shape)\n            self._add_wd_and_summary(weights, self.wd, ""fc_wlosses"")\n            deconv = tf.nn.conv2d_transpose(bottom, weights, output_shape,\n                                            strides=strides, padding=\'SAME\')\n\n            if debug:\n                deconv = tf.Print(deconv, [tf.shape(deconv)],\n                                  message=\'Shape of %s\' % name,\n                                  summarize=4, first_n=1)\n\n        _activation_summary(deconv)\n        return deconv\n\n    def get_deconv_filter(self, f_shape):\n        width = f_shape[0]\n        height = f_shape[1]\n        f = ceil(width/2.0)\n        c = (2 * f - 1 - f % 2) / (2.0 * f)\n        bilinear = np.zeros([f_shape[0], f_shape[1]])\n        for x in range(width):\n            for y in range(height):\n                value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n                bilinear[x, y] = value\n        weights = np.zeros(f_shape)\n        for i in range(f_shape[2]):\n            weights[:, :, i, i] = bilinear\n\n        init = tf.constant_initializer(value=weights,\n                                       dtype=tf.float32)\n        var = tf.get_variable(name=""up_filter"", initializer=init,\n                              shape=weights.shape)\n        return var\n\n    def get_conv_filter(self, name):\n        init = tf.constant_initializer(value=self.data_dict[name][0],\n                                       dtype=tf.float32)\n        shape = self.data_dict[name][0].shape\n        print(\'Layer name: %s\' % name)\n        print(\'Layer shape: %s\' % str(shape))\n        var = tf.get_variable(name=""filter"", initializer=init, shape=shape)\n        if not tf.get_variable_scope().reuse:\n            weight_decay = tf.multiply(tf.nn.l2_loss(var), self.wd,\n                                       name=\'weight_loss\')\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 weight_decay)\n        _variable_summaries(var)\n        return var\n\n    def get_bias(self, name, num_classes=None):\n        bias_wights = self.data_dict[name][1]\n        shape = self.data_dict[name][1].shape\n        if name == \'fc8\':\n            bias_wights = self._bias_reshape(bias_wights, shape[0],\n                                             num_classes)\n            shape = [num_classes]\n        init = tf.constant_initializer(value=bias_wights,\n                                       dtype=tf.float32)\n        var = tf.get_variable(name=""biases"", initializer=init, shape=shape)\n        _variable_summaries(var)\n        return var\n\n    def get_fc_weight(self, name):\n        init = tf.constant_initializer(value=self.data_dict[name][0],\n                                       dtype=tf.float32)\n        shape = self.data_dict[name][0].shape\n        var = tf.get_variable(name=""weights"", initializer=init, shape=shape)\n        if not tf.get_variable_scope().reuse:\n            weight_decay = tf.multiply(tf.nn.l2_loss(var), self.wd,\n                                       name=\'weight_loss\')\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 weight_decay)\n        _variable_summaries(var)\n        return var\n\n    def _bias_reshape(self, bweight, num_orig, num_new):\n        """""" Build bias weights for filter produces with `_summary_reshape`\n\n        """"""\n        n_averaged_elements = num_orig//num_new\n        avg_bweight = np.zeros(num_new)\n        for i in range(0, num_orig, n_averaged_elements):\n            start_idx = i\n            end_idx = start_idx + n_averaged_elements\n            avg_idx = start_idx//n_averaged_elements\n            if avg_idx == num_new:\n                break\n            avg_bweight[avg_idx] = np.mean(bweight[start_idx:end_idx])\n        return avg_bweight\n\n    def _summary_reshape(self, fweight, shape, num_new):\n        """""" Produce weights for a reduced fully-connected layer.\n\n        FC8 of VGG produces 1000 classes. Most semantic segmentation\n        task require much less classes. This reshapes the original weights\n        to be used in a fully-convolutional layer which produces num_new\n        classes. To archive this the average (mean) of n adjanced classes is\n        taken.\n\n        Consider reordering fweight, to perserve semantic meaning of the\n        weights.\n\n        Args:\n          fweight: original weights\n          shape: shape of the desired fully-convolutional layer\n          num_new: number of new classes\n\n\n        Returns:\n          Filter weights for `num_new` classes.\n        """"""\n        num_orig = shape[3]\n        shape[3] = num_new\n        assert(num_new < num_orig)\n        n_averaged_elements = num_orig//num_new\n        avg_fweight = np.zeros(shape)\n        for i in range(0, num_orig, n_averaged_elements):\n            start_idx = i\n            end_idx = start_idx + n_averaged_elements\n            avg_idx = start_idx//n_averaged_elements\n            if avg_idx == num_new:\n                break\n            avg_fweight[:, :, :, avg_idx] = np.mean(\n                fweight[:, :, :, start_idx:end_idx], axis=3)\n        return avg_fweight\n\n    def _variable_with_weight_decay(self, shape, stddev, wd, decoder=False):\n        """"""Helper to create an initialized Variable with weight decay.\n\n        Note that the Variable is initialized with a truncated normal\n        distribution.\n        A weight decay is added only if one is specified.\n\n        Args:\n          name: name of the variable\n          shape: list of ints\n          stddev: standard deviation of a truncated Gaussian\n          wd: add L2Loss weight decay multiplied by this float. If None, weight\n              decay is not added for this Variable.\n\n        Returns:\n          Variable Tensor\n        """"""\n\n        initializer = tf.truncated_normal_initializer(stddev=stddev)\n        var = tf.get_variable(\'weights\', shape=shape,\n                              initializer=initializer)\n\n        collection_name = tf.GraphKeys.REGULARIZATION_LOSSES\n        if wd and (not tf.get_variable_scope().reuse):\n            weight_decay = tf.multiply(\n                tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n            tf.add_to_collection(collection_name, weight_decay)\n        _variable_summaries(var)\n        return var\n\n    def _add_wd_and_summary(self, var, wd, collection_name=None):\n        if collection_name is None:\n            collection_name = tf.GraphKeys.REGULARIZATION_LOSSES\n        if wd and (not tf.get_variable_scope().reuse):\n            weight_decay = tf.multiply(\n                tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n            tf.add_to_collection(collection_name, weight_decay)\n        _variable_summaries(var)\n        return var\n\n    def _bias_variable(self, shape, constant=0.0):\n        initializer = tf.constant_initializer(constant)\n        var = tf.get_variable(name=\'biases\', shape=shape,\n                              initializer=initializer)\n        _variable_summaries(var)\n        return var\n\n    def get_fc_weight_reshape(self, name, shape, num_classes=None):\n        print(\'Layer name: %s\' % name)\n        print(\'Layer shape: %s\' % shape)\n        weights = self.data_dict[name][0]\n        weights = weights.reshape(shape)\n        if num_classes is not None:\n            weights = self._summary_reshape(weights, shape,\n                                            num_new=num_classes)\n        init = tf.constant_initializer(value=weights,\n                                       dtype=tf.float32)\n        var = tf.get_variable(name=""weights"", initializer=init, shape=shape)\n        return var\n\n\ndef _activation_summary(x):\n    """"""Helper to create summaries for activations.\n\n    Creates a summary that provides a histogram of activations.\n    Creates a summary that measure the sparsity of activations.\n\n    Args:\n      x: Tensor\n    Returns:\n      nothing\n    """"""\n    # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on tensorboard.\n    tensor_name = x.op.name\n    # tensor_name = re.sub(\'%s_[0-9]*/\' % TOWER_NAME, \'\', x.op.name)\n    tf.summary.histogram(tensor_name + \'/activations\', x)\n    tf.summary.scalar(tensor_name + \'/sparsity\', tf.nn.zero_fraction(x))\n\n\ndef _variable_summaries(var):\n    """"""Attach a lot of summaries to a Tensor.""""""\n    if not tf.get_variable_scope().reuse:\n        name = var.op.name\n        logging.info(""Creating Summary for: %s"" % name)\n        with tf.name_scope(\'summaries\'):\n            mean = tf.reduce_mean(var)\n            tf.summary.scalar(name + \'/mean\', mean)\n            with tf.name_scope(\'stddev\'):\n                stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n            tf.summary.scalar(name + \'/sttdev\', stddev)\n            tf.summary.scalar(name + \'/max\', tf.reduce_max(var))\n            tf.summary.scalar(name + \'/min\', tf.reduce_min(var))\n            tf.summary.histogram(name, var)\n'"
loss.py,11,"b'""""""This module provides the a softmax cross entropy loss for training FCN.\n\nIn order to train VGG first build the model and then feed apply vgg_fcn.up\nto the loss. The loss function can be used in combination with any optimizer\n(e.g. Adam) to finetune the whole model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef loss(logits, labels, num_classes, head=None):\n    """"""Calculate the loss from the logits and the labels.\n\n    Args:\n      logits: tensor, float - [batch_size, width, height, num_classes].\n          Use vgg_fcn.upscore as logits.\n      labels: Labels tensor, int32 - [batch_size, width, height, num_classes].\n          The ground truth of your data.\n      head: numpy array - [num_classes]\n          Weighting the loss of each class\n          Optional: Prioritize some classes\n\n    Returns:\n      loss: Loss tensor of type float.\n    """"""\n    with tf.name_scope(\'loss\'):\n        logits = tf.reshape(logits, (-1, num_classes))\n        epsilon = tf.constant(value=1e-4)\n        labels = tf.to_float(tf.reshape(labels, (-1, num_classes)))\n\n        softmax = tf.nn.softmax(logits) + epsilon\n\n        if head is not None:\n            cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax),\n                                           head), reduction_indices=[1])\n        else:\n            cross_entropy = -tf.reduce_sum(\n                labels * tf.log(softmax), reduction_indices=[1])\n\n        cross_entropy_mean = tf.reduce_mean(cross_entropy,\n                                            name=\'xentropy_mean\')\n        tf.add_to_collection(\'losses\', cross_entropy_mean)\n\n        loss = tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\')\n    return loss\n'"
test_fcn16_vgg.py,5,"b'#!/usr/bin/env python\n\nimport os\nimport scipy as scp\nimport scipy.misc\n\nimport numpy as np\nimport logging\nimport tensorflow as tf\nimport sys\n\nimport fcn16_vgg\nimport utils\n\nlogging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                    level=logging.INFO,\n                    stream=sys.stdout)\n\nfrom tensorflow.python.framework import ops\n\nimg1 = scp.misc.imread(""./test_data/tabby_cat.png"")\n\nwith tf.Session() as sess:\n    images = tf.placeholder(""float"")\n    feed_dict = {images: img1}\n    batch_images = tf.expand_dims(images, 0)\n\n    vgg_fcn = fcn16_vgg.FCN16VGG()\n    with tf.name_scope(""content_vgg""):\n        vgg_fcn.build(batch_images, debug=True)\n\n    print(\'Finished building Network.\')\n\n    logging.warning(""Score weights are initialized random."")\n    logging.warning(""Do not expect meaningful results."")\n\n    logging.info(""Start Initializing Variabels."")\n\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    print(\'Running the Network\')\n    tensors = [vgg_fcn.pred, vgg_fcn.pred_up]\n    down, up = sess.run(tensors, feed_dict=feed_dict)\n\n    down_color = utils.color_image(down[0])\n    up_color = utils.color_image(up[0])\n\n    scp.misc.imsave(\'fcn16_downsampled.png\', down_color)\n    scp.misc.imsave(\'fcn16_upsampled.png\', up_color)\n'"
test_fcn32_vgg.py,5,"b'#!/usr/bin/env python\n\nimport os\nimport scipy as scp\nimport scipy.misc\n\nimport numpy as np\nimport tensorflow as tf\n\nimport fcn32_vgg\nimport utils\n\nfrom tensorflow.python.framework import ops\n\nimg1 = scp.misc.imread(""./test_data/tabby_cat.png"")\n\nwith tf.Session() as sess:\n    images = tf.placeholder(""float"")\n    feed_dict = {images: img1}\n    batch_images = tf.expand_dims(images, 0)\n\n    vgg_fcn = fcn32_vgg.FCN32VGG()\n    with tf.name_scope(""content_vgg""):\n        vgg_fcn.build(batch_images, debug=True)\n\n    print(\'Finished building Network.\')\n\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    print(\'Running the Network\')\n    tensors = [vgg_fcn.pred, vgg_fcn.pred_up]\n    down, up = sess.run(tensors, feed_dict=feed_dict)\n\n    down_color = utils.color_image(down[0])\n    up_color = utils.color_image(up[0])\n\n    scp.misc.imsave(\'fcn32_downsampled.png\', down_color)\n    scp.misc.imsave(\'fcn32_upsampled.png\', up_color)\n'"
test_fcn8_vgg.py,5,"b'#!/usr/bin/env python\n\nimport os\nimport scipy as scp\nimport scipy.misc\n\nimport numpy as np\nimport logging\nimport tensorflow as tf\nimport sys\n\nimport fcn8_vgg\nimport utils\n\nlogging.basicConfig(format=\'%(asctime)s %(levelname)s %(message)s\',\n                    level=logging.INFO,\n                    stream=sys.stdout)\n\nfrom tensorflow.python.framework import ops\n\nimg1 = scp.misc.imread(""./test_data/tabby_cat.png"")\n\nwith tf.Session() as sess:\n    images = tf.placeholder(""float"")\n    feed_dict = {images: img1}\n    batch_images = tf.expand_dims(images, 0)\n\n    vgg_fcn = fcn8_vgg.FCN8VGG()\n    with tf.name_scope(""content_vgg""):\n        vgg_fcn.build(batch_images, debug=True)\n\n    print(\'Finished building Network.\')\n\n    logging.warning(""Score weights are initialized random."")\n    logging.warning(""Do not expect meaningful results."")\n\n    logging.info(""Start Initializing Variabels."")\n\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    print(\'Running the Network\')\n    tensors = [vgg_fcn.pred, vgg_fcn.pred_up]\n    down, up = sess.run(tensors, feed_dict=feed_dict)\n\n    down_color = utils.color_image(down[0])\n    up_color = utils.color_image(up[0])\n\n    scp.misc.imsave(\'fcn8_downsampled.png\', down_color)\n    scp.misc.imsave(\'fcn8_upsampled.png\', up_color)\n'"
utils.py,0,"b""import numpy as np\n\n\ndef color_image(image, num_classes=20):\n    import matplotlib as mpl\n    import matplotlib.cm\n    norm = mpl.colors.Normalize(vmin=0., vmax=num_classes)\n    mycm = mpl.cm.get_cmap('Set1')\n    return mycm(norm(image))\n"""
