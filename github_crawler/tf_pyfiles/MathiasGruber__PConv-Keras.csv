file_path,api_count,code
main.py,0,"b'import os\nimport gc\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nfrom argparse import ArgumentParser\nfrom copy import deepcopy\nfrom tqdm import tqdm\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, LambdaCallback\nfrom keras import backend as K\nfrom keras.utils import Sequence\nfrom keras_tqdm import TQDMCallback\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\n\nfrom libs.pconv_model import PConvUnet\nfrom libs.util import MaskGenerator\n\n\n# Sample call\nr""""""\n# Train on CelebaHQ\npython main.py --name CelebHQ --train C:\\Documents\\Kaggle\\celebaHQ-512\\train\\ --validation C:\\Documents\\Kaggle\\celebaHQ-512\\val\\ --test C:\\Documents\\Kaggle\\celebaHQ-512\\test\\ --checkpoint ""C:\\Users\\Mathias Felix Gruber\\Documents\\GitHub\\PConv-Keras\\data\\logs\\imagenet_phase1_paperMasks\\weights.35-0.70.h5""\n""""""\n\n\ndef parse_args():\n    parser = ArgumentParser(description=\'Training script for PConv inpainting\')\n\n    parser.add_argument(\n        \'-stage\', \'--stage\',\n        type=str, default=\'train\',\n        help=\'Which stage of training to run\',\n        choices=[\'train\', \'finetune\']\n    )\n\n    parser.add_argument(\n        \'-train\', \'--train\',\n        type=str,\n        help=\'Folder with training images\'\n    )\n    \n    parser.add_argument(\n        \'-validation\', \'--validation\',\n        type=str,\n        help=\'Folder with validation images\'\n    )\n\n    parser.add_argument(\n        \'-test\', \'--test\',\n        type=str,\n        help=\'Folder with testing images\'\n    )\n        \n    parser.add_argument(\n        \'-name\', \'--name\',\n        type=str, default=\'myDataset\',\n        help=\'Dataset name, e.g. \\\'imagenet\\\'\'\n    )\n        \n    parser.add_argument(\n        \'-batch_size\', \'--batch_size\',\n        type=int, default=4,\n        help=\'What batch-size should we use\'\n    )\n\n    parser.add_argument(\n        \'-test_path\', \'--test_path\',\n        type=str, default=\'./data/test_samples/\',\n        help=\'Where to output test images during training\'\n    )\n        \n    parser.add_argument(\n        \'-weight_path\', \'--weight_path\',\n        type=str, default=\'./data/logs/\',\n        help=\'Where to output weights during training\'\n    )\n        \n    parser.add_argument(\n        \'-log_path\', \'--log_path\',\n        type=str, default=\'./data/logs/\',\n        help=\'Where to output tensorboard logs during training\'\n    )\n\n    parser.add_argument(\n        \'-vgg_path\', \'--vgg_path\',\n        type=str, default=\'./data/logs/pytorch_to_keras_vgg16.h5\',\n        help=\'VGG16 weights trained on PyTorch with pixel scaling 1/255.\'\n    )\n\n    parser.add_argument(\n        \'-checkpoint\', \'--checkpoint\',\n        type=str, \n        help=\'Previous weights to be loaded onto model\'\n    )\n        \n    return  parser.parse_args()\n\n\nclass AugmentingDataGenerator(ImageDataGenerator):\n    """"""Wrapper for ImageDataGenerator to return mask & image""""""\n    def flow_from_directory(self, directory, mask_generator, *args, **kwargs):\n        generator = super().flow_from_directory(directory, class_mode=None, *args, **kwargs)        \n        seed = None if \'seed\' not in kwargs else kwargs[\'seed\']\n        while True:\n            \n            # Get augmentend image samples\n            ori = next(generator)\n\n            # Get masks for each image sample            \n            mask = np.stack([\n                mask_generator.sample(seed)\n                for _ in range(ori.shape[0])], axis=0\n            )\n\n            # Apply masks to all image sample\n            masked = deepcopy(ori)\n            masked[mask==0] = 1\n\n            # Yield ([ori, masl],  ori) training batches\n            # print(masked.shape, ori.shape)\n            gc.collect()\n            yield [masked, mask], ori\n\n# Run script\nif __name__ == \'__main__\':\n\n    # Parse command-line arguments\n    args = parse_args()\n\n    if args.stage == \'finetune\' and not args.checkpoint:\n        raise AttributeError(\'If you are finetuning your model, you must supply a checkpoint file\')\n\n    # Create training generator\n    train_datagen = AugmentingDataGenerator(  \n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        rescale=1./255,\n        horizontal_flip=True\n    )\n    train_generator = train_datagen.flow_from_directory(\n        args.train, \n        MaskGenerator(512, 512, 3),\n        target_size=(512, 512), \n        batch_size=args.batch_size\n    )\n\n    # Create validation generator\n    val_datagen = AugmentingDataGenerator(rescale=1./255)\n    val_generator = val_datagen.flow_from_directory(\n        args.validation, \n        MaskGenerator(512, 512, 3), \n        target_size=(512, 512), \n        batch_size=args.batch_size, \n        classes=[\'val\'], \n        seed=42\n    )\n\n    # Create testing generator\n    test_datagen = AugmentingDataGenerator(rescale=1./255)\n    test_generator = test_datagen.flow_from_directory(\n        args.test, \n        MaskGenerator(512, 512, 3), \n        target_size=(512, 512), \n        batch_size=args.batch_size, \n        seed=42\n    )\n\n    # Pick out an example to be send to test samples folder\n    test_data = next(test_generator)\n    (masked, mask), ori = test_data\n\n    def plot_callback(model, path):\n        """"""Called at the end of each epoch, displaying our previous test images,\n        as well as their masked predictions and saving them to disk""""""\n        \n        # Get samples & Display them        \n        pred_img = model.predict([masked, mask])\n        pred_time = datetime.datetime.now().strftime(\'%Y-%m-%d-%H-%M-%S\')\n\n        # Clear current output and display test images\n        for i in range(len(ori)):\n            _, axes = plt.subplots(1, 3, figsize=(20, 5))\n            axes[0].imshow(masked[i,:,:,:])\n            axes[1].imshow(pred_img[i,:,:,:] * 1.)\n            axes[2].imshow(ori[i,:,:,:])\n            axes[0].set_title(\'Masked Image\')\n            axes[1].set_title(\'Predicted Image\')\n            axes[2].set_title(\'Original Image\')\n                    \n            plt.savefig(os.path.join(path, \'/img_{}_{}.png\'.format(i, pred_time)))\n            plt.close()\n\n    # Load the model\n    if args.vgg_path:\n        model = PConvUnet(vgg_weights=args.vgg_path)\n    else:\n        model = PConvUnet()\n    \n    # Loading of checkpoint\n    if args.checkpoint:\n        if args.stage == \'train\':\n            model.load(args.checkpoint)\n        elif args.stage == \'finetune\':\n            model.load(args.checkpoint, train_bn=False, lr=0.00005)\n\n    # Fit model\n    model.fit_generator(\n        train_generator, \n        steps_per_epoch=10000,\n        validation_data=val_generator,\n        validation_steps=1000,\n        epochs=100,  \n        verbose=0,\n        callbacks=[\n            TensorBoard(\n                log_dir=os.path.join(args.log_path, args.name+\'_phase1\'),\n                write_graph=False\n            ),\n            ModelCheckpoint(\n                os.path.join(args.log_path, args.name+\'_phase1\', \'weights.{epoch:02d}-{loss:.2f}.h5\'),\n                monitor=\'val_loss\', \n                save_best_only=True, \n                save_weights_only=True\n            ),\n            LambdaCallback(\n                on_epoch_end=lambda epoch, logs: plot_callback(model, args.test_path)\n            ),\n            TQDMCallback()\n        ]\n    )\n        '"
libs/__init__.py,0,b''
libs/pconv_layer.py,0,"b'\nfrom keras.utils import conv_utils\nfrom keras import backend as K\nfrom keras.engine import InputSpec\nfrom keras.layers import Conv2D\n\n\nclass PConv2D(Conv2D):\n    def __init__(self, *args, n_channels=3, mono=False, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_spec = [InputSpec(ndim=4), InputSpec(ndim=4)]\n\n    def build(self, input_shape):        \n        """"""Adapted from original _Conv() layer of Keras        \n        param input_shape: list of dimensions for [img, mask]\n        """"""\n        \n        if self.data_format == \'channels_first\':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n            \n        if input_shape[0][channel_axis] is None:\n            raise ValueError(\'The channel dimension of the inputs should be defined. Found `None`.\')\n            \n        self.input_dim = input_shape[0][channel_axis]\n        \n        # Image kernel\n        kernel_shape = self.kernel_size + (self.input_dim, self.filters)\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name=\'img_kernel\',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        # Mask kernel\n        self.kernel_mask = K.ones(shape=self.kernel_size + (self.input_dim, self.filters))\n\n        # Calculate padding size to achieve zero-padding\n        self.pconv_padding = (\n            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n        )\n\n        # Window size - used for normalization\n        self.window_size = self.kernel_size[0] * self.kernel_size[1]\n        \n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name=\'bias\',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs, mask=None):\n        \'\'\'\n        We will be using the Keras conv2d method, and essentially we have\n        to do here is multiply the mask with the input X, before we apply the\n        convolutions. For the mask itself, we apply convolutions with all weights\n        set to 1.\n        Subsequently, we clip mask values to between 0 and 1\n        \'\'\' \n\n        # Both image and mask must be supplied\n        if type(inputs) is not list or len(inputs) != 2:\n            raise Exception(\'PartialConvolution2D must be called on a list of two tensors [img, mask]. Instead got: \' + str(inputs))\n\n        # Padding done explicitly so that padding becomes part of the masked partial convolution\n        images = K.spatial_2d_padding(inputs[0], self.pconv_padding, self.data_format)\n        masks = K.spatial_2d_padding(inputs[1], self.pconv_padding, self.data_format)\n\n        # Apply convolutions to mask\n        mask_output = K.conv2d(\n            masks, self.kernel_mask, \n            strides=self.strides,\n            padding=\'valid\',\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate\n        )\n\n        # Apply convolutions to image\n        img_output = K.conv2d(\n            (images*masks), self.kernel, \n            strides=self.strides,\n            padding=\'valid\',\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate\n        )        \n\n        # Calculate the mask ratio on each pixel in the output mask\n        mask_ratio = self.window_size / (mask_output + 1e-8)\n\n        # Clip output to be between 0 and 1\n        mask_output = K.clip(mask_output, 0, 1)\n\n        # Remove ratio values where there are holes\n        mask_ratio = mask_ratio * mask_output\n\n        # Normalize iamge output\n        img_output = img_output * mask_ratio\n\n        # Apply bias only to the image (if chosen to do so)\n        if self.use_bias:\n            img_output = K.bias_add(\n                img_output,\n                self.bias,\n                data_format=self.data_format)\n        \n        # Apply activations on the image\n        if self.activation is not None:\n            img_output = self.activation(img_output)\n            \n        return [img_output, mask_output]\n    \n    def compute_output_shape(self, input_shape):\n        if self.data_format == \'channels_last\':\n            space = input_shape[0][1:-1]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=\'same\',\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)\n            return [new_shape, new_shape]\n        if self.data_format == \'channels_first\':\n            space = input_shape[2:]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=\'same\',\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            new_shape = (input_shape[0], self.filters) + tuple(new_space)\n            return [new_shape, new_shape]\n'"
libs/pconv_model.py,3,"b'import os\nimport sys\nimport numpy as np\nfrom datetime import datetime\n\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.layers import Input, Conv2D, UpSampling2D, Dropout, LeakyReLU, BatchNormalization, Activation, Lambda\nfrom keras.layers.merge import Concatenate\nfrom keras.applications import VGG16\nfrom keras import backend as K\nfrom keras.utils.multi_gpu_utils import multi_gpu_model\n\nfrom libs.pconv_layer import PConv2D\n\n\nclass PConvUnet(object):\n\n    def __init__(self, img_rows=512, img_cols=512, vgg_weights=""imagenet"", inference_only=False, net_name=\'default\', gpus=1, vgg_device=None):\n        """"""Create the PConvUnet. If variable image size, set img_rows and img_cols to None\n        \n        Args:\n            img_rows (int): image height.\n            img_cols (int): image width.\n            vgg_weights (str): which weights to pass to the vgg network.\n            inference_only (bool): initialize BN layers for inference.\n            net_name (str): Name of this network (used in logging).\n            gpus (int): How many GPUs to use for training.\n            vgg_device (str): In case of training with multiple GPUs, specify which device to run VGG inference on.\n                e.g. if training on 8 GPUs, vgg inference could be off-loaded exclusively to one GPU, instead of\n                running on one of the GPUs which is also training the UNet.\n        """"""\n        \n        # Settings\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.img_overlap = 30\n        self.inference_only = inference_only\n        self.net_name = net_name\n        self.gpus = gpus\n        self.vgg_device = vgg_device\n\n        # Scaling for VGG input\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n\n        # Assertions\n        assert self.img_rows >= 256, \'Height must be >256 pixels\'\n        assert self.img_cols >= 256, \'Width must be >256 pixels\'\n\n        # Set current epoch\n        self.current_epoch = 0\n        \n        # VGG layers to extract features from (first maxpooling layers, see pp. 7 of paper)\n        self.vgg_layers = [3, 6, 10]\n\n        # Instantiate the vgg network\n        if self.vgg_device:\n            with tf.device(self.vgg_device):\n                self.vgg = self.build_vgg(vgg_weights)\n        else:\n            self.vgg = self.build_vgg(vgg_weights)\n        \n        # Create UNet-like model\n        if self.gpus <= 1:\n            self.model, inputs_mask = self.build_pconv_unet()\n            self.compile_pconv_unet(self.model, inputs_mask)            \n        else:\n            with tf.device(""/cpu:0""):\n                self.model, inputs_mask = self.build_pconv_unet()\n            self.model = multi_gpu_model(self.model, gpus=self.gpus)\n            self.compile_pconv_unet(self.model, inputs_mask)\n        \n    def build_vgg(self, weights=""imagenet""):\n        """"""\n        Load pre-trained VGG16 from keras applications\n        Extract features to be used in loss function from last conv layer, see architecture at:\n        https://github.com/keras-team/keras/blob/master/keras/applications/vgg16.py\n        """"""        \n            \n        # Input image to extract features from\n        img = Input(shape=(self.img_rows, self.img_cols, 3))\n\n        # Mean center and rescale by variance as in PyTorch\n        processed = Lambda(lambda x: (x-self.mean) / self.std)(img)\n        \n        # If inference only, just return empty model        \n        if self.inference_only:\n            model = Model(inputs=img, outputs=[img for _ in range(len(self.vgg_layers))])\n            model.trainable = False\n            model.compile(loss=\'mse\', optimizer=\'adam\')\n            return model\n                \n        # Get the vgg network from Keras applications\n        if weights in [\'imagenet\', None]:\n            vgg = VGG16(weights=weights, include_top=False)\n        else:\n            vgg = VGG16(weights=None, include_top=False)\n            vgg.load_weights(weights, by_name=True)\n\n        # Output the first three pooling layers\n        vgg.outputs = [vgg.layers[i].output for i in self.vgg_layers]        \n        \n        # Create model and compile\n        model = Model(inputs=img, outputs=vgg(processed))\n        model.trainable = False\n        model.compile(loss=\'mse\', optimizer=\'adam\')\n\n        return model\n        \n    def build_pconv_unet(self, train_bn=True):      \n\n        # INPUTS\n        inputs_img = Input((self.img_rows, self.img_cols, 3), name=\'inputs_img\')\n        inputs_mask = Input((self.img_rows, self.img_cols, 3), name=\'inputs_mask\')\n        \n        # ENCODER\n        def encoder_layer(img_in, mask_in, filters, kernel_size, bn=True):\n            conv, mask = PConv2D(filters, kernel_size, strides=2, padding=\'same\')([img_in, mask_in])\n            if bn:\n                conv = BatchNormalization(name=\'EncBN\'+str(encoder_layer.counter))(conv, training=train_bn)\n            conv = Activation(\'relu\')(conv)\n            encoder_layer.counter += 1\n            return conv, mask\n        encoder_layer.counter = 0\n        \n        e_conv1, e_mask1 = encoder_layer(inputs_img, inputs_mask, 64, 7, bn=False)\n        e_conv2, e_mask2 = encoder_layer(e_conv1, e_mask1, 128, 5)\n        e_conv3, e_mask3 = encoder_layer(e_conv2, e_mask2, 256, 5)\n        e_conv4, e_mask4 = encoder_layer(e_conv3, e_mask3, 512, 3)\n        e_conv5, e_mask5 = encoder_layer(e_conv4, e_mask4, 512, 3)\n        e_conv6, e_mask6 = encoder_layer(e_conv5, e_mask5, 512, 3)\n        e_conv7, e_mask7 = encoder_layer(e_conv6, e_mask6, 512, 3)\n        e_conv8, e_mask8 = encoder_layer(e_conv7, e_mask7, 512, 3)\n        \n        # DECODER\n        def decoder_layer(img_in, mask_in, e_conv, e_mask, filters, kernel_size, bn=True):\n            up_img = UpSampling2D(size=(2,2))(img_in)\n            up_mask = UpSampling2D(size=(2,2))(mask_in)\n            concat_img = Concatenate(axis=3)([e_conv,up_img])\n            concat_mask = Concatenate(axis=3)([e_mask,up_mask])\n            conv, mask = PConv2D(filters, kernel_size, padding=\'same\')([concat_img, concat_mask])\n            if bn:\n                conv = BatchNormalization()(conv)\n            conv = LeakyReLU(alpha=0.2)(conv)\n            return conv, mask\n            \n        d_conv9, d_mask9 = decoder_layer(e_conv8, e_mask8, e_conv7, e_mask7, 512, 3)\n        d_conv10, d_mask10 = decoder_layer(d_conv9, d_mask9, e_conv6, e_mask6, 512, 3)\n        d_conv11, d_mask11 = decoder_layer(d_conv10, d_mask10, e_conv5, e_mask5, 512, 3)\n        d_conv12, d_mask12 = decoder_layer(d_conv11, d_mask11, e_conv4, e_mask4, 512, 3)\n        d_conv13, d_mask13 = decoder_layer(d_conv12, d_mask12, e_conv3, e_mask3, 256, 3)\n        d_conv14, d_mask14 = decoder_layer(d_conv13, d_mask13, e_conv2, e_mask2, 128, 3)\n        d_conv15, d_mask15 = decoder_layer(d_conv14, d_mask14, e_conv1, e_mask1, 64, 3)\n        d_conv16, d_mask16 = decoder_layer(d_conv15, d_mask15, inputs_img, inputs_mask, 3, 3, bn=False)\n        outputs = Conv2D(3, 1, activation = \'sigmoid\', name=\'outputs_img\')(d_conv16)\n        \n        # Setup the model inputs / outputs\n        model = Model(inputs=[inputs_img, inputs_mask], outputs=outputs)\n\n        return model, inputs_mask    \n\n    def compile_pconv_unet(self, model, inputs_mask, lr=0.0002):\n        model.compile(\n            optimizer = Adam(lr=lr),\n            loss=self.loss_total(inputs_mask),\n            metrics=[self.PSNR]\n        )\n\n    def loss_total(self, mask):\n        """"""\n        Creates a loss function which sums all the loss components \n        and multiplies by their weights. See paper eq. 7.\n        """"""\n        def loss(y_true, y_pred):\n\n            # Compute predicted image with non-hole pixels set to ground truth\n            y_comp = mask * y_true + (1-mask) * y_pred\n\n            # Compute the vgg features. \n            if self.vgg_device:\n                with tf.device(self.vgg_device):\n                    vgg_out = self.vgg(y_pred)\n                    vgg_gt = self.vgg(y_true)\n                    vgg_comp = self.vgg(y_comp)\n            else:\n                vgg_out = self.vgg(y_pred)\n                vgg_gt = self.vgg(y_true)\n                vgg_comp = self.vgg(y_comp)\n            \n            # Compute loss components\n            l1 = self.loss_valid(mask, y_true, y_pred)\n            l2 = self.loss_hole(mask, y_true, y_pred)\n            l3 = self.loss_perceptual(vgg_out, vgg_gt, vgg_comp)\n            l4 = self.loss_style(vgg_out, vgg_gt)\n            l5 = self.loss_style(vgg_comp, vgg_gt)\n            l6 = self.loss_tv(mask, y_comp)\n\n            # Return loss function\n            return l1 + 6*l2 + 0.05*l3 + 120*(l4+l5) + 0.1*l6\n\n        return loss\n\n    def loss_hole(self, mask, y_true, y_pred):\n        """"""Pixel L1 loss within the hole / mask""""""\n        return self.l1((1-mask) * y_true, (1-mask) * y_pred)\n    \n    def loss_valid(self, mask, y_true, y_pred):\n        """"""Pixel L1 loss outside the hole / mask""""""\n        return self.l1(mask * y_true, mask * y_pred)\n    \n    def loss_perceptual(self, vgg_out, vgg_gt, vgg_comp): \n        """"""Perceptual loss based on VGG16, see. eq. 3 in paper""""""       \n        loss = 0\n        for o, c, g in zip(vgg_out, vgg_comp, vgg_gt):\n            loss += self.l1(o, g) + self.l1(c, g)\n        return loss\n        \n    def loss_style(self, output, vgg_gt):\n        """"""Style loss based on output/computation, used for both eq. 4 & 5 in paper""""""\n        loss = 0\n        for o, g in zip(output, vgg_gt):\n            loss += self.l1(self.gram_matrix(o), self.gram_matrix(g))\n        return loss\n    \n    def loss_tv(self, mask, y_comp):\n        """"""Total variation loss, used for smoothing the hole region, see. eq. 6""""""\n\n        # Create dilated hole region using a 3x3 kernel of all 1s.\n        kernel = K.ones(shape=(3, 3, mask.shape[3], mask.shape[3]))\n        dilated_mask = K.conv2d(1-mask, kernel, data_format=\'channels_last\', padding=\'same\')\n\n        # Cast values to be [0., 1.], and compute dilated hole region of y_comp\n        dilated_mask = K.cast(K.greater(dilated_mask, 0), \'float32\')\n        P = dilated_mask * y_comp\n\n        # Calculate total variation loss\n        a = self.l1(P[:,1:,:,:], P[:,:-1,:,:])\n        b = self.l1(P[:,:,1:,:], P[:,:,:-1,:])        \n        return a+b\n\n    def fit_generator(self, generator, *args, **kwargs):\n        """"""Fit the U-Net to a (images, targets) generator\n\n        Args:\n            generator (generator): generator supplying input image & mask, as well as targets.\n            *args: arguments to be passed to fit_generator\n            **kwargs: keyword arguments to be passed to fit_generator\n        """"""\n        self.model.fit_generator(\n            generator,\n            *args, **kwargs\n        )\n        \n    def summary(self):\n        """"""Get summary of the UNet model""""""\n        print(self.model.summary())\n\n    def load(self, filepath, train_bn=True, lr=0.0002):\n\n        # Create UNet-like model\n        self.model, inputs_mask = self.build_pconv_unet(train_bn)\n        self.compile_pconv_unet(self.model, inputs_mask, lr) \n\n        # Load weights into model\n        epoch = int(os.path.basename(filepath).split(\'.\')[1].split(\'-\')[0])\n        assert epoch > 0, ""Could not parse weight file. Should include the epoch""\n        self.current_epoch = epoch\n        self.model.load_weights(filepath)        \n\n    @staticmethod\n    def PSNR(y_true, y_pred):\n        """"""\n        PSNR is Peek Signal to Noise Ratio, see https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n        The equation is:\n        PSNR = 20 * log10(MAX_I) - 10 * log10(MSE)\n        \n        Our input is scaled with be within the range -2.11 to 2.64 (imagenet value scaling). We use the difference between these\n        two values (4.75) as MAX_I        \n        """"""        \n        #return 20 * K.log(4.75) / K.log(10.0) - 10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0) \n        return - 10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0) \n\n    @staticmethod\n    def current_timestamp():\n        return datetime.now().strftime(\'%Y-%m-%d-%H-%M-%S\')\n    \n    @staticmethod\n    def l1(y_true, y_pred):\n        """"""Calculate the L1 loss used in all loss calculations""""""\n        if K.ndim(y_true) == 4:\n            return K.mean(K.abs(y_pred - y_true), axis=[1,2,3])\n        elif K.ndim(y_true) == 3:\n            return K.mean(K.abs(y_pred - y_true), axis=[1,2])\n        else:\n            raise NotImplementedError(""Calculating L1 loss on 1D tensors? should not occur for this network"")\n    \n    @staticmethod\n    def gram_matrix(x, norm_by_channels=False):\n        """"""Calculate gram matrix used in style loss""""""\n        \n        # Assertions on input\n        assert K.ndim(x) == 4, \'Input tensor should be a 4d (B, H, W, C) tensor\'\n        assert K.image_data_format() == \'channels_last\', ""Please use channels-last format""        \n        \n        # Permute channels and get resulting shape\n        x = K.permute_dimensions(x, (0, 3, 1, 2))\n        shape = K.shape(x)\n        B, C, H, W = shape[0], shape[1], shape[2], shape[3]\n        \n        # Reshape x and do batch dot product\n        features = K.reshape(x, K.stack([B, C, H*W]))\n        gram = K.batch_dot(features, features, axes=2)\n        \n        # Normalize with channels, height and width\n        gram = gram /  K.cast(C * H * W, x.dtype)\n        \n        return gram\n    \n    # Prediction functions\n    ######################\n    def predict(self, sample, **kwargs):\n        """"""Run prediction using this model""""""\n        return self.model.predict(sample, **kwargs)\n'"
libs/util.py,0,"b'import os\nfrom random import randint, seed\nimport itertools\nimport numpy as np\nimport cv2\n\n\nclass MaskGenerator():\n\n    def __init__(self, height, width, channels=3, rand_seed=None, filepath=None):\n        """"""Convenience functions for generating masks to be used for inpainting training\n        \n        Arguments:\n            height {int} -- Mask height\n            width {width} -- Mask width\n        \n        Keyword Arguments:\n            channels {int} -- Channels to output (default: {3})\n            rand_seed {[type]} -- Random seed (default: {None})\n            filepath {[type]} -- Load masks from filepath. If None, generate masks with OpenCV (default: {None})\n        """"""\n\n        self.height = height\n        self.width = width\n        self.channels = channels\n        self.filepath = filepath\n\n        # If filepath supplied, load the list of masks within the directory\n        self.mask_files = []\n        if self.filepath:\n            filenames = [f for f in os.listdir(self.filepath)]\n            self.mask_files = [f for f in filenames if any(filetype in f.lower() for filetype in [\'.jpeg\', \'.png\', \'.jpg\'])]\n            print("">> Found {} masks in {}"".format(len(self.mask_files), self.filepath))        \n\n        # Seed for reproducibility\n        if rand_seed:\n            seed(rand_seed)\n\n    def _generate_mask(self):\n        """"""Generates a random irregular mask with lines, circles and elipses""""""\n\n        img = np.zeros((self.height, self.width, self.channels), np.uint8)\n\n        # Set size scale\n        size = int((self.width + self.height) * 0.03)\n        if self.width < 64 or self.height < 64:\n            raise Exception(""Width and Height of mask must be at least 64!"")\n        \n        # Draw random lines\n        for _ in range(randint(1, 20)):\n            x1, x2 = randint(1, self.width), randint(1, self.width)\n            y1, y2 = randint(1, self.height), randint(1, self.height)\n            thickness = randint(3, size)\n            cv2.line(img,(x1,y1),(x2,y2),(1,1,1),thickness)\n            \n        # Draw random circles\n        for _ in range(randint(1, 20)):\n            x1, y1 = randint(1, self.width), randint(1, self.height)\n            radius = randint(3, size)\n            cv2.circle(img,(x1,y1),radius,(1,1,1), -1)\n            \n        # Draw random ellipses\n        for _ in range(randint(1, 20)):\n            x1, y1 = randint(1, self.width), randint(1, self.height)\n            s1, s2 = randint(1, self.width), randint(1, self.height)\n            a1, a2, a3 = randint(3, 180), randint(3, 180), randint(3, 180)\n            thickness = randint(3, size)\n            cv2.ellipse(img, (x1,y1), (s1,s2), a1, a2, a3,(1,1,1), thickness)\n        \n        return 1-img\n\n    def _load_mask(self, rotation=True, dilation=True, cropping=True):\n        """"""Loads a mask from disk, and optionally augments it""""""\n\n        # Read image\n        mask = cv2.imread(os.path.join(self.filepath, np.random.choice(self.mask_files, 1, replace=False)[0]))\n        \n        # Random rotation\n        if rotation:\n            rand = np.random.randint(-180, 180)\n            M = cv2.getRotationMatrix2D((mask.shape[1]/2, mask.shape[0]/2), rand, 1.5)\n            mask = cv2.warpAffine(mask, M, (mask.shape[1], mask.shape[0]))\n            \n        # Random dilation\n        if dilation:\n            rand = np.random.randint(5, 47)\n            kernel = np.ones((rand, rand), np.uint8) \n            mask = cv2.erode(mask, kernel, iterations=1)\n            \n        # Random cropping\n        if cropping:\n            x = np.random.randint(0, mask.shape[1] - self.width)\n            y = np.random.randint(0, mask.shape[0] - self.height)\n            mask = mask[y:y+self.height, x:x+self.width]\n\n        return (mask > 1).astype(np.uint8)\n\n    def sample(self, random_seed=None):\n        """"""Retrieve a random mask""""""\n        if random_seed:\n            seed(random_seed)\n        if self.filepath and len(self.mask_files) > 0:\n            return self._load_mask()\n        else:\n            return self._generate_mask()\n\n\nclass ImageChunker(object): \n    \n    def __init__(self, rows, cols, overlap):\n        self.rows = rows\n        self.cols = cols\n        self.overlap = overlap\n    \n    def perform_chunking(self, img_size, chunk_size):\n        """"""\n        Given an image dimension img_size, return list of (start, stop) \n        tuples to perform chunking of chunk_size\n        """"""\n        chunks, i = [], 0\n        while True:\n            chunks.append((i*(chunk_size - self.overlap/2), i*(chunk_size - self.overlap/2)+chunk_size))\n            i+=1\n            if chunks[-1][1] > img_size:\n                break\n        n_count = len(chunks)        \n        chunks[-1] = tuple(x - (n_count*chunk_size - img_size - (n_count-1)*self.overlap/2) for x in chunks[-1])\n        chunks = [(int(x), int(y)) for x, y in chunks]\n        return chunks\n    \n    def get_chunks(self, img, scale=1):\n        """"""\n        Get width and height lists of (start, stop) tuples for chunking of img.\n        """"""\n        x_chunks, y_chunks = [(0, self.rows)], [(0, self.cols)]        \n        if img.shape[0] > self.rows:\n            x_chunks = self.perform_chunking(img.shape[0], self.rows)\n        else:\n            x_chunks = [(0, img.shape[0])]\n        if img.shape[1] > self.cols:\n            y_chunks = self.perform_chunking(img.shape[1], self.cols)\n        else:\n            y_chunks = [(0, img.shape[1])]\n        return x_chunks, y_chunks    \n    \n    def dimension_preprocess(self, img, padding=True):\n        """"""\n        In case of prediction on image of different size than 512x512,\n        this function is used to add padding and chunk up the image into pieces\n        of 512x512, which can then later be reconstructed into the original image\n        using the dimension_postprocess() function.\n        """"""\n    \n        # Assert single image input\n        assert len(img.shape) == 3, ""Image dimension expected to be (H, W, C)""\n    \n        # Check if we are adding padding for too small images\n        if padding:\n            \n            # Check if height is too small\n            if img.shape[0] < self.rows:\n                padding = np.ones((self.rows - img.shape[0], img.shape[1], img.shape[2]))\n                img = np.concatenate((img, padding), axis=0)\n    \n            # Check if width is too small\n            if img.shape[1] < self.cols:\n                padding = np.ones((img.shape[0], self.cols - img.shape[1], img.shape[2]))\n                img = np.concatenate((img, padding), axis=1)\n    \n        # Get chunking of the image\n        x_chunks, y_chunks = self.get_chunks(img)\n    \n        # Chunk up the image\n        images = []\n        for x in x_chunks:\n            for y in y_chunks:\n                images.append(\n                    img[x[0]:x[1], y[0]:y[1], :]\n                )\n        images = np.array(images)        \n        return images\n    \n    def dimension_postprocess(self, chunked_images, original_image, scale=1, padding=True):\n        """"""\n        In case of prediction on image of different size than 512x512,\n        the dimension_preprocess  function is used to add padding and chunk \n        up the image into pieces of 512x512, and this function is used to \n        reconstruct these pieces into the original image.\n        """"""\n    \n        # Assert input dimensions\n        assert len(original_image.shape) == 3, ""Image dimension expected to be (H, W, C)""\n        assert len(chunked_images.shape) == 4, ""Chunked images dimension expected to be (B, H, W, C)""\n        \n        # Check if we are adding padding for too small images\n        if padding:\n    \n            # Check if height is too small\n            if original_image.shape[0] < self.rows:\n                new_images = []\n                for img in chunked_images:\n                    new_images.append(img[0:scale*original_image.shape[0], :, :])\n                chunked_images = np.array(new_images)\n    \n            # Check if width is too small\n            if original_image.shape[1] < self.cols:\n                new_images = []\n                for img in chunked_images:\n                    new_images.append(img[:, 0:scale*original_image.shape[1], :])\n                chunked_images = np.array(new_images)\n            \n        # Put reconstruction into this array\n        new_shape = (\n            original_image.shape[0]*scale,\n            original_image.shape[1]*scale,\n            original_image.shape[2]\n        )\n        reconstruction = np.zeros(new_shape)\n            \n        # Get the chunks for this image    \n        x_chunks, y_chunks = self.get_chunks(original_image)\n        \n        i = 0\n        s = scale\n        for x in x_chunks:\n            for y in y_chunks:\n                \n                prior_fill = reconstruction != 0\n                chunk = np.zeros(new_shape)\n                chunk[x[0]*s:x[1]*s, y[0]*s:y[1]*s, :] += chunked_images[i]\n                chunk_fill = chunk != 0\n                \n                reconstruction += chunk\n                reconstruction[prior_fill & chunk_fill] = reconstruction[prior_fill & chunk_fill] / 2\n    \n                i += 1\n        \n        return reconstruction'"
