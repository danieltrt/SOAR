file_path,api_count,code
config/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-22 \xe4\xb8\x8b\xe5\x8d\x883:25\n# @Author  : Luo Yao\n# @Site    : http://github.com/TJCVRS\n# @File    : __init__.py.py\n# @IDE: PyCharm Community Edition'
config/global_config.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-22 \xe4\xb8\x8b\xe5\x8d\x883:25\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : global_config.py\n# @IDE: PyCharm Community Edition\n""""""\nSet some global configuration\n""""""\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by: from config import cfg\n\ncfg = __C\n\n__C.ARCH = edict()\n\n# Number of units in each LSTM cell\n__C.ARCH.HIDDEN_UNITS = 256\n# Number of stacked LSTM cells\n__C.ARCH.HIDDEN_LAYERS = 2\n# Sequence length.  This has to be the width of the final feature map of the CNN, which is input size width / 4\n# __C.ARCH.SEQ_LENGTH = 70  # cn dataset\n__C.ARCH.SEQ_LENGTH = 25  # synth90k dataset\n# Width x height into which training / testing images are resized before feeding into the network\n# __C.ARCH.INPUT_SIZE = (280, 32)  # cn dataset\n__C.ARCH.INPUT_SIZE = (100, 32)  # synth90k dataset\n# Number of channels in images\n__C.ARCH.INPUT_CHANNELS = 3\n# Number character classes\n# __C.ARCH.NUM_CLASSES = 5825  # cn dataset\n__C.ARCH.NUM_CLASSES = 37  # synth90k dataset\n\n# Train options\n__C.TRAIN = edict()\n\n# Use early stopping?\n__C.TRAIN.EARLY_STOPPING = False\n# Wait at least this many epochs without improvement in the cost function\n__C.TRAIN.PATIENCE_EPOCHS = 6\n# Expect at least this improvement in one epoch in order to reset the early stopping counter\n__C.TRAIN.PATIENCE_DELTA = 1e-3\n\n# Set the shadownet training epochs\n__C.TRAIN.EPOCHS = 80010\n# Set the display step\n__C.TRAIN.DISPLAY_STEP = 1\n# Set the test display step during training process\n__C.TRAIN.TEST_DISPLAY_STEP = 100\n# Set the momentum parameter of the optimizer\n__C.TRAIN.MOMENTUM = 0.9\n# Set the initial learning rate\n__C.TRAIN.LEARNING_RATE = 0.01\n# Set the GPU resource used during training process\n__C.TRAIN.GPU_MEMORY_FRACTION = 0.9\n# Set the GPU allow growth parameter during tensorflow training process\n__C.TRAIN.TF_ALLOW_GROWTH = True\n# Set the shadownet training batch size\n__C.TRAIN.BATCH_SIZE = 64\n# Set the shadownet validation batch size\n__C.TRAIN.VAL_BATCH_SIZE = 32\n# Set the learning rate decay steps\n__C.TRAIN.LR_DECAY_STEPS = 500000\n# Set the learning rate decay rate\n__C.TRAIN.LR_DECAY_RATE = 0.1\n# Update learning rate in jumps?\n__C.TRAIN.LR_STAIRCASE = True\n# Set multi process nums\n__C.TRAIN.CPU_MULTI_PROCESS_NUMS = 6\n# Set Gpu nums\n__C.TRAIN.GPU_NUM = 2\n# Set moving average decay\n__C.TRAIN.MOVING_AVERAGE_DECAY = 0.9999\n# Set val display step\n__C.TRAIN.VAL_DISPLAY_STEP = 1000\n\n# Test options\n__C.TEST = edict()\n\n# Set the GPU resource used during testing process\n__C.TEST.GPU_MEMORY_FRACTION = 0.6\n# Set the GPU allow growth parameter during tensorflow testing process\n__C.TEST.TF_ALLOW_GROWTH = False\n# Set the test batch size\n__C.TEST.BATCH_SIZE = 32\n'"
crnn_model/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-21 \xe4\xb8\x8b\xe5\x8d\x886:37\n# @Author  : Luo Yao\n# @Site    : http://github.com/TJCVRS\n# @File    : __init__.py.py\n# @IDE: PyCharm Community Edition'
crnn_model/cnn_basenet.py,100,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-18 \xe4\xb8\x8b\xe5\x8d\x883:59\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : cnn_basenet.py\n# @IDE: PyCharm Community Edition\n""""""\nThe base convolution neural networks mainly implement some useful cnn functions\n""""""\nimport tensorflow as tf\nfrom tensorflow.python.training import moving_averages\nfrom tensorflow.contrib.framework import add_model_variable\nimport numpy as np\n\n\nclass CNNBaseModel(object):\n    """"""\n    Base model for other specific cnn ctpn_models\n    """"""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def conv2d(inputdata, out_channel, kernel_size, padding=\'SAME\',\n               stride=1, w_init=None, b_init=None,\n               split=1, use_bias=True, data_format=\'NHWC\', name=None):\n        """"""\n        Packing the tensorflow conv2d function.\n        :param name: op name\n        :param inputdata: A 4D tensorflow tensor which ust have known number of channels, but can have other\n        unknown dimensions.\n        :param out_channel: number of output channel.\n        :param kernel_size: int so only support square kernel convolution\n        :param padding: \'VALID\' or \'SAME\'\n        :param stride: int so only support square stride\n        :param w_init: initializer for convolution weights\n        :param b_init: initializer for bias\n        :param split: split channels as used in Alexnet mainly group for GPU memory save.\n        :param use_bias:  whether to use bias.\n        :param data_format: default set to NHWC according tensorflow\n        :return: tf.Tensor named ``output``\n        """"""\n        with tf.variable_scope(name):\n            in_shape = inputdata.get_shape().as_list()\n            channel_axis = 3 if data_format == \'NHWC\' else 1\n            in_channel = in_shape[channel_axis]\n\n            assert in_channel is not None, ""[Conv2D] Input cannot have unknown channel!""\n            assert in_channel % split == 0\n            assert out_channel % split == 0\n\n            padding = padding.upper()\n\n            if isinstance(kernel_size, list):\n                filter_shape = [kernel_size[0], kernel_size[1]] + [in_channel / split, out_channel]\n            else:\n                filter_shape = [kernel_size, kernel_size] + [in_channel / split, out_channel]\n\n            if isinstance(stride, list):\n                strides = [1, stride[0], stride[1], 1] if data_format == \'NHWC\' \\\n                    else [1, 1, stride[0], stride[1]]\n            else:\n                strides = [1, stride, stride, 1] if data_format == \'NHWC\' \\\n                    else [1, 1, stride, stride]\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            w = tf.get_variable(\'W\', filter_shape, initializer=w_init)\n            b = None\n\n            if use_bias:\n                b = tf.get_variable(\'b\', [out_channel], initializer=b_init)\n\n            if split == 1:\n                conv = tf.nn.conv2d(inputdata, w, strides, padding, data_format=data_format)\n            else:\n                inputs = tf.split(inputdata, split, channel_axis)\n                kernels = tf.split(w, split, 3)\n                outputs = [tf.nn.conv2d(i, k, strides, padding, data_format=data_format)\n                           for i, k in zip(inputs, kernels)]\n                conv = tf.concat(outputs, channel_axis)\n\n            ret = tf.identity(tf.nn.bias_add(conv, b, data_format=data_format)\n                              if use_bias else conv, name=name)\n\n        return ret\n\n    @staticmethod\n    def relu(inputdata, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :return:\n        """"""\n        return tf.nn.relu(features=inputdata, name=name)\n\n    @staticmethod\n    def sigmoid(inputdata, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :return:\n        """"""\n        return tf.nn.sigmoid(x=inputdata, name=name)\n\n    @staticmethod\n    def maxpooling(inputdata, kernel_size, stride=None, padding=\'VALID\',\n                   data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param kernel_size:\n        :param stride:\n        :param padding:\n        :param data_format:\n        :return:\n        """"""\n        padding = padding.upper()\n\n        if stride is None:\n            stride = kernel_size\n\n        if isinstance(kernel_size, list):\n            kernel = [1, kernel_size[0], kernel_size[1], 1] if data_format == \'NHWC\' else \\\n                [1, 1, kernel_size[0], kernel_size[1]]\n        else:\n            kernel = [1, kernel_size, kernel_size, 1] if data_format == \'NHWC\' \\\n                else [1, 1, kernel_size, kernel_size]\n\n        if isinstance(stride, list):\n            strides = [1, stride[0], stride[1], 1] if data_format == \'NHWC\' \\\n                else [1, 1, stride[0], stride[1]]\n        else:\n            strides = [1, stride, stride, 1] if data_format == \'NHWC\' \\\n                else [1, 1, stride, stride]\n\n        return tf.nn.max_pool(value=inputdata, ksize=kernel, strides=strides, padding=padding,\n                              data_format=data_format, name=name)\n\n    @staticmethod\n    def avgpooling(inputdata, kernel_size, stride=None, padding=\'VALID\',\n                   data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param kernel_size:\n        :param stride:\n        :param padding:\n        :param data_format:\n        :return:\n        """"""\n        if stride is None:\n            stride = kernel_size\n\n        kernel = [1, kernel_size, kernel_size, 1] if data_format == \'NHWC\' \\\n            else [1, 1, kernel_size, kernel_size]\n\n        strides = [1, stride, stride, 1] if data_format == \'NHWC\' else [1, 1, stride, stride]\n\n        return tf.nn.avg_pool(value=inputdata, ksize=kernel, strides=strides, padding=padding,\n                              data_format=data_format, name=name)\n\n    @staticmethod\n    def globalavgpooling(inputdata, data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param data_format:\n        :return:\n        """"""\n        assert inputdata.shape.ndims == 4\n        assert data_format in [\'NHWC\', \'NCHW\']\n\n        axis = [1, 2] if data_format == \'NHWC\' else [2, 3]\n\n        return tf.reduce_mean(input_tensor=inputdata, axis=axis, name=name)\n\n    @staticmethod\n    def layernorm(inputdata, epsilon=1e-5, use_bias=True, use_scale=True,\n                  data_format=\'NHWC\', name=None):\n        """"""\n        :param name:\n        :param inputdata:\n        :param epsilon: epsilon to avoid divide-by-zero.\n        :param use_bias: whether to use the extra affine transformation or not.\n        :param use_scale: whether to use the extra affine transformation or not.\n        :param data_format:\n        :return:\n        """"""\n        shape = inputdata.get_shape().as_list()\n        ndims = len(shape)\n        assert ndims in [2, 4]\n\n        mean, var = tf.nn.moments(inputdata, list(range(1, len(shape))), keep_dims=True)\n\n        if data_format == \'NCHW\':\n            channnel = shape[1]\n            new_shape = [1, channnel, 1, 1]\n        else:\n            channnel = shape[-1]\n            new_shape = [1, 1, 1, channnel]\n        if ndims == 2:\n            new_shape = [1, channnel]\n\n        if use_bias:\n            beta = tf.get_variable(\'beta\', [channnel], initializer=tf.constant_initializer())\n            beta = tf.reshape(beta, new_shape)\n        else:\n            beta = tf.zeros([1] * ndims, name=\'beta\')\n        if use_scale:\n            gamma = tf.get_variable(\'gamma\', [channnel], initializer=tf.constant_initializer(1.0))\n            gamma = tf.reshape(gamma, new_shape)\n        else:\n            gamma = tf.ones([1] * ndims, name=\'gamma\')\n\n        return tf.nn.batch_normalization(inputdata, mean, var, beta, gamma, epsilon, name=name)\n\n    @staticmethod\n    def instancenorm(inputdata, epsilon=1e-5, data_format=\'NHWC\', use_affine=True, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param epsilon:\n        :param data_format:\n        :param use_affine:\n        :return:\n        """"""\n        shape = inputdata.get_shape().as_list()\n        if len(shape) != 4:\n            raise ValueError(""Input data of instancebn layer has to be 4D tensor"")\n\n        if data_format == \'NHWC\':\n            axis = [1, 2]\n            ch = shape[3]\n            new_shape = [1, 1, 1, ch]\n        else:\n            axis = [2, 3]\n            ch = shape[1]\n            new_shape = [1, ch, 1, 1]\n        if ch is None:\n            raise ValueError(""Input of instancebn require known channel!"")\n\n        mean, var = tf.nn.moments(inputdata, axis, keep_dims=True)\n\n        if not use_affine:\n            return tf.divide(inputdata - mean, tf.sqrt(var + epsilon), name=\'output\')\n\n        beta = tf.get_variable(\'beta\', [ch], initializer=tf.constant_initializer())\n        beta = tf.reshape(beta, new_shape)\n        gamma = tf.get_variable(\'gamma\', [ch], initializer=tf.constant_initializer(1.0))\n        gamma = tf.reshape(gamma, new_shape)\n        return tf.nn.batch_normalization(inputdata, mean, var, beta, gamma, epsilon, name=name)\n\n    @staticmethod\n    def dropout(inputdata, keep_prob, is_training, name, noise_shape=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param keep_prob:\n        :param is_training\n        :param noise_shape:\n        :return:\n        """"""\n\n        return tf.cond(\n            pred=is_training,\n            true_fn=lambda: tf.nn.dropout(\n                inputdata, keep_prob=keep_prob, noise_shape=noise_shape\n            ),\n            false_fn=lambda: inputdata,\n            name=name\n        )\n\n    @staticmethod\n    def fullyconnect(inputdata, out_dim, w_init=None, b_init=None,\n                     use_bias=True, name=None):\n        """"""\n        Fully-Connected layer, takes a N>1D tensor and returns a 2D tensor.\n        It is an equivalent of `tf.layers.dense` except for naming conventions.\n\n        :param inputdata:  a tensor to be flattened except for the first dimension.\n        :param out_dim: output dimension\n        :param w_init: initializer for w. Defaults to `variance_scaling_initializer`.\n        :param b_init: initializer for b. Defaults to zero\n        :param use_bias: whether to use bias.\n        :param name:\n        :return: tf.Tensor: a NC tensor named ``output`` with attribute `variables`.\n        """"""\n        shape = inputdata.get_shape().as_list()[1:]\n        if None not in shape:\n            inputdata = tf.reshape(inputdata, [-1, int(np.prod(shape))])\n        else:\n            inputdata = tf.reshape(inputdata, tf.stack([tf.shape(inputdata)[0], -1]))\n\n        if w_init is None:\n            w_init = tf.contrib.layers.variance_scaling_initializer()\n        if b_init is None:\n            b_init = tf.constant_initializer()\n\n        ret = tf.layers.dense(inputs=inputdata, activation=lambda x: tf.identity(x, name=\'output\'),\n                              use_bias=use_bias, name=name,\n                              kernel_initializer=w_init,\n                              bias_initializer=b_init,\n                              trainable=True, units=out_dim)\n        return ret\n\n    @staticmethod\n    def layerbn(inputdata, is_training, name, momentum=0.999, eps=1e-3):\n        """"""\n\n        :param inputdata:\n        :param is_training:\n        :param name:\n        :param momentum:\n        :param eps:\n        :return:\n        """"""\n\n        return tf.layers.batch_normalization(\n            inputs=inputdata, training=is_training, name=name, momentum=momentum, epsilon=eps)\n\n    @staticmethod\n    def layerbn_distributed(list_input, stats_mode, data_format=\'NHWC\',\n                            float_type=tf.float32, trainable=True,\n                            use_gamma=True, use_beta=True, bn_epsilon=1e-5,\n                            bn_ema=0.9, name=\'BatchNorm\'):\n        """"""\n        Batch norm for distributed training process\n        :param list_input:\n        :param stats_mode:\n        :param data_format:\n        :param float_type:\n        :param trainable:\n        :param use_gamma:\n        :param use_beta:\n        :param bn_epsilon:\n        :param bn_ema:\n        :param name:\n        :return:\n        """"""\n\n        def _get_bn_variables(_n_out, _use_scale, _use_bias, _trainable, _float_type):\n\n            if _use_bias:\n                _beta = tf.get_variable(\'beta\', [_n_out],\n                                        initializer=tf.constant_initializer(),\n                                        trainable=_trainable,\n                                        dtype=_float_type)\n            else:\n                _beta = tf.zeros([_n_out], name=\'beta\')\n            if _use_scale:\n                _gamma = tf.get_variable(\'gamma\', [_n_out],\n                                         initializer=tf.constant_initializer(1.0),\n                                         trainable=_trainable,\n                                         dtype=_float_type)\n            else:\n                _gamma = tf.ones([_n_out], name=\'gamma\')\n\n            _moving_mean = tf.get_variable(\'moving_mean\', [_n_out],\n                                           initializer=tf.constant_initializer(),\n                                           trainable=False,\n                                           dtype=_float_type)\n            _moving_var = tf.get_variable(\'moving_variance\', [_n_out],\n                                          initializer=tf.constant_initializer(1),\n                                          trainable=False,\n                                          dtype=_float_type)\n            return _beta, _gamma, _moving_mean, _moving_var\n\n        def _update_bn_ema(_xn, _batch_mean, _batch_var, _moving_mean, _moving_var, _decay):\n\n            _update_op1 = moving_averages.assign_moving_average(\n                _moving_mean, _batch_mean, _decay, zero_debias=False,\n                name=\'mean_ema_op\')\n            _update_op2 = moving_averages.assign_moving_average(\n                _moving_var, _batch_var, _decay, zero_debias=False,\n                name=\'var_ema_op\')\n            add_model_variable(moving_mean)\n            add_model_variable(moving_var)\n\n            # seems faster than delayed update, but might behave otherwise in distributed settings.\n            with tf.control_dependencies([_update_op1, _update_op2]):\n                return tf.identity(xn, name=\'output\')\n\n        # ======================== Checking valid values =========================\n        if data_format not in [\'NHWC\', \'NCHW\']:\n            raise TypeError(\n                ""Only two data formats are supported at this moment: \'NHWC\' or \'NCHW\', ""\n                ""%s is an unknown data format."" % data_format)\n        assert type(list_input) == list\n\n        # ======================== Setting default values =========================\n        shape = list_input[0].get_shape().as_list()\n        assert len(shape) in [2, 4]\n        n_out = shape[-1]\n        if data_format == \'NCHW\':\n            n_out = shape[1]\n\n        # ======================== Main operations =============================\n        means = []\n        square_means = []\n        for i in range(len(list_input)):\n            with tf.device(\'/gpu:%d\' % i):\n                batch_mean = tf.reduce_mean(list_input[i], [0, 1, 2])\n                batch_square_mean = tf.reduce_mean(tf.square(list_input[i]), [0, 1, 2])\n                means.append(batch_mean)\n                square_means.append(batch_square_mean)\n\n        # if your GPUs have NVLinks and you\'ve install NCCL2, you can change `/cpu:0` to `/gpu:0`\n        with tf.device(\'/cpu:0\'):\n            shape = tf.shape(list_input[0])\n            num = shape[0] * shape[1] * shape[2] * len(list_input)\n            mean = tf.reduce_mean(means, axis=0)\n            var = tf.reduce_mean(square_means, axis=0) - tf.square(mean)\n            var *= tf.cast(num, float_type) / tf.cast(num - 1, float_type)  # unbiased variance\n\n        list_output = []\n        for i in range(len(list_input)):\n            with tf.device(\'/gpu:%d\' % i):\n                with tf.variable_scope(name, reuse=i > 0):\n                    beta, gamma, moving_mean, moving_var = _get_bn_variables(\n                        n_out, use_gamma, use_beta, trainable, float_type)\n\n                    if \'train\' in stats_mode:\n                        xn = tf.nn.batch_normalization(\n                            list_input[i], mean, var, beta, gamma, bn_epsilon)\n                        if tf.get_variable_scope().reuse or \'gather\' not in stats_mode:\n                            list_output.append(xn)\n                        else:\n                            # gather stats and it is the main gpu device.\n                            xn = _update_bn_ema(xn, mean, var, moving_mean, moving_var, bn_ema)\n                            list_output.append(xn)\n                    else:\n                        xn = tf.nn.batch_normalization(\n                            list_input[i], moving_mean, moving_var, beta, gamma, bn_epsilon)\n                        list_output.append(xn)\n\n        return list_output\n\n    @staticmethod\n    def layergn(inputdata, name, group_size=32, esp=1e-5):\n        """"""\n\n        :param inputdata:\n        :param name:\n        :param group_size:\n        :param esp:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            inputdata = tf.transpose(inputdata, [0, 3, 1, 2])\n            n, c, h, w = inputdata.get_shape().as_list()\n            group_size = min(group_size, c)\n            inputdata = tf.reshape(inputdata, [-1, group_size, c // group_size, h, w])\n            mean, var = tf.nn.moments(inputdata, [2, 3, 4], keep_dims=True)\n            inputdata = (inputdata - mean) / tf.sqrt(var + esp)\n\n            # \xe6\xaf\x8f\xe4\xb8\xaa\xe9\x80\x9a\xe9\x81\x93\xe7\x9a\x84gamma\xe5\x92\x8cbeta\n            gamma = tf.Variable(tf.constant(1.0, shape=[c]), dtype=tf.float32, name=\'gamma\')\n            beta = tf.Variable(tf.constant(0.0, shape=[c]), dtype=tf.float32, name=\'beta\')\n            gamma = tf.reshape(gamma, [1, c, 1, 1])\n            beta = tf.reshape(beta, [1, c, 1, 1])\n\n            # \xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xba\xe6\x96\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe6\x8d\xa2 [n, c, h, w, c] \xe5\x88\xb0 [n, h, w, c]\n            output = tf.reshape(inputdata, [-1, c, h, w])\n            output = output * gamma + beta\n            output = tf.transpose(output, [0, 2, 3, 1])\n\n        return output\n\n    @staticmethod\n    def squeeze(inputdata, axis=None, name=None):\n        """"""\n\n        :param inputdata:\n        :param axis:\n        :param name:\n        :return:\n        """"""\n        return tf.squeeze(input=inputdata, axis=axis, name=name)\n\n    @staticmethod\n    def deconv2d(inputdata, out_channel, kernel_size, padding=\'SAME\',\n                 stride=1, w_init=None, b_init=None,\n                 use_bias=True, activation=None, data_format=\'channels_last\',\n                 trainable=True, name=None):\n        """"""\n        Packing the tensorflow conv2d function.\n        :param name: op name\n        :param inputdata: A 4D tensorflow tensor which ust have known number of channels, but can have other\n        unknown dimensions.\n        :param out_channel: number of output channel.\n        :param kernel_size: int so only support square kernel convolution\n        :param padding: \'VALID\' or \'SAME\'\n        :param stride: int so only support square stride\n        :param w_init: initializer for convolution weights\n        :param b_init: initializer for bias\n        :param activation: whether to apply a activation func to deconv result\n        :param use_bias:  whether to use bias.\n        :param data_format: default set to NHWC according tensorflow\n        :param trainable:\n        :return: tf.Tensor named ``output``\n        """"""\n        with tf.variable_scope(name):\n            in_shape = inputdata.get_shape().as_list()\n            channel_axis = 3 if data_format == \'channels_last\' else 1\n            in_channel = in_shape[channel_axis]\n            assert in_channel is not None, ""[Deconv2D] Input cannot have unknown channel!""\n\n            padding = padding.upper()\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            ret = tf.layers.conv2d_transpose(inputs=inputdata, filters=out_channel,\n                                             kernel_size=kernel_size,\n                                             strides=stride, padding=padding,\n                                             data_format=data_format,\n                                             activation=activation, use_bias=use_bias,\n                                             kernel_initializer=w_init,\n                                             bias_initializer=b_init, trainable=trainable,\n                                             name=name)\n        return ret\n\n    @staticmethod\n    def dilation_conv(input_tensor, k_size, out_dims, rate, padding=\'SAME\',\n                      w_init=None, b_init=None, use_bias=False, name=None):\n        """"""\n\n        :param input_tensor:\n        :param k_size:\n        :param out_dims:\n        :param rate:\n        :param padding:\n        :param w_init:\n        :param b_init:\n        :param use_bias:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            in_shape = input_tensor.get_shape().as_list()\n            in_channel = in_shape[3]\n\n            assert in_channel is not None, ""[Conv2D] Input cannot have unknown channel!""\n\n            padding = padding.upper()\n\n            if isinstance(k_size, list):\n                filter_shape = [k_size[0], k_size[1]] + [in_channel, out_dims]\n            else:\n                filter_shape = [k_size, k_size] + [in_channel, out_dims]\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            w = tf.get_variable(\'W\', filter_shape, initializer=w_init)\n            b = None\n\n            if use_bias:\n                b = tf.get_variable(\'b\', [out_dims], initializer=b_init)\n\n            conv = tf.nn.atrous_conv2d(value=input_tensor, filters=w, rate=rate,\n                                       padding=padding, name=\'dilation_conv\')\n\n            if use_bias:\n                ret = tf.add(conv, b)\n            else:\n                ret = conv\n\n        return ret\n\n    @staticmethod\n    def spatial_dropout(input_tensor, keep_prob, is_training, name, seed=1234):\n        """"""\n        \xe7\xa9\xba\xe9\x97\xb4dropout\xe5\xae\x9e\xe7\x8e\xb0\n        :param input_tensor:\n        :param keep_prob:\n        :param is_training:\n        :param name:\n        :param seed:\n        :return:\n        """"""\n\n        def f1():\n            input_shape = input_tensor.get_shape().as_list()\n            noise_shape = tf.constant(value=[input_shape[0], 1, 1, input_shape[3]])\n            return tf.nn.dropout(input_tensor, keep_prob, noise_shape, seed=seed, name=""spatial_dropout"")\n\n        def f2():\n            return input_tensor\n\n        with tf.variable_scope(name_or_scope=name):\n\n            output = tf.cond(is_training, f1, f2)\n\n            return output\n\n    @staticmethod\n    def lrelu(inputdata, name, alpha=0.2):\n        """"""\n\n        :param inputdata:\n        :param alpha:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            return tf.nn.relu(inputdata) - alpha * tf.nn.relu(-inputdata)\n\n    @staticmethod\n    def pad(inputdata, paddings, name):\n        """"""\n\n        :param inputdata:\n        :param paddings:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name):\n            return tf.pad(tensor=inputdata, paddings=paddings)\n'"
crnn_model/crnn_net.py,21,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-21 \xe4\xb8\x8b\xe5\x8d\x886:39\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : crnn_net.py\n# @IDE: PyCharm Community Edition\n""""""\nImplement the crnn model mentioned in An End-to-End Trainable Neural Network for Image-based Sequence\nRecognition and Its Application to Scene Text Recognition paper\n""""""\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n\nfrom crnn_model import cnn_basenet\nfrom config import global_config\n\nCFG = global_config.cfg\n\n\nclass ShadowNet(cnn_basenet.CNNBaseModel):\n    """"""\n        Implement the crnn model for squence recognition\n    """"""\n    def __init__(self, phase, hidden_nums, layers_nums, num_classes):\n        """"""\n\n        :param phase: \'Train\' or \'Test\'\n        :param hidden_nums: Number of hidden units in each LSTM cell (block)\n        :param layers_nums: Number of LSTM cells (blocks)\n        :param num_classes: Number of classes (different symbols) to detect\n        """"""\n        super(ShadowNet, self).__init__()\n        if phase == \'train\':\n            self._phase = tf.constant(\'train\', dtype=tf.string)\n        else:\n            self._phase = tf.constant(\'test\', dtype=tf.string)\n        self._hidden_nums = hidden_nums\n        self._layers_nums = layers_nums\n        self._num_classes = num_classes\n        self._is_training = self._init_phase()\n\n    def _init_phase(self):\n        """"""\n\n        :return:\n        """"""\n        return tf.equal(self._phase, tf.constant(\'train\', dtype=tf.string))\n\n    def _conv_stage(self, inputdata, out_dims, name):\n        """""" Standard VGG convolutional stage: 2d conv, relu, and maxpool\n\n        :param inputdata: 4D tensor batch x width x height x channels\n        :param out_dims: number of output channels / filters\n        :return: the maxpooled output of the stage\n        """"""\n        with tf.variable_scope(name_or_scope=name):\n\n            conv = self.conv2d(\n                inputdata=inputdata, out_channel=out_dims,\n                kernel_size=3, stride=1, use_bias=True, name=\'conv\'\n            )\n            bn = self.layerbn(\n                inputdata=conv, is_training=self._is_training, name=\'bn\'\n            )\n            relu = self.relu(\n                inputdata=bn, name=\'relu\'\n            )\n            max_pool = self.maxpooling(\n                inputdata=relu, kernel_size=2, stride=2, name=\'max_pool\'\n            )\n        return max_pool\n\n    def _feature_sequence_extraction(self, inputdata, name):\n        """""" Implements section 2.1 of the paper: ""Feature Sequence Extraction""\n\n        :param inputdata: eg. batch*32*100*3 NHWC format\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name):\n            conv1 = self._conv_stage(\n                inputdata=inputdata, out_dims=64, name=\'conv1\'\n            )\n            conv2 = self._conv_stage(\n                inputdata=conv1, out_dims=128, name=\'conv2\'\n            )\n            conv3 = self.conv2d(\n                inputdata=conv2, out_channel=256, kernel_size=3, stride=1, use_bias=False, name=\'conv3\'\n            )\n            bn3 = self.layerbn(\n                inputdata=conv3, is_training=self._is_training, name=\'bn3\'\n            )\n            relu3 = self.relu(\n                inputdata=bn3, name=\'relu3\'\n            )\n            conv4 = self.conv2d(\n                inputdata=relu3, out_channel=256, kernel_size=3, stride=1, use_bias=False, name=\'conv4\'\n            )\n            bn4 = self.layerbn(\n                inputdata=conv4, is_training=self._is_training, name=\'bn4\'\n            )\n            relu4 = self.relu(\n                inputdata=bn4, name=\'relu4\')\n            max_pool4 = self.maxpooling(\n                inputdata=relu4, kernel_size=[2, 1], stride=[2, 1], padding=\'VALID\', name=\'max_pool4\'\n            )\n            conv5 = self.conv2d(\n                inputdata=max_pool4, out_channel=512, kernel_size=3, stride=1, use_bias=False, name=\'conv5\'\n            )\n            bn5 = self.layerbn(\n                inputdata=conv5, is_training=self._is_training, name=\'bn5\'\n            )\n            relu5 = self.relu(\n                inputdata=bn5, name=\'bn5\'\n            )\n            conv6 = self.conv2d(\n                inputdata=relu5, out_channel=512, kernel_size=3, stride=1, use_bias=False, name=\'conv6\'\n            )\n            bn6 = self.layerbn(\n                inputdata=conv6, is_training=self._is_training, name=\'bn6\'\n            )\n            relu6 = self.relu(\n                inputdata=bn6, name=\'relu6\'\n            )\n            max_pool6 = self.maxpooling(\n                inputdata=relu6, kernel_size=[2, 1], stride=[2, 1], name=\'max_pool6\'\n            )\n            conv7 = self.conv2d(\n                inputdata=max_pool6, out_channel=512, kernel_size=2, stride=[2, 1], use_bias=False, name=\'conv7\'\n            )\n            bn7 = self.layerbn(\n                inputdata=conv7, is_training=self._is_training, name=\'bn7\'\n            )\n            relu7 = self.relu(\n                inputdata=bn7, name=\'bn7\'\n            )\n\n        return relu7\n\n    def _map_to_sequence(self, inputdata, name):\n        """""" Implements the map to sequence part of the network.\n\n        This is used to convert the CNN feature map to the sequence used in the stacked LSTM layers later on.\n        Note that this determines the length of the sequences that the LSTM expects\n        :param inputdata:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name):\n\n            shape = inputdata.get_shape().as_list()\n            assert shape[1] == 1  # H of the feature map must equal to 1\n\n            ret = self.squeeze(inputdata=inputdata, axis=1, name=\'squeeze\')\n\n        return ret\n\n    def _sequence_label(self, inputdata, name):\n        """""" Implements the sequence label part of the network\n\n        :param inputdata:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name):\n            # construct stack lstm rcnn layer\n            # forward lstm cell\n            fw_cell_list = [tf.nn.rnn_cell.LSTMCell(nh, forget_bias=1.0) for\n                            nh in [self._hidden_nums] * self._layers_nums]\n            # Backward direction cells\n            bw_cell_list = [tf.nn.rnn_cell.LSTMCell(nh, forget_bias=1.0) for\n                            nh in [self._hidden_nums] * self._layers_nums]\n\n            stack_lstm_layer, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n                fw_cell_list, bw_cell_list, inputdata,\n                dtype=tf.float32\n            )\n            stack_lstm_layer = self.dropout(\n                inputdata=stack_lstm_layer,\n                keep_prob=0.5,\n                is_training=self._is_training,\n                name=\'sequence_drop_out\'\n            )\n\n            [batch_s, _, hidden_nums] = inputdata.get_shape().as_list()  # [batch, width, 2*n_hidden]\n\n            shape = tf.shape(stack_lstm_layer)\n            rnn_reshaped = tf.reshape(stack_lstm_layer, [shape[0] * shape[1], shape[2]])\n\n            w = tf.get_variable(\n                name=\'w\',\n                shape=[hidden_nums, self._num_classes],\n                initializer=tf.truncated_normal_initializer(stddev=0.02),\n                trainable=True\n            )\n\n            # Doing the affine projection\n            logits = tf.matmul(rnn_reshaped, w, name=\'logits\')\n\n            logits = tf.reshape(logits, [shape[0], shape[1], self._num_classes], name=\'logits_reshape\')\n\n            raw_pred = tf.argmax(tf.nn.softmax(logits), axis=2, name=\'raw_prediction\')\n\n            # Swap batch and batch axis\n            rnn_out = tf.transpose(logits, [1, 0, 2], name=\'transpose_time_major\')  # [width, batch, n_classes]\n\n        return rnn_out, raw_pred\n\n    def inference(self, inputdata, name, reuse=False):\n        """"""\n        Main routine to construct the network\n        :param inputdata:\n        :param name:\n        :param reuse:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name, reuse=reuse):\n\n            # first apply the cnn feature extraction stage\n            cnn_out = self._feature_sequence_extraction(\n                inputdata=inputdata, name=\'feature_extraction_module\'\n            )\n\n            # second apply the map to sequence stage\n            sequence = self._map_to_sequence(\n                inputdata=cnn_out, name=\'map_to_sequence_module\'\n            )\n\n            # third apply the sequence label stage\n            net_out, raw_pred = self._sequence_label(\n                inputdata=sequence, name=\'sequence_rnn_module\'\n            )\n\n        return net_out\n\n    def compute_loss(self, inputdata, labels, name, reuse):\n        """"""\n\n        :param inputdata:\n        :param labels:\n        :return:\n        """"""\n\n        inference_ret = self.inference(\n            inputdata=inputdata, name=name, reuse=reuse\n        )\n\n        loss = tf.reduce_mean(\n            tf.nn.ctc_loss(\n                labels=labels, inputs=inference_ret,\n                sequence_length=CFG.ARCH.SEQ_LENGTH * np.ones(CFG.TRAIN.BATCH_SIZE)\n            ),\n            name=\'ctc_loss\'\n        )\n\n        return inference_ret, loss\n'"
data_provider/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-22 \xe4\xb8\x8b\xe5\x8d\x881:39\n# @Author  : Luo Yao\n# @Site    : http://github.com/TJCVRS\n# @File    : __init__.py.py\n# @IDE: PyCharm Community Edition'
data_provider/shadownet_data_feed_pipline.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-2-26 \xe4\xb8\x8b\xe5\x8d\x889:03\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : shadownet_data_feed_pipline.py\n# @IDE: PyCharm\n""""""\nSynth90k dataset feed pipline\n""""""\nimport os\nimport os.path as ops\nimport random\nimport time\n\nimport glob\nimport glog as log\nimport tqdm\nimport tensorflow as tf\n\nfrom config import global_config\nfrom local_utils import establish_char_dict\nfrom data_provider import tf_io_pipline_fast_tools\n\nCFG = global_config.cfg\n\n\nclass CrnnDataProducer(object):\n    """"""\n    Convert raw image file into tfrecords\n    """"""\n    def __init__(self, dataset_dir, char_dict_path=None, ord_map_dict_path=None,\n                 writer_process_nums=4):\n        """"""\n        init crnn data producer\n        :param dataset_dir: image dataset root dir\n        :param char_dict_path: char dict path\n        :param ord_map_dict_path: ord map dict path\n        :param writer_process_nums: the number of writer process\n        """"""\n        if not ops.exists(dataset_dir):\n            raise ValueError(\'Dataset dir {:s} not exist\'.format(dataset_dir))\n\n        # Check image source data\n        self._dataset_dir = dataset_dir\n        self._train_annotation_file_path = ops.join(dataset_dir, \'annotation_train.txt\')\n        self._test_annotation_file_path = ops.join(dataset_dir, \'annotation_test.txt\')\n        self._val_annotation_file_path = ops.join(dataset_dir, \'annotation_val.txt\')\n        self._lexicon_file_path = ops.join(dataset_dir, \'lexicon.txt\')\n        self._char_dict_path = char_dict_path\n        self._ord_map_dict_path = ord_map_dict_path\n        self._writer_process_nums = writer_process_nums\n\n        if not self._is_source_data_complete():\n            raise ValueError(\'Source image data is not complete, \'\n                             \'please check if one of the image folder \'\n                             \'or index file is not exist\')\n\n        # Init training example information\n        self._lexicon_list = []\n        self._train_sample_infos = []\n        self._test_sample_infos = []\n        self._val_sample_infos = []\n        self._init_dataset_sample_info()\n\n        # Check if need generate char dict map\n        if char_dict_path is None or ord_map_dict_path is None:\n            os.makedirs(\'./data/char_dict\', exist_ok=True)\n            self._char_dict_path = ops.join(\'./data/char_dict\', \'char_dict.json\')\n            self._ord_map_dict_path = ops.join(\'./data/char_dict\', \'ord_map.json\')\n            self._generate_char_dict()\n\n    def generate_tfrecords(self, save_dir):\n        """"""\n        Generate tensorflow records file\n        :param save_dir: tensorflow records save dir\n        :return:\n        """"""\n        # make save dirs\n        os.makedirs(save_dir, exist_ok=True)\n\n        # generate training example tfrecords\n        log.info(\'Generating training sample tfrecords...\')\n        t_start = time.time()\n\n        tfrecords_writer = tf_io_pipline_fast_tools.CrnnFeatureWriter(\n            annotation_infos=self._train_sample_infos,\n            lexicon_infos=self._lexicon_list,\n            char_dict_path=self._char_dict_path,\n            ord_map_dict_path=self._ord_map_dict_path,\n            tfrecords_save_dir=save_dir,\n            writer_process_nums=self._writer_process_nums,\n            dataset_flag=\'train\'\n        )\n        tfrecords_writer.run()\n\n        log.info(\'Generate training sample tfrecords complete, cost time: {:.5f}\'.format(time.time() - t_start))\n\n        # generate val example tfrecords\n        log.info(\'Generating validation sample tfrecords...\')\n        t_start = time.time()\n\n        tfrecords_writer = tf_io_pipline_fast_tools.CrnnFeatureWriter(\n            annotation_infos=self._val_sample_infos,\n            lexicon_infos=self._lexicon_list,\n            char_dict_path=self._char_dict_path,\n            ord_map_dict_path=self._ord_map_dict_path,\n            tfrecords_save_dir=save_dir,\n            writer_process_nums=self._writer_process_nums,\n            dataset_flag=\'val\'\n        )\n        tfrecords_writer.run()\n\n        log.info(\'Generate validation sample tfrecords complete, cost time: {:.5f}\'.format(time.time() - t_start))\n\n        # generate test example tfrecords\n        log.info(\'Generating testing sample tfrecords....\')\n        t_start = time.time()\n\n        tfrecords_writer = tf_io_pipline_fast_tools.CrnnFeatureWriter(\n            annotation_infos=self._test_sample_infos,\n            lexicon_infos=self._lexicon_list,\n            char_dict_path=self._char_dict_path,\n            ord_map_dict_path=self._ord_map_dict_path,\n            tfrecords_save_dir=save_dir,\n            writer_process_nums=self._writer_process_nums,\n            dataset_flag=\'test\'\n        )\n        tfrecords_writer.run()\n\n        log.info(\'Generate testing sample tfrecords complete, cost time: {:.5f}\'.format(time.time() - t_start))\n\n        return\n\n    def _is_source_data_complete(self):\n        """"""\n        Check if source data complete\n        :return:\n        """"""\n        return \\\n            ops.exists(self._train_annotation_file_path) and ops.exists(self._val_annotation_file_path) \\\n            and ops.exists(self._test_annotation_file_path) and ops.exists(self._lexicon_file_path)\n\n    def _init_dataset_sample_info(self):\n        """"""\n        organize dataset sample information, read all the lexicon information in lexicon list.\n        Train, test, val sample information are lists like\n        [(image_absolute_path_1, image_lexicon_index_1), (image_absolute_path_2, image_lexicon_index_2), ...]\n        :return:\n        """"""\n        # establish lexicon list\n        log.info(\'Start initialize lexicon information list...\')\n        num_lines = sum(1 for _ in open(self._lexicon_file_path, \'r\'))\n        with open(self._lexicon_file_path, \'r\', encoding=\'utf-8\') as file:\n            for line in tqdm.tqdm(file, total=num_lines):\n                self._lexicon_list.append(line.rstrip(\'\\r\').rstrip(\'\\n\'))\n\n        # establish train example info\n        log.info(\'Start initialize train sample information list...\')\n        num_lines = sum(1 for _ in open(self._train_annotation_file_path, \'r\'))\n        with open(self._train_annotation_file_path, \'r\', encoding=\'utf-8\') as file:\n            for line in tqdm.tqdm(file, total=num_lines):\n\n                image_name, label_index = line.rstrip(\'\\r\').rstrip(\'\\n\').split(\' \')\n                image_path = ops.join(self._dataset_dir, image_name)\n                label_index = int(label_index)\n\n                if not ops.exists(image_path):\n                    raise ValueError(\'Example image {:s} not exist\'.format(image_path))\n\n                self._train_sample_infos.append((image_path, label_index))\n\n        # establish val example info\n        log.info(\'Start initialize validation sample information list...\')\n        num_lines = sum(1 for _ in open(self._val_annotation_file_path, \'r\'))\n        with open(self._val_annotation_file_path, \'r\', encoding=\'utf-8\') as file:\n            for line in tqdm.tqdm(file, total=num_lines):\n                image_name, label_index = line.rstrip(\'\\r\').rstrip(\'\\n\').split(\' \')\n                image_path = ops.join(self._dataset_dir, image_name)\n                label_index = int(label_index)\n\n                if not ops.exists(image_path):\n                    raise ValueError(\'Example image {:s} not exist\'.format(image_path))\n\n                self._val_sample_infos.append((image_path, label_index))\n\n        # establish test example info\n        log.info(\'Start initialize testing sample information list...\')\n        num_lines = sum(1 for _ in open(self._test_annotation_file_path, \'r\'))\n        with open(self._test_annotation_file_path, \'r\', encoding=\'utf-8\') as file:\n            for line in tqdm.tqdm(file, total=num_lines):\n                image_name, label_index = line.rstrip(\'\\r\').rstrip(\'\\n\').split(\' \')\n                image_path = ops.join(self._dataset_dir, image_name)\n                label_index = int(label_index)\n\n                if not ops.exists(image_path):\n                    raise ValueError(\'Example image {:s} not exist\'.format(image_path))\n\n                self._test_sample_infos.append((image_path, label_index))\n\n    def _generate_char_dict(self):\n        """"""\n        generate the char dict and ord map dict json file according to the lexicon list.\n        gather all the single characters used in lexicon list.\n        :return:\n        """"""\n        char_lexicon_set = set()\n        for lexcion in self._lexicon_list:\n            for s in lexcion:\n                char_lexicon_set.add(s)\n\n        log.info(\'Char set length: {:d}\'.format(len(char_lexicon_set)))\n\n        char_lexicon_list = list(char_lexicon_set)\n        char_dict_builder = establish_char_dict.CharDictBuilder()\n        char_dict_builder.write_char_dict(char_lexicon_list, save_path=self._char_dict_path)\n        char_dict_builder.map_ord_to_index(char_lexicon_list, save_path=self._ord_map_dict_path)\n\n        log.info(\'Write char dict map complete\')\n\n\nclass CrnnDataFeeder(object):\n    """"""\n    Read training examples from tfrecords for crnn model\n    """"""\n    def __init__(self, dataset_dir, char_dict_path, ord_map_dict_path, flags=\'train\'):\n        """"""\n        crnn net dataset io pip line\n        :param dataset_dir: the root dir of crnn dataset\n        :param char_dict_path: json file path which contains the map relation\n        between ord value and single character\n        :param ord_map_dict_path: json file path which contains the map relation\n        between int index value and char ord value\n        :param flags: flag to determinate for whom the data feeder was used\n        """"""\n        self._dataset_dir = dataset_dir\n\n        self._tfrecords_dir = ops.join(dataset_dir, \'tfrecords\')\n        if not ops.exists(self._tfrecords_dir):\n            raise ValueError(\'{:s} not exist, please check again\'.format(self._tfrecords_dir))\n\n        self._dataset_flags = flags.lower()\n        if self._dataset_flags not in [\'train\', \'test\', \'val\']:\n            raise ValueError(\'flags of the data feeder should be \\\'train\\\', \\\'test\\\', \\\'val\\\'\')\n\n        self._char_dict_path = char_dict_path\n        self._ord_map_dict_path = ord_map_dict_path\n        self._tfrecords_io_reader = tf_io_pipline_fast_tools.CrnnFeatureReader(\n            char_dict_path=self._char_dict_path, ord_map_dict_path=self._ord_map_dict_path)\n        self._tfrecords_io_reader.dataset_flags = self._dataset_flags\n\n    def sample_counts(self):\n        """"""\n        use tf records iter to count the total sample counts of all tfrecords file\n        :return: int: sample nums\n        """"""\n        tfrecords_file_paths = glob.glob(\'{:s}/{:s}*.tfrecords\'.format(self._tfrecords_dir, self._dataset_flags))\n        counts = 0\n\n        for record in tfrecords_file_paths:\n            counts += sum(1 for _ in tf.python_io.tf_record_iterator(record))\n\n        return counts\n\n    def inputs(self, batch_size):\n        """"""\n        Supply the batched data for training, testing and validation. For training and validation\n        this function will run in a infinite loop until user end it outside of the function.\n        For testing this function will raise an tf.errors.OutOfRangeError when reach the end of\n        the dataset. User may catch this exception to terminate a loop.\n        :param batch_size:\n        :return: A tuple (images, labels, image_paths), where:\n                    * images is a float tensor with shape [batch_size, H, W, C]\n                      in the range [-1.0, 1.0].\n                    * labels is an sparse tensor with shape [batch_size, None] with the true label\n                    * image_paths is an tensor with shape [batch_size] with the image\'s absolute file path\n        """"""\n\n        tfrecords_file_paths = glob.glob(\'{:s}/{:s}*.tfrecords\'.format(self._tfrecords_dir, self._dataset_flags))\n\n        if not tfrecords_file_paths:\n            raise ValueError(\'Dataset does not contain any tfrecords for {:s}\'.format(self._dataset_flags))\n\n        random.shuffle(tfrecords_file_paths)\n\n        return self._tfrecords_io_reader.inputs(\n            tfrecords_path=tfrecords_file_paths,\n            batch_size=batch_size,\n            num_threads=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS\n        )\n'"
data_provider/tf_io_pipline_fast_tools.py,16,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-3-21 \xe4\xb8\x8b\xe5\x8d\x883:03\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : tf_io_pipline_fast_tools.py\n# @IDE: PyCharm\n""""""\nEfficient tfrecords writer interface\n""""""\nimport os\nimport os.path as ops\nfrom multiprocessing import Manager\nfrom multiprocessing import Process\nimport time\n\nimport cv2\nimport glog as log\nimport numpy as np\nimport tensorflow as tf\nimport tqdm\n\nfrom config import global_config\nfrom local_utils import establish_char_dict\n\nCFG = global_config.cfg\n\n_SAMPLE_INFO_QUEUE = Manager().Queue()\n_SENTINEL = ("""", [])\n\n\ndef _int64_feature(value):\n    """"""\n    Wrapper for inserting int64 features into Example proto.\n    :param value:\n    :return:\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    value_tmp = []\n    is_int = True\n    for val in value:\n        if not isinstance(val, int):\n            is_int = False\n            value_tmp.append(int(float(val)))\n    if not is_int:\n        value = value_tmp\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _float_feature(value):\n    """"""\n    Wrapper for inserting float features into Example proto.\n    :param value:\n    :return:\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    value_tmp = []\n    is_float = True\n    for val in value:\n        if not isinstance(val, int):\n            is_float = False\n            value_tmp.append(float(val))\n    if is_float is False:\n        value = value_tmp\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _bytes_feature(value):\n    """"""\n    Wrapper for inserting bytes features into Example proto.\n    :param value:\n    :return:\n    """"""\n    if not isinstance(value, bytes):\n        if not isinstance(value, list):\n            value = value.encode(\'utf-8\')\n        else:\n            value = [val.encode(\'utf-8\') for val in value]\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef _is_valid_jpg_file(image_path):\n    """"""\n\n    :param image_path:\n    :return:\n    """"""\n\n    if not ops.exists(image_path):\n        return False\n\n    file = open(image_path, \'rb\')\n    data = file.read(11)\n    if data[:4] != \'\\xff\\xd8\\xff\\xe0\' and data[:4] != \'\\xff\\xd8\\xff\\xe1\':\n        file.close()\n        return False\n    if data[6:] != \'JFIF\\0\' and data[6:] != \'Exif\\0\':\n        file.close()\n        return False\n    file.close()\n\n    file = open(image_path, \'rb\')\n    file.seek(-2, 2)\n    if file.read() != \'\\xff\\xd9\':\n        file.close()\n        return False\n\n    file.close()\n\n    return True\n\n\ndef _write_tfrecords(tfrecords_writer):\n    """"""\n\n    :param tfrecords_writer:\n    :return:\n    """"""\n    while True:\n        sample_info = _SAMPLE_INFO_QUEUE.get()\n\n        if sample_info == _SENTINEL:\n            log.info(\'Process {:d} finished writing work\'.format(os.getpid()))\n            tfrecords_writer.close()\n            break\n\n        sample_path = sample_info[0]\n        sample_label = sample_info[1]\n\n        if _is_valid_jpg_file(sample_path):\n            log.error(\'Image file: {:d} is not a valid jpg file\'.format(sample_path))\n            continue\n\n        try:\n            image = cv2.imread(sample_path, cv2.IMREAD_COLOR)\n            if image is None:\n                continue\n            image = cv2.resize(image, dsize=tuple(CFG.ARCH.INPUT_SIZE), interpolation=cv2.INTER_LINEAR)\n            image = image.tostring()\n        except IOError as err:\n            log.error(err)\n            continue\n\n        features = tf.train.Features(feature={\n            \'labels\': _int64_feature(sample_label),\n            \'images\': _bytes_feature(image),\n            \'imagepaths\': _bytes_feature(sample_path)\n        })\n        tf_example = tf.train.Example(features=features)\n        tfrecords_writer.write(tf_example.SerializeToString())\n        log.debug(\'Process: {:d} get sample from sample_info_queue[current_size={:d}], \'\n                  \'and write it to local file at time: {}\'.format(\n                   os.getpid(), _SAMPLE_INFO_QUEUE.qsize(), time.strftime(\'%H:%M:%S\')))\n\n\nclass _FeatureIO(object):\n    """"""\n    Feature IO Base Class\n    """"""\n    def __init__(self, char_dict_path, ord_map_dict_path):\n        """"""\n\n        :param char_dict_path:\n        :param ord_map_dict_path:\n        """"""\n        self._char_dict = establish_char_dict.CharDictBuilder.read_char_dict(char_dict_path)\n        self._ord_map = establish_char_dict.CharDictBuilder.read_ord_map_dict(ord_map_dict_path)\n        return\n\n    def char_to_int(self, char):\n        """"""\n        convert char into int index, first convert the char into it\'s ord\n        number and the convert the ord number into int index which is stored\n        in ord_map_dict.json file\n        :param char: single character\n        :return: the int index of the character\n        """"""\n        str_key = str(ord(char)) + \'_ord\'\n        try:\n            result = int(self._ord_map[str_key])\n            return result\n        except KeyError:\n            raise KeyError(""Character {} missing in ord_map.json"".format(char))\n\n    def int_to_char(self, number):\n        """"""\n        convert the int index into char\n        :param number: Can be passed as string representing the integer value to look up.\n        :return: Character corresponding to \'number\' in the char_dict\n        """"""\n        # 1 is the default value in sparse_tensor_to_str() This will be skipped when building the resulting strings\n        if number == 1 or number == \'1\':\n            return \'\\x00\'\n        else:\n            return self._char_dict[str(number) + \'_ord\']\n\n    def encode_labels(self, labels):\n        """"""\n        Convert a batch of text labels into int index labels\n        :param labels: List of text labels such as [\'hello world\', \'fuck world\', ...]\n        :return: Two list. One is a list of int index labels another is\n        a list of label length\n        """"""\n        encoded_labels = []\n        lengths = []\n        for label in labels:\n            encode_label = [self.char_to_int(char) for char in label]\n            encoded_labels.append(encode_label)\n            lengths.append(len(label))\n        return encoded_labels, lengths\n\n    def sparse_tensor_to_str(self, sparse_tensor):\n        """"""\n        :param sparse_tensor: prediction or ground truth label\n        :return: String value of the sparse tensor\n        """"""\n        indices = sparse_tensor.indices\n        values = sparse_tensor.values\n        # Translate from consecutive numbering into ord() values\n        values = np.array([self._ord_map[str(tmp) + \'_index\'] for tmp in values])\n        dense_shape = sparse_tensor.dense_shape\n\n        number_lists = np.ones(dense_shape, dtype=values.dtype)\n        str_lists = []\n        res = []\n        for i, index in enumerate(indices):\n            number_lists[index[0], index[1]] = values[i]\n        for number_list in number_lists:\n            # Translate from ord() values into characters\n            str_lists.append([self.int_to_char(val) for val in number_list])\n        for str_list in str_lists:\n            # int_to_char() returns \'\\x00\' for an input == 1, which is the default\n            # value in number_lists, so we skip it when building the result\n            res.append(\'\'.join(c for c in str_list if c != \'\\x00\'))\n        return res\n\n    def sparse_tensor_to_str_for_tf_serving(self, decode_indices, decode_values, decode_dense_shape):\n        """"""\n\n        :param decode_indices:\n        :param decode_values:\n        :param decode_dense_shape:\n        :return:\n        """"""\n        indices = decode_indices\n        values = decode_values\n        # Translate from consecutive numbering into ord() values\n        values = np.array([self._ord_map[str(tmp) + \'_index\'] for tmp in values])\n        dense_shape = decode_dense_shape\n\n        number_lists = np.ones(dense_shape, dtype=values.dtype)\n        str_lists = []\n        res = []\n        for i, index in enumerate(indices):\n            number_lists[index[0], index[1]] = values[i]\n        for number_list in number_lists:\n            # Translate from ord() values into characters\n            str_lists.append([self.int_to_char(val) for val in number_list])\n        for str_list in str_lists:\n            # int_to_char() returns \'\\x00\' for an input == 1, which is the default\n            # value in number_lists, so we skip it when building the result\n            res.append(\'\'.join(c for c in str_list if c != \'\\x00\'))\n        return res\n\n\nclass CrnnFeatureReader(_FeatureIO):\n    """"""\n        Implement the crnn feature reader\n    """"""\n\n    def __init__(self, char_dict_path, ord_map_dict_path, flags=\'train\'):\n        """"""\n\n        :param char_dict_path:\n        :param ord_map_dict_path:\n        :param flags:\n        """"""\n        super(CrnnFeatureReader, self).__init__(char_dict_path, ord_map_dict_path)\n        self._dataset_flag = flags.lower()\n        return\n\n    @property\n    def dataset_flags(self):\n        """"""\n\n        :return:\n        """"""\n        return self._dataset_flag\n\n    @dataset_flags.setter\n    def dataset_flags(self, value):\n        """"""\n\n        :value:\n        :return:\n        """"""\n        if not isinstance(value, str):\n            raise ValueError(\'Dataset flags shoule be str\')\n\n        if value.lower() not in [\'train\', \'val\', \'test\']:\n            raise ValueError(\'Dataset flags shoule be within \\\'train\\\', \\\'val\\\', \\\'test\\\'\')\n\n        self._dataset_flag = value\n\n    @staticmethod\n    def _augment_for_train(input_images, input_labels, input_image_paths):\n        """"""\n\n        :param input_images:\n        :param input_labels:\n        :param input_image_paths:\n        :return:\n        """"""\n        return input_images, input_labels, input_image_paths\n\n    @staticmethod\n    def _augment_for_validation(input_images, input_labels, input_image_paths):\n        """"""\n\n        :param input_images:\n        :param input_labels:\n        :param input_image_paths:\n        :return:\n        """"""\n        return input_images, input_labels, input_image_paths\n\n    @staticmethod\n    def _normalize(input_images, input_labels, input_image_paths):\n        """"""\n\n        :param input_images:\n        :param input_labels:\n        :param input_image_paths:\n        :return:\n        """"""\n        input_images = tf.subtract(tf.divide(input_images, 127.5), 1.0)\n        return input_images, input_labels, input_image_paths\n\n    @staticmethod\n    def _extract_features_batch(serialized_batch):\n        """"""\n\n        :param serialized_batch:\n        :return:\n        """"""\n        features = tf.parse_example(\n            serialized_batch,\n            features={\'images\': tf.FixedLenFeature([], tf.string),\n                      \'imagepaths\': tf.FixedLenFeature([], tf.string),\n                      \'labels\': tf.VarLenFeature(tf.int64),\n                      }\n        )\n        bs = features[\'images\'].shape[0]\n        images = tf.decode_raw(features[\'images\'], tf.uint8)\n        w, h = tuple(CFG.ARCH.INPUT_SIZE)\n        images = tf.cast(x=images, dtype=tf.float32)\n        images = tf.reshape(images, [bs, h, w, CFG.ARCH.INPUT_CHANNELS])\n\n        labels = features[\'labels\']\n        labels = tf.cast(labels, tf.int32)\n\n        imagepaths = features[\'imagepaths\']\n\n        return images, labels, imagepaths\n\n    def inputs(self, tfrecords_path, batch_size, num_threads):\n        """"""\n\n        :param tfrecords_path:\n        :param batch_size:\n        :param num_threads:\n        :return: input_images, input_labels, input_image_names\n        """"""\n        dataset = tf.data.TFRecordDataset(tfrecords_path)\n\n        dataset = dataset.batch(batch_size, drop_remainder=True)\n\n        # The map transformation takes a function and applies it to every element\n        # of the dataset.\n        dataset = dataset.map(map_func=self._extract_features_batch,\n                              num_parallel_calls=num_threads)\n        if self._dataset_flag == \'train\':\n            dataset = dataset.map(map_func=self._augment_for_train,\n                                  num_parallel_calls=num_threads)\n        else:\n            dataset = dataset.map(map_func=self._augment_for_validation,\n                                  num_parallel_calls=num_threads)\n        dataset = dataset.map(map_func=self._normalize,\n                              num_parallel_calls=num_threads)\n\n        # The shuffle transformation uses a finite-sized buffer to shuffle elements\n        # in memory. The parameter is the number of elements in the buffer. For\n        # completely uniform shuffling, set the parameter to be the same as the\n        # number of elements in the dataset.\n        if self._dataset_flag != \'test\':\n            dataset = dataset.shuffle(buffer_size=128)\n            # repeat num epochs\n            dataset = dataset.repeat()\n\n        iterator = dataset.make_one_shot_iterator()\n\n        return iterator.get_next(name=\'{:s}_IteratorGetNext\'.format(self._dataset_flag))\n\n\nclass CrnnFeatureWriter(_FeatureIO):\n    """"""\n    crnn tensorflow tfrecords writer\n    """"""\n\n    def __init__(self, annotation_infos, lexicon_infos,\n                 char_dict_path, ord_map_dict_path,\n                 tfrecords_save_dir, writer_process_nums, dataset_flag):\n        """"""\n        Every file path should be checked outside of the class, make sure the file path is valid when you\n        call the class. Make sure the info list is not empty when you call the class. I will put all the\n        sample information into a queue which may cost lots of memory if you\'ve got really large dataset\n        :param annotation_infos: example info list [(image_absolute_path, lexicon_index), ...]\n        :param lexicon_infos: lexicon info list [lexicon1, lexicon2, ...]\n        :param char_dict_path: char dict file path\n        :param ord_map_dict_path: ord map dict file path\n        :param tfrecords_save_dir: tfrecords save dir\n        :param writer_process_nums: the process nums of which will write the tensorflow examples\n        into local tensorflow records file. Each thread will write down examples into its own\n        local tensorflow records file\n        :param dataset_flag: dataset flag which will be the tfrecords file\'s prefix name\n        """"""\n        super(CrnnFeatureWriter, self).__init__(\n            char_dict_path=char_dict_path,\n            ord_map_dict_path=ord_map_dict_path\n        )\n\n        # init sample info queue\n        self._dataset_flag = dataset_flag\n        self._annotation_infos = annotation_infos\n        self._lexicon_infos = lexicon_infos\n        self._writer_process_nums = writer_process_nums\n        self._init_example_info_queue()\n        self._tfrecords_save_dir = tfrecords_save_dir\n\n    def _init_example_info_queue(self):\n        """"""\n        Read index file and put example info into SAMPLE_INFO_QUEUE\n        :return:\n        """"""\n        log.info(\'Start filling {:s} dataset sample information queue...\'.format(self._dataset_flag))\n\n        t_start = time.time()\n        for annotation_info in tqdm.tqdm(self._annotation_infos):\n            image_path = annotation_info[0]\n            lexicon_index = annotation_info[1]\n\n            try:\n                lexicon_label = [self._lexicon_infos[lexicon_index]]\n                encoded_label, _ = self.encode_labels(lexicon_label)\n\n                _SAMPLE_INFO_QUEUE.put((image_path, encoded_label[0]))\n            except IndexError:\n                log.error(\'Lexicon doesn\\\'t contain lexicon index {:d}\'.format(lexicon_index))\n                continue\n        for i in range(self._writer_process_nums):\n            _SAMPLE_INFO_QUEUE.put(_SENTINEL)\n        log.debug(\'Complete filling dataset sample information queue[current size: {:d}], cost time: {:.5f}s\'.format(\n            _SAMPLE_INFO_QUEUE.qsize(),\n            time.time() - t_start\n        ))\n\n    def run(self):\n        """"""\n\n        :return:\n        """"""\n        log.info(\'Start write tensorflow records for {:s}...\'.format(self._dataset_flag))\n\n        process_pool = []\n        tfwriters = []\n        for i in range(self._writer_process_nums):\n            tfrecords_save_name = \'{:s}_{:d}.tfrecords\'.format(self._dataset_flag, i + 1)\n            tfrecords_save_path = ops.join(self._tfrecords_save_dir, tfrecords_save_name)\n\n            tfrecords_io_writer = tf.python_io.TFRecordWriter(path=tfrecords_save_path)\n            process = Process(\n                target=_write_tfrecords,\n                name=\'Subprocess_{:d}\'.format(i + 1),\n                args=(tfrecords_io_writer,)\n            )\n            process_pool.append(process)\n            tfwriters.append(tfrecords_io_writer)\n            process.start()\n\n        for process in process_pool:\n            process.join()\n\n        log.info(\'Finished writing down the tensorflow records file\')\n'"
local_utils/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-22 \xe4\xb8\x8b\xe5\x8d\x886:45\n# @Author  : Luo Yao\n# @Site    : http://github.com/TJCVRS\n# @File    : __init__.py.py\n# @IDE: PyCharm Community Edition'
local_utils/establish_char_dict.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-30 \xe4\xb8\x8b\xe5\x8d\x884:01\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : establish_char_dict.py\n# @IDE: PyCharm Community Edition\n""""""\nEstablish the char dictionary in order to contain chinese character\n""""""\nimport json\nimport os\nimport os.path as ops\nfrom typing import Iterable\n\n\nclass CharDictBuilder(object):\n    """"""\n        Build and read char dict\n    """"""\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def _read_chars(origin_char_list):\n        """"""\n        Read a list of chars or a file containing it.\n        :param origin_char_list:\n        :return:\n        """"""\n        if isinstance(origin_char_list, str):\n            assert ops.exists(origin_char_list), \\\n                ""Character list %s is not a file or could not be found"" % origin_char_list\n            with open(origin_char_list, \'r\', encoding=\'utf-8\') as origin_f:\n                chars = (l[0] for l in origin_f.readlines())\n        elif isinstance(origin_char_list, Iterable):\n            ok = all(map(lambda s: isinstance(s, str) and len(s) == 1, origin_char_list))\n            assert ok, ""Character list is not an Iterable of strings of length 1""\n            chars = origin_char_list\n        else:\n            raise TypeError(""Character list needs to be a file or a list of strings"")\n        return chars\n\n    @staticmethod\n    def _write_json(save_path, data):\n        """"""\n\n        :param save_path:\n        :param data:\n        :return:\n        """"""\n        if not save_path.endswith(\'.json\'):\n            raise ValueError(\'save path {:s} should be a json file\'.format(save_path))\n        os.makedirs(ops.dirname(save_path), exist_ok=True)\n        with open(save_path, \'w\', encoding=\'utf-8\') as json_f:\n            json.dump(data, json_f, sort_keys=True, indent=4)\n\n    @staticmethod\n    def write_char_dict(origin_char_list, save_path):\n        """"""\n        Writes the ordinal to char map used in int_to_char to decode predictions and labels.\n        The file is read with CharDictBuilder.read_char_dict()\n        :param origin_char_list: Either a path to file with character list, one a character per line, or a list or set\n                                 of characters\n        :param save_path: Destination file, full path.\n        """"""\n        char_dict = {str(ord(c)) + \'_ord\': c for c in CharDictBuilder._read_chars(origin_char_list)}\n        CharDictBuilder._write_json(save_path, char_dict)\n\n    @staticmethod\n    def read_char_dict(dict_path):\n        """"""\n\n        :param dict_path:\n        :return: a dict with ord(char) as key and char as value\n        """"""\n        with open(dict_path, \'r\', encoding=\'utf-8\') as json_f:\n            res = json.load(json_f)\n        return res\n\n    @staticmethod\n    def map_ord_to_index(origin_char_list, save_path):\n        """"""\n        Map ord of character in origin char list into index start from 0 in order to meet the output of the DNN\n        :param origin_char_list:\n        :param save_path:\n        """"""\n        ord_2_index_dict = {str(i) + \'_index\': str(ord(c)) for i, c in\n                            enumerate(CharDictBuilder._read_chars(origin_char_list))}\n        index_2_ord_dict = {str(ord(c)) + \'_ord\': str(i) for i, c in\n                            enumerate(CharDictBuilder._read_chars(origin_char_list))}\n        total_ord_map_index_dict = dict(ord_2_index_dict)\n        total_ord_map_index_dict.update(index_2_ord_dict)\n        CharDictBuilder._write_json(save_path, total_ord_map_index_dict)\n\n    @staticmethod\n    def read_ord_map_dict(ord_map_dict_path):\n        """"""\n\n        :param ord_map_dict_path:\n        :return:\n        """"""\n        with open(ord_map_dict_path, \'r\', encoding=\'utf-8\') as json_f:\n            res = json.load(json_f)\n        return res\n'"
local_utils/evaluation_tools.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-3-12 \xe4\xb8\x8b\xe5\x8d\x889:03\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : evaluation_tools.py\n# @IDE: PyCharm\n""""""\nSome evaluation tools\n""""""\nimport itertools\n\nimport numpy as np\nimport glog as log\nimport matplotlib.pyplot as plt\n\n\nSYNTH90K_CLASS_NAMES = [\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'a\', \'b\',\n                        \'c\', \'d\', \'e\', \'f\', \'g\', \'h\', \'i\', \'j\', \'k\', \'l\', \'m\', \'n\',\n                        \'o\', \'p\', \'q\', \'r\', \'s\', \'t\', \'u\', \'v\', \'w\', \'x\', \'y\', \'z\', \' \']\n\n\ndef compute_accuracy(ground_truth, predictions, display=False, mode=\'per_char\'):\n    """"""\n    Computes accuracy\n    :param ground_truth:\n    :param predictions:\n    :param display: Whether to print values to stdout\n    :param mode: if \'per_char\' is selected then\n                 single_label_accuracy = correct_predicted_char_nums_of_single_sample / single_label_char_nums\n                 avg_label_accuracy = sum(single_label_accuracy) / label_nums\n                 if \'full_sequence\' is selected then\n                 single_label_accuracy = 1 if the prediction result is exactly the same as label else 0\n                 avg_label_accuracy = sum(single_label_accuracy) / label_nums\n    :return: avg_label_accuracy\n    """"""\n    if mode == \'per_char\':\n\n        accuracy = []\n\n        for index, label in enumerate(ground_truth):\n            prediction = predictions[index]\n            total_count = len(label)\n            correct_count = 0\n            try:\n                for i, tmp in enumerate(label):\n                    if tmp == prediction[i]:\n                        correct_count += 1\n            except IndexError:\n                continue\n            finally:\n                try:\n                    accuracy.append(correct_count / total_count)\n                except ZeroDivisionError:\n                    if len(prediction) == 0:\n                        accuracy.append(1)\n                    else:\n                        accuracy.append(0)\n        avg_accuracy = np.mean(np.array(accuracy).astype(np.float32), axis=0)\n    elif mode == \'full_sequence\':\n        try:\n            correct_count = 0\n            for index, label in enumerate(ground_truth):\n                prediction = predictions[index]\n                if prediction == label:\n                    correct_count += 1\n            avg_accuracy = correct_count / len(ground_truth)\n        except ZeroDivisionError:\n            if not predictions:\n                avg_accuracy = 1\n            else:\n                avg_accuracy = 0\n    else:\n        raise NotImplementedError(\'Other accuracy compute mode has not been implemented\')\n\n    if display:\n        print(\'Mean accuracy is {:5f}\'.format(avg_accuracy))\n\n    return avg_accuracy\n\n\ndef plot_confusion_matrix(cm, classes=SYNTH90K_CLASS_NAMES,\n                          normalize=False,\n                          title=\'Confusion matrix\',\n                          cmap=plt.cm.Blues):\n    """"""\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    """"""\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        log.info(""Normalized confusion matrix"")\n    else:\n        log.info(\'Confusion matrix, without normalization\')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = \'.2f\' if normalize else \'d\'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=""center"",\n                 color=""white"" if cm[i, j] > thresh else ""black"")\n\n    plt.ylabel(\'True label\')\n    plt.xlabel(\'Predicted label\')\n    plt.tight_layout()\n\n\ndef print_cm(cm, labels=SYNTH90K_CLASS_NAMES, hide_zeroes=False,\n             hide_diagonal=False, hide_threshold=None):\n    """"""\n    pretty print for confusion matrixes\n    :param cm:\n    :param labels:\n    :param hide_zeroes:\n    :param hide_diagonal:\n    :param hide_threshold:\n    :return:\n    """"""\n    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n    empty_cell = "" "" * columnwidth\n\n    # Begin CHANGES\n    fst_empty_cell = (columnwidth - 3) // 2 * "" "" + ""t/p"" + (columnwidth - 3) // 2 * "" ""\n\n    if len(fst_empty_cell) < len(empty_cell):\n        fst_empty_cell = "" "" * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell\n    # Print header\n    print(""    "" + fst_empty_cell, end="" "")\n    # End CHANGES\n\n    for label in labels:\n        print(""%{0}s"".format(columnwidth) % label, end="" "")\n\n    print()\n    # Print rows\n    for i, label1 in enumerate(labels):\n        print(""    %{0}s"".format(columnwidth) % label1, end="" "")\n        for j in range(len(labels)):\n            cell = ""%{0}.1f"".format(columnwidth) % cm[i, j]\n            if hide_zeroes:\n                cell = cell if float(cm[i, j]) != 0 else empty_cell\n            if hide_diagonal:\n                cell = cell if i != j else empty_cell\n            if hide_threshold:\n                cell = cell if cm[i, j] > hide_threshold else empty_cell\n            print(cell, end="" "")\n        print()\n'"
local_utils/log_utils.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-18 \xe4\xb8\x8b\xe5\x8d\x884:11\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : log_utils.py\n# @IDE: PyCharm Community Edition\n""""""\nSet the log config\n""""""\nimport logging\nfrom logging import handlers\nimport os\nimport os.path as ops\n\n\ndef init_logger(level=logging.DEBUG, when=""D"", backup=7,\n                _format=""%(levelname)s: %(asctime)s: %(filename)s:%(lineno)d * %(thread)d %(message)s"",\n                datefmt=""%m-%d %H:%M:%S""):\n    """"""\n    init_log - initialize log module\n    :param level: msg above the level will be displayed DEBUG < INFO < WARNING < ERROR < CRITICAL\n                  the default value is logging.INFO\n    :param when:  how to split the log file by time interval\n                  \'S\' : Seconds\n                  \'M\' : Minutes\n                  \'H\' : Hours\n                  \'D\' : Days\n                  \'W\' : Week day\n                  default value: \'D\'\n    :param backup: how many backup file to keep default value: 7\n    :param _format: format of the log default format:\n                   %(levelname)s: %(asctime)s: %(filename)s:%(lineno)d * %(thread)d %(message)s\n                   INFO: 12-09 18:02:42: log.py:40 * 139814749787872 HELLO WORLD\n    :param datefmt:\n    :return:\n    """"""\n    formatter = logging.Formatter(_format, datefmt)\n    logger = logging.getLogger()\n    logger.setLevel(level)\n\n    log_path = ops.join(os.getcwd(), \'logs/shadownet.log\')\n    _dir = os.path.dirname(log_path)\n    if not os.path.isdir(_dir):\n        os.makedirs(_dir)\n\n    handler = handlers.TimedRotatingFileHandler(log_path, when=when, backupCount=backup)\n    handler.setLevel(level)\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    handler = handlers.TimedRotatingFileHandler(log_path + "".log.wf"", when=when, backupCount=backup)\n    handler.setLevel(logging.WARNING)\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    handler = logging.StreamHandler()\n    handler.setLevel(level)\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    return logger\n'"
tfserve/crnn_python_client_via_grpc.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-3-27 \xe4\xb8\x8b\xe5\x8d\x884:15\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV\n# @File    : crnn_python_client_via_grpc.py.py\n# @IDE: PyCharm\n""""""\ntest python tensorflow client\n""""""\nimport time\nfrom argparse import ArgumentParser\n\nimport grpc\nimport numpy as np\nimport cv2\nfrom tensorflow import saved_model as sm\nfrom tensorflow.contrib.util import make_tensor_proto\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\nfrom config import global_config\nfrom data_provider import tf_io_pipline_fast_tools\n\nCFG = global_config.cfg\n\nCODEC = tf_io_pipline_fast_tools.CrnnFeatureReader(\n    char_dict_path=\'./data/char_dict/char_dict_en.json\',\n    ord_map_dict_path=\'./data/char_dict/ord_map_en.json\'\n)\n\n\ndef parse_args():\n    """"""\n\n    :return:\n    """"""\n    parser = ArgumentParser(description=\'Request a TensorFlow server for a prediction on the image\')\n    parser.add_argument(\'-s\', \'--server\',\n                        dest=\'server\',\n                        default=\'localhost:9000\',\n                        help=\'prediction service host:port\')\n    parser.add_argument(""-i"", ""--image"",\n                        dest=""image"",\n                        default=\'\',\n                        help=""path to image in JPEG format"", )\n    parser.add_argument(\'-p\', \'--image_path\',\n                        dest=\'image_path\',\n                        default=\'./data/test_images/test_01.jpg\',\n                        help=\'path to images folder\', )\n    parser.add_argument(\'-b\', \'--batch_mode\',\n                        dest=\'batch_mode\',\n                        default=\'true\',\n                        help=\'send image as batch or one-by-one\')\n    args = parser.parse_args()\n\n    return args.server, args.image, args.image_path, args.batch_mode == \'true\'\n\n\ndef make_request(image_path, server):\n    """"""\n\n    :param image_path:\n    :param server:\n    :return:\n    """"""\n    channel = grpc.insecure_channel(server)\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    image = cv2.resize(image, (CFG.ARCH.INPUT_SIZE[0], CFG.ARCH.INPUT_SIZE[1]), interpolation=cv2.INTER_LINEAR)\n    image = np.array(image, np.float32) / 127.5 - 1.0\n\n    image_list = np.array([image], dtype=np.float32)\n\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = \'crnn\'\n    request.model_spec.signature_name = sm.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n\n    request.inputs[\'input_tensor\'].CopyFrom(make_tensor_proto(\n        image_list, shape=[1, CFG.ARCH.INPUT_SIZE[1], CFG.ARCH.INPUT_SIZE[0], 3]))\n\n    try:\n        result = stub.Predict(request, 10.0)\n\n        return result\n    except Exception as err:\n        print(err)\n        return None\n\n\ndef convert_predict_response_into_nparray(response, output_tensor_name):\n    """"""\n\n    :param response:\n    :param output_tensor_name:\n    :return:\n    """"""\n    dims = response.outputs[output_tensor_name].tensor_shape.dim\n    shape = tuple(d.size for d in dims)\n\n    return np.reshape(response.outputs[output_tensor_name].int64_val, shape)\n\n\ndef post_process(tf_serving_request_result):\n    """"""\n\n    :param tf_serving_request_result:\n    :return:\n    """"""\n    decode_indices = convert_predict_response_into_nparray(\n        tf_serving_request_result,\n        \'decodes_indices\'\n    )\n    decode_values = convert_predict_response_into_nparray(\n        tf_serving_request_result,\n        \'decodes_values\'\n    )\n    decode_dense_shape = convert_predict_response_into_nparray(\n        tf_serving_request_result,\n        \'decodes_dense_shape\'\n    )\n\n    prediction = CODEC.sparse_tensor_to_str_for_tf_serving(\n        decode_indices=decode_indices,\n        decode_values=decode_values,\n        decode_dense_shape=decode_dense_shape\n    )[0]\n\n    print(\'Prediction: {:s}\'.format(prediction))\n\n    return\n\n\ndef main():\n    """"""\n\n    :return:\n    """"""\n    server, image, image_path, batch_mode = parse_args()\n\n    print(\'Server: {}\'.format(server))\n\n    start = time.time()\n\n    request_result = make_request(image_path, server)\n\n    post_process(request_result)\n\n    end = time.time()\n    time_diff = end - start\n    print(\'time elapased: {}\'.format(time_diff))\n\n\nif __name__ == \'__main__\':\n    """"""\n    test code\n    """"""\n    main()\n'"
tfserve/crnn_python_client_via_request.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Author  : eldon\n# @Site    : https://github.com/eldon/CRNN_Tensorflow\n# @File    : crnn_python_client_via_request.py\n""""""\nUse shadow net to recognize the scene text of a single image\n""""""\nimport json\nimport os.path as ops\n\nimport cv2\nimport glog as logger\nimport numpy as np\nimport requests\nimport wordninja\n\nfrom config import global_config\nfrom data_provider import tf_io_pipline_fast_tools\n\nCFG = global_config.cfg\nSERVER_URL = \'http://localhost:8501/v1/models/crnn:predict\'\nCHAR_DICT_PATH = \'./data/char_dict/char_dict_en.json\'\nORD_MAP_DICT_PATH = \'./data/char_dict/ord_map_en.json\'\n\n\ndef request_crnn_predict(image_path):\n    """"""\n    request crnn predict\n    :param image_path:\n    :return:\n    """"""\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    # constrain image input size to (100, 32)\n    image = cv2.resize(image, tuple(CFG.ARCH.INPUT_SIZE), interpolation=cv2.INTER_LINEAR)\n    image = np.array(image, np.float32) / 127.5 - 1.0\n\n    response = requests.post(\n        SERVER_URL,\n        data=json.dumps({\n            \'inputs\': [image.tolist()],  # has to be in column format; not a fixed output size\n        }),\n    )\n    response.raise_for_status()\n    outputs = response.json()[\'outputs\']\n\n    # this part can likely be optimized, but oh well\n    codec = tf_io_pipline_fast_tools.CrnnFeatureReader(\n        char_dict_path=CHAR_DICT_PATH,\n        ord_map_dict_path=ORD_MAP_DICT_PATH,\n    )\n\n    preds = codec.sparse_tensor_to_str_for_tf_serving(\n        decode_indices=outputs[\'decodes_indices\'],\n        decode_values=outputs[\'decodes_values\'],\n        decode_dense_shape=outputs[\'decodes_dense_shape\'],\n    )[0]\n    preds = \' \'.join(wordninja.split(preds))\n\n    logger.info(\'Predict image {:s} result: {:s}\'.format(\n        ops.split(image_path)[1], preds)\n    )\n\n\nif __name__ == \'__main__\':\n    """"""\n    test code\n    """"""\n    import sys\n\n    img_path = sys.argv[1]\n    print(img_path)\n    request_crnn_predict(sys.argv[1])\n'"
tfserve/export_saved_model.py,6,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-3-14 \xe4\xb8\x8b\xe5\x8d\x883:18\n# @Author  : MaybeShewill-CV, eldon\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : export_saved_model.py\n# @IDE: PyCharm\n""""""\nConvert ckpt model into tensorflow saved model\n""""""\nimport argparse\nimport os.path as ops\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import saved_model as sm\n\nfrom config import global_config\nfrom crnn_model import crnn_net\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-s\', \'--export_dir\', type=str, help=\'The model export dir\')\n    parser.add_argument(\'-i\', \'--ckpt_path\', type=str, help=\'The pretrained ckpt model weights file path\')\n    parser.add_argument(\'-c\', \'--char_dict_path\', type=str,\n                        help=\'Directory where character dictionaries for the dataset were stored\')\n    parser.add_argument(\'-o\', \'--ord_map_dict_path\', type=str,\n                        help=\'Directory where ord map dictionaries for the dataset were stored\')\n\n    return parser.parse_args()\n\n\ndef build_saved_model(ckpt_path, export_dir):\n    """"""\n    Convert source ckpt weights file into tensorflow saved model\n    :param ckpt_path:\n    :param export_dir:\n    :return:\n    """"""\n\n    if ops.exists(export_dir):\n        raise ValueError(\'Export dir must be a dir path that does not exist\')\n\n    assert ops.exists(ops.split(ckpt_path)[0])\n\n    # build inference tensorflow graph\n    image_size = tuple(CFG.ARCH.INPUT_SIZE)\n    image_tensor = tf.placeholder(\n        dtype=tf.float32,\n        shape=[1, image_size[1], image_size[0], 3],\n        name=\'input_tensor\')\n\n    # set crnn net\n    net = crnn_net.ShadowNet(\n        phase=\'test\',\n        hidden_nums=CFG.ARCH.HIDDEN_UNITS,\n        layers_nums=CFG.ARCH.HIDDEN_LAYERS,\n        num_classes=CFG.ARCH.NUM_CLASSES\n    )\n\n    # compute inference logits\n    inference_ret = net.inference(\n        inputdata=image_tensor,\n        name=\'shadow_net\',\n        reuse=False\n    )\n\n    # beam search decode\n    decodes, _ = tf.nn.ctc_beam_search_decoder(\n        inputs=inference_ret,\n        sequence_length=CFG.ARCH.SEQ_LENGTH * np.ones(1),\n        merge_repeated=False\n    )\n\n    saver = tf.train.Saver()\n\n    # Set sess configuration\n    sess_config = tf.ConfigProto(allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n    sess_config.gpu_options.allocator_type = \'BFC\'\n\n    sess = tf.Session(config=sess_config)\n\n    with sess.as_default():\n        saver.restore(sess=sess, save_path=ckpt_path)\n\n        # set model save builder\n        saved_builder = sm.builder.SavedModelBuilder(export_dir)\n\n        # add tensor need to be saved\n        saved_input_tensor = sm.utils.build_tensor_info(image_tensor)\n        indices_output_tensor_info = sm.utils.build_tensor_info(decodes[0].indices)\n        values_output_tensor_info = sm.utils.build_tensor_info(decodes[0].values)\n        dense_shape_output_tensor_info = sm.utils.build_tensor_info(decodes[0].dense_shape)\n\n        # build SignatureDef protobuf\n        signatur_def = sm.signature_def_utils.build_signature_def(\n            inputs={\'input_tensor\': saved_input_tensor},\n            outputs={\n                \'decodes_indices\': indices_output_tensor_info,\n                \'decodes_values\': values_output_tensor_info,\n                \'decodes_dense_shape\': dense_shape_output_tensor_info,\n            },\n            method_name=sm.signature_constants.PREDICT_METHOD_NAME,\n        )\n\n        # add graph into MetaGraphDef protobuf\n        saved_builder.add_meta_graph_and_variables(\n            sess,\n            tags=[sm.tag_constants.SERVING],\n            signature_def_map={sm.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signatur_def},\n        )\n\n        # save model\n        saved_builder.save()\n\n    return\n\n\nif __name__ == \'__main__\':\n    """"""\n    build saved model\n    """"""\n    # init args\n    args = init_args()\n\n    # build saved model\n    build_saved_model(args.ckpt_path, args.export_dir)\n'"
tools/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-21 \xe4\xb8\x8b\xe5\x8d\x886:37\n# @Author  : Luo Yao\n# @Site    : http://github.com/TJCVRS\n# @File    : __init__.py.py\n# @IDE: PyCharm Community Edition'
tools/apply_ocr_pdf.py,0,b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-10 \xe4\xb8\x8b\xe5\x8d\x885:31\n# @Author  : LuoYao\n# @Site    : ICode\n# @File    : apply_ocr_pdf.py\n# @IDE: PyCharm'
tools/evaluate_shadownet.py,7,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-25 \xe4\xb8\x8b\xe5\x8d\x883:56\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : evaluate_shadownet.py\n# @IDE: PyCharm Community Edition\n""""""\nEvaluate the crnn model on the synth90k test dataset\n""""""\nimport argparse\nimport os.path as ops\nimport math\nimport time\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport glog as log\nimport tqdm\nfrom sklearn.metrics import confusion_matrix\n\nfrom crnn_model import crnn_net\nfrom config import global_config\nfrom data_provider import shadownet_data_feed_pipline\nfrom data_provider import tf_io_pipline_fast_tools\nfrom local_utils import evaluation_tools\n\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n    :return: parsed arguments and (updated) config.cfg object\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-d\', \'--dataset_dir\', type=str,\n                        help=\'Directory containing test_features.tfrecords\')\n    parser.add_argument(\'-c\', \'--char_dict_path\', type=str,\n                        help=\'Directory where character dictionaries for the dataset were stored\')\n    parser.add_argument(\'-o\', \'--ord_map_dict_path\', type=str,\n                        help=\'Directory where ord map dictionaries for the dataset were stored\')\n    parser.add_argument(\'-w\', \'--weights_path\', type=str, required=True,\n                        help=\'Path to pre-trained weights\')\n    parser.add_argument(\'-v\', \'--visualize\', type=args_str2bool, nargs=\'?\', const=False,\n                        help=\'Whether to display images\')\n    parser.add_argument(\'-p\', \'--process_all\', type=args_str2bool, nargs=\'?\', const=False,\n                        help=\'Whether to process all test dataset\')\n\n    return parser.parse_args()\n\n\ndef args_str2bool(arg_value):\n    """"""\n\n    :param arg_value:\n    :return:\n    """"""\n    if arg_value.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n\n    elif arg_value.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Unsupported value encountered.\')\n\n\ndef evaluate_shadownet(dataset_dir, weights_path, char_dict_path,\n                       ord_map_dict_path, is_visualize=False,\n                       is_process_all_data=False):\n    """"""\n\n    :param dataset_dir:\n    :param weights_path:\n    :param char_dict_path:\n    :param ord_map_dict_path:\n    :param is_visualize:\n    :param is_process_all_data:\n    :return:\n    """"""\n    # prepare dataset\n    test_dataset = shadownet_data_feed_pipline.CrnnDataFeeder(\n        dataset_dir=dataset_dir,\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path,\n        flags=\'test\'\n    )\n    test_images, test_labels, test_images_paths = test_dataset.inputs(\n        batch_size=128\n    )\n\n    # set up test sample count\n    if is_process_all_data:\n        log.info(\'Start computing test dataset sample counts\')\n        t_start = time.time()\n        test_sample_count = test_dataset.sample_counts()\n        log.info(\'Test dataset sample counts: {:d}\'.format(test_sample_count))\n        log.info(\'Computing test dataset sample counts finished, cost time: {:.5f}\'.format(time.time() - t_start))\n        num_iterations = int(math.ceil(test_sample_count / 128))\n    else:\n        num_iterations = 1\n\n    # declare crnn net\n    shadownet = crnn_net.ShadowNet(\n        phase=\'test\',\n        hidden_nums=CFG.ARCH.HIDDEN_UNITS,\n        layers_nums=CFG.ARCH.HIDDEN_LAYERS,\n        num_classes=CFG.ARCH.NUM_CLASSES\n    )\n    # set up decoder\n    decoder = tf_io_pipline_fast_tools.CrnnFeatureReader(\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path\n    )\n\n    # compute inference result\n    test_inference_ret = shadownet.inference(\n        inputdata=test_images,\n        name=\'shadow_net\',\n        reuse=False\n    )\n    test_decoded, test_log_prob = tf.nn.ctc_greedy_decoder(\n        test_inference_ret,\n        CFG.ARCH.SEQ_LENGTH * np.ones(128),\n        merge_repeated=True\n    )\n\n    # recover image from [-1.0, 1.0] ---> [0.0, 255.0]\n    test_images = tf.multiply(tf.add(test_images, 1.0), 127.5, name=\'recoverd_test_images\')\n\n    # Set saver configuration\n    saver = tf.train.Saver()\n\n    # Set sess configuration\n    sess_config = tf.ConfigProto(allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n\n    sess = tf.Session(config=sess_config)\n\n    with sess.as_default():\n        saver.restore(sess=sess, save_path=weights_path)\n\n        log.info(\'Start predicting...\')\n\n        per_char_accuracy = 0.0\n        full_sequence_accuracy = 0.0\n\n        total_labels_char_list = []\n        total_predictions_char_list = []\n        epoch_tqdm = tqdm.tqdm(range(num_iterations))\n\n        while True:\n            try:\n                for epoch in epoch_tqdm:\n                    t_start = time.time()\n                    test_predictions_value, test_images_value, test_labels_value, test_images_paths_value = sess.run(\n                        [test_decoded, test_images, test_labels, test_images_paths])\n                    test_images_paths_value = np.reshape(\n                        test_images_paths_value,\n                        newshape=test_images_paths_value.shape[0]\n                    )\n                    test_images_paths_value = [tmp.decode(\'utf-8\') for tmp in test_images_paths_value]\n                    test_images_names_value = [ops.split(tmp)[1] for tmp in test_images_paths_value]\n                    test_labels_value = decoder.sparse_tensor_to_str(test_labels_value)\n                    test_predictions_value = decoder.sparse_tensor_to_str(test_predictions_value[0])\n\n                    per_char_accuracy += evaluation_tools.compute_accuracy(\n                        test_labels_value, test_predictions_value, display=False, mode=\'per_char\'\n                    )\n                    full_sequence_accuracy += evaluation_tools.compute_accuracy(\n                        test_labels_value, test_predictions_value, display=False, mode=\'full_sequence\'\n                    )\n\n                    for index, test_image in enumerate(test_images_value):\n                        log.info(\'Predict {:s} image with gt label: {:s} **** predicted label: {:s}\'.format(\n                            test_images_names_value[index],\n                            test_labels_value[index],\n                            test_predictions_value[index]))\n\n                        if is_visualize:\n                            plt.imshow(np.array(test_image, np.uint8)[:, :, (2, 1, 0)])\n                            plt.show()\n\n                        test_labels_char_list_value = [s for s in test_labels_value[index]]\n                        test_predictions_char_list_value = [s for s in test_predictions_value[index]]\n\n                        if not test_labels_char_list_value or not test_predictions_char_list_value:\n                            continue\n\n                        if len(test_labels_char_list_value) != len(test_predictions_char_list_value):\n                            min_length = min(len(test_labels_char_list_value),\n                                             len(test_predictions_char_list_value))\n                            test_labels_char_list_value = test_labels_char_list_value[:min_length - 1]\n                            test_predictions_char_list_value = test_predictions_char_list_value[:min_length - 1]\n\n                        assert len(test_labels_char_list_value) == len(test_predictions_char_list_value), \\\n                            log.error(\'{}, {}\'.format(test_labels_char_list_value, test_predictions_char_list_value))\n\n                        total_labels_char_list.extend(test_labels_char_list_value)\n                        total_predictions_char_list.extend(test_predictions_char_list_value)\n                        if is_visualize:\n                            plt.imshow(np.array(test_image, np.uint8)[:, :, (2, 1, 0)])\n                    epoch_tqdm.set_description(\'Epoch {:d} cost time: {:.5f}s\'.format(epoch, time.time() - t_start))\n                if num_iterations == 1:\n                    raise tf.errors.OutOfRangeError\n            except tf.errors.OutOfRangeError:\n                log.error(\'End of tfrecords sequence\')\n                break\n            except Exception as err:\n                log.error(err)\n                break\n\n        epoch_tqdm.close()\n        avg_per_char_accuracy = per_char_accuracy / num_iterations\n        avg_full_sequence_accuracy = full_sequence_accuracy / num_iterations\n        log.info(\'Mean test per char accuracy is {:5f}\'.format(avg_per_char_accuracy))\n        log.info(\'Mean test full sequence accuracy is {:5f}\'.format(avg_full_sequence_accuracy))\n\n        # compute confusion matrix\n        cnf_matrix = confusion_matrix(total_labels_char_list, total_predictions_char_list)\n        np.set_printoptions(precision=2)\n        evaluation_tools.plot_confusion_matrix(cm=cnf_matrix, normalize=True)\n\n        plt.show()\n\n\nif __name__ == \'__main__\':\n    """"""\n    test code\n    """"""\n    args = init_args()\n\n    evaluate_shadownet(\n        dataset_dir=args.dataset_dir,\n        weights_path=args.weights_path,\n        char_dict_path=args.char_dict_path,\n        ord_map_dict_path=args.ord_map_dict_path,\n        is_visualize=args.visualize,\n        is_process_all_data=args.process_all\n    )\n'"
tools/recongnize_chinese_pdf.py,6,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-8 \xe4\xb8\x8b\xe5\x8d\x8810:24\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : recongnize_chinese_pdf.py\n# @IDE: PyCharm\n""""""\ntest the model to recognize the chinese pdf file\n""""""\nimport argparse\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nfrom config import global_config\nfrom crnn_model import crnn_net\nfrom data_provider import tf_io_pipline_fast_tools\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n\n    :return: parsed arguments and (updated) config.cfg object\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--image_path\', type=str,\n                        help=\'Path to the image to be tested\',\n                        default=\'data/test_images/test_01.jpg\')\n    parser.add_argument(\'--weights_path\', type=str,\n                        help=\'Path to the pre-trained weights to use\')\n    parser.add_argument(\'-c\', \'--char_dict_path\', type=str,\n                        help=\'Directory where character dictionaries for the dataset were stored\')\n    parser.add_argument(\'-o\', \'--ord_map_dict_path\', type=str,\n                        help=\'Directory where ord map dictionaries for the dataset were stored\')\n    parser.add_argument(\'--save_path\', type=str,\n                        help=\'The output path of recognition result\')\n\n    return parser.parse_args()\n\n\ndef args_str2bool(arg_value):\n    """"""\n\n    :param arg_value:\n    :return:\n    """"""\n    if arg_value.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n\n    elif arg_value.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Unsupported value encountered.\')\n\n\ndef split_pdf_image_into_row_image_block(pdf_image):\n    """"""\n    split the whole pdf image into row image block\n    :param pdf_image: the whole color pdf image\n    :return:\n    """"""\n    gray_image = cv2.cvtColor(pdf_image, cv2.COLOR_BGR2GRAY)\n    binarized_image = cv2.adaptiveThreshold(\n        src=gray_image,\n        maxValue=255,\n        adaptiveMethod=cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n        thresholdType=cv2.THRESH_BINARY,\n        blockSize=11,\n        C=2\n    )\n\n    # sum along the row axis\n    row_sum = np.sum(binarized_image, axis=1)\n    idx_row_sum = np.argwhere(row_sum < row_sum.max())[:, 0]\n\n    split_idx = []\n\n    start_idx = idx_row_sum[0]\n    for index, idx in enumerate(idx_row_sum[:-1]):\n        if idx_row_sum[index + 1] - idx > 5:\n            end_idx = idx\n            split_idx.append((start_idx, end_idx))\n            start_idx = idx_row_sum[index + 1]\n    split_idx.append((start_idx, idx_row_sum[-1]))\n\n    pdf_image_splits = []\n    for index in range(len(split_idx)):\n        idx = split_idx[index]\n        pdf_image_split = pdf_image[idx[0]:idx[1], :, :]\n        pdf_image_splits.append(pdf_image_split)\n\n    return pdf_image_splits\n\n\ndef locate_text_area(pdf_image_row_block):\n    """"""\n    locate the text area of the image row block\n    :param pdf_image_row_block: color pdf image block\n    :return:\n    """"""\n    gray_image = cv2.cvtColor(pdf_image_row_block, cv2.COLOR_BGR2GRAY)\n    binarized_image = cv2.adaptiveThreshold(\n        src=gray_image,\n        maxValue=255,\n        adaptiveMethod=cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n        thresholdType=cv2.THRESH_BINARY,\n        blockSize=11,\n        C=2\n    )\n\n    # sum along the col axis\n    col_sum = np.sum(binarized_image, axis=0)\n    idx_col_sum = np.argwhere(col_sum < col_sum.max())[:, 0]\n\n    start_col = idx_col_sum[0] if idx_col_sum[0] > 0 else 0\n    end_col = idx_col_sum[-1]\n    end_col = end_col if end_col < pdf_image_row_block.shape[1] else pdf_image_row_block.shape[1] - 1\n\n    return pdf_image_row_block[:, start_col:end_col, :]\n\n\ndef recognize(image_path, weights_path, char_dict_path, ord_map_dict_path, output_path):\n    """"""\n\n    :param image_path:\n    :param weights_path:\n    :param char_dict_path:\n    :param ord_map_dict_path:\n    :param output_path:\n    :return:\n    """"""\n    # read pdf image\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n\n    # split pdf image into row block\n    pdf_image_row_blocks = split_pdf_image_into_row_image_block(image)\n\n    # locate the text area in each row block\n    pdf_image_text_areas = []\n    new_heigth = 32\n    max_text_area_length = -1\n    for index, row_block in enumerate(pdf_image_row_blocks):\n        text_area = locate_text_area(row_block)\n        text_area_height = text_area.shape[0]\n        scale = new_heigth / text_area_height\n        max_text_area_length = max(max_text_area_length, int(scale * text_area.shape[1]))\n        pdf_image_text_areas.append(text_area)\n    new_width = max_text_area_length\n    new_width = new_width if new_width > CFG.ARCH.INPUT_SIZE[0] else CFG.ARCH.INPUT_SIZE[0]\n\n    # definite the compute graph\n    inputdata = tf.placeholder(\n        dtype=tf.float32,\n        shape=[1, new_heigth, new_width, CFG.ARCH.INPUT_CHANNELS],\n        name=\'input\'\n    )\n\n    codec = tf_io_pipline_fast_tools.CrnnFeatureReader(\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path\n    )\n\n    net = crnn_net.ShadowNet(\n        phase=\'test\',\n        hidden_nums=CFG.ARCH.HIDDEN_UNITS,\n        layers_nums=CFG.ARCH.HIDDEN_LAYERS,\n        num_classes=CFG.ARCH.NUM_CLASSES\n    )\n\n    inference_ret = net.inference(\n        inputdata=inputdata,\n        name=\'shadow_net\',\n        reuse=False\n    )\n\n    decodes, _ = tf.nn.ctc_beam_search_decoder(\n        inputs=inference_ret,\n        sequence_length=int(new_width / 4) * np.ones(1),\n        merge_repeated=False,\n        beam_width=1\n    )\n\n    # config tf saver\n    saver = tf.train.Saver()\n\n    # config tf session\n    sess_config = tf.ConfigProto(allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TEST.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TEST.TF_ALLOW_GROWTH\n\n    sess = tf.Session(config=sess_config)\n\n    with sess.as_default():\n        saver.restore(sess=sess, save_path=weights_path)\n\n        pdf_recognize_results = []\n\n        for index, pdf_image_text_area in enumerate(pdf_image_text_areas):\n            # resize text area into size (None, new_height)\n            pdf_image_text_area_height = pdf_image_text_area.shape[0]\n            scale = new_heigth / pdf_image_text_area_height\n            new_width_tmp = int(scale * pdf_image_text_area.shape[1])\n            pdf_image_text_area = cv2.resize(\n                pdf_image_text_area, (new_width_tmp, new_heigth),\n                interpolation=cv2.INTER_LINEAR)\n            # pad text area into size (new_width, new_height) if new_width_tmp < new_width\n            if new_width_tmp < new_width:\n                pad_area_width = new_width - new_width_tmp\n                pad_area = np.zeros(shape=[new_heigth, pad_area_width, 3], dtype=np.uint8) + 255\n                pdf_image_text_area = np.concatenate((pdf_image_text_area, pad_area), axis=1)\n\n            pdf_image_text_area = np.array(pdf_image_text_area, np.float32) / 127.5 - 1.0\n\n            preds = sess.run(decodes, feed_dict={inputdata: [pdf_image_text_area]})\n\n            preds = codec.sparse_tensor_to_str(preds[0])\n\n            pdf_recognize_results.append(preds[0])\n\n        output_text = []\n\n        need_tab = True\n        for index, pdf_text in enumerate(pdf_recognize_results):\n            if need_tab:\n                text_console_str = \'----     {:s}\'.format(pdf_text)\n                text_file_str = \'     {:s}\'.format(pdf_text)\n                print(text_console_str)\n                output_text.append(text_file_str)\n                need_tab = \\\n                    index < (len(pdf_recognize_results) - 1) and \\\n                    len(pdf_recognize_results[index + 1]) - len(pdf_text) > 10\n            else:\n                text_console_str = \'---- {:s}\'.format(pdf_text)\n                text_file_str = \' {:s}\'.format(pdf_text)\n                print(text_console_str)\n                output_text.append(text_file_str)\n                need_tab = \\\n                    index < (len(pdf_recognize_results) - 1) and \\\n                    len(pdf_recognize_results[index + 1]) - len(pdf_text) > 10\n\n        res = \'\\n\'.join(output_text)\n\n        with open(output_path, \'w\') as file:\n            file.writelines(res)\n\n    return\n\n\nif __name__ == \'__main__\':\n    """"""\n\n    """"""\n    # init images\n    args = init_args()\n\n    # detect images\n    recognize(\n        image_path=args.image_path,\n        weights_path=args.weights_path,\n        char_dict_path=args.char_dict_path,\n        ord_map_dict_path=args.ord_map_dict_path,\n        output_path=args.save_path\n    )\n'"
tools/test_shadownet.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-29 \xe4\xb8\x8b\xe5\x8d\x883:56\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : test_shadownet.py\n# @IDE: PyCharm Community Edition\n""""""\nUse shadow net to recognize the scene text of a single image\n""""""\nimport argparse\nimport os.path as ops\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport glog as logger\nimport wordninja\n\nfrom config import global_config\nfrom crnn_model import crnn_net\nfrom data_provider import tf_io_pipline_fast_tools\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n\n    :return: parsed arguments and (updated) config.cfg object\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--image_path\', type=str,\n                        help=\'Path to the image to be tested\',\n                        default=\'data/test_images/test_01.jpg\')\n    parser.add_argument(\'--weights_path\', type=str,\n                        help=\'Path to the pre-trained weights to use\')\n    parser.add_argument(\'-c\', \'--char_dict_path\', type=str,\n                        help=\'Directory where character dictionaries for the dataset were stored\')\n    parser.add_argument(\'-o\', \'--ord_map_dict_path\', type=str,\n                        help=\'Directory where ord map dictionaries for the dataset were stored\')\n    parser.add_argument(\'-v\', \'--visualize\', type=args_str2bool, nargs=\'?\', const=True,\n                        help=\'Whether to display images\')\n\n    return parser.parse_args()\n\n\ndef args_str2bool(arg_value):\n    """"""\n\n    :param arg_value:\n    :return:\n    """"""\n    if arg_value.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n\n    elif arg_value.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Unsupported value encountered.\')\n\n\ndef recognize(image_path, weights_path, char_dict_path, ord_map_dict_path, is_vis, is_english=False):\n    """"""\n\n    :param image_path:\n    :param weights_path:\n    :param char_dict_path:\n    :param ord_map_dict_path:\n    :param is_vis:\n    :param is_english:\n    :return:\n    """"""\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    image = cv2.resize(image, dsize=tuple(CFG.ARCH.INPUT_SIZE), interpolation=cv2.INTER_LINEAR)\n    image_vis = image\n    image = np.array(image, np.float32) / 127.5 - 1.0\n\n    inputdata = tf.placeholder(\n        dtype=tf.float32,\n        shape=[1, CFG.ARCH.INPUT_SIZE[1], CFG.ARCH.INPUT_SIZE[0], CFG.ARCH.INPUT_CHANNELS],\n        name=\'input\'\n    )\n\n    codec = tf_io_pipline_fast_tools.CrnnFeatureReader(\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path\n    )\n\n    net = crnn_net.ShadowNet(\n        phase=\'test\',\n        hidden_nums=CFG.ARCH.HIDDEN_UNITS,\n        layers_nums=CFG.ARCH.HIDDEN_LAYERS,\n        num_classes=CFG.ARCH.NUM_CLASSES\n    )\n\n    inference_ret = net.inference(\n        inputdata=inputdata,\n        name=\'shadow_net\',\n        reuse=False\n    )\n\n    decodes, _ = tf.nn.ctc_greedy_decoder(\n        inference_ret,\n        CFG.ARCH.SEQ_LENGTH * np.ones(1),\n        merge_repeated=True\n    )\n\n    # config tf saver\n    saver = tf.train.Saver()\n\n    # config tf session\n    sess_config = tf.ConfigProto(allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TEST.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TEST.TF_ALLOW_GROWTH\n\n    sess = tf.Session(config=sess_config)\n\n    with sess.as_default():\n\n        saver.restore(sess=sess, save_path=weights_path)\n\n        preds = sess.run(decodes, feed_dict={inputdata: [image]})\n\n        preds = codec.sparse_tensor_to_str(preds[0])[0]\n        if is_english:\n            preds = \' \'.join(wordninja.split(preds))\n\n        logger.info(\'Predict image {:s} result: {:s}\'.format(\n            ops.split(image_path)[1], preds)\n        )\n\n        if is_vis:\n            plt.figure(\'CRNN Model Demo\')\n            plt.imshow(image_vis[:, :, (2, 1, 0)])\n            plt.show()\n\n    sess.close()\n\n    return\n\n\nif __name__ == \'__main__\':\n    """"""\n    \n    """"""\n    # init images\n    args = init_args()\n\n    # detect images\n    recognize(\n        image_path=args.image_path,\n        weights_path=args.weights_path,\n        char_dict_path=args.char_dict_path,\n        ord_map_dict_path=args.ord_map_dict_path,\n        is_vis=args.visualize\n    )\n'"
tools/train_shadownet.py,52,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-22 \xe4\xb8\x8b\xe5\x8d\x881:39\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : train_shadownet.py\n# @IDE: PyCharm Community Edition\n""""""\nTrain shadow net script\n""""""\nimport os\nimport os.path as ops\nimport time\nimport math\nimport argparse\n\nimport tensorflow as tf\nimport glog as logger\nimport numpy as np\n\nfrom crnn_model import crnn_net\nfrom local_utils import evaluation_tools\nfrom config import global_config\nfrom data_provider import shadownet_data_feed_pipline\nfrom data_provider import tf_io_pipline_fast_tools\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n    :return: parsed arguments and (updated) config.cfg object\n    """"""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'-d\', \'--dataset_dir\', type=str,\n                        help=\'Directory containing train_features.tfrecords\')\n    parser.add_argument(\'-w\', \'--weights_path\', type=str,\n                        help=\'Path to pre-trained weights to continue training\')\n    parser.add_argument(\'-c\', \'--char_dict_path\', type=str,\n                        help=\'Directory where character dictionaries for the dataset were stored\')\n    parser.add_argument(\'-o\', \'--ord_map_dict_path\', type=str,\n                        help=\'Directory where ord map dictionaries for the dataset were stored\')\n    parser.add_argument(\'-e\', \'--decode_outputs\', type=args_str2bool, default=False,\n                        help=\'Activate decoding of predictions during training (slow!)\')\n    parser.add_argument(\'-m\', \'--multi_gpus\', type=args_str2bool, default=False,\n                        nargs=\'?\', const=True, help=\'Use multi gpus to train\')\n\n    return parser.parse_args()\n\n\ndef args_str2bool(arg_value):\n    """"""\n\n    :param arg_value:\n    :return:\n    """"""\n    if arg_value.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n\n    elif arg_value.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Unsupported value encountered.\')\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n\n    return average_grads\n\n\ndef compute_net_gradients(images, labels, net, optimizer=None, is_net_first_initialized=False):\n    """"""\n    Calculate gradients for single GPU\n    :param images: images for training\n    :param labels: labels corresponding to images\n    :param net: classification model\n    :param optimizer: network optimizer\n    :param is_net_first_initialized: if the network is initialized\n    :return:\n    """"""\n    _, net_loss = net.compute_loss(\n        inputdata=images,\n        labels=labels,\n        name=\'shadow_net\',\n        reuse=is_net_first_initialized\n    )\n\n    if optimizer is not None:\n        grads = optimizer.compute_gradients(net_loss)\n    else:\n        grads = None\n\n    return net_loss, grads\n\n\ndef train_shadownet(dataset_dir, weights_path, char_dict_path, ord_map_dict_path, need_decode=False):\n    """"""\n\n    :param dataset_dir:\n    :param weights_path:\n    :param char_dict_path:\n    :param ord_map_dict_path:\n    :param need_decode:\n    :return:\n    """"""\n    # prepare dataset\n    train_dataset = shadownet_data_feed_pipline.CrnnDataFeeder(\n        dataset_dir=dataset_dir,\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path,\n        flags=\'train\'\n    )\n    val_dataset = shadownet_data_feed_pipline.CrnnDataFeeder(\n        dataset_dir=dataset_dir,\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path,\n        flags=\'val\'\n    )\n    train_images, train_labels, train_images_paths = train_dataset.inputs(\n        batch_size=CFG.TRAIN.BATCH_SIZE\n    )\n    val_images, val_labels, val_images_paths = val_dataset.inputs(\n        batch_size=CFG.TRAIN.BATCH_SIZE\n    )\n\n    # declare crnn net\n    shadownet = crnn_net.ShadowNet(\n        phase=\'train\',\n        hidden_nums=CFG.ARCH.HIDDEN_UNITS,\n        layers_nums=CFG.ARCH.HIDDEN_LAYERS,\n        num_classes=CFG.ARCH.NUM_CLASSES\n    )\n    shadownet_val = crnn_net.ShadowNet(\n        phase=\'test\',\n        hidden_nums=CFG.ARCH.HIDDEN_UNITS,\n        layers_nums=CFG.ARCH.HIDDEN_LAYERS,\n        num_classes=CFG.ARCH.NUM_CLASSES\n    )\n\n    # set up decoder\n    decoder = tf_io_pipline_fast_tools.CrnnFeatureReader(\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path\n    )\n\n    # compute loss and seq distance\n    train_inference_ret, train_ctc_loss = shadownet.compute_loss(\n        inputdata=train_images,\n        labels=train_labels,\n        name=\'shadow_net\',\n        reuse=False\n    )\n    val_inference_ret, val_ctc_loss = shadownet_val.compute_loss(\n        inputdata=val_images,\n        labels=val_labels,\n        name=\'shadow_net\',\n        reuse=True\n    )\n\n    train_decoded, train_log_prob = tf.nn.ctc_greedy_decoder(\n        train_inference_ret,\n        CFG.ARCH.SEQ_LENGTH * np.ones(CFG.TRAIN.BATCH_SIZE),\n        merge_repeated=False\n    )\n    val_decoded, val_log_prob = tf.nn.ctc_greedy_decoder(\n        val_inference_ret,\n        CFG.ARCH.SEQ_LENGTH * np.ones(CFG.TRAIN.BATCH_SIZE),\n        merge_repeated=False\n    )\n\n    train_sequence_dist = tf.reduce_mean(\n        tf.edit_distance(tf.cast(train_decoded[0], tf.int32), train_labels),\n        name=\'train_edit_distance\'\n    )\n    val_sequence_dist = tf.reduce_mean(\n        tf.edit_distance(tf.cast(val_decoded[0], tf.int32), val_labels),\n        name=\'val_edit_distance\'\n    )\n\n    # set learning rate\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    learning_rate = tf.train.polynomial_decay(\n        learning_rate=CFG.TRAIN.LEARNING_RATE,\n        global_step=global_step,\n        decay_steps=CFG.TRAIN.EPOCHS,\n        end_learning_rate=0.000001,\n        power=CFG.TRAIN.LR_DECAY_RATE\n    )\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        optimizer = tf.train.MomentumOptimizer(\n            learning_rate=learning_rate, momentum=0.9).minimize(\n            loss=train_ctc_loss, global_step=global_step)\n\n    # Set tf summary\n    tboard_save_dir = \'tboard/crnn_syn90k\'\n    os.makedirs(tboard_save_dir, exist_ok=True)\n    tf.summary.scalar(name=\'train_ctc_loss\', tensor=train_ctc_loss)\n    tf.summary.scalar(name=\'val_ctc_loss\', tensor=val_ctc_loss)\n    tf.summary.scalar(name=\'learning_rate\', tensor=learning_rate)\n\n    if need_decode:\n        tf.summary.scalar(name=\'train_seq_distance\', tensor=train_sequence_dist)\n        tf.summary.scalar(name=\'val_seq_distance\', tensor=val_sequence_dist)\n\n    merge_summary_op = tf.summary.merge_all()\n\n    # Set saver configuration\n    saver = tf.train.Saver()\n    model_save_dir = \'model/crnn_syn90k\'\n    os.makedirs(model_save_dir, exist_ok=True)\n    train_start_time = time.strftime(\'%Y-%m-%d-%H-%M-%S\', time.localtime(time.time()))\n    model_name = \'shadownet_{:s}.ckpt\'.format(str(train_start_time))\n    model_save_path = ops.join(model_save_dir, model_name)\n\n    # Set sess configuration\n    sess_config = tf.ConfigProto(allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n\n    sess = tf.Session(config=sess_config)\n\n    summary_writer = tf.summary.FileWriter(tboard_save_dir)\n    summary_writer.add_graph(sess.graph)\n\n    # Set the training parameters\n    train_epochs = CFG.TRAIN.EPOCHS\n\n    with sess.as_default():\n        epoch = 0\n        if weights_path is None:\n            logger.info(\'Training from scratch\')\n            init = tf.global_variables_initializer()\n            sess.run(init)\n        else:\n            logger.info(\'Restore model from {:s}\'.format(weights_path))\n            saver.restore(sess=sess, save_path=weights_path)\n            epoch = sess.run(tf.train.get_global_step())\n\n        patience_counter = 1\n        cost_history = [np.inf]\n        while epoch < train_epochs:\n            epoch += 1\n            # setup early stopping\n            if epoch > 1 and CFG.TRAIN.EARLY_STOPPING:\n                # We always compare to the first point where cost didn\'t improve\n                if cost_history[-1 - patience_counter] - cost_history[-1] > CFG.TRAIN.PATIENCE_DELTA:\n                    patience_counter = 1\n                else:\n                    patience_counter += 1\n                if patience_counter > CFG.TRAIN.PATIENCE_EPOCHS:\n                    logger.info(""Cost didn\'t improve beyond {:f} for {:d} epochs, stopping early."".\n                                format(CFG.TRAIN.PATIENCE_DELTA, patience_counter))\n                    break\n\n            if need_decode and epoch % 500 == 0:\n                # train part\n                _, train_ctc_loss_value, train_seq_dist_value, \\\n                    train_predictions, train_labels_sparse, merge_summary_value = sess.run(\n                     [optimizer, train_ctc_loss, train_sequence_dist,\n                      train_decoded, train_labels, merge_summary_op])\n\n                train_labels_str = decoder.sparse_tensor_to_str(train_labels_sparse)\n                train_predictions = decoder.sparse_tensor_to_str(train_predictions[0])\n                avg_train_accuracy = evaluation_tools.compute_accuracy(train_labels_str, train_predictions)\n\n                if epoch % CFG.TRAIN.DISPLAY_STEP == 0:\n                    logger.info(\'Epoch_Train: {:d} cost= {:9f} seq distance= {:9f} train accuracy= {:9f}\'.format(\n                        epoch + 1, train_ctc_loss_value, train_seq_dist_value, avg_train_accuracy))\n\n                # validation part\n                val_ctc_loss_value, val_seq_dist_value, \\\n                    val_predictions, val_labels_sparse = sess.run(\n                     [val_ctc_loss, val_sequence_dist, val_decoded, val_labels])\n\n                val_labels_str = decoder.sparse_tensor_to_str(val_labels_sparse)\n                val_predictions = decoder.sparse_tensor_to_str(val_predictions[0])\n                avg_val_accuracy = evaluation_tools.compute_accuracy(val_labels_str, val_predictions)\n\n                if epoch % CFG.TRAIN.DISPLAY_STEP == 0:\n                    logger.info(\'Epoch_Val: {:d} cost= {:9f} seq distance= {:9f} train accuracy= {:9f}\'.format(\n                        epoch + 1, val_ctc_loss_value, val_seq_dist_value, avg_val_accuracy))\n            else:\n                _, train_ctc_loss_value, merge_summary_value = sess.run(\n                    [optimizer, train_ctc_loss, merge_summary_op])\n\n                if epoch % CFG.TRAIN.DISPLAY_STEP == 0:\n                    logger.info(\'Epoch_Train: {:d} cost= {:9f}\'.format(epoch + 1, train_ctc_loss_value))\n\n            # record history train ctc loss\n            cost_history.append(train_ctc_loss_value)\n            # add training sumary\n            summary_writer.add_summary(summary=merge_summary_value, global_step=epoch)\n\n            if epoch % 2000 == 0:\n                saver.save(sess=sess, save_path=model_save_path, global_step=epoch)\n\n    return np.array(cost_history[1:])  # Don\'t return the first np.inf\n\n\ndef train_shadownet_multi_gpu(dataset_dir, weights_path, char_dict_path, ord_map_dict_path):\n    """"""\n\n    :param dataset_dir:\n    :param weights_path:\n    :param char_dict_path:\n    :param ord_map_dict_path:\n    :return:\n    """"""\n    # prepare dataset information\n    train_dataset = shadownet_data_feed_pipline.CrnnDataFeeder(\n        dataset_dir=dataset_dir,\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path,\n        flags=\'train\'\n    )\n    val_dataset = shadownet_data_feed_pipline.CrnnDataFeeder(\n        dataset_dir=dataset_dir,\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path,\n        flags=\'val\'\n    )\n\n    train_samples = []\n    val_samples = []\n    for i in range(CFG.TRAIN.GPU_NUM):\n        train_samples.append(train_dataset.inputs(batch_size=CFG.TRAIN.BATCH_SIZE))\n        val_samples.append(val_dataset.inputs(batch_size=CFG.TRAIN.BATCH_SIZE))\n\n    # set crnn net\n    shadownet = crnn_net.ShadowNet(\n        phase=\'train\',\n        hidden_nums=CFG.ARCH.HIDDEN_UNITS,\n        layers_nums=CFG.ARCH.HIDDEN_LAYERS,\n        num_classes=CFG.ARCH.NUM_CLASSES\n    )\n    shadownet_val = crnn_net.ShadowNet(\n        phase=\'test\',\n        hidden_nums=CFG.ARCH.HIDDEN_UNITS,\n        layers_nums=CFG.ARCH.HIDDEN_LAYERS,\n        num_classes=CFG.ARCH.NUM_CLASSES\n    )\n\n    # set average container\n    tower_grads = []\n    train_tower_loss = []\n    val_tower_loss = []\n    batchnorm_updates = None\n    train_summary_op_updates = None\n\n    # set lr\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    learning_rate = tf.train.polynomial_decay(\n        learning_rate=CFG.TRAIN.LEARNING_RATE,\n        global_step=global_step,\n        decay_steps=CFG.TRAIN.EPOCHS,\n        end_learning_rate=0.000001,\n        power=CFG.TRAIN.LR_DECAY_RATE\n    )\n\n    # set up optimizer\n    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n\n    # set distributed train op\n    with tf.variable_scope(tf.get_variable_scope()):\n        is_network_initialized = False\n        for i in range(CFG.TRAIN.GPU_NUM):\n            with tf.device(\'/gpu:{:d}\'.format(i)):\n                with tf.name_scope(\'tower_{:d}\'.format(i)) as _:\n                    train_images = train_samples[i][0]\n                    train_labels = train_samples[i][1]\n                    train_loss, grads = compute_net_gradients(\n                        train_images, train_labels, shadownet, optimizer,\n                        is_net_first_initialized=is_network_initialized)\n\n                    is_network_initialized = True\n\n                    # Only use the mean and var in the first gpu tower to update the parameter\n                    if i == 0:\n                        batchnorm_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n                        train_summary_op_updates = tf.get_collection(tf.GraphKeys.SUMMARIES)\n\n                    tower_grads.append(grads)\n                    train_tower_loss.append(train_loss)\n                with tf.name_scope(\'validation_{:d}\'.format(i)) as _:\n                    val_images = val_samples[i][0]\n                    val_labels = val_samples[i][1]\n                    val_loss, _ = compute_net_gradients(\n                        val_images, val_labels, shadownet_val, optimizer,\n                        is_net_first_initialized=is_network_initialized)\n                    val_tower_loss.append(val_loss)\n\n    grads = average_gradients(tower_grads)\n    avg_train_loss = tf.reduce_mean(train_tower_loss)\n    avg_val_loss = tf.reduce_mean(val_tower_loss)\n\n    # Track the moving averages of all trainable variables\n    variable_averages = tf.train.ExponentialMovingAverage(\n        CFG.TRAIN.MOVING_AVERAGE_DECAY, num_updates=global_step)\n    variables_to_average = tf.trainable_variables() + tf.moving_average_variables()\n    variables_averages_op = variable_averages.apply(variables_to_average)\n\n    # Group all the op needed for training\n    batchnorm_updates_op = tf.group(*batchnorm_updates)\n    apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n    train_op = tf.group(apply_gradient_op, variables_averages_op,\n                        batchnorm_updates_op)\n\n    # set tensorflow summary\n    tboard_save_path = \'tboard/crnn_syn90k_multi_gpu\'\n    os.makedirs(tboard_save_path, exist_ok=True)\n\n    summary_writer = tf.summary.FileWriter(tboard_save_path)\n\n    avg_train_loss_scalar = tf.summary.scalar(name=\'average_train_loss\',\n                                              tensor=avg_train_loss)\n    avg_val_loss_scalar = tf.summary.scalar(name=\'average_val_loss\',\n                                            tensor=avg_val_loss)\n    learning_rate_scalar = tf.summary.scalar(name=\'learning_rate_scalar\',\n                                             tensor=learning_rate)\n    train_merge_summary_op = tf.summary.merge(\n        [avg_train_loss_scalar, learning_rate_scalar] + train_summary_op_updates\n    )\n    val_merge_summary_op = tf.summary.merge([avg_val_loss_scalar])\n\n    # set tensorflow saver\n    saver = tf.train.Saver()\n    model_save_dir = \'model/crnn_syn90k_multi_gpu\'\n    os.makedirs(model_save_dir, exist_ok=True)\n    train_start_time = time.strftime(\'%Y-%m-%d-%H-%M-%S\', time.localtime(time.time()))\n    model_name = \'shadownet_{:s}.ckpt\'.format(str(train_start_time))\n    model_save_path = ops.join(model_save_dir, model_name)\n\n    # set sess config\n    sess_config = tf.ConfigProto(device_count={\'GPU\': CFG.TRAIN.GPU_NUM}, allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n    sess_config.gpu_options.allocator_type = \'BFC\'\n\n    # Set the training parameters\n    train_epochs = CFG.TRAIN.EPOCHS\n\n    logger.info(\'Global configuration is as follows:\')\n    logger.info(CFG)\n\n    sess = tf.Session(config=sess_config)\n\n    summary_writer.add_graph(sess.graph)\n\n    with sess.as_default():\n        epoch = 0\n        if weights_path is None:\n            logger.info(\'Training from scratch\')\n            init = tf.global_variables_initializer()\n            sess.run(init)\n        else:\n            logger.info(\'Restore model from last model checkpoint {:s}\'.format(weights_path))\n            saver.restore(sess=sess, save_path=weights_path)\n            epoch = sess.run(tf.train.get_global_step())\n\n        train_cost_time_mean = []\n        val_cost_time_mean = []\n\n        while epoch < train_epochs:\n            epoch += 1\n            # training part\n            t_start = time.time()\n\n            _, train_loss_value, train_summary, lr = \\\n                sess.run(fetches=[train_op,\n                                  avg_train_loss,\n                                  train_merge_summary_op,\n                                  learning_rate])\n\n            if math.isnan(train_loss_value):\n                raise ValueError(\'Train loss is nan\')\n\n            cost_time = time.time() - t_start\n            train_cost_time_mean.append(cost_time)\n\n            summary_writer.add_summary(summary=train_summary,\n                                       global_step=epoch)\n\n            # validation part\n            t_start_val = time.time()\n\n            val_loss_value, val_summary = \\\n                sess.run(fetches=[avg_val_loss,\n                                  val_merge_summary_op])\n\n            summary_writer.add_summary(val_summary, global_step=epoch)\n\n            cost_time_val = time.time() - t_start_val\n            val_cost_time_mean.append(cost_time_val)\n\n            if epoch % CFG.TRAIN.DISPLAY_STEP == 0:\n                logger.info(\'Epoch_Train: {:d} total_loss= {:6f} \'\n                            \'lr= {:6f} mean_cost_time= {:5f}s \'.\n                            format(epoch + 1,\n                                   train_loss_value,\n                                   lr,\n                                   np.mean(train_cost_time_mean)\n                                   ))\n                train_cost_time_mean.clear()\n\n            if epoch % CFG.TRAIN.VAL_DISPLAY_STEP == 0:\n                logger.info(\'Epoch_Val: {:d} total_loss= {:6f} \'\n                            \' mean_cost_time= {:5f}s \'.\n                            format(epoch + 1,\n                                   val_loss_value,\n                                   np.mean(val_cost_time_mean)))\n                val_cost_time_mean.clear()\n\n            if epoch % 5000 == 0:\n                saver.save(sess=sess, save_path=model_save_path, global_step=epoch)\n    sess.close()\n\n    return\n\n\nif __name__ == \'__main__\':\n\n    # init args\n    args = init_args()\n\n    if args.multi_gpus:\n        logger.info(\'Use multi gpus to train the model\')\n        train_shadownet_multi_gpu(\n            dataset_dir=args.dataset_dir,\n            weights_path=args.weights_path,\n            char_dict_path=args.char_dict_path,\n            ord_map_dict_path=args.ord_map_dict_path\n        )\n    else:\n        logger.info(\'Use single gpu to train the model\')\n        train_shadownet(\n            dataset_dir=args.dataset_dir,\n            weights_path=args.weights_path,\n            char_dict_path=args.char_dict_path,\n            ord_map_dict_path=args.ord_map_dict_path,\n            need_decode=args.decode_outputs\n        )\n'"
tools/write_tfrecords.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-3-13 \xe4\xb8\x8b\xe5\x8d\x881:31\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n# @File    : write_tfrecords.py\n# @IDE: PyCharm\n""""""\nWrite tfrecords tools\n""""""\nimport argparse\nimport os\nimport os.path as ops\n\nfrom data_provider import shadownet_data_feed_pipline\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-d\', \'--dataset_dir\', type=str, help=\'The origin synth90k dataset_dir\')\n    parser.add_argument(\'-s\', \'--save_dir\', type=str, help=\'The generated tfrecords save dir\')\n    parser.add_argument(\'-c\', \'--char_dict_path\', type=str, default=None,\n                        help=\'The char dict file path. If it is None the char dict will be\'\n                             \'generated automatically in folder data/char_dict\')\n    parser.add_argument(\'-o\', \'--ord_map_dict_path\', type=str, default=None,\n                        help=\'The ord map dict file path. If it is None the ord map dict will be\'\n                             \'generated automatically in folder data/char_dict\')\n\n    return parser.parse_args()\n\n\ndef write_tfrecords(dataset_dir, char_dict_path, ord_map_dict_path, save_dir):\n    """"""\n    Write tensorflow records for training , testing and validation\n    :param dataset_dir: the root dir of crnn dataset\n    :param char_dict_path: json file path which contains the map relation\n    between ord value and single character\n    :param ord_map_dict_path: json file path which contains the map relation\n    between int index value and char ord value\n    :param save_dir: the root dir of tensorflow records to write into\n    :return:\n    """"""\n    assert ops.exists(dataset_dir), \'{:s} not exist\'.format(dataset_dir)\n\n    os.makedirs(save_dir, exist_ok=True)\n\n    # test crnn data producer\n    producer = shadownet_data_feed_pipline.CrnnDataProducer(\n        dataset_dir=dataset_dir,\n        char_dict_path=char_dict_path,\n        ord_map_dict_path=ord_map_dict_path,\n        writer_process_nums=8\n    )\n\n    producer.generate_tfrecords(\n        save_dir=save_dir\n    )\n\n\nif __name__ == \'__main__\':\n    """"""\n    generate tfrecords\n    """"""\n    args = init_args()\n\n    write_tfrecords(\n        dataset_dir=args.dataset_dir,\n        char_dict_path=args.char_dict_path,\n        ord_map_dict_path=args.ord_map_dict_path,\n        save_dir=args.save_dir\n    )\n'"
