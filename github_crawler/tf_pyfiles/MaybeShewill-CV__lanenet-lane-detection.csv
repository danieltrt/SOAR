file_path,api_count,code
config/global_config.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-1-31 \xe4\xb8\x8a\xe5\x8d\x8811:21\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : global_config.py\n# @IDE: PyCharm Community Edition\n""""""\nSet global configuration\n""""""\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by: from config import cfg\n\ncfg = __C\n\n# Train options\n__C.TRAIN = edict()\n\n# Set the shadownet training epochs\n__C.TRAIN.EPOCHS = 80010\n# Set the display step\n__C.TRAIN.DISPLAY_STEP = 1\n# Set the test display step during training process\n__C.TRAIN.VAL_DISPLAY_STEP = 1000\n# Set the momentum parameter of the optimizer\n__C.TRAIN.MOMENTUM = 0.9\n# Set the initial learning rate\n__C.TRAIN.LEARNING_RATE = 0.0005\n# Set the GPU resource used during training process\n__C.TRAIN.GPU_MEMORY_FRACTION = 0.95\n# Set the GPU allow growth parameter during tensorflow training process\n__C.TRAIN.TF_ALLOW_GROWTH = True\n# Set the shadownet training batch size\n__C.TRAIN.BATCH_SIZE = 4\n# Set the shadownet validation batch size\n__C.TRAIN.VAL_BATCH_SIZE = 4\n# Set the class numbers\n__C.TRAIN.CLASSES_NUMS = 2\n# Set the image height\n__C.TRAIN.IMG_HEIGHT = 256\n# Set the image width\n__C.TRAIN.IMG_WIDTH = 512\n# Set the embedding features dims\n__C.TRAIN.EMBEDDING_FEATS_DIMS = 4\n# Set the random crop pad size\n__C.TRAIN.CROP_PAD_SIZE = 32\n# Set cpu multi process thread nums\n__C.TRAIN.CPU_MULTI_PROCESS_NUMS = 6\n# Set the train moving average decay\n__C.TRAIN.MOVING_AVERAGE_DECAY = 0.9999\n# Set the GPU nums\n__C.TRAIN.GPU_NUM = 2\n\n# Test options\n__C.TEST = edict()\n\n# Set the GPU resource used during testing process\n__C.TEST.GPU_MEMORY_FRACTION = 0.8\n# Set the GPU allow growth parameter during tensorflow testing process\n__C.TEST.TF_ALLOW_GROWTH = True\n# Set the test batch size\n__C.TEST.BATCH_SIZE = 2\n\n# Test options\n__C.POSTPROCESS = edict()\n\n# Set the post process connect components analysis min area threshold\n__C.POSTPROCESS.MIN_AREA_THRESHOLD = 100\n# Set the post process dbscan search radius threshold\n__C.POSTPROCESS.DBSCAN_EPS = 0.35\n# Set the post process dbscan min samples threshold\n__C.POSTPROCESS.DBSCAN_MIN_SAMPLES = 1000\n'"
data_provider/lanenet_data_feed_pipline.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-23 \xe4\xb8\x8b\xe5\x8d\x883:54\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : lanenet_data_feed_pipline.py\n# @IDE: PyCharm\n""""""\nLanenet data feed pip line\n""""""\nimport argparse\nimport glob\nimport os\nimport os.path as ops\nimport random\n\nimport glog as log\nimport tensorflow as tf\n\nfrom config import global_config\nfrom data_provider import tf_io_pipline_tools\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dataset_dir\', type=str, help=\'The source nsfw data dir path\')\n    parser.add_argument(\'--tfrecords_dir\', type=str, help=\'The dir path to save converted tfrecords\')\n\n    return parser.parse_args()\n\n\nclass LaneNetDataProducer(object):\n    """"""\n    Convert raw image file into tfrecords\n    """"""\n\n    def __init__(self, dataset_dir):\n        """"""\n\n        :param dataset_dir:\n        """"""\n        self._dataset_dir = dataset_dir\n\n        self._gt_image_dir = ops.join(dataset_dir, \'gt_image\')\n        self._gt_binary_image_dir = ops.join(dataset_dir, \'gt_binary_image\')\n        self._gt_instance_image_dir = ops.join(dataset_dir, \'gt_instance_image\')\n\n        self._train_example_index_file_path = ops.join(self._dataset_dir, \'train.txt\')\n        self._test_example_index_file_path = ops.join(self._dataset_dir, \'test.txt\')\n        self._val_example_index_file_path = ops.join(self._dataset_dir, \'val.txt\')\n\n        if not self._is_source_data_complete():\n            raise ValueError(\'Source image data is not complete, \'\n                             \'please check if one of the image folder is not exist\')\n\n        if not self._is_training_sample_index_file_complete():\n            self._generate_training_example_index_file()\n\n    def generate_tfrecords(self, save_dir, step_size=10000):\n        """"""\n        Generate tensorflow records file\n        :param save_dir:\n        :param step_size: generate a tfrecord every step_size examples\n        :return:\n        """"""\n\n        def _read_training_example_index_file(_index_file_path):\n\n            assert ops.exists(_index_file_path)\n\n            _example_gt_path_info = []\n            _example_gt_binary_path_info = []\n            _example_gt_instance_path_info = []\n\n            with open(_index_file_path, \'r\') as _file:\n                for _line in _file:\n                    _example_info = _line.rstrip(\'\\r\').rstrip(\'\\n\').split(\' \')\n                    _example_gt_path_info.append(_example_info[0])\n                    _example_gt_binary_path_info.append(_example_info[1])\n                    _example_gt_instance_path_info.append(_example_info[2])\n\n            ret = {\n                \'gt_path_info\': _example_gt_path_info,\n                \'gt_binary_path_info\': _example_gt_binary_path_info,\n                \'gt_instance_path_info\': _example_gt_instance_path_info\n            }\n\n            return ret\n\n        def _split_writing_tfrecords_task(\n                _example_gt_paths, _example_gt_binary_paths, _example_gt_instance_paths, _flags=\'train\'):\n\n            _split_example_gt_paths = []\n            _split_example_gt_binary_paths = []\n            _split_example_gt_instance_paths = []\n            _split_tfrecords_save_paths = []\n\n            for i in range(0, len(_example_gt_paths), step_size):\n                _split_example_gt_paths.append(_example_gt_paths[i:i + step_size])\n                _split_example_gt_binary_paths.append(_example_gt_binary_paths[i:i + step_size])\n                _split_example_gt_instance_paths.append(_example_gt_instance_paths[i:i + step_size])\n\n                if i + step_size > len(_example_gt_paths):\n                    _split_tfrecords_save_paths.append(\n                        ops.join(save_dir, \'{:s}_{:d}_{:d}.tfrecords\'.format(_flags, i, len(_example_gt_paths))))\n                else:\n                    _split_tfrecords_save_paths.append(\n                        ops.join(save_dir, \'{:s}_{:d}_{:d}.tfrecords\'.format(_flags, i, i + step_size)))\n\n            ret = {\n                \'gt_paths\': _split_example_gt_paths,\n                \'gt_binary_paths\': _split_example_gt_binary_paths,\n                \'gt_instance_paths\': _split_example_gt_instance_paths,\n                \'tfrecords_paths\': _split_tfrecords_save_paths\n            }\n\n            return ret\n\n        # make save dirs\n        os.makedirs(save_dir, exist_ok=True)\n\n        # start generating training example tfrecords\n        log.info(\'Start generating training example tfrecords\')\n\n        # collecting train images paths info\n        train_image_paths_info = _read_training_example_index_file(self._train_example_index_file_path)\n        train_gt_images_paths = train_image_paths_info[\'gt_path_info\']\n        train_gt_binary_images_paths = train_image_paths_info[\'gt_binary_path_info\']\n        train_gt_instance_images_paths = train_image_paths_info[\'gt_instance_path_info\']\n\n        # split training images according step size\n        train_split_result = _split_writing_tfrecords_task(\n            train_gt_images_paths, train_gt_binary_images_paths, train_gt_instance_images_paths, _flags=\'train\')\n        train_example_gt_paths = train_split_result[\'gt_paths\']\n        train_example_gt_binary_paths = train_split_result[\'gt_binary_paths\']\n        train_example_gt_instance_paths = train_split_result[\'gt_instance_paths\']\n        train_example_tfrecords_paths = train_split_result[\'tfrecords_paths\']\n\n        for index, example_gt_paths in enumerate(train_example_gt_paths):\n            tf_io_pipline_tools.write_example_tfrecords(\n                example_gt_paths,\n                train_example_gt_binary_paths[index],\n                train_example_gt_instance_paths[index],\n                train_example_tfrecords_paths[index]\n            )\n\n        log.info(\'Generating training example tfrecords complete\')\n\n        # start generating validation example tfrecords\n        log.info(\'Start generating validation example tfrecords\')\n\n        # collecting validation images paths info\n        val_image_paths_info = _read_training_example_index_file(self._val_example_index_file_path)\n        val_gt_images_paths = val_image_paths_info[\'gt_path_info\']\n        val_gt_binary_images_paths = val_image_paths_info[\'gt_binary_path_info\']\n        val_gt_instance_images_paths = val_image_paths_info[\'gt_instance_path_info\']\n\n        # split validation images according step size\n        val_split_result = _split_writing_tfrecords_task(\n            val_gt_images_paths, val_gt_binary_images_paths, val_gt_instance_images_paths, _flags=\'val\')\n        val_example_gt_paths = val_split_result[\'gt_paths\']\n        val_example_gt_binary_paths = val_split_result[\'gt_binary_paths\']\n        val_example_gt_instance_paths = val_split_result[\'gt_instance_paths\']\n        val_example_tfrecords_paths = val_split_result[\'tfrecords_paths\']\n\n        for index, example_gt_paths in enumerate(val_example_gt_paths):\n            tf_io_pipline_tools.write_example_tfrecords(\n                example_gt_paths,\n                val_example_gt_binary_paths[index],\n                val_example_gt_instance_paths[index],\n                val_example_tfrecords_paths[index]\n            )\n\n        log.info(\'Generating validation example tfrecords complete\')\n\n        # generate test example tfrecords\n        log.info(\'Start generating testing example tfrecords\')\n\n        # collecting test images paths info\n        test_image_paths_info = _read_training_example_index_file(self._test_example_index_file_path)\n        test_gt_images_paths = test_image_paths_info[\'gt_path_info\']\n        test_gt_binary_images_paths = test_image_paths_info[\'gt_binary_path_info\']\n        test_gt_instance_images_paths = test_image_paths_info[\'gt_instance_path_info\']\n\n        # split validating images according step size\n        test_split_result = _split_writing_tfrecords_task(\n            test_gt_images_paths, test_gt_binary_images_paths, test_gt_instance_images_paths, _flags=\'test\')\n        test_example_gt_paths = test_split_result[\'gt_paths\']\n        test_example_gt_binary_paths = test_split_result[\'gt_binary_paths\']\n        test_example_gt_instance_paths = test_split_result[\'gt_instance_paths\']\n        test_example_tfrecords_paths = test_split_result[\'tfrecords_paths\']\n\n        for index, example_gt_paths in enumerate(test_example_gt_paths):\n            tf_io_pipline_tools.write_example_tfrecords(\n                example_gt_paths,\n                test_example_gt_binary_paths[index],\n                test_example_gt_instance_paths[index],\n                test_example_tfrecords_paths[index]\n            )\n\n        log.info(\'Generating testing example tfrecords complete\')\n\n        return\n\n    def _is_source_data_complete(self):\n        """"""\n        Check if source data complete\n        :return:\n        """"""\n        return \\\n            ops.exists(self._gt_binary_image_dir) and \\\n            ops.exists(self._gt_instance_image_dir) and \\\n            ops.exists(self._gt_image_dir)\n\n    def _is_training_sample_index_file_complete(self):\n        """"""\n        Check if the training sample index file is complete\n        :return:\n        """"""\n        return \\\n            ops.exists(self._train_example_index_file_path) and \\\n            ops.exists(self._test_example_index_file_path) and \\\n            ops.exists(self._val_example_index_file_path)\n\n    def _generate_training_example_index_file(self):\n        """"""\n        Generate training example index file, split source file into 0.85, 0.1, 0.05 for training,\n        testing and validation. Each image folder are processed separately\n        :return:\n        """"""\n\n        def _gather_example_info():\n            """"""\n\n            :return:\n            """"""\n            _info = []\n\n            for _gt_image_path in glob.glob(\'{:s}/*.png\'.format(self._gt_image_dir)):\n                _gt_binary_image_name = ops.split(_gt_image_path)[1]\n                _gt_binary_image_path = ops.join(self._gt_binary_image_dir, _gt_binary_image_name)\n                _gt_instance_image_name = ops.split(_gt_image_path)[1]\n                _gt_instance_image_path = ops.join(self._gt_instance_image_dir, _gt_instance_image_name)\n\n                assert ops.exists(_gt_binary_image_path), \'{:s} not exist\'.format(_gt_binary_image_path)\n                assert ops.exists(_gt_instance_image_path), \'{:s} not exist\'.format(_gt_instance_image_path)\n\n                _info.append(\'{:s} {:s} {:s}\\n\'.format(\n                    _gt_image_path,\n                    _gt_binary_image_path,\n                    _gt_instance_image_path)\n                )\n\n            return _info\n\n        def _split_training_examples(_example_info):\n            random.shuffle(_example_info)\n\n            _example_nums = len(_example_info)\n\n            _train_example_info = _example_info[:int(_example_nums * 0.85)]\n            _val_example_info = _example_info[int(_example_nums * 0.85):int(_example_nums * 0.9)]\n            _test_example_info = _example_info[int(_example_nums * 0.9):]\n\n            return _train_example_info, _test_example_info, _val_example_info\n\n        train_example_info, test_example_info, val_example_info = _split_training_examples(_gather_example_info())\n\n        random.shuffle(train_example_info)\n        random.shuffle(test_example_info)\n        random.shuffle(val_example_info)\n\n        with open(ops.join(self._dataset_dir, \'train.txt\'), \'w\') as file:\n            file.write(\'\'.join(train_example_info))\n\n        with open(ops.join(self._dataset_dir, \'test.txt\'), \'w\') as file:\n            file.write(\'\'.join(test_example_info))\n\n        with open(ops.join(self._dataset_dir, \'val.txt\'), \'w\') as file:\n            file.write(\'\'.join(val_example_info))\n\n        log.info(\'Generating training example index file complete\')\n\n        return\n\n\nclass LaneNetDataFeeder(object):\n    """"""\n    Read training examples from tfrecords for nsfw model\n    """"""\n\n    def __init__(self, dataset_dir, flags=\'train\'):\n        """"""\n\n        :param dataset_dir:\n        :param flags:\n        """"""\n        self._dataset_dir = dataset_dir\n\n        self._tfrecords_dir = ops.join(dataset_dir, \'tfrecords\')\n        if not ops.exists(self._tfrecords_dir):\n            raise ValueError(\'{:s} not exist, please check again\'.format(self._tfrecords_dir))\n\n        self._dataset_flags = flags.lower()\n        if self._dataset_flags not in [\'train\', \'test\', \'val\']:\n            raise ValueError(\'flags of the data feeder should be \\\'train\\\', \\\'test\\\', \\\'val\\\'\')\n\n    def inputs(self, batch_size, num_epochs):\n        """"""\n        dataset feed pipline input\n        :param batch_size:\n        :param num_epochs:\n        :return: A tuple (images, labels), where:\n                    * images is a float tensor with shape [batch_size, H, W, C]\n                      in the range [-0.5, 0.5].\n                    * labels is an int32 tensor with shape [batch_size] with the true label,\n                      a number in the range [0, CLASS_NUMS).\n        """"""\n        if not num_epochs:\n            num_epochs = None\n\n        tfrecords_file_paths = glob.glob(\'{:s}/{:s}*.tfrecords\'.format(\n            self._tfrecords_dir, self._dataset_flags)\n        )\n        random.shuffle(tfrecords_file_paths)\n\n        with tf.name_scope(\'input_tensor\'):\n\n            # TFRecordDataset opens a binary file and reads one record at a time.\n            # `tfrecords_file_paths` could also be a list of filenames, which will be read in order.\n            dataset = tf.data.TFRecordDataset(tfrecords_file_paths)\n\n            # The map transformation takes a function and applies it to every element\n            # of the dataset.\n            dataset = dataset.map(map_func=tf_io_pipline_tools.decode,\n                                  num_parallel_calls=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS)\n            if self._dataset_flags != \'test\':\n                dataset = dataset.map(map_func=tf_io_pipline_tools.augment_for_train,\n                                      num_parallel_calls=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS)\n            else:\n                dataset = dataset.map(map_func=tf_io_pipline_tools.augment_for_test,\n                                      num_parallel_calls=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS)\n            dataset = dataset.map(map_func=tf_io_pipline_tools.normalize,\n                                  num_parallel_calls=CFG.TRAIN.CPU_MULTI_PROCESS_NUMS)\n\n            # The shuffle transformation uses a finite-sized buffer to shuffle elements\n            # in memory. The parameter is the number of elements in the buffer. For\n            # completely uniform shuffling, set the parameter to be the same as the\n            # number of elements in the dataset.\n            if self._dataset_flags != \'test\':\n                dataset = dataset.shuffle(buffer_size=1000)\n                # repeat num epochs\n                dataset = dataset.repeat()\n\n            dataset = dataset.batch(batch_size, drop_remainder=True)\n\n            iterator = dataset.make_one_shot_iterator()\n\n        return iterator.get_next(name=\'{:s}_IteratorGetNext\'.format(self._dataset_flags))\n\n\nif __name__ == \'__main__\':\n    # init args\n    args = init_args()\n\n    assert ops.exists(args.dataset_dir), \'{:s} not exist\'.format(args.dataset_dir)\n\n    producer = LaneNetDataProducer(dataset_dir=args.dataset_dir)\n    producer.generate_tfrecords(save_dir=args.tfrecords_dir, step_size=1000)\n'"
data_provider/tf_io_pipline_tools.py,42,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-23 \xe4\xb8\x8b\xe5\x8d\x883:53\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : tf_io_pipline_tools.py\n# @IDE: PyCharm\n""""""\ntensorflow io pip line tools\n""""""\nimport os\nimport os.path as ops\n\nimport cv2\nimport glog as log\nimport numpy as np\nimport tensorflow as tf\n\nfrom config import global_config\n\nCFG = global_config.cfg\n\nRESIZE_IMAGE_HEIGHT = CFG.TRAIN.IMG_HEIGHT + CFG.TRAIN.CROP_PAD_SIZE\nRESIZE_IMAGE_WIDTH = CFG.TRAIN.IMG_WIDTH + CFG.TRAIN.CROP_PAD_SIZE\nCROP_IMAGE_HEIGHT = CFG.TRAIN.IMG_HEIGHT\nCROP_IMAGE_WIDTH = CFG.TRAIN.IMG_WIDTH\n\n\ndef int64_feature(value):\n    """"""\n\n    :return:\n    """"""\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef bytes_feature(value):\n    """"""\n\n    :param value:\n    :return:\n    """"""\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef write_example_tfrecords(gt_images_paths, gt_binary_images_paths, gt_instance_images_paths, tfrecords_path):\n    """"""\n    write tfrecords\n    :param gt_images_paths:\n    :param gt_binary_images_paths:\n    :param gt_instance_images_paths:\n    :param tfrecords_path:\n    :return:\n    """"""\n    _tfrecords_dir = ops.split(tfrecords_path)[0]\n    os.makedirs(_tfrecords_dir, exist_ok=True)\n\n    log.info(\'Writing {:s}....\'.format(tfrecords_path))\n\n    with tf.python_io.TFRecordWriter(tfrecords_path) as _writer:\n        for _index, _gt_image_path in enumerate(gt_images_paths):\n\n            # prepare gt image\n            _gt_image = cv2.imread(_gt_image_path, cv2.IMREAD_UNCHANGED)\n            if _gt_image.shape != (RESIZE_IMAGE_WIDTH, RESIZE_IMAGE_HEIGHT, 3):\n                _gt_image = cv2.resize(\n                    _gt_image,\n                    dsize=(RESIZE_IMAGE_WIDTH, RESIZE_IMAGE_HEIGHT),\n                    interpolation=cv2.INTER_LINEAR\n                )\n            _gt_image_raw = _gt_image.tostring()\n\n            # prepare gt binary image\n            _gt_binary_image = cv2.imread(gt_binary_images_paths[_index], cv2.IMREAD_UNCHANGED)\n            if _gt_binary_image.shape != (RESIZE_IMAGE_WIDTH, RESIZE_IMAGE_HEIGHT):\n                _gt_binary_image = cv2.resize(\n                    _gt_binary_image,\n                    dsize=(RESIZE_IMAGE_WIDTH, RESIZE_IMAGE_HEIGHT),\n                    interpolation=cv2.INTER_NEAREST\n                )\n                _gt_binary_image = np.array(_gt_binary_image / 255.0, dtype=np.uint8)\n            _gt_binary_image_raw = _gt_binary_image.tostring()\n\n            # prepare gt instance image\n            _gt_instance_image = cv2.imread(gt_instance_images_paths[_index], cv2.IMREAD_UNCHANGED)\n            if _gt_instance_image.shape != (RESIZE_IMAGE_WIDTH, RESIZE_IMAGE_HEIGHT):\n                _gt_instance_image = cv2.resize(\n                    _gt_instance_image,\n                    dsize=(RESIZE_IMAGE_WIDTH, RESIZE_IMAGE_HEIGHT),\n                    interpolation=cv2.INTER_NEAREST\n                )\n            _gt_instance_image_raw = _gt_instance_image.tostring()\n\n            _example = tf.train.Example(\n                features=tf.train.Features(\n                    feature={\n                        \'gt_image_raw\': bytes_feature(_gt_image_raw),\n                        \'gt_binary_image_raw\': bytes_feature(_gt_binary_image_raw),\n                        \'gt_instance_image_raw\': bytes_feature(_gt_instance_image_raw)\n                    }))\n            _writer.write(_example.SerializeToString())\n\n    log.info(\'Writing {:s} complete\'.format(tfrecords_path))\n\n    return\n\n\ndef decode(serialized_example):\n    """"""\n    Parses an image and label from the given `serialized_example`\n    :param serialized_example:\n    :return:\n    """"""\n    features = tf.parse_single_example(\n        serialized_example,\n        # Defaults are not specified since both keys are required.\n        features={\n            \'gt_image_raw\': tf.FixedLenFeature([], tf.string),\n            \'gt_binary_image_raw\': tf.FixedLenFeature([], tf.string),\n            \'gt_instance_image_raw\': tf.FixedLenFeature([], tf.string)\n        })\n\n    # decode gt image\n    gt_image_shape = tf.stack([RESIZE_IMAGE_HEIGHT, RESIZE_IMAGE_WIDTH, 3])\n    gt_image = tf.decode_raw(features[\'gt_image_raw\'], tf.uint8)\n    gt_image = tf.reshape(gt_image, gt_image_shape)\n\n    # decode gt binary image\n    gt_binary_image_shape = tf.stack([RESIZE_IMAGE_HEIGHT, RESIZE_IMAGE_WIDTH, 1])\n    gt_binary_image = tf.decode_raw(features[\'gt_binary_image_raw\'], tf.uint8)\n    gt_binary_image = tf.reshape(gt_binary_image, gt_binary_image_shape)\n\n    # decode gt instance image\n    gt_instance_image_shape = tf.stack([RESIZE_IMAGE_HEIGHT, RESIZE_IMAGE_WIDTH, 1])\n    gt_instance_image = tf.decode_raw(features[\'gt_instance_image_raw\'], tf.uint8)\n    gt_instance_image = tf.reshape(gt_instance_image, gt_instance_image_shape)\n\n    return gt_image, gt_binary_image, gt_instance_image\n\n\ndef central_crop(image, crop_height, crop_width):\n    """"""\n    Performs central crops of the given image\n    :param image:\n    :param crop_height:\n    :param crop_width:\n    :return:\n    """"""\n    shape = tf.shape(input=image)\n    height, width = shape[0], shape[1]\n\n    amount_to_be_cropped_h = (height - crop_height)\n    crop_top = amount_to_be_cropped_h // 2\n    amount_to_be_cropped_w = (width - crop_width)\n    crop_left = amount_to_be_cropped_w // 2\n\n    return tf.slice(image, [crop_top, crop_left, 0], [crop_height, crop_width, -1])\n\n\ndef augment_for_train(gt_image, gt_binary_image, gt_instance_image):\n    """"""\n\n    :param gt_image:\n    :param gt_binary_image:\n    :param gt_instance_image:\n    :return:\n    """"""\n    # convert image from uint8 to float32\n    gt_image = tf.cast(gt_image, tf.float32)\n    gt_binary_image = tf.cast(gt_binary_image, tf.float32)\n    gt_instance_image = tf.cast(gt_instance_image, tf.float32)\n\n    # apply random color augmentation\n    gt_image, gt_binary_image, gt_instance_image = random_color_augmentation(\n        gt_image, gt_binary_image, gt_instance_image\n    )\n\n    # apply random flip augmentation\n    gt_image, gt_binary_image, gt_instance_image = random_horizon_flip_batch_images(\n        gt_image, gt_binary_image, gt_instance_image\n    )\n\n    # apply random crop image\n    return random_crop_batch_images(\n        gt_image=gt_image,\n        gt_binary_image=gt_binary_image,\n        gt_instance_image=gt_instance_image,\n        cropped_size=[CROP_IMAGE_WIDTH, CROP_IMAGE_HEIGHT]\n    )\n\n\ndef augment_for_test(gt_image, gt_binary_image, gt_instance_image):\n    """"""\n\n    :param gt_image:\n    :param gt_binary_image:\n    :param gt_instance_image:\n    :return:\n    """"""\n    # apply central crop\n    gt_image = central_crop(\n        image=gt_image, crop_height=CROP_IMAGE_HEIGHT, crop_width=CROP_IMAGE_WIDTH\n    )\n    gt_binary_image = central_crop(\n        image=gt_binary_image, crop_height=CROP_IMAGE_HEIGHT, crop_width=CROP_IMAGE_WIDTH\n    )\n    gt_instance_image = central_crop(\n        image=gt_instance_image, crop_height=CROP_IMAGE_HEIGHT, crop_width=CROP_IMAGE_WIDTH\n    )\n\n    return gt_image, gt_binary_image, gt_instance_image\n\n\ndef normalize(gt_image, gt_binary_image, gt_instance_image):\n    """"""\n    Normalize the image data by substracting the imagenet mean value\n    :param gt_image:\n    :param gt_binary_image:\n    :param gt_instance_image:\n    :return:\n    """"""\n\n    if gt_image.get_shape().as_list()[-1] != 3 \\\n            or gt_binary_image.get_shape().as_list()[-1] != 1 \\\n            or gt_instance_image.get_shape().as_list()[-1] != 1:\n        log.error(gt_image.get_shape())\n        log.error(gt_binary_image.get_shape())\n        log.error(gt_instance_image.get_shape())\n        raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n    gt_image = tf.subtract(tf.divide(gt_image, tf.constant(127.5, dtype=tf.float32)),\n                           tf.constant(1.0, dtype=tf.float32))\n\n    return gt_image, gt_binary_image, gt_instance_image\n\n\ndef random_crop_batch_images(gt_image, gt_binary_image, gt_instance_image, cropped_size):\n    """"""\n    Random crop image batch data for training\n    :param gt_image:\n    :param gt_binary_image:\n    :param gt_instance_image:\n    :param cropped_size:\n    :return:\n    """"""\n    concat_images = tf.concat([gt_image, gt_binary_image, gt_instance_image], axis=-1)\n\n    concat_cropped_images = tf.image.random_crop(\n        concat_images,\n        [cropped_size[1], cropped_size[0], tf.shape(concat_images)[-1]],\n        seed=tf.random.set_random_seed(1234)\n    )\n\n    cropped_gt_image = tf.slice(\n        concat_cropped_images,\n        begin=[0, 0, 0],\n        size=[cropped_size[1], cropped_size[0], 3]\n    )\n    cropped_gt_binary_image = tf.slice(\n        concat_cropped_images,\n        begin=[0, 0, 3],\n        size=[cropped_size[1], cropped_size[0], 1]\n    )\n    cropped_gt_instance_image = tf.slice(\n        concat_cropped_images,\n        begin=[0, 0, 4],\n        size=[cropped_size[1], cropped_size[0], 1]\n    )\n\n    return cropped_gt_image, cropped_gt_binary_image, cropped_gt_instance_image\n\n\ndef random_horizon_flip_batch_images(gt_image, gt_binary_image, gt_instance_image):\n    """"""\n    Random horizon flip image batch data for training\n    :param gt_image:\n    :param gt_binary_image:\n    :param gt_instance_image:\n    :return:\n    """"""\n    concat_images = tf.concat([gt_image, gt_binary_image, gt_instance_image], axis=-1)\n\n    [image_height, image_width, _] = gt_image.get_shape().as_list()\n\n    concat_flipped_images = tf.image.random_flip_left_right(\n        image=concat_images,\n        seed=tf.random.set_random_seed(1)\n    )\n\n    flipped_gt_image = tf.slice(\n        concat_flipped_images,\n        begin=[0, 0, 0],\n        size=[image_height, image_width, 3]\n    )\n    flipped_gt_binary_image = tf.slice(\n        concat_flipped_images,\n        begin=[0, 0, 3],\n        size=[image_height, image_width, 1]\n    )\n    flipped_gt_instance_image = tf.slice(\n        concat_flipped_images,\n        begin=[0, 0, 4],\n        size=[image_height, image_width, 1]\n    )\n\n    return flipped_gt_image, flipped_gt_binary_image, flipped_gt_instance_image\n\n\ndef random_color_augmentation(gt_image, gt_binary_image, gt_instance_image):\n    """"""\n    andom color augmentation\n    :param gt_image:\n    :param gt_binary_image:\n    :param gt_instance_image:\n    :return:\n    """"""\n    # first apply random saturation augmentation\n    gt_image = tf.image.random_saturation(gt_image, 0.8, 1.2)\n    # sencond apply random brightness augmentation\n    gt_image = tf.image.random_brightness(gt_image, 0.05)\n    # third apply random contrast augmentation\n    gt_image = tf.image.random_contrast(gt_image, 0.7, 1.3)\n\n    gt_image = tf.clip_by_value(gt_image, 0.0, 255.0)\n\n    return gt_image, gt_binary_image, gt_instance_image\n'"
lanenet_model/lanenet.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-24 \xe4\xb8\x8b\xe5\x8d\x888:50\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : lanenet.py\n# @IDE: PyCharm\n""""""\nImplement LaneNet Model\n""""""\nimport tensorflow as tf\n\nfrom config import global_config\nfrom lanenet_model import lanenet_back_end\nfrom lanenet_model import lanenet_front_end\nfrom semantic_segmentation_zoo import cnn_basenet\n\nCFG = global_config.cfg\n\n\nclass LaneNet(cnn_basenet.CNNBaseModel):\n    """"""\n\n    """"""\n    def __init__(self, phase, net_flag=\'vgg\', reuse=False):\n        """"""\n\n        """"""\n        super(LaneNet, self).__init__()\n        self._net_flag = net_flag\n        self._reuse = reuse\n\n        self._frontend = lanenet_front_end.LaneNetFrondEnd(\n            phase=phase, net_flag=net_flag\n        )\n        self._backend = lanenet_back_end.LaneNetBackEnd(\n            phase=phase\n        )\n\n    def inference(self, input_tensor, name):\n        """"""\n\n        :param input_tensor:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name, reuse=self._reuse):\n            # first extract image features\n            extract_feats_result = self._frontend.build_model(\n                input_tensor=input_tensor,\n                name=\'{:s}_frontend\'.format(self._net_flag),\n                reuse=self._reuse\n            )\n\n            # second apply backend process\n            binary_seg_prediction, instance_seg_prediction = self._backend.inference(\n                binary_seg_logits=extract_feats_result[\'binary_segment_logits\'][\'data\'],\n                instance_seg_logits=extract_feats_result[\'instance_segment_logits\'][\'data\'],\n                name=\'{:s}_backend\'.format(self._net_flag),\n                reuse=self._reuse\n            )\n\n            if not self._reuse:\n                self._reuse = True\n\n        return binary_seg_prediction, instance_seg_prediction\n\n    def compute_loss(self, input_tensor, binary_label, instance_label, name):\n        """"""\n        calculate lanenet loss for training\n        :param input_tensor:\n        :param binary_label:\n        :param instance_label:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name, reuse=self._reuse):\n            # first extract image features\n            extract_feats_result = self._frontend.build_model(\n                input_tensor=input_tensor,\n                name=\'{:s}_frontend\'.format(self._net_flag),\n                reuse=self._reuse\n            )\n\n            # second apply backend process\n            calculated_losses = self._backend.compute_loss(\n                binary_seg_logits=extract_feats_result[\'binary_segment_logits\'][\'data\'],\n                binary_label=binary_label,\n                instance_seg_logits=extract_feats_result[\'instance_segment_logits\'][\'data\'],\n                instance_label=instance_label,\n                name=\'{:s}_backend\'.format(self._net_flag),\n                reuse=self._reuse\n            )\n\n            if not self._reuse:\n                self._reuse = True\n\n        return calculated_losses\n'"
lanenet_model/lanenet_back_end.py,24,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-24 \xe4\xb8\x8b\xe5\x8d\x883:54\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : lanenet_back_end.py\n# @IDE: PyCharm\n""""""\nLaneNet backend branch which is mainly used for binary and instance segmentation loss calculation\n""""""\nimport tensorflow as tf\n\nfrom config import global_config\nfrom lanenet_model import lanenet_discriminative_loss\nfrom semantic_segmentation_zoo import cnn_basenet\n\nCFG = global_config.cfg\n\n\nclass LaneNetBackEnd(cnn_basenet.CNNBaseModel):\n    """"""\n    LaneNet backend branch which is mainly used for binary and instance segmentation loss calculation\n    """"""\n    def __init__(self, phase):\n        """"""\n        init lanenet backend\n        :param phase: train or test\n        """"""\n        super(LaneNetBackEnd, self).__init__()\n        self._phase = phase\n        self._is_training = self._is_net_for_training()\n\n    def _is_net_for_training(self):\n        """"""\n        if the net is used for training or not\n        :return:\n        """"""\n        if isinstance(self._phase, tf.Tensor):\n            phase = self._phase\n        else:\n            phase = tf.constant(self._phase, dtype=tf.string)\n\n        return tf.equal(phase, tf.constant(\'train\', dtype=tf.string))\n\n    @classmethod\n    def _compute_class_weighted_cross_entropy_loss(cls, onehot_labels, logits, classes_weights):\n        """"""\n\n        :param onehot_labels:\n        :param logits:\n        :param classes_weights:\n        :return:\n        """"""\n        loss_weights = tf.reduce_sum(tf.multiply(onehot_labels, classes_weights), axis=3)\n\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=onehot_labels,\n            logits=logits,\n            weights=loss_weights\n        )\n\n        return loss\n\n    def compute_loss(self, binary_seg_logits, binary_label,\n                     instance_seg_logits, instance_label,\n                     name, reuse):\n        """"""\n        compute lanenet loss\n        :param binary_seg_logits:\n        :param binary_label:\n        :param instance_seg_logits:\n        :param instance_label:\n        :param name:\n        :param reuse:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name, reuse=reuse):\n            # calculate class weighted binary seg loss\n            with tf.variable_scope(name_or_scope=\'binary_seg\'):\n                binary_label_onehot = tf.one_hot(\n                    tf.reshape(\n                        tf.cast(binary_label, tf.int32),\n                        shape=[binary_label.get_shape().as_list()[0],\n                               binary_label.get_shape().as_list()[1],\n                               binary_label.get_shape().as_list()[2]]),\n                    depth=CFG.TRAIN.CLASSES_NUMS,\n                    axis=-1\n                )\n\n                binary_label_plain = tf.reshape(\n                    binary_label,\n                    shape=[binary_label.get_shape().as_list()[0] *\n                           binary_label.get_shape().as_list()[1] *\n                           binary_label.get_shape().as_list()[2] *\n                           binary_label.get_shape().as_list()[3]])\n                unique_labels, unique_id, counts = tf.unique_with_counts(binary_label_plain)\n                counts = tf.cast(counts, tf.float32)\n                inverse_weights = tf.divide(\n                    1.0,\n                    tf.log(tf.add(tf.divide(counts, tf.reduce_sum(counts)), tf.constant(1.02)))\n                )\n\n                binary_segmenatation_loss = self._compute_class_weighted_cross_entropy_loss(\n                    onehot_labels=binary_label_onehot,\n                    logits=binary_seg_logits,\n                    classes_weights=inverse_weights\n                )\n\n            # calculate class weighted instance seg loss\n            with tf.variable_scope(name_or_scope=\'instance_seg\'):\n\n                pix_bn = self.layerbn(\n                    inputdata=instance_seg_logits, is_training=self._is_training, name=\'pix_bn\')\n                pix_relu = self.relu(inputdata=pix_bn, name=\'pix_relu\')\n                pix_embedding = self.conv2d(\n                    inputdata=pix_relu,\n                    out_channel=CFG.TRAIN.EMBEDDING_FEATS_DIMS,\n                    kernel_size=1,\n                    use_bias=False,\n                    name=\'pix_embedding_conv\'\n                )\n                pix_image_shape = (pix_embedding.get_shape().as_list()[1], pix_embedding.get_shape().as_list()[2])\n                instance_segmentation_loss, l_var, l_dist, l_reg = \\\n                    lanenet_discriminative_loss.discriminative_loss(\n                        pix_embedding, instance_label, CFG.TRAIN.EMBEDDING_FEATS_DIMS,\n                        pix_image_shape, 0.5, 3.0, 1.0, 1.0, 0.001\n                    )\n\n            l2_reg_loss = tf.constant(0.0, tf.float32)\n            for vv in tf.trainable_variables():\n                if \'bn\' in vv.name or \'gn\' in vv.name:\n                    continue\n                else:\n                    l2_reg_loss = tf.add(l2_reg_loss, tf.nn.l2_loss(vv))\n            l2_reg_loss *= 0.001\n            total_loss = binary_segmenatation_loss + instance_segmentation_loss + l2_reg_loss\n\n            ret = {\n                \'total_loss\': total_loss,\n                \'binary_seg_logits\': binary_seg_logits,\n                \'instance_seg_logits\': pix_embedding,\n                \'binary_seg_loss\': binary_segmenatation_loss,\n                \'discriminative_loss\': instance_segmentation_loss\n            }\n\n        return ret\n\n    def inference(self, binary_seg_logits, instance_seg_logits, name, reuse):\n        """"""\n\n        :param binary_seg_logits:\n        :param instance_seg_logits:\n        :param name:\n        :param reuse:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name, reuse=reuse):\n\n            with tf.variable_scope(name_or_scope=\'binary_seg\'):\n                binary_seg_score = tf.nn.softmax(logits=binary_seg_logits)\n                binary_seg_prediction = tf.argmax(binary_seg_score, axis=-1)\n\n            with tf.variable_scope(name_or_scope=\'instance_seg\'):\n\n                pix_bn = self.layerbn(\n                    inputdata=instance_seg_logits, is_training=self._is_training, name=\'pix_bn\')\n                pix_relu = self.relu(inputdata=pix_bn, name=\'pix_relu\')\n                instance_seg_prediction = self.conv2d(\n                    inputdata=pix_relu,\n                    out_channel=CFG.TRAIN.EMBEDDING_FEATS_DIMS,\n                    kernel_size=1,\n                    use_bias=False,\n                    name=\'pix_embedding_conv\'\n                )\n\n        return binary_seg_prediction, instance_seg_prediction\n'"
lanenet_model/lanenet_discriminative_loss.py,44,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-11 \xe4\xb8\x8b\xe5\x8d\x883:48\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : lanenet_discriminative_loss.py\n# @IDE: PyCharm Community Edition\n""""""\nDiscriminative Loss for instance segmentation\n""""""\nimport tensorflow as tf\n\n\ndef discriminative_loss_single(\n        prediction,\n        correct_label,\n        feature_dim,\n        label_shape,\n        delta_v,\n        delta_d,\n        param_var,\n        param_dist,\n        param_reg):\n    """"""\n    discriminative loss\n    :param prediction: inference of network\n    :param correct_label: instance label\n    :param feature_dim: feature dimension of prediction\n    :param label_shape: shape of label\n    :param delta_v: cut off variance distance\n    :param delta_d: cut off cluster distance\n    :param param_var: weight for intra cluster variance\n    :param param_dist: weight for inter cluster distances\n    :param param_reg: weight regularization\n    """"""\n    correct_label = tf.reshape(\n        correct_label, [label_shape[1] * label_shape[0]]\n    )\n    reshaped_pred = tf.reshape(\n        prediction, [label_shape[1] * label_shape[0], feature_dim]\n    )\n\n    # calculate instance nums\n    unique_labels, unique_id, counts = tf.unique_with_counts(correct_label)\n    counts = tf.cast(counts, tf.float32)\n    num_instances = tf.size(unique_labels)\n\n    # calculate instance pixel embedding mean vec\n    segmented_sum = tf.unsorted_segment_sum(\n        reshaped_pred, unique_id, num_instances)\n    mu = tf.div(segmented_sum, tf.reshape(counts, (-1, 1)))\n    mu_expand = tf.gather(mu, unique_id)\n\n    distance = tf.norm(tf.subtract(mu_expand, reshaped_pred), axis=1, ord=1)\n    distance = tf.subtract(distance, delta_v)\n    distance = tf.clip_by_value(distance, 0., distance)\n    distance = tf.square(distance)\n\n    l_var = tf.unsorted_segment_sum(distance, unique_id, num_instances)\n    l_var = tf.div(l_var, counts)\n    l_var = tf.reduce_sum(l_var)\n    l_var = tf.divide(l_var, tf.cast(num_instances, tf.float32))\n\n    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])\n    mu_band_rep = tf.tile(mu, [1, num_instances])\n    mu_band_rep = tf.reshape(\n        mu_band_rep,\n        (num_instances *\n         num_instances,\n         feature_dim))\n\n    mu_diff = tf.subtract(mu_band_rep, mu_interleaved_rep)\n\n    intermediate_tensor = tf.reduce_sum(tf.abs(mu_diff), axis=1)\n    zero_vector = tf.zeros(1, dtype=tf.float32)\n    bool_mask = tf.not_equal(intermediate_tensor, zero_vector)\n    mu_diff_bool = tf.boolean_mask(mu_diff, bool_mask)\n\n    mu_norm = tf.norm(mu_diff_bool, axis=1, ord=1)\n    mu_norm = tf.subtract(2. * delta_d, mu_norm)\n    mu_norm = tf.clip_by_value(mu_norm, 0., mu_norm)\n    mu_norm = tf.square(mu_norm)\n\n    l_dist = tf.reduce_mean(mu_norm)\n\n    l_reg = tf.reduce_mean(tf.norm(mu, axis=1, ord=1))\n\n    param_scale = 1.\n    l_var = param_var * l_var\n    l_dist = param_dist * l_dist\n    l_reg = param_reg * l_reg\n\n    loss = param_scale * (l_var + l_dist + l_reg)\n\n    return loss, l_var, l_dist, l_reg\n\n\ndef discriminative_loss(prediction, correct_label, feature_dim, image_shape,\n                        delta_v, delta_d, param_var, param_dist, param_reg):\n    """"""\n\n    :return: discriminative loss and its three components\n    """"""\n\n    def cond(label, batch, out_loss, out_var, out_dist, out_reg, i):\n        return tf.less(i, tf.shape(batch)[0])\n\n    def body(label, batch, out_loss, out_var, out_dist, out_reg, i):\n        disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(\n            prediction[i], correct_label[i], feature_dim, image_shape, delta_v, delta_d, param_var, param_dist, param_reg)\n\n        out_loss = out_loss.write(i, disc_loss)\n        out_var = out_var.write(i, l_var)\n        out_dist = out_dist.write(i, l_dist)\n        out_reg = out_reg.write(i, l_reg)\n\n        return label, batch, out_loss, out_var, out_dist, out_reg, i + 1\n\n    # TensorArray is a data structure that support dynamic writing\n    output_ta_loss = tf.TensorArray(\n        dtype=tf.float32, size=0, dynamic_size=True)\n    output_ta_var = tf.TensorArray(\n        dtype=tf.float32, size=0, dynamic_size=True)\n    output_ta_dist = tf.TensorArray(\n        dtype=tf.float32, size=0, dynamic_size=True)\n    output_ta_reg = tf.TensorArray(\n        dtype=tf.float32, size=0, dynamic_size=True)\n\n    _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _ = tf.while_loop(\n        cond, body, [\n            correct_label, prediction, output_ta_loss, output_ta_var, output_ta_dist, output_ta_reg, 0])\n    out_loss_op = out_loss_op.stack()\n    out_var_op = out_var_op.stack()\n    out_dist_op = out_dist_op.stack()\n    out_reg_op = out_reg_op.stack()\n\n    disc_loss = tf.reduce_mean(out_loss_op)\n    l_var = tf.reduce_mean(out_var_op)\n    l_dist = tf.reduce_mean(out_dist_op)\n    l_reg = tf.reduce_mean(out_reg_op)\n\n    return disc_loss, l_var, l_dist, l_reg\n'"
lanenet_model/lanenet_front_end.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-24 \xe4\xb8\x8b\xe5\x8d\x883:53\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : lanenet_front_end.py\n# @IDE: PyCharm\n""""""\nLaneNet frontend branch which is mainly used for feature extraction\n""""""\nfrom semantic_segmentation_zoo import cnn_basenet\nfrom semantic_segmentation_zoo import vgg16_based_fcn\n\n\nclass LaneNetFrondEnd(cnn_basenet.CNNBaseModel):\n    """"""\n    LaneNet frontend which is used to extract image features for following process\n    """"""\n    def __init__(self, phase, net_flag):\n        """"""\n\n        """"""\n        super(LaneNetFrondEnd, self).__init__()\n\n        self._frontend_net_map = {\n            \'vgg\': vgg16_based_fcn.VGG16FCN(phase=phase)\n        }\n\n        self._net = self._frontend_net_map[net_flag]\n\n    def build_model(self, input_tensor, name, reuse):\n        """"""\n\n        :param input_tensor:\n        :param name:\n        :param reuse:\n        :return:\n        """"""\n\n        return self._net.build_model(\n            input_tensor=input_tensor,\n            name=name,\n            reuse=reuse\n        )\n'"
lanenet_model/lanenet_postprocess.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-30 \xe4\xb8\x8a\xe5\x8d\x8810:04\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : lanenet_postprocess.py\n# @IDE: PyCharm Community Edition\n""""""\nLaneNet model post process\n""""""\nimport os.path as ops\nimport math\n\nimport cv2\nimport glog as log\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\nfrom config import global_config\n\nCFG = global_config.cfg\n\n\ndef _morphological_process(image, kernel_size=5):\n    """"""\n    morphological process to fill the hole in the binary segmentation result\n    :param image:\n    :param kernel_size:\n    :return:\n    """"""\n    if len(image.shape) == 3:\n        raise ValueError(\'Binary segmentation result image should be a single channel image\')\n\n    if image.dtype is not np.uint8:\n        image = np.array(image, np.uint8)\n\n    kernel = cv2.getStructuringElement(shape=cv2.MORPH_ELLIPSE, ksize=(kernel_size, kernel_size))\n\n    # close operation fille hole\n    closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel, iterations=1)\n\n    return closing\n\n\ndef _connect_components_analysis(image):\n    """"""\n    connect components analysis to remove the small components\n    :param image:\n    :return:\n    """"""\n    if len(image.shape) == 3:\n        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        gray_image = image\n\n    return cv2.connectedComponentsWithStats(gray_image, connectivity=8, ltype=cv2.CV_32S)\n\n\nclass _LaneFeat(object):\n    """"""\n\n    """"""\n    def __init__(self, feat, coord, class_id=-1):\n        """"""\n        lane feat object\n        :param feat: lane embeddng feats [feature_1, feature_2, ...]\n        :param coord: lane coordinates [x, y]\n        :param class_id: lane class id\n        """"""\n        self._feat = feat\n        self._coord = coord\n        self._class_id = class_id\n\n    @property\n    def feat(self):\n        """"""\n\n        :return:\n        """"""\n        return self._feat\n\n    @feat.setter\n    def feat(self, value):\n        """"""\n\n        :param value:\n        :return:\n        """"""\n        if not isinstance(value, np.ndarray):\n            value = np.array(value, dtype=np.float64)\n\n        if value.dtype != np.float32:\n            value = np.array(value, dtype=np.float64)\n\n        self._feat = value\n\n    @property\n    def coord(self):\n        """"""\n\n        :return:\n        """"""\n        return self._coord\n\n    @coord.setter\n    def coord(self, value):\n        """"""\n\n        :param value:\n        :return:\n        """"""\n        if not isinstance(value, np.ndarray):\n            value = np.array(value)\n\n        if value.dtype != np.int32:\n            value = np.array(value, dtype=np.int32)\n\n        self._coord = value\n\n    @property\n    def class_id(self):\n        """"""\n\n        :return:\n        """"""\n        return self._class_id\n\n    @class_id.setter\n    def class_id(self, value):\n        """"""\n\n        :param value:\n        :return:\n        """"""\n        if not isinstance(value, np.int64):\n            raise ValueError(\'Class id must be integer\')\n\n        self._class_id = value\n\n\nclass _LaneNetCluster(object):\n    """"""\n     Instance segmentation result cluster\n    """"""\n\n    def __init__(self):\n        """"""\n\n        """"""\n        self._color_map = [np.array([255, 0, 0]),\n                           np.array([0, 255, 0]),\n                           np.array([0, 0, 255]),\n                           np.array([125, 125, 0]),\n                           np.array([0, 125, 125]),\n                           np.array([125, 0, 125]),\n                           np.array([50, 100, 50]),\n                           np.array([100, 50, 100])]\n\n    @staticmethod\n    def _embedding_feats_dbscan_cluster(embedding_image_feats):\n        """"""\n        dbscan cluster\n        :param embedding_image_feats:\n        :return:\n        """"""\n        db = DBSCAN(eps=CFG.POSTPROCESS.DBSCAN_EPS, min_samples=CFG.POSTPROCESS.DBSCAN_MIN_SAMPLES)\n        try:\n            features = StandardScaler().fit_transform(embedding_image_feats)\n            db.fit(features)\n        except Exception as err:\n            log.error(err)\n            ret = {\n                \'origin_features\': None,\n                \'cluster_nums\': 0,\n                \'db_labels\': None,\n                \'unique_labels\': None,\n                \'cluster_center\': None\n            }\n            return ret\n        db_labels = db.labels_\n        unique_labels = np.unique(db_labels)\n\n        num_clusters = len(unique_labels)\n        cluster_centers = db.components_\n\n        ret = {\n            \'origin_features\': features,\n            \'cluster_nums\': num_clusters,\n            \'db_labels\': db_labels,\n            \'unique_labels\': unique_labels,\n            \'cluster_center\': cluster_centers\n        }\n\n        return ret\n\n    @staticmethod\n    def _get_lane_embedding_feats(binary_seg_ret, instance_seg_ret):\n        """"""\n        get lane embedding features according the binary seg result\n        :param binary_seg_ret:\n        :param instance_seg_ret:\n        :return:\n        """"""\n        idx = np.where(binary_seg_ret == 255)\n        lane_embedding_feats = instance_seg_ret[idx]\n        # idx_scale = np.vstack((idx[0] / 256.0, idx[1] / 512.0)).transpose()\n        # lane_embedding_feats = np.hstack((lane_embedding_feats, idx_scale))\n        lane_coordinate = np.vstack((idx[1], idx[0])).transpose()\n\n        assert lane_embedding_feats.shape[0] == lane_coordinate.shape[0]\n\n        ret = {\n            \'lane_embedding_feats\': lane_embedding_feats,\n            \'lane_coordinates\': lane_coordinate\n        }\n\n        return ret\n\n    def apply_lane_feats_cluster(self, binary_seg_result, instance_seg_result):\n        """"""\n\n        :param binary_seg_result:\n        :param instance_seg_result:\n        :return:\n        """"""\n        # get embedding feats and coords\n        get_lane_embedding_feats_result = self._get_lane_embedding_feats(\n            binary_seg_ret=binary_seg_result,\n            instance_seg_ret=instance_seg_result\n        )\n\n        # dbscan cluster\n        dbscan_cluster_result = self._embedding_feats_dbscan_cluster(\n            embedding_image_feats=get_lane_embedding_feats_result[\'lane_embedding_feats\']\n        )\n\n        mask = np.zeros(shape=[binary_seg_result.shape[0], binary_seg_result.shape[1], 3], dtype=np.uint8)\n        db_labels = dbscan_cluster_result[\'db_labels\']\n        unique_labels = dbscan_cluster_result[\'unique_labels\']\n        coord = get_lane_embedding_feats_result[\'lane_coordinates\']\n\n        if db_labels is None:\n            return None, None\n\n        lane_coords = []\n\n        for index, label in enumerate(unique_labels.tolist()):\n            if label == -1:\n                continue\n            idx = np.where(db_labels == label)\n            pix_coord_idx = tuple((coord[idx][:, 1], coord[idx][:, 0]))\n            mask[pix_coord_idx] = self._color_map[index]\n            lane_coords.append(coord[idx])\n\n        return mask, lane_coords\n\n\nclass LaneNetPostProcessor(object):\n    """"""\n    lanenet post process for lane generation\n    """"""\n    def __init__(self, ipm_remap_file_path=\'./data/tusimple_ipm_remap.yml\'):\n        """"""\n\n        :param ipm_remap_file_path: ipm generate file path\n        """"""\n        assert ops.exists(ipm_remap_file_path), \'{:s} not exist\'.format(ipm_remap_file_path)\n\n        self._cluster = _LaneNetCluster()\n        self._ipm_remap_file_path = ipm_remap_file_path\n\n        remap_file_load_ret = self._load_remap_matrix()\n        self._remap_to_ipm_x = remap_file_load_ret[\'remap_to_ipm_x\']\n        self._remap_to_ipm_y = remap_file_load_ret[\'remap_to_ipm_y\']\n\n        self._color_map = [np.array([255, 0, 0]),\n                           np.array([0, 255, 0]),\n                           np.array([0, 0, 255]),\n                           np.array([125, 125, 0]),\n                           np.array([0, 125, 125]),\n                           np.array([125, 0, 125]),\n                           np.array([50, 100, 50]),\n                           np.array([100, 50, 100])]\n\n    def _load_remap_matrix(self):\n        """"""\n\n        :return:\n        """"""\n        fs = cv2.FileStorage(self._ipm_remap_file_path, cv2.FILE_STORAGE_READ)\n\n        remap_to_ipm_x = fs.getNode(\'remap_ipm_x\').mat()\n        remap_to_ipm_y = fs.getNode(\'remap_ipm_y\').mat()\n\n        ret = {\n            \'remap_to_ipm_x\': remap_to_ipm_x,\n            \'remap_to_ipm_y\': remap_to_ipm_y,\n        }\n\n        fs.release()\n\n        return ret\n\n    def postprocess(self, binary_seg_result, instance_seg_result=None,\n                    min_area_threshold=100, source_image=None,\n                    data_source=\'tusimple\'):\n        """"""\n\n        :param binary_seg_result:\n        :param instance_seg_result:\n        :param min_area_threshold:\n        :param source_image:\n        :param data_source:\n        :return:\n        """"""\n        # convert binary_seg_result\n        binary_seg_result = np.array(binary_seg_result * 255, dtype=np.uint8)\n\n        # apply image morphology operation to fill in the hold and reduce the small area\n        morphological_ret = _morphological_process(binary_seg_result, kernel_size=5)\n\n        connect_components_analysis_ret = _connect_components_analysis(image=morphological_ret)\n\n        labels = connect_components_analysis_ret[1]\n        stats = connect_components_analysis_ret[2]\n        for index, stat in enumerate(stats):\n            if stat[4] <= min_area_threshold:\n                idx = np.where(labels == index)\n                morphological_ret[idx] = 0\n\n        # apply embedding features cluster\n        mask_image, lane_coords = self._cluster.apply_lane_feats_cluster(\n            binary_seg_result=morphological_ret,\n            instance_seg_result=instance_seg_result\n        )\n\n        if mask_image is None:\n            return {\n                \'mask_image\': None,\n                \'fit_params\': None,\n                \'source_image\': None,\n            }\n\n        # lane line fit\n        fit_params = []\n        src_lane_pts = []  # lane pts every single lane\n        for lane_index, coords in enumerate(lane_coords):\n            if data_source == \'tusimple\':\n                tmp_mask = np.zeros(shape=(720, 1280), dtype=np.uint8)\n                tmp_mask[tuple((np.int_(coords[:, 1] * 720 / 256), np.int_(coords[:, 0] * 1280 / 512)))] = 255\n            elif data_source == \'beec_ccd\':\n                tmp_mask = np.zeros(shape=(1350, 2448), dtype=np.uint8)\n                tmp_mask[tuple((np.int_(coords[:, 1] * 1350 / 256), np.int_(coords[:, 0] * 2448 / 512)))] = 255\n            else:\n                raise ValueError(\'Wrong data source now only support tusimple and beec_ccd\')\n            tmp_ipm_mask = cv2.remap(\n                tmp_mask,\n                self._remap_to_ipm_x,\n                self._remap_to_ipm_y,\n                interpolation=cv2.INTER_NEAREST\n            )\n            nonzero_y = np.array(tmp_ipm_mask.nonzero()[0])\n            nonzero_x = np.array(tmp_ipm_mask.nonzero()[1])\n\n            fit_param = np.polyfit(nonzero_y, nonzero_x, 2)\n            fit_params.append(fit_param)\n\n            [ipm_image_height, ipm_image_width] = tmp_ipm_mask.shape\n            plot_y = np.linspace(10, ipm_image_height, ipm_image_height - 10)\n            fit_x = fit_param[0] * plot_y ** 2 + fit_param[1] * plot_y + fit_param[2]\n            # fit_x = fit_param[0] * plot_y ** 3 + fit_param[1] * plot_y ** 2 + fit_param[2] * plot_y + fit_param[3]\n\n            lane_pts = []\n            for index in range(0, plot_y.shape[0], 5):\n                src_x = self._remap_to_ipm_x[\n                    int(plot_y[index]), int(np.clip(fit_x[index], 0, ipm_image_width - 1))]\n                if src_x <= 0:\n                    continue\n                src_y = self._remap_to_ipm_y[\n                    int(plot_y[index]), int(np.clip(fit_x[index], 0, ipm_image_width - 1))]\n                src_y = src_y if src_y > 0 else 0\n\n                lane_pts.append([src_x, src_y])\n\n            src_lane_pts.append(lane_pts)\n\n        # tusimple test data sample point along y axis every 10 pixels\n        source_image_width = source_image.shape[1]\n        for index, single_lane_pts in enumerate(src_lane_pts):\n            single_lane_pt_x = np.array(single_lane_pts, dtype=np.float32)[:, 0]\n            single_lane_pt_y = np.array(single_lane_pts, dtype=np.float32)[:, 1]\n            if data_source == \'tusimple\':\n                start_plot_y = 240\n                end_plot_y = 720\n            elif data_source == \'beec_ccd\':\n                start_plot_y = 820\n                end_plot_y = 1350\n            else:\n                raise ValueError(\'Wrong data source now only support tusimple and beec_ccd\')\n            step = int(math.floor((end_plot_y - start_plot_y) / 10))\n            for plot_y in np.linspace(start_plot_y, end_plot_y, step):\n                diff = single_lane_pt_y - plot_y\n                fake_diff_bigger_than_zero = diff.copy()\n                fake_diff_smaller_than_zero = diff.copy()\n                fake_diff_bigger_than_zero[np.where(diff <= 0)] = float(\'inf\')\n                fake_diff_smaller_than_zero[np.where(diff > 0)] = float(\'-inf\')\n                idx_low = np.argmax(fake_diff_smaller_than_zero)\n                idx_high = np.argmin(fake_diff_bigger_than_zero)\n\n                previous_src_pt_x = single_lane_pt_x[idx_low]\n                previous_src_pt_y = single_lane_pt_y[idx_low]\n                last_src_pt_x = single_lane_pt_x[idx_high]\n                last_src_pt_y = single_lane_pt_y[idx_high]\n\n                if previous_src_pt_y < start_plot_y or last_src_pt_y < start_plot_y or \\\n                        fake_diff_smaller_than_zero[idx_low] == float(\'-inf\') or \\\n                        fake_diff_bigger_than_zero[idx_high] == float(\'inf\'):\n                    continue\n\n                interpolation_src_pt_x = (abs(previous_src_pt_y - plot_y) * previous_src_pt_x +\n                                          abs(last_src_pt_y - plot_y) * last_src_pt_x) / \\\n                                         (abs(previous_src_pt_y - plot_y) + abs(last_src_pt_y - plot_y))\n                interpolation_src_pt_y = (abs(previous_src_pt_y - plot_y) * previous_src_pt_y +\n                                          abs(last_src_pt_y - plot_y) * last_src_pt_y) / \\\n                                         (abs(previous_src_pt_y - plot_y) + abs(last_src_pt_y - plot_y))\n\n                if interpolation_src_pt_x > source_image_width or interpolation_src_pt_x < 10:\n                    continue\n\n                lane_color = self._color_map[index].tolist()\n                cv2.circle(source_image, (int(interpolation_src_pt_x),\n                                          int(interpolation_src_pt_y)), 5, lane_color, -1)\n        ret = {\n            \'mask_image\': mask_image,\n            \'fit_params\': fit_params,\n            \'source_image\': source_image,\n        }\n\n        return ret\n'"
mnn_project/__init__.py,0,b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 2019/11/5 \xe4\xb8\x8b\xe5\x8d\x885:03\n# @Author  : LuoYao\n# @Site    : ICode\n# @File    : __init__.py.py\n# @IDE: PyCharm'
mnn_project/freeze_lanenet_model.py,11,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 2019/11/5 \xe4\xb8\x8b\xe5\x8d\x884:53\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : freeze_lanenet_model.py.py\n# @IDE: PyCharm\n""""""\nFreeze Lanenet model into frozen pb file\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\nimport tensorflow as tf\n\nfrom lanenet_model import lanenet\n\nMODEL_WEIGHTS_FILE_PATH = \'./test.ckpt\'\nOUTPUT_PB_FILE_PATH = \'./lanenet.pb\'\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-w\', \'--weights_path\', default=MODEL_WEIGHTS_FILE_PATH)\n    parser.add_argument(\'-s\', \'--save_path\', default=OUTPUT_PB_FILE_PATH)\n\n    return parser.parse_args()\n\n\ndef convert_ckpt_into_pb_file(ckpt_file_path, pb_file_path):\n    """"""\n\n    :param ckpt_file_path:\n    :param pb_file_path:\n    :return:\n    """"""\n    # construct compute graph\n    with tf.variable_scope(\'lanenet\'):\n        input_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 256, 512, 3], name=\'input_tensor\')\n\n    net = lanenet.LaneNet(phase=\'test\', net_flag=\'vgg\')\n    binary_seg_ret, instance_seg_ret = net.inference(input_tensor=input_tensor, name=\'lanenet_model\')\n\n    with tf.variable_scope(\'lanenet/\'):\n        binary_seg_ret = tf.cast(binary_seg_ret, dtype=tf.float32)\n        binary_seg_ret = tf.squeeze(binary_seg_ret, axis=0, name=\'final_binary_output\')\n        instance_seg_ret = tf.squeeze(instance_seg_ret, axis=0, name=\'final_pixel_embedding_output\')\n\n    # create a session\n    saver = tf.train.Saver()\n\n    sess_config = tf.ConfigProto()\n    sess_config.gpu_options.per_process_gpu_memory_fraction = 0.85\n    sess_config.gpu_options.allow_growth = False\n    sess_config.gpu_options.allocator_type = \'BFC\'\n\n    sess = tf.Session(config=sess_config)\n\n    with sess.as_default():\n        saver.restore(sess, ckpt_file_path)\n\n        converted_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,\n            input_graph_def=sess.graph.as_graph_def(),\n            output_node_names=[\n                \'lanenet/input_tensor\',\n                \'lanenet/final_binary_output\',\n                \'lanenet/final_pixel_embedding_output\'\n            ]\n        )\n\n        with tf.gfile.GFile(pb_file_path, ""wb"") as f:\n            f.write(converted_graph_def.SerializeToString())\n\n\nif __name__ == \'__main__\':\n    """"""\n    test code\n    """"""\n    args = init_args()\n\n    convert_ckpt_into_pb_file(\n        ckpt_file_path=args.weights_path,\n        pb_file_path=args.save_path\n    )\n'"
semantic_segmentation_zoo/__init__.py,0,b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-24 \xe4\xb8\x8b\xe5\x8d\x886:41\n# @Author  : LuoYao\n# @Site    : ICode\n# @File    : __init__.py.py\n# @IDE: PyCharm'
semantic_segmentation_zoo/cnn_basenet.py,71,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-18 \xe4\xb8\x8b\xe5\x8d\x883:59\n# @Author  : Luo Yao\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : cnn_basenet.py\n# @IDE: PyCharm Community Edition\n""""""\nThe base convolution neural networks mainly implement some useful cnn functions\n""""""\nimport tensorflow as tf\nimport numpy as np\n\n\nclass CNNBaseModel(object):\n    """"""\n    Base model for other specific cnn ctpn_models\n    """"""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def conv2d(inputdata, out_channel, kernel_size, padding=\'SAME\',\n               stride=1, w_init=None, b_init=None,\n               split=1, use_bias=True, data_format=\'NHWC\', name=None):\n        """"""\n        Packing the tensorflow conv2d function.\n        :param name: op name\n        :param inputdata: A 4D tensorflow tensor which ust have known number of channels, but can have other\n        unknown dimensions.\n        :param out_channel: number of output channel.\n        :param kernel_size: int so only support square kernel convolution\n        :param padding: \'VALID\' or \'SAME\'\n        :param stride: int so only support square stride\n        :param w_init: initializer for convolution weights\n        :param b_init: initializer for bias\n        :param split: split channels as used in Alexnet mainly group for GPU memory save.\n        :param use_bias:  whether to use bias.\n        :param data_format: default set to NHWC according tensorflow\n        :return: tf.Tensor named ``output``\n        """"""\n        with tf.variable_scope(name):\n            in_shape = inputdata.get_shape().as_list()\n            channel_axis = 3 if data_format == \'NHWC\' else 1\n            in_channel = in_shape[channel_axis]\n            assert in_channel is not None, ""[Conv2D] Input cannot have unknown channel!""\n            assert in_channel % split == 0\n            assert out_channel % split == 0\n\n            padding = padding.upper()\n\n            if isinstance(kernel_size, list):\n                filter_shape = [kernel_size[0], kernel_size[1]] + [in_channel / split, out_channel]\n            else:\n                filter_shape = [kernel_size, kernel_size] + [in_channel / split, out_channel]\n\n            if isinstance(stride, list):\n                strides = [1, stride[0], stride[1], 1] if data_format == \'NHWC\' \\\n                    else [1, 1, stride[0], stride[1]]\n            else:\n                strides = [1, stride, stride, 1] if data_format == \'NHWC\' \\\n                    else [1, 1, stride, stride]\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            w = tf.get_variable(\'W\', filter_shape, initializer=w_init)\n            b = None\n\n            if use_bias:\n                b = tf.get_variable(\'b\', [out_channel], initializer=b_init)\n\n            if split == 1:\n                conv = tf.nn.conv2d(inputdata, w, strides, padding, data_format=data_format)\n            else:\n                inputs = tf.split(inputdata, split, channel_axis)\n                kernels = tf.split(w, split, 3)\n                outputs = [tf.nn.conv2d(i, k, strides, padding, data_format=data_format)\n                           for i, k in zip(inputs, kernels)]\n                conv = tf.concat(outputs, channel_axis)\n\n            ret = tf.identity(tf.nn.bias_add(conv, b, data_format=data_format)\n                              if use_bias else conv, name=name)\n\n        return ret\n\n    @staticmethod\n    def relu(inputdata, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :return:\n        """"""\n        return tf.nn.relu(features=inputdata, name=name)\n\n    @staticmethod\n    def sigmoid(inputdata, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :return:\n        """"""\n        return tf.nn.sigmoid(x=inputdata, name=name)\n\n    @staticmethod\n    def maxpooling(inputdata, kernel_size, stride=None, padding=\'VALID\',\n                   data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param kernel_size:\n        :param stride:\n        :param padding:\n        :param data_format:\n        :return:\n        """"""\n        padding = padding.upper()\n\n        if stride is None:\n            stride = kernel_size\n\n        if isinstance(kernel_size, list):\n            kernel = [1, kernel_size[0], kernel_size[1], 1] if data_format == \'NHWC\' else \\\n                [1, 1, kernel_size[0], kernel_size[1]]\n        else:\n            kernel = [1, kernel_size, kernel_size, 1] if data_format == \'NHWC\' \\\n                else [1, 1, kernel_size, kernel_size]\n\n        if isinstance(stride, list):\n            strides = [1, stride[0], stride[1], 1] if data_format == \'NHWC\' \\\n                else [1, 1, stride[0], stride[1]]\n        else:\n            strides = [1, stride, stride, 1] if data_format == \'NHWC\' \\\n                else [1, 1, stride, stride]\n\n        return tf.nn.max_pool(value=inputdata, ksize=kernel, strides=strides, padding=padding,\n                              data_format=data_format, name=name)\n\n    @staticmethod\n    def avgpooling(inputdata, kernel_size, stride=None, padding=\'VALID\',\n                   data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param kernel_size:\n        :param stride:\n        :param padding:\n        :param data_format:\n        :return:\n        """"""\n        if stride is None:\n            stride = kernel_size\n\n        kernel = [1, kernel_size, kernel_size, 1] if data_format == \'NHWC\' \\\n            else [1, 1, kernel_size, kernel_size]\n\n        strides = [1, stride, stride, 1] if data_format == \'NHWC\' else [1, 1, stride, stride]\n\n        return tf.nn.avg_pool(value=inputdata, ksize=kernel, strides=strides, padding=padding,\n                              data_format=data_format, name=name)\n\n    @staticmethod\n    def globalavgpooling(inputdata, data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param data_format:\n        :return:\n        """"""\n        assert inputdata.shape.ndims == 4\n        assert data_format in [\'NHWC\', \'NCHW\']\n\n        axis = [1, 2] if data_format == \'NHWC\' else [2, 3]\n\n        return tf.reduce_mean(input_tensor=inputdata, axis=axis, name=name)\n\n    @staticmethod\n    def layernorm(inputdata, epsilon=1e-5, use_bias=True, use_scale=True,\n                  data_format=\'NHWC\', name=None):\n        """"""\n        :param name:\n        :param inputdata:\n        :param epsilon: epsilon to avoid divide-by-zero.\n        :param use_bias: whether to use the extra affine transformation or not.\n        :param use_scale: whether to use the extra affine transformation or not.\n        :param data_format:\n        :return:\n        """"""\n        shape = inputdata.get_shape().as_list()\n        ndims = len(shape)\n        assert ndims in [2, 4]\n\n        mean, var = tf.nn.moments(inputdata, list(range(1, len(shape))), keep_dims=True)\n\n        if data_format == \'NCHW\':\n            channnel = shape[1]\n            new_shape = [1, channnel, 1, 1]\n        else:\n            channnel = shape[-1]\n            new_shape = [1, 1, 1, channnel]\n        if ndims == 2:\n            new_shape = [1, channnel]\n\n        if use_bias:\n            beta = tf.get_variable(\'beta\', [channnel], initializer=tf.constant_initializer())\n            beta = tf.reshape(beta, new_shape)\n        else:\n            beta = tf.zeros([1] * ndims, name=\'beta\')\n        if use_scale:\n            gamma = tf.get_variable(\'gamma\', [channnel], initializer=tf.constant_initializer(1.0))\n            gamma = tf.reshape(gamma, new_shape)\n        else:\n            gamma = tf.ones([1] * ndims, name=\'gamma\')\n\n        return tf.nn.batch_normalization(inputdata, mean, var, beta, gamma, epsilon, name=name)\n\n    @staticmethod\n    def instancenorm(inputdata, epsilon=1e-5, data_format=\'NHWC\', use_affine=True, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param epsilon:\n        :param data_format:\n        :param use_affine:\n        :return:\n        """"""\n        shape = inputdata.get_shape().as_list()\n        if len(shape) != 4:\n            raise ValueError(""Input data of instancebn layer has to be 4D tensor"")\n\n        if data_format == \'NHWC\':\n            axis = [1, 2]\n            ch = shape[3]\n            new_shape = [1, 1, 1, ch]\n        else:\n            axis = [2, 3]\n            ch = shape[1]\n            new_shape = [1, ch, 1, 1]\n        if ch is None:\n            raise ValueError(""Input of instancebn require known channel!"")\n\n        mean, var = tf.nn.moments(inputdata, axis, keep_dims=True)\n\n        if not use_affine:\n            return tf.divide(inputdata - mean, tf.sqrt(var + epsilon), name=\'output\')\n\n        beta = tf.get_variable(\'beta\', [ch], initializer=tf.constant_initializer())\n        beta = tf.reshape(beta, new_shape)\n        gamma = tf.get_variable(\'gamma\', [ch], initializer=tf.constant_initializer(1.0))\n        gamma = tf.reshape(gamma, new_shape)\n        return tf.nn.batch_normalization(inputdata, mean, var, beta, gamma, epsilon, name=name)\n\n    @staticmethod\n    def dropout(inputdata, keep_prob, noise_shape=None, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param keep_prob:\n        :param noise_shape:\n        :return:\n        """"""\n        return tf.nn.dropout(inputdata, keep_prob=keep_prob, noise_shape=noise_shape, name=name)\n\n    @staticmethod\n    def fullyconnect(inputdata, out_dim, w_init=None, b_init=None,\n                     use_bias=True, name=None):\n        """"""\n        Fully-Connected layer, takes a N>1D tensor and returns a 2D tensor.\n        It is an equivalent of `tf.layers.dense` except for naming conventions.\n\n        :param inputdata:  a tensor to be flattened except for the first dimension.\n        :param out_dim: output dimension\n        :param w_init: initializer for w. Defaults to `variance_scaling_initializer`.\n        :param b_init: initializer for b. Defaults to zero\n        :param use_bias: whether to use bias.\n        :param name:\n        :return: tf.Tensor: a NC tensor named ``output`` with attribute `variables`.\n        """"""\n        shape = inputdata.get_shape().as_list()[1:]\n        if None not in shape:\n            inputdata = tf.reshape(inputdata, [-1, int(np.prod(shape))])\n        else:\n            inputdata = tf.reshape(inputdata, tf.stack([tf.shape(inputdata)[0], -1]))\n\n        if w_init is None:\n            w_init = tf.contrib.layers.variance_scaling_initializer()\n        if b_init is None:\n            b_init = tf.constant_initializer()\n\n        ret = tf.layers.dense(inputs=inputdata, activation=lambda x: tf.identity(x, name=\'output\'),\n                              use_bias=use_bias, name=name,\n                              kernel_initializer=w_init, bias_initializer=b_init,\n                              trainable=True, units=out_dim)\n        return ret\n\n    @staticmethod\n    def layerbn(inputdata, is_training, name):\n        """"""\n\n        :param inputdata:\n        :param is_training:\n        :param name:\n        :return:\n        """"""\n\n        return tf.layers.batch_normalization(inputs=inputdata, training=is_training, name=name)\n\n    @staticmethod\n    def layergn(inputdata, name, group_size=32, esp=1e-5):\n        """"""\n\n        :param inputdata:\n        :param name:\n        :param group_size:\n        :param esp:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            inputdata = tf.transpose(inputdata, [0, 3, 1, 2])\n            n, c, h, w = inputdata.get_shape().as_list()\n            group_size = min(group_size, c)\n            inputdata = tf.reshape(inputdata, [-1, group_size, c // group_size, h, w])\n            mean, var = tf.nn.moments(inputdata, [2, 3, 4], keep_dims=True)\n            inputdata = (inputdata - mean) / tf.sqrt(var + esp)\n\n            # \xe6\xaf\x8f\xe4\xb8\xaa\xe9\x80\x9a\xe9\x81\x93\xe7\x9a\x84gamma\xe5\x92\x8cbeta\n            gamma = tf.Variable(tf.constant(1.0, shape=[c]), dtype=tf.float32, name=\'gamma\')\n            beta = tf.Variable(tf.constant(0.0, shape=[c]), dtype=tf.float32, name=\'beta\')\n            gamma = tf.reshape(gamma, [1, c, 1, 1])\n            beta = tf.reshape(beta, [1, c, 1, 1])\n\n            # \xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xba\xe6\x96\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe6\x8d\xa2 [n, c, h, w, c] \xe5\x88\xb0 [n, h, w, c]\n            output = tf.reshape(inputdata, [-1, c, h, w])\n            output = output * gamma + beta\n            output = tf.transpose(output, [0, 2, 3, 1])\n\n        return output\n\n    @staticmethod\n    def squeeze(inputdata, axis=None, name=None):\n        """"""\n\n        :param inputdata:\n        :param axis:\n        :param name:\n        :return:\n        """"""\n        return tf.squeeze(input=inputdata, axis=axis, name=name)\n\n    @staticmethod\n    def deconv2d(inputdata, out_channel, kernel_size, padding=\'SAME\',\n                 stride=1, w_init=None, b_init=None,\n                 use_bias=True, activation=None, data_format=\'channels_last\',\n                 trainable=True, name=None):\n        """"""\n        Packing the tensorflow conv2d function.\n        :param name: op name\n        :param inputdata: A 4D tensorflow tensor which ust have known number of channels, but can have other\n        unknown dimensions.\n        :param out_channel: number of output channel.\n        :param kernel_size: int so only support square kernel convolution\n        :param padding: \'VALID\' or \'SAME\'\n        :param stride: int so only support square stride\n        :param w_init: initializer for convolution weights\n        :param b_init: initializer for bias\n        :param activation: whether to apply a activation func to deconv result\n        :param use_bias:  whether to use bias.\n        :param data_format: default set to NHWC according tensorflow\n        :return: tf.Tensor named ``output``\n        """"""\n        with tf.variable_scope(name):\n            in_shape = inputdata.get_shape().as_list()\n            channel_axis = 3 if data_format == \'channels_last\' else 1\n            in_channel = in_shape[channel_axis]\n            assert in_channel is not None, ""[Deconv2D] Input cannot have unknown channel!""\n\n            padding = padding.upper()\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            ret = tf.layers.conv2d_transpose(inputs=inputdata, filters=out_channel,\n                                             kernel_size=kernel_size,\n                                             strides=stride, padding=padding,\n                                             data_format=data_format,\n                                             activation=activation, use_bias=use_bias,\n                                             kernel_initializer=w_init,\n                                             bias_initializer=b_init, trainable=trainable,\n                                             name=name)\n        return ret\n\n    @staticmethod\n    def dilation_conv(input_tensor, k_size, out_dims, rate, padding=\'SAME\',\n                      w_init=None, b_init=None, use_bias=False, name=None):\n        """"""\n\n        :param input_tensor:\n        :param k_size:\n        :param out_dims:\n        :param rate:\n        :param padding:\n        :param w_init:\n        :param b_init:\n        :param use_bias:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            in_shape = input_tensor.get_shape().as_list()\n            in_channel = in_shape[3]\n            assert in_channel is not None, ""[Conv2D] Input cannot have unknown channel!""\n\n            padding = padding.upper()\n\n            if isinstance(k_size, list):\n                filter_shape = [k_size[0], k_size[1]] + [in_channel, out_dims]\n            else:\n                filter_shape = [k_size, k_size] + [in_channel, out_dims]\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            w = tf.get_variable(\'W\', filter_shape, initializer=w_init)\n            b = None\n\n            if use_bias:\n                b = tf.get_variable(\'b\', [out_dims], initializer=b_init)\n\n            conv = tf.nn.atrous_conv2d(value=input_tensor, filters=w, rate=rate,\n                                       padding=padding, name=\'dilation_conv\')\n\n            if use_bias:\n                ret = tf.add(conv, b)\n            else:\n                ret = conv\n\n        return ret\n\n    @staticmethod\n    def spatial_dropout(input_tensor, keep_prob, is_training, name, seed=1234):\n        """"""\n        \xe7\xa9\xba\xe9\x97\xb4dropout\xe5\xae\x9e\xe7\x8e\xb0\n        :param input_tensor:\n        :param keep_prob:\n        :param is_training:\n        :param name:\n        :param seed:\n        :return:\n        """"""\n\n        def f1():\n            input_shape = input_tensor.get_shape().as_list()\n            noise_shape = tf.constant(value=[input_shape[0], 1, 1, input_shape[3]])\n            return tf.nn.dropout(input_tensor, keep_prob, noise_shape, seed=seed, name=""spatial_dropout"")\n\n        def f2():\n            return input_tensor\n\n        with tf.variable_scope(name_or_scope=name):\n\n            output = tf.cond(is_training, f1, f2)\n\n            return output\n\n    @staticmethod\n    def lrelu(inputdata, name, alpha=0.2):\n        """"""\n\n        :param inputdata:\n        :param alpha:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            return tf.nn.relu(inputdata) - alpha * tf.nn.relu(-inputdata)\n'"
semantic_segmentation_zoo/vgg16_based_fcn.py,22,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-24 \xe4\xb8\x8b\xe5\x8d\x886:42\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : vgg16_based_fcn.py\n# @IDE: PyCharm\n""""""\nImplement VGG16 based fcn net for semantic segmentation\n""""""\nimport collections\n\nimport tensorflow as tf\n\nfrom config import global_config\nfrom semantic_segmentation_zoo import cnn_basenet\n\nCFG = global_config.cfg\n\n\nclass VGG16FCN(cnn_basenet.CNNBaseModel):\n    """"""\n    VGG 16 based fcn net for semantic segmentation\n    """"""\n    def __init__(self, phase):\n        """"""\n\n        """"""\n        super(VGG16FCN, self).__init__()\n        self._phase = phase\n        self._is_training = self._is_net_for_training()\n        self._net_intermediate_results = collections.OrderedDict()\n\n    def _is_net_for_training(self):\n        """"""\n        if the net is used for training or not\n        :return:\n        """"""\n        if isinstance(self._phase, tf.Tensor):\n            phase = self._phase\n        else:\n            phase = tf.constant(self._phase, dtype=tf.string)\n\n        return tf.equal(phase, tf.constant(\'train\', dtype=tf.string))\n\n    def _vgg16_conv_stage(self, input_tensor, k_size, out_dims, name,\n                          stride=1, pad=\'SAME\', need_layer_norm=True):\n        """"""\n        stack conv and activation in vgg16\n        :param input_tensor:\n        :param k_size:\n        :param out_dims:\n        :param name:\n        :param stride:\n        :param pad:\n        :param need_layer_norm:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            conv = self.conv2d(\n                inputdata=input_tensor, out_channel=out_dims,\n                kernel_size=k_size, stride=stride,\n                use_bias=False, padding=pad, name=\'conv\'\n            )\n\n            if need_layer_norm:\n                bn = self.layerbn(inputdata=conv, is_training=self._is_training, name=\'bn\')\n\n                relu = self.relu(inputdata=bn, name=\'relu\')\n            else:\n                relu = self.relu(inputdata=conv, name=\'relu\')\n\n        return relu\n\n    def _decode_block(self, input_tensor, previous_feats_tensor,\n                      out_channels_nums, name, kernel_size=4,\n                      stride=2, use_bias=False,\n                      previous_kernel_size=4, need_activate=True):\n        """"""\n\n        :param input_tensor:\n        :param previous_feats_tensor:\n        :param out_channels_nums:\n        :param kernel_size:\n        :param previous_kernel_size:\n        :param use_bias:\n        :param stride:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name):\n\n            deconv_weights_stddev = tf.sqrt(\n                tf.divide(tf.constant(2.0, tf.float32),\n                          tf.multiply(tf.cast(previous_kernel_size * previous_kernel_size, tf.float32),\n                                      tf.cast(tf.shape(input_tensor)[3], tf.float32)))\n            )\n            deconv_weights_init = tf.truncated_normal_initializer(\n                mean=0.0, stddev=deconv_weights_stddev)\n\n            deconv = self.deconv2d(\n                inputdata=input_tensor, out_channel=out_channels_nums, kernel_size=kernel_size,\n                stride=stride, use_bias=use_bias, w_init=deconv_weights_init,\n                name=\'deconv\'\n            )\n\n            deconv = self.layerbn(inputdata=deconv, is_training=self._is_training, name=\'deconv_bn\')\n\n            deconv = self.relu(inputdata=deconv, name=\'deconv_relu\')\n\n            fuse_feats = tf.add(\n                previous_feats_tensor, deconv, name=\'fuse_feats\'\n            )\n\n            if need_activate:\n\n                fuse_feats = self.layerbn(\n                    inputdata=fuse_feats, is_training=self._is_training, name=\'fuse_gn\'\n                )\n\n                fuse_feats = self.relu(inputdata=fuse_feats, name=\'fuse_relu\')\n\n        return fuse_feats\n\n    def _vgg16_fcn_encode(self, input_tensor, name):\n        """"""\n\n        :param input_tensor:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name):\n            # encode stage 1\n            conv_1_1 = self._vgg16_conv_stage(\n                input_tensor=input_tensor, k_size=3,\n                out_dims=64, name=\'conv1_1\',\n                need_layer_norm=True\n            )\n            conv_1_2 = self._vgg16_conv_stage(\n                input_tensor=conv_1_1, k_size=3,\n                out_dims=64, name=\'conv1_2\',\n                need_layer_norm=True\n            )\n            self._net_intermediate_results[\'encode_stage_1_share\'] = {\n                \'data\': conv_1_2,\n                \'shape\': conv_1_2.get_shape().as_list()\n            }\n\n            # encode stage 2\n            pool1 = self.maxpooling(\n                inputdata=conv_1_2, kernel_size=2,\n                stride=2, name=\'pool1\'\n            )\n            conv_2_1 = self._vgg16_conv_stage(\n                input_tensor=pool1, k_size=3,\n                out_dims=128, name=\'conv2_1\',\n                need_layer_norm=True\n            )\n            conv_2_2 = self._vgg16_conv_stage(\n                input_tensor=conv_2_1, k_size=3,\n                out_dims=128, name=\'conv2_2\',\n                need_layer_norm=True\n            )\n            self._net_intermediate_results[\'encode_stage_2_share\'] = {\n                \'data\': conv_2_2,\n                \'shape\': conv_2_2.get_shape().as_list()\n            }\n\n            # encode stage 3\n            pool2 = self.maxpooling(\n                inputdata=conv_2_2, kernel_size=2,\n                stride=2, name=\'pool2\'\n            )\n            conv_3_1 = self._vgg16_conv_stage(\n                input_tensor=pool2, k_size=3,\n                out_dims=256, name=\'conv3_1\',\n                need_layer_norm=True\n            )\n            conv_3_2 = self._vgg16_conv_stage(\n                input_tensor=conv_3_1, k_size=3,\n                out_dims=256, name=\'conv3_2\',\n                need_layer_norm=True\n            )\n            conv_3_3 = self._vgg16_conv_stage(\n                input_tensor=conv_3_2, k_size=3,\n                out_dims=256, name=\'conv3_3\',\n                need_layer_norm=True\n            )\n            self._net_intermediate_results[\'encode_stage_3_share\'] = {\n                \'data\': conv_3_3,\n                \'shape\': conv_3_3.get_shape().as_list()\n            }\n\n            # encode stage 4\n            pool3 = self.maxpooling(\n                inputdata=conv_3_3, kernel_size=2,\n                stride=2, name=\'pool3\'\n            )\n            conv_4_1 = self._vgg16_conv_stage(\n                input_tensor=pool3, k_size=3,\n                out_dims=512, name=\'conv4_1\',\n                need_layer_norm=True\n            )\n            conv_4_2 = self._vgg16_conv_stage(\n                input_tensor=conv_4_1, k_size=3,\n                out_dims=512, name=\'conv4_2\',\n                need_layer_norm=True\n            )\n            conv_4_3 = self._vgg16_conv_stage(\n                input_tensor=conv_4_2, k_size=3,\n                out_dims=512, name=\'conv4_3\',\n                need_layer_norm=True\n            )\n            self._net_intermediate_results[\'encode_stage_4_share\'] = {\n                \'data\': conv_4_3,\n                \'shape\': conv_4_3.get_shape().as_list()\n            }\n\n            # encode stage 5 for binary segmentation\n            pool4 = self.maxpooling(\n                inputdata=conv_4_3, kernel_size=2,\n                stride=2, name=\'pool4\'\n            )\n            conv_5_1_binary = self._vgg16_conv_stage(\n                input_tensor=pool4, k_size=3,\n                out_dims=512, name=\'conv5_1_binary\',\n                need_layer_norm=True\n            )\n            conv_5_2_binary = self._vgg16_conv_stage(\n                input_tensor=conv_5_1_binary, k_size=3,\n                out_dims=512, name=\'conv5_2_binary\',\n                need_layer_norm=True\n            )\n            conv_5_3_binary = self._vgg16_conv_stage(\n                input_tensor=conv_5_2_binary, k_size=3,\n                out_dims=512, name=\'conv5_3_binary\',\n                need_layer_norm=True\n            )\n            self._net_intermediate_results[\'encode_stage_5_binary\'] = {\n                \'data\': conv_5_3_binary,\n                \'shape\': conv_5_3_binary.get_shape().as_list()\n            }\n\n            # encode stage 5 for instance segmentation\n            conv_5_1_instance = self._vgg16_conv_stage(\n                input_tensor=pool4, k_size=3,\n                out_dims=512, name=\'conv5_1_instance\',\n                need_layer_norm=True\n            )\n            conv_5_2_instance = self._vgg16_conv_stage(\n                input_tensor=conv_5_1_instance, k_size=3,\n                out_dims=512, name=\'conv5_2_instance\',\n                need_layer_norm=True\n            )\n            conv_5_3_instance = self._vgg16_conv_stage(\n                input_tensor=conv_5_2_instance, k_size=3,\n                out_dims=512, name=\'conv5_3_instance\',\n                need_layer_norm=True\n            )\n            self._net_intermediate_results[\'encode_stage_5_instance\'] = {\n                \'data\': conv_5_3_instance,\n                \'shape\': conv_5_3_instance.get_shape().as_list()\n            }\n\n        return\n\n    def _vgg16_fcn_decode(self, name):\n        """"""\n\n        :return:\n        """"""\n        with tf.variable_scope(name):\n\n            # decode part for binary segmentation\n            with tf.variable_scope(name_or_scope=\'binary_seg_decode\'):\n\n                decode_stage_5_binary = self._net_intermediate_results[\'encode_stage_5_binary\'][\'data\']\n\n                decode_stage_4_fuse = self._decode_block(\n                    input_tensor=decode_stage_5_binary,\n                    previous_feats_tensor=self._net_intermediate_results[\'encode_stage_4_share\'][\'data\'],\n                    name=\'decode_stage_4_fuse\', out_channels_nums=512, previous_kernel_size=3\n                )\n                decode_stage_3_fuse = self._decode_block(\n                    input_tensor=decode_stage_4_fuse,\n                    previous_feats_tensor=self._net_intermediate_results[\'encode_stage_3_share\'][\'data\'],\n                    name=\'decode_stage_3_fuse\', out_channels_nums=256\n                )\n                decode_stage_2_fuse = self._decode_block(\n                    input_tensor=decode_stage_3_fuse,\n                    previous_feats_tensor=self._net_intermediate_results[\'encode_stage_2_share\'][\'data\'],\n                    name=\'decode_stage_2_fuse\', out_channels_nums=128\n                )\n                decode_stage_1_fuse = self._decode_block(\n                    input_tensor=decode_stage_2_fuse,\n                    previous_feats_tensor=self._net_intermediate_results[\'encode_stage_1_share\'][\'data\'],\n                    name=\'decode_stage_1_fuse\', out_channels_nums=64\n                )\n                binary_final_logits_conv_weights_stddev = tf.sqrt(\n                    tf.divide(tf.constant(2.0, tf.float32),\n                              tf.multiply(4.0 * 4.0,\n                                          tf.cast(tf.shape(decode_stage_1_fuse)[3], tf.float32)))\n                )\n                binary_final_logits_conv_weights_init = tf.truncated_normal_initializer(\n                    mean=0.0, stddev=binary_final_logits_conv_weights_stddev)\n\n                binary_final_logits = self.conv2d(\n                    inputdata=decode_stage_1_fuse, out_channel=CFG.TRAIN.CLASSES_NUMS,\n                    kernel_size=1, use_bias=False,\n                    w_init=binary_final_logits_conv_weights_init,\n                    name=\'binary_final_logits\')\n\n                self._net_intermediate_results[\'binary_segment_logits\'] = {\n                    \'data\': binary_final_logits,\n                    \'shape\': binary_final_logits.get_shape().as_list()\n                }\n\n            with tf.variable_scope(name_or_scope=\'instance_seg_decode\'):\n\n                decode_stage_5_instance = self._net_intermediate_results[\'encode_stage_5_instance\'][\'data\']\n\n                decode_stage_4_fuse = self._decode_block(\n                    input_tensor=decode_stage_5_instance,\n                    previous_feats_tensor=self._net_intermediate_results[\'encode_stage_4_share\'][\'data\'],\n                    name=\'decode_stage_4_fuse\', out_channels_nums=512, previous_kernel_size=3)\n\n                decode_stage_3_fuse = self._decode_block(\n                    input_tensor=decode_stage_4_fuse,\n                    previous_feats_tensor=self._net_intermediate_results[\'encode_stage_3_share\'][\'data\'],\n                    name=\'decode_stage_3_fuse\', out_channels_nums=256)\n\n                decode_stage_2_fuse = self._decode_block(\n                    input_tensor=decode_stage_3_fuse,\n                    previous_feats_tensor=self._net_intermediate_results[\'encode_stage_2_share\'][\'data\'],\n                    name=\'decode_stage_2_fuse\', out_channels_nums=128)\n\n                decode_stage_1_fuse = self._decode_block(\n                    input_tensor=decode_stage_2_fuse,\n                    previous_feats_tensor=self._net_intermediate_results[\'encode_stage_1_share\'][\'data\'],\n                    name=\'decode_stage_1_fuse\', out_channels_nums=64, need_activate=False)\n\n                self._net_intermediate_results[\'instance_segment_logits\'] = {\n                    \'data\': decode_stage_1_fuse,\n                    \'shape\': decode_stage_1_fuse.get_shape().as_list()\n                }\n\n    def build_model(self, input_tensor, name, reuse=False):\n        """"""\n\n        :param input_tensor:\n        :param name:\n        :param reuse:\n        :return:\n        """"""\n        with tf.variable_scope(name_or_scope=name, reuse=reuse):\n            # vgg16 fcn encode part\n            self._vgg16_fcn_encode(input_tensor=input_tensor, name=\'vgg16_encode_module\')\n            # vgg16 fcn decode part\n            self._vgg16_fcn_decode(name=\'vgg16_decode_module\')\n\n        return self._net_intermediate_results\n\n\nif __name__ == \'__main__\':\n    """"""\n    test code\n    """"""\n    test_in_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 256, 512, 3], name=\'input\')\n    model = VGG16FCN(phase=\'train\')\n    ret = model.build_model(test_in_tensor, name=\'vgg16fcn\')\n    for layer_name, layer_info in ret.items():\n        print(\'layer name: {:s} shape: {}\'.format(layer_name, layer_info[\'shape\']))\n'"
tools/evaluate_lanenet_on_tusimple.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-5-16 \xe4\xb8\x8b\xe5\x8d\x886:26\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : evaluate_lanenet_on_tusimple.py\n# @IDE: PyCharm\n""""""\nEvaluate lanenet model on tusimple lane dataset\n""""""\nimport argparse\nimport glob\nimport os\nimport os.path as ops\nimport time\n\nimport cv2\nimport glog as log\nimport numpy as np\nimport tensorflow as tf\nimport tqdm\n\nfrom config import global_config\nfrom lanenet_model import lanenet\nfrom lanenet_model import lanenet_postprocess\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--image_dir\', type=str, help=\'The source tusimple lane test data dir\')\n    parser.add_argument(\'--weights_path\', type=str, help=\'The model weights path\')\n    parser.add_argument(\'--save_dir\', type=str, help=\'The test output save root dir\')\n\n    return parser.parse_args()\n\n\ndef test_lanenet_batch(src_dir, weights_path, save_dir):\n    """"""\n\n    :param src_dir:\n    :param weights_path:\n    :param save_dir:\n    :return:\n    """"""\n    assert ops.exists(src_dir), \'{:s} not exist\'.format(src_dir)\n\n    os.makedirs(save_dir, exist_ok=True)\n\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 256, 512, 3], name=\'input_tensor\')\n\n    net = lanenet.LaneNet(phase=\'test\', net_flag=\'vgg\')\n    binary_seg_ret, instance_seg_ret = net.inference(input_tensor=input_tensor, name=\'lanenet_model\')\n\n    postprocessor = lanenet_postprocess.LaneNetPostProcessor()\n\n    saver = tf.train.Saver()\n\n    # Set sess configuration\n    sess_config = tf.ConfigProto()\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TEST.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n    sess_config.gpu_options.allocator_type = \'BFC\'\n\n    sess = tf.Session(config=sess_config)\n\n    with sess.as_default():\n\n        saver.restore(sess=sess, save_path=weights_path)\n\n        image_list = glob.glob(\'{:s}/**/*.jpg\'.format(src_dir), recursive=True)\n        avg_time_cost = []\n        for index, image_path in tqdm.tqdm(enumerate(image_list), total=len(image_list)):\n\n            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n            image_vis = image\n            image = cv2.resize(image, (512, 256), interpolation=cv2.INTER_LINEAR)\n            image = image / 127.5 - 1.0\n\n            t_start = time.time()\n            binary_seg_image, instance_seg_image = sess.run(\n                [binary_seg_ret, instance_seg_ret],\n                feed_dict={input_tensor: [image]}\n            )\n            avg_time_cost.append(time.time() - t_start)\n\n            postprocess_result = postprocessor.postprocess(\n                binary_seg_result=binary_seg_image[0],\n                instance_seg_result=instance_seg_image[0],\n                source_image=image_vis\n            )\n\n            if index % 100 == 0:\n                log.info(\'Mean inference time every single image: {:.5f}s\'.format(np.mean(avg_time_cost)))\n                avg_time_cost.clear()\n\n            input_image_dir = ops.split(image_path.split(\'clips\')[1])[0][1:]\n            input_image_name = ops.split(image_path)[1]\n            output_image_dir = ops.join(save_dir, input_image_dir)\n            os.makedirs(output_image_dir, exist_ok=True)\n            output_image_path = ops.join(output_image_dir, input_image_name)\n            if ops.exists(output_image_path):\n                continue\n\n            cv2.imwrite(output_image_path, postprocess_result[\'source_image\'])\n\n    return\n\n\nif __name__ == \'__main__\':\n    """"""\n    test code\n    """"""\n    # init args\n    args = init_args()\n\n    test_lanenet_batch(\n        src_dir=args.image_dir,\n        weights_path=args.weights_path,\n        save_dir=args.save_dir\n    )\n'"
tools/evaluate_model_utils.py,24,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-1-21 \xe4\xb8\x8a\xe5\x8d\x8811:17\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : evaluate_model_utils.py\n# @IDE: PyCharm\n""""""\nCalculate model\'s fp fn and precision\n""""""\nimport tensorflow as tf\n\n\ndef calculate_model_precision(input_tensor, label_tensor):\n    """"""\n    calculate accuracy acc = correct_nums / ground_truth_nums\n    :param input_tensor: binary segmentation logits\n    :param label_tensor: binary segmentation label\n    :return:\n    """"""\n\n    logits = tf.nn.softmax(logits=input_tensor)\n    final_output = tf.expand_dims(tf.argmax(logits, axis=-1), axis=-1)\n\n    idx = tf.where(tf.equal(final_output, 1))\n    pix_cls_ret = tf.gather_nd(label_tensor, idx)\n    accuracy = tf.count_nonzero(pix_cls_ret)\n    accuracy = tf.divide(\n        accuracy,\n        tf.cast(tf.shape(tf.gather_nd(label_tensor, tf.where(tf.equal(label_tensor, 1))))[0], tf.int64))\n\n    return accuracy\n\n\ndef calculate_model_fp(input_tensor, label_tensor):\n    """"""\n    calculate fp figure\n    :param input_tensor:\n    :param label_tensor:\n    :return:\n    """"""\n    logits = tf.nn.softmax(logits=input_tensor)\n    final_output = tf.expand_dims(tf.argmax(logits, axis=-1), axis=-1)\n\n    idx = tf.where(tf.equal(final_output, 1))\n    pix_cls_ret = tf.gather_nd(final_output, idx)\n    false_pred = tf.cast(tf.shape(pix_cls_ret)[0], tf.int64) - tf.count_nonzero(\n        tf.gather_nd(label_tensor, idx)\n    )\n\n    return tf.divide(false_pred, tf.cast(tf.shape(pix_cls_ret)[0], tf.int64))\n\n\ndef calculate_model_fn(input_tensor, label_tensor):\n    """"""\n    calculate fn figure\n    :param input_tensor:\n    :param label_tensor:\n    :return:\n    """"""\n    logits = tf.nn.softmax(logits=input_tensor)\n    final_output = tf.expand_dims(tf.argmax(logits, axis=-1), axis=-1)\n\n    idx = tf.where(tf.equal(label_tensor, 1))\n    pix_cls_ret = tf.gather_nd(final_output, idx)\n    label_cls_ret = tf.gather_nd(label_tensor, tf.where(tf.equal(label_tensor, 1)))\n    mis_pred = tf.cast(tf.shape(label_cls_ret)[0], tf.int64) - tf.count_nonzero(pix_cls_ret)\n\n    return tf.divide(mis_pred, tf.cast(tf.shape(label_cls_ret)[0], tf.int64))\n\n\ndef get_image_summary(img):\n    """"""\n    Make an image summary for 4d tensor image with index idx\n    :param img:\n    """"""\n\n    if len(img.get_shape().as_list()) == 3:\n        img = tf.expand_dims(img, -1)\n\n    image = img - tf.reduce_min(img)\n    image /= tf.reduce_max(img) - tf.reduce_min(img)\n    image *= 255\n\n    return image\n'"
tools/generate_tusimple_dataset.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-18 \xe4\xb8\x8b\xe5\x8d\x887:31\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : generate_tusimple_dataset.py\n# @IDE: PyCharm Community Edition\n""""""\ngenerate tusimple training dataset\n""""""\nimport argparse\nimport glob\nimport json\nimport os\nimport os.path as ops\nimport shutil\n\nimport cv2\nimport numpy as np\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--src_dir\', type=str, help=\'The origin path of unzipped tusimple dataset\')\n\n    return parser.parse_args()\n\n\ndef process_json_file(json_file_path, src_dir, ori_dst_dir, binary_dst_dir, instance_dst_dir):\n    """"""\n\n    :param json_file_path:\n    :param src_dir: origin clip file path\n    :param ori_dst_dir:\n    :param binary_dst_dir:\n    :param instance_dst_dir:\n    :return:\n    """"""\n    assert ops.exists(json_file_path), \'{:s} not exist\'.format(json_file_path)\n\n    image_nums = len(os.listdir(ori_dst_dir))\n\n    with open(json_file_path, \'r\') as file:\n        for line_index, line in enumerate(file):\n            info_dict = json.loads(line)\n\n            image_dir = ops.split(info_dict[\'raw_file\'])[0]\n            image_dir_split = image_dir.split(\'/\')[1:]\n            image_dir_split.append(ops.split(info_dict[\'raw_file\'])[1])\n            image_name = \'_\'.join(image_dir_split)\n            image_path = ops.join(src_dir, info_dict[\'raw_file\'])\n            assert ops.exists(image_path), \'{:s} not exist\'.format(image_path)\n\n            h_samples = info_dict[\'h_samples\']\n            lanes = info_dict[\'lanes\']\n\n            image_name_new = \'{:s}.png\'.format(\'{:d}\'.format(line_index + image_nums).zfill(4))\n\n            src_image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n            dst_binary_image = np.zeros([src_image.shape[0], src_image.shape[1]], np.uint8)\n            dst_instance_image = np.zeros([src_image.shape[0], src_image.shape[1]], np.uint8)\n\n            for lane_index, lane in enumerate(lanes):\n                assert len(h_samples) == len(lane)\n                lane_x = []\n                lane_y = []\n                for index in range(len(lane)):\n                    if lane[index] == -2:\n                        continue\n                    else:\n                        ptx = lane[index]\n                        pty = h_samples[index]\n                        lane_x.append(ptx)\n                        lane_y.append(pty)\n                if not lane_x:\n                    continue\n                lane_pts = np.vstack((lane_x, lane_y)).transpose()\n                lane_pts = np.array([lane_pts], np.int64)\n\n                cv2.polylines(dst_binary_image, lane_pts, isClosed=False,\n                              color=255, thickness=5)\n                cv2.polylines(dst_instance_image, lane_pts, isClosed=False,\n                              color=lane_index * 50 + 20, thickness=5)\n\n            dst_binary_image_path = ops.join(binary_dst_dir, image_name_new)\n            dst_instance_image_path = ops.join(instance_dst_dir, image_name_new)\n            dst_rgb_image_path = ops.join(ori_dst_dir, image_name_new)\n\n            cv2.imwrite(dst_binary_image_path, dst_binary_image)\n            cv2.imwrite(dst_instance_image_path, dst_instance_image)\n            cv2.imwrite(dst_rgb_image_path, src_image)\n\n            print(\'Process {:s} success\'.format(image_name))\n\n\ndef gen_train_sample(src_dir, b_gt_image_dir, i_gt_image_dir, image_dir):\n    """"""\n    generate sample index file\n    :param src_dir:\n    :param b_gt_image_dir:\n    :param i_gt_image_dir:\n    :param image_dir:\n    :return:\n    """"""\n\n    with open(\'{:s}/training/train.txt\'.format(src_dir), \'w\') as file:\n\n        for image_name in os.listdir(b_gt_image_dir):\n            if not image_name.endswith(\'.png\'):\n                continue\n\n            binary_gt_image_path = ops.join(b_gt_image_dir, image_name)\n            instance_gt_image_path = ops.join(i_gt_image_dir, image_name)\n            image_path = ops.join(image_dir, image_name)\n\n            assert ops.exists(image_path), \'{:s} not exist\'.format(image_path)\n            assert ops.exists(instance_gt_image_path), \'{:s} not exist\'.format(instance_gt_image_path)\n\n            b_gt_image = cv2.imread(binary_gt_image_path, cv2.IMREAD_COLOR)\n            i_gt_image = cv2.imread(instance_gt_image_path, cv2.IMREAD_COLOR)\n            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n\n            if b_gt_image is None or image is None or i_gt_image is None:\n                print(\'\xe5\x9b\xbe\xe5\x83\x8f\xe5\xaf\xb9: {:s}\xe6\x8d\x9f\xe5\x9d\x8f\'.format(image_name))\n                continue\n            else:\n                info = \'{:s} {:s} {:s}\'.format(image_path, binary_gt_image_path, instance_gt_image_path)\n                file.write(info + \'\\n\')\n    return\n\n\ndef process_tusimple_dataset(src_dir):\n    """"""\n\n    :param src_dir:\n    :return:\n    """"""\n    traing_folder_path = ops.join(src_dir, \'training\')\n    testing_folder_path = ops.join(src_dir, \'testing\')\n\n    os.makedirs(traing_folder_path, exist_ok=True)\n    os.makedirs(testing_folder_path, exist_ok=True)\n\n    for json_label_path in glob.glob(\'{:s}/label*.json\'.format(src_dir)):\n        json_label_name = ops.split(json_label_path)[1]\n\n        shutil.copyfile(json_label_path, ops.join(traing_folder_path, json_label_name))\n\n    for json_label_path in glob.glob(\'{:s}/test*.json\'.format(src_dir)):\n        json_label_name = ops.split(json_label_path)[1]\n\n        shutil.copyfile(json_label_path, ops.join(testing_folder_path, json_label_name))\n\n    gt_image_dir = ops.join(traing_folder_path, \'gt_image\')\n    gt_binary_dir = ops.join(traing_folder_path, \'gt_binary_image\')\n    gt_instance_dir = ops.join(traing_folder_path, \'gt_instance_image\')\n\n    os.makedirs(gt_image_dir, exist_ok=True)\n    os.makedirs(gt_binary_dir, exist_ok=True)\n    os.makedirs(gt_instance_dir, exist_ok=True)\n\n    for json_label_path in glob.glob(\'{:s}/*.json\'.format(traing_folder_path)):\n        process_json_file(json_label_path, src_dir, gt_image_dir, gt_binary_dir, gt_instance_dir)\n\n    gen_train_sample(src_dir, gt_binary_dir, gt_instance_dir, gt_image_dir)\n\n    return\n\n\nif __name__ == \'__main__\':\n    args = init_args()\n\n    process_tusimple_dataset(args.src_dir)\n'"
tools/test_lanenet.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-23 \xe4\xb8\x8a\xe5\x8d\x8811:33\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : test_lanenet.py\n# @IDE: PyCharm Community Edition\n""""""\ntest LaneNet model on single image\n""""""\nimport argparse\nimport os.path as ops\nimport time\n\nimport cv2\nimport glog as log\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nfrom config import global_config\nfrom lanenet_model import lanenet\nfrom lanenet_model import lanenet_postprocess\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--image_path\', type=str, help=\'The image path or the src image save dir\')\n    parser.add_argument(\'--weights_path\', type=str, help=\'The model weights path\')\n\n    return parser.parse_args()\n\n\ndef args_str2bool(arg_value):\n    """"""\n\n    :param arg_value:\n    :return:\n    """"""\n    if arg_value.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n\n    elif arg_value.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Unsupported value encountered.\')\n\n\ndef minmax_scale(input_arr):\n    """"""\n\n    :param input_arr:\n    :return:\n    """"""\n    min_val = np.min(input_arr)\n    max_val = np.max(input_arr)\n\n    output_arr = (input_arr - min_val) * 255.0 / (max_val - min_val)\n\n    return output_arr\n\n\ndef test_lanenet(image_path, weights_path):\n    """"""\n\n    :param image_path:\n    :param weights_path:\n    :return:\n    """"""\n    assert ops.exists(image_path), \'{:s} not exist\'.format(image_path)\n\n    log.info(\'Start reading image and preprocessing\')\n    t_start = time.time()\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    image_vis = image\n    image = cv2.resize(image, (512, 256), interpolation=cv2.INTER_LINEAR)\n    image = image / 127.5 - 1.0\n    log.info(\'Image load complete, cost time: {:.5f}s\'.format(time.time() - t_start))\n\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=[1, 256, 512, 3], name=\'input_tensor\')\n\n    net = lanenet.LaneNet(phase=\'test\', net_flag=\'vgg\')\n    binary_seg_ret, instance_seg_ret = net.inference(input_tensor=input_tensor, name=\'lanenet_model\')\n\n    postprocessor = lanenet_postprocess.LaneNetPostProcessor()\n\n    saver = tf.train.Saver()\n\n    # Set sess configuration\n    sess_config = tf.ConfigProto()\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TEST.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n    sess_config.gpu_options.allocator_type = \'BFC\'\n\n    sess = tf.Session(config=sess_config)\n\n    with sess.as_default():\n\n        saver.restore(sess=sess, save_path=weights_path)\n\n        t_start = time.time()\n        binary_seg_image, instance_seg_image = sess.run(\n            [binary_seg_ret, instance_seg_ret],\n            feed_dict={input_tensor: [image]}\n        )\n        t_cost = time.time() - t_start\n        log.info(\'Single imgae inference cost time: {:.5f}s\'.format(t_cost))\n\n        postprocess_result = postprocessor.postprocess(\n            binary_seg_result=binary_seg_image[0],\n            instance_seg_result=instance_seg_image[0],\n            source_image=image_vis\n        )\n        mask_image = postprocess_result[\'mask_image\']\n\n        for i in range(CFG.TRAIN.EMBEDDING_FEATS_DIMS):\n            instance_seg_image[0][:, :, i] = minmax_scale(instance_seg_image[0][:, :, i])\n        embedding_image = np.array(instance_seg_image[0], np.uint8)\n\n        plt.figure(\'mask_image\')\n        plt.imshow(mask_image[:, :, (2, 1, 0)])\n        plt.figure(\'src_image\')\n        plt.imshow(image_vis[:, :, (2, 1, 0)])\n        plt.figure(\'instance_image\')\n        plt.imshow(embedding_image[:, :, (2, 1, 0)])\n        plt.figure(\'binary_image\')\n        plt.imshow(binary_seg_image[0] * 255, cmap=\'gray\')\n        plt.show()\n\n        cv2.imwrite(\'instance_mask_image.png\', mask_image)\n        cv2.imwrite(\'source_image.png\', postprocess_result[\'source_image\'])\n        cv2.imwrite(\'binary_mask_image.png\', binary_seg_image[0] * 255)\n\n    sess.close()\n\n    return\n\n\nif __name__ == \'__main__\':\n    """"""\n    test code\n    """"""\n    # init args\n    args = init_args()\n\n    test_lanenet(args.image_path, args.weights_path)\n'"
tools/train_lanenet.py,65,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 19-4-24 \xe4\xb8\x8b\xe5\x8d\x889:33\n# @Author  : MaybeShewill-CV\n# @Site    : https://github.com/MaybeShewill-CV/lanenet-lane-detection\n# @File    : train_lanenet.py\n# @IDE: PyCharm\n""""""\nTrain lanenet script\n""""""\nimport argparse\nimport math\nimport os\nimport os.path as ops\nimport time\n\nimport cv2\nimport glog as log\nimport numpy as np\nimport tensorflow as tf\n\nfrom config import global_config\nfrom data_provider import lanenet_data_feed_pipline\nfrom lanenet_model import lanenet\nfrom tools import evaluate_model_utils\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'-d\', \'--dataset_dir\', type=str,\n                        help=\'Lanenet Dataset dir\')\n    parser.add_argument(\'-w\', \'--weights_path\', type=str,\n                        help=\'Path to pre-trained weights to continue training\')\n    parser.add_argument(\'-m\', \'--multi_gpus\', type=args_str2bool, default=False,\n                        nargs=\'?\', const=True, help=\'Use multi gpus to train\')\n    parser.add_argument(\'--net_flag\', type=str, default=\'vgg\',\n                        help=\'The net flag which determins the net\\\'s architecture\')\n\n    return parser.parse_args()\n\n\ndef args_str2bool(arg_value):\n    """"""\n\n    :param arg_value:\n    :return:\n    """"""\n    if arg_value.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n\n    elif arg_value.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Unsupported value encountered.\')\n\n\ndef minmax_scale(input_arr):\n    """"""\n\n    :param input_arr:\n    :return:\n    """"""\n    min_val = np.min(input_arr)\n    max_val = np.max(input_arr)\n\n    output_arr = (input_arr - min_val) * 255.0 / (max_val - min_val)\n\n    return output_arr\n\n\ndef load_pretrained_weights(variables, pretrained_weights_path, sess):\n    """"""\n\n    :param variables:\n    :param pretrained_weights_path:\n    :param sess:\n    :return:\n    """"""\n    assert ops.exists(pretrained_weights_path), \'{:s} not exist\'.format(pretrained_weights_path)\n\n    pretrained_weights = np.load(\n        \'./data/vgg16.npy\', encoding=\'latin1\').item()\n\n    for vv in variables:\n        weights_key = vv.name.split(\'/\')[-3]\n        if \'conv5\' in weights_key:\n            weights_key = \'{:s}_{:s}\'.format(weights_key.split(\'_\')[0], weights_key.split(\'_\')[1])\n        try:\n            weights = pretrained_weights[weights_key][0]\n            _op = tf.assign(vv, weights)\n            sess.run(_op)\n        except Exception as _:\n            continue\n\n    return\n\n\ndef record_training_intermediate_result(gt_images, gt_binary_labels, gt_instance_labels,\n                                        binary_seg_images, pix_embeddings, flag=\'train\',\n                                        save_dir=\'./tmp\'):\n    """"""\n    record intermediate result during training process for monitoring\n    :param gt_images:\n    :param gt_binary_labels:\n    :param gt_instance_labels:\n    :param binary_seg_images:\n    :param pix_embeddings:\n    :param flag:\n    :param save_dir:\n    :return:\n    """"""\n    os.makedirs(save_dir, exist_ok=True)\n\n    for index, gt_image in enumerate(gt_images):\n        gt_image_name = \'{:s}_{:d}_gt_image.png\'.format(flag, index + 1)\n        gt_image_path = ops.join(save_dir, gt_image_name)\n        gt_image = (gt_images[index] + 1.0) * 127.5\n        cv2.imwrite(gt_image_path, np.array(gt_image, dtype=np.uint8))\n\n        gt_binary_label_name = \'{:s}_{:d}_gt_binary_label.png\'.format(flag, index + 1)\n        gt_binary_label_path = ops.join(save_dir, gt_binary_label_name)\n        cv2.imwrite(gt_binary_label_path, np.array(gt_binary_labels[index][:, :, 0] * 255, dtype=np.uint8))\n\n        gt_instance_label_name = \'{:s}_{:d}_gt_instance_label.png\'.format(flag, index + 1)\n        gt_instance_label_path = ops.join(save_dir, gt_instance_label_name)\n        cv2.imwrite(gt_instance_label_path, np.array(gt_instance_labels[index][:, :, 0], dtype=np.uint8))\n\n        gt_binary_seg_name = \'{:s}_{:d}_gt_binary_seg.png\'.format(flag, index + 1)\n        gt_binary_seg_path = ops.join(save_dir, gt_binary_seg_name)\n        cv2.imwrite(gt_binary_seg_path, np.array(binary_seg_images[index] * 255, dtype=np.uint8))\n\n        embedding_image_name = \'{:s}_{:d}_pix_embedding.png\'.format(flag, index + 1)\n        embedding_image_path = ops.join(save_dir, embedding_image_name)\n        embedding_image = pix_embeddings[index]\n        for i in range(CFG.TRAIN.EMBEDDING_FEATS_DIMS):\n            embedding_image[:, :, i] = minmax_scale(embedding_image[:, :, i])\n        embedding_image = np.array(embedding_image, np.uint8)\n        cv2.imwrite(embedding_image_path, embedding_image)\n\n    return\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n\n    return average_grads\n\n\ndef compute_net_gradients(gt_images, gt_binary_labels, gt_instance_labels,\n                          net, optimizer=None):\n    """"""\n    Calculate gradients for single GPU\n    :param gt_images:\n    :param gt_binary_labels:\n    :param gt_instance_labels:\n    :param net:\n    :param optimizer:\n    :return:\n    """"""\n\n    compute_ret = net.compute_loss(\n        input_tensor=gt_images, binary_label=gt_binary_labels,\n        instance_label=gt_instance_labels, name=\'lanenet_model\'\n    )\n    total_loss = compute_ret[\'total_loss\']\n\n    if optimizer is not None:\n        grads = optimizer.compute_gradients(total_loss)\n    else:\n        grads = None\n\n    return total_loss, grads\n\n\ndef train_lanenet(dataset_dir, weights_path=None, net_flag=\'vgg\'):\n    """"""\n\n    :param dataset_dir:\n    :param net_flag: choose which base network to use\n    :param weights_path:\n    :return:\n    """"""\n    train_dataset = lanenet_data_feed_pipline.LaneNetDataFeeder(\n        dataset_dir=dataset_dir, flags=\'train\'\n    )\n    val_dataset = lanenet_data_feed_pipline.LaneNetDataFeeder(\n        dataset_dir=dataset_dir, flags=\'val\'\n    )\n\n    with tf.device(\'/gpu:1\'):\n        # set lanenet\n        train_net = lanenet.LaneNet(net_flag=net_flag, phase=\'train\', reuse=False)\n        val_net = lanenet.LaneNet(net_flag=net_flag, phase=\'val\', reuse=True)\n\n        # set compute graph node for training\n        train_images, train_binary_labels, train_instance_labels = train_dataset.inputs(\n            CFG.TRAIN.BATCH_SIZE, 1\n        )\n\n        train_compute_ret = train_net.compute_loss(\n            input_tensor=train_images, binary_label=train_binary_labels,\n            instance_label=train_instance_labels, name=\'lanenet_model\'\n        )\n        train_total_loss = train_compute_ret[\'total_loss\']\n        train_binary_seg_loss = train_compute_ret[\'binary_seg_loss\']\n        train_disc_loss = train_compute_ret[\'discriminative_loss\']\n        train_pix_embedding = train_compute_ret[\'instance_seg_logits\']\n\n        train_prediction_logits = train_compute_ret[\'binary_seg_logits\']\n        train_prediction_score = tf.nn.softmax(logits=train_prediction_logits)\n        train_prediction = tf.argmax(train_prediction_score, axis=-1)\n\n        train_accuracy = evaluate_model_utils.calculate_model_precision(\n            train_compute_ret[\'binary_seg_logits\'], train_binary_labels\n        )\n        train_fp = evaluate_model_utils.calculate_model_fp(\n            train_compute_ret[\'binary_seg_logits\'], train_binary_labels\n        )\n        train_fn = evaluate_model_utils.calculate_model_fn(\n            train_compute_ret[\'binary_seg_logits\'], train_binary_labels\n        )\n        train_binary_seg_ret_for_summary = evaluate_model_utils.get_image_summary(\n            img=train_prediction\n        )\n        train_embedding_ret_for_summary = evaluate_model_utils.get_image_summary(\n            img=train_pix_embedding\n        )\n\n        train_cost_scalar = tf.summary.scalar(\n            name=\'train_cost\', tensor=train_total_loss\n        )\n        train_accuracy_scalar = tf.summary.scalar(\n            name=\'train_accuracy\', tensor=train_accuracy\n        )\n        train_binary_seg_loss_scalar = tf.summary.scalar(\n            name=\'train_binary_seg_loss\', tensor=train_binary_seg_loss\n        )\n        train_instance_seg_loss_scalar = tf.summary.scalar(\n            name=\'train_instance_seg_loss\', tensor=train_disc_loss\n        )\n        train_fn_scalar = tf.summary.scalar(\n            name=\'train_fn\', tensor=train_fn\n        )\n        train_fp_scalar = tf.summary.scalar(\n            name=\'train_fp\', tensor=train_fp\n        )\n        train_binary_seg_ret_img = tf.summary.image(\n            name=\'train_binary_seg_ret\', tensor=train_binary_seg_ret_for_summary\n        )\n        train_embedding_feats_ret_img = tf.summary.image(\n            name=\'train_embedding_feats_ret\', tensor=train_embedding_ret_for_summary\n        )\n        train_merge_summary_op = tf.summary.merge(\n            [train_accuracy_scalar, train_cost_scalar, train_binary_seg_loss_scalar,\n             train_instance_seg_loss_scalar, train_fn_scalar, train_fp_scalar,\n             train_binary_seg_ret_img, train_embedding_feats_ret_img]\n        )\n\n        # set compute graph node for validation\n        val_images, val_binary_labels, val_instance_labels = val_dataset.inputs(\n            CFG.TRAIN.VAL_BATCH_SIZE, 1\n        )\n\n        val_compute_ret = val_net.compute_loss(\n            input_tensor=val_images, binary_label=val_binary_labels,\n            instance_label=val_instance_labels, name=\'lanenet_model\'\n        )\n        val_total_loss = val_compute_ret[\'total_loss\']\n        val_binary_seg_loss = val_compute_ret[\'binary_seg_loss\']\n        val_disc_loss = val_compute_ret[\'discriminative_loss\']\n        val_pix_embedding = val_compute_ret[\'instance_seg_logits\']\n\n        val_prediction_logits = val_compute_ret[\'binary_seg_logits\']\n        val_prediction_score = tf.nn.softmax(logits=val_prediction_logits)\n        val_prediction = tf.argmax(val_prediction_score, axis=-1)\n\n        val_accuracy = evaluate_model_utils.calculate_model_precision(\n            val_compute_ret[\'binary_seg_logits\'], val_binary_labels\n        )\n        val_fp = evaluate_model_utils.calculate_model_fp(\n            val_compute_ret[\'binary_seg_logits\'], val_binary_labels\n        )\n        val_fn = evaluate_model_utils.calculate_model_fn(\n            val_compute_ret[\'binary_seg_logits\'], val_binary_labels\n        )\n        val_binary_seg_ret_for_summary = evaluate_model_utils.get_image_summary(\n            img=val_prediction\n        )\n        val_embedding_ret_for_summary = evaluate_model_utils.get_image_summary(\n            img=val_pix_embedding\n        )\n\n        val_cost_scalar = tf.summary.scalar(\n            name=\'val_cost\', tensor=val_total_loss\n        )\n        val_accuracy_scalar = tf.summary.scalar(\n            name=\'val_accuracy\', tensor=val_accuracy\n        )\n        val_binary_seg_loss_scalar = tf.summary.scalar(\n            name=\'val_binary_seg_loss\', tensor=val_binary_seg_loss\n        )\n        val_instance_seg_loss_scalar = tf.summary.scalar(\n            name=\'val_instance_seg_loss\', tensor=val_disc_loss\n        )\n        val_fn_scalar = tf.summary.scalar(\n            name=\'val_fn\', tensor=val_fn\n        )\n        val_fp_scalar = tf.summary.scalar(\n            name=\'val_fp\', tensor=val_fp\n        )\n        val_binary_seg_ret_img = tf.summary.image(\n            name=\'val_binary_seg_ret\', tensor=val_binary_seg_ret_for_summary\n        )\n        val_embedding_feats_ret_img = tf.summary.image(\n            name=\'val_embedding_feats_ret\', tensor=val_embedding_ret_for_summary\n        )\n        val_merge_summary_op = tf.summary.merge(\n            [val_accuracy_scalar, val_cost_scalar, val_binary_seg_loss_scalar,\n             val_instance_seg_loss_scalar, val_fn_scalar, val_fp_scalar,\n             val_binary_seg_ret_img, val_embedding_feats_ret_img]\n        )\n\n        # set optimizer\n        global_step = tf.Variable(0, trainable=False)\n        learning_rate = tf.train.polynomial_decay(\n            learning_rate=CFG.TRAIN.LEARNING_RATE,\n            global_step=global_step,\n            decay_steps=CFG.TRAIN.EPOCHS,\n            power=0.9\n        )\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            optimizer = tf.train.MomentumOptimizer(\n                learning_rate=learning_rate, momentum=CFG.TRAIN.MOMENTUM).minimize(\n                loss=train_total_loss,\n                var_list=tf.trainable_variables(),\n                global_step=global_step\n            )\n\n    # Set tf model save path\n    model_save_dir = \'model/tusimple_lanenet_{:s}\'.format(net_flag)\n    os.makedirs(model_save_dir, exist_ok=True)\n    train_start_time = time.strftime(\'%Y-%m-%d-%H-%M-%S\', time.localtime(time.time()))\n    model_name = \'tusimple_lanenet_{:s}_{:s}.ckpt\'.format(net_flag, str(train_start_time))\n    model_save_path = ops.join(model_save_dir, model_name)\n    saver = tf.train.Saver()\n\n    # Set tf summary save path\n    tboard_save_path = \'tboard/tusimple_lanenet_{:s}\'.format(net_flag)\n    os.makedirs(tboard_save_path, exist_ok=True)\n\n    # Set sess configuration\n    sess_config = tf.ConfigProto(allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n    sess_config.gpu_options.allocator_type = \'BFC\'\n\n    sess = tf.Session(config=sess_config)\n\n    summary_writer = tf.summary.FileWriter(tboard_save_path)\n    summary_writer.add_graph(sess.graph)\n\n    # Set the training parameters\n    train_epochs = CFG.TRAIN.EPOCHS\n\n    log.info(\'Global configuration is as follows:\')\n    log.info(CFG)\n\n    with sess.as_default():\n\n        if weights_path is None:\n            log.info(\'Training from scratch\')\n            init = tf.global_variables_initializer()\n            sess.run(init)\n        else:\n            log.info(\'Restore model from last model checkpoint {:s}\'.format(weights_path))\n            saver.restore(sess=sess, save_path=weights_path)\n\n        if net_flag == \'vgg\' and weights_path is None:\n            load_pretrained_weights(tf.trainable_variables(), \'./data/vgg16.npy\', sess)\n\n        train_cost_time_mean = []\n        for epoch in range(train_epochs):\n            # training part\n            t_start = time.time()\n\n            _, train_c, train_accuracy_figure, train_fn_figure, train_fp_figure, \\\n                lr, train_summary, train_binary_loss, \\\n                train_instance_loss, train_embeddings, train_binary_seg_imgs, train_gt_imgs, \\\n                train_binary_gt_labels, train_instance_gt_labels = \\\n                sess.run([optimizer, train_total_loss, train_accuracy, train_fn, train_fp,\n                          learning_rate, train_merge_summary_op, train_binary_seg_loss,\n                          train_disc_loss, train_pix_embedding, train_prediction,\n                          train_images, train_binary_labels, train_instance_labels])\n\n            if math.isnan(train_c) or math.isnan(train_binary_loss) or math.isnan(train_instance_loss):\n                log.error(\'cost is: {:.5f}\'.format(train_c))\n                log.error(\'binary cost is: {:.5f}\'.format(train_binary_loss))\n                log.error(\'instance cost is: {:.5f}\'.format(train_instance_loss))\n                return\n\n            if epoch % 100 == 0:\n                record_training_intermediate_result(\n                    gt_images=train_gt_imgs, gt_binary_labels=train_binary_gt_labels,\n                    gt_instance_labels=train_instance_gt_labels, binary_seg_images=train_binary_seg_imgs,\n                    pix_embeddings=train_embeddings\n                )\n            summary_writer.add_summary(summary=train_summary, global_step=epoch)\n\n            if epoch % CFG.TRAIN.DISPLAY_STEP == 0:\n                log.info(\'Epoch: {:d} total_loss= {:6f} binary_seg_loss= {:6f} \'\n                         \'instance_seg_loss= {:6f} accuracy= {:6f} fp= {:6f} fn= {:6f}\'\n                         \' lr= {:6f} mean_cost_time= {:5f}s \'.\n                         format(epoch + 1, train_c, train_binary_loss, train_instance_loss, train_accuracy_figure,\n                                train_fp_figure, train_fn_figure, lr, np.mean(train_cost_time_mean)))\n                train_cost_time_mean.clear()\n\n            # validation part\n            val_c, val_accuracy_figure, val_fn_figure, val_fp_figure, \\\n                val_summary, val_binary_loss, val_instance_loss, \\\n                val_embeddings, val_binary_seg_imgs, val_gt_imgs, \\\n                val_binary_gt_labels, val_instance_gt_labels = \\\n                sess.run([val_total_loss, val_accuracy, val_fn, val_fp,\n                          val_merge_summary_op, val_binary_seg_loss,\n                          val_disc_loss, val_pix_embedding, val_prediction,\n                          val_images, val_binary_labels, val_instance_labels])\n\n            if math.isnan(val_c) or math.isnan(val_binary_loss) or math.isnan(val_instance_loss):\n                log.error(\'cost is: {:.5f}\'.format(val_c))\n                log.error(\'binary cost is: {:.5f}\'.format(val_binary_loss))\n                log.error(\'instance cost is: {:.5f}\'.format(val_instance_loss))\n                return\n\n            if epoch % 100 == 0:\n                record_training_intermediate_result(\n                    gt_images=val_gt_imgs, gt_binary_labels=val_binary_gt_labels,\n                    gt_instance_labels=val_instance_gt_labels, binary_seg_images=val_binary_seg_imgs,\n                    pix_embeddings=val_embeddings, flag=\'val\'\n                )\n\n            cost_time = time.time() - t_start\n            train_cost_time_mean.append(cost_time)\n            summary_writer.add_summary(summary=val_summary, global_step=epoch)\n\n            if epoch % CFG.TRAIN.VAL_DISPLAY_STEP == 0:\n                log.info(\'Epoch_Val: {:d} total_loss= {:6f} binary_seg_loss= {:6f} \'\n                         \'instance_seg_loss= {:6f} accuracy= {:6f} fp= {:6f} fn= {:6f}\'\n                         \' mean_cost_time= {:5f}s \'.\n                         format(epoch + 1, val_c, val_binary_loss, val_instance_loss, val_accuracy_figure,\n                                val_fp_figure, val_fn_figure, np.mean(train_cost_time_mean)))\n                train_cost_time_mean.clear()\n\n            if epoch % 2000 == 0:\n                saver.save(sess=sess, save_path=model_save_path, global_step=global_step)\n\n    return\n\n\ndef train_lanenet_multi_gpu(dataset_dir, weights_path=None, net_flag=\'vgg\'):\n    """"""\n    train lanenet with multi gpu\n    :param dataset_dir:\n    :param weights_path:\n    :param net_flag:\n    :return:\n    """"""\n    # set lanenet dataset\n    train_dataset = lanenet_data_feed_pipline.LaneNetDataFeeder(\n        dataset_dir=dataset_dir, flags=\'train\'\n    )\n    val_dataset = lanenet_data_feed_pipline.LaneNetDataFeeder(\n        dataset_dir=dataset_dir, flags=\'val\'\n    )\n\n    # set lanenet\n    train_net = lanenet.LaneNet(net_flag=net_flag, phase=\'train\', reuse=False)\n    val_net = lanenet.LaneNet(net_flag=net_flag, phase=\'val\', reuse=True)\n\n    # set compute graph node\n    train_images, train_binary_labels, train_instance_labels = train_dataset.inputs(\n        CFG.TRAIN.BATCH_SIZE, 1\n    )\n    val_images, val_binary_labels, val_instance_labels = val_dataset.inputs(\n        CFG.TRAIN.VAL_BATCH_SIZE, 1\n    )\n\n    # set average container\n    tower_grads = []\n    train_tower_loss = []\n    val_tower_loss = []\n    batchnorm_updates = None\n    train_summary_op_updates = None\n\n    # set lr\n    global_step = tf.Variable(0, trainable=False)\n    learning_rate = tf.train.polynomial_decay(\n        learning_rate=CFG.TRAIN.LEARNING_RATE,\n        global_step=global_step,\n        decay_steps=CFG.TRAIN.EPOCHS,\n        power=0.9\n    )\n\n    # set optimizer\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=learning_rate, momentum=CFG.TRAIN.MOMENTUM\n    )\n\n    # set distributed train op\n    with tf.variable_scope(tf.get_variable_scope()):\n        for i in range(CFG.TRAIN.GPU_NUM):\n            with tf.device(\'/gpu:{:d}\'.format(i)):\n                with tf.name_scope(\'tower_{:d}\'.format(i)) as _:\n                    train_loss, grads = compute_net_gradients(\n                        train_images, train_binary_labels, train_instance_labels, train_net, optimizer\n                    )\n\n                    # Only use the mean and var in the first gpu tower to update the parameter\n                    if i == 0:\n                        batchnorm_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n                        train_summary_op_updates = tf.get_collection(tf.GraphKeys.SUMMARIES)\n                    tower_grads.append(grads)\n                    train_tower_loss.append(train_loss)\n\n                with tf.name_scope(\'validation_{:d}\'.format(i)) as _:\n                    val_loss, _ = compute_net_gradients(\n                        val_images, val_binary_labels, val_instance_labels, val_net, optimizer)\n                    val_tower_loss.append(val_loss)\n\n    grads = average_gradients(tower_grads)\n    avg_train_loss = tf.reduce_mean(train_tower_loss)\n    avg_val_loss = tf.reduce_mean(val_tower_loss)\n\n    # Track the moving averages of all trainable variables\n    variable_averages = tf.train.ExponentialMovingAverage(\n        CFG.TRAIN.MOVING_AVERAGE_DECAY, num_updates=global_step)\n    variables_to_average = tf.trainable_variables() + tf.moving_average_variables()\n    variables_averages_op = variable_averages.apply(variables_to_average)\n\n    # Group all the op needed for training\n    batchnorm_updates_op = tf.group(*batchnorm_updates)\n    apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n    train_op = tf.group(apply_gradient_op, variables_averages_op,\n                        batchnorm_updates_op)\n\n    # Set tf summary save path\n    tboard_save_path = \'tboard/tusimple_lanenet_multi_gpu_{:s}\'.format(net_flag)\n    os.makedirs(tboard_save_path, exist_ok=True)\n\n    summary_writer = tf.summary.FileWriter(tboard_save_path)\n\n    avg_train_loss_scalar = tf.summary.scalar(\n        name=\'average_train_loss\', tensor=avg_train_loss\n    )\n    avg_val_loss_scalar = tf.summary.scalar(\n        name=\'average_val_loss\', tensor=avg_val_loss\n    )\n    learning_rate_scalar = tf.summary.scalar(\n        name=\'learning_rate_scalar\', tensor=learning_rate\n    )\n\n    train_merge_summary_op = tf.summary.merge(\n        [avg_train_loss_scalar, learning_rate_scalar] + train_summary_op_updates\n    )\n    val_merge_summary_op = tf.summary.merge([avg_val_loss_scalar])\n\n    # set tensorflow saver\n    saver = tf.train.Saver()\n    model_save_dir = \'model/tusimple_lanenet_multi_gpu_{:s}\'.format(net_flag)\n    os.makedirs(model_save_dir, exist_ok=True)\n    train_start_time = time.strftime(\'%Y-%m-%d-%H-%M-%S\', time.localtime(time.time()))\n    model_name = \'tusimple_lanenet_{:s}_{:s}.ckpt\'.format(net_flag, str(train_start_time))\n    model_save_path = ops.join(model_save_dir, model_name)\n\n    # set sess config\n    sess_config = tf.ConfigProto(device_count={\'GPU\': CFG.TRAIN.GPU_NUM}, allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n    sess_config.gpu_options.allocator_type = \'BFC\'\n\n    # Set the training parameters\n    train_epochs = CFG.TRAIN.EPOCHS\n\n    log.info(\'Global configuration is as follows:\')\n    log.info(CFG)\n\n    sess = tf.Session(config=sess_config)\n\n    summary_writer.add_graph(sess.graph)\n\n    with sess.as_default():\n\n        tf.train.write_graph(\n            graph_or_graph_def=sess.graph, logdir=\'\',\n            name=\'{:s}/lanenet_model.pb\'.format(model_save_dir))\n\n        if weights_path is None:\n            log.info(\'Training from scratch\')\n            init = tf.global_variables_initializer()\n            sess.run(init)\n        else:\n            log.info(\'Restore model from last model checkpoint {:s}\'.format(weights_path))\n            saver.restore(sess=sess, save_path=weights_path)\n\n        train_cost_time_mean = []\n        val_cost_time_mean = []\n\n        for epoch in range(train_epochs):\n\n            # training part\n            t_start = time.time()\n\n            _, train_loss_value, train_summary, lr = \\\n                sess.run(\n                    fetches=[train_op, avg_train_loss,\n                             train_merge_summary_op, learning_rate]\n                )\n\n            if math.isnan(train_loss_value):\n                log.error(\'Train loss is nan\')\n                return\n\n            cost_time = time.time() - t_start\n            train_cost_time_mean.append(cost_time)\n\n            summary_writer.add_summary(summary=train_summary, global_step=epoch)\n\n            # validation part\n            t_start_val = time.time()\n\n            val_loss_value, val_summary = \\\n                sess.run(fetches=[avg_val_loss, val_merge_summary_op])\n\n            summary_writer.add_summary(val_summary, global_step=epoch)\n\n            cost_time_val = time.time() - t_start_val\n            val_cost_time_mean.append(cost_time_val)\n\n            if epoch % CFG.TRAIN.DISPLAY_STEP == 0:\n                log.info(\'Epoch_Train: {:d} total_loss= {:6f} \'\n                         \'lr= {:6f} mean_cost_time= {:5f}s \'.\n                         format(epoch + 1,\n                                train_loss_value,\n                                lr,\n                                np.mean(train_cost_time_mean))\n                         )\n                train_cost_time_mean.clear()\n\n            if epoch % CFG.TRAIN.VAL_DISPLAY_STEP == 0:\n                log.info(\'Epoch_Val: {:d} total_loss= {:6f}\'\n                         \' mean_cost_time= {:5f}s \'.\n                         format(epoch + 1,\n                                val_loss_value,\n                                np.mean(val_cost_time_mean))\n                         )\n                val_cost_time_mean.clear()\n\n            if epoch % 2000 == 0:\n                saver.save(sess=sess, save_path=model_save_path, global_step=epoch)\n    return\n\n\nif __name__ == \'__main__\':\n    # init args\n    args = init_args()\n\n    if CFG.TRAIN.GPU_NUM < 2:\n        args.use_multi_gpu = False\n\n    # train lanenet\n    if not args.multi_gpus:\n        train_lanenet(args.dataset_dir, args.weights_path, net_flag=args.net_flag)\n    else:\n        train_lanenet_multi_gpu(args.dataset_dir, args.weights_path, net_flag=args.net_flag)\n'"
