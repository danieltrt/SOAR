file_path,api_count,code
mnist.py,48,"b'from __future__ import print_function\nimport shutil\nimport os.path\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nEXPORT_DIR = \'./model\'\n\nif os.path.exists(EXPORT_DIR):\n    shutil.rmtree(EXPORT_DIR)\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\n# Parameters\nlearning_rate = 0.001\ntraining_iters = 200000\nbatch_size = 128\ndisplay_step = 10\n\n# Network Parameters\nn_input = 784  # MNIST data input (img shape: 28*28)\nn_classes = 10  # MNIST total classes (0-9 digits)\ndropout = 0.75  # Dropout, probability to keep units\n\n# tf Graph input\nx = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\nkeep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\n\n\n# Create some wrappers for simplicity\ndef conv2d(x, W, b, strides=1):\n    # Conv2D wrapper, with bias and relu activation\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=\'SAME\')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\n\ndef maxpool2d(x, k=2):\n    # MaxPool2D wrapper\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n                          padding=\'SAME\')\n\n\n# Create Model\ndef conv_net(x, weights, biases, dropout):\n    # Reshape input picture\n    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n\n    # Convolution Layer\n    conv1 = conv2d(x, weights[\'wc1\'], biases[\'bc1\'])\n    # Max Pooling (down-sampling)\n    conv1 = maxpool2d(conv1, k=2)\n\n    # Convolution Layer\n    conv2 = conv2d(conv1, weights[\'wc2\'], biases[\'bc2\'])\n    # Max Pooling (down-sampling)\n    conv2 = maxpool2d(conv2, k=2)\n\n    # Fully connected layer\n    # Reshape conv2 output to fit fully connected layer input\n    fc1 = tf.reshape(conv2, [-1, weights[\'wd1\'].get_shape().as_list()[0]])\n    fc1 = tf.add(tf.matmul(fc1, weights[\'wd1\']), biases[\'bd1\'])\n    fc1 = tf.nn.relu(fc1)\n    # Apply Dropout\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n    # Output, class prediction\n    out = tf.add(tf.matmul(fc1, weights[\'out\']), biases[\'out\'])\n    return out\n\n\n# Store layers weight & bias\nweights = {\n    # 5x5 conv, 1 input, 32 outputs\n    \'wc1\': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n    # 5x5 conv, 32 inputs, 64 outputs\n    \'wc2\': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n    # fully connected, 7*7*64 inputs, 1024 outputs\n    \'wd1\': tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),\n    # 1024 inputs, 10 outputs (class prediction)\n    \'out\': tf.Variable(tf.random_normal([1024, n_classes]))\n}\n\nbiases = {\n    \'bc1\': tf.Variable(tf.random_normal([32])),\n    \'bc2\': tf.Variable(tf.random_normal([64])),\n    \'bd1\': tf.Variable(tf.random_normal([1024])),\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n# Construct model\npred = conv_net(x, weights, biases, keep_prob)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.initialize_all_variables()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n    # Keep training until reach max iterations\n    while step * batch_size < training_iters:\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        # Run optimization op (backprop)\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n                                       keep_prob: dropout})\n        if step % display_step == 0:\n            # Calculate batch loss and accuracy\n            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n                                                              y: batch_y,\n                                                              keep_prob: 1.})\n            print(""Iter "" + str(step * batch_size) + "", Minibatch Loss= "" + \\\n                  ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \\\n                  ""{:.5f}"".format(acc))\n        step += 1\n    print(""Optimization Finished!"")\n\n    # Calculate accuracy for 256 mnist test images\n    print(""Testing Accuracy:"", \\\n          sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n                                        y: mnist.test.labels[:256],\n                                        keep_prob: 1.}))\n    WC1 = weights[\'wc1\'].eval(sess)\n    BC1 = biases[\'bc1\'].eval(sess)\n    WC2 = weights[\'wc2\'].eval(sess)\n    BC2 = biases[\'bc2\'].eval(sess)\n    WD1 = weights[\'wd1\'].eval(sess)\n    BD1 = biases[\'bd1\'].eval(sess)\n    W_OUT = weights[\'out\'].eval(sess)\n    B_OUT = biases[\'out\'].eval(sess)\n\n# Create new graph for exporting\ng = tf.Graph()\nwith g.as_default():\n    x_2 = tf.placeholder(""float"", shape=[None, 784], name=""input"")\n\n    WC1 = tf.constant(WC1, name=""WC1"")\n    BC1 = tf.constant(BC1, name=""BC1"")\n    x_image = tf.reshape(x_2, [-1, 28, 28, 1])\n    CONV1 = conv2d(x_image, WC1, BC1)\n    MAXPOOL1 = maxpool2d(CONV1, k=2)\n\n    WC2 = tf.constant(WC2, name=""WC2"")\n    BC2 = tf.constant(BC2, name=""BC2"")\n    CONV2 = conv2d(MAXPOOL1, WC2, BC2)\n    MAXPOOL2 = maxpool2d(CONV2, k=2)\n\n    WD1 = tf.constant(WD1, name=""WD1"")\n    BD1 = tf.constant(BD1, name=""BD1"")\n\n    FC1 = tf.reshape(MAXPOOL2, [-1, WD1.get_shape().as_list()[0]])\n    FC1 = tf.add(tf.matmul(FC1, WD1), BD1)\n    FC1 = tf.nn.relu(FC1)\n\n    W_OUT = tf.constant(W_OUT, name=""W_OUT"")\n    B_OUT = tf.constant(B_OUT, name=""B_OUT"")\n\n    # skipped dropout for exported graph as there is no need for already calculated weights\n\n    OUTPUT = tf.nn.softmax(tf.matmul(FC1, W_OUT) + B_OUT, name=""output"")\n\n    sess = tf.Session()\n    init = tf.initialize_all_variables()\n    sess.run(init)\n\n    graph_def = g.as_graph_def()\n    tf.train.write_graph(graph_def, EXPORT_DIR, \'mnist_model_graph.pb\', as_text=False)\n\n    # Test trained model\n    y_train = tf.placeholder(""float"", [None, 10])\n    correct_prediction = tf.equal(tf.argmax(OUTPUT, 1), tf.argmax(y_train, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n\n    print(""check accuracy %g"" % accuracy.eval(\n            {x_2: mnist.test.images, y_train: mnist.test.labels}, sess))\n'"
