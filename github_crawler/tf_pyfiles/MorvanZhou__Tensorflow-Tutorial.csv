file_path,api_count,code
tutorial-contents/201_session.py,5,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\n""""""\nimport tensorflow as tf\n\nm1 = tf.constant([[2, 2]])\nm2 = tf.constant([[3],\n                  [3]])\ndot_operation = tf.matmul(m1, m2)\n\nprint(dot_operation)  # wrong! no result\n\n# method1 use session\nsess = tf.Session()\nresult = sess.run(dot_operation)\nprint(result)\nsess.close()\n\n# method2 use session\nwith tf.Session() as sess:\n    result_ = sess.run(dot_operation)\n    print(result_)'"
tutorial-contents/202_placeholder.py,6,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\n""""""\nimport tensorflow as tf\n\nx1 = tf.placeholder(dtype=tf.float32, shape=None)\ny1 = tf.placeholder(dtype=tf.float32, shape=None)\nz1 = x1 + y1\n\nx2 = tf.placeholder(dtype=tf.float32, shape=[2, 1])\ny2 = tf.placeholder(dtype=tf.float32, shape=[1, 2])\nz2 = tf.matmul(x2, y2)\n\nwith tf.Session() as sess:\n    # when only one operation to run\n    z1_value = sess.run(z1, feed_dict={x1: 1, y1: 2})\n\n    # when run multiple operations\n    z1_value, z2_value = sess.run(\n        [z1, z2],       # run them together\n        feed_dict={\n            x1: 1, y1: 2,\n            x2: [[2], [2]], y2: [[3, 3]]\n        })\n    print(z1_value)\n    print(z2_value)'"
tutorial-contents/203_variable.py,5,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\n""""""\nimport tensorflow as tf\n\nvar = tf.Variable(0)    # our first variable in the ""global_variable"" set\n\nadd_operation = tf.add(var, 1)\nupdate_operation = tf.assign(var, add_operation)\n\nwith tf.Session() as sess:\n    # once define variables, you have to initialize them by doing this\n    sess.run(tf.global_variables_initializer())\n    for _ in range(3):\n        sess.run(update_operation)\n        print(sess.run(var))'"
tutorial-contents/204_activation.py,6,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# fake data\nx = np.linspace(-5, 5, 200)     # x data, shape=(100, 1)\n\n# following are popular activation functions\ny_relu = tf.nn.relu(x)\ny_sigmoid = tf.nn.sigmoid(x)\ny_tanh = tf.nn.tanh(x)\ny_softplus = tf.nn.softplus(x)\n# y_softmax = tf.nn.softmax(x)  softmax is a special kind of activation function, it is about probability\n\nsess = tf.Session()\ny_relu, y_sigmoid, y_tanh, y_softplus = sess.run([y_relu, y_sigmoid, y_tanh, y_softplus])\n\n# plt to visualize these activation function\nplt.figure(1, figsize=(8, 6))\nplt.subplot(221)\nplt.plot(x, y_relu, c=\'red\', label=\'relu\')\nplt.ylim((-1, 5))\nplt.legend(loc=\'best\')\n\nplt.subplot(222)\nplt.plot(x, y_sigmoid, c=\'red\', label=\'sigmoid\')\nplt.ylim((-0.2, 1.2))\nplt.legend(loc=\'best\')\n\nplt.subplot(223)\nplt.plot(x, y_tanh, c=\'red\', label=\'tanh\')\nplt.ylim((-1.2, 1.2))\nplt.legend(loc=\'best\')\n\nplt.subplot(224)\nplt.plot(x, y_softplus, c=\'red\', label=\'softplus\')\nplt.ylim((-0.2, 6))\nplt.legend(loc=\'best\')\n\nplt.show()'"
tutorial-contents/301_simple_regression.py,9,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# fake data\nx = np.linspace(-1, 1, 100)[:, np.newaxis]          # shape (100, 1)\nnoise = np.random.normal(0, 0.1, size=x.shape)\ny = np.power(x, 2) + noise                          # shape (100, 1) + some noise\n\n# plot data\nplt.scatter(x, y)\nplt.show()\n\ntf_x = tf.placeholder(tf.float32, x.shape)     # input x\ntf_y = tf.placeholder(tf.float32, y.shape)     # input y\n\n# neural network layers\nl1 = tf.layers.dense(tf_x, 10, tf.nn.relu)          # hidden layer\noutput = tf.layers.dense(l1, 1)                     # output layer\n\nloss = tf.losses.mean_squared_error(tf_y, output)   # compute cost\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\ntrain_op = optimizer.minimize(loss)\n\nsess = tf.Session()                                 # control training and others\nsess.run(tf.global_variables_initializer())         # initialize var in graph\n\nplt.ion()   # something about plotting\n\nfor step in range(100):\n    # train and net output\n    _, l, pred = sess.run([train_op, loss, output], {tf_x: x, tf_y: y})\n    if step % 5 == 0:\n        # plot and show learning process\n        plt.cla()\n        plt.scatter(x, y)\n        plt.plot(x, pred, \'r-\', lw=5)\n        plt.text(0.5, 0, \'Loss=%.4f\' % l, fontdict={\'size\': 20, \'color\': \'red\'})\n        plt.pause(0.1)\n\nplt.ioff()\nplt.show()'"
tutorial-contents/302_simple_classification.py,11,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# fake data\nn_data = np.ones((100, 2))\nx0 = np.random.normal(2*n_data, 1)      # class0 x shape=(100, 2)\ny0 = np.zeros(100)                      # class0 y shape=(100, )\nx1 = np.random.normal(-2*n_data, 1)     # class1 x shape=(100, 2)\ny1 = np.ones(100)                       # class1 y shape=(100, )\nx = np.vstack((x0, x1))  # shape (200, 2) + some noise\ny = np.hstack((y0, y1))  # shape (200, )\n\n# plot data\nplt.scatter(x[:, 0], x[:, 1], c=y, s=100, lw=0, cmap=\'RdYlGn\')\nplt.show()\n\ntf_x = tf.placeholder(tf.float32, x.shape)     # input x\ntf_y = tf.placeholder(tf.int32, y.shape)     # input y\n\n# neural network layers\nl1 = tf.layers.dense(tf_x, 10, tf.nn.relu)          # hidden layer\noutput = tf.layers.dense(l1, 2)                     # output layer\n\nloss = tf.losses.sparse_softmax_cross_entropy(labels=tf_y, logits=output)           # compute cost\naccuracy = tf.metrics.accuracy(          # return (acc, update_op), and create 2 local variables\n    labels=tf.squeeze(tf_y), predictions=tf.argmax(output, axis=1),)[1]\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\ntrain_op = optimizer.minimize(loss)\n\nsess = tf.Session()                                                                 # control training and others\ninit_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\nsess.run(init_op)     # initialize var in graph\n\nplt.ion()   # something about plotting\nfor step in range(100):\n    # train and net output\n    _, acc, pred = sess.run([train_op, accuracy, output], {tf_x: x, tf_y: y})\n    if step % 2 == 0:\n        # plot and show learning process\n        plt.cla()\n        plt.scatter(x[:, 0], x[:, 1], c=pred.argmax(1), s=100, lw=0, cmap=\'RdYlGn\')\n        plt.text(1.5, -4, \'Accuracy=%.2f\' % acc, fontdict={\'size\': 20, \'color\': \'red\'})\n        plt.pause(0.1)\n\nplt.ioff()\nplt.show()\n'"
tutorial-contents/303_save_reload.py,18,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# fake data\nx = np.linspace(-1, 1, 100)[:, np.newaxis]          # shape (100, 1)\nnoise = np.random.normal(0, 0.1, size=x.shape)\ny = np.power(x, 2) + noise                          # shape (100, 1) + some noise\n\n\ndef save():\n    print(\'This is save\')\n    # build neural network\n    tf_x = tf.placeholder(tf.float32, x.shape)  # input x\n    tf_y = tf.placeholder(tf.float32, y.shape)  # input y\n    l = tf.layers.dense(tf_x, 10, tf.nn.relu)          # hidden layer\n    o = tf.layers.dense(l, 1)                     # output layer\n    loss = tf.losses.mean_squared_error(tf_y, o)   # compute cost\n    train_op = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())  # initialize var in graph\n\n    saver = tf.train.Saver()  # define a saver for saving and restoring\n\n    for step in range(100):                             # train\n        sess.run(train_op, {tf_x: x, tf_y: y})\n\n    saver.save(sess, \'./params\', write_meta_graph=False)  # meta_graph is not recommended\n\n    # plotting\n    pred, l = sess.run([o, loss], {tf_x: x, tf_y: y})\n    plt.figure(1, figsize=(10, 5))\n    plt.subplot(121)\n    plt.scatter(x, y)\n    plt.plot(x, pred, \'r-\', lw=5)\n    plt.text(-1, 1.2, \'Save Loss=%.4f\' % l, fontdict={\'size\': 15, \'color\': \'red\'})\n\n\ndef reload():\n    print(\'This is reload\')\n    # build entire net again and restore\n    tf_x = tf.placeholder(tf.float32, x.shape)  # input x\n    tf_y = tf.placeholder(tf.float32, y.shape)  # input y\n    l_ = tf.layers.dense(tf_x, 10, tf.nn.relu)          # hidden layer\n    o_ = tf.layers.dense(l_, 1)                     # output layer\n    loss_ = tf.losses.mean_squared_error(tf_y, o_)   # compute cost\n\n    sess = tf.Session()\n    # don\'t need to initialize variables, just restoring trained variables\n    saver = tf.train.Saver()  # define a saver for saving and restoring\n    saver.restore(sess, \'./params\')\n\n    # plotting\n    pred, l = sess.run([o_, loss_], {tf_x: x, tf_y: y})\n    plt.subplot(122)\n    plt.scatter(x, y)\n    plt.plot(x, pred, \'r-\', lw=5)\n    plt.text(-1, 1.2, \'Reload Loss=%.4f\' % l, fontdict={\'size\': 15, \'color\': \'red\'})\n    plt.show()\n\n\nsave()\n\n# destroy previous net\ntf.reset_default_graph()\n\nreload()\n'"
tutorial-contents/304_optimizer.py,12,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\nLR = 0.01\nBATCH_SIZE = 32\n\n# fake data\nx = np.linspace(-1, 1, 100)[:, np.newaxis]          # shape (100, 1)\nnoise = np.random.normal(0, 0.1, size=x.shape)\ny = np.power(x, 2) + noise                          # shape (100, 1) + some noise\n\n# plot dataset\nplt.scatter(x, y)\nplt.show()\n\n# default network\nclass Net:\n    def __init__(self, opt, **kwargs):\n        self.x = tf.placeholder(tf.float32, [None, 1])\n        self.y = tf.placeholder(tf.float32, [None, 1])\n        l = tf.layers.dense(self.x, 20, tf.nn.relu)\n        out = tf.layers.dense(l, 1)\n        self.loss = tf.losses.mean_squared_error(self.y, out)\n        self.train = opt(LR, **kwargs).minimize(self.loss)\n\n# different nets\nnet_SGD         = Net(tf.train.GradientDescentOptimizer)\nnet_Momentum    = Net(tf.train.MomentumOptimizer, momentum=0.9)\nnet_RMSprop     = Net(tf.train.RMSPropOptimizer)\nnet_Adam        = Net(tf.train.AdamOptimizer)\nnets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nlosses_his = [[], [], [], []]   # record loss\n\n# training\nfor step in range(300):          # for each training step\n    index = np.random.randint(0, x.shape[0], BATCH_SIZE)\n    b_x = x[index]\n    b_y = y[index]\n\n    for net, l_his in zip(nets, losses_his):\n        _, l = sess.run([net.train, net.loss], {net.x: b_x, net.y: b_y})\n        l_his.append(l)     # loss recoder\n\n# plot loss history\nlabels = [\'SGD\', \'Momentum\', \'RMSprop\', \'Adam\']\nfor i, l_his in enumerate(losses_his):\n    plt.plot(l_his, label=labels[i])\nplt.legend(loc=\'best\')\nplt.xlabel(\'Steps\')\nplt.ylabel(\'Loss\')\nplt.ylim((0, 0.2))\nplt.show()'"
tutorial-contents/305_tensorboard.py,16,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nnumpy\n""""""\nimport tensorflow as tf\nimport numpy as np\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# fake data\nx = np.linspace(-1, 1, 100)[:, np.newaxis]          # shape (100, 1)\nnoise = np.random.normal(0, 0.1, size=x.shape)\ny = np.power(x, 2) + noise                          # shape (100, 1) + some noise\n\nwith tf.variable_scope(\'Inputs\'):\n    tf_x = tf.placeholder(tf.float32, x.shape, name=\'x\')\n    tf_y = tf.placeholder(tf.float32, y.shape, name=\'y\')\n\nwith tf.variable_scope(\'Net\'):\n    l1 = tf.layers.dense(tf_x, 10, tf.nn.relu, name=\'hidden_layer\')\n    output = tf.layers.dense(l1, 1, name=\'output_layer\')\n\n    # add to histogram summary\n    tf.summary.histogram(\'h_out\', l1)\n    tf.summary.histogram(\'pred\', output)\n\nloss = tf.losses.mean_squared_error(tf_y, output, scope=\'loss\')\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\ntf.summary.scalar(\'loss\', loss)     # add loss to scalar summary\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nwriter = tf.summary.FileWriter(\'./log\', sess.graph)     # write to file\nmerge_op = tf.summary.merge_all()                       # operation to merge all summary\n\nfor step in range(100):\n    # train and net output\n    _, result = sess.run([train_op, merge_op], {tf_x: x, tf_y: y})\n    writer.add_summary(result, step)\n\n# Lastly, in your terminal or CMD, type this :\n# $ tensorboard --logdir path/to/log\n# open you google chrome, type the link shown on your terminal or CMD. (something like this: http://localhost:6006)'"
tutorial-contents/306_dataset.py,10,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nMore information about Dataset: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md\n""""""\nimport tensorflow as tf\nimport numpy as np\n\n\n# load your data or create your data in here\nnpx = np.random.uniform(-1, 1, (1000, 1))                           # x data\nnpy = np.power(npx, 2) + np.random.normal(0, 0.1, size=npx.shape)   # y data\nnpx_train, npx_test = np.split(npx, [800])                          # training and test data\nnpy_train, npy_test = np.split(npy, [800])\n\n# use placeholder, later you may need different data, pass the different data into placeholder\ntfx = tf.placeholder(npx_train.dtype, npx_train.shape)\ntfy = tf.placeholder(npy_train.dtype, npy_train.shape)\n\n# create dataloader\ndataset = tf.data.Dataset.from_tensor_slices((tfx, tfy))\ndataset = dataset.shuffle(buffer_size=1000)   # choose data randomly from this buffer\ndataset = dataset.batch(32)                   # batch size you will use\ndataset = dataset.repeat(3)                   # repeat for 3 epochs\niterator = dataset.make_initializable_iterator()  # later we have to initialize this one\n\n# your network\nbx, by = iterator.get_next()                  # use batch to update\nl1 = tf.layers.dense(bx, 10, tf.nn.relu)\nout = tf.layers.dense(l1, npy.shape[1])\nloss = tf.losses.mean_squared_error(by, out)\ntrain = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n\nsess = tf.Session()\n# need to initialize the iterator in this case\nsess.run([iterator.initializer, tf.global_variables_initializer()], feed_dict={tfx: npx_train, tfy: npy_train})\n\nfor step in range(201):\n  try:\n    _, trainl = sess.run([train, loss])                       # train\n    if step % 10 == 0:\n      testl = sess.run(loss, {bx: npx_test, by: npy_test})    # test\n      print(\'step: %i/200\' % step, \'|train loss:\', trainl, \'|test loss:\', testl)\n  except tf.errors.OutOfRangeError:     # if training takes more than 3 epochs, training will be stopped\n    print(\'Finish the last epoch.\')\n    break'"
tutorial-contents/401_CNN.py,17,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\nBATCH_SIZE = 50\nLR = 0.001              # learning rate\n\nmnist = input_data.read_data_sets(\'./mnist\', one_hot=True)  # they has been normalized to range (0,1)\ntest_x = mnist.test.images[:2000]\ntest_y = mnist.test.labels[:2000]\n\n# plot one example\nprint(mnist.train.images.shape)     # (55000, 28 * 28)\nprint(mnist.train.labels.shape)   # (55000, 10)\nplt.imshow(mnist.train.images[0].reshape((28, 28)), cmap=\'gray\')\nplt.title(\'%i\' % np.argmax(mnist.train.labels[0])); plt.show()\n\ntf_x = tf.placeholder(tf.float32, [None, 28*28]) / 255.\nimage = tf.reshape(tf_x, [-1, 28, 28, 1])              # (batch, height, width, channel)\ntf_y = tf.placeholder(tf.int32, [None, 10])            # input y\n\n# CNN\nconv1 = tf.layers.conv2d(   # shape (28, 28, 1)\n    inputs=image,\n    filters=16,\n    kernel_size=5,\n    strides=1,\n    padding=\'same\',\n    activation=tf.nn.relu\n)           # -> (28, 28, 16)\npool1 = tf.layers.max_pooling2d(\n    conv1,\n    pool_size=2,\n    strides=2,\n)           # -> (14, 14, 16)\nconv2 = tf.layers.conv2d(pool1, 32, 5, 1, \'same\', activation=tf.nn.relu)    # -> (14, 14, 32)\npool2 = tf.layers.max_pooling2d(conv2, 2, 2)    # -> (7, 7, 32)\nflat = tf.reshape(pool2, [-1, 7*7*32])          # -> (7*7*32, )\noutput = tf.layers.dense(flat, 10)              # output layer\n\nloss = tf.losses.softmax_cross_entropy(onehot_labels=tf_y, logits=output)           # compute cost\ntrain_op = tf.train.AdamOptimizer(LR).minimize(loss)\n\naccuracy = tf.metrics.accuracy(          # return (acc, update_op), and create 2 local variables\n    labels=tf.argmax(tf_y, axis=1), predictions=tf.argmax(output, axis=1),)[1]\n\nsess = tf.Session()\ninit_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) # the local var is for accuracy_op\nsess.run(init_op)     # initialize var in graph\n\n# following function (plot_with_labels) is for visualization, can be ignored if not interested\nfrom matplotlib import cm\ntry: from sklearn.manifold import TSNE; HAS_SK = True\nexcept: HAS_SK = False; print(\'\\nPlease install sklearn for layer visualization\\n\')\ndef plot_with_labels(lowDWeights, labels):\n    plt.cla(); X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n    for x, y, s in zip(X, Y, labels):\n        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title(\'Visualize last layer\'); plt.show(); plt.pause(0.01)\n\nplt.ion()\nfor step in range(600):\n    b_x, b_y = mnist.train.next_batch(BATCH_SIZE)\n    _, loss_ = sess.run([train_op, loss], {tf_x: b_x, tf_y: b_y})\n    if step % 50 == 0:\n        accuracy_, flat_representation = sess.run([accuracy, flat], {tf_x: test_x, tf_y: test_y})\n        print(\'Step:\', step, \'| train loss: %.4f\' % loss_, \'| test accuracy: %.2f\' % accuracy_)\n\n        if HAS_SK:\n            # Visualization of trained flatten layer (T-SNE)\n            tsne = TSNE(perplexity=30, n_components=2, init=\'pca\', n_iter=5000); plot_only = 500\n            low_dim_embs = tsne.fit_transform(flat_representation[:plot_only, :])\n            labels = np.argmax(test_y, axis=1)[:plot_only]; plot_with_labels(low_dim_embs, labels)\nplt.ioff()\n\n# print 10 predictions from test data\ntest_output = sess.run(output, {tf_x: test_x[:10]})\npred_y = np.argmax(test_output, 1)\nprint(pred_y, \'prediction number\')\nprint(np.argmax(test_y[:10], 1), \'real number\')'"
tutorial-contents/402_RNN_classification.py,14,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# Hyper Parameters\nBATCH_SIZE = 64\nTIME_STEP = 28          # rnn time step / image height\nINPUT_SIZE = 28         # rnn input size / image width\nLR = 0.01               # learning rate\n\n# data\nmnist = input_data.read_data_sets(\'./mnist\', one_hot=True)              # they has been normalized to range (0,1)\ntest_x = mnist.test.images[:2000]\ntest_y = mnist.test.labels[:2000]\n\n# plot one example\nprint(mnist.train.images.shape)     # (55000, 28 * 28)\nprint(mnist.train.labels.shape)   # (55000, 10)\nplt.imshow(mnist.train.images[0].reshape((28, 28)), cmap=\'gray\')\nplt.title(\'%i\' % np.argmax(mnist.train.labels[0]))\nplt.show()\n\n# tensorflow placeholders\ntf_x = tf.placeholder(tf.float32, [None, TIME_STEP * INPUT_SIZE])       # shape(batch, 784)\nimage = tf.reshape(tf_x, [-1, TIME_STEP, INPUT_SIZE])                   # (batch, height, width, channel)\ntf_y = tf.placeholder(tf.int32, [None, 10])                             # input y\n\n# RNN\nrnn_cell = tf.nn.rnn_cell.LSTMCell(num_units=64)\noutputs, (h_c, h_n) = tf.nn.dynamic_rnn(\n    rnn_cell,                   # cell you have chosen\n    image,                      # input\n    initial_state=None,         # the initial hidden state\n    dtype=tf.float32,           # must given if set initial_state = None\n    time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)\n)\noutput = tf.layers.dense(outputs[:, -1, :], 10)              # output based on the last output step\n\nloss = tf.losses.softmax_cross_entropy(onehot_labels=tf_y, logits=output)           # compute cost\ntrain_op = tf.train.AdamOptimizer(LR).minimize(loss)\n\naccuracy = tf.metrics.accuracy(          # return (acc, update_op), and create 2 local variables\n    labels=tf.argmax(tf_y, axis=1), predictions=tf.argmax(output, axis=1),)[1]\n\nsess = tf.Session()\ninit_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) # the local var is for accuracy_op\nsess.run(init_op)     # initialize var in graph\n\nfor step in range(1200):    # training\n    b_x, b_y = mnist.train.next_batch(BATCH_SIZE)\n    _, loss_ = sess.run([train_op, loss], {tf_x: b_x, tf_y: b_y})\n    if step % 50 == 0:      # testing\n        accuracy_ = sess.run(accuracy, {tf_x: test_x, tf_y: test_y})\n        print(\'train loss: %.4f\' % loss_, \'| test accuracy: %.2f\' % accuracy_)\n\n# print 10 predictions from test data\ntest_output = sess.run(output, {tf_x: test_x[:10]})\npred_y = np.argmax(test_output, 1)\nprint(pred_y, \'prediction number\')\nprint(np.argmax(test_y[:10], 1), \'real number\')'"
tutorial-contents/403_RNN_regression.py,12,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Hyper Parameters\nTIME_STEP = 10       # rnn time step\nINPUT_SIZE = 1      # rnn input size\nCELL_SIZE = 32      # rnn cell size\nLR = 0.02           # learning rate\n\n# show data\nsteps = np.linspace(0, np.pi*2, 100, dtype=np.float32)\nx_np = np.sin(steps); y_np = np.cos(steps)    # float32 for converting torch FloatTensor\nplt.plot(steps, y_np, \'r-\', label=\'target (cos)\'); plt.plot(steps, x_np, \'b-\', label=\'input (sin)\')\nplt.legend(loc=\'best\'); plt.show()\n\n# tensorflow placeholders\ntf_x = tf.placeholder(tf.float32, [None, TIME_STEP, INPUT_SIZE])        # shape(batch, 5, 1)\ntf_y = tf.placeholder(tf.float32, [None, TIME_STEP, INPUT_SIZE])          # input y\n\n# RNN\nrnn_cell = tf.nn.rnn_cell.LSTMCell(num_units=CELL_SIZE)\ninit_s = rnn_cell.zero_state(batch_size=1, dtype=tf.float32)    # very first hidden state\noutputs, final_s = tf.nn.dynamic_rnn(\n    rnn_cell,                   # cell you have chosen\n    tf_x,                       # input\n    initial_state=init_s,       # the initial hidden state\n    time_major=False,           # False: (batch, time step, input); True: (time step, batch, input)\n)\nouts2D = tf.reshape(outputs, [-1, CELL_SIZE])                       # reshape 3D output to 2D for fully connected layer\nnet_outs2D = tf.layers.dense(outs2D, INPUT_SIZE)\nouts = tf.reshape(net_outs2D, [-1, TIME_STEP, INPUT_SIZE])          # reshape back to 3D\n\nloss = tf.losses.mean_squared_error(labels=tf_y, predictions=outs)  # compute cost\ntrain_op = tf.train.AdamOptimizer(LR).minimize(loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())     # initialize var in graph\n\nplt.figure(1, figsize=(12, 5)); plt.ion()       # continuously plot\n\nfor step in range(60):\n    start, end = step * np.pi, (step+1)*np.pi   # time range\n    # use sin predicts cos\n    steps = np.linspace(start, end, TIME_STEP)\n    x = np.sin(steps)[np.newaxis, :, np.newaxis]    # shape (batch, time_step, input_size)\n    y = np.cos(steps)[np.newaxis, :, np.newaxis]\n    if \'final_s_\' not in globals():                 # first state, no any hidden state\n        feed_dict = {tf_x: x, tf_y: y}\n    else:                                           # has hidden state, so pass it to rnn\n        feed_dict = {tf_x: x, tf_y: y, init_s: final_s_}\n    _, pred_, final_s_ = sess.run([train_op, outs, final_s], feed_dict)     # train\n\n    # plotting\n    plt.plot(steps, y.flatten(), \'r-\'); plt.plot(steps, pred_.flatten(), \'b-\')\n    plt.ylim((-1.2, 1.2)); plt.draw(); plt.pause(0.05)\n\nplt.ioff(); plt.show()'"
tutorial-contents/404_AutoEncoder.py,14,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport numpy as np\n\ntf.set_random_seed(1)\n\n# Hyper Parameters\nBATCH_SIZE = 64\nLR = 0.002         # learning rate\nN_TEST_IMG = 5\n\n# Mnist digits\nmnist = input_data.read_data_sets(\'./mnist\', one_hot=False)     # use not one-hotted target data\ntest_x = mnist.test.images[:200]\ntest_y = mnist.test.labels[:200]\n\n# plot one example\nprint(mnist.train.images.shape)     # (55000, 28 * 28)\nprint(mnist.train.labels.shape)     # (55000, 10)\nplt.imshow(mnist.train.images[0].reshape((28, 28)), cmap=\'gray\')\nplt.title(\'%i\' % np.argmax(mnist.train.labels[0]))\nplt.show()\n\n# tf placeholder\ntf_x = tf.placeholder(tf.float32, [None, 28*28])    # value in the range of (0, 1)\n\n# encoder\nen0 = tf.layers.dense(tf_x, 128, tf.nn.tanh)\nen1 = tf.layers.dense(en0, 64, tf.nn.tanh)\nen2 = tf.layers.dense(en1, 12, tf.nn.tanh)\nencoded = tf.layers.dense(en2, 3)\n\n# decoder\nde0 = tf.layers.dense(encoded, 12, tf.nn.tanh)\nde1 = tf.layers.dense(de0, 64, tf.nn.tanh)\nde2 = tf.layers.dense(de1, 128, tf.nn.tanh)\ndecoded = tf.layers.dense(de2, 28*28, tf.nn.sigmoid)\n\nloss = tf.losses.mean_squared_error(labels=tf_x, predictions=decoded)\ntrain = tf.train.AdamOptimizer(LR).minimize(loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# initialize figure\nf, a = plt.subplots(2, N_TEST_IMG, figsize=(5, 2))\nplt.ion()   # continuously plot\n\n# original data (first row) for viewing\nview_data = mnist.test.images[:N_TEST_IMG]\nfor i in range(N_TEST_IMG):\n    a[0][i].imshow(np.reshape(view_data[i], (28, 28)), cmap=\'gray\')\n    a[0][i].set_xticks(()); a[0][i].set_yticks(())\n\nfor step in range(8000):\n    b_x, b_y = mnist.train.next_batch(BATCH_SIZE)\n    _, encoded_, decoded_, loss_ = sess.run([train, encoded, decoded, loss], {tf_x: b_x})\n\n    if step % 100 == 0:     # plotting\n        print(\'train loss: %.4f\' % loss_)\n        # plotting decoded image (second row)\n        decoded_data = sess.run(decoded, {tf_x: view_data})\n        for i in range(N_TEST_IMG):\n            a[1][i].clear()\n            a[1][i].imshow(np.reshape(decoded_data[i], (28, 28)), cmap=\'gray\')\n            a[1][i].set_xticks(()); a[1][i].set_yticks(())\n        plt.draw(); plt.pause(0.01)\nplt.ioff()\n\n# visualize in 3D plot\nview_data = test_x[:200]\nencoded_data = sess.run(encoded, {tf_x: view_data})\nfig = plt.figure(2); ax = Axes3D(fig)\nX, Y, Z = encoded_data[:, 0], encoded_data[:, 1], encoded_data[:, 2]\nfor x, y, z, s in zip(X, Y, Z, test_y):\n    c = cm.rainbow(int(255*s/9)); ax.text(x, y, z, s, backgroundcolor=c)\nax.set_xlim(X.min(), X.max()); ax.set_ylim(Y.min(), Y.max()); ax.set_zlim(Z.min(), Z.max())\nplt.show()'"
tutorial-contents/405_DQN_reinforcement_learning.py,21,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\nMore about Reinforcement learning: https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\ngym: 0.8.1\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport gym\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# Hyper Parameters\nBATCH_SIZE = 32\nLR = 0.01                   # learning rate\nEPSILON = 0.9               # greedy policy\nGAMMA = 0.9                 # reward discount\nTARGET_REPLACE_ITER = 100   # target update frequency\nMEMORY_CAPACITY = 2000\nMEMORY_COUNTER = 0          # for store experience\nLEARNING_STEP_COUNTER = 0   # for target updating\nenv = gym.make(\'CartPole-v0\')\nenv = env.unwrapped\nN_ACTIONS = env.action_space.n\nN_STATES = env.observation_space.shape[0]\nMEMORY = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n\n# tf placeholders\ntf_s = tf.placeholder(tf.float32, [None, N_STATES])\ntf_a = tf.placeholder(tf.int32, [None, ])\ntf_r = tf.placeholder(tf.float32, [None, ])\ntf_s_ = tf.placeholder(tf.float32, [None, N_STATES])\n\nwith tf.variable_scope(\'q\'):        # evaluation network\n    l_eval = tf.layers.dense(tf_s, 10, tf.nn.relu, kernel_initializer=tf.random_normal_initializer(0, 0.1))\n    q = tf.layers.dense(l_eval, N_ACTIONS, kernel_initializer=tf.random_normal_initializer(0, 0.1))\n\nwith tf.variable_scope(\'q_next\'):   # target network, not to train\n    l_target = tf.layers.dense(tf_s_, 10, tf.nn.relu, trainable=False)\n    q_next = tf.layers.dense(l_target, N_ACTIONS, trainable=False)\n\nq_target = tf_r + GAMMA * tf.reduce_max(q_next, axis=1)                   # shape=(None, ),\n\na_indices = tf.stack([tf.range(tf.shape(tf_a)[0], dtype=tf.int32), tf_a], axis=1)\nq_wrt_a = tf.gather_nd(params=q, indices=a_indices)     # shape=(None, ), q for current state\n\nloss = tf.reduce_mean(tf.squared_difference(q_target, q_wrt_a))\ntrain_op = tf.train.AdamOptimizer(LR).minimize(loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n\ndef choose_action(s):\n    s = s[np.newaxis, :]\n    if np.random.uniform() < EPSILON:\n        # forward feed the observation and get q value for every actions\n        actions_value = sess.run(q, feed_dict={tf_s: s})\n        action = np.argmax(actions_value)\n    else:\n        action = np.random.randint(0, N_ACTIONS)\n    return action\n\n\ndef store_transition(s, a, r, s_):\n    global MEMORY_COUNTER\n    transition = np.hstack((s, [a, r], s_))\n    # replace the old memory with new memory\n    index = MEMORY_COUNTER % MEMORY_CAPACITY\n    MEMORY[index, :] = transition\n    MEMORY_COUNTER += 1\n\n\ndef learn():\n    # update target net\n    global LEARNING_STEP_COUNTER\n    if LEARNING_STEP_COUNTER % TARGET_REPLACE_ITER == 0:\n        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'q_next\')\n        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'q\')\n        sess.run([tf.assign(t, e) for t, e in zip(t_params, e_params)])\n    LEARNING_STEP_COUNTER += 1\n\n    # learning\n    sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n    b_memory = MEMORY[sample_index, :]\n    b_s = b_memory[:, :N_STATES]\n    b_a = b_memory[:, N_STATES].astype(int)\n    b_r = b_memory[:, N_STATES+1]\n    b_s_ = b_memory[:, -N_STATES:]\n    sess.run(train_op, {tf_s: b_s, tf_a: b_a, tf_r: b_r, tf_s_: b_s_})\n\nprint(\'\\nCollecting experience...\')\nfor i_episode in range(400):\n    s = env.reset()\n    ep_r = 0\n    while True:\n        env.render()\n        a = choose_action(s)\n\n        # take action\n        s_, r, done, info = env.step(a)\n\n        # modify the reward\n        x, x_dot, theta, theta_dot = s_\n        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n        r = r1 + r2\n\n        store_transition(s, a, r, s_)\n\n        ep_r += r\n        if MEMORY_COUNTER > MEMORY_CAPACITY:\n            learn()\n            if done:\n                print(\'Ep: \', i_episode,\n                      \'| Ep_r: \', round(ep_r, 2))\n\n        if done:\n            break\n        s = s_'"
tutorial-contents/406_GAN.py,19,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# Hyper Parameters\nBATCH_SIZE = 64\nLR_G = 0.0001           # learning rate for generator\nLR_D = 0.0001           # learning rate for discriminator\nN_IDEAS = 5             # think of this as number of ideas for generating an art work (Generator)\nART_COMPONENTS = 15     # it could be total point G can draw in the canvas\nPAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])\n\n# show our beautiful painting range\nplt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c=\'#74BCFF\', lw=3, label=\'upper bound\')\nplt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c=\'#FF9359\', lw=3, label=\'lower bound\')\nplt.legend(loc=\'upper right\')\nplt.show()\n\n\ndef artist_works():     # painting from the famous artist (real target)\n    a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis]\n    paintings = a * np.power(PAINT_POINTS, 2) + (a-1)\n    return paintings\n\n\nwith tf.variable_scope(\'Generator\'):\n    G_in = tf.placeholder(tf.float32, [None, N_IDEAS])          # random ideas (could from normal distribution)\n    G_l1 = tf.layers.dense(G_in, 128, tf.nn.relu)\n    G_out = tf.layers.dense(G_l1, ART_COMPONENTS)               # making a painting from these random ideas\n\nwith tf.variable_scope(\'Discriminator\'):\n    real_art = tf.placeholder(tf.float32, [None, ART_COMPONENTS], name=\'real_in\')   # receive art work from the famous artist\n    D_l0 = tf.layers.dense(real_art, 128, tf.nn.relu, name=\'l\')\n    prob_artist0 = tf.layers.dense(D_l0, 1, tf.nn.sigmoid, name=\'out\')              # probability that the art work is made by artist\n    # reuse layers for generator\n    D_l1 = tf.layers.dense(G_out, 128, tf.nn.relu, name=\'l\', reuse=True)            # receive art work from a newbie like G\n    prob_artist1 = tf.layers.dense(D_l1, 1, tf.nn.sigmoid, name=\'out\', reuse=True)  # probability that the art work is made by artist\n\nD_loss = -tf.reduce_mean(tf.log(prob_artist0) + tf.log(1-prob_artist1))\nG_loss = tf.reduce_mean(tf.log(1-prob_artist1))\n\ntrain_D = tf.train.AdamOptimizer(LR_D).minimize(\n    D_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Discriminator\'))\ntrain_G = tf.train.AdamOptimizer(LR_G).minimize(\n    G_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Generator\'))\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nplt.ion()   # something about continuous plotting\nfor step in range(5000):\n    artist_paintings = artist_works()           # real painting from artist\n    G_ideas = np.random.randn(BATCH_SIZE, N_IDEAS)\n    G_paintings, pa0, Dl = sess.run([G_out, prob_artist0, D_loss, train_D, train_G],    # train and get results\n                                    {G_in: G_ideas, real_art: artist_paintings})[:3]\n\n    if step % 50 == 0:  # plotting\n        plt.cla()\n        plt.plot(PAINT_POINTS[0], G_paintings[0], c=\'#4AD631\', lw=3, label=\'Generated painting\',)\n        plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c=\'#74BCFF\', lw=3, label=\'upper bound\')\n        plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c=\'#FF9359\', lw=3, label=\'lower bound\')\n        plt.text(-.5, 2.3, \'D accuracy=%.2f (0.5 for D to converge)\' % pa0.mean(), fontdict={\'size\': 15})\n        plt.text(-.5, 2, \'D score= %.2f (-1.38 for G to converge)\' % -Dl, fontdict={\'size\': 15})\n        plt.ylim((0, 3)); plt.legend(loc=\'upper right\', fontsize=12); plt.draw(); plt.pause(0.01)\n\nplt.ioff()\nplt.show()'"
tutorial-contents/406_conditional_GAN.py,23,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# Hyper Parameters\nBATCH_SIZE = 64\nLR_G = 0.0001           # learning rate for generator\nLR_D = 0.0001           # learning rate for discriminator\nN_IDEAS = 5             # think of this as number of ideas for generating an art work (Generator)\nART_COMPONENTS = 15     # it could be total point G can draw in the canvas\nPAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])\n\n# show our beautiful painting range\nplt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c=\'#74BCFF\', lw=3, label=\'upper bound\')\nplt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c=\'#FF9359\', lw=3, label=\'lower bound\')\nplt.legend(loc=\'upper right\')\nplt.show()\n\n\ndef artist_works():     # painting from the famous artist (real target)\n    a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis]\n    paintings = a * np.power(PAINT_POINTS, 2) + (a-1)\n    labels = (a - 1) > 0.5  # upper paintings (1), lower paintings (0), two classes\n    labels = labels.astype(np.float32)\n    return paintings, labels\n\nart_labels = tf.placeholder(tf.float32, [None, 1])\nwith tf.variable_scope(\'Generator\'):\n    G_in = tf.placeholder(tf.float32, [None, N_IDEAS])          # random ideas (could from normal distribution)\n    G_art = tf.concat((G_in, art_labels), 1)                    # combine ideas with labels\n    G_l1 = tf.layers.dense(G_art, 128, tf.nn.relu)\n    G_out = tf.layers.dense(G_l1, ART_COMPONENTS)               # making a painting from these random ideas\n\nwith tf.variable_scope(\'Discriminator\'):\n    real_in = tf.placeholder(tf.float32, [None, ART_COMPONENTS], name=\'real_in\')   # receive art work from the famous artist + label\n    real_art = tf.concat((real_in, art_labels), 1)                                  # art with labels\n    D_l0 = tf.layers.dense(real_art, 128, tf.nn.relu, name=\'l\')\n    prob_artist0 = tf.layers.dense(D_l0, 1, tf.nn.sigmoid, name=\'out\')              # probability that the art work is made by artist\n    # reuse layers for generator\n    G_art = tf.concat((G_out, art_labels), 1)                                       # art with labels\n    D_l1 = tf.layers.dense(G_art, 128, tf.nn.relu, name=\'l\', reuse=True)            # receive art work from a newbie like G\n    prob_artist1 = tf.layers.dense(D_l1, 1, tf.nn.sigmoid, name=\'out\', reuse=True)  # probability that the art work is made by artist\n\nD_loss = -tf.reduce_mean(tf.log(prob_artist0) + tf.log(1-prob_artist1))\nG_loss = tf.reduce_mean(tf.log(1-prob_artist1))\n\ntrain_D = tf.train.AdamOptimizer(LR_D).minimize(\n    D_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Discriminator\'))\ntrain_G = tf.train.AdamOptimizer(LR_G).minimize(\n    G_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'Generator\'))\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nplt.ion()   # something about continuous plotting\nfor step in range(7000):\n    artist_paintings, labels = artist_works()               # real painting from artist\n    G_ideas = np.random.randn(BATCH_SIZE, N_IDEAS)\n    G_paintings, pa0, Dl = sess.run([G_out, prob_artist0, D_loss, train_D, train_G],    # train and get results\n                                    {G_in: G_ideas, real_in: artist_paintings, art_labels: labels})[:3]\n\n    if step % 50 == 0:  # plotting\n        plt.cla()\n        plt.plot(PAINT_POINTS[0], G_paintings[0], c=\'#4AD631\', lw=3, label=\'Generated painting\',)\n        bound = [0, 0.5] if labels[0, 0] == 0 else [0.5, 1]\n        plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + bound[1], c=\'#74BCFF\', lw=3, label=\'upper bound\')\n        plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + bound[0], c=\'#FF9359\', lw=3, label=\'lower bound\')\n        plt.text(-.5, 2.3, \'D accuracy=%.2f (0.5 for D to converge)\' % pa0.mean(), fontdict={\'size\': 15})\n        plt.text(-.5, 2, \'D score= %.2f (-1.38 for G to converge)\' % -Dl, fontdict={\'size\': 15})\n        plt.text(-.5, 1.7, \'Class = %i\' % int(labels[0, 0]), fontdict={\'size\': 15})\n        plt.ylim((0, 3)); plt.legend(loc=\'upper right\', fontsize=12); plt.draw(); plt.pause(0.1)\n\nplt.ioff()\n\n# plot a generated painting for upper class\nplt.figure(2)\nz = np.random.randn(1, N_IDEAS)\nlabel = np.array([[1.]])            # for upper class\nG_paintings = sess.run(G_out, {G_in: z, art_labels: label})\nplt.plot(PAINT_POINTS[0], G_paintings[0], c=\'#4AD631\', lw=3, label=\'G painting for upper class\',)\nplt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + bound[1], c=\'#74BCFF\', lw=3, label=\'upper bound (class 1)\')\nplt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + bound[0], c=\'#FF9359\', lw=3, label=\'lower bound (class 1)\')\nplt.ylim((0, 3)); plt.legend(loc=\'upper right\', fontsize=12); plt.show()'"
tutorial-contents/407_transfer_learning.py,17,"b'""""""\nThis is a simple example of transfer learning using VGG.\nFine tune a CNN from a classifier to regressor.\nGenerate some fake data for describing cat and tiger length.\n\nFake length setting:\nCat - Normal distribution (40, 8)\nTiger - Normal distribution (100, 30)\n\nThe VGG model and parameters are adopted from:\nhttps://github.com/machrisaa/tensorflow-vgg\n\nLearn more, visit my tutorial site: [\xe8\x8e\xab\xe7\x83\xa6Python](https://morvanzhou.github.io)\n""""""\n\nfrom urllib.request import urlretrieve\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport skimage.io\nimport skimage.transform\nimport matplotlib.pyplot as plt\n\n\ndef download():     # download tiger and kittycat image\n    categories = [\'tiger\', \'kittycat\']\n    for category in categories:\n        os.makedirs(\'./for_transfer_learning/data/%s\' % category, exist_ok=True)\n        with open(\'./for_transfer_learning/imagenet_%s.txt\' % category, \'r\') as file:\n            urls = file.readlines()\n            n_urls = len(urls)\n            for i, url in enumerate(urls):\n                try:\n                    urlretrieve(url.strip(), \'./for_transfer_learning/data/%s/%s\' % (category, url.strip().split(\'/\')[-1]))\n                    print(\'%s %i/%i\' % (category, i, n_urls))\n                except:\n                    print(\'%s %i/%i\' % (category, i, n_urls), \'no image\')\n\n\ndef load_img(path):\n    img = skimage.io.imread(path)\n    img = img / 255.0\n    # print ""Original Image Shape: "", img.shape\n    # we crop image from center\n    short_edge = min(img.shape[:2])\n    yy = int((img.shape[0] - short_edge) / 2)\n    xx = int((img.shape[1] - short_edge) / 2)\n    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    # resize to 224, 224\n    resized_img = skimage.transform.resize(crop_img, (224, 224))[None, :, :, :]   # shape [1, 224, 224, 3]\n    return resized_img\n\n\ndef load_data():\n    imgs = {\'tiger\': [], \'kittycat\': []}\n    for k in imgs.keys():\n        dir = \'./for_transfer_learning/data/\' + k\n        for file in os.listdir(dir):\n            if not file.lower().endswith(\'.jpg\'):\n                continue\n            try:\n                resized_img = load_img(os.path.join(dir, file))\n            except OSError:\n                continue\n            imgs[k].append(resized_img)    # [1, height, width, depth] * n\n            if len(imgs[k]) == 400:        # only use 400 imgs to reduce my memory load\n                break\n    # fake length data for tiger and cat\n    tigers_y = np.maximum(20, np.random.randn(len(imgs[\'tiger\']), 1) * 30 + 100)\n    cat_y = np.maximum(10, np.random.randn(len(imgs[\'kittycat\']), 1) * 8 + 40)\n    return imgs[\'tiger\'], imgs[\'kittycat\'], tigers_y, cat_y\n\n\nclass Vgg16:\n    vgg_mean = [103.939, 116.779, 123.68]\n\n    def __init__(self, vgg16_npy_path=None, restore_from=None):\n        # pre-trained parameters\n        try:\n            self.data_dict = np.load(vgg16_npy_path, encoding=\'latin1\').item()\n        except FileNotFoundError:\n            print(\'Please download VGG16 parameters from here https://mega.nz/#!YU1FWJrA!O1ywiCS2IiOlUCtCpI6HTJOMrneN-Qdv3ywQP5poecM\\nOr from my Baidu Cloud: https://pan.baidu.com/s/1Spps1Wy0bvrQHH2IMkRfpg\')\n\n        self.tfx = tf.placeholder(tf.float32, [None, 224, 224, 3])\n        self.tfy = tf.placeholder(tf.float32, [None, 1])\n\n        # Convert RGB to BGR\n        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=self.tfx * 255.0)\n        bgr = tf.concat(axis=3, values=[\n            blue - self.vgg_mean[0],\n            green - self.vgg_mean[1],\n            red - self.vgg_mean[2],\n        ])\n\n        # pre-trained VGG layers are fixed in fine-tune\n        conv1_1 = self.conv_layer(bgr, ""conv1_1"")\n        conv1_2 = self.conv_layer(conv1_1, ""conv1_2"")\n        pool1 = self.max_pool(conv1_2, \'pool1\')\n\n        conv2_1 = self.conv_layer(pool1, ""conv2_1"")\n        conv2_2 = self.conv_layer(conv2_1, ""conv2_2"")\n        pool2 = self.max_pool(conv2_2, \'pool2\')\n\n        conv3_1 = self.conv_layer(pool2, ""conv3_1"")\n        conv3_2 = self.conv_layer(conv3_1, ""conv3_2"")\n        conv3_3 = self.conv_layer(conv3_2, ""conv3_3"")\n        pool3 = self.max_pool(conv3_3, \'pool3\')\n\n        conv4_1 = self.conv_layer(pool3, ""conv4_1"")\n        conv4_2 = self.conv_layer(conv4_1, ""conv4_2"")\n        conv4_3 = self.conv_layer(conv4_2, ""conv4_3"")\n        pool4 = self.max_pool(conv4_3, \'pool4\')\n\n        conv5_1 = self.conv_layer(pool4, ""conv5_1"")\n        conv5_2 = self.conv_layer(conv5_1, ""conv5_2"")\n        conv5_3 = self.conv_layer(conv5_2, ""conv5_3"")\n        pool5 = self.max_pool(conv5_3, \'pool5\')\n\n        # detach original VGG fc layers and\n        # reconstruct your own fc layers serve for your own purpose\n        self.flatten = tf.reshape(pool5, [-1, 7*7*512])\n        self.fc6 = tf.layers.dense(self.flatten, 256, tf.nn.relu, name=\'fc6\')\n        self.out = tf.layers.dense(self.fc6, 1, name=\'out\')\n\n        self.sess = tf.Session()\n        if restore_from:\n            saver = tf.train.Saver()\n            saver.restore(self.sess, restore_from)\n        else:   # training graph\n            self.loss = tf.losses.mean_squared_error(labels=self.tfy, predictions=self.out)\n            self.train_op = tf.train.RMSPropOptimizer(0.001).minimize(self.loss)\n            self.sess.run(tf.global_variables_initializer())\n\n    def max_pool(self, bottom, name):\n        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=name)\n\n    def conv_layer(self, bottom, name):\n        with tf.variable_scope(name):   # CNN\'s filter is constant, NOT Variable that can be trained\n            conv = tf.nn.conv2d(bottom, self.data_dict[name][0], [1, 1, 1, 1], padding=\'SAME\')\n            lout = tf.nn.relu(tf.nn.bias_add(conv, self.data_dict[name][1]))\n            return lout\n\n    def train(self, x, y):\n        loss, _ = self.sess.run([self.loss, self.train_op], {self.tfx: x, self.tfy: y})\n        return loss\n\n    def predict(self, paths):\n        fig, axs = plt.subplots(1, 2)\n        for i, path in enumerate(paths):\n            x = load_img(path)\n            length = self.sess.run(self.out, {self.tfx: x})\n            axs[i].imshow(x[0])\n            axs[i].set_title(\'Len: %.1f cm\' % length)\n            axs[i].set_xticks(()); axs[i].set_yticks(())\n        plt.show()\n\n    def save(self, path=\'./for_transfer_learning/model/transfer_learn\'):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path, write_meta_graph=False)\n\n\ndef train():\n    tigers_x, cats_x, tigers_y, cats_y = load_data()\n\n    # plot fake length distribution\n    plt.hist(tigers_y, bins=20, label=\'Tigers\')\n    plt.hist(cats_y, bins=10, label=\'Cats\')\n    plt.legend()\n    plt.xlabel(\'length\')\n    plt.show()\n\n    xs = np.concatenate(tigers_x + cats_x, axis=0)\n    ys = np.concatenate((tigers_y, cats_y), axis=0)\n\n    vgg = Vgg16(vgg16_npy_path=\'./for_transfer_learning/vgg16.npy\')\n    print(\'Net built\')\n    for i in range(100):\n        b_idx = np.random.randint(0, len(xs), 6)\n        train_loss = vgg.train(xs[b_idx], ys[b_idx])\n        print(i, \'train loss: \', train_loss)\n\n    vgg.save(\'./for_transfer_learning/model/transfer_learn\')      # save learned fc layers\n\n\ndef eval():\n    vgg = Vgg16(vgg16_npy_path=\'./for_transfer_learning/vgg16.npy\',\n                restore_from=\'./for_transfer_learning/model/transfer_learn\')\n    vgg.predict(\n        [\'./for_transfer_learning/data/kittycat/000129037.jpg\', \'./for_transfer_learning/data/tiger/391412.jpg\'])\n\n\nif __name__ == \'__main__\':\n    # download()\n    # train()\n    eval()\n'"
tutorial-contents/501_dropout.py,18,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# Hyper parameters\nN_SAMPLES = 20\nN_HIDDEN = 300\nLR = 0.01\n\n# training data\nx = np.linspace(-1, 1, N_SAMPLES)[:, np.newaxis]\ny = x + 0.3*np.random.randn(N_SAMPLES)[:, np.newaxis]\n\n# test data\ntest_x = x.copy()\ntest_y = test_x + 0.3*np.random.randn(N_SAMPLES)[:, np.newaxis]\n\n# show data\nplt.scatter(x, y, c=\'magenta\', s=50, alpha=0.5, label=\'train\')\nplt.scatter(test_x, test_y, c=\'cyan\', s=50, alpha=0.5, label=\'test\')\nplt.legend(loc=\'upper left\')\nplt.ylim((-2.5, 2.5))\nplt.show()\n\n# tf placeholders\ntf_x = tf.placeholder(tf.float32, [None, 1])\ntf_y = tf.placeholder(tf.float32, [None, 1])\ntf_is_training = tf.placeholder(tf.bool, None)  # to control dropout when training and testing\n\n# overfitting net\no1 = tf.layers.dense(tf_x, N_HIDDEN, tf.nn.relu)\no2 = tf.layers.dense(o1, N_HIDDEN, tf.nn.relu)\no_out = tf.layers.dense(o2, 1)\no_loss = tf.losses.mean_squared_error(tf_y, o_out)\no_train = tf.train.AdamOptimizer(LR).minimize(o_loss)\n\n# dropout net\nd1 = tf.layers.dense(tf_x, N_HIDDEN, tf.nn.relu)\nd1 = tf.layers.dropout(d1, rate=0.5, training=tf_is_training)   # drop out 50% of inputs\nd2 = tf.layers.dense(d1, N_HIDDEN, tf.nn.relu)\nd2 = tf.layers.dropout(d2, rate=0.5, training=tf_is_training)   # drop out 50% of inputs\nd_out = tf.layers.dense(d2, 1)\nd_loss = tf.losses.mean_squared_error(tf_y, d_out)\nd_train = tf.train.AdamOptimizer(LR).minimize(d_loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nplt.ion()   # something about plotting\n\nfor t in range(500):\n    sess.run([o_train, d_train], {tf_x: x, tf_y: y, tf_is_training: True})  # train, set is_training=True\n\n    if t % 10 == 0:\n        # plotting\n        plt.cla()\n        o_loss_, d_loss_, o_out_, d_out_ = sess.run(\n            [o_loss, d_loss, o_out, d_out], {tf_x: test_x, tf_y: test_y, tf_is_training: False} # test, set is_training=False\n        )\n        plt.scatter(x, y, c=\'magenta\', s=50, alpha=0.3, label=\'train\'); plt.scatter(test_x, test_y, c=\'cyan\', s=50, alpha=0.3, label=\'test\')\n        plt.plot(test_x, o_out_, \'r-\', lw=3, label=\'overfitting\'); plt.plot(test_x, d_out_, \'b--\', lw=3, label=\'dropout(50%)\')\n        plt.text(0, -1.2, \'overfitting loss=%.4f\' % o_loss_, fontdict={\'size\': 20, \'color\':  \'red\'}); plt.text(0, -1.5, \'dropout loss=%.4f\' % d_loss_, fontdict={\'size\': 20, \'color\': \'blue\'})\n        plt.legend(loc=\'upper left\'); plt.ylim((-2.5, 2.5)); plt.pause(0.1)\n\nplt.ioff()\nplt.show()'"
tutorial-contents/502_batch_normalization.py,17,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntf.set_random_seed(1)\nnp.random.seed(1)\n\n# Hyper parameters\nN_SAMPLES = 2000\nBATCH_SIZE = 64\nEPOCH = 12\nLR = 0.03\nN_HIDDEN = 8\nACTIVATION = tf.nn.tanh\nB_INIT = tf.constant_initializer(-0.2)      # use a bad bias initialization\n\n# training data\nx = np.linspace(-7, 10, N_SAMPLES)[:, np.newaxis]\nnp.random.shuffle(x)\nnoise = np.random.normal(0, 2, x.shape)\ny = np.square(x) - 5 + noise\ntrain_data = np.hstack((x, y))\n\n# test data\ntest_x = np.linspace(-7, 10, 200)[:, np.newaxis]\nnoise = np.random.normal(0, 2, test_x.shape)\ntest_y = np.square(test_x) - 5 + noise\n\n# plot input data\nplt.scatter(x, y, c=\'#FF9359\', s=50, alpha=0.5, label=\'train\')\nplt.legend(loc=\'upper left\')\n\n# tensorflow placeholder\ntf_x = tf.placeholder(tf.float32, [None, 1])\ntf_y = tf.placeholder(tf.float32, [None, 1])\ntf_is_train = tf.placeholder(tf.bool, None)     # flag for using BN on training or testing\n\n\nclass NN(object):\n    def __init__(self, batch_normalization=False):\n        self.is_bn = batch_normalization\n\n        self.w_init = tf.random_normal_initializer(0., .1)  # weights initialization\n        self.pre_activation = [tf_x]\n        if self.is_bn:\n            self.layer_input = [tf.layers.batch_normalization(tf_x, training=tf_is_train)]  # for input data\n        else:\n            self.layer_input = [tf_x]\n        for i in range(N_HIDDEN):  # adding hidden layers\n            self.layer_input.append(self.add_layer(self.layer_input[-1], 10, ac=ACTIVATION))\n        self.out = tf.layers.dense(self.layer_input[-1], 1, kernel_initializer=self.w_init, bias_initializer=B_INIT)\n        self.loss = tf.losses.mean_squared_error(tf_y, self.out)\n\n        # !! IMPORTANT !! the moving_mean and moving_variance need to be updated,\n        # pass the update_ops with control_dependencies to the train_op\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            self.train = tf.train.AdamOptimizer(LR).minimize(self.loss)\n\n    def add_layer(self, x, out_size, ac=None):\n        x = tf.layers.dense(x, out_size, kernel_initializer=self.w_init, bias_initializer=B_INIT)\n        self.pre_activation.append(x)\n        # the momentum plays important rule. the default 0.99 is too high in this case!\n        if self.is_bn: x = tf.layers.batch_normalization(x, momentum=0.4, training=tf_is_train)    # when have BN\n        out = x if ac is None else ac(x)\n        return out\n\nnets = [NN(batch_normalization=False), NN(batch_normalization=True)]    # two nets, with and without BN\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# plot layer input distribution\nf, axs = plt.subplots(4, N_HIDDEN+1, figsize=(10, 5))\nplt.ion()   # something about plotting\n\ndef plot_histogram(l_in, l_in_bn, pre_ac, pre_ac_bn):\n    for i, (ax_pa, ax_pa_bn, ax,  ax_bn) in enumerate(zip(axs[0, :], axs[1, :], axs[2, :], axs[3, :])):\n        [a.clear() for a in [ax_pa, ax_pa_bn, ax, ax_bn]]\n        if i == 0: p_range = (-7, 10); the_range = (-7, 10)\n        else: p_range = (-4, 4); the_range = (-1, 1)\n        ax_pa.set_title(\'L\' + str(i))\n        ax_pa.hist(pre_ac[i].ravel(), bins=10, range=p_range, color=\'#FF9359\', alpha=0.5)\n        ax_pa_bn.hist(pre_ac_bn[i].ravel(), bins=10, range=p_range, color=\'#74BCFF\', alpha=0.5)\n        ax.hist(l_in[i].ravel(), bins=10, range=the_range, color=\'#FF9359\')\n        ax_bn.hist(l_in_bn[i].ravel(), bins=10, range=the_range, color=\'#74BCFF\')\n        for a in [ax_pa, ax, ax_pa_bn, ax_bn]:\n            a.set_yticks(()); a.set_xticks(())\n        ax_pa_bn.set_xticks(p_range); ax_bn.set_xticks(the_range); axs[2, 0].set_ylabel(\'Act\'); axs[3, 0].set_ylabel(\'BN Act\')\n    plt.pause(0.01)\n\nlosses = [[], []]   # record test loss\nfor epoch in range(EPOCH):\n    print(\'Epoch: \', epoch)\n    np.random.shuffle(train_data)\n    step = 0\n    in_epoch = True\n    while in_epoch:\n        b_s, b_f = (step*BATCH_SIZE) % len(train_data), ((step+1)*BATCH_SIZE) % len(train_data) # batch index\n        step += 1\n        if b_f < b_s:\n            b_f = len(train_data)\n            in_epoch = False\n        b_x, b_y = train_data[b_s: b_f, 0:1], train_data[b_s: b_f, 1:2]         # batch training data\n        sess.run([nets[0].train, nets[1].train], {tf_x: b_x, tf_y: b_y, tf_is_train: True})     # train\n\n        if step == 1:\n            l0, l1, l_in, l_in_bn, pa, pa_bn = sess.run(\n                [nets[0].loss, nets[1].loss, nets[0].layer_input, nets[1].layer_input,\n                 nets[0].pre_activation, nets[1].pre_activation],\n                {tf_x: test_x, tf_y: test_y, tf_is_train: False})\n            [loss.append(l) for loss, l in zip(losses, [l0, l1])]   # recode test loss\n            plot_histogram(l_in, l_in_bn, pa, pa_bn)     # plot histogram\n\nplt.ioff()\n\n# plot test loss\nplt.figure(2)\nplt.plot(losses[0], c=\'#FF9359\', lw=3, label=\'Original\')\nplt.plot(losses[1], c=\'#74BCFF\', lw=3, label=\'Batch Normalization\')\nplt.ylabel(\'test loss\'); plt.ylim((0, 2000)); plt.legend(loc=\'best\')\n\n# plot prediction line\npred, pred_bn = sess.run([nets[0].out, nets[1].out], {tf_x: test_x, tf_is_train: False})\nplt.figure(3)\nplt.plot(test_x, pred, c=\'#FF9359\', lw=4, label=\'Original\')\nplt.plot(test_x, pred_bn, c=\'#74BCFF\', lw=4, label=\'Batch Normalization\')\nplt.scatter(x[:200], y[:200], c=\'r\', s=50, alpha=0.2, label=\'train\')\nplt.legend(loc=\'best\'); plt.show()'"
tutorial-contents/503_visualize_gradient_descent.py,6,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.1.0\nmatplotlib\nnumpy\n""""""\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nLR = 0.1\nREAL_PARAMS = [1.2, 2.5]\nINIT_PARAMS = [[5, 4],\n               [5, 1],\n               [2, 4.5]][2]\n\nx = np.linspace(-1, 1, 200, dtype=np.float32)   # x data\n\n# Test (1): Visualize a simple linear function with two parameters,\n# you can change LR to 1 to see the different pattern in gradient descent.\n\n# y_fun = lambda a, b: a * x + b\n# tf_y_fun = lambda a, b: a * x + b\n\n\n# Test (2): Using Tensorflow as a calibrating tool for empirical formula like following.\n\n# y_fun = lambda a, b: a * x**3 + b * x**2\n# tf_y_fun = lambda a, b: a * x**3 + b * x**2\n\n\n# Test (3): Most simplest two parameters and two layers Neural Net, and their local & global minimum,\n# you can try different INIT_PARAMS set to visualize the gradient descent.\n\ny_fun = lambda a, b: np.sin(b*np.cos(a*x))\ntf_y_fun = lambda a, b: tf.sin(b*tf.cos(a*x))\n\nnoise = np.random.randn(200)/10\ny = y_fun(*REAL_PARAMS) + noise         # target\n\n# tensorflow graph\na, b = [tf.Variable(initial_value=p, dtype=tf.float32) for p in INIT_PARAMS]\npred = tf_y_fun(a, b)\nmse = tf.reduce_mean(tf.square(y-pred))\ntrain_op = tf.train.GradientDescentOptimizer(LR).minimize(mse)\n\na_list, b_list, cost_list = [], [], []\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for t in range(400):\n        a_, b_, mse_ = sess.run([a, b, mse])\n        a_list.append(a_); b_list.append(b_); cost_list.append(mse_)    # record parameter changes\n        result, _ = sess.run([pred, train_op])                          # training\n\n\n# visualization codes:\nprint(\'a=\', a_, \'b=\', b_)\nplt.figure(1)\nplt.scatter(x, y, c=\'b\')    # plot data\nplt.plot(x, result, \'r-\', lw=2)   # plot line fitting\n# 3D cost figure\nfig = plt.figure(2); ax = Axes3D(fig)\na3D, b3D = np.meshgrid(np.linspace(-2, 7, 30), np.linspace(-2, 7, 30))  # parameter space\ncost3D = np.array([np.mean(np.square(y_fun(a_, b_) - y)) for a_, b_ in zip(a3D.flatten(), b3D.flatten())]).reshape(a3D.shape)\nax.plot_surface(a3D, b3D, cost3D, rstride=1, cstride=1, cmap=plt.get_cmap(\'rainbow\'), alpha=0.5)\nax.scatter(a_list[0], b_list[0], zs=cost_list[0], s=300, c=\'r\')  # initial parameter place\nax.set_xlabel(\'a\'); ax.set_ylabel(\'b\')\nax.plot(a_list, b_list, zs=cost_list, zdir=\'z\', c=\'r\', lw=3)    # plot 3D gradient descent\nplt.show()'"
tutorial-contents/504_distributed_training.py,18,"b'""""""\nKnow more, visit my Python tutorial page: https://morvanzhou.github.io/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntensorflow: 1.4.0\n""""""\n\nimport tensorflow as tf\nimport multiprocessing as mp\nimport numpy as np\nimport os, shutil\n\n\nTRAINING = True\n\n# training data\nx = np.linspace(-1, 1, 100)[:, np.newaxis]\nnoise = np.random.normal(0, 0.1, size=x.shape)\ny = np.power(x, 2) + noise\n\n\ndef work(job_name, task_index, step, lock):\n    # set work\'s ip:port, parameter server and worker are the same steps\n    cluster = tf.train.ClusterSpec({\n        ""ps"": [\'localhost:2221\', ],\n        ""worker"": [\'localhost:2222\', \'localhost:2223\', \'localhost:2224\',]\n    })\n    server = tf.train.Server(cluster, job_name=job_name, task_index=task_index)\n\n    if job_name == \'ps\':\n        # join parameter server\n        print(\'Start Parameter Server: \', task_index)\n        server.join()\n    else:\n        print(\'Start Worker: \', task_index, \'pid: \', mp.current_process().pid)\n        # worker job\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=""/job:worker/task:%d"" % task_index,\n                cluster=cluster)):\n            # build network\n            tf_x = tf.placeholder(tf.float32, x.shape)\n            tf_y = tf.placeholder(tf.float32, y.shape)\n            l1 = tf.layers.dense(tf_x, 10, tf.nn.relu)\n            output = tf.layers.dense(l1, 1)\n            loss = tf.losses.mean_squared_error(tf_y, output)\n            global_step = tf.train.get_or_create_global_step()\n            train_op = tf.train.GradientDescentOptimizer(\n                learning_rate=0.001).minimize(loss, global_step=global_step)\n\n        # set training steps\n        hooks = [tf.train.StopAtStepHook(last_step=100000)]\n\n        # get session\n        with tf.train.MonitoredTrainingSession(master=server.target,\n                                               is_chief=(task_index == 0),\n                                               checkpoint_dir=\'./tmp\',\n                                               hooks=hooks) as mon_sess:\n            print(""Start Worker Session: "", task_index)\n            while not mon_sess.should_stop():\n                # train\n                _, loss_ = mon_sess.run([train_op, loss], {tf_x: x, tf_y: y})\n                with lock:\n                    step.value += 1\n                if step.value % 500 == 0:\n                    print(""Task: "", task_index, ""| Step: "", step.value, ""| Loss: "", loss_)\n        print(\'Worker Done: \', task_index)\n\n\ndef parallel_train():\n    if os.path.exists(\'./tmp\'):\n        shutil.rmtree(\'./tmp\')\n    # use multiprocessing to create a local cluster with 2 parameter servers and 4 workers\n    jobs = [(\'ps\', 0), (\'worker\', 0), (\'worker\', 1), (\'worker\', 2)]\n    step = mp.Value(\'i\', 0)\n    lock = mp.Lock()\n    ps = [mp.Process(target=work, args=(j, i, step, lock), ) for j, i in jobs]\n    [p.start() for p in ps]\n    [p.join() for p in ps]\n\n\ndef eval():\n    tf_x = tf.placeholder(tf.float32, [None, 1])\n    l1 = tf.layers.dense(tf_x, 10, tf.nn.relu)\n    output = tf.layers.dense(l1, 1)\n    saver = tf.train.Saver()\n    sess = tf.Session()\n    saver.restore(sess, tf.train.latest_checkpoint(\'./tmp\'))\n    result = sess.run(output, {tf_x: x})\n    # plot\n    import matplotlib.pyplot as plt\n    plt.scatter(x.ravel(), y, c=\'b\')\n    plt.plot(x.ravel(), result.ravel(), c=\'r\')\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    if TRAINING:\n        parallel_train()\n    else:\n        eval()\n'"
