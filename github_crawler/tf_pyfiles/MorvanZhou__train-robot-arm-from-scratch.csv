file_path,api_count,code
final/env.py,0,"b""import numpy as np\nimport pyglet\n\n\nclass ArmEnv(object):\n    viewer = None\n    dt = .1    # refresh rate\n    action_bound = [-1, 1]\n    goal = {'x': 100., 'y': 100., 'l': 40}\n    state_dim = 9\n    action_dim = 2\n\n    def __init__(self):\n        self.arm_info = np.zeros(\n            2, dtype=[('l', np.float32), ('r', np.float32)])\n        self.arm_info['l'] = 100        # 2 arms length\n        self.arm_info['r'] = np.pi/6    # 2 angles information\n        self.on_goal = 0\n\n    def step(self, action):\n        done = False\n        action = np.clip(action, *self.action_bound)\n        self.arm_info['r'] += action * self.dt\n        self.arm_info['r'] %= np.pi * 2    # normalize\n\n        (a1l, a2l) = self.arm_info['l']  # radius, arm length\n        (a1r, a2r) = self.arm_info['r']  # radian, angle\n        a1xy = np.array([200., 200.])    # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy  # a1 end and a2 start (x1, y1)\n        finger = np.array([np.cos(a1r + a2r), np.sin(a1r + a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n        # normalize features\n        dist1 = [(self.goal['x'] - a1xy_[0]) / 400, (self.goal['y'] - a1xy_[1]) / 400]\n        dist2 = [(self.goal['x'] - finger[0]) / 400, (self.goal['y'] - finger[1]) / 400]\n        r = -np.sqrt(dist2[0]**2+dist2[1]**2)\n\n        # done and reward\n        if self.goal['x'] - self.goal['l']/2 < finger[0] < self.goal['x'] + self.goal['l']/2:\n            if self.goal['y'] - self.goal['l']/2 < finger[1] < self.goal['y'] + self.goal['l']/2:\n                r += 1.\n                self.on_goal += 1\n                if self.on_goal > 50:\n                    done = True\n        else:\n            self.on_goal = 0\n\n        # state\n        s = np.concatenate((a1xy_/200, finger/200, dist1 + dist2, [1. if self.on_goal else 0.]))\n        return s, r, done\n\n    def reset(self):\n        self.goal['x'] = np.random.rand()*400.\n        self.goal['y'] = np.random.rand()*400.\n        self.arm_info['r'] = 2 * np.pi * np.random.rand(2)\n        self.on_goal = 0\n        (a1l, a2l) = self.arm_info['l']  # radius, arm length\n        (a1r, a2r) = self.arm_info['r']  # radian, angle\n        a1xy = np.array([200., 200.])  # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy  # a1 end and a2 start (x1, y1)\n        finger = np.array([np.cos(a1r + a2r), np.sin(a1r + a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n        # normalize features\n        dist1 = [(self.goal['x'] - a1xy_[0])/400, (self.goal['y'] - a1xy_[1])/400]\n        dist2 = [(self.goal['x'] - finger[0])/400, (self.goal['y'] - finger[1])/400]\n        # state\n        s = np.concatenate((a1xy_/200, finger/200, dist1 + dist2, [1. if self.on_goal else 0.]))\n        return s\n\n    def render(self):\n        if self.viewer is None:\n            self.viewer = Viewer(self.arm_info, self.goal)\n        self.viewer.render()\n\n    def sample_action(self):\n        return np.random.rand(2)-0.5    # two radians\n\n\nclass Viewer(pyglet.window.Window):\n    bar_thc = 5\n\n    def __init__(self, arm_info, goal):\n        # vsync=False to not use the monitor FPS, we can speed up training\n        super(Viewer, self).__init__(width=400, height=400, resizable=False, caption='Arm', vsync=False)\n        pyglet.gl.glClearColor(1, 1, 1, 1)\n        self.arm_info = arm_info\n        self.goal_info = goal\n        self.center_coord = np.array([200, 200])\n\n        self.batch = pyglet.graphics.Batch()    # display whole batch at once\n        self.goal = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,    # 4 corners\n            ('v2f', [goal['x'] - goal['l'] / 2, goal['y'] - goal['l'] / 2,                # location\n                     goal['x'] - goal['l'] / 2, goal['y'] + goal['l'] / 2,\n                     goal['x'] + goal['l'] / 2, goal['y'] + goal['l'] / 2,\n                     goal['x'] + goal['l'] / 2, goal['y'] - goal['l'] / 2]),\n            ('c3B', (86, 109, 249) * 4))    # color\n        self.arm1 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [250, 250,                # location\n                     250, 300,\n                     260, 300,\n                     260, 250]),\n            ('c3B', (249, 86, 86) * 4,))    # color\n        self.arm2 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [100, 150,              # location\n                     100, 160,\n                     200, 160,\n                     200, 150]), ('c3B', (249, 86, 86) * 4,))\n\n    def render(self):\n        self._update_arm()\n        self.switch_to()\n        self.dispatch_events()\n        self.dispatch_event('on_draw')\n        self.flip()\n\n    def on_draw(self):\n        self.clear()\n        self.batch.draw()\n\n    def _update_arm(self):\n        # update goal\n        self.goal.vertices = (\n            self.goal_info['x'] - self.goal_info['l']/2, self.goal_info['y'] - self.goal_info['l']/2,\n            self.goal_info['x'] + self.goal_info['l']/2, self.goal_info['y'] - self.goal_info['l']/2,\n            self.goal_info['x'] + self.goal_info['l']/2, self.goal_info['y'] + self.goal_info['l']/2,\n            self.goal_info['x'] - self.goal_info['l']/2, self.goal_info['y'] + self.goal_info['l']/2)\n\n        # update arm\n        (a1l, a2l) = self.arm_info['l']     # radius, arm length\n        (a1r, a2r) = self.arm_info['r']     # radian, angle\n        a1xy = self.center_coord            # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy   # a1 end and a2 start (x1, y1)\n        a2xy_ = np.array([np.cos(a1r+a2r), np.sin(a1r+a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n\n        a1tr, a2tr = np.pi / 2 - self.arm_info['r'][0], np.pi / 2 - self.arm_info['r'].sum()\n        xy01 = a1xy + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n        xy02 = a1xy + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy11 = a1xy_ + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy12 = a1xy_ + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n\n        xy11_ = a1xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n        xy12_ = a1xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy21 = a2xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy22 = a2xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n\n        self.arm1.vertices = np.concatenate((xy01, xy02, xy11, xy12))\n        self.arm2.vertices = np.concatenate((xy11_, xy12_, xy21, xy22))\n\n    # convert the mouse coordinate to goal's coordinate\n    def on_mouse_motion(self, x, y, dx, dy):\n        self.goal_info['x'] = x\n        self.goal_info['y'] = y\n\n\nif __name__ == '__main__':\n    env = ArmEnv()\n    while True:\n        env.render()\n        env.step(env.sample_action())"""
final/main.py,0,"b'""""""\nMake it more robust.\nStop episode once the finger stop at the final position for 50 steps.\nFeature & reward engineering.\n""""""\nfrom final.env import ArmEnv\nfrom final.rl import DDPG\n\nMAX_EPISODES = 900\nMAX_EP_STEPS = 200\nON_TRAIN = True\n\n# set env\nenv = ArmEnv()\ns_dim = env.state_dim\na_dim = env.action_dim\na_bound = env.action_bound\n\n# set RL method (continuous)\nrl = DDPG(a_dim, s_dim, a_bound)\n\nsteps = []\ndef train():\n    # start training\n    for i in range(MAX_EPISODES):\n        s = env.reset()\n        ep_r = 0.\n        for j in range(MAX_EP_STEPS):\n            # env.render()\n\n            a = rl.choose_action(s)\n\n            s_, r, done = env.step(a)\n\n            rl.store_transition(s, a, r, s_)\n\n            ep_r += r\n            if rl.memory_full:\n                # start to learn once has fulfilled the memory\n                rl.learn()\n\n            s = s_\n            if done or j == MAX_EP_STEPS-1:\n                print(\'Ep: %i | %s | ep_r: %.1f | step: %i\' % (i, \'---\' if not done else \'done\', ep_r, j))\n                break\n    rl.save()\n\n\ndef eval():\n    rl.restore()\n    env.render()\n    env.viewer.set_vsync(True)\n    s = env.reset()\n    while True:\n        env.render()\n        a = rl.choose_action(s)\n        s, r, done = env.step(a)\n\n\nif ON_TRAIN:\n    train()\nelse:\n    eval()\n\n\n\n'"
final/rl.py,28,"b""import tensorflow as tf\nimport numpy as np\n\n#####################  hyper parameters  ####################\n\nLR_A = 0.001    # learning rate for actor\nLR_C = 0.001    # learning rate for critic\nGAMMA = 0.9     # reward discount\nTAU = 0.01      # soft replacement\nMEMORY_CAPACITY = 30000\nBATCH_SIZE = 32\n\n\nclass DDPG(object):\n    def __init__(self, a_dim, s_dim, a_bound,):\n        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n        self.pointer = 0\n        self.memory_full = False\n        self.sess = tf.Session()\n        self.a_replace_counter, self.c_replace_counter = 0, 0\n\n        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound[1]\n        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n\n        with tf.variable_scope('Actor'):\n            self.a = self._build_a(self.S, scope='eval', trainable=True)\n            a_ = self._build_a(self.S_, scope='target', trainable=False)\n        with tf.variable_scope('Critic'):\n            # assign self.a = a in memory when calculating q for td_error,\n            # otherwise the self.a is from Actor when updating Actor\n            q = self._build_c(self.S, self.a, scope='eval', trainable=True)\n            q_ = self._build_c(self.S_, a_, scope='target', trainable=False)\n\n        # networks parameters\n        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n\n        # target net replacement\n        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), tf.assign(tc, (1 - TAU) * tc + TAU * ec)]\n                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n\n        q_target = self.R + GAMMA * q_\n        # in the feed_dic for the td_error, the self.a should change to actions in memory\n        td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=self.ce_params)\n\n        a_loss = - tf.reduce_mean(q)    # maximize the q\n        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n\n        self.sess.run(tf.global_variables_initializer())\n\n    def choose_action(self, s):\n        return self.sess.run(self.a, {self.S: s[None, :]})[0]\n\n    def learn(self):\n        # soft target replacement\n        self.sess.run(self.soft_replace)\n\n        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n        bt = self.memory[indices, :]\n        bs = bt[:, :self.s_dim]\n        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n        br = bt[:, -self.s_dim - 1: -self.s_dim]\n        bs_ = bt[:, -self.s_dim:]\n\n        self.sess.run(self.atrain, {self.S: bs})\n        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n\n    def store_transition(self, s, a, r, s_):\n        transition = np.hstack((s, a, [r], s_))\n        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n        self.memory[index, :] = transition\n        self.pointer += 1\n        if self.pointer > MEMORY_CAPACITY:      # indicator for learning\n            self.memory_full = True\n\n    def _build_a(self, s, scope, trainable):\n        with tf.variable_scope(scope):\n            net = tf.layers.dense(s, 300, activation=tf.nn.relu, name='l1', trainable=trainable)\n            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='a', trainable=trainable)\n            return tf.multiply(a, self.a_bound, name='scaled_a')\n\n    def _build_c(self, s, a, scope, trainable):\n        with tf.variable_scope(scope):\n            n_l1 = 300\n            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)\n            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)\n            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n\n    def save(self):\n        saver = tf.train.Saver()\n        saver.save(self.sess, './params', write_meta_graph=False)\n\n    def restore(self):\n        saver = tf.train.Saver()\n        saver.restore(self.sess, './params')\n\n"""
part1/env.py,0,"b'\nclass ArmEnv(object):\n\n    def __init__(self):\n        pass\n\n    def step(self, action):\n        pass\n\n    def reset(self):\n        pass\n\n    def render(self):\n        pass\n\n'"
part1/main.py,0,"b'""""""\nBuild the basic framework for main.py, rl.py and env.py.\n""""""\nfrom part1.env import ArmEnv\nfrom part1.rl import DDPG\n\nMAX_EPISODES = 500\nMAX_EP_STEPS = 200\n\n# set env\nenv = ArmEnv()\ns_dim = env.state_dim\na_dim = env.action_dim\na_bound = env.action_bound\n\n# set RL method\nrl = DDPG(a_dim, s_dim, a_bound)\n\n# start training\nfor i in range(MAX_EPISODES):\n    s = env.reset()\n    for j in range(MAX_EP_STEPS):\n        env.render()\n\n        a = rl.choose_action(s)\n\n        s_, r, done = env.step(a)\n\n        rl.store_transition(s, a, r, s_)\n\n        if rl.memory_full:\n            # start to learn once has fulfilled the memory\n            rl.learn()\n\n        s = s_\n\n# summary:\n""""""\nenv should have at least:\nenv.reset()\nenv.render()\nenv.step()\n\nwhile RL should have at least:\nrl.choose_action()\nrl.store_transition()\nrl.learn()\nrl.memory_full\n""""""\n\n\n\n'"
part1/rl.py,0,"b'\n\nclass DDPG(object):\n    def __init__(self, a_dim, s_dim, a_bound,):\n        pass\n\n    def choose_action(self, s):\n        pass\n\n    def learn(self):\n        pass\n\n    def store_transition(self, s, a, r, s_):\n        pass\n\n\n\n'"
part2/env.py,0,"b""import numpy as np\nimport pyglet\n\n\nclass ArmEnv(object):\n    viewer = None\n\n    def __init__(self):\n        pass\n\n    def step(self, action):\n        pass\n\n    def reset(self):\n        pass\n\n    def render(self):\n        if self.viewer is None:\n            self.viewer = Viewer()\n        self.viewer.render()\n\n\nclass Viewer(pyglet.window.Window):\n    bar_thc = 5\n\n    def __init__(self):\n        # vsync=False to not use the monitor FPS, we can speed up training\n        super(Viewer, self).__init__(width=400, height=400, resizable=False, caption='Arm', vsync=False)\n        pyglet.gl.glClearColor(1, 1, 1, 1)\n\n        self.batch = pyglet.graphics.Batch()    # display whole batch at once\n        self.point = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,    # 4 corners\n            ('v2f', [50, 50,                # location\n                     50, 100,\n                     100, 100,\n                     100, 50]),\n            ('c3B', (86, 109, 249) * 4))    # color\n        self.arm1 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [250, 250,                # location\n                     250, 300,\n                     260, 300,\n                     260, 250]),\n            ('c3B', (249, 86, 86) * 4,))    # color\n        self.arm2 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [100, 150,              # location\n                     100, 160,\n                     200, 160,\n                     200, 150]), ('c3B', (249, 86, 86) * 4,))\n\n    def render(self):\n        self._update_arm()\n        self.switch_to()\n        self.dispatch_events()\n        self.dispatch_event('on_draw')\n        self.flip()\n\n    def on_draw(self):\n        self.clear()\n        self.batch.draw()\n\n    def _update_arm(self):\n        pass\n\n\nif __name__ == '__main__':\n    env = ArmEnv()\n    while True:\n        env.render()"""
part2/main.py,0,"b'""""""\nBuild env.py, add basic components, and visualize it.\n""""""\nfrom part2.env import ArmEnv\nfrom part2.rl import DDPG\n\nMAX_EPISODES = 500\nMAX_EP_STEPS = 200\n\n# set env\nenv = ArmEnv()\ns_dim = env.state_dim\na_dim = env.action_dim\na_bound = env.action_bound\n\n# set RL method\nrl = DDPG(a_dim, s_dim, a_bound)\n\n# start training\nfor i in range(MAX_EPISODES):\n    s = env.reset()\n    for j in range(MAX_EP_STEPS):\n        env.render()\n\n        a = rl.choose_action(s)\n\n        s_, r, done = env.step(a)\n\n        rl.store_transition(s, a, r, s_)\n\n        if rl.memory_full:\n            # start to learn once has fulfilled the memory\n            rl.learn()\n\n        s = s_\n\n# summary:\n""""""\nenv should have at least:\nenv.reset()\nenv.render()\nenv.step()\n\nwhile RL should have at least:\nrl.choose_action()\nrl.store_transition()\nrl.learn()\nrl.memory_full\n""""""\n\n\n\n'"
part2/rl.py,0,"b'\n\nclass DDPG(object):\n    def __init__(self, a_dim, s_dim, a_bound,):\n        pass\n\n    def choose_action(self, s):\n        pass\n\n    def learn(self):\n        pass\n\n    def store_transition(self, s, a, r, s_):\n        pass\n\n\n\n'"
part3/env.py,0,"b""import numpy as np\nimport pyglet\n\n\nclass ArmEnv(object):\n    viewer = None\n    dt = 0.1    # refresh rate\n    action_bound = [-1, 1]\n    goal = {'x': 100., 'y': 100., 'l': 40}\n    state_dim = 2\n    action_dim = 2\n\n    def __init__(self):\n        self.arm_info = np.zeros(\n            2, dtype=[('l', np.float32), ('r', np.float32)])\n        self.arm_info['l'] = 100\n        self.arm_info['r'] = np.pi/6\n\n    def step(self, action):\n        done = False\n        r = 0.\n        action = np.clip(action, *self.action_bound)\n        self.arm_info['r'] += action * self.dt\n        self.arm_info['r'] %= np.pi * 2    # normalize\n\n        # state\n        s = self.arm_info['r']\n\n        (a1l, a2l) = self.arm_info['l']  # radius, arm length\n        (a1r, a2r) = self.arm_info['r']  # radian, angle\n        a1xy = np.array([200., 200.])    # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy  # a1 end and a2 start (x1, y1)\n        finger = np.array([np.cos(a1r + a2r), np.sin(a1r + a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n\n        # done and reward\n        if (self.goal['x'] - self.goal['l']/2 < finger[0] < self.goal['x'] + self.goal['l']/2\n        ) and (self.goal['y'] - self.goal['l']/2 < finger[1] < self.goal['y'] + self.goal['l']/2):\n                done = True\n                r = 1.\n        return s, r, done\n\n    def reset(self):\n        self.arm_info['r'] = 2 * np.pi * np.random.rand(2)\n        return self.arm_info['r']\n\n    def render(self):\n        if self.viewer is None:\n            self.viewer = Viewer(self.arm_info, self.goal)\n        self.viewer.render()\n\n    def sample_action(self):\n        return np.random.rand(2)-0.5    # two radians\n\n\nclass Viewer(pyglet.window.Window):\n    bar_thc = 5\n\n    def __init__(self, arm_info, goal):\n        # vsync=False to not use the monitor FPS, we can speed up training\n        super(Viewer, self).__init__(width=400, height=400, resizable=False, caption='Arm', vsync=False)\n        pyglet.gl.glClearColor(1, 1, 1, 1)\n        self.arm_info = arm_info\n        self.center_coord = np.array([200, 200])\n\n        self.batch = pyglet.graphics.Batch()    # display whole batch at once\n        self.goal = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,    # 4 corners\n            ('v2f', [goal['x'] - goal['l'] / 2, goal['y'] - goal['l'] / 2,                # location\n                     goal['x'] - goal['l'] / 2, goal['y'] + goal['l'] / 2,\n                     goal['x'] + goal['l'] / 2, goal['y'] + goal['l'] / 2,\n                     goal['x'] + goal['l'] / 2, goal['y'] - goal['l'] / 2]),\n            ('c3B', (86, 109, 249) * 4))    # color\n        self.arm1 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [250, 250,                # location\n                     250, 300,\n                     260, 300,\n                     260, 250]),\n            ('c3B', (249, 86, 86) * 4,))    # color\n        self.arm2 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [100, 150,              # location\n                     100, 160,\n                     200, 160,\n                     200, 150]), ('c3B', (249, 86, 86) * 4,))\n\n    def render(self):\n        self._update_arm()\n        self.switch_to()\n        self.dispatch_events()\n        self.dispatch_event('on_draw')\n        self.flip()\n\n    def on_draw(self):\n        self.clear()\n        self.batch.draw()\n\n    def _update_arm(self):\n        (a1l, a2l) = self.arm_info['l']     # radius, arm length\n        (a1r, a2r) = self.arm_info['r']     # radian, angle\n        a1xy = self.center_coord            # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy   # a1 end and a2 start (x1, y1)\n        a2xy_ = np.array([np.cos(a1r+a2r), np.sin(a1r+a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n\n        a1tr, a2tr = np.pi / 2 - self.arm_info['r'][0], np.pi / 2 - self.arm_info['r'].sum()\n        xy01 = a1xy + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n        xy02 = a1xy + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy11 = a1xy_ + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy12 = a1xy_ + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n\n        xy11_ = a1xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n        xy12_ = a1xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy21 = a2xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy22 = a2xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n\n        self.arm1.vertices = np.concatenate((xy01, xy02, xy11, xy12))\n        self.arm2.vertices = np.concatenate((xy11_, xy12_, xy21, xy22))\n\n\nif __name__ == '__main__':\n    env = ArmEnv()\n    while True:\n        env.render()\n        env.step(env.sample_action())"""
part3/main.py,0,"b'""""""\nComplete the env.py\n""""""\nfrom part3.env import ArmEnv\nfrom part3.rl import DDPG\n\nMAX_EPISODES = 500\nMAX_EP_STEPS = 200\n\n# set env\nenv = ArmEnv()\ns_dim = env.state_dim\na_dim = env.action_dim\na_bound = env.action_bound\n\n# set RL method\nrl = DDPG(a_dim, s_dim, a_bound)\n\n# start training\nfor i in range(MAX_EPISODES):\n    s = env.reset()\n    for j in range(MAX_EP_STEPS):\n        env.render()\n\n        a = rl.choose_action(s)\n\n        s_, r, done = env.step(a)\n\n        rl.store_transition(s, a, r, s_)\n\n        if rl.memory_full:\n            # start to learn once has fulfilled the memory\n            rl.learn()\n\n        s = s_\n\n# summary:\n""""""\nenv should have at least:\nenv.reset()\nenv.render()\nenv.step()\n\nwhile RL should have at least:\nrl.choose_action()\nrl.store_transition()\nrl.learn()\nrl.memory_full\n""""""\n\n\n\n'"
part3/rl.py,0,"b'\n\nclass DDPG(object):\n    def __init__(self, a_dim, s_dim, a_bound,):\n        pass\n\n    def choose_action(self, s):\n        pass\n\n    def learn(self):\n        pass\n\n    def store_transition(self, s, a, r, s_):\n        pass\n\n\n\n'"
part4/env.py,0,"b""import numpy as np\nimport pyglet\n\n\nclass ArmEnv(object):\n    viewer = None\n    dt = .1    # refresh rate\n    action_bound = [-1, 1]\n    goal = {'x': 100., 'y': 100., 'l': 40}\n    state_dim = 2\n    action_dim = 2\n\n    def __init__(self):\n        self.arm_info = np.zeros(\n            2, dtype=[('l', np.float32), ('r', np.float32)])\n        self.arm_info['l'] = 100        # 2 arms length\n        self.arm_info['r'] = np.pi/6    # 2 angles information\n\n    def step(self, action):\n        done = False\n        r = 0.\n        action = np.clip(action, *self.action_bound)\n        self.arm_info['r'] += action * self.dt\n        self.arm_info['r'] %= np.pi * 2    # normalize\n\n        # state\n        s = self.arm_info['r']\n\n        (a1l, a2l) = self.arm_info['l']  # radius, arm length\n        (a1r, a2r) = self.arm_info['r']  # radian, angle\n        a1xy = np.array([200., 200.])    # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy  # a1 end and a2 start (x1, y1)\n        finger = np.array([np.cos(a1r + a2r), np.sin(a1r + a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n\n        # done and reward\n        if (self.goal['x'] - self.goal['l']/2 < finger[0] < self.goal['x'] + self.goal['l']/2\n        ) and (self.goal['y'] - self.goal['l']/2 < finger[1] < self.goal['y'] + self.goal['l']/2):\n            done = True\n            r = 1.\n        return s, r, done\n\n    def reset(self):\n        self.arm_info['r'] = 2 * np.pi * np.random.rand(2)\n        return self.arm_info['r']\n\n    def render(self):\n        if self.viewer is None:\n            self.viewer = Viewer(self.arm_info, self.goal)\n        self.viewer.render()\n\n    def sample_action(self):\n        return np.random.rand(2)-0.5    # two radians\n\n\nclass Viewer(pyglet.window.Window):\n    bar_thc = 5\n\n    def __init__(self, arm_info, goal):\n        # vsync=False to not use the monitor FPS, we can speed up training\n        super(Viewer, self).__init__(width=400, height=400, resizable=False, caption='Arm', vsync=False)\n        pyglet.gl.glClearColor(1, 1, 1, 1)\n        self.arm_info = arm_info\n        self.center_coord = np.array([200, 200])\n\n        self.batch = pyglet.graphics.Batch()    # display whole batch at once\n        self.goal = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,    # 4 corners\n            ('v2f', [goal['x'] - goal['l'] / 2, goal['y'] - goal['l'] / 2,                # location\n                     goal['x'] - goal['l'] / 2, goal['y'] + goal['l'] / 2,\n                     goal['x'] + goal['l'] / 2, goal['y'] + goal['l'] / 2,\n                     goal['x'] + goal['l'] / 2, goal['y'] - goal['l'] / 2]),\n            ('c3B', (86, 109, 249) * 4))    # color\n        self.arm1 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [250, 250,                # location\n                     250, 300,\n                     260, 300,\n                     260, 250]),\n            ('c3B', (249, 86, 86) * 4,))    # color\n        self.arm2 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [100, 150,              # location\n                     100, 160,\n                     200, 160,\n                     200, 150]), ('c3B', (249, 86, 86) * 4,))\n\n    def render(self):\n        self._update_arm()\n        self.switch_to()\n        self.dispatch_events()\n        self.dispatch_event('on_draw')\n        self.flip()\n\n    def on_draw(self):\n        self.clear()\n        self.batch.draw()\n\n    def _update_arm(self):\n        (a1l, a2l) = self.arm_info['l']     # radius, arm length\n        (a1r, a2r) = self.arm_info['r']     # radian, angle\n        a1xy = self.center_coord            # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy   # a1 end and a2 start (x1, y1)\n        a2xy_ = np.array([np.cos(a1r+a2r), np.sin(a1r+a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n\n        a1tr, a2tr = np.pi / 2 - self.arm_info['r'][0], np.pi / 2 - self.arm_info['r'].sum()\n        xy01 = a1xy + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n        xy02 = a1xy + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy11 = a1xy_ + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy12 = a1xy_ + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n\n        xy11_ = a1xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n        xy12_ = a1xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy21 = a2xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy22 = a2xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n\n        self.arm1.vertices = np.concatenate((xy01, xy02, xy11, xy12))\n        self.arm2.vertices = np.concatenate((xy11_, xy12_, xy21, xy22))\n\n\nif __name__ == '__main__':\n    env = ArmEnv()\n    while True:\n        env.render()\n        env.step(env.sample_action())"""
part4/main.py,0,"b'""""""\nPlug a RL method to the framework, this method can be discrete or continuous.\nThis script is based on a continuous action RL. If you want to change to discrete RL like DQN,\nplease change the env.py and rl.py correspondingly.\n""""""\nfrom part4.env import ArmEnv\nfrom part4.rl import DDPG\n\nMAX_EPISODES = 500\nMAX_EP_STEPS = 200\nON_TRAIN = True\n\n# set env\nenv = ArmEnv()\ns_dim = env.state_dim\na_dim = env.action_dim\na_bound = env.action_bound\n\n# set RL method (continuous)\nrl = DDPG(a_dim, s_dim, a_bound)\n\n\ndef train():\n    # start training\n    for i in range(MAX_EPISODES):\n        s = env.reset()\n        ep_r = 0.\n        for j in range(MAX_EP_STEPS):\n            env.render()\n\n            a = rl.choose_action(s)\n\n            s_, r, done = env.step(a)\n\n            rl.store_transition(s, a, r, s_)\n\n            ep_r += r\n            if rl.memory_full:\n                # start to learn once has fulfilled the memory\n                rl.learn()\n\n            s = s_\n            if done or j == MAX_EP_STEPS-1:\n                print(\'Ep: %i | %s | ep_r: %.1f | steps: %i\' % (i, \'---\' if not done else \'done\', ep_r, j))\n                break\n    rl.save()\n\n\ndef eval():\n    rl.restore()\n    env.render()\n    env.viewer.set_vsync(True)\n    while True:\n        s = env.reset()\n        for _ in range(200):\n            env.render()\n            a = rl.choose_action(s)\n            s, r, done = env.step(a)\n            if done:\n                break\n\n\nif ON_TRAIN:\n    train()\nelse:\n    eval()\n\n\n\n'"
part4/rl.py,28,"b""import tensorflow as tf\nimport numpy as np\n\n#####################  hyper parameters  ####################\n\nLR_A = 0.001    # learning rate for actor\nLR_C = 0.001    # learning rate for critic\nGAMMA = 0.9     # reward discount\nTAU = 0.01      # soft replacement\nMEMORY_CAPACITY = 10000\nBATCH_SIZE = 32\n\n\nclass DDPG(object):\n    def __init__(self, a_dim, s_dim, a_bound,):\n        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n        self.pointer = 0\n        self.memory_full = False\n        self.sess = tf.Session()\n        self.a_replace_counter, self.c_replace_counter = 0, 0\n\n        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound[1]\n        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n\n        with tf.variable_scope('Actor'):\n            self.a = self._build_a(self.S, scope='eval', trainable=True)\n            a_ = self._build_a(self.S_, scope='target', trainable=False)\n        with tf.variable_scope('Critic'):\n            # assign self.a = a in memory when calculating q for td_error,\n            # otherwise the self.a is from Actor when updating Actor\n            q = self._build_c(self.S, self.a, scope='eval', trainable=True)\n            q_ = self._build_c(self.S_, a_, scope='target', trainable=False)\n\n        # networks parameters\n        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n\n        # target net replacement\n        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), tf.assign(tc, (1 - TAU) * tc + TAU * ec)]\n                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n\n        q_target = self.R + GAMMA * q_\n        # in the feed_dic for the td_error, the self.a should change to actions in memory\n        td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=self.ce_params)\n\n        a_loss = - tf.reduce_mean(q)    # maximize the q\n        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n\n        self.sess.run(tf.global_variables_initializer())\n\n    def choose_action(self, s):\n        return self.sess.run(self.a, {self.S: s[None, :]})[0]\n\n    def learn(self):\n        # soft target replacement\n        self.sess.run(self.soft_replace)\n\n        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n        bt = self.memory[indices, :]\n        bs = bt[:, :self.s_dim]\n        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n        br = bt[:, -self.s_dim - 1: -self.s_dim]\n        bs_ = bt[:, -self.s_dim:]\n\n        self.sess.run(self.atrain, {self.S: bs})\n        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n\n    def store_transition(self, s, a, r, s_):\n        transition = np.hstack((s, a, [r], s_))\n        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n        self.memory[index, :] = transition\n        self.pointer += 1\n        if self.pointer > MEMORY_CAPACITY:      # indicator for learning\n            self.memory_full = True\n\n    def _build_a(self, s, scope, trainable):\n        with tf.variable_scope(scope):\n            net = tf.layers.dense(s, 100, activation=tf.nn.relu, name='l1', trainable=trainable)\n            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='a', trainable=trainable)\n            return tf.multiply(a, self.a_bound, name='scaled_a')\n\n    def _build_c(self, s, a, scope, trainable):\n        with tf.variable_scope(scope):\n            n_l1 = 100\n            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)\n            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)\n            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n\n    def save(self):\n        saver = tf.train.Saver()\n        saver.save(self.sess, './params', write_meta_graph=False)\n\n    def restore(self):\n        saver = tf.train.Saver()\n        saver.restore(self.sess, './params')\n\n"""
part5/env.py,0,"b""import numpy as np\nimport pyglet\n\n\nclass ArmEnv(object):\n    viewer = None\n    dt = .1    # refresh rate\n    action_bound = [-1, 1]\n    goal = {'x': 100., 'y': 100., 'l': 40}\n    state_dim = 9\n    action_dim = 2\n\n    def __init__(self):\n        self.arm_info = np.zeros(\n            2, dtype=[('l', np.float32), ('r', np.float32)])\n        self.arm_info['l'] = 100        # 2 arms length\n        self.arm_info['r'] = np.pi/6    # 2 angles information\n        self.on_goal = 0\n\n    def step(self, action):\n        done = False\n        action = np.clip(action, *self.action_bound)\n        self.arm_info['r'] += action * self.dt\n        self.arm_info['r'] %= np.pi * 2    # normalize\n\n        (a1l, a2l) = self.arm_info['l']  # radius, arm length\n        (a1r, a2r) = self.arm_info['r']  # radian, angle\n        a1xy = np.array([200., 200.])    # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy  # a1 end and a2 start (x1, y1)\n        finger = np.array([np.cos(a1r + a2r), np.sin(a1r + a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n        # normalize features\n        dist1 = [(self.goal['x'] - a1xy_[0]) / 400, (self.goal['y'] - a1xy_[1]) / 400]\n        dist2 = [(self.goal['x'] - finger[0]) / 400, (self.goal['y'] - finger[1]) / 400]\n        r = -np.sqrt(dist2[0]**2+dist2[1]**2)\n\n        # done and reward\n        if (self.goal['x'] - self.goal['l']/2 < finger[0] < self.goal['x'] + self.goal['l']/2\n        ) and (self.goal['y'] - self.goal['l']/2 < finger[1] < self.goal['y'] + self.goal['l']/2):\n            r += 1.\n            self.on_goal += 1\n            if self.on_goal > 50:\n                done = True\n        else:\n            self.on_goal = 0\n\n        # state\n        s = np.concatenate((a1xy_/200, finger/200, dist1 + dist2, [1. if self.on_goal else 0.]))\n        return s, r, done\n\n    def reset(self):\n        self.arm_info['r'] = 2 * np.pi * np.random.rand(2)\n        self.on_goal = 0\n        (a1l, a2l) = self.arm_info['l']  # radius, arm length\n        (a1r, a2r) = self.arm_info['r']  # radian, angle\n        a1xy = np.array([200., 200.])  # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy  # a1 end and a2 start (x1, y1)\n        finger = np.array([np.cos(a1r + a2r), np.sin(a1r + a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n        # normalize features\n        dist1 = [(self.goal['x'] - a1xy_[0])/400, (self.goal['y'] - a1xy_[1])/400]\n        dist2 = [(self.goal['x'] - finger[0])/400, (self.goal['y'] - finger[1])/400]\n        # state\n        s = np.concatenate((a1xy_/200, finger/200, dist1 + dist2, [1. if self.on_goal else 0.]))\n        return s\n\n    def render(self):\n        if self.viewer is None:\n            self.viewer = Viewer(self.arm_info, self.goal)\n        self.viewer.render()\n\n    def sample_action(self):\n        return np.random.rand(2)-0.5    # two radians\n\n\nclass Viewer(pyglet.window.Window):\n    bar_thc = 5\n\n    def __init__(self, arm_info, goal):\n        # vsync=False to not use the monitor FPS, we can speed up training\n        super(Viewer, self).__init__(width=400, height=400, resizable=False, caption='Arm', vsync=False)\n        pyglet.gl.glClearColor(1, 1, 1, 1)\n        self.arm_info = arm_info\n        self.center_coord = np.array([200, 200])\n\n        self.batch = pyglet.graphics.Batch()    # display whole batch at once\n        self.goal = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,    # 4 corners\n            ('v2f', [goal['x'] - goal['l'] / 2, goal['y'] - goal['l'] / 2,                # location\n                     goal['x'] - goal['l'] / 2, goal['y'] + goal['l'] / 2,\n                     goal['x'] + goal['l'] / 2, goal['y'] + goal['l'] / 2,\n                     goal['x'] + goal['l'] / 2, goal['y'] - goal['l'] / 2]),\n            ('c3B', (86, 109, 249) * 4))    # color\n        self.arm1 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [250, 250,                # location\n                     250, 300,\n                     260, 300,\n                     260, 250]),\n            ('c3B', (249, 86, 86) * 4,))    # color\n        self.arm2 = self.batch.add(\n            4, pyglet.gl.GL_QUADS, None,\n            ('v2f', [100, 150,              # location\n                     100, 160,\n                     200, 160,\n                     200, 150]), ('c3B', (249, 86, 86) * 4,))\n\n    def render(self):\n        self._update_arm()\n        self.switch_to()\n        self.dispatch_events()\n        self.dispatch_event('on_draw')\n        self.flip()\n\n    def on_draw(self):\n        self.clear()\n        self.batch.draw()\n\n    def _update_arm(self):\n        (a1l, a2l) = self.arm_info['l']     # radius, arm length\n        (a1r, a2r) = self.arm_info['r']     # radian, angle\n        a1xy = self.center_coord            # a1 start (x0, y0)\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy   # a1 end and a2 start (x1, y1)\n        a2xy_ = np.array([np.cos(a1r+a2r), np.sin(a1r+a2r)]) * a2l + a1xy_  # a2 end (x2, y2)\n\n        a1tr, a2tr = np.pi / 2 - self.arm_info['r'][0], np.pi / 2 - self.arm_info['r'].sum()\n        xy01 = a1xy + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n        xy02 = a1xy + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy11 = a1xy_ + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy12 = a1xy_ + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n\n        xy11_ = a1xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n        xy12_ = a1xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy21 = a2xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy22 = a2xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n\n        self.arm1.vertices = np.concatenate((xy01, xy02, xy11, xy12))\n        self.arm2.vertices = np.concatenate((xy11_, xy12_, xy21, xy22))\n\n\nif __name__ == '__main__':\n    env = ArmEnv()\n    while True:\n        env.render()\n        env.step(env.sample_action())"""
part5/main.py,0,"b'""""""\nMake it more robust.\nStop episode once the finger stop at the final position for 50 steps.\nFeature & reward engineering.\n""""""\nfrom part5.env import ArmEnv\nfrom part5.rl import DDPG\n\nMAX_EPISODES = 500\nMAX_EP_STEPS = 200\nON_TRAIN = True\n\n# set env\nenv = ArmEnv()\ns_dim = env.state_dim\na_dim = env.action_dim\na_bound = env.action_bound\n\n# set RL method (continuous)\nrl = DDPG(a_dim, s_dim, a_bound)\n\nsteps = []\ndef train():\n    # start training\n    for i in range(MAX_EPISODES):\n        s = env.reset()\n        ep_r = 0.\n        for j in range(MAX_EP_STEPS):\n            env.render()\n\n            a = rl.choose_action(s)\n\n            s_, r, done = env.step(a)\n\n            rl.store_transition(s, a, r, s_)\n\n            ep_r += r\n            if rl.memory_full:\n                # start to learn once has fulfilled the memory\n                rl.learn()\n\n            s = s_\n            if done or j == MAX_EP_STEPS-1:\n                print(\'Ep: %i | %s | ep_r: %.1f | step: %i\' % (i, \'---\' if not done else \'done\', ep_r, j))\n                break\n    rl.save()\n\n\ndef eval():\n    rl.restore()\n    env.render()\n    env.viewer.set_vsync(True)\n    while True:\n        s = env.reset()\n        for _ in range(200):\n            env.render()\n            a = rl.choose_action(s)\n            s, r, done = env.step(a)\n            if done:\n                break\n\n\nif ON_TRAIN:\n    train()\nelse:\n    eval()\n\n\n\n'"
part5/rl.py,28,"b""import tensorflow as tf\nimport numpy as np\n\n#####################  hyper parameters  ####################\n\nLR_A = 0.001    # learning rate for actor\nLR_C = 0.001    # learning rate for critic\nGAMMA = 0.9     # reward discount\nTAU = 0.01      # soft replacement\nMEMORY_CAPACITY = 10000\nBATCH_SIZE = 32\n\n\nclass DDPG(object):\n    def __init__(self, a_dim, s_dim, a_bound,):\n        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n        self.pointer = 0\n        self.memory_full = False\n        self.sess = tf.Session()\n        self.a_replace_counter, self.c_replace_counter = 0, 0\n\n        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound[1]\n        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n\n        with tf.variable_scope('Actor'):\n            self.a = self._build_a(self.S, scope='eval', trainable=True)\n            a_ = self._build_a(self.S_, scope='target', trainable=False)\n        with tf.variable_scope('Critic'):\n            # assign self.a = a in memory when calculating q for td_error,\n            # otherwise the self.a is from Actor when updating Actor\n            q = self._build_c(self.S, self.a, scope='eval', trainable=True)\n            q_ = self._build_c(self.S_, a_, scope='target', trainable=False)\n\n        # networks parameters\n        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n\n        # target net replacement\n        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), tf.assign(tc, (1 - TAU) * tc + TAU * ec)]\n                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n\n        q_target = self.R + GAMMA * q_\n        # in the feed_dic for the td_error, the self.a should change to actions in memory\n        td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=self.ce_params)\n\n        a_loss = - tf.reduce_mean(q)    # maximize the q\n        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n\n        self.sess.run(tf.global_variables_initializer())\n\n    def choose_action(self, s):\n        return self.sess.run(self.a, {self.S: s[None, :]})[0]\n\n    def learn(self):\n        # soft target replacement\n        self.sess.run(self.soft_replace)\n\n        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n        bt = self.memory[indices, :]\n        bs = bt[:, :self.s_dim]\n        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n        br = bt[:, -self.s_dim - 1: -self.s_dim]\n        bs_ = bt[:, -self.s_dim:]\n\n        self.sess.run(self.atrain, {self.S: bs})\n        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n\n    def store_transition(self, s, a, r, s_):\n        transition = np.hstack((s, a, [r], s_))\n        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n        self.memory[index, :] = transition\n        self.pointer += 1\n        if self.pointer > MEMORY_CAPACITY:      # indicator for learning\n            self.memory_full = True\n\n    def _build_a(self, s, scope, trainable):\n        with tf.variable_scope(scope):\n            net = tf.layers.dense(s, 100, activation=tf.nn.relu, name='l1', trainable=trainable)\n            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='a', trainable=trainable)\n            return tf.multiply(a, self.a_bound, name='scaled_a')\n\n    def _build_c(self, s, a, scope, trainable):\n        with tf.variable_scope(scope):\n            n_l1 = 100\n            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)\n            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)\n            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n\n    def save(self):\n        saver = tf.train.Saver()\n        saver.save(self.sess, './params', write_meta_graph=False)\n\n    def restore(self):\n        saver = tf.train.Saver()\n        saver.restore(self.sess, './params')\n\n"""
