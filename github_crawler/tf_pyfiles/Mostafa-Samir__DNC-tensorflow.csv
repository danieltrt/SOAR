file_path,api_count,code
dnc/__init__.py,0,b''
dnc/controller.py,31,"b'import tensorflow as tf\nimport numpy as np\n\nclass BaseController:\n\n    def __init__(self, input_size, output_size, memory_read_heads, memory_word_size, batch_size=1):\n        """"""\n        constructs a controller as described in the DNC paper:\n        http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html\n\n        Parameters:\n        ----------\n        input_size: int\n            the size of the data input vector\n        output_size: int\n            the size of the data output vector\n        memory_read_heads: int\n            the number of read haeds in the associated external memory\n        memory_word_size: int\n            the size of the word in the associated external memory\n        batch_size: int\n            the size of the input data batch [optional, usually set by the DNC object]\n        """"""\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.read_heads = memory_read_heads\n        self.word_size = memory_word_size\n        self.batch_size = batch_size\n\n        # indicates if the internal neural network is recurrent\n        # by the existence of recurrent_update and get_state methods\n        has_recurrent_update = callable(getattr(self, \'update_state\', None))\n        has_get_state = callable(getattr(self, \'get_state\', None))\n        self.has_recurrent_nn =  has_recurrent_update and has_get_state\n\n        # the actual size of the neural network input after flatenning and\n        # concatenating the input vector with the previously read vctors from memory\n        self.nn_input_size = self.word_size * self.read_heads + self.input_size\n\n        self.interface_vector_size = self.word_size * self.read_heads + 3 * self.word_size + 5 * self.read_heads + 3\n\n        # define network vars\n        with tf.name_scope(""controller""):\n            self.network_vars()\n\n            self.nn_output_size = None\n            with tf.variable_scope(""shape_inference""):\n                self.nn_output_size = self.get_nn_output_size()\n\n            self.initials()\n\n    def initials(self):\n        """"""\n        sets the initial values of the controller transformation weights matrices\n        this method can be overwritten to use a different initialization scheme\n        """"""\n        # defining internal weights of the controller\n        self.interface_weights = tf.Variable(\n            tf.random_normal([self.nn_output_size, self.interface_vector_size], stddev=0.1),\n            name=\'interface_weights\'\n        )\n        self.nn_output_weights = tf.Variable(\n            tf.random_normal([self.nn_output_size, self.output_size], stddev=0.1),\n            name=\'nn_output_weights\'\n        )\n        self.mem_output_weights = tf.Variable(\n            tf.random_normal([self.word_size * self.read_heads, self.output_size],  stddev=0.1),\n            name=\'mem_output_weights\'\n        )\n\n    def network_vars(self):\n        """"""\n        defines the variables needed by the internal neural network\n        [the variables should be attributes of the class, i.e. self.*]\n        """"""\n        raise NotImplementedError(""network_vars is not implemented"")\n\n\n    def network_op(self, X):\n        """"""\n        defines the controller\'s internal neural network operation\n\n        Parameters:\n        ----------\n        X: Tensor (batch_size, word_size * read_haeds + input_size)\n            the input data concatenated with the previously read vectors from memory\n\n        Returns: Tensor (batch_size, nn_output_size)\n        """"""\n        raise NotImplementedError(""network_op method is not implemented"")\n\n\n    def get_nn_output_size(self):\n        """"""\n        retrives the output size of the defined neural network\n\n        Returns: int\n            the output\'s size\n\n        Raises: ValueError\n        """"""\n\n        input_vector =  np.zeros([self.batch_size, self.nn_input_size], dtype=np.float32)\n        output_vector = None\n\n        if self.has_recurrent_nn:\n            output_vector,_ = self.network_op(input_vector, self.get_state())\n        else:\n            output_vector = self.network_op(input_vector)\n\n        shape = output_vector.get_shape().as_list()\n\n        if len(shape) > 2:\n            raise ValueError(""Expected the neural network to output a 1D vector, but got %dD"" % (len(shape) - 1))\n        else:\n            return shape[1]\n\n\n    def parse_interface_vector(self, interface_vector):\n        """"""\n        pasres the flat interface_vector into its various components with their\n        correct shapes\n\n        Parameters:\n        ----------\n        interface_vector: Tensor (batch_size, interface_vector_size)\n            the flattened inetrface vector to be parsed\n\n        Returns: dict\n            a dictionary with the components of the interface_vector parsed\n        """"""\n\n        parsed = {}\n\n        r_keys_end = self.word_size * self.read_heads\n        r_strengths_end = r_keys_end + self.read_heads\n        w_key_end = r_strengths_end + self.word_size\n        erase_end = w_key_end + 1 + self.word_size\n        write_end = erase_end + self.word_size\n        free_end = write_end + self.read_heads\n\n        r_keys_shape = (-1, self.word_size, self.read_heads)\n        r_strengths_shape = (-1, self.read_heads)\n        w_key_shape = (-1, self.word_size, 1)\n        write_shape = erase_shape = (-1, self.word_size)\n        free_shape = (-1, self.read_heads)\n        modes_shape = (-1, 3, self.read_heads)\n\n        # parsing the vector into its individual components\n        parsed[\'read_keys\'] = tf.reshape(interface_vector[:, :r_keys_end], r_keys_shape)\n        parsed[\'read_strengths\'] = tf.reshape(interface_vector[:, r_keys_end:r_strengths_end], r_strengths_shape)\n        parsed[\'write_key\'] = tf.reshape(interface_vector[:, r_strengths_end:w_key_end], w_key_shape)\n        parsed[\'write_strength\'] = tf.reshape(interface_vector[:, w_key_end], (-1, 1))\n        parsed[\'erase_vector\'] = tf.reshape(interface_vector[:, w_key_end + 1:erase_end], erase_shape)\n        parsed[\'write_vector\'] = tf.reshape(interface_vector[:, erase_end:write_end], write_shape)\n        parsed[\'free_gates\'] = tf.reshape(interface_vector[:, write_end:free_end], free_shape)\n        parsed[\'allocation_gate\'] = tf.expand_dims(interface_vector[:, free_end], 1)\n        parsed[\'write_gate\'] = tf.expand_dims(interface_vector[:, free_end + 1], 1)\n        parsed[\'read_modes\'] = tf.reshape(interface_vector[:, free_end + 2:], modes_shape)\n\n        # transforming the components to ensure they\'re in the right ranges\n        parsed[\'read_strengths\'] = 1 + tf.nn.softplus(parsed[\'read_strengths\'])\n        parsed[\'write_strength\'] = 1 + tf.nn.softplus(parsed[\'write_strength\'])\n        parsed[\'erase_vector\'] = tf.nn.sigmoid(parsed[\'erase_vector\'])\n        parsed[\'free_gates\'] = tf.nn.sigmoid(parsed[\'free_gates\'])\n        parsed[\'allocation_gate\'] = tf.nn.sigmoid(parsed[\'allocation_gate\'])\n        parsed[\'write_gate\'] = tf.nn.sigmoid(parsed[\'write_gate\'])\n        parsed[\'read_modes\'] = tf.nn.softmax(parsed[\'read_modes\'], 1)\n\n        return parsed\n\n    def process_input(self, X, last_read_vectors, state=None):\n        """"""\n        processes input data through the controller network and returns the\n        pre-output and interface_vector\n\n        Parameters:\n        ----------\n        X: Tensor (batch_size, input_size)\n            the input data batch\n        last_read_vectors: (batch_size, word_size, read_heads)\n            the last batch of read vectors from memory\n        state: Tuple\n            state vectors if the network is recurrent\n\n        Returns: Tuple\n            pre-output: Tensor (batch_size, output_size)\n            parsed_interface_vector: dict\n        """"""\n\n        flat_read_vectors = tf.reshape(last_read_vectors, (-1, self.word_size * self.read_heads))\n        complete_input = tf.concat(1, [X, flat_read_vectors])\n        nn_output, nn_state = None, None\n\n        if self.has_recurrent_nn:\n            nn_output, nn_state = self.network_op(complete_input, state)\n        else:\n            nn_output = self.network_op(complete_input)\n\n        pre_output = tf.matmul(nn_output, self.nn_output_weights)\n        interface = tf.matmul(nn_output, self.interface_weights)\n        parsed_interface = self.parse_interface_vector(interface)\n\n        if self.has_recurrent_nn:\n            return pre_output, parsed_interface, nn_state\n        else:\n            return pre_output, parsed_interface\n\n\n    def final_output(self, pre_output, new_read_vectors):\n        """"""\n        returns the final output by taking rececnt memory changes into account\n\n        Parameters:\n        ----------\n        pre_output: Tensor (batch_size, output_size)\n            the ouput vector from the input processing step\n        new_read_vectors: Tensor (batch_size, words_size, read_heads)\n            the newly read vectors from the updated memory\n\n        Returns: Tensor (batch_size, output_size)\n        """"""\n\n        flat_read_vectors = tf.reshape(new_read_vectors, (-1, self.word_size * self.read_heads))\n\n        final_output = pre_output + tf.matmul(flat_read_vectors, self.mem_output_weights)\n\n        return final_output\n'"
dnc/dnc.py,22,"b'import tensorflow as tf\nfrom tensorflow.python.ops.rnn_cell import LSTMStateTuple\nfrom memory import Memory\nimport utility\nimport os\n\nclass DNC:\n\n    def __init__(self, controller_class, input_size, output_size, max_sequence_length,\n                 memory_words_num = 256, memory_word_size = 64, memory_read_heads = 4, batch_size = 1):\n        """"""\n        constructs a complete DNC architecture as described in the DNC paper\n        http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html\n\n        Parameters:\n        -----------\n        controller_class: BaseController\n            a concrete implementation of the BaseController class\n        input_size: int\n            the size of the input vector\n        output_size: int\n            the size of the output vector\n        max_sequence_length: int\n            the maximum length of an input sequence\n        memory_words_num: int\n            the number of words that can be stored in memory\n        memory_word_size: int\n            the size of an individual word in memory\n        memory_read_heads: int\n            the number of read heads in the memory\n        batch_size: int\n            the size of the data batch\n        """"""\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.max_sequence_length = max_sequence_length\n        self.words_num = memory_words_num\n        self.word_size = memory_word_size\n        self.read_heads = memory_read_heads\n        self.batch_size = batch_size\n\n        self.memory = Memory(self.words_num, self.word_size, self.read_heads, self.batch_size)\n        self.controller = controller_class(self.input_size, self.output_size, self.read_heads, self.word_size, self.batch_size)\n\n        # input data placeholders\n        self.input_data = tf.placeholder(tf.float32, [batch_size, None, input_size], name=\'input\')\n        self.target_output = tf.placeholder(tf.float32, [batch_size, None, output_size], name=\'targets\')\n        self.sequence_length = tf.placeholder(tf.int32, name=\'sequence_length\')\n\n        self.build_graph()\n\n\n    def _step_op(self, step, memory_state, controller_state=None):\n        """"""\n        performs a step operation on the input step data\n\n        Parameters:\n        ----------\n        step: Tensor (batch_size, input_size)\n        memory_state: Tuple\n            a tuple of current memory parameters\n        controller_state: Tuple\n            the state of the controller if it\'s recurrent\n\n        Returns: Tuple\n            output: Tensor (batch_size, output_size)\n            memory_view: dict\n        """"""\n\n        last_read_vectors = memory_state[6]\n        pre_output, interface, nn_state = None, None, None\n\n        if self.controller.has_recurrent_nn:\n            pre_output, interface, nn_state = self.controller.process_input(step, last_read_vectors, controller_state)\n        else:\n            pre_output, interface = self.controller.process_input(step, last_read_vectors)\n\n        usage_vector, write_weighting, memory_matrix, link_matrix, precedence_vector = self.memory.write(\n            memory_state[0], memory_state[1], memory_state[5],\n            memory_state[4], memory_state[2], memory_state[3],\n            interface[\'write_key\'],\n            interface[\'write_strength\'],\n            interface[\'free_gates\'],\n            interface[\'allocation_gate\'],\n            interface[\'write_gate\'],\n            interface[\'write_vector\'],\n            interface[\'erase_vector\']\n        )\n\n        read_weightings, read_vectors = self.memory.read(\n            memory_matrix,\n            memory_state[5],\n            interface[\'read_keys\'],\n            interface[\'read_strengths\'],\n            link_matrix,\n            interface[\'read_modes\'],\n        )\n\n        return [\n\n            # report new memory state to be updated outside the condition branch\n            memory_matrix,\n            usage_vector,\n            precedence_vector,\n            link_matrix,\n            write_weighting,\n            read_weightings,\n            read_vectors,\n\n            self.controller.final_output(pre_output, read_vectors),\n            interface[\'free_gates\'],\n            interface[\'allocation_gate\'],\n            interface[\'write_gate\'],\n\n            # report new state of RNN if exists\n            nn_state[0] if nn_state is not None else tf.zeros(1),\n            nn_state[1] if nn_state is not None else tf.zeros(1)\n        ]\n\n\n    def _loop_body(self, time, memory_state, outputs, free_gates, allocation_gates, write_gates,\n                   read_weightings, write_weightings, usage_vectors, controller_state):\n        """"""\n        the body of the DNC sequence processing loop\n\n        Parameters:\n        ----------\n        time: Tensor\n        outputs: TensorArray\n        memory_state: Tuple\n        free_gates: TensorArray\n        allocation_gates: TensorArray\n        write_gates: TensorArray\n        read_weightings: TensorArray,\n        write_weightings: TensorArray,\n        usage_vectors: TensorArray,\n        controller_state: Tuple\n\n        Returns: Tuple containing all updated arguments\n        """"""\n\n        step_input = self.unpacked_input_data.read(time)\n\n        output_list = self._step_op(step_input, memory_state, controller_state)\n\n        # update memory parameters\n\n        new_controller_state = tf.zeros(1)\n        new_memory_state = tuple(output_list[0:7])\n\n        new_controller_state = LSTMStateTuple(output_list[11], output_list[12])\n\n        outputs = outputs.write(time, output_list[7])\n\n        # collecting memory view for the current step\n        free_gates = free_gates.write(time, output_list[8])\n        allocation_gates = allocation_gates.write(time, output_list[9])\n        write_gates = write_gates.write(time, output_list[10])\n        read_weightings = read_weightings.write(time, output_list[5])\n        write_weightings = write_weightings.write(time, output_list[4])\n        usage_vectors = usage_vectors.write(time, output_list[1])\n\n        return (\n            time + 1, new_memory_state, outputs,\n            free_gates,allocation_gates, write_gates,\n            read_weightings, write_weightings,\n            usage_vectors, new_controller_state\n        )\n\n\n    def build_graph(self):\n        """"""\n        builds the computational graph that performs a step-by-step evaluation\n        of the input data batches\n        """"""\n\n        self.unpacked_input_data = utility.unpack_into_tensorarray(self.input_data, 1, self.sequence_length)\n\n        outputs = tf.TensorArray(tf.float32, self.sequence_length)\n        free_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        allocation_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        write_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        read_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n        write_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n        usage_vectors = tf.TensorArray(tf.float32, self.sequence_length)\n\n        controller_state = self.controller.get_state() if self.controller.has_recurrent_nn else (tf.zeros(1), tf.zeros(1))\n        memory_state = self.memory.init_memory()\n        if not isinstance(controller_state, LSTMStateTuple):\n            controller_state = LSTMStateTuple(controller_state[0], controller_state[1])\n        final_results = None\n\n        with tf.variable_scope(""sequence_loop"") as scope:\n            time = tf.constant(0, dtype=tf.int32)\n\n            final_results = tf.while_loop(\n                cond=lambda time, *_: time < self.sequence_length,\n                body=self._loop_body,\n                loop_vars=(\n                    time, memory_state, outputs,\n                    free_gates, allocation_gates, write_gates,\n                    read_weightings, write_weightings,\n                    usage_vectors, controller_state\n                ),\n                parallel_iterations=32,\n                swap_memory=True\n            )\n\n        dependencies = []\n        if self.controller.has_recurrent_nn:\n            dependencies.append(self.controller.update_state(final_results[9]))\n\n        with tf.control_dependencies(dependencies):\n            self.packed_output = utility.pack_into_tensor(final_results[2], axis=1)\n            self.packed_memory_view = {\n                \'free_gates\': utility.pack_into_tensor(final_results[3], axis=1),\n                \'allocation_gates\': utility.pack_into_tensor(final_results[4], axis=1),\n                \'write_gates\': utility.pack_into_tensor(final_results[5], axis=1),\n                \'read_weightings\': utility.pack_into_tensor(final_results[6], axis=1),\n                \'write_weightings\': utility.pack_into_tensor(final_results[7], axis=1),\n                \'usage_vectors\': utility.pack_into_tensor(final_results[8], axis=1)\n            }\n\n\n    def get_outputs(self):\n        """"""\n        returns the graph nodes for the output and memory view\n\n        Returns: Tuple\n            outputs: Tensor (batch_size, time_steps, output_size)\n            memory_view: dict\n        """"""\n        return self.packed_output, self.packed_memory_view\n\n\n    def save(self, session, ckpts_dir, name):\n        """"""\n        saves the current values of the model\'s parameters to a checkpoint\n\n        Parameters:\n        ----------\n        session: tf.Session\n            the tensorflow session to save\n        ckpts_dir: string\n            the path to the checkpoints directories\n        name: string\n            the name of the checkpoint subdirectory\n        """"""\n        checkpoint_dir = os.path.join(ckpts_dir, name)\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n        tf.train.Saver(tf.trainable_variables()).save(session, os.path.join(checkpoint_dir, \'model.ckpt\'))\n\n\n    def restore(self, session, ckpts_dir, name):\n        """"""\n        session: tf.Session\n            the tensorflow session to restore into\n        ckpts_dir: string\n            the path to the checkpoints directories\n        name: string\n            the name of the checkpoint subdirectory\n        """"""\n        tf.train.Saver(tf.trainable_variables()).restore(session, os.path.join(ckpts_dir, name, \'model.ckpt\'))\n'"
dnc/memory.py,38,"b'import tensorflow as tf\nimport numpy as np\nimport utility\n\nclass Memory:\n\n    def __init__(self, words_num=256, word_size=64, read_heads=4, batch_size=1):\n        """"""\n        constructs a memory matrix with read heads and a write head as described\n        in the DNC paper\n        http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html\n\n        Parameters:\n        ----------\n        words_num: int\n            the maximum number of words that can be stored in the memory at the\n            same time\n        word_size: int\n            the size of the individual word in the memory\n        read_heads: int\n            the number of read heads that can read simultaneously from the memory\n        batch_size: int\n            the size of input data batch\n        """"""\n\n        self.words_num = words_num\n        self.word_size = word_size\n        self.read_heads = read_heads\n        self.batch_size = batch_size\n\n        # a words_num x words_num identity matrix\n        self.I = tf.constant(np.identity(words_num, dtype=np.float32))\n\n        # maps the indecies from the 2D array of free list per batch to\n        # their corresponding values in the flat 1D array of ordered_allocation_weighting\n        self.index_mapper = tf.constant(\n            np.cumsum([0] + [words_num] * (batch_size - 1), dtype=np.int32)[:, np.newaxis]\n        )\n\n    def init_memory(self):\n        """"""\n        returns the initial values for the memory Parameters\n\n        Returns: Tuple\n        """"""\n\n        return (\n            tf.fill([self.batch_size, self.words_num, self.word_size], 1e-6),  # initial memory matrix\n            tf.zeros([self.batch_size, self.words_num, ]),  # initial usage vector\n            tf.zeros([self.batch_size, self.words_num, ]),  # initial precedence vector\n            tf.zeros([self.batch_size, self.words_num, self.words_num]),  # initial link matrix\n            tf.fill([self.batch_size, self.words_num, ], 1e-6),  # initial write weighting\n            tf.fill([self.batch_size, self.words_num, self.read_heads], 1e-6),  # initial read weightings\n            tf.fill([self.batch_size, self.word_size, self.read_heads], 1e-6),  # initial read vectors\n        )\n\n    def get_lookup_weighting(self, memory_matrix, keys, strengths):\n        """"""\n        retrives a content-based adderssing weighting given the keys\n\n        Parameters:\n        ----------\n        memory_matrix: Tensor (batch_size, words_num, word_size)\n            the memory matrix to lookup in\n        keys: Tensor (batch_size, word_size, number_of_keys)\n            the keys to query the memory with\n        strengths: Tensor (batch_size, number_of_keys, )\n            the list of strengths for each lookup key\n\n        Returns: Tensor (batch_size, words_num, number_of_keys)\n            The list of lookup weightings for each provided key\n        """"""\n\n        normalized_memory = tf.nn.l2_normalize(memory_matrix, 2)\n        normalized_keys = tf.nn.l2_normalize(keys, 1)\n\n        similiarity = tf.batch_matmul(normalized_memory, normalized_keys)\n        strengths = tf.expand_dims(strengths, 1)\n\n        return tf.nn.softmax(similiarity * strengths, 1)\n\n\n    def update_usage_vector(self, usage_vector, read_weightings, write_weighting, free_gates):\n        """"""\n        updates and returns the usgae vector given the values of the free gates\n        and the usage_vector, read_weightings, write_weighting from previous step\n\n        Parameters:\n        ----------\n        usage_vector: Tensor (batch_size, words_num)\n        read_weightings: Tensor (batch_size, words_num, read_heads)\n        write_weighting: Tensor (batch_size, words_num)\n        free_gates: Tensor (batch_size, read_heads, )\n\n        Returns: Tensor (batch_size, words_num, )\n            the updated usage vector\n        """"""\n        free_gates = tf.expand_dims(free_gates, 1)\n\n        retention_vector = tf.reduce_prod(1 - read_weightings * free_gates, 2)\n        updated_usage = (usage_vector + write_weighting - usage_vector * write_weighting)  * retention_vector\n\n        return updated_usage\n\n\n    def get_allocation_weighting(self, sorted_usage, free_list):\n        """"""\n        retreives the writing allocation weighting based on the usage free list\n\n        Parameters:\n        ----------\n        sorted_usage: Tensor (batch_size, words_num, )\n            the usage vector sorted ascndingly\n        free_list: Tensor (batch, words_num, )\n            the original indecies of the sorted usage vector\n\n        Returns: Tensor (batch_size, words_num, )\n            the allocation weighting for each word in memory\n        """"""\n\n        shifted_cumprod = tf.cumprod(sorted_usage, axis = 1, exclusive=True)\n        unordered_allocation_weighting = (1 - sorted_usage) * shifted_cumprod\n\n        mapped_free_list = free_list + self.index_mapper\n        flat_unordered_allocation_weighting = tf.reshape(unordered_allocation_weighting, (-1,))\n        flat_mapped_free_list = tf.reshape(mapped_free_list, (-1,))\n        flat_container = tf.TensorArray(tf.float32, self.batch_size * self.words_num)\n\n        flat_ordered_weightings = flat_container.scatter(\n            flat_mapped_free_list,\n            flat_unordered_allocation_weighting\n        )\n\n        packed_wightings = flat_ordered_weightings.pack()\n        return tf.reshape(packed_wightings, (self.batch_size, self.words_num))\n\n\n    def update_write_weighting(self, lookup_weighting, allocation_weighting, write_gate, allocation_gate):\n        """"""\n        updates and returns the current write_weighting\n\n        Parameters:\n        ----------\n        lookup_weighting: Tensor (batch_size, words_num, 1)\n            the weight of the lookup operation in writing\n        allocation_weighting: Tensor (batch_size, words_num)\n            the weight of the allocation operation in writing\n        write_gate: (batch_size, 1)\n            the fraction of writing to be done\n        allocation_gate: (batch_size, 1)\n            the fraction of allocation to be done\n\n        Returns: Tensor (batch_size, words_num)\n            the updated write_weighting\n        """"""\n\n        # remove the dimension of 1 from the lookup_weighting\n        lookup_weighting = tf.squeeze(lookup_weighting)\n\n        updated_write_weighting = write_gate * (allocation_gate * allocation_weighting + (1 - allocation_gate) * lookup_weighting)\n\n        return updated_write_weighting\n\n\n    def update_memory(self, memory_matrix, write_weighting, write_vector, erase_vector):\n        """"""\n        updates and returns the memory matrix given the weighting, write and erase vectors\n        and the memory matrix from previous step\n\n        Parameters:\n        ----------\n        memory_matrix: Tensor (batch_size, words_num, word_size)\n            the memory matrix from previous step\n        write_weighting: Tensor (batch_size, words_num)\n            the weight of writing at each memory location\n        write_vector: Tensor (batch_size, word_size)\n            a vector specifying what to write\n        erase_vector: Tensor (batch_size, word_size)\n            a vector specifying what to erase from memory\n\n        Returns: Tensor (batch_size, words_num, word_size)\n            the updated memory matrix\n        """"""\n\n        # expand data with a dimension of 1 at multiplication-adjacent location\n        # to force matmul to behave as an outer product\n        write_weighting = tf.expand_dims(write_weighting, 2)\n        write_vector = tf.expand_dims(write_vector, 1)\n        erase_vector = tf.expand_dims(erase_vector, 1)\n\n        erasing = memory_matrix * (1 - tf.batch_matmul(write_weighting, erase_vector))\n        writing = tf.batch_matmul(write_weighting, write_vector)\n        updated_memory = erasing + writing\n\n        return updated_memory\n\n\n    def update_precedence_vector(self, precedence_vector, write_weighting):\n        """"""\n        updates the precedence vector given the latest write weighting\n        and the precedence_vector from last step\n\n        Parameters:\n        ----------\n        precedence_vector: Tensor (batch_size. words_num)\n            the precedence vector from the last time step\n        write_weighting: Tensor (batch_size,words_num)\n            the latest write weighting for the memory\n\n        Returns: Tensor (batch_size, words_num)\n            the updated precedence vector\n        """"""\n\n        reset_factor = 1 - tf.reduce_sum(write_weighting, 1, keep_dims=True)\n        updated_precedence_vector = reset_factor * precedence_vector + write_weighting\n\n        return updated_precedence_vector\n\n\n    def update_link_matrix(self, precedence_vector, link_matrix, write_weighting):\n        """"""\n        updates and returns the temporal link matrix for the latest write\n        given the precedence vector and the link matrix from previous step\n\n        Parameters:\n        ----------\n        precedence_vector: Tensor (batch_size, words_num)\n            the precedence vector from the last time step\n        link_matrix: Tensor (batch_size, words_num, words_num)\n            the link matrix form the last step\n        write_weighting: Tensor (batch_size, words_num)\n            the latest write_weighting for the memory\n\n        Returns: Tensor (batch_size, words_num, words_num)\n            the updated temporal link matrix\n        """"""\n\n        write_weighting = tf.expand_dims(write_weighting, 2)\n        precedence_vector = tf.expand_dims(precedence_vector, 1)\n\n        reset_factor = 1 - utility.pairwise_add(write_weighting, is_batch=True)\n        updated_link_matrix = reset_factor * link_matrix + tf.batch_matmul(write_weighting, precedence_vector)\n        updated_link_matrix = (1 - self.I) * updated_link_matrix  # eliminates self-links\n\n        return updated_link_matrix\n\n\n    def get_directional_weightings(self, read_weightings, link_matrix):\n        """"""\n        computes and returns the forward and backward reading weightings\n        given the read_weightings from the previous step\n\n        Parameters:\n        ----------\n        read_weightings: Tensor (batch_size, words_num, read_heads)\n            the read weightings from the last time step\n        link_matrix: Tensor (batch_size, words_num, words_num)\n            the temporal link matrix\n\n        Returns: Tuple\n            forward weighting: Tensor (batch_size, words_num, read_heads),\n            backward weighting: Tensor (batch_size, words_num, read_heads)\n        """"""\n\n        forward_weighting = tf.batch_matmul(link_matrix, read_weightings)\n        backward_weighting = tf.batch_matmul(link_matrix, read_weightings, adj_x=True)\n\n        return forward_weighting, backward_weighting\n\n\n    def update_read_weightings(self, lookup_weightings, forward_weighting, backward_weighting, read_mode):\n        """"""\n        updates and returns the current read_weightings\n\n        Parameters:\n        ----------\n        lookup_weightings: Tensor (batch_size, words_num, read_heads)\n            the content-based read weighting\n        forward_weighting: Tensor (batch_size, words_num, read_heads)\n            the forward direction read weighting\n        backward_weighting: Tensor (batch_size, words_num, read_heads)\n            the backward direction read weighting\n        read_mode: Tesnor (batch_size, 3, read_heads)\n            the softmax distribution between the three read modes\n\n        Returns: Tensor (batch_size, words_num, read_heads)\n        """"""\n\n        backward_mode = tf.expand_dims(read_mode[:, 0, :], 1) * backward_weighting\n        lookup_mode = tf.expand_dims(read_mode[:, 1, :], 1) * lookup_weightings\n        forward_mode = tf.expand_dims(read_mode[:, 2, :], 1) * forward_weighting\n        updated_read_weightings = backward_mode + lookup_mode + forward_mode\n\n        return updated_read_weightings\n\n\n    def update_read_vectors(self, memory_matrix, read_weightings):\n        """"""\n        reads, updates, and returns the read vectors of the recently updated memory\n\n        Parameters:\n        ----------\n        memory_matrix: Tensor (batch_size, words_num, word_size)\n            the recently updated memory matrix\n        read_weightings: Tensor (batch_size, words_num, read_heads)\n            the amount of info to read from each memory location by each read head\n\n        Returns: Tensor (word_size, read_heads)\n        """"""\n\n        updated_read_vectors = tf.batch_matmul(memory_matrix, read_weightings, adj_x=True)\n\n        return updated_read_vectors\n\n\n    def write(self, memory_matrix, usage_vector, read_weightings, write_weighting,\n              precedence_vector, link_matrix,  key, strength, free_gates,\n              allocation_gate, write_gate, write_vector, erase_vector):\n        """"""\n        defines the complete pipeline of writing to memory gievn the write variables\n        and the memory_matrix, usage_vector, link_matrix, and precedence_vector from\n        previous step\n\n        Parameters:\n        ----------\n        memory_matrix: Tensor (batch_size, words_num, word_size)\n            the memory matrix from previous step\n        usage_vector: Tensor (batch_size, words_num)\n            the usage_vector from the last time step\n        read_weightings: Tensor (batch_size, words_num, read_heads)\n            the read_weightings from the last time step\n        write_weighting: Tensor (batch_size, words_num)\n            the write_weighting from the last time step\n        precedence_vector: Tensor (batch_size, words_num)\n            the precedence vector from the last time step\n        link_matrix: Tensor (batch_size, words_num, words_num)\n            the link_matrix from previous step\n        key: Tensor (batch_size, word_size, 1)\n            the key to query the memory location with\n        strength: (batch_size, 1)\n            the strength of the query key\n        free_gates: Tensor (batch_size, read_heads)\n            the degree to which location at read haeds will be freed\n        allocation_gate: (batch_size, 1)\n            the fraction of writing that is being allocated in a new locatio\n        write_gate: (batch_size, 1)\n            the amount of information to be written to memory\n        write_vector: Tensor (batch_size, word_size)\n            specifications of what to write to memory\n        erase_vector: Tensor(batch_size, word_size)\n            specifications of what to erase from memory\n\n        Returns : Tuple\n            the updated usage vector: Tensor (batch_size, words_num)\n            the updated write_weighting: Tensor(batch_size, words_num)\n            the updated memory_matrix: Tensor (batch_size, words_num, words_size)\n            the updated link matrix: Tensor(batch_size, words_num, words_num)\n            the updated precedence vector: Tensor (batch_size, words_num)\n        """"""\n\n        lookup_weighting = self.get_lookup_weighting(memory_matrix, key, strength)\n        new_usage_vector = self.update_usage_vector(usage_vector, read_weightings, write_weighting, free_gates)\n\n        sorted_usage, free_list = tf.nn.top_k(-1 * new_usage_vector, self.words_num)\n        sorted_usage = -1 * sorted_usage\n\n        allocation_weighting = self.get_allocation_weighting(sorted_usage, free_list)\n        new_write_weighting = self.update_write_weighting(lookup_weighting, allocation_weighting, write_gate, allocation_gate)\n        new_memory_matrix = self.update_memory(memory_matrix, new_write_weighting, write_vector, erase_vector)\n        new_link_matrix = self.update_link_matrix(precedence_vector, link_matrix, new_write_weighting)\n        new_precedence_vector = self.update_precedence_vector(precedence_vector, new_write_weighting)\n\n        return new_usage_vector, new_write_weighting, new_memory_matrix, new_link_matrix, new_precedence_vector\n\n\n    def read(self, memory_matrix, read_weightings, keys, strengths, link_matrix, read_modes):\n        """"""\n        defines the complete pipeline for reading from memory\n\n        Parameters:\n        ----------\n        memory_matrix: Tensor (batch_size, words_num, word_size)\n            the updated memory matrix from the last writing\n        read_weightings: Tensor (batch_size, words_num, read_heads)\n            the read weightings form the last time step\n        keys: Tensor (batch_size, word_size, read_heads)\n            the kyes to query the memory locations with\n        strengths: Tensor (batch_size, read_heads)\n            the strength of each read key\n        link_matrix: Tensor (batch_size, words_num, words_num)\n            the updated link matrix from the last writing\n        read_modes: Tensor (batch_size, 3, read_heads)\n            the softmax distribution between the three read modes\n\n        Returns: Tuple\n            the updated read_weightings: Tensor(batch_size, words_num, read_heads)\n            the recently read vectors: Tensor (batch_size, word_size, read_heads)\n        """"""\n\n        lookup_weighting = self.get_lookup_weighting(memory_matrix, keys, strengths)\n        forward_weighting, backward_weighting = self.get_directional_weightings(read_weightings, link_matrix)\n        new_read_weightings = self.update_read_weightings(lookup_weighting, forward_weighting, backward_weighting, read_modes)\n        new_read_vectors = self.update_read_vectors(memory_matrix, new_read_weightings)\n\n        return new_read_weightings, new_read_vectors\n'"
dnc/utility.py,8,"b'import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.ops import gen_state_ops\n\ndef pairwise_add(u, v=None, is_batch=False):\n    """"""\n    performs a pairwise summation between vectors (possibly the same)\n\n    Parameters:\n    ----------\n    u: Tensor (n, ) | (n, 1)\n    v: Tensor (n, ) | (n, 1) [optional]\n    is_batch: bool\n        a flag for whether the vectors come in a batch\n        ie.: whether the vectors has a shape of (b,n) or (b,n,1)\n\n    Returns: Tensor (n, n)\n    Raises: ValueError\n    """"""\n    u_shape = u.get_shape().as_list()\n\n    if len(u_shape) > 2 and not is_batch:\n        raise ValueError(""Expected at most 2D tensors, but got %dD"" % len(u_shape))\n    if len(u_shape) > 3 and is_batch:\n        raise ValueError(""Expected at most 2D tensor batches, but got %dD"" % len(u_shape))\n\n    if v is None:\n        v = u\n    else:\n        v_shape = v.get_shape().as_list()\n        if u_shape != v_shape:\n            raise VauleError(""Shapes %s and %s do not match"" % (u_shape, v_shape))\n\n    n = u_shape[0] if not is_batch else u_shape[1]\n\n    column_u = tf.reshape(u, (-1, 1) if not is_batch else (-1, n, 1))\n    U = tf.concat(1 if not is_batch else 2, [column_u] * n)\n\n    if v is u:\n        return U + tf.transpose(U, None if not is_batch else [0, 2, 1])\n    else:\n        row_v = tf.reshape(v, (1, -1) if not is_batch else (-1, 1, n))\n        V = tf.concat(0 if not is_batch else 1, [row_v] * n)\n\n        return U + V\n\n\ndef decaying_softmax(shape, axis):\n    rank = len(shape)\n    max_val = shape[axis]\n\n    weights_vector = np.arange(1, max_val + 1, dtype=np.float32)\n    weights_vector = weights_vector[::-1]\n    weights_vector = np.exp(weights_vector) / np.sum(np.exp(weights_vector))\n\n    container = np.zeros(shape, dtype=np.float32)\n    broadcastable_shape = [1] * rank\n    broadcastable_shape[axis] = max_val\n\n    return container + np.reshape(weights_vector, broadcastable_shape)\n\ndef unpack_into_tensorarray(value, axis, size=None):\n    """"""\n    unpacks a given tensor along a given axis into a TensorArray\n\n    Parameters:\n    ----------\n    value: Tensor\n        the tensor to be unpacked\n    axis: int\n        the axis to unpack the tensor along\n    size: int\n        the size of the array to be used if shape inference resulted in None\n\n    Returns: TensorArray\n        the unpacked TensorArray\n    """"""\n\n    shape = value.get_shape().as_list()\n    rank = len(shape)\n    dtype = value.dtype\n    array_size = shape[axis] if not shape[axis] is None else size\n\n    if array_size is None:\n        raise ValueError(""Can\'t create TensorArray with size None"")\n\n    array = tf.TensorArray(dtype=dtype, size=array_size)\n    dim_permutation = [axis] + range(1, axis) + [0] + range(axis + 1, rank)\n    unpack_axis_major_value = tf.transpose(value, dim_permutation)\n    full_array = array.unpack(unpack_axis_major_value)\n\n    return full_array\n\ndef pack_into_tensor(array, axis):\n    """"""\n    packs a given TensorArray into a tensor along a given axis\n\n    Parameters:\n    ----------\n    array: TensorArray\n        the tensor array to pack\n    axis: int\n        the axis to pack the array along\n\n    Returns: Tensor\n        the packed tensor\n    """"""\n\n    packed_tensor = array.pack()\n    shape = packed_tensor.get_shape()\n    rank = len(shape)\n\n    dim_permutation = [axis] + range(1, axis) + [0]  + range(axis + 1, rank)\n    correct_shape_tensor = tf.transpose(packed_tensor, dim_permutation)\n\n    return correct_shape_tensor\n'"
unit-tests/controller.py,21,"b""import tensorflow as tf\nimport numpy as np\nimport unittest\n\nfrom dnc.controller import BaseController\n\nclass DummyController(BaseController):\n    def network_vars(self):\n        self.W = tf.Variable(tf.truncated_normal([self.nn_input_size, 64]))\n        self.b = tf.Variable(tf.zeros([64]))\n\n    def network_op(self, X):\n        return tf.matmul(X, self.W) + self.b\n\n\nclass DummyRecurrentController(BaseController):\n    def network_vars(self):\n        self.lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(64)\n        self.state = tf.Variable(tf.zeros([self.batch_size, 64]), trainable=False)\n        self.output = tf.Variable(tf.zeros([self.batch_size, 64]), trainable=False)\n\n    def network_op(self, X, state):\n        X = tf.convert_to_tensor(X)\n        return self.lstm_cell(X, state)\n\n    def update_state(self, new_state):\n        return tf.group(\n            self.output.assign(new_state[0]),\n            self.state.assign(new_state[1])\n        )\n\n    def get_state(self):\n        return (self.output, self.state)\n\n\nclass DNCControllerTest(unittest.TestCase):\n\n    def test_construction(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                controller = DummyController(10, 10, 2, 5)\n                rcontroller = DummyRecurrentController(10, 10, 2, 5, 1)\n\n                self.assertFalse(controller.has_recurrent_nn)\n                self.assertEqual(controller.nn_input_size, 20)\n                self.assertEqual(controller.interface_vector_size, 38)\n                self.assertEqual(controller.interface_weights.get_shape().as_list(), [64, 38])\n                self.assertEqual(controller.nn_output_weights.get_shape().as_list(), [64, 10])\n                self.assertEqual(controller.mem_output_weights.get_shape().as_list(), [10, 10])\n\n                self.assertTrue(rcontroller.has_recurrent_nn)\n                self.assertEqual(rcontroller.nn_input_size, 20)\n                self.assertEqual(rcontroller.interface_vector_size, 38)\n                self.assertEqual(rcontroller.interface_weights.get_shape().as_list(), [64, 38])\n                self.assertEqual(rcontroller.nn_output_weights.get_shape().as_list(), [64, 10])\n                self.assertEqual(rcontroller.mem_output_weights.get_shape().as_list(), [10, 10])\n\n\n\n    def test_get_nn_output_size(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as Session:\n\n                controller = DummyController(10, 10, 2, 5)\n                rcontroller = DummyRecurrentController(10, 10, 2, 5, 1)\n\n                self.assertEqual(controller.get_nn_output_size(), 64)\n                self.assertEqual(rcontroller.get_nn_output_size(), 64)\n\n\n    def test_parse_interface_vector(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                controller = DummyController(10, 10, 2, 5)\n                zeta = np.random.uniform(-2, 2, (2, 38)).astype(np.float32)\n\n                read_keys = np.reshape(zeta[:, :10], (-1, 5, 2))\n                read_strengths = 1 + np.log(np.exp(np.reshape(zeta[:, 10:12], (-1, 2, ))) + 1)\n                write_key = np.reshape(zeta[:, 12:17], (-1, 5, 1))\n                write_strength = 1 + np.log(np.exp(np.reshape(zeta[:, 17], (-1, 1))) + 1)\n                erase_vector = 1.0 / (1 + np.exp(-1 * np.reshape(zeta[:, 18:23], (-1, 5))))\n                write_vector = np.reshape(zeta[:, 23:28], (-1, 5))\n                free_gates = 1.0 / (1 + np.exp(-1 * np.reshape(zeta[:, 28:30], (-1, 2))))\n                allocation_gate = 1.0 / (1 + np.exp(-1 * zeta[:, 30, np.newaxis]))\n                write_gate = 1.0 / (1 + np.exp(-1 * zeta[:, 31, np.newaxis]))\n                read_modes = np.reshape(zeta[:, 32:], (-1, 3, 2))\n\n                read_modes = np.transpose(read_modes, [0, 2, 1])\n                read_modes = np.reshape(read_modes, (-1, 3))\n                read_modes = np.exp(read_modes) / np.sum(np.exp(read_modes), axis=-1, keepdims=True)\n                read_modes = np.reshape(read_modes, (2, 2, 3))\n                read_modes = np.transpose(read_modes, [0, 2, 1])\n\n                op = controller.parse_interface_vector(zeta)\n                session.run(tf.initialize_all_variables())\n                parsed = session.run(op)\n\n                self.assertTrue(np.allclose(parsed['read_keys'], read_keys))\n                self.assertTrue(np.allclose(parsed['read_strengths'], read_strengths))\n                self.assertTrue(np.allclose(parsed['write_key'], write_key))\n                self.assertTrue(np.allclose(parsed['write_strength'], write_strength))\n                self.assertTrue(np.allclose(parsed['erase_vector'], erase_vector))\n                self.assertTrue(np.allclose(parsed['write_vector'], write_vector))\n                self.assertTrue(np.allclose(parsed['free_gates'], free_gates))\n                self.assertTrue(np.allclose(parsed['allocation_gate'], allocation_gate))\n                self.assertTrue(np.allclose(parsed['write_gate'], write_gate))\n                self.assertTrue(np.allclose(parsed['read_modes'], read_modes))\n\n\n    def test_process_input(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                controller = DummyController(10, 10, 2, 5)\n                rcontroller = DummyRecurrentController(10, 10, 2, 5, 2)\n\n                input_batch = np.random.uniform(0, 1, (2, 10)).astype(np.float32)\n                last_read_vectors = np.random.uniform(-1, 1, (2, 5, 2)).astype(np.float32)\n\n                v_op, zeta_op = controller.process_input(input_batch, last_read_vectors)\n                rv_op, rzeta_op, rs_op = rcontroller.process_input(input_batch, last_read_vectors, rcontroller.get_state())\n\n                session.run(tf.initialize_all_variables())\n                v, zeta = session.run([v_op, zeta_op])\n                rv, rzeta, rs = session.run([rv_op, rzeta_op, rs_op])\n\n                self.assertEqual(v.shape, (2, 10))\n                self.assertEqual(np.concatenate([np.reshape(val, (2, -1)) for _,val in zeta.iteritems()], axis=1).shape, (2, 38))\n\n                self.assertEqual(rv.shape, (2, 10))\n                self.assertEqual(np.concatenate([np.reshape(val, (2, -1)) for _,val in rzeta.iteritems()], axis=1).shape, (2, 38))\n                self.assertEqual([_s.shape for _s in rs], [(2, 64), (2, 64)])\n\n\n    def test_final_output(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                controller = DummyController(10, 10, 2, 5)\n                output_batch = np.random.uniform(0, 1, (2, 10)).astype(np.float32)\n                new_read_vectors = np.random.uniform(-1, 1, (2, 5, 2)).astype(np.float32)\n\n                op = controller.final_output(output_batch, new_read_vectors)\n                session.run(tf.initialize_all_variables())\n                y = session.run(op)\n\n                self.assertEqual(y.shape, (2, 10))\n\n\nif __name__ == '__main__':\n    unittest.main(verbosity=2)\n"""
unit-tests/dnc.py,22,"b""import tensorflow as tf\nimport numpy as np\nimport unittest\nimport shutil\nimport os\n\nfrom dnc.dnc import DNC\nfrom dnc.memory import Memory\nfrom dnc.controller import BaseController\n\nclass DummyController(BaseController):\n    def network_vars(self):\n        self.W = tf.Variable(tf.truncated_normal([self.nn_input_size, 64]), name='layer_W')\n        self.b = tf.Variable(tf.zeros([64]), name='layer_b')\n\n    def network_op(self, X):\n        return tf.matmul(X, self.W) + self.b\n\nclass DummyRecurrentController(BaseController):\n    def network_vars(self):\n        self.lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(64)\n        self.state = tf.Variable(tf.zeros([self.batch_size, 64]), trainable=False)\n        self.output = tf.Variable(tf.zeros([self.batch_size, 64]), trainable=False)\n\n    def network_op(self, X, state):\n        X = tf.convert_to_tensor(X)\n        return self.lstm_cell(X, state)\n\n    def update_state(self, new_state):\n        return tf.group(\n            self.output.assign(new_state[0]),\n            self.state.assign(new_state[1])\n        )\n\n    def get_state(self):\n        return (self.output, self.state)\n\nclass DNCTest(unittest.TestCase):\n\n    @classmethod\n    def _clear(cls):\n        try:\n            current_dir = os.path.dirname(__file__)\n            ckpts_dir = os.path.join(current_dir, 'checkpoints')\n\n            shutil.rmtree(ckpts_dir)\n        except:\n            # swallow error\n            return\n\n    @classmethod\n    def setUpClass(cls):\n        cls._clear()\n\n\n    @classmethod\n    def tearDownClass(cls):\n        cls._clear()\n\n\n    def test_construction(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                computer = DNC(DummyController, 10, 20, 10, 10, 64, 1)\n                rcomputer = DNC(DummyRecurrentController, 10, 20, 10, 10, 64, 1)\n\n                self.assertEqual(computer.input_size, 10)\n                self.assertEqual(computer.output_size, 20)\n                self.assertEqual(computer.words_num, 10)\n                self.assertEqual(computer.word_size, 64)\n                self.assertEqual(computer.read_heads, 1)\n                self.assertEqual(computer.batch_size, 1)\n\n                self.assertTrue(isinstance(computer.memory, Memory))\n                self.assertTrue(isinstance(computer.controller, DummyController))\n                self.assertTrue(isinstance(rcomputer.controller, DummyRecurrentController))\n\n\n    def test_call(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                computer = DNC(DummyController, 10, 20, 10, 10, 64, 2, batch_size=3)\n                rcomputer = DNC(DummyRecurrentController, 10, 20, 10, 10, 64, 2, batch_size=3)\n                input_batches = np.random.uniform(0, 1, (3, 5, 10)).astype(np.float32)\n\n                session.run(tf.initialize_all_variables())\n                out_view = session.run(computer.get_outputs(), feed_dict={\n                    computer.input_data: input_batches,\n                    computer.sequence_length: 5\n                })\n                out, view = out_view\n\n                rout_rview, ro, rs = session.run([\n                    rcomputer.get_outputs(),\n                    rcomputer.controller.get_state()[0],\n                    rcomputer.controller.get_state()[1]\n                ], feed_dict={\n                    rcomputer.input_data: input_batches,\n                    rcomputer.sequence_length: 5\n                })\n                rout, rview = rout_rview\n\n                self.assertEqual(out.shape, (3, 5, 20))\n                self.assertEqual(view['free_gates'].shape, (3, 5, 2))\n                self.assertEqual(view['allocation_gates'].shape, (3, 5, 1))\n                self.assertEqual(view['write_gates'].shape, (3, 5, 1))\n                self.assertEqual(view['read_weightings'].shape, (3, 5, 10, 2))\n                self.assertEqual(view['write_weightings'].shape, (3, 5, 10))\n\n\n                self.assertEqual(rout.shape, (3, 5, 20))\n                self.assertEqual(rview['free_gates'].shape, (3, 5, 2))\n                self.assertEqual(rview['allocation_gates'].shape, (3, 5, 1))\n                self.assertEqual(rview['write_gates'].shape, (3, 5, 1))\n                self.assertEqual(rview['read_weightings'].shape, (3, 5, 10, 2))\n                self.assertEqual(rview['write_weightings'].shape, (3, 5, 10))\n\n\n    def test_save(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                computer = DNC(DummyController, 10, 20, 10, 10, 64, 2, batch_size=2)\n                session.run(tf.initialize_all_variables())\n                current_dir = os.path.dirname(__file__)\n                ckpts_dir = os.path.join(current_dir, 'checkpoints')\n\n                computer.save(session, ckpts_dir, 'test-save')\n\n                self.assert_(True)\n\n\n    def test_restore(self):\n\n        current_dir = os.path.dirname(__file__)\n        ckpts_dir = os.path.join(current_dir, 'checkpoints')\n\n        model1_output, model1_memview = None, None\n        sample_input = np.random.uniform(0, 1, (2, 5, 10)).astype(np.float32)\n        sample_seq_len = 5\n\n        graph1 = tf.Graph()\n        with graph1.as_default():\n            with tf.Session(graph=graph1) as session1:\n\n                computer = DNC(DummyController, 10, 20, 10, 10, 64, 2, batch_size=2)\n                session1.run(tf.initialize_all_variables())\n\n                saved_weights = session1.run([\n                    computer.controller.nn_output_weights,\n                    computer.controller.interface_weights,\n                    computer.controller.mem_output_weights,\n                    computer.controller.W,\n                    computer.controller.b\n                ])\n\n                computer.save(session1, ckpts_dir, 'test-restore')\n\n        graph2 = tf.Graph()\n        with graph2.as_default():\n            with tf.Session(graph=graph2) as session2:\n\n                computer = DNC(DummyController, 10, 20, 10, 10, 64, 2, batch_size=2)\n                session2.run(tf.initialize_all_variables())\n                computer.restore(session2, ckpts_dir, 'test-restore')\n\n                restored_weights = session2.run([\n                    computer.controller.nn_output_weights,\n                    computer.controller.interface_weights,\n                    computer.controller.mem_output_weights,\n                    computer.controller.W,\n                    computer.controller.b\n                ])\n\n                self.assertTrue(np.product([np.array_equal(restored_weights[i], saved_weights[i]) for i in range(5)]))\n\nif __name__ == '__main__':\n    unittest.main(verbosity=2)\n"""
unit-tests/memory.py,43,"b""import tensorflow as tf\nimport numpy as np\nimport unittest\n\nfrom dnc.memory import Memory\n\ndef random_softmax(shape, axis):\n    rand = np.random.uniform(0, 1, shape).astype(np.float32)\n    return np.exp(rand) / np.sum(np.exp(rand), axis=axis, keepdims=True)\n\nclass DNCMemoryTests(unittest.TestCase):\n\n    def test_construction(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                session.run(tf.initialize_all_variables())\n\n                self.assertEqual(mem.words_num, 4)\n                self.assertEqual(mem.word_size, 5)\n                self.assertEqual(mem.read_heads, 2)\n                self.assertEqual(mem.batch_size, 2)\n\n\n    def test_init_memory(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                M, u, p, L, ww, rw, r = session.run(mem.init_memory())\n\n                self.assertEqual(M.shape, (2, 4, 5))\n                self.assertEqual(u.shape, (2, 4))\n                self.assertEqual(L.shape, (2, 4, 4))\n                self.assertEqual(ww.shape, (2, 4))\n                self.assertEqual(rw.shape, (2, 4, 2))\n                self.assertEqual(r.shape, (2, 5, 2))\n                self.assertEqual(p.shape, (2, 4))\n\n    def test_lookup_weighting(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                initial_mem = np.random.uniform(0, 1, (2, 4, 5)).astype(np.float32)\n                keys = np.random.uniform(0, 1, (2, 5, 2)).astype(np.float32)\n                strengths = np.random.uniform(0, 1, (2 ,2)).astype(np.float32)\n\n                norm_mem = initial_mem / np.sqrt(np.sum(initial_mem ** 2, axis=2, keepdims=True))\n                norm_keys = keys/ np.sqrt(np.sum(keys ** 2, axis = 1, keepdims=True))\n                sim = np.matmul(norm_mem, norm_keys)\n                sim = sim * strengths[:, np.newaxis, :]\n                predicted_wieghts = np.exp(sim) / np.sum(np.exp(sim), axis=1, keepdims=True)\n\n                memory_matrix = tf.convert_to_tensor(initial_mem)\n                op = mem.get_lookup_weighting(memory_matrix, keys, strengths)\n                c = session.run(op)\n\n                self.assertEqual(c.shape, (2, 4, 2))\n                self.assertTrue(np.allclose(c, predicted_wieghts))\n\n\n    def test_update_usage_vector(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                free_gates = np.random.uniform(0, 1, (2, 2)).astype(np.float32)\n                init_read_weightings = random_softmax((2, 4, 2), axis=1)\n                init_write_weightings = random_softmax((2, 4), axis=1)\n                init_usage = np.random.uniform(0, 1, (2, 4)).astype(np.float32)\n\n                psi = np.product(1 - init_read_weightings * free_gates[:, np.newaxis, :], axis=2)\n                predicted_usage = (init_usage + init_write_weightings - init_usage * init_write_weightings) * psi\n\n\n                read_weightings = tf.convert_to_tensor(init_read_weightings)\n                write_weighting = tf.convert_to_tensor(init_write_weightings)\n                usage_vector = tf.convert_to_tensor(init_usage)\n\n                op = mem.update_usage_vector(usage_vector, read_weightings, write_weighting, free_gates)\n                u = session.run(op)\n\n                self.assertEqual(u.shape, (2, 4))\n                self.assertTrue(np.array_equal(u, predicted_usage))\n\n\n    def test_get_allocation_weighting(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                mock_usage = np.random.uniform(0.01, 1, (2, 4)).astype(np.float32)\n                sorted_usage = np.sort(mock_usage, axis=1)\n                free_list = np.argsort(mock_usage, axis=1)\n\n                predicted_weights = np.zeros((2, 4)).astype(np.float32)\n                for i in range(2):\n                    for j in range(4):\n                        product_list = [mock_usage[i, free_list[i,k]] for k in range(j)]\n                        predicted_weights[i, free_list[i,j]] = (1 - mock_usage[i, free_list[i, j]]) * np.product(product_list)\n\n                op = mem.get_allocation_weighting(sorted_usage, free_list)\n                a = session.run(op)\n\n                self.assertEqual(a.shape, (2, 4))\n                self.assertTrue(np.allclose(a, predicted_weights))\n\n\n    def test_updated_write_weighting(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                write_gate = np.random.uniform(0, 1, (2,1)).astype(np.float32)\n                allocation_gate = np.random.uniform(0, 1, (2,1)).astype(np.float32)\n                lookup_weighting = random_softmax((2, 4, 1), axis=1)\n                allocation_weighting = random_softmax((2, 4), axis=1)\n\n                predicted_weights = write_gate * (allocation_gate * allocation_weighting + (1 - allocation_gate) * np.squeeze(lookup_weighting))\n\n                op = mem.update_write_weighting(lookup_weighting, allocation_weighting, write_gate, allocation_gate)\n                w_w = session.run(op)\n\n                self.assertEqual(w_w.shape, (2,4))\n                self.assertTrue(np.allclose(w_w, predicted_weights))\n\n\n    def test_update_memory(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                write_weighting = random_softmax((2, 4), axis=1)\n                write_vector = np.random.uniform(0, 1, (2, 5)).astype(np.float32)\n                erase_vector = np.random.uniform(0, 1, (2, 5)).astype(np.float32)\n                memory_matrix = np.random.uniform(-1, 1, (2, 4, 5)).astype(np.float32)\n\n                ww = write_weighting[:, :, np.newaxis]\n                v, e = write_vector[:, np.newaxis, :], erase_vector[:, np.newaxis, :]\n                predicted = memory_matrix * (1 - np.matmul(ww, e)) + np.matmul(ww, v)\n\n                memory_matrix = tf.convert_to_tensor(memory_matrix)\n\n                op = mem.update_memory(memory_matrix, write_weighting, write_vector, erase_vector)\n                M = session.run(op)\n\n                self.assertEqual(M.shape, (2, 4, 5))\n                self.assertTrue(np.allclose(M, predicted))\n\n    def test_update_precedence_vector(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                write_weighting = random_softmax((2, 4), axis=1)\n                initial_precedence = random_softmax((2, 4), axis=1)\n                predicted = (1 - write_weighting.sum(axis=1, keepdims=True)) * initial_precedence + write_weighting\n\n                precedence_vector = tf.convert_to_tensor(initial_precedence)\n\n                op = mem.update_precedence_vector(precedence_vector, write_weighting)\n                p = session.run(op)\n\n                self.assertEqual(p.shape, (2,4))\n                self.assertTrue(np.allclose(p, predicted))\n\n\n    def test_update_link_matrix(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                _write_weighting = random_softmax((2, 4), axis=1)\n                _precedence_vector = random_softmax((2, 4), axis=1)\n                initial_link = np.random.uniform(0, 1, (2, 4, 4)).astype(np.float32)\n                np.fill_diagonal(initial_link[0,:], 0)\n                np.fill_diagonal(initial_link[1,:], 0)\n\n                # calculate the updated link iteratively as in paper\n                # to check the correcteness of the vectorized implementation\n                predicted = np.zeros((2,4,4), dtype=np.float32)\n                for i in range(4):\n                    for j in range(4):\n                        if i != j:\n                            reset_factor = (1 - _write_weighting[:,i] - _write_weighting[:,j])\n                            predicted[:, i, j]  = reset_factor * initial_link[:, i , j] + _write_weighting[:, i] * _precedence_vector[:, j]\n\n                link_matrix = tf.convert_to_tensor(initial_link)\n                precedence_vector = tf.convert_to_tensor(_precedence_vector)\n\n                write_weighting = tf.constant(_write_weighting)\n\n                op = mem.update_link_matrix(precedence_vector, link_matrix, write_weighting)\n                L = session.run(op)\n\n                self.assertTrue(np.allclose(L, predicted))\n\n\n    def test_get_directional_weightings(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                _link_matrix = np.random.uniform(0, 1, (2, 4, 4)).astype(np.float32)\n                _read_weightings = random_softmax((2, 4, 2), axis=1)\n                predicted_forward = np.matmul(_link_matrix, _read_weightings)\n                predicted_backward = np.matmul(np.transpose(_link_matrix, [0, 2, 1]), _read_weightings)\n\n                read_weightings = tf.convert_to_tensor(_read_weightings)\n\n                fop, bop = mem.get_directional_weightings(read_weightings, _link_matrix)\n\n                forward_weighting, backward_weighting = session.run([fop, bop])\n\n                self.assertTrue(np.allclose(forward_weighting, predicted_forward))\n                self.assertTrue(np.allclose(backward_weighting, predicted_backward))\n\n\n\n    def test_update_read_weightings(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                mem = Memory(4, 5, 2, 2)\n                lookup_weightings = random_softmax((2, 4, 2), axis=1)\n                forward_weighting = random_softmax((2, 4, 2), axis=1)\n                backward_weighting = random_softmax((2, 4, 2), axis=1)\n                read_mode = random_softmax((2, 3, 2), axis=1)\n                predicted_weights = np.zeros((2, 4, 2)).astype(np.float32)\n\n                # calculate the predicted weights using iterative method from paper\n                # to check the correcteness of the vectorized implementation\n                for i in range(2):\n                    predicted_weights[:, :, i] = read_mode[:, 0,i, np.newaxis] * backward_weighting[:, :, i] + read_mode[:, 1, i, np.newaxis] * lookup_weightings[:, :, i] + read_mode[:, 2, i, np.newaxis] * forward_weighting[:, :, i]\n\n                op = mem.update_read_weightings(lookup_weightings, forward_weighting, backward_weighting, read_mode)\n                session.run(tf.initialize_all_variables())\n                w_r = session.run(op)\n                #updated_read_weightings = session.run(mem.read_weightings.value())\n\n                self.assertTrue(np.allclose(w_r, predicted_weights))\n                #self.assertTrue(np.allclose(updated_read_weightings, predicted_weights))\n\n\n    def test_update_read_vectors(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph = graph) as session:\n\n                mem = Memory(4, 5, 2, 4)\n                memory_matrix = np.random.uniform(-1, 1, (4, 4, 5)).astype(np.float32)\n                read_weightings = random_softmax((4, 4, 2), axis=1)\n                predicted = np.matmul(np.transpose(memory_matrix, [0, 2, 1]), read_weightings)\n\n                op = mem.update_read_vectors(memory_matrix, read_weightings)\n                session.run(tf.initialize_all_variables())\n                r = session.run(op)\n                #updated_read_vectors = session.run(mem.read_vectors.value())\n\n                self.assertTrue(np.allclose(r, predicted))\n                #self.assertTrue(np.allclose(updated_read_vectors, predicted))\n\n    def test_write(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph = graph) as session:\n\n                mem = Memory(4, 5, 2, 1)\n                M, u, p, L, ww, rw, r = session.run(mem.init_memory())\n                key = np.random.uniform(0, 1, (1, 5, 1)).astype(np.float32)\n                strength = np.random.uniform(0, 1, (1, 1)).astype(np.float32)\n                free_gates = np.random.uniform(0, 1, (1, 2)).astype(np.float32)\n                write_gate = np.random.uniform(0, 1, (1, 1)).astype(np.float32)\n                allocation_gate = np.random.uniform(0, 1, (1,1)).astype(np.float32)\n                write_vector = np.random.uniform(0, 1, (1, 5)).astype(np.float32)\n                erase_vector = np.zeros((1, 5)).astype(np.float32)\n\n                u_op, ww_op, M_op, L_op, p_op = mem.write(\n                    M, u, rw, ww, p, L,\n                    key, strength, free_gates, allocation_gate,\n                    write_gate , write_vector, erase_vector\n                )\n                session.run(tf.initialize_all_variables())\n                u, ww, M, L, p = session.run([u_op, ww_op, M_op, L_op, p_op])\n\n                self.assertEqual(u.shape, (1, 4))\n                self.assertEqual(ww.shape, (1, 4))\n                self.assertEqual(M.shape, (1, 4, 5))\n                self.assertEqual(L.shape, (1, 4, 4))\n                self.assertEqual(p.shape, (1, 4))\n\n\n\n    def test_read(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph = graph) as session:\n                mem = Memory(4, 5, 2, 1)\n                M, u, p, L, ww, rw, r = session.run(mem.init_memory())\n                keys = np.random.uniform(0, 1, (1, 5, 2)).astype(np.float32)\n                strengths = np.random.uniform(0, 1, (1, 2)).astype(np.float32)\n                link_matrix = np.random.uniform(0, 1, (1, 4, 4)).astype(np.float32)\n                read_modes = random_softmax((1, 3, 2), axis=1).astype(np.float32)\n                memory_matrix = np.random.uniform(-1, 1, (1, 4, 5)).astype(np.float32)\n\n                wr_op, r_op = mem.read(memory_matrix, rw, keys, strengths, link_matrix, read_modes)\n                session.run(tf.initialize_all_variables())\n                wr, r = session.run([wr_op, r_op])\n\n                self.assertEqual(wr.shape, (1, 4, 2))\n                self.assertEqual(r.shape, (1, 5, 2))\n\n\nif __name__ == '__main__':\n    unittest.main(verbosity=2)\n"""
unit-tests/utility.py,14,"b'import tensorflow as tf\nimport numpy as np\nimport unittest\n\nimport dnc.utility as util\n\nclass DNCUtilityTests(unittest.TestCase):\n\n    def test_pairwise_add(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                _u = np.array([5, 6])\n                _v = np.array([1, 2])\n\n                predicted_U = np.array([[10, 11], [11, 12]])\n                predicted_UV = np.array([[6, 7], [7, 8]])\n\n                u = tf.constant(_u)\n                v = tf.constant(_v)\n\n                U_op = util.pairwise_add(u)\n                UV_op = util.pairwise_add(u, v)\n\n                U, UV = session.run([U_op, UV_op])\n\n                self.assertTrue(np.allclose(U, predicted_U))\n                self.assertTrue(np.allclose(UV, predicted_UV))\n\n\n    def test_pairwise_add_with_batch(self):\n        graph = tf.Graph()\n        with graph.as_default():\n            with tf.Session(graph=graph) as session:\n\n                _u = np.array([[5, 6], [7, 8]])\n                _v = np.array([[1, 2], [3, 4]])\n\n                predicted_U = np.array([[[10, 11], [11, 12]], [[14, 15], [15, 16]]])\n                predicted_UV = np.array([[[6, 7], [7, 8]], [[10, 11], [11, 12]]])\n\n                u = tf.constant(_u)\n                v = tf.constant(_v)\n\n                U_op = util.pairwise_add(u, is_batch=True)\n                UV_op = util.pairwise_add(u, v, is_batch=True)\n\n                U, UV = session.run([U_op, UV_op])\n\n                self.assertTrue(np.allclose(U, predicted_U))\n                self.assertTrue(np.allclose(UV, predicted_UV))\n\n\n    def test_unpack_into_tensorarray(self):\n         graph = tf.Graph()\n         with graph.as_default():\n             with tf.Session(graph=graph) as session:\n\n                 T = tf.random_normal([5, 10, 7, 7])\n                 ta = util.unpack_into_tensorarray(T, axis=1)\n\n                 vT, vTA5 = session.run([T, ta.read(5)])\n\n                 self.assertEqual(vTA5.shape, (5, 7, 7))\n                 self.assertTrue(np.allclose(vT[:, 5, :, :], vTA5))\n\n\n    def test_pack_into_tensor(self):\n         graph = tf.Graph()\n         with graph.as_default():\n             with tf.Session(graph=graph) as session:\n\n                T = tf.random_normal([5, 10, 7, 7])\n                ta = util.unpack_into_tensorarray(T, axis=1)\n                pT = util.pack_into_tensor(ta, axis=1)\n\n                vT, vPT = session.run([T, pT])\n\n                self.assertEqual(vPT.shape, (5, 10, 7, 7))\n                self.assertTrue(np.allclose(vT, vPT)) \n\n\nif __name__ == ""__main__"":\n    unittest.main(verbosity=2)\n'"
tasks/babi/preprocess.py,0,"b'import sys\nimport pickle\nimport getopt\nimport numpy as np\nfrom shutil import rmtree\nfrom os import listdir, mkdir\nfrom os.path import join, isfile, isdir, dirname, basename, normpath, abspath, exists\n\ndef llprint(message):\n    sys.stdout.write(message)\n    sys.stdout.flush()\n\ndef create_dictionary(files_list):\n    """"""\n    creates a dictionary of unique lexicons in the dataset and their mapping to numbers\n\n    Parameters:\n    ----------\n    files_list: list\n        the list of files to scan through\n\n    Returns: dict\n        the constructed dictionary of lexicons\n    """"""\n\n    lexicons_dict = {}\n    id_counter = 0\n\n    llprint(""Creating Dictionary ... 0/%d"" % (len(files_list)))\n\n    for indx, filename in enumerate(files_list):\n        with open(filename, \'r\') as fobj:\n            for line in fobj:\n\n                # first seperate . and ? away from words into seperate lexicons\n                line = line.replace(\'.\', \' .\')\n                line = line.replace(\'?\', \' ?\')\n                line = line.replace(\',\', \' \')\n\n                for word in line.split():\n                    if not word.lower() in lexicons_dict and word.isalpha():\n                        lexicons_dict[word.lower()] = id_counter\n                        id_counter += 1\n\n        llprint(""\\rCreating Dictionary ... %d/%d"" % ((indx + 1), len(files_list)))\n\n    print ""\\rCreating Dictionary ... Done!""\n    return lexicons_dict\n\n\ndef encode_data(files_list, lexicons_dictionary, length_limit=None):\n    """"""\n    encodes the dataset into its numeric form given a constructed dictionary\n\n    Parameters:\n    ----------\n    files_list: list\n        the list of files to scan through\n    lexicons_dictionary: dict\n        the mappings of unique lexicons\n\n    Returns: tuple (dict, int)\n        the data in its numeric form, maximum story length\n    """"""\n\n    files = {}\n    story_inputs = None\n    story_outputs = None\n    stories_lengths = []\n    answers_flag = False  # a flag to specify when to put data into outputs list\n    limit = length_limit if not length_limit is None else float(""inf"")\n\n    llprint(""Encoding Data ... 0/%d"" % (len(files_list)))\n\n    for indx, filename in enumerate(files_list):\n\n        files[filename] = []\n\n        with open(filename, \'r\') as fobj:\n            for line in fobj:\n\n                # first seperate . and ? away from words into seperate lexicons\n                line = line.replace(\'.\', \' .\')\n                line = line.replace(\'?\', \' ?\')\n                line = line.replace(\',\', \' \')\n\n                answers_flag = False  # reset as answers end by end of line\n\n                for i, word in enumerate(line.split()):\n\n                    if word == \'1\' and i == 0:\n                        # beginning of a new story\n                        if not story_inputs is None:\n                            stories_lengths.append(len(story_inputs))\n                            if len(story_inputs) <= limit:\n                                files[filename].append({\n                                    \'inputs\':story_inputs,\n                                    \'outputs\': story_outputs\n                                })\n                        story_inputs = []\n                        story_outputs = []\n\n                    if word.isalpha() or word == \'?\' or word == \'.\':\n                        if not answers_flag:\n                            story_inputs.append(lexicons_dictionary[word.lower()])\n                        else:\n                            story_inputs.append(lexicons_dictionary[\'-\'])\n                            story_outputs.append(lexicons_dictionary[word.lower()])\n\n                        # set the answers_flags if a question mark is encountered\n                        if not answers_flag:\n                            answers_flag = (word == \'?\')\n\n        llprint(""\\rEncoding Data ... %d/%d"" % (indx + 1, len(files_list)))\n\n    print ""\\rEncoding Data ... Done!""\n    return files, stories_lengths\n\n\nif __name__ == \'__main__\':\n    task_dir = dirname(abspath(__file__))\n    options,_ = getopt.getopt(sys.argv[1:], \'\', [\'data_dir=\', \'single_train\', \'length_limit=\'])\n    data_dir = None\n    joint_train = True\n    length_limit = None\n    files_list = []\n\n    if not exists(join(task_dir, \'data\')):\n        mkdir(join(task_dir, \'data\'))\n\n    for opt in options:\n        if opt[0] == \'--data_dir\':\n            data_dir = opt[1]\n        if opt[0] == \'--single_train\':\n            joint_train = False\n        if opt[0] == \'--length_limit\':\n            length_limit = int(opt[1])\n\n    if data_dir is None:\n        raise ValueError(""data_dir argument cannot be None"")\n\n    for entryname in listdir(data_dir):\n        entry_path = join(data_dir, entryname)\n        if isfile(entry_path):\n            files_list.append(entry_path)\n\n    lexicon_dictionary = create_dictionary(files_list)\n    lexicon_count = len(lexicon_dictionary)\n\n    # append used punctuation to dictionary\n    lexicon_dictionary[\'?\'] = lexicon_count\n    lexicon_dictionary[\'.\'] = lexicon_count + 1\n    lexicon_dictionary[\'-\'] = lexicon_count + 2\n\n    encoded_files, stories_lengths = encode_data(files_list, lexicon_dictionary, length_limit)\n\n    stories_lengths = np.array(stories_lengths)\n    length_limit = np.max(stories_lengths) if length_limit is None else length_limit\n    print ""Total Number of stories: %d"" % (len(stories_lengths))\n    print ""Number of stories with lengthes > %d: %d (%% %.2f) [discarded]"" % (length_limit, np.sum(stories_lengths > length_limit), np.mean(stories_lengths > length_limit) * 100.0)\n    print ""Number of Remaining Stories: %d"" % (len(stories_lengths[stories_lengths <= length_limit]))\n\n    processed_data_dir = join(task_dir, \'data\', basename(normpath(data_dir)))\n    train_data_dir = join(processed_data_dir, \'train\')\n    test_data_dir = join(processed_data_dir, \'test\')\n    if exists(processed_data_dir) and isdir(processed_data_dir):\n        rmtree(processed_data_dir)\n\n    mkdir(processed_data_dir)\n    mkdir(train_data_dir)\n    mkdir(test_data_dir)\n\n    llprint(""Saving processed data to disk ... "")\n\n    pickle.dump(lexicon_dictionary, open(join(processed_data_dir, \'lexicon-dict.pkl\'), \'wb\'))\n\n    joint_train_data = []\n\n    for filename in encoded_files:\n        if filename.endswith(""test.txt""):\n            pickle.dump(encoded_files[filename], open(join(test_data_dir, basename(filename) + \'.pkl\'), \'wb\'))\n        elif filename.endswith(""train.txt""):\n            if not joint_train:\n                pickle.dump(encoded_files[filename], open(join(train_data_dir, basename(filename) + \'.pkl\'), \'wb\'))\n            else:\n                joint_train_data.extend(encoded_files[filename])\n\n    if joint_train:\n        pickle.dump(joint_train_data, open(join(train_data_dir, \'train.pkl\'), \'wb\'))\n\n    llprint(""Done!\\n"")\n'"
tasks/babi/recurrent_controller.py,4,"b'import numpy as np\nimport tensorflow as tf\nfrom dnc.controller import BaseController\n\n""""""\nA 1-layer LSTM recurrent neural network with 256 hidden units\nNote: the state of the LSTM is not saved in a variable becuase we want\nthe state to reset to zero on every input sequnece\n""""""\n\nclass RecurrentController(BaseController):\n\n    def network_vars(self):\n        self.lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(256)\n        self.state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n\n    def network_op(self, X, state):\n        X = tf.convert_to_tensor(X)\n        return self.lstm_cell(X, state)\n\n    def get_state(self):\n        return self.state\n\n    def update_state(self, new_state):\n        return tf.no_op()\n'"
tasks/babi/test.py,3,"b'# -*- coding: utf-8 -*-\n\nfrom recurrent_controller import RecurrentController\nfrom dnc.dnc import DNC\nimport tensorflow as tf\nimport numpy as np\nimport pickle\nimport sys\nimport os\nimport re\n\ndef llprint(message):\n    sys.stdout.write(message)\n    sys.stdout.flush()\n\ndef load(path):\n    return pickle.load(open(path, \'rb\'))\n\ndef onehot(index, size):\n    vec = np.zeros(size, dtype=np.float32)\n    vec[index] = 1.0\n    return vec\n\ndef prepare_sample(sample, target_code, word_space_size):\n    input_vec = np.array(sample[0][\'inputs\'], dtype=np.float32)\n    output_vec = np.array(sample[0][\'inputs\'], dtype=np.float32)\n    seq_len = input_vec.shape[0]\n    weights_vec = np.zeros(seq_len, dtype=np.float32)\n\n    target_mask = (input_vec == target_code)\n    output_vec[target_mask] = sample[0][\'outputs\']\n    weights_vec[target_mask] = 1.0\n\n    input_vec = np.array([onehot(code, word_space_size) for code in input_vec])\n    output_vec = np.array([onehot(code, word_space_size) for code in output_vec])\n\n    return (\n        np.reshape(input_vec, (1, -1, word_space_size)),\n        np.reshape(output_vec, (1, -1, word_space_size)),\n        seq_len,\n        np.reshape(weights_vec, (1, -1, 1))\n    )\n\nckpts_dir = \'./checkpoints/\'\nlexicon_dictionary = load(\'./data/en-10k/lexicon-dict.pkl\')\nquestion_code = lexicon_dictionary[""?""]\ntarget_code = lexicon_dictionary[""-""]\ntest_files = []\n\nfor entryname in os.listdir(\'./data/en-10k/test/\'):\n    entry_path = os.path.join(\'./data/en-10k/test/\', entryname)\n    if os.path.isfile(entry_path):\n        test_files.append(entry_path)\n\ngraph = tf.Graph()\nwith graph.as_default():\n    with tf.Session(graph=graph) as session:\n        \n        ncomputer = DNC(\n            RecurrentController,\n            input_size=len(lexicon_dictionary),\n            output_size=len(lexicon_dictionary),\n            max_sequence_length=100,\n            memory_words_num=256,\n            memory_word_size=64,\n            memory_read_heads=4,\n        )\n        \n        ncomputer.restore(session, ckpts_dir, \'step-500005\')\n        \n        outputs, _ = ncomputer.get_outputs()\n        softmaxed = tf.nn.softmax(outputs)\n        \n        tasks_results = {}\n        tasks_names = {}\n        for test_file in test_files:\n            test_data = load(test_file)\n            task_regexp = r\'qa([0-9]{1,2})_([a-z\\-]*)_test.txt.pkl\'\n            task_filename = os.path.basename(test_file)\n            task_match_obj = re.match(task_regexp, task_filename)\n            task_number = task_match_obj.group(1)\n            task_name = task_match_obj.group(2).replace(\'-\', \' \')\n            tasks_names[task_number] = task_name\n            counter = 0\n            results = []\n            \n            llprint(""%s ... %d/%d"" % (task_name, counter, len(test_data)))\n            \n            for story in test_data:\n                astory = np.array(story[\'inputs\'])\n                questions_indecies = np.argwhere(astory == question_code)\n                questions_indecies = np.reshape(questions_indecies, (-1,))\n                target_mask = (astory == target_code)\n                \n                desired_answers = np.array(story[\'outputs\'])\n                input_vec, _, seq_len, _ = prepare_sample([story], target_code, len(lexicon_dictionary))\n                softmax_output = session.run(softmaxed, feed_dict={\n                        ncomputer.input_data: input_vec,\n                        ncomputer.sequence_length: seq_len\n                })\n\n                softmax_output = np.squeeze(softmax_output, axis=0)\n                given_answers = np.argmax(softmax_output[target_mask], axis=1)\n                \n                \n                answers_cursor = 0\n                for question_indx in questions_indecies:\n                    question_grade = []\n                    targets_cursor = question_indx + 1\n                    while targets_cursor < len(astory) and astory[targets_cursor] == target_code:\n                        question_grade.append(given_answers[answers_cursor] == desired_answers[answers_cursor])\n                        answers_cursor += 1\n                        targets_cursor += 1\n                    results.append(np.prod(question_grade))\n                \n                counter += 1\n                llprint(""\\r%s ... %d/%d"" % (task_name, counter, len(test_data)))\n                \n            error_rate = 1. - np.mean(results)\n            tasks_results[task_number] = error_rate\n            llprint(""\\r%s ... %.3f%% Error Rate.\\n"" % (task_name, error_rate * 100))\n        \n        print ""\\n""\n        print ""%-27s%-27s%s"" % (""Task"", ""Result"", ""Paper\'s Mean"")\n        print ""-------------------------------------------------------------------""\n        paper_means = {\n            \'1\': \'9.0\xc2\xb112.6%\', \'2\': \'39.2\xc2\xb120.5%\', \'3\': \'39.6\xc2\xb116.4%\',\n            \'4\': \'0.4\xc2\xb10.7%\', \'5\': \'1.5\xc2\xb11.0%\', \'6\': \'6.9\xc2\xb17.5%\', \'7\': \'9.8\xc2\xb17.0%\',\n            \'8\': \'5.5\xc2\xb15.9%\', \'9\': \'7.7\xc2\xb18.3%\', \'10\': \'9.6\xc2\xb111.4%\', \'11\':\'3.3\xc2\xb15.7%\',\n            \'12\': \'5.0\xc2\xb16.3%\', \'13\': \'3.1\xc2\xb13.6%\', \'14\': \'11.0\xc2\xb17.5%\', \'15\': \'27.2\xc2\xb120.1%\',\n            \'16\': \'53.6\xc2\xb11.9%\', \'17\': \'32.4\xc2\xb18.0%\', \'18\': \'4.2\xc2\xb11.8%\', \'19\': \'64.6\xc2\xb137.4%\',\n            \'20\': \'0.0\xc2\xb10.1%\', \'mean\': \'16.7\xc2\xb17.6%\', \'fail\': \'11.2\xc2\xb15.4\'\n        }\n        for k in range(20):\n            task_id = str(k + 1)\n            task_result = ""%.2f%%"" % (tasks_results[task_id] * 100)\n            print ""%-27s%-27s%s"" % (tasks_names[task_id], task_result, paper_means[task_id])\n        print ""-------------------------------------------------------------------""\n        all_tasks_results = [v for _,v in tasks_results.iteritems()]\n        results_mean = ""%.2f%%"" % (np.mean(all_tasks_results) * 100)\n        failed_count = ""%d"" % (np.sum(np.array(all_tasks_results) > 0.05))\n        \n        print ""%-27s%-27s%s"" % (""Mean Err."", results_mean, paper_means[\'mean\'])\n        print ""%-27s%-27s%s"" % (""Failed (err. > 5%)"", failed_count, paper_means[\'fail\'])\n'"
tasks/babi/train.py,13,"b'import warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport tensorflow as tf\nimport numpy as np\nimport pickle\nimport getopt\nimport time\nimport sys\nimport os\n\nfrom dnc.dnc import DNC\nfrom recurrent_controller import RecurrentController\n\ndef llprint(message):\n    sys.stdout.write(message)\n    sys.stdout.flush()\n\ndef load(path):\n    return pickle.load(open(path, \'rb\'))\n\ndef onehot(index, size):\n    vec = np.zeros(size, dtype=np.float32)\n    vec[index] = 1.0\n    return vec\n\ndef prepare_sample(sample, target_code, word_space_size):\n    input_vec = np.array(sample[0][\'inputs\'], dtype=np.float32)\n    output_vec = np.array(sample[0][\'inputs\'], dtype=np.float32)\n    seq_len = input_vec.shape[0]\n    weights_vec = np.zeros(seq_len, dtype=np.float32)\n\n    target_mask = (input_vec == target_code)\n    output_vec[target_mask] = sample[0][\'outputs\']\n    weights_vec[target_mask] = 1.0\n\n    input_vec = np.array([onehot(code, word_space_size) for code in input_vec])\n    output_vec = np.array([onehot(code, word_space_size) for code in output_vec])\n\n    return (\n        np.reshape(input_vec, (1, -1, word_space_size)),\n        np.reshape(output_vec, (1, -1, word_space_size)),\n        seq_len,\n        np.reshape(weights_vec, (1, -1, 1))\n    )\n\n\n\nif __name__ == \'__main__\':\n\n    dirname = os.path.dirname(__file__)\n    ckpts_dir = os.path.join(dirname , \'checkpoints\')\n    data_dir = os.path.join(dirname, \'data\', \'en-10k\')\n    tb_logs_dir = os.path.join(dirname, \'logs\')\n\n    llprint(""Loading Data ... "")\n    lexicon_dict = load(os.path.join(data_dir, \'lexicon-dict.pkl\'))\n    data = load(os.path.join(data_dir, \'train\', \'train.pkl\'))\n    llprint(""Done!\\n"")\n\n    batch_size = 1\n    input_size = output_size = len(lexicon_dict)\n    sequence_max_length = 100\n    word_space_size = len(lexicon_dict)\n    words_count = 256\n    word_size = 64\n    read_heads = 4\n\n    learning_rate = 1e-4\n    momentum = 0.9\n\n    from_checkpoint = None\n    iterations = 100000\n    start_step = 0\n\n    options,_ = getopt.getopt(sys.argv[1:], \'\', [\'checkpoint=\', \'iterations=\', \'start=\'])\n\n    for opt in options:\n        if opt[0] == \'--checkpoint\':\n            from_checkpoint = opt[1]\n        elif opt[0] == \'--iterations\':\n            iterations = int(opt[1])\n        elif opt[0] == \'--start\':\n            start_step = int(opt[1])\n\n    graph = tf.Graph()\n    with graph.as_default():\n        with tf.Session(graph=graph) as session:\n\n            llprint(""Building Computational Graph ... "")\n\n            optimizer = tf.train.RMSPropOptimizer(learning_rate, momentum=momentum)\n            summerizer = tf.train.SummaryWriter(tb_logs_dir, session.graph)\n\n            ncomputer = DNC(\n                RecurrentController,\n                input_size,\n                output_size,\n                sequence_max_length,\n                words_count,\n                word_size,\n                read_heads,\n                batch_size\n            )\n\n            output, _ = ncomputer.get_outputs()\n\n            loss_weights = tf.placeholder(tf.float32, [batch_size, None, 1])\n            loss = tf.reduce_mean(\n                loss_weights * tf.nn.softmax_cross_entropy_with_logits(output, ncomputer.target_output)\n            )\n\n            summeries = []\n\n            gradients = optimizer.compute_gradients(loss)\n            for i, (grad, var) in enumerate(gradients):\n                if grad is not None:\n                    gradients[i] = (tf.clip_by_value(grad, -10, 10), var)\n            for (grad, var) in gradients:\n                if grad is not None:\n                    summeries.append(tf.histogram_summary(var.name + \'/grad\', grad))\n\n            apply_gradients = optimizer.apply_gradients(gradients)\n\n            summeries.append(tf.scalar_summary(""Loss"", loss))\n\n            summerize_op = tf.merge_summary(summeries)\n            no_summerize = tf.no_op()\n\n            llprint(""Done!\\n"")\n\n            llprint(""Initializing Variables ... "")\n            session.run(tf.initialize_all_variables())\n            llprint(""Done!\\n"")\n\n            if from_checkpoint is not None:\n                llprint(""Restoring Checkpoint %s ... "" % (from_checkpoint))\n                ncomputer.restore(session, ckpts_dir, from_checkpoint)\n                llprint(""Done!\\n"")\n\n\n            last_100_losses = []\n\n            start = 0 if start_step == 0 else start_step + 1\n            end = start_step + iterations + 1\n\n            start_time_100 = time.time()\n            end_time_100 = None\n            avg_100_time = 0.\n            avg_counter = 0\n\n            for i in xrange(start, end + 1):\n                try:\n                    llprint(""\\rIteration %d/%d"" % (i, end))\n\n                    sample = np.random.choice(data, 1)\n                    input_data, target_output, seq_len, weights = prepare_sample(sample, lexicon_dict[\'-\'], word_space_size)\n\n                    summerize = (i % 100 == 0)\n                    take_checkpoint = (i != 0) and (i % end == 0)\n\n                    loss_value, _, summary = session.run([\n                        loss,\n                        apply_gradients,\n                        summerize_op if summerize else no_summerize\n                    ], feed_dict={\n                        ncomputer.input_data: input_data,\n                        ncomputer.target_output: target_output,\n                        ncomputer.sequence_length: seq_len,\n                        loss_weights: weights\n                    })\n\n                    last_100_losses.append(loss_value)\n                    summerizer.add_summary(summary, i)\n\n                    if summerize:\n                        llprint(""\\n\\tAvg. Cross-Entropy: %.7f\\n"" % (np.mean(last_100_losses)))\n\n                        end_time_100 = time.time()\n                        elapsed_time = (end_time_100 - start_time_100) / 60\n                        avg_counter += 1\n                        avg_100_time += (1. / avg_counter) * (elapsed_time - avg_100_time)\n                        estimated_time = (avg_100_time * ((end - i) / 100.)) / 60.\n\n                        print ""\\tAvg. 100 iterations time: %.2f minutes"" % (avg_100_time)\n                        print ""\\tApprox. time to completion: %.2f hours"" % (estimated_time)\n\n                        start_time_100 = time.time()\n                        last_100_losses = []\n\n                    if take_checkpoint:\n                        llprint(""\\nSaving Checkpoint ... ""),\n                        ncomputer.save(session, ckpts_dir, \'step-%d\' % (i))\n                        llprint(""Done!\\n"")\n\n                except KeyboardInterrupt:\n\n                    llprint(""\\nSaving Checkpoint ... ""),\n                    ncomputer.save(session, ckpts_dir, \'step-%d\' % (i))\n                    llprint(""Done!\\n"")\n                    sys.exit(0)\n'"
tasks/copy/feedforward_controller.py,14,"b'import numpy as np\nimport tensorflow as tf\nfrom dnc.controller import BaseController\n\n\n""""""\nA 2-Layers feedforward neural network with 128, 256 nodes respectively\n""""""\n\nclass FeedforwardController(BaseController):\n\n    def network_vars(self):\n        initial_std = lambda in_nodes: np.min(1e-2, np.sqrt(2.0 / in_nodes))\n        input_ = self.nn_input_size\n\n        self.W1 = tf.Variable(tf.truncated_normal([input_, 128], stddev=initial_std(input_)), name=\'layer1_W\')\n        self.W2 = tf.Variable(tf.truncated_normal([128, 256], stddev=initial_std(128)), name=\'layer2_W\')\n        self.b1 = tf.Variable(tf.zeros([128]), name=\'layer1_b\')\n        self.b2 = tf.Variable(tf.zeros([256]), name=\'layer2_b\')\n\n\n    def network_op(self, X):\n        l1_output = tf.matmul(X, self.W1) + self.b1\n        l1_activation = tf.nn.relu(l1_output)\n\n        l2_output = tf.matmul(l1_activation, self.W2) + self.b2\n        l2_activation = tf.nn.relu(l2_output)\n\n        return l2_activation\n\n    def initials(self):\n        initial_std = lambda in_nodes: np.min(1e-2, np.sqrt(2.0 / in_nodes))\n\n        # defining internal weights of the controller\n        self.interface_weights = tf.Variable(\n            tf.truncated_normal([self.nn_output_size, self.interface_vector_size], stddev=initial_std(self.nn_output_size)),\n            name=\'interface_weights\'\n        )\n        self.nn_output_weights = tf.Variable(\n            tf.truncated_normal([self.nn_output_size, self.output_size], stddev=initial_std(self.nn_output_size)),\n            name=\'nn_output_weights\'\n        )\n        self.mem_output_weights = tf.Variable(\n            tf.truncated_normal([self.word_size * self.read_heads, self.output_size],  stddev=initial_std(self.word_size * self.read_heads)),\n            name=\'mem_output_weights\'\n        )\n'"
tasks/copy/train-series.py,13,"b'import warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport tensorflow as tf\nimport numpy as np\nimport getopt\nimport sys\nimport os\n\nfrom dnc.dnc import DNC\nfrom feedforward_controller import FeedforwardController\n\ndef llprint(message):\n    sys.stdout.write(message)\n    sys.stdout.flush()\n\ndef generate_data(batch_size, length, size):\n\n    input_data = np.zeros((batch_size, 2 * length + 1, size), dtype=np.float32)\n    target_output = np.zeros((batch_size, 2 * length + 1, size), dtype=np.float32)\n\n    sequence = np.random.binomial(1, 0.5, (batch_size, length, size - 1))\n\n    input_data[:, :length, :size - 1] = sequence\n    input_data[:, length, -1] = 1  # the end symbol\n    target_output[:, length + 1:, :size - 1] = sequence\n\n    return input_data, target_output\n\n\ndef binary_cross_entropy(predictions, targets):\n\n    return tf.reduce_mean(\n        -1 * targets * tf.log(predictions) - (1 - targets) * tf.log(1 - predictions)\n    )\n\n\nif __name__ == \'__main__\':\n\n    dirname = os.path.dirname(__file__)\n    ckpts_dir = os.path.join(dirname , \'checkpoints\')\n    tb_logs_dir = os.path.join(dirname, \'logs\')\n\n    batch_size = 1\n    input_size = output_size = 6\n    series_length = 2\n    sequence_max_length = 22\n    words_count = 10\n    word_size = 10\n    read_heads = 1\n\n    learning_rate = 1e-4\n    momentum = 0.9\n\n    from_checkpoint = None\n    iterations = 100000\n    start_step = 0\n\n    options,_ = getopt.getopt(sys.argv[1:], \'\', [\'checkpoint=\', \'iterations=\', \'start=\', \'length=\'])\n\n    for opt in options:\n        if opt[0] == \'--checkpoint\':\n            from_checkpoint = opt[1]\n        elif opt[0] == \'--iterations\':\n            iterations = int(opt[1])\n        elif opt[0] == \'--start\':\n            start_step = int(opt[1])\n        elif opt[0] == \'--length\':\n            series_length = int(opt[1])\n            sequence_max_length = 11 * int(opt[1])\n\n    graph = tf.Graph()\n\n    with graph.as_default():\n        with tf.Session(graph=graph) as session:\n\n            llprint(""Building Computational Graph ... "")\n\n            optimizer = tf.train.RMSPropOptimizer(learning_rate, momentum=momentum)\n            summerizer = tf.train.SummaryWriter(tb_logs_dir, session.graph)\n\n            ncomputer = DNC(\n                FeedforwardController,\n                input_size,\n                output_size,\n                sequence_max_length,\n                words_count,\n                word_size,\n                read_heads,\n                batch_size\n            )\n\n            output, _ = ncomputer.get_outputs()\n            squashed_output = tf.clip_by_value(tf.sigmoid(output), 1e-6, 1. - 1e-6)\n\n            loss = binary_cross_entropy(squashed_output, ncomputer.target_output)\n\n            summeries = []\n\n            gradients = optimizer.compute_gradients(loss)\n            for i, (grad, var) in enumerate(gradients):\n                if grad is not None:\n                    summeries.append(tf.histogram_summary(var.name + \'/grad\', grad))\n                    gradients[i] = (tf.clip_by_value(grad, -10, 10), var)\n\n            apply_gradients = optimizer.apply_gradients(gradients)\n\n            summeries.append(tf.scalar_summary(""Loss"", loss))\n\n            summerize_op = tf.merge_summary(summeries)\n            no_summerize = tf.no_op()\n\n            llprint(""Done!\\n"")\n\n            llprint(""Initializing Variables ... "")\n            session.run(tf.initialize_all_variables())\n            llprint(""Done!\\n"")\n\n            if from_checkpoint is not None:\n                llprint(""Restoring Checkpoint %s ... "" % (from_checkpoint))\n                ncomputer.restore(session, ckpts_dir, from_checkpoint)\n                llprint(""Done!\\n"")\n\n\n            last_100_losses = []\n\n            start = 0 if start_step == 0 else start_step + 1\n            end = start_step + iterations + 1\n\n            for i in xrange(start, end):\n                llprint(""\\rIteration %d/%d"" % (i, end - 1))\n\n                input_series = []\n                output_series = []\n\n                for k in range(series_length):\n                    input_data, target_output = generate_data(batch_size, 5, input_size)\n                    input_series.append(input_data)\n                    output_series.append(target_output)\n\n                one_big_input = np.concatenate(input_series, axis=1)\n                one_big_output = np.concatenate(output_series, axis=1)\n\n                summerize = (i % 100 == 0)\n                take_checkpoint = (i != 0) and (i % iterations == 0)\n\n                loss_value, _, summary = session.run([\n                    loss,\n                    apply_gradients,\n                    summerize_op if summerize else no_summerize\n                ], feed_dict={\n                    ncomputer.input_data: one_big_input,\n                    ncomputer.target_output: one_big_output,\n                    ncomputer.sequence_length: sequence_max_length\n                })\n\n                last_100_losses.append(loss_value)\n                summerizer.add_summary(summary, i)\n\n                if summerize:\n                    llprint(""\\n\\tAvg. Logistic Loss: %.4f\\n"" % (np.mean(last_100_losses)))\n                    last_100_losses = []\n\n                if take_checkpoint:\n                    llprint(""\\nSaving Checkpoint ... ""),\n                    ncomputer.save(session, ckpts_dir, \'step-%d\' % (i))\n                    llprint(""Done!\\n"")\n'"
tasks/copy/train.py,13,"b'import warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport tensorflow as tf\nimport numpy as np\nimport getopt\nimport sys\nimport os\n\nfrom dnc.dnc import DNC\nfrom feedforward_controller import FeedforwardController\n\ndef llprint(message):\n    sys.stdout.write(message)\n    sys.stdout.flush()\n\ndef generate_data(batch_size, length, size):\n\n    input_data = np.zeros((batch_size, 2 * length + 1, size), dtype=np.float32)\n    target_output = np.zeros((batch_size, 2 * length + 1, size), dtype=np.float32)\n\n    sequence = np.random.binomial(1, 0.5, (batch_size, length, size - 1))\n\n    input_data[:, :length, :size - 1] = sequence\n    input_data[:, length, -1] = 1  # the end symbol\n    target_output[:, length + 1:, :size - 1] = sequence\n\n    return input_data, target_output\n\n\ndef binary_cross_entropy(predictions, targets):\n\n    return tf.reduce_mean(\n        -1 * targets * tf.log(predictions) - (1 - targets) * tf.log(1 - predictions)\n    )\n\n\nif __name__ == \'__main__\':\n\n    dirname = os.path.dirname(__file__)\n    ckpts_dir = os.path.join(dirname , \'checkpoints\')\n    tb_logs_dir = os.path.join(dirname, \'logs\')\n\n    batch_size = 1\n    input_size = output_size = 6\n    sequence_max_length = 10\n    words_count = 15\n    word_size = 10\n    read_heads = 1\n\n    learning_rate = 1e-4\n    momentum = 0.9\n\n    from_checkpoint = None\n    iterations = 100000\n\n    options,_ = getopt.getopt(sys.argv[1:], \'\', [\'checkpoint=\', \'iterations=\'])\n\n    for opt in options:\n        if opt[0] == \'--checkpoint\':\n            from_checkpoint = opt[1]\n        elif opt[0] == \'--iterations\':\n            iterations = int(opt[1])\n\n    graph = tf.Graph()\n\n    with graph.as_default():\n        with tf.Session(graph=graph) as session:\n\n            llprint(""Building Computational Graph ... "")\n\n            optimizer = tf.train.RMSPropOptimizer(learning_rate, momentum=momentum)\n\n            ncomputer = DNC(\n                FeedforwardController,\n                input_size,\n                output_size,\n                2 * sequence_max_length + 1,\n                words_count,\n                word_size,\n                read_heads,\n                batch_size\n            )\n\n            # squash the DNC output between 0 and 1\n            output, _ = ncomputer.get_outputs()\n            squashed_output = tf.clip_by_value(tf.sigmoid(output), 1e-6, 1. - 1e-6)\n\n            loss = binary_cross_entropy(squashed_output, ncomputer.target_output)\n\n            summeries = []\n\n            gradients = optimizer.compute_gradients(loss)\n            for i, (grad, var) in enumerate(gradients):\n                if grad is not None:\n                    summeries.append(tf.histogram_summary(var.name + \'/grad\', grad))\n                    gradients[i] = (tf.clip_by_value(grad, -10, 10), var)\n\n            apply_gradients = optimizer.apply_gradients(gradients)\n\n            summeries.append(tf.scalar_summary(""Loss"", loss))\n\n            summerize_op = tf.merge_summary(summeries)\n            no_summerize = tf.no_op()\n\n            summerizer = tf.train.SummaryWriter(tb_logs_dir, session.graph)\n\n            llprint(""Done!\\n"")\n\n            llprint(""Initializing Variables ... "")\n            session.run(tf.initialize_all_variables())\n            llprint(""Done!\\n"")\n\n            if from_checkpoint is not None:\n                llprint(""Restoring Checkpoint %s ... "" % (from_checkpoint))\n                ncomputer.restore(session, ckpts_dir, from_checkpoint)\n                llprint(""Done!\\n"")\n\n\n            last_100_losses = []\n\n            for i in xrange(iterations + 1):\n                llprint(""\\rIteration %d/%d"" % (i, iterations))\n\n                random_length = np.random.randint(1, sequence_max_length + 1)\n                input_data, target_output = generate_data(batch_size, random_length, input_size)\n\n                summerize = (i % 100 == 0)\n                take_checkpoint = (i != 0) and (i % iterations == 0)\n\n                loss_value, _, summary = session.run([\n                    loss,\n                    apply_gradients,\n                    summerize_op if summerize else no_summerize\n                ], feed_dict={\n                    ncomputer.input_data: input_data,\n                    ncomputer.target_output: target_output,\n                    ncomputer.sequence_length: 2 * random_length + 1\n                })\n\n                last_100_losses.append(loss_value)\n                summerizer.add_summary(summary, i)\n\n                if summerize:\n                    llprint(""\\n\\tAvg. Logistic Loss: %.4f\\n"" % (np.mean(last_100_losses)))\n                    last_100_losses = []\n\n                if take_checkpoint:\n                    llprint(""\\nSaving Checkpoint ... ""),\n                    ncomputer.save(session, ckpts_dir, \'step-%d\' % (i))\n                    llprint(""Done!\\n"")\n'"
