file_path,api_count,code
base/base_model.py,7,"b'import tensorflow as tf\n\n\nclass BaseModel:\n    def __init__(self, config):\n        self.config = config\n        # init the global step\n        self.init_global_step()\n        # init the epoch counter\n        self.init_cur_epoch()\n\n    # save function that saves the checkpoint in the path defined in the config file\n    def save(self, sess):\n        print(""Saving model..."")\n        self.saver.save(sess, self.config.checkpoint_dir, self.global_step_tensor)\n        print(""Model saved"")\n\n    # load latest checkpoint from the experiment path defined in the config file\n    def load(self, sess):\n        latest_checkpoint = tf.train.latest_checkpoint(self.config.checkpoint_dir)\n        if latest_checkpoint:\n            print(""Loading model checkpoint {} ...\\n"".format(latest_checkpoint))\n            self.saver.restore(sess, latest_checkpoint)\n            print(""Model loaded"")\n\n    # just initialize a tensorflow variable to use it as epoch counter\n    def init_cur_epoch(self):\n        with tf.variable_scope(\'cur_epoch\'):\n            self.cur_epoch_tensor = tf.Variable(0, trainable=False, name=\'cur_epoch\')\n            self.increment_cur_epoch_tensor = tf.assign(self.cur_epoch_tensor, self.cur_epoch_tensor + 1)\n\n    # just initialize a tensorflow variable to use it as global step counter\n    def init_global_step(self):\n        # DON\'T forget to add the global step tensor to the tensorflow trainer\n        with tf.variable_scope(\'global_step\'):\n            self.global_step_tensor = tf.Variable(0, trainable=False, name=\'global_step\')\n\n    def init_saver(self):\n        # just copy the following line in your child class\n        # self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep)\n        raise NotImplementedError\n\n    def build_model(self):\n        raise NotImplementedError\n'"
base/base_train.py,1,"b'import tensorflow as tf\n\n\nclass BaseTrain:\n    def __init__(self, sess, model, data, config, logger):\n        self.model = model\n        self.logger = logger\n        self.config = config\n        self.sess = sess\n        self.data = data\n        self.init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n        self.sess.run(self.init)\n\n    def train(self):\n        for cur_epoch in range(self.model.cur_epoch_tensor.eval(self.sess), self.config.num_epochs + 1, 1):\n            self.train_epoch()\n            self.sess.run(self.model.increment_cur_epoch_tensor)\n\n    def train_epoch(self):\n        """"""\n        implement the logic of epoch:\n        -loop over the number of iterations in the config and call the train step\n        -add any summaries you want using the summary\n        """"""\n        raise NotImplementedError\n\n    def train_step(self):\n        """"""\n        implement the logic of the train step\n        - run the tensorflow session\n        - return any metrics you need to summarize\n        """"""\n        raise NotImplementedError\n'"
data_loader/data_generator.py,0,"b'import numpy as np\n\n\nclass DataGenerator:\n    def __init__(self, config):\n        self.config = config\n        # load data here\n        self.input = np.ones((500, 784))\n        self.y = np.ones((500, 10))\n\n    def next_batch(self, batch_size):\n        idx = np.random.choice(500, batch_size)\n        yield self.input[idx], self.y[idx]\n'"
mains/example.py,1,"b'import tensorflow as tf\n\nfrom data_loader.data_generator import DataGenerator\nfrom models.example_model import ExampleModel\nfrom trainers.example_trainer import ExampleTrainer\nfrom utils.config import process_config\nfrom utils.dirs import create_dirs\nfrom utils.logger import Logger\nfrom utils.utils import get_args\n\n\ndef main():\n    # capture the config path from the run arguments\n    # then process the json configuration file\n    try:\n        args = get_args()\n        config = process_config(args.config)\n\n    except:\n        print(""missing or invalid arguments"")\n        exit(0)\n\n    # create the experiments dirs\n    create_dirs([config.summary_dir, config.checkpoint_dir])\n    # create tensorflow session\n    sess = tf.Session()\n    # create your data generator\n    data = DataGenerator(config)\n    \n    # create an instance of the model you want\n    model = ExampleModel(config)\n    # create tensorboard logger\n    logger = Logger(sess, config)\n    # create trainer and pass all the previous components to it\n    trainer = ExampleTrainer(sess, model, data, config, logger)\n    #load model if exists\n    model.load(sess)\n    # here you train your model\n    trainer.train()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
models/example_model.py,13,"b'from base.base_model import BaseModel\nimport tensorflow as tf\n\n\nclass ExampleModel(BaseModel):\n    def __init__(self, config):\n        super(ExampleModel, self).__init__(config)\n        self.build_model()\n        self.init_saver()\n\n    def build_model(self):\n        self.is_training = tf.placeholder(tf.bool)\n\n        self.x = tf.placeholder(tf.float32, shape=[None] + self.config.state_size)\n        self.y = tf.placeholder(tf.float32, shape=[None, 10])\n\n        # network architecture\n        d1 = tf.layers.dense(self.x, 512, activation=tf.nn.relu, name=""dense1"")\n        d2 = tf.layers.dense(d1, 10, name=""dense2"")\n\n        with tf.name_scope(""loss""):\n            self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=d2))\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                self.train_step = tf.train.AdamOptimizer(self.config.learning_rate).minimize(self.cross_entropy,\n                                                                                         global_step=self.global_step_tensor)\n            correct_prediction = tf.equal(tf.argmax(d2, 1), tf.argmax(self.y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\n    def init_saver(self):\n        # here you initialize the tensorflow saver that will be used in saving the checkpoints.\n        self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep)\n\n'"
models/template_model.py,1,"b'from base.base_model import BaseModel\nimport tensorflow as tf\n\n\nclass TemplateModel(BaseModel):\n    def __init__(self, config):\n        super(TemplateModel, self).__init__(config)\n\n        self.build_model()\n        self.init_saver()\n\n    def build_model(self):\n        # here you build the tensorflow graph of any model you want and also define the loss.\n        pass\n\n    def init_saver(self):\n        # here you initialize the tensorflow saver that will be used in saving the checkpoints.\n        # self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep)\n\n        pass\n'"
trainers/example_trainer.py,0,"b""from base.base_train import BaseTrain\nfrom tqdm import tqdm\nimport numpy as np\n\n\nclass ExampleTrainer(BaseTrain):\n    def __init__(self, sess, model, data, config,logger):\n        super(ExampleTrainer, self).__init__(sess, model, data, config,logger)\n\n    def train_epoch(self):\n        loop = tqdm(range(self.config.num_iter_per_epoch))\n        losses = []\n        accs = []\n        for _ in loop:\n            loss, acc = self.train_step()\n            losses.append(loss)\n            accs.append(acc)\n        loss = np.mean(losses)\n        acc = np.mean(accs)\n\n        cur_it = self.model.global_step_tensor.eval(self.sess)\n        summaries_dict = {\n            'loss': loss,\n            'acc': acc,\n        }\n        self.logger.summarize(cur_it, summaries_dict=summaries_dict)\n        self.model.save(self.sess)\n\n    def train_step(self):\n        batch_x, batch_y = next(self.data.next_batch(self.config.batch_size))\n        feed_dict = {self.model.x: batch_x, self.model.y: batch_y, self.model.is_training: True}\n        _, loss, acc = self.sess.run([self.model.train_step, self.model.cross_entropy, self.model.accuracy],\n                                     feed_dict=feed_dict)\n        return loss, acc\n"""
trainers/template_trainer.py,0,"b'from base.base_train import BaseTrain\nfrom tqdm import tqdm\nimport numpy as np\n\n\nclass TemplateTrainer(BaseTrain):\n    def __init__(self, sess, model, data, config, logger):\n        super(TemplateTrainer, self).__init__(sess, model, data, config, logger)\n\n    def train_epoch(self):\n        """"""\n       implement the logic of epoch:\n       -loop on the number of iterations in the config and call the train step\n       -add any summaries you want using the summary\n        """"""\n        pass\n\n    def train_step(self):\n        """"""\n       implement the logic of the train step\n       - run the tensorflow session\n       - return any metrics you need to summarize\n       """"""\n        pass\n'"
utils/__init__.py,0,b''
utils/config.py,0,"b'import json\nfrom bunch import Bunch\nimport os\n\n\ndef get_config_from_json(json_file):\n    """"""\n    Get the config from a json file\n    :param json_file:\n    :return: config(namespace) or config(dictionary)\n    """"""\n    # parse the configurations from the config json file provided\n    with open(json_file, \'r\') as config_file:\n        config_dict = json.load(config_file)\n\n    # convert the dictionary to a namespace using bunch lib\n    config = Bunch(config_dict)\n\n    return config, config_dict\n\n\ndef process_config(json_file):\n    config, _ = get_config_from_json(json_file)\n    config.summary_dir = os.path.join(""../experiments"", config.exp_name, ""summary/"")\n    config.checkpoint_dir = os.path.join(""../experiments"", config.exp_name, ""checkpoint/"")\n    return config\n'"
utils/dirs.py,0,"b'import os\n\n\ndef create_dirs(dirs):\n    """"""\n    dirs - a list of directories to create if these directories are not found\n    :param dirs:\n    :return exit_code: 0:success -1:failed\n    """"""\n    try:\n        for dir_ in dirs:\n            if not os.path.exists(dir_):\n                os.makedirs(dir_)\n        return 0\n    except Exception as err:\n        print(""Creating directories error: {0}"".format(err))\n        exit(-1)\n'"
utils/logger.py,7,"b'import tensorflow as tf\nimport os\n\n\nclass Logger:\n    def __init__(self, sess,config):\n        self.sess = sess\n        self.config = config\n        self.summary_placeholders = {}\n        self.summary_ops = {}\n        self.train_summary_writer = tf.summary.FileWriter(os.path.join(self.config.summary_dir, ""train""),\n                                                          self.sess.graph)\n        self.test_summary_writer = tf.summary.FileWriter(os.path.join(self.config.summary_dir, ""test""))\n\n    # it can summarize scalars and images.\n    def summarize(self, step, summarizer=""train"", scope="""", summaries_dict=None):\n        """"""\n        :param step: the step of the summary\n        :param summarizer: use the train summary writer or the test one\n        :param scope: variable scope\n        :param summaries_dict: the dict of the summaries values (tag,value)\n        :return:\n        """"""\n        summary_writer = self.train_summary_writer if summarizer == ""train"" else self.test_summary_writer\n        with tf.variable_scope(scope):\n\n            if summaries_dict is not None:\n                summary_list = []\n                for tag, value in summaries_dict.items():\n                    if tag not in self.summary_ops:\n                        if len(value.shape) <= 1:\n                            self.summary_placeholders[tag] = tf.placeholder(\'float32\', value.shape, name=tag)\n                        else:\n                            self.summary_placeholders[tag] = tf.placeholder(\'float32\', [None] + list(value.shape[1:]), name=tag)\n                        if len(value.shape) <= 1:\n                            self.summary_ops[tag] = tf.summary.scalar(tag, self.summary_placeholders[tag])\n                        else:\n                            self.summary_ops[tag] = tf.summary.image(tag, self.summary_placeholders[tag])\n\n                    summary_list.append(self.sess.run(self.summary_ops[tag], {self.summary_placeholders[tag]: value}))\n\n                for summary in summary_list:\n                    summary_writer.add_summary(summary, step)\n                summary_writer.flush()\n'"
utils/utils.py,0,"b""import argparse\n\n\ndef get_args():\n    argparser = argparse.ArgumentParser(description=__doc__)\n    argparser.add_argument(\n        '-c', '--config',\n        metavar='C',\n        default='None',\n        help='The Configuration file')\n    args = argparser.parse_args()\n    return args\n"""
