file_path,api_count,code
GRU.py,9,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python2\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport hashlib\nimport numbers\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import nest\nfrom tensorflow.contrib.rnn import RNNCell\nfrom layers import gated_attention\nfrom params import Params\n\n_BIAS_VARIABLE_NAME = ""bias""\n_WEIGHTS_VARIABLE_NAME = ""kernel""\n\nclass SRUCell(RNNCell):\n    """"""Simple Recurrent Unit (SRU).\n       This implementation is based on:\n       Tao Lei and Yu Zhang,\n       ""Training RNNs as Fast as CNNs,""\n       https://arxiv.org/abs/1709.02755\n    """"""\n\n    def __init__(self, num_units, activation=None, is_training = True, reuse=None):\n        self._num_units = num_units\n        self._activation = activation or tf.tanh\n        self._is_training = is_training\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run one step of SRU.""""""\n        with tf.variable_scope(scope or type(self).__name__):  # ""SRUCell""\n            with tf.variable_scope(""x_hat""):\n                x = linear([inputs], self._num_units, False)\n            with tf.variable_scope(""gates""):\n                concat = tf.sigmoid(linear([inputs], 2 * self._num_units, True))\n                f, r = tf.split(concat, 2, axis = 1)\n            with tf.variable_scope(""candidates""):\n                c = self._activation(f * state + (1 - f) * x)\n                # variational dropout as suggested in the paper (disabled)\n                # if self._is_training and Params.dropout is not None:\n                #     c = tf.nn.dropout(c, keep_prob = 1 - Params.dropout)\n            # highway connection\n            # Our implementation is slightly different to the paper\n            # https://arxiv.org/abs/1709.02755 in a way that highway network\n            # uses x_hat instead of the cell inputs. Check equation (7) from the original\n            # paper for SRU.\n            h = r * c + (1 - r) * x\n        return h, c\n\nclass GRUCell(RNNCell):\n  """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).""""""\n\n  def __init__(self,\n               num_units,\n               activation=None,\n               reuse=None,\n               kernel_initializer=None,\n               bias_initializer=None,\n\t\t\t   is_training = True):\n    super(GRUCell, self).__init__(_reuse=reuse)\n    self._num_units = num_units\n    self._activation = activation or math_ops.tanh\n    self._kernel_initializer = kernel_initializer\n    self._bias_initializer = bias_initializer\n    self._is_training = is_training\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope = None):\n    """"""Gated recurrent unit (GRU) with nunits cells.""""""\n    if inputs.shape.as_list()[-1] != self._num_units:\n        with vs.variable_scope(""projection""):\n            res = linear(inputs, self._num_units, False, )\n    else:\n        res = inputs\n    with vs.variable_scope(""gates""):  # Reset gate and update gate.\n      # We start with bias of 1.0 to not reset and not update.\n      bias_ones = self._bias_initializer\n      if self._bias_initializer is None:\n        dtype = [a.dtype for a in [inputs, state]][0]\n        bias_ones = init_ops.constant_initializer(1.0, dtype=dtype)\n      value = math_ops.sigmoid(\n          linear([inputs, state], 2 * self._num_units, True, bias_ones,\n                  self._kernel_initializer))\n      r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n    with vs.variable_scope(""candidate""):\n      c = self._activation(\n          linear([inputs, r * state], self._num_units, True,\n                  self._bias_initializer, self._kernel_initializer))\n    #   recurrent dropout as proposed in https://arxiv.org/pdf/1603.05118.pdf (currently disabled)\n      #if self._is_training and Params.dropout is not None:\n        #c = tf.nn.dropout(c, 1 - Params.dropout)\n    new_h = u * state + (1 - u) * c\n    return new_h + res, new_h\n\nclass gated_attention_Wrapper(RNNCell):\n  def __init__(self,\n               num_units,\n               memory,\n               params,\n               self_matching = False,\n               memory_len = None,\n               reuse=None,\n               kernel_initializer=None,\n               bias_initializer=None,\n\t\t\t   is_training = True,\n               use_SRU = False):\n    super(gated_attention_Wrapper, self).__init__(_reuse=reuse)\n    cell = SRUCell if use_SRU else GRUCell\n    self._cell = cell(num_units, is_training = is_training)\n    self._num_units = num_units\n    self._activation = math_ops.tanh\n    self._kernel_initializer = kernel_initializer\n    self._bias_initializer = bias_initializer\n    self._attention = memory\n    self._params = params\n    self._self_matching = self_matching\n    self._memory_len = memory_len\n    self._is_training = is_training\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope = None):\n    """"""Gated recurrent unit (GRU) with nunits cells.""""""\n    with vs.variable_scope(""attention_pool""):\n        inputs = gated_attention(self._attention,\n                                inputs,\n                                state,\n                                self._num_units,\n                                params = self._params,\n                                self_matching = self._self_matching,\n                                memory_len = self._memory_len)\n    output, new_state = self._cell(inputs, state, scope)\n    return output, new_state\n\n\ndef linear(args,\n            output_size,\n            bias,\n            bias_initializer=None,\n            kernel_initializer=None):\n  """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_initializer: starting value to initialize the bias\n      (default is all zeros).\n    kernel_initializer: starting value to initialize the weight.\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  """"""\n  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(""`args` must be specified"")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape() for a in args]\n  for shape in shapes:\n    if shape.ndims != 2:\n      raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)\n    if shape[1].value is None:\n      raise ValueError(""linear expects shape[1] to be provided for shape %s, ""\n                       ""but saw %s"" % (shape, shape[1]))\n    else:\n      total_arg_size += shape[1].value\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  scope = vs.get_variable_scope()\n  with vs.variable_scope(scope) as outer_scope:\n    weights = vs.get_variable(\n        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n        dtype=dtype,\n        initializer=kernel_initializer)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], weights)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n    if not bias:\n      return res\n    with vs.variable_scope(outer_scope) as inner_scope:\n      inner_scope.set_partitioner(None)\n      if bias_initializer is None:\n        bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n      biases = vs.get_variable(\n          _BIAS_VARIABLE_NAME, [output_size],\n          dtype=dtype,\n          initializer=bias_initializer)\n    return nn_ops.bias_add(res, biases)\n'"
data_load.py,8,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python2\n\nfrom functools import wraps\nimport threading\n\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom params import Params\nimport numpy as np\nimport tensorflow as tf\nfrom process import *\nfrom sklearn.model_selection import train_test_split\n\n# Adapted from the `sugartensor` code.\n# https://github.com/buriburisuri/sugartensor/blob/master/sugartensor/sg_queue.py\ndef producer_func(func):\n    r""""""Decorates a function `func` as producer_func.\n    Args:\n      func: A function to decorate.\n    """"""\n    @wraps(func)\n    def wrapper(inputs, dtypes, capacity, num_threads):\n        r""""""\n        Args:\n            inputs: A inputs queue list to enqueue\n            dtypes: Data types of each tensor\n            capacity: Queue capacity. Default is 32.\n            num_threads: Number of threads. Default is 1.\n        """"""\n        # enqueue function\n        def enqueue_func(sess, op):\n            # read data from source queue\n            data = func(sess.run(inputs))\n            # create feeder dict\n            feed_dict = {}\n            for ph, col in zip(placeholders, data):\n                feed_dict[ph] = col\n            # run session\n            sess.run(op, feed_dict=feed_dict)\n\n        # create place holder list\n        placeholders = []\n        for dtype in dtypes:\n            placeholders.append(tf.placeholder(dtype=dtype))\n\n        # create FIFO queue\n        queue = tf.FIFOQueue(capacity, dtypes=dtypes)\n\n        # enqueue operation\n        enqueue_op = queue.enqueue(placeholders)\n\n        # create queue runner\n        runner = _FuncQueueRunner(enqueue_func, queue, [enqueue_op] * num_threads)\n\n        # register to global collection\n        tf.train.add_queue_runner(runner)\n\n        # return de-queue operation\n        return queue.dequeue()\n\n    return wrapper\n\n\nclass _FuncQueueRunner(tf.train.QueueRunner):\n\n    def __init__(self, func, queue=None, enqueue_ops=None, close_op=None,\n                 cancel_op=None, queue_closed_exception_types=None,\n                 queue_runner_def=None):\n        # save ad-hoc function\n        self.func = func\n        # call super()\n        super(_FuncQueueRunner, self).__init__(queue, enqueue_ops, close_op, cancel_op,\n                                               queue_closed_exception_types, queue_runner_def)\n\n    # pylint: disable=broad-except\n    def _run(self, sess, enqueue_op, coord=None):\n\n        if coord:\n            coord.register_thread(threading.current_thread())\n        decremented = False\n        try:\n            while True:\n                if coord and coord.should_stop():\n                    break\n                try:\n                    self.func(sess, enqueue_op)  # call enqueue function\n                except self._queue_closed_exception_types:  # pylint: disable=catching-non-exception\n                    # This exception indicates that a queue was closed.\n                    with self._lock:\n                        self._runs_per_session[sess] -= 1\n                        decremented = True\n                        if self._runs_per_session[sess] == 0:\n                            try:\n                                sess.run(self._close_op)\n                            except Exception as e:\n                                # Intentionally ignore errors from close_op.\n                                logging.vlog(1, ""Ignored exception: %s"", str(e))\n                        return\n        except Exception as e:\n            # This catches all other exceptions.\n            if coord:\n                coord.request_stop(e)\n            else:\n                logging.error(""Exception in QueueRunner: %s"", str(e))\n                with self._lock:\n                    self._exceptions_raised.append(e)\n                raise\n        finally:\n            # Make sure we account for all terminations: normal or errors.\n            if not decremented:\n                with self._lock:\n                    self._runs_per_session[sess] -= 1\n\ndef load_data(dir_):\n    # Target indices\n    indices = load_target(dir_ + Params.target_dir)\n\n    # Load question data\n    print(""Loading question data..."")\n    q_word_ids, _ = load_word(dir_ + Params.q_word_dir)\n    q_char_ids, q_char_len, q_word_len = load_char(dir_ + Params.q_chars_dir)\n\n    # Load passage data\n    print(""Loading passage data..."")\n    p_word_ids, _ = load_word(dir_ + Params.p_word_dir)\n    p_char_ids, p_char_len, p_word_len = load_char(dir_ + Params.p_chars_dir)\n\n    # Get max length to pad\n    p_max_word = Params.max_p_len#np.max(p_word_len)\n    p_max_char = Params.max_char_len#,max_value(p_char_len))\n    q_max_word = Params.max_q_len#,np.max(q_word_len)\n    q_max_char = Params.max_char_len#,max_value(q_char_len))\n\n    # pad_data\n    print(""Preparing data..."")\n    p_word_ids = pad_data(p_word_ids,p_max_word)\n    q_word_ids = pad_data(q_word_ids,q_max_word)\n    p_char_ids = pad_char_data(p_char_ids,p_max_char,p_max_word)\n    q_char_ids = pad_char_data(q_char_ids,q_max_char,q_max_word)\n\n    # to numpy\n    indices = np.reshape(np.asarray(indices,np.int32),(-1,2))\n    p_word_len = np.reshape(np.asarray(p_word_len,np.int32),(-1,1))\n    q_word_len = np.reshape(np.asarray(q_word_len,np.int32),(-1,1))\n    # p_char_len = pad_data(p_char_len,p_max_word)\n    # q_char_len = pad_data(q_char_len,q_max_word)\n    p_char_len = pad_char_len(p_char_len, p_max_word, p_max_char)\n    q_char_len = pad_char_len(q_char_len, q_max_word, q_max_char)\n\n    for i in range(p_word_len.shape[0]):\n        if p_word_len[i,0] > p_max_word:\n            p_word_len[i,0] = p_max_word\n    for i in range(q_word_len.shape[0]):\n        if q_word_len[i,0] > q_max_word:\n            q_word_len[i,0] = q_max_word\n\n    # shapes of each data\n    shapes=[(p_max_word,),(q_max_word,),\n            (p_max_word,p_max_char,),(q_max_word,q_max_char,),\n            (1,),(1,),\n            (p_max_word,),(q_max_word,),\n            (2,)]\n\n    return ([p_word_ids, q_word_ids,\n            p_char_ids, q_char_ids,\n            p_word_len, q_word_len,\n            p_char_len, q_char_len,\n            indices], shapes)\n\ndef get_dev():\n    devset, shapes = load_data(Params.dev_dir)\n    indices = devset[-1]\n    # devset = [np.reshape(input_, shapes[i]) for i,input_ in enumerate(devset)]\n\n    dev_ind = np.arange(indices.shape[0],dtype = np.int32)\n    np.random.shuffle(dev_ind)\n    return devset, dev_ind\n\ndef get_batch(is_training = True):\n    """"""Loads training data and put them in queues""""""\n    with tf.device(\'/cpu:0\'):\n        # Load dataset\n        input_list, shapes = load_data(Params.train_dir if is_training else Params.dev_dir)\n        indices = input_list[-1]\n\n        train_ind = np.arange(indices.shape[0],dtype = np.int32)\n        np.random.shuffle(train_ind)\n\n        size = Params.data_size\n        if Params.data_size > indices.shape[0] or Params.data_size == -1:\n            size = indices.shape[0]\n        ind_list = tf.convert_to_tensor(train_ind[:size])\n\n        # Create Queues\n        ind_list = tf.train.slice_input_producer([ind_list], shuffle=True)\n\n        @producer_func\n        def get_data(ind):\n            \'\'\'From `_inputs`, which has been fetched from slice queues,\n               then enqueue them again.\n            \'\'\'\n            return [np.reshape(input_[ind], shapes[i]) for i,input_ in enumerate(input_list)]\n\n        data = get_data(inputs=ind_list,\n                        dtypes=[np.int32]*9,\n                        capacity=Params.batch_size*32,\n                        num_threads=6)\n\n        # create batch queues\n        batch = tf.train.batch(data,\n                                shapes=shapes,\n                                num_threads=2,\n                                batch_size=Params.batch_size,\n                                capacity=Params.batch_size*32,\n                                dynamic_pad=True)\n\n    return batch, size // Params.batch_size\n'"
demo.py,2,"b'#!/usr/bin/env python\n# coding=utf-8\n\nimport tensorflow as tf\nimport bottle\nfrom bottle import route, run\nimport threading\n\nfrom params import Params\nfrom process import *\nfrom time import sleep\n\napp = bottle.Bottle()\nquery = []\nresponse = """"\n\n@app.get(""/"")\ndef home():\n    with open(\'demo.html\', \'r\') as fl:\n        html = fl.read()\n        return html\n\n@app.post(\'/answer\')\ndef answer():\n    passage = bottle.request.json[\'passage\']\n    question = bottle.request.json[\'question\']\n    # if not passage or not question:\n    #     exit()\n    global query, response\n    query = (passage, question)\n    while not response:\n        sleep(0.1)\n    print(""received response: {}"".format(response))\n    Final_response = {""answer"": response}\n    response = []\n    return Final_response\n\nclass Demo(object):\n    def __init__(self, model):\n        run_event = threading.Event()\n        run_event.set()\n        threading.Thread(target=self.demo_backend, args = [model, run_event]).start()\n        app.run(port=8080, host=\'0.0.0.0\')\n        try:\n            while 1:\n                sleep(.1)\n        except KeyboardInterrupt:\n            print ""Closing server...""\n            run_event.clear()\n\n    def demo_backend(self, model, run_event):\n        global query, response\n        dict_ = pickle.load(open(Params.data_dir + ""dictionary.pkl"",""r""))\n\n        with model.graph.as_default():\n            sv = tf.train.Supervisor()\n            with sv.managed_session() as sess:\n                sv.saver.restore(sess, tf.train.latest_checkpoint(Params.logdir))\n                while run_event.is_set():\n                    sleep(0.1)\n                    if query:\n                        data, shapes = dict_.realtime_process(query)\n                        fd = {m:d for i,(m,d) in enumerate(zip(model.data, data))}\n                        ids = sess.run([model.output_index], feed_dict = fd)\n                        ids = ids[0][0]\n                        if ids[0] == ids[1]:\n                            ids[1] += 1\n                        passage_t = tokenize_corenlp(query[0])\n                        response = "" "".join(passage_t[ids[0]:ids[1]])\n                        query = []\n'"
evaluate.py,0,"b'"""""" Official evaluation script for v1.1 of the SQuAD dataset. """"""\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\ndef f1_and_EM(index, ground_truth, passage, dict_):\n    if index[0] == index[1]:\n        pred_ind = [passage[index[0]].tolist()]\n    else:\n        pred_ind = passage[index[0]:index[1]].tolist()\n        if pred_ind is None:\n            pred_ind = [0]\n    if ground_truth[0] == ground_truth[1]:\n        answer_ind = [passage[ground_truth[0]].tolist()]\n    else:\n        answer_ind = passage[ground_truth[0]:ground_truth[1]].tolist()\n    answer = dict_.ind2word(pred_ind)\n    answer_ = dict_.ind2word(answer_ind)\n    f1 = f1_score(answer,answer_)\n    EM = exact_match_score(answer,answer_)\n    return f1, EM\n\ndef normalize_answer(s):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef evaluate(dataset, predictions):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        for paragraph in article[\'paragraphs\']:\n            for qa in paragraph[\'qas\']:\n                total += 1\n                if qa[\'id\'] not in predictions:\n                    message = \'Unanswered question \' + qa[\'id\'] + \\\n                              \' will receive score 0.\'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x[\'text\'], qa[\'answers\']))\n                prediction = predictions[qa[\'id\']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {\'exact_match\': exact_match, \'f1\': f1}\n\n\nif __name__ == \'__main__\':\n    expected_version = \'1.1\'\n    parser = argparse.ArgumentParser(\n        description=\'Evaluation for SQuAD \' + expected_version)\n    parser.add_argument(\'dataset_file\', help=\'Dataset file\')\n    parser.add_argument(\'prediction_file\', help=\'Prediction File\')\n    args = parser.parse_args()\n    with open(args.dataset_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if (dataset_json[\'version\'] != expected_version):\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    with open(args.prediction_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    print(json.dumps(evaluate(dataset, predictions)))\n'"
layers.py,61,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python2\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.contrib.rnn import MultiRNNCell\nfrom tensorflow.contrib.rnn import RNNCell\nfrom params import Params\nfrom zoneout import ZoneoutWrapper\n\'\'\'\nattention weights from https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\nW_u^Q.shape:    (2 * attn_size, attn_size)\nW_u^P.shape:    (2 * attn_size, attn_size)\nW_v^P.shape:    (attn_size, attn_size)\nW_g.shape:      (4 * attn_size, 4 * attn_size)\nW_h^P.shape:    (2 * attn_size, attn_size)\nW_v^Phat.shape: (2 * attn_size, attn_size)\nW_h^a.shape:    (2 * attn_size, attn_size)\nW_v^Q.shape:    (attn_size, attn_size)\n\'\'\'\n\ndef get_attn_params(attn_size,initializer = tf.truncated_normal_initializer):\n    \'\'\'\n    Args:\n        attn_size: the size of attention specified in https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\n        initializer: the author of the original paper used gaussian initialization however I found xavier converge faster\n\n    Returns:\n        params: A collection of parameters used throughout the layers\n    \'\'\'\n    with tf.variable_scope(""attention_weights""):\n        params = {""W_u_Q"":tf.get_variable(""W_u_Q"",dtype = tf.float32, shape = (2 * attn_size, attn_size), initializer = initializer()),\n                #""W_ru_Q"":tf.get_variable(""W_ru_Q"",dtype = tf.float32, shape = (2 * attn_size, 2 * attn_size), initializer = initializer()),\n                ""W_u_P"":tf.get_variable(""W_u_P"",dtype = tf.float32, shape = (2 * attn_size, attn_size), initializer = initializer()),\n                ""W_v_P"":tf.get_variable(""W_v_P"",dtype = tf.float32, shape = (attn_size, attn_size), initializer = initializer()),\n                ""W_v_P_2"":tf.get_variable(""W_v_P_2"",dtype = tf.float32, shape = (2 * attn_size, attn_size), initializer = initializer()),\n                ""W_g"":tf.get_variable(""W_g"",dtype = tf.float32, shape = (4 * attn_size, 4 * attn_size), initializer = initializer()),\n                ""W_h_P"":tf.get_variable(""W_h_P"",dtype = tf.float32, shape = (2 * attn_size, attn_size), initializer = initializer()),\n                ""W_v_Phat"":tf.get_variable(""W_v_Phat"",dtype = tf.float32, shape = (2 * attn_size, attn_size), initializer = initializer()),\n                ""W_h_a"":tf.get_variable(""W_h_a"",dtype = tf.float32, shape = (2 * attn_size, attn_size), initializer = initializer()),\n                ""W_v_Q"":tf.get_variable(""W_v_Q"",dtype = tf.float32, shape = (attn_size,  attn_size), initializer = initializer()),\n                ""v"":tf.get_variable(""v"",dtype = tf.float32, shape = (attn_size), initializer =initializer())}\n        return params\n\ndef encoding(word, char, word_embeddings, char_embeddings, scope = ""embedding""):\n    with tf.variable_scope(scope):\n        word_encoding = tf.nn.embedding_lookup(word_embeddings, word)\n        char_encoding = tf.nn.embedding_lookup(char_embeddings, char)\n        return word_encoding, char_encoding\n\ndef apply_dropout(inputs, size = None, is_training = True):\n    \'\'\'\n    Implementation of Zoneout from https://arxiv.org/pdf/1606.01305.pdf\n    \'\'\'\n    if Params.dropout is None and Params.zoneout is None:\n        return inputs\n    if Params.zoneout is not None:\n        return ZoneoutWrapper(inputs, state_zoneout_prob= Params.zoneout, is_training = is_training)\n    elif is_training:\n        return tf.contrib.rnn.DropoutWrapper(inputs,\n                                            output_keep_prob = 1 - Params.dropout,\n                                            # variational_recurrent = True,\n                                            # input_size = size,\n                                            dtype = tf.float32)\n    else:\n        return inputs\n\ndef bidirectional_GRU(inputs, inputs_len, cell = None, cell_fn = tf.contrib.rnn.GRUCell, units = Params.attn_size, layers = 1, scope = ""Bidirectional_GRU"", output = 0, is_training = True, reuse = None):\n    \'\'\'\n    Bidirectional recurrent neural network with GRU cells.\n\n    Args:\n        inputs:     rnn input of shape (batch_size, timestep, dim)\n        inputs_len: rnn input_len of shape (batch_size, )\n        cell:       rnn cell of type RNN_Cell.\n        output:     if 0, output returns rnn output for every timestep,\n                    if 1, output returns concatenated state of backward and\n                    forward rnn.\n    \'\'\'\n    with tf.variable_scope(scope, reuse = reuse):\n        if cell is not None:\n            (cell_fw, cell_bw) = cell\n        else:\n            shapes = inputs.get_shape().as_list()\n            if len(shapes) > 3:\n                inputs = tf.reshape(inputs,(shapes[0]*shapes[1],shapes[2],-1))\n                inputs_len = tf.reshape(inputs_len,(shapes[0]*shapes[1],))\n\n            # if no cells are provided, use standard GRU cell implementation\n            if layers > 1:\n                cell_fw = MultiRNNCell([apply_dropout(cell_fn(units), size = inputs.shape[-1] if i == 0 else units, is_training = is_training) for i in range(layers)])\n                cell_bw = MultiRNNCell([apply_dropout(cell_fn(units), size = inputs.shape[-1] if i == 0 else units, is_training = is_training) for i in range(layers)])\n            else:\n                cell_fw, cell_bw = [apply_dropout(cell_fn(units), size = inputs.shape[-1], is_training = is_training) for _ in range(2)]\n\t\t\t\t\n        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs,\n                                                        sequence_length = inputs_len,\n                                                        dtype=tf.float32)\n        if output == 0:\n            return tf.concat(outputs, 2)\n        elif output == 1:\n            return tf.reshape(tf.concat(states,1),(Params.batch_size, shapes[1], 2*units))\n\ndef pointer_net(passage, passage_len, question, question_len, cell, params, scope = ""pointer_network""):\n    \'\'\'\n    Answer pointer network as proposed in https://arxiv.org/pdf/1506.03134.pdf.\n\n    Args:\n        passage:        RNN passage output from the bidirectional readout layer (batch_size, timestep, dim)\n        passage_len:    variable lengths for passage length\n        question:       RNN question output of shape (batch_size, timestep, dim) for question pooling\n        question_len:   Variable lengths for question length\n        cell:           rnn cell of type RNN_Cell.\n        params:         Appropriate weight matrices for attention pooling computation\n\n    Returns:\n        softmax logits for the answer pointer of the beginning and the end of the answer span\n    \'\'\'\n    with tf.variable_scope(scope):\n        weights_q, weights_p = params\n        shapes = passage.get_shape().as_list()\n        initial_state = question_pooling(question, units = Params.attn_size, weights = weights_q, memory_len = question_len, scope = ""question_pooling"")\n        inputs = [passage, initial_state]\n        p1_logits = attention(inputs, Params.attn_size, weights_p, memory_len = passage_len, scope = ""attention"")\n        scores = tf.expand_dims(p1_logits, -1)\n        attention_pool = tf.reduce_sum(scores * passage,1)\n        _, state = cell(attention_pool, initial_state)\n        inputs = [passage, state]\n        p2_logits = attention(inputs, Params.attn_size, weights_p, memory_len = passage_len, scope = ""attention"", reuse = True)\n        return tf.stack((p1_logits,p2_logits),1)\n\ndef attention_rnn(inputs, inputs_len, units, attn_cell, bidirection = True, scope = ""gated_attention_rnn"", is_training = True):\n    with tf.variable_scope(scope):\n        if bidirection:\n            outputs = bidirectional_GRU(inputs,\n                                        inputs_len,\n                                        cell = attn_cell,\n                                        scope = scope + ""_bidirectional"",\n                                        output = 0,\n                                        is_training = is_training)\n        else:\n            outputs, _ = tf.nn.dynamic_rnn(attn_cell, inputs,\n                                            sequence_length = inputs_len,\n                                            dtype=tf.float32)\n        return outputs\n\ndef question_pooling(memory, units, weights, memory_len = None, scope = ""question_pooling""):\n    with tf.variable_scope(scope):\n        shapes = memory.get_shape().as_list()\n        V_r = tf.get_variable(""question_param"", shape = (Params.max_q_len, units), initializer = tf.contrib.layers.xavier_initializer(), dtype = tf.float32)\n        inputs_ = [memory, V_r]\n        attn = attention(inputs_, units, weights, memory_len = memory_len, scope = ""question_attention_pooling"")\n        attn = tf.expand_dims(attn, -1)\n        return tf.reduce_sum(attn * memory, 1)\n\ndef gated_attention(memory, inputs, states, units, params, self_matching = False, memory_len = None, scope=""gated_attention""):\n    with tf.variable_scope(scope):\n        weights, W_g = params\n        inputs_ = [memory, inputs]\n        states = tf.reshape(states,(Params.batch_size,Params.attn_size))\n        if not self_matching:\n            inputs_.append(states)\n        scores = attention(inputs_, units, weights, memory_len = memory_len)\n        scores = tf.expand_dims(scores,-1)\n        attention_pool = tf.reduce_sum(scores * memory, 1)\n        inputs = tf.concat((inputs,attention_pool),axis = 1)\n        g_t = tf.sigmoid(tf.matmul(inputs,W_g))\n        return g_t * inputs\n\ndef mask_attn_score(score, memory_sequence_length, score_mask_value = -1e8):\n    score_mask = tf.sequence_mask(\n        memory_sequence_length, maxlen=score.shape[1])\n    score_mask_values = score_mask_value * tf.ones_like(score)\n    return tf.where(score_mask, score, score_mask_values)\n\ndef attention(inputs, units, weights, scope = ""attention"", memory_len = None, reuse = None):\n    with tf.variable_scope(scope, reuse = reuse):\n        outputs_ = []\n        weights, v = weights\n        for i, (inp,w) in enumerate(zip(inputs,weights)):\n            shapes = inp.shape.as_list()\n            inp = tf.reshape(inp, (-1, shapes[-1]))\n            if w is None:\n                w = tf.get_variable(""w_%d""%i, dtype = tf.float32, shape = [shapes[-1],Params.attn_size], initializer = tf.contrib.layers.xavier_initializer())\n            outputs = tf.matmul(inp, w)\n            # Hardcoded attention output reshaping. Equation (4), (8), (9) and (11) in the original paper.\n            if len(shapes) > 2:\n                outputs = tf.reshape(outputs, (shapes[0], shapes[1], -1))\n            elif len(shapes) == 2 and shapes[0] is Params.batch_size:\n                outputs = tf.reshape(outputs, (shapes[0],1,-1))\n            else:\n                outputs = tf.reshape(outputs, (1, shapes[0],-1))\n            outputs_.append(outputs)\n        outputs = sum(outputs_)\n        if Params.bias:\n            b = tf.get_variable(""b"", shape = outputs.shape[-1], dtype = tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n            outputs += b\n        scores = tf.reduce_sum(tf.tanh(outputs) * v, [-1])\n        if memory_len is not None:\n            scores = mask_attn_score(scores, memory_len)\n        return tf.nn.softmax(scores) # all attention output is softmaxed now\n\ndef cross_entropy(output, target):\n    cross_entropy = target * tf.log(output + 1e-8)\n    cross_entropy = -tf.reduce_sum(cross_entropy, 2) # sum across passage timestep\n    cross_entropy = tf.reduce_mean(cross_entropy, 1) # average across pointer networks output\n    return tf.reduce_mean(cross_entropy) # average across batch size\n\ndef total_params():\n    total_parameters = 0\n    for variable in tf.trainable_variables():\n        shape = variable.get_shape()\n        variable_parametes = 1\n        for dim in shape:\n            variable_parametes *= dim.value\n        total_parameters += variable_parametes\n    print(""Total number of trainable parameters: {}"".format(total_parameters))\n'"
model.py,54,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python2\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom data_load import get_batch, get_dev\nfrom params import Params\nfrom layers import *\nfrom GRU import gated_attention_Wrapper, GRUCell, SRUCell\nfrom evaluate import *\nimport numpy as np\nimport cPickle as pickle\nfrom process import *\nfrom demo import Demo\n\noptimizer_factory = {""adadelta"":tf.train.AdadeltaOptimizer,\n            ""adam"":tf.train.AdamOptimizer,\n            ""gradientdescent"":tf.train.GradientDescentOptimizer,\n            ""adagrad"":tf.train.AdagradOptimizer}\n\nclass Model(object):\n    def __init__(self,is_training = True, demo = False):\n        # Build the computational graph when initializing\n        self.is_training = is_training\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            self.global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n            if demo:\n                self.passage_w = tf.placeholder(tf.int32,\n                                        [1, Params.max_p_len,],""passage_w"")\n                self.question_w = tf.placeholder(tf.int32,\n                                        [1, Params.max_q_len,],""passage_q"")\n                self.passage_c = tf.placeholder(tf.int32,\n                                        [1, Params.max_p_len,Params.max_char_len],""passage_pc"")\n                self.question_c = tf.placeholder(tf.int32,\n                                        [1, Params.max_q_len,Params.max_char_len],""passage_qc"")\n                self.passage_w_len_ = tf.placeholder(tf.int32,\n                                        [1,1],""passage_w_len_"")\n                self.question_w_len_ = tf.placeholder(tf.int32,\n                                        [1,1],""question_w_len_"")\n                self.passage_c_len = tf.placeholder(tf.int32,\n                                        [1, Params.max_p_len],""passage_c_len"")\n                self.question_c_len = tf.placeholder(tf.int32,\n                                        [1, Params.max_q_len],""question_c_len"")\n                self.data = (self.passage_w,\n                            self.question_w,\n                            self.passage_c,\n                            self.question_c,\n                            self.passage_w_len_,\n                            self.question_w_len_,\n                            self.passage_c_len,\n                            self.question_c_len)\n            else:\n                self.data, self.num_batch = get_batch(is_training = is_training)\n                (self.passage_w,\n                self.question_w,\n                self.passage_c,\n                self.question_c,\n                self.passage_w_len_,\n                self.question_w_len_,\n                self.passage_c_len,\n                self.question_c_len,\n                self.indices) = self.data\n                \n            self.passage_w_len = tf.squeeze(self.passage_w_len_, -1)\n            self.question_w_len = tf.squeeze(self.question_w_len_, -1)\n\n            self.encode_ids()\n            self.params = get_attn_params(Params.attn_size, initializer = tf.contrib.layers.xavier_initializer)\n            self.attention_match_rnn()\n            self.bidirectional_readout()\n            self.pointer_network()\n            self.outputs()\n\n            if is_training:\n                self.loss_function()\n                self.summary()\n                self.init_op = tf.global_variables_initializer()\n            total_params()\n\n    def encode_ids(self):\n        with tf.device(\'/cpu:0\'):\n            self.char_embeddings = tf.Variable(tf.constant(0.0, shape=[Params.char_vocab_size, Params.char_emb_size]),trainable=True, name=""char_embeddings"")\n            self.word_embeddings = tf.Variable(tf.constant(0.0, shape=[Params.vocab_size, Params.emb_size]),trainable=False, name=""word_embeddings"")\n            self.word_embeddings_placeholder = tf.placeholder(tf.float32,[Params.vocab_size, Params.emb_size],""word_embeddings_placeholder"")\n            self.emb_assign = tf.assign(self.word_embeddings, self.word_embeddings_placeholder)\n\n        # Embed the question and passage information for word and character tokens\n        self.passage_word_encoded, self.passage_char_encoded = encoding(self.passage_w,\n                                        self.passage_c,\n                                        word_embeddings = self.word_embeddings,\n                                        char_embeddings = self.char_embeddings,\n                                        scope = ""passage_embeddings"")\n        self.question_word_encoded, self.question_char_encoded = encoding(self.question_w,\n                                        self.question_c,\n                                        word_embeddings = self.word_embeddings,\n                                        char_embeddings = self.char_embeddings,\n                                        scope = ""question_embeddings"")\n\n        self.passage_char_encoded = bidirectional_GRU(self.passage_char_encoded,\n                                self.passage_c_len,\n                                cell_fn = SRUCell if Params.SRU else GRUCell,\n                                scope = ""passage_char_encoding"",\n                                output = 1,\n                                is_training = self.is_training)\n        self.question_char_encoded = bidirectional_GRU(self.question_char_encoded,\n                                self.question_c_len,\n                                cell_fn = SRUCell if Params.SRU else GRUCell,\n                                scope = ""question_char_encoding"",\n                                output = 1,\n                                is_training = self.is_training)\n        self.passage_encoding = tf.concat((self.passage_word_encoded, self.passage_char_encoded),axis = 2)\n        self.question_encoding = tf.concat((self.question_word_encoded, self.question_char_encoded),axis = 2)\n\n        # Passage and question encoding\n        #cell = [MultiRNNCell([GRUCell(Params.attn_size, is_training = self.is_training) for _ in range(3)]) for _ in range(2)]\n        self.passage_encoding = bidirectional_GRU(self.passage_encoding,\n                                self.passage_w_len,\n                                cell_fn = SRUCell if Params.SRU else GRUCell,\n                                layers = Params.num_layers,\n                                scope = ""passage_encoding"",\n                                output = 0,\n                                is_training = self.is_training)\n        #cell = [MultiRNNCell([GRUCell(Params.attn_size, is_training = self.is_training) for _ in range(3)]) for _ in range(2)]\n        self.question_encoding = bidirectional_GRU(self.question_encoding,\n                                self.question_w_len,\n                                cell_fn = SRUCell if Params.SRU else GRUCell,\n                                layers = Params.num_layers,\n                                scope = ""question_encoding"",\n                                output = 0,\n                                is_training = self.is_training)\n\n    def attention_match_rnn(self):\n        # Apply gated attention recurrent network for both query-passage matching and self matching networks\n        with tf.variable_scope(""attention_match_rnn""):\n            memory = self.question_encoding\n            inputs = self.passage_encoding\n            scopes = [""question_passage_matching"", ""self_matching""]\n            params = [(([self.params[""W_u_Q""],\n                    self.params[""W_u_P""],\n                    self.params[""W_v_P""]],self.params[""v""]),\n                    self.params[""W_g""]),\n                (([self.params[""W_v_P_2""],\n                    self.params[""W_v_Phat""]],self.params[""v""]),\n                    self.params[""W_g""])]\n            for i in range(2):\n                args = {""num_units"": Params.attn_size,\n                        ""memory"": memory,\n                        ""params"": params[i],\n                        ""self_matching"": False if i == 0 else True,\n                        ""memory_len"": self.question_w_len if i == 0 else self.passage_w_len,\n                        ""is_training"": self.is_training,\n                        ""use_SRU"": Params.SRU}\n                cell = [apply_dropout(gated_attention_Wrapper(**args), size = inputs.shape[-1], is_training = self.is_training) for _ in range(2)]\n                inputs = attention_rnn(inputs,\n                            self.passage_w_len,\n                            Params.attn_size,\n                            cell,\n                            scope = scopes[i])\n                memory = inputs # self matching (attention over itself)\n            self.self_matching_output = inputs\n\n    def bidirectional_readout(self):\n        self.final_bidirectional_outputs = bidirectional_GRU(self.self_matching_output,\n                                    self.passage_w_len,\n                                    cell_fn = SRUCell if Params.SRU else GRUCell,\n                                    # layers = Params.num_layers, # or 1? not specified in the original paper\n                                    scope = ""bidirectional_readout"",\n                                    output = 0,\n                                    is_training = self.is_training)\n\n    def pointer_network(self):\n        params = (([self.params[""W_u_Q""],self.params[""W_v_Q""]],self.params[""v""]),\n                ([self.params[""W_h_P""],self.params[""W_h_a""]],self.params[""v""]))\n        cell = apply_dropout(GRUCell(Params.attn_size*2), size = self.final_bidirectional_outputs.shape[-1], is_training = self.is_training)\n        self.points_logits = pointer_net(self.final_bidirectional_outputs, self.passage_w_len, self.question_encoding, self.question_w_len, cell, params, scope = ""pointer_network"")\n\n    def outputs(self):\n        self.logit_1, self.logit_2 = tf.split(self.points_logits, 2, axis = 1)\n        self.logit_1 = tf.transpose(self.logit_1, [0, 2, 1])\n        self.dp = tf.matmul(self.logit_1, self.logit_2)\n        self.dp = tf.matrix_band_part(self.dp, 0, 15)\n        self.output_index_1 = tf.argmax(tf.reduce_max(self.dp, axis = 2), -1)\n        self.output_index_2 = tf.argmax(tf.reduce_max(self.dp, axis = 1), -1)\n        self.output_index = tf.stack([self.output_index_1, self.output_index_2], axis = 1)\n        # self.output_index = tf.argmax(self.points_logits, axis = 2)\n\n    def loss_function(self):\n        with tf.variable_scope(""loss""):\n            shapes = self.passage_w.shape\n            self.indices_prob = tf.one_hot(self.indices, shapes[1])\n            self.mean_loss = cross_entropy(self.points_logits, self.indices_prob)\n            self.optimizer = optimizer_factory[Params.optimizer](**Params.opt_arg[Params.optimizer])\n\n            if Params.clip:\n                # gradient clipping by norm\n                gradients, variables = zip(*self.optimizer.compute_gradients(self.mean_loss))\n                gradients, _ = tf.clip_by_global_norm(gradients, Params.norm)\n                self.train_op = self.optimizer.apply_gradients(zip(gradients, variables), global_step = self.global_step)\n            else:\n                self.train_op = self.optimizer.minimize(self.mean_loss, global_step = self.global_step)\n\n    def summary(self):\n        self.F1 = tf.Variable(tf.constant(0.0, shape=(), dtype = tf.float32),trainable=False, name=""F1"")\n        self.F1_placeholder = tf.placeholder(tf.float32, shape = (), name = ""F1_placeholder"")\n        self.EM = tf.Variable(tf.constant(0.0, shape=(), dtype = tf.float32),trainable=False, name=""EM"")\n        self.EM_placeholder = tf.placeholder(tf.float32, shape = (), name = ""EM_placeholder"")\n        self.dev_loss = tf.Variable(tf.constant(5.0, shape=(), dtype = tf.float32),trainable=False, name=""dev_loss"")\n        self.dev_loss_placeholder = tf.placeholder(tf.float32, shape = (), name = ""dev_loss"")\n        self.metric_assign = tf.group(tf.assign(self.F1, self.F1_placeholder),tf.assign(self.EM, self.EM_placeholder),tf.assign(self.dev_loss, self.dev_loss_placeholder))\n        tf.summary.scalar(\'loss_training\', self.mean_loss)\n        tf.summary.scalar(\'loss_dev\', self.dev_loss)\n        tf.summary.scalar(""F1_Score"",self.F1)\n        tf.summary.scalar(""Exact_Match"",self.EM)\n        tf.summary.scalar(\'learning_rate\', Params.opt_arg[Params.optimizer][\'learning_rate\'])\n        self.merged = tf.summary.merge_all()\n\ndef debug():\n    model = Model(is_training = False)\n    print(""Built model"")\n\ndef test():\n    model = Model(is_training = False); print(""Built model"")\n    dict_ = pickle.load(open(Params.data_dir + ""dictionary.pkl"",""r""))\n    with model.graph.as_default():\n        sv = tf.train.Supervisor()\n        with sv.managed_session() as sess:\n            sv.saver.restore(sess, tf.train.latest_checkpoint(Params.logdir))\n            EM, F1 = 0.0, 0.0\n            for step in tqdm(range(model.num_batch), total = model.num_batch, ncols=70, leave=False, unit=\'b\'):\n                index, ground_truth, passage = sess.run([model.output_index, model.indices, model.passage_w])\n                for batch in range(Params.batch_size):\n                    f1, em = f1_and_EM(index[batch], ground_truth[batch], passage[batch], dict_)\n                    F1 += f1\n                    EM += em\n            F1 /= float(model.num_batch * Params.batch_size)\n            EM /= float(model.num_batch * Params.batch_size)\n            print(""Exact_match: {}\\nF1_score: {}"".format(EM,F1))\n\ndef main():\n    model = Model(is_training = True); print(""Built model"")\n    dict_ = pickle.load(open(Params.data_dir + ""dictionary.pkl"",""r""))\n    init = False\n    devdata, dev_ind = get_dev()\n    if not os.path.isfile(os.path.join(Params.logdir,""checkpoint"")):\n        init = True\n        glove = np.memmap(Params.data_dir + ""glove.np"", dtype = np.float32, mode = ""r"")\n        glove = np.reshape(glove,(Params.vocab_size,Params.emb_size))\n    with model.graph.as_default():\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        sv = tf.train.Supervisor(logdir=Params.logdir,\n                        save_model_secs=0,\n                        global_step = model.global_step,\n                        init_op = model.init_op)\n        with sv.managed_session(config = config) as sess:\n            if init: sess.run(model.emb_assign, {model.word_embeddings_placeholder:glove})\n            for epoch in range(1, Params.num_epochs+1):\n                if sv.should_stop(): break\n                for step in tqdm(range(model.num_batch), total = model.num_batch, ncols=70, leave=False, unit=\'b\'):\n                    sess.run(model.train_op)\n                    if step % Params.save_steps == 0:\n                        gs = sess.run(model.global_step)\n                        sv.saver.save(sess, Params.logdir + \'/model_epoch_%d_step_%d\'%(gs//model.num_batch, gs%model.num_batch))\n                        sample = np.random.choice(dev_ind, Params.batch_size)\n                        feed_dict = {data: devdata[i][sample] for i,data in enumerate(model.data)}\n                        index, dev_loss = sess.run([model.output_index, model.mean_loss], feed_dict = feed_dict)\n                        F1, EM = 0.0, 0.0\n                        for batch in range(Params.batch_size):\n                            f1, em = f1_and_EM(index[batch], devdata[8][sample][batch], devdata[0][sample][batch], dict_)\n                            F1 += f1\n                            EM += em\n                        F1 /= float(Params.batch_size)\n                        EM /= float(Params.batch_size)\n                        sess.run(model.metric_assign,{model.F1_placeholder: F1, model.EM_placeholder: EM, model.dev_loss_placeholder: dev_loss})\n                        print(""\\nDev_loss: {}\\nDev_Exact_match: {}\\nDev_F1_score: {}"".format(dev_loss,EM,F1))\n\nif __name__ == \'__main__\':\n    if Params.mode.lower() == ""debug"":\n        print(""Debugging..."")\n        debug()\n    elif Params.mode.lower() == ""test"":\n        print(""Testing on dev set..."")\n        test()\n    elif Params.mode.lower() == ""demo"":\n        print(""Run the local host for online demo..."")\n        model = Model(is_training = False, demo = True); print(""Built model"")\n        demo_run = Demo(model)\n    elif Params.mode.lower() == ""train"":\n        print(""Training..."")\n        main()\n    else:\n        print(""Invalid mode."")\n'"
params.py,0,"b'class Params():\n\n    # data\n    data_size = -1 # -1 to use all data\n    num_epochs = 10\n    train_prop = 0.9 # Not implemented atm\n    data_dir = ""./data/""\n    train_dir = data_dir + ""trainset/""\n    dev_dir = data_dir + ""devset/""\n    logdir = ""./train/train""\n    glove_dir = ""./glove.840B.300d.txt"" # Glove file name (If you want to use your own glove, replace the file name here)\n    glove_char = ""./glove.840B.300d.char.txt"" # Character Glove file name\n    coreNLP_dir = ""./stanford-corenlp-full-2017-06-09"" # Directory to Stanford coreNLP tool\n\n    # Data dir\n    target_dir = ""indices.txt""\n    q_word_dir = ""words_questions.txt""\n    q_chars_dir = ""chars_questions.txt""\n    p_word_dir = ""words_context.txt""\n    p_chars_dir = ""chars_context.txt""\n\n    # Training\n\t# NOTE: To use demo, put batch_size == 1\n    mode = ""train"" # case-insensitive options: [""train"", ""test"", ""debug""]\n    dropout = 0.2 # dropout probability, if None, don\'t use dropout\n    zoneout = None # zoneout probability, if None, don\'t use zoneout\n    optimizer = ""adam"" # Options: [""adadelta"", ""adam"", ""gradientdescent"", ""adagrad""]\n    batch_size = 50 if mode is not ""test"" else 100# Size of the mini-batch for training\n    save_steps = 50 # Save the model at every 50 steps\n    clip = True # clip gradient norm\n    norm = 5.0 # global norm\n    # NOTE: Change the hyperparameters of your learning algorithm here\n    opt_arg = {\'adadelta\':{\'learning_rate\':1, \'rho\': 0.95, \'epsilon\':1e-6},\n                \'adam\':{\'learning_rate\':1e-3, \'beta1\':0.9, \'beta2\':0.999, \'epsilon\':1e-8},\n                \'gradientdescent\':{\'learning_rate\':1},\n                \'adagrad\':{\'learning_rate\':1}}\n\n    # Architecture\n    SRU = True # Use SRU cell, if False, use standard GRU cell\n    max_p_len = 300 # Maximum number of words in each passage context\n    max_q_len = 30 # Maximum number of words in each question context\n    max_char_len = 16 # Maximum number of characters in a word\n    vocab_size = 91605 # Number of vocabs in glove.840B.300d.txt + 1 for an unknown token\n    char_vocab_size = 95 # Number of characters in glove.840B.300d.char.txt + 1 for an unknown character\n    emb_size = 300 # Embeddings size for words\n    char_emb_size = 8 # Embeddings size for characters\n    attn_size = 75 # RNN cell and attention module size\n    num_layers = 3 # Number of layers at question-passage matching\n    bias = True # Use bias term in attention\n'"
process.py,0,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python2\n\nimport cPickle as pickle\nimport numpy as np\nimport json\nimport codecs\nimport unicodedata\nimport re\nimport sys\nimport os\nimport argparse\n\nfrom tqdm import tqdm\nfrom params import Params\n\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-p\',\'--process\', default = False, type = str2bool, help=\'Use the coreNLP tokenizer.\', required=False)\nparser.add_argument(\'-r\',\'--reduce_glove\', default = False, type = str2bool, help=\'Reduce glove size.\', required=False)\nargs = parser.parse_args()\n\nimport spacy\nnlp = spacy.blank(\'en\')\n\ndef tokenize_corenlp(text):\n    parsed = nlp(text)\n    tokens = [i.text for i in parsed]\n    return tokens\n\nclass data_loader(object):\n    def __init__(self,use_pretrained = None):\n        self.c_dict = {""_UNK"":0}\n        self.w_dict = {""_UNK"":0}\n        self.w_occurence = 0\n        self.c_occurence = 0\n        self.w_count = 1\n        self.c_count = 1\n        self.w_unknown_count = 0\n        self.c_unknown_count = 0\n        self.append_dict = True\n        self.invalid_q = 0\n\n        if use_pretrained:\n            self.append_dict = False\n            self.w_dict, self.w_count = self.process_glove(Params.glove_dir, self.w_dict, self.w_count, Params.emb_size)\n            self.c_dict, self.c_count = self.process_glove(Params.glove_char, self.c_dict, self.c_count, 300)\n            self.ids2word = {v: k for k, v in self.w_dict.iteritems()}\n            self.ids2char = {v: k for k, v in self.c_dict.iteritems()}\n\n    def ind2word(self,ids):\n        output = []\n        for i in ids:\n            output.append(str(self.ids2word[i]))\n        return "" "".join(output)\n\n    def ind2char(self,ids):\n        output = []\n        for i in ids:\n            for j in i:\n                output.append(str(self.ids2char[j]))\n            output.append("" "")\n        return """".join(output)\n\n    def process_glove(self, wordvecs, dict_, count, emb_size):\n        print(""Reading GloVe from: {}"".format(wordvecs))\n        with codecs.open(wordvecs,""rb"",""utf-8"") as f:\n            line = f.readline()\n            i = 0\n            while line:\n                vocab = line.split("" "")\n                if len(vocab) != emb_size + 1:\n                    line = f.readline()\n                    continue\n                vocab = normalize_text(\'\'.join(vocab[0:-emb_size]).decode(""utf-8""))\n                if vocab not in dict_:\n                    dict_[vocab] = count\n                line = f.readline()\n                count += 1\n                i += 1\n                if i % 100 == 0:\n                    sys.stdout.write(""\\rProcessing line %d       ""%i)\n            print("""")\n        return dict_, count\n\n    def process_json(self,file_dir,out_dir, write_ = True):\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n        self.data = json.load(codecs.open(file_dir,""rb"",""utf-8""))\n        self.loop(self.data, out_dir, write_ = write_)\n        with codecs.open(""dictionary.txt"",""wb"",""utf-8"") as f:\n            for key, value in sorted(self.w_dict.iteritems(), key=lambda (k,v): (v,k)):\n                f.write(""%s: %s"" % (key, value) + ""\\n"")\n\n    def loop(self, data, dir_ = Params.train_dir, write_ = True):\n        for topic in tqdm(data[\'data\'],total = len(data[\'data\'])):\n            for para in topic[\'paragraphs\']:\n\n                words_c,chars_c = self.add_to_dict(para[\'context\'])\n                if len(words_c) >= Params.max_p_len:\n                    continue\n\n                for qas in para[\'qas\']:\n                    question = qas[\'question\']\n                    words,chars = self.add_to_dict(question)\n                    if len(words) >= Params.max_q_len:\n                        continue\n                    ans = qas[\'answers\'][0]\n                    ans_ids,_ = self.add_to_dict(ans[\'text\'])\n                    answers = find_answer_index(words_c, ans_ids)\n                    for answer in answers:\n                        start_i, finish_i = answer\n                        if start_i == -1:\n                            self.invalid_q += 1\n                            continue\n                        if write_:\n                            write_file([str(start_i),str(finish_i)],dir_ + Params.target_dir)\n                            write_file(words,dir_ + Params.q_word_dir)\n                            write_file(chars,dir_ + Params.q_chars_dir)\n                            write_file(words_c,dir_ + Params.p_word_dir)\n                            write_file(chars_c,dir_ + Params.p_chars_dir)\n\n    def process_word(self,line):\n        for word in line:\n            word = word.replace("" "","""").strip()\n            word = normalize_text(\'\'.join(word).decode(""utf-8""))\n            if word:\n                if not word in self.w_dict:\n                    self.w_dict[word] = self.w_count\n                    self.w_count += 1\n\n    def process_char(self,line):\n        for char in line.strip():\n            if char:\n                if char != "" "":\n                    if not char in self.c_dict:\n                        self.c_dict[char] = self.c_count\n                        self.c_count += 1\n\n    def add_to_dict(self, line):\n        splitted_line = tokenize_corenlp(line)\n\n        if self.append_dict:\n            self.process_word(splitted_line)\n            self.process_char("""".join(splitted_line))\n\n        words = []\n        chars = []\n        for i,word in enumerate(splitted_line):\n            word = word.replace("" "","""").strip()\n            word = normalize_text(\'\'.join(word).decode(""utf-8""))\n            if word:\n                if i > 0:\n                    chars.append(""_SPC"")\n                for char in word:\n                    char = self.c_dict.get(char,self.c_dict[""_UNK""])\n                    chars.append(str(char))\n                    self.c_occurence += 1\n                    if char == 0:\n                        self.c_unknown_count += 1\n\n                word = self.w_dict.get(word.strip().strip("" ""),self.w_dict[""_UNK""])\n                words.append(str(word))\n                self.w_occurence += 1\n                if word == 0:\n                    self.w_unknown_count += 1\n        return (words, chars)\n\n    def realtime_process(self, data):\n        p,q = data\n        p_max_word = Params.max_p_len\n        p_max_char = Params.max_char_len\n        q_max_word = Params.max_q_len\n        q_max_char = Params.max_char_len\n\n        pw,pc = self.add_to_dict(p)\n        qw,qc = self.add_to_dict(q)\n        p_word_len = [len(pw)]\n        q_word_len = [len(qw)]\n        pc, pcl = get_char_line("" "".join(pc))\n        qc, qcl = get_char_line("" "".join(qc))\n\n        p_word_ids = pad_data([pw],p_max_word)\n        q_word_ids = pad_data([qw],q_max_word)\n\n        p_word_len = np.reshape(np.asarray(p_word_len,np.int32),(-1,1))\n        q_word_len = np.reshape(np.asarray(q_word_len,np.int32),(-1,1))\n\n        p_char_len = pad_char_len([pcl], p_max_word, p_max_char)\n        q_char_len = pad_char_len([qcl], q_max_word, q_max_char)\n        p_char_ids = pad_char_data([pc],p_max_char,p_max_word)\n        q_char_ids = pad_char_data([qc],q_max_char,q_max_word)\n\n        shapes=[(p_max_word,),(q_max_word,),\n                (p_max_word,p_max_char,),(q_max_word,q_max_char,),\n                (1,),(1,),\n                (p_max_word,),(q_max_word,)]\n\n\n        return ([p_word_ids, q_word_ids,\n                p_char_ids, q_char_ids,\n                p_word_len, q_word_len,\n                p_char_len, q_char_len], shapes)\n\n\ndef load_glove(dir_, name, vocab_size):\n    glove = np.zeros((vocab_size, Params.emb_size),dtype = np.float32)\n    with codecs.open(dir_,""rb"",""utf-8"") as f:\n        line = f.readline()\n        i = 1\n        while line:\n            if i % 100 == 0:\n                sys.stdout.write(""\\rProcessing %d vocabs       ""%i)\n            vector = line.split("" "")\n            if len(vector) != Params.emb_size + 1:\n                line = f.readline()\n                continue\n            name_ = vector[0]\n            vector = vector[-Params.emb_size:]\n            if vector:\n                try:\n                    vector = [float(n) for n in vector]\n                except:\n                    assert 0\n                vector = np.asarray(vector, np.float32)\n                try:\n                    glove[i] = vector\n                except:\n                    assert 0\n            line = f.readline()\n            i += 1\n    print(""\\n"")\n    glove_map = np.memmap(Params.data_dir + name + "".np"", dtype=\'float32\', mode=\'write\', shape=(vocab_size, Params.emb_size))\n    glove_map[:] = glove\n    del glove_map\n\ndef reduce_glove(dir_, dict_):\n    glove_f = []\n    with codecs.open(dir_, ""rb"", ""utf-8"") as f:\n        line = f.readline()\n        i = 0\n        while line:\n            i += 1\n            if i % 100 == 0:\n                sys.stdout.write(""\\rProcessing %d vocabs       ""%i)\n            vector = line.split("" "")\n            if len(vector) != Params.emb_size + 1:\n                line = f.readline()\n                continue\n            vocab = normalize_text(\'\'.join(vector[0:-Params.emb_size]).decode(""utf-8""))\n            if vocab not in dict_:\n                line = f.readline()\n                continue\n            glove_f.append(line)\n            line = f.readline()\n    print(""\\nTotal number of lines: {}\\nReduced vocab size: {}"".format(i, len(glove_f)))\n    with codecs.open(dir_, ""wb"", ""utf-8"") as f:\n        for line in glove_f[:-1]:\n            f.write(line)\n        f.write(glove_f[-1].strip(""\\n""))\n\ndef find_answer_index(context, answer):\n    window_len = len(answer)\n    answers = []\n    if window_len == 1:\n        indices = [i for i, ctx in enumerate(context) if ctx == answer[0]]\n        for i in indices:\n            answers.append((i,i))\n        if not indices:\n            answers.append((-1,-1))\n        return answers\n    for i in range(len(context)):\n        if context[i:i+window_len] == answer:\n            answers.append((i, i + window_len))\n    if len(answers) == 0:\n        return [(-1, -1)]\n    else:\n        return answers\n\ndef normalize_text(text):\n    return unicodedata.normalize(\'NFD\', text)\n\ndef write_file(indices, dir_, separate = ""\\n""):\n    with codecs.open(dir_,""ab"",""utf-8"") as f:\n        f.write("" "".join(indices) + separate)\n\ndef pad_data(data, max_word):\n    padded_data = np.zeros((len(data),max_word),dtype = np.int32)\n    for i,line in enumerate(data):\n        for j,word in enumerate(line):\n            if j >= max_word:\n                break\n            padded_data[i,j] = word\n    return padded_data\n\ndef pad_char_len(data, max_word, max_char):\n    padded_data = np.zeros((len(data), max_word), dtype=np.int32)\n    for i, line in enumerate(data):\n        for j, word in enumerate(line):\n            if j >= max_word:\n                break\n            padded_data[i, j] = word if word <= max_char else max_char\n    return padded_data\n\ndef pad_char_data(data, max_char, max_words):\n    padded_data = np.zeros((len(data),max_words,max_char),dtype = np.int32)\n    for i,line in enumerate(data):\n        for j,word in enumerate(line):\n            if j >= max_words:\n                break\n            for k,char in enumerate(word):\n                if k >= max_char:\n                    # ignore the rest of the word if it\'s longer than the limit\n                    break\n                padded_data[i,j,k] = char\n    return padded_data\n\ndef get_char_line(line):\n    line = line.split(""_SPC"")\n    c_len = []\n    chars = []\n    for word in line:\n        c = [int(w) for w in word.split()]\n        c_len.append(len(c))\n        chars.append(c)\n    return chars, c_len\n\ndef load_target(dir):\n    data = []\n    count = 0\n    with codecs.open(dir,""rb"",""utf-8"") as f:\n        line = f.readline()\n        while count < 1000 if Params.mode == ""debug"" else line:\n            line = [int(w) for w in line.split()]\n            data.append(line)\n            count += 1\n            line = f.readline()\n    return data\n\ndef load_word(dir):\n    data = []\n    w_len = []\n    count = 0\n    with codecs.open(dir,""rb"",""utf-8"") as f:\n        line = f.readline()\n        while count < 1000 if Params.mode == ""debug"" else line:\n            line = [int(w) for w in line.split()]\n            data.append(line)\n            count += 1\n            w_len.append(len(line))\n            line = f.readline()\n    return data, w_len\n\ndef load_char(dir):\n    data = []\n    w_len = []\n    c_len_ = []\n    count = 0\n    with codecs.open(dir,""rb"",""utf-8"") as f:\n        line = f.readline()\n        while count < 1000 if Params.mode == ""debug"" else line:\n            c_len = []\n            chars = []\n            line = line.split(""_SPC"")\n            for word in line:\n                c = [int(w) for w in word.split()]\n                c_len.append(len(c))\n                chars.append(c)\n            data.append(chars)\n            line = f.readline()\n            count += 1\n            c_len_.append(c_len)\n            w_len.append(len(c_len))\n    return data, c_len_, w_len\n\ndef max_value(inputlist):\n    max_val = 0\n    for list_ in inputlist:\n        for val in list_:\n            if val > max_val:\n                max_val = val\n    return max_val\n\ndef main():\n    if args.reduce_glove:\n        print(""Reducing Glove Matrix"")\n        loader = data_loader(use_pretrained = False)\n        loader.process_json(Params.data_dir + ""train-v1.1.json"", out_dir = Params.train_dir, write_ = False)\n        loader.process_json(Params.data_dir + ""dev-v1.1.json"", out_dir = Params.dev_dir, write_ = False)\n        reduce_glove(Params.glove_dir, loader.w_dict)\n    with open(Params.data_dir + \'dictionary.pkl\',\'wb\') as dictionary:\n        loader = data_loader(use_pretrained = True)\n        print(""Tokenizing training data."")\n        loader.process_json(Params.data_dir + ""train-v1.1.json"", out_dir = Params.train_dir)\n        print(""Tokenizing dev data."")\n        loader.process_json(Params.data_dir + ""dev-v1.1.json"", out_dir = Params.dev_dir)\n        pickle.dump(loader, dictionary, pickle.HIGHEST_PROTOCOL)\n    print(""Tokenizing complete"")\n    if os.path.isfile(Params.data_dir + ""glove.np""): exit()\n    load_glove(Params.glove_dir,""glove"",vocab_size = loader.w_count)\n    load_glove(Params.glove_char,""glove_char"", vocab_size = loader.c_count)\n    print(""Processing complete"")\n    print(""Unknown word ratio: {} / {}"".format(loader.w_unknown_count,loader.w_occurence))\n    print(""Unknown character ratio: {} / {}"".format(loader.c_unknown_count,loader.c_occurence))\n\nif __name__ == ""__main__"":\n    main()\n'"
zoneout.py,3,"b'# from ipywidgets import interact\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\n\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import variable_scope\r\n\r\n# Wrapper for the TF RNN cell\r\n# For an LSTM, the \'cell\' is a tuple containing state and cell\r\n# We use TF\'s dropout to implement zoneout\r\nclass ZoneoutWrapper(tf.nn.rnn_cell.RNNCell):\r\n  """"""Operator adding zoneout to all states (states+cells) of the given cell.""""""\r\n\r\n  def __init__(self, cell, state_zoneout_prob, is_training=True, seed=None):\r\n    if not isinstance(cell, tf.nn.rnn_cell.RNNCell):\r\n      raise TypeError(""The parameter cell is not an RNNCell."")\r\n    if (isinstance(state_zoneout_prob, float) and\r\n        not (state_zoneout_prob >= 0.0 and state_zoneout_prob <= 1.0)):\r\n      raise ValueError(""Parameter zoneout_prob must be between 0 and 1: %d""\r\n                       % zoneout_prob)\r\n    self._cell = cell\r\n    self._zoneout_prob = state_zoneout_prob\r\n    self._seed = seed\r\n    self.is_training = is_training\r\n\r\n  @property\r\n  def state_size(self):\r\n    return self._cell.state_size\r\n\r\n  @property\r\n  def output_size(self):\r\n    return self._cell.output_size\r\n\r\n  def __call__(self, inputs, state, scope=None):\r\n    output, new_state = self._cell(inputs, state, scope)\r\n    if self.is_training:\r\n        new_state = (1 - self._zoneout_prob) * tf.nn.dropout(\r\n                      new_state - state, (1 - self._zoneout_prob), seed=self._seed) + state\r\n    else:\r\n        new_state = self._zoneout_prob * state + (1 - self._zoneout_prob) * new_state\r\n    return output, new_state\r\n'"
