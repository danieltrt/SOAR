file_path,api_count,code
demo_streaming_asr.py,0,"b'from frame_asr import FrameASR\nimport numpy as np\nimport pyaudio as pa\nimport time\n\nCHANNELS = 1\nRATE = 16000\nDURATION = 2.0\nCHUNK_SIZE = int(DURATION*RATE)\n\np = pa.PyAudio()\n\nprint(\'Available audio input devices:\')\nfor i in range(p.get_device_count()):\n    dev = p.get_device_info_by_index(i)\n    if dev.get(\'maxInputChannels\'):\n        print(i, dev.get(\'name\'))\nprint(\'Please type input device ID:\')\ndev_idx = int(input())\n\n\nasr = FrameASR()\nprint(\'Initialization was successful\')\n\n\ndef callback(in_data, frame_count, time_info, status):\n    signal = np.frombuffer(in_data, dtype=np.int16)\n    pred = asr.transcribe(signal)\n    if len(pred.strip()):\n        print(\'""{}""\'.format(pred))\n    return (in_data, pa.paContinue)\n\n\nstream = p.open(format=pa.paInt16,\n                channels=CHANNELS,\n                rate=RATE,\n                input=True,\n                input_device_index=dev_idx,\n                stream_callback=callback,\n                frames_per_buffer=CHUNK_SIZE)\n\nstream.start_stream()\n\nwhile stream.is_active():\n    time.sleep(0.1)\n\nstream.stop_stream()\nstream.close()\np.terminate()\n\n'"
frame_asr.py,5,"b'# Copyright (c) 2019 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport scipy.io.wavfile as wave\nimport tensorflow as tf\n\nfrom collections import defaultdict\n\nfrom open_seq2seq.utils.utils import get_base_config, check_logdir,\\\n                                     create_model, get_interactive_infer_results\n\nfrom open_seq2seq.data.speech2text.speech_utils import get_speech_features_from_file, get_speech_features \n\nfrom ctc_decoders import Scorer, BeamDecoder\n\n\n\n# Define the command line arguments that one would pass to run.py here\nMODEL_PARAMS = [""--config_file=models/Jasper-Mini-for-Jetson/config_infer_stream.py"",\n                ""--mode=interactive_infer"",\n                ""--logdir=models/Jasper-Mini-for-Jetson/"",\n                ""--batch_size_per_gpu=1"",\n                ""--num_gpus=1"",\n                ""--use_horovod=False"",\n                ""--decoder_params/infer_logits_to_pickle=True"",\n                ""--data_layer_params/pad_to=0""\n]\n\n\ndef softmax(x):\n    \'\'\'\n    Naive softmax implementation for NumPy\n    \'\'\'\n    m = np.expand_dims(np.max(x, axis=-1), -1)\n    e = np.exp(x - m)\n    return e / np.expand_dims(e.sum(axis=-1), -1)\n\n\nclass FrameASR:\n    \n    def __init__(self, model_params=MODEL_PARAMS, scope_name=\'S2T\', \n                 sr=16000, frame_len=0.2, frame_overlap=2.4, \n                 timestep_duration=0.02, \n                 ext_model_infer_func=None, merge=True,\n                 beam_width=1, language_model=None, \n                 alpha=2.8, beta=1.0):\n        \'\'\'\n        Args:\n          model_params: list of OpenSeq2Seq arguments (same as for run.py)\n          scope_name: model\'s scope name\n          sr: sample rate, Hz\n          frame_len: frame\'s duration, seconds\n          frame_overlap: duration of overlaps before and after current frame, seconds\n            frame_overlap should be multiple of frame_len\n          timestep_duration: time per step at model\'s output, seconds\n          ext_model_infer_func: callback for external inference engine,\n            if it is not None, then we don\'t build TF inference graph\n          merge: whether to do merge in greedy decoder\n          beam_width: beam width for beam search decoder if larger than 1\n          language_model: path to LM (to use with beam search decoder)\n          alpha: LM weight (trade-off between acoustic and LM scores)\n          beta: word weight (added per every transcribed word in prediction)\n        \'\'\'\n        if ext_model_infer_func is None:\n            # Build TF inference graph\n            self.model_S2T, checkpoint_S2T = self._get_model(model_params, scope_name)\n\n            # Create the session and load the checkpoints\n            sess_config = tf.ConfigProto(allow_soft_placement=True)\n            sess_config.gpu_options.allow_growth = True\n            self.sess = tf.InteractiveSession(config=sess_config)\n            vars_S2T = {}\n            for v in tf.get_collection(tf.GraphKeys.VARIABLES):\n                if scope_name in v.name:\n                    vars_S2T[\'/\'.join(v.op.name.split(\'/\')[1:])] = v\n            saver_S2T = tf.train.Saver(vars_S2T)\n            saver_S2T.restore(self.sess, checkpoint_S2T)\n            self.params = self.model_S2T.params\n        else:\n            # No TF, load pre-, post-processing parameters from config,\n            # use external inference engine\n            _, base_config, _, _ = get_base_config(model_params)\n            self.params = base_config\n\n        self.ext_model_infer_func = ext_model_infer_func\n\n        self.vocab = self._load_vocab(\n            self.model_S2T.params[\'data_layer_params\'][\'vocab_file\']\n        )\n        self.sr = sr\n        self.frame_len = frame_len\n        self.n_frame_len = int(frame_len * sr)\n        self.frame_overlap = frame_overlap\n        self.n_frame_overlap = int(frame_overlap * sr)\n        if self.n_frame_overlap % self.n_frame_len:\n            raise ValueError(\n                ""\'frame_overlap\' should be multiple of \'frame_len\'""\n            )\n        self.n_timesteps_overlap = int(frame_overlap / timestep_duration) - 2\n        self.buffer = np.zeros(shape=2*self.n_frame_overlap + self.n_frame_len, dtype=np.float32)\n        self.merge = merge\n        self._beam_decoder = None\n        # greedy decoder\'s state (unmerged transcription)\n        self.text = \'\'\n        # forerunner greedy decoder\'s state (unmerged transcription)\n        self.forerunner_text = \'\'\n\n        self.offset = 5\n        # self._calibrate_offset()\n        if beam_width > 1:\n          if language_model is None:\n            self._beam_decoder = BeamDecoder(self.vocab, beam_width)\n          else:\n            self._scorer = Scorer(alpha, beta, language_model, self.vocab)\n            self._beam_decoder = BeamDecoder(self.vocab, beam_width, ext_scorer=self._scorer)\n        self.reset()\n\n\n    def _get_audio(self, wav):\n        """"""Parses audio from wav and returns array of audio features.\n        Args:\n            wav: numpy array containing wav\n \n        Returns:\n            tuple: source audio features as ``np.array``, length of source sequence,\n            sample id.\n        """"""\n        source, audio_duration = get_speech_features(\n            wav, 16000., self.params[\'data_layer_params\']\n        )\n\n        return source, \\\n            np.int32([len(source)]), np.int32([0]), \\\n            np.float32([audio_duration])\n\n\n    def _parse_audio_element(self, id_and_audio_filename):\n        """"""Parses audio from file and returns array of audio features.\n        Args:\n            id_and_audio_filename: tuple of sample id and corresponding\n            audio file name.\n        Returns:\n            tuple: source audio features as ``np.array``, length of source sequence,\n            sample id.\n        """"""\n        idx, audio_filename = id_and_audio_filename\n        source, audio_duration = get_speech_features_from_file(\n            audio_filename,\n            params=self.params\n        )\n        return source, \\\n            np.int32([len(source)]), np.int32([idx]), \\\n            np.float32([audio_duration])\n\n\n    def _preprocess_audio(self, model_in):\n        audio_arr = []\n        audio_length_arr = []\n\n        for line in model_in:\n          if isinstance(line, str):\n            features, features_length, _, _ = self._parse_audio_element([0, line])\n          elif isinstance(line, np.ndarray):\n            features, features_length, _, _ = self._get_audio(line)\n          else:\n            raise ValueError(\n                ""Speech2Text\'s interactive inference mode only supports string or"",\n                ""numpy array as input. Got {}"". format(type(line))\n            )\n          audio_arr.append(features)\n          audio_length_arr.append(features_length)\n        max_len = np.max(audio_length_arr)\n        pad_to = self.params.get(""pad_to"", 8)\n        if pad_to > 0 and self.params.get(\'backend\') == \'librosa\':\n          max_len += (pad_to - max_len % pad_to) % pad_to\n        for idx in range(len(audio_arr)):\n          audio_arr[idx] = np.pad(\n              audio_arr[idx], ((0, max_len-len(audio_arr[idx])), (0, 0)),\n              ""constant"", constant_values=0.\n          )\n\n        audio_features = np.reshape(\n            audio_arr,\n            [self.params[\'batch_size_per_gpu\'],\n             -1,\n             self.params[\'data_layer_params\'][\'num_audio_features\']]\n        )\n        features_length = np.reshape(audio_length_arr, [self.params[\'batch_size_per_gpu\']])\n        return [audio_features, features_length]\n\n\n    def _decode(self, frame, offset=0, merge=False):\n        assert len(frame)==self.n_frame_len\n        self.buffer[:-self.n_frame_len] = self.buffer[self.n_frame_len:]\n        self.buffer[-self.n_frame_len:] = frame\n\n        audio_features, features_length = self._preprocess_audio([self.buffer])\n        if self.ext_model_infer_func is None:\n            logits = get_interactive_infer_results(\n                self.model_S2T, self.sess, \n                model_in={\'audio_features\': audio_features,\n                          \'features_length\': features_length})[0][0]\n        else:\n            # TODO: check ext_model_infer_func parameters and return value\n            logits = self.ext_model_infer_func(audio_features, features_length)\n\n        if self._beam_decoder is None:\n          decoded_forerunner = self._greedy_decoder(\n              logits[self.n_timesteps_overlap:], \n              self.vocab\n          )\n          decoded = decoded_forerunner[:-self.n_timesteps_overlap-offset]\n\n          forerunner_idx = max(0, len(self.forerunner_text) - \\\n              (self.n_timesteps_overlap + offset))\n          self.forerunner_text = self.forerunner_text[:forerunner_idx] + \\\n              decoded_forerunner\n          self.text += decoded\n          if merge:\n            decoded = self.greedy_merge(self.text)\n            decoded_forerunner = self.greedy_merge(self.forerunner_text)\n        else:\n          decoded = self._beam_decoder.decode(softmax(\n              logits[self.n_timesteps_overlap:-self.n_timesteps_overlap-offset]\n          ))[0][-1]\n\n        return [decoded, decoded_forerunner]\n\n    \n    def transcribe(self, frame=None):\n        if frame is None:\n            frame = np.zeros(shape=self.n_frame_len, dtype=np.float32)\n        if len(frame) < self.n_frame_len:\n            frame = np.pad(frame, [0, self.n_frame_len - len(frame)], \'constant\')\n        return self._decode(frame, self.offset, self.merge)\n    \n    \n    def _calibrate_offset(self, wav_file, max_offset=10, n_calib_inter=100):\n        \'\'\'\n        Calibrate offset for frame-by-frame decoding\n        \'\'\'\n        sr, signal = wave.read(wav_file)\n        \n        # warmup\n        n_warmup = 1 + int(np.ceil(2.0 * self.frame_overlap / self.frame_len))\n        for i in range(n_warmup):\n            decoded, _ = self._decode(signal[self.n_frame_len*i:self.n_frame_len*(i+1)], offset=0)\n        \n        i = n_warmup\n        \n        offsets = defaultdict(lambda: 0)\n        while i < n_warmup + n_calib_inter and (i+1)*self.n_frame_len < signal.shape[0]:\n            decoded_prev = decoded\n            decoded, _ = self._decode(signal[self.n_frame_len*i:self.n_frame_len*(i+1)], offset=0)\n            for offset in range(max_offset, 0, -1):\n                if decoded[:offset] == decoded_prev[-offset:] and decoded[:offset] != \'\'.join([\'_\']*offset):\n                    offsets[offset] += 1\n                    break\n            i += 1\n        self.offset = max(offsets, key=offsets.get)\n       \n        \n    def reset(self):\n        \'\'\'\n        Reset frame_history and decoder\'s state\n        \'\'\'\n        self.buffer=np.zeros(shape=self.buffer.shape, dtype=np.float32)\n        if self._beam_decoder is not None:\n            self._beam_decoder.reset()\n        self.prev_char = \'\'\n        self.text = \'\'\n        self.forerunner_text = \'\'\n        \n    @staticmethod\n    def _get_model(args, scope):\n        \'\'\'\n        A simpler version of what run.py does. It returns the created model and its saved checkpoint\n        \'\'\'\n        with tf.variable_scope(scope):\n            args, base_config, base_model, config_module = get_base_config(args)\n            checkpoint = check_logdir(args, base_config)\n            model = create_model(args, base_config, config_module, base_model, None)\n        return model, checkpoint\n\n    @staticmethod\n    def _load_vocab(vocab_file):\n        vocab = []\n        with open(vocab_file, \'r\') as f:\n            for line in f:\n                vocab.append(line[0])\n        vocab.append(\'_\')\n        return vocab\n\n    @staticmethod\n    def _greedy_decoder(logits, vocab):\n        s = \'\'\n        for i in range(logits.shape[0]):\n            s += vocab[np.argmax(logits[i])]\n        return s\n\n    def greedy_merge(self, s, prev_char=\'\'):\n        s_merged = \'\'\n        \n        for i in range(len(s)):\n            if s[i] != prev_char:\n                prev_char = s[i]\n                if prev_char != \'_\':\n                    s_merged += prev_char\n        return s_merged\n\n'"
run.py,3,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\nimport tensorflow as tf\n\nif hasattr(tf.compat, \'v1\'):\n  tf.compat.v1.disable_eager_execution()\n\nfrom open_seq2seq.utils.utils import deco_print, get_base_config, create_model,\\\n                                     create_logdir, check_logdir, \\\n                                     check_base_model_logdir\nfrom open_seq2seq.utils import train, infer, evaluate\n\ndef main():\n  # Parse args and create config\n  args, base_config, base_model, config_module = get_base_config(sys.argv[1:])\n\n  if args.mode == ""interactive_infer"":\n    raise ValueError(\n        ""Interactive infer is meant to be run from an IPython"",\n        ""notebook not from run.py.""\n    )\n\n#   restore_best_checkpoint = base_config.get(\'restore_best_checkpoint\', False)\n#   # Check logdir and create it if necessary\n#   checkpoint = check_logdir(args, base_config, restore_best_checkpoint)\n\n  load_model = base_config.get(\'load_model\', None)\n  restore_best_checkpoint = base_config.get(\'restore_best_checkpoint\', False)\n  base_ckpt_dir = check_base_model_logdir(load_model, args,\n                                          restore_best_checkpoint)\n  base_config[\'load_model\'] = base_ckpt_dir\n\n  # Check logdir and create it if necessary\n  checkpoint = check_logdir(args, base_config, restore_best_checkpoint)\n\n  # Initilize Horovod\n  if base_config[\'use_horovod\']:\n    import horovod.tensorflow as hvd\n    hvd.init()\n    if hvd.rank() == 0:\n      deco_print(""Using horovod"")\n    from mpi4py import MPI\n    MPI.COMM_WORLD.Barrier()\n  else:\n    hvd = None\n\n  if args.enable_logs:\n    if hvd is None or hvd.rank() == 0:\n      old_stdout, old_stderr, stdout_log, stderr_log = create_logdir(\n          args,\n          base_config\n      )\n    base_config[\'logdir\'] = os.path.join(base_config[\'logdir\'], \'logs\')\n\n  if args.mode == \'train\' or args.mode == \'train_eval\' or args.benchmark:\n    if hvd is None or hvd.rank() == 0:\n      if checkpoint is None or args.benchmark:\n        if base_ckpt_dir:\n          deco_print(""Starting training from the base model"")\n        else:\n          deco_print(""Starting training from scratch"")\n      else:\n        deco_print(\n            ""Restored checkpoint from {}. Resuming training"".format(checkpoint),\n        )\n  elif args.mode == \'eval\' or args.mode == \'infer\':\n    if hvd is None or hvd.rank() == 0:\n      deco_print(""Loading model from {}"".format(checkpoint))\n\n  # Create model and train/eval/infer\n  with tf.Graph().as_default():\n    model = create_model(\n        args, base_config, config_module, base_model, hvd, checkpoint)\n    hooks = None\n    if (\'train_params\' in config_module and\n        \'hooks\' in config_module[\'train_params\']):\n      hooks = config_module[\'train_params\'][\'hooks\']\n    if args.mode == ""train_eval"":\n      train(\n        model[0], eval_model=model[1], debug_port=args.debug_port,\n        custom_hooks=hooks)\n    elif args.mode == ""train"":\n      train(\n        model, eval_model=None, debug_port=args.debug_port, custom_hooks=hooks)\n    elif args.mode == ""eval"":\n      evaluate(model, checkpoint)\n    elif args.mode == ""infer"":\n      infer(model, checkpoint, args.infer_output_file)\n\n  if args.enable_logs and (hvd is None or hvd.rank() == 0):\n    sys.stdout = old_stdout\n    sys.stderr = old_stderr\n    stdout_log.close()\n    stderr_log.close()\n\n\nif __name__ == \'__main__\':\n  main()\n'"
tokenizer_wrapper.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n#from open_seq2seq.data.text2text.text2text import SpecialTextTokens\n\nimport argparse\nimport sentencepiece as spm\n\nimport os\nimport sys\nimport codecs\n\n\nvocab_size = 32768\n\ndef train_tokenizer_model(args):\n  print(""========> Training tokenizer model"")\n  vocab_size = args.vocab_size\n  model_prefix = args.model_prefix\n  input_file = args.text_input\n\n  spm.SentencePieceTrainer.Train(\n    ""--input={0} --model_type=bpe --model_prefix={1} --vocab_size={2} --pad_id={3} --eos_id={4} --bos_id={5} --unk_id={6} --character_coverage=1.0""\n      .format(input_file,\n              model_prefix, vocab_size, 0, # PAD. TODO: these should not be hardcoded\n              1, 2, # EOS, SID\n              3) # UNK \n  )\n\ndef tokenize(args):\n  print(""========> Using tokenizer model"")\n  model_prefix1 = args.model_prefix1\n  model_prefix2 = args.model_prefix2\n  input_file1 = args.text_input1\n  input_file2 = args.text_input2\n  tokenized_output1 = args.tokenized_output1\n  tokenized_output2 = args.tokenized_output2\n\n  sp1 = spm.SentencePieceProcessor()\n  sp1.Load(model_prefix1+"".model"")\n  sp2 = spm.SentencePieceProcessor()\n  sp2.Load(model_prefix2 + "".model"")\n\n  ind = 0\n  with open(input_file1, \'r\') as file1, open(input_file2, \'r\') as file2:\n    with open(tokenized_output1, \'w\') as ofile1, open(tokenized_output2, \'w\') as ofile2:\n      while True: # YaY!\n        _src_raw = file1.readline()\n        _tgt_raw = file2.readline()\n\n        if not _src_raw or not _tgt_raw:\n          break\n\n        src_raw = _src_raw.strip()\n        tgt_raw = _tgt_raw.strip()\n\n        try:\n          encoded_src_list = sp1.EncodeAsPieces(src_raw)\n          encoded_tgt_list = sp2.EncodeAsPieces(tgt_raw)\n        except:\n          continue\n\n        encoded_src = \' \'.join([w for w in encoded_src_list])\n        encoded_tgt = \' \'.join([w for w in encoded_tgt_list])\n\n        ofile1.write(encoded_src + ""\\n"")\n        ofile2.write(encoded_tgt + ""\\n"")\n        ind += 1\n\ndef encode(args):\n  print(""========> Encoding..."")\n  model_prefix1 = args.model_prefix\n  input_file1 = args.text_input\n  tokenized_output1 = args.tokenized_output\n  sp1 = spm.SentencePieceProcessor()\n  sp1.Load(model_prefix1+"".model"")\n  ind = 0\n  with open(input_file1, \'r\') as file1:\n    with open(tokenized_output1, \'w\') as ofile1:\n      while True: # YaY!\n        _src_raw = file1.readline()\n        if not _src_raw:\n          break\n        src_raw = _src_raw.strip()\n        try:\n          encoded_src_list = sp1.EncodeAsPieces(src_raw)\n        except:\n          continue\n        \n        if sys.version_info < (3, 0):\n          encoded_src = \' \'.join([w for w in encoded_src_list])\n        else:\n          encoded_src = \' \'.join([w.decode(""utf-8"") for w in encoded_src_list])\n\n        ofile1.write(encoded_src + ""\\n"")\n        ind += 1\n  print(""========> ...Done"")\n\n\ndef detokenize(args):\n  print(""========> Detokenizing"")\n  model_prefix = args.model_prefix\n  sp = spm.SentencePieceProcessor()\n  sp.Load(model_prefix+"".model"")\n  input_file = args.text_input\n  output_file = args.decoded_output\n  with open(output_file, \'w\') as otpt:\n    with open(input_file, \'r\') as inpt:\n      for line in inpt:\n        decoded_line = sp.DecodePieces(line.split("" ""))\n        if sys.version_info >= (3, 0):\n          otpt.write(decoded_line)\n        else:\n          otpt.write(decoded_line.decode(""utf-8""))\n\n\ndef main():\n  parser = argparse.ArgumentParser(description=\'Input Parameters\')\n  parser.add_argument(""--text_input"",\n                      help=""Path to text"")\n  parser.add_argument(""--decoded_output"",\n                      help=""Path were to save decoded output during decoding"")\n  parser.add_argument(""--text_input1"",\n                      help=""Path to src text when tokenizing"")\n  parser.add_argument(""--text_input2"",\n                      help=""Path to tgt text when tokenizing"")\n  parser.add_argument(""--tokenized_output"",\n                      help=""Path to tokenized src text results"")\n  parser.add_argument(""--tokenized_output1"",\n                      help=""Path to tokenized src text results"")\n  parser.add_argument(""--tokenized_output2"",\n                      help=""Path to tokenized tgt text results"")\n  parser.add_argument(""--model_prefix"",\n                      help=""model prefix"")\n  parser.add_argument(""--model_prefix1"",\n                      help=""model prefix for src when tokenizing"")\n  parser.add_argument(""--model_prefix2"",\n                      help=""model prefix for tgt when tokenizing"")\n  parser.add_argument(\'--vocab_size\', type=int, default=vocab_size,\n                      help=\'Vocabulary size\')\n  parser.add_argument(\'--mode\', required=True,\n                      help=\'train, tokenize, encode, or detokenize\')\n  args, unknown = parser.parse_known_args()\n  if args.mode == ""train"":\n    train_tokenizer_model(args)\n  elif args.mode == ""tokenize"":\n    tokenize(args)\n  elif args.mode == ""detokenize"":\n    detokenize(args)\n  elif args.mode == ""encode"":\n    encode(args)\n  else:\n    raise ValueError(\'Unknown mode: {0}\', args.mode)\n\nif __name__ == \'__main__\':\n  main()\n'"
ctc_decoder_with_lm/ctc-test.py,15,"b""import numpy as np\nimport pickle\nimport tensorflow as tf\n\n\ndef load_test_sample(pickle_file):\n  with open(pickle_file, 'rb') as f:\n    seq, label = pickle.load(f, encoding='bytes')\n  return seq, label \n\ndef load_vocab(vocab_file):\n  vocab = []\n  with open(vocab_file, 'r') as f:\n    for line in f:\n      vocab.append(line[0])\n  vocab.append('_')\n  return vocab\n\n\nclass CTCCustomDecoderTests(tf.test.TestCase):\n\n  def setUp(self):\n    self.seq, self.label = load_test_sample('ctc_decoder_with_lm/ctc-test.pickle')\n    self.vocab = load_vocab('open_seq2seq/test_utils/toy_speech_data/vocab.txt')\n    self.beam_width = 16\n    self.tol = 1e-3\n\n\n  def test_decoders(self):\n    '''\n    Test all CTC decoders on a sample transcript ('ten seconds').\n    Standard TF decoders should output 'then seconds'.\n    Custom CTC decoder with LM rescoring should yield 'ten seconds'.\n    '''\n    logits = tf.constant(self.seq)\n    seq_len = tf.constant([self.seq.shape[0]])\n\n    greedy_decoded = tf.nn.ctc_greedy_decoder(logits, seq_len, \n        merge_repeated=True)\n\n    beam_search_decoded = tf.nn.ctc_beam_search_decoder(logits, seq_len, \n        beam_width=self.beam_width, \n        top_paths=1, \n        merge_repeated=False)\n\n    custom_op_module = tf.load_op_library('ctc_decoder_with_lm/libctc_decoder_with_kenlm.so')\n    decoded_ixs, decoded_vals, decoded_shapes, log_probabilities = (\n        custom_op_module.ctc_beam_search_decoder_with_lm(\n            logits, seq_len, beam_width=self.beam_width,\n            model_path='ctc_decoder_with_lm/ctc-test-lm.binary', \n            trie_path='ctc_decoder_with_lm/ctc-test-lm.trie',\n            alphabet_path='open_seq2seq/test_utils/toy_speech_data/vocab.txt',\n            alpha=2.0,\n            beta=0.5,\n            trie_weight=0.1,\n            top_paths=1, merge_repeated=False\n        )\n    )\n\n    with tf.Session() as sess:\n      res_greedy, res_beam, res_ixs, res_vals, res_probs = sess.run([greedy_decoded, \n          beam_search_decoded, decoded_ixs, decoded_vals, log_probabilities])\n\n    decoded_greedy, prob_greedy = res_greedy\n    decoded_text = ''.join([self.vocab[c] for c in decoded_greedy[0].values])\n    self.assertTrue( abs(7079.117 + prob_greedy[0][0]) < self.tol )\n    self.assertTrue( decoded_text == 'then seconds' )\n\n    decoded_beam, prob_beam = res_beam\n    decoded_text = ''.join([self.vocab[c] for c in decoded_beam[0].values])\n    if tf.__version__ >= '1.11':\n      # works for newer versions only (with CTC decoder fix)\n      self.assertTrue( abs(1.1842575 + prob_beam[0][0]) < self.tol )\n    self.assertTrue( decoded_text == 'then seconds' )\n\n    self.assertTrue( abs(4.619581 + res_probs[0][0]) < self.tol )\n    decoded_text = ''.join([self.vocab[c] for c in res_vals[0]])\n    self.assertTrue( decoded_text == self.label )\n\n\n  def test_beam_decoders(self):\n    '''\n    Test on random data that custom decoder outputs the same transcript\n    if its parameters are equal to zero: alpha = beta = trie_weight = 0.0\n    '''\n    np.random.seed(1234)\n    logits = tf.constant(np.random.uniform(size=self.seq.shape).astype(np.float32))\n    seq_len = tf.constant([self.seq.shape[0]])\n\n    beam_search_decoded = tf.nn.ctc_beam_search_decoder(logits, seq_len,\n        beam_width=self.beam_width,\n        top_paths=1,\n        merge_repeated=False)\n\n    custom_op_module = tf.load_op_library('ctc_decoder_with_lm/libctc_decoder_with_kenlm.so')\n    decoded_ixs, decoded_vals, decoded_shapes, log_probabilities = (\n        custom_op_module.ctc_beam_search_decoder_with_lm(\n            logits, seq_len, beam_width=self.beam_width,\n            model_path='ctc_decoder_with_lm/ctc-test-lm.binary',\n            trie_path='ctc_decoder_with_lm/ctc-test-lm.trie',\n            alphabet_path='open_seq2seq/test_utils/toy_speech_data/vocab.txt',\n            alpha=0.0,\n            beta=0.0,\n            trie_weight=0.0,\n            top_paths=1, merge_repeated=False\n        )\n    )\n\n    with tf.Session() as sess:\n      res_beam, res_ixs, res_vals, res_probs = sess.run([beam_search_decoded,\n          decoded_ixs, decoded_vals, log_probabilities])\n\n    decoded_beam, prob_beam = res_beam\n    prob1 = prob_beam[0][0]\n    decoded_text1 = ''.join([self.vocab[c] for c in decoded_beam[0].values])\n\n    prob2 = res_probs[0][0]\n    if tf.__version__ >= '1.11':\n      # works for newer versions only (with CTC decoder fix)\n      self.assertTrue( abs(prob1 - prob2) < self.tol )\n    self.assertTrue( prob2 < 0 )\n    decoded_text2 = ''.join([self.vocab[c] for c in res_vals[0]])\n\n    self.assertTrue( decoded_text1 == decoded_text2 )\n\n    \nif __name__ == '__main__':\n  tf.test.main()\n\n"""
decoders/__init__.py,0,b''
decoders/_init_paths.py,0,"b'""""""Set up paths for DS2""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\nimport sys\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\n\nthis_dir = os.path.dirname(__file__)\n\n# Add project path to PYTHONPATH\nproj_path = os.path.join(this_dir, \'..\')\nadd_path(proj_path)\n'"
decoders/ctc_decoders.py,0,"b'""""""Wrapper for various CTC decoders in SWIG.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport swig_decoders\n\n\nclass Scorer(swig_decoders.Scorer):\n    """"""Wrapper for Scorer.\n\n    :param alpha: Parameter associated with language model. Don\'t use\n                  language model when alpha = 0.\n    :type alpha: float\n    :param beta: Parameter associated with word count. Don\'t use word\n                 count when beta = 0.\n    :type beta: float\n    :model_path: Path to load language model.\n    :type model_path: basestring\n    """"""\n\n    def __init__(self, alpha, beta, model_path, vocabulary):\n        swig_decoders.Scorer.__init__(self, alpha, beta, model_path, vocabulary)\n\n\nclass BeamDecoder(swig_decoders.BeamDecoder):\n    """"""Wrapper for BeamDecoder.\n    """"""\n    def __init__(self, vocabulary, beam_size, \n                 cutoff_prob=1.0,\n                 cutoff_top_n=40,\n                 ext_scorer=None):\n        swig_decoders.BeamDecoder.__init__(self, vocabulary, beam_size, \n                                           cutoff_prob,\n                                           cutoff_top_n,\n                                           ext_scorer)\n\n    def decode(self, probs_seq):\n        beam_results = swig_decoders.BeamDecoder.decode(\n            self,\n            probs_seq.tolist()\n        )\n        beam_results = [(res[0], res[1]) for res in beam_results]\n        return beam_results\n\n\ndef ctc_greedy_decoder(probs_seq, vocabulary):\n    """"""Wrapper for ctc best path decoder in swig.\n\n    :param probs_seq: 2-D list of probability distributions over each time\n                      step, with each element being a list of normalized\n                      probabilities over vocabulary and blank.\n    :type probs_seq: 2-D list\n    :param vocabulary: Vocabulary list.\n    :type vocabulary: list\n    :return: Decoding result string.\n    :rtype: basestring\n    """"""\n    result = swig_decoders.ctc_greedy_decoder(probs_seq.tolist(), vocabulary)\n    return result\n\n\ndef ctc_beam_search_decoder(probs_seq,\n                            vocabulary,\n                            beam_size,\n                            cutoff_prob=1.0,\n                            cutoff_top_n=40,\n                            ext_scoring_func=None):\n    """"""Wrapper for the CTC Beam Search Decoder.\n\n    :param probs_seq: 2-D list of probability distributions over each time\n                      step, with each element being a list of normalized\n                      probabilities over vocabulary and blank.\n    :type probs_seq: 2-D list\n    :param vocabulary: Vocabulary list.\n    :type vocabulary: list\n    :param beam_size: Width for beam search.\n    :type beam_size: int\n    :param cutoff_prob: Cutoff probability in pruning,\n                        default 1.0, no pruning.\n    :type cutoff_prob: float\n    :param cutoff_top_n: Cutoff number in pruning, only top cutoff_top_n\n                         characters with highest probs in vocabulary will be\n                         used in beam search, default 40.\n    :type cutoff_top_n: int\n    :param ext_scoring_func: External scoring function for\n                             partially decoded sentence, e.g. word count\n                             or language model.\n    :type external_scoring_func: callable\n    :return: List of tuples of log probability and sentence as decoding\n             results, in descending order of the probability.\n    :rtype: list\n    """"""\n    beam_results = swig_decoders.ctc_beam_search_decoder(\n        probs_seq.tolist(), vocabulary, beam_size, cutoff_prob, cutoff_top_n,\n        ext_scoring_func)\n    beam_results = [(res[0], res[1]) for res in beam_results]\n    return beam_results\n\n\ndef ctc_beam_search_decoder_batch(probs_split,\n                                  vocabulary,\n                                  beam_size,\n                                  num_processes,\n                                  cutoff_prob=1.0,\n                                  cutoff_top_n=40,\n                                  ext_scoring_func=None):\n    """"""Wrapper for the batched CTC beam search decoder.\n\n    :param probs_seq: 3-D list with each element as an instance of 2-D list\n                      of probabilities used by ctc_beam_search_decoder().\n    :type probs_seq: 3-D list\n    :param vocabulary: Vocabulary list.\n    :type vocabulary: list\n    :param beam_size: Width for beam search.\n    :type beam_size: int\n    :param num_processes: Number of parallel processes.\n    :type num_processes: int\n    :param cutoff_prob: Cutoff probability in vocabulary pruning,\n                        default 1.0, no pruning.\n    :type cutoff_prob: float\n    :param cutoff_top_n: Cutoff number in pruning, only top cutoff_top_n\n                         characters with highest probs in vocabulary will be\n                         used in beam search, default 40.\n    :type cutoff_top_n: int\n    :param num_processes: Number of parallel processes.\n    :type num_processes: int\n    :param ext_scoring_func: External scoring function for\n                             partially decoded sentence, e.g. word count\n                             or language model.\n    :type external_scoring_function: callable\n    :return: List of tuples of log probability and sentence as decoding\n             results, in descending order of the probability.\n    :rtype: list\n    """"""\n    probs_split = [probs_seq.tolist() for probs_seq in probs_split]\n\n    batch_beam_results = swig_decoders.ctc_beam_search_decoder_batch(\n        probs_split, vocabulary, beam_size, num_processes, cutoff_prob,\n        cutoff_top_n, ext_scoring_func)\n    batch_beam_results = [\n        [(res[0], res[1]) for res in beam_results]\n        for beam_results in batch_beam_results\n    ]\n    return batch_beam_results\n'"
decoders/setup.py,0,"b'""""""Script to build and install decoder package.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom setuptools import setup, Extension, distutils\nimport glob\nimport platform\nimport os, sys\nimport multiprocessing.pool\nimport argparse\n\nparser = argparse.ArgumentParser(description=__doc__)\nparser.add_argument(\n    ""--num_processes"",\n    default=1,\n    type=int,\n    help=""Number of cpu processes to build package. (default: %(default)d)"")\nargs = parser.parse_known_args()\n\n# reconstruct sys.argv to pass to setup below\nsys.argv = [sys.argv[0]] + args[1]\n\n\n# monkey-patch for parallel compilation\n# See: https://stackoverflow.com/a/13176803\ndef parallelCCompile(self,\n                     sources,\n                     output_dir=None,\n                     macros=None,\n                     include_dirs=None,\n                     debug=0,\n                     extra_preargs=None,\n                     extra_postargs=None,\n                     depends=None):\n    # those lines are copied from distutils.ccompiler.CCompiler directly\n    macros, objects, extra_postargs, pp_opts, build = self._setup_compile(\n        output_dir, macros, include_dirs, sources, depends, extra_postargs)\n    cc_args = self._get_cc_args(pp_opts, debug, extra_preargs)\n\n    # parallel code\n    def _single_compile(obj):\n        try:\n            src, ext = build[obj]\n        except KeyError:\n            return\n        self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)\n\n    # convert to list, imap is evaluated on-demand\n    thread_pool = multiprocessing.pool.ThreadPool(args[0].num_processes)\n    list(thread_pool.imap(_single_compile, objects))\n    return objects\n\n\ndef compile_test(header, library):\n    dummy_path = os.path.join(os.path.dirname(__file__), ""dummy"")\n    command = ""bash -c \\""g++ -include "" + header \\\n                + "" -l"" + library + "" -x c++ - <<<\'int main() {}\' -o "" \\\n                + dummy_path + "" >/dev/null 2>/dev/null && rm "" \\\n                + dummy_path + "" 2>/dev/null\\""""\n    return os.system(command) == 0\n\n\n# hack compile to support parallel compiling\ndistutils.ccompiler.CCompiler.compile = parallelCCompile\n\nFILES = glob.glob(\'kenlm/util/*.cc\') \\\n        + glob.glob(\'kenlm/lm/*.cc\') \\\n        + glob.glob(\'kenlm/util/double-conversion/*.cc\')\n\nFILES += glob.glob(\'openfst-1.6.3/src/lib/*.cc\')\n\nFILES = [\n    fn for fn in FILES\n    if not (fn.endswith(\'main.cc\') or fn.endswith(\'test.cc\') or fn.endswith(\n        \'unittest.cc\'))\n]\n\nLIBS = [\'stdc++\']\nif platform.system() != \'Darwin\':\n    LIBS.append(\'rt\')\n\nARGS = [\'-O3\', \'-DNDEBUG\', \'-DKENLM_MAX_ORDER=6\', \'-std=c++11\']\n\nif compile_test(\'zlib.h\', \'z\'):\n    ARGS.append(\'-DHAVE_ZLIB\')\n    LIBS.append(\'z\')\n\nif compile_test(\'bzlib.h\', \'bz2\'):\n    ARGS.append(\'-DHAVE_BZLIB\')\n    LIBS.append(\'bz2\')\n\nif compile_test(\'lzma.h\', \'lzma\'):\n    ARGS.append(\'-DHAVE_XZLIB\')\n    LIBS.append(\'lzma\')\n\nos.system(\'swig -python -c++ ./decoders.i\')\n\ndecoders_module = [\n    Extension(\n        name=\'_swig_decoders\',\n        sources=FILES + glob.glob(\'*.cxx\') + glob.glob(\'*.cpp\'),\n        language=\'c++\',\n        include_dirs=[\n            \'.\',\n            \'kenlm\',\n            \'openfst-1.6.3/src/include\',\n            \'ThreadPool\',\n        ],\n        libraries=LIBS,\n        extra_compile_args=ARGS)\n]\n\nsetup(\n    name=\'ctc_decoders\',\n    version=\'1.1\',\n    description=""""""CTC decoders"""""",\n    ext_modules=decoders_module,\n    py_modules=[\'ctc_decoders\', \'swig_decoders\'], )\n'"
external_lm_rescore/process_beam_dump.py,3,"b'# coding: utf-8\nimport sys\nsys.path.append(""./transformerxl"")\nsys.path.append(""./transformerxl/utils"")\nimport argparse\nfrom typing import List\nimport torch\nimport random\nfrom utils.vocabulary import Vocab\nfrom torch.nn.utils.rnn import pad_sequence\n\nparser = argparse.ArgumentParser(\n  description=\'Process OS2S output with external LM\')\nparser.add_argument(\'--beam_dump\', type=str, default=\'\',\n                    help=\'path to OS2S beam dump\')\nparser.add_argument(\'--beam_dump_with_lm\', type=str, default=\'\',\n                    help=\'this is where beam dump will be augmented \'\n                         \'with LM score\')\nparser.add_argument(\'--model\', type=str, default=\'\',\n                    help=\'path to neural language model\')\nparser.add_argument(\'--vocab\', type=str, default=\'\',\n                    help=\'path to vocabluary\')\nparser.add_argument(\'--reference\', type=str, default=\'\',\n                    help=\'path to reference against which to compare\')\n\nargs = parser.parse_args()\n\n\ndef levenshtein(a, b):\n  """"""Calculates the Levenshtein distance between a and b.\n  The code was copied from: http://hetland.org/coding/python/levenshtein.py\n  """"""\n  n, m = len(a), len(b)\n  if n > m:\n    # Make sure n <= m, to use O(min(n,m)) space\n    a, b = b, a\n    n, m = m, n\n\n  current = list(range(n + 1))\n  for i in range(1, m + 1):\n    previous, current = current, [i] + [0] * n\n    for j in range(1, n + 1):\n      add, delete = previous[j] + 1, current[j - 1] + 1\n      change = previous[j - 1]\n      if a[j - 1] != b[i - 1]:\n        change = change + 1\n      current[j] = min(add, delete, change)\n\n  return current[n]\n\ndef score_fun_linear(s1, s2, s3, s4):\n  return s4 + s1\n\n\nclass Scorer:\n  def __init__(self, model, path_2_vocab, score_fn=score_fun_linear):\n    self._model = model\n    self._model.eval()\n    self._model.crit.keep_order=True\n    self._vocab = Vocab(vocab_file=path_2_vocab)\n    self._vocab.build_vocab()\n    self._score_fn = score_fn\n\n    print(\'---->>> Testing Model.\')\n    self.test_model(candidates=[\'they had one night in which to prepare for deach\',\n                                \'they had one night in which to prepare for death\',\n                                \'i hate school\', \'i love school\',\n                                \'the fox jumps on a grass\',\n                                \'the crox jump a la glass\'])\n    print(\'---->>> Done testing model\')\n\n\n  @staticmethod\n  def chunks(l, n):\n    for i in range(0, len(l), n):\n      yield l[i:i + n]\n\n\n  def nlm_compute(self, candidates_full, batch_size=256):\n    results = torch.zeros(len(candidates_full))\n    with torch.no_grad():\n      for j, candidates in enumerate(self.chunks(candidates_full, batch_size)):\n        sents = self._vocab.encode_sents(\n          [[\'<S>\'] + string.strip().lower().split() + [\'<S>\'] for string in candidates])\n        seq_lens = torch.tensor([x.shape[0] for x in sents], dtype=torch.long)\n        sents_th = torch.zeros(seq_lens.max(), seq_lens.shape[0],dtype=torch.long).cuda()\n        for i, sent in enumerate(sents):\n          sents_th[:seq_lens[i], i] = sent\n       \n        mems = tuple()\n        ret = self._model(sents_th[:-1], sents_th[1:], *mems)\n        max_len = seq_lens.max()-1\n        mask = torch.arange(max_len).expand(seq_lens.shape[0], max_len) >= seq_lens.unsqueeze(1)-1\n        result = -1 * ret[0].masked_fill(mask.transpose(0,1).to(""cuda""), 0).sum(dim=0)\n        results[j*batch_size:j*batch_size + len(result)] = result\n    return results\n  \n\n  def test_model(self, candidates):\n    for item in zip(list(self.nlm_compute(candidates).cpu().detach().numpy()), candidates):\n      print(""{0} ---- {1}"".format(item[0], item[1]))\n\n\n  def chose_best_candidate(self, candidates: List) -> str:\n    candidates_t = [c[3] for c in candidates]\n    nln_scores = self.nlm_compute(candidates_t)\n    candidate = candidates[0][3]\n    score = -100000000000.0\n    for i in range(len(candidates)):\n      s1 = candidates[i][0]\n      s2 = candidates[i][1]\n      s3 = candidates[i][2]\n      s4 = nln_scores[i].item()\n      new_score = self._score_fn(s1, s2, s3, s4)\n      if new_score > score:\n        candidate = candidates[i][3]\n        score = new_score\n    return (candidate, nln_scores)\n\ndef main():\n  if args.beam_dump == \'\':\n    print(""Please provide path to OS2S beam dump"")\n    exit(1)\n\n  with open(args.model, \'rb\') as f:\n    #rnn_lm = torch.load(f)\n    # after load the rnn params are not a continuous chunk of memory\n    # this makes them a continuous chunk, and will speed up forward pass\n    #rnn_lm.rnn.flatten_parameters()\n    lm = torch.load(f)\n  #lm = InferenceModel(rnn_lm)\n  scorer = Scorer(lm, args.vocab)\n  #scorer = Scorer(rnn_lm, args.vocab)\n\n  reference_strings = []\n  first = True\n  with open(args.reference, \'r\') as inpr:\n    for line in inpr:\n      if first: # skip header\n        first = False\n        continue\n      reference_strings.append(line.split(\',\')[2])\n\n  print(\'Read {0} reference lines from {1}\'.format(len(reference_strings),\n                                                   args.reference))\n\n  scores = 0\n  words = 0\n  counter = 0\n  with open(args.beam_dump, \'r\') as inpf:\n    with open(args.beam_dump_with_lm, \'w\') as outf:\n      candidate_list = []\n      for line in inpf:\n        sline = line.strip()\n        # sample begin\n        if sline == ""B=>>>>>>>>"":\n          candidate_list = []\n        # sample end\n        elif sline == ""E=>>>>>>>>"":\n          if counter % 100 == 0:\n            print(""Processed {0} candidates"".format(counter))\n          candidate, nlm_scores = scorer.chose_best_candidate(candidate_list)\n          words += len(reference_strings[counter].split())\n          scores += levenshtein(reference_strings[counter].split(),\n                                candidate.split())\n          counter += 1\n          # output augmented scores:\n          outf.write(""B=>>>>>>>>\\n"")\n          assert(len(nlm_scores) == len(candidate_list))\n          for i in range(len(nlm_scores)):\n            outf.write(""\\t"".join(\n              [str(nlm_scores[i].item())] + [str(t) for t in\n                                             list(candidate_list[i])]) + ""\\n"")\n          outf.write(""E=>>>>>>>>\\n"")\n\n        else:\n          sparts = sline.split()\n          s1 = float(sparts[0])\n          s2 = float(sparts[1])\n          s3 = float(sparts[2])\n          c = \' \'.join(sparts[3:])\n          candidate_list.append((s1, s2, s3, c))\n  print(""WER: {0} after processing {1} predictions"".format((scores*1.0)/words,\n                                                           counter))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
open_seq2seq/__init__.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\n""""""\nThis package provides multi-node, multi-GPU sequence to sequence learning\n""""""'"
scripts/build_lm.py,0,"b""import pandas as pd\nimport os\nimport argparse\n\ndef get_corpus(csv_files):\n  '''\n  Get text corpus from a list of CSV files\n  '''\n  SEP = '\\n'\n  corpus = ''\n  for f in csv_files:\n    df = pd.read_csv(f)\n    corpus += SEP.join(df['transcript']) + SEP\n  return corpus\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='Build N-gram LM model from CSV files')\n  parser.add_argument('csv', metavar='csv', type=str, nargs='+',\n                      help='CSV file with transcripts')\n  parser.add_argument('--n', type=int, help='n for n-grams', default=3)\n  args = parser.parse_args()\n\n  corpus = get_corpus(args.csv)\n\n  path_prefix, _ = os.path.splitext(args.csv[0])\n  corpus_name = path_prefix + '.txt'\n  arpa_name = path_prefix + '.arpa'\n  lm_name = path_prefix + '-lm.binary'\n  with open(corpus_name, 'w') as f:\n    f.write(corpus)\n\n  command = 'kenlm/build/bin/lmplz --text {} --arpa {} --o {}'.format(\n    corpus_name, arpa_name, args.n)\n  print(command)\n  os.system(command)\n\n  command = 'kenlm/build/bin/build_binary trie -q 8 -b 7 -a 256 {} {}'.format(\n    arpa_name, lm_name)\n  print(command)\n  os.system(command)\n\n  command = 'ctc_decoder_with_lm/generate_trie'\n  if os.path.isfile(command) and os.access(command, os.X_OK):\n    trie_name = path_prefix + '-lm.trie'\n    command += ' open_seq2seq/test_utils/toy_speech_data/vocab.txt {} {} {}'.format(\n        lm_name, corpus_name, trie_name)\n    print('INFO: Generating a trie for custom TF op based CTC decoder.')\n    print(command)\n    os.system(command)\n  else:\n    print('INFO: Skipping trie generation, since no custom TF op based CTC decoder found.')\n    print('INFO: Please use Baidu CTC decoder with this language model.')\n\n"""
scripts/build_lm_text.py,0,"b""import pandas as pd\nimport os\nimport argparse\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='Build N-gram LM model from text file')\n  parser.add_argument('text', metavar='text', type=str,\n                      help='text file')\n  parser.add_argument('--n', type=int, help='n for n-grams', default=3)\n  args = parser.parse_args()\n\n  path_prefix, _ = os.path.splitext(args.text)\n  corpus_name = args.text\n  arpa_name = path_prefix + '.arpa'\n  lm_name = path_prefix + '-lm.binary'\n\n  command = 'kenlm/build/bin/lmplz --text {} --arpa {} --o {}'.format(\n    corpus_name, arpa_name, args.n)\n  print(command)\n  os.system(command)\n\n  command = 'kenlm/build/bin/build_binary trie -q 8 -b 7 -a 256 {} {}'.format(\n    arpa_name, lm_name)\n  print(command)\n  os.system(command)\n\n  command = 'ctc_decoder_with_lm/generate_trie'\n  if os.path.isfile(command) and os.access(command, os.X_OK):\n    trie_name = path_prefix + '-lm.trie'\n    command += ' open_seq2seq/test_utils/toy_speech_data/vocab.txt {} {} {}'.format(\n        lm_name, corpus_name, trie_name)\n    print('INFO: Generating a trie for custom TF op based CTC decoder.')\n    print(command)\n    os.system(command)\n  else:\n    print('INFO: Skipping trie generation, since no custom TF op based CTC decoder found.')\n    print('INFO: Please use Baidu CTC decoder with this language model.')\n\n"""
scripts/calibrate_model.py,3,"b'# Copyright (c) 2017 NVIDIA Corporation\n""""""This file helps to calculate word to speech alignments for your model\nPlease execute get_calibration_files.sh before running this script\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\nimport pickle\nimport json\nimport numpy as np\nimport tensorflow as tf\nsys.path.append(os.getcwd())\n\nfrom open_seq2seq.utils.utils import deco_print, get_calibration_config, create_model,\\\n                                create_logdir, check_logdir, \\\n                                check_base_model_logdir\nfrom open_seq2seq.utils import infer\nfrom open_seq2seq.utils.ctc_decoder import ctc_greedy_decoder\n\nif hasattr(tf.compat, \'v1\'):\n  tf.compat.v1.disable_eager_execution()\n\n\ndef run():\n  """"""This function executes a saved checkpoint for\n  50 LibriSpeech dev clean files whose alignments are stored in\n  calibration/target.json\n  This function saves a pickle file with logits after running\n  through the model as calibration/sample.pkl\n\n  :return: None\n  """"""\n  args, base_config, base_model, config_module = get_calibration_config(sys.argv[1:])\n  config_module[""infer_params""][""data_layer_params""][""dataset_files""] = \\\n    [""calibration/sample.csv""]\n  config_module[""base_params""][""decoder_params""][""infer_logits_to_pickle""] = True\n  load_model = base_config.get(\'load_model\', None)\n  restore_best_checkpoint = base_config.get(\'restore_best_checkpoint\',\n                                            False)\n  base_ckpt_dir = check_base_model_logdir(load_model, args,\n                                          restore_best_checkpoint)\n  base_config[\'load_model\'] = base_ckpt_dir\n\n  # Check logdir and create it if necessary\n  checkpoint = check_logdir(args, base_config, restore_best_checkpoint)\n\n  # Initilize Horovod\n  if base_config[\'use_horovod\']:\n    import horovod.tensorflow as hvd\n    hvd.init()\n    if hvd.rank() == 0:\n      deco_print(""Using horovod"")\n    from mpi4py import MPI\n    MPI.COMM_WORLD.Barrier()\n  else:\n    hvd = None\n\n  if args.enable_logs:\n    if hvd is None or hvd.rank() == 0:\n      old_stdout, old_stderr, stdout_log, stderr_log = create_logdir(\n          args, base_config\n      )\n      base_config[\'logdir\'] = os.path.join(base_config[\'logdir\'], \'logs\')\n\n  if args.mode == \'infer\':\n    if hvd is None or hvd.rank() == 0:\n      deco_print(""Loading model from {}"".format(checkpoint))\n  else:\n    print(""Run in infer mode only"")\n    sys.exit()\n  with tf.Graph().as_default():\n    model = create_model(\n        args, base_config, config_module, base_model, hvd, checkpoint)\n    infer(model, checkpoint, args.infer_output_file)\n\n  return args.calibration_out\n\n\ndef calibrate(source, target):\n  """"""This function calculates the mean start and end shift\n  needed for your model to get word to speech alignments\n  """"""\n  print(""calibrating {}"".format(source))\n  start_shift = []\n  end_shift = []\n  dump = pickle.load(open(source, ""rb""))\n  results = dump[""logits""]\n  vocab = dump[""vocab""]\n  step_size = dump[""step_size""]\n  blank_idx = len(vocab)\n  with open(target, ""r"") as read_file:\n    target = json.load(read_file)\n  for wave_file in results:\n\n    transcript, start, end = ctc_greedy_decoder(results[wave_file], vocab,\n                                                step_size, blank_idx, 0, 0)\n    words = transcript.split("" "")\n    k = 0\n    print(words)\n    alignments = []\n    for new_word in words:\n      alignments.append({""word"": new_word, ""start"": start[k], ""end"": end[k]})\n      k += 1\n    if len(target[wave_file][""words""]) == len(words):\n      for i, new_word in enumerate(target[wave_file][""words""]):\n        if new_word[""case""] == ""success"" and \\\n          new_word[""alignedWord""] == alignments[i][""word""]:\n          start_shift.append(new_word[""start""] - alignments[i][""start""])\n          end_shift.append(new_word[""end""] - alignments[i][""end""])\n  mean_start_shift = np.mean(start_shift)\n  mean_end_shift = np.mean(end_shift)\n  return mean_start_shift, mean_end_shift\n\n\nif __name__ == \'__main__\':\n  calibration_out = run()\n  start_mean, end_mean = calibrate(""calibration/sample.pkl"",\n                                   ""calibration/target.json"")\n  print(""Mean start shift is {:.5f} seconds"".format(start_mean))\n  print(""Mean end shift is: {:.5f} seconds"".format(end_mean))\n  with open(calibration_out, ""w"") as f:\n    string = ""{} {}"".format(start_mean, end_mean)\n    f.write(string)\n'"
scripts/change_sample_rate.py,0,"b'import os\nimport sys\nimport argparse\nimport librosa\n\nparser = argparse.ArgumentParser(description=\'Conversion parameters\')\nparser.add_argument(""--source_dir"", required=False, type=str, default=""calibration/sound_files/"",\n                    help=""Path to source of flac LibriSpeech files"")\nparser.add_argument(""--target_dir"", required=False, type=str, default=""calibration/sound_files_wav/"",\n                    help=""Path to source of flac LibriSpeech files"")\nparser.add_argument(""--sample_rate"", required=False, type=int, default=16000,\n                    help=""Output sample rate"")\nargs = parser.parse_args()\nsource_dir = args.source_dir\nsample_rate = args.sample_rate\ntarget_dir = args.target_dir\n\ndef getListOfFiles(dirName):\n  """"""create a list of file and sub directories\n  names in the given directory\n  """"""\n  listOfFile = os.listdir(dirName)\n  allFiles = list()\n  # Iterate over all the entries\n  for entry in listOfFile:\n    # Create full path\n    fullPath = os.path.join(dirName, entry)\n    # If entry is a directory then get the list of files in this directory\n    if os.path.isdir(fullPath):\n      allFiles = allFiles + getListOfFiles(fullPath)\n    else:\n      if fullPath[-3:] == ""wav"" or fullPath[-4:] == ""flac"":\n        allFiles.append(fullPath)\n  return allFiles\n\n\ndef convert_to_wav(flac_files,sample_rate,target_dir):\n  """"""This function converts flac input to wav output of given sample rate""""""\n  for sound_file in flac_files:\n    dir_tree = sound_file.split(""/"")[-4:]\n    save_path = \'/\'.join(dir_tree[:-1])\n    name = dir_tree[-1][:-4] + ""wav""\n    if not os.path.isdir(save_path):\n      os.makedirs(save_path)\n    sig, sr = librosa.load(sound_file, sample_rate)\n    output_dir = target_dir+save_path\n    if not os.path.isdir(output_dir):\n      os.makedirs(output_dir)\n    librosa.output.write_wav(output_dir + ""/"" + name, sig, sample_rate)\n\nflac_files = getListOfFiles(source_dir)\nconvert_to_wav(flac_files,sample_rate,target_dir)\n'"
scripts/ctc_decoders_test.py,13,"b""import numpy as np\nimport pickle\nimport tensorflow as tf\n\nfrom ctc_decoders import Scorer, ctc_beam_search_decoder\n\n\n\ndef load_test_sample(pickle_file):\n  with open(pickle_file, 'rb') as f:\n    seq, label = pickle.load(f, encoding='bytes')\n  return seq, label \n\ndef load_vocab(vocab_file):\n  vocab = []\n  with open(vocab_file, 'r') as f:\n    for line in f:\n      vocab.append(line[0])\n  vocab.append('_')\n  return vocab\n\ndef softmax(x):\n  m = np.expand_dims(np.max(x, axis=-1), -1)\n  e = np.exp(x - m)\n  return e / np.expand_dims(e.sum(axis=-1), -1)\n\n\nclass CTCCustomDecoderTests(tf.test.TestCase):\n\n  def setUp(self):\n    self.seq, self.label = load_test_sample('ctc_decoder_with_lm/ctc-test.pickle')\n    self.vocab = load_vocab('open_seq2seq/test_utils/toy_speech_data/vocab.txt')\n    self.beam_width = 16\n    self.tol = 1e-3\n\n\n  def test_decoders(self):\n    '''\n    Test all CTC decoders on a sample transcript ('ten seconds').\n    Standard TF decoders should output 'then seconds'.\n    Custom CTC decoder with LM rescoring should yield 'ten seconds'.\n    '''\n    logits = tf.constant(self.seq)\n    seq_len = tf.constant([self.seq.shape[0]])\n\n    greedy_decoded = tf.nn.ctc_greedy_decoder(logits, seq_len, \n        merge_repeated=True)\n\n    beam_search_decoded = tf.nn.ctc_beam_search_decoder(logits, seq_len, \n        beam_width=self.beam_width, \n        top_paths=1, \n        merge_repeated=False)\n\n    with tf.Session() as sess:\n      res_greedy, res_beam = sess.run([greedy_decoded, \n          beam_search_decoded])\n\n    decoded_greedy, prob_greedy = res_greedy\n    decoded_text = ''.join([self.vocab[c] for c in decoded_greedy[0].values])\n    self.assertTrue( abs(7079.117 + prob_greedy[0][0]) < self.tol )\n    self.assertTrue( decoded_text == 'then seconds' )\n\n    decoded_beam, prob_beam = res_beam\n    decoded_text = ''.join([self.vocab[c] for c in decoded_beam[0].values])\n    if tf.__version__ >= '1.11':\n      # works for newer versions only (with CTC decoder fix)\n      self.assertTrue( abs(1.1842 + prob_beam[0][0]) < self.tol )\n    self.assertTrue( decoded_text == 'then seconds' )\n\n    scorer = Scorer(alpha=2.0, beta=0.5,\n        model_path='ctc_decoder_with_lm/ctc-test-lm.binary', \n        vocabulary=self.vocab[:-1])\n    res = ctc_beam_search_decoder(softmax(self.seq.squeeze()), self.vocab[:-1],\n                                  beam_size=self.beam_width,\n                                  ext_scoring_func=scorer)\n    res_prob, decoded_text = res[0]\n    self.assertTrue( abs(4.0845 + res_prob) < self.tol )\n    self.assertTrue( decoded_text == self.label )\n\n\n  def test_beam_decoders(self):\n    '''\n    Test on random data that custom decoder outputs the same transcript\n    as standard TF beam search decoder\n    '''\n    seq = np.random.uniform(size=self.seq.shape).astype(np.float32)\n    logits = tf.constant(seq)\n    seq_len = tf.constant([self.seq.shape[0]])\n\n    beam_search_decoded = tf.nn.ctc_beam_search_decoder(logits, seq_len,\n        beam_width=self.beam_width,\n        top_paths=1,\n        merge_repeated=False)\n\n\n    with tf.Session() as sess:\n      res_beam = sess.run(beam_search_decoded)\n    decoded_beam, prob_beam = res_beam\n    prob1 = prob_beam[0][0]\n    decoded_text1 = ''.join([self.vocab[c] for c in decoded_beam[0].values])\n\n    res = ctc_beam_search_decoder(softmax(seq.squeeze()), self.vocab[:-1],\n                                  beam_size=self.beam_width)\n    prob2, decoded_text2 = res[0]\n\n    if tf.__version__ >= '1.11':\n      # works for newer versions only (with CTC decoder fix)\n      self.assertTrue( abs(prob1 - prob2) < self.tol )\n    self.assertTrue( prob2 < 0 )\n\n    self.assertTrue( decoded_text1 == decoded_text2 )\n\n    \nif __name__ == '__main__':\n  tf.test.main()\n\n"""
scripts/decode.py,0,"b'\'\'\'\nInterface to Baidu\'s CTC decoders\nfrom https://github.com/PaddlePaddle/DeepSpeech/decoders/swig\n\'\'\'\n\nimport argparse\n\nimport pickle\nimport numpy as np\n\nfrom ctc_decoders import Scorer\nfrom ctc_decoders import ctc_greedy_decoder\nfrom ctc_decoders import ctc_beam_search_decoder_batch, ctc_beam_search_decoder\nfrom collections import defaultdict\nimport multiprocessing\n\n\nparser = argparse.ArgumentParser(\n    description=\'CTC decoding and tuning with LM rescoring\'\n)\nparser.add_argument(\'--mode\',\n    help=\'either \\\'eval\\\' (default) or \\\'infer\\\'\',\n    default=\'eval\'\n)\nparser.add_argument(\'--infer_output_file\',\n    help=\'output CSV file for \\\'infer\\\' mode\',\n    required=False\n)\nparser.add_argument(\'--logits\', \n    help=\'pickle file with CTC logits\',\n    required=True\n)\nparser.add_argument(\'--labels\',\n    help=\'CSV file with audio filenames \\\n      (and ground truth transcriptions for \\\'eval\\\' mode)\',\n    required=True\n)\nparser.add_argument(\'--lm\',\n    help=\'KenLM binary file\',\n    required=True\n)\nparser.add_argument(\'--vocab\',\n    help=\'vocab file with characters (alphabet)\',\n    required=True\n)\nparser.add_argument(\'--alpha\', type=float,\n    help=\'value of LM weight\',\n    required=True\n)\nparser.add_argument(\'--alpha_max\', type=float,\n    help=\'maximum value of LM weight (for a grid search in \\\'eval\\\' mode)\',\n    required=False\n)\nparser.add_argument(\'--alpha_step\', type=float,\n    help=\'step for LM weight\\\'s tuning in \\\'eval\\\' mode\',\n    required=False, default=0.1\n)\nparser.add_argument(\'--beta\', type=float,\n    help=\'value of word count weight\',\n    required=True\n)\nparser.add_argument(\'--beta_max\', type=float,\n    help=\'maximum value of word count weight (for a grid search in \\\n      \\\'eval\\\' mode\',\n    required=False\n)\nparser.add_argument(\'--beta_step\', type=float,\n    help=\'step for word count weight\\\'s tuning in \\\'eval\\\' mode\',\n    required=False, default=0.1\n)\nparser.add_argument(\'--beam_width\', type=int,\n    help=\'beam width for beam search decoder\',\n    required=False, default=128\n)\nparser.add_argument(\'--dump_all_beams_to\', \n    help=\'filename to dump all beams in eval mode for debug purposes\',\n    required=False, default=\'\')\nargs = parser.parse_args()\n\nif args.alpha_max is None:\n  args.alpha_max = args.alpha\n# include alpha_max in tuning range\nargs.alpha_max += args.alpha_step/10.0\n\nif args.beta_max is None:\n  args.beta_max = args.beta\n# include beta_max in tuning range\nargs.beta_max += args.beta_step/10.0\n\nnum_cpus = multiprocessing.cpu_count()\n\n\ndef levenshtein(a, b):\n  """"""Calculates the Levenshtein distance between a and b.\n  The code was taken from: http://hetland.org/coding/python/levenshtein.py\n  """"""\n  n, m = len(a), len(b)\n  if n > m:\n    # Make sure n <= m, to use O(min(n,m)) space\n    a, b = b, a\n    n, m = m, n\n  current = list(range(n + 1))\n  for i in range(1, m + 1):\n    previous, current = current, [i] + [0] * n\n    for j in range(1, n + 1):\n      add, delete = previous[j] + 1, current[j - 1] + 1\n      change = previous[j - 1]\n      if a[j - 1] != b[i - 1]:\n        change = change + 1\n      current[j] = min(add, delete, change)\n  return current[n]\n\n\ndef load_dump(pickle_file):\n  with open(pickle_file, \'rb\') as f:\n    data = pickle.load(f, encoding=\'bytes\')\n  return data\n\n\ndef get_logits(data, labels):\n  \'\'\'\n  Get logits from pickled data.\n  There are two versions of pickle file (and data):\n  1. raw logits NumPy array\n  2. dictionary with logits and additional meta information\n  \'\'\'\n  if isinstance(data, np.ndarray):\n    # convert NumPy array to dict format\n    logits = {}\n    for idx, line in enumerate(labels):\n      audio_filename = line[0]\n      logits[audio_filename] = data[idx]\n  else:\n    logits = data[\'logits\']\n  return logits\n\n\ndef load_labels(csv_file):\n  labels = np.loadtxt(csv_file, skiprows=1, delimiter=\',\', dtype=str)\n  return labels\n\n    \ndef load_vocab(vocab_file):\n  vocab = []\n  with open(vocab_file, \'r\') as f:\n    for line in f:\n      vocab.append(line[0])\n  vocab.append(\'_\')\n  return vocab\n\n\ndef greedy_decoder(logits, vocab, merge=True):\n  s = \'\'\n  c = \'\'\n  for i in range(logits.shape[0]):\n    c_i = vocab[np.argmax(logits[i])]\n    if merge and c_i == c:\n      continue \n    s += c_i\n    c = c_i\n  if merge:\n    s = s.replace(\'_\', \'\')\n  return s\n\n\ndef softmax(x):\n  m = np.expand_dims(np.max(x, axis=-1), -1)\n  e = np.exp(x - m)\n  return e / np.expand_dims(e.sum(axis=-1), -1)\n\n\ndef evaluate_wer(logits, labels, vocab, decoder):\n  total_dist = 0.0\n  total_count = 0.0\n  wer_per_sample = np.empty(shape=len(labels))\n    \n  empty_preds = 0\n  for idx, line in enumerate(labels):\n    audio_filename = line[0]\n    label = line[-1]\n    pred = decoder(logits[audio_filename], vocab)\n    dist = levenshtein(label.lower().split(), pred.lower().split())\n    if pred==\'\':\n      empty_preds += 1\n    total_dist += dist\n    total_count += len(label.split())\n    wer_per_sample[idx] = dist / len(label.split())\n  print(\'# empty preds: {}\'.format(empty_preds))\n  wer = total_dist / total_count\n  return wer, wer_per_sample\n\n\ndata = load_dump(args.logits)\nlabels = load_labels(args.labels)\nlogits = get_logits(data, labels)\nvocab = load_vocab(args.vocab)\nvocab[-1] = \'_\'\n\nprobs_batch = []\nfor line in labels:\n  audio_filename = line[0]\n  probs_batch.append(softmax(logits[audio_filename]))\n\nif args.mode == \'eval\':\n  wer, _ = evaluate_wer(logits, labels, vocab, greedy_decoder)\n  print(\'Greedy WER = {:.4f}\'.format(wer))\n  best_result = {\'wer\': 1e6, \'alpha\': 0.0, \'beta\': 0.0, \'beams\': None} \n  for alpha in np.arange(args.alpha, args.alpha_max, args.alpha_step):\n    for beta in np.arange(args.beta, args.beta_max, args.beta_step):\n      scorer = Scorer(alpha, beta, model_path=args.lm, vocabulary=vocab[:-1])\n      res = ctc_beam_search_decoder_batch(probs_batch, vocab[:-1], \n                                          beam_size=args.beam_width, \n                                          num_processes=num_cpus,\n                                          ext_scoring_func=scorer)\n      total_dist = 0.0\n      total_count = 0.0\n      for idx, line in enumerate(labels):\n        label = line[-1]\n        score, text = [v for v in zip(*res[idx])]\n        pred = text[0]\n        dist = levenshtein(label.lower().split(), pred.lower().split())\n        total_dist += dist\n        total_count += len(label.split())\n      wer = total_dist / total_count\n      if wer < best_result[\'wer\']:\n        best_result[\'wer\'] = wer\n        best_result[\'alpha\'] = alpha\n        best_result[\'beta\'] = beta\n        best_result[\'beams\'] = res\n      print(\'alpha={:.2f}, beta={:.2f}: WER={:.4f}\'.format(alpha, beta, wer))\n  print(\'BEST: alpha={:.2f}, beta={:.2f}, WER={:.4f}\'.format(\n        best_result[\'alpha\'], best_result[\'beta\'], best_result[\'wer\']))\n    \n  if args.dump_all_beams_to:\n   with open(args.dump_all_beams_to, \'w\') as f:\n     for beam in best_result[\'beams\']:\n       f.write(\'B=>>>>>>>>\\n\')\n       for pred in beam:\n         f.write(\'{} 0.0 0.0 {}\\n\'.format(pred[0], pred[1]))\n       f.write(\'E=>>>>>>>>\\n\')\n\nelif args.mode == \'infer\':\n  scorer = Scorer(args.alpha, args.beta, model_path=args.lm, vocabulary=vocab[:-1])\n  res = ctc_beam_search_decoder_batch(probs_batch, vocab[:-1], \n                                      beam_size=args.beam_width, \n                                      num_processes=num_cpus,\n                                      ext_scoring_func=scorer)\n  infer_preds = np.empty(shape=(len(labels), 2), dtype=object)\n  for idx, line in enumerate(labels):\n    filename = line[0]\n    score, text = [v for v in zip(*res[idx])]\n    infer_preds[idx, 0] = filename\n    infer_preds[idx, 1] = text[0]\n    \n  np.savetxt(args.infer_output_file, infer_preds, fmt=\'%s\', delimiter=\',\',\n             header=\'wav_filename,transcript\')\n\n'"
scripts/dump_to_time.py,0,"b'# Copyright (c) 2019 NVIDIA Corporation\n""""""This file takes given a logits output pickle and start and end shifts\n  words to speech and writes them in a csv file\n\n""""""\nfrom __future__ import absolute_import, division, print_function\nimport pickle\nimport argparse\nimport sys\nimport csv\nimport os\nsys.path.append(os.getcwd())\nfrom open_seq2seq.utils.ctc_decoder import ctc_greedy_decoder\n\n\nparser = argparse.ArgumentParser(description=""Infer words\' timestamps from logits\' dumps"")\nparser.add_argument(""--dumpfile"", required=True,\n                    help=""Path to the dumped logits file"")\nparser.add_argument(""--start_shift"", type=float, default=None, help=""Calibration start shift"")\nparser.add_argument(""--end_shift"", type=float, default=None, help=""Calibration end shift"")\nparser.add_argument(""--calibration_file"", default=None, help=""Calibration parameters filepath"")\nparser.add_argument(""--save_file"", default=""sample.csv"")\nargs = parser.parse_args()\ndump = pickle.load(open(args.dumpfile, ""rb""))\nresults = dump[""logits""]\nvocab = dump[""vocab""]\nstep_size = dump[""step_size""]\nstart_shift = args.start_shift\nend_shift = args.end_shift\nsave_file = args.save_file\ncalibration_file = args.calibration_file\n\nif start_shift is None and end_shift is None:\n  if calibration_file is None:\n    print(\'Warning: no calibration parameters were provided, using zeros instead\')\n    start_shift, end_shift = 0, 0\n  else:\n    with open(calibration_file) as calib:\n      line = calib.readline().split()\n      start_shift = float(line[0])\n      end_shift = float(line[1])\n\n# suppose CTC blank symbol is appended to the end of vocab\nblank_idx = len(vocab)\n\nwith open(save_file, ""w"") as csv_file:\n  writer = csv.writer(csv_file, delimiter=\',\')\n  writer.writerow([""wav_filename"", ""transcript"", ""start_time"", ""end_time""])\n\n  for r in results:\n    letters, starts, ends = ctc_greedy_decoder(results[r], vocab, step_size, 28, start_shift, end_shift)\n    writer.writerow([r, letters,\n                     \' \'.join([\'{:.5f}\'.format(f) for f in starts]),\n                     \' \'.join([\'{:.5f}\'.format(f) for f in ends])])\n\n  print(""Results written to : {}"".format(save_file))\n\n'"
scripts/get_best_accuracy.py,0,"b'\'\'\'\nReturn the best evaluation accuracy from a file\noutput-ed by the sentiment analysis model\n\'\'\'\nimport sys\n\ndef get_best_accuracy(output_file):\n\toutput = open(output_file, \'r\')\n\tkeyword = ""***     EVAL Accuracy: ""\n\tbest_acc = 0.0\n\tloss, stat, step = \'\', \'\', \'\'\n\tget_stat = False\n\tn = len(keyword)\n\tm = len(""***     Validation loss: "")\n\tlast = \'\'\n\tget_step = False\n\tfor line in output.readlines():\n\t\tline = line.strip()\n\t\tif get_stat:\n\t\t\tstat = line\n\t\t\tget_stat = False\n\t\t\tget_step = True\n\t\telif get_step:\n\t\t\tstep = line\n\t\t\tget_step = False\n\t\telse:\n\t\t\tidx = line.find(keyword)\n\t\t\tif idx != -1:\n\t\t\t\tacc = float(line[n:])\n\t\t\t\tif acc > best_acc:\n\t\t\t\t\tbest_acc = acc\n\t\t\t\t\tloss = last\n\t\t\t\t\tget_stat = True\n\t\t\tlast = line\n\n\n\tprint(""***     Best accuracy:"", str(best_acc))\n\tprint(loss)\n\tprint(stat)\n\tprint(step)\n\nif __name__ == \'__main__\':\n\tif len(sys.argv) < 2:\n\t\traise ValueError(\'No output file provided to analyze\')\n\toutput_file = sys.argv[1]\n\tget_best_accuracy(output_file)'"
scripts/import_librivox.py,0,"b'#!/usr/bin/env python\n\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n#\n# Copyright (c) 2018 Mozilla Corporation\n\n\nfrom __future__ import absolute_import, division, print_function\n\n# Make sure we can import stuff from util/\n# This script needs to be run from the root of the DeepSpeech repository\nimport sys\nimport os\nsys.path.insert(1, os.path.join(sys.path[0], \'..\'))\n\nimport codecs\nimport fnmatch\nimport pandas\nimport tqdm\nimport subprocess\nimport tarfile\nimport unicodedata\n\nfrom sox import Transformer\nimport urllib\nfrom tensorflow.python.platform import gfile\n\ndef _maybe_download(fname, data_dir, data_url):\n  data_path = os.path.join(data_dir, fname)\n  if not os.path.exists(data_path):\n    print(""Can\'t find \'{}\'. Downloading..."".format(data_path))\n    urllib.request.urlretrieve(data_url, filename=data_path + \'.tmp\')\n    os.rename(data_path + \'.tmp\', data_path)\n  else:\n    print(""Skipping file \'{}\'"".format(data_path))\n  return data_path\n\ndef _download_and_preprocess_data(data_dir):\n  # Conditionally download data to data_dir\n  print(""Downloading Librivox data set (55GB) into {} if not already present..."".format(data_dir))\n  with tqdm.tqdm(total=7) as bar:\n    TRAIN_CLEAN_100_URL = ""http://www.openslr.org/resources/12/train-clean-100.tar.gz""\n    TRAIN_CLEAN_360_URL = ""http://www.openslr.org/resources/12/train-clean-360.tar.gz""\n    TRAIN_OTHER_500_URL = ""http://www.openslr.org/resources/12/train-other-500.tar.gz""\n\n    DEV_CLEAN_URL = ""http://www.openslr.org/resources/12/dev-clean.tar.gz""\n    DEV_OTHER_URL = ""http://www.openslr.org/resources/12/dev-other.tar.gz""\n\n    TEST_CLEAN_URL = ""http://www.openslr.org/resources/12/test-clean.tar.gz""\n    TEST_OTHER_URL = ""http://www.openslr.org/resources/12/test-other.tar.gz""\n\n    def filename_of(x): return os.path.split(x)[1]\n    train_clean_100 = _maybe_download(filename_of(TRAIN_CLEAN_100_URL), data_dir, TRAIN_CLEAN_100_URL)\n    bar.update(1)\n    train_clean_360 = _maybe_download(filename_of(TRAIN_CLEAN_360_URL), data_dir, TRAIN_CLEAN_360_URL)\n    bar.update(1)\n    train_other_500 = _maybe_download(filename_of(TRAIN_OTHER_500_URL), data_dir, TRAIN_OTHER_500_URL)\n    bar.update(1)\n\n    dev_clean = _maybe_download(filename_of(DEV_CLEAN_URL), data_dir, DEV_CLEAN_URL)\n    bar.update(1)\n    dev_other = _maybe_download(filename_of(DEV_OTHER_URL), data_dir, DEV_OTHER_URL)\n    bar.update(1)\n\n    test_clean = _maybe_download(filename_of(TEST_CLEAN_URL), data_dir, TEST_CLEAN_URL)\n    bar.update(1)\n    test_other = _maybe_download(filename_of(TEST_OTHER_URL), data_dir, TEST_OTHER_URL)\n    bar.update(1)\n\n  # Conditionally extract LibriSpeech data\n  # We extract each archive into data_dir, but test for existence in\n  # data_dir/LibriSpeech because the archives share that root.\n  print(""Extracting librivox data if not already extracted..."")\n  with tqdm.tqdm(total=7) as bar:\n    LIBRIVOX_DIR = ""LibriSpeech""\n    work_dir = os.path.join(data_dir, LIBRIVOX_DIR)\n\n    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""train-clean-100""), train_clean_100)\n    bar.update(1)\n    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""train-clean-360""), train_clean_360)\n    bar.update(1)\n    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""train-other-500""), train_other_500)\n    bar.update(1)\n\n    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""dev-clean""), dev_clean)\n    bar.update(1)\n    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""dev-other""), dev_other)\n    bar.update(1)\n\n    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""test-clean""), test_clean)\n    bar.update(1)\n    _maybe_extract(data_dir, os.path.join(LIBRIVOX_DIR, ""test-other""), test_other)\n    bar.update(1)\n\n  # Convert FLAC data to wav, from:\n  # data_dir/LibriSpeech/split/1/2/1-2-3.flac\n  # to:\n  # data_dir/LibriSpeech/split-wav/1-2-3.wav\n  #\n  # And split LibriSpeech transcriptions, from:\n  # data_dir/LibriSpeech/split/1/2/1-2.trans.txt\n  # to:\n  # data_dir/LibriSpeech/split-wav/1-2-0.txt\n  # data_dir/LibriSpeech/split-wav/1-2-1.txt\n  # data_dir/LibriSpeech/split-wav/1-2-2.txt\n  # ...\n  print(""Converting FLAC to WAV and splitting transcriptions..."")\n  with tqdm.tqdm(total=7) as bar:\n    train_100 = _convert_audio_and_split_sentences(work_dir, ""train-clean-100"", ""train-clean-100-wav"")\n    bar.update(1)\n    train_360 = _convert_audio_and_split_sentences(work_dir, ""train-clean-360"", ""train-clean-360-wav"")\n    bar.update(1)\n    train_500 = _convert_audio_and_split_sentences(work_dir, ""train-other-500"", ""train-other-500-wav"")\n    bar.update(1)\n\n    dev_clean = _convert_audio_and_split_sentences(work_dir, ""dev-clean"", ""dev-clean-wav"")\n    bar.update(1)\n    dev_other = _convert_audio_and_split_sentences(work_dir, ""dev-other"", ""dev-other-wav"")\n    bar.update(1)\n\n    test_clean = _convert_audio_and_split_sentences(work_dir, ""test-clean"", ""test-clean-wav"")\n    bar.update(1)\n    test_other = _convert_audio_and_split_sentences(work_dir, ""test-other"", ""test-other-wav"")\n    bar.update(1)\n\n  # Write sets to disk as CSV files\n  train_100.to_csv(os.path.join(data_dir, ""librivox-train-clean-100.csv""), index=False)\n  train_360.to_csv(os.path.join(data_dir, ""librivox-train-clean-360.csv""), index=False)\n  train_500.to_csv(os.path.join(data_dir, ""librivox-train-other-500.csv""), index=False)\n\n  dev_clean.to_csv(os.path.join(data_dir, ""librivox-dev-clean.csv""), index=False)\n  dev_other.to_csv(os.path.join(data_dir, ""librivox-dev-other.csv""), index=False)\n\n  test_clean.to_csv(os.path.join(data_dir, ""librivox-test-clean.csv""), index=False)\n  test_other.to_csv(os.path.join(data_dir, ""librivox-test-other.csv""), index=False)\n\ndef _maybe_extract(data_dir, extracted_data, archive):\n  # If data_dir/extracted_data does not exist, extract archive in data_dir\n  if not gfile.Exists(os.path.join(data_dir, extracted_data)):\n    tar = tarfile.open(archive)\n    tar.extractall(data_dir)\n    tar.close()\n\ndef _convert_audio_and_split_sentences(extracted_dir, data_set, dest_dir):\n  source_dir = os.path.join(extracted_dir, data_set)\n  target_dir = os.path.join(extracted_dir, dest_dir)\n\n  if not os.path.exists(target_dir):\n    os.makedirs(target_dir)\n\n  # Loop over transcription files and split each one\n  #\n  # The format for each file 1-2.trans.txt is:\n  # 1-2-0 transcription of 1-2-0.flac\n  # 1-2-1 transcription of 1-2-1.flac\n  # ...\n  #\n  # Each file is then split into several files:\n  # 1-2-0.txt (contains transcription of 1-2-0.flac)\n  # 1-2-1.txt (contains transcription of 1-2-1.flac)\n  # ...\n  #\n  # We also convert the corresponding FLACs to WAV in the same pass\n  files = []\n  for root, dirnames, filenames in os.walk(source_dir):\n    for filename in fnmatch.filter(filenames, \'*.trans.txt\'):\n      trans_filename = os.path.join(root, filename)\n      with codecs.open(trans_filename, ""r"", ""utf-8"") as fin:\n        for line in fin:\n          # Parse each segment line\n          first_space = line.find("" "")\n          seqid, transcript = line[:first_space], line[first_space+1:]\n\n          # We need to do the encode-decode dance here because encode\n          # returns a bytes() object on Python 3, and text_to_char_array\n          # expects a string.\n          transcript = unicodedata.normalize(""NFKD"", transcript) \\\n                      .encode(""ascii"", ""ignore"")   \\\n                      .decode(""ascii"", ""ignore"")\n\n          transcript = transcript.lower().strip()\n\n          # Convert corresponding FLAC to a WAV\n          flac_file = os.path.join(root, seqid + "".flac"")\n          wav_file = os.path.join(target_dir, seqid + "".wav"")\n          if not os.path.exists(wav_file):\n            Transformer().build(flac_file, wav_file)\n          wav_filesize = os.path.getsize(wav_file)\n\n          files.append((os.path.abspath(wav_file), wav_filesize, transcript))\n\n  return pandas.DataFrame(data=files, columns=[""wav_filename"", ""wav_filesize"", ""transcript""])\n\nif __name__ == ""__main__"":\n  _download_and_preprocess_data(sys.argv[1])\n'"
scripts/nsr_create_syn_train_csv.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport string\nimport os\nimport pandas as pd\n\nif __name__ == \'__main__\':\n  synthetic_data_root = ""/data/speech/librispeech-syn/""\n  synthetic_data_sample = synthetic_data_root + ""{{}}/sample_step0_{}_syn.wav""\n\n  in_char = ""\\""\'\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d\xc3\xa0\xc3\xa2\xc3\xa8\xc3\xa9\xc3\xaa\xc3\xbc""\n  out_char = ""\'\'\'\'\'aaeeeu""\n  punctuation = string.punctuation.replace(""\'"", """")\n  table = str.maketrans(in_char, out_char, punctuation)\n\n  def _normalize_transcript(text):\n    """"""Parses the transcript to remove punctation, lowercase all characters, and\n       all non-ascii characters\n\n    Args:\n      text: the string to parse\n\n    Returns:\n      text: the normalized text\n    """"""\n    text = text.translate(table)\n    text = text.lower()\n    text = text.strip()\n    return text\n\n  names = [""wav_filename"", ""wav_filesize"", ""transcript""]\n\n  generated_files = pd.read_csv(\n      ""generate.csv"", encoding=\'utf-8\', sep=\'\\x7c\',\n      header=None, quoting=3, names=names)\n  num_files = len(generated_files)\n  for i, row in enumerate(generated_files.itertuples()):\n    generated_files.iat[i, 0] = synthetic_data_sample.format(i)\n    line = _normalize_transcript(generated_files.iat[i, 2])\n    generated_files.iat[i, 1] = -1\n    generated_files.iat[i, 2] = line\n    if i % int(num_files/10) == 0:\n      print(""Processed {} out of {}"".format(i, num_files))\n  generated_files.to_csv(\n      os.path.join(synthetic_data_root, ""synthetic_data.csv""), encoding=\'utf-8\',\n      sep=\',\', quoting=3, index=False)\n'"
scripts/tacotron_gst_combine_csv.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nif __name__ == \'__main__\':\n  data_root = ""/data/speech/MAILABS""\n  sub_dirs = [""en_US/by_book/male/elliot_miller/hunters_space"",\n              ""en_US/by_book/male/elliot_miller/pink_fairy_book"",\n              ""en_US/by_book/male/elliot_miller/pirates_of_ersatz"",\n              ""en_US/by_book/male/elliot_miller/poisoned_pen"",\n              ""en_US/by_book/male/elliot_miller/silent_bullet"",\n              ""en_US/by_book/female/mary_ann/northandsouth"",\n              ""en_US/by_book/female/mary_ann/midnight_passenger"",\n              ""en_US/by_book/female/judy_bieber/dorothy_and_wizard_oz"",\n              ""en_US/by_book/female/judy_bieber/emerald_city_of_oz"",\n              ""en_US/by_book/female/judy_bieber/ozma_of_oz"",\n              ""en_US/by_book/female/judy_bieber/rinkitink_in_oz"",\n              ""en_US/by_book/female/judy_bieber/sky_island"",\n              ""en_US/by_book/female/judy_bieber/the_master_key"",\n              ""en_US/by_book/female/judy_bieber/the_sea_fairies""]\n\n  # Check to make sure all the csvs can be found\n  while True:\n    check = 0\n    for sub_dir in sub_dirs:\n      csv = os.path.join(data_root, sub_dir, ""metadata.csv"")\n      if not os.path.isfile(csv):\n        print((""{} cannot be found. Please ensure that you have""\n               ""entered the correct directory where you extracted the MAILABS""\n               ""dataset"").format(csv))\n        break\n      else:\n        check += 1\n    if check == len(sub_dirs):\n      break\n    data_root = input(""Please input where you extracted the MAILABS US dataset: "")\n\n\n  # Load all csvs\n  names = [""1"", ""2"", ""3""]\n  _files = None\n  for sub_dir in sub_dirs:\n    csv = os.path.join(data_root, sub_dir, ""metadata.csv"")\n    files = pd.read_csv(\n        csv, encoding=\'utf-8\', sep=\'\\x7c\', header=None, quoting=3, names=names)\n    files[\'1\'] = sub_dir + \'/wavs/\' + files[\'1\'].astype(str)\n    if _files is None:\n      _files = files\n    else:\n      _files = _files.append(files)\n\n  # Optionally split data into train and validation sets\n  num_files = _files.shape[0]\n  np.random.shuffle(_files.values)\n\n  # Option 1: Take x% for train and 100-x % for val\n  # x = 0.8\n  # train, val = np.split(_files, [int(num_files/10.*x)])\n\n  # Option 2: Take x files for val, and rest for train\n  # x = 32\n  # train = _files[:-x]\n  # val = _files[-x:]\n\n  # Option 3: Don\'t have a validation set\n  train = _files\n  val = None\n\n  # Save new csvs\n  train_csv = os.path.join(data_root, ""train.csv"")\n  val_csv = os.path.join(data_root, ""val.csv"")\n\n  train.to_csv(\n      train_csv, encoding=\'utf-8\', sep=\'\\x7c\',\n      header=None, quoting=3, index=False)\n  if val:\n    val.to_csv(\n        val_csv, encoding=\'utf-8\', sep=\'\\x7c\',\n        header=None, quoting=3, index=False)\n\n  print(""Change dataset_location in tacotron_gst.py to {}"".format(data_root))\n'"
scripts/tacotron_gst_create_infer_csv.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nif __name__ == \'__main__\':\n  MAILABS_data_root = ""/data/speech/MAILABS""\n  libri_data_root = ""/data/speech/librispeech""\n\n  libri_csvs = [""librivox-train-clean-100.csv"",\n                ""librivox-train-clean-360.csv"",\n                ""librivox-train-other-500.csv""]\n  sub_dirs = [""en_US/by_book/male/elliot_miller/hunters_space"",\n              ""en_US/by_book/male/elliot_miller/pink_fairy_book"",\n              ""en_US/by_book/male/elliot_miller/pirates_of_ersatz"",\n              ""en_US/by_book/male/elliot_miller/poisoned_pen"",\n              ""en_US/by_book/male/elliot_miller/silent_bullet"",\n              ""en_US/by_book/female/mary_ann/northandsouth"",\n              ""en_US/by_book/female/mary_ann/midnight_passenger"",\n              ""en_US/by_book/female/judy_bieber/dorothy_and_wizard_oz"",\n              ""en_US/by_book/female/judy_bieber/emerald_city_of_oz"",\n              ""en_US/by_book/female/judy_bieber/ozma_of_oz"",\n              ""en_US/by_book/female/judy_bieber/rinkitink_in_oz"",\n              ""en_US/by_book/female/judy_bieber/sky_island"",\n              ""en_US/by_book/female/judy_bieber/the_master_key"",\n              ""en_US/by_book/female/judy_bieber/the_sea_fairies""]\n\n  # Check to make sure all the csvs can be found\n  while True:\n    check = 0\n    for sub_dir in sub_dirs:\n      csv = os.path.join(MAILABS_data_root, sub_dir, ""metadata.csv"")\n      if not os.path.isfile(csv):\n        print((""{} cannot be found. Please ensure that you have""\n               ""entered the correct directory where you extracted the MAILABS""\n               ""dataset"").format(csv))\n        break\n      else:\n        check += 1\n    if check == len(sub_dirs):\n      break\n    MAILABS_data_root = input(""Please input where you extracted the MAILABS US""\n                              "" dataset: "")\n\n  while True:\n    check = 0\n    for csv_file in libri_csvs:\n      csv = os.path.join(libri_data_root, csv_file)\n      if not os.path.isfile(csv):\n        print((""{} cannot be found. Please ensure that you have""\n               ""entered the correct directory where you extracted the""\n               ""librispeech dataset"").format(csv))\n        break\n      else:\n        check += 1\n    if check == len(libri_csvs):\n      break\n    libri_data_root = input(""Please input where you extracted the librispeech""\n                            "" dataset: "")\n\n\n  # Load libri csvs\n  libri_files = None\n  for csv in libri_csvs:\n    csv = os.path.join(libri_data_root, csv)\n    file = pd.read_csv(csv, encoding=\'utf-8\', quoting=3)\n    if libri_files is None:\n      libri_files = file\n    else:\n      libri_files = libri_files.append(file)\n\n  # Load MAILABS csvs\n  MAILABS_files = None\n  names = [""1"", ""2"", ""3""]\n  for sub_dir in sub_dirs:\n    csv = os.path.join(MAILABS_data_root, sub_dir, ""metadata.csv"")\n    files = pd.read_csv(\n        csv, encoding=\'utf-8\', sep=\'\\x7c\', header=None, quoting=3, names=names)\n    files[\'1\'] = sub_dir + \'/wavs/\' + files[\'1\'].astype(str)\n    if MAILABS_files is None:\n      MAILABS_files = files\n    else:\n      MAILABS_files = MAILABS_files.append(files)\n\n  num_M_files = MAILABS_files.shape[0]\n  np.random.shuffle(MAILABS_files.values)\n\n  curr_M_i = 0\n  num_libri = libri_files.shape[0]\n\n  # Mix MAILABS wavs with libri transcripts\n  for i, row in enumerate(libri_files.itertuples()):\n    libri_files.iat[i, 0] = MAILABS_files.iloc[curr_M_i, 0]\n    libri_files.iat[i, 1] = -1\n    curr_M_i += 1\n    if curr_M_i >= num_M_files:\n      curr_M_i = 0\n    if i % int(num_libri/100) == 0:\n      print(""Processed {} out of {}"".format(i, num_libri))\n  libri_files.to_csv(\n      ""generate.csv"", encoding=\'utf-8\', sep=\'\\x7c\',\n      header=None, quoting=3, index=False)\n'"
scripts/tacotron_gst_create_syn_data.py,4,"b'# Copyright (c) 2018 NVIDIA Corporation\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom open_seq2seq.utils.utils import get_base_config, check_logdir,\\\n                                     create_model, deco_print\nfrom open_seq2seq.models.text2speech import save_audio\n\nif __name__ == \'__main__\':\n  # Define the command line arguments that one would pass to run.py here\n  config_file_path = ""example_configs/text2speech/tacotron_gst.py""\n  checkpoint_path = ""result/tacotron-gst-8gpu/logs/""\n  syn_save_dir = ""/data/speech/LibriSpeech-Syn/syn""\n\n  args_T2S = [""--config_file={}"".format(config_file_path),\n              ""--mode=infer"",\n              ""--logdir={}"".format(checkpoint_path),\n              ""--batch_size_per_gpu=256"",\n              ""--infer_output_file="",\n              ""--num_gpus=1"",\n              ""--use_horovod=False""]\n\n  # A simpler version of what run.py does. It returns the created model and\n  # its saved checkpoint\n  def get_model(args):\n    args, base_config, base_model, config_module = get_base_config(args)\n    checkpoint = check_logdir(args, base_config)\n    model = create_model(args, base_config, config_module, base_model, None)\n    return model, checkpoint\n\n  # A variant of iterate_data\n  def iterate_data(model, sess, verbose, num_steps=None):\n    # Helper function to save audio\n    def infer(outputs, i):\n      predicted_final_specs = outputs[1]\n      sequence_lengths = outputs[4]\n      for j in range(len(predicted_final_specs)):\n        predicted_final_spec = predicted_final_specs[j]\n        audio_length = sequence_lengths[j]\n\n        if audio_length > 2:\n          if ""both"" in model.get_data_layer().params[\'output_type\']:\n            predicted_mag_spec = outputs[5][j][:audio_length - 1, :]\n          else:\n            predicted_final_spec = predicted_final_spec[:audio_length - 1, :]\n            predicted_mag_spec = model.get_data_layer().get_magnitude_spec(\n                predicted_final_spec, is_mel=True)\n          save_audio(\n              predicted_mag_spec,\n              syn_save_dir,\n              0,\n              n_fft=model.get_data_layer().n_fft,\n              sampling_rate=model.get_data_layer().sampling_rate,\n              mode=""syn"",\n              number=i * batch_size + j,\n              save_format=""disk"",\n              gl_iters=4,\n              verbose=False\n          )\n        else:\n          print(""WARNING: An audio file was not saved, this will error out in""\n                ""future steps"")\n\n    total_time = 0.0\n    bench_start = model.params.get(\'bench_start\', 10)\n\n    size_defined = model.get_data_layer().get_size_in_samples() is not None\n    if size_defined:\n      dl_sizes = []\n\n    total_samples = []\n    fetches = []\n\n    # on horovod num_gpus is 1\n    for worker_id in range(model.num_gpus):\n      cur_fetches = [\n          model.get_data_layer(worker_id).input_tensors,\n          model.get_output_tensors(worker_id),\n      ]\n      if size_defined:\n        dl_sizes.append(model.get_data_layer(worker_id).get_size_in_samples())\n      try:\n        total_objects = 0.0\n        cur_fetches.append(model.get_num_objects_per_step(worker_id))\n      except NotImplementedError:\n        total_objects = None\n        deco_print(""WARNING: Can\'t compute number of objects per step, since ""\n                   ""train model does not define get_num_objects_per_step method."")\n      fetches.append(cur_fetches)\n      total_samples.append(0.0)\n\n    sess.run([model.get_data_layer(i).iterator.initializer\n              for i in range(model.num_gpus)])\n\n    step = 0\n    processed_batches = 0\n    if verbose:\n      if model.on_horovod:\n        ending = "" on worker {}"".format(model.hvd.rank())\n      else:\n        ending = """"\n\n    while True:\n      tm = time.time()\n      fetches_vals = {}\n      if size_defined:\n        fetches_to_run = {}\n        # removing finished data layers\n        for worker_id in range(model.num_gpus):\n          if total_samples[worker_id] < dl_sizes[worker_id]:\n            fetches_to_run[worker_id] = fetches[worker_id]\n        fetches_vals = sess.run(fetches_to_run)\n      else:\n        # if size is not defined we have to process fetches sequentially, so not\n        # to lose data when exception is thrown on one data layer\n        for worker_id, one_fetch in enumerate(fetches):\n          try:\n            fetches_vals[worker_id] = sess.run(one_fetch)\n          except tf.errors.OutOfRangeError:\n            continue\n\n      if step >= bench_start:\n        total_time += time.time() - tm\n\n      # looping over num_gpus. In Horovod case this loop is ""dummy"",\n      # since num_gpus = 1\n      for worker_id, fetches_val in fetches_vals.items():\n        inputs, outputs = fetches_val[:2]\n\n        if total_objects is not None:\n          total_objects += np.sum(fetches_val[-1])\n\n        # assuming any element of inputs[""source_tensors""] .shape[0] is batch size\n        batch_size = inputs[""source_tensors""][0].shape[0]\n        total_samples[worker_id] += batch_size\n\n        if size_defined:\n          # this data_layer is at the last batch with few more elements, cutting\n          if total_samples[worker_id] > dl_sizes[worker_id]:\n            last_batch_size = dl_sizes[worker_id] % batch_size\n            for key, value in inputs.items():\n              inputs[key] = model.clip_last_batch(value, last_batch_size)\n            outputs = model.clip_last_batch(outputs, last_batch_size)\n\n        infer(outputs, processed_batches)\n\n        processed_batches += 1\n\n\n      if verbose:\n        if size_defined:\n          data_size = int(np.sum(np.ceil(np.array(dl_sizes) / batch_size)))\n          if step == 0 or len(fetches_vals) == 0 or \\\n             (data_size > 10 and processed_batches % (data_size // 10) == 0):\n            deco_print(""Processed {}/{} batches{}"".format(\n                processed_batches, data_size, ending\n            ))\n        else:\n          deco_print(""Processed {} batches{}"".format(processed_batches, ending),\n                     end=\'\\r\')\n\n      if len(fetches_vals) == 0:\n        break\n      step += 1\n      # break early in the case of INT8 calibration\n      if num_steps is not None and step >= num_steps:\n        break\n\n    if verbose:\n      if step > bench_start:\n        deco_print(\n            ""Avg time per step{}: {:.3}s"".format(\n                ending, 1.0 * total_time / (step - bench_start)\n            ),\n        )\n        if total_objects is not None:\n          avg_objects = 1.0 * total_objects / total_time\n          deco_print(""Avg objects per second{}: {:.3f}"".format(ending,\n                                                               avg_objects))\n      else:\n        deco_print(""Not enough steps for benchmarking{}"".format(ending))\n\n  model_T2S, checkpoint_T2S = get_model(args_T2S)\n\n  # Create the session and load the checkpoints\n  sess_config = tf.ConfigProto(allow_soft_placement=True)\n  sess_config.gpu_options.allow_growth = True\n  sess = tf.InteractiveSession(config=sess_config)\n  saver_T2S = tf.train.Saver()\n  saver_T2S.restore(sess, checkpoint_T2S)\n\n  iterate_data(model_T2S, sess, True)\n'"
scripts/tacotron_save_spec.py,5,"b'%matplotlib inline\n# Replace the first box of Interactive_Infer_example.ipynb with this\n\nimport IPython\nimport librosa\n\nimport numpy as np\nimport scipy.io.wavfile as wave\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom open_seq2seq.utils.utils import deco_print, get_base_config, check_logdir,\\\n                                     create_logdir, create_model, get_interactive_infer_results\nfrom open_seq2seq.models.text2speech import save_audio\n\nargs_T2S = [""--config_file=Infer_T2S/config.py"",\n        ""--mode=interactive_infer"",\n        ""--logdir=Infer_T2S/"",\n        ""--batch_size_per_gpu=1"",\n]\n\n# A simpler version of what run.py does. It returns the created model and its saved checkpoint\ndef get_model(args, scope):\n    with tf.variable_scope(scope):\n        args, base_config, base_model, config_module = get_base_config(args)\n        checkpoint = check_logdir(args, base_config)\n        model = create_model(args, base_config, config_module, base_model, None)\n    return model, checkpoint\n\nmodel_T2S, checkpoint_T2S = get_model(args_T2S, ""T2S"")\n\n# Create the session and load the checkpoints\nsess_config = tf.ConfigProto(allow_soft_placement=True)\nsess_config.gpu_options.allow_growth = True\nsess = tf.InteractiveSession(config=sess_config)\nvars_T2S = {}\nfor v in tf.get_collection(tf.GraphKeys.VARIABLES):\n    if ""T2S"" in v.name:\n        vars_T2S[""/"".join(v.op.name.split(""/"")[1:])] = v\nsaver_T2S = tf.train.Saver(vars_T2S)\nsaver_T2S.restore(sess, checkpoint_T2S)\n\n# line = ""I was trained using Nvidia\'s Open Sequence to Sequence framework.""\n\n# Define the inference function\nn_fft = model_T2S.get_data_layer().n_fft\nsampling_rate = model_T2S.get_data_layer().sampling_rate\ndef infer(line):\n    print(""Input English"")\n    print(line)\n    \n    # Generate speech\n    results = get_interactive_infer_results(model_T2S, sess, model_in=[line])\n    audio_length = results[1][4][0]\n\n    if model_T2S.get_data_layer()._both:\n        prediction = results[1][5][0]\n\n    else:\n        prediction = results[1][1][0]\n\n    prediction = prediction[:audio_length-1,:]\n    mag_prediction = model_T2S.get_data_layer().get_magnitude_spec(prediction)\n\n    mag_prediction_squared = np.clip(mag_prediction, a_min=0, a_max=255)\n    mag_prediction_squared = mag_prediction_squared**1.5\n    mag_prediction_squared = np.square(mag_prediction_squared)\n\n    mel_basis = librosa.filters.mel(sr=22050, n_fft=1024, n_mels=80, htk=True, norm=None)\n    mel = np.dot(mel_basis, mag_prediction_squared.T)\n    mel = np.log(np.clip(mel, a_min=1e-5, a_max=None))\n    np.save(""spec2"", mel)\n\n    plt.imshow(mel)\n    plt.gca().invert_yaxis()\n    plt.show()\n\n    wav = save_audio(mag_prediction, ""unused"", ""unused"", sampling_rate=sampling_rate, save_format=""np.array"", n_fft=n_fft)\n    audio = IPython.display.Audio(wav, rate=sampling_rate)\n    print(""Generated Audio"")\n    IPython.display.display(audio)\n'"
scripts/wavenet_naive_infer.py,5,"b'# Replace the first box of Interactive_Infer_example.ipynb with this\n\nimport IPython\nimport librosa\n\nimport numpy as np\nimport scipy.io.wavfile as wave\nimport tensorflow as tf\n\nfrom open_seq2seq.utils.utils import deco_print, get_base_config, check_logdir,\\\n                                     create_logdir, create_model, get_interactive_infer_results\nfrom open_seq2seq.models.text2speech_wavenet import save_audio\n\nargs_T2S = [""--config_file=Infer_T2S_Wave/config.py"",\n        ""--mode=interactive_infer"",\n        ""--logdir=Infer_T2S_Wave/"",\n        ""--batch_size_per_gpu=1"",\n]\n\n# A simpler version of what run.py does. It returns the created model and its \n# saved checkpoint\ndef get_model(args, scope):\n    with tf.variable_scope(scope):\n        args, base_config, base_model, config_module = get_base_config(args)\n        checkpoint = check_logdir(args, base_config)\n        model = create_model(args, base_config, config_module, base_model, None)\n    return model, checkpoint\n\nmodel_T2S, checkpoint_T2S = get_model(args_T2S, ""T2S"")\n\n# Create the session and load the checkpoints\nsess_config = tf.ConfigProto(allow_soft_placement=True)\nsess_config.gpu_options.allow_growth = True\nsess = tf.InteractiveSession(config=sess_config)\n\nvars_T2S = {}\nfor v in tf.get_collection(tf.GraphKeys.VARIABLES):\n    if ""T2S"" in v.name:\n        vars_T2S[""/"".join(v.op.name.split(""/"")[1:])] = v\n\nsaver_T2S = tf.train.Saver(vars_T2S)\nsaver_T2S.restore(sess, checkpoint_T2S)\n\n# Define the inference function\nn_fft = model_T2S.get_data_layer().n_fft\nsampling_rate = model_T2S.get_data_layer().sampling_rate\ndef infer(line):\n    """"""\n    Infers one value at a time using a sliding window with width equal to the\n    receptive field. \n    """"""\n\n    print(""Input File"")\n    print(line) \n    \n    GET_SPEC_FROM_WAV = False\n    max_steps = 200000\n    receptive_field = 6139 # 3x10\n    \n    source = np.zeros([1, receptive_field])\n    src_length = np.full([1], receptive_field)\n    audio = []\n    spec_offset = 0\n    \n    if GET_SPEC_FROM_WAV: # get spectrogram from .wav file\n        file_name = str.encode(line)\n        spec, spec_length = model_T2S.get_data_layer(). \\\n            _parse_spectrogram_element(file_name)\n    \n    else: # get spectrogram from .npy file\n        spec = np.load(line + "".npy"").T\n        spec = np.repeat(spec, 256, axis=0)\n        spec_length = spec.shape[0]\n\n    spec = np.expand_dims(spec, axis=0)\n    spec_length = np.reshape(spec_length, [1])\n    \n    while(spec_offset < max_steps):\n        output = get_interactive_infer_results(\n            model_T2S, sess, \n            model_in=(source, src_length, spec, spec_length, spec_offset)\n        )\n        \n        predicted = output[-1][0]\n        audio.append(predicted)\n        \n        source[0][0] = predicted\n        source[0] = np.roll(source[0], -1)\n        \n        if spec_offset % 1000 == 0:\n            print(""Saving audio for step {}"".format(spec_offset))\n            wav = save_audio(\n                np.array(audio), ""result"", 0, \n                sampling_rate=sampling_rate, mode=""infer""\n            )\n            \n        spec_offset += 1\n'"
example_configs/image2label/alexnet_owt.py,20,"b'# pylint: skip-file\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders.cnn_encoder import CNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedDecoder\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.data import ImagenetDataLayer\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nimport tensorflow as tf\n\n\nbase_model = Image2Label\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_epochs"": 120,\n\n  ""num_gpus"": 4,\n  ""batch_size_per_gpu"": 256,\n  ""dtype"": tf.float32,\n\n  ""save_summaries_steps"": 2000,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 2000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 5000,\n  ""logdir"": ""experiments/alexnet-imagenet"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.90,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.04,\n    ""power"": 1.0,\n  },\n\n  ""initializer"": tf.variance_scaling_initializer,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0005,\n  },\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""encoder"": CNNEncoder,\n  ""encoder_params"": {\n    \'data_format\': \'channels_first\',\n    \'cnn_layers\': [\n      (tf.layers.conv2d, {\n        \'filters\': 64, \'kernel_size\': (11, 11),\n        \'strides\': (4, 4), \'padding\': \'VALID\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.max_pooling2d, {\n        \'pool_size\': (3, 3), \'strides\': (2, 2),\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 192, \'kernel_size\': (5, 5),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.max_pooling2d, {\n        \'pool_size\': (3, 3), \'strides\': (2, 2),\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 384, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 256, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 256, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.max_pooling2d, {\n        \'pool_size\': (3, 3), \'strides\': (2, 2),\n      }),\n    ],\n    \'fc_layers\': [\n      (tf.layers.dense, {\'units\': 4096, \'activation\': tf.nn.relu}),\n      (tf.layers.dropout, {\'rate\': 0.5}),\n      (tf.layers.dense, {\'units\': 4096, \'activation\': tf.nn.relu}),\n      (tf.layers.dropout, {\'rate\': 0.5}),\n    ],\n  },\n\n  ""decoder"": FullyConnectedDecoder,\n  ""decoder_params"": {\n    ""output_dim"": 1000,\n  },\n  ""loss"": CrossEntropyLoss,\n  ""data_layer"": ImagenetDataLayer,\n  ""data_layer_params"": {\n    ""data_dir"": ""data/tf-imagenet"",\n    ""image_size"": 227,\n    ""num_classes"": 1000,\n  },\n}'"
example_configs/image2label/cifar-nv.py,23,"b'# pylint: skip-file\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders.cnn_encoder import CNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedDecoder\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.data.image2label.image2label import CifarDataLayer\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nimport tensorflow as tf\n\n\nbase_model = Image2Label\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_epochs"": 200,\n\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 32,\n  ""dtype"": tf.float32,\n\n  ""save_summaries_steps"": 2000,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 2000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 5000,\n  ""logdir"": ""experiments/test-cifar"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.90,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.001,\n    ""power"": 1.0,\n  },\n\n  ""initializer"": tf.variance_scaling_initializer,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0002,\n  },\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""encoder"": CNNEncoder,\n  ""encoder_params"": {\n    \'data_format\': \'channels_first\',\n    \'cnn_layers\': [\n      # block 1\n      (tf.layers.conv2d, {\n        \'filters\': 128, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 128, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 128, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': None, \'use_bias\': False,\n      }),\n      (tf.layers.batch_normalization, {\'momentum\': 0.9, \'epsilon\': 0.0001}),\n      (tf.nn.relu, {}),\n      (tf.layers.max_pooling2d, {\n        \'pool_size\': 3, \'strides\': 2, \'padding\': \'SAME\',\n      }),\n      # block 2\n      (tf.layers.conv2d, {\n        \'filters\': 256, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 256, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 256, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': None, \'use_bias\': False,\n      }),\n      (tf.layers.batch_normalization, {\'momentum\': 0.9, \'epsilon\': 0.0001}),\n      (tf.nn.relu, {}),\n      (tf.layers.max_pooling2d, {\n        \'pool_size\': 3, \'strides\': 2, \'padding\': \'SAME\',\n      }),\n      # block 3\n      (tf.layers.conv2d, {\n        \'filters\': 320, \'kernel_size\': (3, 3),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n      (tf.layers.conv2d, {\n        \'filters\': 320, \'kernel_size\': (1, 1),\n        \'strides\': (1, 1), \'padding\': \'SAME\',\n        \'activation\': tf.nn.relu,\n      }),\n    ],\n  },\n\n  ""decoder"": FullyConnectedDecoder,\n  ""decoder_params"": {\n    ""output_dim"": 10,\n  },\n  ""loss"": CrossEntropyLoss,\n  ""data_layer"": CifarDataLayer,\n  ""data_layer_params"": {\n    ""data_dir"": ""data/cifar-10-batches-bin"",\n  },\n}\n'"
example_configs/image2label/resnet-50-v2-mp.py,2,"b'# pylint: skip-file\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders import ResNetEncoder\nfrom open_seq2seq.decoders import FullyConnectedDecoder\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.data import ImagenetDataLayer\nfrom open_seq2seq.optimizers.lr_policies import piecewise_constant, poly_decay\nimport tensorflow as tf\n\ndata_root = """"\nbase_model = Image2Label\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": True,\n  ""num_epochs"": 100,\n\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 128,\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n\n  ""save_summaries_steps"": 2000,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 2000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 5000,\n  ""logdir"": ""logs/rn50"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.90,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.4,  # 8 GPU-s ,\n    ""power"": 2,\n  },\n\n  ""initializer"": tf.variance_scaling_initializer,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0001,\n  },\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""encoder"": ResNetEncoder,\n  ""encoder_params"": {\n    \'resnet_size\': 50,\n    ""regularize_bn"": False,\n  },\n  ""decoder"": FullyConnectedDecoder,\n  ""decoder_params"": {\n    ""output_dim"": 1000,\n  },\n  ""loss"": CrossEntropyLoss,\n  ""data_layer"": ImagenetDataLayer,\n  ""data_layer_params"": {\n    ""data_dir"": data_root+""data"", # ""data"",\n    ""image_size"": 224,\n    ""num_classes"": 1000,\n  },\n}\n'"
example_configs/image2label/resnet-50-v2.py,3,"b'# pylint: skip-file\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders import ResNetEncoder\nfrom open_seq2seq.decoders import FullyConnectedDecoder\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.data import ImagenetDataLayer\nfrom open_seq2seq.optimizers.lr_policies import piecewise_constant\nimport tensorflow as tf\n\n\nbase_model = Image2Label\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_epochs"": 100,\n\n  ""num_gpus"": 8,\n  ""batch_size_per_gpu"": 32,\n  ""dtype"": tf.float32,\n\n  ""save_summaries_steps"": 2000,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 2000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 5000,\n  ""logdir"": ""experiments/resnet50-imagenet"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.90,\n  },\n  ""lr_policy"": piecewise_constant,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.1,\n    ""boundaries"": [30, 60, 80, 90],\n    ""decay_rates"": [0.1, 0.01, 0.001, 1e-4],\n  },\n\n  ""initializer"": tf.variance_scaling_initializer,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0001,\n  },\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""encoder"": ResNetEncoder,\n  ""encoder_params"": {\n    \'resnet_size\': 50,\n    ""regularize_bn"": False,\n  },\n  ""decoder"": FullyConnectedDecoder,\n  ""decoder_params"": {\n    ""output_dim"": 1000,\n  },\n  ""loss"": CrossEntropyLoss,\n  ""data_layer"": ImagenetDataLayer,\n  ""data_layer_params"": {\n    ""data_dir"": ""data/tf-imagenet"",\n    ""image_size"": 224,\n  },\n}\n'"
example_configs/image2label/resnet-50v2-adamw.py,2,"b'# pylint: skip-file\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders import ResNetEncoder\nfrom open_seq2seq.decoders import FullyConnectedDecoder\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.data import ImagenetDataLayer\n\nimport tensorflow as tf\n\ndata_root = """"\n\nbase_model = Image2Label\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"":  False, #True,\n  ""num_gpus"": 8,\n  ""batch_size_per_gpu"": 128,\n\n  ""num_epochs"": 100,\n\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n\n  ""save_summaries_steps"": 2000,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 10000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 50000,\n  ""logdir"": ""logs/rn50-adamw"",\n\n  ""optimizer"": ""AdamW"",\n  ""optimizer_params"": {\n    ""beta1"": 0.9,\n    ""beta2"": 0.999,\n    ""epsilon"": 1e-08,\n    ""weight_decay"": 0.1,\n  },\n\n  ""lr_policy"": tf.train.cosine_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.002, # 8 GPUs\n  },\n\n  ""initializer"": tf.variance_scaling_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""encoder"": ResNetEncoder,\n  ""encoder_params"": {\n    \'resnet_size\': 50,\n    ""regularize_bn"": False,\n  },\n  ""decoder"": FullyConnectedDecoder,\n  ""decoder_params"": {\n    ""output_dim"": 1000,\n  },\n  ""loss"": CrossEntropyLoss,\n  ""data_layer"": ImagenetDataLayer,\n  ""data_layer_params"": {\n    ""data_dir"": data_root+""data"", # ""data"",\n    ""image_size"": 224,\n    ""num_classes"": 1000,\n  },\n}\n'"
example_configs/image2label/resnet-50v2-nvgrad.py,1,"b'# pylint: skip-file\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders import ResNetEncoder\nfrom open_seq2seq.decoders import FullyConnectedDecoder\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.data import ImagenetDataLayer\nfrom open_seq2seq.optimizers.lr_policies import piecewise_constant,  poly_decay\nfrom open_seq2seq.optimizers.novograd  import NovoGrad\n\nimport tensorflow as tf\n\ndata_root =""""\nbase_model = Image2Label\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": True, # False, #\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 128,\n  ""iter_size"": 1,\n  ""num_epochs"": 100,\n\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n\n  ""save_summaries_steps"": 2000,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 10000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 5000,\n  ""logdir"": ""logs/rn50/nvgd_lr0.02_wd0.001"",\n\n  ""optimizer"": NovoGrad,\n  ""optimizer_params"": {\n    ""beta1"": 0.95,\n    ""beta2"": 0.98,\n    ""epsilon"": 1e-08,\n    ""weight_decay"": 0.004,\n    ""grad_averaging"": False\n  },\n\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.03,\n    ""power"": 2,\n  },\n\n  ""initializer"": tf.variance_scaling_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""encoder"": ResNetEncoder,\n  ""encoder_params"": {\n    \'resnet_size\': 50,\n    ""regularize_bn"": False,\n  },\n  ""decoder"": FullyConnectedDecoder,\n  ""decoder_params"": {\n    ""output_dim"": 1000,\n  },\n  ""loss"": CrossEntropyLoss,\n  ""data_layer"": ImagenetDataLayer,\n  ""data_layer_params"": {\n    ""data_dir"": data_root+""data"", # ""data"",\n    ""image_size"": 224,\n    ""num_classes"": 1000,\n  },\n}\n'"
example_configs/image2label/resnet_commands.py,2,"b'# pylint: skip-file\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders import ResNetEncoder\nfrom open_seq2seq.decoders import FullyConnectedDecoder\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.data import SpeechCommandsDataLayer\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nimport tensorflow as tf\n\n\nbase_model = Image2Label\n\ndataset_version = ""v1-12""\ndataset_location = ""data/speech_commands_v0.01""\n\nif dataset_version == ""v1-12"":\n  num_labels = 12\nelif dataset_version == ""v1-30"":\n  num_labels = 30\nelse: \n  num_labels = 35\n  dataset_location = ""data/speech_commands_v0.02""\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n\n  ""num_epochs"": 20,\n  ""batch_size_per_gpu"": 32,\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": 512.0,\n\n  ""save_summaries_steps"": 10000,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 1000,\n  ""eval_steps"": 100,\n  ""save_checkpoint_steps"": 10000,\n  ""logdir"": ""result/"" + dataset_version + ""-resnet"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.95,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.2,\n    ""power"": 2,\n  },\n\n  ""initializer"": tf.variance_scaling_initializer,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0001,\n  },\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""encoder"": ResNetEncoder,\n  ""encoder_params"": {\n    \'resnet_size\': 50,\n    ""regularize_bn"": False,\n  },\n  ""decoder"": FullyConnectedDecoder,\n  ""decoder_params"": {\n    ""output_dim"": num_labels,\n  },\n  ""loss"": CrossEntropyLoss,\n  ""data_layer"": SpeechCommandsDataLayer,\n  ""data_layer_params"": {\n    ""dataset_location"": dataset_location,\n    ""num_audio_features"": 120,\n    ""audio_length"": 120,\n    ""num_labels"": num_labels,\n    ""cache_data"": True,\n    ""augment_data"": True,\n    ""model_format"": ""resnet""\n  },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      dataset_version + ""-train.txt""\n    ],\n    ""shuffle"": True,\n    ""repeat"": True\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      dataset_version + ""-val.txt""\n    ],\n    ""shuffle"": False,\n    ""repeat"": False\n  },\n}\n'"
example_configs/image2label/resnet_commands_8gpu.py,2,"b'# pylint: skip-file\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders import ResNetEncoder\nfrom open_seq2seq.decoders import FullyConnectedDecoder\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.data import SpeechCommandsDataLayer\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nimport tensorflow as tf\n\n\nbase_model = Image2Label\n\ndataset_version = ""v1-12""\ndataset_location = ""/data/speech-commands/v1""\n\nif dataset_version == ""v1-12"":\n  num_labels = 12\nelif dataset_version == ""v1-30"":\n  num_labels = 30\nelse: \n  num_labels = 35\n  dataset_location = ""/data/speech-commands/v2""\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": True,\n  ""num_gpus"": 8,\n\n  ""num_epochs"": 100,\n  ""batch_size_per_gpu"": 32,\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": 512.0,\n\n  ""save_summaries_steps"": 10000,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 1000,\n  ""eval_steps"": 1000,\n  ""save_checkpoint_steps"": 10000,\n  ""logdir"": ""result/resnet_commands"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.95,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.2,\n    ""power"": 2,\n  },\n\n  ""initializer"": tf.variance_scaling_initializer,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0001,\n  },\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""encoder"": ResNetEncoder,\n  ""encoder_params"": {\n    \'resnet_size\': 50,\n    ""regularize_bn"": False,\n  },\n  ""decoder"": FullyConnectedDecoder,\n  ""decoder_params"": {\n    ""output_dim"": num_labels,\n  },\n  ""loss"": CrossEntropyLoss,\n  ""data_layer"": SpeechCommandsDataLayer,\n  ""data_layer_params"": {\n    ""dataset_location"": dataset_location,\n    ""num_audio_features"": 120,\n    ""audio_length"": 120,\n    ""num_labels"": num_labels,\n    ""cache_data"": True,\n    ""augment_data"": True,\n    ""model_format"": ""resnet""\n  },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      dataset_version + ""-train.txt""\n    ],\n    ""shuffle"": True,\n    ""repeat"": True\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      dataset_version + ""-val.txt""\n    ],\n    ""shuffle"": False,\n    ""repeat"": False\n  },\n}\n'"
example_configs/lm/lstm-test-small-cudnn.py,4,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import WKTDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WikiText-2-raw DATA]""\nprocessed_data_folder = \'wkt2-processed-data\'\n\nbase_model = LSTMLM\nbptt = 12\nsteps = 10\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""use_horovod"": False,\n  ""num_gpus"": 2,\n\n  ""batch_size_per_gpu"": 160,\n  ""num_epochs"": 1500,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""processed_data_folder"": processed_data_folder,\n  ""logdir"": ""LSTM-FP32-2GPU-SMALL"",\n  ""eval_steps"": steps * 2,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 9e-4\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""dtype"": tf.float32,\n  # ""dtype"": ""mixed"",\n  # ""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": True,\n    ""cudnn_rnn_type"": tf.contrib.cudnn_rnn.CudnnLSTM,\n    ""core_cell"": None,\n    ""core_cell_params"": {\n        ""num_units"": 128,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 2,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.6,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.6,\n    ""recurrent_keep_prob"": 0.7,\n    \'encoder_emb_keep_prob\': 0.37,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 64,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": False,\n    ""average_across_timestep"": True,\n    ""do_mask"": False,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""rand_start"": True,\n    ""shuffle"": False,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n    ""small"": True,\n  },\n}\neval_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""bptt"": bptt,\n    ""small"": True,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n    ""seed_tokens"": ""something The only game"",\n  },\n}\n'"
example_configs/lm/lstm-test-small-mixed.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import WKTDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WikiText-2-raw DATA]""\nprocessed_data_folder = \'wkt2-processed-folder\'\nbase_model = LSTMLM\nbptt = 12\nsteps = 40\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""processed_data_folder"": processed_data_folder,\n  ""use_horovod"": False,\n  ""num_gpus"": 2,\n\n  ""batch_size_per_gpu"": 160,\n  ""num_epochs"": 1500,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""logdir"": ""LSTM-FP32-2GPU-SMALL-MIXED"",\n  ""eval_steps"": steps * 2,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 9e-4\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  # ""dtype"": tf.float32,\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 128,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 2,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.6,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.6,\n    ""recurrent_keep_prob"": 0.7,\n    \'encoder_emb_keep_prob\': 0.37,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 64,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": False,\n    ""average_across_timestep"": True,\n    ""do_mask"": False,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""rand_start"": True,\n    ""shuffle"": False,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n    ""small"": True,\n  },\n}\neval_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""bptt"": bptt,\n    ""small"": True,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n    ""seed_tokens"": ""something The only game"",\n  },\n}\n'"
example_configs/lm/lstm-test-small.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import WKTDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WikiText-2-raw DATA]""\nprocessed_data_folder = \'wkt2-processed-data\'\n\nbase_model = LSTMLM\nbptt = 12\nsteps = 10\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""use_horovod"": False,\n  ""num_gpus"": 2,\n\n  ""batch_size_per_gpu"": 160,\n  ""num_epochs"": 1500,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""processed_data_folder"": processed_data_folder,\n  ""logdir"": ""LSTM-FP32-2GPU-SMALL"",\n  ""eval_steps"": steps * 2,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 9e-4\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""dtype"": tf.float32,\n  # ""dtype"": ""mixed"",\n  # ""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 128,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 2,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.6,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.6,\n    ""recurrent_keep_prob"": 0.7,\n    \'encoder_emb_keep_prob\': 0.37,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 64,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": False,\n    ""average_across_timestep"": True,\n    ""do_mask"": False,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""rand_start"": True,\n    ""shuffle"": False,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n    ""small"": True,\n  },\n}\neval_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""bptt"": bptt,\n    ""small"": True,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""processed_data_folder"": processed_data_folder,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n    ""seed_tokens"": ""something The only game"",\n  },\n}\n'"
example_configs/lm/lstm-wkt103-mixed.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import WKTDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import BasicSampledSequenceLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WikiText-103-raw DATA]""\nprocessed_data_folder = \'wkt103-processed-data\'\n\nbase_model = LSTMLM\nbptt = 96\nsteps = 40\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""use_horovod"": True,\n  ""num_gpus"": 8,\n\n  ""batch_size_per_gpu"": 224,\n  ""eval_batch_size_per_gpu"": 56,\n  ""num_epochs"": 1500,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""logdir"": ""LSTM-WKT103-MIXED"",\n  ""processed_data_folder"": processed_data_folder,\n  ""eval_steps"": steps * 4,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  # ""max_grad_norm"": 0.25,\n  # ""dtype"": tf.float32,\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 1024,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 3,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.85,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.85,\n    ""recurrent_keep_prob"": 0.7,\n    \'encoder_emb_keep_prob\': 0.8,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 320,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n    ""num_sampled"": 8192,\n  },\n\n  ""decoder"": FakeDecoder,\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": BasicSampledSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": False,\n    ""average_across_timestep"": True,\n    ""do_mask"": False,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""rand_start"": True,\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n  },\n}\neval_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""bptt"": bptt,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n    ""seed_tokens"": ""something The only game"",\n  },\n}\n'"
example_configs/lm/lstm-wkt2-fp32.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import WKTDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WikiText-2-raw DATA]""\nprocessed_data_folder = \'wkt2-processed-data\'\n\nbase_model = LSTMLM\nbptt = 96\nsteps = 40\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""use_horovod"": True,\n  ""num_gpus"": 2,\n\n  ""batch_size_per_gpu"": 160,\n  ""num_epochs"": 1500,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""logdir"": ""LSTM-WKT2-FP32"",\n  ""processed_data_folder"": processed_data_folder,\n  ""eval_steps"": steps * 2,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 4e-4\n  },\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  # ""max_grad_norm"": 0.25,\n  ""dtype"": tf.float32,\n  #""dtype"": ""mixed"",\n  #""automatic_loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 896,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 3,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.6,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.6,\n    ""recurrent_keep_prob"": 0.7,\n    \'encoder_emb_keep_prob\': 0.37,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 256,\n    ""num_tokens_gen"": 10,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": False,\n    ""average_across_timestep"": True,\n    ""do_mask"": False,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""rand_start"": True,\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n  },\n}\neval_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""bptt"": bptt,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": WKTDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""bptt"": bptt,\n    ""seed_tokens"": ""something The only game"",\n  },\n}\n'"
example_configs/speech2text/ds2_large_8gpus.py,4,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import DeepSpeech2Encoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_gpus"": 8,\n  ""batch_size_per_gpu"": 16,\n\n  ""num_epochs"": 50,\n\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 5000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 1000,\n  ""logdir"": ""experiments/librispeech"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.90,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.001,\n    ""power"": 0.5,\n  },\n  ""larc_params"": {\n    ""larc_eta"": 0.001,\n  },\n  ""dtype"": tf.float32,\n  # weight decay\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0005\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": DeepSpeech2Encoder,\n  ""encoder_params"": {\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [11, 41], ""stride"": [2, 2],\n        ""num_channels"": 32, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [11, 21], ""stride"": [1, 2],\n        ""num_channels"": 32, ""padding"": ""SAME""\n      },\n    ],\n    ""num_rnn_layers"": 5,\n    ""rnn_cell_dim"": 800,\n\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": ""cudnn_gru"",\n    ""rnn_unidirectional"": False,\n\n    ""row_conv"": False,\n\n    ""n_hidden"": 1600,\n\n    ""dropout_keep_prob"": 0.5,\n    ""activation_fn"": tf.nn.relu,\n    ""data_format"": ""channels_first"",\n  },\n\n  ""decoder"": FullyConnectedCTCDecoder,\n  ""decoder_params"": {\n    ""use_language_model"": False,\n\n    # params for decoding the sequence with language model\n    ""beam_width"": 512,\n    ""alpha"": 2.0,\n    ""beta"": 1.0,\n\n    ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n    ""lm_path"": ""language_model/4-gram.binary"",\n    ""trie_path"": ""language_model/trie.binary"",\n    ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n  },\n  ""loss"": CTCLoss,\n  ""loss_params"": {},\n}\n\ntrain_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""augmentation"": {\'time_stretch_ratio\': 0.05,\n                     \'noise_level_min\': -90,\n                     \'noise_level_max\': -60},\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""data/librispeech/librivox-train-clean-100.csv"",\n      ""data/librispeech/librivox-train-clean-360.csv"",\n      ""data/librispeech/librivox-train-other-500.csv"",\n    ],\n    ""max_duration"": 16.7,\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""data/librispeech/librivox-dev-clean.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/speech2text/ds2_large_8gpus_mp.py,3,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import DeepSpeech2Encoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n#data_root = ""[REPLACE THIS TO THE PATH WITH YOUR LIBRISPEECH DATA]""\ndata_root =  ""/raid/data""\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_gpus"": 8,\n  ""batch_size_per_gpu"": 16,\n  # ""max_steps"": 1000,\n  ""num_epochs"": 50,\n\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 5000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 10000,\n  ""logdir"": ""experiments/librispeech"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.90,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.001,\n    ""power"": 0.5,\n  },\n  ""larc_params"": {\n    ""larc_eta"": 0.001,\n  },\n  ""dtype"": ""mixed"",\n  # weight decay\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0005\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": DeepSpeech2Encoder,\n  ""encoder_params"": {\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [11, 41], ""stride"": [2, 2],\n        ""num_channels"": 32, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [11, 21], ""stride"": [1, 2],\n        ""num_channels"": 32, ""padding"": ""SAME""\n      },\n    ],\n    ""num_rnn_layers"": 5,\n    ""rnn_cell_dim"": 800,\n\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": ""cudnn_gru"",\n    ""rnn_unidirectional"": False,\n\n    ""row_conv"": False,\n\n    ""n_hidden"": 1600,\n\n    ""dropout_keep_prob"": 0.5,\n    ""activation_fn"": tf.nn.relu,\n    # ""data_format"": ""BCFT"", # ""channels_first"",\'BCTF\', \'BTFC\', \'BCFT\', \'BFTC\'\n  },\n\n  ""decoder"": FullyConnectedCTCDecoder,\n  ""decoder_params"": {\n    ""use_language_model"": False,\n\n    # params for decoding the sequence with language model\n    ""beam_width"": 512,\n    ""alpha"": 2.0,\n    ""beta"": 1.0,\n\n    ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n    ""lm_path"": ""language_model/4-gram.binary"",\n    ""trie_path"": ""language_model/trie.binary"",\n    ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n  },\n  ""loss"": CTCLoss,\n  ""loss_params"": {},\n}\n\ntrain_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""augmentation"": {\'time_stretch_ratio\': 0.05,\n                     \'noise_level_min\': -90,\n                     \'noise_level_max\': -60},\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      data_root + ""/librispeech/librivox-train-clean-100.csv"",\n      data_root + ""/librispeech/librivox-train-clean-360.csv"",\n      data_root + ""/librispeech/librivox-train-other-500.csv"",\n    ],\n    ""max_duration"": 16.7,\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      data_root + ""/librispeech/librivox-dev-clean.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/speech2text/ds2_medium_4gpus.py,3,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import DeepSpeech2Encoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\nbase_model = Speech2Text\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_gpus"": 4,\n  ""batch_size_per_gpu"": 32,\n\n  ""num_epochs"": 50,\n\n  ""save_summaries_steps"": 1000,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 10000,\n  ""eval_steps"": 10000,\n  ""save_checkpoint_steps"": 1000,\n  ""logdir"": ""experiments/ds2/base_000"",\n\n  ""optimizer"": ""Adam"",\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.0002,\n    ""power"": 0.5\n  },\n  # weight decay\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0005\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n\n  ""encoder"": DeepSpeech2Encoder,\n  ""encoder_params"": {\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [11, 41], ""stride"": [2, 2],\n        ""num_channels"": 32, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [11, 21], ""stride"": [1, 2],\n        ""num_channels"": 64, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [11, 21], ""stride"": [1, 2],\n        ""num_channels"": 96, ""padding"": ""SAME""\n      },\n    ],\n    ""num_rnn_layers"": 3,\n    ""rnn_cell_dim"": 1024,\n\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": ""cudnn_gru"",\n    ""rnn_unidirectional"": True,\n\n    ""row_conv"": True,\n    ""row_conv_width"": 8,\n\n    ""n_hidden"": 2048,\n\n    ""dropout_keep_prob"": 0.5,\n    ""activation_fn"": tf.nn.relu,\n    ""data_format"": ""channels_first"",\n  },\n\n  ""decoder"": FullyConnectedCTCDecoder,\n  ""decoder_params"": {\n    ""use_language_model"": False,\n\n    # params for decoding the sequence with language model\n    ""beam_width"": 512,\n    ""alpha"": 2.0,\n    ""beta"": 1.0,\n\n    ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n    ""lm_path"": ""language_model/4-gram.binary"",\n    ""trie_path"": ""language_model/trie.binary"",\n    ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n  },\n  ""loss"": CTCLoss,\n  ""loss_params"": {},\n}\n\ntrain_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""augmentation"": {\'time_stretch_ratio\': 0.05,\n                     \'noise_level_min\': -90,\n                     \'noise_level_max\': -60},\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""data/librispeech/librivox-train-clean-100.csv"",\n      ""data/librispeech/librivox-train-clean-360.csv"",\n      ""data/librispeech/librivox-train-other-500.csv""\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""data/librispeech/librivox-dev-clean.csv""\n    ],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/speech2text/ds2_small_1gpu.py,4,"b'# pylint: skip-file\nimport tensorflow as tf\n\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.encoders import DeepSpeech2Encoder\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.optimizers.lr_policies import exp_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_epochs"": 12,\n\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 32,\n\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 5000,\n  ""eval_steps"": 5000,\n  ""save_checkpoint_steps"": 1000,\n  ""logdir"": ""experiments/librispeech-quick"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.0001,\n    ""begin_decay_at"": 0,\n    ""decay_steps"": 5000,\n    ""decay_rate"": 0.9,\n    ""use_staircase_decay"": True,\n    ""min_lr"": 0.0,\n  },\n  ""dtype"": tf.float32,\n  # weight decay\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0005\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": DeepSpeech2Encoder,\n  ""encoder_params"": {\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [11, 41], ""stride"": [2, 2],\n        ""num_channels"": 32, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [11, 21], ""stride"": [1, 2],\n        ""num_channels"": 32, ""padding"": ""SAME""\n      }\n    ],\n    ""num_rnn_layers"": 2,\n    ""rnn_cell_dim"": 512,\n\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": ""cudnn_gru"",\n    ""rnn_unidirectional"": False,\n\n    ""row_conv"": False,\n\n    ""n_hidden"": 1024,\n\n    ""dropout_keep_prob"": 0.5,\n    ""activation_fn"": tf.nn.relu,\n    ""data_format"": ""channels_first"",\n  },\n\n  ""decoder"": FullyConnectedCTCDecoder,\n  ""decoder_params"": {\n    ""use_language_model"": False,\n\n    # params for decoding the sequence with language model\n    ""beam_width"": 512,\n    ""alpha"": 2.0,\n    ""beta"": 1.0,\n\n    ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n    ""lm_path"": ""language_model/4-gram.binary"",\n    ""trie_path"": ""language_model/trie.binary"",\n    ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n  },\n  ""loss"": CTCLoss,\n  ""loss_params"": {},\n}\n\ntrain_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 96,\n    ""input_type"": ""spectrogram"",\n    ""augmentation"": {\n      \'time_stretch_ratio\': 0.05,\n      \'noise_level_min\': -90,\n      \'noise_level_max\': -60\n    },\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""data/librispeech/librivox-train-clean-100.csv"",\n      ""data/librispeech/librivox-train-clean-360.csv"",\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 96,\n    ""input_type"": ""spectrogram"",\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""data/librispeech/librivox-dev-clean.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/speech2text/ds2_toy_config.py,4,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import DeepSpeech2Encoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  #""num_epochs"":  200,\n  ""max_steps"": 1000,\n\n  ""num_gpus"": 2,\n  ""batch_size_per_gpu"": 8,\n\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 100,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 500,\n  ""logdir"": ""ds2_log/toy"",\n\n  ""optimizer"": ""Momentum"",\n  ""optimizer_params"": {\n    ""momentum"": 0.90,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.001,\n    ""power"": 2,\n  },\n  ""larc_params"": {\n    ""larc_eta"": 0.001,\n  },\n  ""dtype"": tf.float32,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": DeepSpeech2Encoder,\n  ""encoder_params"": {\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [11, 41], ""stride"": [2, 2],\n        ""num_channels"": 32, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [11, 21], ""stride"": [1, 2],\n        ""num_channels"": 64, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [11, 21], ""stride"": [1, 2],\n        ""num_channels"": 96, ""padding"": ""SAME""\n      },\n    ],\n    ""data_format"": ""BFTC"",  #""channels_last"", ""channels_first"",\'BCTF\', \'BTFC\', \'BCFT\', \'BFTC\'\n    ""n_hidden"": 256,\n\n    ""rnn_cell_dim"": 256,\n    ""rnn_type"": ""gru"",\n    ""num_rnn_layers"": 1,\n    ""rnn_unidirectional"": False,\n    ""row_conv"": False,\n    ""row_conv_width"": 8,\n    ""use_cudnn_rnn"": True,\n\n    ""dropout_keep_prob"": 1.0,\n\n    ""initializer"": tf.contrib.layers.xavier_initializer,\n    ""initializer_params"": {\n      \'uniform\': False,\n    },\n    ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n\n  },\n\n  ""decoder"": FullyConnectedCTCDecoder,\n  ""decoder_params"": {\n    ""initializer"": tf.contrib.layers.xavier_initializer,\n    ""use_language_model"": False,\n\n    # params for decoding the sequence with language model\n    ""beam_width"": 64,\n    ""alpha"": 1.0,\n    ""beta"": 1.5,\n\n    ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n    ""lm_path"": ""language_model/4-gram.binary"",\n    ""trie_path"": ""language_model/trie.binary"",\n    ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n  },\n  ""loss"": CTCLoss,\n  ""loss_params"": {},\n}\n\ntrain_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 160,\n    ""input_type"": ""spectrogram"",\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/speech2text/jasper-Mini-for-Jetson.py,4,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data.speech2text.speech2text import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nfrom open_seq2seq.optimizers.novograd import NovoGrad\n\nresidual_dense = False  # Enable or disable Dense Residual\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 50,\n\n    ""num_gpus"": 1,\n    ""batch_size_per_gpu"": 32,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 2200,\n    ""eval_steps"": 2200,\n    ""save_checkpoint_steps"": 1100,\n    ""logdir"": ""jasper_log_folder"",\n    ""num_checkpoints"": 2,\n\n    ""optimizer"": NovoGrad,\n    ""optimizer_params"": {\n        ""beta1"": 0.95,\n        ""beta2"": 0.98,\n        ""epsilon"": 1e-08,\n        ""weight_decay"": 0.001,\n        ""grad_averaging"": False,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.02,\n        ""min_lr"": 1e-5,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""dtype"": tf.float32,\n    # ""loss_scaling"": ""Backoff"",\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 1,\n                ""kernel_size"": [11], ""stride"": [2],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"": [1]\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 3,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"": [1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 1,\n                ""kernel_size"": [29], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"": [2]\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"": [1]\n            }\n        ],\n\n        ""dropout_keep_prob"": 1.0,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": tf.nn.relu,\n        ""data_format"": ""channels_last"",\n        ""use_conv_mask"": True,\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n        ""infer_logits_to_pickle"": False,\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""norm_per_feature"": True,\n        ""window"": ""hanning"",\n        ""precompute_mel_basis"": True,\n        ""sample_freq"": 16000,\n        ""pad_to"": 16,\n        ""dither"": 1e-5,\n        ""backend"": ""librosa"",\n    },\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""augmentation"": {\n            \'n_freq_mask\': 2,\n            \'n_time_mask\': 2,\n            \'width_freq_mask\': 6,\n            \'width_time_mask\': 6,\n        },\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-train-clean-100.csv"",\n            ""/data/librispeech/librivox-train-clean-360.csv"",\n            ""/data/librispeech/librivox-train-other-500.csv""\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
example_configs/speech2text/jasper10x5_LibriSpeech_nvgrad.py,3,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data.speech2text.speech2text import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nfrom open_seq2seq.optimizers.novograd import NovoGrad\n\nresidual_dense = True # Enable or disable Dense Residual\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 400,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 32,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 2200,\n    ""eval_steps"": 2200,\n    ""save_checkpoint_steps"": 1100,\n    ""logdir"": ""jasper_log_folder"",\n    ""num_checkpoints"": 2,\n\n    ""optimizer"": NovoGrad,\n    ""optimizer_params"": {\n        ""beta1"": 0.95,\n        ""beta2"": 0.98,\n        ""epsilon"": 1e-08,\n        ""weight_decay"": 0.001,\n        ""grad_averaging"": False,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.02,\n        ""min_lr"": 1e-5,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""dtype"": ""mixed"",\n    ""loss_scaling"": ""Backoff"",\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [11], ""stride"": [2],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [29], ""stride"": [1],\n                ""num_channels"": 896, ""padding"": ""SAME"",\n                ""dilation"":[2], ""dropout_keep_prob"": 0.6,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.6,\n            }\n        ],\n\n        ""dropout_keep_prob"": 0.7,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": tf.nn.relu,\n        ""data_format"": ""channels_last"",\n        ""use_conv_mask"": True,\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n\n        # params for decoding the sequence with language model\n        # ""beam_width"": 2048,\n        # ""alpha"": 2.0,\n        # ""beta"": 1.5,\n\n        # ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n        # ""lm_path"": ""language_model/4-gram.binary"",\n        # ""trie_path"": ""language_model/trie.binary"",\n        # ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n\n        ""infer_logits_to_pickle"": False,\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""norm_per_feature"": True,\n        ""window"": ""hanning"",\n        ""precompute_mel_basis"": True,\n        ""sample_freq"": 16000,\n        ""pad_to"": 16,\n        ""dither"": 1e-5,\n        ""backend"": ""librosa""\n    },\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""augmentation"": {\n            \'speed_perturbation_ratio\': [0.9, 1., 1.1],\n        },\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-train-clean-100.csv"",\n            ""/data/librispeech/librivox-train-clean-360.csv"",\n            ""/data/librispeech/librivox-train-other-500.csv""\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
example_configs/speech2text/jasper10x5_LibriSpeech_nvgrad_masks.py,3,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data.speech2text.speech2text import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nfrom open_seq2seq.optimizers.novograd import NovoGrad\n\nresidual_dense = True # Enable or disable Dense Residual\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 400,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 32,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 2200,\n    ""eval_steps"": 2200,\n    ""save_checkpoint_steps"": 1100,\n    ""logdir"": ""jasper_log_folder"",\n    ""num_checkpoints"": 2,\n\n    ""optimizer"": NovoGrad,\n    ""optimizer_params"": {\n        ""beta1"": 0.95,\n        ""beta2"": 0.98,\n        ""epsilon"": 1e-08,\n        ""weight_decay"": 0.001,\n        ""grad_averaging"": False,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.02,\n        ""min_lr"": 1e-5,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""dtype"": ""mixed"",\n    ""loss_scaling"": ""Backoff"",\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [11], ""stride"": [2],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 5,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [29], ""stride"": [1],\n                ""num_channels"": 896, ""padding"": ""SAME"",\n                ""dilation"":[2], ""dropout_keep_prob"": 0.6,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.6,\n            }\n        ],\n\n        ""dropout_keep_prob"": 0.7,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": tf.nn.relu,\n        ""data_format"": ""channels_last"",\n        ""use_conv_mask"": True,\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n        ""infer_logits_to_pickle"": False,\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""norm_per_feature"": True,\n        ""window"": ""hanning"",\n        ""precompute_mel_basis"": True,\n        ""sample_freq"": 16000,\n        ""pad_to"": 16,\n        ""dither"": 1e-5,\n        ""backend"": ""librosa""\n    },\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""augmentation"": {\n            \'n_freq_mask\': 2,\n            \'n_time_mask\': 2,\n            \'width_freq_mask\': 6,\n            \'width_time_mask\': 6,\n        },\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-train-clean-100.csv"",\n            ""/data/librispeech/librivox-train-clean-360.csv"",\n            ""/data/librispeech/librivox-train-other-500.csv""\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
example_configs/speech2text/jasper_commands.py,3,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedSCDecoder\nfrom open_seq2seq.data import SpeechCommandsDataLayer\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\nbase_model = Image2Label\n\ndataset_version = ""v1-12""\ndataset_location = ""data/speech_commands_v0.01""\n\nif dataset_version == ""v1-12"":\n  num_labels = 12\nelif dataset_version == ""v1-30"":\n  num_labels = 30\nelse: \n  num_labels = 35\n  dataset_location = ""data/speech_commands_v0.02""\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": False,\n    ""num_epochs"": 20,\n\n    ""num_gpus"": 1,\n    ""batch_size_per_gpu"": 64,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 10000,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 1000,\n    ""eval_steps"": 100,\n    ""save_checkpoint_steps"": 10000,\n    ""logdir"": ""result/"" + dataset_version + ""-jasper"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.95,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.05,\n        ""min_lr"": 1e-5,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""regularizer"": tf.contrib.layers.l2_regularizer,\n    ""regularizer_params"": {\n        \'scale\': 0.001\n    },\n\n    ""dtype"": ""mixed"",\n    ""loss_scaling"": ""Backoff"",\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [11,1], ""stride"": [2,1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [11,1], ""stride"": [1,1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [11,1], ""stride"": [1,1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [13,1], ""stride"": [2,1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [13,1], ""stride"": [1,1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [13,1], ""stride"": [1,1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [17,1], ""stride"": [2,1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [17,1], ""stride"": [1,1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [17,1], ""stride"": [1,1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [21,1], ""stride"": [1,1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [21,1], ""stride"": [1,1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [25,1], ""stride"": [1,1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [25,1], ""stride"": [1,1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [29,1], ""stride"": [1,1],\n                ""num_channels"": 896, ""padding"": ""SAME"",\n                ""dilation"":[2,1], ""dropout_keep_prob"": 0.6,\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [1,1], ""stride"": [1,1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.6,\n            }\n        ],\n\n        ""dropout_keep_prob"": 0.7,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": tf.nn.relu,\n        ""data_format"": ""channels_last"",\n    },\n\n    ""decoder"": FullyConnectedSCDecoder,\n    ""decoder_params"": {\n        ""output_dim"": num_labels,\n    },\n\n    ""loss"": CrossEntropyLoss,\n    ""data_layer"": SpeechCommandsDataLayer,\n    ""data_layer_params"": {\n        ""dataset_location"": dataset_location,\n        ""num_audio_features"": 128,\n        ""audio_length"": 128,\n        ""num_labels"": num_labels,\n        ""cache_data"": True,\n        ""augment_data"": True,\n        ""model_format"": ""jasper""\n    },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      dataset_version + ""-train.txt""\n    ],\n    ""shuffle"": True,\n    ""repeat"": True\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      dataset_version + ""-val.txt""\n    ],\n    ""shuffle"": False,\n    ""repeat"": False\n  },\n}\n'"
example_configs/speech2text/jasper_commands_8gpu.py,3,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Image2Label\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedSCDecoder\nfrom open_seq2seq.data import SpeechCommandsDataLayer\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\nbase_model = Image2Label\n\ndataset_version = ""v1-12""\ndataset_location = ""/data/speech-commands/v1""\n\nif dataset_version == ""v1-12"":\n  num_labels = 12\nelif dataset_version == ""v1-30"":\n  num_labels = 30\nelse: \n  num_labels = 35\n  dataset_location = ""/data/speech-commands/v2""\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 200,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 64,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 10000,\n    ""print_loss_steps"": 100,\n    ""print_samples_steps"": 1000,\n    ""eval_steps"": 1000,\n    ""save_checkpoint_steps"": 10000,\n    ""logdir"": ""result/jasper_commands"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.95,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.05,\n        ""min_lr"": 1e-5,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""regularizer"": tf.contrib.layers.l2_regularizer,\n    ""regularizer_params"": {\n        \'scale\': 0.001\n    },\n\n    ""dtype"": ""mixed"",\n    ""loss_scaling"": ""Backoff"",\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [11,1], ""stride"": [2,1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [11,1], ""stride"": [1,1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [11,1], ""stride"": [1,1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [13,1], ""stride"": [2,1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [13,1], ""stride"": [1,1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [13,1], ""stride"": [1,1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [17,1], ""stride"": [2,1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [17,1], ""stride"": [1,1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [17,1], ""stride"": [1,1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.8,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [21,1], ""stride"": [1,1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [21,1], ""stride"": [1,1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [25,1], ""stride"": [1,1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 3,\n                ""kernel_size"": [25,1], ""stride"": [1,1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.7,\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [29,1], ""stride"": [1,1],\n                ""num_channels"": 896, ""padding"": ""SAME"",\n                ""dilation"":[2,1], ""dropout_keep_prob"": 0.6,\n            },\n            {\n                ""type"": ""conv2d"", ""repeat"": 1,\n                ""kernel_size"": [1,1], ""stride"": [1,1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1,1], ""dropout_keep_prob"": 0.6,\n            }\n        ],\n\n        ""dropout_keep_prob"": 0.7,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": tf.nn.relu,\n        ""data_format"": ""channels_last"",\n    },\n\n    ""decoder"": FullyConnectedSCDecoder,\n    ""decoder_params"": {\n        ""output_dim"": num_labels,\n    },\n\n    ""loss"": CrossEntropyLoss,\n    ""data_layer"": SpeechCommandsDataLayer,\n    ""data_layer_params"": {\n        ""dataset_location"": dataset_location,\n        ""num_audio_features"": 128,\n        ""audio_length"": 128,\n        ""num_labels"": num_labels,\n        ""cache_data"": True,\n        ""augment_data"": True,\n        ""model_format"": ""jasper""\n    },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      dataset_version + ""-train.txt""\n    ],\n    ""shuffle"": True,\n    ""repeat"": True\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 4,\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      dataset_version + ""-val.txt""\n    ],\n    ""shuffle"": False,\n    ""repeat"": False\n  },\n}\n'"
example_configs/speech2text/jca_large_8gpus.py,5,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import ListenAttendSpellEncoder\nfrom open_seq2seq.decoders import JointCTCAttentionDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import MultiTaskCTCEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nfrom open_seq2seq.decoders import ListenAttendSpellDecoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 50,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 64,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 1100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 200,\n    ""eval_steps"": 1100,\n    ""save_checkpoint_steps"": 1100,\n    ""logdir"": ""jca_log_folder"",\n\n    ""optimizer"": ""Adam"",\n    ""optimizer_params"": {\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 1e-3,\n        ""power"": 2.0,\n        ""min_lr"": 1e-5\n    },\n\n    ""max_grad_norm"": 1.0,\n\n    ""regularizer"": tf.contrib.layers.l2_regularizer,\n    ""regularizer_params"": {\n        \'scale\': 0.0001\n    },\n\n    #""dtype"": ""mixed"",\n    #""loss_scaling"": ""Backoff"",\n\n    ""dtype"": tf.float32,\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": ListenAttendSpellEncoder,\n    ""encoder_params"": {\n\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [11], ""stride"": [2],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 7,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [11], ""stride"": [2],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 4,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dropout_keep_prob"": 0.7,\n            },\n        ],\n\n        ""recurrent_layers"": [],\n\n        ""dropout_keep_prob"": 0.8,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n        ""data_format"": ""channels_last"",\n    },\n\n    ""decoder"": JointCTCAttentionDecoder,\n    ""decoder_params"": {\n\n        ""attn_decoder"": ListenAttendSpellDecoder,\n        ""attn_decoder_params"": {\n            ""tgt_emb_size"": 256,\n            ""pos_embedding"": True,\n\n            ""attention_params"": {\n                ""attention_dim"": 256,\n                ""attention_type"": ""chorowski"",\n                ""use_coverage"": True,\n                ""num_heads"": 1,\n                ""plot_attention"": True,\n\n            },\n\n            ""rnn_type"": ""lstm"",\n            ""hidden_dim"": 512,\n            ""num_layers"": 1,\n\n            ""dropout_keep_prob"": 0.8,\n        },\n\n        ""ctc_decoder"": FullyConnectedCTCDecoder,\n        ""ctc_decoder_params"": {\n            ""initializer"": tf.contrib.layers.xavier_initializer,\n            ""use_language_model"": False,\n        },\n\n        ""beam_search_params"": {\n            ""beam_width"": 4,\n        },\n\n        ""language_model_params"": {\n            # params for decoding the sequence with language model\n            ""use_language_model"": False,\n        },\n\n    },\n\n    ""loss"": MultiTaskCTCEntropyLoss,\n    ""loss_params"": {\n\n        ""seq_loss_params"": {\n            ""offset_target_by_one"": False,\n            ""average_across_timestep"": True,\n            ""do_mask"": True\n        },\n\n        ""ctc_loss_params"": {\n        },\n\n        ""lambda_value"": 0.25,\n    }\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-train-clean-100.csv"",\n            ""data/librispeech/librivox-train-clean-360.csv"",\n            ""data/librispeech/librivox-train-other-500.csv"",\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n        ""autoregressive"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n        ""autoregressive"": True,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n        ""autoregressive"": True,\n    },\n}\n'"
example_configs/speech2text/lstm_small_1gpu.py,4,"b'# pylint: skip-file\nimport os\nimport tensorflow as tf\n\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.encoders import DeepSpeech2Encoder\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.optimizers.lr_policies import exp_decay\n\n\nbase_model = Speech2Text\ndataset_location = os.path.expanduser(""~/datasets/speech/librispeech/"")\n\n### INPUT FEATURES CONFIG ####\n# input_type = ""spectrogram""\n# num_audio_features = 96\n\ninput_type = ""mfcc""\nnum_audio_features = 13  # primary MFCC coefficients\n\n\n### PREPROCESSING CACHING CONFIG ###\ntrain_cache_features = False\neval_cache_features = True\ncache_format = \'hdf5\'\ncache_regenerate = False\n\n### RNN CONFIG ####\nnum_rnn_layers = 2\nrnn_cell_dim = 512\nrnn_type = ""cudnn_lstm""\nrnn_unidirectional = True\n\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_epochs"": 50,\n\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 64,\n\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 250,\n  ""eval_steps"": 250,\n  ""save_checkpoint_steps"": 250,\n  ""logdir"": ""logs/librispeech-"" +\n            rnn_type + str(num_rnn_layers) + ""x"" + str(rnn_cell_dim) + ""-"" +\n            input_type + str(num_audio_features),\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.001,\n    ""begin_decay_at"": 0,\n    ""decay_steps"": 500,\n    ""decay_rate"": 0.9,\n    ""use_staircase_decay"": True,\n    ""min_lr"": 1e-8,\n  },\n  # ""dtype"": tf.float32,\n  ""dtype"": ""mixed"",\n  ""max_grad_norm"": 0.25,\n  ""loss_scaling"": ""Backoff"",\n\n  # weight decay\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 0.0005\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": DeepSpeech2Encoder,\n  ""encoder_params"": {\n\n    # CONV layers\n    ""conv_layers"": [  # no CONV layers needed? with MFCC input\n    ],\n\n    # RNN layers\n    ""num_rnn_layers"": num_rnn_layers,\n    ""rnn_cell_dim"": rnn_cell_dim,\n\n    ""use_cudnn_rnn"": True if \'cudnn\' in rnn_type else False,\n    ""rnn_type"": rnn_type,\n    ""rnn_unidirectional"": rnn_unidirectional,\n\n    ""row_conv"": False,\n\n    # FC layers\n    ""n_hidden"": 512,\n\n    ""dropout_keep_prob"": 0.5,\n    ""activation_fn"": tf.nn.relu,\n    ""data_format"": ""channels_first"",\n  },\n\n  ""decoder"": FullyConnectedCTCDecoder,\n  ""decoder_params"": {\n    ""use_language_model"": False,\n    # params for decoding the sequence with language model\n    ""beam_width"": 512,\n    ""alpha"": 2.0,\n    ""beta"": 1.0,\n\n    ""decoder_library_path"":\n      ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n    ""lm_path"": ""language_model/4-gram.binary"",\n    ""trie_path"": ""language_model/trie.binary"",\n    ""alphabet_config_path"":\n      ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n  },\n  ""loss"": CTCLoss,\n  ""loss_params"": {},\n}\n\ntrain_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""cache_features"": train_cache_features,\n    ""cache_format"": cache_format,\n    ""cache_regenerate"": cache_regenerate,\n    ""num_audio_features"": num_audio_features,\n    ""input_type"": input_type,\n\n    ""augmentation"": {\n      \'time_stretch_ratio\': 0.05,\n      \'noise_level_min\': -90,\n      \'noise_level_max\': -60\n    },\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      os.path.join(dataset_location, ""librivox-train-clean-100.csv""),\n      os.path.join(dataset_location, ""librivox-train-clean-360.csv""),\n    ],\n    ""max_duration"": 16.7,\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer"": Speech2TextDataLayer,\n  ""data_layer_params"": {\n    ""cache_features"": eval_cache_features,\n    ""cache_format"": cache_format,\n    ""cache_regenerate"": cache_regenerate,\n    ""num_audio_features"": num_audio_features,\n    ""input_type"": input_type,\n\n    ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    ""dataset_files"": [\n      os.path.join(dataset_location, ""librivox-dev-clean.csv""),\n    ],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/speech2text/quartznet15x5_LibriSpeech.py,4,"b'# pylint: skip-file\n# QuartzNet paper: https://arxiv.org/abs/1910.10261\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data.speech2text.speech2text import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import cosine_decay\nfrom open_seq2seq.optimizers.novograd import NovoGrad\n\nresidual_dense = False  # Enable or disable Dense Residual\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 400,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 32,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 2200,\n    ""eval_steps"": 2200,\n    ""save_checkpoint_steps"": 1100,\n    ""logdir"": ""jasper_log_folder"",\n    ""num_checkpoints"": 2,\n\n    ""optimizer"": NovoGrad,\n    ""optimizer_params"": {\n        ""beta1"": 0.95,\n        ""beta2"": 0.5,\n        ""epsilon"": 1e-08,\n        ""weight_decay"": 0.001,\n        ""grad_averaging"": False,\n    },\n    ""lr_policy"": cosine_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.01,\n        ""min_lr"": 0.0,\n        ""warmup_steps"": 1000\n    },\n\n    ""dtype"": tf.float32,\n    # ""loss_scaling"": ""Backoff"",\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 1,\n                ""kernel_size"": [33], ""stride"": [2],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1]\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [33], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [33], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [33], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [39], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [39], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [39], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [51], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [51], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [51], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [63], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [63], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [63], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [75], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [75], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 5,\n                ""kernel_size"": [75], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""sep_conv1d"", ""repeat"": 1,\n                ""kernel_size"": [87], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[2],\n                ""residual"": True, ""residual_dense"": residual_dense\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1]\n            }\n        ],\n\n        ""dropout_keep_prob"": 1.0,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": tf.nn.relu,\n        ""data_format"": ""channels_last"",\n        ""use_conv_mask"": True,\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n        ""infer_logits_to_pickle"": False,\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""norm_per_feature"": True,\n        ""window"": ""hanning"",\n        ""precompute_mel_basis"": True,\n        ""sample_freq"": 16000,\n        ""pad_to"": 16,\n        ""dither"": 1e-5,\n        ""backend"": ""librosa"",\n    },\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""augmentation"": {\n            \'n_freq_mask\': 2,\n            \'n_time_mask\': 2,\n            \'width_freq_mask\': 6,\n            \'width_time_mask\': 6,\n        },\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-train-clean-100.csv"",\n            ""/data/librispeech/librivox-train-clean-360.csv"",\n            ""/data/librispeech/librivox-train-other-500.csv""\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
example_configs/speech2text/w2l_large_8gpus.py,5,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 200,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 32,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 2200,\n    ""eval_steps"": 2200,\n    ""save_checkpoint_steps"": 1100,\n    ""logdir"": ""w2l_log_folder"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.90,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.05,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""regularizer"": tf.contrib.layers.l2_regularizer,\n    ""regularizer_params"": {\n        \'scale\': 0.001\n    },\n\n    ""dtype"": tf.float32,\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [29], ""stride"": [1],\n                ""num_channels"": 896, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.6,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.6,\n            }\n        ],\n\n        ""dropout_keep_prob"": 0.7,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n        ""data_format"": ""channels_last"",\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n\n        # params for decoding the sequence with language model\n        ""beam_width"": 512,\n        ""alpha"": 2.0,\n        ""beta"": 1.5,\n\n        ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n        ""lm_path"": ""language_model/4-gram.binary"",\n        ""trie_path"": ""language_model/trie.binary"",\n        ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-train-clean-100.csv"",\n            ""data/librispeech/librivox-train-clean-360.csv"",\n            ""data/librispeech/librivox-train-other-500.csv"",\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
example_configs/speech2text/w2l_large_8gpus_mp.py,4,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 200,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 64,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 2200,\n    ""eval_steps"": 2200,\n    ""save_checkpoint_steps"": 1100,\n    ""logdir"": ""w2l_log_folder"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.90,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.05,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""regularizer"": tf.contrib.layers.l2_regularizer,\n    ""regularizer_params"": {\n        \'scale\': 0.001\n    },\n\n    ""dtype"": ""mixed"",\n    ""loss_scaling"": ""Backoff"",\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [29], ""stride"": [1],\n                ""num_channels"": 896, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.6,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.6,\n            }\n        ],\n\n        ""dropout_keep_prob"": 0.7,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n        ""data_format"": ""channels_last"",\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n\n        # params for decoding the sequence with language model\n        ""beam_width"": 512,\n        ""alpha"": 2.0,\n        ""beta"": 1.5,\n\n        ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n        ""lm_path"": ""language_model/4-gram.binary"",\n        ""trie_path"": ""language_model/trie.binary"",\n        ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-train-clean-100.csv"",\n            ""data/librispeech/librivox-train-clean-360.csv"",\n            ""data/librispeech/librivox-train-other-500.csv"",\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
example_configs/speech2text/w2lplus_large_8gpus.py,5,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 200,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 32,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 2200,\n    ""eval_steps"": 2200,\n    ""save_checkpoint_steps"": 1100,\n    ""num_checkpoints"": 5,\n    ""logdir"": ""w2l_log_folder"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.90,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.05,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""regularizer"": tf.contrib.layers.l2_regularizer,\n    ""regularizer_params"": {\n        \'scale\': 0.001\n    },\n\n    ""dtype"": tf.float32,\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [11], ""stride"": [2],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [29], ""stride"": [1],\n                ""num_channels"": 896, ""padding"": ""SAME"",\n                ""dilation"":[2], ""dropout_keep_prob"": 0.6,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.6,\n            }\n        ],\n\n        ""dropout_keep_prob"": 0.7,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n        ""data_format"": ""channels_last"",\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n\n        # params for decoding the sequence with language model\n        ""beam_width"": 512,\n        ""alpha"": 2.0,\n        ""beta"": 1.5,\n\n        ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n        ""lm_path"": ""language_model/4-gram.binary"",\n        ""trie_path"": ""language_model/trie.binary"",\n        ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-train-clean-100.csv"",\n            ""data/librispeech/librivox-train-clean-360.csv"",\n            ""data/librispeech/librivox-train-other-500.csv"",\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
example_configs/speech2text/w2lplus_large_8gpus_mp.py,4,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""random_seed"": 0,\n    ""use_horovod"": True,\n    ""num_epochs"": 200,\n\n    ""num_gpus"": 8,\n    ""batch_size_per_gpu"": 64,\n    ""iter_size"": 1,\n\n    ""save_summaries_steps"": 100,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 2200,\n    ""eval_steps"": 2200,\n    ""save_checkpoint_steps"": 1100,\n    ""num_checkpoints"": 5,\n    ""logdir"": ""w2l_log_folder"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.90,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.05,\n        ""power"": 2.0,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n\n    ""regularizer"": tf.contrib.layers.l2_regularizer,\n    ""regularizer_params"": {\n        \'scale\': 0.001\n    },\n\n    ""dtype"": ""mixed"",\n    ""loss_scaling"": ""Backoff"",\n\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [11], ""stride"": [2],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [11], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [13], ""stride"": [1],\n                ""num_channels"": 384, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [17], ""stride"": [1],\n                ""num_channels"": 512, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.8,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [21], ""stride"": [1],\n                ""num_channels"": 640, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [25], ""stride"": [1],\n                ""num_channels"": 768, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.7,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [29], ""stride"": [1],\n                ""num_channels"": 896, ""padding"": ""SAME"",\n                ""dilation"":[2], ""dropout_keep_prob"": 0.6,\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 1024, ""padding"": ""SAME"",\n                ""dilation"":[1], ""dropout_keep_prob"": 0.6,\n            }\n        ],\n\n        ""dropout_keep_prob"": 0.7,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n        ""data_format"": ""channels_last"",\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        # ""use_language_model"": False,\n\n        # params for decoding the sequence with language model\n        # ""beam_width"": 512,\n        # ""alpha"": 2.5,\n        # ""beta"": 0.,\n\n        # ""decoder_library_path"": ""ctc_decoder_with_lm/libctc_decoder_with_kenlm.so"",\n        # ""lm_path"": ""language_model/4-gram.binary"",\n        # ""trie_path"": ""language_model/trie.binary"",\n        # ""alphabet_config_path"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""infer_logits_to_pickle"": False,\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-train-clean-100.csv"",\n            ""data/librispeech/librivox-train-clean-360.csv"",\n            ""data/librispeech/librivox-train-other-500.csv"",\n        ],\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-dev-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n\ninfer_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""data/librispeech/librivox-test-clean.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
example_configs/text2speech/centaur_float.py,9,"b'# pylint: skip-file\nimport os\n\nimport tensorflow as tf\n\nfrom open_seq2seq.data import Text2SpeechDataLayer\nfrom open_seq2seq.decoders import CentaurDecoder\nfrom open_seq2seq.encoders import CentaurEncoder\nfrom open_seq2seq.losses import Text2SpeechLoss\nfrom open_seq2seq.models import Text2SpeechCentaur\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\nfrom open_seq2seq.optimizers.novograd import NovoGrad\n\nbase_model = Text2SpeechCentaur\n\ndataset = ""LJ""\ndataset_location = ""/data/LJSpeech""\noutput_type = ""both""\n\ntrim = False\nexp_mag = True\nmag_num_feats = 513\ntrain = ""train.csv""\nvalid = ""test.csv""\nbatch_size = 32\nnum_audio_features = {\n  ""mel"": 80,\n  ""magnitude"": mag_num_feats\n}\ndata_min = {\n  ""mel"": 1e-2,\n  ""magnitude"": 1e-5,\n}\n\ndebug = False\n\nnum_gpus = 8 if not debug else 1\n\nreduction_factor = 2\nattention_layers = 4\nencoder_hidden_size = 256\ndecoder_hidden_size = 512\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": True if not debug else False,\n  ""max_steps"": 1000000,\n  ""bench_start"": 0,\n\n  ""num_gpus"": num_gpus,\n  ""batch_size_per_gpu"": batch_size,\n\n  ""save_summaries_steps"": 1000 if not debug else 10,\n  ""print_loss_steps"": 1000 if not debug else 10,\n  ""print_samples_steps"": 1000 if not debug else 10,\n  ""eval_steps"": 5000 if not debug else 50,\n  ""save_checkpoint_steps"": 5000,\n  ""save_to_tensorboard"": True,\n  ""logdir"": ""result/centaur-float"",\n  ""max_grad_norm"": 1.,\n\n  ""optimizer"": NovoGrad,\n  ""optimizer_params"": {\n    ""beta1"": 0.95,\n    ""beta2"": 0.98,\n    ""epsilon"": 1e-08,\n    ""weight_decay"": 0.001,\n\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.02,\n    ""power"": 2.0,\n  },\n  ""dtype"": tf.float32,\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [""learning_rate"", ""variables"", ""gradients"", ""larc_summaries"",\n                ""variable_norm"", ""gradient_norm"", ""global_gradient_norm""],\n\n  ""encoder"": CentaurEncoder,\n  ""encoder_params"": {\n    ""src_vocab_size"": 94,\n    ""embedding_size"": encoder_hidden_size,\n    ""output_size"": encoder_hidden_size,\n    ""pad_embeddings_2_eight"": True,\n    ""cnn_dropout_prob"": 0.1,\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [3], ""stride"": [1],\n        ""num_channels"": encoder_hidden_size, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.relu\n      },\n      {\n        ""kernel_size"": [3], ""stride"": [1],\n        ""num_channels"": encoder_hidden_size, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.relu\n      },\n      {\n        ""kernel_size"": [3], ""stride"": [1],\n        ""num_channels"": encoder_hidden_size, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.relu\n      },\n      {\n        ""kernel_size"": [3], ""stride"": [1],\n        ""num_channels"": encoder_hidden_size, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.relu\n      }\n    ]\n  },\n\n  ""decoder"": CentaurDecoder,\n  ""decoder_params"": {\n    ""attention_layers"": attention_layers,\n    ""self_attention_conv_params"": {\n      ""kernel_size"": [5],\n      ""stride"": [1],\n      ""num_channels"": decoder_hidden_size,\n      ""padding"": ""VALID"",\n      ""is_causal"": True,\n      ""activation_fn"": tf.nn.relu\n    },\n\n    ""window_size"": 4,\n    ""back_step_size"": 0,\n    ""force_layers"": [1, 3],\n\n    ""hidden_size"": decoder_hidden_size,\n    ""reduction_factor"": reduction_factor,\n    ""prenet_layers"": 2,\n    ""prenet_hidden_size"": decoder_hidden_size,\n    ""prenet_use_inference_dropout"": False,\n    ""cnn_dropout_prob"": 0.1,\n    ""prenet_dropout"": 0.5,\n    ""conv_layers"":\n      [\n        {\n          ""kernel_size"": [5],\n          ""stride"": [1],\n          ""num_channels"": decoder_hidden_size,\n          ""padding"": ""VALID"",\n          ""is_causal"": True,\n          ""activation_fn"": tf.nn.relu\n        }\n      ] * 4,\n    ""mag_conv_layers"":\n      [\n        {\n          ""kernel_size"": [5],\n          ""stride"": [1],\n          ""num_channels"": decoder_hidden_size,\n          ""padding"": ""VALID"",\n          ""is_causal"": True,\n          ""activation_fn"": tf.nn.relu\n        }\n      ] * 4,\n    ""attention_dropout"": 0.1,\n    ""layer_postprocess_dropout"": 0.1\n  },\n\n  ""loss"": Text2SpeechLoss,\n  ""loss_params"": {\n    ""use_mask"": True,\n    ""l1_norm"": True\n  },\n\n  ""data_layer"": Text2SpeechDataLayer,\n  ""data_layer_params"": {\n    ""dataset"": dataset,\n    ""use_cache"": True,\n    ""num_audio_features"": num_audio_features,\n    ""output_type"": output_type,\n    ""vocab_file"": ""open_seq2seq/test_utils/vocab_tts.txt"",\n    ""dataset_location"": dataset_location,\n    ""mag_power"": 1,\n    ""pad_EOS"": True,\n    ""feature_normalize"": False,\n    ""feature_normalize_mean"": 0.,\n    ""feature_normalize_std"": 1.,\n    ""data_min"": data_min,\n    ""mel_type"": ""htk"",\n    ""trim"": trim,\n    ""duration_max"": 1024,\n    ""duration_min"": 24,\n    ""exp_mag"": exp_mag\n  },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, train),\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, valid),\n    ],\n    ""duration_max"": 1000,\n    ""duration_min"": 0,\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, ""infer.csv""),\n    ],\n    ""duration_max"": 1000,\n    ""duration_min"": 0,\n    ""shuffle"": False,\n  },\n}\n\ninteractive_infer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [],\n    ""duration_max"": 1000,\n    ""duration_min"": 0,\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/text2speech/tacotron_float.py,10,"b'# pylint: skip-file\nimport os\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechTacotron\nfrom open_seq2seq.encoders import Tacotron2Encoder\nfrom open_seq2seq.decoders import Tacotron2Decoder\nfrom open_seq2seq.data import Text2SpeechDataLayer\nfrom open_seq2seq.losses import Text2SpeechLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr, transformer_policy, exp_decay\n\n\nbase_model = Text2SpeechTacotron\n\ndataset = ""LJ""\ndataset_location = ""/data/speech/LJSpeech""\noutput_type = ""both""\n\nif dataset == ""MAILABS"":\n  trim = True\n  mag_num_feats = 401\n  train = ""train.csv""\n  val = ""val.csv""\n  batch_size = 32\nelif dataset == ""LJ"":\n  trim = False\n  mag_num_feats = 513\n  train = ""train_32.csv""\n  val = ""val_32.csv""\n  batch_size = 48\nelse:\n  raise ValueError(""Unknown dataset"")\n\nexp_mag = False\nif output_type == ""magnitude"":\n  num_audio_features = mag_num_feats\n  data_min = 1e-5\nelif output_type == ""mel"":\n  num_audio_features = 80\n  data_min = 1e-2\nelif output_type == ""both"":\n  num_audio_features = {\n      ""mel"": 80,\n      ""magnitude"": mag_num_feats\n  }\n  data_min = {\n      ""mel"": 1e-2,\n      ""magnitude"": 1e-5,\n  }\n  exp_mag = True\nelse:\n  raise ValueError(""Unknown param for output_type"")\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""max_steps"": 100000,\n\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": batch_size,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""save_to_tensorboard"": True,\n  ""logdir"": ""result/tacotron-LJ-float"",\n  ""max_grad_norm"":1.,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 20000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 45000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": tf.float32,\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": Tacotron2Encoder,\n  ""encoder_params"": {\n    ""cnn_dropout_prob"": 0.5,\n    ""rnn_dropout_prob"": 0.,\n    \'src_emb_size\': 512,\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      }\n    ],\n    ""activation_fn"": tf.nn.relu,\n\n    ""num_rnn_layers"": 1,\n    ""rnn_cell_dim"": 256,\n    ""rnn_unidirectional"": False,\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": tf.contrib.cudnn_rnn.CudnnLSTM,\n    ""zoneout_prob"": 0.,\n\n    ""data_format"": ""channels_last"",\n  },\n\n  ""decoder"": Tacotron2Decoder,\n  ""decoder_params"": {\n    ""zoneout_prob"": 0.,\n    ""dropout_prob"": 0.1,\n    \n    \'attention_type\': \'location\',\n    \'attention_layer_size\': 128,\n    \'attention_bias\': True,\n\n    \'decoder_cell_units\': 1024,\n    \'decoder_cell_type\': tf.nn.rnn_cell.LSTMCell,\n    \'decoder_layers\': 2,\n    \n    \'enable_prenet\': True,\n    \'prenet_layers\': 2,\n    \'prenet_units\': 256,\n\n    \'enable_postnet\': True,\n    ""postnet_keep_dropout_prob"": 0.5,\n    ""postnet_data_format"": ""channels_last"",\n    ""postnet_conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": -1, ""padding"": ""SAME"",\n        ""activation_fn"": None\n      }\n    ],\n    ""mask_decoder_sequence"": True,\n    ""parallel_iterations"": 32,\n  },\n  \n  ""loss"": Text2SpeechLoss,\n  ""loss_params"": {\n    ""use_mask"": True\n  },\n\n  ""data_layer"": Text2SpeechDataLayer,\n  ""data_layer_params"": {\n    ""dataset"": dataset,\n    ""num_audio_features"": num_audio_features,\n    ""output_type"": output_type,\n    ""vocab_file"": ""open_seq2seq/test_utils/vocab_tts.txt"",\n    \'dataset_location\':dataset_location,\n    ""mag_power"": 1,\n    ""pad_EOS"": True,\n    ""feature_normalize"": False,\n    ""feature_normalize_mean"": 0.,\n    ""feature_normalize_std"": 1.,\n    ""data_min"":data_min,\n    ""mel_type"":\'htk\',\n    ""trim"": trim,   \n    ""duration_max"":1024,\n    ""duration_min"":24,\n    ""exp_mag"": exp_mag\n  },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, train),\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, val),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, ""test.csv""),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n\ninteractive_infer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/text2speech/tacotron_float_8gpu.py,10,"b'# pylint: skip-file\nimport os\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechTacotron\nfrom open_seq2seq.encoders import Tacotron2Encoder\nfrom open_seq2seq.decoders import Tacotron2Decoder\nfrom open_seq2seq.data import Text2SpeechDataLayer\nfrom open_seq2seq.losses import Text2SpeechLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr, transformer_policy, exp_decay\n\n\nbase_model = Text2SpeechTacotron\n\ndataset = ""LJ""\ndataset_location = ""/data/speech/LJSpeech""\noutput_type = ""both""\n\nif dataset == ""MAILABS"":\n  trim = True\n  mag_num_feats = 401\n  train = ""train.csv""\n  val = ""val.csv""\n  batch_size = 32\nelif dataset == ""LJ"":\n  trim = False\n  mag_num_feats = 513\n  train = ""train_32.csv""\n  val = ""val_32.csv""\n  batch_size = 48\nelse:\n  raise ValueError(""Unknown dataset"")\n\nexp_mag = False\nif output_type == ""magnitude"":\n  num_audio_features = mag_num_feats\n  data_min = 1e-5\nelif output_type == ""mel"":\n  num_audio_features = 80\n  data_min = 1e-2\nelif output_type == ""both"":\n  num_audio_features = {\n      ""mel"": 80,\n      ""magnitude"": mag_num_feats\n  }\n  data_min = {\n      ""mel"": 1e-2,\n      ""magnitude"": 1e-5,\n  }\n  exp_mag = True\nelse:\n  raise ValueError(""Unknown param for output_type"")\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": True,\n  ""max_steps"": 40000,\n\n  ""batch_size_per_gpu"": batch_size,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""save_to_tensorboard"": True,\n  ""logdir"": ""result/tacotron-LJ-float-8gpu"",\n  ""max_grad_norm"":1.,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 10000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 20000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": tf.float32,\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": Tacotron2Encoder,\n  ""encoder_params"": {\n    ""cnn_dropout_prob"": 0.5,\n    ""rnn_dropout_prob"": 0.,\n    \'src_emb_size\': 512,\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      }\n    ],\n    ""activation_fn"": tf.nn.relu,\n\n    ""num_rnn_layers"": 1,\n    ""rnn_cell_dim"": 256,\n    ""rnn_unidirectional"": False,\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": tf.contrib.cudnn_rnn.CudnnLSTM,\n    ""zoneout_prob"": 0.,\n\n    ""data_format"": ""channels_last"",\n  },\n\n  ""decoder"": Tacotron2Decoder,\n  ""decoder_params"": {\n    ""zoneout_prob"": 0.,\n    ""dropout_prob"": 0.1,\n    \n    \'attention_type\': \'location\',\n    \'attention_layer_size\': 128,\n    \'attention_bias\': True,\n\n    \'decoder_cell_units\': 1024,\n    \'decoder_cell_type\': tf.nn.rnn_cell.LSTMCell,\n    \'decoder_layers\': 2,\n    \n    \'enable_prenet\': True,\n    \'prenet_layers\': 2,\n    \'prenet_units\': 256,\n\n    \'enable_postnet\': True,\n    ""postnet_keep_dropout_prob"": 0.5,\n    ""postnet_data_format"": ""channels_last"",\n    ""postnet_conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": -1, ""padding"": ""SAME"",\n        ""activation_fn"": None\n      }\n    ],\n    ""mask_decoder_sequence"": True,\n    ""parallel_iterations"": 32,\n  },\n  \n  ""loss"": Text2SpeechLoss,\n  ""loss_params"": {\n    ""use_mask"": True\n  },\n\n  ""data_layer"": Text2SpeechDataLayer,\n  ""data_layer_params"": {\n    ""dataset"": dataset,\n    ""num_audio_features"": num_audio_features,\n    ""output_type"": output_type,\n    ""vocab_file"": ""open_seq2seq/test_utils/vocab_tts.txt"",\n    \'dataset_location\':dataset_location,\n    ""mag_power"": 1,\n    ""pad_EOS"": True,\n    ""feature_normalize"": False,\n    ""feature_normalize_mean"": 0.,\n    ""feature_normalize_std"": 1.,\n    ""data_min"":data_min,\n    ""mel_type"":\'htk\',\n    ""trim"": trim,   \n    ""duration_max"":1024,\n    ""duration_min"":24,\n    ""exp_mag"": exp_mag\n  },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, train),\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, val),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, ""test.csv""),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}'"
example_configs/text2speech/tacotron_gst.py,11,"b'# pylint: skip-file\nimport os\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechTacotron\nfrom open_seq2seq.encoders import Tacotron2Encoder\nfrom open_seq2seq.decoders import Tacotron2Decoder\nfrom open_seq2seq.data import Text2SpeechDataLayer\nfrom open_seq2seq.losses import Text2SpeechLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr, transformer_policy, exp_decay\n\n\nbase_model = Text2SpeechTacotron\n\ndataset = ""MAILABS""\ndataset_location = DL_REPLACE\noutput_type = ""both""\n\nif dataset == ""MAILABS"":\n  trim = True\n  mag_num_feats = 401\n  train = ""train.csv""\n  val = ""val.csv""\n  batch_size = 32\nelif dataset == ""LJ"":\n  trim = False\n  mag_num_feats = 513\n  train = ""train_32.csv""\n  val = ""val_32.csv""\n  batch_size = 48\nelse:\n  raise ValueError(""Unknown dataset"")\n\nexp_mag = False\nif output_type == ""magnitude"":\n  num_audio_features = mag_num_feats\n  data_min = 1e-5\nelif output_type == ""mel"":\n  num_audio_features = 80\n  data_min = 1e-2\nelif output_type == ""both"":\n  num_audio_features = {\n      ""mel"": 80,\n      ""magnitude"": mag_num_feats\n  }\n  data_min = {\n      ""mel"": 1e-2,\n      ""magnitude"": 1e-5,\n  }\n  exp_mag = True\nelse:\n  raise ValueError(""Unknown param for output_type"")\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""num_gpus"": 2,\n  ""num_epochs"": 25,\n\n  ""batch_size_per_gpu"": batch_size,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""save_to_tensorboard"": True,\n  ""logdir"": ""result/tacotron-gst-8gpu"",\n  ""max_grad_norm"":1.,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 10000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 20000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": tf.float32,\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": Tacotron2Encoder,\n  ""encoder_params"": {\n    ""cnn_dropout_prob"": 0.5,\n    ""rnn_dropout_prob"": 0.,\n    \'src_emb_size\': 512,\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      }\n    ],\n    ""activation_fn"": tf.nn.relu,\n\n    ""num_rnn_layers"": 1,\n    ""rnn_cell_dim"": 256,\n    ""rnn_unidirectional"": False,\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": tf.contrib.cudnn_rnn.CudnnLSTM,\n    ""zoneout_prob"": 0.,\n\n    ""data_format"": ""channels_last"",\n\n    ""style_embedding_enable"": True,\n    ""style_embedding_params"": {\n      ""conv_layers"": [\n        {\n          ""kernel_size"": [3,3], ""stride"": [2,2],\n          ""num_channels"": 32, ""padding"": ""SAME""\n        },\n        {\n          ""kernel_size"": [3,3], ""stride"": [2,2],\n          ""num_channels"": 32, ""padding"": ""SAME""\n        },\n        {\n          ""kernel_size"": [3,3], ""stride"": [2,2],\n          ""num_channels"": 64, ""padding"": ""SAME""\n        },\n        {\n          ""kernel_size"": [3,3], ""stride"": [2,2],\n          ""num_channels"": 64, ""padding"": ""SAME""\n        },\n        {\n          ""kernel_size"": [3,3], ""stride"": [2,2],\n          ""num_channels"": 128, ""padding"": ""SAME""\n        },\n        {\n          ""kernel_size"": [3,3], ""stride"": [2,2],\n          ""num_channels"": 128, ""padding"": ""SAME""\n        }\n      ],\n      ""num_rnn_layers"": 1,\n      ""rnn_cell_dim"": 128,\n      ""rnn_unidirectional"": True,\n      ""rnn_type"": tf.nn.rnn_cell.GRUCell,\n      ""emb_size"": 512,\n      \'attention_layer_size\': 512,\n      ""num_tokens"": 32,\n      ""num_heads"": 8\n    }\n  },\n\n  ""decoder"": Tacotron2Decoder,\n  ""decoder_params"": {\n    ""zoneout_prob"": 0.,\n    ""dropout_prob"": 0.1,\n    \n    \'attention_type\': \'location\',\n    \'attention_layer_size\': 128,\n    \'attention_bias\': True,\n\n    \'decoder_cell_units\': 1024,\n    \'decoder_cell_type\': tf.nn.rnn_cell.LSTMCell,\n    \'decoder_layers\': 2,\n    \n    \'enable_prenet\': True,\n    \'prenet_layers\': 2,\n    \'prenet_units\': 256,\n\n    \'enable_postnet\': True,\n    ""postnet_keep_dropout_prob"": 0.5,\n    ""postnet_data_format"": ""channels_last"",\n    ""postnet_conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": -1, ""padding"": ""SAME"",\n        ""activation_fn"": None\n      }\n    ],\n    ""mask_decoder_sequence"": True,\n    ""parallel_iterations"": 32,\n  },\n  \n  ""loss"": Text2SpeechLoss,\n  ""loss_params"": {\n    ""use_mask"": True\n  },\n\n  ""data_layer"": Text2SpeechDataLayer,\n  ""data_layer_params"": {\n    ""dataset"": dataset,\n    ""num_audio_features"": num_audio_features,\n    ""output_type"": output_type,\n    ""vocab_file"": ""open_seq2seq/test_utils/vocab_tts.txt"",\n    \'dataset_location\':dataset_location,\n    ""mag_power"": 1,\n    ""pad_EOS"": True,\n    ""feature_normalize"": False,\n    ""feature_normalize_mean"": 0.,\n    ""feature_normalize_std"": 1.,\n    ""data_min"":data_min,\n    ""mel_type"":\'htk\',\n    ""trim"": trim,   \n    ""duration_max"":1024,\n    ""duration_min"":24,\n    ""exp_mag"": exp_mag\n  },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, train),\n    ],\n    ""shuffle"": True,\n    ""style_input"": ""wav""\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, val),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n    ""style_input"": ""wav""\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [""generate.csv""],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n    ""style_input"": ""wav""\n  },\n}'"
example_configs/text2speech/tacotron_mixed.py,9,"b'# pylint: skip-file\nimport os\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechTacotron\nfrom open_seq2seq.encoders import Tacotron2Encoder\nfrom open_seq2seq.decoders import Tacotron2Decoder\nfrom open_seq2seq.data import Text2SpeechDataLayer\nfrom open_seq2seq.losses import Text2SpeechLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr, transformer_policy, exp_decay\n\n\nbase_model = Text2SpeechTacotron\n\ndataset = ""LJ""\ndataset_location = ""/data/speech/LJSpeech""\noutput_type = ""both""\n\nif dataset == ""MAILABS"":\n  trim = True\n  mag_num_feats = 401\n  train = ""train.csv""\n  val = ""val.csv""\n  batch_size = 80\nelif dataset == ""LJ"":\n  trim = False\n  mag_num_feats = 513\n  train = ""train_32.csv""\n  val = ""val_32.csv""\n  batch_size = 96\nelse:\n  raise ValueError(""Unknown dataset"")\n\nexp_mag = False\nif output_type == ""magnitude"":\n  num_audio_features = mag_num_feats\n  data_min = 1e-5\nelif output_type == ""mel"":\n  num_audio_features = 80\n  data_min = 1e-2\nelif output_type == ""both"":\n  num_audio_features = {\n      ""mel"": 80,\n      ""magnitude"": mag_num_feats\n  }\n  data_min = {\n      ""mel"": 1e-2,\n      ""magnitude"": 1e-5,\n  }\n  exp_mag = True\nelse:\n  raise ValueError(""Unknown param for output_type"")\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""max_steps"": 100000,\n\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": batch_size,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""save_to_tensorboard"": True,\n  ""logdir"": ""result/tacotron-LJ-mixed"",\n  ""max_grad_norm"":1.,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 20000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 45000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": Tacotron2Encoder,\n  ""encoder_params"": {\n    ""cnn_dropout_prob"": 0.5,\n    ""rnn_dropout_prob"": 0.,\n    \'src_emb_size\': 512,\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      }\n    ],\n    ""activation_fn"": tf.nn.relu,\n\n    ""num_rnn_layers"": 1,\n    ""rnn_cell_dim"": 256,\n    ""rnn_unidirectional"": False,\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": tf.contrib.cudnn_rnn.CudnnLSTM,\n    ""zoneout_prob"": 0.,\n\n    ""data_format"": ""channels_last"",\n  },\n\n  ""decoder"": Tacotron2Decoder,\n  ""decoder_params"": {\n    ""zoneout_prob"": 0.,\n    ""dropout_prob"": 0.1,\n    \n    \'attention_type\': \'location\',\n    \'attention_layer_size\': 128,\n    \'attention_bias\': True,\n\n    \'decoder_cell_units\': 1024,\n    \'decoder_cell_type\': tf.nn.rnn_cell.LSTMCell,\n    \'decoder_layers\': 2,\n    \n    \'enable_prenet\': True,\n    \'prenet_layers\': 2,\n    \'prenet_units\': 256,\n\n    \'enable_postnet\': True,\n    ""postnet_keep_dropout_prob"": 0.5,\n    ""postnet_data_format"": ""channels_last"",\n    ""postnet_conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": -1, ""padding"": ""SAME"",\n        ""activation_fn"": None\n      }\n    ],\n    ""mask_decoder_sequence"": True,\n    ""parallel_iterations"": 32,\n  },\n  \n  ""loss"": Text2SpeechLoss,\n  ""loss_params"": {\n    ""use_mask"": True\n  },\n\n  ""data_layer"": Text2SpeechDataLayer,\n  ""data_layer_params"": {\n    ""dataset"": dataset,\n    ""num_audio_features"": num_audio_features,\n    ""output_type"": output_type,\n    ""vocab_file"": ""open_seq2seq/test_utils/vocab_tts.txt"",\n    \'dataset_location\':dataset_location,\n    ""mag_power"": 1,\n    ""pad_EOS"": True,\n    ""feature_normalize"": False,\n    ""feature_normalize_mean"": 0.,\n    ""feature_normalize_std"": 1.,\n    ""data_min"":data_min,\n    ""mel_type"":\'htk\',\n    ""trim"": trim,   \n    ""duration_max"":1024,\n    ""duration_min"":24,\n    ""exp_mag"": exp_mag\n  },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, train),\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, val),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, ""test.csv""),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n\ninteractive_infer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/text2speech/tacotron_mixed_8gpu.py,9,"b'# pylint: skip-file\nimport os\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechTacotron\nfrom open_seq2seq.encoders import Tacotron2Encoder\nfrom open_seq2seq.decoders import Tacotron2Decoder\nfrom open_seq2seq.data import Text2SpeechDataLayer\nfrom open_seq2seq.losses import Text2SpeechLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr, transformer_policy, exp_decay\n\n\nbase_model = Text2SpeechTacotron\n\ndataset = ""LJ""\ndataset_location = ""/data/speech/LJSpeech""\noutput_type = ""both""\n\nif dataset == ""MAILABS"":\n  trim = True\n  mag_num_feats = 401\n  train = ""train.csv""\n  val = ""val.csv""\n  batch_size = 80\nelif dataset == ""LJ"":\n  trim = False\n  mag_num_feats = 513\n  train = ""train_32.csv""\n  val = ""val_32.csv""\n  batch_size = 96\nelse:\n  raise ValueError(""Unknown dataset"")\n\nexp_mag = False\nif output_type == ""magnitude"":\n  num_audio_features = mag_num_feats\n  data_min = 1e-5\nelif output_type == ""mel"":\n  num_audio_features = 80\n  data_min = 1e-2\nelif output_type == ""both"":\n  num_audio_features = {\n      ""mel"": 80,\n      ""magnitude"": mag_num_feats\n  }\n  data_min = {\n      ""mel"": 1e-2,\n      ""magnitude"": 1e-5,\n  }\n  exp_mag = True\nelse:\n  raise ValueError(""Unknown param for output_type"")\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": True,\n  ""max_steps"": 40000,\n\n  ""batch_size_per_gpu"": batch_size,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""save_to_tensorboard"": True,\n  ""logdir"": ""result/tacotron-LJ-mixed-8gpu"",\n  ""max_grad_norm"":1.,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 10000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 20000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": Tacotron2Encoder,\n  ""encoder_params"": {\n    ""cnn_dropout_prob"": 0.5,\n    ""rnn_dropout_prob"": 0.,\n    \'src_emb_size\': 512,\n    ""conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME""\n      }\n    ],\n    ""activation_fn"": tf.nn.relu,\n\n    ""num_rnn_layers"": 1,\n    ""rnn_cell_dim"": 256,\n    ""rnn_unidirectional"": False,\n    ""use_cudnn_rnn"": True,\n    ""rnn_type"": tf.contrib.cudnn_rnn.CudnnLSTM,\n    ""zoneout_prob"": 0.,\n\n    ""data_format"": ""channels_last"",\n  },\n\n  ""decoder"": Tacotron2Decoder,\n  ""decoder_params"": {\n    ""zoneout_prob"": 0.,\n    ""dropout_prob"": 0.1,\n    \n    \'attention_type\': \'location\',\n    \'attention_layer_size\': 128,\n    \'attention_bias\': True,\n\n    \'decoder_cell_units\': 1024,\n    \'decoder_cell_type\': tf.nn.rnn_cell.LSTMCell,\n    \'decoder_layers\': 2,\n    \n    \'enable_prenet\': True,\n    \'prenet_layers\': 2,\n    \'prenet_units\': 256,\n\n    \'enable_postnet\': True,\n    ""postnet_keep_dropout_prob"": 0.5,\n    ""postnet_data_format"": ""channels_last"",\n    ""postnet_conv_layers"": [\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": 512, ""padding"": ""SAME"",\n        ""activation_fn"": tf.nn.tanh\n      },\n      {\n        ""kernel_size"": [5], ""stride"": [1],\n        ""num_channels"": -1, ""padding"": ""SAME"",\n        ""activation_fn"": None\n      }\n    ],\n    ""mask_decoder_sequence"": True,\n    ""parallel_iterations"": 32,\n  },\n  \n  ""loss"": Text2SpeechLoss,\n  ""loss_params"": {\n    ""use_mask"": True\n  },\n\n  ""data_layer"": Text2SpeechDataLayer,\n  ""data_layer_params"": {\n    ""dataset"": dataset,\n    ""num_audio_features"": num_audio_features,\n    ""output_type"": output_type,\n    ""vocab_file"": ""open_seq2seq/test_utils/vocab_tts.txt"",\n    \'dataset_location\':dataset_location,\n    ""mag_power"": 1,\n    ""pad_EOS"": True,\n    ""feature_normalize"": False,\n    ""feature_normalize_mean"": 0.,\n    ""feature_normalize_std"": 1.,\n    ""data_min"":data_min,\n    ""mel_type"":\'htk\',\n    ""trim"": trim,   \n    ""duration_max"":1024,\n    ""duration_min"":24,\n    ""exp_mag"": exp_mag\n  },\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, train),\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, val),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      os.path.join(dataset_location, ""test.csv""),\n    ],\n    ""duration_max"":10000,\n    ""duration_min"":0,\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/text2speech/wavenet_float.py,3,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechWavenet\nfrom open_seq2seq.encoders import WavenetEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.losses import WavenetLoss\nfrom open_seq2seq.data import WavenetDataLayer\nfrom open_seq2seq.optimizers.lr_policies import exp_decay\nfrom open_seq2seq.parts.convs2s.utils import gated_linear_units\n\nbase_model = Text2SpeechWavenet\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""max_steps"": 1000000,\n\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 2,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""logdir"": ""result/wavenet-LJ-float"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 20000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 45000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": tf.float32,\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    ""scale"": 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [],\n\n  ""encoder"": WavenetEncoder,\n  ""encoder_params"": {\n    ""layer_type"": ""conv1d"",\n    ""kernel_size"": 3,\n    ""strides"": 1,\n    ""padding"": ""VALID"",\n    ""blocks"": 3,\n    ""layers_per_block"": 10,\n    ""filters"": 64,\n    ""quantization_channels"": 256\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""loss"": WavenetLoss,\n\n  ""data_layer"": WavenetDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 80,\n    ""dataset_location"": ""data/speech/LJSpeech/wavs/""\n  }\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""data/speech/LJSpeech/train.csv"",\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""data/speech/LJSpeech/val.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""data/speech/LJSpeech/test.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninteractive_infer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/text2speech/wavenet_float_8gpu.py,3,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechWavenet\nfrom open_seq2seq.encoders import WavenetEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.losses import WavenetLoss\nfrom open_seq2seq.data import WavenetDataLayer\nfrom open_seq2seq.optimizers.lr_policies import exp_decay\nfrom open_seq2seq.parts.convs2s.utils import gated_linear_units\n\nbase_model = Text2SpeechWavenet\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": True,\n  ""max_steps"": 1000000,\n\n  ""num_gpus"": 8,\n  ""batch_size_per_gpu"": 1,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""logdir"": ""result/wavenet-LJ-float"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 20000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 45000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": tf.float32,\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    ""scale"": 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [],\n\n  ""encoder"": WavenetEncoder,\n  ""encoder_params"": {\n    ""layer_type"": ""conv1d"",\n    ""kernel_size"": 3,\n    ""strides"": 1,\n    ""padding"": ""VALID"",\n    ""blocks"": 3,\n    ""layers_per_block"": 10,\n    ""filters"": 64,\n    ""quantization_channels"": 256\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""loss"": WavenetLoss,\n\n  ""data_layer"": WavenetDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 80,\n    ""dataset_location"": ""/data/LJSpeech-1.1-partitioned/wavs/""\n  }\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""/data/LJSpeech-1.1-partitioned/train.csv"",\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""/data/LJSpeech-1.1-partitioned/val.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""/data/LJSpeech-1.1-partitioned/test.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninteractive_infer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/text2speech/wavenet_mixed.py,2,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechWavenet\nfrom open_seq2seq.encoders import WavenetEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.losses import WavenetLoss\nfrom open_seq2seq.data import WavenetDataLayer\nfrom open_seq2seq.optimizers.lr_policies import exp_decay\nfrom open_seq2seq.parts.convs2s.utils import gated_linear_units\n\nbase_model = Text2SpeechWavenet\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": False,\n  ""max_steps"": 1000000,\n\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 4,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""logdir"": ""result/wavenet-LJ-mixed"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 20000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 45000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    ""scale"": 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [],\n\n  ""encoder"": WavenetEncoder,\n  ""encoder_params"": {\n    ""layer_type"": ""conv1d"",\n    ""kernel_size"": 3,\n    ""strides"": 1,\n    ""padding"": ""VALID"",\n    ""blocks"": 3,\n    ""layers_per_block"": 10,\n    ""filters"": 64,\n    ""quantization_channels"": 256\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""loss"": WavenetLoss,\n\n  ""data_layer"": WavenetDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 80,\n    ""dataset_location"": ""data/speech/LJSpeech/wavs/""\n  }\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""data/speech/LJSpeech/train.csv"",\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""data/speech/LJSpeech/val.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""data/speech/LJSpeech/test.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninteractive_infer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/text2speech/wavenet_mixed_8gpu.py,2,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2SpeechWavenet\nfrom open_seq2seq.encoders import WavenetEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.losses import WavenetLoss\nfrom open_seq2seq.data import WavenetDataLayer\nfrom open_seq2seq.optimizers.lr_policies import exp_decay\nfrom open_seq2seq.parts.convs2s.utils import gated_linear_units\n\nbase_model = Text2SpeechWavenet\n\nbase_params = {\n  ""random_seed"": 0,\n  ""use_horovod"": True,\n  ""max_steps"": 1000000,\n\n  ""num_gpus"": 8,\n  ""batch_size_per_gpu"": 2,\n\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 500,\n  ""eval_steps"": 500,\n  ""save_checkpoint_steps"": 2500,\n  ""logdir"": ""result/wavenet-LJ-mixed"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-3,\n    ""decay_steps"": 20000,\n    ""decay_rate"": 0.1,\n    ""use_staircase_decay"": False,\n    ""begin_decay_at"": 45000,\n    ""min_lr"": 1e-5,\n  },\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    ""scale"": 1e-6\n  },\n  ""initializer"": tf.contrib.layers.xavier_initializer,\n\n  ""summaries"": [],\n\n  ""encoder"": WavenetEncoder,\n  ""encoder_params"": {\n    ""layer_type"": ""conv1d"",\n    ""kernel_size"": 3,\n    ""strides"": 1,\n    ""padding"": ""VALID"",\n    ""blocks"": 3,\n    ""layers_per_block"": 10,\n    ""filters"": 64,\n    ""quantization_channels"": 256\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""loss"": WavenetLoss,\n\n  ""data_layer"": WavenetDataLayer,\n  ""data_layer_params"": {\n    ""num_audio_features"": 80,\n    ""dataset_location"": ""/data/LJSpeech-1.1-partitioned/wavs/""\n  }\n}\n\ntrain_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""/data/LJSpeech-1.1-partitioned/train.csv"",\n    ],\n    ""shuffle"": True,\n  },\n}\n\neval_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""/data/LJSpeech-1.1-partitioned/val.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninfer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [\n      ""/data/LJSpeech-1.1-partitioned/test.csv"",\n    ],\n    ""shuffle"": False,\n  },\n}\n\ninteractive_infer_params = {\n  ""data_layer_params"": {\n    ""dataset_files"": [],\n    ""shuffle"": False,\n  },\n}\n'"
example_configs/transfer/imdb-from-scratch.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import IMDBDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR IMDB DATA]""\nprocessed_data_folder = \'imdb-processed-data-wkt2\'\n\nbase_model = LSTMLM\nmax_length = 256\nbinary = True\nsteps = 40\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""use_horovod"": False,\n  ""num_gpus"": 2,\n\n  ""batch_size_per_gpu"": 160,\n  ""num_epochs"": 1500,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""logdir"": ""IMDB-START"",\n  ""lm_vocab_file"": \'wkt2-processed-data/vocab.txt\',\n  # ""lm_vocab_file"": \'[LINK TO THE VOCAB FILE IN THE PROCESSED DATA USED TO TRAIN THE BASE LM]\'\n  ""processed_data_folder"": processed_data_folder,\n  ""eval_steps"": steps * 2,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  # luong10 decay scheme\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 9e-4\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  # ""max_grad_norm"": 0.25,\n  ""dtype"": tf.float32,\n  #""dtype"": ""mixed"",\n  #""automatic_loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 896,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 3,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.6, \n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.6,\n    ""recurrent_keep_prob"": 0.7,\n    \'encoder_emb_keep_prob\': 0.37,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 256,\n    ""num_tokens_gen"": 10,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": CrossEntropyLoss,\n}\n\ntrain_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""binary"": binary,\n    ""max_length"": max_length,\n  },\n}\neval_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    # ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""binary"": binary,\n    ""max_length"": max_length,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    # ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""binary"": binary,\n    ""max_length"": max_length,\n  },\n}\n'"
example_configs/transfer/imdb-wkt103.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import IMDBDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR IMDB DATA]""\nprocessed_data_folder = \'imdb-processed-data-wkt103\'\n\nbase_model = LSTMLM\nmax_length = 256\nbinary = True\nsteps = 10\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n\n  ""batch_size_per_gpu"": 16,\n  ""eval_batch_size_per_gpu"": 64,\n  ""num_epochs"": 100,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""load_model"": ""WKT103-CPT"",\n  ""logdir"": ""IMDB-WKT103-EXP1"",\n  ""lm_vocab_file"": \'wkt103-processed-data/vocab.txt\',\n  # ""lm_vocab_file"": \'[LINK TO THE VOCAB FILE IN THE PROCESSED DATA USED TO TRAIN THE BASE LM]\'\n  ""processed_data_folder"": processed_data_folder,\n  ""eval_steps"": steps,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  # luong10 decay scheme\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-4\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  # ""max_grad_norm"": 0.25,\n  ""dtype"": tf.float32,\n  #""dtype"": ""mixed"",\n  #""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 1024,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 3,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.8,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.8,\n    ""recurrent_keep_prob"": 1.0,\n    \'encoder_emb_keep_prob\': 0.6,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 256,\n    ""num_tokens_gen"": 10,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": CrossEntropyLoss,\n}\n\ntrain_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""binary"": binary,\n    ""max_length"": max_length,\n    ""get_stats"": True,\n    # ""small"": True,\n  },\n}\neval_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    # ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""binary"": binary,\n    ""max_length"": max_length,\n    # ""small"": True,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    # ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""binary"": binary,\n    ""max_length"": max_length,\n  },\n}\n'"
example_configs/transfer/imdb-wkt2-cudnn.py,4,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import IMDBDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR IMDB DATA]""\nprocessed_data_folder = \'imdb-processed-data-wkt2\'\n\nbase_model = LSTMLM\nmax_length = 256\nbinary = True\nsteps = 10\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n\n  ""batch_size_per_gpu"": 16,\n  ""eval_batch_size_per_gpu"": 64,\n  ""num_epochs"": 100,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""load_model"": ""LSTM-FP32-2GPU-SMALL"",\n  ""logdir"": ""IMDB-WKT103-CUDNN-MIXED"",\n  ""lm_vocab_file"": \'wkt2-processed-data/vocab.txt\',\n  # ""lm_vocab_file"": \'[LINK TO THE VOCAB FILE IN THE PROCESSED DATA USED TO TRAIN THE BASE LM]\'\n  ""processed_data_folder"": processed_data_folder,\n  ""eval_steps"": steps,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  # luong10 decay scheme\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-4\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  # ""max_grad_norm"": 0.25,\n  # ""dtype"": tf.float32,\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": True,\n    ""cudnn_rnn_type"": tf.contrib.cudnn_rnn.CudnnLSTM,\n    ""core_cell"": None,\n    ""core_cell_params"": {\n        ""num_units"": 1024,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 3,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.8,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.8,\n    ""recurrent_keep_prob"": 1.0,\n    \'encoder_emb_keep_prob\': 0.6,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 256,\n    ""num_tokens_gen"": 10,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": CrossEntropyLoss,\n}\n\ntrain_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""binary"": binary,\n    ""max_length"": max_length,\n    ""get_stats"": True,\n    # ""small"": True,\n  },\n}\neval_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    # ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""binary"": binary,\n    ""max_length"": max_length,\n    # ""small"": True,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    # ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""binary"": binary,\n    ""max_length"": max_length,\n  },\n}\n'"
example_configs/transfer/imdb-wkt2.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import IMDBDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import CrossEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR IMDB DATA]""\nprocessed_data_folder = \'imdb-processed-data-wkt2\'\n\nbase_model = LSTMLM\nmax_length = 256\nbinary = True\nsteps = 5\n\nbase_params = {\n  ""restore_best_checkpoint"": True,\n  ""use_horovod"": False,\n  ""num_gpus"": 2,\n\n  ""batch_size_per_gpu"": 16,\n  ""num_epochs"": 25,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""load_model"": ""WKT2-CPT"",\n  ""logdir"": ""IMDB-WKT2"",\n  ""lm_vocab_file"": \'wkt2-processed-data/vocab.txt\',\n  # ""lm_vocab_file"": \'[LINK TO THE VOCAB FILE IN THE PROCESSED DATA USED TO TRAIN THE BASE LM]\'\n  ""processed_data_folder"": processed_data_folder,\n  ""eval_steps"": steps * 2,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  # luong10 decay scheme\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-5\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  # ""max_grad_norm"": 0.25,\n  ""dtype"": tf.float32,\n  #""dtype"": ""mixed"",\n  #""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 896,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 3,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 1.0,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 1.0,\n    ""recurrent_keep_prob"": 1.0,\n    \'encoder_emb_keep_prob\': 1.0,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 256,\n    ""num_tokens_gen"": 10,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-4,\n  },\n\n  ""loss"": CrossEntropyLoss,\n}\n\ntrain_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""binary"": binary,\n    ""max_length"": max_length,\n    # ""small"": True,\n  },\n}\neval_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    # ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""binary"": binary,\n    ""max_length"": max_length,\n    # ""small"": True,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": IMDBDataLayer,\n  ""data_layer_params"": {\n    # ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""binary"": binary,\n    ""max_length"": max_length,\n  },\n}\n'"
example_configs/transfer/sst-wkt2-small.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import SSTDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import BasicSequenceLoss, CrossEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\nbase_model = LSTMLM\nsteps = 10\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR SST DATA]""\nprocessed_data_folder = \'sst-processed-data-wkt2\'\nbinary = True\nmax_length = 96\n\nbase_params = {\n  ""restore_best_checkpoint"": True, # best checkpoint is only saved when using train_eval mode\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n\n  ""batch_size_per_gpu"": 20, \n  ""eval_batch_size_per_gpu"": 80,\n  ""num_epochs"": 120,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""load_model"": ""LSTM-FP32-2GPU-SMALL"",\n  ""lm_vocab_file"": \'wkt2-processed-data/vocab.txt\',\n  # ""lm_vocab_file"": \'[LINK TO THE VOCAB FILE IN THE PROCESSED DATA USED TO TRAIN THE BASE LM]\'\n  ""logdir"": ""SST-WKT2-SMALL"",\n  ""processed_data_folder"": processed_data_folder,\n  ""eval_steps"": steps,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  # luong10 decay scheme\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-4\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  # ""max_grad_norm"": 0.25,\n  ""dtype"": tf.float32,\n  #""dtype"": ""mixed"",\n  #""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": { # will need to update\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": { # need different initializers for embeddings and for weights\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 128,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 3,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.8,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.8,\n    ""recurrent_keep_prob"": 1.0,\n    \'encoder_emb_keep_prob\': 0.7,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 64,\n    ""num_tokens_gen"": 10,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n    ""use_cell_state"": True,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": CrossEntropyLoss,\n}\n\ntrain_params = {\n  ""data_layer"": SSTDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""max_length"": max_length,\n    ""get_stats"": True,\n  },\n}\neval_params = {\n  ""data_layer"": SSTDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""max_length"": max_length,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": SSTDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""max_length"": max_length,\n  },\n}'"
example_configs/transfer/sst-wkt2.py,3,"b'import tensorflow as tf\n\nfrom open_seq2seq.models import LSTMLM\nfrom open_seq2seq.encoders import LMEncoder\nfrom open_seq2seq.decoders import FakeDecoder\nfrom open_seq2seq.data import SSTDataLayer\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.losses import BasicSequenceLoss, CrossEntropyLoss\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\nbase_model = LSTMLM\nsteps = 10\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR SST DATA]""\nprocessed_data_folder = \'sst-processed-data-wkt2\'\nbinary = True\nmax_length = 96\n\nbase_params = {\n  ""restore_best_checkpoint"": True, # best checkpoint is only saved when using train_eval mode\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n\n  ""batch_size_per_gpu"": 20, \n  ""eval_batch_size_per_gpu"": 80,\n  ""num_epochs"": 120,\n  ""save_summaries_steps"": steps,\n  ""print_loss_steps"": steps,\n  ""print_samples_steps"": steps,\n  ""save_checkpoint_steps"": steps,\n  ""load_model"": ""WKT2-CPT"",\n  ""lm_vocab_file"": \'wkt2-processed-data/vocab.txt\',\n  # ""lm_vocab_file"": \'[LINK TO THE VOCAB FILE IN THE PROCESSED DATA USED TO TRAIN THE BASE LM]\'\n  ""logdir"": ""SST-WKT2-EXP10"",\n  ""processed_data_folder"": processed_data_folder,\n  ""eval_steps"": steps,\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  # luong10 decay scheme\n\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1e-4\n  },\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  # ""max_grad_norm"": 0.25,\n  ""dtype"": tf.float32,\n  #""dtype"": ""mixed"",\n  #""loss_scaling"": ""Backoff"",\n  ""encoder"": LMEncoder,\n  ""encoder_params"": { # will need to update\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": { # need different initializers for embeddings and for weights\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""use_cudnn_rnn"": False,\n    ""cudnn_rnn_type"": None,\n    ""core_cell"": WeightDropLayerNormBasicLSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 896,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 3,\n    ""encoder_dp_input_keep_prob"": 1.0,\n    ""encoder_dp_output_keep_prob"": 0.8,\n    ""encoder_last_input_keep_prob"": 1.0,\n    ""encoder_last_output_keep_prob"": 0.8,\n    ""recurrent_keep_prob"": 1.0,\n    \'encoder_emb_keep_prob\': 0.7,\n    ""encoder_use_skip_connections"": False,\n    ""emb_size"": 256,\n    ""num_tokens_gen"": 10,\n    ""sampling_prob"": 0.0, # 0 is always use the ground truth\n    ""fc_use_bias"": True,\n    ""weight_tied"": True,\n    ""awd_initializer"": False,\n    ""use_cell_state"": True,\n  },\n\n  ""decoder"": FakeDecoder,\n\n  ""regularizer"": tf.contrib.layers.l2_regularizer,\n  ""regularizer_params"": {\n    \'scale\': 2e-6,\n  },\n\n  ""loss"": CrossEntropyLoss,\n}\n\ntrain_params = {\n  ""data_layer"": SSTDataLayer,\n  ""data_layer_params"": {\n    ""data_root"": data_root,\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""max_length"": max_length,\n  },\n}\neval_params = {\n  ""data_layer"": SSTDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""max_length"": max_length,\n  },\n}\n\ninfer_params = {\n  ""data_layer"": SSTDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": False,\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""rand_start"": False,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""max_length"": max_length,\n  },\n}\n'"
external_lm_rescore/transformerxl/mem_transformer.py,0,"b'import sys\nimport math\nimport functools\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nsys.path.append(\'utils\')\nfrom proj_adaptive_softmax import ProjectedAdaptiveLogSoftmax\nfrom log_uniform_sampler import LogUniformSampler, sample_logits\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, demb):\n        super(PositionalEmbedding, self).__init__()\n\n        self.demb = demb\n\n        inv_freq = 1 / (10000 ** (torch.arange(0.0, demb, 2.0) / demb))\n        self.register_buffer(\'inv_freq\', inv_freq)\n\n    def forward(self, pos_seq, bsz=None):\n        sinusoid_inp = torch.ger(pos_seq, self.inv_freq)\n        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n\n        if bsz is not None:\n            return pos_emb[:,None,:].expand(-1, bsz, -1)\n        else:\n            return pos_emb[:,None,:]\n\n\nclass PositionwiseFF(nn.Module):\n    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False):\n        super(PositionwiseFF, self).__init__()\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.CoreNet = nn.Sequential(\n            nn.Linear(d_model, d_inner), nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(d_inner, d_model),\n            nn.Dropout(dropout),\n        )\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.pre_lnorm = pre_lnorm\n\n    def forward(self, inp):\n        if self.pre_lnorm:\n            ##### layer normalization + positionwise feed-forward\n            core_out = self.CoreNet(self.layer_norm(inp))\n\n            ##### residual connection\n            output = core_out + inp\n        else:\n            ##### positionwise feed-forward\n            core_out = self.CoreNet(inp)\n\n            ##### residual connection + layer normalization\n            output = self.layer_norm(inp + core_out)\n\n        return output\n\nclass MultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0, \n                 pre_lnorm=False):\n        super(MultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.q_net = nn.Linear(d_model, n_head * d_head, bias=False)\n        self.kv_net = nn.Linear(d_model, 2 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n    def forward(self, h, attn_mask=None, mems=None):\n        ##### multihead attention\n        # [hlen x bsz x n_head x d_head]\n\n        if mems is not None:\n            c = torch.cat([mems, h], 0)\n        else:\n            c = h\n\n        if self.pre_lnorm:\n            ##### layer normalization\n            c = self.layer_norm(c)\n\n        head_q = self.q_net(h)\n        head_k, head_v = torch.chunk(self.kv_net(c), 2, -1)\n\n        head_q = head_q.view(h.size(0), h.size(1), self.n_head, self.d_head)\n        head_k = head_k.view(c.size(0), c.size(1), self.n_head, self.d_head)\n        head_v = head_v.view(c.size(0), c.size(1), self.n_head, self.d_head)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = torch.einsum(\'ibnd,jbnd->ijbn\', (head_q, head_k))\n        attn_score.mul_(self.scale)\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float(\'inf\'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float(\'inf\'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        # [qlen x klen x bsz x n_head] + [klen x bsz x n_head x d_head] -> [qlen x bsz x n_head x d_head]\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, head_v))\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = h + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(h + attn_out)\n\n        return output\n\nclass RelMultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False):\n        super(RelMultiHeadAttn, self).__init__()\n\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n    def _parallelogram_mask(self, h, w, left=False):\n        mask = torch.ones((h, w)).byte()\n        m = min(h, w)\n        mask[:m,:m] = torch.triu(mask[:m,:m])\n        mask[-m:,-m:] = torch.tril(mask[-m:,-m:])\n\n        if left:\n            return mask\n        else:\n            return mask.flip(0)\n\n    def _shift(self, x, qlen, klen, mask, left=False):\n        if qlen > 1:\n            zero_pad = torch.zeros((x.size(0), qlen-1, x.size(2), x.size(3)),\n                                    device=x.device, dtype=x.dtype)\n        else:\n            zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype)\n\n        if left:\n            mask = mask.flip(1)\n            x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1)\n        else:\n            x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1)\n\n        x = x_padded.masked_select(mask[:,:,None,None]) \\\n                    .view(qlen, klen, x.size(2), x.size(3))\n\n        return x\n\n    def _rel_shift(self, x, zero_triu=False):\n        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n                               device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=1)\n\n        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])\n\n        x = x_padded[1:].view_as(x)\n\n        if zero_triu:\n            ones = torch.ones((x.size(0), x.size(1)))\n            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n\n        return x\n\n    def forward(self, w, r, attn_mask=None, mems=None):\n        raise NotImplementedError\n\nclass RelPartialLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelPartialLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n\n    def forward(self, w, r, r_w_bias, r_r_bias, attn_mask=None, mems=None):\n        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n\n        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                # qlen x n_head x d_head\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias                                         # qlen x bsz x n_head x d_head\n        AC = torch.einsum(\'ibnd,jbnd->ijbn\', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + r_r_bias\n        BD = torch.einsum(\'ibnd,jnd->ijbn\', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[None,:,:,None], -float(\'inf\')).type_as(attn_score)\n            elif attn_mask.dim() == 3:\n                attn_score = attn_score.float().masked_fill(\n                    attn_mask[:,:,:,None], -float(\'inf\')).type_as(attn_score)\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass RelLearnableMultiHeadAttn(RelMultiHeadAttn):\n    def __init__(self, *args, **kwargs):\n        super(RelLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n\n    def forward(self, w, r_emb, r_w_bias, r_bias, attn_mask=None, mems=None):\n        # r_emb: [klen, n_head, d_head], used for term B\n        # r_w_bias: [n_head, d_head], used for term C\n        # r_bias: [klen, n_head], used for term D\n\n        qlen, bsz = w.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)\n\n        if klen > r_emb.size(0):\n            r_emb_pad = r_emb[0:1].expand(klen-r_emb.size(0), -1, -1)\n            r_emb = torch.cat([r_emb_pad, r_emb], 0)\n            r_bias_pad = r_bias[0:1].expand(klen-r_bias.size(0), -1)\n            r_bias = torch.cat([r_bias_pad, r_bias], 0)\n        else:\n            r_emb = r_emb[-klen:]\n            r_bias = r_bias[-klen:]\n\n        #### compute attention score\n        rw_head_q = w_head_q + r_w_bias[None]                                   # qlen x bsz x n_head x d_head\n\n        AC = torch.einsum(\'ibnd,jbnd->ijbn\', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n        B_ = torch.einsum(\'ibnd,jnd->ijbn\', (w_head_q, r_emb))                  # qlen x klen x bsz x n_head\n        D_ = r_bias[None, :, None]                                              # 1    x klen x 1   x n_head\n        BD = self._rel_shift(B_ + D_)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and attn_mask.any().item():\n            if attn_mask.dim() == 2:\n                attn_score.masked_fill_(attn_mask[None,:,:,None], -float(\'inf\'))\n            elif attn_mask.dim() == 3:\n                attn_score.masked_fill_(attn_mask[:,:,:,None], -float(\'inf\'))\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        #### compute attention vector\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            output = w + attn_out\n        else:\n            ##### residual connection + layer normalization\n            output = self.layer_norm(w + attn_out)\n\n        return output\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout, **kwargs):\n        super(DecoderLayer, self).__init__()\n\n        self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass RelLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelLearnableMultiHeadAttn(n_head, d_model, d_head, dropout,\n                                         **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, r_emb, r_w_bias, r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r_emb, r_w_bias, r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\nclass RelPartialLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n                 **kwargs):\n        super(RelPartialLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model,\n                            d_head, dropout, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'))\n\n    def forward(self, dec_inp, r, r_w_bias, r_r_bias, dec_attn_mask=None, mems=None):\n\n        output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,\n                               attn_mask=dec_attn_mask,\n                               mems=mems)\n        output = self.pos_ff(output)\n\n        return output\n\n\nclass AdaptiveEmbedding(nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, \n                 sample_softmax=False):\n        super(AdaptiveEmbedding, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n\n        self.cutoffs = cutoffs + [n_token]\n        print(\'cutoffs\', cutoffs)\n        self.div_val = div_val\n        self.d_proj = d_proj\n\n        self.emb_scale = d_proj ** 0.5\n\n        self.cutoff_ends = [0] + self.cutoffs\n\n        self.emb_layers = nn.ModuleList()\n        self.emb_projs = nn.ParameterList()\n        if div_val == 1:\n            self.emb_layers.append(\n                nn.Embedding(n_token, d_embed, sparse=sample_softmax>0)\n            )\n            if d_proj != d_embed:\n                self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_embed)))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed // (div_val ** i)\n                print(r_idx, l_idx, d_emb_i)\n                self.emb_layers.append(nn.Embedding(r_idx-l_idx, d_emb_i))\n\n                self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_emb_i)))\n\n    def forward(self, inp):\n        if self.div_val == 1:\n            embed = self.emb_layers[0](inp)\n            if self.d_proj != self.d_embed:\n                embed  = F.linear(embed, self.emb_projs[0])\n        else:\n            param = next(self.parameters())\n            inp_flat = inp.view(-1)\n            emb_flat = torch.zeros([inp_flat.size(0), self.d_proj], \n                dtype=param.dtype, device=param.device)\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n\n                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n\n                if indices_i.numel() == 0:\n                    continue\n\n                inp_i = inp_flat.index_select(0, indices_i) - l_idx\n                emb_i = self.emb_layers[i](inp_i)\n                emb_i = F.linear(emb_i, self.emb_projs[i])\n\n                emb_flat.index_copy_(0, indices_i, emb_i)\n\n            embed = emb_flat.view(*inp.size(), self.d_proj)\n\n        embed.mul_(self.emb_scale)\n\n        return embed\n\nclass MemTransformerLM(nn.Module):\n    def __init__(self, n_token, n_layer, n_head, d_model, d_head, d_inner,\n                 dropout, dropatt, tie_weight=True, d_embed=None, \n                 div_val=1, tie_projs=[False], pre_lnorm=False,\n                 tgt_len=None, ext_len=None, mem_len=None, \n                 cutoffs=[], adapt_inp=False,\n                 same_length=False, attn_type=0, clamp_len=-1, \n                 sample_softmax=-1):\n        super(MemTransformerLM, self).__init__()\n        self.n_token = n_token\n\n        d_embed = d_model if d_embed is None else d_embed\n        self.d_embed = d_embed\n        self.d_model = d_model\n        self.n_head = n_head\n        self.d_head = d_head\n\n        self.word_emb = AdaptiveEmbedding(n_token, d_embed, d_model, cutoffs, \n                                          div_val=div_val)\n\n        self.drop = nn.Dropout(dropout)\n\n        self.n_layer = n_layer\n\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n        self.max_klen = tgt_len + ext_len + mem_len\n\n        self.attn_type = attn_type\n\n        self.layers = nn.ModuleList()\n        if attn_type == 0: # the default attention\n            for i in range(n_layer):\n                self.layers.append(\n                    RelPartialLearnableDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n        elif attn_type == 1: # learnable embeddings\n            for i in range(n_layer):\n                self.layers.append(\n                    RelLearnableDecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n        elif attn_type in [2, 3]: # absolute embeddings\n            for i in range(n_layer):\n                self.layers.append(\n                    DecoderLayer(\n                        n_head, d_model, d_head, d_inner, dropout,\n                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n                )\n\n        self.sample_softmax = sample_softmax\n        # use sampled softmax\n        if sample_softmax > 0:\n            self.out_layer = nn.Linear(d_model, n_token)\n            if tie_weight:\n                self.out_layer.weight = self.word_emb.weight\n            self.tie_weight = tie_weight\n            self.sampler = LogUniformSampler(n_token, sample_softmax)\n\n        # use adaptive softmax (including standard softmax)\n        else:\n            self.crit = ProjectedAdaptiveLogSoftmax(n_token, d_embed, d_model, \n                                                    cutoffs, div_val=div_val)\n\n            if tie_weight:\n                for i in range(len(self.crit.out_layers)):\n                    self.crit.out_layers[i].weight = self.word_emb.emb_layers[i].weight\n\n            if tie_projs:\n                for i, tie_proj in enumerate(tie_projs):\n                    if tie_proj and div_val == 1 and d_model != d_embed:\n                        self.crit.out_projs[i] = self.word_emb.emb_projs[0]\n                    elif tie_proj and div_val != 1:\n                        self.crit.out_projs[i] = self.word_emb.emb_projs[i]\n\n        self.same_length = same_length\n        self.clamp_len = clamp_len\n\n        self._create_params()\n\n    def backward_compatible(self):\n        self.sample_softmax = -1\n\n    def _create_params(self):\n        if self.attn_type == 0: # default attention\n            self.pos_emb = PositionalEmbedding(self.d_model)\n            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n        elif self.attn_type == 1: # learnable\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.n_head, self.d_head))\n            self.r_bias = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head))\n        elif self.attn_type == 2: # absolute standard\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        elif self.attn_type == 3: # absolute deeper SA\n            self.r_emb = nn.Parameter(torch.Tensor(\n                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n\n    def init_mems(self):\n        if self.mem_len > 0:\n            mems = []\n            param = next(self.parameters())\n            for i in range(self.n_layer+1):\n                empty = torch.empty(0, dtype=param.dtype, device=param.device)\n                mems.append(empty)\n\n            return mems\n        else:\n            return None\n\n    def _update_mems(self, hids, mems, qlen, mlen):\n        # does not deal with None\n        if mems is None: return None\n\n        # mems is not None\n        assert len(hids) == len(mems), \'len(hids) != len(mems)\'\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n            beg_idx = max(0, end_idx - self.mem_len)\n            for i in range(len(hids)):\n\n                cat = torch.cat([mems[i], hids[i]], dim=0)\n                new_mems.append(cat[beg_idx:end_idx].detach())\n\n        return new_mems\n\n    def _forward(self, dec_inp, mems=None):\n        qlen, bsz = dec_inp.size()\n\n        word_emb = self.word_emb(dec_inp)\n\n        mlen = mems[0].size(0) if mems is not None else 0\n        klen = mlen + qlen\n        if self.same_length:\n            all_ones = word_emb.new_ones(qlen, klen)\n            mask_len = klen - self.mem_len\n            if mask_len > 0:\n                mask_shift_len = qlen - mask_len\n            else:\n                mask_shift_len = qlen\n            dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n                    + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1\n        else:\n            dec_attn_mask = torch.triu(\n                word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]\n\n        hids = []\n        if self.attn_type == 0: # default\n            pos_seq = torch.arange(klen-1, -1, -1.0, device=word_emb.device, \n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb)\n            pos_emb = self.drop(pos_emb)\n\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                core_out = layer(core_out, pos_emb, self.r_w_bias,\n                        self.r_r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n                hids.append(core_out)\n        elif self.attn_type == 1: # learnable\n            core_out = self.drop(word_emb)\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                if self.clamp_len > 0:\n                    r_emb = self.r_emb[i][-self.clamp_len :]\n                    r_bias = self.r_bias[i][-self.clamp_len :]\n                else:\n                    r_emb, r_bias = self.r_emb[i], self.r_bias[i]\n\n                mems_i = None if mems is None else mems[i]\n                core_out = layer(core_out, r_emb, self.r_w_bias[i],\n                        r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n                hids.append(core_out)\n        elif self.attn_type == 2: # absolute\n            pos_seq = torch.arange(klen - 1, -1, -1.0, device=word_emb.device,\n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb + pos_emb[-qlen:])\n\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and i == 0:\n                    mems_i += pos_emb[:mlen]\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n                hids.append(core_out)\n        elif self.attn_type == 3:\n            core_out = self.drop(word_emb)\n\n            hids.append(core_out)\n            for i, layer in enumerate(self.layers):\n                mems_i = None if mems is None else mems[i]\n                if mems_i is not None and mlen > 0:\n                    cur_emb = self.r_emb[i][:-qlen]\n                    cur_size = cur_emb.size(0)\n                    if cur_size < mlen:\n                        cur_emb_pad = cur_emb[0:1].expand(mlen-cur_size, -1, -1)\n                        cur_emb = torch.cat([cur_emb_pad, cur_emb], 0)\n                    else:\n                        cur_emb = cur_emb[-mlen:]\n                    mems_i += cur_emb.view(mlen, 1, -1)\n                core_out += self.r_emb[i][-qlen:].view(qlen, 1, -1)\n\n                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n                                 mems=mems_i)\n                hids.append(core_out)\n\n        core_out = self.drop(core_out)\n\n        new_mems = self._update_mems(hids, mems, mlen, qlen)\n\n        return core_out, new_mems\n\n    def forward(self, data, target, *mems):\n        # nn.DataParallel does not allow size(0) tensors to be broadcasted.\n        # So, have to initialize size(0) mems inside the model forward.\n        # Moreover, have to return new_mems to allow nn.DataParallel to piece\n        # them together.\n        if not mems: mems = self.init_mems()\n\n        tgt_len = target.size(0)\n        hidden, new_mems = self._forward(data, mems=mems)\n\n        pred_hid = hidden[-tgt_len:]\n        if self.sample_softmax > 0 and self.training:\n            assert self.tie_weight\n            logit = sample_logits(self.word_emb,\n                self.out_layer.bias, target, pred_hid, self.sampler)\n            loss = -F.log_softmax(logit, -1)[:, :, 0]\n        else:\n            loss = self.crit(pred_hid.view(-1, pred_hid.size(-1)), target.view(-1))\n            loss = loss.view(tgt_len, -1)\n\n        if new_mems is None:\n            return [loss]\n        else:\n            return [loss] + new_mems\n\nif __name__ == \'__main__\':\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\'unit test\')\n\n    parser.add_argument(\'--n_layer\', type=int, default=4, help=\'\')\n    parser.add_argument(\'--n_rel_layer\', type=int, default=4, help=\'\')\n    parser.add_argument(\'--n_head\', type=int, default=2, help=\'\')\n    parser.add_argument(\'--d_head\', type=int, default=2, help=\'\')\n    parser.add_argument(\'--d_model\', type=int, default=200, help=\'\')\n    parser.add_argument(\'--d_embed\', type=int, default=200, help=\'\')\n    parser.add_argument(\'--d_inner\', type=int, default=200, help=\'\')\n    parser.add_argument(\'--dropout\', type=float, default=0.0, help=\'\')\n    parser.add_argument(\'--cuda\', action=\'store_true\', help=\'\')\n    parser.add_argument(\'--seed\', type=int, default=1111, help=\'\')\n    parser.add_argument(\'--multi_gpu\', action=\'store_true\', help=\'\')\n\n    args = parser.parse_args()\n\n    device = torch.device(""cuda"" if args.cuda else ""cpu"")\n\n    B = 4\n    tgt_len, mem_len, ext_len = 36, 36, 0\n    data_len = tgt_len * 20\n    args.n_token = 10000\n\n    import data_utils\n\n    data = torch.LongTensor(data_len*B).random_(0, args.n_token).to(device)\n    diter = data_utils.LMOrderedIterator(data, B, tgt_len, device=device, ext_len=ext_len)\n\n    cutoffs = [args.n_token // 2]\n    tie_projs = [False] + [True] * len(cutoffs)\n\n    for div_val in [1, 2]:\n        for d_embed in [200, 100]:\n            model = MemTransformerLM(args.n_token, args.n_layer, args.n_head,\n                            args.d_model, args.d_head, args.d_inner, args.dropout,\n                            dropatt=args.dropout, tie_weight=True, \n                            d_embed=d_embed, div_val=div_val, \n                            tie_projs=tie_projs, pre_lnorm=True,\n                            tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len, \n                            cutoffs=cutoffs, attn_type=0).to(device)\n\n            print(sum(p.numel() for p in model.parameters()))\n\n            mems = tuple()\n            for idx, (inp, tgt, seqlen) in enumerate(diter):\n                print(\'batch {}\'.format(idx))\n                out = model(inp, tgt, *mems)\n                mems = out[1:]\n'"
open_seq2seq/data/__init__.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom .data_layer import DataLayer\nfrom .speech2text.speech2text import Speech2TextDataLayer\nfrom .speech2text.speech_commands import SpeechCommandsDataLayer\nfrom .image2label.image2label import ImagenetDataLayer\nfrom .lm.lmdata import WKTDataLayer, IMDBDataLayer, SSTDataLayer\nfrom .text2speech.text2speech import Text2SpeechDataLayer\nfrom .text2speech.text2speech_wavenet import WavenetDataLayer\n'"
open_seq2seq/data/data_layer.py,4,"b'# Copyright (c) 2017 NVIDIA Corporation\n""""""Data layer classes""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport abc\nimport copy\n\nimport six\nimport tensorflow as tf\n\nfrom open_seq2seq.utils.utils import check_params\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass DataLayer:\n  """"""Abstract class from which all data layers must inherit.""""""\n  @staticmethod\n  def get_required_params():\n    """"""Static method with description of required parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **have to** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {\n        \'mode\': [\'train\', \'eval\', \'infer\'],\n    }\n\n  @staticmethod\n  def get_optional_params():\n    """"""Static method with description of optional parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **can** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {\n        \'batch_size\': int,\n        \'shuffle\': bool,\n        \'repeat\': bool,\n        \'dtype\': [tf.float32, tf.float16],\n        \'interactive\': bool,\n        \'cache_features\': bool,\n        \'cache_format\': str,\n        \'cache_regenerate\': bool\n    }\n\n  @abc.abstractmethod\n  def __init__(self, params, model, num_workers, worker_id):\n    """"""Data layer constructor.\n    The TensorFlow graph should not be created here, but rather in the\n    :meth:`self.build_graph() <build_graph>` method.\n\n    Args:\n      params (dict): parameters describing the data layer.\n          All supported parameters are listed in :meth:`get_required_params`,\n          :meth:`get_optional_params` functions.\n      model (instance of a class derived from :class:`Model<models.model.Model>`):\n          parent model that created this data layer.\n          Could be None if no model access is required for the use case.\n      num_workers (int): number of Horovod processes or number of GPUs\n          if Horovod is not used.\n      worker_id (int): Horovod process id or GPU id if Horovod is not used.\n\n    Config parameters:\n\n    * **shuffle** (bool) --- whether to shuffle dataset after an epoch.\n      Typically will be True for train and False for inference and evaluation.\n    * **dtype** --- data dtype. Could be either ``tf.float16`` or ``tf.float32``.\n    """"""\n    check_params(params, self.get_required_params(), self.get_optional_params())\n    self._params = copy.deepcopy(params)\n    self._model = model\n\n    if \'dtype\' not in self._params:\n      if self._model:\n        self._params[\'dtype\'] = self._model.get_tf_dtype()\n      else:\n        self._params[\'dtype\'] = tf.float32\n\n    if \'shuffle\' not in params:\n      self._params[\'shuffle\'] = (self._params[\'mode\'] == \'train\')\n\n    if self._params[\'mode\'] != \'train\' and self._params[\'shuffle\']:\n      raise ValueError(""Shuffle should not be performed in eval or infer modes"")\n\n    # should be used for correct evaluation on multiple GPUs\n    self._num_workers = num_workers\n    self._worker_id = worker_id\n\n  @property\n  def params(self):\n    """"""Parameters used to construct the data layer (dictionary).""""""\n    return self._params\n\n  @abc.abstractmethod\n  def build_graph(self):\n    """"""Here all TensorFlow graph construction should happen.""""""\n    pass\n\n  @property\n  @abc.abstractmethod\n  def iterator(self):\n    """"""``tf.data.Dataset`` iterator.\n    Should be created by :meth:`self.build_graph()<build_graph>`.\n    """"""\n    pass\n\n  @property\n  @abc.abstractmethod\n  def input_tensors(self):\n    """"""Dictionary containing input tensors.\n    This dictionary has to define the following keys: `source_tensors`,\n    which should contain all tensors describing the input object (i.e. tensors\n    that are passed to the encoder, e.g. input sequence and input length). And\n    when ``self.params[\'mode\'] != ""infer""`` data layer should also define\n    `target_tensors` which is the list of all tensors related to the\n    corresponding target object (i.e. tensors taht are passed to the decoder and\n    loss, e.g. target sequence and target length). Note that all tensors have\n    to be created inside :meth:`self.build_graph()<build_graph>` method.\n    """"""\n    pass\n\n  def create_interactive_placeholders(self):\n    """"""A function that must be defined for data layers that support interactive\n    infer. This function is intended to create placeholders that will be passed\n    to self._input_tensors that will be passed to the model.\n    """"""\n    pass\n\n  def create_feed_dict(self, model_in):\n    """"""A function that must be defined for data layers that support interactive\n    infer. Given input which is an abstract data element to be defined by the\n    data layer. The intended use is for the user to build and pass model_in from\n    the jupyter notebook. Given model_in, the data layer must preprocess the raw\n    data, and create the feed dict that defines the placeholders defined in\n    create_interactive_placeholders().\n    """"""\n    pass\n\n  def get_size_in_samples(self):\n    """"""Should return the dataset size in samples.\n    That is, the number of objects in the dataset. This method is used to\n    calculate a valid epoch size. If this method is not defined, you will need\n    to make sure that your dataset for evaluation is created only for\n    one epoch. You will also not be able to use ``num_epochs`` parameter in the\n    base config.\n\n    Returns:\n      int: dataset size in samples.\n    """"""\n    return None\n'"
open_seq2seq/data/utils.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport io\n\nfrom six.moves import range\n\n\ndef pad_vocab_to_eight(vocab):\n  """"""Pads vocabulary so that it is divisible by 8.\n\n  Args:\n    vocab (dict): vocabulary in the form token->id\n\n  Returns:\n    dict: vocab with new tokens added if necessary, such that the total\n    vocab size is divisible by 8.\n  """"""\n  v_len = len(vocab)\n  if v_len % 8 == 0:\n    return vocab\n  for id_add in range(0, 8 - v_len % 8):\n    vocab[\'<$\'+str(id_add)+\'$>\'] = v_len + id_add\n  return vocab\n\n\ndef load_pre_existing_vocabulary(path, min_idx=0, read_chars=False):\n  """"""Loads pre-existing vocabulary into memory.\n\n  The vocabulary file should contain a token on each line with optional\n  token count on the same line that will be ignored. Example::\n\n    a 1234\n    b 4321\n    c 32342\n    d\n    e\n    word 234\n\n  Args:\n    path (str): path to vocabulary.\n    min_idx (int, optional): minimum id to assign for a token.\n    read_chars (bool, optional): whether to read only the\n        first symbol of the line.\n\n  Returns:\n     dict: vocabulary dictionary mapping tokens (chars/words) to int ids.\n  """"""\n  idx = min_idx\n  vocab_dict = {}\n  with io.open(path, newline=\'\', encoding=\'utf-8\') as f:\n    for line in f:\n      # ignoring empty lines\n      if not line or line == \'\\n\':\n        continue\n      if read_chars:\n        token = line[0]\n      else:\n        token = line.rstrip().split(\'\\t\')[0]\n      vocab_dict[token] = idx\n      idx += 1\n  return vocab_dict\n'"
open_seq2seq/decoders/__init__.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nThis package contains various decoder.\nA Decoder typically takes representation and produces data.\n""""""\nfrom .decoder import Decoder\nfrom .fc_decoders import FullyConnectedCTCDecoder, FullyConnectedDecoder, FullyConnectedSCDecoder\nfrom .rnn_decoders import RNNDecoderWithAttention, \\\n    BeamSearchRNNDecoderWithAttention\nfrom .transformer_decoder import TransformerDecoder\nfrom .convs2s_decoder import ConvS2SDecoder\nfrom .lm_decoders import FakeDecoder\nfrom .tacotron2_decoder import Tacotron2Decoder\nfrom .las_decoder import ListenAttendSpellDecoder\nfrom .jca_decoder import JointCTCAttentionDecoder\nfrom .centaur_decoder import CentaurDecoder'"
open_seq2seq/decoders/centaur_decoder.py,72,"b'# Copyright (c) 2019 NVIDIA Corporation\nimport tensorflow as tf\nfrom tensorflow.python.ops import math_ops\n\nfrom open_seq2seq.parts.centaur import AttentionBlock\nfrom open_seq2seq.parts.centaur import ConvBlock\nfrom open_seq2seq.parts.centaur import Prenet\nfrom open_seq2seq.parts.transformer import utils\nfrom open_seq2seq.parts.transformer.common import LayerNormalization\nfrom .decoder import Decoder\n\n\nclass CentaurDecoder(Decoder):\n  """"""\n  Centaur decoder that consists of attention blocks\n  followed by convolutional layers.\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(Decoder.get_required_params(), **{\n        ""prenet_layers"": int,\n        ""prenet_hidden_size"": int,\n        ""hidden_size"": int,\n        ""conv_layers"": list,\n        ""mag_conv_layers"": None,\n        ""attention_dropout"": float,\n        ""layer_postprocess_dropout"": float\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Decoder.get_optional_params(), **{\n        ""prenet_activation_fn"": None,\n        ""prenet_dropout"": float,\n        ""prenet_use_inference_dropout"": bool,\n        ""cnn_dropout_prob"": float,\n        ""bn_momentum"": float,\n        ""bn_epsilon"": float,\n        ""reduction_factor"": int,\n        ""attention_layers"": int,\n        ""self_attention_conv_params"": dict,\n        ""attention_heads"": int,\n        ""attention_cnn_dropout_prob"": float,\n        ""window_size"": int,\n        ""back_step_size"": int,\n        ""force_layers"": list\n    })\n\n  def __init__(self, params, model, name=""centaur_decoder"", mode=""train""):\n    """"""\n    Centaur decoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **prenet_layers** (int) --- number of fully-connected layers to use.\n    * **prenet_hidden_size** (int) --- number of units in each pre-net layer.\n    * **hidden_size** (int) --- dimensionality of hidden embeddings.\n    * **conv_layers** (list) --- list with the description of convolutional\n      layers. For example::\n        ""conv_layers"": [\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""VALID"", ""is_causal"": True\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""VALID"", ""is_causal"": True\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""VALID"", ""is_causal"": True\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""VALID"", ""is_causal"": True\n          }\n        ]\n    * **mag_conv_layers** (list) --- list with the description of convolutional\n      layers to reconstruct magnitude.\n    * **attention_dropout** (float) --- dropout rate for attention layers.\n    * **layer_postprocess_dropout** (float) --- dropout rate for\n      transformer block sublayers.\n    * **prenet_activation_fn** (callable) --- activation function to use for the\n      prenet lyaers. Defaults to relu.\n    * **prenet_dropout** (float) --- dropout rate for the pre-net. Defaults to 0.5.\n    * **prenet_use_inference_dropout** (bool) --- whether to use dropout during the inference.\n      Defaults to False.\n    * **cnn_dropout_prob** (float) --- dropout probabilty for cnn layers.\n      Defaults to 0.5.\n    * **bn_momentum** (float) --- momentum for batch norm. Defaults to 0.95.\n    * **bn_epsilon** (float) --- epsilon for batch norm. Defaults to 1e-8.\n    * **reduction_factor** (int) --- number of frames to predict in a time.\n      Defaults to 1.\n    * **attention_layers** (int) --- number of attention blocks. Defaults to 4.\n    * **self_attention_conv_params** (dict) --- description of convolutional\n      layer inside attention blocks. Defaults to None.\n    * **attention_heads** (int) --- number of attention heads. Defaults to 1.\n    * **attention_cnn_dropout_prob** (float) --- dropout rate for convolutional\n      layers inside attention blocks. Defaults to 0.5.\n    * **window_size** (int) --- size of attention window for forcing\n      monotonic attention during the inference. Defaults to None.\n    * **back_step_size** (int) --- number of steps attention is allowed to\n      go back during the inference. Defaults to 0.\n    * **force_layers** (list) --- indices of layers where forcing of\n      monotonic attention should be enabled. Defaults to all layers.\n    """"""\n\n    super(CentaurDecoder, self).__init__(params, model, name, mode)\n\n    data_layer_params = model.get_data_layer().params\n    n_feats = data_layer_params[""num_audio_features""]\n    use_mag = ""both"" in data_layer_params[""output_type""]\n\n    self.training = mode == ""train""\n    self.prenet = None\n    self.linear_projection = None\n    self.attentions = []\n    self.output_normalization = None\n    self.conv_layers = []\n    self.mag_conv_layers = []\n    self.stop_token_projection_layer = None\n    self.mel_projection_layer = None\n    self.mag_projection_layer = None\n\n    self.n_mel = n_feats[""mel""] if use_mag else n_feats\n    self.n_mag = n_feats[""magnitude""] if use_mag else None\n    self.reduction_factor = params.get(""reduction_factor"", 1)\n\n  def _build_layers(self):\n    regularizer = self._params.get(""regularizer"", None)\n    inference_dropout = self._params.get(""prenet_use_inference_dropout"", False)\n\n    self.prenet = Prenet(\n        n_layers=self._params[""prenet_layers""],\n        hidden_size=self._params[""prenet_hidden_size""],\n        activation_fn=self._params.get(""prenet_activation_fn"", tf.nn.relu),\n        dropout=self._params.get(""prenet_dropout"", 0.5),\n        regularizer=regularizer,\n        training=self.training or inference_dropout,\n        dtype=self._params[""dtype""]\n    )\n\n    cnn_dropout_prob = self._params.get(""cnn_dropout_prob"", 0.5)\n    bn_momentum = self._params.get(""bn_momentum"", 0.95)\n    bn_epsilon = self._params.get(""bn_epsilon"", -1e8)\n\n    self.linear_projection = tf.layers.Dense(\n        name=""linear_projection"",\n        units=self._params[""hidden_size""],\n        use_bias=False,\n        kernel_regularizer=regularizer,\n        dtype=self._params[""dtype""]\n    )\n\n    n_layers = self._params.get(""attention_layers"", 4)\n    n_heads = self._params.get(""attention_heads"", 1)\n    conv_params = self._params.get(""self_attention_conv_params"", None)\n    force_layers = self._params.get(""force_layers"", range(n_layers))\n\n    for index in range(n_layers):\n      window_size = None\n\n      if index in force_layers:\n        window_size = self._params.get(""window_size"", None)\n\n      attention = AttentionBlock(\n          name=""attention_block_%d"" % index,\n          hidden_size=self._params[""hidden_size""],\n          attention_dropout=self._params[""attention_dropout""],\n          layer_postprocess_dropout=self._params[""layer_postprocess_dropout""],\n          regularizer=regularizer,\n          training=self.training,\n          cnn_dropout_prob=self._params.get(""attention_cnn_dropout_prob"", 0.5),\n          conv_params=conv_params,\n          n_heads=n_heads,\n          window_size=window_size,\n          back_step_size=self._params.get(""back_step_size"", None)\n      )\n      self.attentions.append(attention)\n\n    self.output_normalization = LayerNormalization(self._params[""hidden_size""])\n\n    for index, params in enumerate(self._params[""conv_layers""]):\n      if params[""num_channels""] == -1:\n        params[""num_channels""] = self.n_mel * self.reduction_factor\n\n      layer = ConvBlock.create(\n          index=index,\n          conv_params=params,\n          regularizer=regularizer,\n          bn_momentum=bn_momentum,\n          bn_epsilon=bn_epsilon,\n          cnn_dropout_prob=cnn_dropout_prob,\n          training=self.training\n      )\n      self.conv_layers.append(layer)\n\n    for index, params in enumerate(self._params[""mag_conv_layers""]):\n      if params[""num_channels""] == -1:\n        params[""num_channels""] = self.n_mag * self.reduction_factor\n\n      layer = ConvBlock.create(\n          index=index,\n          conv_params=params,\n          regularizer=regularizer,\n          bn_momentum=bn_momentum,\n          bn_epsilon=bn_epsilon,\n          cnn_dropout_prob=cnn_dropout_prob,\n          training=self.training\n      )\n      self.mag_conv_layers.append(layer)\n\n    self.stop_token_projection_layer = tf.layers.Dense(\n        name=""stop_token_projection"",\n        units=1 * self.reduction_factor,\n        use_bias=True,\n        kernel_regularizer=regularizer\n    )\n\n    self.mel_projection_layer = tf.layers.Dense(\n        name=""mel_projection"",\n        units=self.n_mel * self.reduction_factor,\n        use_bias=True,\n        kernel_regularizer=regularizer\n    )\n\n    self.mag_projection_layer = tf.layers.Dense(\n        name=""mag_projection"",\n        units=self.n_mag * self.reduction_factor,\n        use_bias=True,\n        kernel_regularizer=regularizer\n    )\n\n  def _decode(self, input_dict):\n    self._build_layers()\n\n    if ""target_tensors"" in input_dict:\n      targets = input_dict[""target_tensors""][0]\n    else:\n      targets = None\n\n    encoder_outputs = input_dict[""encoder_output""][""outputs""]\n    attention_bias = input_dict[""encoder_output""][""inputs_attention_bias""]\n    spec_length = None\n\n    if self.mode == ""train"" or self.mode == ""eval"":\n      spec_length = None\n\n      if ""target_tensors"" in input_dict:\n        spec_length = input_dict[""target_tensors""][2]\n\n    if self.training:\n      return self._train(targets, encoder_outputs, attention_bias, spec_length)\n\n    return self._infer(encoder_outputs, attention_bias, spec_length)\n\n  def _decode_pass(self,\n                   decoder_inputs,\n                   encoder_outputs,\n                   enc_dec_attention_bias,\n                   sequence_lengths=None,\n                   alignment_positions=None):\n    y = self.prenet(decoder_inputs)\n    y = self.linear_projection(y)\n\n    with tf.variable_scope(""decoder_pos_encoding""):\n      pos_encoding = self._positional_encoding(y, self.params[""dtype""])\n      y += pos_encoding\n\n    with tf.variable_scope(""encoder_pos_encoding""):\n      pos_encoding = self._positional_encoding(encoder_outputs, self.params[""dtype""])\n      encoder_outputs += pos_encoding\n\n    for i, attention in enumerate(self.attentions):\n      positions = None\n\n      if alignment_positions is not None:\n        positions = alignment_positions[i, :, :, :]\n\n      y = attention(y, encoder_outputs, enc_dec_attention_bias, positions=positions)\n\n    y = self.output_normalization(y)\n\n    with tf.variable_scope(""conv_layers""):\n      for layer in self.conv_layers:\n        y = layer(y)\n\n    stop_token_logits = self.stop_token_projection_layer(y)\n    mel_spec = self.mel_projection_layer(y)\n\n    with tf.variable_scope(""mag_conv_layers""):\n      for layer in self.mag_conv_layers:\n        y = layer(y)\n\n    mag_spec = self.mag_projection_layer(y)\n\n    if sequence_lengths is None:\n      batch_size = tf.shape(y)[0]\n      sequence_lengths = tf.zeros([batch_size])\n\n    return {\n        ""spec"": mel_spec,\n        ""post_net_spec"": mel_spec,\n        ""alignments"": None,\n        ""stop_token_logits"": stop_token_logits,\n        ""lengths"": sequence_lengths,\n        ""mag_spec"": mag_spec\n    }\n\n  def _train(self, targets, encoder_outputs, enc_dec_attention_bias, sequence_lengths):\n    # Shift targets to the right, and remove the last element\n    with tf.name_scope(""shift_targets""):\n      n_features = self.n_mel + self.n_mag\n      targets = targets[:, :, :n_features]\n      targets = self._shrink(targets, n_features, self.reduction_factor)\n      decoder_inputs = tf.pad(targets, [[0, 0], [1, 0], [0, 0]])[:, :-1, :]\n\n    outputs = self._decode_pass(\n        decoder_inputs=decoder_inputs,\n        encoder_outputs=encoder_outputs,\n        enc_dec_attention_bias=enc_dec_attention_bias,\n        sequence_lengths=sequence_lengths\n    )\n\n    with tf.variable_scope(""alignments""):\n      weights = []\n\n      for index in range(len(self.attentions)):\n        op = ""ForwardPass/centaur_decoder/attention_block_%d/attention/attention/attention_weights"" % index\n        weights_operation = tf.get_default_graph().get_operation_by_name(op)\n        weight = weights_operation.values()[0]\n        weights.append(weight)\n\n      outputs[""alignments""] = [tf.stack(weights)]\n\n    return self._convert_outputs(\n        outputs,\n        self.reduction_factor,\n        self._model.params[""batch_size_per_gpu""]\n    )\n\n  def _infer(self, encoder_outputs, enc_dec_attention_bias, sequence_lengths):\n    if sequence_lengths is None:\n      maximum_iterations = self._model.get_data_layer()._params.get(""duration_max"", 1000)\n    else:\n      maximum_iterations = tf.reduce_max(sequence_lengths)\n\n    maximum_iterations //= self.reduction_factor\n\n    state, state_shape_invariants = self._inference_initial_state(\n        encoder_outputs,\n        enc_dec_attention_bias\n    )\n\n    state = tf.while_loop(\n        cond=self._inference_cond,\n        body=self._inference_step,\n        loop_vars=[state],\n        shape_invariants=state_shape_invariants,\n        back_prop=False,\n        maximum_iterations=maximum_iterations,\n        parallel_iterations=1\n    )\n\n    return self._convert_outputs(\n        state[""outputs""],\n        self.reduction_factor,\n        self._model.params[""batch_size_per_gpu""]\n    )\n\n  def _inference_initial_state(self, encoder_outputs, encoder_decoder_attention_bias):\n    """"""Create initial state for inference.""""""\n\n    with tf.variable_scope(""inference_initial_state""):\n      batch_size = tf.shape(encoder_outputs)[0]\n      n_layers = self._params.get(""attention_layers"", 1)\n      n_heads = self._params.get(""attention_heads"", 1)\n      n_features = self.n_mel + self.n_mag\n\n      state = {\n          ""iteration"": tf.constant(0),\n          ""inputs"": tf.zeros([batch_size, 1, n_features * self.reduction_factor]),\n          ""finished"": tf.cast(tf.zeros([batch_size]), tf.bool),\n          ""alignment_positions"": tf.zeros([n_layers, batch_size, n_heads, 1], dtype=tf.int32),\n          ""outputs"": {\n              ""spec"": tf.zeros([batch_size, 0, self.n_mel * self.reduction_factor]),\n              ""post_net_spec"": tf.zeros([batch_size, 0, self.n_mel * self.reduction_factor]),\n              ""alignments"": [\n                  tf.zeros([0, 0, 0, 0, 0])\n              ],\n              ""stop_token_logits"": tf.zeros([batch_size, 0, 1 * self.reduction_factor]),\n              ""lengths"": tf.zeros([batch_size], dtype=tf.int32),\n              ""mag_spec"": tf.zeros([batch_size, 0, self.n_mag * self.reduction_factor])\n          },\n          ""encoder_outputs"": encoder_outputs,\n          ""encoder_decoder_attention_bias"": encoder_decoder_attention_bias\n      }\n\n      state_shape_invariants = {\n          ""iteration"": tf.TensorShape([]),\n          ""inputs"": tf.TensorShape([None, None, n_features * self.reduction_factor]),\n          ""finished"": tf.TensorShape([None]),\n          ""alignment_positions"": tf.TensorShape([n_layers, None, n_heads, None]),\n          ""outputs"": {\n              ""spec"": tf.TensorShape([None, None, self.n_mel * self.reduction_factor]),\n              ""post_net_spec"": tf.TensorShape([None, None, self.n_mel * self.reduction_factor]),\n              ""alignments"": [\n                  tf.TensorShape([None, None, None, None, None]),\n              ],\n              ""stop_token_logits"": tf.TensorShape([None, None, 1 * self.reduction_factor]),\n              ""lengths"": tf.TensorShape([None]),\n              ""mag_spec"": tf.TensorShape([None, None, None])\n          },\n          ""encoder_outputs"": encoder_outputs.shape,\n          ""encoder_decoder_attention_bias"": encoder_decoder_attention_bias.shape\n      }\n\n      return state, state_shape_invariants\n\n  def _inference_cond(self, state):\n    """"""Check if it\'s time to stop inference.""""""\n\n    with tf.variable_scope(""inference_cond""):\n      all_finished = math_ops.reduce_all(state[""finished""])\n      return tf.logical_not(all_finished)\n\n  def _inference_step(self, state):\n    """"""Make one inference step.""""""\n\n    decoder_inputs = state[""inputs""]\n    encoder_outputs = state[""encoder_outputs""]\n    enc_dec_attention_bias = state[""encoder_decoder_attention_bias""]\n    alignment_positions = state[""alignment_positions""]\n\n    outputs = self._decode_pass(\n        decoder_inputs=decoder_inputs,\n        encoder_outputs=encoder_outputs,\n        enc_dec_attention_bias=enc_dec_attention_bias,\n        alignment_positions=alignment_positions\n    )\n\n    with tf.variable_scope(""inference_step""):\n      next_inputs_mel = outputs[""post_net_spec""][:, -1:, :]\n      next_inputs_mel = self._expand(next_inputs_mel, self.reduction_factor)\n      next_inputs_mag = outputs[""mag_spec""][:, -1:, :]\n      next_inputs_mag = self._expand(next_inputs_mag, self.reduction_factor)\n      next_inputs = tf.concat([next_inputs_mel, next_inputs_mag], axis=-1)\n\n      n_features = self.n_mel + self.n_mag\n      next_inputs = self._shrink(next_inputs, n_features, self.reduction_factor)\n\n      # Set zero if sequence is finished\n      next_inputs = tf.where(\n          state[""finished""],\n          tf.zeros_like(next_inputs),\n          next_inputs\n      )\n      next_inputs = tf.concat([decoder_inputs, next_inputs], 1)\n\n      # Update lengths\n      lengths = state[""outputs""][""lengths""]\n      lengths = tf.where(\n          state[""finished""],\n          lengths,\n          lengths + 1 * self.reduction_factor\n      )\n      outputs[""lengths""] = lengths\n\n      # Update spec, post_net_spec and mag_spec\n      for key in [""spec"", ""post_net_spec"", ""mag_spec""]:\n        output = outputs[key][:, -1:, :]\n        output = tf.where(state[""finished""], tf.zeros_like(output), output)\n        outputs[key] = tf.concat([state[""outputs""][key], output], 1)\n\n      # Update stop token logits\n      stop_token_logits = outputs[""stop_token_logits""][:, -1:, :]\n      stop_token_logits = tf.where(\n          state[""finished""],\n          tf.zeros_like(stop_token_logits) + 1e9,\n          stop_token_logits\n      )\n      stop_prediction = tf.sigmoid(stop_token_logits)\n      stop_prediction = tf.reduce_max(stop_prediction, axis=-1)\n\n      # Uncomment next line if you want to use stop token predictions\n      finished = tf.reshape(tf.cast(tf.round(stop_prediction), tf.bool), [-1])\n      finished = tf.reshape(finished, [-1])\n\n      stop_token_logits = tf.concat(\n          [state[""outputs""][""stop_token_logits""], stop_token_logits],\n          axis=1\n      )\n      outputs[""stop_token_logits""] = stop_token_logits\n\n      with tf.variable_scope(""alignments""):\n        forward = ""ForwardPass"" if self.mode == ""infer"" else ""ForwardPass_1""\n        weights = []\n\n        for index in range(len(self.attentions)):\n          op = forward + ""/centaur_decoder/while/attention_block_%d/attention/attention/attention_weights"" % index\n          weights_operation = tf.get_default_graph().get_operation_by_name(op)\n          weight = weights_operation.values()[0]\n          weights.append(weight)\n\n        weights = tf.stack(weights)\n        outputs[""alignments""] = [weights]\n\n      alignment_positions = tf.argmax(\n          weights,\n          axis=-1,\n          output_type=tf.int32\n      )[:, :, :, -1:]\n      state[""alignment_positions""] = tf.concat(\n          [state[""alignment_positions""], alignment_positions],\n          axis=-1\n      )\n\n      state[""iteration""] = state[""iteration""] + 1\n      state[""inputs""] = next_inputs\n      state[""finished""] = finished\n      state[""outputs""] = outputs\n\n    return state\n\n  @staticmethod\n  def _shrink(values, last_dim, reduction_factor):\n    """"""Shrink the given input by reduction_factor.""""""\n\n    shape = tf.shape(values)\n    new_shape = [\n        shape[0],\n        shape[1] // reduction_factor,\n        last_dim * reduction_factor\n    ]\n    values = tf.reshape(values, new_shape)\n    return values\n\n  @staticmethod\n  def _expand(values, reduction_factor):\n    """"""Expand the given input by reduction_factor.""""""\n\n    shape = tf.shape(values)\n    new_shape = [\n        shape[0],\n        shape[1] * reduction_factor,\n        shape[2] // reduction_factor\n    ]\n    values = tf.reshape(values, new_shape)\n    return values\n\n  @staticmethod\n  def _positional_encoding(x, dtype):\n    """"""Add positional encoding to the given input.""""""\n\n    length = tf.shape(x)[1]\n    features_count = tf.shape(x)[2]\n    features_count += features_count % 2\n    pos_encoding = utils.get_position_encoding(length, features_count)\n    position_encoding = tf.cast(pos_encoding, dtype)\n    position_encoding = position_encoding[:, :features_count]\n    return position_encoding\n\n  @staticmethod\n  def _convert_outputs(outputs, reduction_factor, batch_size):\n    """"""Convert output of the decoder to appropriate format.""""""\n\n    with tf.variable_scope(""output_converter""):\n      for key in [""spec"", ""post_net_spec"", ""stop_token_logits"", ""mag_spec""]:\n        outputs[key] = CentaurDecoder._expand(outputs[key], reduction_factor)\n\n      alignments = []\n      for sample in range(batch_size):\n        alignments.append([outputs[""alignments""][0][:, sample, :, :, :]])\n\n      return {\n          ""outputs"": [\n              outputs[""spec""],\n              outputs[""post_net_spec""],\n              alignments,\n              tf.sigmoid(outputs[""stop_token_logits""]),\n              outputs[""lengths""],\n              outputs[""mag_spec""]\n          ],\n          ""stop_token_prediction"": outputs[""stop_token_logits""]\n      }\n'"
open_seq2seq/decoders/convs2s_decoder.py,23,"b'from __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nimport math\nfrom .decoder import Decoder\n\nfrom open_seq2seq.parts.transformer import beam_search\n\nfrom open_seq2seq.parts.transformer import embedding_layer\nfrom open_seq2seq.parts.transformer.utils import get_padding\n\nfrom open_seq2seq.parts.convs2s import ffn_wn_layer, conv_wn_layer, attention_wn_layer\nfrom open_seq2seq.parts.convs2s.utils import gated_linear_units\n\n# Default value used if max_input_length is not given\nMAX_INPUT_LENGTH = 128\n\n\nclass ConvS2SDecoder(Decoder):\n\n  @staticmethod\n  def get_required_params():\n    """"""Static method with description of required parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **have to** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return dict(\n        Decoder.get_required_params(), **{\n            \'batch_size\': int,\n            \'tgt_emb_size\': int,\n            \'tgt_vocab_size\': int,\n            \'shared_embed\': bool,\n            \'embedding_dropout_keep_prob\': float,\n            \'conv_nchannels_kwidth\': list,\n            \'hidden_dropout_keep_prob\': float,\n            \'out_dropout_keep_prob\': float,\n            \'beam_size\': int,\n            \'alpha\': float,\n            \'extra_decode_length\': int,\n            \'EOS_ID\': int,\n        })\n\n  @staticmethod\n  def get_optional_params():\n    """"""Static method with description of optional parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **can** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return dict(\n        Decoder.get_optional_params(),\n        **{\n            \'pad_embeddings_2_eight\': bool,\n            # set the default to False later.\n            ""pos_embed"": bool,\n            # if not provided, tgt_emb_size is used as the default value\n            \'out_emb_size\': int,\n            \'max_input_length\': int,\n            \'GO_SYMBOL\': int,\n            \'PAD_SYMBOL\': int,\n            \'END_SYMBOL\': int,\n            \'conv_activation\': None,\n            \'normalization_type\': str,\n            \'scaling_factor\': float,\n            \'init_var\': None,\n        })\n\n  def _cast_types(self, input_dict):\n    return input_dict\n\n  def __init__(self, params, model, name=""convs2s_decoder"", mode=\'train\'):\n    super(ConvS2SDecoder, self).__init__(params, model, name, mode)\n    self.embedding_softmax_layer = None\n    self.position_embedding_layer = None\n    self.layers = []\n    self._tgt_vocab_size = self.params[\'tgt_vocab_size\']\n    self._tgt_emb_size = self.params[\'tgt_emb_size\']\n    self._mode = mode\n    self._pad_sym = self.params.get(\'PAD_SYMBOL\', 0)\n    self._pad2eight = params.get(\'pad_embeddings_2_eight\', False)\n    self.scaling_factor = self.params.get(""scaling_factor"", math.sqrt(0.5))\n    self.normalization_type = self.params.get(""normalization_type"", ""weight_norm"")\n    self.conv_activation = self.params.get(""conv_activation"", gated_linear_units)\n    self.max_input_length = self.params.get(""max_input_length"", MAX_INPUT_LENGTH)\n    self.init_var = self.params.get(\'init_var\', None)\n    self.regularizer = self.params.get(\'regularizer\', None)\n\n  def _decode(self, input_dict):\n    targets = input_dict[\'target_tensors\'][0] \\\n              if \'target_tensors\' in input_dict else None\n\n    encoder_outputs = input_dict[\'encoder_output\'][\'outputs\']\n    encoder_outputs_b = input_dict[\'encoder_output\'].get(\n        \'outputs_b\', encoder_outputs)\n\n    inputs_attention_bias = input_dict[\'encoder_output\'].get(\n        \'inputs_attention_bias_cs2s\', None)\n\n    with tf.name_scope(""decode""):\n      # prepare decoder layers\n      if len(self.layers) == 0:\n        knum_list = list(zip(*self.params.get(""conv_nchannels_kwidth"")))[0]\n        kwidth_list = list(zip(*self.params.get(""conv_nchannels_kwidth"")))[1]\n\n        # preparing embedding layers\n        with tf.variable_scope(""embedding""):\n          if \'embedding_softmax_layer\' in input_dict[\'encoder_output\'] \\\n                  and self.params[\'shared_embed\']:\n            self.embedding_softmax_layer = \\\n              input_dict[\'encoder_output\'][\'embedding_softmax_layer\']\n          else:\n            self.embedding_softmax_layer = embedding_layer.EmbeddingSharedWeights(\n                vocab_size=self._tgt_vocab_size,\n                hidden_size=self._tgt_emb_size,\n                pad_vocab_to_eight=self._pad2eight,\n                init_var=0.1,\n                embed_scale=False,\n                pad_sym=self._pad_sym,\n                mask_paddings=True)\n\n        if self.params.get(""pos_embed"", True):\n          with tf.variable_scope(""pos_embedding""):\n            if \'position_embedding_layer\' in input_dict[\'encoder_output\'] \\\n                    and self.params[\'shared_embed\']:\n              self.position_embedding_layer = \\\n                input_dict[\'encoder_output\'][\'position_embedding_layer\']\n            else:\n              self.position_embedding_layer = embedding_layer.EmbeddingSharedWeights(\n                  vocab_size=self.max_input_length,\n                  hidden_size=self._tgt_emb_size,\n                  pad_vocab_to_eight=self._pad2eight,\n                  init_var=0.1,\n                  embed_scale=False,\n                  pad_sym=self._pad_sym,\n                  mask_paddings=True)\n        else:\n          self.position_embedding_layer = None\n\n        # linear projection before cnn layers\n        self.layers.append(\n            ffn_wn_layer.FeedFowardNetworkNormalized(\n                self._tgt_emb_size,\n                knum_list[0],\n                dropout=self.params[""embedding_dropout_keep_prob""],\n                var_scope_name=""linear_mapping_before_cnn_layers"",\n                mode=self.mode,\n                normalization_type=self.normalization_type,\n                regularizer=self.regularizer,\n                init_var=self.init_var)\n          )\n\n        for i in range(len(knum_list)):\n          in_dim = knum_list[i] if i == 0 else knum_list[i - 1]\n          out_dim = knum_list[i]\n\n          # linear projection is needed for residual connections if\n          # input and output of a cnn layer do not match\n          if in_dim != out_dim:\n            linear_proj = ffn_wn_layer.FeedFowardNetworkNormalized(\n                in_dim,\n                out_dim,\n                var_scope_name=""linear_mapping_cnn_"" + str(i + 1),\n                dropout=1.0,\n                mode=self.mode,\n                normalization_type=self.normalization_type,\n                regularizer = self.regularizer,\n                init_var = self.init_var,\n            )\n          else:\n            linear_proj = None\n\n          conv_layer = conv_wn_layer.Conv1DNetworkNormalized(\n              in_dim,\n              out_dim,\n              kernel_width=kwidth_list[i],\n              mode=self.mode,\n              layer_id=i + 1,\n              hidden_dropout=self.params[""hidden_dropout_keep_prob""],\n              conv_padding=""VALID"",\n              decode_padding=True,\n              activation=self.conv_activation,\n              normalization_type=self.normalization_type,\n              regularizer=self.regularizer,\n              init_var=self.init_var\n          )\n\n          att_layer = attention_wn_layer.AttentionLayerNormalized(\n              out_dim,\n              embed_size=self._tgt_emb_size,\n              layer_id=i + 1,\n              add_res=True,\n              mode=self.mode,\n              normalization_type=self.normalization_type,\n              scaling_factor=self.scaling_factor,\n              regularizer=self.regularizer,\n              init_var=self.init_var\n          )\n\n          self.layers.append([linear_proj, conv_layer, att_layer])\n\n        # linear projection after cnn layers\n        self.layers.append(\n            ffn_wn_layer.FeedFowardNetworkNormalized(\n                knum_list[-1],\n                self.params.get(""out_emb_size"", self._tgt_emb_size),\n                dropout=1.0,\n                var_scope_name=""linear_mapping_after_cnn_layers"",\n                mode=self.mode,\n                normalization_type=self.normalization_type,\n                regularizer=self.regularizer,\n                init_var=self.init_var))\n\n        if not self.params[\'shared_embed\']:\n          self.layers.append(\n              ffn_wn_layer.FeedFowardNetworkNormalized(\n                  self.params.get(""out_emb_size"", self._tgt_emb_size),\n                  self._tgt_vocab_size,\n                  dropout=self.params[""out_dropout_keep_prob""],\n                  var_scope_name=""linear_mapping_to_vocabspace"",\n                  mode=self.mode,\n                  normalization_type=self.normalization_type,\n                  regularizer=self.regularizer,\n                  init_var=self.init_var))\n        else:\n          # if embedding is shared,\n          # the shared embedding is used as the final linear projection to vocab space\n          self.layers.append(None)\n\n      if targets is None:\n        return self.predict(encoder_outputs, encoder_outputs_b,\n                            inputs_attention_bias)\n      else:\n        logits = self.decode_pass(targets, encoder_outputs, encoder_outputs_b,\n                                  inputs_attention_bias)\n      return {\n          ""logits"": logits,\n          ""outputs"": [tf.argmax(logits, axis=-1)],\n          ""final_state"": None,\n          ""final_sequence_lengths"": None\n      }\n\n  def decode_pass(self, targets, encoder_outputs, encoder_outputs_b,\n                  inputs_attention_bias):\n    """"""Generate logits for each value in the target sequence.\n\n    Args:\n      targets: target values for the output sequence.\n        int tensor with shape [batch_size, target_length]\n      encoder_outputs: continuous representation of input sequence.\n        float tensor with shape [batch_size, input_length, hidden_size]\n        float tensor with shape [batch_size, input_length, hidden_size]\n      encoder_outputs_b: continuous representation of input sequence\n        which includes the source embeddings.\n        float tensor with shape [batch_size, input_length, hidden_size]\n      inputs_attention_bias: float tensor with shape [batch_size, 1, input_length]\n\n    Returns:\n      float32 tensor with shape [batch_size, target_length, vocab_size]\n    """"""\n\n    # Prepare inputs to decoder layers by applying embedding\n    # and adding positional encoding.\n    decoder_inputs = self.embedding_softmax_layer(targets)\n\n    if self.position_embedding_layer is not None:\n      with tf.name_scope(""add_pos_encoding""):\n        pos_input = tf.range(\n            0,\n            tf.shape(decoder_inputs)[1],\n            delta=1,\n            dtype=tf.int32,\n            name=\'range\')\n        pos_encoding = self.position_embedding_layer(pos_input)\n        decoder_inputs = decoder_inputs + tf.cast(\n            x=pos_encoding, dtype=decoder_inputs.dtype)\n\n    if self.mode == ""train"":\n      decoder_inputs = tf.nn.dropout(decoder_inputs,\n                                     self.params[""embedding_dropout_keep_prob""])\n\n    # mask the paddings in the target\n    inputs_padding = get_padding(\n        targets, padding_value=self._pad_sym, dtype=decoder_inputs.dtype)\n    decoder_inputs *= tf.expand_dims(1.0 - inputs_padding, 2)\n\n    # do decode\n    logits = self._call(\n        decoder_inputs=decoder_inputs,\n        encoder_outputs_a=encoder_outputs,\n        encoder_outputs_b=encoder_outputs_b,\n        input_attention_bias=inputs_attention_bias)\n\n    return logits\n\n  def _call(self, decoder_inputs, encoder_outputs_a, encoder_outputs_b,\n            input_attention_bias):\n    # run input into the decoder layers and returns the logits\n    target_embed = decoder_inputs\n    with tf.variable_scope(""linear_layer_before_cnn_layers""):\n      outputs = self.layers[0](decoder_inputs)\n\n    for i in range(1, len(self.layers) - 2):\n      linear_proj, conv_layer, att_layer = self.layers[i]\n\n      with tf.variable_scope(""layer_%d"" % i):\n        if linear_proj is not None:\n          res_inputs = linear_proj(outputs)\n        else:\n          res_inputs = outputs\n\n        with tf.variable_scope(""conv_layer""):\n          outputs = conv_layer(outputs)\n\n        with tf.variable_scope(""attention_layer""):\n          outputs = att_layer(outputs, target_embed, encoder_outputs_a,\n                              encoder_outputs_b, input_attention_bias)\n        outputs = (outputs + res_inputs) * self.scaling_factor\n\n\n    with tf.variable_scope(""linear_layer_after_cnn_layers""):\n      outputs = self.layers[-2](outputs)\n\n    if self.mode == ""train"":\n      outputs = tf.nn.dropout(outputs, self.params[""out_dropout_keep_prob""])\n\n    with tf.variable_scope(""pre_softmax_projection""):\n      if self.layers[-1] is None:\n        logits = self.embedding_softmax_layer.linear(outputs)\n      else:\n        logits = self.layers[-1](outputs)\n\n    return tf.cast(logits, dtype=tf.float32)\n\n  def predict(self, encoder_outputs, encoder_outputs_b, inputs_attention_bias):\n    """"""Return predicted sequence.""""""\n    batch_size = tf.shape(encoder_outputs)[0]\n    input_length = tf.shape(encoder_outputs)[1]\n\n    max_decode_length = input_length + self.params[""extra_decode_length""]\n\n    symbols_to_logits_fn = self._get_symbols_to_logits_fn()\n\n    # Create initial set of IDs that will be passed into symbols_to_logits_fn.\n    initial_ids = tf.zeros(\n        [batch_size], dtype=tf.int32) + self.params[""GO_SYMBOL""]\n\n    cache = {}\n    # Add encoder outputs and attention bias to the cache.\n    cache[""encoder_outputs""] = encoder_outputs\n    cache[""encoder_outputs_b""] = encoder_outputs_b\n    if inputs_attention_bias is not None:\n      cache[""inputs_attention_bias""] = inputs_attention_bias\n\n    # Use beam search to find the top beam_size sequences and scores.\n    decoded_ids, scores = beam_search.sequence_beam_search(\n        symbols_to_logits_fn=symbols_to_logits_fn,\n        initial_ids=initial_ids,\n        initial_cache=cache,\n        vocab_size=self.params[""tgt_vocab_size""],\n        beam_size=self.params[""beam_size""],\n        alpha=self.params[""alpha""],\n        max_decode_length=max_decode_length,\n        eos_id=self.params[""EOS_ID""])\n\n    # Get the top sequence for each batch element\n    top_decoded_ids = decoded_ids[:, 0, :]\n    top_scores = scores[:, 0]\n\n    # this isn\'t particularly efficient\n    logits = self.decode_pass(top_decoded_ids, encoder_outputs,\n                              encoder_outputs_b, inputs_attention_bias)\n\n    return {\n        ""logits"": logits,\n        ""outputs"": [top_decoded_ids],\n        ""final_state"": None,\n        ""final_sequence_lengths"": None\n    }\n\n  def _get_symbols_to_logits_fn(self):\n    """"""Returns a decoding function that calculates logits of the next tokens.""""""\n\n    def symbols_to_logits_fn(ids, i, cache):\n      """"""Generate logits for next potential IDs.\n\n      Args:\n        ids: Current decoded sequences.\n          int tensor with shape [batch_size * beam_size, i - 1]\n        i: Loop index\n        cache: dictionary of values storing the encoder output, encoder-decoder\n          attention bias, and previous decoder attention values.\n\n      Returns:\n        Tuple of\n          (logits with shape [batch_size * beam_size, vocab_size],\n           updated cache values)\n      """"""\n\n      # pass the decoded ids from the beginneing up to the current into the decoder\n      # not efficient\n      decoder_outputs = self.decode_pass(ids, cache.get(""encoder_outputs""),\n                                         cache.get(""encoder_outputs_b""),\n                                         cache.get(""inputs_attention_bias""))\n\n      logits = decoder_outputs[:, i, :]\n      return logits, cache\n\n    return symbols_to_logits_fn\n'"
open_seq2seq/decoders/decoder.py,5,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport abc\nimport copy\n\nimport six\nimport tensorflow as tf\n\nfrom open_seq2seq.optimizers.mp_wrapper import mp_regularizer_wrapper\nfrom open_seq2seq.utils.utils import check_params, cast_types\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Decoder:\n  """"""Abstract class from which all decoders must inherit.\n  """"""\n  @staticmethod\n  def get_required_params():\n    """"""Static method with description of required parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **have to** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {}\n\n  @staticmethod\n  def get_optional_params():\n    """"""Static method with description of optional parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **can** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {\n        \'regularizer\': None,  # any valid TensorFlow regularizer\n        \'regularizer_params\': dict,\n        \'initializer\': None,  # any valid TensorFlow initializer\n        \'initializer_params\': dict,\n        \'dtype\': [tf.float32, tf.float16, \'mixed\'],\n    }\n\n  def __init__(self, params, model, name=""decoder"", mode=\'train\'):\n    """"""Decoder constructor.\n    Note that decoder constructors should not modify TensorFlow graph, all\n    graph construction should happen in the :meth:`self._decode() <_decode>`\n    method.\n\n    Args:\n      params (dict): parameters describing the decoder.\n          All supported parameters are listed in :meth:`get_required_params`,\n          :meth:`get_optional_params` functions.\n      model (instance of a class derived from :class:`Model<models.model.Model>`):\n          parent model that created this decoder.\n          Could be None if no model access is required for the use case.\n      name (str): name for decoder variable scope.\n      mode (str): mode decoder is going to be run in.\n          Could be ""train"", ""eval"" or ""infer"".\n\n    Config parameters:\n\n    * **initializer** --- any valid TensorFlow initializer. If no initializer\n      is provided, model initializer will be used.\n    * **initializer_params** (dict) --- dictionary that will be passed to\n      initializer ``__init__`` method.\n    * **regularizer** --- and valid TensorFlow regularizer. If no regularizer\n      is provided, model regularizer will be used.\n    * **regularizer_params** (dict) --- dictionary that will be passed to\n      regularizer ``__init__`` method.\n    * **dtype** --- model dtype. Could be either ``tf.float16``, ``tf.float32``\n      or ""mixed"". For details see\n      :ref:`mixed precision training <mixed_precision>` section in docs. If no\n      dtype is provided, model dtype will be used.\n    """"""\n    check_params(params, self.get_required_params(), self.get_optional_params())\n    self._params = copy.deepcopy(params)\n    self._model = model\n\n    if \'dtype\' not in self._params:\n      if self._model:\n        self._params[\'dtype\'] = self._model.params[\'dtype\']\n      else:\n        self._params[\'dtype\'] = tf.float32\n\n    self._name = name\n    self._mode = mode\n    self._compiled = False\n\n  def decode(self, input_dict):\n    """"""Wrapper around :meth:`self._decode() <_decode>` method.\n    Here name, initializer and dtype are set in the variable scope and then\n    :meth:`self._decode() <_decode>` method is called.\n\n    Args:\n      input_dict (dict): see :meth:`self._decode() <_decode>` docs.\n\n    Returns:\n      see :meth:`self._decode() <_decode>` docs.\n    """"""\n    if not self._compiled:\n      if \'regularizer\' not in self._params:\n        if self._model and \'regularizer\' in self._model.params:\n          self._params[\'regularizer\'] = copy.deepcopy(\n              self._model.params[\'regularizer\']\n          )\n          self._params[\'regularizer_params\'] = copy.deepcopy(\n              self._model.params[\'regularizer_params\']\n          )\n\n      if \'regularizer\' in self._params:\n        init_dict = self._params.get(\'regularizer_params\', {})\n        if self._params[\'regularizer\'] is not None:\n          self._params[\'regularizer\'] = self._params[\'regularizer\'](**init_dict)\n        if self._params[\'dtype\'] == \'mixed\':\n          self._params[\'regularizer\'] = mp_regularizer_wrapper(\n              self._params[\'regularizer\'],\n          )\n\n      if self._params[\'dtype\'] == \'mixed\':\n        self._params[\'dtype\'] = tf.float16\n      \n    if \'initializer\' in self.params:\n      init_dict = self.params.get(\'initializer_params\', {})\n      initializer = self.params[\'initializer\'](**init_dict)\n    else:\n      initializer = None\n\n    self._compiled = True\n\n    with tf.variable_scope(self._name, initializer=initializer,\n                           dtype=self.params[\'dtype\']):\n      return self._decode(self._cast_types(input_dict))\n\n  def _cast_types(self, input_dict):\n    """"""This function performs automatic cast of all inputs to decoder dtype.\n\n    Args:\n      input_dict (dict): dictionary passed to :meth:`self._decode() <_decode>`\n          method.\n\n    Returns:\n      dict: same as input_dict, but with all Tensors cast to decoder dtype.\n    """"""\n    return cast_types(input_dict, self.params[\'dtype\'])\n\n  @abc.abstractmethod\n  def _decode(self, input_dict):\n    """"""This is the main function which should construct decoder graph.\n    Typically, decoder will take hidden representation from encoder as an input\n    and produce some output sequence as an output.\n\n    Args:\n      input_dict (dict): dictionary containing decoder inputs.\n          If the decoder is used with :class:`models.encoder_decoder` class,\n          ``input_dict`` will have the following content::\n            {\n              ""encoder_output"": dictionary returned from encoder.encode() method\n              ""target_tensors"": data_layer.input_tensors[\'target_tensors\']\n            }\n\n\n    Returns:\n      dict: dictionary of decoder outputs. Typically this will be just::\n\n          {\n            ""logits"": logits that will be passed to Loss\n            ""outputs"": list with actual decoded outputs, e.g. characters\n                       instead of logits\n          }\n    """"""\n    pass\n\n  @property\n  def params(self):\n    """"""Parameters used to construct the decoder (dictionary)""""""\n    return self._params\n\n  @property\n  def mode(self):\n    """"""Mode decoder is run in.""""""\n    return self._mode\n\n  @property\n  def name(self):\n    """"""Decoder name.""""""\n    return self._name\n'"
open_seq2seq/decoders/fc_decoders.py,15,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""This module defines various fully-connected decoders (consisting of one\nfully connected layer).\n\nThese classes are usually used for models that are not really\nsequence-to-sequence and thus should be artificially split into encoder and\ndecoder by cutting, for example, on the last fully-connected layer.\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nimport tensorflow as tf\n\nfrom .decoder import Decoder\n\n\nclass FullyConnectedDecoder(Decoder):\n  """"""Simple decoder consisting of one fully-connected layer.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Decoder.get_required_params(), **{\n        \'output_dim\': int,\n    })\n\n  def __init__(self, params, model,\n               name=""fully_connected_decoder"", mode=\'train\'):\n    """"""Fully connected decoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **output_dim** (int) --- output dimension.\n    """"""\n    super(FullyConnectedDecoder, self).__init__(params, model, name, mode)\n\n  def _decode(self, input_dict):\n    """"""This method performs linear transformation of input.\n\n    Args:\n      input_dict (dict): input dictionary that has to contain\n          the following fields::\n            input_dict = {\n              \'encoder_output\': {\n                \'outputs\': output of encoder (shape=[batch_size, num_features])\n              }\n            }\n\n    Returns:\n      dict: dictionary with the following tensors::\n\n        {\n          \'logits\': logits with the shape=[batch_size, output_dim]\n          \'outputs\': [logits] (same as logits but wrapped in list)\n        }\n    """"""\n    inputs = input_dict[\'encoder_output\'][\'outputs\']\n    regularizer = self.params.get(\'regularizer\', None)\n\n    # activation is linear by default\n    logits = tf.layers.dense(\n        inputs=inputs,\n        units=self.params[\'output_dim\'],\n        kernel_regularizer=regularizer,\n        name=\'fully_connected\',\n    )\n    return {\'logits\': logits, \'outputs\': [logits]}\n\n\nclass FullyConnectedTimeDecoder(Decoder):\n  """"""Fully connected decoder that operates on inputs with time dimension.\n  That is, input shape should be ``[batch size, time length, num features]``.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Decoder.get_required_params(), **{\n        \'tgt_vocab_size\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Decoder.get_optional_params(), **{\n        \'logits_to_outputs_func\': None,  # user defined function\n        \'infer_logits_to_pickle\': bool,\n    })\n\n  def __init__(self, params, model,\n               name=""fully_connected_time_decoder"", mode=\'train\'):\n    """"""Fully connected time decoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **tgt_vocab_size** (int) --- target vocabulary size, i.e. number of\n      output features.\n    * **logits_to_outputs_func** --- function that maps produced logits to\n      decoder outputs, i.e. actual text sequences.\n    """"""\n    super(FullyConnectedTimeDecoder, self).__init__(params, model, name, mode)\n\n  def _decode(self, input_dict):\n    """"""Creates TensorFlow graph for fully connected time decoder.\n\n    Args:\n      input_dict (dict): input dictionary that has to contain\n          the following fields::\n            input_dict = {\n              \'encoder_output\': {\n                ""outputs"": tensor with shape [batch_size, time length, hidden dim]\n                ""src_length"": tensor with shape [batch_size]\n              }\n            }\n\n    Returns:\n      dict: dictionary with the following tensors::\n\n        {\n          \'logits\': logits with the shape=[time length, batch_size, tgt_vocab_size]\n          \'outputs\': logits_to_outputs_func(logits, input_dict)\n        }\n    """"""\n    inputs = input_dict[\'encoder_output\'][\'outputs\']\n    regularizer = self.params.get(\'regularizer\', None)\n\n    batch_size, _, n_hidden = inputs.get_shape().as_list()\n    # reshape from [B, T, A] --> [B*T, A].\n    # Output shape: [n_steps * batch_size, n_hidden]\n    inputs = tf.reshape(inputs, [-1, n_hidden])\n\n    # activation is linear by default\n    logits = tf.layers.dense(\n        inputs=inputs,\n        units=self.params[\'tgt_vocab_size\'],\n        kernel_regularizer=regularizer,\n        name=\'fully_connected\',\n    )\n    logits = tf.reshape(\n        logits,\n        [batch_size, -1, self.params[\'tgt_vocab_size\']],\n        name=""logits"",\n    )\n    # converting to time_major=True shape\n    if not(self._mode==\'infer\' and self.params.get(\'infer_logits_to_pickle\')):\n      logits = tf.transpose(logits, [1, 0, 2])\n    if \'logits_to_outputs_func\' in self.params:\n      outputs = self.params[\'logits_to_outputs_func\'](logits, input_dict)\n\n      return {\n          \'outputs\': outputs,\n          \'logits\': logits,\n          \'src_length\': input_dict[\'encoder_output\'][\'src_length\'],\n      }\n    return {\'logits\': logits,\n            \'src_length\': input_dict[\'encoder_output\'][\'src_length\']}\n\n\nclass FullyConnectedCTCDecoder(FullyConnectedTimeDecoder):\n  """"""Fully connected time decoder that provides a CTC-based text\n  generation (either with or without language model). If language model is not\n  used, ``tf.nn.ctc_greedy_decoder`` will be used as text generation method.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return FullyConnectedTimeDecoder.get_required_params()\n\n  @staticmethod\n  def get_optional_params():\n    return dict(FullyConnectedTimeDecoder.get_optional_params(), **{\n        \'use_language_model\': bool,\n        \'decoder_library_path\': str,\n        \'beam_width\': int,\n        \'alpha\': float,\n        \'beta\': float,\n        \'trie_weight\': float,\n        \'lm_path\': str,\n        \'trie_path\': str,\n        \'alphabet_config_path\': str,\n    })\n\n  def __init__(self, params, model,\n               name=""fully_connected_ctc_decoder"", mode=\'train\'):\n    """"""Fully connected CTC decoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **use_language_model** (bool) --- whether to use language model for\n      output text generation. If False, other config parameters are not used.\n    * **decoder_library_path** (string) --- path to the ctc decoder with\n      language model library.\n    * **lm_path** (string) --- path to the language model file.\n    * **trie_path** (string) --- path to the prefix trie file.\n    * **alphabet_config_path** (string) --- path to the alphabet file.\n    * **beam_width** (int) --- beam width for beam search.\n    * **alpha** (float) --- weight that is assigned to language model\n      probabilities.\n    * **beta** (float) --- weight that is assigned to the\n      word count.\n    * **trie_weight** (float) --- weight for prefix tree vocabulary\n      based character level rescoring.\n    """"""\n    super(FullyConnectedCTCDecoder, self).__init__(params, model, name, mode)\n\n    self.params[\'use_language_model\'] = self.params.get(\'use_language_model\', \n                                                        False)\n    if self.params[\'use_language_model\']:\n      # creating decode_with_lm function if it is compiled\n      lib_path = self.params[\'decoder_library_path\']\n      if not os.path.exists(os.path.abspath(lib_path)):\n        raise IOError(\'Can\\\'t find the decoder with language model library. \'\n                      \'Make sure you have built it and \'\n                      \'check that you provide the correct \'\n                      \'path in the --decoder_library_path parameter.\')\n\n      custom_op_module = tf.load_op_library(lib_path)\n\n      def decode_with_lm(logits, decoder_input,\n                         beam_width=self.params[\'beam_width\'],\n                         top_paths=1, merge_repeated=False):\n        sequence_length = decoder_input[\'encoder_output\'][\'src_length\']\n        if logits.dtype.base_dtype != tf.float32:\n          logits = tf.cast(logits, tf.float32)\n        decoded_ixs, decoded_vals, decoded_shapes, log_probabilities = (\n            custom_op_module.ctc_beam_search_decoder_with_lm(\n                logits, sequence_length, beam_width=beam_width,\n                model_path=self.params[\'lm_path\'], trie_path=self.params[\'trie_path\'],\n                alphabet_path=self.params[\'alphabet_config_path\'],\n                alpha=self.params[\'alpha\'],\n                beta=self.params[\'beta\'],\n                trie_weight=self.params.get(\'trie_weight\', 0.1),\n                top_paths=top_paths, merge_repeated=merge_repeated,\n            )\n        )\n        return [tf.SparseTensor(decoded_ixs[0], decoded_vals[0],\n                                decoded_shapes[0])]\n\n      self.params[\'logits_to_outputs_func\'] = decode_with_lm\n    else:\n      def decode_without_lm(logits, decoder_input, merge_repeated=True):\n        if logits.dtype.base_dtype != tf.float32:\n          logits = tf.cast(logits, tf.float32)\n        decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(\n            logits, decoder_input[\'encoder_output\'][\'src_length\'],\n            merge_repeated,\n        )\n        return decoded\n\n      self.params[\'logits_to_outputs_func\'] = decode_without_lm\n\n\nclass FullyConnectedSCDecoder(Decoder):\n  """"""Fully connected decoder constructor for speech commands.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Decoder.get_required_params(), **{\n        \'output_dim\': int,\n    })\n\n  def __init__(self, params, model,\n               name=""fully_connected_decoder"", mode=\'train\'):\n    """"""Fully connected decoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **output_dim** (int) --- output dimension.\n    """"""\n    super(FullyConnectedSCDecoder, self).__init__(params, model, name, mode)\n\n  def _decode(self, input_dict):\n    """"""This method performs linear transformation of input.\n\n    Args:\n      input_dict (dict): input dictionary that has to contain\n          the following fields::\n            input_dict = {\n              \'encoder_output\': {\n                \'outputs\': output of encoder (shape=[batch_size, num_features])\n              }\n            }\n\n    Returns:\n      dict: dictionary with the following tensors::\n\n        {\n          \'logits\': logits with the shape=[batch_size, output_dim]\n          \'outputs\': [logits] (same as logits but wrapped in list)\n        }\n    """"""\n    inputs = input_dict[\'encoder_output\'][\'outputs\']\n    lengths = input_dict[\'encoder_output\'][\'src_length\']\n    regularizer = self.params.get(\'regularizer\', None)\n\n    inputs = tf.layers.flatten(inputs=inputs)\n\n    # activation is linear by default\n    logits = tf.layers.dense(\n        inputs=inputs,\n        units=self.params[\'output_dim\'],\n        kernel_regularizer=regularizer,\n        name=\'fully_connected\',\n    )\n\n    return {\'logits\': logits, \'outputs\': [logits]}\n'"
open_seq2seq/decoders/jca_decoder.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom .decoder import Decoder\n\n\nclass JointCTCAttentionDecoder(Decoder):\n  """"""Joint CTC Attention like decoder.\n  Combines CTC and Attention based decoder.\n  Use only outputs from the Attention decoder during inference.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Decoder.get_required_params(), **{\n        \'ctc_decoder\': None,\n        \'attn_decoder\': None,\n        \'attn_decoder_params\': dict,\n        \'ctc_decoder_params\': dict,\n        \'beam_search_params\': dict,\n        \'language_model_params\': dict,\n        \'GO_SYMBOL\': int,  # symbol id\n        \'END_SYMBOL\': int,  # symbol id\n        \'tgt_vocab_size\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Decoder.get_optional_params(), **{\n    })\n\n  def __init__(self, params, model, name=\'jca_decoder\', mode=\'train\'):\n    """"""Initializes RNN decoder with embedding.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **ctc_decoder** (any class derived from\n      :class:`Decoder <decoders.decoder.Decoder>`) --- CTC decoder class to use.\n    * **attn_decoder** (any class derived from\n      :class:`Decoder <decoders.decoder.Decoder>`) --- Attention decoder class to use.\n    * **attn_decoder_params** (dict) --- parameters for the attention decoder.\n    * **ctc_decoder_params** (dict) --- parameters for the ctc decoder.\n    * **beam_search_params** (dict) --- beam search parameters for decoding using the attention based decoder.\n    * **language_model_params** (dict) --- language model parameters for decoding with an external language model.\n\n    * **GO_SYMBOL** (int) --- GO symbol id, must be the same as used in\n      data layer.\n    * **END_SYMBOL** (int) --- END symbol id, must be the same as used in\n      data layer.\n    * **tgt_vocab_size** (int) --- vocabulary size of the targets to use for final softmax.\n    """"""\n    super(JointCTCAttentionDecoder, self).__init__(params, model, name, mode)\n\n    self.ctc_params = self.params[\'ctc_decoder_params\']\n    self.attn_params = self.params[\'attn_decoder_params\']\n    self.beam_search_params = self.params[\'beam_search_params\']\n    self.lang_model_params = self.params[\'language_model_params\']\n\n    self.attn_params.update(self.beam_search_params)\n    self.attn_params.update(self.lang_model_params)\n\n    self.ctc_params[\'tgt_vocab_size\'] = self.params[\'tgt_vocab_size\'] - 1\n    self.attn_params[\'tgt_vocab_size\'] = self.params[\'tgt_vocab_size\']\n    self.attn_params[\'GO_SYMBOL\'] = self.params[\'GO_SYMBOL\']\n    self.attn_params[\'END_SYMBOL\'] = self.params[\'END_SYMBOL\']\n\n    self.ctc_decoder = self.params[\'ctc_decoder\'](\n        params=self.ctc_params, mode=mode, model=model)\n    self.attn_decoder = self.params[\'attn_decoder\'](\n        params=self.attn_params, mode=mode, model=model)\n\n  def _decode(self, input_dict):\n    """"""Joint decoder that combines Attention and CTC outputs.\n\n    Args:\n      input_dict (dict): Python dictionary with inputs to decoder.\n\n    Config parameters:\n\n    * **src_inputs** --- Decoder input Tensor of shape [batch_size, time, dim]\n    * **src_lengths** --- Decoder input lengths Tensor of shape [batch_size]\n    * **tgt_inputs** --- Only during training. labels Tensor of the\n      shape [batch_size, time].\n    * **tgt_lengths** --- Only during training. label lengths\n      Tensor of the shape [batch_size].\n\n    Returns:\n      dict: Python dictionary with:\n      * outputs - tensor of shape [batch_size, time] from the Attention decoder\n      * seq_outputs - output dictionary from the Attention decoder\n      * ctc_outputs - output dictionary from the CTC decoder\n    """"""\n\n    seq_outputs = self.attn_decoder.decode(input_dict=input_dict)\n    ctc_outputs = self.ctc_decoder.decode(input_dict=input_dict)\n\n    return {\n        \'outputs\': seq_outputs[\'outputs\'],\n        \'seq_outputs\': seq_outputs,\n        \'ctc_outputs\': ctc_outputs,\n    }\n'"
open_seq2seq/decoders/las_decoder.py,56,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.parts.rnns.attention_wrapper import BahdanauAttention, \\\n    LuongAttention, \\\n    LocationSensitiveAttention, \\\n    AttentionWrapper\nfrom open_seq2seq.parts.rnns.rnn_beam_search_decoder import BeamSearchDecoder\nfrom open_seq2seq.parts.rnns.utils import single_cell\nfrom open_seq2seq.parts.rnns.helper import TrainingHelper, GreedyEmbeddingHelper\nfrom .decoder import Decoder\n\ncells_dict = {\n    ""lstm"": tf.nn.rnn_cell.BasicLSTMCell,\n    ""gru"": tf.nn.rnn_cell.GRUCell,\n}\n\n\nclass FullyConnected(tf.layers.Layer):\n  """"""Fully connected layer\n  """"""\n\n  def __init__(\n      self,\n      hidden_dims,\n      dropout_keep_prob=1.0,\n      mode=\'train\',\n      name=""fully_connected"",\n  ):\n    """"""See parent class for arguments description.\n\n    Config parameters:\n\n    * **hidden_dims** (list) --- list of integers describing the hidden dimensions of a fully connected layer.\n    * **dropout_keep_prob** (float, optional) - dropout input keep probability.\n    """"""\n    super(FullyConnected, self).__init__(name=name)\n\n    self.dense_layers = []\n    i = -1\n    for i in range(len(hidden_dims) - 1):\n      self.dense_layers.append(tf.layers.Dense(\n          name=""{}_{}"".format(name, i), units=hidden_dims[i], use_bias=True, activation=tf.nn.relu)\n      )\n    self.dense_layers.append(tf.layers.Dense(\n        name=""{}_{}"".format(name, i + 1), units=hidden_dims[i + 1], use_bias=True)\n    )\n    self.output_dim = hidden_dims[i + 1]\n    self.mode = mode\n    self.dropout_keep_prob = dropout_keep_prob\n\n  def call(self, inputs):\n    """"""\n    Args:\n      inputs: Similar to tf.layers.Dense layer inputs. Internally calls a stack of dense layers.\n    """"""\n    training = (self.mode == ""train"")\n    dropout_keep_prob = self.dropout_keep_prob if training else 1.0\n    for layer in self.dense_layers:\n      inputs = tf.nn.dropout(x=inputs, keep_prob=dropout_keep_prob)\n      inputs = layer(inputs)\n    return inputs\n\n  def compute_output_shape(self, input_shape):\n    input_shape = tf.TensorShape(input_shape).as_list()\n    return tf.TensorShape([input_shape[0], self.output_dim])\n\n\nclass ListenAttendSpellDecoder(Decoder):\n  """"""Listen Attend Spell like decoder with attention mechanism.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Decoder.get_required_params(), **{\n        \'GO_SYMBOL\': int,  # symbol id\n        \'END_SYMBOL\': int,  # symbol id\n        \'tgt_vocab_size\': int,\n        \'tgt_emb_size\': int,\n        \'attention_params\': dict,\n        \'rnn_type\': None,\n        \'hidden_dim\': int,\n        \'num_layers\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Decoder.get_optional_params(), **{\n        \'dropout_keep_prob\': float,\n        \'pos_embedding\': bool,\n        \'beam_width\': int,\n        \'use_language_model\': bool,\n    })\n\n  def __init__(self, params, model, name=\'las_decoder\', mode=\'train\'):\n    """"""Initializes decoder with embedding.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **GO_SYMBOL** (int) --- GO symbol id, must be the same as used in\n      data layer.\n    * **END_SYMBOL** (int) --- END symbol id, must be the same as used in\n      data layer.\n    * **tgt_vocab_size** (int) --- vocabulary size of the targets to use for final softmax.\n    * **tgt_emb_size** (int) --- embedding size to use.\n    * **attention_params** (dict) - parameters for attention mechanism.\n    * **rnn_type** (String) - String indicating the rnn type. Accepts [\'lstm\', \'gru\'].\n    * **hidden_dim** (int) - Hidden domension to be used the RNN decoder.\n    * **num_layers** (int) - Number of decoder RNN layers.\n    * **dropout_keep_prob** (float, optional) - dropout input keep probability.\n    * **pos_embedding** (bool, optional) - Whether to use encoder and decoder positional embedding. Default is False.\n    * **beam_width** (int, optional) - Beam width used while decoding with beam search. Uses greedy decoding if the value is set to 1. Default is 1.\n    * **use_language_model** (bool, optional) - Boolean indicating whether to use language model for decoding. Default is False.\n    """"""\n    super(ListenAttendSpellDecoder, self).__init__(params, model, name, mode)\n    self.GO_SYMBOL = self.params[\'GO_SYMBOL\']\n    self.END_SYMBOL = self.params[\'END_SYMBOL\']\n    self._tgt_vocab_size = self.params[\'tgt_vocab_size\']\n    self._tgt_emb_size = self.params[\'tgt_emb_size\']\n\n  def _decode(self, input_dict):\n    """"""Decodes representation into data.\n\n    Args:\n      input_dict (dict): Python dictionary with inputs to decoder.\n\n\n    Config parameters:\n\n    * **src_inputs** --- Decoder input Tensor of shape [batch_size, time, dim]\n      or [time, batch_size, dim].\n    * **src_lengths** --- Decoder input lengths Tensor of shape [batch_size]\n    * **tgt_inputs** --- Only during training. labels Tensor of the\n      shape [batch_size, time] or [time, batch_size].\n    * **tgt_lengths** --- Only during training. labels lengths\n      Tensor of the shape [batch_size].\n\n    Returns:\n      dict: Python dictionary with:\n      * outputs - [predictions, alignments, enc_src_lengths].\n        predictions are the final predictions of the model. tensor of shape [batch_size, time].\n        alignments are the attention probabilities if attention is used. None if \'plot_attention\' in attention_params is set to False.\n        enc_src_lengths are the lengths of the input. tensor of shape [batch_size].\n      * logits - logits with the shape=[batch_size, output_dim].\n      * tgt_length - tensor of shape [batch_size] indicating the predicted sequence lengths.\n    """"""\n    encoder_outputs = input_dict[\'encoder_output\'][\'outputs\']\n    enc_src_lengths = input_dict[\'encoder_output\'][\'src_length\']\n\n    self._batch_size = int(encoder_outputs.get_shape()[0])\n    self._beam_width = self.params.get(""beam_width"", 1)\n\n    tgt_inputs = None\n    tgt_lengths = None\n    if \'target_tensors\' in input_dict:\n      tgt_inputs = input_dict[\'target_tensors\'][0]\n      tgt_lengths = input_dict[\'target_tensors\'][1]\n      tgt_inputs = tf.concat(\n          [tf.fill([self._batch_size, 1], self.GO_SYMBOL), tgt_inputs[:, :-1]], -1)\n\n    layer_type = self.params[\'rnn_type\']\n    num_layers = self.params[\'num_layers\']\n    attention_params = self.params[\'attention_params\']\n    hidden_dim = self.params[\'hidden_dim\']\n    dropout_keep_prob = self.params.get(\n        \'dropout_keep_prob\', 1.0) if self._mode == ""train"" else 1.0\n\n    # To-Do Seperate encoder and decoder position embeddings\n    use_positional_embedding = self.params.get(""pos_embedding"", False)\n    use_language_model = self.params.get(""use_language_model"", False)\n    use_beam_search_decoder = (\n        self._beam_width != 1) and (self._mode == ""infer"")\n\n    self._target_emb_layer = tf.get_variable(\n        name=\'TargetEmbeddingMatrix\',\n        shape=[self._tgt_vocab_size, self._tgt_emb_size],\n        dtype=tf.float32,\n    )\n\n    if use_positional_embedding:\n      self.enc_pos_emb_size = int(encoder_outputs.get_shape()[-1])\n      self.enc_pos_emb_layer = tf.get_variable(\n          name=\'EncoderPositionEmbeddingMatrix\',\n          shape=[1024, self.enc_pos_emb_size],\n          dtype=tf.float32,\n      )\n      encoder_output_positions = tf.range(\n          0,\n          tf.shape(encoder_outputs)[1],\n          delta=1,\n          dtype=tf.int32,\n          name=\'positional_inputs\'\n      )\n      encoder_position_embeddings = tf.cast(\n          tf.nn.embedding_lookup(\n              self.enc_pos_emb_layer, encoder_output_positions),\n          dtype=encoder_outputs.dtype\n      )\n      encoder_outputs += encoder_position_embeddings\n\n      self.dec_pos_emb_size = self._tgt_emb_size\n      self.dec_pos_emb_layer = tf.get_variable(\n          name=\'DecoderPositionEmbeddingMatrix\',\n          shape=[1024, self.dec_pos_emb_size],\n          dtype=tf.float32,\n      )\n\n    output_projection_layer = FullyConnected(\n        [self._tgt_vocab_size],\n        dropout_keep_prob=dropout_keep_prob,\n        mode=self._mode,\n    )\n\n    rnn_cell = cells_dict[layer_type]\n\n    dropout = tf.nn.rnn_cell.DropoutWrapper\n\n    multirnn_cell = tf.nn.rnn_cell.MultiRNNCell(\n        [dropout(rnn_cell(hidden_dim),\n                 output_keep_prob=dropout_keep_prob)\n         for _ in range(num_layers)]\n    )\n\n    if use_beam_search_decoder:\n      encoder_outputs = tf.contrib.seq2seq.tile_batch(\n          encoder_outputs,\n          multiplier=self._beam_width,\n      )\n      enc_src_lengths = tf.contrib.seq2seq.tile_batch(\n          enc_src_lengths,\n          multiplier=self._beam_width,\n      )\n\n    attention_dim = attention_params[""attention_dim""]\n    attention_type = attention_params[""attention_type""]\n    num_heads = attention_params[""num_heads""]\n    plot_attention = attention_params[""plot_attention""]\n    if plot_attention:\n      if use_beam_search_decoder:\n        plot_attention = False\n        print(""Plotting Attention is disabled for Beam Search Decoding"")\n      if num_heads != 1:\n        plot_attention = False\n        print(""Plotting Attention is disabled for Multi Head Attention"")\n      if self.params[\'dtype\'] != tf.float32:\n        plot_attention = False\n        print(""Plotting Attention is disabled for Mixed Precision Mode"")\n\n    attention_params_dict = {}\n    if attention_type == ""bahadanu"":\n      AttentionMechanism = BahdanauAttention\n      attention_params_dict[""normalize""] = False,\n    elif attention_type == ""chorowski"":\n      AttentionMechanism = LocationSensitiveAttention\n      attention_params_dict[""use_coverage""] = attention_params[""use_coverage""]\n      attention_params_dict[""location_attn_type""] = attention_type\n      attention_params_dict[""location_attention_params""] = {\n          \'filters\': 10, \'kernel_size\': 101}\n    elif attention_type == ""zhaopeng"":\n      AttentionMechanism = LocationSensitiveAttention\n      attention_params_dict[""use_coverage""] = attention_params[""use_coverage""]\n      attention_params_dict[""query_dim""] = hidden_dim\n      attention_params_dict[""location_attn_type""] = attention_type\n\n    attention_mechanism = []\n\n    for head in range(num_heads):\n      attention_mechanism.append(\n          AttentionMechanism(\n              num_units=attention_dim,\n              memory=encoder_outputs,\n              memory_sequence_length=enc_src_lengths,\n              probability_fn=tf.nn.softmax,\n              dtype=tf.get_variable_scope().dtype,\n              **attention_params_dict\n          )\n      )\n\n    multirnn_cell_with_attention = AttentionWrapper(\n        cell=multirnn_cell,\n        attention_mechanism=attention_mechanism,\n        attention_layer_size=[hidden_dim for i in range(num_heads)],\n        output_attention=True,\n        alignment_history=plot_attention,\n    )\n\n    if self._mode == ""train"":\n      decoder_output_positions = tf.range(\n          0,\n          tf.shape(tgt_inputs)[1],\n          delta=1,\n          dtype=tf.int32,\n          name=\'positional_inputs\'\n      )\n      tgt_input_vectors = tf.nn.embedding_lookup(\n          self._target_emb_layer, tgt_inputs)\n      if use_positional_embedding:\n        tgt_input_vectors += tf.nn.embedding_lookup(self.dec_pos_emb_layer,\n                                                    decoder_output_positions)\n      tgt_input_vectors = tf.cast(\n          tgt_input_vectors,\n          dtype=self.params[\'dtype\'],\n      )\n      # helper = tf.contrib.seq2seq.TrainingHelper(\n      helper = TrainingHelper(\n          inputs=tgt_input_vectors,\n          sequence_length=tgt_lengths,\n      )\n    elif self._mode == ""infer"" or self._mode == ""eval"":\n      embedding_fn = lambda ids: tf.cast(\n          tf.nn.embedding_lookup(self._target_emb_layer, ids),\n          dtype=self.params[\'dtype\'],\n      )\n      pos_embedding_fn = None\n      if use_positional_embedding:\n        pos_embedding_fn = lambda ids: tf.cast(\n            tf.nn.embedding_lookup(self.dec_pos_emb_layer, ids),\n            dtype=self.params[\'dtype\'],\n        )\n\n      # helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n      helper = GreedyEmbeddingHelper(\n          embedding=embedding_fn,\n          start_tokens=tf.fill([self._batch_size], self.GO_SYMBOL),\n          end_token=self.END_SYMBOL,\n          positional_embedding=pos_embedding_fn\n      )\n\n    if self._mode != ""infer"":\n      maximum_iterations = tf.reduce_max(tgt_lengths)\n    else:\n      maximum_iterations = tf.reduce_max(enc_src_lengths)\n\n    if not use_beam_search_decoder:\n      decoder = tf.contrib.seq2seq.BasicDecoder(\n          cell=multirnn_cell_with_attention,\n          helper=helper,\n          initial_state=multirnn_cell_with_attention.zero_state(\n              batch_size=self._batch_size, dtype=encoder_outputs.dtype,\n          ),\n          output_layer=output_projection_layer,\n      )\n    else:\n      batch_size_tensor = tf.constant(self._batch_size)\n      decoder = BeamSearchDecoder(\n          cell=multirnn_cell_with_attention,\n          embedding=embedding_fn,\n          start_tokens=tf.tile([self.GO_SYMBOL], [self._batch_size]),\n          end_token=self.END_SYMBOL,\n          initial_state=multirnn_cell_with_attention.zero_state(\n              dtype=encoder_outputs.dtype,\n              batch_size=batch_size_tensor * self._beam_width,\n          ),\n          beam_width=self._beam_width,\n          output_layer=output_projection_layer,\n          length_penalty_weight=0.0,\n      )\n\n    final_outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n        decoder=decoder,\n        impute_finished=self.mode != ""infer"",\n        maximum_iterations=maximum_iterations,\n    )\n\n    if plot_attention:\n      alignments = tf.transpose(\n          final_state.alignment_history[0].stack(), [1, 0, 2]\n      )\n    else:\n      alignments = None\n\n    if not use_beam_search_decoder:\n      outputs = tf.argmax(final_outputs.rnn_output, axis=-1)\n      logits = final_outputs.rnn_output\n      return_outputs = [outputs, alignments, enc_src_lengths]\n    else:\n      outputs = final_outputs.predicted_ids[:, :, 0]\n      logits = final_outputs.predicted_ids[:, :, 0]\n      return_outputs = [outputs, enc_src_lengths]\n\n    if self.mode == ""eval"":\n      max_len = tf.reduce_max(tgt_lengths)\n      logits = tf.while_loop(\n          lambda logits: max_len > tf.shape(logits)[1],\n          lambda logits: tf.concat([logits, tf.fill(\n              [tf.shape(logits)[0], 1, tf.shape(logits)[2]], tf.cast(1.0, self.params[\'dtype\']))], 1),\n          loop_vars=[logits],\n          back_prop=False,\n      )\n\n    return {\n        \'outputs\': return_outputs,\n        \'logits\': logits,\n        \'tgt_length\': final_sequence_lengths,\n    }\n'"
open_seq2seq/decoders/lm_decoders.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""This module defines various fully-connected decoders (consisting of one\nfully connected layer).\n\nThese classes are usually used for models that are not really\nsequence-to-sequence and thus should be artificially split into encoder and\ndecoder by cutting, for example, on the last fully-connected layer.\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nfrom .decoder import Decoder\n\n\nclass FakeDecoder(Decoder):\n  """"""Fake decoder for LM\n  """"""\n  def __init__(self, params, model,\n               name=""fake_decoder"", mode=\'train\'):\n    super(FakeDecoder, self).__init__(params, model, name, mode)\n\n  def _decode(self, input_dict):\n    """"""This method performs linear transformation of input.\n\n    Args:\n      input_dict (dict): input dictionary that has to contain\n          the following fields::\n            input_dict = {\n              \'encoder_output\': {\n                \'outputs\': output of encoder (shape=[batch_size, num_features])\n              }\n            }\n\n    Returns:\n      dict: dictionary with the following tensors::\n\n        {\n          \'logits\': logits with the shape=[batch_size, output_dim]\n          \'outputs\': [logits] (same as logits but wrapped in list)\n        }\n    """"""\n    # return {\'logits\': input_dict[\'encoder_output\'][\'logits\'],\n    #         \'outputs\': [input_dict[\'encoder_output\'][\'outputs\']]}\n    # if \'logits\' in input_dict[\'encoder_output\']:\n    #   return {\'logits\': input_dict[\'encoder_output\'][\'logits\'],\n    #           \'outputs\': [input_dict[\'encoder_output\'][\'outputs\']]}\n    # else:\n    #   return {}\n    return input_dict[\'encoder_output\']\n'"
open_seq2seq/decoders/rnn_decoders.py,47,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nRNN-based decoders.\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport copy\n\nimport tensorflow as tf\n\nfrom open_seq2seq.parts.rnns.attention_wrapper import BahdanauAttention, \\\n                                                      LuongAttention, \\\n                                                      AttentionWrapper\nfrom open_seq2seq.parts.rnns.gnmt import GNMTAttentionMultiCell, \\\n                                         gnmt_residual_fn\nfrom open_seq2seq.parts.rnns.rnn_beam_search_decoder import BeamSearchDecoder\nfrom open_seq2seq.parts.rnns.utils import single_cell\nfrom .decoder import Decoder\n\n\nclass RNNDecoderWithAttention(Decoder):\n  """"""Typical RNN decoder with attention mechanism.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Decoder.get_required_params(), **{\n        \'GO_SYMBOL\': int,  # symbol id\n        \'END_SYMBOL\': int,  # symbol id\n        \'tgt_vocab_size\': int,\n        \'tgt_emb_size\': int,\n        \'attention_layer_size\': int,\n        \'attention_type\': [\'bahdanau\', \'luong\', \'gnmt\', \'gnmt_v2\'],\n        \'core_cell\': None,\n        \'decoder_layers\': int,\n        \'decoder_use_skip_connections\': bool,\n        \'batch_size\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Decoder.get_optional_params(), **{\n        \'core_cell_params\': dict,\n        \'bahdanau_normalize\': bool,\n        \'luong_scale\': bool,\n        \'decoder_dp_input_keep_prob\': float,\n        \'decoder_dp_output_keep_prob\': float,\n        \'time_major\': bool,\n        \'use_swap_memory\': bool,\n        \'proj_size\': int,\n        \'num_groups\': int,\n        \'PAD_SYMBOL\': int,  # symbol id\n        \'weight_tied\': bool,\n    })\n\n  def __init__(self, params, model,\n               name=\'rnn_decoder_with_attention\', mode=\'train\'):\n    """"""Initializes RNN decoder with embedding.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **batch_size** (int) --- batch size.\n    * **GO_SYMBOL** (int) --- GO symbol id, must be the same as used in\n      data layer.\n    * **END_SYMBOL** (int) --- END symbol id, must be the same as used in\n      data layer.\n    * **tgt_emb_size** (int) --- embedding size to use.\n    * **core_cell_params** (dict) - parameters for RNN class\n    * **core_cell** (string) - RNN class.\n    * **decoder_dp_input_keep_prob** (float) - dropout input keep probability.\n    * **decoder_dp_output_keep_prob** (float) - dropout output keep probability.\n    * **decoder_use_skip_connections** (bool) - use residual connections or not.\n    * **attention_type** (string) - bahdanau, luong, gnmt or gnmt_v2.\n    * **bahdanau_normalize** (bool, optional) - whether to use normalization in\n      bahdanau attention.\n    * **luong_scale** (bool, optional) - whether to use scale in luong attention\n    * ... add any cell-specific parameters here as well.\n    """"""\n    super(RNNDecoderWithAttention, self).__init__(params, model, name, mode)\n    self._batch_size = self.params[\'batch_size\']\n    self.GO_SYMBOL = self.params[\'GO_SYMBOL\']\n    self.END_SYMBOL = self.params[\'END_SYMBOL\']\n    self._tgt_vocab_size = self.params[\'tgt_vocab_size\']\n    self._tgt_emb_size = self.params[\'tgt_emb_size\']\n    self._weight_tied = self.params.get(\'weight_tied\', False)\n\n  def _build_attention(self,\n                       encoder_outputs,\n                       encoder_sequence_length):\n    """"""Builds Attention part of the graph.\n    Currently supports ""bahdanau"" and ""luong"".\n    """"""\n    with tf.variable_scope(""AttentionMechanism""):\n      attention_depth = self.params[\'attention_layer_size\']\n      if self.params[\'attention_type\'] == \'bahdanau\':\n        if \'bahdanau_normalize\' in self.params:\n          bah_normalize = self.params[\'bahdanau_normalize\']\n        else:\n          bah_normalize = False\n        attention_mechanism = BahdanauAttention(\n            num_units=attention_depth,\n            memory=encoder_outputs,\n            normalize=bah_normalize,\n            memory_sequence_length=encoder_sequence_length,\n            probability_fn=tf.nn.softmax,\n            dtype=tf.get_variable_scope().dtype\n        )\n      elif self.params[\'attention_type\'] == \'luong\':\n        if \'luong_scale\' in self.params:\n          luong_scale = self.params[\'luong_scale\']\n        else:\n          luong_scale = False\n        attention_mechanism = LuongAttention(\n            num_units=attention_depth,\n            memory=encoder_outputs,\n            scale=luong_scale,\n            memory_sequence_length=encoder_sequence_length,\n            probability_fn=tf.nn.softmax,\n            dtype=tf.get_variable_scope().dtype\n        )\n      elif self.params[\'attention_type\'] == \'gnmt\' or \\\n           self.params[\'attention_type\'] == \'gnmt_v2\':\n        attention_mechanism = BahdanauAttention(\n            num_units=attention_depth,\n            memory=encoder_outputs,\n            normalize=True,\n            memory_sequence_length=encoder_sequence_length,\n            probability_fn=tf.nn.softmax,\n            dtype=tf.get_variable_scope().dtype\n        )\n      else:\n        raise ValueError(\'Unknown Attention Type\')\n      return attention_mechanism\n\n  @staticmethod\n  def _add_residual_wrapper(cells, start_ind=1):\n    for idx, cell in enumerate(cells):\n      if idx >= start_ind:\n        cells[idx] = tf.contrib.rnn.ResidualWrapper(  # pylint: disable=no-member\n            cell,\n            residual_fn=gnmt_residual_fn,\n        )\n    return cells\n\n  def _decode(self, input_dict):\n    """"""Decodes representation into data.\n\n    Args:\n      input_dict (dict): Python dictionary with inputs to decoder.\n\n\n    Config parameters:\n\n    * **src_inputs** --- Decoder input Tensor of shape [batch_size, time, dim]\n      or [time, batch_size, dim]\n    * **src_lengths** --- Decoder input lengths Tensor of shape [batch_size]\n    * **tgt_inputs** --- Only during training. labels Tensor of the\n      shape [batch_size, time] or [time, batch_size].\n    * **tgt_lengths** --- Only during training. labels lengths\n      Tensor of the shape [batch_size].\n\n    Returns:\n      dict: Python dictionary with:\n      * final_outputs - tensor of shape [batch_size, time, dim]\n                        or [time, batch_size, dim]\n      * final_state - tensor with decoder final state\n      * final_sequence_lengths - tensor of shape [batch_size, time]\n                                 or [time, batch_size]\n    """"""\n    encoder_outputs = input_dict[\'encoder_output\'][\'outputs\']\n    enc_src_lengths = input_dict[\'encoder_output\'][\'src_lengths\']\n    tgt_inputs = input_dict[\'target_tensors\'][0] if \'target_tensors\' in \\\n                                                    input_dict else None\n    tgt_lengths = input_dict[\'target_tensors\'][1] if \'target_tensors\' in \\\n                                                     input_dict else None\n\n    self._output_projection_layer = tf.layers.Dense(\n        self._tgt_vocab_size, use_bias=False,\n    )\n\n    if not self._weight_tied:\n      self._dec_emb_w = tf.get_variable(\n          name=\'DecoderEmbeddingMatrix\',\n          shape=[self._tgt_vocab_size, self._tgt_emb_size],\n          dtype=tf.float32\n      )\n    else:\n      fake_input = tf.zeros(shape=(1, self._tgt_emb_size))\n      fake_output = self._output_projection_layer.apply(fake_input)\n      with tf.variable_scope(""dense"", reuse=True):\n        dense_weights = tf.get_variable(""kernel"")\n        self._dec_emb_w = tf.transpose(dense_weights)\n\n    if self._mode == ""train"":\n      dp_input_keep_prob = self.params[\'decoder_dp_input_keep_prob\']\n      dp_output_keep_prob = self.params[\'decoder_dp_output_keep_prob\']\n    else:\n      dp_input_keep_prob = 1.0\n      dp_output_keep_prob = 1.0\n\n    residual_connections = self.params[\'decoder_use_skip_connections\']\n\n    # list of cells\n    cell_params = self.params.get(\'core_cell_params\', {})\n    self._decoder_cells = [\n        single_cell(\n            cell_class=self.params[\'core_cell\'],\n            cell_params=self.params.get(\'core_cell_params\', {}),\n            dp_input_keep_prob=dp_input_keep_prob,\n            dp_output_keep_prob=dp_output_keep_prob,\n            # residual connections are added a little differently for GNMT\n            residual_connections=False if self.params[\'attention_type\'].startswith(\'gnmt\')\n                                 else residual_connections,\n        ) for _ in range(self.params[\'decoder_layers\'] - 1)\n    ]\n\n    last_cell_params = copy.deepcopy(cell_params)\n    if self._weight_tied:\n      last_cell_params[\'num_units\'] = self._tgt_emb_size\n\n    last_cell = single_cell(\n            cell_class=self.params[\'core_cell\'],\n            cell_params=last_cell_params,\n            dp_input_keep_prob=dp_input_keep_prob,\n            dp_output_keep_prob=dp_output_keep_prob,\n            # residual connections are added a little differently for GNMT\n            residual_connections=False if self.params[\'attention_type\'].startswith(\'gnmt\')\n                                 else residual_connections,\n        )\n    self._decoder_cells.append(last_cell)\n\n\n\n    attention_mechanism = self._build_attention(\n        encoder_outputs,\n        enc_src_lengths,\n    )\n    if self.params[\'attention_type\'].startswith(\'gnmt\'):\n      attention_cell = self._decoder_cells.pop(0)\n      attention_cell = AttentionWrapper(\n          attention_cell,\n          attention_mechanism=attention_mechanism,\n          attention_layer_size=None,\n          output_attention=False,\n          name=""gnmt_attention"",\n      )\n      attentive_decoder_cell = GNMTAttentionMultiCell(\n          attention_cell,\n          self._add_residual_wrapper(self._decoder_cells) if residual_connections else self._decoder_cells,\n          use_new_attention=(self.params[\'attention_type\'] == \'gnmt_v2\'),\n      )\n    else:\n      attentive_decoder_cell = AttentionWrapper(\n          # pylint: disable=no-member\n          cell=tf.contrib.rnn.MultiRNNCell(self._decoder_cells),\n          attention_mechanism=attention_mechanism,\n      )\n    if self._mode == ""train"":\n      input_vectors = tf.cast(\n        tf.nn.embedding_lookup(self._dec_emb_w, tgt_inputs),\n        dtype=self.params[\'dtype\'],\n      )\n      helper = tf.contrib.seq2seq.TrainingHelper(  # pylint: disable=no-member\n          inputs=input_vectors,\n          sequence_length=tgt_lengths,\n      )\n      decoder = tf.contrib.seq2seq.BasicDecoder(  # pylint: disable=no-member\n          cell=attentive_decoder_cell,\n          helper=helper,\n          output_layer=self._output_projection_layer,\n          initial_state=attentive_decoder_cell.zero_state(\n              self._batch_size, dtype=encoder_outputs.dtype,\n          ),\n      )\n    elif self._mode == ""infer"" or self._mode == ""eval"":\n      embedding_fn = lambda ids: tf.cast(\n          tf.nn.embedding_lookup(self._dec_emb_w, ids),\n          dtype=self.params[\'dtype\'],\n      )\n      # pylint: disable=no-member\n      helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n          embedding=embedding_fn,\n          start_tokens=tf.fill([self._batch_size], self.GO_SYMBOL),\n          end_token=self.END_SYMBOL,\n      )\n      decoder = tf.contrib.seq2seq.BasicDecoder(  # pylint: disable=no-member\n          cell=attentive_decoder_cell,\n          helper=helper,\n          initial_state=attentive_decoder_cell.zero_state(\n              batch_size=self._batch_size, dtype=encoder_outputs.dtype,\n          ),\n          output_layer=self._output_projection_layer,\n      )\n    else:\n      raise ValueError(\n          ""Unknown mode for decoder: {}"".format(self._mode)\n      )\n\n    time_major = self.params.get(""time_major"", False)\n    use_swap_memory = self.params.get(""use_swap_memory"", False)\n    if self._mode == \'train\':\n      maximum_iterations = tf.reduce_max(tgt_lengths)\n    else:\n      maximum_iterations = tf.reduce_max(enc_src_lengths) * 2\n\n    # pylint: disable=no-member\n    final_outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n        decoder=decoder,\n        impute_finished=True,\n        maximum_iterations=maximum_iterations,\n        swap_memory=use_swap_memory,\n        output_time_major=time_major,\n    )\n\n    return {\'logits\': final_outputs.rnn_output if not time_major else\n            tf.transpose(final_outputs.rnn_output, perm=[1, 0, 2]),\n            \'outputs\': [tf.argmax(final_outputs.rnn_output, axis=-1)],\n            \'final_state\': final_state,\n            \'final_sequence_lengths\': final_sequence_lengths}\n\n\nclass BeamSearchRNNDecoderWithAttention(RNNDecoderWithAttention):\n  """"""\n  Beam search version of RNN-based decoder with attention.\n  Can be used only during Inference (mode=infer)\n  """"""\n  @staticmethod\n  def get_optional_params():\n    return dict(RNNDecoderWithAttention.get_optional_params(), **{\n        \'length_penalty\': float,\n        \'beam_width\': int,\n    })\n\n  def __init__(self, params, model,\n               name=""rnn_decoder_with_attention"", mode=\'train\'):\n    """"""Initializes beam search decoder.\n\n    Args:\n      params(dict): dictionary with decoder parameters\n\n    Config parameters:\n\n    * **batch_size** --- batch size\n    * **GO_SYMBOL** --- GO symbol id, must be the same as used in data layer\n    * **END_SYMBOL** --- END symbol id, must be the same as used in data layer\n    * **tgt_vocab_size** --- vocabulary size of target\n    * **tgt_emb_size** --- embedding to use\n    * **decoder_cell_units** --- number of units in RNN\n    * **decoder_cell_type** --- RNN type: lstm, gru, glstm, etc.\n    * **decoder_dp_input_keep_prob** ---\n    * **decoder_dp_output_keep_prob** ---\n    * **decoder_use_skip_connections** --- use residual connections or not\n    * **attention_type** --- bahdanau, luong, gnmt, gnmt_v2\n    * **bahdanau_normalize** --- (optional)\n    * **luong_scale** --- (optional)\n    * **mode** --- train or infer\n    ... add any cell-specific parameters here as well\n    """"""\n    super(BeamSearchRNNDecoderWithAttention, self).__init__(\n        params, model, name, mode,\n    )\n    if self._mode != \'infer\':\n      raise ValueError(\n          \'BeamSearch decoder only supports infer mode, but got {}\'.format(\n              self._mode,\n          )\n      )\n    if ""length_penalty"" not in self.params:\n      self._length_penalty_weight = 0.0\n    else:\n      self._length_penalty_weight = self.params[""length_penalty""]\n    # beam_width of 1 should be same as argmax decoder\n    if ""beam_width"" not in self.params:\n      self._beam_width = 1\n    else:\n      self._beam_width = self.params[""beam_width""]\n\n\n  def _decode(self, input_dict):\n    """"""Decodes representation into data.\n\n    Args:\n      input_dict (dict): Python dictionary with inputs to decoder\n\n    Must define:\n      * src_inputs - decoder input Tensor of shape [batch_size, time, dim]\n                     or [time, batch_size, dim]\n      * src_lengths - decoder input lengths Tensor of shape [batch_size]\n    Does not need tgt_inputs and tgt_lengths\n\n    Returns:\n      dict: a Python dictionary with:\n      * final_outputs - tensor of shape [batch_size, time, dim] or\n                        [time, batch_size, dim]\n      * final_state - tensor with decoder final state\n      * final_sequence_lengths - tensor of shape [batch_size, time] or\n                                 [time, batch_size]\n    """"""\n    encoder_outputs = input_dict[\'encoder_output\'][\'outputs\']\n    enc_src_lengths = input_dict[\'encoder_output\'][\'src_lengths\']\n\n    \n\n    self._output_projection_layer = tf.layers.Dense(\n        self._tgt_vocab_size, use_bias=False,\n    )\n\n    if not self._weight_tied:\n      self._dec_emb_w = tf.get_variable(\n          name=\'DecoderEmbeddingMatrix\',\n          shape=[self._tgt_vocab_size, self._tgt_emb_size],\n          dtype=tf.float32\n      )\n    else:\n      fake_input = tf.zeros(shape=(1, self._tgt_emb_size))\n      fake_output = self._output_projection_layer.apply(fake_input)\n      with tf.variable_scope(""dense"", reuse=True):\n        dense_weights = tf.get_variable(""kernel"")\n        self._dec_emb_w = tf.transpose(dense_weights)\n\n\n\n    if self._mode == ""train"":\n      dp_input_keep_prob = self.params[\'decoder_dp_input_keep_prob\']\n      dp_output_keep_prob = self.params[\'decoder_dp_output_keep_prob\']\n    else:\n      dp_input_keep_prob = 1.0\n      dp_output_keep_prob = 1.0\n\n    residual_connections = self.params[\'decoder_use_skip_connections\']\n    # list of cells\n    cell_params = self.params.get(\'core_cell_params\', {})\n    \n\n    self._decoder_cells = [\n        single_cell(\n            cell_class=self.params[\'core_cell\'],\n            cell_params=cell_params,\n            dp_input_keep_prob=dp_input_keep_prob,\n            dp_output_keep_prob=dp_output_keep_prob,\n            # residual connections are added a little differently for GNMT\n            residual_connections=False if self.params[\'attention_type\'].startswith(\'gnmt\')\n                                 else residual_connections,\n        ) for _ in range(self.params[\'decoder_layers\'] - 1)\n    ]\n\n    last_cell_params = copy.deepcopy(cell_params)\n    if self._weight_tied:\n      last_cell_params[\'num_units\'] = self._tgt_emb_size\n\n    last_cell = single_cell(\n            cell_class=self.params[\'core_cell\'],\n            cell_params=last_cell_params,\n            dp_input_keep_prob=dp_input_keep_prob,\n            dp_output_keep_prob=dp_output_keep_prob,\n            # residual connections are added a little differently for GNMT\n            residual_connections=False if self.params[\'attention_type\'].startswith(\'gnmt\')\n                                 else residual_connections,\n        )\n    self._decoder_cells.append(last_cell)\n\n    # pylint: disable=no-member\n    tiled_enc_outputs = tf.contrib.seq2seq.tile_batch(\n        encoder_outputs,\n        multiplier=self._beam_width,\n    )\n    # pylint: disable=no-member\n    tiled_enc_src_lengths = tf.contrib.seq2seq.tile_batch(\n        enc_src_lengths,\n        multiplier=self._beam_width,\n    )\n    attention_mechanism = self._build_attention(\n        tiled_enc_outputs,\n        tiled_enc_src_lengths,\n    )\n\n    if self.params[\'attention_type\'].startswith(\'gnmt\'):\n      attention_cell = self._decoder_cells.pop(0)\n      attention_cell = AttentionWrapper(\n          attention_cell,\n          attention_mechanism=attention_mechanism,\n          attention_layer_size=None,  # don\'t use attention layer.\n          output_attention=False,\n          name=""gnmt_attention"",\n      )\n      attentive_decoder_cell = GNMTAttentionMultiCell(\n          attention_cell,\n          self._add_residual_wrapper(self._decoder_cells) if residual_connections else self._decoder_cells,\n          use_new_attention=(self.params[\'attention_type\'] == \'gnmt_v2\')\n      )\n    else:  # non-GNMT\n      attentive_decoder_cell = AttentionWrapper(\n          # pylint: disable=no-member\n          cell=tf.contrib.rnn.MultiRNNCell(self._decoder_cells),\n          attention_mechanism=attention_mechanism,\n      )\n    batch_size_tensor = tf.constant(self._batch_size)\n    embedding_fn = lambda ids: tf.cast(\n        tf.nn.embedding_lookup(self._dec_emb_w, ids),\n        dtype=self.params[\'dtype\'],\n    )\n    decoder = BeamSearchDecoder(\n        cell=attentive_decoder_cell,\n        embedding=embedding_fn,\n        start_tokens=tf.tile([self.GO_SYMBOL], [self._batch_size]),\n        end_token=self.END_SYMBOL,\n        initial_state=attentive_decoder_cell.zero_state(\n            dtype=encoder_outputs.dtype,\n            batch_size=batch_size_tensor * self._beam_width,\n        ),\n        beam_width=self._beam_width,\n        output_layer=self._output_projection_layer,\n        length_penalty_weight=self._length_penalty_weight\n    )\n\n    time_major = self.params.get(""time_major"", False)\n    use_swap_memory = self.params.get(""use_swap_memory"", False)\n    final_outputs, final_state, final_sequence_lengths = \\\n        tf.contrib.seq2seq.dynamic_decode(  # pylint: disable=no-member\n            decoder=decoder,\n            maximum_iterations=tf.reduce_max(enc_src_lengths) * 2,\n            swap_memory=use_swap_memory,\n            output_time_major=time_major,\n        )\n\n    return {\'logits\': final_outputs.predicted_ids[:, :, 0] if not time_major else\n            tf.transpose(final_outputs.predicted_ids[:, :, 0], perm=[1, 0, 2]),\n            \'outputs\': [final_outputs.predicted_ids[:, :, 0]],\n            \'final_state\': final_state,\n            \'final_sequence_lengths\': final_sequence_lengths}\n'"
open_seq2seq/decoders/tacotron2_decoder.py,39,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nTacotron2 decoder\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\nfrom open_seq2seq.parts.rnns.utils import single_cell\nfrom open_seq2seq.parts.rnns.attention_wrapper import BahdanauAttention, \\\n                                                 LocationSensitiveAttention, \\\n                                                 AttentionWrapper\nfrom open_seq2seq.parts.tacotron.tacotron_helper import TacotronHelper, \\\n                                                        TacotronTrainingHelper\nfrom open_seq2seq.parts.tacotron.tacotron_decoder import TacotronDecoder\nfrom open_seq2seq.parts.cnns.conv_blocks import conv_bn_actv\nfrom .decoder import Decoder\n\n\nclass Prenet():\n  """"""\n  Fully connected prenet used in the decoder\n  """"""\n  def __init__(\n      self,\n      num_units,\n      num_layers,\n      activation_fn=None,\n      dtype=None\n  ):\n    """"""Prenet initializer\n\n    Args:\n      num_units (int): number of units in the fully connected layer\n      num_layers (int): number of fully connected layers\n      activation_fn (callable): any valid activation function\n      dtype (dtype): the data format for this layer\n    """"""\n    assert (\n        num_layers > 0\n    ), ""If the prenet is enabled, there must be at least 1 layer""\n    self.prenet_layers = []\n    self._output_size = num_units\n\n    for idx in range(num_layers):\n      self.prenet_layers.append(\n          tf.layers.Dense(\n              name=""prenet_{}"".format(idx + 1),\n              units=num_units,\n              activation=activation_fn,\n              use_bias=True,\n              dtype=dtype\n          )\n      )\n\n  def __call__(self, inputs):\n    """"""\n    Applies the prenet to the inputs\n    """"""\n    for layer in self.prenet_layers:\n      inputs = tf.layers.dropout(layer(inputs), rate=0.5, training=True)\n    return inputs\n\n  @property\n  def output_size(self):\n    return self._output_size\n\n  def add_regularization(self, regularizer):\n    """"""\n    Adds regularization to all prenet kernels\n    """"""\n    for layer in self.prenet_layers:\n      for weights in layer.trainable_variables:\n        if ""bias"" not in weights.name:\n          # print(""Added regularizer to {}"".format(weights.name))\n          if weights.dtype.base_dtype == tf.float16:\n            tf.add_to_collection(\n                \'REGULARIZATION_FUNCTIONS\', (weights, regularizer)\n            )\n          else:\n            tf.add_to_collection(\n                ops.GraphKeys.REGULARIZATION_LOSSES, regularizer(weights)\n            )\n\n\nclass Tacotron2Decoder(Decoder):\n  """"""\n  Tacotron 2 Decoder\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(\n        Decoder.get_required_params(), **{\n            \'attention_layer_size\': int,\n            \'attention_type\': [\'bahdanau\', \'location\', None],\n            \'decoder_cell_units\': int,\n            \'decoder_cell_type\': None,\n            \'decoder_layers\': int,\n        }\n    )\n\n  @staticmethod\n  def get_optional_params():\n    return dict(\n        Decoder.get_optional_params(), **{\n            \'bahdanau_normalize\': bool,\n            \'time_major\': bool,\n            \'use_swap_memory\': bool,\n            \'enable_prenet\': bool,\n            \'prenet_layers\': int,\n            \'prenet_units\': int,\n            \'prenet_activation\': None,\n            \'enable_postnet\': bool,\n            \'postnet_conv_layers\': list,\n            \'postnet_bn_momentum\': float,\n            \'postnet_bn_epsilon\': float,\n            \'postnet_data_format\': [\'channels_first\', \'channels_last\'],\n            \'postnet_keep_dropout_prob\': float,\n            \'mask_decoder_sequence\': bool,\n            \'attention_bias\': bool,\n            \'zoneout_prob\': float,\n            \'dropout_prob\': float,\n            \'parallel_iterations\': int,\n        }\n    )\n\n  def __init__(self, params, model, name=\'tacotron_2_decoder\', mode=\'train\'):\n    """"""Tacotron-2 like decoder constructor. A lot of optional configurations are\n    currently for testing. Not all configurations are supported. Use of thed\n    efault config is recommended.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **attention_layer_size** (int) --- size of attention layer.\n    * **attention_type** (string) --- Determines whether attention mechanism to\n      use, should be one of \'bahdanau\', \'location\', or None.\n      Use of \'location\'-sensitive attention is strongly recommended.\n    * **bahdanau_normalize** (bool) --- Whether to enable weight norm on the\n      attention parameters. Defaults to False.\n    * **decoder_cell_units** (int) --- dimension of decoder RNN cells.\n    * **decoder_layers** (int) --- number of decoder RNN layers to use.\n    * **decoder_cell_type** (callable) --- could be ""lstm"", ""gru"", ""glstm"", or\n      ""slstm"". Currently, only \'lstm\' has been tested. Defaults to \'lstm\'.\n    * **time_major** (bool) --- whether to output as time major or batch major.\n      Default is False for batch major.\n    * **use_swap_memory** (bool) --- default is False.\n    * **enable_prenet** (bool) --- whether to use the fully-connected prenet in\n      the decoder. Defaults to True\n    * **prenet_layers** (int) --- number of fully-connected layers to use.\n      Defaults to 2.\n    * **prenet_units** (int) --- number of units in each layer. Defaults to 256.\n    * **prenet_activation** (callable) --- activation function to use for the\n      prenet lyaers. Defaults to relu\n    * **enable_postnet** (bool) --- whether to use the convolutional postnet in\n      the decoder. Defaults to True\n    * **postnet_conv_layers** (bool) --- list with the description of\n      convolutional layers. Must be passed if postnet is enabled\n      For example::\n        ""postnet_conv_layers"": [\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME"",\n            ""activation_fn"": tf.nn.tanh\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME"",\n            ""activation_fn"": tf.nn.tanh\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME"",\n            ""activation_fn"": tf.nn.tanh\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME"",\n            ""activation_fn"": tf.nn.tanh\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 80, ""padding"": ""SAME"",\n            ""activation_fn"": None\n          }\n        ]\n    * **postnet_bn_momentum** (float) --- momentum for batch norm.\n      Defaults to 0.1.\n    * **postnet_bn_epsilon** (float) --- epsilon for batch norm.\n      Defaults to 1e-5.\n    * **postnet_data_format** (string) --- could be either ""channels_first"" or\n      ""channels_last"". Defaults to ""channels_last"".\n    * **postnet_keep_dropout_prob** (float) --- keep probability for dropout in\n      the postnet conv layers. Default to 0.5.\n    * **mask_decoder_sequence** (bool) --- Defaults to True.\n    * **attention_bias** (bool) --- Wether to use a bias term when calculating\n      the attention. Only works for ""location"" attention. Defaults to False.\n    * **zoneout_prob** (float) --- zoneout probability for rnn layers.\n      Defaults to 0.\n    * **dropout_prob** (float) --- dropout probability for rnn layers.\n      Defaults to 0.1\n    * **parallel_iterations** (int) --- Number of parallel_iterations for\n      tf.while loop inside dynamic_decode. Defaults to 32.\n    """"""\n\n    super(Tacotron2Decoder, self).__init__(params, model, name, mode)\n    self._model = model\n    self._n_feats = self._model.get_data_layer().params[\'num_audio_features\']\n    if ""both"" in self._model.get_data_layer().params[\'output_type\']:\n      self._both = True\n      if not self.params.get(\'enable_postnet\', True):\n        raise ValueError(\n            ""postnet must be enabled for both mode""\n        )\n    else:\n      self._both = False\n\n  def _build_attention(\n      self,\n      encoder_outputs,\n      encoder_sequence_length,\n      attention_bias,\n  ):\n    """"""\n    Builds Attention part of the graph.\n    Currently supports ""bahdanau"", and ""location""\n    """"""\n    with tf.variable_scope(""AttentionMechanism""):\n      attention_depth = self.params[\'attention_layer_size\']\n      if self.params[\'attention_type\'] == \'location\':\n        attention_mechanism = LocationSensitiveAttention(\n            num_units=attention_depth,\n            memory=encoder_outputs,\n            memory_sequence_length=encoder_sequence_length,\n            probability_fn=tf.nn.softmax,\n            dtype=tf.get_variable_scope().dtype,\n            use_bias=attention_bias,\n        )\n      elif self.params[\'attention_type\'] == \'bahdanau\':\n        bah_normalize = self.params.get(\'bahdanau_normalize\', False)\n        attention_mechanism = BahdanauAttention(\n            num_units=attention_depth,\n            memory=encoder_outputs,\n            normalize=bah_normalize,\n            memory_sequence_length=encoder_sequence_length,\n            probability_fn=tf.nn.softmax,\n            dtype=tf.get_variable_scope().dtype\n        )\n      else:\n        raise ValueError(\'Unknown Attention Type\')\n      return attention_mechanism\n\n  def _decode(self, input_dict):\n    """"""\n    Decodes representation into data\n\n    Args:\n      input_dict (dict): Python dictionary with inputs to decoder. Must define:\n          * src_inputs - decoder input Tensor of shape [batch_size, time, dim]\n            or [time, batch_size, dim]\n          * src_lengths - decoder input lengths Tensor of shape [batch_size]\n          * tgt_inputs - Only during training. labels Tensor of the\n            shape [batch_size, time, num_features] or\n            [time, batch_size, num_features]\n          * stop_token_inputs - Only during training. labels Tensor of the\n            shape [batch_size, time, 1] or [time, batch_size, 1]\n          * tgt_lengths - Only during training. labels lengths\n            Tensor of the shape [batch_size]\n\n    Returns:\n      dict:\n        A python dictionary containing:\n\n          * outputs - array containing:\n\n              * decoder_output - tensor of shape [batch_size, time,\n                num_features] or [time, batch_size, num_features]. Spectrogram\n                representation learned by the decoder rnn\n              * spectrogram_prediction - tensor of shape [batch_size, time,\n                num_features] or [time, batch_size, num_features]. Spectrogram\n                containing the residual corrections from the postnet if enabled\n              * alignments - tensor of shape [batch_size, time, memory_size]\n                or [time, batch_size, memory_size]. The alignments learned by\n                the attention layer\n              * stop_token_prediction - tensor of shape [batch_size, time, 1]\n                or [time, batch_size, 1]. The stop token predictions\n              * final_sequence_lengths - tensor of shape [batch_size]\n          * stop_token_predictions - tensor of shape [batch_size, time, 1]\n            or [time, batch_size, 1]. The stop token predictions for use inside\n            the loss function.\n    """"""\n    encoder_outputs = input_dict[\'encoder_output\'][\'outputs\']\n    enc_src_lengths = input_dict[\'encoder_output\'][\'src_length\']\n    if self._mode == ""train"":\n      spec = input_dict[\'target_tensors\'][0] if \'target_tensors\' in \\\n                                                    input_dict else None\n      spec_length = input_dict[\'target_tensors\'][2] if \'target_tensors\' in \\\n                                                    input_dict else None\n    _batch_size = encoder_outputs.get_shape().as_list()[0]\n\n    training = (self._mode == ""train"")\n    regularizer = self.params.get(\'regularizer\', None)\n\n    if self.params.get(\'enable_postnet\', True):\n      if ""postnet_conv_layers"" not in self.params:\n        raise ValueError(\n            ""postnet_conv_layers must be passed from config file if postnet is""\n            ""enabled""\n        )\n\n    if self._both:\n      num_audio_features = self._n_feats[""mel""]\n      if self._mode == ""train"":\n        spec, _ = tf.split(\n            spec,\n            [self._n_feats[\'mel\'], self._n_feats[\'magnitude\']],\n            axis=2\n        )\n    else:\n      num_audio_features = self._n_feats\n\n    output_projection_layer = tf.layers.Dense(\n        name=""output_proj"",\n        units=num_audio_features,\n        use_bias=True,\n    )\n    stop_token_projection_layer = tf.layers.Dense(\n        name=""stop_token_proj"",\n        units=1,\n        use_bias=True,\n    )\n\n    prenet = None\n    if self.params.get(\'enable_prenet\', True):\n      prenet = Prenet(\n          self.params.get(\'prenet_units\', 256),\n          self.params.get(\'prenet_layers\', 2),\n          self.params.get(""prenet_activation"", tf.nn.relu),\n          self.params[""dtype""]\n      )\n\n    cell_params = {}\n    cell_params[""num_units""] = self.params[\'decoder_cell_units\']\n    decoder_cells = [\n        single_cell(\n            cell_class=self.params[\'decoder_cell_type\'],\n            cell_params=cell_params,\n            zoneout_prob=self.params.get(""zoneout_prob"", 0.),\n            dp_output_keep_prob=1.-self.params.get(""dropout_prob"", 0.1),\n            training=training,\n        ) for _ in range(self.params[\'decoder_layers\'])\n    ]\n\n    if self.params[\'attention_type\'] is not None:\n      attention_mechanism = self._build_attention(\n          encoder_outputs, enc_src_lengths,\n          self.params.get(""attention_bias"", False)\n      )\n\n      attention_cell = tf.contrib.rnn.MultiRNNCell(decoder_cells)\n\n      attentive_cell = AttentionWrapper(\n          cell=attention_cell,\n          attention_mechanism=attention_mechanism,\n          alignment_history=True,\n          output_attention=""both"",\n      )\n\n      decoder_cell = attentive_cell\n\n    if self.params[\'attention_type\'] is None:\n      decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cells)\n\n    if self._mode == ""train"":\n      train_and_not_sampling = True\n      helper = TacotronTrainingHelper(\n          inputs=spec,\n          sequence_length=spec_length,\n          prenet=None,\n          model_dtype=self.params[""dtype""],\n          mask_decoder_sequence=self.params.get(""mask_decoder_sequence"", True)\n      )\n    elif self._mode == ""eval"" or self._mode == ""infer"":\n      train_and_not_sampling = False\n      inputs = tf.zeros(\n          (_batch_size, 1, num_audio_features), dtype=self.params[""dtype""]\n      )\n      helper = TacotronHelper(\n          inputs=inputs,\n          prenet=None,\n          mask_decoder_sequence=self.params.get(""mask_decoder_sequence"", True)\n      )\n    else:\n      raise ValueError(""Unknown mode for decoder: {}"".format(self._mode))\n    decoder = TacotronDecoder(\n        decoder_cell=decoder_cell,\n        helper=helper,\n        initial_decoder_state=decoder_cell.zero_state(\n            _batch_size, self.params[""dtype""]\n        ),\n        attention_type=self.params[""attention_type""],\n        spec_layer=output_projection_layer,\n        stop_token_layer=stop_token_projection_layer,\n        prenet=prenet,\n        dtype=self.params[""dtype""],\n        train=train_and_not_sampling\n    )\n\n    if self._mode == \'train\':\n      maximum_iterations = tf.reduce_max(spec_length)\n    else:\n      maximum_iterations = tf.reduce_max(enc_src_lengths) * 10\n\n    outputs, final_state, sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n        # outputs, final_state, sequence_lengths, final_inputs = dynamic_decode(\n        decoder=decoder,\n        impute_finished=False,\n        maximum_iterations=maximum_iterations,\n        swap_memory=self.params.get(""use_swap_memory"", False),\n        output_time_major=self.params.get(""time_major"", False),\n        parallel_iterations=self.params.get(""parallel_iterations"", 32)\n    )\n\n    decoder_output = outputs.rnn_output\n    stop_token_logits = outputs.stop_token_output\n\n    with tf.variable_scope(""decoder""):\n      # If we are in train and doing sampling, we need to do the projections\n      if train_and_not_sampling:\n        decoder_spec_output = output_projection_layer(decoder_output)\n        stop_token_logits = stop_token_projection_layer(decoder_spec_output)\n        decoder_output = decoder_spec_output\n\n    ## Add the post net ##\n    if self.params.get(\'enable_postnet\', True):\n      dropout_keep_prob = self.params.get(\'postnet_keep_dropout_prob\', 0.5)\n\n      top_layer = decoder_output\n      for i, conv_params in enumerate(self.params[\'postnet_conv_layers\']):\n        ch_out = conv_params[\'num_channels\']\n        kernel_size = conv_params[\'kernel_size\']  # [time, freq]\n        strides = conv_params[\'stride\']\n        padding = conv_params[\'padding\']\n        activation_fn = conv_params[\'activation_fn\']\n\n        if ch_out == -1:\n          if self._both:\n            ch_out = self._n_feats[""mel""]\n          else:\n            ch_out = self._n_feats\n\n        top_layer = conv_bn_actv(\n            layer_type=""conv1d"",\n            name=""conv{}"".format(i + 1),\n            inputs=top_layer,\n            filters=ch_out,\n            kernel_size=kernel_size,\n            activation_fn=activation_fn,\n            strides=strides,\n            padding=padding,\n            regularizer=regularizer,\n            training=training,\n            data_format=self.params.get(\'postnet_data_format\', \'channels_last\'),\n            bn_momentum=self.params.get(\'postnet_bn_momentum\', 0.1),\n            bn_epsilon=self.params.get(\'postnet_bn_epsilon\', 1e-5),\n        )\n        top_layer = tf.layers.dropout(\n            top_layer, rate=1. - dropout_keep_prob, training=training\n        )\n\n    else:\n      top_layer = tf.zeros(\n          [\n              _batch_size, maximum_iterations,\n              outputs.rnn_output.get_shape()[-1]\n          ],\n          dtype=self.params[""dtype""]\n      )\n\n    if regularizer and training:\n      vars_to_regularize = []\n      vars_to_regularize += attentive_cell.trainable_variables\n      vars_to_regularize += attention_mechanism.memory_layer.trainable_variables\n      vars_to_regularize += output_projection_layer.trainable_variables\n      vars_to_regularize += stop_token_projection_layer.trainable_variables\n\n      for weights in vars_to_regularize:\n        if ""bias"" not in weights.name:\n          # print(""Added regularizer to {}"".format(weights.name))\n          if weights.dtype.base_dtype == tf.float16:\n            tf.add_to_collection(\n                \'REGULARIZATION_FUNCTIONS\', (weights, regularizer)\n            )\n          else:\n            tf.add_to_collection(\n                ops.GraphKeys.REGULARIZATION_LOSSES, regularizer(weights)\n            )\n\n      if self.params.get(\'enable_prenet\', True):\n        prenet.add_regularization(regularizer)\n\n    if self.params[\'attention_type\'] is not None:\n      alignments = tf.transpose(\n          final_state.alignment_history.stack(), [1, 0, 2]\n      )\n    else:\n      alignments = tf.zeros([_batch_size, _batch_size, _batch_size])\n\n    spectrogram_prediction = decoder_output + top_layer\n    if self._both:\n      mag_spec_prediction = spectrogram_prediction\n      mag_spec_prediction = conv_bn_actv(\n          layer_type=""conv1d"",\n          name=""conv_0"",\n          inputs=mag_spec_prediction,\n          filters=256,\n          kernel_size=4,\n          activation_fn=tf.nn.relu,\n          strides=1,\n          padding=""SAME"",\n          regularizer=regularizer,\n          training=training,\n          data_format=self.params.get(\'postnet_data_format\', \'channels_last\'),\n          bn_momentum=self.params.get(\'postnet_bn_momentum\', 0.1),\n          bn_epsilon=self.params.get(\'postnet_bn_epsilon\', 1e-5),\n      )\n      mag_spec_prediction = conv_bn_actv(\n          layer_type=""conv1d"",\n          name=""conv_1"",\n          inputs=mag_spec_prediction,\n          filters=512,\n          kernel_size=4,\n          activation_fn=tf.nn.relu,\n          strides=1,\n          padding=""SAME"",\n          regularizer=regularizer,\n          training=training,\n          data_format=self.params.get(\'postnet_data_format\', \'channels_last\'),\n          bn_momentum=self.params.get(\'postnet_bn_momentum\', 0.1),\n          bn_epsilon=self.params.get(\'postnet_bn_epsilon\', 1e-5),\n      )\n      if self._model.get_data_layer()._exp_mag:\n        mag_spec_prediction = tf.exp(mag_spec_prediction)\n      mag_spec_prediction = tf.layers.conv1d(\n          mag_spec_prediction,\n          self._n_feats[""magnitude""],\n          1,\n          name=""post_net_proj"",\n          use_bias=False,\n      )\n    else:\n      mag_spec_prediction = tf.zeros([_batch_size, _batch_size, _batch_size])\n\n    stop_token_prediction = tf.sigmoid(stop_token_logits)\n    outputs = [\n        decoder_output, spectrogram_prediction, alignments,\n        stop_token_prediction, sequence_lengths, mag_spec_prediction\n    ]\n\n    return {\n        \'outputs\': outputs,\n        \'stop_token_prediction\': stop_token_logits,\n    }\n'"
open_seq2seq/decoders/transformer_decoder.py,23,"b'# This code is heavily based on the code from MLPerf\n# https://github.com/mlperf/reference/tree/master/translation/tensorflow/transformer\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom open_seq2seq.parts.transformer import utils, attention_layer, \\\n                                           ffn_layer, beam_search\nfrom open_seq2seq.parts.transformer.common import PrePostProcessingWrapper, \\\n                                    LayerNormalization, Transformer_BatchNorm\nfrom .decoder import Decoder\n\n\nclass TransformerDecoder(Decoder):\n  @staticmethod\n  def get_required_params():\n    """"""Static method with description of required parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **have to** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return dict(Decoder.get_required_params(), **{\n        \'EOS_ID\': int,\n        \'layer_postprocess_dropout\': float,\n        \'num_hidden_layers\': int,\n        \'hidden_size\': int,\n        \'num_heads\': int,\n        \'attention_dropout\': float,\n        \'relu_dropout\': float,\n        \'filter_size\': int,\n        \'batch_size\': int,\n        \'tgt_vocab_size\': int,\n        \'beam_size\': int,\n        \'alpha\': float,\n        \'extra_decode_length\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    """"""Static method with description of optional parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **can** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return dict(Decoder.get_optional_params(), **{\n        \'regularizer\': None,  # any valid TensorFlow regularizer\n        \'regularizer_params\': dict,\n        \'initializer\': None,  # any valid TensorFlow initializer\n        \'initializer_params\': dict,\n        \'GO_SYMBOL\': int,\n        \'PAD_SYMBOL\': int,\n        \'END_SYMBOL\': int,\n        \'norm_params\': dict,\n    })\n\n  def _cast_types(self, input_dict):\n    return input_dict\n\n  def __init__(self, params, model,\n               name=""transformer_decoder"", mode=\'train\'):\n    super(TransformerDecoder, self).__init__(params, model, name, mode)\n    self.embedding_softmax_layer = None\n    self.output_normalization = None\n    self._mode = mode\n    self.layers = []\n    # in original T paper embeddings are shared between encoder and decoder\n    # also final projection = transpose(E_weights), we currently only support\n    # this behaviour\n    self.params[\'shared_embed\'] = True\n    self.norm_params = self.params.get(""norm_params"", {""type"": ""layernorm_L2"" })\n    self.regularizer = self.params.get(""regularizer"", None)\n    if self.regularizer != None:\n      self.regularizer_params = params.get(""regularizer_params"", {\'scale\': 0.0})\n      self.regularizer=self.regularizer(self.regularizer_params[\'scale\']) \\\n        if self.regularizer_params[\'scale\'] > 0.0 else None\n      #print(""reg"", self.regularizer)\n\n  def _decode(self, input_dict):\n    if \'target_tensors\' in input_dict:\n      targets = input_dict[\'target_tensors\'][0]\n    else:\n      targets = None\n    encoder_outputs = input_dict[\'encoder_output\'][\'outputs\']\n    inputs_attention_bias = (\n      input_dict[\'encoder_output\'][\'inputs_attention_bias\']\n    )\n    self.embedding_softmax_layer = (\n      input_dict[\'encoder_output\'][\'embedding_softmax_layer\']\n    )\n\n    with tf.name_scope(""decode""):\n      training = (self.mode == ""train"")\n      # prepare decoder layers\n      if len(self.layers) == 0:\n        for _ in range(self.params[""num_hidden_layers""]):\n          self_attention_layer = attention_layer.SelfAttention(\n            hidden_size=self.params[""hidden_size""],\n            num_heads=self.params[""num_heads""],\n            attention_dropout=self.params[""attention_dropout""],\n            train=training,\n            regularizer=self.regularizer\n          )\n          enc_dec_attention_layer = attention_layer.Attention(\n            hidden_size=self.params[""hidden_size""],\n            num_heads=self.params[""num_heads""],\n            attention_dropout=self.params[""attention_dropout""],\n            train=training,\n            regularizer=self.regularizer\n          )\n          feed_forward_network = ffn_layer.FeedFowardNetwork(\n            hidden_size=self.params[""hidden_size""],\n            filter_size=self.params[""filter_size""],\n            relu_dropout=self.params[""relu_dropout""],\n            train=training,\n            regularizer=self.regularizer\n          )\n\n          self.layers.append([\n              PrePostProcessingWrapper(self_attention_layer, self.params,\n                                       training),\n              PrePostProcessingWrapper(enc_dec_attention_layer, self.params,\n                                       training),\n              PrePostProcessingWrapper(feed_forward_network, self.params,\n                                       training)\n          ])\n        print(""Decoder:"", self.norm_params[""type""], self.mode)\n        if self.norm_params[""type""] == ""batch_norm"":\n          self.output_normalization = Transformer_BatchNorm(\n            training=training,\n            params=self.norm_params)\n        else:\n          self.output_normalization = LayerNormalization(\n            hidden_size=self.params[""hidden_size""],\n            params=self.norm_params)\n\n      if targets is None:\n        return self.predict(encoder_outputs, inputs_attention_bias)\n      else:\n        logits = self.decode_pass(targets, encoder_outputs,\n                                  inputs_attention_bias)\n        return {""logits"": logits,\n                ""outputs"": [tf.argmax(logits, axis=-1)],\n                ""final_state"": None,\n                ""final_sequence_lengths"": None}\n\n  def _call(self, decoder_inputs, encoder_outputs, decoder_self_attention_bias,\n            attention_bias, cache=None):\n    for n, layer in enumerate(self.layers):\n      self_attention_layer = layer[0]\n      enc_dec_attention_layer = layer[1]\n      feed_forward_network = layer[2]\n\n      # Run inputs through the sublayers.\n      layer_name = ""layer_%d"" % n\n      layer_cache = cache[layer_name] if cache is not None else None\n      with tf.variable_scope(layer_name):\n        with tf.variable_scope(""self_attention""):\n          # TODO: Figure out why this is needed\n          # decoder_self_attention_bias = tf.cast(x=decoder_self_attention_bias,\n          #                                      dtype=decoder_inputs.dtype)\n          decoder_inputs = self_attention_layer(\n              decoder_inputs, decoder_self_attention_bias, cache=layer_cache,\n          )\n        with tf.variable_scope(""encdec_attention""):\n          decoder_inputs = enc_dec_attention_layer(\n              decoder_inputs, encoder_outputs, attention_bias,\n          )\n        with tf.variable_scope(""ffn""):\n          decoder_inputs = feed_forward_network(decoder_inputs)\n\n    return self.output_normalization(decoder_inputs)\n\n  def decode_pass(self, targets, encoder_outputs, inputs_attention_bias):\n    """"""Generate logits for each value in the target sequence.\n\n    Args:\n      targets: target values for the output sequence.\n        int tensor with shape [batch_size, target_length]\n      encoder_outputs: continuous representation of input sequence.\n        float tensor with shape [batch_size, input_length, hidden_size]\n      inputs_attention_bias: float tensor with shape [batch_size, 1, 1, input_length]\n\n    Returns:\n      float32 tensor with shape [batch_size, target_length, vocab_size]\n    """"""\n    # Prepare inputs to decoder layers by shifting targets, adding positional\n    # encoding and applying dropout.\n    decoder_inputs = self.embedding_softmax_layer(targets)\n    with tf.name_scope(""shift_targets""):\n      # Shift targets to the right, and remove the last element\n      decoder_inputs = tf.pad(\n          decoder_inputs, [[0, 0], [1, 0], [0, 0]],\n      )[:, :-1, :]\n    with tf.name_scope(""add_pos_encoding""):\n      length = tf.shape(decoder_inputs)[1]\n      # decoder_inputs += utils.get_position_encoding(\n      #    length, self.params[""hidden_size""])\n      decoder_inputs += tf.cast(\n          utils.get_position_encoding(length, self.params[""hidden_size""]),\n          dtype=self.params[\'dtype\'],\n      )\n    if self.mode == ""train"":\n        decoder_inputs = tf.nn.dropout(decoder_inputs,\n            keep_prob = 1 - self.params[""layer_postprocess_dropout""] )\n\n    # Run values\n    decoder_self_attention_bias = utils.get_decoder_self_attention_bias(length,\n                                            dtype = tf.float32\n                                            # dtype=self._params[""dtype""]\n                                            )\n\n    # do decode\n    outputs = self._call(\n        decoder_inputs=decoder_inputs,\n        encoder_outputs=encoder_outputs,\n        decoder_self_attention_bias=decoder_self_attention_bias,\n        attention_bias=inputs_attention_bias,\n    )\n\n    logits = self.embedding_softmax_layer.linear(outputs)\n    return logits\n\n  def _get_symbols_to_logits_fn(self, max_decode_length):\n    """"""Returns a decoding function that calculates logits of the next tokens.""""""\n\n    timing_signal = utils.get_position_encoding(\n        max_decode_length + 1, self.params[""hidden_size""],\n    )\n    decoder_self_attention_bias = utils.get_decoder_self_attention_bias(\n        max_decode_length, dtype = tf.float32\n        # dtype=self._params[""dtype""]\n    )\n\n    def symbols_to_logits_fn(ids, i, cache):\n      """"""Generate logits for next potential IDs.\n\n      Args:\n        ids: Current decoded sequences.\n          int tensor with shape [batch_size * beam_size, i + 1]\n        i: Loop index\n        cache: dictionary of values storing the encoder output, encoder-decoder\n          attention bias, and previous decoder attention values.\n\n      Returns:\n        Tuple of\n          (logits with shape [batch_size * beam_size, vocab_size],\n           updated cache values)\n      """"""\n      # Set decoder input to the last generated IDs\n      decoder_input = ids[:, -1:]\n\n      # Preprocess decoder input by getting embeddings and adding timing signal.\n      decoder_input = self.embedding_softmax_layer(decoder_input)\n      decoder_input += tf.cast(x=timing_signal[i:i + 1],\n                               dtype=decoder_input.dtype)\n\n      self_attention_bias = decoder_self_attention_bias[:, :, i:i + 1, :i + 1]\n\n      decoder_outputs = self._call(\n          decoder_input, cache.get(""encoder_outputs""), self_attention_bias,\n          cache.get(""encoder_decoder_attention_bias""), cache,\n      )\n      logits = self.embedding_softmax_layer.linear(decoder_outputs)\n      logits = tf.squeeze(logits, axis=[1])\n      return tf.cast(logits, tf.float32), cache\n\n    return symbols_to_logits_fn\n\n  def predict(self, encoder_outputs, encoder_decoder_attention_bias):\n    """"""Return predicted sequence.""""""\n    batch_size = tf.shape(encoder_outputs)[0]\n    input_length = tf.shape(encoder_outputs)[1]\n    max_decode_length = input_length + self.params[""extra_decode_length""]\n\n    symbols_to_logits_fn = self._get_symbols_to_logits_fn(max_decode_length)\n\n    # Create initial set of IDs that will be passed into symbols_to_logits_fn.\n    initial_ids = tf.zeros([batch_size], dtype=tf.int32)\n\n    # Create cache storing decoder attention values for each layer.\n    cache = {\n        ""layer_%d"" % layer: {\n            ""k"": tf.zeros([batch_size, 0,\n                           self.params[""hidden_size""]],\n                          dtype=encoder_outputs.dtype),\n            ""v"": tf.zeros([batch_size, 0,\n                           self.params[""hidden_size""]],\n                          dtype=encoder_outputs.dtype),\n        } for layer in range(self.params[""num_hidden_layers""])\n    }\n\n    # Add encoder output and attention bias to the cache.\n    cache[""encoder_outputs""] = encoder_outputs\n    cache[""encoder_decoder_attention_bias""] = encoder_decoder_attention_bias\n\n    # Use beam search to find the top beam_size sequences and scores.\n    decoded_ids, scores = beam_search.sequence_beam_search(\n        symbols_to_logits_fn=symbols_to_logits_fn,\n        initial_ids=initial_ids,\n        initial_cache=cache,\n        vocab_size=self.params[""tgt_vocab_size""],\n        beam_size=self.params[""beam_size""],\n        alpha=self.params[""alpha""],\n        max_decode_length=max_decode_length,\n        eos_id=self.params[""EOS_ID""],\n    )\n\n    # Get the top sequence for each batch element\n    top_decoded_ids = decoded_ids[:, 0, 1:]\n\n    # this isn\'t particularly efficient\n    logits = self.decode_pass(top_decoded_ids, encoder_outputs,\n                              encoder_decoder_attention_bias)\n    return {""logits"": logits,\n            ""outputs"": [top_decoded_ids],\n            ""final_state"": None,\n            ""final_sequence_lengths"": None}\n'"
open_seq2seq/encoders/__init__.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nThis package contains various encoders.\nAn encoder typically takes data and produces representation.\n""""""\nfrom .encoder import Encoder\nfrom .rnn_encoders import UnidirectionalRNNEncoderWithEmbedding, \\\n                          BidirectionalRNNEncoderWithEmbedding, \\\n                          GNMTLikeEncoderWithEmbedding,\\\n                          GNMTLikeEncoderWithEmbedding_cuDNN\nfrom .transformer_encoder import TransformerEncoder\nfrom .ds2_encoder import DeepSpeech2Encoder\nfrom .resnet_encoder import ResNetEncoder\nfrom .tacotron2_encoder import Tacotron2Encoder\nfrom .tdnn_encoder import TDNNEncoder\nfrom .las_encoder import ListenAttendSpellEncoder\nfrom .convs2s_encoder import ConvS2SEncoder\nfrom .lm_encoders import LMEncoder\nfrom .wavenet_encoder import WavenetEncoder\nfrom .centaur_encoder import CentaurEncoder'"
open_seq2seq/encoders/centaur_encoder.py,1,"b'import tensorflow as tf\n\nfrom open_seq2seq.encoders import Encoder\nfrom open_seq2seq.parts.centaur import ConvBlock\nfrom open_seq2seq.parts.transformer import embedding_layer\nfrom open_seq2seq.parts.transformer import utils\n\n\nclass CentaurEncoder(Encoder):\n  """"""\n  Centaur encoder that consists of convolutional layers.\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        ""src_vocab_size"": int,\n        ""embedding_size"": int,\n        ""output_size"": int,\n        ""conv_layers"": list\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        ""pad_embeddings_2_eight"": bool,\n        ""regularizer"": None,\n        ""bn_momentum"": float,\n        ""bn_epsilon"": float,\n        ""cnn_dropout_prob"": float,\n        ""norm_type"": str\n    })\n\n  def __init__(self, params, model, name=""centaur_encoder"", mode=""train""):\n    """"""\n    Centaur encoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **src_vocab_size** (int) --- number of symbols in alphabet.\n    * **embedding_size** (int) --- dimensionality of character embedding.\n    * **output_size** (int) --- dimensionality of output embedding.\n    * **conv_layers** (list) --- list with the description of convolutional\n      layers. For example::\n        ""conv_layers"": [\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME""\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME""\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME""\n          }\n        ]\n    * **bn_momentum** (float) --- momentum for batch norm. Defaults to 0.95.\n    * **bn_epsilon** (float) --- epsilon for batch norm. Defaults to 1e-8.\n    * **cnn_dropout_prob** (float) --- dropout probabilty for cnn layers.\n      Defaults to 0.5.\n\n    """"""\n\n    super(CentaurEncoder, self).__init__(params, model, name=name, mode=mode)\n    self.training = mode == ""train""\n    self.layers = []\n\n  def _build_layers(self):\n    regularizer = self._params.get(""regularizer"", None)\n\n    embedding = embedding_layer.EmbeddingSharedWeights(\n        vocab_size=self._params[""src_vocab_size""],\n        hidden_size=self._params[""embedding_size""],\n        pad_vocab_to_eight=self.params.get(""pad_embeddings_2_eight"", False),\n        regularizer=regularizer\n    )\n    self.layers.append(embedding)\n\n    cnn_dropout_prob = self._params.get(""cnn_dropout_prob"", 0.5)\n    bn_momentum = self._params.get(""bn_momentum"", 0.95)\n    bn_epsilon = self._params.get(""bn_epsilon"", -1e8)\n\n    for index, params in enumerate(self._params[""conv_layers""]):\n      layer = ConvBlock.create(\n          index=index,\n          conv_params=params,\n          regularizer=regularizer,\n          bn_momentum=bn_momentum,\n          bn_epsilon=bn_epsilon,\n          cnn_dropout_prob=cnn_dropout_prob,\n          training=self.training\n      )\n\n      self.layers.append(layer)\n\n    linear_projection = tf.layers.Dense(\n        name=""linear_projection"",\n        units=self._params[""output_size""],\n        use_bias=False,\n        kernel_regularizer=regularizer\n    )\n    self.layers.append(linear_projection)\n\n  def _encode(self, input_dict):\n    if not self.layers:\n      self._build_layers()\n\n    x = input_dict[""source_tensors""][0]\n    text_len = input_dict[""source_tensors""][1]\n\n    # Apply all layers\n    y = x\n    for layer in self.layers:\n      y = layer(y)\n\n    inputs_attention_bias = utils.get_padding_bias(x)\n\n    return {\n        ""outputs"": y,\n        ""inputs_attention_bias"": inputs_attention_bias,\n        ""src_lengths"": text_len\n    }\n'"
open_seq2seq/encoders/cnn_encoder.py,14,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nThis module contains classes and functions to build ""general"" convolutional\nneural networks from the description of arbitrary ""layers"".\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport copy\n\nimport tensorflow as tf\n\ntry:\n  from inspect import signature\nexcept ImportError:\n  from funcsigs import signature\n\nfrom open_seq2seq.utils.utils import deco_print\nfrom .encoder import Encoder\n\n\ndef build_layer(inputs, layer, layer_params, data_format,\n                regularizer, training, verbose=True):\n  """"""This function builds a layer from the layer function and it\'s parameters.\n\n  It will automatically add regularizer parameter to the layer_params if the\n  layer supports regularization. To check this, it will look for the\n  ""regularizer"", ""kernel_regularizer"" and ""gamma_regularizer"" names in this\n  order in the ``layer`` call signature. If one of this parameters is supported\n  it will pass regularizer object as a value for that parameter. Based on the\n  same ""checking signature"" technique ""data_format"" and ""training"" parameters\n  will try to be added. Finally, ""axis"" parameter will try to be specified with\n  axis = ``1 if data_format == \'channels_first\' else 3``. This is required for\n  automatic building batch normalization layer.\n\n  Args:\n    inputs: input Tensor that will be passed to the layer. Note that layer has\n        to accept input as the first parameter.\n    layer: layer function or class with ``__call__`` method defined.\n    layer_params (dict): parameters passed to the ``layer``.\n    data_format (string): data format (""channels_first"" or ""channels_last"")\n        that will be tried to be passed as an additional argument.\n    regularizer: regularizer instance that will be tried to be passed as an\n        additional argument.\n    training (bool): whether layer is built in training mode. Will be tried to\n        be passed as an additional argument.\n    verbose (bool): whether to print information about built layers.\n\n  Returns:\n    Tensor with layer output.\n  """"""\n  layer_params_cp = copy.deepcopy(layer_params)\n  for reg_name in [\'regularizer\', \'kernel_regularizer\', \'gamma_regularizer\']:\n    if reg_name not in layer_params_cp and \\\n       reg_name in signature(layer).parameters:\n      layer_params_cp.update({reg_name: regularizer})\n\n  if \'data_format\' not in layer_params_cp and \\\n     \'data_format\' in signature(layer).parameters:\n    layer_params_cp.update({\'data_format\': data_format})\n\n  # necessary to check axis for correct batch normalization processing\n  if \'axis\' not in layer_params_cp and \\\n     \'axis\' in signature(layer).parameters:\n    layer_params_cp.update({\'axis\': 1 if data_format == \'channels_first\' else 3})\n\n  if \'training\' not in layer_params_cp and \\\n     \'training\' in signature(layer).parameters:\n    layer_params_cp.update({\'training\': training})\n\n  outputs = layer(inputs, **layer_params_cp)\n\n  if verbose:\n    if hasattr(layer, \'_tf_api_names\'):\n      layer_name = layer._tf_api_names[0]\n    else:\n      layer_name = layer\n    deco_print(""Building layer: {}(inputs, {})"".format(\n        layer_name,\n        "", "".join(""{}={}"".format(key, value)\n                  for key, value in layer_params_cp.items())\n    ))\n  return outputs\n\n\nclass CNNEncoder(Encoder):\n  """"""General CNN encoder that can be used to construct various different models.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        \'cnn_layers\': list,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'data_format\': [\'channels_first\', \'channels_last\'],\n        \'fc_layers\': list,\n    })\n\n  def __init__(self, params, model, name=""cnn_encoder"", mode=\'train\'):\n    """"""CNN Encoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **cnn_layers** (list) --- list with the description of ""convolutional""\n      layers. For example::\n        ""conv_layers"": [\n            (tf.layers.conv2d, {\n                \'filters\': 64, \'kernel_size\': (11, 11),\n                \'strides\': (4, 4), \'padding\': \'VALID\',\n                \'activation\': tf.nn.relu,\n            }),\n            (tf.layers.max_pooling2d, {\n                \'pool_size\': (3, 3), \'strides\': (2, 2),\n            }),\n            (tf.layers.conv2d, {\n                \'filters\': 192, \'kernel_size\': (5, 5),\n                \'strides\': (1, 1), \'padding\': \'SAME\',\n            }),\n            (tf.layers.batch_normalization, {\'momentum\': 0.9, \'epsilon\': 0.0001}),\n            (tf.nn.relu, {}),\n        ]\n      Note that you don\'t need to provide ""regularizer"", ""training"",\n      ""data_format"" and ""axis"" parameters since they will be\n      automatically added. ""axis"" will be derived from ""data_format"" and will\n      be ``1 if data_format == ""channels_first"" else 3``.\n\n    * **fc_layers** (list) --- list with the description of ""fully-connected""\n      layers. The only different from convolutional layers is that the input\n      will be automatically reshaped to 2D (batch size x num features).\n      For example::\n        \'fc_layers\': [\n            (tf.layers.dense, {\'units\': 4096, \'activation\': tf.nn.relu}),\n            (tf.layers.dropout, {\'rate\': 0.5}),\n            (tf.layers.dense, {\'units\': 4096, \'activation\': tf.nn.relu}),\n            (tf.layers.dropout, {\'rate\': 0.5}),\n        ],\n      Note that you don\'t need to provide ""regularizer"", ""training"",\n      ""data_format"" and ""axis"" parameters since they will be\n      automatically added. ""axis"" will be derived from ""data_format"" and will\n      be ``1 if data_format == ""channels_first"" else 3``.\n\n    * **data_format** (string) --- could be either ""channels_first"" or\n      ""channels_last"". Defaults to ""channels_first"".\n    """"""\n    super(CNNEncoder, self).__init__(params, model, name, mode)\n\n  def _encode(self, input_dict):\n    regularizer = self.params.get(\'regularizer\', None)\n    data_format = self.params.get(\'data_format\', \'channels_first\')\n\n    x = input_dict[\'source_tensors\'][0]\n    if data_format == \'channels_first\':\n      x = tf.transpose(x, [0, 3, 1, 2])\n\n    for layer, layer_params in self.params[\'cnn_layers\']:\n      x = build_layer(x, layer, layer_params, data_format,\n                      regularizer, self.mode == \'train\')\n\n    if data_format == \'channels_first\':\n      x = tf.transpose(x, [0, 2, 3, 1])\n\n    fc_layers = self.params.get(\'fc_layers\', [])\n\n    # if fully connected layers exist, flattening the output and applying them\n    if fc_layers:\n      input_shape = x.get_shape().as_list()\n      num_inputs = input_shape[1] * input_shape[2] * input_shape[3]\n      x = tf.reshape(x, [-1, num_inputs])\n      for layer, layer_params in fc_layers:\n        x = build_layer(x, layer, layer_params, data_format, regularizer,\n                        self.mode == \'train\')\n    else:\n      # if there are no fully connected layers, doing average pooling\n      x = tf.reduce_mean(x, [1, 2])\n\n    return {\'outputs\': x}\n'"
open_seq2seq/encoders/convs2s_encoder.py,15,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nConv-based encoder\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nimport math\nfrom .encoder import Encoder\n\nfrom open_seq2seq.parts.transformer import embedding_layer\nfrom open_seq2seq.parts.transformer.utils import get_padding_bias, get_padding\nfrom open_seq2seq.parts.convs2s import ffn_wn_layer, conv_wn_layer\nfrom open_seq2seq.parts.convs2s.utils import gated_linear_units\n\n# Default value used if max_input_length is not given\nMAX_INPUT_LENGTH = 128\n\n\nclass ConvS2SEncoder(Encoder):\n  """"""\n  Fully convolutional Encoder of ConvS2S\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(\n        Encoder.get_required_params(), **{\n            ""src_emb_size"": int,\n            ""src_vocab_size"": int,\n            ""pad_embeddings_2_eight"": bool,\n            ""conv_nchannels_kwidth"": list,\n            ""embedding_dropout_keep_prob"": float,\n            ""hidden_dropout_keep_prob"": float,\n        })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(\n        Encoder.get_optional_params(), **{\n            ""att_layer_num"": int,\n            \'max_input_length\': int,\n            \'PAD_SYMBOL\': int,\n            \'conv_activation\': None,\n            \'normalization_type\': str,\n            \'scaling_factor\': float,\n            \'init_var\': None,\n      })\n\n  def __init__(self,\n               params,\n               model,\n               name=""convs2s_encoder_with_emb"",\n               mode=\'train\'):\n    super(ConvS2SEncoder, self).__init__(params, model, name=name, mode=mode)\n\n    self._src_vocab_size = self.params[\'src_vocab_size\']\n    self._src_emb_size = self.params[\'src_emb_size\']\n    self.layers = []\n    self._mode = mode\n    self._pad_sym = self.params.get(\'PAD_SYMBOL\', 0)\n    self._pad2eight = params.get(\'pad_embeddings_2_eight\', False)\n    self.scaling_factor = self.params.get(""scaling_factor"", math.sqrt(0.5))\n    self.normalization_type = self.params.get(""normalization_type"", ""weight_norm"")\n    self.conv_activation = self.params.get(""conv_activation"", gated_linear_units)\n    self.regularizer = self.params.get(\'regularizer\', None)\n    self.init_var = self.params.get(\'init_var\', None)\n\n  def _encode(self, input_dict):\n    inputs = input_dict[\'source_tensors\'][0]\n    source_length = input_dict[\'source_tensors\'][1]\n\n    with tf.variable_scope(""encode""):\n      # prepare encoder graph\n      if len(self.layers) == 0:\n        knum_list = list(zip(*self.params.get(""conv_nchannels_kwidth"")))[0]\n        kwidth_list = list(zip(*self.params.get(""conv_nchannels_kwidth"")))[1]\n\n        with tf.variable_scope(""embedding""):\n          self.embedding_softmax_layer = embedding_layer.EmbeddingSharedWeights(\n              vocab_size=self._src_vocab_size,\n              hidden_size=self._src_emb_size,\n              pad_vocab_to_eight=self._pad2eight,\n              init_var=0.1,\n              embed_scale=False,\n              pad_sym=self._pad_sym,\n              mask_paddings=True)\n\n        with tf.variable_scope(""pos_embedding""):\n          self.position_embedding_layer = embedding_layer.EmbeddingSharedWeights(\n              vocab_size=self.params.get(""max_input_length"", MAX_INPUT_LENGTH),\n              hidden_size=self._src_emb_size,\n              pad_vocab_to_eight=self._pad2eight,\n              init_var=0.1,\n              embed_scale=False,\n              pad_sym=self._pad_sym,\n              mask_paddings=True)\n\n        # linear projection before cnn layers\n        self.layers.append(\n            ffn_wn_layer.FeedFowardNetworkNormalized(\n                self._src_emb_size,\n                knum_list[0],\n                dropout=self.params[""embedding_dropout_keep_prob""],\n                var_scope_name=""linear_mapping_before_cnn_layers"",\n                mode=self.mode,\n                normalization_type=self.normalization_type,\n                regularizer=self.regularizer,\n                init_var=self.init_var))\n\n        for i in range(len(knum_list)):\n          in_dim = knum_list[i] if i == 0 else knum_list[i - 1]\n          out_dim = knum_list[i]\n\n          # linear projection is needed for residual connections if\n          # input and output of a cnn layer do not match\n          if in_dim != out_dim:\n            linear_proj = ffn_wn_layer.FeedFowardNetworkNormalized(\n                in_dim,\n                out_dim,\n                var_scope_name=""linear_mapping_cnn_"" + str(i + 1),\n                dropout=1.0,\n                mode=self.mode,\n                normalization_type=self.normalization_type,\n                regularizer=self.regularizer,\n                init_var=self.init_var)\n          else:\n            linear_proj = None\n\n          conv_layer = conv_wn_layer.Conv1DNetworkNormalized(\n              in_dim,\n              out_dim,\n              kernel_width=kwidth_list[i],\n              mode=self.mode,\n              layer_id=i + 1,\n              hidden_dropout=self.params[""hidden_dropout_keep_prob""],\n              conv_padding=""SAME"",\n              decode_padding=False,\n              activation=self.conv_activation,\n              normalization_type=self.normalization_type,\n              regularizer=self.regularizer,\n              init_var=self.init_var)\n\n          self.layers.append([linear_proj, conv_layer])\n\n        # linear projection after cnn layers\n        self.layers.append(\n            ffn_wn_layer.FeedFowardNetworkNormalized(\n                knum_list[-1],\n                self._src_emb_size,\n                dropout=1.0,\n                var_scope_name=""linear_mapping_after_cnn_layers"",\n                mode=self.mode,\n                normalization_type=self.normalization_type,\n                regularizer=self.regularizer,\n                init_var=self.init_var))\n\n      encoder_inputs = self.embedding_softmax_layer(inputs)\n      inputs_attention_bias = get_padding_bias(\n          inputs, res_rank=3, pad_sym=self._pad_sym)\n\n      with tf.name_scope(""add_pos_encoding""):\n        pos_input = tf.range(\n            0,\n            tf.shape(encoder_inputs)[1],\n            delta=1,\n            dtype=tf.int32,\n            name=\'range\')\n        pos_encoding = self.position_embedding_layer(pos_input)\n        encoder_inputs = encoder_inputs + tf.cast(\n            x=pos_encoding, dtype=encoder_inputs.dtype)\n\n      if self.mode == ""train"":\n        encoder_inputs = tf.nn.dropout(\n            encoder_inputs, self.params[""embedding_dropout_keep_prob""])\n\n      # mask the paddings in the input given to cnn layers\n      inputs_padding = get_padding(\n          inputs, self._pad_sym, dtype=encoder_inputs.dtype)\n      padding_mask = tf.expand_dims(1 - inputs_padding, 2)\n      encoder_inputs *= padding_mask\n\n      outputs, outputs_b, final_state = self._call(encoder_inputs, padding_mask)\n\n    return {\n        \'outputs\': outputs,\n        \'outputs_b\': outputs_b,\n        \'inputs_attention_bias_cs2s\': inputs_attention_bias,\n        \'state\': final_state,\n        \'src_lengths\': source_length,  # should it include paddings or not?\n        \'embedding_softmax_layer\': self.embedding_softmax_layer,\n        \'encoder_input\': inputs\n    }\n\n  def _call(self, encoder_inputs, padding_mask):\n    # Run inputs through the sublayers.\n    with tf.variable_scope(""linear_layer_before_cnn_layers""):\n      outputs = self.layers[0](encoder_inputs)\n\n    for i in range(1, len(self.layers) - 1):\n      linear_proj, conv_layer = self.layers[i]\n\n      with tf.variable_scope(""layer_%d"" % i):\n        if linear_proj is not None:\n          res_inputs = linear_proj(outputs)\n        else:\n          res_inputs = outputs\n\n        if padding_mask is not None:\n          outputs *= padding_mask\n\n        outputs = conv_layer(outputs)\n        outputs = (outputs + res_inputs) * self.scaling_factor\n\n    with tf.variable_scope(""linear_layer_after_cnn_layers""):\n      outputs = self.layers[-1](outputs)\n\n      if padding_mask is not None:\n        outputs *= padding_mask\n\n      # Gradients are scaled as the gradients from\n      # all decoder attention layers enters the encoder\n      scale = 1.0 / (\n          2.0 * self.params.get(""att_layer_num"", 1))\n      outputs = (1.0 - scale) * tf.stop_gradient(outputs) + scale * outputs\n\n      outputs_b = (outputs + encoder_inputs) * self.scaling_factor\n\n      if padding_mask is not None:\n        outputs_b *= padding_mask\n\n      # Average of the encoder outputs is calculated as the final state of the encoder\n      # it can be used for decoders which just accept the final state\n      final_state = tf.reduce_mean(outputs_b, 1)\n    return outputs, outputs_b, final_state\n\n  @property\n  def src_vocab_size(self):\n    return self._src_vocab_size\n\n  @property\n  def src_emb_size(self):\n    return self._src_emb_size\n'"
open_seq2seq/encoders/ds2_encoder.py,40,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\nfrom six.moves import range\n\nfrom open_seq2seq.parts.cnns.conv_blocks import conv_bn_actv\nfrom .encoder import Encoder\n\n\ndef rnn_cell(rnn_cell_dim, layer_type, dropout_keep_prob=1.0):\n  """"""Helper function that creates RNN cell.""""""\n  if layer_type == ""layernorm_lstm"":\n    # pylint: disable=no-member\n    cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n        num_units=rnn_cell_dim, dropout_keep_prob=dropout_keep_prob)\n  else:\n    if layer_type == ""lstm"":\n      cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_cell_dim)\n    elif layer_type == ""gru"":\n      cell = tf.nn.rnn_cell.GRUCell(rnn_cell_dim)\n    elif layer_type == ""cudnn_gru"":\n      # pylint: disable=no-member\n      cell = tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(rnn_cell_dim)\n    elif layer_type == ""cudnn_lstm"":\n      # pylint: disable=no-member\n      cell = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(rnn_cell_dim)\n    else:\n      raise ValueError(""Error: not supported rnn type:{}"".format(layer_type))\n\n    cell = tf.nn.rnn_cell.DropoutWrapper(\n        cell, output_keep_prob=dropout_keep_prob)\n  return cell\n\n\ndef row_conv(name, input_layer, batch, channels, width, activation_fn,\n             regularizer, training, data_format, bn_momentum, bn_epsilon):\n  """"""Helper function that applies ""row"" or ""in plane"" convolution.""""""\n  if width < 2:\n    return input_layer\n\n  if data_format == \'channels_last\':\n    x = tf.reshape(input_layer, [batch, -1, 1, channels])\n  else:\n    input_layer = tf.transpose(input_layer, [0, 2, 1])  # B C T\n    x = tf.reshape(input_layer, [batch, channels, -1, 1])\n  cast_back = False\n  if x.dtype.base_dtype == tf.float16:\n    x = tf.cast(x, tf.float32)\n    cast_back = True\n  filters = tf.get_variable(\n      name + \'/w\',\n      shape=[width, 1, channels, 1],\n      regularizer=regularizer,\n      dtype=tf.float32,\n  )\n  strides = [1, 1, 1, 1]\n  y = tf.nn.depthwise_conv2d(\n      name=name + \'/conv\',\n      input=x,\n      filter=filters,\n      strides=strides,\n      padding=\'SAME\',\n      data_format=\'NHWC\' if data_format == \'channels_last\' else \'NCHW\',\n  )\n  bn = tf.layers.batch_normalization(\n      name=""{}/bn"".format(name),\n      inputs=y,\n      gamma_regularizer=regularizer,\n      training=training,\n      axis=-1 if data_format == \'channels_last\' else 1,\n      momentum=bn_momentum,\n      epsilon=bn_epsilon,\n  )\n  output = activation_fn(bn)\n  if data_format == \'channels_first\':\n    output = tf.transpose(output, [0, 2, 3, 1])\n  output = tf.reshape(output, [batch, -1, channels])\n  if cast_back:\n    output = tf.cast(output, tf.float16)\n  return output\n\n\nclass DeepSpeech2Encoder(Encoder):\n  """"""DeepSpeech-2 like encoder.""""""\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        \'dropout_keep_prob\': float,\n        \'conv_layers\': list,\n        \'activation_fn\': None,  # any valid callable\n        \'num_rnn_layers\': int,\n        \'row_conv\': bool,\n        \'n_hidden\': int,\n        \'use_cudnn_rnn\': bool,\n        \'rnn_cell_dim\': int,\n        \'rnn_type\': [\'layernorm_lstm\', \'lstm\', \'gru\',\n                     \'cudnn_gru\', \'cudnn_lstm\'],\n        \'rnn_unidirectional\': bool,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'row_conv_width\': int,\n        \'data_format\': [\'channels_first\', \'channels_last\', \'BCTF\', \'BTFC\', \'BCFT\', \'BFTC\'],\n        \'bn_momentum\': float,\n        \'bn_epsilon\': float,\n    })\n\n  def __init__(self, params, model, name=""ds2_encoder"", mode=\'train\'):\n    """"""DeepSpeech-2 like encoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **dropout_keep_prop** (float) --- keep probability for dropout.\n    * **conv_layers** (list) --- list with the description of convolutional\n      layers. For example::\n        ""conv_layers"": [\n          {\n            ""kernel_size"": [11, 41], ""stride"": [2, 2],\n            ""num_channels"": 32, ""padding"": ""SAME"",\n          },\n          {\n            ""kernel_size"": [11, 21], ""stride"": [1, 2],\n            ""num_channels"": 64, ""padding"": ""SAME"",\n          },\n          {\n            ""kernel_size"": [11, 21], ""stride"": [1, 2],\n            ""num_channels"": 96, ""padding"": ""SAME"",\n          },\n        ]\n    * **activation_fn** --- activation function to use.\n    * **num_rnn_layers** --- number of RNN layers to use.\n    * **rnn_type** (string) --- could be ""lstm"", ""gru"", ""cudnn_gru"",\n      ""cudnn_lstm"" or ""layernorm_lstm"".\n    * **rnn_unidirectional** (bool) --- whether to use uni-directional or\n      bi-directional RNNs.\n    * **rnn_cell_dim** (int) --- dimension of RNN cells.\n    * **row_conv** (bool) --- whether to use a ""row"" (""in plane"") convolutional\n      layer after RNNs.\n    * **row_conv_width** (int) --- width parameter for ""row""\n      convolutional layer.\n    * **n_hidden** (int) --- number of hidden units for the last fully connected\n      layer.\n    * **data_format** (string) --- could be either\n      ""channels_first"", ""channels_last"", ""BCTF"", ""BTFC"", ""BCFT"", ""BFTC"".\n       Defaults to ""channels_last"".\n    * **bn_momentum** (float) --- momentum for batch norm. Defaults to 0.99.\n    * **bn_epsilon** (float) --- epsilon for batch norm. Defaults to 1e-3.\n    """"""\n    super(DeepSpeech2Encoder, self).__init__(params, model, name, mode)\n\n  def _encode(self, input_dict):\n    """"""Creates TensorFlow graph for DeepSpeech-2 like encoder.\n\n    Args:\n      input_dict (dict): input dictionary that has to contain\n          the following fields::\n            input_dict = {\n              ""source_tensors"": [\n                src_sequence (shape=[batch_size, sequence length, num features]),\n                src_length (shape=[batch_size])\n              ]\n            }\n\n    Returns:\n      dict: dictionary with the following tensors::\n\n        {\n          \'outputs\': hidden state, shape=[batch_size, sequence length, n_hidden]\n          \'src_length\': tensor, shape=[batch_size]\n        }\n    """"""\n\n    source_sequence, src_length = input_dict[\'source_tensors\']\n\n    training = (self._mode == ""train"")\n    dropout_keep_prob = self.params[\'dropout_keep_prob\'] if training else 1.0\n    regularizer = self.params.get(\'regularizer\', None)\n    data_format = self.params.get(\'data_format\', \'channels_last\')\n    bn_momentum = self.params.get(\'bn_momentum\', 0.99)\n    bn_epsilon = self.params.get(\'bn_epsilon\', 1e-3)\n\n    input_layer = tf.expand_dims(source_sequence, axis=-1) # BTFC\n    # print(""<<< input   :"", input_layer.get_shape().as_list())\n\n    batch_size = input_layer.get_shape().as_list()[0]\n    freq = input_layer.get_shape().as_list()[2]\n\n    # supported data_formats:\n    #    BTFC = channel_last (legacy)\n    #    BCTF = channel_first(legacy)\n    #    BFTC\n    #    BCFT\n\n    if data_format==\'channels_last\' or data_format==\'BTFC\':\n      layout  = \'BTFC\'\n      dformat = \'channels_last\'\n    elif data_format==\'channels_first\' or data_format==\'BCTF\':\n      layout  = \'BCTF\'\n      dformat = \'channels_first\'\n    elif data_format==\'BFTC\':\n      layout  = \'BFTC\'\n      dformat = \'channels_last\'\n    elif data_format==\'BCFT\':\n      layout  = \'BCFT\'\n      dformat = \'channels_first\'\n    else:\n      print(""WARNING: unsupported data format: will use channels_last (BTFC) instead"")\n      layout  = \'BTFC\'\n      dformat = \'channels_last\'\n\n    #input_layer is BTFC\n\n    if   layout == \'BCTF\':\n      top_layer = tf.transpose(input_layer, [0, 3, 1, 2])\n    elif layout == \'BFTC\':\n      top_layer = tf.transpose(input_layer, [0, 2, 1, 3])\n    elif layout == \'BCFT\':\n      top_layer = tf.transpose(input_layer, [0, 3, 2, 1])\n    else:\n      top_layer = input_layer\n\n    # print(""<<< pre-conv:"", top_layer.get_shape().as_list())\n\n    # ----- Convolutional layers ---------------------------------------------\n    conv_layers = self.params[\'conv_layers\']\n\n    for idx_conv in range(len(conv_layers)):\n      ch_out = conv_layers[idx_conv][\'num_channels\']\n      kernel_size = conv_layers[idx_conv][\'kernel_size\']  # [T,F] format\n      strides = conv_layers[idx_conv][\'stride\']           # [T,F] format\n      padding = conv_layers[idx_conv][\'padding\']\n\n      if padding == ""VALID"":\n        src_length = (src_length - kernel_size[0] + strides[0]) // strides[0]\n        freq = (freq - kernel_size[1] + strides[1]) // strides[1]\n      else:\n        src_length = (src_length + strides[0] - 1) // strides[0]\n        freq = (freq + strides[1] -1) // strides[1]\n\n      if layout == \'BFTC\' or layout == \'BCFT\':\n        kernel_size = kernel_size[::-1]\n        strides = strides[::-1]\n        # print(kernel_size, strides)\n\n      top_layer = conv_bn_actv(\n          layer_type=""conv2d"",\n          name=""conv{}"".format(idx_conv + 1),\n          inputs=top_layer,\n          filters=ch_out,\n          kernel_size=kernel_size,\n          activation_fn=self.params[\'activation_fn\'],\n          strides=strides,\n          padding=padding,\n          regularizer=regularizer,\n          training=training,\n          data_format=dformat,\n          bn_momentum=bn_momentum,\n          bn_epsilon=bn_epsilon,\n      )\n      # print(idx_conv, ""++++"", top_layer.get_shape().as_list())\n\n    # convert layout --> BTFC\n    # if data_format == \'channels_first\':\n    #   top_layer = tf.transpose(top_layer, [0, 2, 3, 1])\n\n    if   layout == \'BCTF\': # BCTF --> BTFC\n      top_layer = tf.transpose(top_layer, [0, 2, 3, 1])\n    elif layout == \'BFTC\': # BFTC --> BTFC\n      top_layer = tf.transpose(top_layer, [0, 2, 1, 3])\n    elif layout == \'BCFT\': # BCFT --> BTFC\n      top_layer = tf.transpose(top_layer, [0, 3, 2, 1])\n\n\n    # print("">>> post-conv:"", top_layer.get_shape().as_list())\n\n    # reshape to [B, T, FxC]\n    f = top_layer.get_shape().as_list()[2]\n    c = top_layer.get_shape().as_list()[3]\n    fc = f * c\n    top_layer = tf.reshape(top_layer, [batch_size, -1, fc])\n\n    # ----- RNN ---------------------------------------------------------------\n    num_rnn_layers = self.params[\'num_rnn_layers\']\n    if num_rnn_layers > 0:\n      rnn_cell_dim = self.params[\'rnn_cell_dim\']\n      rnn_type = self.params[\'rnn_type\']\n      if self.params[\'use_cudnn_rnn\']:\n        # reshape to [B, T, C] --> [T, B, C]\n        rnn_input = tf.transpose(top_layer, [1, 0, 2])\n        if self.params[\'rnn_unidirectional\']:\n          direction = cudnn_rnn_ops.CUDNN_RNN_UNIDIRECTION\n        else:\n          direction = cudnn_rnn_ops.CUDNN_RNN_BIDIRECTION\n\n        if rnn_type == ""cudnn_gru"" or rnn_type == ""gru"":\n          # pylint: disable=no-member\n          rnn_block = tf.contrib.cudnn_rnn.CudnnGRU(\n              num_layers=num_rnn_layers,\n              num_units=rnn_cell_dim,\n              direction=direction,\n              dropout=1.0 - dropout_keep_prob,\n              dtype=rnn_input.dtype,\n              name=""cudnn_gru"",\n          )\n        elif rnn_type == ""cudnn_lstm"" or rnn_type == ""lstm"":\n          # pylint: disable=no-member\n          rnn_block = tf.contrib.cudnn_rnn.CudnnLSTM(\n              num_layers=num_rnn_layers,\n              num_units=rnn_cell_dim,\n              direction=direction,\n              dropout=1.0 - dropout_keep_prob,\n              dtype=rnn_input.dtype,\n              name=""cudnn_lstm"",\n          )\n        else:\n          raise ValueError(\n              ""{} is not a valid rnn_type for cudnn_rnn layers"".format(\n                  rnn_type)\n          )\n        top_layer, state = rnn_block(rnn_input)\n        top_layer = tf.transpose(top_layer, [1, 0, 2])\n      else:\n        rnn_input = top_layer\n        multirnn_cell_fw = tf.nn.rnn_cell.MultiRNNCell(\n            [rnn_cell(rnn_cell_dim=rnn_cell_dim, layer_type=rnn_type,\n                      dropout_keep_prob=dropout_keep_prob)\n             for _ in range(num_rnn_layers)]\n        )\n        if self.params[\'rnn_unidirectional\']:\n          top_layer, state = tf.nn.dynamic_rnn(\n              cell=multirnn_cell_fw,\n              inputs=rnn_input,\n              sequence_length=src_length,\n              dtype=rnn_input.dtype,\n              time_major=False,\n          )\n        else:\n          multirnn_cell_bw = tf.nn.rnn_cell.MultiRNNCell(\n              [rnn_cell(rnn_cell_dim=rnn_cell_dim, layer_type=rnn_type,\n                        dropout_keep_prob=dropout_keep_prob)\n               for _ in range(num_rnn_layers)]\n          )\n          top_layer, state = tf.nn.bidirectional_dynamic_rnn(\n              cell_fw=multirnn_cell_fw, cell_bw=multirnn_cell_bw,\n              inputs=rnn_input,\n              sequence_length=src_length,\n              dtype=rnn_input.dtype,\n              time_major=False\n          )\n          # concat 2 tensors [B, T, n_cell_dim] --> [B, T, 2*n_cell_dim]\n          top_layer = tf.concat(top_layer, 2)\n    # -- end of rnn------------------------------------------------------------\n\n    if self.params[\'row_conv\']:\n      channels = top_layer.get_shape().as_list()[-1]\n      top_layer = row_conv(\n          name=""row_conv"",\n          input_layer=top_layer,\n          batch=batch_size,\n          channels=channels,\n          activation_fn=self.params[\'activation_fn\'],\n          width=self.params[\'row_conv_width\'],\n          regularizer=regularizer,\n          training=training,\n          data_format=data_format,\n          bn_momentum=bn_momentum,\n          bn_epsilon=bn_epsilon,\n      )\n\n    # Reshape [B, T, C] --> [B*T, C]\n    c = top_layer.get_shape().as_list()[-1]\n    top_layer = tf.reshape(top_layer, [-1, c])\n\n    # --- hidden layer with clipped ReLU activation and dropout---------------\n    top_layer = tf.layers.dense(\n        inputs=top_layer,\n        units=self.params[\'n_hidden\'],\n        kernel_regularizer=regularizer,\n        activation=self.params[\'activation_fn\'],\n        name=\'fully_connected\',\n    )\n    outputs = tf.nn.dropout(x=top_layer, keep_prob=dropout_keep_prob)\n\n    # reshape from  [B*T,A] --> [B, T, A].\n    # Output shape: [batch_size, n_steps, n_hidden]\n    outputs = tf.reshape(\n        outputs,\n        [batch_size, -1, self.params[\'n_hidden\']],\n    )\n\n    return {\n        \'outputs\': outputs,\n        \'src_length\': src_length,\n    }\n'"
open_seq2seq/encoders/encoder.py,5,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport abc\nimport copy\n\nimport six\nimport tensorflow as tf\n\nfrom open_seq2seq.optimizers.mp_wrapper import mp_regularizer_wrapper\nfrom open_seq2seq.utils.utils import check_params, cast_types\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Encoder:\n  """"""Abstract class from which all encoders must inherit.\n  """"""\n  @staticmethod\n  def get_required_params():\n    """"""Static method with description of required parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **have to** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {}\n\n  @staticmethod\n  def get_optional_params():\n    """"""Static method with description of optional parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **can** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {\n        \'regularizer\': None,  # any valid TensorFlow regularizer\n        \'regularizer_params\': dict,\n        \'initializer\': None,  # any valid TensorFlow initializer\n        \'initializer_params\': dict,\n        \'dtype\': [tf.float32, tf.float16, \'mixed\'],\n    }\n\n  def __init__(self, params, model, name=""encoder"", mode=\'train\'):\n    """"""Encoder constructor.\n    Note that encoder constructors should not modify TensorFlow graph, all\n    graph construction should happen in the :meth:`self._encode() <_encode>`\n    method.\n\n    Args:\n      params (dict): parameters describing the encoder.\n          All supported parameters are listed in :meth:`get_required_params`,\n          :meth:`get_optional_params` functions.\n      model (instance of a class derived from :class:`Model<models.model.Model>`):\n          parent model that created this encoder.\n          Could be None if no model access is required for the use case.\n      name (str): name for encoder variable scope.\n      mode (str): mode encoder is going to be run in.\n          Could be ""train"", ""eval"" or ""infer"".\n\n    Config parameters:\n\n    * **initializer** --- any valid TensorFlow initializer. If no initializer\n      is provided, model initializer will be used.\n    * **initializer_params** (dict) --- dictionary that will be passed to\n      initializer ``__init__`` method.\n    * **regularizer** --- and valid TensorFlow regularizer. If no regularizer\n      is provided, model regularizer will be used.\n    * **regularizer_params** (dict) --- dictionary that will be passed to\n      regularizer ``__init__`` method.\n    * **dtype** --- model dtype. Could be either ``tf.float16``, ``tf.float32``\n      or ""mixed"". For details see\n      :ref:`mixed precision training <mixed_precision>` section in docs. If no\n      dtype is provided, model dtype will be used.\n    """"""\n    check_params(params, self.get_required_params(), self.get_optional_params())\n    self._params = copy.deepcopy(params)\n    self._model = model\n\n    if \'dtype\' not in self._params:\n      if self._model:\n        self._params[\'dtype\'] = self._model.params[\'dtype\']\n      else:\n        self._params[\'dtype\'] = tf.float32\n\n    self._name = name\n    self._mode = mode\n    self._compiled = False\n\n  def encode(self, input_dict):\n    """"""Wrapper around :meth:`self._encode() <_encode>` method.\n    Here name, initializer and dtype are set in the variable scope and then\n    :meth:`self._encode() <_encode>` method is called.\n\n    Args:\n      input_dict (dict): see :meth:`self._encode() <_encode>` docs.\n\n    Returns:\n      see :meth:`self._encode() <_encode>` docs.\n    """"""\n    if not self._compiled:\n      if \'regularizer\' not in self._params:\n        if self._model and \'regularizer\' in self._model.params:\n          self._params[\'regularizer\'] = copy.deepcopy(\n              self._model.params[\'regularizer\']\n          )\n          self._params[\'regularizer_params\'] = copy.deepcopy(\n              self._model.params[\'regularizer_params\']\n          )\n\n      if \'regularizer\' in self._params:\n        init_dict = self._params.get(\'regularizer_params\', {})\n        if self._params[\'regularizer\'] is not None:\n          self._params[\'regularizer\'] = self._params[\'regularizer\'](**init_dict)\n        if self._params[\'dtype\'] == \'mixed\':\n          self._params[\'regularizer\'] = mp_regularizer_wrapper(\n              self._params[\'regularizer\'],\n          )\n\n      if self._params[\'dtype\'] == \'mixed\':\n        self._params[\'dtype\'] = tf.float16\n\n    if \'initializer\' in self.params:\n      init_dict = self.params.get(\'initializer_params\', {})\n      initializer = self.params[\'initializer\'](**init_dict)\n    else:\n      initializer = None\n\n    self._compiled = True\n\n    with tf.variable_scope(self._name, initializer=initializer,\n                           dtype=self.params[\'dtype\']):\n      return self._encode(self._cast_types(input_dict))\n\n  def _cast_types(self, input_dict):\n    """"""This function performs automatic cast of all inputs to encoder dtype.\n\n    Args:\n      input_dict (dict): dictionary passed to :meth:`self._encode() <_encode>`\n          method.\n\n    Returns:\n      dict: same as input_dict, but with all Tensors cast to encoder dtype.\n    """"""\n    return cast_types(input_dict, self.params[\'dtype\'])\n\n  @abc.abstractmethod\n  def _encode(self, input_dict):\n    """"""This is the main function which should construct encoder graph.\n    Typically, encoder will take raw input sequence as an input and\n    produce some hidden representation as an output.\n\n    Args:\n      input_dict (dict): dictionary containing encoder inputs.\n          If the encoder is used with :class:`models.encoder_decoder` class,\n          ``input_dict`` will have the following content::\n            {\n              ""source_tensors"": data_layer.input_tensors[\'source_tensors\']\n            }\n\n    Returns:\n      dict:\n        dictionary of encoder outputs. Return all necessary outputs.\n        Typically this will be just::\n          {\n            ""outputs"": outputs,\n            ""state"": state,\n          }\n    """"""\n    pass\n\n  @property\n  def params(self):\n    """"""Parameters used to construct the encoder (dictionary).""""""\n    return self._params\n\n  @property\n  def mode(self):\n    """"""Mode encoder is run in.""""""\n    return self._mode\n\n  @property\n  def name(self):\n    """"""Encoder name.""""""\n    return self._name\n'"
open_seq2seq/encoders/las_encoder.py,9,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom .encoder import Encoder\nfrom open_seq2seq.parts.cnns.conv_blocks import conv_actv, conv_bn_actv\n\ncells_dict = {\n    ""lstm"": tf.nn.rnn_cell.BasicLSTMCell,\n    ""gru"": tf.nn.rnn_cell.GRUCell,\n}\n\ndef rnn_layer(layer_type, num_layers, name, inputs, src_length, hidden_dim,\n              regularizer, training, dropout_keep_prob=1.0):\n  """"""Helper function that applies convolution and activation.\n    Args:\n      layer_type: the following types are supported\n        \'lstm\', \'gru\'\n  """"""\n  rnn_cell = cells_dict[layer_type]\n  dropout = tf.nn.rnn_cell.DropoutWrapper\n\n  multirnn_cell_fw = tf.nn.rnn_cell.MultiRNNCell(\n      [dropout(rnn_cell(hidden_dim),\n               output_keep_prob=dropout_keep_prob)\n       for _ in range(num_layers)]\n  )\n\n  multirnn_cell_bw = tf.nn.rnn_cell.MultiRNNCell(\n      [dropout(rnn_cell(hidden_dim),\n               output_keep_prob=dropout_keep_prob)\n       for _ in range(num_layers)]\n  )\n\n  output, state = tf.nn.bidirectional_dynamic_rnn(\n      cell_fw=multirnn_cell_fw, cell_bw=multirnn_cell_bw,\n      inputs=inputs,\n      sequence_length=src_length,\n      dtype=inputs.dtype,\n      scope=name,\n  )\n  output = tf.concat(output, 2)\n\n  return output\n\n\nclass ListenAttendSpellEncoder(Encoder):\n  """"""Listen Attend Spell like encoder with support for reduction in time dimension of the input.\n  Can use convolutional layers, recurrent layers or both.\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        \'dropout_keep_prob\': float,\n        \'recurrent_layers\': list,\n        \'convnet_layers\': list,\n        \'activation_fn\': None,  # any valid callable\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'data_format\': [\'channels_first\', \'channels_last\'],\n        \'normalization\': [None, \'batch_norm\'],\n        \'bn_momentum\': float,\n        \'bn_epsilon\': float,\n    })\n\n  def __init__(self, params, model, name=""las_encoder"", mode=\'train\'):\n    """"""Listen Attend Spell like encoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **dropout_keep_prop** (float) --- keep probability for dropout.\n    * **convnet_layers** (list) --- list with the description of convolutional\n      layers. For example::\n        ""convnet_layers"": [\n          {\n            ""type"": ""conv1d"", ""repeat"" : 5,\n            ""kernel_size"": [7], ""stride"": [1],\n            ""num_channels"": 250, ""padding"": ""SAME""\n          },\n          {\n            ""type"": ""conv1d"", ""repeat"" : 1,\n            ""kernel_size"": [1], ""stride"": [2],\n            ""num_channels"": 1000, ""padding"": ""SAME""\n          },\n        ]\n    * **recurrent_layers** (list) --- list with the description of recurrent\n      layers. For example::\n        ""recurrent_layers"": [\n            {\n                ""type"": ""lstm"", ""num_layers"": 1,\n                ""hidden_dim"": 512, ""dropout_keep_prob"": 0.8,\n                ""pool"": True, ""pool_size"":[2], ""stride"": [2],\n            },\n            {\n                ""type"": ""lstm"", ""num_layers"": 3,\n                ""hidden_dim"": 512, ""dropout_keep_prob"": 0.8,\n                ""pool"": False, ""pool_size"":[-1], ""stride"": [-1],\n            },\n        ], \n    * **activation_fn** --- activation function to use.\n    * **data_format** (string) --- could be either ""channels_first"" or\n      ""channels_last"". Defaults to ""channels_last"".\n    * **normalization** --- normalization to use. Accepts [None, \'batch_norm\'].\n      Use None if you don\'t want to use normalization. Defaults to \'batch_norm\'.     \n    * **bn_momentum** (float) --- momentum for batch norm. Defaults to 0.90.\n    * **bn_epsilon** (float) --- epsilon for batch norm. Defaults to 1e-3.\n    """"""\n    super(ListenAttendSpellEncoder, self).__init__(params, model, name, mode)\n\n  def _encode(self, input_dict):\n    """"""Creates TensorFlow graph for Wav2Letter like encoder.\n\n    Args:\n      input_dict (dict): input dictionary that has to contain\n          the following fields::\n            input_dict = {\n              ""source_tensors"": [\n                src_sequence (shape=[batch_size, sequence length, num features]),\n                src_length (shape=[batch_size])\n              ]\n            }\n\n    Returns:\n      dict: dictionary with the following tensors::\n\n        {\n          \'outputs\': hidden state, shape=[batch_size, sequence length, n_hidden]\n          \'src_length\': tensor, shape=[batch_size]\n        }\n    """"""\n\n    source_sequence, src_length = input_dict[\'source_tensors\']\n\n    training = (self._mode == ""train"")\n    dropout_keep_prob = self.params[\'dropout_keep_prob\'] if training else 1.0\n    regularizer = self.params.get(\'regularizer\', None)\n    normalization = self.params.get(\'normalization\', \'batch_norm\')\n    data_format = self.params.get(\'data_format\', \'channels_last\')\n\n    normalization_params = {}\n    if normalization is None:\n      conv_block = conv_actv\n    elif normalization == ""batch_norm"":\n      conv_block = conv_bn_actv\n      normalization_params[\'bn_momentum\'] = self.params.get(\n          \'bn_momentum\', 0.90)\n      normalization_params[\'bn_epsilon\'] = self.params.get(\'bn_epsilon\', 1e-3)\n    else:\n      raise ValueError(""Incorrect normalization"")\n\n    conv_feats = source_sequence\n\n    # ----- Convolutional layers ---------------------------------------------\n    convnet_layers = self.params[\'convnet_layers\']\n\n    for idx_convnet in range(len(convnet_layers)):\n      layer_type = convnet_layers[idx_convnet][\'type\']\n      layer_repeat = convnet_layers[idx_convnet][\'repeat\']\n      ch_out = convnet_layers[idx_convnet][\'num_channels\']\n      kernel_size = convnet_layers[idx_convnet][\'kernel_size\']\n      strides = convnet_layers[idx_convnet][\'stride\']\n      padding = convnet_layers[idx_convnet][\'padding\']\n      dropout_keep = convnet_layers[idx_convnet].get(\n          \'dropout_keep_prob\', dropout_keep_prob) if training else 1.0\n\n      if padding == ""VALID"":\n        src_length = (src_length - kernel_size[0]) // strides[0] + 1\n      else:\n        src_length = (src_length + strides[0] - 1) // strides[0]\n\n      for idx_layer in range(layer_repeat):\n        conv_feats = conv_block(\n            layer_type=layer_type,\n            name=""conv{}{}"".format(\n                idx_convnet + 1, idx_layer + 1),\n            inputs=conv_feats,\n            filters=ch_out,\n            kernel_size=kernel_size,\n            activation_fn=self.params[\'activation_fn\'],\n            strides=strides,\n            padding=padding,\n            regularizer=regularizer,\n            training=training,\n            data_format=data_format,\n            **normalization_params\n        )\n        conv_feats = tf.nn.dropout(x=conv_feats, keep_prob=dropout_keep)\n\n    rnn_feats = conv_feats\n    rnn_block = rnn_layer\n\n    # ----- Recurrent layers ---------------------------------------------\n    recurrent_layers = self.params[\'recurrent_layers\']\n\n    for idx_rnn in range(len(recurrent_layers)):\n      layer_type = recurrent_layers[idx_rnn][\'type\']\n      num_layers = recurrent_layers[idx_rnn][\'num_layers\']\n      hidden_dim = recurrent_layers[idx_rnn][\'hidden_dim\']\n      dropout_keep = recurrent_layers[idx_rnn].get(\n          \'dropout_keep_prob\', dropout_keep_prob) if training else 1.0\n      use_pool = recurrent_layers[idx_rnn][\'pool\']\n      pool_size = recurrent_layers[idx_rnn][\'pool_size\']\n      strides = recurrent_layers[idx_rnn][\'stride\']\n\n      rnn_feats = rnn_block(\n          layer_type=layer_type,\n          num_layers=num_layers,\n          name=""rnn{}"".format(\n              idx_rnn + 1),\n          inputs=rnn_feats,\n          src_length=src_length,\n          hidden_dim=hidden_dim,\n          regularizer=regularizer,\n          training=training,\n          dropout_keep_prob=dropout_keep,\n      )\n\n      if use_pool:\n        rnn_feats = tf.layers.max_pooling1d(\n          inputs=rnn_feats,\n          pool_size=pool_size,\n          strides=strides,\n        )\n        src_length = (src_length - pool_size[0]) // strides[0] + 1\n    outputs = rnn_feats\n\n    return {\n        \'outputs\': outputs,\n        \'src_length\': src_length,\n    }\n'"
open_seq2seq/encoders/lm_encoders.py,30,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nRNN-based encoders\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport copy, inspect\nimport tensorflow as tf\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\nfrom open_seq2seq.optimizers.mp_wrapper import mp_regularizer_wrapper\nfrom open_seq2seq.parts.rnns.utils import single_cell\nfrom .encoder import Encoder\n# from open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\n\n\nclass LMEncoder(Encoder):\n  """"""\n  RNN-based encoder with embeddings for language modeling\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n      \'vocab_size\': int,\n      \'emb_size\': int,\n      \'encoder_layers\': int,\n      \'encoder_use_skip_connections\': bool,\n      \'core_cell\': None,\n      \'core_cell_params\': dict,\n      \'end_token\': int,\n      ""batch_size"": int,\n      ""use_cudnn_rnn"": bool,\n      ""cudnn_rnn_type"": None\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n      \'encoder_dp_input_keep_prob\': float,\n      \'encoder_dp_output_keep_prob\': float,\n      ""encoder_last_input_keep_prob"": float,\n      ""encoder_last_output_keep_prob"": float,\n      \'encoder_emb_keep_prob\': float,\n      \'variational_recurrent\': bool,\n      \'time_major\': bool,\n      \'use_swap_memory\': bool,\n      \'proj_size\': int,\n      \'num_groups\': int,\n      \'num_tokens_gen\': int,\n      \'fc_use_bias\': bool,\n      \'seed_tokens\': list,\n      \'sampling_prob\': float,\n      \'schedule_learning\': bool,\n      \'weight_tied\': bool,\n      \'awd_initializer\': bool,\n      ""recurrent_keep_prob"": float,\n      ""input_weight_keep_prob"": float,\n      ""recurrent_weight_keep_prob"": float,\n      ""weight_variational"": bool,\n      ""dropout_seed"": int,\n      ""num_sampled"": int,\n      ""fc_dim"": int,\n      ""use_cell_state"": bool,\n    })\n\n  def __init__(self, params, model,\n               name=""rnn_encoder_awd"", mode=\'train\'):\n    """"""\n    Initializes bi-directional encoder with embeddings\n    :param params: dictionary with encoder parameters\n\n    Many of the techniques in this implementation is taken from the paper\n    ""Regularizing and Optimizing LSTM Language Models"" (Merity et al., 2017)\n    https://arxiv.org/pdf/1708.02182.pdf\n\n    Must define:\n      * vocab_size - data vocabulary size\n      * emb_size - size of embedding to use\n      * encoder_cell_units - number of units in RNN cell\n      * encoder_cell_type - cell type: lstm, gru, etc.\n      * encoder_layers - number of layers\n      * encoder_use_skip_connections - true/false\n      * time_major (optional)\n      * use_swap_memory (optional)\n      * mode - train or infer\n      * input_weight_keep_prob: keep probability for dropout of W \n                                (kernel used to multiply with the input tensor)\n      * recurrent_weight_keep_prob: keep probability for dropout of U\n                                  (kernel used to multiply with last hidden state tensor)\n      * recurrent_keep_prob: keep probability for dropout\n                            when applying tanh for the input transform step\n      * weight_variational: whether to keep the same weight dropout mask\n                            at every timestep. This feature is not yet implemented.\n      * emb_keep_prob: keep probability for dropout of the embedding matrix\n      * encoder_dp_input_keep_prob: keep probability for dropout on input of a LSTM cell\n                                    in the layer which is not the last layer\n      * encoder_dp_output_keep_prob: keep probability for dropout on output of a LSTM cell\n                                    in the layer which is not the last layer\n      * encoder_last_input_keep_prob: like ``encoder_dp_input_keep_prob`` but for the \n                                      cell in the last layer\n      * encoder_dp_output_keep_prob: like ``encoder_dp_output_keep_prob`` but for the \n                                      cell in the last layer\n      * weight_tied: whether to tie the embedding matrix to the last linear layer.\n                     can only do so if the dimension of the last output layer is\n                     the same as the vocabulary size\n      * use_cell_state: if set to True, concat the last hidden state and \n                        the last cell state to input into the last output layer.\n                        This only works for the text classification task, not the\n                        language modeling phase.\n      For different ways to do dropout for LSTM cells, please read this article:\n      https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b\n\n    :param encoder_params:\n    """"""\n    super(LMEncoder, self).__init__(\n      params, model, name=name, mode=mode,\n    )\n    self._vocab_size = self.params[\'vocab_size\']\n    self._emb_size = self.params[\'emb_size\']\n    self._sampling_prob = self.params.get(\'sampling_prob\', 0.0)\n    self._schedule_learning = self.params.get(\'schedule_learning\', False)\n    self._weight_tied = self.params.get(\'weight_tied\', False)\n    self.params[\'encoder_last_input_keep_prob\'] = self.params.get(\'encoder_last_input_keep_prob\', 1.0)\n    self.params[\'encoder_last_output_keep_prob\'] = self.params.get(\'encoder_last_output_keep_prob\', 1.0)\n    self.params[\'encoder_emb_keep_prob\'] = self.params.get(\'encoder_emb_keep_prob\', 1.0)\n    self.params[\'variational_recurrent\'] = self.params.get(\'variational_recurrent\', False)\n    self.params[\'awd_initializer\'] = self.params.get(\'awd_initializer\', False)\n    self.params[\'recurrent_keep_prob\'] = self.params.get(\'recurrent_keep_prob\', 1.0)\n    self.params[\'input_weight_keep_prob\'] = self.params.get(\'input_weight_keep_prob\', 1.0)\n    self.params[\'recurrent_weight_keep_prob\'] = self.params.get(\'recurrent_weight_keep_prob\', 1.0)\n    self.params[\'weight_variational\'] = self.params.get(\'weight_variational\', False)\n    self.params[\'dropout_seed\'] = self.params.get(\'dropout_seed\', 1822)\n    self._fc_dim = self.params.get(\'fc_dim\', self._vocab_size)\n    self._num_sampled = self.params.get(\'num_sampled\', self._fc_dim) # if num_sampled not defined, take full softmax\n    self._lm_phase = self._fc_dim == self._vocab_size\n    self._num_tokens_gen = self.params.get(\'num_tokens_gen\', 200)\n    self._batch_size = self.params[\'batch_size\']\n    \n    if mode == \'infer\' and self._lm_phase:\n      self._batch_size = len(self.params[\'seed_tokens\'])\n    self._use_cell_state = self.params.get(\'use_cell_state\', False)\n\n  def encode(self, input_dict):\n    """"""Wrapper around :meth:`self._encode() <_encode>` method.\n    Here name, initializer and dtype are set in the variable scope and then\n    :meth:`self._encode() <_encode>` method is called.\n\n    Args:\n      input_dict (dict): see :meth:`self._encode() <_encode>` docs.\n\n    Returns:\n      see :meth:`self._encode() <_encode>` docs.\n    """"""\n\n    if not self._compiled:\n      if \'regularizer\' not in self._params:\n        if self._model and \'regularizer\' in self._model.params:\n          self._params[\'regularizer\'] = copy.deepcopy(\n              self._model.params[\'regularizer\']\n          )\n          self._params[\'regularizer_params\'] = copy.deepcopy(\n              self._model.params[\'regularizer_params\']\n          )\n\n      if \'regularizer\' in self._params:\n        init_dict = self._params.get(\'regularizer_params\', {})\n        self._params[\'regularizer\'] = self._params[\'regularizer\'](**init_dict)\n        if self._params[\'dtype\'] == \'mixed\':\n          self._params[\'regularizer\'] = mp_regularizer_wrapper(\n              self._params[\'regularizer\'],\n          )\n\n      if self._params[\'dtype\'] == \'mixed\':\n        self._params[\'dtype\'] = tf.float16\n\n    self._compiled = True\n\n    with tf.variable_scope(self._name, dtype=self.params[\'dtype\']):\n      return self._encode(self._cast_types(input_dict))\n\n  def _encode(self, input_dict):\n    """"""\n    Encodes data into representation\n    :param input_dict: a Python dictionary.\n    Must define:\n      * src_inputs - a Tensor of shape [batch_size, time] or [time, batch_size]\n                    (depending on time_major param)\n      * src_lengths - a Tensor of shape [batch_size]\n    :return: a Python dictionary with:\n      * encoder_outputs - a Tensor of shape\n                          [batch_size, time, representation_dim]\n      or [time, batch_size, representation_dim]\n      * encoder_state - a Tensor of shape [batch_size, dim]\n      * src_lengths - (copy ref from input) a Tensor of shape [batch_size]\n    """"""\n    time_major = self.params.get(""time_major"", False)\n    use_swap_memory = self.params.get(""use_swap_memory"", False)\n\n    regularizer = self.params.get(\'regularizer\', None)\n    fc_use_bias = self.params.get(\'fc_use_bias\', True)\n\n    use_cudnn_rnn = self.params.get(""use_cudnn_rnn"", False)\n    cudnn_rnn_type = self.params.get(""cudnn_rnn_type"", None)\n\n    if \'initializer\' in self.params:\n      init_dict = self.params.get(\'initializer_params\', {})\n      initializer = self.params[\'initializer\'](**init_dict)\n    else:\n      initializer = None\n\n    if self._mode == ""train"":\n      dp_input_keep_prob = self.params[\'encoder_dp_input_keep_prob\']\n      dp_output_keep_prob = self.params[\'encoder_dp_output_keep_prob\']\n      last_input_keep_prob = self.params[\'encoder_last_input_keep_prob\']\n      last_output_keep_prob = self.params[\'encoder_last_output_keep_prob\']\n      emb_keep_prob = self.params[\'encoder_emb_keep_prob\']\n      recurrent_keep_prob = self.params[\'recurrent_keep_prob\']\n      input_weight_keep_prob = self.params[\'input_weight_keep_prob\']\n      recurrent_weight_keep_prob = self.params[\'recurrent_weight_keep_prob\']\n\n    else:\n      dp_input_keep_prob, dp_output_keep_prob = 1.0, 1.0\n      last_input_keep_prob, last_output_keep_prob = 1.0, 1.0\n      emb_keep_prob, recurrent_keep_prob = 1.0, 1.0\n      input_weight_keep_prob, recurrent_weight_keep_prob = 1.0, 1.0\n\n\n    self._output_layer = tf.layers.Dense(\n      self._fc_dim, \n      kernel_regularizer=regularizer,\n      kernel_initializer=initializer,\n      use_bias=fc_use_bias,\n      dtype=self._params[\'dtype\']\n    )\n\n    if self._weight_tied:\n      last_cell_params = copy.deepcopy(self.params[\'core_cell_params\'])\n      last_cell_params[\'num_units\'] = self._emb_size\n    else:\n      last_cell_params = self.params[\'core_cell_params\']\n    \n    last_output_dim = last_cell_params[\'num_units\']\n\n    if self._use_cell_state:\n      last_output_dim = 2 * last_output_dim\n\n\n    fake_input = tf.zeros(shape=(1, last_output_dim), \n                          dtype=self._params[\'dtype\'])\n    fake_output = self._output_layer.apply(fake_input)\n    with tf.variable_scope(""dense"", reuse=True):\n      dense_weights = tf.get_variable(""kernel"")\n      dense_biases = tf.get_variable(""bias"")\n    \n    if self._weight_tied and self._lm_phase:\n      enc_emb_w = tf.transpose(dense_weights)\n    else:\n      enc_emb_w = tf.get_variable(\n        name=""EncoderEmbeddingMatrix"",\n        shape=[self._vocab_size, self._emb_size],\n        dtype=self._params[\'dtype\']\n      )\n\n    self._enc_emb_w = tf.nn.dropout(enc_emb_w, keep_prob=emb_keep_prob)\n\n    if use_cudnn_rnn:\n      if self._mode == \'train\' or self._mode == \'eval\':\n        all_cudnn_classes = [\n            i[1]\n            for i in inspect.getmembers(tf.contrib.cudnn_rnn, inspect.isclass)\n        ]\n\n        if not cudnn_rnn_type in all_cudnn_classes:\n          raise TypeError(""rnn_type must be a Cudnn RNN class"")\n\n        rnn_block = cudnn_rnn_type(\n            num_layers=self.params[\'encoder_layers\'],\n            num_units=self._emb_size, \n            dtype=self._params[\'dtype\'],\n            name=""cudnn_rnn""\n        )\n      else:\n        # Transferring weights from model trained with CudnnLSTM/CudnnGRU\n        # to CudnnCompatibleLSTMCell/CudnnCompatibleGRUCell for inference\n        if \'CudnnLSTM\' in str(cudnn_rnn_type):\n          cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(num_units=self._emb_size)\n        elif \'CudnnGRU\' in str(cudnn_rnn_type):\n          cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(num_units=self._emb_size)\n\n        fwd_cells = [cell() for _ in range(self.params[\'encoder_layers\'])]\n        self._encoder_cell_fw = tf.nn.rnn_cell.MultiRNNCell(fwd_cells)\n    else:\n      fwd_cells = [\n        single_cell(cell_class=self.params[\'core_cell\'],\n                    cell_params=self.params[\'core_cell_params\'],\n                    dp_input_keep_prob=dp_input_keep_prob,\n                    dp_output_keep_prob=dp_output_keep_prob,\n                    recurrent_keep_prob=recurrent_keep_prob,\n                    input_weight_keep_prob=input_weight_keep_prob,\n                    recurrent_weight_keep_prob=recurrent_weight_keep_prob,\n                    weight_variational=self.params[\'weight_variational\'],\n                    dropout_seed=self.params[\'dropout_seed\'],\n                    residual_connections=self.params[\'encoder_use_skip_connections\'],\n                    awd_initializer=self.params[\'awd_initializer\'],\n                    dtype=self._params[\'dtype\']\n                    ) for _ in range(self.params[\'encoder_layers\'] - 1)]\n\n      fwd_cells.append(\n        single_cell(cell_class=self.params[\'core_cell\'],\n                    cell_params=last_cell_params,\n                    dp_input_keep_prob=last_input_keep_prob,\n                    dp_output_keep_prob=last_output_keep_prob,\n                    recurrent_keep_prob=recurrent_keep_prob,\n                    input_weight_keep_prob=input_weight_keep_prob,\n                    recurrent_weight_keep_prob=recurrent_weight_keep_prob,\n                    weight_variational=self.params[\'weight_variational\'],\n                    dropout_seed=self.params[\'dropout_seed\'],\n                    residual_connections=self.params[\'encoder_use_skip_connections\'],\n                    awd_initializer=self.params[\'awd_initializer\'],\n                    dtype=self._params[\'dtype\']\n                    )\n        )\n\n      self._encoder_cell_fw = tf.contrib.rnn.MultiRNNCell(fwd_cells)\n\n    time_major = self.params.get(""time_major"", False)\n    use_swap_memory = self.params.get(""use_swap_memory"", False)\n\n    source_sequence = input_dict[\'source_tensors\'][0]\n    source_length = input_dict[\'source_tensors\'][1]\n\n    # Inference for language modeling requires a different graph\n    if (not self._lm_phase) or self._mode == \'train\' or self._mode == \'eval\':\n      embedded_inputs = tf.cast(tf.nn.embedding_lookup(\n        self.enc_emb_w,\n        source_sequence,\n      ), self.params[\'dtype\'])\n\n      if use_cudnn_rnn:\n        # The CudnnLSTM will return encoder_state as a tuple of hidden \n        # and cell values that. The hidden and cell tensors are stored for\n        # each LSTM Layer.\n\n        # reshape to [B, T, C] --> [T, B, C]\n        if time_major == False:\n          embedded_inputs = tf.transpose(embedded_inputs, [1, 0, 2])\n\n        rnn_block.build(embedded_inputs.get_shape())\n        encoder_outputs, encoder_state = rnn_block(embedded_inputs)\n        encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n      else:\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n          cell=self._encoder_cell_fw,\n          inputs=embedded_inputs,\n          sequence_length=source_length,\n          time_major=time_major,\n          swap_memory=use_swap_memory,\n          dtype=self._params[\'dtype\'],\n          scope=\'decoder\',\n        )\n\n      if not self._lm_phase:\n        # CudnnLSTM stores cell and hidden state differently\n        if use_cudnn_rnn:\n          if self._use_cell_state:\n            encoder_outputs = tf.concat([encoder_state[0][-1], encoder_state[1][-1]], axis=1)\n          else:\n            encoder_outputs = encoder_state[0][-1]\n        else:\n          if self._use_cell_state:\n            encoder_outputs = tf.concat([encoder_state[-1].h, encoder_state[-1].c], axis=1)\n          else:\n            encoder_outputs = encoder_state[-1].h\n\n      if self._mode == \'train\' and self._num_sampled < self._fc_dim: # sampled softmax\n        output_dict = {\'weights\': enc_emb_w,\n                    \'bias\': dense_biases,\n                    \'inputs\': encoder_outputs,\n                    \'logits\': encoder_outputs,\n                    \'outputs\': [encoder_outputs],\n                    \'num_sampled\': self._num_sampled}\n      else: # full softmax\n        logits = self._output_layer.apply(encoder_outputs)\n        output_dict = {\'logits\': logits, \'outputs\': [logits]}\n    else: # infer in LM phase\n      # This portion of graph is required to restore weights from CudnnLSTM to \n      # CudnnCompatibleLSTMCell/CudnnCompatibleGRUCell\n      if use_cudnn_rnn:\n        embedded_inputs = tf.cast(tf.nn.embedding_lookup(\n          self.enc_emb_w,\n          source_sequence,\n        ), self.params[\'dtype\'])\n\n        # Scope must remain unset to restore weights\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n            cell=self._encoder_cell_fw,\n            inputs=embedded_inputs,\n            sequence_length=source_length,\n            time_major=time_major,\n            swap_memory=use_swap_memory,\n            dtype=self._params[\'dtype\']\n        )\n\n      embedding_fn = lambda ids: tf.cast(tf.nn.embedding_lookup(\n                                          self.enc_emb_w,\n                                          ids,\n                                        ), self.params[\'dtype\'])\n\n      helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n        embedding=embedding_fn,#self._dec_emb_w,\n        start_tokens = tf.constant(self.params[\'seed_tokens\']),\n        end_token=self.params[\'end_token\'])\n      \n      decoder = tf.contrib.seq2seq.BasicDecoder(\n        cell=self._encoder_cell_fw,\n        helper=helper,\n        initial_state=self._encoder_cell_fw.zero_state(\n          batch_size=self._batch_size, dtype=self._params[\'dtype\'],\n        ),\n        output_layer=self._output_layer,\n      )\n      maximum_iterations = tf.constant(self._num_tokens_gen)\n\n      final_outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n        decoder=decoder,\n        impute_finished=False,\n        maximum_iterations=maximum_iterations,\n        swap_memory=use_swap_memory,\n        output_time_major=time_major,\n      )\n      output_dict = {\'logits\': final_outputs.rnn_output,\n            \'outputs\': [tf.argmax(final_outputs.rnn_output, axis=-1)],\n            \'final_state\': final_state,\n            \'final_sequence_lengths\': final_sequence_lengths}\n\n    return output_dict\n\n  @property\n  def vocab_size(self):\n    return self._vocab_size\n\n  @property\n  def emb_size(self):\n    return self._emb_size\n\n  @property\n  def enc_emb_w(self):\n    return self._enc_emb_w'"
open_seq2seq/encoders/resnet_blocks.py,16,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for Residual Networks.\n\nResidual networks (\'v1\' ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant was introduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer\nrather than after.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom six.moves import range\n\nimport tensorflow as tf\n\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format, regularizer, momentum, epsilon):\n  """"""Performs a batch normalization using a standard set of parameters.""""""\n  # We set fused=True for a significant performance boost. See\n  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n  return tf.layers.batch_normalization(\n      inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n      momentum=momentum, epsilon=epsilon, center=True,\n      scale=True, training=training, fused=True, gamma_regularizer=regularizer)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                 Should be a positive integer.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    A tensor with the same format as the input with the data either intact\n    (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  pad_total = kernel_size - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n\n  if data_format == \'channels_first\':\n    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                    [pad_beg, pad_end], [pad_beg, pad_end]])\n  else:\n    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                    [pad_beg, pad_end], [0, 0]])\n  return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides,\n                         data_format, regularizer):\n  """"""Strided 2-D convolution with explicit padding.""""""\n  # The padding is consistent and is based only on `kernel_size`, not on the\n  # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n  if strides > 1:\n    inputs = fixed_padding(inputs, kernel_size, data_format)\n\n  return tf.layers.conv2d(\n      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n      padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n      data_format=data_format, kernel_regularizer=regularizer)\n\n\n################################################################################\n# ResNet block definitions.\n################################################################################\ndef building_block_v1(inputs, filters, training, projection_shortcut, strides,\n                      data_format, regularizer, bn_regularizer,\n                      bn_momentum, bn_epsilon):\n  """"""A single block for ResNet v1, without a bottleneck.\n\n  Convolution then batch normalization then ReLU as described by:\n    Deep Residual Learning for Image Recognition\n    https://arxiv.org/pdf/1512.03385.pdf\n    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference\n      mode. Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts\n      (typically a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n  shortcut = inputs\n\n  if projection_shortcut is not None:\n    shortcut = projection_shortcut(inputs)\n    shortcut = batch_norm(inputs=shortcut, training=training,\n                          data_format=data_format, regularizer=bn_regularizer)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format, regularizer=regularizer)\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs = tf.nn.relu(inputs)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n      data_format=data_format, regularizer=regularizer)\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs += shortcut\n  inputs = tf.nn.relu(inputs)\n\n  return inputs\n\n\ndef building_block_v2(inputs, filters, training, projection_shortcut, strides,\n                      data_format, regularizer, bn_regularizer,\n                      bn_momentum, bn_epsilon):\n  """"""A single block for ResNet v2, without a bottleneck.\n\n  Batch normalization then ReLu then convolution as described by:\n    Identity Mappings in Deep Residual Networks\n    https://arxiv.org/pdf/1603.05027.pdf\n    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference\n      mode. Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts\n      (typically a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n  shortcut = inputs\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs = tf.nn.relu(inputs)\n\n  # The projection shortcut should come after the first batch norm and ReLU\n  # since it performs a 1x1 convolution.\n  if projection_shortcut is not None:\n    shortcut = projection_shortcut(inputs)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format, regularizer=regularizer)\n\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs = tf.nn.relu(inputs)\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n      data_format=data_format, regularizer=regularizer)\n\n  return inputs + shortcut\n\n\ndef bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n                        strides, data_format, regularizer, bn_regularizer,\n                        bn_momentum, bn_epsilon):\n  """"""A single block for ResNet v1, with a bottleneck.\n\n  Similar to _building_block_v1(), except using the ""bottleneck"" blocks\n  described in:\n    Convolution then batch normalization then ReLU as described by:\n      Deep Residual Learning for Image Recognition\n      https://arxiv.org/pdf/1512.03385.pdf\n      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference\n      mode. Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts\n      (typically a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n  shortcut = inputs\n\n  if projection_shortcut is not None:\n    shortcut = projection_shortcut(inputs)\n    shortcut = batch_norm(inputs=shortcut, training=training,\n                          data_format=data_format, regularizer=bn_regularizer,\n                          momentum=bn_momentum, epsilon=bn_epsilon)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n      data_format=data_format, regularizer=regularizer)\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs = tf.nn.relu(inputs)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format, regularizer=regularizer)\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs = tf.nn.relu(inputs)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n      data_format=data_format, regularizer=regularizer)\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs += shortcut\n  inputs = tf.nn.relu(inputs)\n\n  return inputs\n\n\ndef bottleneck_block_v2(inputs, filters, training, projection_shortcut,\n                        strides, data_format, regularizer, bn_regularizer,\n                        bn_momentum, bn_epsilon):\n  """"""A single block for ResNet v2, without a bottleneck.\n\n  Similar to _building_block_v2(), except using the ""bottleneck"" blocks\n  described in:\n    Convolution then batch normalization then ReLU as described by:\n      Deep Residual Learning for Image Recognition\n      https://arxiv.org/pdf/1512.03385.pdf\n      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n  Adapted to the ordering conventions of:\n    Batch normalization then ReLu then convolution as described by:\n      Identity Mappings in Deep Residual Networks\n      https://arxiv.org/pdf/1603.05027.pdf\n      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference\n      mode. Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts\n      (typically a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n  shortcut = inputs\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs = tf.nn.relu(inputs)\n\n  # The projection shortcut should come after the first batch norm and ReLU\n  # since it performs a 1x1 convolution.\n  if projection_shortcut is not None:\n    shortcut = projection_shortcut(inputs)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n      data_format=data_format, regularizer=regularizer)\n\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs = tf.nn.relu(inputs)\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format, regularizer=regularizer)\n\n  inputs = batch_norm(inputs, training, data_format, regularizer=bn_regularizer,\n                      momentum=bn_momentum, epsilon=bn_epsilon)\n  inputs = tf.nn.relu(inputs)\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n      data_format=data_format, regularizer=regularizer)\n\n  return inputs + shortcut\n\n\ndef block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,\n                training, name, data_format, regularizer, bn_regularizer,\n                bn_momentum, bn_epsilon):\n  """"""Creates one layer of blocks for the ResNet model.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the first convolution of the layer.\n    bottleneck: Is the block created a bottleneck block.\n    block_fn: The block to use within the model, either `building_block` or\n      `bottleneck_block`.\n    blocks: The number of blocks contained in the layer.\n    strides: The stride to use for the first convolution of the layer. If\n      greater than 1, this layer will ultimately downsample the input.\n    training: Either True or False, whether we are currently training the\n      model. Needed for batch norm.\n    name: A string name for the tensor output of the block layer.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block layer.\n  """"""\n\n  # Bottleneck blocks end with 4x the number of filters as they start with\n  filters_out = filters * 4 if bottleneck else filters\n\n  def projection_shortcut(inputs):\n    return conv2d_fixed_padding(\n        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n        data_format=data_format, regularizer=regularizer)\n\n  # Only the first block per block_layer uses projection_shortcut and strides\n  inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n                    data_format, regularizer=regularizer,\n                    bn_regularizer=bn_regularizer,\n                    bn_momentum=bn_momentum, bn_epsilon=bn_epsilon)\n\n  for _ in range(1, blocks):\n    inputs = block_fn(inputs, filters, training, None, 1, data_format,\n                      regularizer=regularizer, bn_regularizer=bn_regularizer,\n                      bn_momentum=bn_momentum, bn_epsilon=bn_epsilon)\n\n  return tf.identity(inputs, name)\n'"
open_seq2seq/encoders/resnet_encoder.py,9,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nfrom .resnet_blocks import conv2d_fixed_padding, batch_norm, block_layer, \\\n                           bottleneck_block_v1, bottleneck_block_v2, \\\n                           building_block_v1, building_block_v2\nfrom .encoder import Encoder\n\n\nclass ResNetEncoder(Encoder):\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'resnet_size\': int,\n        \'block_sizes\': list,\n        \'block_strides\': list,\n        \'version\': [1, 2],\n        \'bottleneck\': bool,\n        \'final_size\': int,\n        \'first_num_filters\': int,\n        \'first_kernel_size\': int,\n        \'first_conv_stride\': int,\n        \'first_pool_size\': int,\n        \'first_pool_stride\': int,\n        \'data_format\': [\'channels_first\', \'channels_last\'],\n        \'regularize_bn\': bool,\n        \'bn_momentum\': float,\n        \'bn_epsilon\': float,\n    })\n\n  def __init__(self, params, model, name=""resnet_encoder"", mode=\'train\'):\n    super(ResNetEncoder, self).__init__(params, model, name, mode)\n\n  def _encode(self, input_dict):\n    inputs = input_dict[\'source_tensors\'][0]\n    if \'resnet_size\' not in self.params and \'block_sizes\' not in self.params:\n      raise ValueError(\'Either ""resnet_size"" or ""block_sizes"" \'\n                       \'have to be specified in the config\')\n    if \'resnet_size\' in self.params and \'block_sizes\' in self.params:\n      raise ValueError(\'""resnet_size"" and ""block_sizes"" cannot \'\n                       \'be specified together\')\n    if \'resnet_size\' in self.params:\n      if self.params[\'resnet_size\'] < 50:\n        bottleneck = self.params.get(\'bottleneck\', False)\n        final_size = self.params.get(\'final_size\', 512)\n      else:\n        bottleneck = self.params.get(\'bottleneck\', True)\n        final_size = self.params.get(\'final_size\', 2048)\n      block_sizes_dict = {\n          18: [2, 2, 2, 2],\n          34: [3, 4, 6, 3],\n          50: [3, 4, 6, 3],\n          101: [3, 4, 23, 3],\n          152: [3, 8, 36, 3],\n          200: [3, 24, 36, 3],\n      }\n      block_sizes = block_sizes_dict[self.params[\'resnet_size\']]\n    else:\n      if \'bottleneck\' not in self.params:\n        raise ValueError(\'If ""resnet_size"" not specified you have to provide \'\n                         \'""bottleneck"" parameter\')\n      if \'final_size\' not in self.params:\n        raise ValueError(\'If ""resnet_size"" not specified you have to provide \'\n                         \'""final_size"" parameter\')\n      bottleneck = self.params[\'bottleneck\']\n      final_size = self.params[\'final_size\']\n      block_sizes = self.params[\'block_sizes\']\n\n    num_filters = self.params.get(\'first_num_filters\', 64)\n    kernel_size = self.params.get(\'first_kernel_size\', 7)\n    conv_stride = self.params.get(\'first_conv_stride\', 2)\n    first_pool_size = self.params.get(\'first_pool_size\', 3)\n    first_pool_stride = self.params.get(\'first_pool_stride\', 2)\n\n    block_strides = self.params.get(\'block_strides\', [1, 2, 2, 2])\n    version = self.params.get(\'version\', 2)\n    data_format = self.params.get(\'data_format\', \'channels_first\')\n    bn_momentum = self.params.get(\'bn_momentum\', 0.997)\n    bn_epsilon = self.params.get(\'bn_epsilon\', 1e-5)\n\n    if bottleneck:\n      if version == 1:\n        block_fn = bottleneck_block_v1\n      else:\n        block_fn = bottleneck_block_v2\n    else:\n      if version == 1:\n        block_fn = building_block_v1\n      else:\n        block_fn = building_block_v2\n\n    training = self.mode == \'train\'\n    regularizer = self.params.get(\'regularizer\', None)\n    regularize_bn = self.params.get(\'regularize_bn\', True)\n    bn_regularizer = regularizer if regularize_bn else None\n\n    if data_format == \'channels_first\':\n      inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=num_filters, kernel_size=kernel_size,\n        strides=conv_stride, data_format=data_format, regularizer=regularizer,\n    )\n    inputs = tf.identity(inputs, \'initial_conv\')\n\n    if version == 1:\n      inputs = batch_norm(inputs, training, data_format,\n                          regularizer=bn_regularizer,\n                          momentum=bn_momentum, epsilon=bn_epsilon)\n      inputs = tf.nn.relu(inputs)\n\n    if first_pool_size:\n      inputs = tf.layers.max_pooling2d(\n          inputs=inputs, pool_size=first_pool_size,\n          strides=first_pool_stride, padding=\'SAME\',\n          data_format=data_format,\n      )\n      inputs = tf.identity(inputs, \'initial_max_pool\')\n\n    for i, num_blocks in enumerate(block_sizes):\n      cur_num_filters = num_filters * (2**i)\n      inputs = block_layer(\n          inputs=inputs, filters=cur_num_filters, bottleneck=bottleneck,\n          block_fn=block_fn, blocks=num_blocks,\n          strides=block_strides[i], training=training,\n          name=\'block_layer{}\'.format(i + 1), data_format=data_format,\n          regularizer=regularizer, bn_regularizer=bn_regularizer,\n          bn_momentum=bn_momentum, bn_epsilon=bn_epsilon,\n      )\n    if version == 2:\n      inputs = batch_norm(inputs, training, data_format,\n                          regularizer=bn_regularizer,\n                          momentum=bn_momentum, epsilon=bn_epsilon)\n      inputs = tf.nn.relu(inputs)\n\n    # The current top layer has shape\n    # `batch_size x pool_size x pool_size x final_size`.\n    # ResNet does an Average Pooling layer over pool_size,\n    # but that is the same as doing a reduce_mean. We do a reduce_mean\n    # here because it performs better than AveragePooling2D.\n    axes = [2, 3] if data_format == \'channels_first\' else [1, 2]\n    inputs = tf.reduce_mean(inputs, axes, keepdims=True)\n    inputs = tf.identity(inputs, \'final_reduce_mean\')\n\n    outputs = tf.reshape(inputs, [-1, final_size])\n\n    return {\'outputs\': outputs}\n'"
open_seq2seq/encoders/rnn_encoders.py,41,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nRNN-based encoders\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n\nfrom open_seq2seq.parts.rnns.utils import single_cell\nfrom .encoder import Encoder\n\n\nclass UnidirectionalRNNEncoderWithEmbedding(Encoder):\n  """"""\n  Uni-directional RNN decoder with embeddings.\n  Can support various RNN cell types.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        \'src_vocab_size\': int,\n        \'src_emb_size\': int,\n        \'core_cell\': None,\n        \'core_cell_params\': dict,\n        \'encoder_layers\': int,\n        \'encoder_use_skip_connections\': bool,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'encoder_dp_input_keep_prob\': float,\n        \'encoder_dp_output_keep_prob\': float,\n        \'time_major\': bool,\n        \'use_swap_memory\': bool,\n        \'proj_size\': int,\n        \'num_groups\': int,\n    })\n\n  def __init__(self, params, model,\n               name=""unidir_rnn_encoder_with_emb"", mode=\'train\'):\n    """"""Initializes uni-directional encoder with embeddings.\n\n    Args:\n       params (dict): dictionary with encoder parameters\n          Must define:\n            * src_vocab_size - data vocabulary size\n            * src_emb_size - size of embedding to use\n            * encoder_cell_units - number of units in RNN cell\n            * encoder_cell_type - cell type: lstm, gru, etc.\n            * encoder_layers - number of layers\n            * encoder_dp_input_keep_prob -\n            * encoder_dp_output_keep_prob -\n            * encoder_use_skip_connections - true/false\n            * time_major (optional)\n            * use_swap_memory (optional)\n            * mode - train or infer\n            ... add any cell-specific parameters here as well\n    """"""\n    super(UnidirectionalRNNEncoderWithEmbedding, self).__init__(\n        params,\n        model,\n        name=name,\n        mode=mode,\n    )\n\n    self._src_vocab_size = self.params[\'src_vocab_size\']\n    self._src_emb_size = self.params[\'src_emb_size\']\n\n    self._enc_emb_w = None\n    self._encoder_cell_fw = None\n\n  def _encode(self, input_dict):\n    """"""Encodes data into representation.\n\n    Args:\n      input_dict: a Python dictionary.\n        Must define:\n          * src_inputs - a Tensor of shape [batch_size, time] or\n                         [time, batch_size]\n                         (depending on time_major param)\n          * src_lengths - a Tensor of shape [batch_size]\n\n    Returns:\n       a Python dictionary with:\n      * encoder_outputs - a Tensor of shape\n                          [batch_size, time, representation_dim]\n      or [time, batch_size, representation_dim]\n      * encoder_state - a Tensor of shape [batch_size, dim]\n      * src_lengths - (copy ref from input) a Tensor of shape [batch_size]\n    """"""\n    # TODO: make a separate level of config for cell_params?\n    source_sequence = input_dict[\'source_tensors\'][0]\n    source_length = input_dict[\'source_tensors\'][1]\n\n    self._enc_emb_w = tf.get_variable(\n        name=""EncoderEmbeddingMatrix"",\n        shape=[self._src_vocab_size, self._src_emb_size],\n        dtype=tf.float32,\n    )\n\n    if self._mode == ""train"":\n      dp_input_keep_prob = self.params[\'encoder_dp_input_keep_prob\']\n      dp_output_keep_prob = self.params[\'encoder_dp_output_keep_prob\']\n    else:\n      dp_input_keep_prob = 1.0\n      dp_output_keep_prob = 1.0\n\n    fwd_cells = [\n        single_cell(\n            cell_class=self.params[\'core_cell\'],\n            cell_params=self.params.get(\'core_cell_params\', {}),\n            dp_input_keep_prob=dp_input_keep_prob,\n            dp_output_keep_prob=dp_output_keep_prob,\n            residual_connections=self.params[\'encoder_use_skip_connections\']\n        ) for _ in range(self.params[\'encoder_layers\'])\n    ]\n    # pylint: disable=no-member\n    self._encoder_cell_fw = tf.contrib.rnn.MultiRNNCell(fwd_cells)\n\n    time_major = self.params.get(""time_major"", False)\n    use_swap_memory = self.params.get(""use_swap_memory"", False)\n\n    embedded_inputs = tf.cast(\n        tf.nn.embedding_lookup(\n            self.enc_emb_w,\n            source_sequence,\n        ),\n        self.params[\'dtype\'],\n    )\n\n    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n        cell=self._encoder_cell_fw,\n        inputs=embedded_inputs,\n        sequence_length=source_length,\n        time_major=time_major,\n        swap_memory=use_swap_memory,\n        dtype=embedded_inputs.dtype,\n    )\n    return {\'outputs\': encoder_outputs,\n            \'state\': encoder_state,\n            \'src_lengths\': source_length,\n            \'encoder_input\': source_sequence}\n\n  @property\n  def src_vocab_size(self):\n    return self._src_vocab_size\n\n  @property\n  def src_emb_size(self):\n    return self._src_emb_size\n\n  @property\n  def enc_emb_w(self):\n    return self._enc_emb_w\n\n\nclass BidirectionalRNNEncoderWithEmbedding(Encoder):\n  """"""\n  Bi-directional RNN-based encoder with embeddings.\n  Can support various RNN cell types.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        \'src_vocab_size\': int,\n        \'src_emb_size\': int,\n        \'encoder_layers\': int,\n        \'encoder_use_skip_connections\': bool,\n        \'core_cell\': None,\n        \'core_cell_params\': dict,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'encoder_dp_input_keep_prob\': float,\n        \'encoder_dp_output_keep_prob\': float,\n        \'time_major\': bool,\n        \'use_swap_memory\': bool,\n        \'proj_size\': int,\n        \'num_groups\': int,\n    })\n\n  def __init__(self, params, model,\n               name=""bidir_rnn_encoder_with_emb"", mode=\'train\'):\n    """"""Initializes bi-directional encoder with embeddings.\n\n    Args:\n      params (dict): dictionary with encoder parameters\n        Must define:\n          * src_vocab_size - data vocabulary size\n          * src_emb_size - size of embedding to use\n          * encoder_cell_units - number of units in RNN cell\n          * encoder_cell_type - cell type: lstm, gru, etc.\n          * encoder_layers - number of layers\n          * encoder_dp_input_keep_prob -\n          * encoder_dp_output_keep_prob -\n          * encoder_use_skip_connections - true/false\n          * time_major (optional)\n          * use_swap_memory (optional)\n          * mode - train or infer\n          ... add any cell-specific parameters here as well\n\n    Returns:\n      encoder_params\n    """"""\n    super(BidirectionalRNNEncoderWithEmbedding, self).__init__(\n        params, model, name=name, mode=mode,\n    )\n\n    self._src_vocab_size = self.params[\'src_vocab_size\']\n    self._src_emb_size = self.params[\'src_emb_size\']\n\n    self._enc_emb_w = None\n    self._encoder_cell_fw = None\n    self._encoder_cell_bw = None\n\n  def _encode(self, input_dict):\n    """"""Encodes data into representation.\n    Args:\n      input_dict: a Python dictionary.\n        Must define:\n          *src_inputs - a Tensor of shape [batch_size, time] or\n                        [time, batch_size]\n                        (depending on time_major param)\n          * src_lengths - a Tensor of shape [batch_size]\n\n    Returns:\n      a Python dictionary with:\n      * encoder_outputs - a Tensor of shape\n                          [batch_size, time, representation_dim]\n      or [time, batch_size, representation_dim]\n      * encoder_state - a Tensor of shape [batch_size, dim]\n      * src_lengths - (copy ref from input) a Tensor of shape [batch_size]\n    """"""\n    source_sequence = input_dict[\'source_tensors\'][0]\n    source_length = input_dict[\'source_tensors\'][1]\n    time_major = self.params.get(""time_major"", False)\n    use_swap_memory = self.params.get(""use_swap_memory"", False)\n\n    self._enc_emb_w = tf.get_variable(\n        name=""EncoderEmbeddingMatrix"",\n        shape=[self._src_vocab_size, self._src_emb_size],\n        dtype=tf.float32\n    )\n\n    if self._mode == ""train"":\n      dp_input_keep_prob = self.params[\'encoder_dp_input_keep_prob\']\n      dp_output_keep_prob = self.params[\'encoder_dp_output_keep_prob\']\n    else:\n      dp_input_keep_prob = 1.0\n      dp_output_keep_prob = 1.0\n\n    fwd_cells = [\n        single_cell(\n            cell_class=self.params[\'core_cell\'],\n            cell_params=self.params.get(\'core_cell_params\', {}),\n            dp_input_keep_prob=dp_input_keep_prob,\n            dp_output_keep_prob=dp_output_keep_prob,\n            residual_connections=self.params[\'encoder_use_skip_connections\'],\n        ) for _ in range(self.params[\'encoder_layers\'])\n    ]\n    bwd_cells = [\n        single_cell(\n            cell_class=self.params[\'core_cell\'],\n            cell_params=self.params.get(\'core_cell_params\', {}),\n            dp_input_keep_prob=dp_input_keep_prob,\n            dp_output_keep_prob=dp_output_keep_prob,\n            residual_connections=self.params[\'encoder_use_skip_connections\'],\n        ) for _ in range(self.params[\'encoder_layers\'])\n    ]\n\n    with tf.variable_scope(""FW""):\n      # pylint: disable=no-member\n      self._encoder_cell_fw = tf.contrib.rnn.MultiRNNCell(fwd_cells)\n\n    with tf.variable_scope(""BW""):\n      # pylint: disable=no-member\n      self._encoder_cell_bw = tf.contrib.rnn.MultiRNNCell(bwd_cells)\n\n    embedded_inputs = tf.cast(\n        tf.nn.embedding_lookup(\n            self.enc_emb_w,\n            source_sequence,\n        ),\n        self.params[\'dtype\']\n    )\n\n    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n        cell_fw=self._encoder_cell_fw,\n        cell_bw=self._encoder_cell_bw,\n        inputs=embedded_inputs,\n        sequence_length=source_length,\n        time_major=time_major,\n        swap_memory=use_swap_memory,\n        dtype=embedded_inputs.dtype,\n    )\n    encoder_outputs = tf.concat(encoder_output, 2)\n    return {\'outputs\': encoder_outputs,\n            \'state\': encoder_state,\n            \'src_lengths\': source_length,\n            \'encoder_input\': source_sequence}\n\n  @property\n  def src_vocab_size(self):\n    return self._src_vocab_size\n\n  @property\n  def src_emb_size(self):\n    return self._src_emb_size\n\n  @property\n  def enc_emb_w(self):\n    return self._enc_emb_w\n\n\nclass GNMTLikeEncoderWithEmbedding(Encoder):\n  """"""\n  Encoder similar to the one used in\n  GNMT model: https://arxiv.org/abs/1609.08144.\n  Must have at least 2 layers\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        \'src_vocab_size\': int,\n        \'src_emb_size\': int,\n        \'core_cell\': None,\n        \'core_cell_params\': dict,\n        \'encoder_layers\': int,\n        \'encoder_use_skip_connections\': bool,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'encoder_dp_input_keep_prob\': float,\n        \'encoder_dp_output_keep_prob\': float,\n        \'time_major\': bool,\n        \'use_swap_memory\': bool,\n        \'proj_size\': int,\n        \'num_groups\': int,\n    })\n\n  def __init__(self, params, model,\n               name=""gnmt_encoder_with_emb"", mode=\'train\'):\n    """"""Encodes data into representation.\n\n    Args:\n      params (dict): a Python dictionary.\n        Must define:\n          * src_inputs - a Tensor of shape [batch_size, time] or\n                         [time, batch_size]\n                         (depending on time_major param)\n          * src_lengths - a Tensor of shape [batch_size]\n\n    Returns:\n      a Python dictionary with:\n      * encoder_outputs - a Tensor of shape\n                          [batch_size, time, representation_dim]\n      or [time, batch_size, representation_dim]\n      * encoder_state - a Tensor of shape [batch_size, dim]\n      * src_lengths - (copy ref from input) a Tensor of shape [batch_size]\n    """"""\n    super(GNMTLikeEncoderWithEmbedding, self).__init__(\n        params, model, name=name, mode=mode,\n    )\n\n    self._src_vocab_size = self.params[\'src_vocab_size\']\n    self._src_emb_size = self.params[\'src_emb_size\']\n\n    self._encoder_l1_cell_fw = None\n    self._encoder_l1_cell_bw = None\n    self._encoder_cells = None\n    self._enc_emb_w = None\n\n  def _encode(self, input_dict):\n    source_sequence = input_dict[\'source_tensors\'][0]\n    source_length = input_dict[\'source_tensors\'][1]\n    self._enc_emb_w = tf.get_variable(\n        name=""EncoderEmbeddingMatrix"",\n        shape=[self._src_vocab_size, self._src_emb_size],\n        dtype=tf.float32,\n    )\n\n    if self.params[\'encoder_layers\'] < 2:\n      raise ValueError(""GNMT encoder must have at least 2 layers"")\n\n    with tf.variable_scope(""Level1FW""):\n      self._encoder_l1_cell_fw = single_cell(\n          cell_class=self.params[\'core_cell\'],\n          cell_params=self.params.get(\'core_cell_params\', {}),\n          dp_input_keep_prob=1.0,\n          dp_output_keep_prob=1.0,\n          residual_connections=False,\n      )\n\n    with tf.variable_scope(""Level1BW""):\n      self._encoder_l1_cell_bw = single_cell(\n          cell_class=self.params[\'core_cell\'],\n          cell_params=self.params.get(\'core_cell_params\', {}),\n          dp_input_keep_prob=1.0,\n          dp_output_keep_prob=1.0,\n          residual_connections=False,\n      )\n\n    if self._mode == ""train"":\n      dp_input_keep_prob = self.params[\'encoder_dp_input_keep_prob\']\n      dp_output_keep_prob = self.params[\'encoder_dp_output_keep_prob\']\n    else:\n      dp_input_keep_prob = 1.0\n      dp_output_keep_prob = 1.0\n\n    with tf.variable_scope(""UniDirLevel""):\n      self._encoder_cells = [\n          single_cell(\n              cell_class=self.params[\'core_cell\'],\n              cell_params=self.params.get(\'core_cell_params\', {}),\n              dp_input_keep_prob=dp_input_keep_prob,\n              dp_output_keep_prob=dp_output_keep_prob,\n              residual_connections=False,\n          ) for _ in range(self.params[\'encoder_layers\'] - 1)\n      ]\n\n      # add residual connections starting from the third layer\n      for idx, cell in enumerate(self._encoder_cells):\n        if idx > 0:\n          # pylint: disable=no-member\n          self._encoder_cells[idx] = tf.contrib.rnn.ResidualWrapper(cell)\n\n    time_major = self.params.get(""time_major"", False)\n    use_swap_memory = self.params.get(""use_swap_memory"", False)\n    embedded_inputs = tf.cast(\n        tf.nn.embedding_lookup(\n            self.enc_emb_w,\n            source_sequence,\n        ),\n        self.params[\'dtype\'],\n    )\n\n    # first bi-directional layer\n    _encoder_output, _ = tf.nn.bidirectional_dynamic_rnn(\n        cell_fw=self._encoder_l1_cell_fw,\n        cell_bw=self._encoder_l1_cell_bw,\n        inputs=embedded_inputs,\n        sequence_length=source_length,\n        swap_memory=use_swap_memory,\n        time_major=time_major,\n        dtype=embedded_inputs.dtype,\n    )\n    encoder_l1_outputs = tf.concat(_encoder_output, 2)\n\n    # stack of unidirectional layers\n    # pylint: disable=no-member\n    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n        cell=tf.contrib.rnn.MultiRNNCell(self._encoder_cells),\n        inputs=encoder_l1_outputs,\n        sequence_length=source_length,\n        swap_memory=use_swap_memory,\n        time_major=time_major,\n        dtype=encoder_l1_outputs.dtype,\n    )\n\n    return {\'outputs\': encoder_outputs,\n            \'state\': encoder_state,\n            \'src_lengths\': source_length,\n            \'encoder_input\': source_sequence}\n\n  @property\n  def src_vocab_size(self):\n    return self._src_vocab_size\n\n  @property\n  def src_emb_size(self):\n    return self._src_emb_size\n\n  @property\n  def enc_emb_w(self):\n    return self._enc_emb_w\n\n\nclass GNMTLikeEncoderWithEmbedding_cuDNN(Encoder):\n  """"""\n    Encoder similar to the one used in\n    GNMT model: https://arxiv.org/abs/1609.08144.\n    Must have at least 2 layers. Uses cuDNN RNN blocks for efficiency\n    """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        \'src_vocab_size\': int,\n        \'src_emb_size\': int,\n        \'encoder_cell_units\': int,\n        \'encoder_cell_type\': [\'lstm\', \'gru\'],\n        \'encoder_layers\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'encoder_dp_output_keep_prob\': float,\n    })\n\n  def __init__(self, params, model,\n               name=""gnmt_encoder_with_emb_cudnn"", mode=\'train\'):\n    """"""Encodes data into representation\n\n    Args:\n      params (dict): a Python dictionary.\n        Must define:\n          * src_inputs - a Tensor of shape [batch_size, time] or\n                         [time, batch_size]\n                         (depending on time_major param)\n          * src_lengths - a Tensor of shape [batch_size]\n\n    Returns:\n      a Python dictionary with:\n      * encoder_outputs - a Tensor of shape\n                          [batch_size, time, representation_dim]\n      or [time, batch_size, representation_dim]\n      * encoder_state - a Tensor of shape [batch_size, dim]\n      * src_lengths - (copy ref from input) a Tensor of shape [batch_size]\n    """"""\n    super(GNMTLikeEncoderWithEmbedding_cuDNN, self).__init__(\n        params, model, name=name, mode=mode,\n    )\n\n    self._src_vocab_size = self.params[\'src_vocab_size\']\n    self._src_emb_size = self.params[\'src_emb_size\']\n\n    self._enc_emb_w = None\n\n  def _encode(self, input_dict):\n    source_sequence = input_dict[\'source_tensors\'][0]\n    source_length = input_dict[\'source_tensors\'][1]\n    self._enc_emb_w = tf.get_variable(\n        name=""EncoderEmbeddingMatrix"",\n        shape=[self._src_vocab_size, self._src_emb_size],\n        dtype=tf.float32\n    )\n\n    if self.params[\'encoder_layers\'] < 2:\n      raise ValueError(""GNMT encoder must have at least 2 layers"")\n\n    if self._mode == ""train"":\n      dp_output_keep_prob = self.params[\'encoder_dp_output_keep_prob\']\n    else:\n      dp_output_keep_prob = 1.0\n\n    # source_sequence is of [batch, time] shape\n    embedded_inputs = tf.cast(\n        tf.nn.embedding_lookup(\n            self.enc_emb_w,\n            tf.transpose(source_sequence), # cudnn wants [time, batch, ...]\n        ),\n        self.params[\'dtype\'],\n    )\n\n    with tf.variable_scope(""Bi_Directional_Layer""):\n      direction = cudnn_rnn_ops.CUDNN_RNN_BIDIRECTION\n      if self.params[\'encoder_cell_type\'] == ""gru"":\n        # pylint: disable=no-member\n        bidirectional_block = tf.contrib.cudnn_rnn.CudnnGRU(\n            num_layers=1,\n            num_units=self.params[\'encoder_cell_units\'],\n            direction=direction,\n            dropout=0.0,\n            dtype=self.params[\'dtype\'],\n            name=""cudnn_gru_bidi"",\n        )\n      elif self.params[\'encoder_cell_type\'] == ""lstm"":\n        # pylint: disable=no-member\n        bidirectional_block = tf.contrib.cudnn_rnn.CudnnLSTM(\n            num_layers=1,\n            num_units=self.params[\'encoder_cell_units\'],\n            direction=direction,\n            dropout=0.0,\n            dtype=self.params[\'dtype\'],\n            name=""cudnn_lstm_bidi"",\n        )\n      else:\n        raise ValueError(\n            ""{} is not a valid rnn_type for cudnn_rnn layers"".format(\n                self.params[\'encoder_cell_units\']\n            )\n        )\n      bidi_output, bidi_state = bidirectional_block(embedded_inputs)\n\n    with tf.variable_scope(""Uni_Directional_Layer""):\n      direction = cudnn_rnn_ops.CUDNN_RNN_UNIDIRECTION\n      layer_input = bidi_output\n      for ind in range(self.params[\'encoder_layers\'] - 1):\n        with tf.variable_scope(""uni_layer_{}"".format(ind)):\n          if self.params[\'encoder_cell_type\'] == ""gru"":\n            # pylint: disable=no-member\n            unidirectional_block = tf.contrib.cudnn_rnn.CudnnGRU(\n                num_layers=1,\n                num_units=self.params[\'encoder_cell_units\'],\n                direction=direction,\n                dropout=1.0 - dp_output_keep_prob,\n                dtype=self.params[\'dtype\'],\n                name=""cudnn_gru_uni_{}"".format(ind),\n            )\n          elif self.params[\'encoder_cell_type\'] == ""lstm"":\n            # pylint: disable=no-member\n            unidirectional_block = tf.contrib.cudnn_rnn.CudnnLSTM(\n                num_layers=1,\n                num_units=self.params[\'encoder_cell_units\'],\n                direction=direction,\n                dropout=1.0 - dp_output_keep_prob,\n                dtype=self.params[\'dtype\'],\n                name=""cudnn_lstm_uni_{}"".format(ind),\n            )\n          layer_output, encoder_state = unidirectional_block(layer_input)\n          if ind > 0:  # add residual connection\n            layer_output = layer_input + layer_output\n          layer_input = layer_output\n\n    return {\'outputs\': tf.transpose(layer_input, perm=[1, 0, 2]),\n            \'state\': None,\n            \'src_lengths\': source_length,\n            \'encoder_input\': source_sequence}\n\n  @property\n  def src_vocab_size(self):\n    return self._src_vocab_size\n\n  @property\n  def src_emb_size(self):\n    return self._src_emb_size\n\n  @property\n  def enc_emb_w(self):\n    return self._enc_emb_w\n'"
open_seq2seq/encoders/tacotron2_encoder.py,46,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nimport inspect\n\nimport tensorflow as tf\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\nfrom tensorflow.python.framework import ops\nfrom open_seq2seq.parts.cnns.conv_blocks import conv_bn_actv\nfrom open_seq2seq.parts.rnns.utils import single_cell\nfrom open_seq2seq.parts.transformer import attention_layer\n\nfrom .encoder import Encoder\n\n\nclass Tacotron2Encoder(Encoder):\n  """"""Tacotron-2 like encoder.\n\n  Consists of an embedding layer followed by a convolutional layer followed by\n  a recurrent layer.\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(\n        Encoder.get_required_params(),\n        **{\n            \'cnn_dropout_prob\': float,\n            \'rnn_dropout_prob\': float,\n            \'src_emb_size\': int,\n            \'conv_layers\': list,\n            \'activation_fn\': None,  # any valid callable\n            \'num_rnn_layers\': int,\n            \'rnn_cell_dim\': int,\n            \'use_cudnn_rnn\': bool,\n            \'rnn_type\': None,\n            \'rnn_unidirectional\': bool,\n        }\n    )\n\n  @staticmethod\n  def get_optional_params():\n    return dict(\n        Encoder.get_optional_params(), **{\n            \'data_format\': [\'channels_first\', \'channels_last\'],\n            \'bn_momentum\': float,\n            \'bn_epsilon\': float,\n            \'zoneout_prob\': float,\n            \'style_embedding_enable\': bool,\n            \'style_embedding_params\': dict,\n        }\n    )\n\n  def __init__(self, params, model, name=""tacotron2_encoder"", mode=\'train\'):\n    """"""Tacotron-2 like encoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **cnn_dropout_prob** (float) --- dropout probabilty for cnn layers.\n    * **rnn_dropout_prob** (float) --- dropout probabilty for cnn layers.\n    * **src_emb_size** (int) --- dimensionality of character embedding.\n    * **conv_layers** (list) --- list with the description of convolutional\n      layers. For example::\n        ""conv_layers"": [\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME""\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME""\n          },\n          {\n            ""kernel_size"": [5], ""stride"": [1],\n            ""num_channels"": 512, ""padding"": ""SAME""\n          }\n        ]\n    * **activation_fn** (callable) --- activation function to use for conv\n      layers.\n    * **num_rnn_layers** --- number of RNN layers to use.\n    * **rnn_cell_dim** (int) --- dimension of RNN cells.\n    * **rnn_type** (callable) --- Any valid RNN Cell class. Suggested class is\n      lstm\n    * **rnn_unidirectional** (bool) --- whether to use uni-directional or\n      bi-directional RNNs.\n    * **zoneout_prob** (float) --- zoneout probability. Defaults to 0.\n    * **use_cudnn_rnn** (bool) --- need to be enabled in rnn_type is a Cudnn\n      class.\n    * **data_format** (string) --- could be either ""channels_first"" or\n      ""channels_last"". Defaults to ""channels_last"".\n    * **bn_momentum** (float) --- momentum for batch norm. Defaults to 0.1.\n    * **bn_epsilon** (float) --- epsilon for batch norm. Defaults to 1e-5.\n    * **style_embedding_enable** (bool) --- Whether to enable GST. Defaults to\n      False.\n    * **style_embedding_params** (dict) --- Parameters for GST layer. See\n      _embed_style documentation.\n    """"""\n    super(Tacotron2Encoder, self).__init__(params, model, name, mode)\n\n  def _encode(self, input_dict):\n    """"""Creates TensorFlow graph for Tacotron-2 like encoder.\n\n    Args:\n       input_dict (dict): dictionary with inputs.\n        Must define:\n\n            source_tensors - array containing [\n\n              * source_sequence: tensor of shape [batch_size, sequence length]\n              * src_length: tensor of shape [batch_size]\n\n            ]\n\n    Returns:\n      dict: A python dictionary containing:\n\n          * outputs - tensor containing the encoded text to be passed to the\n            attention layer\n          * src_length - the length of the encoded text\n    """"""\n\n    text = input_dict[\'source_tensors\'][0]\n    text_len = input_dict[\'source_tensors\'][1]\n\n    training = (self._mode == ""train"")\n    regularizer = self.params.get(\'regularizer\', None)\n    data_format = self.params.get(\'data_format\', \'channels_last\')\n    src_vocab_size = self._model.get_data_layer().params[\'src_vocab_size\']\n    zoneout_prob = self.params.get(\'zoneout_prob\', 0.)\n\n    # if src_vocab_size % 8 != 0:\n    #   src_vocab_size += 8 - (src_vocab_size % 8)\n\n    # ----- Embedding layer -----------------------------------------------\n    enc_emb_w = tf.get_variable(\n        name=""EncoderEmbeddingMatrix"",\n        shape=[src_vocab_size, self.params[\'src_emb_size\']],\n        dtype=self.params[\'dtype\'],\n        # initializer=tf.random_normal_initializer()\n    )\n\n    embedded_inputs = tf.cast(\n        tf.nn.embedding_lookup(\n            enc_emb_w,\n            text,\n        ), self.params[\'dtype\']\n    )\n\n    # ----- Style layer ---------------------------------------------------\n    if self.params.get(""style_embedding_enable"", False):\n      if ""style_embedding_params"" not in self.params:\n        raise ValueError(\n            ""style_embedding_params must be passed if style embedding is"",\n            ""enabled""\n        )\n      with tf.variable_scope(""style_encoder""):\n        if (self._model.get_data_layer().params.get(""style_input"", None)\n            == ""wav""):\n          style_spec = input_dict[\'source_tensors\'][2]\n          style_len = input_dict[\'source_tensors\'][3]\n          style_embedding = self._embed_style(style_spec, style_len)\n        else:\n          raise ValueError(""The data layer\'s style input parameter must be set."")\n        style_embedding = tf.expand_dims(style_embedding, 1)\n        style_embedding = tf.tile(\n            style_embedding,\n            [1, tf.reduce_max(text_len), 1]\n        )\n\n    # ----- Convolutional layers -----------------------------------------------\n    input_layer = embedded_inputs\n\n    if data_format == \'channels_last\':\n      top_layer = input_layer\n    else:\n      top_layer = tf.transpose(input_layer, [0, 2, 1])\n\n    for i, conv_params in enumerate(self.params[\'conv_layers\']):\n      ch_out = conv_params[\'num_channels\']\n      kernel_size = conv_params[\'kernel_size\']  # [time, freq]\n      strides = conv_params[\'stride\']\n      padding = conv_params[\'padding\']\n\n      if padding == ""VALID"":\n        text_len = (text_len - kernel_size[0] + strides[0]) // strides[0]\n      else:\n        text_len = (text_len + strides[0] - 1) // strides[0]\n\n      top_layer = conv_bn_actv(\n          layer_type=""conv1d"",\n          name=""conv{}"".format(i + 1),\n          inputs=top_layer,\n          filters=ch_out,\n          kernel_size=kernel_size,\n          activation_fn=self.params[\'activation_fn\'],\n          strides=strides,\n          padding=padding,\n          regularizer=regularizer,\n          training=training,\n          data_format=data_format,\n          bn_momentum=self.params.get(\'bn_momentum\', 0.1),\n          bn_epsilon=self.params.get(\'bn_epsilon\', 1e-5),\n      )\n      top_layer = tf.layers.dropout(\n          top_layer, rate=self.params[""cnn_dropout_prob""], training=training\n      )\n\n    if data_format == \'channels_first\':\n      top_layer = tf.transpose(top_layer, [0, 2, 1])\n\n    # ----- RNN ---------------------------------------------------------------\n    num_rnn_layers = self.params[\'num_rnn_layers\']\n    if num_rnn_layers > 0:\n      cell_params = {}\n      cell_params[""num_units""] = self.params[\'rnn_cell_dim\']\n      rnn_type = self.params[\'rnn_type\']\n      rnn_input = top_layer\n      rnn_vars = []\n\n      if self.params[""use_cudnn_rnn""]:\n        if self._mode == ""infer"":\n          cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(\n              cell_params[""num_units""]\n          )\n          cells_fw = [cell() for _ in range(1)]\n          cells_bw = [cell() for _ in range(1)]\n          (top_layer, _, _) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n              cells_fw, cells_bw, rnn_input,\n              sequence_length=text_len,\n              dtype=rnn_input.dtype,\n              time_major=False)\n        else:\n          all_cudnn_classes = [\n              i[1]\n              for i in inspect.getmembers(tf.contrib.cudnn_rnn, inspect.isclass)\n          ]\n          if not rnn_type in all_cudnn_classes:\n            raise TypeError(""rnn_type must be a Cudnn RNN class"")\n          if zoneout_prob != 0.:\n            raise ValueError(\n                ""Zoneout is currently not supported for cudnn rnn classes""\n            )\n\n          rnn_input = tf.transpose(top_layer, [1, 0, 2])\n          if self.params[\'rnn_unidirectional\']:\n            direction = cudnn_rnn_ops.CUDNN_RNN_UNIDIRECTION\n          else:\n            direction = cudnn_rnn_ops.CUDNN_RNN_BIDIRECTION\n\n          rnn_block = rnn_type(\n              num_layers=num_rnn_layers,\n              num_units=cell_params[""num_units""],\n              direction=direction,\n              dtype=rnn_input.dtype,\n              name=""cudnn_rnn""\n          )\n          rnn_block.build(rnn_input.get_shape())\n          top_layer, _ = rnn_block(rnn_input)\n          top_layer = tf.transpose(top_layer, [1, 0, 2])\n          rnn_vars += rnn_block.trainable_variables\n\n      else:\n        multirnn_cell_fw = tf.nn.rnn_cell.MultiRNNCell(\n            [\n                single_cell(\n                    cell_class=rnn_type,\n                    cell_params=cell_params,\n                    zoneout_prob=zoneout_prob,\n                    training=training,\n                    residual_connections=False\n                ) for _ in range(num_rnn_layers)\n            ]\n        )\n        rnn_vars += multirnn_cell_fw.trainable_variables\n        if self.params[\'rnn_unidirectional\']:\n          top_layer, _ = tf.nn.dynamic_rnn(\n              cell=multirnn_cell_fw,\n              inputs=rnn_input,\n              sequence_length=text_len,\n              dtype=rnn_input.dtype,\n              time_major=False,\n          )\n        else:\n          multirnn_cell_bw = tf.nn.rnn_cell.MultiRNNCell(\n              [\n                  single_cell(\n                      cell_class=rnn_type,\n                      cell_params=cell_params,\n                      zoneout_prob=zoneout_prob,\n                      training=training,\n                      residual_connections=False\n                  ) for _ in range(num_rnn_layers)\n              ]\n          )\n          top_layer, _ = tf.nn.bidirectional_dynamic_rnn(\n              cell_fw=multirnn_cell_fw,\n              cell_bw=multirnn_cell_bw,\n              inputs=rnn_input,\n              sequence_length=text_len,\n              dtype=rnn_input.dtype,\n              time_major=False\n          )\n          # concat 2 tensors [B, T, n_cell_dim] --> [B, T, 2*n_cell_dim]\n          top_layer = tf.concat(top_layer, 2)\n          rnn_vars += multirnn_cell_bw.trainable_variables\n\n      if regularizer and training:\n        cell_weights = []\n        cell_weights += rnn_vars\n        cell_weights += [enc_emb_w]\n        for weights in cell_weights:\n          if ""bias"" not in weights.name:\n            # print(""Added regularizer to {}"".format(weights.name))\n            if weights.dtype.base_dtype == tf.float16:\n              tf.add_to_collection(\n                  \'REGULARIZATION_FUNCTIONS\', (weights, regularizer)\n              )\n            else:\n              tf.add_to_collection(\n                  ops.GraphKeys.REGULARIZATION_LOSSES, regularizer(weights)\n              )\n\n    # -- end of rnn------------------------------------------------------------\n\n    top_layer = tf.layers.dropout(\n        top_layer, rate=self.params[""rnn_dropout_prob""], training=training\n    )\n    outputs = top_layer\n    if self.params.get(""style_embedding_enable"", False):\n      outputs = tf.concat([outputs, style_embedding], axis=-1)\n\n    return {\n        \'outputs\': outputs,\n        \'src_length\': text_len\n    }\n\n  def _embed_style(self, style_spec, style_len):\n    """"""\n    Code that implements the reference encoder as described in ""Towards\n    end-to-end prosody transfer for expressive speech synthesis with Tacotron"",\n    and ""Style Tokens: Unsupervised Style Modeling, Control and Transfer in\n    End-to-End Speech Synthesis""\n\n    Config parameters:\n\n    * **conv_layers** (list) --- See the conv_layers parameter for the\n      Tacotron-2 model.\n    * **num_rnn_layers** (int) --- Number of rnn layers in the reference encoder\n    * **rnn_cell_dim** (int) --- Size of rnn layer\n    * **rnn_unidirectional** (bool) --- Uni- or bi-directional rnn.\n    * **rnn_type** --- Must be a valid tf rnn cell class\n    * **emb_size** (int) --- Size of gst\n    * **attention_layer_size** (int) --- Size of linear layers in attention\n    * **num_tokens** (int) --- Number of tokens for gst\n    * **num_heads** (int) --- Number of attention heads\n    """"""\n    training = (self._mode == ""train"")\n    regularizer = self.params.get(\'regularizer\', None)\n    data_format = self.params.get(\'data_format\', \'channels_last\')\n    batch_size = style_spec.get_shape().as_list()[0]\n\n    top_layer = tf.expand_dims(style_spec, -1)\n    params = self.params[\'style_embedding_params\']\n    if ""conv_layers"" in params:\n      for i, conv_params in enumerate(params[\'conv_layers\']):\n        ch_out = conv_params[\'num_channels\']\n        kernel_size = conv_params[\'kernel_size\']  # [time, freq]\n        strides = conv_params[\'stride\']\n        padding = conv_params[\'padding\']\n\n        if padding == ""VALID"":\n          style_len = (style_len - kernel_size[0] + strides[0]) // strides[0]\n        else:\n          style_len = (style_len + strides[0] - 1) // strides[0]\n\n        top_layer = conv_bn_actv(\n            layer_type=""conv2d"",\n            name=""conv{}"".format(i + 1),\n            inputs=top_layer,\n            filters=ch_out,\n            kernel_size=kernel_size,\n            activation_fn=self.params[\'activation_fn\'],\n            strides=strides,\n            padding=padding,\n            regularizer=regularizer,\n            training=training,\n            data_format=data_format,\n            bn_momentum=self.params.get(\'bn_momentum\', 0.1),\n            bn_epsilon=self.params.get(\'bn_epsilon\', 1e-5),\n        )\n\n      if data_format == \'channels_first\':\n        top_layer = tf.transpose(top_layer, [0, 2, 1])\n\n    top_layer = tf.concat(tf.unstack(top_layer, axis=2), axis=-1)\n\n    num_rnn_layers = params[\'num_rnn_layers\']\n    if num_rnn_layers > 0:\n      cell_params = {}\n      cell_params[""num_units""] = params[\'rnn_cell_dim\']\n      rnn_type = params[\'rnn_type\']\n      rnn_input = top_layer\n      rnn_vars = []\n\n      multirnn_cell_fw = tf.nn.rnn_cell.MultiRNNCell(\n          [\n              single_cell(\n                  cell_class=rnn_type,\n                  cell_params=cell_params,\n                  training=training,\n                  residual_connections=False\n              ) for _ in range(num_rnn_layers)\n          ]\n      )\n      rnn_vars += multirnn_cell_fw.trainable_variables\n      if params[\'rnn_unidirectional\']:\n        top_layer, final_state = tf.nn.dynamic_rnn(\n            cell=multirnn_cell_fw,\n            inputs=rnn_input,\n            sequence_length=style_len,\n            dtype=rnn_input.dtype,\n            time_major=False,\n        )\n        final_state = final_state[0]\n      else:\n        multirnn_cell_bw = tf.nn.rnn_cell.MultiRNNCell(\n            [\n                single_cell(\n                    cell_class=rnn_type,\n                    cell_params=cell_params,\n                    training=training,\n                    residual_connections=False\n                ) for _ in range(num_rnn_layers)\n            ]\n        )\n        top_layer, final_state = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=multirnn_cell_fw,\n            cell_bw=multirnn_cell_bw,\n            inputs=rnn_input,\n            sequence_length=style_len,\n            dtype=rnn_input.dtype,\n            time_major=False\n        )\n        # concat 2 tensors [B, T, n_cell_dim] --> [B, T, 2*n_cell_dim]\n        final_state = tf.concat((final_state[0][0].h, final_state[1][0].h), 1)\n        rnn_vars += multirnn_cell_bw.trainable_variables\n\n      top_layer = final_state\n      # Apply linear layer\n      top_layer = tf.layers.dense(\n          top_layer,\n          128,\n          activation=tf.nn.tanh,\n          kernel_regularizer=regularizer,\n          name=""reference_activation""\n      )\n      if regularizer and training:\n        cell_weights = rnn_vars\n        for weights in cell_weights:\n          if ""bias"" not in weights.name:\n            # print(""Added regularizer to {}"".format(weights.name))\n            if weights.dtype.base_dtype == tf.float16:\n              tf.add_to_collection(\n                  \'REGULARIZATION_FUNCTIONS\', (weights, regularizer)\n              )\n            else:\n              tf.add_to_collection(\n                  ops.GraphKeys.REGULARIZATION_LOSSES, regularizer(weights)\n              )\n\n    num_units = params[""num_tokens""]\n    att_size = params[""attention_layer_size""]\n\n    # Randomly initilized tokens\n    gst_embedding = tf.get_variable(\n        ""token_embeddings"",\n        shape=[num_units, params[""emb_size""]],\n        dtype=self.params[""dtype""],\n        initializer=tf.random_uniform_initializer(\n            minval=-1.,\n            maxval=1.,\n            dtype=self.params[""dtype""]\n        ),\n        trainable=False\n    )\n\n    attention = attention_layer.Attention(\n        params[""attention_layer_size""], params[""num_heads""],\n        0.,\n        training,\n        mode=""bahdanau""\n    )\n\n    top_layer = tf.expand_dims(top_layer, 1)\n    gst_embedding = tf.nn.tanh(gst_embedding)\n    gst_embedding = tf.expand_dims(gst_embedding, 0)\n    gst_embedding = tf.tile(gst_embedding, [batch_size, 1, 1])\n    token_embeddings = attention(top_layer, gst_embedding, None)\n    token_embeddings = tf.squeeze(token_embeddings, 1)\n\n    return token_embeddings\n'"
open_seq2seq/encoders/tdnn_encoder.py,10,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom .encoder import Encoder\nfrom open_seq2seq.data.speech2text.speech2text import Speech2TextDataLayer\nfrom open_seq2seq.parts.cnns.conv_blocks import conv_actv, conv_bn_actv,\\\n                                                conv_ln_actv, conv_in_actv,\\\n                                                conv_bn_res_bn_actv\n\n\nclass TDNNEncoder(Encoder):\n  """"""General time delay neural network (TDNN) encoder. Fully convolutional model\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(Encoder.get_required_params(), **{\n        \'dropout_keep_prob\': float,\n        \'convnet_layers\': list,\n        \'activation_fn\': None,  # any valid callable\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Encoder.get_optional_params(), **{\n        \'data_format\': [\'channels_first\', \'channels_last\'],\n        \'normalization\': [None, \'batch_norm\', \'layer_norm\', \'instance_norm\'],\n        \'bn_momentum\': float,\n        \'bn_epsilon\': float,\n        \'use_conv_mask\': bool,\n        \'drop_block_prob\': float,\n        \'drop_block_index\': int,\n    })\n\n  def __init__(self, params, model, name=""w2l_encoder"", mode=\'train\'):\n    """"""TDNN encoder constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **dropout_keep_prob** (float) --- keep probability for dropout.\n    * **convnet_layers** (list) --- list with the description of convolutional\n      layers. For example::\n        ""convnet_layers"": [\n          {\n            ""type"": ""conv1d"", ""repeat"" : 5,\n            ""kernel_size"": [7], ""stride"": [1],\n            ""num_channels"": 250, ""padding"": ""SAME""\n          },\n          {\n            ""type"": ""conv1d"", ""repeat"" : 3,\n            ""kernel_size"": [11], ""stride"": [1],\n            ""num_channels"": 500, ""padding"": ""SAME""\n          },\n          {\n            ""type"": ""conv1d"", ""repeat"" : 1,\n            ""kernel_size"": [32], ""stride"": [1],\n            ""num_channels"": 1000, ""padding"": ""SAME""\n          },\n          {\n            ""type"": ""conv1d"", ""repeat"" : 1,\n            ""kernel_size"": [1], ""stride"": [1],\n            ""num_channels"": 1000, ""padding"": ""SAME""\n          },\n        ]\n    * **activation_fn** --- activation function to use.\n    * **data_format** (string) --- could be either ""channels_first"" or\n      ""channels_last"". Defaults to ""channels_last"".\n    * **normalization** --- normalization to use. Accepts [None, \'batch_norm\'].\n      Use None if you don\'t want to use normalization. Defaults to \'batch_norm\'.\n    * **bn_momentum** (float) --- momentum for batch norm. Defaults to 0.90.\n    * **bn_epsilon** (float) --- epsilon for batch norm. Defaults to 1e-3.\n    * **drop_block_prob** (float) --- probability of dropping encoder blocks.\n      Defaults to 0.0 which corresponds to training without dropping blocks.\n    * **drop_block_index** (int) -- index of the block to drop on inference.\n      Defaults to -1 which corresponds to keeping all blocks.\n    * **use_conv_mask** (bool) --- whether to apply a sequence mask prior to\n      convolution operations. Defaults to False for backwards compatibility.\n      Recommended to set as True\n    """"""\n    super(TDNNEncoder, self).__init__(params, model, name, mode)\n\n  def _encode(self, input_dict):\n    """"""Creates TensorFlow graph for Wav2Letter like encoder.\n\n    Args:\n      input_dict (dict): input dictionary that has to contain\n          the following fields::\n            input_dict = {\n              ""source_tensors"": [\n                src_sequence (shape=[batch_size, sequence length, num features]),\n                src_length (shape=[batch_size])\n              ]\n            }\n\n    Returns:\n      dict: dictionary with the following tensors::\n\n        {\n          \'outputs\': hidden state, shape=[batch_size, sequence length, n_hidden]\n          \'src_length\': tensor, shape=[batch_size]\n        }\n    """"""\n\n    source_sequence, src_length = input_dict[\'source_tensors\']\n    \n    num_pad = tf.constant(0)\n\n    if isinstance(self._model.get_data_layer(), Speech2TextDataLayer):\n      pad_to = 0\n      if self._model.get_data_layer().params.get(\'backend\', \'psf\') == \'librosa\':\n        pad_to = self._model.get_data_layer().params.get(""pad_to"", 8)\n        \n      if pad_to > 0:\n        num_pad = tf.mod(pad_to - tf.mod(tf.reduce_max(src_length), pad_to), pad_to)\n    else:\n      print(""WARNING: TDNNEncoder is currently meant to be used with the"",\n            ""Speech2Text data layer. Assuming that this data layer does not"",\n            ""do additional padding past padded_batch."")\n\n    max_len = tf.reduce_max(src_length) + num_pad\n\n    training = (self._mode == ""train"")\n    dropout_keep_prob = self.params[\'dropout_keep_prob\'] if training else 1.0\n    regularizer = self.params.get(\'regularizer\', None)\n    data_format = self.params.get(\'data_format\', \'channels_last\')\n    normalization = self.params.get(\'normalization\', \'batch_norm\')\n\n    drop_block_prob = self.params.get(\'drop_block_prob\', 0.0)\n    drop_block_index = self.params.get(\'drop_block_index\', -1)\n\n    normalization_params = {}\n\n    if self.params.get(""use_conv_mask"", False):\n      mask = tf.sequence_mask(\n          lengths=src_length, maxlen=max_len,\n          dtype=source_sequence.dtype\n      )\n      mask = tf.expand_dims(mask, 2)\n\n    if normalization is None:\n      conv_block = conv_actv\n    elif normalization == ""batch_norm"":\n      conv_block = conv_bn_actv\n      normalization_params[\'bn_momentum\'] = self.params.get(\n          \'bn_momentum\', 0.90)\n      normalization_params[\'bn_epsilon\'] = self.params.get(\'bn_epsilon\', 1e-3)\n    elif normalization == ""layer_norm"":\n      conv_block = conv_ln_actv\n    elif normalization == ""instance_norm"":\n      conv_block = conv_in_actv\n    else:\n      raise ValueError(""Incorrect normalization"")\n\n    conv_inputs = source_sequence\n    if data_format == \'channels_last\':\n      conv_feats = conv_inputs  # B T F\n    else:\n      conv_feats = tf.transpose(conv_inputs, [0, 2, 1])  # B F T\n\n    residual_aggregation = []\n\n    # ----- Convolutional layers ---------------------------------------------\n    convnet_layers = self.params[\'convnet_layers\']\n\n    for idx_convnet in range(len(convnet_layers)):\n      layer_type = convnet_layers[idx_convnet][\'type\']\n      layer_repeat = convnet_layers[idx_convnet][\'repeat\']\n      ch_out = convnet_layers[idx_convnet][\'num_channels\']\n      kernel_size = convnet_layers[idx_convnet][\'kernel_size\']\n      strides = convnet_layers[idx_convnet][\'stride\']\n      padding = convnet_layers[idx_convnet][\'padding\']\n      dilation = convnet_layers[idx_convnet][\'dilation\']\n      dropout_keep = convnet_layers[idx_convnet].get(\n          \'dropout_keep_prob\', dropout_keep_prob) if training else 1.0\n      residual = convnet_layers[idx_convnet].get(\'residual\', False)\n      residual_dense = convnet_layers[idx_convnet].get(\'residual_dense\', False)\n\n\n      # For the first layer in the block, apply a mask\n      if self.params.get(""use_conv_mask"", False):\n        conv_feats = conv_feats * mask\n\n      if residual:\n        layer_res = conv_feats\n        if residual_dense:\n          residual_aggregation.append(layer_res)\n          layer_res = residual_aggregation\n\n      for idx_layer in range(layer_repeat):\n\n        if padding == ""VALID"":\n          src_length = (src_length - kernel_size[0]) // strides[0] + 1\n          max_len = (max_len - kernel_size[0]) // strides[0] + 1\n        else:\n          src_length = (src_length + strides[0] - 1) // strides[0]\n          max_len = (max_len + strides[0] - 1) // strides[0]\n\n        # For all layers other than first layer, apply mask\n        if idx_layer > 0 and self.params.get(""use_conv_mask"", False):\n          conv_feats = conv_feats * mask\n\n        # Since we have a stride 2 layer, we need to update mask for future operations\n        if (self.params.get(""use_conv_mask"", False) and\n            (padding == ""VALID"" or strides[0] > 1)):\n          mask = tf.sequence_mask(\n              lengths=src_length,\n              maxlen=max_len,\n              dtype=conv_feats.dtype\n          )\n          mask = tf.expand_dims(mask, 2)\n\n        if residual and idx_layer == layer_repeat - 1:\n          conv_feats = conv_bn_res_bn_actv(\n              layer_type=layer_type,\n              name=""conv{}{}"".format(\n                  idx_convnet + 1, idx_layer + 1),\n              inputs=conv_feats,\n              res_inputs=layer_res,\n              filters=ch_out,\n              kernel_size=kernel_size,\n              activation_fn=self.params[\'activation_fn\'],\n              strides=strides,\n              padding=padding,\n              dilation=dilation,\n              regularizer=regularizer,\n              training=training,\n              data_format=data_format,\n              drop_block_prob=drop_block_prob,\n              drop_block=(drop_block_index == idx_convnet),\n              **normalization_params\n          )\n        else:\n          conv_feats = conv_block(\n              layer_type=layer_type,\n              name=""conv{}{}"".format(\n                  idx_convnet + 1, idx_layer + 1),\n              inputs=conv_feats,\n              filters=ch_out,\n              kernel_size=kernel_size,\n              activation_fn=self.params[\'activation_fn\'],\n              strides=strides,\n              padding=padding,\n              dilation=dilation,\n              regularizer=regularizer,\n              training=training,\n              data_format=data_format,\n              **normalization_params\n          )\n\n        conv_feats = tf.nn.dropout(x=conv_feats, keep_prob=dropout_keep)\n\n    outputs = conv_feats\n\n    if data_format == \'channels_first\':\n      outputs = tf.transpose(outputs, [0, 2, 1])\n\n    return {\n        \'outputs\': outputs,\n        \'src_length\': src_length,\n    }\n'"
open_seq2seq/encoders/transformer_encoder.py,8,"b'# This code is heavily based on the code from MLPerf\n# https://github.com/mlperf/reference/tree/master/translation/tensorflow\n# /transformer\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom open_seq2seq.encoders import Encoder\nfrom open_seq2seq.parts.transformer import attention_layer, ffn_layer, utils, \\\n                                           embedding_layer\nfrom open_seq2seq.parts.transformer.common import PrePostProcessingWrapper, \\\n                                    LayerNormalization, Transformer_BatchNorm\n\n\nclass TransformerEncoder(Encoder):\n  """"""Transformer model encoder""""""\n\n  @staticmethod\n  def get_required_params():\n    """"""Static method with description of required parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **have to** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return dict(Encoder.get_required_params(), **{\n        ""encoder_layers"": int,\n        ""hidden_size"": int,\n        ""num_heads"": int,\n        ""attention_dropout"": float,\n        ""filter_size"": int,\n        ""src_vocab_size"": int,\n        ""relu_dropout"": float,\n        ""layer_postprocess_dropout"": float,\n        ""remove_padding"": bool,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    """"""Static method with description of optional parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **can** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return dict(Encoder.get_optional_params(), **{\n        \'regularizer\': None,  # any valid TensorFlow regularizer\n        \'regularizer_params\': dict,\n        \'initializer\': None,  # any valid TensorFlow initializer\n        \'initializer_params\': dict,\n        \'pad_embeddings_2_eight\': bool,\n        \'norm_params\': dict,\n    })\n\n  def __init__(self, params, model, name=""transformer_encoder"", mode=\'train\' ):\n    super(TransformerEncoder, self).__init__(\n        params, model, name=name, mode=mode,\n    )\n    self.layers = []\n    self.output_normalization = None\n    self._mode = mode\n\n    self.embedding_softmax_layer = None\n    self.norm_params = self.params.get(""norm_params"", {""type"": ""layernorm_L2""})\n    self.regularizer = self.params.get(""regularizer"", None)\n    if self.regularizer != None:\n      self.regularizer_params = params.get(""regularizer_params"", {\'scale\': 0.0})\n      self.regularizer=self.regularizer(self.regularizer_params[\'scale\']) \\\n        if self.regularizer_params[\'scale\'] > 0.0 else None\n\n\n  def _call(self, encoder_inputs, attention_bias, inputs_padding):\n    for n, layer in enumerate(self.layers):\n      # Run inputs through the sublayers.\n      self_attention_layer = layer[0]\n      feed_forward_network = layer[1]\n\n      with tf.variable_scope(""layer_%d"" % n):\n        with tf.variable_scope(""self_attention""):\n          encoder_inputs = self_attention_layer(encoder_inputs, attention_bias)\n        with tf.variable_scope(""ffn""):\n          encoder_inputs = feed_forward_network(encoder_inputs, inputs_padding)\n\n    return self.output_normalization(encoder_inputs)\n\n  def _encode(self, input_dict):\n    training = (self.mode == ""train"")\n\n    if len(self.layers) == 0:\n      # prepare encoder graph\n      self.embedding_softmax_layer = embedding_layer.EmbeddingSharedWeights(\n          self.params[""src_vocab_size""], self.params[""hidden_size""],\n          pad_vocab_to_eight=self.params.get(\'pad_embeddings_2_eight\', False),\n      )\n\n      for _ in range(self.params[\'encoder_layers\']):\n        # Create sublayers for each layer.\n        self_attention_layer = attention_layer.SelfAttention(\n          hidden_size=self.params[""hidden_size""],\n          num_heads=self.params[""num_heads""],\n          attention_dropout=self.params[""attention_dropout""],\n          train=training,\n          regularizer=self.regularizer\n        )\n        feed_forward_network = ffn_layer.FeedFowardNetwork(\n          hidden_size=self.params[""hidden_size""],\n          filter_size=self.params[""filter_size""],\n          relu_dropout=self.params[""relu_dropout""],\n          train=training,\n          regularizer=self.regularizer\n        )\n\n        self.layers.append([\n            PrePostProcessingWrapper(self_attention_layer, self.params,\n                                     training),\n            PrePostProcessingWrapper(feed_forward_network, self.params,\n                                     training)\n        ])\n\n      # final normalization layer.\n      print(""Encoder:"", self.norm_params[""type""], self.mode)\n      if self.norm_params[""type""] ==""batch_norm"":\n        self.output_normalization = Transformer_BatchNorm(\n          training=training,\n          params=self.norm_params)\n      else:\n        self.output_normalization = LayerNormalization(\n          hidden_size=self.params[""hidden_size""], params=self.norm_params)\n\n    # actual encoder part\n    with tf.name_scope(""encode""):\n      inputs = input_dict[\'source_tensors\'][0]\n      # Prepare inputs to the layer stack by adding positional encodings and\n      # applying dropout.\n      embedded_inputs = self.embedding_softmax_layer(inputs)\n      if self.params[""remove_padding""]:\n        inputs_padding = utils.get_padding(inputs)\n        #inputs_padding = utils.get_padding(inputs,dtype=self._params[""dtype""])\n      else:\n        inputs_padding = None\n      inputs_attention_bias = utils.get_padding_bias(inputs)\n      # inputs_attention_bias = utils.get_padding_bias(inputs, dtype=self._params[""dtype""])\n\n      with tf.name_scope(""add_pos_encoding""):\n        length = tf.shape(embedded_inputs)[1]\n        pos_encoding = utils.get_position_encoding(\n            length, self.params[""hidden_size""],\n        )\n        encoder_inputs = embedded_inputs + tf.cast(x=pos_encoding,\n                                                   dtype=embedded_inputs.dtype)\n\n      if self.mode == ""train"":\n        encoder_inputs = tf.nn.dropout(encoder_inputs,\n            keep_prob = 1.0 - self.params[""layer_postprocess_dropout""],\n        )\n\n      encoded = self._call(encoder_inputs, inputs_attention_bias,\n                           inputs_padding)\n      return {\'outputs\': encoded,\n              \'inputs_attention_bias\': inputs_attention_bias,\n              \'state\': None,\n              \'src_lengths\': input_dict[\'source_tensors\'][1],\n              \'embedding_softmax_layer\': self.embedding_softmax_layer,\n              \'encoder_input\': inputs}\n'"
open_seq2seq/encoders/wavenet_encoder.py,34,"b'# Copyright (c) 2018 NVIDIA Corporation\n\nimport tensorflow as tf\nfrom math import ceil\nfrom open_seq2seq.parts.cnns.conv_blocks import conv_actv, conv_bn_actv\n\nfrom .encoder import Encoder\n\n\ndef _get_receptive_field(kernel_size, blocks, layers_per_block):\n  dilations = [2 ** i for i in range(layers_per_block)]\n  return (kernel_size - 1) * blocks * sum(dilations) + 1\n\ndef _mu_law_encode(signal, channels, dtype):\n  mu = tf.saturate_cast(channels - 1, dtype)\n  safe_audio_abs = tf.minimum(tf.abs(signal), 1.0)\n  magnitude = tf.log1p(mu * safe_audio_abs) / tf.log1p(mu)\n  signal = tf.sign(signal) * magnitude\n  return tf.cast((signal + 1) / 2 * mu + 0.5, tf.int32)\n\ndef _mu_law_decode(output, channels):\n  mu = channels - 1\n  signal = 2 * (tf.to_float(output) / mu) - 1\n  magnitude = (1 / mu) * ((1 + mu)**abs(signal) - 1)\n  return tf.sign(signal) * magnitude\n\ndef conv_1x1(\n    layer_type, name, inputs, filters, strides, regularizer, training,\n    data_format):\n  """"""\n  Defines a single 1x1 convolution for convenience\n  """"""\n\n  return conv_actv(\n      layer_type=layer_type,\n      name=name,\n      inputs=inputs,\n      filters=filters,\n      kernel_size=1,\n      activation_fn=None,\n      strides=strides,\n      padding=""SAME"",\n      regularizer=regularizer,\n      training=training,\n      data_format=data_format,\n  )\n\ndef causal_conv_bn_actv(\n    layer_type, name, inputs, filters, kernel_size, activation_fn, strides,\n    padding, regularizer, training, data_format, bn_momentum, bn_epsilon,\n    dilation=1):\n  """"""\n  Defines a single dilated causal convolutional layer with batch norm\n  """"""\n\n  block = conv_bn_actv(\n      layer_type=layer_type,\n      name=name,\n      inputs=inputs,\n      filters=filters,\n      kernel_size=kernel_size,\n      activation_fn=activation_fn,\n      strides=strides,\n      padding=padding,\n      regularizer=regularizer,\n      training=training,\n      data_format=data_format,\n      bn_momentum=bn_momentum,\n      bn_epsilon=bn_epsilon,\n      dilation=dilation\n  )\n\n  # pad the left side of the time-series with an amount of zeros based on the\n  # dilation rate\n  block = tf.pad(block, [[0, 0], [dilation * (kernel_size - 1), 0], [0, 0]])\n  return block\n\ndef wavenet_conv_block(\n    layer_type, name, inputs, condition_filter, condition_gate, filters,\n    kernel_size, strides, padding, regularizer, training, data_format,\n    bn_momentum, bn_epsilon, layers_per_block):\n  """"""\n  Defines a single WaveNet block using the architecture specified in the\n  original paper, including skip and residual connections\n  """"""\n\n  skips = None\n  for layer in range(layers_per_block):\n    # split source along channels\n    source_shape = inputs.get_shape().as_list()\n    source_filter = inputs[:, :, 0:int(source_shape[2] / 2)]\n    source_gate = inputs[:, :, int(source_shape[2] / 2):]\n\n    dilation = 2 ** layer\n\n    source_filter = causal_conv_bn_actv(\n        layer_type=layer_type,\n        name=""filter_{}_{}"".format(name, layer),\n        inputs=source_filter,\n        filters=filters,\n        kernel_size=kernel_size,\n        activation_fn=None,\n        strides=strides,\n        padding=padding,\n        regularizer=regularizer,\n        training=training,\n        data_format=data_format,\n        bn_momentum=bn_momentum,\n        bn_epsilon=bn_epsilon,\n        dilation=dilation\n    )\n\n    source_gate = causal_conv_bn_actv(\n        layer_type=layer_type,\n        name=""gate_{}_{}"".format(name, layer),\n        inputs=source_gate,\n        filters=filters,\n        kernel_size=kernel_size,\n        activation_fn=None,\n        strides=strides,\n        padding=padding,\n        regularizer=regularizer,\n        training=training,\n        data_format=data_format,\n        bn_momentum=bn_momentum,\n        bn_epsilon=bn_epsilon,\n        dilation=dilation\n    )\n\n    if condition_filter is not None and condition_gate is not None:\n      source_filter = tf.tanh(tf.add(source_filter, condition_filter))\n      source_gate = tf.sigmoid(tf.add(source_gate, condition_gate))\n    else:\n      source_filter = tf.tanh(source_filter)\n      source_gate = tf.sigmoid(source_gate)\n\n    conv_feats = tf.multiply(source_filter, source_gate)\n\n    residual = conv_1x1(\n        layer_type=layer_type,\n        name=""residual_1x1_{}_{}"".format(name, layer),\n        inputs=conv_feats,\n        filters=filters,\n        strides=strides,\n        regularizer=regularizer,\n        training=training,\n        data_format=data_format\n    )\n\n    inputs = tf.add(inputs, residual)\n\n    skip = conv_1x1(\n        layer_type=layer_type,\n        name=""skip_1x1_{}_{}"".format(name, layer),\n        inputs=conv_feats,\n        filters=filters,\n        strides=strides,\n        regularizer=regularizer,\n        training=training,\n        data_format=data_format\n    )\n\n    if skips is None:\n      skips = skip\n    else:\n      skips = tf.add(skips, skip)\n\n  return inputs, skips\n\nclass WavenetEncoder(Encoder):\n\n  """"""\n  WaveNet like encoder.\n\n  Consists of several blocks of dilated causal convolutions.\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(\n        Encoder.get_required_params(),\n        **{\n            ""layer_type"": str,\n            ""kernel_size"": int,\n            ""strides"": int,\n            ""padding"": str,\n            ""blocks"": int,\n            ""layers_per_block"": int,\n            ""filters"": int,\n            ""quantization_channels"": int\n        }\n    )\n\n  @staticmethod\n  def get_optional_params():\n    return dict(\n        Encoder.get_optional_params(),\n        **{\n            ""data_format"": str,\n            ""bn_momentum"": float,\n            ""bn_epsilon"": float\n        }\n    )\n\n  def __init__(self, params, model, name=""wavenet_encoder"", mode=""train""):\n    """"""\n    WaveNet like encoder constructor.\n\n    Config parameters:\n    * **layer_type** (str) --- type of convolutional layer, currently only\n      supports ""conv1d""\n    * **kernel_size** (int) --- size of kernel\n    * **strides** (int) --- size of stride\n    * **padding** (str) --- padding, can be ""SAME"" or ""VALID""\n\n    * **blocks** (int) --- number of dilation cycles\n    * **layers_per_block** (int) --- number of dilated convolutional layers in\n      each block\n    * **filters** (int) --- number of output channels\n    * **quantization_channels** (int) --- depth of mu-law quantized input\n\n    * **data_format** (string) --- could be either ""channels_first"" or\n      ""channels_last"". Defaults to ""channels_last"".\n    * **bn_momentum** (float) --- momentum for batch norm. Defaults to 0.1.\n    * **bn_epsilon** (float) --- epsilon for batch norm. Defaults to 1e-5.\n    """"""\n\n    super(WavenetEncoder, self).__init__(params, model, name, mode)\n\n  def _encode(self, input_dict):\n    """"""\n    Creates TensorFlow graph for WaveNet like encoder.\n    ...\n    """"""\n\n    training = (self._mode == ""train"" or self._mode == ""eval"")\n\n    if training:\n      source, src_length, condition, spec_length = input_dict[""source_tensors""]\n      spec_offset = 0\n    else:\n      source, src_length, condition, spec_length, spec_offset = \\\n        input_dict[""source_tensors""]\n\n    regularizer = self.params.get(""regularizer"", None)\n    data_format = self.params.get(""data_format"", ""channels_last"")\n\n    if data_format != ""channels_last"":\n      source = tf.transpose(source, [0, 2, 1])\n      condition = tf.transpose(condition, [0, 2, 1])\n\n    dtype = self.params[""dtype""]\n    layer_type = self.params[""layer_type""]\n    kernel_size = self.params[""kernel_size""]\n    strides = self.params[""strides""]\n    padding = self.params[""padding""]\n    blocks = self.params[""blocks""]\n    layers_per_block = self.params[""layers_per_block""]\n    filters = self.params[""filters""]\n    quantization_channels = self.params[""quantization_channels""]\n\n    bn_momentum = self.params.get(""bn_momentum"", 0.1)\n    bn_epsilon = self.params.get(""bn_epsilon"", 1e-5)\n    local_conditioning = self.params.get(""local_conditioning"", True)\n\n    receptive_field = _get_receptive_field(\n        kernel_size, blocks, layers_per_block\n    )\n\n    # ----- Preprocessing -----------------------------------------------\n\n    encoded_inputs = _mu_law_encode(source, quantization_channels, dtype)\n\n    if training:\n      # remove last sample to maintain causality\n      inputs = tf.slice(\n          encoded_inputs, [0, 0], [-1, tf.shape(encoded_inputs)[1] - 1]\n      )\n    else:\n      inputs = encoded_inputs\n\n    inputs = tf.one_hot(inputs, depth=quantization_channels, axis=-1)\n    inputs = tf.saturate_cast(inputs, dtype)\n\n    if local_conditioning:\n      # split condition along channels\n      condition_shape = condition.get_shape().as_list()\n      condition_filter = condition[:, :, 0:int(condition_shape[2] / 2)]\n      condition_gate = condition[:, :, int(condition_shape[2] / 2):]\n\n      condition_filter = conv_1x1(\n          layer_type=layer_type,\n          name=""filter_condition"",\n          inputs=condition_filter,\n          filters=filters,\n          strides=strides,\n          regularizer=regularizer,\n          training=training,\n          data_format=data_format\n      )\n\n      condition_gate = conv_1x1(\n          layer_type=layer_type,\n          name=""gate_condition"",\n          inputs=condition_gate,\n          filters=filters,\n          strides=strides,\n          regularizer=regularizer,\n          training=training,\n          data_format=data_format\n      )\n\n      if training:\n        # remove last sample to maintain causality\n        condition_filter = condition_filter[:, :-1, :]\n        condition_gate = condition_gate[:, :-1, :]\n      else:\n        # pad with zeros to align the condition to the source for\n        # autoregressive inference\n        zeros = tf.saturate_cast(\n            tf.zeros([condition_shape[0], receptive_field, filters]),\n            dtype\n        )\n        condition_filter = tf.concat([zeros, condition_filter], axis=1)\n        condition_gate = tf.concat([zeros, condition_gate], axis=1)\n\n        condition_filter = condition_filter[\n            :, spec_offset:spec_offset + receptive_field, :\n        ]\n        condition_gate = condition_gate[\n            :, spec_offset:spec_offset + receptive_field, :\n        ]\n\n    else:\n      condition_filter = None\n      condition_gate = None\n\n    # ----- Convolutional layers -----------------------------------------------\n\n    # first causal convolutional layer\n    inputs = causal_conv_bn_actv(\n        layer_type=layer_type,\n        name=""preprocess"",\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        activation_fn=None,\n        strides=strides,\n        padding=padding,\n        regularizer=regularizer,\n        training=training,\n        data_format=data_format,\n        bn_momentum=bn_momentum,\n        bn_epsilon=bn_epsilon,\n        dilation=1\n    )\n\n    # dilation stack\n    skips = None\n    for block in range(blocks):\n      inputs, skip = wavenet_conv_block(\n          layer_type=layer_type,\n          name=block,\n          inputs=inputs,\n          condition_filter=condition_filter,\n          condition_gate=condition_gate,\n          filters=filters,\n          kernel_size=kernel_size,\n          strides=strides,\n          padding=padding,\n          regularizer=regularizer,\n          training=training,\n          data_format=data_format,\n          bn_momentum=bn_momentum,\n          bn_epsilon=bn_epsilon,\n          layers_per_block=layers_per_block\n      )\n\n      if skips is None:\n        skips = skip\n      else:\n        skips = tf.add(skips, skip)\n\n    outputs = tf.add(skips, inputs)\n\n    # postprocessing\n    outputs = tf.nn.relu(outputs)\n    outputs = conv_1x1(\n        layer_type=layer_type,\n        name=""postprocess_1"",\n        inputs=outputs,\n        filters=filters,\n        strides=strides,\n        regularizer=regularizer,\n        training=training,\n        data_format=data_format\n    )\n\n    outputs = tf.nn.relu(outputs)\n    outputs = conv_1x1(\n        layer_type=layer_type,\n        name=""postprocess_2"",\n        inputs=outputs,\n        filters=quantization_channels,\n        strides=strides,\n        regularizer=regularizer,\n        training=training,\n        data_format=data_format\n    )\n\n    if training:\n      # remove samples that would be predicted without the full receptive field\n      prediction = tf.slice(outputs, [0, receptive_field - 1, 0], [-1, -1, -1])\n      target_output = tf.slice(encoded_inputs, [0, receptive_field], [-1, -1])\n    else:\n      prediction = outputs\n      target_output = encoded_inputs\n\n    # decode the predicted signal as audio\n    audio = tf.argmax(tf.nn.softmax(outputs), axis=-1, output_type=tf.int32)\n    audio = tf.expand_dims(audio, -1)\n    audio = _mu_law_decode(audio, self.params[""quantization_channels""])\n    audio = tf.cast(audio, tf.float32)\n\n    return { ""logits"": prediction, ""outputs"": [target_output, audio] }\n'"
open_seq2seq/losses/__init__.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""\nLosses to be used in seq2seq models\n""""""\nfrom .sequence_loss import BasicSequenceLoss, CrossEntropyWithSmoothing, \\\n    PaddedCrossEntropyLossWithSmoothing, BasicSampledSequenceLoss\nfrom .ctc_loss import CTCLoss\nfrom .cross_entropy_loss import CrossEntropyLoss\nfrom .wavenet_loss import WavenetLoss\nfrom .jca_loss import MultiTaskCTCEntropyLoss\nfrom .text2speech_loss import Text2SpeechLoss'"
open_seq2seq/losses/cross_entropy_loss.py,1,"b'# Copyright (c) 2018 NVIDIA Corporation\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom .loss import Loss\n\n\nclass CrossEntropyLoss(Loss):\n  """"""Implementation of the usual cross_entropy loss with softmax.""""""\n\n  def __init__(self, params, model, name=""cross_entropy_loss""):\n    super(CrossEntropyLoss, self).__init__(params, model, name)\n\n  def _compute_loss(self, input_dict):\n    logits = input_dict[\'decoder_output\'][\'logits\']\n    labels = input_dict[\'target_tensors\'][0]\n    return tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels)\n'"
open_seq2seq/losses/ctc_loss.py,8,"b'# Copyright (c) 2018 NVIDIA Corporation\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.utils.utils import mask_nans, deco_print\nfrom .loss import Loss\n\n\ndef dense_to_sparse(dense_tensor, sequence_length):\n  indices = tf.where(tf.sequence_mask(sequence_length))\n  values = tf.gather_nd(dense_tensor, indices)\n  shape = tf.shape(dense_tensor, out_type=tf.int64)\n  return tf.SparseTensor(indices, values, shape)\n\n\nclass CTCLoss(Loss):\n  """"""Implementation of the CTC loss.""""""\n  @staticmethod\n  def get_optional_params():\n    return dict(Loss.get_optional_params(), **{\n        \'mask_nan\': bool,\n    })\n\n  def __init__(self, params, model, name=""ctc_loss""):\n    """"""CTC loss constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **mask_nan** (bool) --- whether to mask nans in the loss output. Defaults\n      to True.\n    """"""\n    super(CTCLoss, self).__init__(params, model, name)\n    self._mask_nan = self.params.get(""mask_nan"", True)\n    # this loss can only operate in full precision\n    # if self.params[\'dtype\'] != tf.float32:\n    #   deco_print(""Warning: defaulting CTC loss to work in float32"")\n    self.params[\'dtype\'] = tf.float32\n\n  def _compute_loss(self, input_dict):\n    """"""CTC loss graph construction.\n\n    Expects the following inputs::\n\n      input_dict = {\n\n      }\n\n    Args:\n      input_dict (dict): input dictionary that has to contain\n          the following fields::\n            input_dict = {\n              ""decoder_output"": {\n                ""logits"": tensor, shape [batch_size, time length, tgt_vocab_size]\n                ""src_length"": tensor, shape [batch_size]\n              },\n              ""target_tensors"": [\n                tgt_sequence (shape=[batch_size, time length, num features]),\n                tgt_length (shape=[batch_size])\n              ]\n            }\n\n    Returns:\n      averaged CTC loss.\n    """"""\n    logits = input_dict[\'decoder_output\'][\'logits\']\n    tgt_sequence, tgt_length = input_dict[\'target_tensors\']\n    # this loss needs an access to src_length since they\n    # might get changed in the encoder\n    src_length = input_dict[\'decoder_output\'][\'src_length\']\n\n    # Compute the CTC loss\n    total_loss = tf.nn.ctc_loss(\n        labels=dense_to_sparse(tgt_sequence, tgt_length),\n        inputs=logits,\n        sequence_length=src_length,\n        ignore_longer_outputs_than_inputs=True,\n    )\n\n    if self._mask_nan:\n      total_loss = mask_nans(total_loss)\n\n    # Calculate the average loss across the batch\n    avg_loss = tf.reduce_mean(total_loss)\n    return avg_loss\n'"
open_seq2seq/losses/jca_loss.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom .loss import Loss\nfrom .ctc_loss import CTCLoss\nfrom .sequence_loss import BasicSequenceLoss\n\n# To-Do Replace this with a generic multi-task loss.\n\n\nclass MultiTaskCTCEntropyLoss(Loss):\n  """"""\n  MultiTask CTC and cross entropy loss.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Loss.get_required_params(), **{\n        \'ctc_loss_params\': dict,\n        \'seq_loss_params\': dict,\n        \'lambda_value\': float,\n        \'tgt_vocab_size\': int,\n        \'batch_size\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Loss.get_optional_params(), **{\n    })\n\n  def __init__(self, params, model, name=""basic_sequence_loss""):\n    """"""Constructor.\n\n    Args:\n      params (dict): dictionary with loss parameters.\n        Should contain the following:\n        * ctc_loss_params: Parameters required for CTC loss.\n        * seq_loss_params: Parameters required for Sequence loss.\n        * lambda_value: lambda value used to combine the two losses.\n        * tgt_vocab_size: Target vocabulary size.\n        * batch_size: Size of the per-worker batch.\n    """"""\n    super(MultiTaskCTCEntropyLoss, self).__init__(params, model, name)\n    self.ctc_loss_params = self.params[""ctc_loss_params""]\n    self.seq_loss_params = self.params[""seq_loss_params""]\n    self.lambda_value = self.params[""lambda_value""]\n\n    self.seq_loss_params[""batch_size""] = self.params[""batch_size""]\n    self.seq_loss_params[""tgt_vocab_size""] = self.params[""tgt_vocab_size""]\n\n    self.ctc_loss = CTCLoss(self.ctc_loss_params, model)\n    self.seq_loss = BasicSequenceLoss(self.seq_loss_params, model)\n\n  def _compute_loss(self, input_dict):\n    """"""Computes multi-task ctc and cross entropy loss.\n\n    Args:\n      input_dict (dict): inputs to compute loss::\n        {\n              ""logits"": logits tensor of shape [batch_size, T, dim]\n              ""target_sequence"": tensor of shape [batch_size, T]\n              ""tgt_lengths"": tensor of shape [batch_size] or None\n        }\n\n    Returns:\n       Singleton loss tensor\n    """"""\n\n    ctc_loss_input_dict = {\n        ""decoder_output"": input_dict[\'decoder_output\'][\'ctc_outputs\'],\n        ""target_tensors"": input_dict[\'target_tensors\'],\n    }\n\n    seq_loss_input_dict = {\n        ""decoder_output"": input_dict[\'decoder_output\'][\'seq_outputs\'],\n        ""target_tensors"": input_dict[\'target_tensors\'],\n    }\n\n    ctc_loss_value = self.ctc_loss.compute_loss(ctc_loss_input_dict)\n    sequence_loss_value = self.seq_loss.compute_loss(seq_loss_input_dict)\n\n    return (1 - self.lambda_value) * sequence_loss_value + self.lambda_value * ctc_loss_value\n'"
open_seq2seq/losses/loss.py,4,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport abc\nimport copy\n\nimport six\nimport tensorflow as tf\n\nfrom open_seq2seq.utils.utils import check_params, cast_types\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Loss:\n  """"""Abstract class from which all losses must inherit.\n  """"""\n  @staticmethod\n  def get_required_params():\n    """"""Static method with description of required parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **have to** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {}\n\n  @staticmethod\n  def get_optional_params():\n    """"""Static method with description of optional parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **can** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {\n        \'dtype\': [tf.float16, tf.float32],\n    }\n\n  def __init__(self, params, model, name=""loss""):\n    """"""Loss constructor.\n    Note that loss constructors should not modify TensorFlow graph, all\n    graph construction should happen in the\n    :meth:`self._compute_loss() <_compute_loss>` method.\n\n    Args:\n      params (dict): parameters describing the loss.\n          All supported parameters are listed in :meth:`get_required_params`,\n          :meth:`get_optional_params` functions.\n      model (instance of a class derived from :class:`Model<models.model.Model>`):\n          parent model that created this loss.\n          Could be None if no model access is required for the use case.\n      name (str): name for loss variable scope.\n\n    Config parameters:\n\n    * **dtype** --- data dtype. Could be either ``tf.float16`` or ``tf.float32``.\n    """"""\n    check_params(params, self.get_required_params(), self.get_optional_params())\n    self._params = copy.deepcopy(params)\n    self._model = model\n\n    if \'dtype\' not in self._params:\n      if self._model:\n        self._params[\'dtype\'] = self._model.get_tf_dtype()\n      else:\n        self._params[\'dtype\'] = tf.float32\n\n    self._name = name\n\n  def compute_loss(self, input_dict):\n    """"""Wrapper around :meth:`self._compute_loss() <_compute_loss>` method.\n    Here name and dtype are set in the variable scope and then\n    :meth:`self._compute_loss() <_compute_loss>` method is called.\n\n    Args:\n      input_dict (dict): see :meth:`self._compute_loss() <_compute_loss>` docs.\n\n    Returns:\n      see :meth:`self._compute_loss() <_compute_loss>` docs.\n    """"""\n    with tf.variable_scope(self._name, dtype=self.params[\'dtype\']):\n      return self._compute_loss(self._cast_types(input_dict))\n\n  def _cast_types(self, input_dict):\n    """"""This function performs automatic cast of all inputs to the loss dtype.\n\n    Args:\n      input_dict (dict): dictionary passed to\n          :meth:`self._compute_loss() <_compute_loss>` method.\n\n    Returns:\n      dict: same as input_dict, but with all Tensors cast to the loss dtype.\n    """"""\n    return cast_types(input_dict, self.params[\'dtype\'])\n\n  @abc.abstractmethod\n  def _compute_loss(self, input_dict):\n    """"""This is the main function which should construct loss graph.\n    Typically, loss will take decoder-produced logits as an input and\n    return a singleton loss tensor.\n\n    Args:\n      input_dict (dict): dictionary containing loss inputs.\n          If the loss is used with :class:`models.encoder_decoder` class,\n          ``input_dict`` will have the following content::\n            {\n              ""decoder_output"": dictionary returned from decoder.decode() method\n              ""target_tensors"": data_layer.input_tensors[\'target_tensors\']\n            }\n\n    Returns:\n      singleton loss tensor. This tensor will be computed independently\n      for each GPU batch and then averaged\n      (``reduce_mean``) over the number of GPUs (or Horovod workers).\n    """"""\n    pass\n\n  @property\n  def params(self):\n    """"""Parameters used to construct the loss (dictionary).""""""\n    return self._params\n\n  @property\n  def name(self):\n    """"""Loss name.""""""\n    return self._name\n'"
open_seq2seq/losses/sequence_loss.py,89,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom .loss import Loss\n\n\nclass BasicSequenceLoss(Loss):\n  """"""\n  Basic sequence-to-sequence loss. This one does not use one-hot encodings\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Loss.get_required_params(), **{\n        \'tgt_vocab_size\': int,\n        \'batch_size\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Loss.get_optional_params(), **{\n        \'offset_target_by_one\': bool,\n        \'average_across_timestep\': bool,\n        \'do_mask\': bool,\n    })\n\n  def __init__(self, params, model, name=""basic_sequence_loss""):\n    """"""Constructor.\n\n    Args:\n      params (dict): dictionary with loss parameters.\n        Should contain the following:\n        * tgt_vocab_size: Target vocabulary size\n        * batch_size_per_gpu: Size of the per-worker batch\n        * offset_target_by_one: (default: True). Keep it true for\n        auto-regressive models\n        * average_across_timestep: (default: False). If True, will average\n          loss across timesteps, else it will sum across timesteps\n        * do_mask: (default: True) whether to mask based on tgt_lengths\n          (which is passed as part of loss_input_dict to compute_loss\n          and has to be not None then)\n    """"""\n    super(BasicSequenceLoss, self).__init__(params, model, name)\n    self._tgt_vocab_size = self.params[""tgt_vocab_size""]\n    self._batch_size = self.params[""batch_size""]\n    self._offset_target_by_one = self.params.get(""offset_target_by_one"", True)\n    self._average_across_timestep = self.params.get(""average_across_timestep"",\n                                                    False)\n    self._do_mask = self.params.get(""do_mask"", True)\n\n  def _compute_loss(self, input_dict):\n    """"""Computes cross entropy based sequence-to-sequence loss.\n\n    Args:\n      input_dict (dict): inputs to compute loss::\n        {\n              ""logits"": logits tensor of shape [batch_size, T, dim]\n              ""target_sequence"": tensor of shape [batch_size, T]\n              ""tgt_lengths"": tensor of shape [batch_size] or None\n        }\n\n    Returns:\n       Singleton loss tensor\n    """"""\n    logits = input_dict[""decoder_output""][""logits""]\n    target_sequence = input_dict[\'target_tensors\'][0]\n    tgt_lengths = input_dict[\'target_tensors\'][1]\n\n    if self._offset_target_by_one:\n      # this is necessary for auto-regressive models\n      current_ts = tf.cast(tf.minimum(\n          tf.shape(target_sequence)[1],\n          tf.shape(logits)[1],\n      ), tf.int32) - 1\n\n      logits = tf.slice(\n          logits,\n          begin=[0, 0, 0],\n          size=[-1, current_ts, -1],\n      )\n      target_sequence = tf.slice(target_sequence,\n                                 begin=[0, 1],\n                                 size=[-1, current_ts])\n    else:\n      current_ts = tf.cast(tf.minimum(\n          tf.shape(target_sequence)[1],\n          tf.shape(logits)[1],\n      ),tf.int32)\n\n    # Cast logits after potential slice\n    if logits.dtype.base_dtype != tf.float32:\n      logits = tf.cast(logits, tf.float32)\n\n    if self._do_mask:\n      if tgt_lengths is None:\n        raise ValueError(""If you are masking loss, tgt_lengths can\'t be None"")\n      mask = tf.sequence_mask(lengths=tgt_lengths - 1,\n                              maxlen=current_ts,\n                              dtype=logits.dtype)\n    else:\n      mask = tf.cast(tf.ones_like(target_sequence), logits.dtype)\n\n    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=tf.reshape(target_sequence, shape=[-1]),\n        logits=tf.reshape(logits, shape=[-1, self._tgt_vocab_size]),\n    )\n    if self._average_across_timestep:\n      loss = tf.reduce_mean(crossent * tf.reshape(mask, shape=[-1]))\n    else:\n      loss = tf.reduce_sum(crossent * tf.reshape(mask, shape=[-1]))\n      loss /= self._batch_size\n    return loss\n\n\nclass CrossEntropyWithSmoothing(Loss):\n  """"""Softmax cross entropy loss with label smoothing.\n  This one uses one-hot encodings for labels.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Loss.get_required_params(), **{\n        \'tgt_vocab_size\': int,\n        \'batch_size\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Loss.get_optional_params(), **{\n        \'offset_target_by_one\': bool,\n        \'average_across_timestep\': bool,\n        \'do_mask\': bool,\n        \'label_smoothing\': float,\n    })\n\n  def __init__(self, params, model, name=""cross_entropy_with_smoothing""):\n    """"""Constructor.\n\n    Args:\n      params (dict): dictionary with loss parameters.\n        Should contain the following:\n          * tgt_vocab_size: Target vocabulary size\n          * batch_size_per_gpu: Size of the per-worker batch\n          * offset_target_by_one: (default: True). Keep it true for\n            auto-regressive models\n          * do_mask: (default: True) whether to mask based on tgt_lengths\n            (which is passed as part of loss_input_dict to compute_loss\n            and has to be not None then)\n    """"""\n    super(CrossEntropyWithSmoothing, self).__init__(params, model, name)\n    self._tgt_vocab_size = self.params[""tgt_vocab_size""]\n    self._batch_size = self.params[""batch_size""]\n    self._offset_target_by_one = self.params.get(""offset_target_by_one"", True)\n    self._do_mask = self.params.get(""do_mask"", True)\n    self._label_smoothing = self.params.get(""label_smoothing"", 0.0)\n    self._average_across_timestep = self.params.get(""average_across_timestep"",\n                                                    False)\n\n  def _compute_loss(self, input_dict):\n    """"""Computes cross entropy based sequence-to-sequence loss\n    with label smoothing.\n\n    Args:\n      input_dict (dict): inputs to compute loss::\n        {\n            ""logits"": logits tensor of shape [batch_size, T, dim]\n            ""target_sequence"": tensor of shape [batch_size, T]\n            ""tgt_lengths"": tensor of shape [batch_size] or None\n        }\n    Returns:\n      Singleton loss tensor\n    """"""\n    logits = input_dict[""decoder_output""][""logits""]\n    target_sequence = input_dict[""target_tensors""][0]\n    tgt_lengths = input_dict[""target_tensors""][1]\n\n    if self._offset_target_by_one:\n      # this is necessary for auto-regressive models\n      current_ts = tf.cast(tf.minimum(\n          tf.shape(target_sequence)[1],\n          tf.shape(logits)[1],\n      ),tf.int32) - 1\n\n      logits = tf.slice(\n          logits,\n          begin=[0, 0, 0],\n          size=[-1, current_ts, -1],\n      )\n      target_sequence = tf.slice(target_sequence,\n                                 begin=[0, 1],\n                                 size=[-1, current_ts])\n    else:\n      current_ts = tf.cast(tf.minimum(\n          tf.shape(target_sequence)[1],\n          tf.shape(logits)[1],\n      ), tf.int32)\n\n    # Cast logits after potential slice\n    if logits.dtype.base_dtype != tf.float32:\n      logits = tf.cast(logits, tf.float32)\n\n    if self._do_mask:\n      if tgt_lengths is None:\n        raise ValueError(""If you are masking loss, tgt_lengths can\'t be None"")\n      mask = tf.sequence_mask(lengths=tgt_lengths - 1,\n                              maxlen=current_ts,\n                              dtype=tf.float32)\n    else:\n      mask = tf.cast(tf.ones_like(target_sequence), logits.dtype)\n\n    labels = tf.one_hot(indices=tf.reshape(target_sequence, shape=[-1]),\n                        depth=self._tgt_vocab_size)\n    logits = tf.reshape(logits, shape=[-1, self._tgt_vocab_size])\n    mask = tf.reshape(mask, shape=[-1])\n\n    loss = tf.losses.softmax_cross_entropy(\n        onehot_labels=labels,\n        logits=logits,\n        weights=mask,\n        label_smoothing=self._label_smoothing,\n        reduction=tf.losses.Reduction.NONE,\n    )\n\n    loss = tf.reduce_sum(loss * tf.reshape(mask, shape=[-1]))\n    if self._average_across_timestep:\n      loss /= tf.reduce_sum(mask)\n    else:\n      loss /= self._batch_size\n    return loss\n\n\nclass PaddedCrossEntropyLossWithSmoothing(Loss):\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Loss.get_optional_params(), **{\n        \'batch_size\': int,\n        \'tgt_vocab_size\': int,\n        \'label_smoothing\': float,\n        \'pad_embeddings_2_eight\': bool,\n    })\n\n  def __init__(self, params, model, name=""padded_cross_entropy_with_smoothing""):\n    super(PaddedCrossEntropyLossWithSmoothing, self).__init__(params, model,\n                                                              name)\n    if self.params.get(\'pad_embeddings_2_eight\', False):\n      if self.params[""tgt_vocab_size""] % 8 == 0:\n        self._tgt_vocab_size = self.params[""tgt_vocab_size""]\n      else:\n        self._tgt_vocab_size = self.params[""tgt_vocab_size""] + \\\n            (8 - self.params[""tgt_vocab_size""] % 8)\n    else:\n      self._tgt_vocab_size = self.params[""tgt_vocab_size""]\n    self._label_smoothing = self.params.get(""label_smoothing"", 0.0)\n\n  def _compute_loss(self, input_dict):\n    logits = input_dict[""decoder_output""][""logits""]\n    logits = tf.cast(logits, dtype=tf.float32)\n    if logits is None:\n      return 0.0\n    labels = input_dict[""target_tensors""][0]\n\n    def _pad_tensors_to_same_length(x, y):\n      """""" Pad x and y so that the results have the\n      same length (second dimension).\n      """"""\n      with tf.name_scope(""pad_to_same_length""):\n        x_length = tf.shape(x)[1]\n        y_length = tf.shape(y)[1]\n\n        max_length = tf.maximum(x_length, y_length)\n\n        x = tf.pad(x, [[0, 0], [0, max_length - x_length], [0, 0]])\n        y = tf.pad(y, [[0, 0], [0, max_length - y_length]])\n        return x, y\n\n    with tf.name_scope(name=""loss"", values= [logits, labels]):\n      logits, labels = _pad_tensors_to_same_length(logits, labels)\n\n      # Calculate smoothing cross entropy\n      with tf.name_scope(name=""smoothing_cross_entropy"", values=[logits, labels]):\n        confidence = 1.0 - self._label_smoothing\n        low_confidence = (1.0 - confidence) / \\\n            tf.cast(self._tgt_vocab_size - 1, dtype=tf.float32)\n        soft_targets = tf.one_hot(\n            tf.cast(labels, tf.int32),\n            depth=self._tgt_vocab_size,\n            on_value=confidence,\n            off_value=low_confidence,\n        )\n        xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=logits, labels=soft_targets,\n        )\n\n        # Calculate the best (lowest) possible value of cross entropy, and\n        # subtract from the cross entropy loss.\n        normalizing_constant = -(\n            confidence * tf.log(confidence) +\n            tf.cast(self._tgt_vocab_size - 1, dtype=tf.float32) *\n            low_confidence * tf.log(low_confidence + 1e-20)\n        )\n        xentropy -= normalizing_constant\n\n      weights = tf.cast(tf.not_equal(labels, 0), dtype=tf.float32)\n      xentropy = xentropy * weights\n      loss = tf.reduce_sum(xentropy * weights) / tf.reduce_sum(weights)\n\n      return loss\n\n\nclass BasicSampledSequenceLoss(Loss):\n  """"""\n  Basic sampled sequence-to-sequence loss. This is used when the full softmax\n  is computational prohibitive.\n  This one does not use one-hot encodings.\n  """"""\n  @staticmethod\n  def get_required_params():\n    return dict(Loss.get_required_params(), **{\n        \'tgt_vocab_size\': int,\n        \'batch_size\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Loss.get_optional_params(), **{\n        \'offset_target_by_one\': bool,\n        \'average_across_timestep\': bool,\n        \'do_mask\': bool,\n        \'hid_dim\': int,\n    })\n\n  def __init__(self, params, model, name=""basic_sampled_sequence_loss""):\n    """"""Constructor.\n\n    Args:\n      params (dict): dictionary with loss parameters.\n        Should contain the following:\n        * tgt_vocab_size: Target vocabulary size\n        * batch_size_per_gpu: Size of the per-worker batch\n        * offset_target_by_one: (default: True). Keep it true for\n        auto-regressive models\n        * average_across_timestep: (default: False). If True, will average\n          loss across timesteps, else it will sum across timesteps\n        * do_mask: (default: True) whether to mask based on tgt_lengths\n          (which is passed as part of loss_input_dict to compute_loss\n          and has to be not None then)\n    """"""\n    super(BasicSampledSequenceLoss, self).__init__(params, model, name)\n    self._tgt_vocab_size = self.params[""tgt_vocab_size""]\n    self._batch_size = self.params[""batch_size""]\n    self._offset_target_by_one = self.params.get(""offset_target_by_one"", True)\n    self._average_across_timestep = self.params.get(\n        ""average_across_timestep"", False)\n    self._do_mask = self.params.get(""do_mask"", True)\n\n  def _compute_loss(self, input_dict):\n    """"""Computes cross entropy based sequence-to-sequence loss.\n\n    Args:\n      input_dict (dict): inputs to compute loss::\n        {\n              ""logits"": logits tensor of shape [batch_size, T, dim]\n              ""target_sequence"": tensor of shape [batch_size, T]\n              ""tgt_lengths"": tensor of shape [batch_size] or None\n        }\n\n    Returns:\n       Singleton loss tensor\n    """"""\n    target_sequence = input_dict[\'target_tensors\'][0]\n    tgt_lengths = input_dict[\'target_tensors\'][1]\n\n    if \'weights\' in input_dict[\'decoder_output\']:\n      print(""Because \'weights\' is in the input_dict, we are using sampled softmax loss."")\n      inputs = input_dict[""decoder_output""][\'inputs\']\n      self._hid_dim = inputs.get_shape().as_list()[-1]\n      inputs = tf.reshape(inputs, (-1, self._hid_dim))\n      targets = tf.reshape(target_sequence, (-1, 1))\n\n      weights = input_dict[""decoder_output""][\'weights\']\n      biases = input_dict[""decoder_output""][\'bias\']\n      \n      if inputs.dtype.base_dtype != tf.float32:\n        inputs = tf.cast(inputs, tf.float32)\n      if weights.dtype.base_dtype != tf.float32:\n        weights = tf.cast(weights, tf.float32)\n      if biases.dtype.base_dtype != tf.float32:\n        biases = tf.cast(biases, tf.float32)\n      crossent = tf.nn.sampled_softmax_loss(weights, \n                                            biases, \n                                            targets, \n                                            inputs,\n                                            input_dict[\'decoder_output\'][\'num_sampled\'],\n                                            self._tgt_vocab_size)\n\n\n      if self._average_across_timestep:\n        loss = tf.reduce_mean(crossent)\n      else:\n        loss = tf.reduce_sum(crossent)\n        loss /= self._batch_size\n\n    else:\n      print(""Because \'weights\' is not in the input_dict, we are using normal softmax loss."")\n      logits = input_dict[""decoder_output""][""logits""]\n\n      if self._offset_target_by_one:\n        # this is necessary for auto-regressive models\n        current_ts = tf.cast(tf.minimum(\n            tf.shape(target_sequence)[1],\n            tf.shape(logits)[1],\n        ), tf.int32) - 1\n\n        logits = tf.slice(\n            logits,\n            begin=[0, 0, 0],\n            size=[-1, current_ts, -1],\n        )\n        target_sequence = tf.slice(target_sequence,\n                                   begin=[0, 1],\n                                   size=[-1, current_ts])\n      else:\n        current_ts = tf.cast(tf.minimum(\n            tf.shape(target_sequence)[1],\n            tf.shape(logits)[1],\n        ),tf.int32)\n\n      # Cast logits after potential slice\n      if logits.dtype.base_dtype != tf.float32:\n        logits = tf.cast(logits, tf.float32)\n\n      if self._do_mask:\n        if tgt_lengths is None:\n          raise ValueError(\n              ""If you are masking loss, tgt_lengths can\'t be None"")\n        mask = tf.sequence_mask(lengths=tgt_lengths - 1,\n                                maxlen=current_ts,\n                                dtype=logits.dtype)\n      else:\n        mask = tf.cast(tf.ones_like(target_sequence), logits.dtype)\n\n      crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n          labels=tf.reshape(target_sequence, shape=[-1]),\n          logits=tf.reshape(logits, shape=[-1, self._tgt_vocab_size]),\n      )\n\n      if self._average_across_timestep:\n        loss = tf.reduce_mean(crossent * tf.reshape(mask, shape=[-1]))\n      else:\n        loss = tf.reduce_sum(crossent * tf.reshape(mask, shape=[-1]))\n        loss /= self._batch_size\n    return loss\n'"
open_seq2seq/losses/sequence_loss_test.py,5,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom open_seq2seq.losses.sequence_loss import CrossEntropyWithSmoothing, \\\n  BasicSequenceLoss\n\n\nclass CrossEntropyWithSmoothingEqualsBasicSequenceLossTest(tf.test.TestCase):\n  def setUp(self):\n    print(""Setting Up  CrossEntropyWithSmoothingEqualsBasicSequenceLoss Test"")\n\n  def tearDown(self):\n    print(""Tear down  CrossEntropyWithSmoothingEqualsBasicSequenceLoss Test"")\n\n  def test_compute_loss(self):\n    seq_length = 13\n    tgt_vocab_size = 12\n\n    for offset in [0, 3, 4]:\n      for batch_size in [1, 4, 8]:\n        for _o in [True, False]:\n          for _m in [True, False]:\n            loss_params = {\n                ""do_mask"": _m,\n                ""tgt_vocab_size"": tgt_vocab_size,\n                ""batch_size"": batch_size,\n                ""offset_target_by_one"": _o,\n            }\n\n            targets = tf.placeholder(dtype=tf.int32, shape=[batch_size,\n                                                            seq_length])\n            logits = tf.placeholder(dtype=tf.float32, shape=[batch_size,\n                                                             seq_length,\n                                                             tgt_vocab_size])\n            tgt_lengths = tf.placeholder(dtype=tf.int32, shape=[batch_size])\n            xentropy = CrossEntropyWithSmoothing(params=loss_params, model=None)\n            sparse_xentropy = BasicSequenceLoss(params=loss_params, model=None)\n            loss_input_dict = {\n                ""decoder_output"": {""logits"": logits},\n                ""target_tensors"": [targets, tgt_lengths],\n            }\n            l1 = sparse_xentropy.compute_loss(input_dict=loss_input_dict)\n            l2 = xentropy.compute_loss(input_dict=loss_input_dict)\n            with self.test_session(use_gpu=True) as sess:\n              tgts = np.random.randint(tgt_vocab_size,\n                                       size=(batch_size, seq_length))\n              lgts = np.random.random(size=[batch_size,\n                                            seq_length, tgt_vocab_size])\n              feed_dict = {\n                  targets: tgts,\n                  logits: lgts,\n                  tgt_lengths: np.array([seq_length-offset]*batch_size)\n              }\n              loss1 = sess.run(l1, feed_dict=feed_dict)\n              loss2 = sess.run(l2, feed_dict=feed_dict)\n              self.assertAlmostEqual(loss1, loss2, 4)\n              print(""Loss: {}"".format(loss1))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
open_seq2seq/losses/text2speech_loss.py,40,"b'# Copyright (c) 2019 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom .loss import Loss\n\n\nclass Text2SpeechLoss(Loss):\n  """"""\n  Default text-to-speech loss.\n  """"""\n\n  @staticmethod\n  def get_optional_params():\n    return {\n        ""use_mask"": bool,\n        ""scale"": float,\n        ""stop_token_weight"": float,\n        ""mel_weight"": float,\n        ""mag_weight"": float,\n        ""l1_norm"": bool\n    }\n\n  def __init__(self, params, model, name=""text2speech_loss""):\n    super(Text2SpeechLoss, self).__init__(params, model, name)\n    self._n_feats = self._model.get_data_layer().params[""num_audio_features""]\n\n    if ""both"" in self._model.get_data_layer().params[""output_type""]:\n      self._both = True\n    else:\n      self._both = False\n\n  def _compute_loss(self, input_dict):\n    """"""\n    Computes loss for text-to-speech model.\n\n    Args:\n      input_dict (dict):\n        * ""decoder_output"": dictionary containing:\n            ""outputs"": array containing:\n                * mel: mel-spectrogram predicted by the decoder [batch, time, n_mel]\n                * post_net_mel: spectrogram after adding the residual\n                  corrections from the post net of shape [batch, time, feats]\n                * mag: mag-spectrogram predicted by the decoder [batch, time, n_mag]\n            ""stop_token_predictions"": stop_token predictions of shape [batch, time, 1]\n\n        * ""target_tensors"": array containing:\n            * spec: the true spectrogram of shape [batch, time, feats]\n            * stop_token: the stop_token of shape [batch, time]\n            * spec_length: the length of specs [batch]\n\n    Returns:\n      Singleton loss tensor\n    """"""\n\n    decoder_predictions = input_dict[""decoder_output""][""outputs""][0]\n    post_net_predictions = input_dict[""decoder_output""][""outputs""][1]\n    stop_token_predictions = input_dict[""decoder_output""][""stop_token_prediction""]\n\n    if self._both:\n      mag_pred = input_dict[""decoder_output""][""outputs""][5]\n      mag_pred = tf.cast(mag_pred, dtype=tf.float32)\n\n    spec = input_dict[""target_tensors""][0]\n    stop_token = input_dict[""target_tensors""][1]\n    stop_token = tf.expand_dims(stop_token, -1)\n    spec_lengths = input_dict[""target_tensors""][2]\n\n    batch_size = tf.shape(spec)[0]\n    num_feats = tf.shape(spec)[2]\n\n    decoder_predictions = tf.cast(decoder_predictions, dtype=tf.float32)\n    post_net_predictions = tf.cast(post_net_predictions, dtype=tf.float32)\n    stop_token_predictions = tf.cast(stop_token_predictions, dtype=tf.float32)\n    spec = tf.cast(spec, dtype=tf.float32)\n    stop_token = tf.cast(stop_token, dtype=tf.float32)\n\n    max_length = tf.cast(\n        tf.maximum(\n            tf.shape(spec)[1],\n            tf.shape(decoder_predictions)[1],\n        ), tf.int32\n    )\n\n    decoder_pad = tf.zeros(\n        [\n            batch_size,\n            max_length - tf.shape(decoder_predictions)[1],\n            tf.shape(decoder_predictions)[2]\n        ]\n    )\n    stop_token_pred_pad = tf.zeros(\n        [batch_size, max_length - tf.shape(decoder_predictions)[1], 1]\n    )\n    spec_pad = tf.zeros([batch_size, max_length - tf.shape(spec)[1], num_feats])\n    stop_token_pad = tf.ones([batch_size, max_length - tf.shape(spec)[1], 1])\n    decoder_predictions = tf.concat(\n        [decoder_predictions, decoder_pad],\n        axis=1\n    )\n    post_net_predictions = tf.concat(\n        [post_net_predictions, decoder_pad],\n        axis=1\n    )\n    stop_token_predictions = tf.concat(\n        [stop_token_predictions, stop_token_pred_pad],\n        axis=1\n    )\n    spec = tf.concat([spec, spec_pad], axis=1)\n    stop_token = tf.concat([stop_token, stop_token_pad], axis=1)\n\n    if self.params.get(""l1_norm"", False):\n      loss_f = tf.losses.absolute_difference\n    else:\n      loss_f = tf.losses.mean_squared_error\n\n    if self._both:\n      mag_pad = tf.zeros(\n          [\n              batch_size,\n              max_length - tf.shape(mag_pred)[1],\n              tf.shape(mag_pred)[2]\n          ]\n      )\n      mag_pred = tf.concat(\n          [mag_pred, mag_pad],\n          axis=1\n      )\n\n      spec, mag_target = tf.split(\n          spec,\n          [self._n_feats[""mel""], self._n_feats[""magnitude""]],\n          axis=2\n      )\n\n    decoder_target = spec\n    post_net_target = spec\n\n    if self.params.get(""use_mask"", True):\n      mask = tf.sequence_mask(\n          lengths=spec_lengths,\n          maxlen=max_length,\n          dtype=tf.float32\n      )\n      mask = tf.expand_dims(mask, axis=-1)\n\n      decoder_loss = loss_f(\n          labels=decoder_target,\n          predictions=decoder_predictions,\n          weights=mask\n      )\n      post_net_loss = loss_f(\n          labels=post_net_target,\n          predictions=post_net_predictions,\n          weights=mask\n      )\n\n      if self._both:\n        mag_loss = loss_f(\n            labels=mag_target,\n            predictions=mag_pred,\n            weights=mask\n        )\n\n      stop_token_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n          labels=stop_token,\n          logits=stop_token_predictions\n      )\n      stop_token_loss = stop_token_loss * mask\n      stop_token_loss = tf.reduce_sum(stop_token_loss) / tf.reduce_sum(mask)\n    else:\n      decoder_loss = loss_f(\n          labels=decoder_target,\n          predictions=decoder_predictions\n      )\n      post_net_loss = loss_f(\n          labels=post_net_target,\n          predictions=post_net_predictions\n      )\n      if self._both:\n        mag_loss = loss_f(\n            labels=mag_target,\n            predictions=mag_pred\n        )\n      stop_token_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n          labels=stop_token,\n          logits=stop_token_predictions\n      )\n      stop_token_loss = tf.reduce_mean(stop_token_loss)\n\n    mel_weight = self.params.get(""mel_weight"", 1.0)\n    decoder_loss = mel_weight * decoder_loss\n    post_net_loss = mel_weight * post_net_loss\n\n    stop_token_weight = self.params.get(""stop_token_weight"", 1.0)\n    stop_token_loss = stop_token_weight * stop_token_loss\n\n    loss = decoder_loss + post_net_loss + stop_token_loss\n\n    if self._both:\n      mag_weight = self.params.get(""mag_weight"", 1.0)\n      loss += mag_weight * mag_loss\n\n    if self.params.get(""scale"", None):\n      loss = loss * self.params[""scale""]\n\n    return loss\n'"
open_seq2seq/losses/wavenet_loss.py,3,"b'# Copyright (c) 2018 NVIDIA Corporation\n\nimport tensorflow as tf\n\nfrom .loss import Loss\n\nclass WavenetLoss(Loss):\n\n  def __init__(self, params, model, name=""wavenet_loss""):\n    super(WavenetLoss, self).__init__(params, model, name)\n    self._n_feats = self._model.get_data_layer().params[""num_audio_features""]\n\n  def get_required_params(self):\n    return {}\n\n  def get_optional_params(self):\n    return {}\n\n  def _compute_loss(self, input_dict):\n    """"""\n    Computes the cross-entropy loss for WaveNet.\n\n    Args:\n      input_dict (dict):\n        * ""decoder_output"": array containing: [\n          * logits: predicted output signal as logits\n          * outputs: array containing: [\n            * ground truth signal as encoded labels\n            * mu-law decoded audio\n          ]\n        ]\n    """"""\n\n    prediction = tf.cast(input_dict[""decoder_output""][""logits""], tf.float32)\n    target_output = input_dict[""decoder_output""][""outputs""][0]\n\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=prediction, \n        labels=target_output\n    )\n    loss = tf.reduce_mean(loss)\n\n    return loss\n'"
open_seq2seq/models/__init__.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\n""""""All base models available in OpenSeq2Seq.""""""\nfrom .model import Model\nfrom .text2text import Text2Text\nfrom .speech2text import Speech2Text\nfrom .image2label import Image2Label\nfrom .lstm_lm import LSTMLM\nfrom .text2speech_tacotron import Text2SpeechTacotron\nfrom .text2speech_wavenet import Text2SpeechWavenet\nfrom .text2speech_centaur import Text2SpeechCentaur\n'"
open_seq2seq/models/encoder_decoder.py,2,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.models.model import Model\nfrom open_seq2seq.utils.utils import deco_print\n\nclass EncoderDecoderModel(Model):\n  """"""\n  Standard encoder-decoder class with one encoder and one decoder.\n  ""encoder-decoder-loss"" models should inherit from this class.\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(Model.get_required_params(), **{\n        \'encoder\': None,  # could be any user defined class\n        \'decoder\': None,  # could be any user defined class\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(Model.get_optional_params(), **{\n        \'encoder_params\': dict,\n        \'decoder_params\': dict,\n        \'loss\': None,  # could be any user defined class\n        \'loss_params\': dict,\n    })\n\n  def __init__(self, params, mode=""train"", hvd=None):\n    """"""Encoder-decoder model constructor.\n    Note that TensorFlow graph should not be created here. All graph creation\n    logic is happening inside\n    :meth:`self._build_forward_pass_graph() <_build_forward_pass_graph>` method.\n\n    Args:\n      params (dict): parameters describing the model.\n          All supported parameters are listed in :meth:`get_required_params`,\n          :meth:`get_optional_params` functions.\n      mode (string, optional): ""train"", ""eval"" or ""infer"".\n          If mode is ""train"" all parts of the graph will be built\n          (model, loss, optimizer).\n          If mode is ""eval"", only model and loss will be built.\n          If mode is ""infer"", only model will be built.\n      hvd (optional): if Horovod is used, this should be\n          ``horovod.tensorflow`` module.\n          If Horovod is not used, it should be None.\n\n    Config parameters:\n\n    * **encoder** (any class derived from\n      :class:`Encoder <encoders.encoder.Encoder>`) --- encoder class to use.\n    * **encoder_params** (dict) --- dictionary with encoder configuration. For\n      complete list of possible parameters see the corresponding class docs.\n    * **decoder** (any class derived from\n      :class:`Decoder <decoders.decoder.Decoder>`) --- decoder class to use.\n    * **decoder_params** (dict) --- dictionary with decoder configuration. For\n      complete list of possible parameters see the corresponding class docs.\n    * **loss** (any class derived from\n      :class:`Loss <losses.loss.Loss>`) --- loss class to use.\n    * **loss_params** (dict) --- dictionary with loss configuration. For\n      complete list of possible parameters see the corresponding class docs.\n    """"""\n    super(EncoderDecoderModel, self).__init__(params=params, mode=mode, hvd=hvd)\n\n    if \'encoder_params\' not in self.params:\n      self.params[\'encoder_params\'] = {}\n    if \'decoder_params\' not in self.params:\n      self.params[\'decoder_params\'] = {}\n    if \'loss_params\' not in self.params:\n      self.params[\'loss_params\'] = {}\n\n    self._encoder = self._create_encoder()\n    self._decoder = self._create_decoder()\n    if self.mode == \'train\' or self.mode == \'eval\':\n      self._loss_computator = self._create_loss()\n    else:\n      self._loss_computator = None\n\n  def _create_encoder(self):\n    """"""This function should return encoder class.\n    Overwrite this function if additional parameters need to be specified for\n    encoder, besides provided in the config.\n\n    Returns:\n      instance of a class derived from :class:`encoders.encoder.Encoder`.\n    """"""\n    params = self.params[\'encoder_params\']\n    return self.params[\'encoder\'](params=params, mode=self.mode, model=self)\n\n  def _create_decoder(self):\n    """"""This function should return decoder class.\n    Overwrite this function if additional parameters need to be specified for\n    decoder, besides provided in the config.\n\n    Returns:\n      instance of a class derived from :class:`decoders.decoder.Decoder`.\n    """"""\n    params = self.params[\'decoder_params\']\n    return self.params[\'decoder\'](params=params, mode=self.mode, model=self)\n\n  def _create_loss(self):\n    """"""This function should return loss class.\n    Overwrite this function if additional parameters need to be specified for\n    loss, besides provided in the config.\n\n    Returns:\n      instance of a class derived from :class:`losses.loss.Loss`.\n    """"""\n    return self.params[\'loss\'](params=self.params[\'loss_params\'], model=self)\n\n  def _build_forward_pass_graph(self, input_tensors, gpu_id=0):\n    """"""TensorFlow graph for encoder-decoder-loss model is created here.\n    This function connects encoder, decoder and loss together. As an input for\n    encoder it will specify source tensors (as returned from\n    the data layer). As an input for decoder it will specify target tensors\n    as well as all output returned from encoder. For loss it\n    will also specify target tensors and all output returned from\n    decoder. Note that loss will only be built for mode == ""train"" or ""eval"".\n\n    Args:\n      input_tensors (dict): ``input_tensors`` dictionary that has to contain\n          ``source_tensors`` key with the list of all source tensors, and\n          ``target_tensors`` with the list of all target tensors. Note that\n          ``target_tensors`` only need to be provided if mode is\n          ""train"" or ""eval"".\n      gpu_id (int, optional): id of the GPU where the current copy of the model\n          is constructed. For Horovod this is always zero.\n\n    Returns:\n      tuple: tuple containing loss tensor as returned from\n      ``loss.compute_loss()`` and list of outputs tensors, which is taken from\n      ``decoder.decode()[\'outputs\']``. When ``mode == \'infer\'``, loss will\n      be None.\n    """"""\n    if not isinstance(input_tensors, dict) or \\\n       \'source_tensors\' not in input_tensors:\n      raise ValueError(\'Input tensors should be a dict containing \'\n                       \'""source_tensors"" key\')\n\n    if not isinstance(input_tensors[\'source_tensors\'], list):\n      raise ValueError(\'source_tensors should be a list\')\n\n    source_tensors = input_tensors[\'source_tensors\']\n    if self.mode == ""train"" or self.mode == ""eval"":\n      if \'target_tensors\' not in input_tensors:\n        raise ValueError(\'Input tensors should contain ""target_tensors"" key\'\n                         \'when mode != ""infer""\')\n      if not isinstance(input_tensors[\'target_tensors\'], list):\n        raise ValueError(\'target_tensors should be a list\')\n      target_tensors = input_tensors[\'target_tensors\']\n\n    with tf.variable_scope(""ForwardPass""):\n      encoder_input = {""source_tensors"": source_tensors}\n      encoder_output = self.encoder.encode(input_dict=encoder_input)\n\n      decoder_input = {""encoder_output"": encoder_output}\n      if self.mode == ""train"" or self.mode == ""eval"":\n        decoder_input[\'target_tensors\'] = target_tensors\n      decoder_output = self.decoder.decode(input_dict=decoder_input)\n      model_outputs = decoder_output.get(""outputs"", None)\n\n      if self.mode == ""train"" or self.mode == ""eval"":\n        with tf.variable_scope(""Loss""):\n          loss_input_dict = {\n              ""decoder_output"": decoder_output,\n              ""target_tensors"": target_tensors,\n          }\n          loss = self.loss_computator.compute_loss(loss_input_dict)\n      else:\n        deco_print(""Inference Mode. Loss part of graph isn\'t built."")\n        loss = None\n      return loss, model_outputs\n\n  @property\n  def encoder(self):\n    """"""Model encoder.""""""\n    return self._encoder\n\n  @property\n  def decoder(self):\n    """"""Model decoder.""""""\n    return self._decoder\n\n  @property\n  def loss_computator(self):\n    """"""Model loss computator.""""""\n    return self._loss_computator\n'"
open_seq2seq/models/image2label.py,1,"b'# Copyright (c) 2018 NVIDIA Corporation\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom open_seq2seq.utils.utils import deco_print\nfrom .encoder_decoder import EncoderDecoderModel\n\n\nclass Image2Label(EncoderDecoderModel):\n  def maybe_print_logs(self, input_values, output_values, training_step):\n    labels = input_values[\'target_tensors\'][0]\n    logits = output_values[0]\n\n    labels = np.where(labels == 1)[1]\n\n    total = logits.shape[0]\n    top1 = np.sum(np.argmax(logits, axis=1) == labels)\n    top5 = np.sum(labels[:, np.newaxis] == np.argpartition(logits, -5)[:, -5:])\n\n    top1 = 1.0 * top1 / total\n    top5 = 1.0 * top5 / total\n    deco_print(""Train batch top-1: {:.4f}"".format(top1), offset=4)\n    deco_print(""Train batch top-5: {:.4f}"".format(top5), offset=4)\n    return {\n        ""Train batch top-1"": top1,\n        ""Train batch top-5"": top5,\n    }\n\n  def finalize_evaluation(self, results_per_batch, training_step=None):\n    top1 = 0.0\n    top5 = 0.0\n    total = 0.0\n\n    for cur_total, cur_top1, cur_top5 in results_per_batch:\n      top1 += cur_top1\n      top5 += cur_top5\n      total += cur_total\n\n    top1 = 1.0 * top1 / total\n    top5 = 1.0 * top5 / total\n    deco_print(""Validation top-1: {:.4f}"".format(top1), offset=4)\n    deco_print(""Validation top-5: {:.4f}"".format(top5), offset=4)\n    return {\n        ""Eval top-1"": top1,\n        ""Eval top-5"": top5,\n    }\n\n  def evaluate(self, input_values, output_values):\n    logits = output_values[0]\n    labels = input_values[\'target_tensors\'][0]\n    labels = np.where(labels == 1)[1]\n\n    total = logits.shape[0]\n    top1 = np.sum(np.equal(np.argmax(logits, axis=1), labels))\n    top5 = np.sum(np.equal(labels[:, np.newaxis],\n                           np.argpartition(logits, -5)[:, -5:]))\n    return total, top1, top5\n\n  def _get_num_objects_per_step(self, worker_id=0):\n    """"""Returns number of images in current batch, i.e. batch size.""""""\n    data_layer = self.get_data_layer(worker_id)\n    num_images = tf.shape(data_layer.input_tensors[\'source_tensors\'][0])[0]\n    return num_images\n'"
open_seq2seq/models/lstm_lm.py,3,"b'import random\nimport numpy as np\nimport tensorflow as tf\n\nfrom .encoder_decoder import EncoderDecoderModel\nfrom open_seq2seq.data import WKTDataLayer\nfrom open_seq2seq.utils.utils import deco_print, array_to_string\nfrom open_seq2seq.utils import metrics\n\nclass LSTMLM(EncoderDecoderModel):\n  """"""\n  An example class implementing an LSTM language model.\n  """"""\n  def __init__(self, params, mode=""train"", hvd=None):\n    super(EncoderDecoderModel, self).__init__(params=params, mode=mode, hvd=hvd)\n\n    if \'encoder_params\' not in self.params:\n      self.params[\'encoder_params\'] = {}\n    if \'decoder_params\' not in self.params:\n      self.params[\'decoder_params\'] = {}\n    if \'loss_params\' not in self.params:\n      self.params[\'loss_params\'] = {}\n\n    self._lm_phase = isinstance(self.get_data_layer(), WKTDataLayer)\n\n    self._encoder = self._create_encoder()\n    self._decoder = self._create_decoder()\n    if self.mode == \'train\' or self.mode == \'eval\':\n      self._loss_computator = self._create_loss()\n    else:\n      self._loss_computator = None\n\n    self.delimiter = self.get_data_layer().delimiter\n\n  def _create_encoder(self):\n    self._print_f1 = False\n    self.params[\'encoder_params\'][\'vocab_size\'] = (\n      self.get_data_layer().vocab_size\n    )\n    self.params[\'encoder_params\'][\'end_token\'] = (\n      self.get_data_layer().end_token\n    )\n    self.params[\'encoder_params\'][\'batch_size\'] = (\n      self.get_data_layer().batch_size\n    )\n    if not self._lm_phase:\n      self.params[\'encoder_params\'][\'fc_dim\'] = (\n        self.get_data_layer().num_classes\n      )\n      if self.params[\'encoder_params\'][\'fc_dim\'] == 2:\n        self._print_f1 = True\n    if self._lm_phase:\n      self.params[\'encoder_params\'][\'seed_tokens\'] = (\n        self.get_data_layer().params[\'seed_tokens\']\n      )\n    return super(LSTMLM, self)._create_encoder()\n\n  def _create_loss(self):\n    if self._lm_phase:\n      self.params[\'loss_params\'][\'batch_size\'] = (\n        self.get_data_layer().batch_size\n      )\n      self.params[\'loss_params\'][\'tgt_vocab_size\'] = (\n        self.get_data_layer().vocab_size\n      )\n\n    return super(LSTMLM, self)._create_loss()\n\n\n  def infer(self, input_values, output_values):\n    if self._lm_phase:\n      vocab = self.get_data_layer().corp.dictionary.idx2word\n      seed_tokens = self.params[\'encoder_params\'][\'seed_tokens\']\n      for i in range(len(seed_tokens)):\n        print(\'Seed:\', vocab[seed_tokens[i]] + \'\\n\')\n        deco_print(\n          ""Output: "" + array_to_string(\n            output_values[0][i],\n            vocab=self.get_data_layer().corp.dictionary.idx2word,\n            delim=self.delimiter,\n          ),\n          offset=4,\n        )\n      return []\n    else:\n      ex, elen_x = input_values[\'source_tensors\']\n      ey, elen_y = None, None\n      if \'target_tensors\' in input_values:\n        ey, elen_y = input_values[\'target_tensors\']\n\n      n_samples = len(ex)\n      results = []\n      for i in range(n_samples):\n        current_x = array_to_string(\n          ex[i][:elen_x[i]],\n          vocab=self.get_data_layer().corp.dictionary.idx2word,\n          delim=self.delimiter,\n        ),\n        current_pred = np.argmax(output_values[0][i])\n        curret_y = None\n        if ey is not None:\n          current_y = np.argmax(ey[i])\n\n        results.append((current_x[0], current_pred, current_y))\n      return results\n  \n\n  def maybe_print_logs(self, input_values, output_values, training_step):\n    x, len_x = input_values[\'source_tensors\']\n    y, len_y = input_values[\'target_tensors\']    \n\n    x_sample = x[0]\n    len_x_sample = len_x[0]\n    y_sample = y[0]\n    len_y_sample = len_y[0]\n\n    deco_print(\n      ""Train Source[0]:     "" + array_to_string(\n        x_sample[:len_x_sample],\n        vocab=self.get_data_layer().corp.dictionary.idx2word,\n        delim=self.delimiter,\n      ),\n      offset=4,\n    )\n\n    if self._lm_phase:\n      deco_print(\n        ""Train Target[0]:     "" + array_to_string(\n          y_sample[:len_y_sample],\n          vocab=self.get_data_layer().corp.dictionary.idx2word,\n          delim=self.delimiter,\n        ),\n        offset=4,\n      )\n    else:\n      deco_print(\n        ""TRAIN Target[0]:     "" + str(np.argmax(y_sample)),\n        offset=4,\n      )\n      samples = output_values[0][0]\n      deco_print(\n        ""TRAIN Prediction[0]:     "" + str(samples),\n        offset=4,\n      )\n      labels = np.argmax(y, 1)\n      preds = np.argmax(output_values[0], axis=-1)\n      print(\'Labels\', labels)\n      print(\'Preds\', preds)\n\n      deco_print(\n        ""Accuracy: {:.4f}"".format(metrics.accuracy(labels, preds)),\n        offset = 4,\n      )\n\n      if self._print_f1:\n        deco_print(\n          ""Precision: {:.4f} | Recall: {:.4f} | F1: {:.4f}""\n              .format(metrics.precision(labels, preds), \n                      metrics.recall(labels, preds),\n                      metrics.f1(labels, preds)),\n          offset = 4,\n        )\n\n    return {}\n\n  def evaluate(self, input_values, output_values):\n    ex, elen_x = input_values[\'source_tensors\']\n    ey, elen_y = input_values[\'target_tensors\']\n\n    x_sample = ex[0]\n    len_x_sample = elen_x[0]\n    y_sample = ey[0]\n    len_y_sample = elen_y[0]\n    \n    return_values = {}\n    \n    if self._lm_phase:\n      flip = random.random()\n      if flip <= 0.9:\n        return return_values\n\n      deco_print(\n        ""*****EVAL Source[0]:     "" + array_to_string(\n          x_sample[:len_x_sample],\n          vocab=self.get_data_layer().corp.dictionary.idx2word,\n          delim=self.delimiter,\n        ),\n        offset=4,\n      )\n      samples = np.argmax(output_values[0][0], axis=-1)\n      deco_print(\n        ""*****EVAL Target[0]:     "" + array_to_string(\n          y_sample[:len_y_sample],\n          vocab=self.get_data_layer().corp.dictionary.idx2word,\n          delim=self.delimiter,\n        ),\n        offset=4,\n      )\n    \n      deco_print(\n        ""*****EVAL Prediction[0]: "" + array_to_string(\n          samples,\n          vocab=self.get_data_layer().corp.dictionary.idx2word,\n          delim=self.delimiter,\n        ),\n        offset=4,\n      )\n    else:\n      deco_print(\n        ""*****EVAL Source[0]:     "" + array_to_string(\n          x_sample[:len_x_sample],\n          vocab=self.get_data_layer().corp.dictionary.idx2word,\n          delim=self.delimiter,\n        ),\n        offset=4,\n      )\n      samples = output_values[0][0]\n      deco_print(\n        ""EVAL Target[0]:     "" + str(np.argmax(y_sample)),\n        offset=4,\n      )\n      deco_print(\n        ""EVAL Prediction[0]:     "" + str(samples),\n        offset=4,\n      )\n\n      labels = np.argmax(ey, 1)\n      preds = np.argmax(output_values[0], axis=-1)\n      print(\'Labels\', labels)\n      print(\'Preds\', preds)\n\n      return_values[\'accuracy\'] = metrics.accuracy(labels, preds)\n\n      if self._print_f1:\n        return_values[\'true_pos\'] = metrics.true_positives(labels, preds)\n        return_values[\'pred_pos\'] = np.sum(preds)\n        return_values[\'actual_pos\'] = np.sum(labels)\n\n    return return_values\n\n  def finalize_evaluation(self, results_per_batch, training_step=None):\n    accuracies = []\n    true_pos, pred_pos, actual_pos = 0.0, 0.0, 0.0\n\n    for results in results_per_batch:\n      if not \'accuracy\' in results:\n        return {}\n      accuracies.append(results[\'accuracy\'])\n      if \'true_pos\' in results:\n        true_pos += results[\'true_pos\']\n        pred_pos += results[\'pred_pos\']\n        actual_pos += results[\'actual_pos\']\n\n    deco_print(\n      ""EVAL Accuracy: {:.4f}"".format(np.mean(accuracies)),\n      offset = 4,\n    )\n\n    if true_pos > 0:\n      prec = true_pos / pred_pos\n      rec = true_pos / actual_pos\n      f1 = 2.0 * prec * rec / (rec + prec)\n      deco_print(\n        ""EVAL Precision: {:.4f} | Recall: {:.4f} | F1: {:.4f} | True pos: {}""\n            .format(prec, rec, f1, true_pos),\n        offset = 4,\n      )\n    return {}\n\n  def finalize_inference(self, results_per_batch, output_file):\n    out = open(output_file, \'w\')\n    out.write(\'\\t\'.join([\'Source\', \'Pred\', \'Label\']) + \'\\n\')\n    preds, labels = [], []\n\n    for results in results_per_batch:\n      for x, pred, y in results:\n        out.write(\'\\t\'.join([x, str(pred), str(y)]) + \'\\n\')\n        preds.append(pred)\n        labels.append(y)\n\n    if len(labels) > 0 and labels[0] is not None:\n      preds = np.asarray(preds)\n      labels = np.asarray(labels)\n      deco_print(\n        ""TEST Accuracy: {:.4f}"".format(metrics.accuracy(labels, preds)),\n        offset = 4,\n      )\n      deco_print(\n        ""TEST Precision: {:.4f} | Recall: {:.4f} | F1: {:.4f}""\n            .format(metrics.precision(labels, preds), \n                    metrics.recall(labels, preds),\n                    metrics.f1(labels, preds)),\n        offset = 4,\n      )\n    return {}\n\n  def _get_num_objects_per_step(self, worker_id=0):\n    """"""Returns number of source tokens + number of target tokens in batch.""""""\n    data_layer = self.get_data_layer(worker_id)\n    # sum of source length in batch\n    num_tokens = tf.reduce_sum(data_layer.input_tensors[\'source_tensors\'][1])\n    if self.mode != ""infer"":\n      # sum of target length in batch\n      num_tokens += tf.reduce_sum(data_layer.input_tensors[\'target_tensors\'][1])\n    else:\n      # TODO: this is not going to be correct when batch size > 1, since it will\n      #       count padding?\n      num_tokens += tf.reduce_sum(tf.shape(self.get_output_tensors(worker_id)[0]))\n    return num_tokens\n'"
open_seq2seq/models/model.py,33,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nimport abc\nimport six\nimport tensorflow as tf\nimport numpy as np\nimport copy\nimport time\nimport re\n\ntry:\n  from inspect import signature\nexcept ImportError:\n  from funcsigs import signature\n\nfrom open_seq2seq.utils.utils import deco_print, clip_last_batch\nfrom open_seq2seq.optimizers import optimize_loss, get_regularization_loss\nfrom open_seq2seq.utils.utils import check_params\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Model:\n  """"""Abstract class that any model should inherit from.\n  It automatically enables multi-GPU (or Horovod) computation,\n  has mixed precision support, logs training summaries, etc.\n  """"""\n  @staticmethod\n  def get_required_params():\n    """"""Static method with description of required parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **have to** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {\n        \'use_horovod\': bool,\n        \'batch_size_per_gpu\': int,\n        \'data_layer\': None,  # could be any user defined class\n    }\n\n  @staticmethod\n  def get_optional_params():\n    """"""Static method with description of optional parameters.\n\n      Returns:\n        dict:\n            Dictionary containing all the parameters that **can** be\n            included into the ``params`` parameter of the\n            class :meth:`__init__` method.\n    """"""\n    return {\n        \'logdir\': str,\n        \'num_gpus\': int,  # cannot be used when gpu_ids is specified\n        \'gpu_ids\': list,  # cannot be used when num_gpus is specified\n\n        \'load_model\': str,\n\n        \'save_summaries_steps\': None,  # could be int or None\n        \'print_loss_steps\': None,  # could be int or None\n        \'print_samples_steps\': None,  # could be int or None\n        \'print_bench_info_steps\': None,  # could be int or None\n        \'save_checkpoint_steps\': None,  # could be int or None\n        \'num_checkpoints\': int,  # maximum number of last checkpoints to keep\n        \'restore_best_checkpoint\': bool, # if True,restore best check point instead of latest checkpoint\n        \'eval_steps\': int,\n        \'finetune\': bool,\n        \'eval_batch_size_per_gpu\': int,\n        \'hooks\': list,\n\n        \'random_seed\': int,\n        \'num_epochs\': int,\n        \'max_steps\': int,\n        \'bench_start\': int,\n\n        \'data_layer_params\': dict,\n        \'optimizer\': None,  # could be class or string\n        \'optimizer_params\': dict,\n        \'freeze_variables_regex\' : None, # could be str or None\n        \'initializer\': None,  # any valid TensorFlow initializer\n        \'initializer_params\': dict,\n        \'regularizer\': None,  # any valid TensorFlow regularizer\n        \'regularizer_params\': dict,\n        \'dtype\': [tf.float16, tf.float32, \'mixed\'],\n        \'lr_policy\': None,  # any valid learning rate policy function\n        \'lr_policy_params\': dict,\n        \'max_grad_norm\': float,\n        \'larc_params\': dict,\n        \'loss_scaling\': None,  # float, ""Backoff"" or ""LogMax""\n        \'loss_scaling_params\': dict,\n        \'summaries\': list,\n        \'iter_size\': int,\n        \'lm_vocab_file\': str, #TODO: move this paramters to lstm_lm.py\n        \'processed_data_folder\': str,\n\n        # Parameters for TensorRT (infer mode only)\n        \'use_trt\': bool,\n        \'trt_precision_mode\': str,\n        \'trt_max_workspace_size_bytes\': int,\n        \'trt_minimum_segment_size\': int,\n        \'trt_is_dynamic_op\': bool,\n        \'trt_maximum_cached_engines\': int,\n\n        # Parameters for XLA\n        \'use_xla_jit\' : bool,\n    }\n\n  def __init__(self, params, mode=""train"", hvd=None):\n    """"""Model constructor.\n    The TensorFlow graph should not be created here, but rather in the\n    :meth:`self.compile() <compile>` method.\n\n    Args:\n      params (dict): parameters describing the model.\n          All supported parameters are listed in :meth:`get_required_params`,\n          :meth:`get_optional_params` functions.\n      mode (string, optional): ""train"", ""eval"" or ""infer"".\n          If mode is ""train"" all parts of the graph will be built\n          (model, loss, optimizer).\n          If mode is ""eval"", only model and loss will be built.\n          If mode is ""infer"", only model will be built.\n      hvd (optional): if Horovod is used, this should be\n          ``horovod.tensorflow`` module.\n          If Horovod is not used, it should be None.\n\n    Config parameters:\n\n    * **random_seed** (int) --- random seed to use.\n    * **use_horovod** (bool) --- whether to use Horovod for distributed\n      execution.\n    * **num_gpus** (int) --- number of GPUs to use. This parameter cannot be\n      used if ``gpu_ids`` is specified. When ``use_horovod`` is True\n      this parameter is ignored.\n    * **gpu_ids** (list of ints) --- GPU ids to use. This parameter cannot be\n      used if ``num_gpus`` is specified. When ``use_horovod`` is True\n      this parameter is ignored.\n    * **batch_size_per_gpu** (int) --- batch size to use for each GPU.\n    * **eval_batch_size_per_gpu** (int) --- batch size to use for each GPU during\n      inference. This is for when training and inference have different computation\n      and memory requirements, such as when training uses sampled softmax and\n      inference uses full softmax. If not specified, it\'s set\n      to ``batch_size_per_gpu``.\n    * **restore_best_checkpoint** (bool) --- if set to True, when doing evaluation \n      and inference, the model will load the best checkpoint instead of the latest\n      checkpoint. Best checkpoint is evaluated based on evaluation results, so \n      it\'s only available when the model is trained untder ``train_eval`` mode.\n      Default to False.\n    * **load_model** (str) --- points to the location of the pretrained model for\n      transfer learning. If specified, during training, the system will look\n      into the checkpoint in this folder and restore all variables whose names and \n      shapes match a variable in the new model.\n    * **num_epochs** (int) --- number of epochs to run training for.\n      This parameter cannot be used if ``max_steps`` is specified.\n    * **max_steps** (int) --- number of steps to run training for.\n      This parameter cannot be used if ``num_epochs`` is specified.\n    * **save_summaries_steps** (int or None) --- how often to save summaries.\n      Setting it to None disables summaries saving.\n    * **print_loss_steps** (int or None) --- how often to print loss during\n      training. Setting it to None disables loss printing.\n    * **print_samples_steps** (int or None) --- how often to print training\n      samples (input sequences, correct answers and model predictions).\n      Setting it to None disables samples printing.\n    * **print_bench_info_steps** (int or None) --- how often to print training\n      benchmarking information (average number of objects processed per step).\n      Setting it to None disables intermediate benchmarking printing, but\n      the average information across the whole training will always be printed\n      after the last iteration.\n    * **save_checkpoint_steps** (int or None) --- how often to save model\n      checkpoints. Setting it to None disables checkpoint saving.\n    * **num_checkpoints** (int) --- number of last checkpoints to keep.\n    * **eval_steps** (int) --- how often to run evaluation during training.\n      This parameter is only checked if ``--mode`` argument of ``run.py`` is\n      ""train\\_eval"". If no evaluation is needed you should use ""train"" mode.\n    * **logdir** (string) --- path to the log directory where all checkpoints\n      and summaries will be saved.\n    * **data_layer** (any class derived from\n      :class:`DataLayer <data.data_layer.DataLayer>`) --- data layer class\n      to use.\n    * **data_layer_params** (dict) --- dictionary with data layer\n      configuration.\n      For complete list of possible parameters see the corresponding\n      class docs.\n    * **optimizer** (string or TensorFlow optimizer class) --- optimizer to\n      use for training. Could be either ""Adam"", ""Adagrad"", ""Ftrl"", ""Momentum"",\n      ""RMSProp"", ""SGD"" or any valid TensorFlow optimizer class.\n    * **optimizer_params** (dict) --- dictionary that will be passed to\n      optimizer ``__init__`` method.\n    * **initializer** --- any valid TensorFlow initializer.\n    * **initializer_params** (dict) --- dictionary that will be passed to\n      initializer ``__init__`` method.\n    * **freeze_variables_regex** (str or None) --- if zero or more characters\n      at the beginning of the name of a trainable variable match this\n      pattern, then this variable will be frozen during training.\n      Setting it to None disables freezing of variables.\n    * **regularizer** --- and valid TensorFlow regularizer.\n    * **regularizer_params** (dict) --- dictionary that will be passed to\n      regularizer ``__init__`` method.\n    * **dtype** --- model dtype. Could be either ``tf.float16``,\n      ``tf.float32`` or ""mixed"". For details see\n      :ref:`mixed precision training <mixed_precision>` section in docs.\n    * **lr_policy** --- any valid learning rate policy function. For examples,\n      see :any:`optimizers.lr_policies` module.\n    * **lr_policy_params** (dict) --- dictionary containing lr_policy\n      parameters.\n    * **max_grad_norm** (float) --- maximum value of gradient norm. Clipping\n      will be performed if some gradients exceed this value (this is checked\n      for each variable independently).\n    * **loss_scaling** --- could be float or string. If float, static loss\n      scaling is applied. If string, the corresponding automatic\n      loss scaling algorithm is used. Must be one of \'Backoff\'\n      of \'LogMax\' (case insensitive). Only used when dtype=""mixed"". For details\n      see :ref:`mixed precision training <mixed_precision>` section in docs.\n    * **loss_scaling_params** (dict) --- dictionary containing loss scaling\n      parameters.\n    * **summaries** (list) --- which summaries to log. Could contain\n      ""learning_rate"", ""gradients"", ""gradient_norm"", ""global_gradient_norm"",\n      ""variables"", ""variable_norm"", ""loss_scale"".\n    * **iter_size** (int) --- use this parameter to emulate large batches.\n      The gradients will be accumulated for ``iter_size`` number of steps before\n      applying update.\n    * **larc_params** --- dictionary with parameters for LARC (or LARS)\n      optimization algorithms. Can contain the following parameters:\n\n      * **larc_mode** --- Could be either ""scale"" (LARS) or ""clip"" (LARC).\n        Note that it works in addition to any other optimization algorithm\n        since we treat\n        it as adaptive gradient clipping and learning rate adjustment.\n      * **larc_eta** (float) --- LARC or LARS scaling parameter.\n      * **min_update** (float) --- minimal value of the LARC (LARS) update.\n      * **epsilon** (float) --- small number added to gradient norm in\n        denominator for numerical stability.\n    """"""\n    check_params(params, self.get_required_params(), self.get_optional_params())\n\n    self._params = copy.deepcopy(params)\n\n    if self._params.get(\'iter_size\', 1) > 1 and hvd is None:\n      raise ValueError(""iter_size is only supported in Horovod mode"")\n\n    # parameter checks\n    self._mode = mode\n    self._interactive = False\n    if self._mode == ""interactive_infer"":\n      self._mode = ""infer""\n      self._interactive = True\n\n    if self._mode not in [""train"", ""infer"", ""eval""]:\n      raise ValueError(""Mode has to be one of [\'train\', \'infer\', \'eval\']"")\n\n    if ""use_trt"" in params and self._mode != ""infer"":\n      raise ValueError(""TensorRT can only be used in inference mode."")\n\n    if ""max_steps"" in params and ""num_epochs"" in params:\n      raise ValueError(""You can\'t provide both max_steps and num_epochs. ""\n                       ""Please, remove one of them from the config."")\n    if mode == ""train"":\n      if ""max_steps"" not in params and ""num_epochs"" not in params:\n        raise ValueError(""For training mode either max_steps or ""\n                         ""num_epochs has to be provided"")\n\n    if \'print_samples_steps\' not in self._params:\n      self._params[\'print_samples_steps\'] = None\n    if \'print_loss_steps\' not in self._params:\n      self._params[\'print_loss_steps\'] = None\n    if \'save_checkpoint_steps\' not in self._params:\n      self._params[\'save_checkpoint_steps\'] = None\n    if \'save_summaries_steps\' not in self._params:\n      self._params[\'save_summaries_steps\'] = None\n    if \'print_bench_info_steps\' not in self._params:\n      self._params[\'print_bench_info_steps\'] = None\n\n    self._params[\'num_checkpoints\'] = self._params.get(\'num_checkpoints\', 5)\n    self._params[\'finetune\'] = self._params.get(\'finetune\', False)\n    # self._params[\'base_logdir\'] = self._params.get(\'base_logdir\', None)\n    self._params[\'load_model\'] = self._params.get(\'load_model\', None)\n    self._params[\'load_fc\'] = self._params.get(\'load_fc\', False)\n    self._params[\'eval_batch_size_per_gpu\'] = self._params.get(\n        \'eval_batch_size_per_gpu\',\n        self._params[\'batch_size_per_gpu\']\n    )\n\n    # checking that frequencies of samples and loss are aligned\n    s_fr = self._params[\'print_samples_steps\']\n    l_fr = self._params[\'print_loss_steps\']\n    if s_fr is not None and l_fr is not None and s_fr % l_fr != 0:\n      raise ValueError(""print_samples_steps has to be a multiple of ""\n                       ""print_loss_steps."")\n\n    self._hvd = hvd\n    if self._hvd:\n      self._gpu_ids = range(1)\n    else:\n      if \'gpu_ids\' in self._params:\n        self._gpu_ids = self._params[\'gpu_ids\']\n      elif \'num_gpus\' in self._params:\n        self._gpu_ids = range(self._params[\'num_gpus\'])\n      else:\n        raise ValueError(\'Either ""gpu_ids"" or ""num_gpus"" has to \'\n                         \'be specified in the config\')\n\n    if self._interactive and len(self._gpu_ids) > 1:\n      raise ValueError(""Interactive infer is meant to be used with 1 gpu"")\n\n    # setting random seed\n    rs = self._params.get(\'random_seed\', int(time.time()))\n    if self.on_horovod:\n      rs += hvd.rank()\n    tf.set_random_seed(rs)\n    np.random.seed(rs)\n\n    if \'dtype\' not in self._params:\n      self._params[\'dtype\'] = tf.float32\n\n    dl_params = self._params.get(\'data_layer_params\', {})\n    if mode == \'train\':\n      dl_params[\'batch_size\'] = self._params[\'batch_size_per_gpu\']\n    else:\n      dl_params[\'batch_size\'] = self._params[\'eval_batch_size_per_gpu\']\n    if \'lm_vocab_file\' in self._params:\n      dl_params[\'lm_vocab_file\'] = self._params[\'lm_vocab_file\']\n    if \'processed_data_folder\' in self._params:\n      dl_params[\'processed_data_folder\'] = self._params[\'processed_data_folder\']\n    dl_params[\'mode\'] = self._mode\n    dl_params[\'interactive\'] = self._interactive\n\n\n    if self.on_horovod:\n      self._data_layer = self._params[\'data_layer\'](\n          params=dl_params, model=self,\n          num_workers=self._hvd.size(), worker_id=self._hvd.rank(),\n      )\n    else:\n      self._data_layers = []\n      for worker_id in range(self.num_gpus):\n        self._data_layers.append(self._params[\'data_layer\'](\n            params=dl_params, model=self,\n            num_workers=self.num_gpus, worker_id=worker_id,\n        ))\n\n    if self._mode == ""train"":\n      if ""max_steps"" in self._params:\n        self._last_step = self._params[""max_steps""]\n        self._steps_in_epoch = None\n      else:\n        # doing a few less steps if data size is not divisible by the batch size\n        self._steps_in_epoch = self.get_data_layer().get_size_in_samples() // \\\n                               self.get_data_layer().params[\'batch_size\']\n        if self._steps_in_epoch is None:\n          raise ValueError(\'The data_layer is not compatible with \'\n                           \'epoch execution, since it does not provide \'\n                           \'get_size_in_samples() method. Either update the \'\n                           \'data layer or switch to using ""max_steps"" \'\n                           \'paremeter.\')\n        if self.on_horovod:\n          self._steps_in_epoch //= self._hvd.size()\n        else:\n          self._steps_in_epoch //= self.num_gpus\n        self._steps_in_epoch //= self._params.get(\'iter_size\', 1)\n        if self._steps_in_epoch == 0:\n          raise ValueError(""Overall batch size is too big for this dataset."")\n        self._last_step = self._params[\'num_epochs\'] * self._steps_in_epoch\n\n    if self.on_horovod:\n      self._output = None\n    else:\n      self._outputs = [None] * self.num_gpus\n\n    self.loss = None\n    self.train_op = None\n    self.eval_losses = None\n    self._num_objects_per_step = None\n    self.skip_update_ph = None\n\n  def compile(self, force_var_reuse=False, checkpoint=None):\n    """"""TensorFlow graph is built here.""""""\n    if \'initializer\' not in self.params:\n      initializer = None\n    else:\n      init_dict = self.params.get(\'initializer_params\', {})\n      initializer = self.params[\'initializer\'](**init_dict)\n\n    if not self.on_horovod:  # not using Horovod\n      # below we follow data parallelism for multi-GPU training\n      losses = []\n      for gpu_cnt, gpu_id in enumerate(self._gpu_ids):\n        with tf.device(""/gpu:{}"".format(gpu_id)), tf.variable_scope(\n            name_or_scope=tf.get_variable_scope(),\n            # re-using variables across GPUs.\n            reuse=force_var_reuse or (gpu_cnt > 0),\n            initializer=initializer,\n            dtype=self.get_tf_dtype(),\n        ):\n          deco_print(""Building graph on GPU:{}"".format(gpu_id))\n\n          if self._interactive:\n            self.get_data_layer(gpu_cnt).create_interactive_placeholders()\n          else:\n            self.get_data_layer(gpu_cnt).build_graph()\n          input_tensors = self.get_data_layer(gpu_cnt).input_tensors\n          if self.params.get(""use_trt"", False):\n            # Build TF-TRT graph\n            loss, self._outputs[gpu_cnt] = self.build_trt_forward_pass_graph(\n                input_tensors,\n                gpu_id=gpu_cnt,\n                checkpoint=checkpoint\n            )\n          else:\n            # Build regular TF graph\n            loss, self._outputs[gpu_cnt] = self._build_forward_pass_graph(\n                input_tensors,\n                gpu_id=gpu_cnt\n            )\n          if self._outputs[gpu_cnt] is not None and \\\n             not isinstance(self._outputs[gpu_cnt], list):\n            raise ValueError(\'Decoder outputs have to be either None or list\')\n          if self._mode == ""train"" or self._mode == ""eval"":\n            losses.append(loss)\n                \n      # end of for gpu_ind loop\n      if self._mode == ""train"":\n        self.loss = tf.reduce_mean(losses)\n      if self._mode == ""eval"":\n        self.eval_losses = losses\n    else:  # is using Horovod\n      # gpu_id should always be zero, since Horovod takes care of isolating\n      # different processes to 1 GPU only\n      with tf.device(""/gpu:0""), tf.variable_scope(\n          name_or_scope=tf.get_variable_scope(),\n          reuse=force_var_reuse,\n          initializer=initializer,\n          dtype=self.get_tf_dtype(),\n      ):\n        deco_print(\n            ""Building graph in Horovod rank: {}"".format(self._hvd.rank())\n        )\n        self.get_data_layer().build_graph()\n        input_tensors = self.get_data_layer().input_tensors\n\n        if self.params.get(""use_trt"", False):\n          # Build TF-TRT graph\n          all_loss, self._output = self.build_trt_forward_pass_graph(\n              input_tensors,\n              gpu_id=0,\n              checkpoint=checkpoint\n          )\n        else:\n          # Build regular TF graph\n          all_loss, self._output = self._build_forward_pass_graph(\n              input_tensors,\n              gpu_id=0\n          )\n        if isinstance(all_loss, (dict,)):\n            loss = all_loss[\'loss\']\n        else:\n          loss = all_loss\n\n        if self._output is not None and not isinstance(self._output, list):\n          raise ValueError(\'Decoder outputs have to be either None or list\')\n\n        if self._mode == ""train"":\n          self.loss = loss\n        if self._mode == ""eval"":\n          self.eval_losses = [loss]\n\n    try:\n      self._num_objects_per_step = [self._get_num_objects_per_step(worker_id)\n                                    for worker_id in range(self.num_gpus)]\n    except NotImplementedError:\n      pass\n\n    if self._mode == ""train"":\n      if \'lr_policy\' not in self.params:\n        lr_policy = None\n      else:\n        lr_params = self.params.get(\'lr_policy_params\', {})\n        # adding default decay_steps = max_steps if lr_policy supports it and\n        # different value is not provided\n        func_params = signature(self.params[\'lr_policy\']).parameters\n        if \'decay_steps\' in func_params and \'decay_steps\' not in lr_params:\n          lr_params[\'decay_steps\'] = self._last_step\n          if \'begin_decay_at\' in func_params:\n            if \'warmup_steps\' in func_params:\n              lr_params[\'begin_decay_at\'] = max(\n                  lr_params.get(\'begin_decay_at\', 0),\n                  lr_params.get(\'warmup_steps\', 0)\n              )\n            lr_params[\'decay_steps\'] -= lr_params.get(\'begin_decay_at\', 0)\n          \n        if \'steps_per_epoch\' in func_params and \\\n           \'steps_per_epoch\' not in lr_params and \'num_epochs\' in self.params:\n          lr_params[\'steps_per_epoch\'] = self.steps_in_epoch\n        lr_policy = lambda gs: self.params[\'lr_policy\'](global_step=gs,\n                                                        **lr_params)\n\n      if self.params.get(\'iter_size\', 1) > 1:\n        self.skip_update_ph = tf.placeholder(tf.bool)\n\n      var_list = tf.trainable_variables()\n      freeze_variables_regex = self.params.get(\'freeze_variables_regex\', None)\n      if freeze_variables_regex is not None:\n        pattern = re.compile(freeze_variables_regex)\n        var_list = [var for var in tf.trainable_variables()\n                    if not pattern.match(var.name)]\n\n      self.train_op = optimize_loss(\n          loss=tf.cast(self.loss, tf.float32) + get_regularization_loss(),\n          dtype=self.params[\'dtype\'],\n          optimizer=self.params[\'optimizer\'],\n          optimizer_params=self.params.get(\'optimizer_params\', {}),\n          var_list=var_list,\n          clip_gradients=self.params.get(\'max_grad_norm\', None),\n          learning_rate_decay_fn=lr_policy,\n          summaries=self.params.get(\'summaries\', None),\n          larc_params=self.params.get(\'larc_params\', None),\n          loss_scaling=self.params.get(\'loss_scaling\', 1.0),\n          loss_scaling_params=self.params.get(\'loss_scaling_params\', None),\n          on_horovod=self.on_horovod,\n          iter_size=self.params.get(\'iter_size\', 1),\n          skip_update_ph=self.skip_update_ph,\n          model=self\n      )\n      tf.summary.scalar(name=""train_loss"", tensor=self.loss)\n      if self.steps_in_epoch:\n        tf.summary.scalar(\n            name=""epoch"",\n            tensor=tf.floor(tf.train.get_global_step() /\n                            tf.constant(self.steps_in_epoch, dtype=tf.int64)),\n        )\n\n      if not self.on_horovod or self._hvd.rank() == 0:\n        if freeze_variables_regex is not None:\n          deco_print(\'Complete list of variables:\')\n          for var in tf.trainable_variables():\n            deco_print(\'{}\'.format(var.name), offset=2)\n        deco_print(""Trainable variables:"")\n        total_params = 0\n        unknown_shape = False\n        for var in var_list:\n          var_params = 1\n          deco_print(\'{}\'.format(var.name), offset=2)\n          deco_print(\'shape: {}, {}\'.format(var.get_shape(), var.dtype),\n                     offset=4)\n          if var.get_shape():\n            for dim in var.get_shape():\n              var_params *= dim.value\n            total_params += var_params\n          else:\n            unknown_shape = True\n        if unknown_shape:\n          deco_print(""Encountered unknown variable shape, can\'t compute total ""\n                     ""number of parameters."")\n        else:\n          deco_print(\'Total trainable parameters: {}\'.format(total_params))\n\n  def build_trt_forward_pass_graph(self, input_tensors, gpu_id=0,\n                                   checkpoint=None):\n    """"""Wrapper around _build_forward_pass_graph which converts graph using\n    TF-TRT""""""\n    import tensorflow.contrib.tensorrt as trt\n    # Default parameters\n    trt_params = {\n        ""batch_size_per_gpu"": 64,\n        ""trt_max_workspace_size_bytes"": (4096 << 20) - 1000,\n        ""trt_precision_mode"": ""FP32"",\n        ""trt_minimum_segment_size"": 10,\n        ""trt_is_dynamic_op"": True,\n        ""trt_maximum_cached_engines"": 1\n    }\n    # Update params from user config\n    for key in trt_params:\n      if key in self.params:\n        trt_params[key] = self.params[key]\n    # Create temporary graph which will contain the native TF graph\n    tf_config = tf.ConfigProto()\n    tf_config.gpu_options.allow_growth = True\n    temp_graph = tf.Graph()\n    input_map = {}\n    # We have to deconstruct SparseTensors into their 3 internal tensors\n    # (indicies, values, dense_shape). This maps each tensor name to a list of\n    # all 3 tensor names in its SparseTensor.\n    output_sparse_tensor_map = {}\n    with temp_graph.as_default() as tf_graph:\n      with tf.Session(config=tf_config) as tf_sess:\n        # Create temporary input placeholders used to build native TF graph\n        input_placeholders = {\'source_tensors\': []}\n        for i, original_input in enumerate(input_tensors[\'source_tensors\']):\n          name = \'input_map_%d\' % i\n          input_placeholders[\'source_tensors\'].append(\n              tf.placeholder(shape=original_input.shape,\n                             dtype=original_input.dtype,\n                             name=name))\n          # And map it back to original input\n          input_map[name] = original_input\n        # Build native graph\n        loss, outputs = self._build_forward_pass_graph(\n            input_placeholders,\n            gpu_id=gpu_id\n        )\n        # Gather output tensors\n        output_node_names = []\n        output_node_names_and_ports = []\n        for x in outputs:\n          if isinstance(x, tf.SparseTensor):\n            components = [x.indices.name, x.values.name, x.dense_shape.name]\n            fetch_names = [tensor.split(\':\')[0] for tensor in components]\n            # Remove duplicates (i.e. if SparseTensor is output of one node)\n            fetch_names = list(set(fetch_names))\n            output_node_names.extend(fetch_names)\n            output_node_names_and_ports.extend(components)\n            # Add all components to map so SparseTensor can be reconstructed\n            # from tensor components which will be outputs of new graph\n            for tensor in components:\n              output_sparse_tensor_map[tensor] = components\n          else:\n            output_node_names.append(x.name.split(\':\')[0])\n            output_node_names_and_ports.append(x.name)\n        # Restore checkpoint here because we have to freeze the graph\n        tf_saver = tf.train.Saver()\n        tf_saver.restore(save_path=checkpoint, sess=tf_sess)\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\n            tf_sess,\n            tf_sess.graph_def,\n            output_node_names=output_node_names\n        )\n        num_nodes = len(frozen_graph.node)\n        print(\'Converting graph using TensorFlow-TensorRT...\')\n        frozen_graph = trt.create_inference_graph(\n            input_graph_def=frozen_graph,\n            outputs=output_node_names,\n            max_batch_size=trt_params[""batch_size_per_gpu""],\n            max_workspace_size_bytes=trt_params[""trt_max_workspace_size_bytes""],\n            precision_mode=trt_params[""trt_precision_mode""],\n            minimum_segment_size=trt_params[""trt_minimum_segment_size""],\n            is_dynamic_op=trt_params[""trt_is_dynamic_op""],\n            maximum_cached_engines=trt_params[""trt_maximum_cached_engines""]\n        )\n        # Remove unused inputs from input_map.\n        inputs_to_remove = []\n        for k in input_map:\n          if k not in [node.name for node in frozen_graph.node]:\n            inputs_to_remove.append(k)\n        for k in inputs_to_remove:\n          del input_map[k]\n        print(\'Total node count before and after TF-TRT conversion:\',\n              num_nodes, \'->\', len(frozen_graph.node))\n        print(\'TRT node count:\',\n              len([1 for n in frozen_graph.node if str(n.op) == \'TRTEngineOp\']))\n    # Perform calibration for INT8 precision mode\n    if self.params.get(""trt_precision_mode"", ""FP32"").upper() == \'INT8\':\n      with tf.Session(config=tf_config) as tf_sess:\n        calib_graph = frozen_graph\n        num_iterations = 10\n        print(\'Calibrating INT8...\')\n        outputs = tf.import_graph_def(\n            calib_graph,\n            input_map=input_map,\n            return_elements=output_node_names_and_ports,\n            name=\'\')\n        self._num_objects_per_step = [self._get_num_objects_per_step(worker_id)\n                                      for worker_id in range(self.num_gpus)]\n        results_per_batch = iterate_data(\n            self, tf_sess, compute_loss=False, mode=\'infer\', verbose=False,\n            num_steps=num_iterations\n        )\n        frozen_graph = trt.calib_graph_to_infer_graph(calib_graph)\n        del calib_graph\n        print(\'INT8 graph created.\')\n        print(\'Nodes INT8:\', len(frozen_graph.node))\n    # Import TRT converted graph to default graph, mapping it to the original\n    # input tensors.\n    outputs = tf.import_graph_def(\n        frozen_graph,\n        input_map=input_map,\n        return_elements=output_node_names_and_ports,\n        name=\'\')\n    # Reconstruct SparseTensors\n    final_outputs = []\n    for tensor in outputs:\n      if tensor.name in output_sparse_tensor_map:\n        component_names = output_sparse_tensor_map[tensor.name]\n        # Find tensors in outputs for components\n        component_tensors = [[x for x in outputs if x.name == name][0]\n                             for name in component_names]\n        # Remove all components from outputs so we don\'t create duplicates of\n        # this SparseTensor\n        for x in component_tensors:\n          if x in outputs:\n            outputs.remove(x)\n        final_outputs.append(tf.SparseTensor(*component_tensors))\n      else:\n        final_outputs.append(tensor)\n    return loss, final_outputs\n\n  @abc.abstractmethod\n  def _build_forward_pass_graph(self, input_tensors, gpu_id=0):\n    """"""Abstract method. Should create the graph of the forward pass of the model.\n\n    Args:\n      input_tensors: ``input_tensors`` defined by the data_layer class.\n      gpu_id (int, optional): id of the GPU where the current copy of the model\n          is constructed. For Horovod this is always zero.\n\n    Returns:\n      tuple: tuple containing loss tensor and list of outputs tensors.\n\n      Loss tensor will be automatically provided to the optimizer and\n      corresponding :attr:`train_op` will be created.\n\n      Samples tensors are stored in the :attr:`_outputs` attribute and can be\n      accessed by calling :meth:`get_output_tensors` function. For example,\n      this happens inside :class:`utils.hooks.RunEvaluationHook`\n      to fetch output values for evaluation.\n\n      Both loss and outputs can be None when corresponding part of the graph\n      is not built.\n    """"""\n    pass\n\n  def maybe_print_logs(self, input_values, output_values, training_step):\n    """"""This method can be used to print logs that help to visualize training.\n    For example, you can print sample input sequences and their corresponding\n    predictions. This method will be called every ``print_samples_steps``\n    (config parameter) iterations and input/output values will be populated\n    automatically by calling ``sess.run`` on corresponding tensors. Note that\n    this method is not abstract and does not have to be implemented in\n    derived classes. But if additional printing functionality is required,\n    overwriting this method can be a useful way to add it.\n\n    Args:\n      input_values: evaluation of\n          :meth:`self.get_data_layer(0).input_tensors\n          <data.data_layer.DataLayer.input_tensors>`, that is, input tensors\n          for one batch on the *first* GPU.\n      output_values: evaluation of\n          :meth:`self.get_output_tensors(0) <get_output_tensors>`,\n          that is, output tensors for one batch on the *first* GPU.\n      training_step (int): Current training step.\n\n    Returns:\n      dict: dictionary with values that need to be logged to TensorBoard\n      (can be empty).\n    """"""\n    # by default return an empty dictionary and do nothing\n    return {}\n\n  def evaluate(self, input_values, output_values):\n    """"""This method can be used in conjunction with\n    :meth:`self.finalize_evaluation()<finalize_evaluation>` to calculate\n    evaluation metrics.\n    For example, for speech-to-text models these methods can calculate\n    word-error-rate on the validation data. For text-to-text models, these\n    methods can compute BLEU score. Look at the corresponding derived classes\n    for examples of this. These methods will be called every\n    ``eval_steps`` (config parameter) iterations and\n    input/output values will be populated automatically by calling ``sess.run``\n    on corresponding tensors (using evaluation model).\n    The :meth:`self.evaluate()<evaluate>` method is called on each batch data\n    and it\'s results will be collected and provided to\n    :meth:`self.finalize_evaluation()<finalize_evaluation>` for finalization.\n    Note that\n    this function is not abstract and does not have to be implemented in\n    derived classes. But if evaluation functionality is required,\n    overwriting this function can be a useful way to add it.\n\n    Args:\n      input_values: evaluation of\n          :meth:`self.get_data_layer().input_tensors\n          <data.data_layer.DataLayer.input_tensors>` concatenated  across\n          all workers. That is, input tensors for one batch combined\n          from *all* GPUs.\n      output_values: evaluation of\n          :meth:`self.get_output_tensors() <get_output_tensors>` concatenated\n          across all workers. That is, output tensors for one batch combined\n          from *all* GPUs.\n\n    Returns:\n      list: all necessary values for evaluation finalization (e.g. accuracy on\n      current batch, which will then be averaged in finalization method).\n    """"""\n    return []\n\n  def finalize_evaluation(self, results_per_batch, training_step=None):\n    """"""This method can be used in conjunction with\n    :meth:`self.evaluate()<evaluate>` to calculate\n    evaluation metrics.\n    For example, for speech-to-text models these methods can calculate\n    word-error-rate on the validation data. For text-to-text models, these\n    methods can compute BLEU score. Look at the corresponding derived classes\n    for examples of this. These methods will be called every\n    ``eval_steps`` (config parameter) iterations and\n    input/output values will be populated automatically by calling ``sess.run``\n    on corresponding tensors (using evaluation model).\n    The :meth:`self.evaluate()<evaluate>` method is called on each batch data\n    and it\'s results will be collected and provided to\n    :meth:`self.finalize_evaluation()<finalize_evaluation>` for finalization.\n    Note that\n    these methods are not abstract and does not have to be implemented in\n    derived classes. But if evaluation functionality is required,\n    overwriting these methods can be a useful way to add it.\n\n    Args:\n      results_per_batch (list): aggregation of values returned from all calls\n          to :meth:`self.evaluate()<evaluate>` method (number of calls will be\n          equal to number of evaluation batches).\n      training_step (int): current training step. Will only be passed if mode\n          is ""train_eval"".\n\n    Returns:\n      dict: dictionary with values that need to be logged to TensorBoard\n      (can be empty).\n    """"""\n    # by default return an empty dictionary and do nothing\n    return {}\n\n  def infer(self, input_values, output_values):\n    """"""This method is analogous to :meth:`self.evaluate()<evaluate>`, but used\n    in conjunction with :meth:`self.finalize_inference()<finalize_inference>`\n    to perform inference.\n\n    Args:\n      input_values: evaluation of\n          :meth:`self.get_data_layer().input_tensors\n          <data.data_layer.DataLayer.input_tensors>` concatenated  across\n          all workers. That is, input tensors for one batch combined\n          from *all* GPUs.\n      output_values: evaluation of\n          :meth:`self.get_output_tensors() <get_output_tensors>` concatenated\n          across all workers. That is, output tensors for one batch combined\n          from *all* GPUs.\n\n    Returns:\n      list: all necessary values for inference finalization (e.g. this method\n      can return final generated sequences for each batch which will then be\n      saved to file in :meth:`self.finalize_inference()<finalize_inference>`\n      method).\n    """"""\n    return []\n\n  def finalize_inference(self, results_per_batch, output_file):\n    """"""This method should be implemented if the model support inference mode.\n    For example for speech-to-text and text-to-text models, this method will\n    log the corresponding input-output pair to the output_file.\n\n    Args:\n      results_per_batch (list): aggregation of values returned from all calls\n          to :meth:`self.evaluate()<evaluate>` method (number of calls will be\n          equal to number of evaluation batches).\n      output_file (str): name of the output file that inference results should\n          be saved to.\n    """"""\n    pass\n\n  def clip_last_batch(self, last_batch, true_size):\n    """"""This method performs last batch clipping.\n    Used in cases when dataset is not divisible by the batch size and model\n    does not support dynamic batch sizes. In those cases, the last batch will\n    contain some data from the ""next epoch"" and this method can be used\n    to remove that data. This method works for both\n    dense and sparse tensors. In most cases you will not need to overwrite this\n    method.\n\n    Args:\n      last_batch (list): list with elements that could be either ``np.array``\n          or ``tf.SparseTensorValue`` containing data for last batch. The\n          assumption is that the first axis of all data tensors will correspond\n          to the current batch size.\n      true_size (int): true size that the last batch should be cut to.\n    """"""\n    return clip_last_batch(last_batch, true_size)\n\n  def get_output_tensors(self, worker_id=0):\n    """"""Returns output tensors generated by :meth:`_build_forward_pass_graph.`\n    When using Horovod, ``worker_id`` parameter is ignored. When using\n    tower-based multi-GPU approach, ``worker_id`` can be used to select tensors\n    for corresponding tower/GPU.\n\n    Args:\n      worker_id (int): id of the worker to get tensors from\n          (not used for Horovod).\n\n    Returns:\n      output tensors.\n    """"""\n    if self.on_horovod:\n      return self._output\n    else:\n      return self._outputs[worker_id]\n\n  def get_data_layer(self, worker_id=0):\n    """"""Returns model data layer.\n    When using Horovod, ``worker_id`` parameter is ignored. When using\n    tower-based multi-GPU approach, ``worker_id`` can be used to select\n    data layer for corresponding tower/GPU.\n\n    Args:\n      worker_id (int): id of the worker to get data layer from\n          (not used for Horovod).\n\n    Returns:\n      model data layer.\n    """"""\n    if self.on_horovod:\n      return self._data_layer\n    else:\n      return self._data_layers[worker_id]\n\n  def get_tf_dtype(self):\n    """"""Returns actual TensorFlow dtype that will be used as variables dtype.""""""\n    if self.params[\'dtype\'] == ""mixed"":\n      return tf.float16\n    else:\n      return self.params[\'dtype\']\n\n  def _get_num_objects_per_step(self, worker_id=0):\n    """"""Define this method if you need benchmarking functionality.\n    For example, for translation models, this method should return number of\n    tokens in current batch, for image recognition model should return number\n    of images in current batch.\n\n    Args:\n      worker_id (int): id of the worker to get data layer from\n          (not used for Horovod).\n\n    Returns:\n      tf.Tensor with number of objects in batch.\n    """"""\n    raise NotImplementedError()\n\n  def get_num_objects_per_step(self, worker_id=0):\n    if self._num_objects_per_step:\n      return self._num_objects_per_step[worker_id]\n    else:\n      raise NotImplementedError()\n\n  @property\n  def params(self):\n    """"""Parameters used to construct the model (dictionary).""""""\n    return self._params\n\n  @property\n  def steps_in_epoch(self):\n    """"""Number of steps in epoch.\n    This parameter is only populated if ``num_epochs`` was specified in the\n    config (otherwise it is None).\n    It is used in training hooks to correctly print epoch number.\n    """"""\n    return self._steps_in_epoch\n\n  @property\n  def last_step(self):\n    """"""Number of steps the training should be run for.""""""\n    return self._last_step\n\n  @property\n  def num_gpus(self):\n    """"""Number of GPUs the model will be run on.\n    For Horovod this is always 1 and actual number of GPUs is controlled by\n    Open-MPI parameters.\n    """"""\n    return len(self._gpu_ids)\n\n  @property\n  def mode(self):\n    """"""Mode the model is executed in (""train"", ""eval"" or ""infer"").""""""\n    return self._mode\n\n  @property\n  def on_horovod(self):\n    """"""Whether the model is run on Horovod or not.""""""\n    return self._hvd is not None\n\n  @property\n  def hvd(self):\n    """"""horovod.tensorflow module""""""\n    return self._hvd\n'"
open_seq2seq/models/speech2text.py,5,"b'# Copyright (c) 2018 NVIDIA Corporation\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom six.moves import range\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\nfrom open_seq2seq.utils.utils import deco_print\nfrom .encoder_decoder import EncoderDecoderModel\n\nimport pickle\n\n\ndef sparse_tensor_to_chars(tensor, idx2char):\n  text = [\'\'] * tensor.dense_shape[0]\n  for idx_tuple, value in zip(tensor.indices, tensor.values):\n    text[idx_tuple[0]] += idx2char[value]\n  return text\n\n\ndef sparse_tensor_to_chars_bpe(tensor):\n  idx = [[] for _ in range(tensor.dense_shape[0])]\n  for idx_tuple, value in zip(tensor.indices, tensor.values):\n    idx[idx_tuple[0]].append(int(value))\n  \n  return idx\n\n\ndef dense_tensor_to_chars(tensor, idx2char, startindex, endindex):\n  batch_size = len(tensor)\n  text = [\'\'] * batch_size\n  for batch_num in range(batch_size):\n    \'\'\'text[batch_num] = """".join([idx2char[idx] for idx in tensor[batch_num]\n                               if idx not in [startindex, endindex]])\'\'\'\n\n    text[batch_num] = """"\n    for idx in tensor[batch_num]:\n      if idx == endindex:\n        break\n      text[batch_num] += idx2char[idx]\n  return text\n\n\ndef levenshtein(a, b):\n  """"""Calculates the Levenshtein distance between a and b.\n  The code was copied from: http://hetland.org/coding/python/levenshtein.py\n  """"""\n  n, m = len(a), len(b)\n  if n > m:\n    # Make sure n <= m, to use O(min(n,m)) space\n    a, b = b, a\n    n, m = m, n\n\n  current = list(range(n + 1))\n  for i in range(1, m + 1):\n    previous, current = current, [i] + [0] * n\n    for j in range(1, n + 1):\n      add, delete = previous[j] + 1, current[j - 1] + 1\n      change = previous[j - 1]\n      if a[j - 1] != b[i - 1]:\n        change = change + 1\n      current[j] = min(add, delete, change)\n\n  return current[n]\n\n\ndef plot_attention(alignments, pred_text, encoder_len, training_step):\n\n  alignments = alignments[:len(pred_text), :encoder_len]\n  fig = plt.figure(figsize=(15, 10))\n  ax = fig.add_subplot(1, 1, 1)\n\n  img = ax.imshow(alignments, interpolation=\'nearest\', cmap=\'Blues\')\n  ax.grid()\n  #fig.savefig(\'/home/rgadde/Desktop/OpenSeq2Seq/plots/file{}.png\'.format(training_step), dpi=300)\n\n  sbuffer = BytesIO()\n  fig.savefig(sbuffer, dpi=300)\n  summary = tf.Summary.Image(\n      encoded_image_string=sbuffer.getvalue(),\n      height=int(fig.get_figheight() * 2),\n      width=int(fig.get_figwidth() * 2)\n  )\n  summary = tf.Summary.Value(\n      tag=""attention_summary_step_{}"".format(int(training_step / 2200)), image=summary)\n\n  plt.close(fig)\n  return summary\n\n\nclass Speech2Text(EncoderDecoderModel):\n\n  def _create_decoder(self):\n    data_layer = self.get_data_layer()\n    self.params[\'decoder_params\'][\'tgt_vocab_size\'] = (\n        data_layer.params[\'tgt_vocab_size\']\n    )\n\n    self.dump_outputs = self.params[\'decoder_params\'].get(\'infer_logits_to_pickle\', False)\n\n    self.is_bpe = data_layer.params.get(\'bpe\', False)\n    self.tensor_to_chars = sparse_tensor_to_chars\n    self.tensor_to_char_params = {}\n    self.autoregressive = data_layer.params.get(\'autoregressive\', False)\n    if self.autoregressive:\n      self.params[\'decoder_params\'][\'GO_SYMBOL\'] = data_layer.start_index\n      self.params[\'decoder_params\'][\'END_SYMBOL\'] = data_layer.end_index\n      self.tensor_to_chars = dense_tensor_to_chars\n      self.tensor_to_char_params[\'startindex\'] = data_layer.start_index\n      self.tensor_to_char_params[\'endindex\'] = data_layer.end_index\n\n    return super(Speech2Text, self)._create_decoder()\n\n  def _create_loss(self):\n    if self.get_data_layer().params.get(\'autoregressive\', False):\n      self.params[\'loss_params\'][\n          \'batch_size\'] = self.params[\'batch_size_per_gpu\']\n      self.params[\'loss_params\'][\'tgt_vocab_size\'] = (\n          self.get_data_layer().params[\'tgt_vocab_size\']\n      )\n    return super(Speech2Text, self)._create_loss()\n\n  def _build_forward_pass_graph(self, input_tensors, gpu_id=0):\n    """"""TensorFlow graph for speech2text model is created here.\n    This function connects encoder, decoder and loss together. As an input for\n    encoder it will specify source tensors (as returned from\n    the data layer). As an input for decoder it will specify target tensors\n    as well as all output returned from encoder. For loss it\n    will also specify target tensors and all output returned from\n    decoder. Note that loss will only be built for mode == ""train"" or ""eval"".\n\n    Args:\n      input_tensors (dict): ``input_tensors`` dictionary that has to contain\n          ``source_tensors`` key with the list of all source tensors, and\n          ``target_tensors`` with the list of all target tensors. Note that\n          ``target_tensors`` only need to be provided if mode is\n          ""train"" or ""eval"".\n      gpu_id (int, optional): id of the GPU where the current copy of the model\n          is constructed. For Horovod this is always zero.\n\n    Returns:\n      tuple: tuple containing loss tensor as returned from\n      ``loss.compute_loss()`` and list of outputs tensors, which is taken from\n      ``decoder.decode()[\'outputs\']``. When ``mode == \'infer\'``, loss will\n      be None.\n    """"""\n    if not isinstance(input_tensors, dict) or \\\n       \'source_tensors\' not in input_tensors:\n      raise ValueError(\'Input tensors should be a dict containing \'\n                       \'""source_tensors"" key\')\n\n    if not isinstance(input_tensors[\'source_tensors\'], list):\n      raise ValueError(\'source_tensors should be a list\')\n\n    source_tensors = input_tensors[\'source_tensors\']\n    if self.mode == ""train"" or self.mode == ""eval"":\n      if \'target_tensors\' not in input_tensors:\n        raise ValueError(\'Input tensors should contain ""target_tensors"" key\'\n                         \'when mode != ""infer""\')\n      if not isinstance(input_tensors[\'target_tensors\'], list):\n        raise ValueError(\'target_tensors should be a list\')\n      target_tensors = input_tensors[\'target_tensors\']\n\n    with tf.variable_scope(""ForwardPass""):\n      encoder_input = {""source_tensors"": source_tensors}\n      encoder_output = self.encoder.encode(input_dict=encoder_input)\n\n      decoder_input = {""encoder_output"": encoder_output}\n      if self.mode == ""train"" or self.mode == ""eval"":\n        decoder_input[\'target_tensors\'] = target_tensors\n      decoder_output = self.decoder.decode(input_dict=decoder_input)\n      model_outputs = decoder_output.get(""outputs"", None)\n\n      if self.mode == ""train"" or self.mode == ""eval"":\n        with tf.variable_scope(""Loss""):\n          loss_input_dict = {\n              ""decoder_output"": decoder_output,\n              ""target_tensors"": target_tensors,\n          }\n          loss = self.loss_computator.compute_loss(loss_input_dict)\n      else:\n        deco_print(""Inference Mode. Loss part of graph isn\'t built."")\n        loss = None\n        if self.dump_outputs:\n          model_logits = decoder_output.get(""logits"", None)\n          return loss, [model_logits]\n    return loss, model_outputs\n\n\n\n  def maybe_print_logs(self, input_values, output_values, training_step):\n    y, len_y = input_values[\'target_tensors\']\n    decoded_sequence = output_values\n    y_one_sample = y[0]\n    len_y_one_sample = len_y[0]\n    decoded_sequence_one_batch = decoded_sequence[0]\n\n    if self.is_bpe:\n      dec_list = sparse_tensor_to_chars_bpe(decoded_sequence_one_batch)[0]\n      true_text = self.get_data_layer().sp.DecodeIds(y_one_sample[:len_y_one_sample].tolist())\n      pred_text = self.get_data_layer().sp.DecodeIds(dec_list)\n\n    else:\n      # we also clip the sample by the correct length\n      true_text = """".join(map(\n          self.get_data_layer().params[\'idx2char\'].get,\n          y_one_sample[:len_y_one_sample],\n      ))\n      pred_text = """".join(self.tensor_to_chars(\n          decoded_sequence_one_batch,\n          self.get_data_layer().params[\'idx2char\'],\n          **self.tensor_to_char_params\n      )[0])\n    sample_wer = levenshtein(true_text.split(), pred_text.split()) / \\\n        len(true_text.split())\n\n    self.autoregressive = self.get_data_layer().params.get(\'autoregressive\', False)\n    self.plot_attention = False  # (output_values[1] != None).all()\n    if self.plot_attention:\n      attention_summary = plot_attention(\n          output_values[1][0], pred_text, output_values[2][0], training_step)\n\n    deco_print(""Sample WER: {:.4f}"".format(sample_wer), offset=4)\n    deco_print(""Sample target:     "" + true_text, offset=4)\n    deco_print(""Sample prediction: "" + pred_text, offset=4)\n\n    if self.plot_attention:\n      return {\n          \'Sample WER\': sample_wer,\n          \'Attention Summary\': attention_summary,\n      }\n    else:\n      return {\n          \'Sample WER\': sample_wer,\n      }\n    \n  def finalize_evaluation(self, results_per_batch, training_step=None):\n    total_word_lev = 0.0\n    total_word_count = 0.0\n    for word_lev, word_count in results_per_batch:\n      total_word_lev += word_lev\n      total_word_count += word_count\n\n    total_wer = 1.0 * total_word_lev / total_word_count\n    deco_print(""Validation WER:  {:.4f}"".format(total_wer), offset=4)\n    return {\n        ""Eval WER"": total_wer,\n    }\n\n  def evaluate(self, input_values, output_values):\n    total_word_lev = 0.0\n    total_word_count = 0.0\n\n    decoded_sequence = output_values[0]\n\n    if self.is_bpe:\n      decoded_texts = sparse_tensor_to_chars_bpe(decoded_sequence)\n    else:\n      decoded_texts = self.tensor_to_chars(\n          decoded_sequence,\n          self.get_data_layer().params[\'idx2char\'],\n          **self.tensor_to_char_params\n      )\n\n    batch_size = input_values[\'source_tensors\'][0].shape[0]\n    for sample_id in range(batch_size):\n      # y is the third returned input value, thus input_values[2]\n      # len_y is the fourth returned input value\n      y = input_values[\'target_tensors\'][0][sample_id]\n      len_y = input_values[\'target_tensors\'][1][sample_id]\n      if self.is_bpe:\n        true_text = self.get_data_layer().sp.DecodeIds(y[:len_y].tolist())\n        pred_text = self.get_data_layer().sp.DecodeIds(decoded_texts[sample_id])\n      else:\n        true_text = """".join(map(self.get_data_layer().params[\'idx2char\'].get,\n                              y[:len_y]))\n        pred_text = """".join(decoded_texts[sample_id])\n      if self.get_data_layer().params.get(\'autoregressive\', False):\n        true_text = true_text[:-4]\n\n      # print(\'TRUE_TEXT: ""{}""\'.format(true_text))\n      # print(\'PRED_TEXT: ""{}""\'.format(pred_text))\n\n      total_word_lev += levenshtein(true_text.split(), pred_text.split())\n      total_word_count += len(true_text.split())\n\n    return total_word_lev, total_word_count\n\n  def infer(self, input_values, output_values):\n    preds = []\n    decoded_sequence = output_values[0]\n\n    if self.dump_outputs:\n      # decoded_sequence has \'time_major\' shape: [T, B, C]\n      for i in range(decoded_sequence.shape[0]):\n        preds.append(decoded_sequence[i, :, :].squeeze())\n    else:\n      decoded_texts = self.tensor_to_chars(\n          decoded_sequence,\n          self.get_data_layer().params[\'idx2char\'],\n          **self.tensor_to_char_params\n      )\n      for decoded_text in decoded_texts:\n        preds.append("""".join(decoded_text))\n\n    return preds, input_values[\'source_ids\']\n\n  def finalize_inference(self, results_per_batch, output_file):\n    preds = []\n    ids = []\n\n    for result, idx in results_per_batch:\n      preds.extend(result)\n      ids.extend(idx)\n\n    preds = np.array(preds)\n    ids = np.hstack(ids)\n    # restoring the correct order\n    preds = preds[np.argsort(ids)]\n    if self.dump_outputs:\n      dump_out = {}\n      dump_results = {}\n      files = self.get_data_layer().all_files\n      for i, f in enumerate(files):\n        dump_results[f] = preds[i]\n      dump_out[""logits""] = dump_results\n      step_size = self.get_data_layer().params[""window_stride""]\n      scale = 1\n      # check strides in convolutional layers\n      for layers in [\'convnet_layers\', \'conv_layers\', \'cnn_layers\']:\n        convs = self.encoder.params.get(layers)\n        if convs:\n          for c in convs:\n            scale *= c[""stride""][0]\n      dump_out[""step_size""] = scale*step_size\n      dump_out[""vocab""] = self.get_data_layer().params[\'idx2char\']\n      with open(output_file, \'wb\') as f:\n        pickle.dump(dump_out, f, protocol=pickle.HIGHEST_PROTOCOL)\n      f.close()\n    else:\n      pd.DataFrame(\n          {\n              \'wav_filename\': self.get_data_layer().all_files,\n              \'predicted_transcript\': preds,\n          },\n          columns=[\'wav_filename\', \'predicted_transcript\'],\n      ).to_csv(output_file, index=False)\n\n  def _get_num_objects_per_step(self, worker_id=0):\n    """"""Returns number of audio frames in current batch.""""""\n    data_layer = self.get_data_layer(worker_id)\n    num_frames = tf.reduce_sum(data_layer.input_tensors[\'source_tensors\'][1])\n    return num_frames\n'"
open_seq2seq/models/speech2text_ds2_test.py,1,"b""# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.test_utils.test_speech_configs.ds2_test_config import \\\n    base_params, train_params, eval_params, base_model\nfrom .speech2text_test import Speech2TextModelTests\n\n\nclass DS2ModelTests(Speech2TextModelTests):\n\n  def setUp(self):\n    self.base_model = base_model\n    self.base_params = base_params\n    self.train_params = train_params\n    self.eval_params = eval_params\n\n  def tearDown(self):\n    pass\n\n  def test_regularizer(self):\n    return self.regularizer_test()\n\n  def test_convergence(self):\n    return self.convergence_test(5.0, 30.0, 0.1)\n\n  def test_convergence_with_iter_size(self):\n    return self.convergence_with_iter_size_test()\n\n  def test_infer(self):\n    return self.infer_test()\n\n  def test_mp_collection(self):\n    return self.mp_collection_test(14, 7)\n\n  def test_levenshtein(self):\n    return self.levenshtein_test()\n\n  def test_maybe_functions(self):\n    return self.maybe_functions_test()\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
open_seq2seq/models/speech2text_test.py,20,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport os\nimport tempfile\n\nimport numpy as np\nimport numpy.testing as npt\nimport pandas as pd\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom open_seq2seq.utils import train, evaluate, infer\nfrom open_seq2seq.utils.utils import get_available_gpus\nfrom .speech2text import levenshtein\n\n\nclass Speech2TextModelTests(tf.test.TestCase):\n\n  def setUp(self):\n    # define this values in subclasses\n    self.base_params = None\n    self.train_params = None\n    self.eval_params = None\n    self.base_model = None\n\n  def run_model(self, train_config, eval_config, hvd=None):\n    with tf.Graph().as_default() as g:\n      # pylint: disable=not-callable\n      train_model = self.base_model(params=train_config, mode=""train"", hvd=hvd)\n      train_model.compile()\n      eval_model = self.base_model(params=eval_config, mode=""eval"", hvd=hvd)\n      eval_model.compile(force_var_reuse=True)\n\n      train(train_model, eval_model)\n      saver = tf.train.Saver()\n      checkpoint = tf.train.latest_checkpoint(train_model.params[\'logdir\'])\n      with self.test_session(g, use_gpu=True) as sess:\n        saver.restore(sess, checkpoint)\n        sess.run([train_model.get_data_layer(i).iterator.initializer\n                  for i in range(train_model.num_gpus)])\n        sess.run([eval_model.get_data_layer(i).iterator.initializer\n                  for i in range(eval_model.num_gpus)])\n\n        weights = sess.run(tf.trainable_variables())\n        loss = sess.run(train_model.loss)\n        eval_losses = sess.run(eval_model.eval_losses)\n        eval_loss = np.mean(eval_losses)\n        weights_new = sess.run(tf.trainable_variables())\n\n        # checking that the weights has not changed from\n        # just computing the loss\n        for w, w_new in zip(weights, weights_new):\n          npt.assert_allclose(w, w_new)\n      eval_dict = evaluate(eval_model, checkpoint)\n    return loss, eval_loss, eval_dict\n\n  def prepare_config(self):\n    self.base_params[\'logdir\'] = tempfile.mktemp()\n    train_config = copy.deepcopy(self.base_params)\n    eval_config = copy.deepcopy(self.base_params)\n    train_config.update(copy.deepcopy(self.train_params))\n    eval_config.update(copy.deepcopy(self.eval_params))\n    return train_config, eval_config\n\n  def regularizer_test(self):\n    for dtype in [tf.float16, tf.float32, \'mixed\']:\n      train_config, eval_config = self.prepare_config()\n      train_config[\'num_epochs\'] = 60\n      train_config.update({\n          ""dtype"": dtype,\n          # pylint: disable=no-member\n          ""regularizer"": tf.contrib.layers.l2_regularizer,\n          ""regularizer_params"": {\n              \'scale\': 1e4,\n          },\n      })\n      eval_config.update({\n          ""dtype"": dtype,\n      })\n      loss, eval_loss, eval_dict = self.run_model(train_config, eval_config)\n\n      self.assertGreaterEqual(loss, 400.0)\n      self.assertGreaterEqual(eval_loss, 400.0)\n      self.assertGreaterEqual(eval_dict[\'Eval WER\'], 0.9)\n\n  def convergence_test(self, train_loss_threshold,\n                       eval_loss_threshold, eval_wer_threshold):\n    for dtype in [tf.float32, ""mixed""]:\n      train_config, eval_config = self.prepare_config()\n      train_config.update({\n          ""dtype"": dtype,\n      })\n      eval_config.update({\n          ""dtype"": dtype,\n      })\n      loss, eval_loss, eval_dict = self.run_model(train_config, eval_config)\n\n      self.assertLess(loss, train_loss_threshold)\n      self.assertLess(eval_loss, eval_loss_threshold)\n      self.assertLess(eval_dict[\'Eval WER\'], eval_wer_threshold)\n\n  def finetuning_test(self, train_loss_threshold,\n                      eval_loss_threshold, eval_wer_threshold):\n    for dtype in [tf.float32, ""mixed""]:\n\n      # pre-training\n      train_config, eval_config = self.prepare_config()\n      train_config.update({\n          ""dtype"": dtype,\n      })\n      eval_config.update({\n          ""dtype"": dtype,\n      })\n      loss, eval_loss, eval_dict = self.run_model(train_config, eval_config)\n\n      self.assertLess(loss, train_loss_threshold)\n      self.assertLess(eval_loss, eval_loss_threshold)\n      self.assertLess(eval_dict[\'Eval WER\'], eval_wer_threshold)\n\n      # finetuning\n      restore_dir = train_config[\'logdir\']\n      train_config[\'logdir\'] = tempfile.mktemp()\n      eval_config[\'logdir\'] = train_config[\'logdir\']\n      train_config.update({\n          ""load_model"": restore_dir,\n          ""lr_policy_params"": {\n               ""learning_rate"": 0.0001,\n               ""power"": 2,\n          }\n      })\n      loss_ft, eval_loss_ft, eval_dict_ft = self.run_model(train_config, eval_config)\n\n      self.assertLess(loss_ft, train_loss_threshold)\n      self.assertLess(eval_loss_ft, eval_loss_threshold)\n      self.assertLess(eval_dict_ft[\'Eval WER\'], eval_wer_threshold)\n\n  def convergence_with_iter_size_test(self):\n    try:\n      import horovod.tensorflow as hvd\n      hvd.init()\n    except ImportError:\n      print(""Horovod not installed skipping test_convergence_with_iter_size"")\n      return\n\n    for dtype in [tf.float32, ""mixed""]:\n      train_config, eval_config = self.prepare_config()\n      train_config.update({\n          ""dtype"": dtype,\n          ""iter_size"": 5,\n          ""batch_size_per_gpu"": 2,\n          ""use_horovod"": True,\n          ""num_epochs"": 200,\n      })\n      eval_config.update({\n          ""dtype"": dtype,\n          ""iter_size"": 5,\n          ""batch_size_per_gpu"": 2,\n          ""use_horovod"": True,\n      })\n      loss, eval_loss, eval_dict = self.run_model(\n          train_config, eval_config, hvd,\n      )\n\n      self.assertLess(loss, 10.0)\n      self.assertLess(eval_loss, 30.0)\n      self.assertLess(eval_dict[\'Eval WER\'], 0.2)\n\n  def infer_test(self):\n    train_config, infer_config = self.prepare_config()\n    train_config[\'num_epochs\'] = 250\n    infer_config[\'batch_size_per_gpu\'] = 4\n\n    with tf.Graph().as_default() as g:\n      with self.test_session(g, use_gpu=True) as sess:\n        gpus = get_available_gpus()\n\n    if len(gpus) > 1:\n      infer_config[\'num_gpus\'] = 2\n    else:\n      infer_config[\'num_gpus\'] = 1\n\n    with tf.Graph().as_default():\n      # pylint: disable=not-callable\n      train_model = self.base_model(\n          params=train_config, mode=""train"", hvd=None)\n      train_model.compile()\n      train(train_model, None)\n\n    with tf.Graph().as_default():\n      # pylint: disable=not-callable\n      infer_model = self.base_model(\n          params=infer_config, mode=""infer"", hvd=None)\n      infer_model.compile()\n\n      print(train_model.params[\'logdir\'])\n      output_file = os.path.join(train_model.params[\'logdir\'], \'infer_out.csv\')\n      infer(\n          infer_model,\n          tf.train.latest_checkpoint(train_model.params[\'logdir\']),\n          output_file,\n      )\n      pred_csv = pd.read_csv(output_file)\n      true_csv = pd.read_csv(\n          \'open_seq2seq/test_utils/toy_speech_data/toy_data.csv\',\n      )\n      for pred_row, true_row in zip(pred_csv.as_matrix(), true_csv.as_matrix()):\n        # checking file name\n        self.assertEqual(pred_row[0], true_row[0])\n        # checking prediction: no more than 5 chars difference\n        self.assertLess(levenshtein(pred_row[-1], true_row[-1]), 5)\n\n  def mp_collection_test(self, num_train_vars, num_master_copies):\n    train_config, eval_config = self.prepare_config()\n    train_config[\'dtype\'] = \'mixed\'\n\n    with tf.Graph().as_default():\n      # pylint: disable=not-callable\n      model = self.base_model(params=train_config, mode=""train"", hvd=None)\n      model.compile()\n      self.assertEqual(len(tf.trainable_variables()), num_train_vars)\n      self.assertEqual(\n          len(tf.get_collection(\'FP32_MASTER_COPIES\')),\n          num_master_copies,  # exclude batch norm beta, gamma and row_conv vars\n      )\n\n  def levenshtein_test(self):\n    sample1 = \'this is a great day\'\n    sample2 = \'this is great day\'\n    self.assertEqual(levenshtein(sample1.split(), sample2.split()), 1)\n    self.assertEqual(levenshtein(sample2.split(), sample1.split()), 1)\n    sample1 = \'this is a great day\'\n    sample2 = \'this great day\'\n    self.assertEqual(levenshtein(sample1.split(), sample2.split()), 2)\n    self.assertEqual(levenshtein(sample2.split(), sample1.split()), 2)\n    sample1 = \'this is a great day\'\n    sample2 = \'this great day\'\n    self.assertEqual(levenshtein(sample1.split(), sample2.split()), 2)\n    self.assertEqual(levenshtein(sample2.split(), sample1.split()), 2)\n    sample1 = \'this is a great day\'\n    sample2 = \'this day is a great\'\n    self.assertEqual(levenshtein(sample1.split(), sample2.split()), 2)\n    self.assertEqual(levenshtein(sample2.split(), sample1.split()), 2)\n    sample1 = \'this is a great day\'\n    sample2 = \'this day is great\'\n    self.assertEqual(levenshtein(sample1.split(), sample2.split()), 3)\n    self.assertEqual(levenshtein(sample2.split(), sample1.split()), 3)\n\n    sample1 = \'london is the capital of great britain\'\n    sample2 = \'london capital gret britain\'\n    self.assertEqual(levenshtein(sample1.split(), sample2.split()), 4)\n    self.assertEqual(levenshtein(sample2.split(), sample1.split()), 4)\n    self.assertEqual(levenshtein(sample1, sample2), 11)\n    self.assertEqual(levenshtein(sample2, sample1), 11)\n\n  def maybe_functions_test(self):\n    train_config, eval_config = self.prepare_config()\n\n    with tf.Graph().as_default():\n      # pylint: disable=not-callable\n      model = self.base_model(params=train_config, mode=""train"", hvd=None)\n      model.compile()\n    model._gpu_ids = range(5)\n    model.params[\'batch_size_per_gpu\'] = 2\n    char2idx = model.get_data_layer().params[\'char2idx\']\n    inputs = [\n        [\'this is a great day\', \'london is the capital of great britain\'],\n        [\'ooo\', \'lll\'],\n        [\'a b c\\\' asdf\', \'blah blah bblah\'],\n        [\'this is great day\', \'london capital gret britain\'],\n        [\'aaaaaaaasdfdasdf\', \'df d sdf asd fd f sdf df blah\\\' blah\'],\n    ]\n    outputs = [\n        [\'this is great a day\', \'london capital gret britain\'],\n        [\'ooo\', \'lll\'],\n        [\'aaaaaaaasdfdasdf\', \'df d sdf asd fd f sdf df blah blah\'],\n        [\'this is a great day\', \'london is the capital of great britain\'],\n        [\'a b c\\\' asdf\', \'blah blah\\\' bblah\'],\n    ]\n    y = [None] * len(inputs)\n    len_y = [None] * len(inputs)\n    indices, values, dense_shape = [], [], []\n\n    num_gpus = len(inputs)\n    for gpu_id in range(num_gpus):\n      num_samples = len(inputs[gpu_id])\n      max_len = np.max(list(map(len, inputs[gpu_id])))\n      y[gpu_id] = np.zeros((num_samples, max_len), dtype=np.int)\n      len_y[gpu_id] = np.zeros(num_samples, dtype=np.int)\n      for sample_id in range(num_samples):\n        num_letters = len(inputs[gpu_id][sample_id])\n        len_y[gpu_id][sample_id] = num_letters\n        for letter_id in range(num_letters):\n          y[gpu_id][sample_id, letter_id] = char2idx[\n              inputs[gpu_id][sample_id][letter_id]\n          ]\n\n    num_gpus = len(outputs)\n    for gpu_id in range(num_gpus):\n      num_samples = len(outputs[gpu_id])\n      max_len = np.max(list(map(len, outputs[gpu_id])))\n      dense_shape.append(np.array((num_samples, max_len)))\n      values.append([])\n      indices.append([])\n      for sample_id in range(num_samples):\n        num_letters = len(outputs[gpu_id][sample_id])\n        for letter_id in range(num_letters):\n          values[gpu_id].append(\n              char2idx[outputs[gpu_id][sample_id][letter_id]]\n          )\n          indices[gpu_id].append(np.array([sample_id, letter_id]))\n      values[gpu_id] = np.array(values[gpu_id], dtype=np.int)\n      indices[gpu_id] = np.array(indices[gpu_id], dtype=np.int)\n\n    x = [np.empty(2)] * len(y)\n    len_x = [None] * len(y)\n    input_values = list(zip(x, len_x, y, len_y))\n    output_values = [\n        [tf.SparseTensorValue(indices[i], values[i], dense_shape[i])]\n        for i in range(num_gpus)\n    ]\n\n    results = []\n    for inp, out in zip(input_values, output_values):\n      inp_dict = {\'source_tensors\': [inp[0], inp[1]],\n                  \'target_tensors\': [inp[2], inp[3]]}\n      results.append(model.evaluate(inp_dict, out))\n    for inp, out in zip(input_values, output_values):\n      inp_dict = {\'source_tensors\': [inp[0], inp[1]],\n                  \'target_tensors\': [inp[2], inp[3]]}\n      results.append(model.evaluate(inp_dict, out))\n    output_dict = model.finalize_evaluation(results)\n\n    w_lev = 0.0\n    w_len = 0.0\n    for batch_id in range(len(inputs)):\n      for sample_id in range(len(inputs[batch_id])):\n        input_sample = inputs[batch_id][sample_id]\n        output_sample = outputs[batch_id][sample_id]\n        w_lev += levenshtein(input_sample.split(), output_sample.split())\n        w_len += len(input_sample.split())\n\n    self.assertEqual(output_dict[\'Eval WER\'], w_lev / w_len)\n    self.assertEqual(output_dict[\'Eval WER\'], 37 / 40.0)\n\n    inp_dict = {\'source_tensors\': [input_values[0][0], input_values[0][1]],\n                \'target_tensors\': [input_values[0][2], input_values[0][3]]}\n    output_dict = model.maybe_print_logs(inp_dict, output_values[0], 0)\n    self.assertEqual(output_dict[\'Sample WER\'], 0.4)\n'"
open_seq2seq/models/speech2text_w2l_test.py,1,"b""# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.test_utils.test_speech_configs.w2l_test_config import \\\n    base_params, train_params, eval_params, base_model\nfrom .speech2text_test import Speech2TextModelTests\n\n\nclass W2LModelTests(Speech2TextModelTests):\n\n  def setUp(self):\n    self.base_model = base_model\n    self.base_params = base_params\n    self.train_params = train_params\n    self.eval_params = eval_params\n\n  def tearDown(self):\n    pass\n\n  def test_convergence(self):\n    return self.convergence_test(5.0, 30.0, 0.1)\n\n  def test_mp_collection(self):\n    return self.mp_collection_test(14, 6)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
open_seq2seq/models/text2speech.py,9,"b'# Copyright (c) 2019 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport librosa\nimport matplotlib as mpl\nimport numpy as np\nfrom scipy.io.wavfile import write\nfrom six import BytesIO\nfrom six.moves import range\n\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom .encoder_decoder import EncoderDecoderModel\n\n\ndef plot_spectrograms(\n        specs,\n        titles,\n        stop_token_pred,\n        audio_length,\n        logdir,\n        train_step,\n        stop_token_target=None,\n        number=0,\n        append=False,\n        save_to_tensorboard=False\n):\n  """"""\n  Helper function to create a image to be logged to disk or a tf.Summary to be\n  logged to tensorboard.\n\n  Args:\n    specs (array): array of images to show\n    titles (array): array of titles. Must match lengths of specs array\n    stop_token_pred (np.array): np.array of size [time, 1] containing the stop\n      token predictions from the model.\n    audio_length (int): lenth of the predicted spectrogram\n    logdir (str): dir to save image file is save_to_tensorboard is disabled.\n    train_step (int): current training step\n    stop_token_target (np.array): np.array of size [time, 1] containing the stop\n      token target.\n    number (int): Current sample number (used if evaluating more than 1 sample\n      from a batch)\n    append (str): Optional string to append to file name eg. train, eval, infer\n    save_to_tensorboard (bool): If False, the created image is saved to the\n      logdir as a png file. If True, the function returns a tf.Summary object\n      containing the image and will be logged to the current tensorboard file.\n\n  Returns:\n    tf.Summary or None\n  """"""\n  num_figs = len(specs) + 1\n  fig, ax = plt.subplots(nrows=num_figs, figsize=(8, num_figs * 3))\n\n  for i, (spec, title) in enumerate(zip(specs, titles)):\n    spec = np.pad(spec, ((1, 1), (1, 1)), ""constant"", constant_values=0.)\n    spec = spec.astype(float)\n    colour = ax[i].imshow(\n      spec.T, cmap=\'viridis\', interpolation=None, aspect=\'auto\'\n    )\n    ax[i].invert_yaxis()\n    ax[i].set_title(title)\n    fig.colorbar(colour, ax=ax[i])\n  if stop_token_target is not None:\n    stop_token_target = stop_token_target.astype(float)\n    ax[-1].plot(stop_token_target, \'r.\')\n  stop_token_pred = stop_token_pred.astype(float)\n  ax[-1].plot(stop_token_pred, \'g.\')\n  ax[-1].axvline(x=audio_length)\n  ax[-1].set_xlim(0, len(specs[0]))\n  ax[-1].set_title(""stop token"")\n\n  plt.xlabel(\'time\')\n  plt.tight_layout()\n\n  cb = fig.colorbar(colour, ax=ax[-1])\n  cb.remove()\n\n  if save_to_tensorboard:\n    tag = ""{}_image"".format(append)\n    iostream = BytesIO()\n    fig.savefig(iostream, dpi=300)\n    summary = tf.Summary.Image(\n        encoded_image_string=iostream.getvalue(),\n        height=int(fig.get_figheight() * 300),\n        width=int(fig.get_figwidth() * 300)\n    )\n    summary = tf.Summary.Value(tag=tag, image=summary)\n    plt.close(fig)\n\n    return summary\n  else:\n    if append:\n      name = \'{}/Output_step{}_{}_{}.png\'.format(\n          logdir, train_step, number, append\n      )\n    else:\n      name = \'{}/Output_step{}_{}.png\'.format(logdir, train_step, number)\n    if logdir[0] != \'/\':\n      name = ""./"" + name\n    # save\n    fig.savefig(name, dpi=300)\n\n    plt.close(fig)\n    return None\n\n\ndef save_audio(\n        magnitudes,\n        logdir,\n        step,\n        sampling_rate,\n        n_fft=1024,\n        mode=""train"",\n        number=0,\n        save_format=""tensorboard"",\n        power=1.5,\n        gl_iters=50,\n        verbose=True,\n        max_normalization=False\n):\n  """"""\n  Helper function to create a wav file to be logged to disk or a tf.Summary to\n  be logged to tensorboard.\n\n  Args:\n    magnitudes (np.array): np.array of size [time, n_fft/2 + 1] containing the\n      energy spectrogram.\n    logdir (str): dir to save image file is save_to_tensorboard is disabled.\n    step (int): current training step\n    n_fft (int): number of filters for fft and ifft.\n    sampling_rate (int): samplng rate in Hz of the audio to be saved.\n    number (int): Current sample number (used if evaluating more than 1 sample\n    mode (str): Optional string to append to file name eg. train, eval, infer\n      from a batch)\n    save_format: save_audio can either return the np.array containing the\n      generated sound, log the wav file to the disk, or return a tensorboard\n      summary object. Each method can be enabled by passing save_format as\n      ""np.array"", ""tensorboard"", or ""disk"" respectively.\n\n  Returns:\n    tf.Summary or None\n  """"""\n  # Clip signal max and min\n  if np.min(magnitudes) < 0 or np.max(magnitudes) > 255:\n    if verbose:\n      print(""WARNING: {} audio was clipped at step {}"".format(mode.capitalize(), step))\n    magnitudes = np.clip(magnitudes, a_min=0, a_max=255)\n  signal = griffin_lim(magnitudes.T ** power, n_iters=gl_iters, n_fft=n_fft)\n\n  if max_normalization:\n    signal /= np.max(np.abs(signal))\n\n  if save_format == ""np.array"":\n    return signal\n  elif save_format == ""tensorboard"":\n    tag = ""{}_audio"".format(mode)\n    iostream = BytesIO()\n    write(iostream, sampling_rate, signal)\n    summary = tf.Summary.Audio(encoded_audio_string=iostream.getvalue())\n    summary = tf.Summary.Value(tag=tag, audio=summary)\n    return summary\n  elif save_format == ""disk"":\n    file_name = \'{}/sample_step{}_{}_{}.wav\'.format(logdir, step, number, mode)\n    if logdir[0] != \'/\':\n      file_name = ""./"" + file_name\n    write(file_name, sampling_rate, signal)\n    return None\n  else:\n    print((\n            ""WARN: The save format passed to save_audio was not understood. No ""\n            ""sound files will be saved for the current step. ""\n            ""Received \'{}\'.""\n            ""Expected one of \'np.array\', \'tensorboard\', or \'disk\'""\n          ).format(save_format))\n    return None\n\n\ndef griffin_lim(magnitudes, n_iters=50, n_fft=1024):\n  """"""\n  Griffin-Lim algorithm to convert magnitude spectrograms to audio signals\n  """"""\n\n  phase = np.exp(2j * np.pi * np.random.rand(*magnitudes.shape))\n  complex_spec = magnitudes * phase\n  signal = librosa.istft(complex_spec)\n  if not np.isfinite(signal).all():\n    print(""WARNING: audio was not finite, skipping audio saving"")\n    return np.array([0])\n\n  for _ in range(n_iters):\n    _, phase = librosa.magphase(librosa.stft(signal, n_fft=n_fft))\n    complex_spec = magnitudes * phase\n    signal = librosa.istft(complex_spec)\n  return signal\n\n\nclass Text2Speech(EncoderDecoderModel):\n  """"""\n  Text-to-speech data layer.\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(\n        EncoderDecoderModel.get_required_params(), **{\n            ""save_to_tensorboard"": bool,\n        }\n    )\n\n  def __init__(self, params, mode=""train"", hvd=None):\n    super(Text2Speech, self).__init__(params, mode=mode, hvd=hvd)\n    self._save_to_tensorboard = self.params[""save_to_tensorboard""]\n\n  def print_logs(self,\n                 mode,\n                 specs,\n                 titles,\n                 stop_token_pred,\n                 stop_target,\n                 audio_length,\n                 step,\n                 predicted_final_spec,\n                 predicted_mag_spec=None):\n    """"""\n    Save audio files and plots.\n\n    Args:\n      mode: ""train"" or ""eval"".\n      specs: spectograms to plot.\n      titles: spectogram titles.\n      stop_token_pred: stop token prediction.\n      stop_target: stop target.\n      audio_length: length of the audio.\n      step: current step.\n      predicted_final_spec: predicted mel spectogram.\n      predicted_mag_spec: predicted magnitude spectogram.\n\n    Returns:\n      Dictionary to log.\n    """"""\n\n    dict_to_log = {}\n\n    im_summary = plot_spectrograms(\n        specs,\n        titles,\n        stop_token_pred,\n        audio_length,\n        self.params[""logdir""],\n        step,\n        append=mode,\n        save_to_tensorboard=self._save_to_tensorboard,\n        stop_token_target=stop_target\n    )\n\n    dict_to_log[\'image\'] = im_summary\n\n    if audio_length < 3:\n      return {}\n\n    if self._save_to_tensorboard:\n      save_format = ""tensorboard""\n    else:\n      save_format = ""disk""\n\n    if predicted_mag_spec is not None:\n      predicted_mag_spec = predicted_mag_spec[:audio_length - 1, :]\n\n      if self.get_data_layer()._exp_mag is False:\n        predicted_mag_spec = np.exp(predicted_mag_spec)\n\n      predicted_mag_spec = self.get_data_layer().get_magnitude_spec(predicted_mag_spec)\n      wav_summary = save_audio(\n          predicted_mag_spec,\n          self.params[""logdir""],\n          step,\n          n_fft=self.get_data_layer().n_fft,\n          sampling_rate=self.get_data_layer().sampling_rate,\n          mode=mode + ""_mag"",\n          save_format=save_format\n      )\n      dict_to_log[\'audio_mag\'] = wav_summary\n\n    predicted_final_spec = predicted_final_spec[:audio_length - 1, :]\n    predicted_final_spec = self.get_data_layer().get_magnitude_spec(\n        predicted_final_spec,\n        is_mel=True\n    )\n    wav_summary = save_audio(\n        predicted_final_spec,\n        self.params[""logdir""],\n        step,\n        n_fft=self.get_data_layer().n_fft,\n        sampling_rate=self.get_data_layer().sampling_rate,\n        mode=mode,\n        save_format=save_format,\n        max_normalization=self.get_data_layer().max_normalization\n    )\n    dict_to_log[\'audio\'] = wav_summary\n\n    if self._save_to_tensorboard:\n      return dict_to_log\n\n    return {}\n\n  def infer(self, input_values, output_values):\n    if self.on_horovod:\n      raise ValueError(""Inference is not supported on horovod"")\n\n    return [input_values, output_values]\n\n  def evaluate(self, input_values, output_values):\n    # Need to reduce amount of data sent for horovod\n    # Use last element\n    idx = -1\n    output_values = [(item[idx]) for item in output_values]\n    input_values = {\n        key: [value[0][idx], value[1][idx]] for key, value in input_values.items()\n    }\n    return [input_values, output_values]\n\n  def get_alignments(self, attention_mask):\n    """"""\n    Get attention alignment plots.\n\n    Args:\n      attention_mask: attention alignment.\n\n    Returns:\n      Specs and titles to plot.\n    """"""\n\n    raise NotImplementedError()\n\n  def finalize_inference(self, results_per_batch, output_file):\n    print(""output_file is ignored for tts"")\n    print(""results are logged to the logdir"")\n\n    batch_size = len(results_per_batch[0][0][""source_tensors""][0])\n    for i, sample in enumerate(results_per_batch):\n      output_values = sample[1]\n      predicted_final_specs = output_values[1]\n      attention_mask = output_values[2]\n      stop_tokens = output_values[3]\n      sequence_lengths = output_values[4]\n\n      for j in range(len(predicted_final_specs)):\n        predicted_final_spec = predicted_final_specs[j]\n        attention_mask_sample = attention_mask[j]\n        stop_tokens_sample = stop_tokens[j]\n\n        specs = [predicted_final_spec]\n        titles = [""final spectrogram""]\n        audio_length = sequence_lengths[j]\n\n        alignment_specs, alignment_titles = self.get_alignments(attention_mask_sample)\n        specs += alignment_specs\n        titles += alignment_titles\n\n        if ""mel"" in self.get_data_layer().params[""output_type""]:\n          mag_spec = self.get_data_layer().get_magnitude_spec(predicted_final_spec)\n          log_mag_spec = np.log(np.clip(mag_spec, a_min=1e-5, a_max=None))\n          specs.append(log_mag_spec)\n          titles.append(""magnitude spectrogram"")\n        elif ""both"" in self.get_data_layer().params[""output_type""]:\n          mag_spec = self.get_data_layer().get_magnitude_spec(predicted_final_spec, is_mel=True)\n          specs.append(mag_spec)\n          titles.append(""mag spectrogram from mel basis"")\n          specs.append(output_values[5][j])\n          titles.append(""mag spectrogram from proj layer"")\n\n        im_summary = plot_spectrograms(\n            specs,\n            titles,\n            stop_tokens_sample,\n            audio_length,\n            self.params[""logdir""],\n            0,\n            number=i * batch_size + j,\n            append=""infer""\n        )\n\n        if audio_length > 2:\n          if ""both"" in self.get_data_layer().params[""output_type""]:\n            predicted_mag_spec = output_values[5][j][:audio_length - 1, :]\n            wav_summary = save_audio(\n              predicted_mag_spec,\n              self.params[""logdir""],\n              0,\n              n_fft=self.get_data_layer().n_fft,\n              sampling_rate=self.get_data_layer().sampling_rate,\n              mode=""infer_mag"",\n              number=i * batch_size + j,\n              save_format=""disk"",\n              max_normalization=self.get_data_layer().max_normalization\n            )\n          predicted_final_spec = predicted_final_spec[:audio_length - 1, :]\n          predicted_final_spec = self.get_data_layer().get_magnitude_spec(predicted_final_spec, is_mel=True)\n          wav_summary = save_audio(\n              predicted_final_spec,\n              self.params[""logdir""],\n              0,\n              n_fft=self.get_data_layer().n_fft,\n              sampling_rate=self.get_data_layer().sampling_rate,\n              mode=""infer"",\n              number=i * batch_size + j,\n              save_format=""disk"",\n              max_normalization=self.get_data_layer().max_normalization\n          )\n\n  def finalize_evaluation(self, results_per_batch, training_step=None, samples_count=1):\n    sample = results_per_batch[0]\n\n    input_values = sample[0]\n    output_values = sample[1]\n    y_sample, stop_target = input_values[""target_tensors""]\n    predicted_spec = output_values[0]\n    predicted_final_spec = output_values[1]\n    attention_mask = output_values[2]\n    stop_token_pred = output_values[3]\n    audio_length = output_values[4]\n\n    max_length = np.max([\n      y_sample.shape[0],\n      predicted_final_spec.shape[0],\n    ])\n    predictions_pad = np.zeros(\n        [max_length - np.shape(predicted_final_spec)[0], np.shape(predicted_final_spec)[-1]]\n    )\n    stop_token_pred_pad = np.zeros(\n        [max_length - np.shape(predicted_final_spec)[0], 1]\n    )\n    spec_pad = np.zeros([max_length - np.shape(y_sample)[0], np.shape(y_sample)[-1]])\n    stop_token_pad = np.zeros([max_length - np.shape(y_sample)[0]])\n\n    predicted_spec = np.concatenate(\n        [predicted_spec, predictions_pad], axis=0\n    )\n    predicted_final_spec = np.concatenate(\n        [predicted_final_spec, predictions_pad], axis=0\n    )\n    stop_token_pred = np.concatenate(\n        [stop_token_pred, stop_token_pred_pad], axis=0\n    )\n    y_sample = np.concatenate([y_sample, spec_pad], axis=0)\n    stop_target = np.concatenate([stop_target, stop_token_pad], axis=0)\n\n    specs = [\n        y_sample,\n        predicted_spec,\n        predicted_final_spec\n    ]\n    titles = [\n        ""training data"",\n        ""decoder results"",\n        ""post net results""\n    ]\n\n    alignment_specs, alignment_titles = self.get_alignments(attention_mask)\n    specs += alignment_specs\n    titles += alignment_titles\n\n    predicted_mag_spec = None\n\n    if ""both"" in self.get_data_layer().params[""output_type""]:\n      n_feats = self.get_data_layer().params[""num_audio_features""]\n      predicted_mag_spec = output_values[5]\n      mag_pred_pad = np.zeros(\n          [max_length - np.shape(predicted_mag_spec)[0], n_feats[""magnitude""]]\n      )\n      predicted_mag_spec = np.concatenate([predicted_mag_spec, mag_pred_pad], axis=0)\n      specs.append(predicted_mag_spec)\n      titles.append(""magnitude spectrogram"")\n      mel, mag = np.split(\n          y_sample,\n          [n_feats[""mel""]],\n          axis=1\n      )\n      specs.insert(0, mel)\n      specs[1] = mag\n      titles.insert(0, ""target mel"")\n      titles[1] = ""target mag""\n\n    return self.print_logs(\n        mode=""eval"",\n        specs=specs,\n        titles=titles,\n        stop_token_pred=stop_token_pred,\n        stop_target=stop_target[0],\n        audio_length=audio_length,\n        step=training_step,\n        predicted_final_spec=predicted_final_spec,\n        predicted_mag_spec=predicted_mag_spec\n    )\n\n  def maybe_print_logs(self, input_values, output_values, training_step):\n    spec, stop_target, _ = input_values[\'target_tensors\']\n    predicted_decoder_spec = output_values[0]\n    predicted_final_spec = output_values[1]\n    attention_mask = output_values[2]\n    stop_token_pred = output_values[3]\n    y_sample = spec[0]\n    stop_target = stop_target[0]\n    predicted_spec = predicted_decoder_spec[0]\n    predicted_final_spec = predicted_final_spec[0]\n    alignment = attention_mask[0]\n    stop_token_pred = stop_token_pred[0]\n    audio_length = output_values[4][0]\n\n    specs = [\n        y_sample,\n        predicted_spec,\n        predicted_final_spec\n    ]\n\n    titles = [\n        ""training data"",\n        ""decoder results"",\n        ""post net results""\n    ]\n\n    alignment_specs, alignment_titles = self.get_alignments(alignment)\n    specs += alignment_specs\n    titles += alignment_titles\n\n    predicted_mag_spec = None\n\n    if ""both"" in self.get_data_layer().params[""output_type""]:\n      predicted_mag_spec = output_values[5][0]\n      specs.append(predicted_mag_spec)\n      titles.append(""magnitude spectrogram"")\n      n_feats = self.get_data_layer().params[""num_audio_features""]\n      mel, mag = np.split(\n          y_sample,\n          [n_feats[""mel""]],\n          axis=1\n      )\n      specs.insert(0, mel)\n      specs[1] = mag\n      titles.insert(0, ""target mel"")\n      titles[1] = ""target mag""\n\n    return self.print_logs(\n        mode=""train"",\n        specs=specs,\n        titles=titles,\n        stop_token_pred=stop_token_pred,\n        stop_target=stop_target,\n        audio_length=audio_length,\n        step=training_step,\n        predicted_final_spec=predicted_final_spec,\n        predicted_mag_spec=predicted_mag_spec\n    )\n\n'"
open_seq2seq/models/text2speech_centaur.py,0,"b'# Copyright (c) 2019 NVIDIA Corporation\n\nfrom six.moves import range\n\nfrom .text2speech import Text2Speech\n\n\nclass Text2SpeechCentaur(Text2Speech):\n  """"""\n  Text-to-speech data layer for Centaur.\n  """"""\n\n  def get_alignments(self, attention_mask):\n    alignments_name = [""dec_enc_alignment""]\n\n    specs = []\n    titles = []\n\n    for name, alignment in zip(alignments_name, attention_mask):\n      for layer in range(len(alignment)):\n        for head in range(alignment.shape[1]):\n          specs.append(alignment[layer][head])\n          titles.append(""{}_layer_{}_head_{}"".format(name, layer, head))\n\n    return specs, titles\n'"
open_seq2seq/models/text2speech_tacotron.py,0,"b'# Copyright (c) 2019 NVIDIA Corporation\n\nfrom .text2speech import Text2Speech\n\n\nclass Text2SpeechTacotron(Text2Speech):\n  """"""\n  Text-to-speech data layer for Tacotron.\n  """"""\n\n  def get_alignments(self, attention_mask):\n    specs = [attention_mask]\n    titles = [""alignments""]\n    return specs, titles\n'"
open_seq2seq/models/text2speech_wavenet.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\nimport numpy as np\nfrom scipy.io.wavfile import write\n\nfrom .encoder_decoder import EncoderDecoderModel\n\ndef save_audio(signal, logdir, step, sampling_rate, mode):\n  signal = np.float32(signal)\n  file_name = \'{}/sample_step{}_{}.wav\'.format(logdir, step, mode)\n  if logdir[0] != \'/\':\n    file_name = ""./"" + file_name\n  write(file_name, sampling_rate, signal)\n\nclass Text2SpeechWavenet(EncoderDecoderModel):\n\n  @staticmethod\n  def get_required_params():\n    return dict(\n        EncoderDecoderModel.get_required_params(), **{}\n    )\n\n  def __init__(self, params, mode=""train"", hvd=None):\n    super(Text2SpeechWavenet, self).__init__(params, mode=mode, hvd=hvd)\n\n  def maybe_print_logs(self, input_values, output_values, training_step):\n    save_audio(\n        output_values[1][-1],\n        self.params[""logdir""],\n        training_step,\n        sampling_rate=22050,\n        mode=""train""\n    )\n    return {}\n\n  def evaluate(self, input_values, output_values):\n    return output_values[1][-1]\n\n  def finalize_evaluation(self, results_per_batch, training_step=None):\n    save_audio(\n        results_per_batch[0],\n        self.params[""logdir""],\n        training_step,\n        sampling_rate=22050,\n        mode=""eval""\n    )\n    return {}\n\n  def infer(self, input_values, output_values):\n    return output_values[1][-1]\n\n  def finalize_inference(self, results_per_batch, output_file):\n    return {}\n'"
open_seq2seq/models/text2text.py,4,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport codecs\nimport re\n\nimport nltk\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.utils.utils import deco_print, array_to_string, \\\n                                     text_ids_to_string\nfrom .encoder_decoder import EncoderDecoderModel\n\n\ndef transform_for_bleu(row, vocab, ignore_special=False,\n                       delim=\' \', bpe_used=False):\n  n = len(vocab)\n  if ignore_special:\n    f_row = []\n    for i in range(0, len(row)):\n      char_id = row[i]\n      if char_id == SpecialTextTokens.EOS_ID.value:\n        break\n      if char_id != SpecialTextTokens.PAD_ID.value and \\\n         char_id != SpecialTextTokens.S_ID.value:\n        f_row += [char_id]\n    sentence = [vocab[r] for r in f_row if 0 < r < n]\n  else:\n    sentence = [vocab[r] for r in row if 0 < r < n]\n\n  if bpe_used:\n    sentence = delim.join(sentence)\n    sentence = re.sub(""@@ "", """", sentence)\n    sentence = sentence.split(delim)\n\n  return sentence\n\n\ndef calculate_bleu(preds, targets):\n  """"""Function to calculate BLEU score.\n\n  Args:\n    preds: list of lists\n    targets: list of lists\n\n  Returns:\n    float32: BLEU score\n  """"""\n  bleu_score = nltk.translate.bleu_score.corpus_bleu(\n      targets, preds, emulate_multibleu=True,\n  )\n  return bleu_score\n\n\nclass Text2Text(EncoderDecoderModel):\n  """"""An example class implementing classical text-to-text model.""""""\n  def _create_encoder(self):\n    self.params[\'encoder_params\'][\'src_vocab_size\'] = (\n        self.get_data_layer().params[\'src_vocab_size\']\n    )\n    return super(Text2Text, self)._create_encoder()\n\n  def _create_decoder(self):\n    self.params[\'decoder_params\'][\'batch_size\'] = (\n        self.params[\'batch_size_per_gpu\']\n    )\n    self.params[\'decoder_params\'][\'tgt_vocab_size\'] = (\n        self.get_data_layer().params[\'tgt_vocab_size\']\n    )\n    return super(Text2Text, self)._create_decoder()\n\n  def _create_loss(self):\n    self.params[\'loss_params\'][\'batch_size\'] = self.params[\'batch_size_per_gpu\']\n    self.params[\'loss_params\'][\'tgt_vocab_size\'] = (\n        self.get_data_layer().params[\'tgt_vocab_size\']\n    )\n    return super(Text2Text, self)._create_loss()\n\n  def infer(self, input_values, output_values):\n    input_strings, output_strings = [], []\n    input_values = input_values[\'source_tensors\']\n    for input_sample, output_sample in zip(input_values, output_values):\n      for i in range(0, input_sample.shape[0]):  # iterate over batch dimension\n        output_strings.append(text_ids_to_string(\n            output_sample[i],\n            self.get_data_layer().params[\'target_idx2seq\'],\n            S_ID=self.decoder.params.get(\'GO_SYMBOL\',\n                                         SpecialTextTokens.S_ID.value),\n            EOS_ID=self.decoder.params.get(\'END_SYMBOL\',\n                                           SpecialTextTokens.EOS_ID),\n            PAD_ID=self.decoder.params.get(\'PAD_SYMBOL\',\n                                           SpecialTextTokens.PAD_ID),\n            ignore_special=True, delim=\' \',\n        ))\n        input_strings.append(text_ids_to_string(\n            input_sample[i],\n            self.get_data_layer().params[\'source_idx2seq\'],\n            S_ID=self.decoder.params.get(\'GO_SYMBOL\',\n                                         SpecialTextTokens.S_ID.value),\n            EOS_ID=self.decoder.params.get(\'END_SYMBOL\',\n                                       SpecialTextTokens.EOS_ID.value),\n            PAD_ID=self.decoder.params.get(\'PAD_SYMBOL\',\n                                           SpecialTextTokens.PAD_ID),\n            ignore_special=True, delim=\' \',\n        ))\n    return input_strings, output_strings\n\n  def finalize_inference(self, results_per_batch, output_file):\n    with codecs.open(output_file, \'w\', \'utf-8\') as fout:\n      step = 0\n      for input_strings, output_strings in results_per_batch:\n        for input_string, output_string in zip(input_strings, output_strings):\n          fout.write(output_string + ""\\n"")\n          if step % 200 == 0:\n            deco_print(""Input sequence:  {}"".format(input_string))\n            deco_print(""Output sequence: {}"".format(output_string))\n            deco_print("""")\n          step += 1\n\n  def maybe_print_logs(self, input_values, output_values, training_step):\n    x, len_x = input_values[\'source_tensors\']\n    y, len_y = input_values[\'target_tensors\']\n    samples = output_values[0]\n\n    x_sample = x[0]\n    len_x_sample = len_x[0]\n    y_sample = y[0]\n    len_y_sample = len_y[0]\n\n    deco_print(\n        ""Train Source[0]:     "" + array_to_string(\n            x_sample[:len_x_sample],\n            vocab=self.get_data_layer().params[\'source_idx2seq\'],\n            delim=self.get_data_layer().params[""delimiter""],\n        ),\n        offset=4,\n    )\n    deco_print(\n        ""Train Target[0]:     "" + array_to_string(\n            y_sample[:len_y_sample],\n            vocab=self.get_data_layer().params[\'target_idx2seq\'],\n            delim=self.get_data_layer().params[""delimiter""],\n        ),\n        offset=4,\n    )\n    deco_print(\n        ""Train Prediction[0]: "" + array_to_string(\n            samples[0, :],\n            vocab=self.get_data_layer().params[\'target_idx2seq\'],\n            delim=self.get_data_layer().params[""delimiter""],\n        ),\n        offset=4,\n    )\n    return {}\n\n  def evaluate(self, input_values, output_values):\n    ex, elen_x = input_values[\'source_tensors\']\n    ey, elen_y = input_values[\'target_tensors\']\n\n    x_sample = ex[0]\n    len_x_sample = elen_x[0]\n    y_sample = ey[0]\n    len_y_sample = elen_y[0]\n\n    deco_print(\n        ""*****EVAL Source[0]:     "" + array_to_string(\n            x_sample[:len_x_sample],\n            vocab=self.get_data_layer().params[\'source_idx2seq\'],\n            delim=self.get_data_layer().params[""delimiter""],\n        ),\n        offset=4,\n    )\n    deco_print(\n        ""*****EVAL Target[0]:     "" + array_to_string(\n            y_sample[:len_y_sample],\n            vocab=self.get_data_layer().params[\'target_idx2seq\'],\n            delim=self.get_data_layer().params[""delimiter""],\n        ),\n        offset=4,\n    )\n    samples = output_values[0]\n    deco_print(\n        ""*****EVAL Prediction[0]: "" + array_to_string(\n            samples[0, :],\n            vocab=self.get_data_layer().params[\'target_idx2seq\'],\n            delim=self.get_data_layer().params[""delimiter""],\n        ),\n        offset=4,\n    )\n    preds, targets = [], []\n\n    if self.params.get(\'eval_using_bleu\', True):\n      preds.extend([transform_for_bleu(\n          sample,\n          vocab=self.get_data_layer().params[\'target_idx2seq\'],\n          ignore_special=True,\n          delim=self.get_data_layer().params[""delimiter""],\n          bpe_used=self.params.get(\'bpe_used\', False),\n      ) for sample in samples])\n      targets.extend([[transform_for_bleu(\n          yi,\n          vocab=self.get_data_layer().params[\'target_idx2seq\'],\n          ignore_special=True,\n          delim=self.get_data_layer().params[""delimiter""],\n          bpe_used=self.params.get(\'bpe_used\', False),\n      )] for yi in ey])\n\n    return preds, targets\n\n  def finalize_evaluation(self, results_per_batch, training_step=None):\n    preds, targets = [], []\n    for preds_cur, targets_cur in results_per_batch:\n      if self.params.get(\'eval_using_bleu\', True):\n        preds.extend(preds_cur)\n        targets.extend(targets_cur)\n\n    if self.params.get(\'eval_using_bleu\', True):\n      eval_bleu = calculate_bleu(preds, targets)\n      deco_print(""Eval BLUE score: {}"".format(eval_bleu), offset=4)\n      return {\'Eval_BLEU_Score\': eval_bleu}\n\n    return {}\n\n  def _get_num_objects_per_step(self, worker_id=0):\n    """"""Returns number of source tokens + number of target tokens in batch.""""""\n    data_layer = self.get_data_layer(worker_id)\n    # sum of source length in batch\n    num_tokens = tf.reduce_sum(data_layer.input_tensors[\'source_tensors\'][1])\n    if self.mode != ""infer"":\n      # sum of target length in batch\n      num_tokens += tf.reduce_sum(data_layer.input_tensors[\'target_tensors\'][1])\n    else:\n      # this will count padding for batch size > 1. Need to be changed\n      # if that\'s not expected behaviour\n      num_tokens += tf.reduce_sum(\n          tf.shape(self.get_output_tensors(worker_id)[0])\n      )\n    return num_tokens\n'"
open_seq2seq/models/text2text_test.py,9,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport runpy\n\nimport tensorflow as tf\n\nfrom open_seq2seq.test_utils.create_reversed_examples import create_data, \\\n                                                             remove_data\n\n\nclass BasicText2TextWithAttentionTest(tf.test.TestCase):\n  def setUp(self):\n    print(""Setting Up BasicSeq2SeqWithAttention"")\n    create_data(train_corpus_size=500, data_path=\'tmp2\')\n\n  def tearDown(self):\n    print(""Tear down BasicSeq2SeqWithAttention"")\n    remove_data(data_path=\'tmp2\')\n\n  def test_train(self):\n    config_module = runpy.run_path(\n        ""./example_configs/text2text/toy-reversal/nmt-reversal-RR.py""\n    )\n    train_config = config_module[\'base_params\']\n    if \'train_params\' in config_module:\n      train_config.update(config_module[\'train_params\'])\n\n    # TODO: should we maybe have just a single directory parameter?\n    train_config[\'data_layer_params\'][\'src_vocab_file\'] = (\n        ""tmp2/vocab/source.txt""\n    )\n    train_config[\'data_layer_params\'][\'tgt_vocab_file\'] = (\n        ""tmp2/vocab/target.txt""\n    )\n    train_config[\'data_layer_params\'][\'source_file\'] = (\n        ""tmp2/train/source.txt""\n    )\n    train_config[\'data_layer_params\'][\'target_file\'] = (\n        ""tmp2/train/target.txt""\n    )\n\n    step = 0\n    with tf.Graph().as_default():\n      model = config_module[\'base_model\'](train_config, ""train"", None)\n      model.compile()\n      with self.test_session(use_gpu=True) as sess:\n        tf.global_variables_initializer().run()\n        sess.run(model.get_data_layer().iterator.initializer)\n        while True:\n          try:\n            loss, _ = sess.run([model.loss, model.train_op])\n          except tf.errors.OutOfRangeError:\n            break\n          step += 1\n          if step >= 25:\n            break\n\n\nclass BasicText2TextWithAttentionTestOnHorovod(tf.test.TestCase):\n  def setUp(self):\n    print(""Setting Up BasicSeq2SeqWithAttention on Horovod"")\n    create_data(train_corpus_size=500, data_path=\'tmp3\')\n\n  def tearDown(self):\n    print(""Tear down BasicSeq2SeqWithAttention on Horovod"")\n    remove_data(data_path=\'tmp3\')\n\n  def test_train(self):\n    try:\n      import horovod.tensorflow as hvd\n    except ImportError:\n      print(""Could not test on Horovod. Is it installed?"")\n      return\n\n    print(""Attempting BasicSeq2SeqWithAttention on Horovod"")\n    hvd.init()\n    config_module = runpy.run_path(\n        ""./example_configs/text2text/toy-reversal/nmt-reversal-RR.py""\n    )\n    train_config = config_module[\'base_params\']\n    if \'train_params\' in config_module:\n      train_config.update(config_module[\'train_params\'])\n\n    train_config[\'data_layer_params\'][\'src_vocab_file\'] = (\n        ""tmp3/vocab/source.txt""\n    )\n    train_config[\'data_layer_params\'][\'tgt_vocab_file\'] = (\n        ""tmp3/vocab/target.txt""\n    )\n    train_config[\'data_layer_params\'][\'source_file\'] = (\n        ""tmp3/train/source.txt""\n    )\n    train_config[\'data_layer_params\'][\'target_file\'] = (\n        ""tmp3/train/target.txt""\n    )\n    train_config[""use_horovod""] = True\n    step = 0\n    with tf.Graph().as_default():\n      model = config_module[\'base_model\'](train_config, ""train"", None)\n      model.compile()\n      with self.test_session(use_gpu=True) as sess:\n        tf.global_variables_initializer().run()\n        sess.run(model.get_data_layer().iterator.initializer)\n        while True:\n          try:\n            loss, _ = sess.run(\n                [model.loss, model.train_op]\n            )\n          except tf.errors.OutOfRangeError:\n            break\n          step += 1\n          if step >= 25:\n            break\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
open_seq2seq/optimizers/__init__.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom .optimizers import optimize_loss, get_regularization_loss\n'"
open_seq2seq/optimizers/automatic_loss_scaler.py,55,"b""# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.utils.utils import check_params\n\nclass AutomaticLossScaler(object):\n  SUPPORTED_ALGOS = ['backoff', 'logmax']\n\n  def __init__(self, algorithm='Backoff', params=None):\n    algorithm = algorithm.lower().strip()\n    if algorithm == 'backoff':\n      self.scaler = BackoffScaler(params)\n    elif algorithm == 'logmax':\n      self.scaler = LogMaxScaler(params)  # ppf(.999)\n    else:\n      raise ValueError('Unknown scaling algorithm: {}'.format(algorithm))\n\n  def update_op(self, has_nan, amax):\n    return self.scaler.update_op(has_nan, amax)\n\n  @property\n  def loss_scale(self):\n    return self.scaler.loss_scale\n\n  @staticmethod\n  def check_grads(grads_and_vars):\n    has_nan_ops = []\n    amax_ops = []\n\n    for grad, _ in grads_and_vars:\n      if grad is not None:\n        if isinstance(grad, tf.IndexedSlices):\n          x = grad.values\n        else:\n          x = grad\n\n        has_nan_ops.append(tf.reduce_any(tf.is_nan(x)))\n        amax_ops.append(tf.reduce_max(tf.abs(x)))\n\n    has_nan = tf.reduce_any(has_nan_ops)\n    amax = tf.reduce_max(amax_ops)\n    return has_nan, amax\n\n\nclass BackoffScaler(object):\n  def __init__(self, params):\n    if params is None:\n      params = {}\n    check_params(\n        config=params,\n        required_dict={},\n        optional_dict={\n            'scale_min': float,\n            'scale_max': float,\n            'step_factor': float,\n            'step_window': int\n        },\n    )\n    self.scale_min = params.get('scale_min', 1.0)\n    self.scale_max = params.get('scale_max', 2.**14)\n    self.step_factor = params.get('step_factor', 2.0)\n    self.step_window = params.get('step_window', 2000)\n\n    self.iteration = tf.Variable(initial_value=0,\n                                 trainable=False,\n                                 dtype=tf.int64)\n    self.last_overflow_iteration = tf.Variable(initial_value=-1,\n                                               trainable=False,\n                                               dtype=tf.int64)\n    self.scale = tf.Variable(initial_value=self.scale_max,\n                             trainable=False)\n\n  def update_op(self, has_nan, amax):\n    def overflow_case():\n      new_scale_val = tf.clip_by_value(self.scale / self.step_factor,\n                                       self.scale_min, self.scale_max)\n      scale_assign = tf.assign(self.scale, new_scale_val)\n      overflow_iter_assign = tf.assign(self.last_overflow_iteration,\n                                       self.iteration)\n      with tf.control_dependencies([scale_assign, overflow_iter_assign]):\n        return tf.identity(self.scale)\n\n    def scale_case():\n      since_overflow = self.iteration - self.last_overflow_iteration\n      should_update = tf.equal(since_overflow % self.step_window, 0)\n      def scale_update_fn():\n        new_scale_val = tf.clip_by_value(self.scale * self.step_factor,\n                                         self.scale_min, self.scale_max)\n        return tf.assign(self.scale, new_scale_val)\n      return tf.cond(should_update,\n                     scale_update_fn,\n                     lambda: self.scale)\n\n    iter_update = tf.assign_add(self.iteration, 1)\n    overflow = tf.logical_or(has_nan, tf.is_inf(amax))\n\n    update_op = tf.cond(overflow,\n                        overflow_case,\n                        scale_case)\n    with tf.control_dependencies([update_op]):\n      return tf.identity(iter_update)\n\n  @property\n  def loss_scale(self):\n    return self.scale\n\n\nclass LogMaxScaler(object):\n  def __init__(self, params):\n    if params is None:\n      params = {}\n    check_params(\n        config=params,\n        required_dict={},\n        optional_dict={\n            'scale_min': float,\n            'scale_max': float,\n            'log_max': float,\n            'beta1': float,\n            'beta2': float,\n            'overflow_std_dev': float\n        },\n    )\n    self.scale_min = params.get('scale_min', 1.0)\n    self.scale_max = params.get('scale_max', 2.**14)\n    self.log_max = params.get('log_max', 16.)\n    self.beta1 = params.get('beta1', 0.99)\n    self.beta2 = params.get('beta2', 0.999)\n    self.overflow_std_dev = params.get('overflow_std_dev', 3.09)\n\n    self.iteration = tf.Variable(initial_value=0,\n                                 trainable=False,\n                                 dtype=tf.int64)\n    self.scale = tf.Variable(initial_value=1.0,\n                             trainable=False)\n    self.x_hat = tf.Variable(initial_value=0,\n                             trainable=False,\n                             dtype=tf.float32)\n    self.slow_x_hat = tf.Variable(initial_value=0,\n                                  trainable=False,\n                                  dtype=tf.float32)\n    self.xsquared_hat = tf.Variable(initial_value=0,\n                                    trainable=False,\n                                    dtype=tf.float32)\n    self.b1_correction = tf.Variable(initial_value=1.,\n                                     trainable=False,\n                                     dtype=tf.float32)\n    self.b2_correction = tf.Variable(initial_value=1.,\n                                     trainable=False,\n                                     dtype=tf.float32)\n\n  # NB: assumes that `amax` is already has been downscaled\n  def update_op(self, has_nan, amax):\n    is_nonfinite = tf.logical_or(has_nan, tf.is_inf(amax))\n    x = tf.cond(is_nonfinite,\n                lambda: tf.pow(2., self.log_max),\n                lambda: tf.log(amax) / tf.log(tf.constant(2.)))\n\n    x_hat_assn = tf.assign(self.x_hat, self.beta1 * self.x_hat +\n                           (1 - self.beta1) * x)\n    b1_corr_assn = tf.assign(self.b1_correction,\n                             self.b1_correction * self.beta1)\n    with tf.control_dependencies([x_hat_assn, b1_corr_assn]):\n      mu = self.x_hat.read_value() / (1 - self.b1_correction.read_value())\n\n    slow_x_hat_assn = tf.assign(self.slow_x_hat, self.beta2 * self.slow_x_hat +\n                                (1 - self.beta2) * x)\n    xsquared_hat_assn = tf.assign(\n        self.xsquared_hat,\n        self.beta2 * self.xsquared_hat + (1 - self.beta2) * (x * x),\n    )\n    b2_corr_assn = tf.assign(self.b2_correction,\n                             self.b2_correction * self.beta2)\n    with tf.control_dependencies([slow_x_hat_assn, xsquared_hat_assn,\n                                  b2_corr_assn]):\n      e_xsquared = self.xsquared_hat.read_value() / \\\n                   (1 - self.b2_correction.read_value())\n      slow_mu = self.slow_x_hat.read_value() / \\\n                (1 - self.b2_correction.read_value())\n\n    sigma2 = e_xsquared - (slow_mu * slow_mu)\n    sigma = tf.sqrt(tf.maximum(sigma2, tf.constant(0.)))\n\n    log_cutoff = sigma * self.overflow_std_dev + mu\n    log_difference = 16 - log_cutoff\n    proposed_scale = tf.pow(2., log_difference)\n    scale_update = tf.assign(\n        self.scale,\n        tf.clip_by_value(proposed_scale, self.scale_min, self.scale_max),\n    )\n    iter_update = tf.assign_add(self.iteration, 1)\n\n    with tf.control_dependencies([scale_update]):\n      return tf.identity(iter_update)\n\n  @property\n  def loss_scale(self):\n    return self.scale\n"""
open_seq2seq/optimizers/lr_policies.py,21,"b'# Copyright (c) 2017 NVIDIA Corporation\n""""""\nModule containing various learning rate policies. Learning rate policy can\nbe any function that takes arbitrary arguments from the config (with additional\n``global_step`` variable provided automatically) and returns learning rate\nvalue for the current step.\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport math\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n\ndef fixed_lr(global_step, learning_rate):\n  """"""Fixed learning rate policy.\n  This function always returns ``learning_rate``, ignoring ``global_step``\n  value.\n\n  Args:\n    global_step: global step TensorFlow tensor (ignored for this policy).\n    learning_rate (float): fixed learning rate to use.\n\n  Returns:\n    learning rate at step ``global_step``.\n  """"""\n  return learning_rate\n\n\ndef piecewise_constant(global_step, learning_rate, boundaries,\n                       decay_rates, steps_per_epoch=None):\n  """"""Piecewise constant learning rate decay.\n  When defined in the config, only ``boundaries`` and ``decay_rates`` need to\n  be provided (other parameters are automatically populated by\n  :class:`Model<models.model.Model>` class). ``boundaries`` are treated as\n  epochs if ``num_epochs`` is provided in the config, otherwise treated as\n  steps.\n\n  Args:\n    global_step: global step TensorFlow tensor.\n    learning_rate (float): initial learning rate to use.\n    boundaries (list): could be either defined in steps\n        (if ``batches_per_epoch=None``) or in epochs if ``batches_per_epoch``\n        parameter is defined.\n    decay_rates: multiplier of the initial learning rate for each boundary.\n    steps_per_epoch: number of batches in one training epoch. If provided,\n        boundaries are treated as epochs, otherwise as steps.\n\n  Returns:\n    learning rate at step ``global_step``.\n  """"""\n  if steps_per_epoch is not None:\n    boundaries = [steps_per_epoch * epoch for epoch in boundaries]\n  decay_rates = [1.0] + decay_rates\n  vals = [learning_rate * decay for decay in decay_rates]\n  return tf.train.piecewise_constant(global_step, boundaries, vals)\n\n\ndef exp_decay(global_step, learning_rate, decay_steps, decay_rate,\n              use_staircase_decay, begin_decay_at=0, min_lr=0.0):\n  """"""Exponential decay learning rate policy.\n  This function is equivalent to ``tensorflow.train.exponential_decay`` with\n  some additional functionality. Namely, it adds ``begin_decay_at`` parameter\n  and ``min_lr`` parameter which are the first step to start decaying learning\n  rate and minimal value of the learning rate correspondingly.\n\n  Args:\n    global_step: global step TensorFlow tensor.\n    learning_rate (float): initial learning rate to use.\n    decay_steps (int): number of steps to apply decay for.\n    decay_rate (float): the rate of the decay.\n    use_staircase_decay (bool): whether to use staircase decay.\n    begin_decay_at (int): the first step to start decaying learning rate.\n    min_lr (float): minimal value of the learning rate.\n\n  Returns:\n    learning rate at step ``global_step``.\n  """"""\n  new_lr = tf.cond(\n      global_step < begin_decay_at,\n      lambda: learning_rate,\n      lambda: tf.train.exponential_decay(\n          learning_rate=learning_rate,\n          global_step=global_step-begin_decay_at,\n          decay_steps=decay_steps,\n          decay_rate=decay_rate,\n          staircase=use_staircase_decay),\n      name=""learning_rate"",\n  )\n  final_lr = tf.maximum(min_lr, new_lr)\n  return final_lr\n\n\ndef poly_decay(global_step, learning_rate, decay_steps, power=1.0,\n               begin_decay_at=0, min_lr=0.0, warmup_steps=0):\n  """"""Polynomial decay learning rate policy.\n  This function is equivalent to ``tensorflow.train.polynomial_decay`` with\n  some additional functionality. Namely, it adds ``begin_decay_at`` parameter\n  which is the first step to start decaying learning rate.\n\n  Args:\n    global_step: global step TensorFlow tensor.\n    learning_rate (float): initial learning rate to use.\n    decay_steps (int): number of steps to apply decay for.\n    power (float): power for polynomial decay.\n    begin_decay_at (int): the first step to start decaying learning rate.\n    min_lr (float): minimal value of the learning rate\n        (same as ``end_learning_rate`` TensorFlow parameter).\n\n  Returns:\n    learning rate at step ``global_step``.\n  """"""\n  if warmup_steps > 0:\n    learning_rate = tf.cond(\n      global_step < warmup_steps,\n      lambda: (learning_rate*tf.cast(global_step,tf.float32)/tf.cast(warmup_steps,tf.float32)),\n      lambda: learning_rate,\n    )\n  lr = tf.cond(\n      global_step < begin_decay_at,\n      lambda: learning_rate,\n      lambda: tf.train.polynomial_decay(\n          learning_rate=learning_rate,\n          global_step=global_step-begin_decay_at,\n          decay_steps=decay_steps,\n          end_learning_rate=min_lr,\n          power=power),\n      name=""learning_rate""\n  )\n  return lr\n\n\ndef cosine_decay(global_step, learning_rate, decay_steps, power=1.0,\n               begin_decay_at=0, min_lr=0.0, warmup_steps=0):\n  """"""cosine decay learning rate policy.\n  This function is equivalent to ``tensorflow.train.cosine_decay`` with\n  some additional functionality. Namely, it adds ``begin_decay_at`` parameter\n  which is the first step to start decaying learning rate.\n\n  Args:\n    global_step: global step TensorFlow tensor.\n    learning_rate (float): initial learning rate to use.\n    decay_steps (int): number of steps to apply decay for.\n    power (float): power for polynomial decay.\n    begin_decay_at (int): the first step to start decaying learning rate.\n    min_lr (float): minimal value of the learning rate\n        (same as ``end_learning_rate`` TensorFlow parameter).\n\n  Returns:\n    learning rate at step ``global_step``.\n  """"""\n  if warmup_steps > 0:\n    learning_rate = tf.cond(\n      global_step < warmup_steps,\n      lambda: (learning_rate*tf.cast(global_step,tf.float32)/tf.cast(warmup_steps,tf.float32)),\n      lambda: learning_rate,\n    )\n  lr = tf.cond(\n      global_step < begin_decay_at,\n      lambda: learning_rate,\n      lambda: tf.train.cosine_decay(\n          learning_rate=learning_rate,\n          global_step=global_step-begin_decay_at,\n          decay_steps=decay_steps,\n          alpha=min_lr\n          ),\n      name=""learning_rate""\n  )\n  return lr\n\n\ndef transformer_policy(global_step, learning_rate, d_model, warmup_steps,\n                       max_lr=None, coefficient=1.0, dtype=tf.float32):\n  """"""Transformer\'s learning rate policy from\n  https://arxiv.org/pdf/1706.03762.pdf\n  with a hat (max_lr) (also called ""noam"" learning rate decay scheme).\n\n  Args:\n    global_step: global step TensorFlow tensor (ignored for this policy).\n    learning_rate (float): initial learning rate to use.\n    d_model (int): model dimensionality.\n    warmup_steps (int): number of warm-up steps.\n    max_lr (float): maximal learning rate, i.e. hat.\n    coefficient (float): optimizer adjustment.\n        Recommended 0.002 if using ""Adam"" else 1.0.\n    dtype: dtype for this policy.\n\n  Returns:\n    learning rate at step ``global_step``.\n  """"""\n  step_num = tf.cast(global_step, dtype=dtype)\n  ws = tf.cast(warmup_steps, dtype=dtype)\n\n  decay = coefficient * d_model ** -0.5 * tf.minimum(\n      (step_num + 1) * ws ** -1.5, (step_num + 1) ** -0.5\n  )\n\n  new_lr = decay * learning_rate\n  if max_lr is not None:\n    return tf.minimum(max_lr, new_lr)\n  return new_lr\n\ndef inv_poly_decay(global_step, learning_rate, decay_steps, min_lr,\n              power=1.0, begin_decay_at=0, warmup_steps=0,\n              name=""learning_rate""):\n  """"""Inverse poly decay learning rate policy.\n  lr  = initial lr / ( 1+ decay * t)^power\n  This function is similar to ``tensorflow.train.inv_time_decay`` with\n  some additional functionality. Namely, it adds :\n  ``min_lr`` - end learning rate  with 0.00001\n  ``power``  - power\n  ``begin_decay_at``-  first step to start decaying learning rate.\n\n  Args:\n    global_step: global step TensorFlow tensor.\n    learning_rate (float): initial learning rate to use.\n    decay_steps (int): number of steps to apply decay for.\n    power (float): power for inv_time_decay.\n    begin_decay_at (int): the first step to start decaying learning rate.\n    min_lr (float): minimal value of the learning rate\n        (same as ``end_learning_rate`` TensorFlow parameter).\n\n  Returns:\n    learning rate at step ``global_step``.\n  """"""\n  min_lr=max(min_lr, 1e-8)\n  min_lr=min(min_lr, learning_rate)\n  if power <= 0.:\n    raise ValueError(""Inv poly decay requires power >  0."")\n  if global_step is None:\n    raise ValueError(""Inv poly decay requires global_step"")\n\n  with ops.name_scope(name, ""InvDecay"",\n                      [learning_rate, global_step]) as name:\n    scale = (math.pow(learning_rate / min_lr, 1./power) - 1.) / decay_steps\n\n    learning_rate = ops.convert_to_tensor(learning_rate, name=""learning_rate"")\n\n    decay_steps = tf.cast(decay_steps, tf.float32)\n    global_step = tf.cast(global_step, tf.float32)\n    denom = tf.pow(1. + scale * global_step , power)\n    lr = tf.div(learning_rate,  denom, name=name)\n\n  return lr\n\n\n\n\n'"
open_seq2seq/optimizers/mp_wrapper.py,22,"b'# Copyright (c) 2018 NVIDIA Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom .automatic_loss_scaler import AutomaticLossScaler\n\n\n# pylint: disable=abstract-method\nclass MixedPrecisionOptimizerWrapper(tf.train.Optimizer):\n  def __init__(self, optimizer, loss_scale=None):\n    super(MixedPrecisionOptimizerWrapper, self).__init__(\n        optimizer._use_locking,\n        optimizer._name + \'-MP\',\n    )\n    self._optimizer = optimizer\n    self._fp32_to_fp16 = {}\n    self._loss_scaler = None\n    if loss_scale is None:\n      self._loss_scale = 1.0\n    elif isinstance(loss_scale, float):\n      self._loss_scale = loss_scale\n    elif isinstance(loss_scale, AutomaticLossScaler):\n      self._loss_scaler = loss_scale\n      self._loss_scale = self._loss_scaler.loss_scale\n\n  def compute_gradients(self, loss, var_list=None,\n                        gate_gradients=tf.train.Optimizer.GATE_OP,\n                        aggregation_method=None,\n                        colocate_gradients_with_ops=False,\n                        grad_loss=None):\n    loss *= self._loss_scale\n    grads_and_vars_fp16 = self._optimizer.compute_gradients(\n        loss, var_list=var_list,\n        gate_gradients=gate_gradients,\n        aggregation_method=aggregation_method,\n        colocate_gradients_with_ops=colocate_gradients_with_ops,\n        grad_loss=grad_loss,\n    )\n\n    # collecting regularization functions\n    reg_var_funcs = tf.get_collection(\'REGULARIZATION_FUNCTIONS\')\n    reg_funcs = dict(map(lambda x: (x[0].name, x[1]), reg_var_funcs))\n\n    # creating FP-32 variables and filling the fp32 dict\n    grads_and_vars_fp32 = []\n    with tf.variable_scope(\'FP32-master-copy\'):\n      for grad, var in grads_and_vars_fp16:\n        if var.dtype.base_dtype == tf.float16:\n          fp32_var = tf.Variable(\n              initial_value=tf.cast(var.initialized_value(), tf.float32),\n              name=var.name.split(\':\')[0],\n              expected_shape=var.shape,\n              dtype=tf.float32,\n              trainable=False,\n              # necessary for cudnn_rnn layers which have unknown shape\n              validate_shape=bool(var.get_shape()),\n              collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n                           ""FP32_MASTER_COPIES""],\n          )\n          self._fp32_to_fp16[fp32_var.name] = var\n          fp32_grad = tf.cast(grad, tf.float32)\n          # adding regularization part with respect to fp32 copy\n          if var.name in reg_funcs:\n            fp32_grad += self._loss_scale * tf.gradients(\n                # pylint: disable=no-member\n                tf.contrib.layers.apply_regularization(\n                    reg_funcs[var.name],\n                    [fp32_var],\n                ),\n                fp32_var,\n            )[0]\n          grads_and_vars_fp32.append((fp32_grad, fp32_var))\n        else:\n          grads_and_vars_fp32.append((grad, var))\n\n    grads_and_vars_fp32 = _scale_grads(grads_and_vars_fp32,\n                                       1.0 / self._loss_scale)\n    return grads_and_vars_fp32\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    def apply_ops_wrapper():\n      update_op = self._optimizer.apply_gradients(grads_and_vars,\n                                                  global_step, name)\n      apply_ops = []\n      with tf.control_dependencies([update_op]):\n        for grad, var in grads_and_vars:\n          if var.name in self._fp32_to_fp16:\n            dst_var = self._fp32_to_fp16[var.name]\n            apply_ops.append(\n                tf.assign(dst_var, tf.saturate_cast(var, tf.float16))\n            )\n      if apply_ops:\n        return tf.group(apply_ops)\n      return update_op\n\n    if self._loss_scaler:\n      grad_has_nans, grad_amax = AutomaticLossScaler.check_grads(grads_and_vars)\n      should_skip_update = tf.logical_or(tf.is_inf(grad_amax), grad_has_nans)\n      loss_scale_update_op = self._loss_scaler.update_op(grad_has_nans,\n                                                         grad_amax)\n      with tf.control_dependencies([loss_scale_update_op]):\n        return tf.cond(should_skip_update, tf.no_op, apply_ops_wrapper)\n    else:\n      return apply_ops_wrapper()\n\n\ndef mp_regularizer_wrapper(regularizer):\n  def func_wrapper(weights):\n    if weights.dtype.base_dtype == tf.float16:\n      tf.add_to_collection(\'REGULARIZATION_FUNCTIONS\', (weights, regularizer))\n      # disabling the inner regularizer\n      return None\n    return regularizer(weights)\n\n  return func_wrapper\n\n\ndef _scale_grads(grads_and_vars, scale):\n  scaled_grads_and_vars = []\n  for grad, var in grads_and_vars:\n    if grad is not None:\n      if isinstance(grad, tf.IndexedSlices):\n        grad_values = grad.values * scale\n        grad = tf.IndexedSlices(grad_values, grad.indices, grad.dense_shape)\n      else:\n        grad *= scale\n    scaled_grads_and_vars.append((grad, var))\n  return scaled_grads_and_vars\n'"
open_seq2seq/optimizers/mp_wrapper_test.py,36,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport numpy.testing as npt\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom open_seq2seq.optimizers import optimize_loss\nfrom open_seq2seq.optimizers.mp_wrapper import mp_regularizer_wrapper, \\\n                                               MixedPrecisionOptimizerWrapper\nfrom .lr_policies import fixed_lr\n\n\nclass MixedPrecisionOptimizerTests(tf.test.TestCase):\n  def setUp(self):\n    pass\n\n  def tearDown(self):\n    pass\n\n  def test_regularization_normal(self):\n    n_samples = 3\n    n_hid = 2\n    scale_init = 1e-4\n    wd = 1e-4\n    X = np.ones((n_samples, n_hid)) / n_hid\n    y = np.ones((n_samples, 1)) * scale_init\n\n    for dtype in [tf.float16, tf.float32]:\n      # pylint: disable=no-member\n      regularizer = tf.contrib.layers.l2_regularizer(wd)\n\n      with tf.Graph().as_default() as g:\n        x_ph = tf.placeholder(dtype, [n_samples, n_hid])\n        y_ph = tf.placeholder(dtype, [n_samples, 1])\n\n        y_pred = tf.layers.dense(\n            x_ph, 1, kernel_regularizer=regularizer,\n            use_bias=False,\n            kernel_initializer=tf.constant_initializer(scale_init, dtype=dtype),\n        )\n        loss = tf.reduce_mean((y_ph - y_pred) ** 2)\n        reg_loss = tf.losses.get_regularization_loss()\n        loss += reg_loss\n        opt = tf.train.AdamOptimizer()\n        grad = opt.compute_gradients(loss)[0][0]\n\n        with self.test_session(g, use_gpu=True) as sess:\n          sess.run(tf.global_variables_initializer())\n          reg_loss_val, grad_val = sess.run([reg_loss, grad],\n                                            {x_ph: X, y_ph: y})\n      if dtype == tf.float16:\n        self.assertEqual(reg_loss_val, 0.0)\n        npt.assert_allclose(grad_val, np.zeros((2, 1), dtype=np.float16))\n      else:\n        self.assertAlmostEqual(reg_loss_val, 1e-12)\n        npt.assert_allclose(grad_val, np.ones((2, 1)) * 1e-8)\n\n  def test_regularization_mixed(self):\n    n_samples = 3\n    n_hid = 2\n    scale_init = 1e-4\n    wd = 1e-4\n    X = np.ones((n_samples, n_hid)) / n_hid\n    y = np.ones((n_samples, 1)) * scale_init\n\n    dtype = tf.float16\n    # pylint: disable=no-member\n    regularizer = mp_regularizer_wrapper(tf.contrib.layers.l2_regularizer(wd))\n\n    with tf.Graph().as_default() as g:\n      x_ph = tf.placeholder(dtype, [n_samples, n_hid])\n      y_ph = tf.placeholder(dtype, [n_samples, 1])\n\n      y_pred = tf.layers.dense(\n          x_ph, 1, kernel_regularizer=regularizer,\n          use_bias=False,\n          kernel_initializer=tf.constant_initializer(scale_init, dtype=dtype),\n      )\n      loss = tf.reduce_mean((y_ph - y_pred) ** 2)\n      reg_loss = tf.losses.get_regularization_loss()\n      loss += tf.cast(reg_loss, loss.dtype)\n      opt = MixedPrecisionOptimizerWrapper(tf.train.AdamOptimizer())\n      grad = opt.compute_gradients(loss)[0][0]\n\n      with self.test_session(g, use_gpu=True) as sess:\n        sess.run(tf.global_variables_initializer())\n        reg_loss_val, grad_val = sess.run([reg_loss, grad],\n                                          {x_ph: X, y_ph: y})\n\n    self.assertAlmostEqual(reg_loss_val, 0.0)\n    self.assertEqual(reg_loss.name, ""Const_1:0"")\n    npt.assert_allclose(grad_val, np.ones((2, 1)) * 1e-8, atol=1e-11)\n\n  def test_convergence(self):\n    for dtype in [\'mixed\', tf.float32]:\n      with tf.Graph().as_default() as g:\n        n_samples = 10\n        n_hid = 10\n        var_dtype = tf.float32 if dtype == tf.float32 else tf.float16\n\n        np.random.seed(0)\n        X = np.random.rand(n_samples, n_hid)\n        y = np.random.rand(n_samples, 1)\n        w = np.linalg.solve(X.T.dot(X), X.T.dot(y))\n\n        x_ph = tf.placeholder(var_dtype, [n_samples, n_hid])\n        y_ph = tf.placeholder(var_dtype, [n_samples, 1])\n\n        y_pred = tf.layers.dense(x_ph, 1, use_bias=False)\n        loss = tf.losses.mean_squared_error(y_ph, y_pred)\n        loss += tf.losses.get_regularization_loss()\n        train_op = optimize_loss(loss, ""Adam"", {},\n                                 lambda gs: fixed_lr(gs, 0.05), dtype=dtype)\n\n        with self.test_session(g, use_gpu=True) as sess:\n          sess.run(tf.global_variables_initializer())\n          for i in range(6000):\n            sess.run(train_op, {x_ph: X, y_ph: y})\n          w_learned = sess.run(tf.trainable_variables()[0])\n\n        npt.assert_allclose(w_learned, w, atol=0.01)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
open_seq2seq/optimizers/novograd.py,8,"b'# Copyright (c) 2019 NVIDIA Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import training_ops\nfrom tensorflow.train import MomentumOptimizer\nimport tensorflow as tf\n\nclass NovoGrad(MomentumOptimizer):\n  """"""\n  Optimizer that implements SGD with layer-wise normalized gradients,\n  when normalization is done by sqrt(ema(sqr(grads))), similar to Adam\n\n    ```\n    Second moment = ema of Layer-wise sqr of grads:\n       v_t <-- beta2*v_{t-1} + (1-beta2)*(g_t)^2\n\n    First moment has two mode:\n    1. moment of grads normalized by u_t:\n       m_t <- beta1*m_{t-1} + lr_t * [ g_t/sqrt(v_t+epsilon)]\n    1. moment similar to Adam: ema of grads normalized by u_t:\n       m_t <- beta1*m_{t-1} + lr_t * [(1-beta1)*(g_t/sqrt(v_t+epsilon))]\n\n    if weight decay add wd term after grads are rescaled by 1/sqrt(v_t):\n       m_t <- beta1*m_{t-1} + lr_t * [g_t/sqrt(v_t+epsilon) + wd*w_{t-1}]\n\n    Weight update:\n       w_t <- w_{t-1} - *m_t\n    ```\n\n  """"""\n\n  def __init__(self,\n               learning_rate=1.0,\n               beta1=0.95,\n               beta2=0.98,\n               epsilon=1e-8,\n               weight_decay=0.0,\n               grad_averaging=False,\n               use_locking=False,\n               name=\'NovoGrad\'):\n    """"""Constructor:\n\n    Args:\n      learning_rate: A `Tensor` or a floating point value.  The learning rate.\n      beta1: A `Tensor` or a float, used in ema for momentum.Default = 0.95.\n      beta2: A `Tensor` or a float, used in ema for grad norms.Default = 0.99.\n      epsilon: a float.  Default = 1e-8.\n      weight_decay: A `Tensor` or a float, Default = 0.0.\n      grad_averaging: switch between Momentum and SAG, Default = False,\n      use_locking: If `True` use locks for update operations.\n      name: Optional, name prefix for the ops created when applying\n        gradients.  Defaults to ""NovoGrad"".\n      use_nesterov: If `True` use Nesterov Momentum.\n\n    """"""\n    super(NovoGrad, self).__init__(learning_rate, momentum=beta1,\n                                   use_locking=use_locking, name=name,\n                                   use_nesterov=False)\n    self._beta1 = beta1\n    self._beta2 = beta2\n    self._epsilon = epsilon\n    self._wd  = weight_decay\n    self._grad_averaging  = grad_averaging\n    self._grads_ema = None\n\n    # Tensor versions, converted to tensors in apply_gradients\n    # self._beta1_t = None\n    # self._beta2_t = None\n    # self._wd_t = None\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    # self._beta1_t = ops.convert_to_tensor(self._beta1, name=\'beta1\', dtype = tf.float32)\n    # self._beta2_t = ops.convert_to_tensor(self._beta2, name=\'beta2\', dtype = tf.float32)\n\n    # init ema variables if required\n    len_vars = len(grads_and_vars)\n    if self._grads_ema is None:\n      self._grads_ema = [None] * len_vars\n      for i in range(len_vars):\n        self._grads_ema[i] = tf.get_variable(name=""nvgrad2_ema"" + str(i),\n                                     shape=[], dtype=tf.float32,\n                                     initializer=tf.keras.initializers.Zeros(),\n                                     trainable=False)\n\n    # compute ema for grads^2 for each layer\n    for i, (grad, var) in enumerate(grads_and_vars):\n      g_2 = tf.reduce_sum(tf.square(x=tf.cast(grad, tf.float32)))\n      self._grads_ema[i] = tf.cond(tf.equal(self._grads_ema[i], 0.),\n                  lambda: g_2,\n                  lambda: self._grads_ema[i]*self._beta2 + g_2*(1.-self._beta2)\n                  )\n\n      grad *= 1.0 / tf.sqrt(self._grads_ema[i] + self._epsilon)\n      # weight decay\n      if (self._wd > 0.):\n        grad += (self._wd * var)\n      # Momentum --> SAG\n      if self._grad_averaging:\n        grad *= (1.-self._beta1)\n      grads_and_vars[i] = (grad, var)\n\n    # call Momentum to do update\n    return super(NovoGrad, self).apply_gradients(\n         grads_and_vars, global_step=global_step, name=name)\n\n'"
open_seq2seq/optimizers/optimizers.py,68,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Optimizer ops for use in layers and tf.learn.""""""\n\n# This file was copy-pasted from TF repo on 10/04/2017 by Oleksii Kuchaiev\n# The following changes were made:\n# LARC support to ""optimize_loss"" function\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport collections\nimport six\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\nfrom open_seq2seq.utils.utils import mask_nans, check_params\nfrom .automatic_loss_scaler import AutomaticLossScaler\nfrom .mp_wrapper import MixedPrecisionOptimizerWrapper\n\nOPTIMIZER_CLS_NAMES = {\n    ""Adagrad"": tf.train.AdagradOptimizer,\n    ""Adam"": tf.train.AdamOptimizer,\n    ""Ftrl"": tf.train.FtrlOptimizer,\n    ""Momentum"": tf.train.MomentumOptimizer,\n    ""RMSProp"": tf.train.RMSPropOptimizer,\n    ""SGD"": tf.train.GradientDescentOptimizer,\n    ""AdamW"": tf.contrib.opt.AdamWOptimizer,\n}\n\nOPTIMIZER_SUMMARIES = [\n    ""learning_rate"",\n    ""gradients"",\n    ""gradient_norm"",\n    ""global_gradient_norm"",\n    ""variables"",\n    ""variable_norm"",\n    ""larc_summaries"",\n    ""loss_scale""\n]\n\n\n# necessary to redefine this function for pure float16 support\ndef get_regularization_loss(scope=None, name=""total_regularization_loss""):\n  """"""Gets the total regularization loss.\n\n  Args:\n    scope: An optional scope name for filtering the losses to return.\n    name: The name of the returned tensor.\n\n  Returns:\n    A scalar regularization loss.\n  """"""\n  losses = tf.losses.get_regularization_losses(scope)\n  if losses:\n    return tf.add_n(list(map(lambda x: tf.cast(x, tf.float32), losses)),\n                    name=name)\n  else:\n    return tf.constant(0.0)\n\n\ndef reduce_gradients(grads_and_vars, on_horovod, model=None):\n  if on_horovod:\n    from horovod.tensorflow import allreduce, size\n\n    if size() > 1:\n      averaged_grads_and_vars = []\n      with tf.name_scope(""all_reduce""):\n        for grad, var in grads_and_vars:\n          if grad is not None:\n            if isinstance(grad, tf.IndexedSlices):\n              if model._decoder.params.get(\'shared_embed\', False):\n                from tensorflow.python.training.optimizer import _deduplicate_indexed_slices\n                summed_values, unique_indices = _deduplicate_indexed_slices(\n                    values=grad.values, indices=grad.indices)\n                gradient_no_duplicate_indices = tf.IndexedSlices(\n                    indices=unique_indices,\n                    values=summed_values,\n                    dense_shape=grad.dense_shape)\n                grad = tf.convert_to_tensor(gradient_no_duplicate_indices)\n            avg_grad = allreduce(grad)\n            averaged_grads_and_vars.append((avg_grad, var))\n          else:\n            averaged_grads_and_vars.append((None, var))\n      return averaged_grads_and_vars\n    else:\n      return grads_and_vars\n  else:\n    raise NotImplementedError(""Reduce in tower-mode is not implemented."")\n\n\ndef optimize_loss(loss,\n                  optimizer,\n                  optimizer_params,\n                  learning_rate_decay_fn,\n                  var_list=None,\n                  dtype=tf.float32,\n                  clip_gradients=None,\n                  summaries=None,\n                  larc_params=None,\n                  loss_scaling=1.0,\n                  loss_scaling_params=None,\n                  on_horovod=False,\n                  iter_size=1,\n                  skip_update_ph=None,\n                  model=None):\n  """"""Given loss and parameters for optimizer, returns a training op.\n\n  Args:\n    loss: Scalar `Tensor`.\n    optimizer: string or class of optimizer, used as trainer.\n        string should be name of optimizer, like \'SGD\',\n        \'Adam\', \'Adagrad\'. Full list in OPTIMIZER_CLS_NAMES constant.\n        class should be sub-class of `tf.Optimizer` that implements\n        `compute_gradients` and `apply_gradients` functions.\n    optimizer_params: parameters of the optimizer.\n    var_list: List of trainable variables. Can be used to freeze\n        certain trainable variables by excluding them from this list. \n        If set to None, all trainable variables will be optimized.\n    dtype: model dtype (tf.float16, tf.float32 or ""mixed"").\n    learning_rate_decay_fn: function, takes `global_step`\n        `Tensor`s, returns `Tensor`.\n        Can be used to implement any learning rate decay\n        functions.\n        For example: `tf.train.exponential_decay`.\n        Ignored if `learning_rate` is not supplied.\n    clip_gradients: float, max gradient norm to clip to.\n    summaries: List of internal quantities to visualize on tensorboard. If not\n        set only the loss and the learning rate will be reported. The\n        complete list is in OPTIMIZER_SUMMARIES.\n    larc_params: If not None, LARC re-scaling will\n        be applied with corresponding parameters.\n    loss_scaling: could be float or string. If float, static loss scaling\n        is applied. If string, the corresponding automatic\n        loss scaling algorithm is used. Must be one of \'Backoff\'\n        of \'LogMax\' (case insensitive). Only used when dtype=""mixed"".\n    on_horovod: whether the model is run on horovod.\n\n  Returns:\n    training op.\n  """"""\n  if summaries is None:\n    summaries = [""learning_rate"", ""global_gradient_norm"", ""loss_scale""]\n  else:\n    for summ in summaries:\n      if summ not in OPTIMIZER_SUMMARIES:\n        raise ValueError(\n            ""Summaries should be one of [{}], you provided {}."".format(\n                "", "".join(OPTIMIZER_SUMMARIES), summ,\n            )\n        )\n  if clip_gradients is not None and larc_params is not None:\n    raise AttributeError(\n        ""LARC and gradient norm clipping should not be used together""\n    )\n\n  global_step = tf.train.get_or_create_global_step()\n  lr = learning_rate_decay_fn(global_step)\n  if ""learning_rate"" in summaries:\n    tf.summary.scalar(""learning_rate"", lr)\n\n  with tf.variable_scope(""Loss_Optimization""):\n    update_ops = set(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n    loss = control_flow_ops.with_dependencies(list(update_ops), loss)\n\n    if optimizer==""AdamW"":\n      optimizer_params[""weight_decay""] = optimizer_params[""weight_decay""]*lr\n\n    # Create optimizer, given specified parameters.\n    if isinstance(optimizer, six.string_types):\n      if optimizer not in OPTIMIZER_CLS_NAMES:\n        raise ValueError(\n            ""Optimizer name should be one of [{}], you provided {}."".format(\n                "", "".join(OPTIMIZER_CLS_NAMES), optimizer\n            )\n        )\n      optimizer = OPTIMIZER_CLS_NAMES[optimizer]\n\n    opt = optimizer(learning_rate=lr, **optimizer_params)\n\n    if isinstance(loss_scaling, six.string_types):\n      loss_scaling = AutomaticLossScaler(\n          algorithm=loss_scaling,\n          params=loss_scaling_params\n      )\n      if ""loss_scale"" in summaries:\n        tf.summary.scalar(""loss_scale"", loss_scaling.loss_scale)\n\n    if dtype == \'mixed\':\n      opt = MixedPrecisionOptimizerWrapper(opt, loss_scale=loss_scaling)\n\n    # Compute gradients.\n    grads_and_vars = opt.compute_gradients(\n        loss, colocate_gradients_with_ops=True, var_list=var_list\n    )\n\n    if on_horovod:\n      if iter_size > 1:\n        grads_and_vars_accum = []\n        accum_ops = []\n        for grad, var in grads_and_vars:\n          # necessary to use tf.Variable directly to instantiate cudnn rnn cells\n          # which don\'t have explicit shape.\n          grad_accum = tf.Variable(\n              initial_value=tf.zeros_like(var),\n              name=grad.name.split("":"")[0] + ""_accum"",\n              expected_shape=var.shape,\n              dtype=grad.dtype,\n              trainable=False,\n              validate_shape=bool(var.get_shape())\n          )\n          if isinstance(grad, tf.IndexedSlices):\n            add_grads = tf.scatter_nd_add(grad_accum, grad.indices,\n                                          grad.values / iter_size)\n          else:\n            add_grads = grad_accum + grad / iter_size\n\n          accum_ops.append(tf.assign(grad_accum, add_grads))\n          grads_and_vars_accum.append((grad_accum, var))\n\n        accum_op = tf.group(accum_ops)\n\n        def update_and_clear_op():\n          with tf.control_dependencies([accum_op]):\n            red_grad_updates = opt.apply_gradients(\n                post_process_gradients(\n                    reduce_gradients(grads_and_vars_accum, on_horovod=True, model=model),\n                    lr=lr,\n                    clip_gradients=clip_gradients,\n                    larc_params=larc_params,\n                    summaries=summaries,\n                ),\n                global_step=global_step,\n            )\n\n          with tf.control_dependencies([red_grad_updates]):\n            return tf.group([tf.assign(g, tf.zeros_like(g))\n                             for g, v in grads_and_vars_accum])\n\n        grad_updates = tf.cond(\n            pred=skip_update_ph,\n            true_fn=lambda: accum_op,\n            false_fn=update_and_clear_op,\n        )\n      else:\n        grad_updates = opt.apply_gradients(\n            post_process_gradients(\n                reduce_gradients(grads_and_vars, on_horovod=True, model=model),\n                lr=lr,\n                clip_gradients=clip_gradients,\n                larc_params=larc_params,\n                summaries=summaries,\n            ),\n            global_step=global_step,\n        )\n    else:\n      grad_updates = opt.apply_gradients(\n          post_process_gradients(\n              grads_and_vars,\n              lr=lr,\n              clip_gradients=clip_gradients,\n              larc_params=larc_params,\n              summaries=summaries,\n          ),\n          global_step=global_step,\n      )\n\n    # Ensure the train_tensor computes grad_updates.\n    train_tensor = control_flow_ops.with_dependencies([grad_updates], loss)\n\n    return train_tensor\n\n\ndef post_process_gradients(grads_and_vars, summaries, lr,\n                           clip_gradients, larc_params):\n  """"""Applies post processing to gradients, i.e. clipping, LARC, summaries.""""""\n  if ""global_gradient_norm"" in summaries:\n    tf.summary.scalar(\n        ""global_gradient_norm"",\n        _global_norm_with_cast(grads_and_vars),\n    )\n\n  # Optionally clip gradients by global norm.\n  if clip_gradients is not None:\n    grads_and_vars = _clip_gradients_by_norm(grads_and_vars, clip_gradients)\n\n  # Add histograms for variables, gradients and gradient norms.\n  for gradient, variable in grads_and_vars:\n    if isinstance(gradient, tf.IndexedSlices):\n      grad_values = gradient.values\n    else:\n      grad_values = gradient\n\n    if isinstance(variable, tf.IndexedSlices):\n      var_values = variable.values\n    else:\n      var_values = variable\n\n    if grad_values is not None:\n      var_name = variable.name.replace("":"", ""_"")\n      if ""gradients"" in summaries:\n        # need to mask nans for automatic loss scaling\n        tf.summary.histogram(""gradients/%s"" % var_name, mask_nans(grad_values))\n      if ""gradient_norm"" in summaries:\n        tf.summary.scalar(""gradient_norm/%s"" % var_name, tf.norm(grad_values))\n      if ""variables"" in summaries:\n        tf.summary.histogram(""variables/%s"" % var_name, var_values)\n      if ""variable_norm"" in summaries:\n        tf.summary.scalar(""variable_norm/%s"" % var_name, tf.norm(var_values))\n\n  if clip_gradients is not None and ""global_gradient_norm"" in summaries:\n    tf.summary.scalar(\n        ""global_clipped_gradient_norm"",\n        _global_norm_with_cast(grads_and_vars),\n    )\n\n  # LARC gradient re-scaling\n  if larc_params is not None:\n    check_params(\n        config=larc_params,\n        required_dict={\'larc_eta\': float},\n        optional_dict={\n            \'larc_mode\': [\'clip\', \'scale\'],\n            \'min_update\': float,\n            \'epsilon\': float\n        },\n    )\n    larc_eta = larc_params[\'larc_eta\']\n    larc_mode = larc_params.get(\'larc_mode\', \'clip\')\n    min_update = larc_params.get(\'min_update\', 1e-7)\n    eps = larc_params.get(\'epsilon\', 1e-7)\n\n    grads_and_vars_larc = [None] * len(grads_and_vars)\n    for idx, (g, v) in enumerate(grads_and_vars):\n      var_dtype = v.dtype\n      v_norm = tf.norm(tensor=tf.cast(v, tf.float32), ord=2)\n      g_norm = tf.norm(tensor=tf.cast(g, tf.float32), ord=2)\n\n      if larc_mode == \'clip\':\n        larc_grad_update = tf.maximum(\n            larc_eta * v_norm / (lr * (g_norm + eps)),\n            min_update,\n        )\n        if ""larc_summaries"" in summaries:\n          tf.summary.scalar(\'larc_clip_on/{}\'.format(v.name),\n                            tf.cast(tf.less(larc_grad_update, 1.0), tf.int32))\n        larc_grad_update = tf.minimum(larc_grad_update, 1.0)\n      else:\n        larc_grad_update = tf.maximum(\n            larc_eta * v_norm / (g_norm + eps),\n            min_update,\n        )\n      larc_grad_update = tf.saturate_cast(larc_grad_update, var_dtype)\n      grads_and_vars_larc[idx] = (larc_grad_update * g, v)\n\n      # adding additional summary\n      if ""larc_summaries"" in summaries:\n        tf.summary.scalar(\'larc_grad_update/{}\'.format(v.name),\n                          larc_grad_update)\n        tf.summary.scalar(""larc_final_lr/{}"".format(v.name),\n                          tf.cast(lr, var_dtype) * larc_grad_update)\n    grads_and_vars = grads_and_vars_larc\n  return grads_and_vars\n\n\ndef _global_norm_with_cast(grads_and_vars):\n  return tf.global_norm(list(map(\n      lambda x: tf.cast(x, tf.float32),\n      list(zip(*grads_and_vars))[0]\n  )))\n\n\ndef _clip_gradients_by_norm(grads_and_vars, clip_gradients):\n  """"""Clips gradients by global norm.""""""\n  gradients, variables = zip(*grads_and_vars)\n  dtypes = [var.dtype for var in variables]\n\n  # Clip gradients in float32\n  clipped_gradients, _ = _clip_by_global_norm(\n      gradients,\n      clip_gradients,\n      use_norm=_global_norm_with_cast(grads_and_vars)\n  )\n\n  # Convert gradients back to the proper dtype\n  clipped_gradients = [\n      tf.cast(grad, dtype)\n      for grad, dtype in zip(clipped_gradients, dtypes)\n  ]\n\n  return list(zip(clipped_gradients, variables))\n\ndef _clip_by_global_norm(t_list, clip_norm, use_norm, name=None):\n  """"""Clips values of multiple tensors by the ratio of the sum of their norms.\n  Given a tuple or list of tensors `t_list`, and a clipping ratio `clip_norm`,\n  this operation returns a list of clipped tensors `list_clipped`\n  and the global norm (`global_norm`) of all tensors in `t_list`. The global\n  norm is expected to be pre-computed and passed as use_norm.\n  To perform the clipping, the values `t_list[i]` are set to:\n      t_list[i] * clip_norm / max(global_norm, clip_norm)\n  where:\n      global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n  If `clip_norm > global_norm` then the entries in `t_list` remain as they are,\n  otherwise they\'re all shrunk by the global ratio.\n  Any of the entries of `t_list` that are of type `None` are ignored.\n  This is the correct way to perform gradient clipping (for example, see\n  [Pascanu et al., 2012](http://arxiv.org/abs/1211.5063)\n  ([pdf](http://arxiv.org/pdf/1211.5063.pdf))).\n  However, it is slower than `clip_by_norm()` because all the parameters must be\n  ready before the clipping operation can be performed.\n\n  Args:\n    t_list: A tuple or list of mixed `Tensors`, `IndexedSlices`, or None.\n    clip_norm: A 0-D (scalar) `Tensor` > 0. The clipping ratio.\n    use_norm: A 0-D (scalar) `Tensor` of type `float` (optional). The global\n      norm to use. If not provided, `global_norm()` is used to compute the norm.\n    name: A name for the operation (optional).\n\n  Returns:\n    list_clipped: A list of `Tensors` of the same type as `list_t`.\n    global_norm: A 0-D (scalar) `Tensor` representing the global norm.\n\n  Raises:\n    TypeError: If `t_list` is not a sequence.\n  """"""\n  if (not isinstance(t_list, collections.Sequence)\n      or isinstance(t_list, six.string_types)):\n    raise TypeError(""t_list should be a sequence"")\n  t_list = list(t_list)\n\n  # Removed as use_norm should always be passed\n  # if use_norm is None:\n  #   use_norm = global_norm(t_list, name)\n\n  with tf.name_scope(name, ""clip_by_global_norm"",\n                     t_list + [clip_norm]) as name:\n    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n    scale = clip_norm * tf.minimum(\n        1.0 / use_norm,\n        tf.ones([1], dtype=use_norm.dtype) / clip_norm)\n\n    values = [\n        tf.cast(\n            tf.convert_to_tensor(\n                t.values if isinstance(t, tf.IndexedSlices) else t,\n                name=""t_%d"" % i),\n            dtype=tf.float32\n        )\n        if t is not None else t\n        for i, t in enumerate(t_list)]\n\n    values_clipped = []\n    for i, v in enumerate(values):\n      if v is None:\n        values_clipped.append(None)\n      else:\n        with tf.colocate_with(v):\n          values_clipped.append(\n              tf.identity(v * scale, name=""%s_%d"" % (name, i)))\n\n    list_clipped = [\n        tf.IndexedSlices(c_v, t.indices, t.dense_shape)\n        if isinstance(t, tf.IndexedSlices)\n        else c_v\n        for (c_v, t) in zip(values_clipped, t_list)]\n\n  return list_clipped, use_norm\n  \n'"
open_seq2seq/optimizers/optimizers_test.py,14,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport numpy.testing as npt\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom open_seq2seq.optimizers import optimize_loss\nfrom .lr_policies import fixed_lr\n\n\nclass IterSizeTests(tf.test.TestCase):\n  def setUp(self):\n    pass\n\n  def tearDown(self):\n    pass\n\n  def test_updates(self):\n    try:\n      import horovod.tensorflow as hvd\n      hvd.init()\n    except ImportError:\n      print(""Horovod not installed skipping test_updates"")\n      return\n\n    dtype = tf.float32\n    with tf.Graph().as_default() as g:\n      n_samples = 10\n      n_hid = 10\n      var_dtype = tf.float32 if dtype == tf.float32 else tf.float16\n\n      np.random.seed(0)\n      X = np.random.rand(n_samples, n_hid)\n      y = np.random.rand(n_samples, 1)\n      w = np.linalg.solve(X.T.dot(X), X.T.dot(y))\n\n      x_ph = tf.placeholder(var_dtype, [n_samples, n_hid])\n      y_ph = tf.placeholder(var_dtype, [n_samples, 1])\n\n      y_pred = tf.layers.dense(x_ph, 1, use_bias=False)\n      loss = tf.losses.mean_squared_error(y_ph, y_pred)\n      loss += tf.losses.get_regularization_loss()\n      skip_update_ph = tf.placeholder(tf.bool)\n      iter_size = 8\n      train_op = optimize_loss(loss, ""SGD"", {},\n                               lambda gs: fixed_lr(gs, 0.1), dtype=dtype,\n                               iter_size=iter_size, on_horovod=True,\n                               skip_update_ph=skip_update_ph)\n      grad_accum = [var for var in tf.global_variables() if \'accum\' in var.name][0]\n      var = tf.trainable_variables()[0]\n      with self.test_session(g, use_gpu=True) as sess:\n        sess.run(tf.global_variables_initializer())\n        for _ in range(3):\n          g, v = sess.run([grad_accum, var])\n          npt.assert_allclose(g, np.zeros(g.shape))\n\n          true_g = 2 * (X.T.dot(X).dot(v) - X.T.dot(y)) / X.shape[0] / iter_size\n\n          sess.run(train_op, {x_ph: X, y_ph: y, skip_update_ph: True})\n          g_new, v_new = sess.run([grad_accum, var])\n          npt.assert_allclose(g_new, true_g, atol=1e-7)\n          npt.assert_allclose(v_new, v)\n\n          sess.run(train_op, {x_ph: X, y_ph: y, skip_update_ph: True})\n          g_new, v_new = sess.run([grad_accum, var])\n          npt.assert_allclose(g_new, true_g * 2, atol=1e-7)\n          npt.assert_allclose(v_new, v)\n\n          sess.run(train_op, {x_ph: X, y_ph: y, skip_update_ph: True})\n          g_new, v_new = sess.run([grad_accum, var])\n          npt.assert_allclose(g_new, true_g * 3, atol=1e-7)\n          npt.assert_allclose(v_new, v)\n\n          sess.run(train_op, {x_ph: X, y_ph: y, skip_update_ph: False})\n          g_new, v_new = sess.run([grad_accum, var])\n          npt.assert_allclose(g_new, np.zeros(g.shape))\n          npt.assert_allclose(v_new, v - 0.1 * true_g * 4, atol=1e-7)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
open_seq2seq/parts/__init__.py,0,b'# Copyright (c) 2017 NVIDIA Corporation\n'
open_seq2seq/test_utils/__init__.py,0,b''
open_seq2seq/test_utils/create_reversed_examples.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nimport numpy as np\nimport os\nimport errno\nimport io\nimport shutil\n\n\ndef create_source(size, source_vocab, vocab_map):\n  source = []\n  for i in range(0, size):\n    new_rol = []\n    for j in range(0, np.random.randint(low=5, high=51)):\n      new_dig = np.random.randint(low=0, high=len(vocab_map))\n      new_rol.append(vocab_map[new_dig])\n      if new_dig not in source_vocab:\n        source_vocab[new_dig] = 0\n      else:\n        source_vocab[new_dig] += 1\n    source.append(new_rol)\n  return source\n\n\ndef create_target(size, source):\n  target = []\n  for i in range(0, size):\n    new_row = list(reversed(source[i]))\n    target.append(new_row)\n  return target\n\n\ndef write_to_file(path, data):\n  with io.open(path, \'w\', encoding=\'utf-8\') as f:\n    for row in data:\n      f.write(\' \'.join(row) + \'\\n\')\n  f.close()\n\n\ndef write_vocab_to_file(path, data, vocab_map):\n  with io.open(path, \'w\', encoding=\'utf-8\') as f:\n    for key, value in data.items():\n      f.write(vocab_map[key]+\'\\t\'+str(value)+\'\\n\')\n  f.close()\n\n\ndef create_directory(path):\n  try:\n    os.makedirs(path)\n  except OSError as e:\n    if e.errno != errno.EEXIST:\n      raise\n\n\ndef create_data(train_corpus_size=10000, dev_corpus_size=1000,\n                test_corpus_size=2000, data_path=""./toy_text_data""):\n\n  train_path = os.path.join(data_path, ""train"")\n  dev_path = os.path.join(data_path, ""dev"")\n  test_path = os.path.join(data_path, ""test"")\n  vocab_path = os.path.join(data_path, ""vocab"")\n\n  train_source_path = os.path.join(train_path, ""source.txt"")\n  train_target_path = os.path.join(train_path, ""target.txt"")\n\n  dev_source_path = os.path.join(dev_path, ""source.txt"")\n  dev_target_path = os.path.join(dev_path, ""target.txt"")\n\n  test_source_path = os.path.join(test_path, ""source.txt"")\n  test_target_path = os.path.join(test_path, ""target.txt"")\n\n  vocab_source_path = os.path.join(vocab_path, ""source.txt"")\n  vocab_target_path = os.path.join(vocab_path, ""target.txt"")\n\n  source_vocab = {}\n\n  vocab_map = {0: \'\\u03B1\',\n               1: \'\\u03B2\',\n               2: \'\\u03B3\',\n               3: \'\\u03B4\',\n               4: \'\\u03B5\',\n               5: \'\\u03B6\',\n               6: \'\\u03B7\',\n               7: \'\\u03B8\',\n               8: \'\\u03B9\',\n               9: \'\\u03BA\'}\n\n  create_directory(train_path)\n  create_directory(test_path)\n  create_directory(dev_path)\n  create_directory(vocab_path)\n\n  train_source = create_source(train_corpus_size, source_vocab, vocab_map)\n  write_to_file(train_source_path, train_source)\n  write_to_file(\n    train_target_path,\n    create_target(train_corpus_size, train_source),\n  )\n\n  dev_source = create_source(dev_corpus_size, source_vocab, vocab_map)\n  write_to_file(dev_source_path, dev_source)\n  write_to_file(dev_target_path, create_target(dev_corpus_size, dev_source))\n\n  test_source = create_source(test_corpus_size, source_vocab, vocab_map)\n  write_to_file(test_source_path, test_source)\n  write_to_file(test_target_path, create_target(test_corpus_size, test_source))\n\n  write_vocab_to_file(vocab_source_path, source_vocab, vocab_map)\n  # in our case, source and target vocabs are the same\n  write_vocab_to_file(vocab_target_path, source_vocab, vocab_map)\n\n\ndef remove_data(data_path=""./toy_text_data""):\n  shutil.rmtree(data_path)\n\n\nif __name__ == \'__main__\':\n  create_data(data_path=\'./toy_text_data\')\n'"
open_seq2seq/utils/__init__.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom .funcs import train, infer, evaluate\n'"
open_seq2seq/utils/ctc_decoder.py,0,"b'# Copyright (c) 2019 NVIDIA Corporation\n""""""This file has a CTC Greedy decoder""""""\nimport numpy as np\n\ndef ctc_greedy_decoder(logits, wordmap, step_size,\n                       blank_idx, start_shift, end_shift):\n  """"""Decodes logits to chars using greedy ctc format,\n  outputs start and end time for every word\n  Args :\n    logits: time x chars (log probabilities)\n    wordmap: vocab (maps index to characters)\n    step_size: number of steps to take in time domain per block of input\n    blank_idx: index of blank char\n    start_shift: shift needed for start of word in time domain\n    end_shift: shift needed in end of word in time domain\n  """"""\n  prev_idx = -1\n  output = []\n  start = []\n  end = []\n  lst_letter = -1\n  for i, log_prob in enumerate(logits):\n    idx = np.argmax(log_prob)\n    if idx not in (blank_idx, prev_idx):\n      if len(output) == 0:\n        start.append(step_size*i+start_shift)\n      else:\n        if output[-1] == "" "":\n          start.append(max(step_size*i+start_shift, end[-1]))\n      output += wordmap[idx]\n      if output[-1] == "" "":\n        end.append(step_size*lst_letter+end_shift)\n      lst_letter = i\n    prev_idx = idx\n  end.append(step_size*lst_letter+end_shift)\n  output = """".join(output)\n  output = output.strip("" "")\n  return output, start, end\n'"
open_seq2seq/utils/funcs.py,21,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.python import debug as tf_debug\nfrom six.moves import range\n\nfrom open_seq2seq.utils.utils import deco_print, get_results_for_epoch, \\\n                                     collect_if_horovod\nfrom .hooks import PrintSamplesHook, RunEvaluationHook, PrintLossAndTimeHook, \\\n                   BroadcastGlobalVariablesHook\nfrom .helpers import TransferMonitoredTrainingSession, TransferScaffold, \\\n                     get_assign_ops_and_restore_dict, run_assign_and_saver\nfrom open_seq2seq.data import WKTDataLayer\n\n\ndef train(train_model, eval_model=None, debug_port=None, custom_hooks=None):\n  if eval_model is not None and \'eval_steps\' not in eval_model.params:\n    raise ValueError(""eval_steps parameter has to be specified ""\n                     ""if eval_model is provided"")\n  hvd = train_model.hvd\n  if hvd:\n    master_worker = hvd.rank() == 0\n  else:\n    master_worker = True\n\n  # initializing session parameters\n  sess_config = tf.ConfigProto(allow_soft_placement=True)\n  # pylint: disable=no-member\n  sess_config.gpu_options.allow_growth = True\n  if hvd is not None:\n    # pylint: disable=no-member\n    sess_config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n  if train_model.params.get(\'use_xla_jit\', False):\n    sess_config.graph_options.optimizer_options.global_jit_level = (\n        tf.OptimizerOptions.ON_1)\n\n  # defining necessary hooks\n  hooks = [tf.train.StopAtStepHook(last_step=train_model.last_step)]\n  if custom_hooks:\n    for custom_hook in custom_hooks:\n      hooks.append(custom_hook(train_model=train_model, eval_model=eval_model))\n\n  if hvd is not None:\n    hooks.append(BroadcastGlobalVariablesHook(0))\n\n  if master_worker:\n    checkpoint_dir = train_model.params[\'logdir\']\n    load_model_dir = train_model.params[\'load_model\']\n  else:\n    checkpoint_dir = None\n    load_model_dir = None\n\n  if eval_model is not None:\n    # noinspection PyTypeChecker\n    hooks.append(\n        RunEvaluationHook(\n            every_steps=eval_model.params[\'eval_steps\'],\n            model=eval_model,\n            last_step=train_model.last_step,\n            print_ppl=isinstance(eval_model.get_data_layer(), WKTDataLayer),\n        ),\n    )\n\n  if master_worker:\n    if train_model.params[\'save_checkpoint_steps\'] is not None:\n      # noinspection PyTypeChecker\n      saver = tf.train.Saver(\n          save_relative_paths=True,\n          max_to_keep=train_model.params[\'num_checkpoints\']\n      )\n      hooks.append(tf.train.CheckpointSaverHook(\n          checkpoint_dir,\n          saver=saver,\n          save_steps=train_model.params[\'save_checkpoint_steps\'],\n      ))\n    if train_model.params[\'print_loss_steps\'] is not None:\n      # noinspection PyTypeChecker\n      hooks.append(PrintLossAndTimeHook(\n          every_steps=train_model.params[\'print_loss_steps\'],\n          model=train_model,\n          print_ppl=isinstance(train_model.get_data_layer(), WKTDataLayer),\n      ))\n    if train_model.params[\'print_samples_steps\'] is not None:\n      # noinspection PyTypeChecker\n      hooks.append(PrintSamplesHook(\n          every_steps=train_model.params[\'print_samples_steps\'],\n          model=train_model,\n      ))\n\n  total_time = 0.0\n  bench_start = train_model.params.get(\'bench_start\', 10)\n\n  if debug_port:\n    hooks.append(\n        tf_debug.TensorBoardDebugHook(""localhost:{}"".format(debug_port))\n    )\n\n  if train_model.on_horovod:\n    init_data_layer = train_model.get_data_layer().iterator.initializer\n  else:\n    init_data_layer = tf.group(\n        [train_model.get_data_layer(i).iterator.initializer\n         for i in range(train_model.num_gpus)]\n    )\n\n  # We restore only if the user provides load_model_dir. load_model_dir is the\n  # directory containing the checkpoint we want to load partial or all weights\n  # from.. Useful for transer learning or if we do not want to overwrite our\n  # checkpoint. \n  restoring = load_model_dir and not tf.train.latest_checkpoint(checkpoint_dir)\n  if restoring:\n    vars_in_checkpoint = {}\n    for var_name, var_shape in tf.train.list_variables(load_model_dir):\n        vars_in_checkpoint[var_name] = var_shape\n\n    print(\'VARS_IN_CHECKPOINT:\')\n    print(vars_in_checkpoint)\n\n    vars_to_load = []\n    for var in tf.global_variables():\n      var_name = var.name.split(\':\')[0]\n      if var_name in vars_in_checkpoint:\n        if var.shape == vars_in_checkpoint[var_name] and \\\n            \'global_step\' not in var_name:\n          vars_to_load.append(var)\n\n    print(\'VARS_TO_LOAD:\')\n    for var in vars_to_load:\n        print(var)\n\n    load_model_fn = tf.contrib.framework.assign_from_checkpoint_fn(\n        tf.train.latest_checkpoint(load_model_dir), vars_to_load\n    )\n    scaffold = tf.train.Scaffold(\n        local_init_op=tf.group(tf.local_variables_initializer(), init_data_layer),\n        init_fn = lambda scaffold_self, sess: load_model_fn(sess)\n    )\n\n  else:\n    scaffold = tf.train.Scaffold(\n        local_init_op=tf.group(tf.local_variables_initializer(), init_data_layer)\n    )\n  fetches = [train_model.train_op]\n  try:\n    total_objects = 0.0\n    # on horovod num_gpus is 1\n    for worker_id in range(train_model.num_gpus):\n      fetches.append(train_model.get_num_objects_per_step(worker_id))\n  except NotImplementedError:\n    deco_print(""WARNING: Can\'t compute number of objects per step, since ""\n               ""train model does not define get_num_objects_per_step method."")\n\n  # starting training\n  sess = tf.train.MonitoredTrainingSession(\n      scaffold=scaffold,\n      checkpoint_dir=checkpoint_dir,\n      save_summaries_steps=train_model.params[\'save_summaries_steps\'],\n      config=sess_config,\n      save_checkpoint_secs=None,\n      log_step_count_steps=train_model.params[\'save_summaries_steps\'],\n      stop_grace_period_secs=300,\n      hooks=hooks)\n  step = 0\n  num_bench_updates = 0\n  while True:\n    if sess.should_stop():\n      break\n    tm = time.time()\n    try:\n      feed_dict = {}\n      iter_size = train_model.params.get(\'iter_size\', 1)\n      if iter_size > 1:\n        feed_dict[train_model.skip_update_ph] = step % iter_size != 0\n      if step % iter_size == 0:\n        if step >= bench_start:\n          num_bench_updates += 1\n        fetches_vals = sess.run(fetches, feed_dict)\n      else:\n        # necessary to skip ""no-update"" steps when iter_size > 1\n        def run_with_no_hooks(step_context):\n          return step_context.session.run(fetches, feed_dict)\n        fetches_vals = sess.run_step_fn(run_with_no_hooks)\n    except tf.errors.OutOfRangeError:\n      break\n    if step >= bench_start:\n      total_time += time.time() - tm\n      if len(fetches) > 1:\n        for i in range(train_model.num_gpus):\n          total_objects += np.sum(fetches_vals[i + 1])\n        if train_model.params[\'print_bench_info_steps\'] is not None:\n          if step % train_model.params[\'print_bench_info_steps\'] == 0:\n            total_objects_cur = collect_if_horovod(total_objects, hvd,\n                                                   mode=""sum"")\n            if master_worker:\n              avg_objects = 1.0 * total_objects_cur / total_time\n              deco_print(""Avg objects per second: {:.3f}"".format(avg_objects))\n\n    step += 1\n  sess.close()\n\n  if len(fetches) > 1:\n    total_objects = collect_if_horovod(total_objects, hvd, mode=""sum"")\n\n  if master_worker:\n    deco_print(""Finished training"")\n    if step > bench_start:\n      avg_time = 1.0 * total_time / num_bench_updates\n      deco_print(""Avg time per step: {:.3f}s"".format(avg_time))\n      if len(fetches) > 1:\n        avg_objects = 1.0 * total_objects / total_time\n        deco_print(""Avg objects per second: {:.3f}"".format(avg_objects))\n    else:\n      deco_print(""Not enough steps for benchmarking"")\n\n\ndef restore_and_get_results(model, checkpoint, mode):\n  if not model.params.get(""use_trt"", False):\n    # Checkpoint is restored prior to freezing graph when using TRT\n    saver = tf.train.Saver()\n  sess_config = tf.ConfigProto(allow_soft_placement=True)\n  # pylint: disable=no-member\n  sess_config.gpu_options.allow_growth = True\n  if model.hvd:\n    # pylint: disable=no-member\n    sess_config.gpu_options.visible_device_list = str(model.hvd.local_rank())\n  with tf.Session(config=sess_config) as sess:\n    if not model.params.get(""use_trt"", False):\n      assign_ops, restore_dict = get_assign_ops_and_restore_dict(\n          checkpoint, True)\n      if assign_ops:\n        run_assign_and_saver(sess, checkpoint, assign_ops, restore_dict)\n      else:\n        saver = tf.train.Saver()\n        saver.restore(sess, checkpoint)\n    results_per_batch = get_results_for_epoch(\n        model, sess, mode=mode, compute_loss=False, verbose=True,\n    )\n  return results_per_batch\n\n\ndef infer(model, checkpoint, output_file):\n  results_per_batch = restore_and_get_results(model, checkpoint, mode=""infer"")\n  if not model.on_horovod or model.hvd.rank() == 0:\n    model.finalize_inference(results_per_batch, output_file)\n    deco_print(""Finished inference"")\n\ndef evaluate(model, checkpoint):\n  results_per_batch = restore_and_get_results(model, checkpoint, mode=""eval"")\n  if not model.on_horovod or model.hvd.rank() == 0:\n    eval_dict = model.finalize_evaluation(results_per_batch)\n    deco_print(""Finished evaluation"")\n    return eval_dict\n  return None\n'"
open_seq2seq/utils/helpers.py,44,"b'\'\'\'\nThis file modifies standard TensorFlow modules necessary for transfer learning,\nsuch as MonitoredTrainingSession, ChiefSessionCreator, Scaffold, SessionManager\n\'\'\'\nimport re\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import resources\nfrom tensorflow.python.training import saver as training_saver\n\nFP32_TEST = re.compile(r\'Loss_Optimization\\/FP32-master-copy\\/\')\n\n# Value that indicates no value was provided.\nUSE_DEFAULT = object()\n\ndef TransferMonitoredTrainingSession(master=\'\',  # pylint: disable=invalid-name\n                                     is_chief=True,\n                                     checkpoint_dir=None,\n                                     scaffold=None,\n                                     hooks=None,\n                                     chief_only_hooks=None,\n                                     save_checkpoint_secs=USE_DEFAULT,\n                                     save_summaries_steps=USE_DEFAULT,\n                                     save_summaries_secs=USE_DEFAULT,\n                                     config=None,\n                                     stop_grace_period_secs=120,\n                                     log_step_count_steps=100,\n                                     max_wait_secs=7200,\n                                     save_checkpoint_steps=USE_DEFAULT,\n                                     summary_dir=None,\n                                     load_model_dir=None,\n                                     load_fc=False):\n  """"""Creates a `MonitoredSession` for training.\n  For a chief, this utility sets proper session initializer/restorer. It also\n  creates hooks related to checkpoint and summary saving. For workers, this\n  utility sets proper session creator which waits for the chief to\n  initialize/restore. Please check `tf.train.MonitoredSession` for more\n  information.\n  Args:\n    master: `String` the TensorFlow master to use.\n    is_chief: If `True`, it will take care of initialization and recovery the\n      underlying TensorFlow session. If `False`, it will wait on a chief to\n      initialize or recover the TensorFlow session.\n    checkpoint_dir: A string.  Optional path to a directory where to restore\n      variables.\n    scaffold: A `Scaffold` used for gathering or building supportive ops. If\n      not specified, a default one is created. It\'s used to finalize the graph.\n    hooks: Optional list of `SessionRunHook` objects.\n    chief_only_hooks: list of `SessionRunHook` objects. Activate these hooks if\n      `is_chief==True`, ignore otherwise.\n    save_checkpoint_secs: The frequency, in seconds, that a checkpoint is saved\n      using a default checkpoint saver. If both `save_checkpoint_steps` and\n      `save_checkpoint_secs` are set to `None`, then the default checkpoint\n      saver isn\'t used. If both are provided, then only `save_checkpoint_secs`\n      is used. Default 600.\n    save_summaries_steps: The frequency, in number of global steps, that the\n      summaries are written to disk using a default summary saver. If both\n      `save_summaries_steps` and `save_summaries_secs` are set to `None`, then\n      the default summary saver isn\'t used. Default 100.\n    save_summaries_secs: The frequency, in secs, that the summaries are written\n      to disk using a default summary saver.  If both `save_summaries_steps` and\n      `save_summaries_secs` are set to `None`, then the default summary saver\n      isn\'t used. Default not enabled.\n    config: an instance of `tf.ConfigProto` proto used to configure the session.\n      It\'s the `config` argument of constructor of `tf.Session`.\n    stop_grace_period_secs: Number of seconds given to threads to stop after\n      `close()` has been called.\n    log_step_count_steps: The frequency, in number of global steps, that the\n      global step/sec is logged.\n    max_wait_secs: Maximum time workers should wait for the session to\n      become available. This should be kept relatively short to help detect\n      incorrect code, but sometimes may need to be increased if the chief takes\n      a while to start up.\n    save_checkpoint_steps: The frequency, in number of global steps, that a\n      checkpoint is saved using a default checkpoint saver. If both\n      `save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then\n      the default checkpoint saver isn\'t used. If both are provided, then only\n      `save_checkpoint_secs` is used. Default not enabled.\n    summary_dir: A string.  Optional path to a directory where to\n      save summaries. If None, checkpoint_dir is used instead.\n    load_model_dir (str): The location of the checkpoint file used to load the\n      model weights.\n  Returns:\n    A `MonitoredSession` object.\n  """"""\n  if save_summaries_steps == USE_DEFAULT and save_summaries_secs == USE_DEFAULT:\n    save_summaries_steps = 100\n    save_summaries_secs = None\n  elif save_summaries_secs == USE_DEFAULT:\n    save_summaries_secs = None\n  elif save_summaries_steps == USE_DEFAULT:\n    save_summaries_steps = None\n\n  if (save_checkpoint_steps == USE_DEFAULT and\n      save_checkpoint_secs == USE_DEFAULT):\n    save_checkpoint_steps = None\n    save_checkpoint_secs = 600\n  elif save_checkpoint_secs == USE_DEFAULT:\n    save_checkpoint_secs = None\n  elif save_checkpoint_steps == USE_DEFAULT:\n    save_checkpoint_steps = None\n\n  if not is_chief:\n    session_creator = tf.train.WorkerSessionCreator(\n        scaffold=scaffold,\n        master=master,\n        config=config,\n        max_wait_secs=max_wait_secs)\n    return tf.train.MonitoredSession(\n        session_creator=session_creator, hooks=hooks or [],\n        stop_grace_period_secs=stop_grace_period_secs)\n\n  all_hooks = []\n  if chief_only_hooks:\n    all_hooks.extend(chief_only_hooks)\n\n  restore_all = False\n  if not load_model_dir:\n    load_model_dir = checkpoint_dir\n    restore_all = True\n\n  assign_ops, restore_dict = get_assign_ops_and_restore_dict(\n      tf.train.latest_checkpoint(load_model_dir), restore_all)\n\n  if ((restore_all or tf.train.latest_checkpoint(checkpoint_dir))\n      and len(assign_ops) == 0):\n  # Checking to see if we can use the default TensorFlow Session Creator\n  # We need two conditions to be true:\n  # 1a) We are not loading partial vars through load_model_dir OR\n  # 1b) There is a saved checkpoint file from which we can load\n  # 2) if there is no dtype mismatch between checkpoint vars and vars in graph\n    session_creator = tf.train.ChiefSessionCreator(\n        scaffold=scaffold,\n        checkpoint_dir=checkpoint_dir,\n        master=master,\n        config=config)\n\n  else: # load variables from the base model\'s checkpoint\n    if load_model_dir:\n      print(""Loading the base model from {}."".format(load_model_dir))\n    session_creator = TransferChiefSessionCreator(\n        scaffold=scaffold,\n        checkpoint_dir=load_model_dir,\n        master=master,\n        config=config,\n        load_fc=load_fc,\n        assign_ops=assign_ops,\n        restore_dict=restore_dict)\n\n  summary_dir = summary_dir or checkpoint_dir\n  if summary_dir:\n    if log_step_count_steps and log_step_count_steps > 0:\n      all_hooks.append(\n          tf.train.StepCounterHook(\n              output_dir=summary_dir, every_n_steps=log_step_count_steps))\n\n    if (save_summaries_steps and save_summaries_steps > 0) or (\n        save_summaries_secs and save_summaries_secs > 0):\n      all_hooks.append(tf.train.SummarySaverHook(\n          scaffold=scaffold,\n          save_steps=save_summaries_steps,\n          save_secs=save_summaries_secs,\n          output_dir=summary_dir))\n\n  if checkpoint_dir:\n    if (save_checkpoint_secs and save_checkpoint_secs > 0) or (\n        save_checkpoint_steps and save_checkpoint_steps > 0):\n      all_hooks.append(tf.train.CheckpointSaverHook(\n          checkpoint_dir,\n          save_steps=save_checkpoint_steps,\n          save_secs=save_checkpoint_secs,\n          scaffold=scaffold))\n\n  if hooks:\n    all_hooks.extend(hooks)\n  return tf.train.MonitoredSession(\n      session_creator=session_creator, hooks=all_hooks,\n      stop_grace_period_secs=stop_grace_period_secs)\n\nclass TransferChiefSessionCreator(tf.train.SessionCreator):\n  def __init__(self,\n               scaffold=None,\n               master=\'\',\n               config=None,\n               checkpoint_dir=None,\n               checkpoint_filename_with_path=None,\n               load_fc=False,\n               assign_ops=None,\n               restore_dict=None):\n    """"""Initializes a chief session creator.\n    Args:\n      scaffold: A `Scaffold` used for gathering or building supportive ops. If\n        not specified a default one is created. It\'s used to finalize the graph.\n      master: `String` representation of the TensorFlow master to use.\n      config: `ConfigProto` proto used to configure the session.\n      checkpoint_dir: A string.  Optional path to a directory where to restore\n        variables.\n      checkpoint_filename_with_path: Full file name path to the checkpoint file.\n    """"""\n    self._checkpoint_dir = checkpoint_dir\n    self._checkpoint_filename_with_path = checkpoint_filename_with_path\n    self._scaffold = scaffold or TransferScaffold()\n    self._session_manager = None\n    self._master = master\n    self._config = config\n    self._load_fc = load_fc\n    self._assign_ops = assign_ops\n    self._restore_dict = restore_dict\n\n  def _get_session_manager(self):\n    if self._session_manager:\n      return self._session_manager\n\n    self._session_manager = TransferSessionManager(\n        local_init_op=self._scaffold.local_init_op,\n        ready_op=self._scaffold.ready_op,\n        ready_for_local_init_op=self._scaffold.ready_for_local_init_op,\n        graph=tf.get_default_graph())\n    return self._session_manager\n\n  def create_session(self):\n    print(\'SCAFFOLD TYPE:\', type(self._scaffold))\n    self._scaffold.finalize()\n    # tf.get_default_graph()._unsafe_unfinalize()\n\n    return self._get_session_manager().prepare_session(\n        self._master,\n        saver=self._scaffold.saver,\n        checkpoint_dir=self._checkpoint_dir,\n        checkpoint_filename_with_path=self._checkpoint_filename_with_path,\n        config=self._config,\n        init_op=self._scaffold.init_op,\n        init_feed_dict=self._scaffold.init_feed_dict,\n        init_fn=self._scaffold.init_fn,\n        load_fc=self._load_fc,\n        assign_ops=self._assign_ops,\n        restore_dict=self._restore_dict)\n\nclass TransferScaffold(tf.train.Scaffold):\n  def finalize(self):\n    """"""Creates operations if needed and finalizes the graph.""""""\n    if self._init_op is None:\n      def default_init_op():\n        return tf.group(\n            tf.global_variables_initializer(),\n            resources.initialize_resources(resources.shared_resources()))\n      self._init_op = TransferScaffold.get_or_default(\n          \'init_op\',\n          tf.GraphKeys.INIT_OP,\n          default_init_op)\n    if self._ready_op is None:\n      def default_ready_op():\n        return tf.concat([\n            tf.report_uninitialized_variables(),\n            resources.report_uninitialized_resources()\n        ], 0)\n      self._ready_op = TransferScaffold.get_or_default(\n          \'ready_op\', tf.GraphKeys.READY_OP,\n          default_ready_op)\n    if self._ready_for_local_init_op is None:\n      def default_ready_for_local_init_op():\n        return tf.report_uninitialized_variables(\n            tf.global_variables())\n      self._ready_for_local_init_op = TransferScaffold.get_or_default(\n          \'ready_for_local_init_op\', tf.GraphKeys.READY_FOR_LOCAL_INIT_OP,\n          default_ready_for_local_init_op)\n    if self._local_init_op is None:\n      self._local_init_op = TransferScaffold.get_or_default(\n          \'local_init_op\', tf.GraphKeys.LOCAL_INIT_OP,\n          TransferScaffold.default_local_init_op)\n    if self._summary_op is None:\n      self._summary_op = TransferScaffold.get_or_default(\n          \'summary_op\', tf.GraphKeys.SUMMARY_OP, tf.summary.merge_all)\n    # pylint: disable=g-long-lambda\n    if self._saver is None:\n      self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n    # pylint: enable=g-long-lambda\n    self._saver.build()\n\n    # ops.get_default_graph().finalize()\n    # logging.info(\'Graph was finalized.\')\n    return self\n\nclass TransferSessionManager(tf.train.SessionManager):\n  def _restore_checkpoint(self,\n                          master,\n                          sess,\n                          saver=None,\n                          checkpoint_dir=None,\n                          checkpoint_filename_with_path=None,\n                          wait_for_checkpoint=False,\n                          max_wait_secs=7200,\n                          config=None,\n                          load_fc=False,\n                          assign_ops=None,\n                          restore_dict=None):\n    """"""Creates a `Session`, and tries to restore a checkpoint.\n    Args:\n      master: `String` representation of the TensorFlow master to use.\n      saver: A `Saver` object used to restore a model.\n      checkpoint_dir: Path to the checkpoint files. The latest checkpoint in the\n        dir will be used to restore.\n      checkpoint_filename_with_path: Full file name path to the checkpoint file.\n      wait_for_checkpoint: Whether to wait for checkpoint to become available.\n      max_wait_secs: Maximum time to wait for checkpoints to become available.\n      config: Optional `ConfigProto` proto used to configure the session.\n    Returns:\n      A pair (sess, is_restored) where \'is_restored\' is `True` if\n      the session could be restored, `False` otherwise.\n    Raises:\n      ValueError: If both checkpoint_dir and checkpoint_filename_with_path are\n        set.\n    """"""\n    self._target = master\n    # sess = tf.Session(self._target, graph=self._graph, config=config)\n\n    if checkpoint_dir and checkpoint_filename_with_path:\n      raise ValueError(""Can not provide both checkpoint_dir and ""\n                       ""checkpoint_filename_with_path."")\n    # If either saver or checkpoint_* is not specified, cannot restore. Just\n    # return.\n    print(\'checkpoint_dir\', checkpoint_dir)\n    print(\'checkpoint_filename_with_path\', checkpoint_filename_with_path)\n    if not saver or not (checkpoint_dir or checkpoint_filename_with_path):\n      return sess, False\n\n    if checkpoint_filename_with_path:\n      # saver.restore(sess, checkpoint_filename_with_path)\n      # restore_certain_variables(sess, checkpoint_filename_with_path)\n      run_assign_and_saver(\n          sess, checkpoint_filename_with_path, assign_ops, restore_dict)\n      return sess, True\n\n    # Waits up until max_wait_secs for checkpoint to become available.\n    wait_time = 0\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    while not ckpt or not ckpt.model_checkpoint_path:\n      if wait_for_checkpoint and wait_time < max_wait_secs:\n        tf.logging.info(""Waiting for checkpoint to be available."")\n        time.sleep(self._recovery_wait_secs)\n        wait_time += self._recovery_wait_secs\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n      else:\n        return sess, False\n\n    # Loads the checkpoint.\n    ckpt_file = ckpt.model_checkpoint_path\n    # restore_certain_variables(sess, ckpt_file)\n    run_assign_and_saver(sess, ckpt_file, assign_ops, restore_dict)\n    saver.recover_last_checkpoints(ckpt.all_model_checkpoint_paths)\n    return sess, True\n\n  def prepare_session(self,\n                      master,\n                      init_op=None,\n                      saver=None,\n                      checkpoint_dir=None,\n                      checkpoint_filename_with_path=None,\n                      wait_for_checkpoint=False,\n                      max_wait_secs=7200,\n                      config=None,\n                      init_feed_dict=None,\n                      init_fn=None,\n                      load_fc=False,\n                      assign_ops=None,\n                      restore_dict=None):\n    """"""Creates a `Session`. Makes sure the model is ready to be used.\n      Creates a `Session` on \'master\'. If a `saver` object is passed in, and\n      `checkpoint_dir` points to a directory containing valid checkpoint\n      files, then it will try to recover the model from checkpoint. If\n      no checkpoint files are available, and `wait_for_checkpoint` is\n      `True`, then the process would check every `recovery_wait_secs`,\n      up to `max_wait_secs`, for recovery to succeed.\n      If the model cannot be recovered successfully then it is initialized by\n      running the `init_op` and calling `init_fn` if they are provided.\n      The `local_init_op` is also run after init_op and init_fn, regardless of\n      whether the model was recovered successfully, but only if\n      `ready_for_local_init_op` passes.\n      If the model is recovered from a checkpoint it is assumed that all\n      global variables have been initialized, in particular neither `init_op`\n      nor `init_fn` will be executed.\n      It is an error if the model cannot be recovered and no `init_op`\n      or `init_fn` or `local_init_op` are passed.\n      Args:\n        master: `String` representation of the TensorFlow master to use.\n        init_op: Optional `Operation` used to initialize the model.\n        saver: A `Saver` object used to restore a model.\n        checkpoint_dir: Path to the checkpoint files. The latest checkpoint in\n          the dir will be used to restore.\n        checkpoint_filename_with_path: Full file name path to the checkpoint\n          file.\n        wait_for_checkpoint: Whether to wait for checkpoint to become available.\n        max_wait_secs: Maximum time to wait for checkpoints to become available.\n        config: Optional `ConfigProto` proto used to configure the session.\n        init_feed_dict: Optional dictionary that maps `Tensor` objects to feed\n          values.  This feed dictionary is passed to the session `run()` call\n          when running the init op.\n        init_fn: Optional callable used to initialize the model. Called after\n          the optional `init_op` is called.  The callable must accept one\n          argument, the session being initialized.\n      Returns:\n        A `Session` object that can be used to drive the model.\n      Raises:\n        RuntimeError: If the model cannot be initialized or recovered.\n        ValueError: If both checkpoint_dir and checkpoint_filename_with_path are\n          set.\n    """"""\n    sess = tf.Session(master, graph=self._graph, config=config)\n    if init_op is None and not init_fn and self._local_init_op is None:\n      raise RuntimeError(""Model is not initialized and no init_op or ""\n                         ""init_fn or local_init_op was given"")\n    if init_op is not None:\n      sess.run(init_op, feed_dict=init_feed_dict)\n    if init_fn:\n      init_fn(sess)\n    sess.run(tf.local_variables_initializer()) # why do i have to add this?\n    print(""LOCAL INIT OP"", self._local_init_op)\n    sess, is_loaded_from_checkpoint = self._restore_checkpoint(\n        master,\n        sess,\n        saver,\n        checkpoint_dir=checkpoint_dir,\n        checkpoint_filename_with_path=checkpoint_filename_with_path,\n        wait_for_checkpoint=wait_for_checkpoint,\n        max_wait_secs=max_wait_secs,\n        config=config,\n        load_fc=load_fc,\n        assign_ops=assign_ops,\n        restore_dict=restore_dict)\n\n\n    local_init_success, msg = self._try_run_local_init_op(sess)\n    if not local_init_success:\n      raise RuntimeError(\n          ""Init operations did not make model ready for local_init.  ""\n          ""Init op: %s, init fn: %s, error: %s"" % (_maybe_name(init_op),\n                                                   init_fn,\n                                                   msg))\n\n    is_ready, msg = self._model_ready(sess)\n    if not is_ready:\n      raise RuntimeError(\n          ""Init operations did not make model ready.  ""\n          ""Init op: %s, init fn: %s, local_init_op: %s, error: %s"" %\n          (_maybe_name(init_op), init_fn, self._local_init_op, msg))\n    return sess\n\ndef _restore_embed(embed_var, var_to_shape_map, reader):\n  if len([var for var in var_to_shape_map if \'EmbeddingMatrix\' in var]) > 0:\n    return None, None # assume same name\n  for var in var_to_shape_map:\n    if (var.endswith(\'dense/kernel\')\n        and var_to_shape_map[var] == tf.transpose(embed_var).shape):\n      print(\'Assigning\', var, \'to\', embed_var.name)\n      tensor = reader.get_tensor(var).T\n      if tensor.dtype != var.dtype.as_numpy_dtype():\n        return embed_var.assign(tf.cast(tensor, embed_var.dtype)), True\n      return embed_var, False\n  return None, None\n\ndef get_assign_ops_and_restore_dict(filename, restore_all=False):\n  """"""Helper function to read variable checkpoints from filename.\n  Iterates through all vars in restore_all=False else all trainable vars. It\n  attempts to match variables by name and variable shape. Returns a possibly\n  empty list of assign_ops, and a possibly empty dictionary for tf.train.Saver()\n  """"""\n  def check_name_and_shape(name, var, shape_map):\n    if name in shape_map:\n      # Cannot check variables with unknown sizes such as cudnn rnns\n      if str(var.shape) == ""<unknown>"":\n        # Just return True and hope the shapes match\n        return True\n      if var.shape == shape_map[name]:\n        return True\n    return False\n\n  assign_ops = []\n  restore_dict = {}\n\n  try:\n    reader = tf.train.NewCheckpointReader(filename)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n\n    variables = tf.trainable_variables()\n    if restore_all:\n      variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n    for var in variables:\n      idx = var.name.find("":"")\n      if idx != -1:\n        true_name = var.name[:idx]\n      loss_idx = re.search(""Loss_Optimization"", true_name)\n      if \'EmbeddingMatrix\' in true_name:\n        embed_restore, assign = _restore_embed(var, var_to_shape_map, reader)\n        if assign:\n          assign_ops.append(embed_restore)\n        else:\n          restore_dict[true_name] = embed_restore\n      if check_name_and_shape(true_name, var, var_to_shape_map):\n        tensor = reader.get_tensor(true_name)\n        if tensor.dtype != var.dtype.as_numpy_dtype():\n          assign_ops.append(var.assign(tf.cast(tensor, var.dtype)))\n        else:\n          restore_dict[true_name] = var\n      elif loss_idx:\n        loss_idx = loss_idx.end()\n        if FP32_TEST.search(true_name):\n          true_name = FP32_TEST.sub("""", true_name)\n        else:\n          true_name = (true_name[:loss_idx]\n                       + ""/Loss_Optimization/FP32-master-copy""\n                       + true_name[loss_idx:])\n        if check_name_and_shape(true_name, var, var_to_shape_map):\n          tensor = reader.get_tensor(true_name)\n          if tensor.dtype != var.dtype.as_numpy_dtype():\n            assign_ops.append(var.assign(tf.cast(tensor, var.dtype)))\n          else:\n            restore_dict[true_name] = var\n      else:\n        print(""Not restoring {}"".format(var.name))\n        if true_name not in var_to_shape_map:\n          print(""true name [{}] was not in shape map"".format(true_name))\n        else:\n          if var.shape != var_to_shape_map[true_name]:\n            print((""var.shape [{}] does not match var_to_shape_map[true_name]""\n                   ""[{}]"").format(var.shape, var_to_shape_map[true_name]))\n        print(""WARNING: Run will mostly error out due to this"")\n  except Exception as e:  # pylint: disable=broad-except\n    print(str(e))\n    if ""corrupted compressed block contents"" in str(e):\n      print(""It\'s likely that your checkpoint file has been compressed ""\n            ""with SNAPPY."")\n    if (""Data loss"" in str(e) and\n        (any([e in filename for e in ["".index"", "".meta"", "".data""]]))):\n      proposed_file = ""."".join(filename.split(""."")[0:-1])\n      v2_file_error_template = """"""\n      It\'s likely that this is a V2 checkpoint and you need to provide the\n      filename *prefix*.  Try removing the \'.\' and extension.  Try:\n      inspect checkpoint --file_name = {}""""""\n      print(v2_file_error_template.format(proposed_file))\n    raise ValueError(""Error in loading checkpoint"")\n  return assign_ops, restore_dict\n\ndef run_assign_and_saver(sess, filename, assign_ops, restore_dict):\n  """"""Helper function to restore variables. All variables with the same dtype\n  can be restored using tf.train.Saver(). All variables with different dtype\n  are restored using assign_ops\n  """"""\n  if restore_dict:\n    restorer = tf.train.Saver(restore_dict)\n    restorer.restore(sess, filename)\n  if assign_ops:\n    sess.run(assign_ops)\n\ndef _maybe_name(obj):\n  """"""Returns object name if it has one, or a message otherwise.\n  This is useful for names that apper in error messages.\n  Args:\n    obj: Object to get the name of.\n  Returns:\n    name, ""None"", or a ""no name"" message.\n  """"""\n  if obj is None:\n    return ""None""\n  elif hasattr(obj, ""name""):\n    return obj.name\n  else:\n    return ""<no name for %s>"" % type(obj)\n'"
open_seq2seq/utils/hooks.py,24,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport math\nimport os\nimport time\n\nimport tensorflow as tf\n\nfrom open_seq2seq.utils.utils import deco_print, log_summaries_from_dict, \\\n                                     get_results_for_epoch\n\n\nclass BroadcastGlobalVariablesHook(tf.train.SessionRunHook):\n  """"""\n  SessionRunHook that will broadcast all global variables from root rank\n  to all other processes during initialization.\n  This is necessary to ensure consistent initialization of all workers when\n  training is started with random weights or restored from a checkpoint.\n  """"""\n\n  def __init__(self, root_rank, device=\'\'):\n    """"""Construct a new BroadcastGlobalVariablesHook that will broadcast all\n    global variables from root rank to all other processes during initialization.\n    Args:\n      root_rank:\n        Rank that will send data, other ranks will receive data.\n      device:\n        Device to be used for broadcasting. Uses GPU by default\n        if Horovod was build with HOROVOD_GPU_BROADCAST.\n    """"""\n    super(BroadcastGlobalVariablesHook, self).__init__()\n    self.root_rank = root_rank\n    self.bcast_op = None\n    self.device = device\n\n  def begin(self):\n    def broadcast_global_variables(root_rank):\n      from horovod.tensorflow.mpi_ops import broadcast\n      ops = []\n      for var in tf.global_variables():\n        if var.dtype.base_dtype == tf.float16:\n          ops.append(tf.assign(var, tf.cast(broadcast(tf.cast(var, tf.float32),\n                                                      root_rank), tf.float16)))\n        else:\n          ops.append(tf.assign(var, broadcast(var, root_rank)))\n      return tf.group(*ops)\n\n    if not self.bcast_op or self.bcast_op.graph != tf.get_default_graph():\n      with tf.device(self.device):\n        self.bcast_op = broadcast_global_variables(self.root_rank)\n\n  def after_create_session(self, session, coord):\n    session.run(self.bcast_op)\n\n\nclass PrintSamplesHook(tf.train.SessionRunHook):\n  """"""Session hook that prints training samples and prediction from time to time\n  """"""\n  def __init__(self, every_steps, model):\n    super(PrintSamplesHook, self).__init__()\n    self._timer = tf.train.SecondOrStepTimer(every_steps=every_steps)\n    self._iter_count = 0\n    self._global_step = None\n    self._model = model\n    # using only first GPU\n    output_tensors = model.get_output_tensors(0)\n    self._fetches = [\n        model.get_data_layer(0).input_tensors,\n        output_tensors,\n    ]\n\n  def begin(self):\n    self._iter_count = 0\n    self._global_step = tf.train.get_global_step()\n\n  def before_run(self, run_context):\n    if self._timer.should_trigger_for_step(self._iter_count):\n      return tf.train.SessionRunArgs([self._fetches, self._global_step])\n    return tf.train.SessionRunArgs([[], self._global_step])\n\n  def after_run(self, run_context, run_values):\n    results, step = run_values.results\n    self._iter_count = step\n\n    if not results:\n      return\n    self._timer.update_last_triggered_step(self._iter_count - 1)\n\n    input_values, output_values = results\n    dict_to_log = self._model.maybe_print_logs(input_values, output_values, step)\n    # optionally logging to tensorboard any values\n    # returned from maybe_print_logs\n    if self._model.params[\'save_summaries_steps\'] and dict_to_log:\n      log_summaries_from_dict(\n          dict_to_log,\n          self._model.params[\'logdir\'],\n          step,\n      )\n\n\nclass PrintLossAndTimeHook(tf.train.SessionRunHook):\n  """"""Session hook that prints training samples and prediction from time to time\n  """"""\n  def __init__(self, every_steps, model, print_ppl=False):\n    super(PrintLossAndTimeHook, self).__init__()\n    self._timer = tf.train.SecondOrStepTimer(every_steps=every_steps)\n    self._every_steps = every_steps\n    self._iter_count = 0\n    self._global_step = None\n    self._model = model\n    self._fetches = [model.loss]\n    self._last_time = time.time()\n    self._print_ppl = print_ppl\n\n  def begin(self):\n    self._iter_count = 0\n    self._global_step = tf.train.get_global_step()\n\n  def before_run(self, run_context):\n    if self._timer.should_trigger_for_step(self._iter_count):\n      return tf.train.SessionRunArgs([self._fetches, self._global_step])\n    return tf.train.SessionRunArgs([[], self._global_step])\n\n  def after_run(self, run_context, run_values):\n    results, step = run_values.results\n    self._iter_count = step\n\n    if not results:\n      return\n    self._timer.update_last_triggered_step(self._iter_count - 1)\n\n    if self._model.steps_in_epoch is None:\n      deco_print(""Global step {}:"".format(step), end="" "")\n    else:\n      deco_print(\n          ""Epoch {}, global step {}:"".format(\n              step // self._model.steps_in_epoch, step),\n          end="" "",\n      )\n\n    loss = results[0]\n    if not self._model.on_horovod or self._model.hvd.rank() == 0:\n      if self._print_ppl:\n        deco_print(""Train loss: {:.4f} | ppl = {:.4f} | bpc = {:.4f}""\n                   .format(loss, math.exp(loss),\n                           loss/math.log(2)),\n                   start="""", end="", "")\n      else:\n        deco_print(\n          ""Train loss: {:.4f} "".format(loss),\n          offset=4)\n\n    tm = (time.time() - self._last_time) / self._every_steps\n    m, s = divmod(tm, 60)\n    h, m = divmod(m, 60)\n\n    deco_print(\n        ""time per step = {}:{:02}:{:.3f}"".format(int(h), int(m), s),\n        start="""",\n    )\n    self._last_time = time.time()\n\n\nclass RunEvaluationHook(tf.train.SessionRunHook):\n  """"""Session hook that runs evaluation on a validation set\n  """"""\n  def __init__(self, every_steps, model, last_step=-1, print_ppl=False):\n    super(RunEvaluationHook, self).__init__()\n    self._timer = tf.train.SecondOrStepTimer(every_steps=every_steps)\n    self._iter_count = 0\n    self._global_step = None\n    self._model = model\n    self._triggered = False\n    self._last_step = last_step\n    self._eval_saver = tf.train.Saver(\n        save_relative_paths=True,\n        max_to_keep=self._model.params[\'num_checkpoints\']\n    )\n    self._best_eval_loss = 1e9\n    self._print_ppl = print_ppl\n\n  def begin(self):\n    self._iter_count = 0\n    self._global_step = tf.train.get_global_step()\n\n  def before_run(self, run_context):\n    self._triggered = self._timer.should_trigger_for_step(self._iter_count)\n    return tf.train.SessionRunArgs([[], self._global_step])\n\n  def after_run(self, run_context, run_values):\n    results, step = run_values.results\n    self._iter_count = step\n\n    if not self._triggered and step != self._last_step - 1:\n      return\n    self._timer.update_last_triggered_step(self._iter_count - 1)\n\n    if not self._model.on_horovod or self._model.hvd.rank() == 0:\n      deco_print(""Running evaluation on a validation set:"")\n\n    results_per_batch, total_loss = get_results_for_epoch(\n        self._model, run_context.session, mode=""eval"", compute_loss=True,\n    )\n\n\n    if not self._model.on_horovod or self._model.hvd.rank() == 0:\n      if self._print_ppl:\n        deco_print(""Validation loss: {:.4f} | ppl = {:.4f} | bpc = {:.4f}""\n                   .format(total_loss, math.exp(total_loss),\n                           total_loss/math.log(2)), offset=4)\n      else:\n        deco_print(\n          ""Validation loss: {:.4f} "".format(total_loss),\n          offset=4)\n\n\n      dict_to_log = self._model.finalize_evaluation(results_per_batch, step)\n      dict_to_log[\'eval_loss\'] = total_loss\n\n      if self._print_ppl:\n        # Add bpc and ppl metrics to tensorboard\n        dict_to_log[\'ppl\'] = math.exp(total_loss)\n        dict_to_log[\'bpc\'] = math.exp(total_loss/math.log(2))\n        \n      # saving the best validation model\n      if self._model.params[\'save_checkpoint_steps\'] and \\\n         total_loss < self._best_eval_loss:\n        self._best_eval_loss = total_loss\n        self._eval_saver.save(\n            run_context.session,\n            os.path.join(self._model.params[\'logdir\'], \'best_models\',\n                         \'val_loss={:.4f}-step\'.format(total_loss)),\n            global_step=step + 1,\n        )\n\n      # optionally logging to tensorboard any values\n      # returned from maybe_print_logs\n      if self._model.params[\'save_summaries_steps\']:\n        log_summaries_from_dict(\n            dict_to_log,\n            self._model.params[\'logdir\'],\n            step,\n        )\n'"
open_seq2seq/utils/metrics.py,0,"b""# Copyright (c) 2018 NVIDIA Corporation\n'''\nThis file implements function to calcuate basic metrics.\n'''\nimport numpy as np\nimport tensorflow as tf\n\ndef true_positives(labels, preds):\n  return np.sum(np.logical_and(labels, preds)) \n\ndef accuracy(labels, preds):\n  return np.sum(np.equal(labels, preds)) / len(preds)\n\ndef recall(labels, preds):\n  return true_positives(labels, preds) / np.sum(labels)\n\ndef precision(labels, preds):\n  return true_positives(labels, preds) / np.sum(preds)\n\ndef f1(labels, preds):\n  rec = recall(labels, preds)\n  pre = precision(labels, preds)\n  if rec == 0 or pre == 0:\n    return 0\n  return 2 * rec * pre / (rec + pre)\n"""
open_seq2seq/utils/utils.py,23,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport ast\nimport copy\nimport datetime\nimport os\nimport pprint\nimport runpy\nimport shutil\nimport subprocess\nimport sys\nimport time\n\n\nimport numpy as np\nimport six\nfrom six import string_types\nfrom six.moves import range\nimport tensorflow as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.client import device_lib\n\n\ndef get_available_gpus():\n  # WARNING: this method will take all the memory on all devices!\n  local_device_protos = device_lib.list_local_devices()\n  return [x.name for x in local_device_protos if x.device_type == \'GPU\']\n\n\ndef clip_sparse(value, size):\n  dense_shape_clipped = value.dense_shape\n  dense_shape_clipped[0] = size\n  indices_clipped = []\n  values_clipped = []\n  for idx_tuple, val in zip(value.indices, value.values):\n    if idx_tuple[0] < size:\n      indices_clipped.append(idx_tuple)\n      values_clipped.append(val)\n  return tf.SparseTensorValue(np.array(indices_clipped),\n                              np.array(values_clipped),\n                              dense_shape_clipped)\n\n\ndef collect_if_horovod(value, hvd, mode=\'sum\'):\n  """"""Collects values from all workers if run on Horovod.\n  Note, that on all workers except first this function will return None.\n\n  Args:\n    value: value to collect.\n    hvd: horovod.tensorflow module or None\n    mode: could be ""sum"", ""mean"" or ""gather"", indicating reduce_sum or gather.\n        For ""sum"" and ""mean"" value has to be numerical, for ""gather"", value has\n        to be iterable.\n\n  Returns:\n    collected results if run on Horovod or value otherwise.\n  """"""\n  if hvd is None:\n    return value\n\n  import mpi4py.rc\n  mpi4py.rc.initialize = False\n  from mpi4py import MPI\n\n  values = MPI.COMM_WORLD.gather(value)\n  # synchronize all workers\n  MPI.COMM_WORLD.Barrier()\n\n  if MPI.COMM_WORLD.Get_rank() != 0:\n    return None\n\n  if mode == \'sum\':\n    return np.sum(values)\n  elif mode == \'mean\':\n    return np.mean(values)\n  elif mode == \'gather\':\n    return [item for sl in values for item in sl]\n  else:\n    raise ValueError(""Incorrect mode: {}"".format(mode))\n\n\ndef clip_last_batch(last_batch, true_size):\n  last_batch_clipped = []\n  for val in last_batch:\n    if isinstance(val, tf.SparseTensorValue):\n      last_batch_clipped.append(clip_sparse(val, true_size))\n    else:\n      last_batch_clipped.append(val[:true_size])\n  return last_batch_clipped\n\n\ndef iterate_data(model, sess, compute_loss, mode, verbose, num_steps=None):\n  total_time = 0.0\n  bench_start = model.params.get(\'bench_start\', 10)\n  results_per_batch = []\n\n  size_defined = model.get_data_layer().get_size_in_samples() is not None\n  if size_defined:\n    dl_sizes = []\n\n  if compute_loss:\n    total_loss = 0.0\n\n  total_samples = []\n  fetches = []\n\n  # on horovod num_gpus is 1\n  for worker_id in range(model.num_gpus):\n    cur_fetches = [\n        model.get_data_layer(worker_id).input_tensors,\n        model.get_output_tensors(worker_id),\n    ]\n    if compute_loss:\n      cur_fetches.append(model.eval_losses[worker_id])\n    if size_defined:\n      dl_sizes.append(model.get_data_layer(worker_id).get_size_in_samples())\n    try:\n      total_objects = 0.0\n      cur_fetches.append(model.get_num_objects_per_step(worker_id))\n    except NotImplementedError:\n      total_objects = None\n      deco_print(""WARNING: Can\'t compute number of objects per step, since ""\n                 ""train model does not define get_num_objects_per_step method."")\n    fetches.append(cur_fetches)\n    total_samples.append(0.0)\n\n  sess.run([model.get_data_layer(i).iterator.initializer\n            for i in range(model.num_gpus)])\n\n  step = 0\n  processed_batches = 0\n  if verbose:\n    if model.on_horovod:\n      ending = "" on worker {}"".format(model.hvd.rank())\n    else:\n      ending = """"\n\n  while True:\n    tm = time.time()\n    fetches_vals = {}\n    if size_defined:\n      fetches_to_run = {}\n      # removing finished data layers\n      for worker_id in range(model.num_gpus):\n        if total_samples[worker_id] < dl_sizes[worker_id]:\n          fetches_to_run[worker_id] = fetches[worker_id]\n      fetches_vals = sess.run(fetches_to_run)\n    else:\n      # if size is not defined we have to process fetches sequentially, so not\n      # to lose data when exception is thrown on one data layer\n      for worker_id, one_fetch in enumerate(fetches):\n        try:\n          fetches_vals[worker_id] = sess.run(one_fetch)\n        except tf.errors.OutOfRangeError:\n          continue\n\n    if step >= bench_start:\n      total_time += time.time() - tm\n\n    # looping over num_gpus. In Horovod case this loop is ""dummy"",\n    # since num_gpus = 1\n    for worker_id, fetches_val in fetches_vals.items():\n      if compute_loss:\n        inputs, outputs, loss = fetches_val[:3]\n      else:\n        inputs, outputs = fetches_val[:2]\n\n      if total_objects is not None and step >= bench_start:\n        total_objects += np.sum(fetches_val[-1])\n\n      # assuming any element of inputs[""source_tensors""] .shape[0] is batch size\n      batch_size = inputs[""source_tensors""][0].shape[0]\n      total_samples[worker_id] += batch_size\n\n      if size_defined:\n        # this data_layer is at the last batch with few more elements, cutting\n        if total_samples[worker_id] > dl_sizes[worker_id]:\n          last_batch_size = dl_sizes[worker_id] % batch_size\n          for key, value in inputs.items():\n            inputs[key] = model.clip_last_batch(value, last_batch_size)\n          outputs = model.clip_last_batch(outputs, last_batch_size)\n\n      processed_batches += 1\n\n      if compute_loss:\n        total_loss += loss * batch_size\n\n      if mode == \'eval\':\n        results_per_batch.append(model.evaluate(inputs, outputs))\n      elif mode == \'infer\':\n        results_per_batch.append(model.infer(inputs, outputs))\n      else:\n        raise ValueError(""Unknown mode: {}"".format(mode))\n\n    if verbose:\n      if size_defined:\n        data_size = int(np.sum(np.ceil(np.array(dl_sizes) / batch_size)))\n        if step == 0 or len(fetches_vals) == 0 or \\\n           (data_size > 10 and processed_batches % (data_size // 10) == 0):\n          deco_print(""Processed {}/{} batches{}"".format(\n              processed_batches, data_size, ending\n          ))\n      else:\n        deco_print(""Processed {} batches{}"".format(processed_batches, ending),\n                   end=\'\\r\')\n\n    if len(fetches_vals) == 0:\n      break\n    step += 1\n    # break early in the case of INT8 calibration\n    if num_steps is not None and step >= num_steps:\n      break\n\n  if verbose:\n    if step > bench_start:\n      deco_print(\n          ""Avg time per step{}: {:.3}s"".format(\n              ending, 1.0 * total_time / (step - bench_start)\n          ),\n      )\n      if total_objects is not None:\n        avg_objects = 1.0 * total_objects / total_time\n        deco_print(""Avg objects per second{}: {:.3f}"".format(ending,\n                                                             avg_objects))\n    else:\n      deco_print(""Not enough steps for benchmarking{}"".format(ending))\n\n\n  if compute_loss:\n    return results_per_batch, total_loss, np.sum(total_samples)\n  else:\n    return results_per_batch\n\n\ndef get_results_for_epoch(model, sess, compute_loss, mode, verbose=False):\n  if compute_loss:\n    results_per_batch, total_loss, total_samples = iterate_data(\n        model, sess, compute_loss, mode, verbose,\n    )\n  else:\n    results_per_batch = iterate_data(\n        model, sess, compute_loss, mode, verbose,\n    )\n\n  if compute_loss:\n    total_samples = collect_if_horovod(total_samples, model.hvd, \'sum\')\n    total_loss = collect_if_horovod(total_loss, model.hvd, \'sum\')\n  results_per_batch = collect_if_horovod(results_per_batch, model.hvd, \'gather\')\n\n  if results_per_batch is None:\n    # returning dummy tuple of correct shape if not in master worker\n    if compute_loss:\n      return None, None\n    else:\n      return None\n\n  if compute_loss:\n    return results_per_batch, total_loss / total_samples\n  else:\n    return results_per_batch\n\n\ndef log_summaries_from_dict(dict_to_log, output_dir, step):\n  """"""\n  A function that writes values from dict_to_log to a tensorboard\n  log file inside output_dir.\n\n  Args:\n    dict_to_log (dict):\n      A dictiontary containing the tags and scalar values to log.\n      The dictionary values could also contain tf.Summary.Value objects\n      to support logging of image and audio data. In this mode, the\n      dictionary key is ignored, as tf.Summary.Value already contains a\n      tag.\n    output_dir (str): dir containing the tensorboard file\n    step (int): current training step\n  """"""\n  sm_writer = tf.summary.FileWriterCache.get(output_dir)\n  for tag, value in dict_to_log.items():\n    if isinstance(value, tf.Summary.Value):\n      sm_writer.add_summary(\n          tf.Summary(value=[value]),\n          global_step=step,\n      )\n    else:\n      sm_writer.add_summary(\n          tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)]),\n          global_step=step,\n      )\n    sm_writer.flush()\n\n\ndef get_git_hash():\n  try:\n    return subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\'],\n                                   stderr=subprocess.STDOUT).decode()\n  except subprocess.CalledProcessError as e:\n    return ""{}\\n"".format(e.output.decode(""utf-8""))\n\n\ndef get_git_diff():\n  try:\n    return subprocess.check_output([\'git\', \'diff\'],\n                                   stderr=subprocess.STDOUT).decode()\n  except subprocess.CalledProcessError as e:\n    return ""{}\\n"".format(e.output.decode(""utf-8""))\n\n\nclass Logger(object):\n  def __init__(self, stream, log_file):\n    self.stream = stream\n    self.log = log_file\n\n  def write(self, msg):\n    self.stream.write(msg)\n    self.log.write(msg)\n\n  def flush(self):\n    self.stream.flush()\n    self.log.flush()\n\n\ndef flatten_dict(dct):\n  flat_dict = {}\n  for key, value in dct.items():\n    if isinstance(value, (int, float, string_types, bool)):\n      flat_dict.update({key: value})\n    elif isinstance(value, dict):\n      flat_dict.update(\n          {key + \'/\' + k: v for k, v in flatten_dict(dct[key]).items()}\n      )\n  return flat_dict\n\n\ndef nest_dict(flat_dict):\n  nst_dict = {}\n  for key, value in flat_dict.items():\n    nest_keys = key.split(\'/\')\n    cur_dict = nst_dict\n    for i in range(len(nest_keys) - 1):\n      if nest_keys[i] not in cur_dict:\n        cur_dict[nest_keys[i]] = {}\n      cur_dict = cur_dict[nest_keys[i]]\n    cur_dict[nest_keys[-1]] = value\n  return nst_dict\n\n\ndef nested_update(org_dict, upd_dict):\n  for key, value in upd_dict.items():\n    if isinstance(value, dict):\n      if key in org_dict:\n        if not isinstance(org_dict[key], dict):\n          raise ValueError(\n              ""Mismatch between org_dict and upd_dict at node {}"".format(key)\n          )\n        nested_update(org_dict[key], value)\n      else:\n        org_dict[key] = value\n    else:\n      org_dict[key] = value\n\n\ndef mask_nans(x):\n  x_zeros = tf.zeros_like(x)\n  x_mask = tf.is_finite(x)\n  y = tf.where(x_mask, x, x_zeros)\n  return y\n\n\ndef deco_print(line, offset=0, start=""*** "", end=\'\\n\'):\n  if six.PY2:\n    print((start + "" "" * offset + line).encode(\'utf-8\'), end=end)\n  else:\n    print(start + "" "" * offset + line, end=end)\n\n\ndef array_to_string(row, vocab, delim=\' \'):\n  n = len(vocab)\n  return delim.join(map(lambda x: vocab[x], [r for r in row if 0 <= r < n]))\n\n\ndef text_ids_to_string(row, vocab, S_ID, EOS_ID, PAD_ID,\n                       ignore_special=False, delim=\' \'):\n  """"""For _-to-text outputs this function takes a row with ids,\n  target vocabulary and prints it as a human-readable string\n  """"""\n  n = len(vocab)\n  if ignore_special:\n    f_row = []\n    for char_id in row:\n      if char_id == EOS_ID:\n        break\n      if char_id != PAD_ID and char_id != S_ID:\n        f_row += [char_id]\n    return delim.join(map(lambda x: vocab[x], [r for r in f_row if 0 < r < n]))\n  else:\n    return delim.join(map(lambda x: vocab[x], [r for r in row if 0 < r < n]))\n\n\ndef check_params(config, required_dict, optional_dict):\n  if required_dict is None or optional_dict is None:\n    return\n\n  for pm, vals in required_dict.items():\n    if pm not in config:\n      raise ValueError(""{} parameter has to be specified"".format(pm))\n    else:\n      if vals == str:\n        vals = string_types\n      if vals and isinstance(vals, list) and config[pm] not in vals:\n        raise ValueError(""{} has to be one of {}"".format(pm, vals))\n      if vals and not isinstance(vals, list) and not isinstance(config[pm], vals):\n        raise ValueError(""{} has to be of type {}"".format(pm, vals))\n\n  for pm, vals in optional_dict.items():\n    if vals == str:\n      vals = string_types\n    if pm in config:\n      if vals and isinstance(vals, list) and config[pm] not in vals:\n        raise ValueError(""{} has to be one of {}"".format(pm, vals))\n      if vals and not isinstance(vals, list) and not isinstance(config[pm], vals):\n        raise ValueError(""{} has to be of type {}"".format(pm, vals))\n\n  for pm in config:\n    if pm not in required_dict and pm not in optional_dict:\n      raise ValueError(""Unknown parameter: {}"".format(pm))\n\n\ndef cast_types(input_dict, dtype):\n  cast_input_dict = {}\n  for key, value in input_dict.items():\n    if isinstance(value, tf.Tensor):\n      if value.dtype == tf.float16 or value.dtype == tf.float32:\n        if value.dtype.base_dtype != dtype.base_dtype:\n          cast_input_dict[key] = tf.cast(value, dtype)\n          continue\n    if isinstance(value, dict):\n      cast_input_dict[key] = cast_types(input_dict[key], dtype)\n      continue\n    if isinstance(value, list):\n      cur_list = []\n      for nest_value in value:\n        if isinstance(nest_value, tf.Tensor):\n          if nest_value.dtype == tf.float16 or nest_value.dtype == tf.float32:\n            if nest_value.dtype.base_dtype != dtype.base_dtype:\n              cur_list.append(tf.cast(nest_value, dtype))\n              continue\n        cur_list.append(nest_value)\n      cast_input_dict[key] = cur_list\n      continue\n    cast_input_dict[key] = input_dict[key]\n  return cast_input_dict\n\ndef get_interactive_infer_results(model, sess, model_in):\n  fetches = [\n      model.get_data_layer().input_tensors,\n      model.get_output_tensors(),\n  ]\n\n  feed_dict = model.get_data_layer().create_feed_dict(model_in)\n\n  inputs, outputs = sess.run(fetches, feed_dict=feed_dict)\n\n  return model.infer(inputs, outputs)\n\ndef get_base_config(args):\n  """"""This function parses the command line arguments, reads the config file, and\n  gets the base_model from the config.\n\n  Args:\n    args (str): The command line arugments\n\n  Returns\n    args (dict): The arguments parsed into a dictionary\n    base_config (dict): The config read from the file and ammended with the\n      command line arguments\n    base_model (OpenSeq2Seq model): The model specified in the config file\n    config_module (dict): The raw config file processed by runpy\n  """"""\n  parser = argparse.ArgumentParser(description=\'Experiment parameters\')\n  parser.add_argument(""--config_file"", required=True,\n                      help=""Path to the configuration file"")\n  parser.add_argument(""--mode"", default=\'train\',\n                      help=""Could be \\""train\\"", \\""eval\\"", ""\n                           ""\\""train_eval\\"" or \\""infer\\"""")\n  parser.add_argument(""--infer_output_file"", default=\'infer-out.txt\',\n                      help=""Path to the output of inference"")\n  parser.add_argument(\'--continue_learning\', dest=\'continue_learning\',\n                      action=\'store_true\', help=""whether to continue learning"")\n  parser.add_argument(\'--no_dir_check\', dest=\'no_dir_check\',\n                      action=\'store_true\',\n                      help=""whether to check that everything is correct ""\n                           ""with log directory"")\n  parser.add_argument(\'--benchmark\', dest=\'benchmark\', action=\'store_true\',\n                      help=\'automatic config change for benchmarking\')\n  parser.add_argument(\'--bench_steps\', type=int, default=\'20\',\n                      help=\'max_steps for benchmarking\')\n  parser.add_argument(\'--bench_start\', type=int,\n                      help=\'first step to start counting time for benchmarking\')\n  parser.add_argument(\'--debug_port\', type=int,\n                      help=\'run TensorFlow in debug mode on specified port\')\n  parser.add_argument(\'--enable_logs\', dest=\'enable_logs\', action=\'store_true\',\n                      help=\'whether to log output, git info, cmd args, etc.\')\n  parser.add_argument(\'--use_xla_jit\', dest=\'use_xla_jit\', action=\'store_true\',\n                      help=\'whether to use XLA_JIT to compile and run the model.\')\n  args, unknown = parser.parse_known_args(args)\n\n  if args.mode not in [\n      \'train\',\n      \'eval\',\n      \'train_eval\',\n      \'infer\',\n      \'interactive_infer\'\n  ]:\n    raise ValueError(""Mode has to be one of ""\n                     ""[\'train\', \'eval\', \'train_eval\', \'infer\', ""\n                     ""\'interactive_infer\']"")\n  config_module = runpy.run_path(args.config_file, init_globals={\'tf\': tf})\n\n  base_config = config_module.get(\'base_params\', None)\n  if base_config is None:\n    raise ValueError(\'base_config dictionary has to be \'\n                     \'defined in the config file\')\n  base_config[\'use_xla_jit\'] = args.use_xla_jit or base_config.get(\'use_xla_jit\', False)\n\n  base_model = config_module.get(\'base_model\', None)\n  if base_model is None:\n    raise ValueError(\'base_config class has to be defined in the config file\')\n\n  # after we read the config, trying to overwrite some of the properties\n  # with command line arguments that were passed to the script\n  parser_unk = argparse.ArgumentParser()\n  for pm, value in flatten_dict(base_config).items():\n    if type(value) == int or type(value) == float or \\\n       isinstance(value, string_types):\n      parser_unk.add_argument(\'--\' + pm, default=value, type=type(value))\n    elif type(value) == bool:\n      parser_unk.add_argument(\'--\' + pm, default=value, type=ast.literal_eval)\n  config_update = parser_unk.parse_args(unknown)\n  nested_update(base_config, nest_dict(vars(config_update)))\n\n  return args, base_config, base_model, config_module\n\ndef get_calibration_config(arguments):\n  """"""This function parses the command line arguments, reads the config file, and\n  gets the base_model from the config.\n\n  Args:\n    args (str): The command line arguments\n\n  Returns\n    args (dict): The arguments parsed into a dictionary\n    base_config (dict): The config read from the file and ammended with the\n      command line arguments\n    base_model (OpenSeq2Seq model): The model specified in the config file\n    config_module (dict): The raw config file processed by runpy\n  """"""\n\n  parser = argparse.ArgumentParser(description=\'Calibration parameters\')\n  parser.add_argument(""--config_file"", required=True,\n                      help=""Path to the configuration file"")\n  parser.add_argument(""--infer_output_file"", default=""calibration/sample.pkl"",\n                      help=""Path to the output of inference"")\n  parser.add_argument(""--calibration_out"", default = ""calibration.txt"",\n                      help=""Path to calibration output"")\n\n  class CustomSpace(object):\n    def __init__(self, **kwargs):\n      for name in kwargs:\n        setattr(self, name, kwargs[name])\n\n    __hash__ = None\n    def __repr__(self):\n      type_name = type(self).__name__\n      arg_strings = []\n      for arg in self._get_args():\n          arg_strings.append(repr(arg))\n      for name, value in self._get_kwargs():\n          arg_strings.append(\'%s=%r\' % (name, value))\n      return \'%s(%s)\' % (type_name, \', \'.join(arg_strings))\n\n    def _get_kwargs(self):\n      return sorted(self.__dict__.items())\n\n    def _get_args(self):\n      return []\n\n    def __eq__(self, other):\n      if not isinstance(other, CustomSpace):\n        return NotImplemented\n      return vars(self) == vars(other)\n\n    def __ne__(self, other):\n      if not isinstance(other, CustomSpace):\n        return NotImplemented\n      return not (self == other)\n\n    def __contains__(self, key):\n      return key in self.__dict__\n\n  custom_dict = {""benchmark"":False,\n                 ""enable_logs"":False,\n                 ""mode"":""infer"",\n                 ""continue_learning"":False,\n                 }\n  args, unknown = parser.parse_known_args(arguments,namespace=CustomSpace(**custom_dict))\n  config_module = runpy.run_path(args.config_file, init_globals={\'tf\': tf})\n\n  base_config = config_module.get(\'base_params\', None)\n  if base_config is None:\n    raise ValueError(\'base_config dictionary has to be \'\n                     \'defined in the config file\')\n  base_model = config_module.get(\'base_model\', None)\n  if base_model is None:\n    raise ValueError(\'base_config class has to be defined in the config file\')\n\n  # after we read the config, trying to overwrite some of the properties\n  # with command line arguments that were passed to the script\n  parser_unk = argparse.ArgumentParser()\n  for pm, value in flatten_dict(base_config).items():\n    if type(value) == int or type(value) == float or \\\n       isinstance(value, string_types):\n      parser_unk.add_argument(\'--\' + pm, default=value, type=type(value))\n    elif type(value) == bool:\n      parser_unk.add_argument(\'--\' + pm, default=value, type=ast.literal_eval)\n  config_update = parser_unk.parse_args(unknown)\n  nested_update(base_config, nest_dict(vars(config_update)))\n\n  return args, base_config, base_model, config_module\ndef check_logdir(args, base_config, restore_best_checkpoint=False):\n  """"""A helper function that ensures the logdir is setup correctly\n\n  Args:\n    args (dict): Dictionary as returned from get_base_config()\n    base_config (dict): Dictionary as returned from get_base_config()\n    restore_best_checkpoint (bool): If True, will look for ckpt_dir + /best_models\n  Returns:\n    checkpoint: Either None if continue-learning is not set and training, or\n      the name of the checkpoint used to restore the model\n  """"""\n  # checking that everything is correct with log directory\n  logdir = base_config[\'logdir\']\n  if args.benchmark:\n    args.no_dir_check = True\n  try:\n    if args.enable_logs:\n      ckpt_dir = os.path.join(logdir, \'logs\')\n    else:\n      ckpt_dir = logdir\n\n    if args.mode == \'train\' or args.mode == \'train_eval\':\n      if os.path.isfile(logdir):\n        raise IOError(""There is a file with the same name as \\""logdir\\"" ""\n                      ""parameter. You should change the log directory path ""\n                      ""or delete the file to continue."")\n\n      # check if ""logdir"" directory exists and non-empty\n      if os.path.isdir(logdir) and os.listdir(logdir) != []:\n        if not args.continue_learning:\n          raise IOError(""Log directory is not empty. If you want to continue ""\n                        ""learning, you should provide ""\n                        ""\\""--continue_learning\\"" flag"")\n        checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n        if checkpoint is None:\n          raise IOError(\n              ""There is no valid TensorFlow checkpoint in the ""\n              ""{} directory. Can\'t load model"".format(ckpt_dir)\n          )\n      else:\n        if args.continue_learning:\n          raise IOError(""The log directory is empty or does not exist. ""\n                        ""You should probably not provide ""\n                        ""\\""--continue_learning\\"" flag?"")\n        checkpoint = None\n    elif (args.mode == \'infer\' or args.mode == \'eval\' or\n        args.mode == \'interactive_infer\'):\n      if os.path.isdir(logdir) and os.listdir(logdir) != []:\n      # if os.path.isdir(logdir) and \'checkpoint\' in os.listdir(logdir):\n        best_ckpt_dir = os.path.join(ckpt_dir, \'best_models\')\n        if restore_best_checkpoint and os.path.isdir(best_ckpt_dir):\n          deco_print(""Restoring from the best checkpoint"")\n          checkpoint = tf.train.latest_checkpoint(best_ckpt_dir)\n          ckpt_dir = best_ckpt_dir\n        else:\n          deco_print(""Restoring from the latest checkpoint"")\n          checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n\n        if checkpoint is None:\n          raise IOError(\n              ""There is no valid TensorFlow checkpoint in the ""\n              ""{} directory. Can\'t load model"".format(ckpt_dir)\n          )\n      else:\n        raise IOError(\n            ""{} does not exist or is empty, can\'t restore model"".format(\n                ckpt_dir\n            )\n        )\n  except IOError as e:\n    if args.no_dir_check:\n      print(""Warning: {}"".format(e))\n      print(""Resuming operation since no_dir_check argument was provided"")\n    else:\n      raise\n\n  return checkpoint\n\ndef check_base_model_logdir(base_logdir, args, restore_best_checkpoint=False):\n  """"""A helper function that ensures the logdir is setup correctly\n\n  Args:\n    args (dict): Dictionary as returned from get_base_config()\n    base_config (dict): Dictionary as returned from get_base_config()\n    restore_best_checkpoint (bool): If True, will look for ckpt_dir + /best_models\n  Returns:\n    checkpoint: Either None if continue-learning is not set and training, or\n      the name of the checkpoint used to restore the model\n  """"""\n  # checking that everything is correct with log directory\n  if not base_logdir:\n    return \'\'\n\n  if (not os.path.isdir(base_logdir)) or len(os.listdir(base_logdir)) == 0:\n    raise IOError(""The log directory for the base model is empty or does not exist."")\n\n  if args.enable_logs:\n    ckpt_dir = os.path.join(base_logdir, \'logs\')\n    if not os.path.isdir(ckpt_dir):\n      raise IOError(""There\'s no folder \'logs\' in the base model logdir. \\\n                    If checkpoints exist, put them in the \'logs\' folder."")\n  else:\n    ckpt_dir = base_logdir\n\n  if restore_best_checkpoint and os.path.isdir(os.path.join(ckpt_dir, \'best_models\')):\n    ckpt_dir = os.path.join(ckpt_dir, \'best_models\')\n\n  checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n  if checkpoint is None:\n    raise IOError(\n        ""There is no valid TensorFlow checkpoint in the \\\n        {} directory. Can\'t load model"".format(ckpt_dir))\n\n  return ckpt_dir\n\ndef create_logdir(args, base_config):\n  """"""A helper function that ensures the logdir and log files are setup corretly.\n  Only called in --enable_logs is set.\n\n   Args:\n    args (dict): Dictionary as returned from get_base_config()\n    base_config (dict): Dictionary as returned from get_base_config()\n\n  Returns:\n    Some objects that need to be cleaned up in run.py\n  """"""\n  logdir = base_config[\'logdir\']\n  if not os.path.exists(logdir):\n    os.makedirs(logdir)\n\n  tm_suf = datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n  shutil.copy(\n      args.config_file,\n      os.path.join(logdir, \'config_{}.py\'.format(tm_suf)),\n  )\n\n  with open(os.path.join(logdir, \'cmd-args_{}.log\'.format(tm_suf)),\n            \'w\') as f:\n    f.write("" "".join(sys.argv))\n\n  with open(os.path.join(logdir, \'git-info_{}.log\'.format(tm_suf)),\n            \'w\') as f:\n    f.write(\'commit hash: {}\'.format(get_git_hash()))\n    f.write(get_git_diff())\n\n  old_stdout = sys.stdout\n  old_stderr = sys.stderr\n  stdout_log = open(\n      os.path.join(logdir, \'stdout_{}.log\'.format(tm_suf)), \'a\', 1\n  )\n  stderr_log = open(\n      os.path.join(logdir, \'stderr_{}.log\'.format(tm_suf)), \'a\', 1\n  )\n  sys.stdout = Logger(sys.stdout, stdout_log)\n  sys.stderr = Logger(sys.stderr, stderr_log)\n\n  return old_stdout, old_stderr, stdout_log, stderr_log\n\ndef create_model(args, base_config, config_module, base_model, hvd,\n                 checkpoint=None):\n  """"""A helpful function that creates the train, eval, and infer models as\n  needed.\n\n  Args:\n    args (dict): Dictionary as returned from get_base_config()\n    base_config (dict): Dictionary as returned from get_base_config()\n    config_module: config_module as returned from get_base_config()\n    base_model (OpenSeq2Seq model): Dictionary as returned from\n      get_base_config()\n    hvd: Either None if Horovod is not enabled, or the Horovod library\n    checkpoint (str): checkpoint path as returned from\n      tf.train.latest_checkpoint\n\n  Returns:\n    model: A compiled model. For the \'train_eval\' mode, a tuple containing the\n      (train_model, eval_model) is returned.\n  """"""\n  train_config = copy.deepcopy(base_config)\n  eval_config = copy.deepcopy(base_config)\n  infer_config = copy.deepcopy(base_config)\n\n  if args.mode == \'train\' or args.mode == \'train_eval\':\n    if \'train_params\' in config_module:\n      nested_update(train_config, copy.deepcopy(config_module[\'train_params\']))\n    if hvd is None or hvd.rank() == 0:\n      deco_print(""Training config:"")\n      pprint.pprint(train_config)\n  if args.mode == \'eval\' or args.mode == \'train_eval\':\n    if \'eval_params\' in config_module:\n      nested_update(eval_config, copy.deepcopy(config_module[\'eval_params\']))\n    if hvd is None or hvd.rank() == 0:\n      deco_print(""Evaluation config:"")\n      pprint.pprint(eval_config)\n  if args.mode == ""infer"":\n    if args.infer_output_file is None:\n      raise ValueError(""\\""infer_output_file\\"" command line parameter is ""\n                       ""required in inference mode"")\n    if ""infer_params"" in config_module:\n      nested_update(infer_config, copy.deepcopy(config_module[\'infer_params\']))\n    if hvd is None or hvd.rank() == 0:\n      deco_print(""Inference config:"")\n      pprint.pprint(infer_config)\n  if args.mode == ""interactive_infer"":\n    if ""interactive_infer_params"" in config_module:\n      nested_update(\n          infer_config,\n          copy.deepcopy(config_module[\'interactive_infer_params\'])\n      )\n    if hvd is None or hvd.rank() == 0:\n      deco_print(""Inference config:"")\n      pprint.pprint(infer_config)\n\n\n  if args.benchmark:\n    deco_print(""Adjusting config for benchmarking"")\n    train_config[\'print_samples_steps\'] = None\n    train_config[\'print_loss_steps\'] = 1\n    train_config[\'save_summaries_steps\'] = None\n    train_config[\'save_checkpoint_steps\'] = None\n    train_config[\'logdir\'] = str("""")\n    if \'num_epochs\' in train_config:\n      del train_config[\'num_epochs\']\n    train_config[\'max_steps\'] = args.bench_steps\n    if args.bench_start:\n      train_config[\'bench_start\'] = args.bench_start\n    elif \'bench_start\' not in train_config:\n      train_config[\'bench_start\'] = 10  # default value\n\n    if hvd is None or hvd.rank() == 0:\n      deco_print(""New benchmarking config:"")\n      pprint.pprint(train_config)\n    args.mode = ""train""\n\n  if args.mode == \'train_eval\':\n    train_model = base_model(params=train_config, mode=""train"", hvd=hvd)\n    train_model.compile()\n    eval_model = base_model(params=eval_config, mode=""eval"", hvd=hvd)\n    eval_model.compile(force_var_reuse=True)\n    model = (train_model, eval_model)\n  elif args.mode == \'train\':\n    model = base_model(params=train_config, mode=""train"", hvd=hvd)\n    model.compile()\n  elif args.mode == \'eval\':\n    model = base_model(params=eval_config, mode=""eval"", hvd=hvd)\n    model.compile(force_var_reuse=False)\n  else:\n    model = base_model(params=infer_config, mode=args.mode, hvd=hvd)\n    model.compile(checkpoint=checkpoint)\n\n  return model\n'"
open_seq2seq/utils/utils_test.py,4,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport tempfile\n\nimport numpy as np\nimport numpy.testing as npt\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom open_seq2seq.test_utils.test_speech_configs.ds2_test_config import \\\n    base_params, train_params, eval_params, base_model\nfrom open_seq2seq.utils.utils import get_results_for_epoch, get_available_gpus\n\n\nclass UtilsTests(tf.test.TestCase):\n\n  def setUp(self):\n    base_params[\'logdir\'] = tempfile.mktemp()\n    self.train_config = copy.deepcopy(base_params)\n    self.eval_config = copy.deepcopy(base_params)\n    self.train_config.update(copy.deepcopy(train_params))\n    self.eval_config.update(copy.deepcopy(eval_params))\n\n  def tearDown(self):\n    pass\n\n  def test_get_results_for_epoch(self):\n    # this will take all gpu memory, but that\'s probably fine for tests\n    gpus = get_available_gpus()\n    length_list = []\n    for num_gpus in [1, 2, 3]:\n      if num_gpus > len(gpus):\n        continue\n      for bs in [1, 2, 3, 5, 7]:\n        if bs * num_gpus > 10:\n          continue\n        with tf.Graph().as_default() as g:\n          self.eval_config[\'batch_size_per_gpu\'] = bs\n          self.eval_config[\'num_gpus\'] = num_gpus\n          model = base_model(params=self.eval_config, mode=""infer"", hvd=None)\n          model.compile()\n          model.infer = lambda inputs, outputs: inputs\n          model.finalize_inference = lambda results: results\n\n          with self.test_session(g, use_gpu=True) as sess:\n            sess.run(tf.global_variables_initializer())\n            inputs_per_batch = get_results_for_epoch(\n                model, sess, False, ""infer"")\n            length = np.hstack([inp[\'source_tensors\'][1]\n                                for inp in inputs_per_batch])\n            ids = np.hstack([inp[\'source_ids\'] for inp in inputs_per_batch])\n            length_list.append(length[np.argsort(ids)])\n\n    for i in range(len(length_list) - 1):\n      npt.assert_allclose(length_list[i], length_list[i + 1])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
docs/sources/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# OpenSeq2Seq documentation build configuration file, created by\n# sphinx-quickstart on Thu Apr 12 14:49:40 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'../../../\'))\nsys.path.insert(0, os.path.abspath(\'../../../open_seq2seq\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n  \'sphinx.ext.autodoc\',\n  \'sphinx.ext.viewcode\',\n  \'sphinx.ext.napoleon\',\n  \'sphinx.ext.mathjax\',\n  \'sphinxcontrib.bibtex\',\n]\n\n# Napoleon settings\nnapoleon_include_init_with_doc = True\nnapoleon_include_private_with_doc = True\nnapoleon_numpy_docstring = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'OpenSeq2Seq\'\ncopyright = \'2018, NVIDIA\'\nauthor = \'NVIDIA\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.2\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.2\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {\n  \'display_version\': False,\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \'logo.png\'\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\nhtml_favicon = \'favicon.ico\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\ndef setup(app):\n  app.add_stylesheet(""theme_override.css"")\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'h\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'r\', \'sv\', \'tr\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'OpenSeq2Seqdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'OpenSeq2Seq.tex\', \'OpenSeq2Seq Documentation\',\n     \'Oleksii Kuchaiev, Boris Ginsburg, Vitaliy Lavrukhin, Igor Gitman\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n  (master_doc, \'openseq2seq\', \'OpenSeq2Seq Documentation\',\n   [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (master_doc, \'OpenSeq2Seq\', \'OpenSeq2Seq Documentation\',\n   author, \'OpenSeq2Seq\', \'One line description of project.\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n'"
example_configs/text2text/en-de/en-de-convs2s-8-gpu.py,1,"b'from __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\n\nfrom open_seq2seq.encoders import ConvS2SEncoder\nfrom open_seq2seq.decoders import ConvS2SDecoder\n\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.optimizers.lr_policies import transformer_policy\nfrom open_seq2seq.parts.convs2s.utils import gated_linear_units\n\nimport math\n""""""\nThis configuration file describes a variant of ConvS2S model from\nhttps://arxiv.org/pdf/1705.03122\n""""""\n\n# REPLACE THIS TO THE PATH WITH YOUR WMT DATA\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n\nbase_model = Text2Text\nnum_layers = 15\nd_model = 512\nhidden_before_last = 512\n\nconv_act = gated_linear_units\nnormalization_type = ""weight_norm""\nscaling_factor = math.sqrt(0.5)\n\nmax_length = 64\n\nbase_params = {\n  ""use_horovod"": True,\n  ""num_gpus"": 1, # Use 8 horovod workers to train on 8 GPUs\n\n  # max_step is set for 35 epochs on 8 gpus with batch size of 64,\n  # 4.5M is the size of the dataset\n  ""max_steps"": 310000,\n  ""batch_size_per_gpu"": 64,\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 100,\n  ""eval_steps"": 4000,\n  ""save_checkpoint_steps"": 4000,\n  ""logdir"": ""ConvSeq2Seq-8GPUs-FP32"",\n\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": transformer_policy,\n  ""lr_policy_params"": {\n    ""learning_rate"": 9,\n    ""max_lr"": 1e-3,\n    ""warmup_steps"": 4000,\n    ""d_model"": d_model,\n  },\n\n  ""max_grad_norm"": 0.1,\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""dtype"": tf.float32, # to enable mixed precision, comment this line and uncomment two below lines\n  #""dtype"": ""mixed"",\n  #""loss_scaling"": ""Backoff"",\n\n  ""encoder"": ConvS2SEncoder,\n  ""encoder_params"": {\n\n    ""src_emb_size"": d_model,\n    ""pad_embeddings_2_eight"": True,\n    ""att_layer_num"": num_layers,\n\n    # original ConvS2S paper\n    #""conv_nchannels_kwidth"": [(512, 3)]*10 + [(768, 3)]*3 + [(2048, 1)]*2,\n\n    # fairseq config\n    ""conv_nchannels_kwidth"": [(512, 3)]*9 + [(1024, 3)]*4 + [(2048, 1)]*2,\n\n    ""embedding_dropout_keep_prob"": 0.8,\n    ""hidden_dropout_keep_prob"": 0.8,\n\n    ""max_input_length"": max_length,\n\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n\n    ""conv_activation"": conv_act,\n    \'normalization_type\': normalization_type,\n    ""scaling_factor"": scaling_factor,\n  },\n\n\n  ""decoder"": ConvS2SDecoder,\n  ""decoder_params"": {\n\n    ""shared_embed"": True,\n    ""tgt_emb_size"": d_model,\n    ""pad_embeddings_2_eight"": True,\n    ""out_emb_size"": hidden_before_last,\n    ""pos_embed"": False,\n\n    # original ConvS2S paper\n    #""conv_nchannels_kwidth"": [(512, 3)]*10 + [(768, 3)]*3 + [(2048, 1)]*2,\n\n    # fairseq config\n    ""conv_nchannels_kwidth"": [(512, 3)]*9 + [(1024, 3)]*4 + [(2048, 1)]*2,\n\n    ""embedding_dropout_keep_prob"": 0.8,\n    ""hidden_dropout_keep_prob"": 0.8,\n    ""out_dropout_keep_prob"": 0.8,\n\n    ""max_input_length"": max_length,\n    ""extra_decode_length"": 56,\n    ""beam_size"": 5,\n    ""alpha"": 0.6,\n\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n\n    ""conv_activation"": conv_act,\n    \'normalization_type\': normalization_type,\n    ""scaling_factor"": scaling_factor,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": True,\n    ""average_across_timestep"": True,\n    ""do_mask"": True\n  }\n\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    ""source_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""target_file"": data_root + ""train.clean.de.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 2,\n    ""max_length"": max_length,\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 64,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt13-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt13-en-de.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""max_length"": max_length,\n    ""prefetch_buffer_size"": 1,\n    },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": max_length*2,\n    ""prefetch_buffer_size"": 1,\n  },\n\n}'"
example_configs/text2text/en-de/en-de-gnmt-like-4GPUs.py,6,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import GNMTLikeEncoderWithEmbedding\nfrom open_seq2seq.decoders import RNNDecoderWithAttention, \\\n  BeamSearchRNNDecoderWithAttention\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.optimizers.lr_policies import exp_decay\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n\nbase_model = Text2Text\n\nbase_params = {\n  ""use_horovod"": False,\n  ""num_gpus"": 4,\n  ""max_steps"": 340000,\n  ""batch_size_per_gpu"": 32,\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 48,\n  ""print_samples_steps"": 48,\n  ""eval_steps"": 4001,\n  ""save_checkpoint_steps"": 4000,\n  ""logdir"": ""GNMT-4GPUs-FP32"",\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  # luong10 decay scheme\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.0008,\n    ""begin_decay_at"": 170000,\n    ""decay_steps"": 17000,\n    ""decay_rate"": 0.5,\n    ""use_staircase_decay"": True,\n    ""min_lr"": 0.0000005,\n  },\n  #""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n  #              \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""max_grad_norm"": 32768.0,\n  ""dtype"": tf.float32, # to enable mixed precision, comment this line and uncomment two below lines\n  #""dtype"": ""mixed"",\n  #""loss_scaling"": ""Backoff"",\n  ""encoder"": GNMTLikeEncoderWithEmbedding,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 1024,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 7,\n    ""encoder_dp_input_keep_prob"": 0.8,\n    ""encoder_dp_output_keep_prob"": 1.0,\n    ""encoder_use_skip_connections"": True,\n    ""src_emb_size"": 1024,\n  },\n\n  ""decoder"": RNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n       ""minval"": -0.1,\n       ""maxval"": 0.1,\n     },\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 1024,\n        ""forget_bias"": 1.0,\n    },\n    ""decoder_layers"": 8,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": True,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n\n    ""tgt_emb_size"": 1024,\n    ""attention_type"": ""gnmt_v2"",\n    ""attention_layer_size"": 1024,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": True,\n    ""average_across_timestep"": True,\n    ""do_mask"": True\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    ""source_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""target_file"": data_root + ""train.clean.de.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""max_length"": 50,\n  },\n}\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt13-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt13-en-de.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""max_length"": 32,\n  },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""decoder"": BeamSearchRNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""beam_width"": 10,\n    ""length_penalty"": 1.0,\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 1024,\n        ""forget_bias"": 1.0,\n    },\n    ""decoder_layers"": 8,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": True,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n    ""tgt_emb_size"": 1024,\n    ""attention_type"": ""gnmt_v2"",\n    ""attention_layer_size"": 1024,\n  },\n\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 512,\n  },\n}\n'"
example_configs/text2text/en-de/en-de-gnmt-like-weight-tied-2GPUs.py,6,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import GNMTLikeEncoderWithEmbedding\nfrom open_seq2seq.decoders import RNNDecoderWithAttention, \\\n  BeamSearchRNNDecoderWithAttention\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.optimizers.lr_policies import exp_decay\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n\nbase_model = Text2Text\n\nbase_params = {\n  ""use_horovod"": False,\n  ""num_gpus"": 2,\n  ""max_steps"": 340000,\n  ""batch_size_per_gpu"": 32,\n  ""save_summaries_steps"": 10,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 10,\n  ""eval_steps"": 40,\n  ""save_checkpoint_steps"": 40,\n  ""logdir"": ""GNMT-2GPUs-WT"",\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  # luong10 decay scheme\n  ""lr_policy"": exp_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.0008,\n    ""begin_decay_at"": 170000,\n    ""decay_steps"": 17000,\n    ""decay_rate"": 0.5,\n    ""use_staircase_decay"": True,\n    ""min_lr"": 0.0000005,\n  },\n  #""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n  #              \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  ""max_grad_norm"": 32768.0,\n  ""dtype"": tf.float32,\n  #""dtype"": ""mixed"",\n  #""automatic_loss_scaling"": ""Backoff"",\n  ""encoder"": GNMTLikeEncoderWithEmbedding,\n  ""encoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n      ""minval"": -0.1,\n      ""maxval"": 0.1,\n    },\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 200,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 7,\n    ""encoder_dp_input_keep_prob"": 0.8,\n    ""encoder_dp_output_keep_prob"": 1.0,\n    ""encoder_use_skip_connections"": True,\n    ""src_emb_size"": 80,\n  },\n\n  ""decoder"": RNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""initializer"": tf.random_uniform_initializer,\n    ""initializer_params"": {\n       ""minval"": -0.1,\n       ""maxval"": 0.1,\n     },\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 200,\n        ""forget_bias"": 1.0,\n    },\n    ""decoder_layers"": 8,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": True,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n\n    ""tgt_emb_size"": 80,\n    ""attention_type"": ""gnmt_v2"",\n    ""attention_layer_size"": 1024,\n    ""weight_tied"": True,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": True,\n    ""average_across_timestep"": True,\n    ""do_mask"": True\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root+""vocab.bpe.32000"",\n    ""tgt_vocab_file"": data_root+""vocab.bpe.32000"",\n    ""source_file"": data_root+""train.tok.clean.bpe.32000.en"",\n    ""target_file"": data_root+""train.tok.clean.bpe.32000.de"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 8,\n    ""max_length"": 50,\n  },\n}\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root+""vocab.bpe.32000"",\n    ""tgt_vocab_file"": data_root+""vocab.bpe.32000"",\n    ""source_file"": data_root+""newstest2013.tok.bpe.32000.en"",\n    ""target_file"": data_root+""newstest2013.tok.bpe.32000.de"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""max_length"": 32,\n  },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""decoder"": BeamSearchRNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""beam_width"": 10,\n    ""length_penalty"": 1.0,\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 200,\n        ""forget_bias"": 1.0,\n    },\n    ""decoder_layers"": 8,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": True,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n    ""tgt_emb_size"": 80,\n    ""attention_type"": ""gnmt_v2"",\n    ""attention_layer_size"": 1024,\n    ""weight_tied"": True, # make sure that weight tied for Training/eval is true too\n  },\n\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root+""vocab.bpe.32000"",\n    ""tgt_vocab_file"": data_root+""vocab.bpe.32000"",\n    ""source_file"": data_root+""newstest2014.tok.bpe.32000.en"",\n    # this is intentional\n    ""target_file"": data_root+""newstest2014.tok.bpe.32000.en"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 64,\n  },\n}\n'"
example_configs/text2text/en-de/en-de-nmt-small.py,6,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import BidirectionalRNNEncoderWithEmbedding\nfrom open_seq2seq.decoders import RNNDecoderWithAttention, \\\n  BeamSearchRNNDecoderWithAttention\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n\n# This model should run fine on single GPU such as 1080ti or better\nbase_model = Text2Text\n\nbase_params = {\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n  ""max_steps"": 160082,\n  ""batch_size_per_gpu"": 128,\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 48,\n  ""print_samples_steps"": 48,\n  ""eval_steps"": 1000,\n  ""save_checkpoint_steps"": 2001,\n  ""logdir"": ""nmt-small-en-de"",\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {},\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.001,\n  },\n  ""larc_params"": {\n    ""larc_eta"": 0.001,\n  },\n  ""dtype"": tf.float32,\n  #""dtype"": ""mixed"",\n  #""loss_scaling"": ""Backoff"",\n\n  ""encoder"": BidirectionalRNNEncoderWithEmbedding,\n  ""encoder_params"": {\n    ""initializer"": tf.glorot_uniform_initializer,\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 512,\n        ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 2,\n    ""encoder_dp_input_keep_prob"": 0.8,\n    ""encoder_dp_output_keep_prob"": 1.0,\n    ""encoder_use_skip_connections"": False,\n    ""src_emb_size"": 512,\n    ""use_swap_memory"": True,\n  },\n\n  ""decoder"": RNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""initializer"": tf.glorot_uniform_initializer,\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 512,\n        ""forget_bias"": 1.0,\n    },\n    ""decoder_layers"": 2,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": False,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""tgt_emb_size"": 512,\n    ""attention_type"": ""gnmt_v2"",\n    ""attention_layer_size"": 512,\n    ""use_swap_memory"": True,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": True,\n    ""average_across_timestep"": False,\n    ""do_mask"": True\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    ""source_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""target_file"": data_root + ""train.clean.de.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 2,\n    ""max_length"": 50,\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 32,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt13-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt13-en-de.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""prefetch_buffer_size"": 1,\n    ""max_length"": 16,\n  },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""decoder"": BeamSearchRNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""beam_width"": 10,\n    ""length_penalty"": 1.0,\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n        ""num_units"": 512,\n        ""forget_bias"": 1.0,\n    },\n    ""decoder_layers"": 2,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": False,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n    ""tgt_emb_size"": 512,\n    ""attention_type"": ""gnmt_v2"",\n    ""attention_layer_size"": 512,\n  },\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    # this is intentional\n    ""target_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n    ""prefetch_buffer_size"": 1,\n  },\n}\n'"
example_configs/text2text/en-de/transformer-base.py,2,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import TransformerEncoder\nfrom open_seq2seq.decoders import TransformerDecoder\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import PaddedCrossEntropyLossWithSmoothing\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\nfrom open_seq2seq.optimizers.lr_policies import transformer_policy\nimport tensorflow as tf\n\n""""""\nThis configuration file describes a variant of Transformer model from\nhttps://arxiv.org/abs/1706.03762\n""""""\n\nbase_model = Text2Text\nd_model = 512\nnum_layers = 6\n\n# REPLACE THIS TO THE PATH WITH YOUR WMT DATA\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n#data_root = ""/raid/wmt16/""\n\nbase_params = {\n  ""use_horovod"": True,\n  ""num_gpus"": 1, # when using Horovod we set number of workers with params to mpirun\n  ""batch_size_per_gpu"": 256,  # this size is in sentence pairs, reduce it if you get OOM\n  ""max_steps"": 300000,\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 100,\n  ""eval_steps"": 4001,\n  ""save_checkpoint_steps"": 299998,\n  ""logdir"": ""logs/transformer/"",\n  ""dtype"": tf.float32, # to enable mixed precision, comment this line and uncomment two below lines\n  #""dtype"": ""mixed"",\n  #""loss_scaling"": ""Backoff"",\n\n  ""optimizer"": tf.contrib.opt.LazyAdamOptimizer,\n  ""optimizer_params"": {\n    ""beta1"": 0.9,\n    ""beta2"": 0.997,\n    ""epsilon"": 1e-09,\n  },\n\n  ""lr_policy"": transformer_policy,\n  ""lr_policy_params"": {\n    ""learning_rate"": 2.0,\n    ""warmup_steps"": 8000,\n    ""d_model"": d_model,\n  },\n\n  ""encoder"": TransformerEncoder,\n  ""encoder_params"": {\n    ""encoder_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 8,\n    ""attention_dropout"": 0.1,\n    ""filter_size"": 4 * d_model,\n    ""relu_dropout"": 0.1,\n    ""layer_postprocess_dropout"": 0.1,\n    ""pad_embeddings_2_eight"": True,\n    ""remove_padding"": True,\n  },\n\n  ""decoder"": TransformerDecoder,\n  ""decoder_params"": {\n    ""layer_postprocess_dropout"": 0.1,\n    ""num_hidden_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 8,\n    ""attention_dropout"": 0.1,\n    ""relu_dropout"": 0.1,\n    ""filter_size"": 4 * d_model,\n    ""beam_size"": 4,\n    ""alpha"": 0.6,\n    ""extra_decode_length"": 50,\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""loss"": PaddedCrossEntropyLossWithSmoothing,\n  ""loss_params"": {\n    ""label_smoothing"": 0.1,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    ""source_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""target_file"": data_root + ""train.clean.de.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""max_length"": 56,\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt13-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt13-en-de.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n    },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n  },\n}\n'"
example_configs/text2text/en-de/transformer-big.py,2,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import TransformerEncoder\nfrom open_seq2seq.decoders import TransformerDecoder\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import PaddedCrossEntropyLossWithSmoothing\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\nfrom open_seq2seq.optimizers.lr_policies import transformer_policy\nimport tensorflow as tf\n\n""""""\nThis configuration file describes a variant of Transformer model from\nhttps://arxiv.org/abs/1706.03762\n""""""\n\nbase_model = Text2Text\nd_model = 1024\nnum_layers = 6\n\n# REPLACE THIS TO THE PATH WITH YOUR WMT DATA\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n\nbase_params = {\n  ""use_horovod"": True,\n  ""num_gpus"": 1, # when using Horovod we set number of workers with params to mpirun\n  ""batch_size_per_gpu"": 256,  # this size is in sentence pairs, reduce it if you get OOM\n  ""max_steps"": 300000,\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 100,\n  ""eval_steps"": 4000,\n  ""save_checkpoint_steps"": 299998,\n  ""logdir"": ""Transformer-BIG"",\n  #""dtype"": tf.float32, # to enable mixed precision, comment this line and uncomment two below lines\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n\n  ""optimizer"": tf.contrib.opt.LazyAdamOptimizer,\n  ""optimizer_params"": {\n    ""beta1"": 0.9,\n    ""beta2"": 0.997,\n    ""epsilon"": 1e-09,\n  },\n\n  ""lr_policy"": transformer_policy,\n  ""lr_policy_params"": {\n    ""learning_rate"": 2.0,\n    ""warmup_steps"": 8000,\n    ""d_model"": d_model,\n  },\n\n  ""encoder"": TransformerEncoder,\n  ""encoder_params"": {\n    ""encoder_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""attention_dropout"": 0.1,\n    ""filter_size"": 4 * d_model,\n    ""relu_dropout"": 0.3,\n    ""layer_postprocess_dropout"": 0.3,\n    ""pad_embeddings_2_eight"": True,\n    ""remove_padding"": True,\n  },\n\n  ""decoder"": TransformerDecoder,\n  ""decoder_params"": {\n    ""layer_postprocess_dropout"": 0.3,\n    ""num_hidden_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""attention_dropout"": 0.1,\n    ""relu_dropout"": 0.3,\n    ""filter_size"": 4 * d_model,\n    ""beam_size"": 4,\n    ""alpha"": 0.6,\n    ""extra_decode_length"": 50,\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""loss"": PaddedCrossEntropyLossWithSmoothing,\n  ""loss_params"": {\n    ""label_smoothing"": 0.1,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    ""source_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""target_file"": data_root + ""train.clean.de.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""max_length"": 56,\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt13-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt13-en-de.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""max_length"": 256,\n    },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n  },\n}\n'"
example_configs/text2text/en-de/transformer-bn.py,3,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import TransformerEncoder\nfrom open_seq2seq.decoders import TransformerDecoder\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import PaddedCrossEntropyLossWithSmoothing\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\nfrom open_seq2seq.optimizers.lr_policies import transformer_policy, poly_decay\nimport tensorflow as tf\n\n""""""\nThis configuration file describes a variant of Transformer model from\nhttps://arxiv.org/abs/1706.03762\n""""""\n\nbase_model = Text2Text\nd_model = 1024\nnum_layers = 6\n\nregularizer=tf.contrib.layers.l2_regularizer # None\nregularizer_params = {\'scale\': 0.001}\n\nnorm_params= {\n  ""type"": ""batch_norm"", # ""layernorm_L1"" , ""layernorm_L2"" #\n  ""momentum"":0.95,\n  ""epsilon"": 0.00001,\n  ""center_scale"": False, #True,\n  ""regularizer"":regularizer,\n  ""regularizer_params"": regularizer_params\n}\n\nattention_dropout = 0.02\ndropout = 0.3\n\n# REPLACE THIS TO THE PATH WITH YOUR WMT DATA\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n\nbase_params = {\n  ""use_horovod"": False, #True,\n  ""num_gpus"": 2, #8, # when using Horovod we set number of workers with params to mpirun\n  ""batch_size_per_gpu"": 128,  # this size is in sentence pairs, reduce it if you get OOM\n  ""max_steps"":  1000000,\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 10000,\n  ""eval_steps"": 10000,\n  ""save_checkpoint_steps"": 99999,\n  ""logdir"": ""logs/tr-bn2-reg"",\n  #""dtype"": tf.float32, # to enable mixed precision, comment this line and uncomment two below lines\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n\n  # ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n  #               \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n  #""iter_size"": 1,\n\n  ""optimizer"": tf.contrib.opt.LazyAdamOptimizer,\n  ""optimizer_params"": {\n    ""beta1"": 0.9,\n    ""beta2"": 0.997,\n    ""epsilon"": 1e-09,\n  },\n  ""lr_policy"": transformer_policy,\n  ""lr_policy_params"": {\n    ""learning_rate"": 2.0,\n    ""warmup_steps"": 8000,\n    ""d_model"": d_model,\n  },\n\n  # ""optimizer"": ""Momentum"",\n  # ""optimizer_params"": {\n  #   ""momentum"": 0.95,\n  # },\n  # ""lr_policy"": poly_decay,  # fixed_lr,\n  # ""lr_policy_params"": {\n  #   ""learning_rate"": 0.1, #  0,2 for 4 GPU\n  #   ""power"": 2,\n  # },\n\n  ""larc_params"": {\n    ""larc_eta"": 0.001,\n  },\n\n  ""encoder"": TransformerEncoder,\n  ""encoder_params"": {\n    ""encoder_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""filter_size"": 4 * d_model,\n    ""attention_dropout"": attention_dropout,  # 0.1,\n    ""relu_dropout"": dropout,                 # 0.3,\n    ""layer_postprocess_dropout"": dropout,    # 0.3,\n    ""pad_embeddings_2_eight"": True,\n    ""remove_padding"": True,\n    ""norm_params"": norm_params,\n    ""regularizer"": regularizer,\n    ""regularizer_params"": regularizer_params,\n  },\n\n  ""decoder"": TransformerDecoder,\n  ""decoder_params"": {\n    ""num_hidden_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""filter_size"": 4 * d_model,\n    ""attention_dropout"": attention_dropout,  # 0.1,\n    ""relu_dropout"": dropout,                 # 0.3,\n    ""layer_postprocess_dropout"": dropout,    # 0.3,\n    ""beam_size"": 4,\n    ""alpha"": 0.6,\n    ""extra_decode_length"": 50,\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n    ""norm_params"": norm_params,\n    ""regularizer"": regularizer,\n    ""regularizer_params"": regularizer_params,\n  },\n\n  ""loss"": PaddedCrossEntropyLossWithSmoothing,\n  ""loss_params"": {\n    ""label_smoothing"": 0.1,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    # ""source_file"": data_root + ""wmt13-en-de.src.BPE_common.32K.tok"",\n    # ""target_file"": data_root + ""wmt13-en-de.ref.BPE_common.32K.tok"",\n    ""source_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""target_file"": data_root + ""train.clean.de.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 4500000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""max_length"": 64,\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt13-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt13-en-de.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"":  True,\n    ""max_length"": 256,\n    },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"":  False,\n    ""max_length"": 256,\n  },\n}\n'"
example_configs/text2text/en-de/transformer-nvgrad.py,1,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import TransformerEncoder\nfrom open_seq2seq.decoders import TransformerDecoder\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import PaddedCrossEntropyLossWithSmoothing\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\nfrom open_seq2seq.optimizers.lr_policies import transformer_policy, poly_decay\nfrom open_seq2seq.optimizers.novograd  import NovoGrad\nimport tensorflow as tf\n\n\n""""""\nThis configuration file describes a variant of Transformer model from\nhttps://arxiv.org/abs/1706.03762\n""""""\n\nbase_model = Text2Text\nd_model = 1024\nnum_layers = 6\n\nnorm_params= {\n  ""type"":  ""layernorm_L2"",\n  ""momentum"":0.95,\n  ""epsilon"": 0.00001,\n}\n\nattention_dropout = 0.1\ndropout = 0.3\n\n# REPLACE THIS TO THE PATH WITH YOUR WMT DATA\n#data_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\ndata_root = ""/data/wmt16-ende-sp/""\n\nbase_params = {\n  ""use_horovod"": True,\n  ""num_gpus"": 1, #8, # when using Horovod we set number of workers with params to mpirun\n  ""batch_size_per_gpu"": 128,  # this size is in sentence pairs, reduce it if you get OOM\n  ""max_steps"":  600000,\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 10000,\n  ""eval_steps"": 10000,\n  ""save_checkpoint_steps"": 99999,\n  ""logdir"": ""tr-nvgrad2_0.90.99-b128-lr0.1-fp16"",\n  # ""dtype"": tf.float32, # to enable mixed precision, comment this line and uncomment two below lines\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n\n  ""optimizer"": NovoGrad,\n  ""optimizer_params"": {\n    ""beta1"": 0.95,\n    ""beta2"": 0.99,\n    ""epsilon"":  1e-08,\n    ""weight_decay"": 0.00001,\n    ""grad_averaging"": False,\n  },\n  ""lr_policy"": poly_decay,\n  ""lr_policy_params"": {\n    ""learning_rate"": 0.04,\n    ""power"": 2,\n  },\n\n  ""encoder"": TransformerEncoder,\n  ""encoder_params"": {\n    ""encoder_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""filter_size"": 4 * d_model,\n    ""attention_dropout"": attention_dropout,  # 0.1,\n    ""relu_dropout"": dropout,                 # 0.3,\n    ""layer_postprocess_dropout"": dropout,    # 0.3,\n    ""pad_embeddings_2_eight"": True,\n    ""remove_padding"": True,\n    ""norm_params"": norm_params,\n  },\n\n  ""decoder"": TransformerDecoder,\n  ""decoder_params"": {\n    ""num_hidden_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""filter_size"": 4 * d_model,\n    ""attention_dropout"": attention_dropout,  # 0.1,\n    ""relu_dropout"": dropout,                 # 0.3,\n    ""layer_postprocess_dropout"": dropout,    # 0.3,\n    ""beam_size"": 4,\n    ""alpha"": 0.6,\n    ""extra_decode_length"": 50,\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n    ""norm_params"": norm_params,\n  },\n\n  ""loss"": PaddedCrossEntropyLossWithSmoothing,\n  ""loss_params"": {\n    ""label_smoothing"": 0.1,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    ""source_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""target_file"": data_root + ""train.clean.de.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""max_length"": 56,\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt13-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt13-en-de.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""max_length"": 256,\n    },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt14-en-de.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n  },\n}\n'"
example_configs/text2text/en-es/transformer-big.py,2,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import TransformerEncoder\nfrom open_seq2seq.decoders import TransformerDecoder\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import PaddedCrossEntropyLossWithSmoothing\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\nfrom open_seq2seq.optimizers.lr_policies import transformer_policy\nimport tensorflow as tf\n\n""""""\nThis configuration file describes a variant of Transformer model from\nhttps://arxiv.org/abs/1706.03762\n""""""\n\nbase_model = Text2Text\nd_model = 1024\nnum_layers = 6\n\n# REPLACE THIS TO THE PATH WITH YOUR WMT DATA\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n\nbase_params = {\n  ""use_horovod"": True,\n  ""num_gpus"": 1, # when using Horovod we set number of workers with params to mpirun\n  ""batch_size_per_gpu"": 256,  # this size is in sentence pairs, reduce it if you get OOM\n  ""max_steps"": 300000,\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 100,\n  ""eval_steps"": 4001,\n  ""save_checkpoint_steps"": 299998,\n  ""logdir"": ""Transformer-BIG"",\n  #""dtype"": tf.float32, # to enable mixed precision, comment this line and uncomment two below lines\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n\n  ""optimizer"": tf.contrib.opt.LazyAdamOptimizer,\n  ""optimizer_params"": {\n    ""beta1"": 0.9,\n    ""beta2"": 0.997,\n    ""epsilon"": 1e-09,\n  },\n\n  ""lr_policy"": transformer_policy,\n  ""lr_policy_params"": {\n    ""learning_rate"": 2.0,\n    ""warmup_steps"": 8000,\n    ""d_model"": d_model,\n  },\n\n  ""encoder"": TransformerEncoder,\n  ""encoder_params"": {\n    ""encoder_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""attention_dropout"": 0.1,\n    ""filter_size"": 4 * d_model,\n    ""relu_dropout"": 0.3,\n    ""layer_postprocess_dropout"": 0.3,\n    ""pad_embeddings_2_eight"": True,\n    ""remove_padding"": True,\n  },\n\n  ""decoder"": TransformerDecoder,\n  ""decoder_params"": {\n    ""layer_postprocess_dropout"": 0.3,\n    ""num_hidden_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""attention_dropout"": 0.1,\n    ""relu_dropout"": 0.3,\n    ""filter_size"": 4 * d_model,\n    ""beam_size"": 4,\n    ""alpha"": 0.6,\n    ""extra_decode_length"": 50,\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""loss"": PaddedCrossEntropyLossWithSmoothing,\n  ""loss_params"": {\n    ""label_smoothing"": 0.1,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    ""source_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""target_file"": data_root + ""train.clean.es.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""max_length"": 56,\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt13-en-es.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt13-en-es.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""max_length"": 256,\n    },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""source_file"": data_root+""wmt14-en-es.src.BPE_common.32K.tok"",\n    ""target_file"": data_root+""wmt14-en-es.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n  },\n}\n'"
example_configs/text2text/es-en/transformer-big.py,2,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import TransformerEncoder\nfrom open_seq2seq.decoders import TransformerDecoder\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import PaddedCrossEntropyLossWithSmoothing\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\nfrom open_seq2seq.optimizers.lr_policies import transformer_policy\nimport tensorflow as tf\n\n""""""\nThis configuration file describes a variant of Transformer model from\nhttps://arxiv.org/abs/1706.03762\n""""""\n\nbase_model = Text2Text\nd_model = 1024\nnum_layers = 6\n\n# REPLACE THIS TO THE PATH WITH YOUR WMT DATA\ndata_root = ""[REPLACE THIS TO THE PATH WITH YOUR WMT DATA]""\n\nbase_params = {\n  ""use_horovod"": True,\n  ""num_gpus"": 1, # when using Horovod we set number of workers with params to mpirun\n  ""batch_size_per_gpu"": 256,  # this size is in sentence pairs, reduce it if you get OOM\n  ""max_steps"": 300000,\n  ""save_summaries_steps"": 100,\n  ""print_loss_steps"": 100,\n  ""print_samples_steps"": 100,\n  ""eval_steps"": 4001,\n  ""save_checkpoint_steps"": 299998,\n  ""logdir"": ""Transformer-BIG"",\n  #""dtype"": tf.float32, # to enable mixed precision, comment this line and uncomment two below lines\n  ""dtype"": ""mixed"",\n  ""loss_scaling"": ""Backoff"",\n\n  ""optimizer"": tf.contrib.opt.LazyAdamOptimizer,\n  ""optimizer_params"": {\n    ""beta1"": 0.9,\n    ""beta2"": 0.997,\n    ""epsilon"": 1e-09,\n  },\n\n  ""lr_policy"": transformer_policy,\n  ""lr_policy_params"": {\n    ""learning_rate"": 2.0,\n    ""warmup_steps"": 8000,\n    ""d_model"": d_model,\n  },\n\n  ""encoder"": TransformerEncoder,\n  ""encoder_params"": {\n    ""encoder_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""attention_dropout"": 0.1,\n    ""filter_size"": 4 * d_model,\n    ""relu_dropout"": 0.3,\n    ""layer_postprocess_dropout"": 0.3,\n    ""pad_embeddings_2_eight"": True,\n    ""remove_padding"": True,\n  },\n\n  ""decoder"": TransformerDecoder,\n  ""decoder_params"": {\n    ""layer_postprocess_dropout"": 0.3,\n    ""num_hidden_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 16,\n    ""attention_dropout"": 0.1,\n    ""relu_dropout"": 0.3,\n    ""filter_size"": 4 * d_model,\n    ""beam_size"": 4,\n    ""alpha"": 0.6,\n    ""extra_decode_length"": 50,\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""loss"": PaddedCrossEntropyLossWithSmoothing,\n  ""loss_params"": {\n    ""label_smoothing"": 0.1,\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""pad_vocab_to_eight"": True,\n    ""src_vocab_file"": data_root + ""m_common.vocab"",\n    ""tgt_vocab_file"": data_root + ""m_common.vocab"",\n    ""target_file"": data_root + ""train.clean.en.shuffled.BPE_common.32K.tok"",\n    ""source_file"": data_root + ""train.clean.es.shuffled.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 25000,\n    ""repeat"": True,\n    ""map_parallel_calls"": 16,\n    ""max_length"": 56,\n  },\n}\n\neval_params = {\n  ""batch_size_per_gpu"": 16,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""target_file"": data_root+""wmt13-en-es.src.BPE_common.32K.tok"",\n    ""source_file"": data_root+""wmt13-en-es.ref.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""max_length"": 256,\n    },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": data_root+""m_common.vocab"",\n    ""tgt_vocab_file"": data_root+""m_common.vocab"",\n    ""target_file"": data_root+""wmt14-en-es.src.BPE_common.32K.tok"",\n    ""source_file"": data_root+""wmt14-en-es.src.BPE_common.32K.tok"",\n    ""delimiter"": "" "",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n  },\n}\n'"
example_configs/text2text/toy-reversal/nmt-reversal-CC.py,1,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.models import Text2Text\n\nfrom open_seq2seq.decoders import ConvS2SDecoder\nfrom open_seq2seq.encoders import ConvS2SEncoder\n\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import BasicSequenceLoss\n\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\n""""""\nThis configuration file describes fully convolutional model (ConvS2S)\non the toy task of reversing sequences\n""""""\n\nbase_model = Text2Text\nd_model = 128\nnum_layers = 2\n\nbase_params = {\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 64,\n  ""max_steps"": 1000,\n  ""save_summaries_steps"": 10,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 20,\n  ""eval_steps"": 50,\n  ""save_checkpoint_steps"": 200,\n\n  ""logdir"": ""ReversalTask-Conv-Conv"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {""epsilon"": 1e-9},\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    \'learning_rate\': 1e-3\n  },\n\n  ""max_grad_norm"": 3.0,\n  ""dtype"": tf.float32,\n  # ""dtype"": ""mixed"",\n  # ""loss_scaling"": ""Backoff"",\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": ConvS2SEncoder,\n  ""encoder_params"": {\n    ""src_emb_size"": d_model,\n    ""embedding_dropout_keep_prob"": 0.9,\n    ""pad_embeddings_2_eight"": False,\n    ""att_layer_num"": num_layers,\n\n    ""conv_nchannels_kwidth"": [(d_model, 3)] * num_layers,\n\n    ""hidden_dropout_keep_prob"": 0.9,\n\n    ""max_input_length"": 100,\n\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""decoder"": ConvS2SDecoder,\n  ""decoder_params"": {\n    ""shared_embed"": True,\n    ""tgt_emb_size"": d_model,\n    ""embedding_dropout_keep_prob"": 0.9,\n    ""pad_embeddings_2_eight"": False,\n    ""pos_embed"": True,\n\n    ""conv_nchannels_kwidth"": [(d_model, 3)] * num_layers,\n\n    ""hidden_dropout_keep_prob"": 0.9,\n    ""out_dropout_keep_prob"": 0.9,\n\n    ""max_input_length"": 120,\n    ""extra_decode_length"": 10,\n    ""beam_size"": 5,\n    ""alpha"": 0.6,\n\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": True,\n    ""average_across_timestep"": True,\n    ""do_mask"": True\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/train/source.txt"",\n    ""target_file"": ""toy_text_data/train/target.txt"",\n    ""shuffle"": True,\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n\neval_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/dev/source.txt"",\n    ""target_file"": ""toy_text_data/dev/target.txt"",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""source_file"": ""toy_text_data/test/source.txt"",\n    # this is intentional to be sure model is not using ground truth\n    ""target_file"": ""toy_text_data/test/source.txt"",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n'"
example_configs/text2text/toy-reversal/nmt-reversal-CR.py,3,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.decoders import RNNDecoderWithAttention, BeamSearchRNNDecoderWithAttention\nfrom open_seq2seq.encoders import ConvS2SEncoder\n\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import BasicSequenceLoss\n\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\n""""""\nThis configuration file describes convolutional encoder and rnn decoder with attention\non the toy task of reversing sequences\n""""""\n\nbase_model = Text2Text\nd_model = 128\nnum_layers = 2\n\nbase_params = {\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 64,\n  ""max_steps"": 1000,\n  ""save_summaries_steps"": 10,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 20,\n  ""eval_steps"": 50,\n  ""save_checkpoint_steps"": 200,\n\n  ""logdir"": ""ReversalTask-Conv-RNN"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {""epsilon"": 1e-9},\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    \'learning_rate\': 1e-3\n  },\n\n  ""max_grad_norm"": 3.0,\n  ""dtype"": tf.float32,\n  # ""dtype"": ""mixed"",\n  # ""loss_scaling"": ""Backoff"",\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": ConvS2SEncoder,\n  ""encoder_params"": {\n    ""src_emb_size"": d_model,\n    ""att_layer_num"": num_layers,\n    ""embedding_dropout_keep_prob"": 0.9,\n    ""pad_embeddings_2_eight"": True,\n\n    ""hidden_dropout_keep_prob"": 0.9,\n\n    ""conv_nchannels_kwidth"": [(d_model, 3)] * num_layers,\n\n    ""max_input_length"": 100,\n\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""decoder"": RNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n      ""num_units"": d_model,\n    },\n    ""decoder_layers"": num_layers,\n\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": False,\n\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n\n    ""tgt_emb_size"": d_model,\n    ""attention_type"": ""luong"",\n    ""luong_scale"": False,\n    ""attention_layer_size"": 128,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": True,\n    ""average_across_timestep"": True,\n    ""do_mask"": True\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/train/source.txt"",\n    ""target_file"": ""toy_text_data/train/target.txt"",\n    ""shuffle"": True,\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n\neval_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/dev/source.txt"",\n    ""target_file"": ""toy_text_data/dev/target.txt"",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""decoder"": BeamSearchRNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n      ""num_units"": d_model,\n    },\n    ""decoder_layers"": num_layers,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": False,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n    ""tgt_emb_size"": d_model,\n    ""attention_type"": ""luong"",\n    ""luong_scale"": False,\n    ""attention_layer_size"": d_model,\n    ""beam_width"": 5,\n    ""length_penalty"": 1.0,\n  },\n\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""source_file"": ""toy_text_data/test/source.txt"",\n    ""target_file"": ""toy_text_data/test/source.txt"",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n\n}\n'"
example_configs/text2text/toy-reversal/nmt-reversal-RC.py,2,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import BidirectionalRNNEncoderWithEmbedding\nfrom open_seq2seq.decoders import ConvS2SDecoder\n\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import BasicSequenceLoss\n\nfrom open_seq2seq.data.text2text.tokenizer import EOS_ID\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\n""""""\nThis configuration file describes bidirectional rnn based encoder and convolutional decoder\non the toy task of reversing sequences\n""""""\n\nbase_model = Text2Text\nd_model = 128\nnum_layers = 2\n\nbase_params = {\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 64,\n  ""max_steps"": 1000,\n  ""save_summaries_steps"": 10,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 20,\n  ""eval_steps"": 50,\n  ""save_checkpoint_steps"": 200,\n\n  ""logdir"": ""ReversalTask-RNN-Conv"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {""epsilon"": 1e-9},\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    \'learning_rate\': 1e-3\n  },\n\n  ""max_grad_norm"": 3.0,\n  ""dtype"": tf.float32,\n  # ""dtype"": ""mixed"",\n  # ""loss_scaling"": ""Backoff"",\n\n  ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n  ""encoder"": BidirectionalRNNEncoderWithEmbedding,\n  ""encoder_params"": {\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n      ""num_units"": int(d_model/2),\n    },\n\n    ""encoder_layers"": num_layers,\n    ""encoder_dp_input_keep_prob"": 0.8,\n    ""encoder_dp_output_keep_prob"": 1.0,\n    ""encoder_use_skip_connections"": False,\n    ""src_emb_size"": d_model,\n  },\n\n  ""decoder"": ConvS2SDecoder,\n  ""decoder_params"": {\n    ""shared_embed"": True,\n    ""tgt_emb_size"": d_model,\n\n    ""conv_nchannels_kwidth"": [(d_model, 3)] * num_layers,\n\n    ""embedding_dropout_keep_prob"": 0.9,\n    ""hidden_dropout_keep_prob"": 0.9,\n    ""out_dropout_keep_prob"": 0.9,\n\n    ""max_input_length"": 100,\n    ""extra_decode_length"": 10,\n    ""beam_size"": 5,\n    ""alpha"": 0.6,\n\n    ""EOS_ID"": EOS_ID,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {\n    ""offset_target_by_one"": True,\n    ""average_across_timestep"": True,\n    ""do_mask"": True\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/train/source.txt"",\n    ""target_file"": ""toy_text_data/train/target.txt"",\n    ""shuffle"": True,\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n\neval_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/dev/source.txt"",\n    ""target_file"": ""toy_text_data/dev/target.txt"",\n    ""shuffle"": False,\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""source_file"": ""toy_text_data/test/source.txt"",\n    # this is intentional to be sure model is not using ground truth\n    ""target_file"": ""toy_text_data/test/source.txt"",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n'"
example_configs/text2text/toy-reversal/nmt-reversal-RR.py,4,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import BidirectionalRNNEncoderWithEmbedding\nfrom open_seq2seq.decoders import RNNDecoderWithAttention, \\\n  BeamSearchRNNDecoderWithAttention\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import BasicSequenceLoss\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.optimizers.lr_policies import fixed_lr\n\n""""""\nThis configuration file describes classic RNN-based encoder-decoder model\nwith attention on the toy task of reversing sequences\n""""""\n\nbase_model = Text2Text\n\nbase_params = {\n  ""use_horovod"": False,\n  #""iter_size"": 10,\n  # set this to number of available GPUs\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 64,\n  ""max_steps"": 800,\n  ""save_summaries_steps"": 10,\n  ""print_loss_steps"": 10,\n  ""print_samples_steps"": 20,\n  ""eval_steps"": 50,\n  ""save_checkpoint_steps"": 300,\n  ""logdir"": ""ReversalTask-RNN-RNN"",\n\n  ""optimizer"": ""Adam"",\n  ""optimizer_params"": {""epsilon"": 1e-4},\n  ""lr_policy"": fixed_lr,\n  ""lr_policy_params"": {\n    \'learning_rate\': 0.001\n  },\n  ""max_grad_norm"": 3.0,\n  ""dtype"": tf.float32,\n  # ""dtype"": ""mixed"",\n  # ""loss_scaling"": ""Backoff"",\n\n  ""encoder"": BidirectionalRNNEncoderWithEmbedding,\n  ""encoder_params"": {\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n      ""num_units"": 128,\n      ""forget_bias"": 1.0,\n    },\n    ""encoder_layers"": 1,\n    ""encoder_dp_input_keep_prob"": 0.8,\n    ""encoder_dp_output_keep_prob"": 1.0,\n    ""encoder_use_skip_connections"": False,\n    ""src_emb_size"": 128,\n  },\n\n  ""decoder"": RNNDecoderWithAttention,\n  ""decoder_params"": {\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n      ""num_units"": 128,\n      # ""forget_bias"": 1.0,\n    },\n    ""decoder_layers"": 1,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": False,    \n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""tgt_emb_size"": 128,\n    ""attention_type"": ""luong"",\n    ""luong_scale"": False,\n    ""attention_layer_size"": 128,\n  },\n\n  ""loss"": BasicSequenceLoss,\n  ""loss_params"": {    \n    ""offset_target_by_one"": True,\n    ""average_across_timestep"": False,\n    ""do_mask"": True\n  }\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/train/source.txt"",\n    ""target_file"": ""toy_text_data/train/target.txt"",\n    ""shuffle"": True,\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n\neval_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/dev/source.txt"",\n    ""target_file"": ""toy_text_data/dev/target.txt"",\n    ""shuffle"": False,\n    # because we evaluate many times\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""decoder"": BeamSearchRNNDecoderWithAttention,\n  ""decoder_params"": {\n    #""decoder_cell_type"": ""lstm"",\n    #""decoder_cell_units"": 128,\n    ""core_cell"": tf.nn.rnn_cell.LSTMCell,\n    ""core_cell_params"": {\n      ""num_units"": 128,\n      ""forget_bias"": 1.0,\n    },\n    ""decoder_layers"": 1,\n    ""decoder_dp_input_keep_prob"": 0.8,\n    ""decoder_dp_output_keep_prob"": 1.0,\n    ""decoder_use_skip_connections"": False,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n    ""tgt_emb_size"": 128,\n    ""attention_type"": ""luong"",\n    ""luong_scale"": False,\n    ""attention_layer_size"": 128,\n    ""beam_width"": 10,\n    ""length_penalty"": 1.0,\n  },\n\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""source_file"": ""toy_text_data/test/source.txt"",\n    ""target_file"": ""toy_text_data/test/target.txt"",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n  },\n}'"
example_configs/text2text/toy-reversal/nmt-reversal-TT.py,2,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nfrom open_seq2seq.models import Text2Text\nfrom open_seq2seq.encoders import TransformerEncoder\nfrom open_seq2seq.decoders import TransformerDecoder\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.losses import PaddedCrossEntropyLossWithSmoothing\nfrom open_seq2seq.data.text2text.text2text import SpecialTextTokens\nfrom open_seq2seq.optimizers.lr_policies import transformer_policy\nimport tensorflow as tf\n\n""""""\nThis configuration file describes a tiny variant of Transformer model from\nhttps://arxiv.org/abs/1706.03762 on the toy task of reversing sequences\n""""""\n\nbase_model = Text2Text\nd_model = 128\nnum_layers = 2\n\nbase_params = {\n  ""use_horovod"": False,\n  ""num_gpus"": 1,\n  ""batch_size_per_gpu"": 64,\n  ""max_steps"": 800,\n  ""save_summaries_steps"": 50,\n  ""print_loss_steps"": 50,\n  ""print_samples_steps"": 50,\n  ""eval_steps"": 50,\n  ""save_checkpoint_steps"": 300,\n  ""logdir"": ""ReversalTask-Transformer-Transformer"",\n  ""dtype"": tf.float32,\n  # ""dtype"": ""mixed"",\n  # ""loss_scaling"": ""Backoff"",\n\n  ""optimizer"": tf.contrib.opt.LazyAdamOptimizer,\n  ""optimizer_params"": {\n    ""beta1"": 0.9,\n    ""beta2"": 0.997,\n    ""epsilon"": 0.000000001,\n  },\n  ""lr_policy"": transformer_policy,\n  ""lr_policy_params"": {\n    ""learning_rate"": 1.0,\n    ""warmup_steps"": 200,\n    ""d_model"": d_model,\n  },\n  ""encoder"": TransformerEncoder,\n  ""encoder_params"": {\n    ""encoder_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 8,\n    ""attention_dropout"": 0.1,\n    ""filter_size"": 4*d_model,\n    ""relu_dropout"": 0.1,\n    ""layer_postprocess_dropout"": 0.1,\n    ""remove_padding"": True,\n  },\n\n  ""decoder"": TransformerDecoder,\n  ""decoder_params"": {\n    ""layer_postprocess_dropout"": 0.1,\n    ""num_hidden_layers"": num_layers,\n    ""hidden_size"": d_model,\n    ""num_heads"": 8,\n    ""attention_dropout"": 0.1,\n    ""relu_dropout"": 0.1,\n    ""filter_size"": 4*d_model,\n    ""beam_size"": 5,\n    ""alpha"": 1.0,\n    ""extra_decode_length"": 2,\n    ""EOS_ID"": SpecialTextTokens.EOS_ID.value,\n    ""GO_SYMBOL"": SpecialTextTokens.S_ID.value,\n    ""END_SYMBOL"": SpecialTextTokens.EOS_ID.value,\n    ""PAD_SYMBOL"": SpecialTextTokens.PAD_ID.value,\n  },\n\n  ""loss"": PaddedCrossEntropyLossWithSmoothing,\n  ""loss_params"": {}\n}\n\ntrain_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/train/source.txt"",\n    ""target_file"": ""toy_text_data/train/target.txt"",\n    ""shuffle"": True,\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n    ""use_start_token"": False,\n  },\n}\n\neval_params = {\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/target.txt"",\n    ""source_file"": ""toy_text_data/dev/source.txt"",\n    ""target_file"": ""toy_text_data/dev/target.txt"",\n    ""shuffle"": False,\n    # because we evaluate many times\n    ""repeat"": True,\n    ""max_length"": 56,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n    ""use_start_token"": False,\n  },\n}\n\ninfer_params = {\n  ""batch_size_per_gpu"": 1,\n  ""data_layer"": ParallelTextDataLayer,\n  ""data_layer_params"": {\n    ""src_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""tgt_vocab_file"": ""toy_text_data/vocab/source.txt"",\n    ""source_file"": ""toy_text_data/test/source.txt"",\n    ""target_file"": ""toy_text_data/test/target.txt"",\n    ""shuffle"": False,\n    ""repeat"": False,\n    ""max_length"": 256,\n    ""delimiter"": "" "",\n    ""special_tokens_already_in_vocab"": False,\n    ""use_start_token"": False,\n  },\n}\n'"
external_lm_rescore/transformerxl/utils/adaptive_softmax.py,0,"b'from collections import defaultdict\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AdaptiveLogSoftmax(nn.Module):\n    def __init__(self, in_features, n_classes, cutoffs, keep_order=False):\n        super(AdaptiveLogSoftmax, self).__init__()\n\n        cutoffs = list(cutoffs)\n\n        if (cutoffs != sorted(cutoffs)) \\\n                or (min(cutoffs) <= 0) \\\n                or (max(cutoffs) >= (n_classes - 1)) \\\n                or (len(set(cutoffs)) != len(cutoffs)) \\\n                or any([int(c) != c for c in cutoffs]):\n\n            raise ValueError(""cutoffs should be a sequence of unique, positive ""\n                             ""integers sorted in an increasing order, where ""\n                             ""each value is between 1 and n_classes-1"")\n\n        self.in_features = in_features\n        self.n_classes = n_classes\n        self.cutoffs = cutoffs + [n_classes]\n\n        self.shortlist_size = self.cutoffs[0]\n        self.n_clusters = len(self.cutoffs) - 1\n        self.head_size = self.shortlist_size + self.n_clusters\n\n        self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.in_features))\n        self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n\n        self.keep_order = keep_order\n\n\n    def forward(self, hidden, target, weight, bias, keep_order=False):\n        if hidden.size(0) != target.size(0):\n            raise RuntimeError(\'Input and target should have the same size \'\n                               \'in the batch dimension.\')\n\n        head_weight = torch.cat(\n            [weight[:self.shortlist_size], self.cluster_weight], dim=0)\n        head_bias = torch.cat(\n            [bias[:self.shortlist_size], self.cluster_bias], dim=0)\n\n        head_logit = F.linear(hidden, head_weight, bias=head_bias)\n        head_logprob = F.log_softmax(head_logit, dim=1)\n\n        nll = torch.zeros_like(target,\n                dtype=hidden.dtype, device=hidden.device)\n\n        offset = 0\n        cutoff_values = [0] + self.cutoffs\n        for i in range(len(cutoff_values) - 1):\n            l_idx, h_idx = cutoff_values[i], cutoff_values[i + 1]\n\n            mask_i = (target >= l_idx) & (target < h_idx)\n            indices_i = mask_i.nonzero().squeeze()\n\n            if indices_i.numel() == 0:\n                continue\n\n            target_i = target.index_select(0, indices_i) - l_idx\n            head_logprob_i = head_logprob.index_select(0, indices_i)\n\n            if i == 0:\n                logprob_i = head_logprob_i.gather(1, target_i[:,None]).squeeze(1)\n            else:\n                weight_i = weight[l_idx:h_idx]\n                bias_i = bias[l_idx:h_idx]\n\n                hidden_i = hidden.index_select(0, indices_i)\n\n                tail_logit_i = F.linear(hidden_i, weight_i, bias=bias_i)\n                tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n\n                logprob_i = head_logprob_i[:, -i] \\\n                          + tail_logprob_i.gather(1, target_i[:,None]).squeeze(1)\n\n            if (hasattr(self, \'keep_order\') and self.keep_order) or keep_order:\n                nll.index_copy_(0, indices_i, -logprob_i)\n            else:\n                nll[offset:offset+logprob_i.size(0)].copy_(-logprob_i)\n\n            offset += logprob_i.size(0)\n\n        return nll\n'"
external_lm_rescore/transformerxl/utils/data_parallel.py,0,"b'\nfrom torch.nn.parallel import DataParallel\nimport torch\nfrom torch.nn.parallel._functions import Scatter\nfrom torch.nn.parallel.parallel_apply import parallel_apply\n\ndef scatter(inputs, target_gpus, chunk_sizes, dim=0):\n    r""""""\n    Slices tensors into approximately equal chunks and\n    distributes them across given GPUs. Duplicates\n    references to objects that are not tensors.\n    """"""\n    def scatter_map(obj):\n        if isinstance(obj, torch.Tensor):\n            try:\n                return Scatter.apply(target_gpus, chunk_sizes, dim, obj)\n            except:\n                print(\'obj\', obj.size())\n                print(\'dim\', dim)\n                print(\'chunk_sizes\', chunk_sizes)\n                quit()\n        if isinstance(obj, tuple) and len(obj) > 0:\n            return list(zip(*map(scatter_map, obj)))\n        if isinstance(obj, list) and len(obj) > 0:\n            return list(map(list, zip(*map(scatter_map, obj))))\n        if isinstance(obj, dict) and len(obj) > 0:\n            return list(map(type(obj), zip(*map(scatter_map, obj.items()))))\n        return [obj for targets in target_gpus]\n\n    # After scatter_map is called, a scatter_map cell will exist. This cell\n    # has a reference to the actual function scatter_map, which has references\n    # to a closure that has a reference to the scatter_map cell (because the\n    # fn is recursive). To avoid this reference cycle, we set the function to\n    # None, clearing the cell\n    try:\n        return scatter_map(inputs)\n    finally:\n        scatter_map = None\n\ndef scatter_kwargs(inputs, kwargs, target_gpus, chunk_sizes, dim=0):\n    r""""""Scatter with support for kwargs dictionary""""""\n    inputs = scatter(inputs, target_gpus, chunk_sizes, dim) if inputs else []\n    kwargs = scatter(kwargs, target_gpus, chunk_sizes, dim) if kwargs else []\n    if len(inputs) < len(kwargs):\n        inputs.extend([() for _ in range(len(kwargs) - len(inputs))])\n    elif len(kwargs) < len(inputs):\n        kwargs.extend([{} for _ in range(len(inputs) - len(kwargs))])\n    inputs = tuple(inputs)\n    kwargs = tuple(kwargs)\n    return inputs, kwargs\n\nclass BalancedDataParallel(DataParallel):\n    def __init__(self, gpu0_bsz, *args, **kwargs):\n        self.gpu0_bsz = gpu0_bsz\n        super().__init__(*args, **kwargs)\n\n    def forward(self, *inputs, **kwargs):\n        if not self.device_ids:\n            return self.module(*inputs, **kwargs)\n        if self.gpu0_bsz == 0:\n            device_ids = self.device_ids[1:]\n        else:\n            device_ids = self.device_ids\n        inputs, kwargs = self.scatter(inputs, kwargs, device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        replicas = self.replicate(self.module, self.device_ids)\n        if self.gpu0_bsz == 0:\n            replicas = replicas[1:]\n        outputs = self.parallel_apply(replicas, device_ids, inputs, kwargs)\n        return self.gather(outputs, self.output_device)\n\n    def parallel_apply(self, replicas, device_ids, inputs, kwargs):\n        return parallel_apply(replicas, inputs, kwargs, device_ids)\n\n    def scatter(self, inputs, kwargs, device_ids):\n        bsz = inputs[0].size(self.dim)\n        num_dev = len(self.device_ids)\n        gpu0_bsz = self.gpu0_bsz\n        bsz_unit = (bsz - gpu0_bsz) // (num_dev - 1)\n        if gpu0_bsz < bsz_unit:\n            chunk_sizes = [gpu0_bsz] + [bsz_unit] * (num_dev - 1)\n            delta = bsz - sum(chunk_sizes)\n            for i in range(delta):\n                chunk_sizes[i + 1] += 1\n            if gpu0_bsz == 0:\n                chunk_sizes = chunk_sizes[1:]\n        else:\n            return super().scatter(inputs, kwargs, device_ids)\n        return scatter_kwargs(inputs, kwargs, device_ids, chunk_sizes, dim=self.dim)\n\n'"
external_lm_rescore/transformerxl/utils/exp_utils.py,0,"b""import functools\nimport os, shutil\n\nimport numpy as np\n\nimport torch\n\n\ndef logging(s, log_path, print_=True, log_=True):\n    if print_:\n        print(s)\n    if log_:\n        with open(log_path, 'a+') as f_log:\n            f_log.write(s + '\\n')\n\ndef get_logger(log_path, **kwargs):\n    return functools.partial(logging, log_path=log_path, **kwargs)\n\ndef create_exp_dir(dir_path, scripts_to_save=None, debug=False):\n    if debug:\n        print('Debug Mode : no experiment dir created')\n        return functools.partial(logging, log_path=None, log_=False)\n\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n    print('Experiment dir : {}'.format(dir_path))\n    if scripts_to_save is not None:\n        script_path = os.path.join(dir_path, 'scripts')\n        if not os.path.exists(script_path):\n            os.makedirs(script_path)\n        for script in scripts_to_save:\n            dst_file = os.path.join(dir_path, 'scripts', os.path.basename(script))\n            shutil.copyfile(script, dst_file)\n\n    return get_logger(log_path=os.path.join(dir_path, 'log.txt'))\n\ndef save_checkpoint(model, optimizer, path, epoch):\n    torch.save(model, os.path.join(path, 'model_{}.pt'.format(epoch)))\n    torch.save(optimizer.state_dict(), os.path.join(path, 'optimizer_{}.pt'.format(epoch)))\n"""
external_lm_rescore/transformerxl/utils/log_uniform_sampler.py,0,"b'import torch\nfrom torch import nn\nimport numpy as np\n\nclass LogUniformSampler(object):\n    def __init__(self, range_max, n_sample):\n        """"""\n        Reference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n            `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n\n        expected count can be approximated by 1 - (1 - p)^n\n        and we use a numerically stable version -expm1(num_tries * log1p(-p))\n\n        Our implementation fixes num_tries at 2 * n_sample, and the actual #samples will vary from run to run\n        """"""\n        with torch.no_grad():\n            self.range_max = range_max\n            log_indices = torch.arange(1., range_max+2., 1.).log_()\n            self.dist = (log_indices[1:] - log_indices[:-1]) / log_indices[-1]\n            # print(\'P\', self.dist.numpy().tolist()[-30:])\n\n            self.log_q = (- (-self.dist.double().log1p_() * 2 * n_sample).expm1_()).log_().float()\n\n        self.n_sample = n_sample\n\n    def sample(self, labels):\n        """"""\n            labels: [b1, b2]\n        Return\n            true_log_probs: [b1, b2]\n            samp_log_probs: [n_sample]\n            neg_samples: [n_sample]\n        """"""\n\n        # neg_samples = torch.empty(0).long()\n        n_sample = self.n_sample\n        n_tries = 2 * n_sample\n\n        with torch.no_grad():\n            neg_samples = torch.multinomial(self.dist, n_tries, replacement=True).unique()\n            device = labels.device\n            neg_samples = neg_samples.to(device)\n            true_log_probs = self.log_q[labels].to(device)\n            samp_log_probs = self.log_q[neg_samples].to(device)\n            return true_log_probs, samp_log_probs, neg_samples\n\ndef sample_logits(embedding, bias, labels, inputs, sampler):\n    """"""\n        embedding: an nn.Embedding layer\n        bias: [n_vocab]\n        labels: [b1, b2]\n        inputs: [b1, b2, n_emb]\n        sampler: you may use a LogUniformSampler\n    Return\n        logits: [b1, b2, 1 + n_sample]\n    """"""\n    true_log_probs, samp_log_probs, neg_samples = sampler.sample(labels)\n    n_sample = neg_samples.size(0)\n    b1, b2 = labels.size(0), labels.size(1)\n    all_ids = torch.cat([labels.view(-1), neg_samples])\n    all_w = embedding(all_ids)\n    true_w = all_w[: -n_sample].view(b1, b2, -1)\n    sample_w = all_w[- n_sample:].view(n_sample, -1)\n\n    all_b = bias[all_ids]\n    true_b = all_b[: -n_sample].view(b1, b2)\n    sample_b = all_b[- n_sample:]\n\n    hit = (labels[:, :, None] == neg_samples).detach()\n\n    true_logits = torch.einsum(\'ijk,ijk->ij\',\n        [true_w, inputs]) + true_b - true_log_probs\n    sample_logits = torch.einsum(\'lk,ijk->ijl\',\n        [sample_w, inputs]) + sample_b - samp_log_probs\n    sample_logits.masked_fill_(hit, -1e30)\n    logits = torch.cat([true_logits[:, :, None], sample_logits], -1)\n\n    return logits\n\n\n# class LogUniformSampler(object):\n#     def __init__(self, range_max, unique=False):\n#         """"""\n#         Reference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n#             `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n#         """"""\n#         self.range_max = range_max\n#         log_indices = torch.arange(1., range_max+2., 1.).log_()\n#         self.dist = (log_indices[1:] - log_indices[:-1]) / log_indices[-1]\n\n#         self.unique = unique\n\n#         if self.unique:\n#             self.exclude_mask = torch.ByteTensor(range_max).fill_(0)\n\n#     def sample(self, n_sample, labels):\n#         pos_sample, new_labels = labels.unique(return_inverse=True)\n#         n_pos_sample = pos_sample.size(0)\n#         n_neg_sample = n_sample - n_pos_sample\n\n#         if self.unique:\n#             self.exclude_mask.index_fill_(0, pos_sample, 1)\n#             sample_dist = self.dist.clone().masked_fill_(self.exclude_mask, 0)\n#             self.exclude_mask.index_fill_(0, pos_sample, 0)\n#         else:\n#             sample_dist = self.dist\n\n#         neg_sample = torch.multinomial(sample_dist, n_neg_sample)\n\n#         sample = torch.cat([pos_sample, neg_sample])\n#         sample_prob = self.dist[sample]\n\n#         return new_labels, sample, sample_prob\n\n\nif __name__ == \'__main__\':\n    S, B = 3, 4\n    n_vocab = 10000\n    n_sample = 5\n    H = 32\n\n    labels = torch.LongTensor(S, B).random_(0, n_vocab)\n\n    # sampler = LogUniformSampler(n_vocab, unique=False)\n    # new_labels, sample, sample_prob = sampler.sample(n_sample, labels)\n\n    sampler = LogUniformSampler(n_vocab, unique=True)\n    # true_probs, samp_probs, neg_samples = sampler.sample(n_sample, labels)\n\n    # print(\'true_probs\', true_probs.numpy().tolist())\n    # print(\'samp_probs\', samp_probs.numpy().tolist())\n    # print(\'neg_samples\', neg_samples.numpy().tolist())\n\n    # print(\'sum\', torch.sum(sampler.dist).item())\n\n    # assert torch.all(torch.sort(sample.unique())[0].eq(torch.sort(sample)[0])).item()\n\n    embedding = nn.Embedding(n_vocab, H)\n    bias = torch.zeros(n_vocab)\n    inputs = torch.Tensor(S, B, H).normal_()\n\n    logits, out_labels = sample_logits(embedding, bias, labels, inputs, sampler, n_sample)\n    print(\'logits\', logits.detach().numpy().tolist())\n    print(\'logits shape\', logits.size())\n    print(\'out_labels\', out_labels.detach().numpy().tolist())\n    print(\'out_labels shape\', out_labels.size())\n\n'"
external_lm_rescore/transformerxl/utils/proj_adaptive_softmax.py,0,"b""from collections import defaultdict\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nCUDA_MAJOR = int(torch.version.cuda.split('.')[0])\nCUDA_MINOR = int(torch.version.cuda.split('.')[1])\n\nclass ProjectedAdaptiveLogSoftmax(nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1,\n                 keep_order=False):\n        super(ProjectedAdaptiveLogSoftmax, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n        self.d_proj = d_proj\n\n        self.cutoffs = cutoffs + [n_token]\n        self.cutoff_ends = [0] + self.cutoffs\n        self.div_val = div_val\n\n        self.shortlist_size = self.cutoffs[0]\n        self.n_clusters = len(self.cutoffs) - 1\n        self.head_size = self.shortlist_size + self.n_clusters\n\n        if self.n_clusters > 0:\n            self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n            self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n\n        self.out_layers = nn.ModuleList()\n        self.out_projs = nn.ParameterList()\n\n        if div_val == 1:\n            for i in range(len(self.cutoffs)):\n                if d_proj != d_embed:\n                    self.out_projs.append(\n                        nn.Parameter(torch.Tensor(d_proj, d_embed))\n                    )\n                else:\n                    self.out_projs.append(None)\n\n            self.out_layers.append(nn.Linear(d_embed, n_token))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed // (div_val ** i)\n\n                self.out_projs.append(\n                    nn.Parameter(torch.Tensor(d_proj, d_emb_i))\n                )\n\n                self.out_layers.append(nn.Linear(d_emb_i, r_idx-l_idx))\n\n        self.keep_order = keep_order\n\n    def _compute_logit(self, hidden, weight, bias, proj):\n        if proj is None:\n            logit = F.linear(hidden, weight, bias=bias)\n        else:\n            # if CUDA_MAJOR <= 9 and CUDA_MINOR <= 1:\n            proj_hid = F.linear(hidden, proj.t().contiguous())\n            logit = F.linear(proj_hid, weight, bias=bias)\n            # else:\n            #     logit = torch.einsum('bd,de,ev->bv', (hidden, proj, weight.t()))\n            #     if bias is not None:\n            #         logit = logit + bias\n\n        return logit\n\n    def forward(self, hidden, target, keep_order=False):\n        '''\n            hidden :: [len*bsz x d_proj]\n            target :: [len*bsz]\n        '''\n\n        if hidden.size(0) != target.size(0):\n            raise RuntimeError('Input and target should have the same size '\n                               'in the batch dimension.')\n\n        if self.n_clusters == 0:\n            logit = self._compute_logit(hidden, self.out_layers[0].weight,\n                                        self.out_layers[0].bias, self.out_projs[0])\n            nll = -F.log_softmax(logit, dim=-1) \\\n                    .gather(1, target.unsqueeze(1)).squeeze(1)\n        else:\n            # construct weights and biases\n            weights, biases = [], []\n            for i in range(len(self.cutoffs)):\n                if self.div_val == 1:\n                    l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n                    weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                    bias_i = self.out_layers[0].bias[l_idx:r_idx]\n                else:\n                    weight_i = self.out_layers[i].weight\n                    bias_i = self.out_layers[i].bias\n\n                if i == 0:\n                    weight_i = torch.cat(\n                        [weight_i, self.cluster_weight], dim=0)\n                    bias_i = torch.cat(\n                        [bias_i, self.cluster_bias], dim=0)\n\n                weights.append(weight_i)\n                biases.append(bias_i)\n\n            head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]\n\n            head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n            head_logprob = F.log_softmax(head_logit, dim=1)\n\n            nll = torch.zeros_like(target,\n                    dtype=hidden.dtype, device=hidden.device)\n\n            offset = 0\n            cutoff_values = [0] + self.cutoffs\n            for i in range(len(cutoff_values) - 1):\n                l_idx, r_idx = cutoff_values[i], cutoff_values[i + 1]\n\n                mask_i = (target >= l_idx) & (target < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n\n                if indices_i.numel() == 0:\n                    continue\n\n                target_i = target.index_select(0, indices_i) - l_idx\n                head_logprob_i = head_logprob.index_select(0, indices_i)\n\n                if i == 0:\n                    logprob_i = head_logprob_i.gather(1, target_i[:,None]).squeeze(1)\n                else:\n                    weight_i, bias_i, proj_i = weights[i], biases[i], self.out_projs[i]\n\n                    hidden_i = hidden.index_select(0, indices_i)\n\n                    tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                    tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n\n                    logprob_i = head_logprob_i[:, -i] \\\n                              + tail_logprob_i.gather(1, target_i[:,None]).squeeze(1)\n\n                if (hasattr(self, 'keep_order') and self.keep_order) or keep_order:\n                    nll.index_copy_(0, indices_i, -logprob_i)\n                else:\n                    nll[offset:offset+logprob_i.size(0)].copy_(-logprob_i)\n\n                offset += logprob_i.size(0)\n\n        return nll\n"""
external_lm_rescore/transformerxl/utils/vocabulary.py,0,"b'import os\nfrom collections import Counter, OrderedDict\n\nimport torch\n\nclass Vocab(object):\n    def __init__(self, special=[], min_freq=0, max_size=None, lower_case=True,\n                 delimiter=None, vocab_file=None):\n        self.counter = Counter()\n        self.special = special\n        self.min_freq = min_freq\n        self.max_size = max_size\n        self.lower_case = lower_case\n        self.delimiter = delimiter\n        self.vocab_file = vocab_file\n\n    def tokenize(self, line, add_eos=False, add_double_eos=False):\n        line = line.strip()\n        # convert to lower case\n        if self.lower_case:\n            line = line.lower()\n\n        # empty delimiter \'\' will evaluate False\n        if self.delimiter == \'\':\n            symbols = line\n        else:\n            symbols = line.split(self.delimiter)\n\n        if add_double_eos: # lm1b\n            return [\'<S>\'] + symbols + [\'<S>\']\n        elif add_eos:\n            return symbols + [\'<eos>\']\n        else:\n            return symbols\n\n    def count_file(self, path, verbose=False, add_eos=False):\n        if verbose: print(\'counting file {} ...\'.format(path))\n        assert os.path.exists(path)\n\n        sents = []\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    print(\'    line {}\'.format(idx))\n                symbols = self.tokenize(line, add_eos=add_eos)\n                self.counter.update(symbols)\n                sents.append(symbols)\n\n        return sents\n\n    def count_sents(self, sents, verbose=False):\n        """"""\n            sents : a list of sentences, each a list of tokenized symbols\n        """"""\n        if verbose: print(\'counting {} sents ...\'.format(len(sents)))\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                print(\'    line {}\'.format(idx))\n            self.counter.update(symbols)\n\n    def _build_from_file(self, vocab_file):\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n\n        with open(vocab_file, \'r\', encoding=\'utf-8\') as f:\n            for line in f:\n                symb = line.strip().split()[0]\n                self.add_symbol(symb)\n        self.unk_idx = self.sym2idx[\'<UNK>\']\n\n    def build_vocab(self):\n        if self.vocab_file:\n            print(\'building vocab from {}\'.format(self.vocab_file))\n            self._build_from_file(self.vocab_file)\n            print(\'final vocab size {}\'.format(len(self)))\n        else:\n            print(\'building vocab with min_freq={}, max_size={}\'.format(\n                self.min_freq, self.max_size))\n            self.idx2sym = []\n            self.sym2idx = OrderedDict()\n\n            for sym in self.special:\n                self.add_special(sym)\n\n            for sym, cnt in self.counter.most_common(self.max_size):\n                if cnt < self.min_freq: break\n                self.add_symbol(sym)\n\n            print(\'final vocab size {} from {} unique tokens\'.format(\n                len(self), len(self.counter)))\n\n    def encode_file(self, path, ordered=False, verbose=False, add_eos=True,\n            add_double_eos=False):\n        if verbose: print(\'encoding file {} ...\'.format(path))\n        print(path)\n        assert os.path.exists(path)\n        encoded = []\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                # if verbose and idx > 0 and idx % 1 == 0:\n                    print(\'    line {}\'.format(idx))\n                symbols = self.tokenize(line, add_eos=add_eos,\n                    add_double_eos=add_double_eos)\n                encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def encode_sents(self, sents, ordered=False, verbose=False):\n        if verbose: print(\'encoding {} sents ...\'.format(len(sents)))\n        encoded = []\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                print(\'    line {}\'.format(idx))\n            encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def add_special(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n            setattr(self, \'{}_idx\'.format(sym.strip(\'<>\')), self.sym2idx[sym])\n\n    def add_symbol(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n\n    def get_sym(self, idx):\n        assert 0 <= idx < len(self), \'Index {} out of range\'.format(idx)\n        return self.idx2sym[idx]\n\n    def get_idx(self, sym):\n        if sym in self.sym2idx:\n            return self.sym2idx[sym]\n        else:\n            # print(\'encounter unk {}\'.format(sym))\n            assert \'<eos>\' not in sym\n            assert hasattr(self, \'unk_idx\')\n            return self.sym2idx.get(sym, self.unk_idx)\n\n    def get_symbols(self, indices):\n        return [self.get_sym(idx) for idx in indices]\n\n    def get_indices(self, symbols):\n        return [self.get_idx(sym) for sym in symbols]\n\n    def convert_to_tensor(self, symbols):\n        return torch.LongTensor(self.get_indices(symbols))\n\n    def convert_to_sent(self, indices, exclude=None):\n        if exclude is None:\n            return \' \'.join([self.get_sym(idx) for idx in indices])\n        else:\n            return \' \'.join([self.get_sym(idx) for idx in indices if idx not in exclude])\n\n    def __len__(self):\n        return len(self.idx2sym)\n'"
open_seq2seq/data/image2label/__init__.py,0,b''
open_seq2seq/data/image2label/cifar10_download_and_extract.py,1,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Downloads and extracts the binary version of the CIFAR-10 dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nDATA_URL = \'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\'\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \'--data_dir\', type=str, default=\'data/\',\n    help=\'Directory to download data and extract the tarball\')\n\n\ndef main(_):\n  """"""Download and extract the tarball from Alex\'s website.""""""\n  if not os.path.exists(FLAGS.data_dir):\n    os.makedirs(FLAGS.data_dir)\n\n  filename = DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(FLAGS.data_dir, filename)\n\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, 100.0 * count * block_size / total_size))\n      sys.stdout.flush()\n\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n\n  tarfile.open(filepath, \'r:gz\').extractall(FLAGS.data_dir)\n\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
open_seq2seq/data/image2label/image2label.py,18,"b'# This code is heavily based on the code from TensorFlow official models\n# https://github.com/tensorflow/models/tree/master/official/resnet\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom open_seq2seq.data.data_layer import DataLayer\nfrom .imagenet_preprocessing import parse_record\n\n\nclass CifarDataLayer(DataLayer):\n  _HEIGHT = 28\n  _WIDTH = 28\n  _NUM_CHANNELS = 3\n  _DEFAULT_IMAGE_BYTES = 32 * 32 * 3\n  # The record is the image plus a one-byte label\n  _RECORD_BYTES = _DEFAULT_IMAGE_BYTES + 1\n  _NUM_CLASSES = 10\n  _NUM_DATA_FILES = 5\n\n  _NUM_IMAGES = {\n      \'train\': 50000,\n      \'validation\': 10000,\n  }\n\n  @staticmethod\n  def get_required_params():\n    return dict(DataLayer.get_required_params(), **{\n        \'data_dir\': str,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(DataLayer.get_optional_params(), **{\n        \'num_parallel_calls\': int,\n        \'shuffle_buffer\': int,\n        \'image_size\': int,\n        \'num_classes\': int,\n    })\n\n  def __init__(self, params, model, num_workers, worker_id):\n    super(CifarDataLayer, self).__init__(params, model,\n                                         num_workers, worker_id)\n    if self.params[\'mode\'] == \'infer\':\n      raise ValueError(\'Inference is not supported on CifarDataLayer\')\n\n    if self.params[\'mode\'] == \'train\':\n      filenames = [\n          os.path.join(self.params[\'data_dir\'], \'data_batch_{}.bin\'.format(i))\n          for i in range(1, self._NUM_DATA_FILES + 1)\n      ]\n    else:\n      filenames = [os.path.join(self.params[\'data_dir\'], \'test_batch.bin\')]\n\n    self.file_names = filenames\n    self._train_size = 50000\n    self._valid_size = 10000\n    self._iterator = None\n    self._input_tensors = None\n\n  def preprocess_image(self, image, is_training):\n    """"""Preprocess a single image of layout [height, width, depth].""""""\n    if is_training:\n      # Resize the image to add four extra pixels on each side.\n      image = tf.image.resize_image_with_crop_or_pad(\n          image, self._HEIGHT + 8, self._WIDTH + 8\n      )\n\n      # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n      image = tf.random_crop(image, [self._HEIGHT, self._WIDTH,\n                                     self._NUM_CHANNELS])\n\n      # Randomly flip the image horizontally.\n      image = tf.image.random_flip_left_right(image)\n\n    else:\n      image = tf.image.resize_image_with_crop_or_pad(\n          image, self._HEIGHT, self._WIDTH\n      )\n\n    # Subtract off the mean and divide by the variance of the pixels.\n    image = tf.image.per_image_standardization(image)\n\n    return image\n\n  def parse_record(self, raw_record, is_training, num_classes=10):\n    """"""Parse CIFAR-10 image and label from a raw record.""""""\n    # Convert bytes to a vector of uint8 that is record_bytes long.\n    record_vector = tf.decode_raw(raw_record, tf.uint8)\n\n    # The first byte represents the label, which we convert from uint8 to int32\n    # and then to one-hot.\n    label = tf.cast(record_vector[0], tf.int32)\n\n    # The remaining bytes after the label represent the image, which we reshape\n    # from [depth * height * width] to [depth, height, width].\n    depth_major = tf.reshape(record_vector[1:self._RECORD_BYTES],\n                             [3, 32, 32])\n\n    # Convert from [depth, height, width] to [height, width, depth], and cast as\n    # float32.\n    image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n\n    image = self.preprocess_image(image, is_training)\n    label = tf.one_hot(tf.reshape(label, shape=[]), num_classes)\n\n    return image, label\n\n  def build_graph(self):\n    dataset = tf.data.FixedLengthRecordDataset(self.file_names,\n                                               self._RECORD_BYTES)\n\n    dataset = dataset.prefetch(buffer_size=self.params[\'batch_size\'])\n    if self.params[\'shuffle\']:\n      # shuffling images\n      dataset = dataset.shuffle(buffer_size=self.params.get(\'shuffle_buffer\',\n                                                            1500))\n    dataset = dataset.repeat()\n\n    dataset = dataset.map(\n        lambda value: self.parse_record(\n            raw_record=value,\n            is_training=self.params[\'mode\'] == \'train\',\n        ),\n        num_parallel_calls=self.params.get(\'num_parallel_calls\', 16),\n    )\n\n    dataset = dataset.batch(self.params[\'batch_size\'])\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n\n    self._iterator = dataset.make_initializable_iterator()\n    inputs, labels = self.iterator.get_next()\n    if self.params[\'mode\'] == \'train\':\n      tf.summary.image(\'augmented_images\', inputs, max_outputs=1)\n    self._input_tensors = {\n        \'source_tensors\': [inputs],\n        \'target_tensors\': [labels],\n    }\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  def get_size_in_samples(self):\n    if self.params[\'mode\'] == \'train\':\n      return self._train_size\n    return len(np.arange(self._valid_size)[self._worker_id::self._num_workers])\n\n\nclass ImagenetDataLayer(DataLayer):\n  @staticmethod\n  def get_required_params():\n    return dict(DataLayer.get_required_params(), **{\n        \'data_dir\': str,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(DataLayer.get_optional_params(), **{\n        \'num_parallel_calls\': int,\n        \'shuffle_buffer\': int,\n        \'image_size\': int,\n        \'num_classes\': int,\n    })\n\n  def __init__(self, params, model, num_workers, worker_id):\n    super(ImagenetDataLayer, self).__init__(params, model,\n                                            num_workers, worker_id)\n    if self.params[\'mode\'] == \'infer\':\n      raise ValueError(\'Inference is not supported on ImagenetDataLayer\')\n\n    if self.params[\'mode\'] == \'train\':\n      filenames = [\n          os.path.join(self.params[\'data_dir\'],\n                       \'train-{:05d}-of-01024\'.format(i))\n          for i in range(1024)  # number of training files\n      ]\n    else:\n      filenames = [\n          os.path.join(self.params[\'data_dir\'],\n                       \'validation-{:05d}-of-00128\'.format(i))\n          for i in range(128)  # number of validation files\n      ]\n\n    self._train_size = 1281167\n    self._valid_size = 0\n\n    self.file_names = self.split_data(filenames)\n\n    # TODO: rewrite this somehow?\n    if self.params[\'mode\'] != \'train\':\n      for file_name in self.file_names:\n        for _ in tf.python_io.tf_record_iterator(file_name):\n          self._valid_size += 1\n\n    self._iterator = None\n    self._input_tensors = None\n\n  def build_graph(self):\n    dataset = tf.data.Dataset.from_tensor_slices(self.file_names)\n\n    if self.params[\'shuffle\']:\n      # shuffling input files\n      dataset = dataset.shuffle(buffer_size=1024)\n\n    # convert to individual records\n    dataset = dataset.flat_map(tf.data.TFRecordDataset)\n\n    dataset = dataset.prefetch(buffer_size=self.params[\'batch_size\']*10)\n\n    if self.params[\'mode\'] == \'train\' and self.params[\'shuffle\']:\n      print(""training with shuffle"")\n      # shuffling images\n      dataset = dataset.shuffle(buffer_size=self.params.get(\'shuffle_buffer\',\n                                                            1024))\n    dataset = dataset.repeat()\n\n    dataset = dataset.map(\n        lambda value: parse_record(\n            raw_record=value,\n            is_training=self.params[\'mode\'] == \'train\',\n            image_size=self.params.get(\'image_size\', 224),\n            num_classes=self.params.get(\'num_classes\', 1000),\n        ),\n        num_parallel_calls=self.params.get(\'num_parallel_calls\', 16),\n    )\n\n    dataset = dataset.batch(self.params[\'batch_size\'])\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n\n    self._iterator = dataset.make_initializable_iterator()\n    inputs, labels = self.iterator.get_next()\n    if self.params[\'mode\'] == \'train\':\n      tf.summary.image(\'augmented_images\', inputs, max_outputs=1)\n    self._input_tensors = {\n        \'source_tensors\': [inputs],\n        \'target_tensors\': [labels],\n    }\n\n  def split_data(self, data):\n    if self.params[\'mode\'] != \'train\' and self._num_workers is not None:\n      size = len(data)\n      start = size // self._num_workers * self._worker_id\n      if self._worker_id == self._num_workers - 1:\n        end = size\n      else:\n        end = size // self._num_workers * (self._worker_id + 1)\n      return data[start:end]\n    return data\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  def get_size_in_samples(self):\n    if self.params[\'mode\'] == \'train\':\n      return self._train_size\n    return self._valid_size\n'"
open_seq2seq/data/image2label/imagenet_preprocessing.py,38,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\nTraining images are sampled using the provided bounding boxes, and subsequently\ncropped to the sampled bounding box. Images are additionally flipped randomly,\nthen resized to the target output size (without aspect-ratio preservation).\nImages used during evaluation are resized (with aspect-ratio preservation) and\ncentrally cropped.\nAll images undergo mean color subtraction.\nNote that these steps are colloquially referred to as ""ResNet preprocessing,""\nand they differ from ""VGG preprocessing,"" which does not use bounding boxes\nand instead does an aspect-preserving resize followed by random crop during\ntraining. (These both differ from ""Inception preprocessing,"" which introduces\ncolor distortion steps.)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n_CHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\n\n\ndef _decode_crop_and_flip(image_buffer, bbox, num_channels):\n  """"""Crops the given image to a random part of the image, and randomly flips.\n  We use the fused decode_and_crop op, which performs better than the two ops\n  used separately in series, but note that this requires that the image be\n  passed in as an un-decoded string Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    num_channels: Integer depth of the image buffer for decoding.\n\n  Returns:\n    3-D tensor with cropped image.\n  """"""\n  # A large fraction of image datasets contain a human-annotated bounding box\n  # delineating the region of the image containing the object of interest.  We\n  # choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an\n  # allowed range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n  sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n      tf.image.extract_jpeg_shape(image_buffer),\n      bounding_boxes=bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=[0.75, 1.33],\n      area_range=[0.05, 1.0],\n      max_attempts=100,\n      use_image_if_no_bounding_boxes=True)\n  bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n  # Reassemble the bounding box in the format the crop op requires.\n  offset_y, offset_x, _ = tf.unstack(bbox_begin)\n  target_height, target_width, _ = tf.unstack(bbox_size)\n  crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n  # Use the fused decode and crop op here, which is faster than each in series.\n  cropped = tf.image.decode_and_crop_jpeg(\n      image_buffer, crop_window, channels=num_channels)\n\n  # Flip to add a little more random distortion in.\n  cropped = tf.image.random_flip_left_right(cropped)\n  return cropped\n\n\ndef _central_crop(image, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image: a 3-D image tensor\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    3-D tensor with cropped image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  amount_to_be_cropped_h = (height - crop_height)\n  crop_top = amount_to_be_cropped_h // 2\n  amount_to_be_cropped_w = (width - crop_width)\n  crop_left = amount_to_be_cropped_w // 2\n  return tf.slice(\n      image, [crop_top, crop_left, 0], [crop_height, crop_width, -1])\n\n\ndef _mean_image_subtraction_and_normalization(image, means, num_channels):\n  """"""Subtracts the given means from each image channel and divides by 127.5.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction_and_normalization(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image and normalized image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  means = tf.expand_dims(tf.expand_dims(means, 0), 0)\n\n  return (image - means) / 127.5\n\n\ndef _smallest_size_at_least(height, width, resize_min):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: an int32 scalar tensor indicating the new width.\n  """"""\n  resize_min = tf.cast(resize_min, tf.float32)\n\n  # Convert to floats to make subsequent calculations go smoothly.\n  height, width = tf.cast(height, tf.float32), tf.cast(width, tf.float32)\n\n  smaller_dim = tf.minimum(height, width)\n  scale_ratio = resize_min / smaller_dim\n\n  # Convert back to ints to make heights and widths that TF ops will accept.\n  new_height = tf.cast(height * scale_ratio, tf.int32)\n  new_width = tf.cast(width * scale_ratio, tf.int32)\n\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, resize_min):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)\n\n\ndef _resize_image(image, height, width):\n  """"""Simple wrapper around tf.resize_images.\n  This is primarily to make sure we use the same `ResizeMethod` and other\n  details each time.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    height: The target height for the resized image.\n    width: The target width for the resized image.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image. The first two\n      dimensions have the shape [height, width].\n  """"""\n  return tf.image.resize_images(\n      image, [height, width], method=tf.image.ResizeMethod.BILINEAR,\n      align_corners=False)\n\n\ndef preprocess_image(image_buffer, bbox, output_height, output_width,\n                     num_channels, is_training=False):\n  """"""Preprocesses the given image.\n  Preprocessing includes decoding, cropping, and resizing for both training\n  and eval images. Training preprocessing, however, introduces some random\n  distortion of the image to improve accuracy.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    num_channels: Integer depth of the image buffer for decoding.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    # For training, we want to randomize some of the distortions.\n    image = _decode_crop_and_flip(image_buffer, bbox, num_channels)\n    image = _resize_image(image, output_height, output_width)\n  else:\n    # For validation, we want to decode, resize, then just crop the middle.\n    image = tf.image.decode_jpeg(image_buffer, channels=num_channels)\n    image = _aspect_preserving_resize(image, _RESIZE_MIN)\n    image = _central_crop(image, output_height, output_width)\n\n  image.set_shape([output_height, output_width, num_channels])\n\n  return _mean_image_subtraction_and_normalization(image, _CHANNEL_MEANS,\n                                                   num_channels)\n\n\ndef _parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields (values are included as examples):\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/class/label\': tf.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n  }\n  sparse_float32 = tf.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.parse_single_example(example_serialized, feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(bbox, [0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox\n\n\ndef parse_record(raw_record, is_training, image_size=224, num_classes=1000):\n  """"""Parses a record containing a training example of an image.\n  The input record is parsed into a label and image, and the image is passed\n  through preprocessing steps (cropping, flipping, and so on).\n\n  Args:\n    raw_record: scalar Tensor tf.string containing a serialized\n        Example protocol buffer.\n    is_training: A boolean denoting whether the input is for training.\n    image_size (int): size that images should be resized to.\n    num_classes (int): number of output classes.\n\n  Returns:\n    Tuple with processed image tensor and one-hot-encoded label tensor.\n  """"""\n  image_buffer, label, bbox = _parse_example_proto(raw_record)\n\n  image = preprocess_image(\n      image_buffer=image_buffer,\n      bbox=bbox,\n      output_height=image_size,\n      output_width=image_size,\n      num_channels=3,\n      is_training=is_training)\n\n  # subtracting 1 to make labels go from 0 to 999\n  label = tf.one_hot(tf.reshape(label - 1, shape=[]), num_classes)\n\n  return image, label\n'"
open_seq2seq/data/lm/__init__.py,0,b'\n'
open_seq2seq/data/lm/lmdata.py,13,"b'# Copyright (c) 2018 NVIDIA Corporation\n# -*- coding: utf-8 -*-\nimport random\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom enum import Enum\nfrom open_seq2seq.data.data_layer import DataLayer\nfrom open_seq2seq.data.utils import load_pre_existing_vocabulary, pad_vocab_to_eight\nfrom open_seq2seq.data.text2text.t2t import _read_and_batch_from_files\n\nfrom open_seq2seq.data.lm.lmutils import Dictionary, Corpus, IMDBCorpus, SSTCorpus\n\nclass WKTDataLayer(DataLayer):\n  \'\'\'\n  WKTDataLayer does the necessary pre-processing to make the WikiText datasets \n  ready to be fed into the model. We use the ``word_token`` method \n  available in the ``nltk`` package. \n  You can download the datasets here:\n  https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\n  bptt: backpropagation through time - the length of the sequences used for training\n  rand_start: whether to start from a random starting index between (0, bptt)\n  \'\'\'\n  @staticmethod\n  def get_required_params():\n    return dict(DataLayer.get_required_params(), **{\n      \'repeat\': bool,\n      \'bptt\': int,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(DataLayer.get_optional_params(), **{\n      \'data_root\': str,\n      \'rand_start\': bool,\n      \'small\': bool,\n      \'use_targets\': bool,\n      \'delimiter\': str,\n      \'map_parallel_calls\': int,\n      \'prefetch_buffer_size\': int,\n      \'pad_lengths_to_eight\': bool,\n      \'pad_vocab_to_eight\': bool,\n      \'seed_tokens\': str,\n      \'shuffle_buffer_size\': int,\n      \'processed_data_folder\': str,\n    })\n\n  def __init__(self, params, model, num_workers=1, worker_id=0):\n    super(WKTDataLayer, self).__init__(params, model,\n                                          num_workers, worker_id)\n\n    self._processed_data_folder = self.params.get(\'processed_data_folder\', \'wkt-processed_data\')\n    self._data_root = self.params.get(\'data_root\', None)\n\n    self.corp = Corpus(self._data_root, self._processed_data_folder)\n\n    seed_tokens = self.params.get(\'seed_tokens\', \'The\').split()\n    \n    self.end_token = self.corp.dictionary.word2idx[self.corp.dictionary.EOS]\n    self.params[\'seed_tokens\'] = [self.corp.dictionary.word2idx[seed_token] for seed_token in seed_tokens]\n    \n    if self.params[\'mode\'] == \'infer\':\n      self.corp.content = self.params[\'seed_tokens\']\n\n    if self.params[\'mode\'] == \'train\':\n      self.batch_size = self.params[\'batch_size\']\n      self.corp.content = self.corp.train\n    elif self.params[\'mode\'] == \'eval\':\n      self.batch_size = self.params[\'batch_size\']\n      self.corp.content = self.corp.valid\n    else:\n      if len(self.corp.content) < self.params[\'batch_size\']:\n        self.batch_size = len(self.corp.content)\n      else:\n        self.batch_size = self.params[\'batch_size\']\n\n    self.vocab_file = (self._processed_data_folder, \'vocab.txt\')\n    self.bptt = self.params[\'bptt\']\n    self.rand_start = self.params.get(\'rand_start\', False)\n    self._map_parallel_calls = self.params.get(\'map_parallel_calls\', 8)\n    self._pad_lengths_to_eight = self.params.get(\'pad_lengths_to_eight\', False)\n    self._prefetch_buffer_size = self.params.get(\'prefetch_buffer_size\',\n                                                 tf.contrib.data.AUTOTUNE)\n    self._shuffle_buffer_size = self.params.get(\'shuffle_buffer_size\', -1)\n    self._num_workers = num_workers\n    self._worker_id = worker_id\n    self.delimiter = self.params.get(""delimiter"", "" "")\n    self._small = self.params.get(""small"", False)\n    self.start = 0\n\n    # load source and target vocabularies to RAM\n    if self._small:\n      if self.params[\'mode\'] == \'eval\':\n        self.corp.content = self.corp.content[:200]\n      else:\n        self.corp.content = self.corp.content[:9004]\n\n    if self.params.get(\'pad_vocab_to_eight\', False):\n      self.corp.content = pad_vocab_to_eight(self.corp.content)\n\n    self.dataset_size = len(self.corp.content)\n    self.vocab_size = len(self.corp.dictionary.idx2word)\n    self._input_tensors = {}\n\n  def gen(self):\n    while True:\n      if self.rand_start:\n        self.start = random.randint(0, self.bptt - 1)\n\n      n_samples = (self.dataset_size - self.start - 1) // self.bptt\n\n      for i in range(n_samples):\n        begin = self.start + i * self.bptt\n        yield (self.corp.content[begin : begin + self.bptt], self.corp.content[begin + 1 : begin + self.bptt + 1])\n\n  def gen_infer(self):\n    while True:\n      for seed in self.corp.content:\n        yield ([seed], [seed])\n      \n  def build_graph(self):\n    if self.params[\'mode\'] == \'train\' or self.params[\'mode\'] == \'eval\':\n      gen = self.gen\n      batch_shape = self.bptt\n    else:\n      gen = self.gen_infer\n      batch_shape = 1\n    \n    _src_tgt_dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32), \n                                (tf.TensorShape([batch_shape]), tf.TensorShape([batch_shape])))\n\n    if self._num_workers > 1:\n      _src_tgt_dataset = _src_tgt_dataset\\\n        .shard(num_shards=self._num_workers, index=self._worker_id)\n\n    if self.params[\'shuffle\']:\n      bf_size = self.get_size_in_samples() if self._shuffle_buffer_size == -1 \\\n                                           else self._shuffle_buffer_size\n      _src_tgt_dataset = _src_tgt_dataset.shuffle(buffer_size=bf_size)\n\n    else:\n      _src_tgt_dataset = _src_tgt_dataset\n\n    if self.params[\'repeat\']:\n      _src_tgt_dataset = _src_tgt_dataset.repeat()\n\n    _src_tgt_dataset = _src_tgt_dataset.map(lambda x, y: ((x, tf.size(x)), (y, tf.size(y))), \n                            num_parallel_calls=self._map_parallel_calls)\n\n    self.batched_dataset = _src_tgt_dataset.batch(self.batch_size)\n\n    self._iterator = self.batched_dataset.make_initializable_iterator()\n\n    if self.params[\'mode\'] == \'train\' or self.params[\'mode\'] == \'eval\':\n      t1, t2 = self.iterator.get_next()\n      x, x_length = t1[0], t1[1]\n      y, y_length = t2[0], t2[1]\n      self._input_tensors[\'source_tensors\'] = [x, x_length]\n      self._input_tensors[\'target_tensors\'] = [y, y_length]\n    else: # this is unncessary\n      t1, _ = self.iterator.get_next()\n      self._input_tensors[\'source_tensors\'] = [t1[0], t1[1]]\n\n  def get_size_in_samples(self):\n    if self.params[\'mode\'] == \'train\' or self.params[\'mode\'] == \'eval\':\n      return (self.dataset_size - self.start) // self.bptt\n    return len(self.corp.content)\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n\nclass TextClassificationDataLayer(DataLayer):\n  \'\'\'\n  The base ckass to process data for text classification tasks.\n  If the data has already been processed, it shoud load the processed\n  data instead of re-processing it.\n  \'\'\'\n  @staticmethod\n  def get_required_params():\n    return dict(DataLayer.get_required_params(), **{\n      \'lm_vocab_file\': str,\n      \'shuffle\': bool,\n      \'repeat\': bool,\n      \'max_length\': int,\n      \'processed_data_folder\': str,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(DataLayer.get_optional_params(), **{\n      \'rand_start\': bool,\n      \'small\': bool,\n      \'use_targets\': bool,\n      \'delimiter\': str,\n      \'map_parallel_calls\': int,\n      \'prefetch_buffer_size\': int,\n      \'pad_lengths_to_eight\': bool,\n      \'pad_vocab_to_eight\': bool,\n      \'shuffle_buffer_size\': int,\n      \'data_root\': str,\n      \'binary\': bool,\n      \'num_classes\': int,\n      \'get_stats\': bool,\n    })\n\n  def __init__(self, params, model, num_workers=1, worker_id=0):\n    super(TextClassificationDataLayer, self).__init__(params, model,\n                                          num_workers, worker_id)\n\n    self._data_root = self.params.get(\'data_root\', None)\n    self._binary = self.params.get(\'binary\', True)\n    self._get_stats = self.params.get(\'get_stats\', False)\n    self._lm_vocab_file = self.params[\'lm_vocab_file\']\n\n    self._map_parallel_calls = self.params.get(\'map_parallel_calls\', 8)\n    self._pad_lengths_to_eight = self.params.get(\'pad_lengths_to_eight\', False)\n    self._prefetch_buffer_size = self.params.get(\'prefetch_buffer_size\',\n                                                 tf.contrib.data.AUTOTUNE)\n    self._shuffle_buffer_size = self.params.get(\'shuffle_buffer_size\', -1)\n    self._num_workers = num_workers\n    self._worker_id = worker_id\n    self._small = self.params.get(""small"", False)\n    self._max_length = self.params[\'max_length\']\n    self.delimiter = self.params.get(""delimiter"", "" "")\n    self.EOS_ID = -1\n    self.batch_size = self.params[\'batch_size\']\n\n    if self._pad_lengths_to_eight and not (self._max_length % 8 == 0):\n      raise ValueError(""If padding to 8 in data layer, then ""\n                       ""max_length should be multiple of 8"")\n    self._input_tensors = {}\n\n  def gen(self):\n    while True:\n      for review, raw_rating in self.corp.content:\n        if len(review) > self._max_length:\n          review = review[-self._max_length:]\n        rating = np.zeros(self.num_classes)\n        rating[raw_rating] = 1\n        yield (review, rating)\n\n  def build_graph(self):\n    _src_tgt_dataset = tf.data.Dataset.from_generator(self.gen, \n                                        (tf.int32, tf.int32), \n                                        (tf.TensorShape([None]), tf.TensorShape([self.num_classes])))\n\n    if self._num_workers > 1:\n      _src_tgt_dataset = _src_tgt_dataset\\\n        .shard(num_shards=self._num_workers, index=self._worker_id)\n\n    if self.params[\'shuffle\']:\n      bf_size = self.get_size_in_samples() if self._shuffle_buffer_size == -1 \\\n                                           else self._shuffle_buffer_size\n      _src_tgt_dataset = _src_tgt_dataset.shuffle(buffer_size=bf_size)\n\n    if self.params[\'repeat\']:\n      _src_tgt_dataset = _src_tgt_dataset.repeat()\n\n    _src_tgt_dataset = _src_tgt_dataset.map(lambda x, y: ((x, tf.size(x)), (y, tf.size(y))), \n                            num_parallel_calls=self._map_parallel_calls)\n\n    self.batched_dataset = _src_tgt_dataset.padded_batch(\n      self.batch_size,\n      padded_shapes=((tf.TensorShape([None]),\n                      tf.TensorShape([])),\n                     (tf.TensorShape([None]),\n                      tf.TensorShape([]))),\n      padding_values=(\n      (self.EOS_ID, 0),\n      (self.EOS_ID, 0))).prefetch(buffer_size=self._prefetch_buffer_size)\n\n    self._iterator = self.batched_dataset.make_initializable_iterator()\n\n    t1, t2 = self.iterator.get_next()\n    x, x_length = t1[0], t1[1]\n    y, y_length = t2[0], t2[1]\n    self._input_tensors[\'source_tensors\'] = [x, x_length]\n    self._input_tensors[\'target_tensors\'] = [y, y_length]\n\n  def get_size_in_samples(self):\n    return self.dataset_size\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n\nclass IMDBDataLayer(TextClassificationDataLayer):\n  \'\'\'\n  Data layer to process the raw IMDB data, which can be downloaded here:\n  http://ai.stanford.edu/~amaas/data/sentiment/\n\n  \'\'\'\n  def __init__(self, params, model, num_workers=1, worker_id=0):\n    super(IMDBDataLayer, self).__init__(params, model, num_workers, worker_id)\n    self._processed_data_folder = self.params[\'processed_data_folder\']\n\n    if self._binary:\n      self.num_classes = 2\n    else:\n      self.num_classes = 10\n\n    self.corp = IMDBCorpus(self._data_root, \n                          self._processed_data_folder, \n                          self._lm_vocab_file, \n                          self._binary,\n                          get_stats=self._get_stats)\n    \n    if self.params[\'mode\'] == \'train\':\n      self.corp.content = self.corp.train\n    elif self.params[\'mode\'] == \'eval\':\n      self.corp.content = self.corp.valid\n    else:\n      self.corp.content = self.corp.test    \n    \n    if self._small:\n      if self.params[\'mode\'] == \'eval\':\n        self.corp.content = self.corp.content[:self.batch_size * 2]\n      else:\n        self.corp.content = self.corp.content[:self.batch_size * 4]\n\n    self.dataset_size = len(self.corp.content)\n    self.vocab_size = len(self.corp.dictionary.idx2word)\n    self.EOS_ID = self.corp.dictionary.word2idx[self.corp.dictionary.EOS]\n    self.end_token = self.corp.dictionary.word2idx[self.corp.dictionary.EOS]\n\nclass SSTDataLayer(TextClassificationDataLayer):\n  \'\'\'\n  Data layer to process the raw SST (Stanford Sentiment Treebank).\n  Read about the dataset here:\n  https://nlp.stanford.edu/sentiment/\n  Download the preprocessed version that can be used for this DataLayer here:\n  https://github.com/NVIDIA/sentiment-discovery/tree/master/data/binary_sst\n  \'\'\'\n  def __init__(self, params, model, num_workers=1, worker_id=0):\n    super(SSTDataLayer, self).__init__(params, model, num_workers, worker_id)\n    self._processed_data_folder = self.params[\'processed_data_folder\']\n    self.corp = SSTCorpus(self._data_root, \n                          self._processed_data_folder, \n                          self._lm_vocab_file,\n                          get_stats=self._get_stats)\n    \n    if self.params[\'mode\'] == \'train\':\n      self.corp.content = self.corp.train\n    elif self.params[\'mode\'] == \'eval\':\n      self.corp.content = self.corp.valid\n    else:\n      self.corp.content = self.corp.test\n    self.num_classes = 2\n    self.dataset_size = len(self.corp.content)\n    self.vocab_size = len(self.corp.dictionary.idx2word)\n    self.EOS_ID = self.corp.dictionary.word2idx[self.corp.dictionary.EOS]\n    self.end_token = self.corp.dictionary.word2idx[self.corp.dictionary.EOS]\n'"
open_seq2seq/data/lm/lmutils.py,0,"b'# -*- coding: utf-8 -*-\nfrom collections import Counter\nimport glob\nimport os\nimport pathlib\nimport random\nimport re\nimport shutil\n\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport pandas as pd\n\nclass Dictionary(object):\n  \'\'\'\n  Adapted from salesforce\'s repo:\n  https://github.com/salesforce/awd-lstm-lm/blob/master/data.py\n  \'\'\'\n  def __init__(self, limit=3, vocab_link=None): # do we need limit?\n    self.word2idx = {}\n    self.idx2word = []\n    self.counter = Counter()\n    self.UNK = \'<unk>\'\n    self.EOS = \'<eos>\'\n    if vocab_link and os.path.isfile(vocab_link):\n      self.load_vocab(vocab_link)\n\n  def add_word(self, word):\n    if word not in self.word2idx:\n      self.idx2word.append(word)\n      self.word2idx[word] = len(self.idx2word) - 1\n    token_id = self.word2idx[word]\n    self.counter[token_id] += 1\n    return self.word2idx[word]\n\n  def load_vocab(self, vocab_link):\n    vocab_file = open(vocab_link, \'r\')\n    lines = vocab_file.readlines()\n    n = int(lines[-1].strip())\n    self.idx2word = [0 for _ in range(n)]\n    for line in lines[:-1]:\n      parts = line.strip().split(\'\\t\')\n      token_id, word, count = int(parts[0]), parts[1], int(parts[2]) \n      self.word2idx[word] = token_id\n      self.idx2word[token_id] = word\n      self.counter[token_id] = count\n    if not self.UNK in self.word2idx:\n      self.add_word(self.UNK)\n    if not self.EOS in self.word2idx:\n      self.add_word(self.EOS)\n\n\n  def __len__(self):\n    return len(self.idx2word)\n\ndef check_exist(proc_path):\n  filenames = [\'train.ids\', \'valid.ids\', \'test.ids\']\n  paths = [os.path.join(proc_path, name) for name in filenames]\n  paths.append(proc_path)\n  for name in paths:\n    if not os.path.exists(name):\n      return False\n  return True\n\ndef list2str(list):\n  return \'\\t\'.join([str(num) for num in list])\n\ndef unzip(data):\n  tmp = [list(t) for t in zip(*data)]\n  return (tmp[0], tmp[1])\n\nclass Corpus(object):\n  def __init__(self, raw_path, proc_path, change_contraction=True, limit=3):\n    pathlib.Path(proc_path).mkdir(exist_ok=True)\n    self.limit = limit\n    self.dictionary = Dictionary(limit)\n    self.vocab_link = \'vocab.txt\'\n    exists = check_exist(proc_path)\n    self.change_contraction = change_contraction\n\n    if not exists:\n      print(\'Creating corpus from raw data ...\')\n      if raw_path and \'raw\' in raw_path:\n        self._change_names(raw_path)\n      if not raw_path:\n        raise ValueError(""data_root [directory to the original data] must be specified"")\n      self.preprocess(raw_path, proc_path)\n      self.create_dictionary(proc_path, os.path.join(proc_path, \'train.txt\'))\n      self.dictionary = Dictionary(limit)\n      self.dictionary.load_vocab(os.path.join(proc_path, self.vocab_link))\n      self.train = self.tokenize(proc_path, proc_path, \'train.txt\')\n      self.valid = self.tokenize(proc_path, proc_path, \'valid.txt\')\n      self.test = self.tokenize(proc_path, proc_path, \'test.txt\')\n    else:\n      self.load_corpus(proc_path)\n\n  def _change_names(self, raw_path):\n    if os.path.isfile(os.path.join(raw_path, \'wiki.train.raw\')):\n      os.rename(os.path.join(raw_path, \'wiki.train.raw\'), os.path.join(raw_path, \'train.txt\'))\n      os.rename(os.path.join(raw_path, \'wiki.valid.raw\'), os.path.join(raw_path, \'valid.txt\'))\n      os.rename(os.path.join(raw_path, \'wiki.test.raw\'), os.path.join(raw_path, \'test.txt\'))\n\n  def preprocess(self, raw_path, proc_path):\n    for filename in [\'train.txt\', \'valid.txt\', \'test.txt\']:\n      in_ = open(os.path.join(raw_path, filename), \'r\')\n      out = open(os.path.join(proc_path, filename), \'w\')\n      for line in in_:\n        line = re.sub(\'@-@\', \'-\', line)\n        line = re.sub(\'-\', \' - \', line)\n        line = re.sub(\'etc .\', \'etc.\', line)\n        if self.change_contraction:\n          line = re.sub(""n \'t"", "" n\'t"", line)\n        tokens = []\n        for token in line.split():\n          tokens.append(token.strip())\n        out.write(\' \'.join(tokens) + \'\\n\')\n\n  def create_dictionary(self, proc_path, filename):\n    \'\'\'\n    Add words to the dictionary only if it\'s in the train file\n    \'\'\'\n    self.dictionary.add_word(self.dictionary.UNK)\n    with open(filename, \'r\') as f:\n      f.readline()\n      for line in f:\n        words = line.split() + [self.dictionary.EOS]\n        for word in words:\n          self.dictionary.add_word(word)\n\n    with open(os.path.join(proc_path, self.vocab_link), \'w\') as f:\n      f.write(\'\\t\'.join([\'0\', self.dictionary.UNK, \'0\']) + \'\\n\')\n      idx = 1\n      for token_id, count in self.dictionary.counter.most_common():\n        if count < self.limit:\n          f.write(str(idx) + \'\\n\')\n          return\n        f.write(\'\\t\'.join([str(idx), \n              self.dictionary.idx2word[token_id], \n              str(count)]) + \'\\n\')\n        idx += 1\n      \n  def tokenize(self, raw_path, proc_path, filename):\n    unk_id = self.dictionary.word2idx[self.dictionary.UNK]\n    out = open(os.path.join(proc_path, filename[:-3] + \'ids\'), \'w\')\n    with open(os.path.join(raw_path, filename), \'r\') as f:\n      ids = []\n      for line in f:\n        words = line.split() + [self.dictionary.EOS]\n        for word in words:\n          ids.append(self.dictionary.word2idx.get(word, unk_id))\n    out.write(list2str(ids))\n    out.close()\n\n    return np.asarray(ids)\n\n  def load_ids(self, filename):\n    ids = open(filename, \'r\').read().strip().split(\'\\t\')\n    return np.asarray([int(i) for i in ids])\n\n  def list2str(self, list):\n    return \'\\t\'.join([str(num) for num in list])\n\n  def load_corpus(self, proc_path):\n    print(\'Loading corpus from processed data ...\')\n    self.dictionary.load_vocab(os.path.join(proc_path, self.vocab_link))\n    self.train = self.load_ids(os.path.join(proc_path, \'train.ids\'))\n    self.valid = self.load_ids(os.path.join(proc_path, \'valid.ids\'))\n    self.test = self.load_ids(os.path.join(proc_path, \'test.ids\'))\n\nclass IMDBCorpus(object):\n  def __init__(self, raw_path, proc_path, lm_vocab_link, binary=True, get_stats=False):\n    exists = check_exist(proc_path)\n    pathlib.Path(proc_path).mkdir(exist_ok=True)\n    self.dictionary = Dictionary(vocab_link=lm_vocab_link)\n    self.binary = binary\n    self.raw_path = raw_path\n    self.proc_path = proc_path\n    self._get_stats = get_stats\n\n    if not exists:\n      print(\'Creating corpus from raw data ...\')\n      if not raw_path:\n        raise ValueError(""data_root [directory to the original data] must be specified"")\n      self.preprocess()\n    else:\n      self.load_corpus(proc_path)\n\n  def check_oov(self, txt):\n    txt = txt.lower()\n    txt = re.sub(\'thats\', ""that\'s"", txt)\n    txt = re.sub(\'wouldnt\', ""wounldn\'t"", txt)\n    txt = re.sub(\'couldnt\', ""couldn\'t"", txt)\n    txt = re.sub(\'cant\', ""can\'t"", txt)\n    txt = re.sub(\'dont\', ""don\'t"", txt)\n    txt = re.sub(""didnt"", ""didn\'t"", txt)\n    txt = re.sub(""isnt"", ""isn\'t"", txt)\n    txt = re.sub(""wasnt"", ""wasn\'t"", txt)\n    return word_tokenize(txt)\n\n  def tokenize(self, txt):\n    txt = re.sub(\'<br />\', \' \', txt)\n    txt = re.sub(\'\xc2\x96\', \' \', txt)\n    txt = re.sub(\'\xc2\x97\', \' \', txt)\n    txt = re.sub(\'-\', \' - \', txt)\n    txt = re.sub(\'\\.\', \' . \', txt)\n    txt = re.sub(\'\\+\', \' + \', txt)\n    txt = re.sub(\'\\*\', \' * \', txt)\n    txt = re.sub(\'/\', \' / \', txt)\n    txt = re.sub(\'`\', ""\'"", txt)\n    txt = re.sub(\' ms \\.\', "" ms."", txt)\n    txt = re.sub(\'Ms \\.\', ""Ms."", txt)\n    \n    words = []\n    for token in word_tokenize(txt):\n      if not token in self.dictionary.word2idx:\n        if token.startswith(""\'""):\n          words.append(""\'"")\n          token = token[1:]\n        if not token in self.dictionary.word2idx:\n          tokens = self.check_oov(token)\n          words.extend(tokens)\n        else:\n          words.append(token)\n      else:\n        words.append(token) \n    \n    txt = \' \'.join(words)\n    txt = re.sub(""\'\'"", \'""\', txt)\n    txt = re.sub(""\' \'"", \'""\', txt)\n    txt = re.sub(""``"", \'""\', txt)\n    txt = re.sub(\'etc \\.\', \'etc. \', txt)\n    txt = re.sub(\' etc \', \' etc. \', txt)\n    return txt\n\n  def tokenize_folder(self, mode, token_file, rating_file):\n    review_outfile = open(token_file, \'w\')\n    rating_outfile = open(rating_file, \'w\')\n    for sent in [\'pos\', \'neg\']:\n      files = glob.glob(os.path.join(self.raw_path, mode, sent, \'*.txt\'))\n      for file in files:\n        in_file = open(file, \'r\')\n        txt = self.tokenize(in_file.read())\n        review_outfile.write(txt + ""\\n"")\n        if self.binary:\n          if sent == \'pos\':\n            rating = ""1""\n          else:\n            rating = ""0""\n        else:\n          idx = file.rfind(""_"")\n          rating = str(int(file[idx + 1:-4]) - 1)\n        rating_outfile.write(rating + \'\\n\')\n        in_file.close()\n\n  def txt2ids(self, mode, token_file, rating_file):\n    if self._get_stats:\n      import matplotlib\n      matplotlib.use(""TkAgg"")\n      from matplotlib import pyplot as plt\n    rating_lines = open(rating_file, \'r\').readlines()\n    ratings = [int(line.strip()) for line in rating_lines]\n    reviews = []\n    unk_id = self.dictionary.word2idx[self.dictionary.UNK]\n    unseen = []\n    all_tokens = 0\n    all_unseen = 0\n    for line in open(token_file, \'r\'):\n      tokens = line.strip().split()\n      reviews.append([self.dictionary.word2idx.get(token, unk_id) for token in tokens])\n      if self._get_stats:\n        for token in tokens:\n          all_tokens += 1\n          if not token in self.dictionary.word2idx:\n            unseen.append(token)\n            all_unseen += 1\n\n    if self._get_stats:\n      counter = Counter(unseen)\n\n      out = open(os.path.join(self.proc_path, mode + \'_unseen.txt\'), \'w\')\n      for key, count in counter.most_common():\n          out.write(key + \'\\t\' + str(count) + \'\\n\')\n\n      lengths = np.asarray([len(review) for review in reviews])\n      stat_file = open(os.path.join(self.proc_path, \'statistics.txt\'), \'w\')\n      stat_file.write(mode + \'\\n\')\n      short_lengths = [l for l in lengths if l <= 256]\n      stat_file.write(\'\\t\'.join([\'Min\', \'Max\', \'Mean\', \'Median\', \'STD\', \'Total\', \'<=256\']) + \'\\n\')\n      stats = [np.min(lengths), np.max(lengths), np.mean(lengths), np.median(lengths), np.std(lengths), len(lengths), len(short_lengths)]\n      stat_file.write(\'\\t\'.join([str(t) for t in stats]) + \'\\n\')\n      stat_file.write(\'Total {} unseen out of {} all tokens. Probability {}.\\n\'.\n        format(all_unseen, all_tokens, all_unseen / all_tokens))\n      plt.hist(lengths, bins=20)\n      plt.savefig(os.path.join(self.proc_path, mode + \'_hist.png\'))\n      plt.hist(short_lengths, bins=20)\n      plt.savefig(os.path.join(self.proc_path, mode + \'_short_hist.png\'))\n\n    return list(zip(reviews, ratings))\n\n  def preprocess_folder(self, mode):\n    token_file = os.path.join(self.proc_path, mode + \'.tok\')\n    rating_file = os.path.join(self.proc_path, mode + \'.inter.rat\')\n    self.tokenize_folder(mode, token_file, rating_file)\n    return self.txt2ids(mode, token_file, rating_file)\n\n  def partition(self, data, val_count=1000):\n    random.shuffle(data)\n    return data[val_count:], data[:val_count]\n\n  def ids2file(self):\n    for mode in [\'train\', \'valid\', \'test\']:\n      data = getattr(self, mode)\n      review_out = open(os.path.join(self.proc_path, mode + \'.ids\'), \'w\')\n      rating_out = open(os.path.join(self.proc_path, mode + \'.rat\'), \'w\')\n      for review, rating in data:\n        review_out.write(list2str(review) + \'\\n\')\n        rating_out.write(str(rating) + \'\\n\')\n\n  def preprocess(self):\n    os.makedirs(self.proc_path, exist_ok=True)\n    train = self.preprocess_folder(\'train\')\n    self.train, self.valid = self.partition(train)\n    self.test = self.preprocess_folder(\'test\')\n    self.ids2file()\n\n  def load_ids(self, mode):\n    review_lines = open(os.path.join(self.proc_path, mode + \'.ids\')).readlines()\n    rating_lines = open(os.path.join(self.proc_path, mode + \'.rat\')).readlines()\n    ratings = [int(line.strip()) for line in rating_lines]\n    reviews = [[int(i) for i in line.strip().split(\'\\t\')] for line in review_lines]\n    return list(zip(reviews, ratings))\n\n  def load_corpus(self, proc_path):\n    print(\'Loading corpus from processed data ...\')\n    self.train = self.load_ids(\'train\')\n    self.valid = self.load_ids(\'valid\')\n    self.test = self.load_ids(\'test\')\n\nclass SSTCorpus(object):\n  def __init__(self, raw_path, proc_path, lm_vocab_link, get_stats=False):\n    exists = check_exist(proc_path)\n    pathlib.Path(proc_path).mkdir(exist_ok=True)\n    self.dictionary = Dictionary(vocab_link=lm_vocab_link)\n    self.raw_path = raw_path\n    self.proc_path = proc_path\n    self._get_stats = get_stats\n\n    if not exists:\n      print(\'Creating corpus from raw data ...\')\n      if not raw_path:\n        raise ValueError(""data_root [directory to the original data] must be specified"")\n      self.preprocess()\n    else:\n      self.load_corpus(proc_path)\n\n  def check_oov(self, txt):\n    txt = txt.lower()\n    txt = re.sub(\'thats\', ""that\'s"", txt)\n    txt = re.sub(\'wouldnt\', ""wounldn\'t"", txt)\n    txt = re.sub(\'couldnt\', ""couldn\'t"", txt)\n    txt = re.sub(\'cant\', ""can\'t"", txt)\n    txt = re.sub(\'dont\', ""don\'t"", txt)\n    txt = re.sub(""didnt"", ""didn\'t"", txt)\n    txt = re.sub(""isnt"", ""isn\'t"", txt)\n    txt = re.sub(""wasnt"", ""wasn\'t"", txt)\n    return word_tokenize(txt)\n\n  def tokenize(self, txt):\n    txt = re.sub(\'-\', \' - \', txt)\n    txt = re.sub(\'\\+\', \' + \', txt)\n    txt = re.sub(\'\\*\', \' * \', txt)\n    txt = re.sub(\'/\', \' / \', txt)\n    txt = re.sub(\'`\', ""\'"", txt)\n    \n    words = []\n    for token in word_tokenize(txt):\n      if not token in self.dictionary.word2idx:\n        if token.startswith(""\'""):\n          words.append(""\'"")\n          token = token[1:]\n        if not token in self.dictionary.word2idx:\n          tokens = self.check_oov(token)\n          words.extend(tokens)\n        else:\n          words.append(token)\n      else:\n        words.append(token) \n    \n    txt = \' \'.join(words)\n    txt = re.sub(""\'\'"", \'""\', txt)\n    txt = re.sub(""\' \'"", \'""\', txt)\n    txt = re.sub(""``"", \'""\', txt)\n    txt = re.sub(\'etc \\.\', \'etc. \', txt)\n    txt = re.sub(\' etc \', \' etc. \', txt)\n    return txt\n\n  def tokenize_file(self, mode):\n    data = pd.read_csv(os.path.join(self.raw_path, mode + \'.csv\'))\n\n    if mode == \'val\':\n      mode = \'valid\'\n    review_file = open(os.path.join(self.proc_path, mode + \'.tok\'), \'w\')\n    rating_file = open(os.path.join(self.proc_path, mode + \'.rat\'), \'w\')\n    for _, row in data.iterrows():\n      review = self.tokenize(row[\'sentence\'])\n      review_file.write(review + \'\\n\')\n      rating_file.write(str(row[\'label\']) + \'\\n\')\n\n  def txt2ids(self, mode):\n    if self._get_stats:\n      import matplotlib\n      matplotlib.use(""TkAgg"")\n      from matplotlib import pyplot as plt\n\n    reviews = []\n    unk_id = self.dictionary.word2idx[self.dictionary.UNK]\n    unseen = []\n    all_tokens = 0\n    all_unseen = 0\n\n    rating_lines = open(os.path.join(self.proc_path, mode + \'.rat\'), \'r\').readlines()\n    ratings = [int(line.strip()) for line in rating_lines]\n\n    for line in open(os.path.join(self.proc_path, mode + \'.tok\'), \'r\'):\n      tokens = line.strip().split()\n      reviews.append([self.dictionary.word2idx.get(token, unk_id) for token in tokens])\n      if self._get_stats:\n        for token in tokens:\n          all_tokens += 1\n          if not token in self.dictionary.word2idx:\n            unseen.append(token)\n            all_unseen += 1\n\n    if self._get_stats:\n      counter = Counter(unseen)\n\n      out = open(os.path.join(self.proc_path, mode + \'_unseen.txt\'), \'w\')\n      for key, count in counter.most_common():\n          out.write(key + \'\\t\' + str(count) + \'\\n\')\n\n      lengths = np.asarray([len(review) for review in reviews])\n      stat_file = open(os.path.join(self.proc_path, \'statistics.txt\'), \'a\')\n      stat_file.write(mode + \'\\n\')\n      short_lengths = [l for l in lengths if l <= 96]\n      stat_file.write(\'\\t\'.join([\'Min\', \'Max\', \'Mean\', \'Median\', \'STD\', \'Total\', \'<=96\']) + \'\\n\')\n      stats = [np.min(lengths), np.max(lengths), np.mean(lengths), np.median(lengths), np.std(lengths), len(lengths), len(short_lengths)]\n      stat_file.write(\'\\t\'.join([str(t) for t in stats]) + \'\\n\')\n      stat_file.write(\'Total {} unseen out of {} all tokens. Probability {}.\\n\'.\n        format(all_unseen, all_tokens, all_unseen / all_tokens))\n      plt.hist(lengths, bins=20)\n      plt.savefig(os.path.join(self.proc_path, mode + \'_hist.png\'))\n      plt.hist(short_lengths, bins=20)\n      plt.savefig(os.path.join(self.proc_path, mode + \'_short_hist.png\'))\n\n    return list(zip(reviews, ratings))\n\n  def preprocess_file(self, mode):\n    self.tokenize_file(mode)\n    if mode == \'val\':\n      mode = \'valid\'\n    return self.txt2ids(mode)\n\n  def ids2file(self):\n    for mode in [\'train\', \'valid\', \'test\']:\n      data = getattr(self, mode)\n      review_out = open(os.path.join(self.proc_path, mode + \'.ids\'), \'w\')\n      rating_out = open(os.path.join(self.proc_path, mode + \'.rat\'), \'w\')\n      for review, rating in data:\n        review_out.write(list2str(review) + \'\\n\')\n        rating_out.write(str(rating) + \'\\n\')\n\n  def preprocess(self):\n    os.makedirs(self.proc_path, exist_ok=True)\n    self.train = self.preprocess_file(\'train\')\n    self.valid = self.preprocess_file(\'val\')\n    self.test = self.preprocess_file(\'test\')\n    self.ids2file()\n\n  def load_ids(self, mode):\n    review_lines = open(os.path.join(self.proc_path, mode + \'.ids\')).readlines()\n    rating_lines = open(os.path.join(self.proc_path, mode + \'.rat\')).readlines()\n    ratings = [int(line.strip()) for line in rating_lines]\n    reviews = [[int(i) for i in line.strip().split(\'\\t\')] for line in review_lines]\n    return list(zip(reviews, ratings))\n\n  def load_corpus(self, proc_path):\n    print(\'Loading corpus from processed data ...\')\n    self.train = self.load_ids(\'train\')\n    self.valid = self.load_ids(\'valid\')\n    self.test = self.load_ids(\'test\')\n\n# SSTCorpus(\'/home/chipn/data/binary_sst\', \'sst-processed-data-wkt2\' , \'/home/chipn/dev/OpenSeq2Seq/wkt2-processed-data/vocab.txt\')\n# SSTCorpus(\'/home/chipn/data/binary_sst\', \'sst-processed-data-wkt103\' , \'/home/chipn/dev/OpenSeq2Seq/wkt103-processed-data/vocab.txt\')\n# IMDBCorpus(\'/home/chipn/data/aclImdb\', \'imdb-processed-data-wkt103\' , \'/home/chipn/dev/OpenSeq2Seq/wkt103-processed-data/vocab.txt\')\n# IMDBCorpus(\'/home/chipn/data/aclImdb\', \'imdb-processed-data-wkt2\' , \'/home/chipn/dev/OpenSeq2Seq/wkt2-processed-data/vocab.txt\')'"
open_seq2seq/data/speech2text/__init__.py,0,b''
open_seq2seq/data/speech2text/speech2text.py,29,"b'# Copyright (c) 2018 NVIDIA Corporation\n""""""Data Layer for Speech-to-Text models""""""\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport six\nimport math\nimport librosa\nfrom six import string_types\nfrom six.moves import range\n\nfrom open_seq2seq.data.data_layer import DataLayer\nfrom open_seq2seq.data.utils import load_pre_existing_vocabulary\nfrom .speech_utils import get_speech_features_from_file, get_speech_features\nimport sentencepiece as spm\n\n# numpy.fft MKL bug: https://github.com/IntelPython/mkl_fft/issues/11\nif hasattr(np.fft, \'restore_all\'):\n  np.fft.restore_all()\n\nclass Speech2TextDataLayer(DataLayer):\n  """"""Speech-to-text data layer class.""""""\n  @staticmethod\n  def get_required_params():\n    return dict(DataLayer.get_required_params(), **{\n        \'num_audio_features\': int,\n        \'input_type\': [\'spectrogram\', \'mfcc\', \'logfbank\'],\n        \'vocab_file\': str,\n        \'dataset_files\': list,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(DataLayer.get_optional_params(), **{\n        \'backend\': [\'psf\', \'librosa\'],\n        \'augmentation\': dict,\n        \'pad_to\': int,\n        \'max_duration\': float,\n        \'min_duration\': float,\n        \'bpe\': bool,\n        \'autoregressive\': bool,\n        \'syn_enable\': bool,\n        \'syn_subdirs\': list,\n        \'window_size\': float,\n        \'window_stride\': float,\n        \'dither\': float,\n        \'norm_per_feature\': bool,\n        \'window\': [\'hanning\', \'hamming\', \'none\'],\n        \'num_fft\': int,\n        \'precompute_mel_basis\': bool,\n        \'sample_freq\': int,\n        \'gain\': float,\n        \'features_mean\': np.ndarray,\n        \'features_std_dev\': np.ndarray,\n    })\n\n  def __init__(self, params, model, num_workers, worker_id):\n    """"""Speech-to-text data layer constructor.\n    See parent class for arguments description.\n    Config parameters:\n    * **backend** (str) --- audio pre-processing backend\n      (\'psf\' [default] or librosa [recommended]).\n    * **num_audio_features** (int) --- number of audio features to extract.\n    * **input_type** (str) --- could be either ""spectrogram"" or ""mfcc"".\n    * **vocab_file** (str) --- path to vocabulary file or sentencepiece model.\n    * **dataset_files** (list) --- list with paths to all dataset .csv files.\n    * **augmentation** (dict) --- optional dictionary with data augmentation\n      parameters. Can contain ""speed_perturbation_ratio"", ""noise_level_min"" and\n      ""noise_level_max"" parameters, e.g.::\n        {\n          \'speed_perturbation_ratio\': 0.05,\n          \'noise_level_min\': -90,\n          \'noise_level_max\': -60,\n        }\n      For additional details on these parameters see\n      :func:`data.speech2text.speech_utils.augment_audio_signal` function.\n    * **pad_to** (int) --- align audio sequence length to pad_to value.\n    * **max_duration** (float) --- drop all samples longer than\n      **max_duration** (seconds)\n    * **min_duration** (float) --- drop all samples shorter than\n      **min_duration** (seconds)\n    * **bpe** (bool) --- use BPE encodings\n    * **autoregressive** (bool) --- boolean indicating whether the model is\n      autoregressive.\n    * **syn_enable** (bool) --- boolean indicating whether the model is\n      using synthetic data.\n    * **syn_subdirs** (list) --- must be defined if using synthetic mode.\n      Contains a list of subdirectories that hold the synthetica wav files.\n    * **window_size** (float) --- window\'s duration (in seconds)\n    * **window_stride** (float) --- window\'s stride (in seconds)\n    * **dither** (float) --- weight of Gaussian noise to apply to input signal\n      for dithering/preventing quantization noise\n    * **num_fft** (int) --- size of fft window to use if features require fft,\n          defaults to smallest power of 2 larger than window size\n    * **norm_per_feature** (bool) --- if True, the output features will be\n      normalized (whitened) individually. if False, a global mean/std over all\n      features will be used for normalization.\n    * **window** (str) --- window function to apply before FFT\n      (\'hanning\', \'hamming\', \'none\')\n    * **num_fft** (int) --- optional FFT size\n    * **precompute_mel_basis** (bool) --- compute and store mel basis. If False,\n      it will compute it for every get_speech_features call. Default: False\n    * **sample_freq** (int) --- required for precompute_mel_basis\n    """"""\n    super(Speech2TextDataLayer, self).__init__(params, model,\n                                               num_workers, worker_id)\n \n    self.params[\'autoregressive\'] = self.params.get(\'autoregressive\', False)\n    self.autoregressive = self.params[\'autoregressive\']\n    self.params[\'bpe\'] = self.params.get(\'bpe\', False)\n    if self.params[\'bpe\']:\n      self.sp = spm.SentencePieceProcessor()\n      self.sp.Load(self.params[\'vocab_file\'])\n      self.params[\'tgt_vocab_size\'] = len(self.sp) + 1\n    else:\n      self.params[\'char2idx\'] = load_pre_existing_vocabulary(\n          self.params[\'vocab_file\'], read_chars=True,\n      )\n      if not self.autoregressive:\n        # add one for implied blank token\n        self.params[\'tgt_vocab_size\'] = len(self.params[\'char2idx\']) + 1\n      else:\n        num_chars_orig = len(self.params[\'char2idx\'])\n        self.params[\'tgt_vocab_size\'] = num_chars_orig + 2\n        self.start_index = num_chars_orig\n        self.end_index = num_chars_orig + 1\n        self.params[\'char2idx\'][\'<S>\'] = self.start_index\n        self.params[\'char2idx\'][\'</S>\'] = self.end_index\n        self.target_pad_value = self.end_index\n      self.params[\'idx2char\'] = {i: w for w,\n                                 i in self.params[\'char2idx\'].items()}\n    self.target_pad_value = 0\n\n    self._files = None\n    if self.params[""interactive""]:\n      return\n    for csv in params[\'dataset_files\']:\n      files = pd.read_csv(csv, encoding=\'utf-8\')\n      if self._files is None:\n        self._files = files\n      else:\n        self._files = self._files.append(files)\n\n    if self.params[\'mode\'] != \'infer\':\n      cols = [\'wav_filename\', \'transcript\']\n    else:\n      cols = \'wav_filename\'\n\n    self.all_files = self._files.loc[:, cols].values\n    self._files = self.split_data(self.all_files)\n\n    self._size = self.get_size_in_samples()\n    self._dataset = None\n    self._iterator = None\n    self._input_tensors = None\n\n    self.params[\'min_duration\'] = self.params.get(\'min_duration\', -1.0)\n    self.params[\'max_duration\'] = self.params.get(\'max_duration\', -1.0)\n    self.params[\'window_size\'] = self.params.get(\'window_size\', 20e-3)\n    self.params[\'window_stride\'] = self.params.get(\'window_stride\', 10e-3)\n    self.params[\'sample_freq\'] = self.params.get(\'sample_freq\', 16000)\n\n    mel_basis = None\n    if (self.params.get(""precompute_mel_basis"", False) and\n        self.params[""input_type""] == ""logfbank""):\n      num_fft = (\n          self.params.get(""num_fft"", None) or\n          2**math.ceil(math.log2(\n              self.params[\'window_size\']*self.params[\'sample_freq\'])\n          )\n      )\n      mel_basis = librosa.filters.mel(\n          self.params[\'sample_freq\'],\n          num_fft,\n          n_mels=self.params[\'num_audio_features\'],\n          fmin=0,\n          fmax=int(self.params[\'sample_freq\']/2)\n      )\n    self.params[\'mel_basis\'] = mel_basis\n\n    if \'n_freq_mask\' in self.params.get(\'augmentation\', {}):\n      width_freq_mask = self.params[\'augmentation\'].get(\'width_freq_mask\', 10)\n      if width_freq_mask > self.params[\'num_audio_features\']:\n        raise ValueError(\n            ""\'width_freq_mask\'={} should be smaller "".format(width_freq_mask)+\n            ""than \'num_audio_features\'={}"".format(\n               self.params[\'num_audio_features\']\n            )\n        )\n\n\n    if \'time_stretch_ratio\' in self.params.get(\'augmentation\', {}):\n      print(""WARNING: Please update time_stretch_ratio to speed_perturbation_ratio"")\n      self.params[\'augmentation\'][\'speed_perturbation_ratio\'] = self.params[\'augmentation\'][\'time_stretch_ratio\']\n\n  def split_data(self, data):\n    if self.params[\'mode\'] != \'train\' and self._num_workers is not None:\n      size = len(data)\n      start = size // self._num_workers * self._worker_id\n      if self._worker_id == self._num_workers - 1:\n        end = size\n      else:\n        end = size // self._num_workers * (self._worker_id + 1)\n      return data[start:end]\n    else:\n      return data\n\n  @property\n  def iterator(self):\n    """"""Underlying tf.data iterator.""""""\n    return self._iterator\n\n  def build_graph(self):\n    with tf.device(\'/cpu:0\'):\n\n      """"""Builds data processing graph using ``tf.data`` API.""""""\n      if self.params[\'mode\'] != \'infer\':\n        self._dataset = tf.data.Dataset.from_tensor_slices(self._files)\n        if self.params[\'shuffle\']:\n          self._dataset = self._dataset.shuffle(self._size)\n        self._dataset = self._dataset.repeat()\n        self._dataset = self._dataset.prefetch(tf.contrib.data.AUTOTUNE)\n        self._dataset = self._dataset.map(\n            lambda line: tf.py_func(\n                self._parse_audio_transcript_element,\n                [line],\n                [self.params[\'dtype\'], tf.int32, tf.int32, tf.int32, tf.float32],\n                stateful=False,\n            ),\n            num_parallel_calls=8,\n        )\n        if self.params[\'max_duration\'] > 0:\n          self._dataset = self._dataset.filter(\n              lambda x, x_len, y, y_len, duration:\n              tf.less_equal(duration, self.params[\'max_duration\'])\n          )\n        if self.params[\'min_duration\'] > 0:\n          self._dataset = self._dataset.filter(\n              lambda x, x_len, y, y_len, duration:\n              tf.greater_equal(duration, self.params[\'min_duration\'])\n          )\n        self._dataset = self._dataset.map(\n            lambda x, x_len, y, y_len, duration:\n            [x, x_len, y, y_len],\n            num_parallel_calls=8,\n        )\n        self._dataset = self._dataset.padded_batch(\n            self.params[\'batch_size\'],\n            padded_shapes=([None, self.params[\'num_audio_features\']],\n                           1, [None], 1),\n            padding_values=(\n                tf.cast(0, self.params[\'dtype\']), 0, self.target_pad_value, 0),\n        )\n      else:\n        indices = self.split_data(\n            np.array(list(map(str, range(len(self.all_files)))))\n        )\n        self._dataset = tf.data.Dataset.from_tensor_slices(\n            np.hstack((indices[:, np.newaxis], self._files[:, np.newaxis]))\n        )\n        self._dataset = self._dataset.repeat()\n        self._dataset = self._dataset.prefetch(tf.contrib.data.AUTOTUNE)\n        self._dataset = self._dataset.map(\n            lambda line: tf.py_func(\n                self._parse_audio_element,\n                [line],\n                [self.params[\'dtype\'], tf.int32, tf.int32, tf.float32],\n                stateful=False,\n            ),\n            num_parallel_calls=8,\n        )\n        if self.params[\'max_duration\'] > 0:\n          self._dataset = self._dataset.filter(\n              lambda x, x_len, idx, duration:\n              tf.less_equal(duration, self.params[\'max_duration\'])\n          )\n        if self.params[\'min_duration\'] > 0:\n            self._dataset = self._dataset.filter(\n              lambda x, x_len, y, y_len, duration:\n              tf.greater_equal(duration, self.params[\'min_duration\'])\n          )\n        self._dataset = self._dataset.map(\n            lambda x, x_len, idx, duration:\n            [x, x_len, idx],\n            num_parallel_calls=16,\n        )\n        self._dataset = self._dataset.padded_batch(\n            self.params[\'batch_size\'],\n            padded_shapes=([None, self.params[\'num_audio_features\']], 1, 1)\n        )\n\n      self._iterator = self._dataset.prefetch(tf.contrib.data.AUTOTUNE)\\\n                           .make_initializable_iterator()\n\n      if self.params[\'mode\'] != \'infer\':\n        x, x_length, y, y_length = self._iterator.get_next()\n        # need to explicitly set batch size dimension\n        # (it is employed in the model)\n        y.set_shape([self.params[\'batch_size\'], None])\n        y_length = tf.reshape(y_length, [self.params[\'batch_size\']])\n      else:\n        x, x_length, x_id = self._iterator.get_next()\n        x_id = tf.reshape(x_id, [self.params[\'batch_size\']])\n\n      x.set_shape([self.params[\'batch_size\'], None,\n                   self.params[\'num_audio_features\']])\n      x_length = tf.reshape(x_length, [self.params[\'batch_size\']])\n\n      pad_to = self.params.get(""pad_to"", 8)\n      if pad_to > 0 and self.params.get(\'backend\') == \'librosa\':\n        # we do padding with TF for librosa backend\n        num_pad = tf.mod(pad_to - tf.mod(tf.reduce_max(x_length), pad_to), pad_to)\n        x = tf.pad(x, [[0, 0], [0, num_pad], [0, 0]])\n\n      self._input_tensors = {}\n      self._input_tensors[""source_tensors""] = [x, x_length]\n      if self.params[\'mode\'] != \'infer\':\n        self._input_tensors[\'target_tensors\'] = [y, y_length]\n      else:\n        self._input_tensors[\'source_ids\'] = [x_id]\n\n  def create_interactive_placeholders(self):\n    self._x = tf.placeholder(\n        dtype=self.params[\'dtype\'],\n        shape=[\n            self.params[\'batch_size\'],\n            None,\n            self.params[\'num_audio_features\']\n        ]\n    )\n    self._x_length = tf.placeholder(\n        dtype=tf.int32,\n        shape=[self.params[\'batch_size\']]\n    )\n    self._x_id = tf.placeholder(\n        dtype=tf.int32,\n        shape=[self.params[\'batch_size\']]\n    )\n\n    self._input_tensors = {}\n    self._input_tensors[""source_tensors""] = [self._x, self._x_length]\n    self._input_tensors[\'source_ids\'] = [self._x_id]\n\n  def create_feed_dict(self, model_in):\n    """""" Creates the feed dict for interactive infer\n\n    Args:\n      model_in (str or np.array): Either a str that contains the file path of the\n        wav file, or a numpy array containing 1-d wav file.\n\n    Returns:\n      feed_dict (dict): Dictionary with values for the placeholders.\n    """"""\n    audio_arr = []\n    audio_length_arr = []\n    x_id_arr = []\n    for line in model_in:\n      if isinstance(line, string_types):\n        audio, audio_length, x_id, _ = self._parse_audio_element([0, line])\n      elif isinstance(line, np.ndarray):\n        audio, audio_length, x_id, _ = self._get_audio(line)\n      else:\n        raise ValueError(\n            ""Speech2Text\'s interactive inference mode only supports string or"",\n            ""numpy array as input. Got {}"". format(type(line))\n        )\n      audio_arr.append(audio)\n      audio_length_arr.append(audio_length)\n      x_id_arr.append(x_id)\n    max_len = np.max(audio_length_arr)\n    pad_to = self.params.get(""pad_to"", 8)\n    if pad_to > 0 and self.params.get(\'backend\') == \'librosa\':\n      max_len += (pad_to - max_len % pad_to) % pad_to\n    for i, audio in enumerate(audio_arr):\n      audio = np.pad(\n          audio, ((0, max_len-len(audio)), (0, 0)),\n          ""constant"", constant_values=0.\n      )\n      audio_arr[i] = audio\n\n    audio = np.reshape(\n        audio_arr,\n        [self.params[\'batch_size\'],\n         -1,\n         self.params[\'num_audio_features\']]\n    )\n    audio_length = np.reshape(audio_length_arr, [self.params[\'batch_size\']])\n    x_id = np.reshape(x_id_arr, [self.params[\'batch_size\']])\n\n    feed_dict = {\n        self._x: audio,\n        self._x_length: audio_length,\n        self._x_id: x_id,\n    }\n    return feed_dict\n\n  def _parse_audio_transcript_element(self, element):\n    """"""Parses tf.data element from TextLineDataset into audio and text.\n    Args:\n      element: tf.data element from TextLineDataset.\n    Returns:\n      tuple: source audio features as ``np.array``, length of source sequence,\n      target text as `np.array` of ids, target text length.\n    """"""\n    audio_filename, transcript = element\n    if not six.PY2:\n      transcript = str(transcript, \'utf-8\')\n      audio_filename = str(audio_filename, \'utf-8\')\n    if self.params[\'bpe\']:\n      target_indices = self.sp.EncodeAsIds(transcript)\n    else:\n      target_indices = [self.params[\'char2idx\'][c] for c in transcript]\n    if self.autoregressive:\n      target_indices = target_indices + [self.end_index]\n    target = np.array(target_indices)\n\n    if self.params.get(""syn_enable"", False):\n      audio_filename = audio_filename.format(np.random.choice(self.params[""syn_subdirs""]))\n\n    source, audio_duration = get_speech_features_from_file(\n        audio_filename,\n        params=self.params\n    )\n    return source.astype(self.params[\'dtype\'].as_numpy_dtype()), \\\n        np.int32([len(source)]), \\\n        np.int32(target), \\\n        np.int32([len(target)]), \\\n        np.float32([audio_duration])\n\n  def _get_audio(self, wav):\n    """"""Parses audio from wav and returns array of audio features.\n    Args:\n      wav: numpy array containing wav\n\n    Returns:\n      tuple: source audio features as ``np.array``, length of source sequence,\n      sample id.\n    """"""\n    source, audio_duration = get_speech_features(\n        wav, 16000., self.params\n    )\n\n    return source.astype(self.params[\'dtype\'].as_numpy_dtype()), \\\n        np.int32([len(source)]), np.int32([0]), \\\n        np.float32([audio_duration])\n\n  def _parse_audio_element(self, id_and_audio_filename):\n    """"""Parses audio from file and returns array of audio features.\n    Args:\n      id_and_audio_filename: tuple of sample id and corresponding\n          audio file name.\n    Returns:\n      tuple: source audio features as ``np.array``, length of source sequence,\n      sample id.\n    """"""\n    idx, audio_filename = id_and_audio_filename\n    source, audio_duration = get_speech_features_from_file(\n        audio_filename,\n        params=self.params\n    )\n    return source.astype(self.params[\'dtype\'].as_numpy_dtype()), \\\n        np.int32([len(source)]), np.int32([idx]), \\\n        np.float32([audio_duration])\n\n  @property\n  def input_tensors(self):\n    """"""Dictionary with input tensors.\n    ``input_tensors[""source_tensors""]`` contains:\n      * source_sequence\n        (shape=[batch_size x sequence length x num_audio_features])\n      * source_length (shape=[batch_size])\n    ``input_tensors[""target_tensors""]`` contains:\n      * target_sequence\n        (shape=[batch_size x sequence length])\n      * target_length (shape=[batch_size])\n    """"""\n    return self._input_tensors\n\n  def get_size_in_samples(self):\n    """"""Returns the number of audio files.""""""\n    return len(self._files)\n'"
open_seq2seq/data/speech2text/speech_commands.py,5,"b'import os\nimport six\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport librosa\n\nfrom open_seq2seq.data.data_layer import DataLayer\nfrom open_seq2seq.data.text2speech.speech_utils import \\\n  get_speech_features_from_file\n\nclass SpeechCommandsDataLayer(DataLayer):\n\n  @staticmethod\n  def get_required_params():\n    return dict(DataLayer.get_required_params(), ** {\n        ""dataset_files"": list,\n        ""dataset_location"": str,\n        ""num_audio_features"": int,\n        ""audio_length"": int,\n        ""num_labels"": int,\n        ""model_format"": str\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(DataLayer.get_optional_params(), **{\n        ""cache_data"": bool,\n        ""augment_data"": bool\n    })\n\n  def split_data(self, data):\n    if self.params[""mode""] != ""train"" and self._num_workers is not None:\n      size = len(data)\n      start = size // self._num_workers * self._worker_id\n\n      if self._worker_id == self._num_workers - 1:\n        end = size\n      else:\n        end = size // self._num_workers * (self._worker_id + 1)\n\n      return data[start:end]\n\n    return data\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  def get_size_in_samples(self):\n    if self._files is not None:\n      return len(self._files)\n    else:\n      return 0\n\n  def __init__(self, params, model, num_workers=None, worker_id=None):\n    """"""\n    ResNet Speech Commands data layer constructor.\n\n    Config parameters:\n\n    * **dataset_files** (list) --- list with paths to all dataset .csv files\n    * **dataset_location** (str) --- string with path to directory where .wavs\n      are stored\n    * **num_audio_features** (int) --- number of spectrogram audio features and \n      image length\n    * **audio_length** (int) --- cropping length of spectrogram and image width\n    * **num_labels** (int) --- number of classes in dataset\n    * **model_format** (str) --- determines input format, should be one of\n      ""jasper"" or ""resnet""\n    \n    * **cache_data** (bool) --- cache the training data in the first epoch\n    * **augment_data** (bool) --- add time stretch and noise to training data\n    """"""\n\n    super(SpeechCommandsDataLayer, self).__init__(params, model, num_workers, worker_id)\n\n    if self.params[""mode""] == ""infer"":\n      raise ValueError(""Inference is not supported on SpeechCommandsDataLayer"")\n\n    self._files = None\n    for file in self.params[""dataset_files""]:\n      csv_file = pd.read_csv(\n        os.path.join(self.params[""dataset_location""], file),\n        encoding=""utf-8"",\n        sep="","",\n        header=None,\n        names=[""label"", ""wav_filename""],\n        dtype=str\n      )\n\n    if self._files is None:\n      self._files = csv_file\n    else:\n      self._files.append(csv_file)\n\n    cols = [""label"", ""wav_filename""]\n\n    if self._files is not None:\n      all_files = self._files.loc[:, cols].values\n      self._files = self.split_data(all_files)\n\n    self._size = self.get_size_in_samples()\n    self._iterator = None\n    self._input_tensors = None\n\n  def preprocess_image(self, image):\n    """"""Crops or pads a spectrogram into a fixed dimension square image\n    """"""\n    num_audio_features = self.params[""num_audio_features""]\n    audio_length = self.params[""audio_length""]\n\n    if image.shape[0] > audio_length: # randomly slice\n      offset = np.random.randint(0, image.shape[0] - audio_length + 1)\n      image = image[offset:offset + audio_length, :]\n\n    else: # symmetrically pad with zeros\n      pad_left = (audio_length - image.shape[0]) // 2\n      pad_right = (audio_length - image.shape[0]) // 2\n\n      if (audio_length - image.shape[0]) % 2 == 1:\n        pad_right += 1\n\n      image = np.pad(\n          image, \n          ((pad_left, pad_right), (0, 0)), \n          ""constant""\n      )\n\n    assert image.shape == (audio_length, num_audio_features)\n\n    # add dummy dimension\n    if self.params[""model_format""] == ""jasper"": # for batch norm\n      image = np.expand_dims(image, 1)\n    else: # for channel\n      image = np.expand_dims(image, -1)\n\n    return image\n\n  def parse_element(self, element):\n    """"""Reads an audio file and returns the augmented spectrogram image\n    """"""\n    label, audio_filename = element\n\n    if six.PY2:\n      audio_filename = unicode(audio_filename, ""utf-8"")\n    else:\n      audio_filename = str(audio_filename, ""utf-8"")\n\n    file_path = os.path.join(\n        self.params[""dataset_location""],\n        audio_filename\n    )\n\n    if self.params[""mode""] == ""train"" and self.params.get(""augment_data"", False):\n      augmentation = { \n        ""pitch_shift_steps"": 2,\n        ""time_stretch_ratio"": 0.2,\n        ""noise_level_min"": -90,\n        ""noise_level_max"": -46,\n      }\n    else:\n      augmentation = None\n\n    spectrogram = get_speech_features_from_file(\n        file_path,\n        self.params[""num_audio_features""],\n        features_type=""mel"",\n        data_min=1e-5,\n        augmentation=augmentation\n    )\n\n    image = self.preprocess_image(spectrogram)\n    \n    return image.astype(self.params[""dtype""].as_numpy_dtype()), \\\n        np.int32(self.params[""num_audio_features""]), np.int32(label) \n\n  def build_graph(self):\n    dataset = tf.data.Dataset.from_tensor_slices(self._files)\n\n    cache_data = self.params.get(""cache_data"", False)\n\n    if not cache_data:\n      if self.params[""shuffle""]:\n        dataset = dataset.shuffle(self._size)\n\n    dataset = dataset.map(\n        lambda line: tf.py_func(\n            self.parse_element,\n            [line],\n            [self.params[""dtype""], tf.int32, tf.int32],\n            stateful=False\n        ),\n        num_parallel_calls=8\n    )\n\n    if cache_data:\n      dataset = dataset.cache()\n      if self.params[""shuffle""]:\n        dataset = dataset.shuffle(self._size)  \n\n    if self.params[""repeat""]:\n      dataset = dataset.repeat()\n\n    dataset = dataset.batch(self.params[""batch_size""])\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n\n    self._iterator = dataset.make_initializable_iterator()\n    inputs, lengths, labels = self._iterator.get_next()\n\n    if self.params[""model_format""] == ""jasper"": \n      inputs.set_shape([\n          self.params[""batch_size""], \n          self.params[""audio_length""],\n          1,\n          self.params[""num_audio_features""],\n      ]) # B T 1 C\n      lengths.set_shape([self.params[""batch_size""]])\n      source_tensors = [inputs, lengths]\n    else:\n      inputs.set_shape([\n          self.params[""batch_size""], \n          self.params[""num_audio_features""], \n          self.params[""num_audio_features""], \n          1\n      ]) # B W L C\n      source_tensors = [inputs]\n    \n    labels = tf.one_hot(labels, self.params[""num_labels""])\n    labels.set_shape([self.params[""batch_size""], self.params[""num_labels""]])\n\n    self._input_tensors = {\n        ""source_tensors"": source_tensors,\n        ""target_tensors"": [labels]\n    }'"
open_seq2seq/data/speech2text/speech_commands_preprocessing.py,0,"b'# Produces labelled .csv files containing balanced samples of classes\n# and their labels for the chosen dataset\nimport os\nimport random\nimport librosa\nimport numpy as np\n\n\n# choose one of three datasets\n# \t1) v1-12: V1 dataset with 12 classes, including unknown and silence\n# \t2) v1-30: V1 dataset with 30 classes, without unknown and silence\n# \t3) v2: V2 dataset with 35 classes\nDATASET = ""v1-12""\n\nif DATASET == ""v1-12"":\n\tclasses = [""yes"", ""no"", ""up"", ""down"", ""left"", ""right"", ""on"", ""off"", ""stop"", ""go"", ""unknown"", ""silence""]\nelif DATASET == ""v1-30"":\n\tclasses = [""yes"", ""no"", ""up"", ""down"", ""left"", ""right"", ""on"", ""off"", ""stop"", ""go"", ""bed"", ""bird"", ""cat"", ""dog"", ""eight"", ""five"", ""four"", ""happy"", ""house"", ""marvin"", ""nine"", ""one"", ""seven"", ""sheila"", ""six"", ""three"", ""tree"", ""two"", ""wow"", ""zero""]\nelif DATASET == ""v2"":\n\tclasses = [""backward"", ""bed"", ""bird"", ""cat"", ""dog"", ""down"", ""eight"", ""five"", ""follow"", ""forward"", ""four"", ""go"", ""happy"", ""house"", ""learn"", ""left"", ""marvin"", ""nine"", ""no"", ""off"", ""on"", ""one"", ""right"", ""seven"", ""sheila"", ""six"", ""stop"", ""three"", ""tree"", ""two"", ""up"", ""visual"", ""wow"", ""yes"", ""zero""]\nelse:\n\tprint(""Dataset not supported"")\n\texit()\n\nroot_dir = ""../../../data""\nif ""v1"" in DATASET:\n\troot_dir = os.path.join(root_dir, ""speech_commands_v0.01"")\nelse:\n\troot_dir = os.path.join(root_dir, ""speech_commands_v0.02"")\n\n\neval_batch = 16\ntrain_split = 0.8\ntest_split = val_split = (1 - train_split) / 2\n\ndata_list = []\nmin_samples_per_class = None\nmax_samples_per_class = 5000\n\n# build a list of all available samples\nfor idx, label in enumerate(classes):\n\tclass_list = []\n\n\tif label == ""unknown"":\n\t\tunknowns = [""bed"", ""bird"", ""cat"", ""dog"", ""eight"", ""five"", ""four"", ""happy"", ""house"", ""marvin"", ""nine"", ""one"", ""seven"", ""sheila"", ""six"", ""three"", ""tree"", ""two"", ""wow"", ""zero""]\n\t\t\n\t\tfor unknown in unknowns:\n\t\t\tfolder = os.path.join(root_dir, unknown)\n\n\t\t\tfor file in os.listdir(folder):\n\t\t\t\tfile_path = ""{}/{}"".format(unknown, file)\n\t\t\t\tclass_list.append(file_path)\n\n\telif label == ""silence"":\n\t\tsilence_path = os.path.join(root_dir, ""silence"")\n\t\tif not os.path.exists(silence_path):\n\t\t\tos.mkdir(silence_path)\n\n\t\tsilence_stride = 2000\n\t\tsampling_rate = 16000\n\t\tfolder = os.path.join(root_dir, ""_background_noise_"")\n\n\t\tfor file in os.listdir(folder):\n\t\t\tif "".wav"" in file:\n\t\t\t\tload_path = os.path.join(folder, file)\n\t\t\t\ty, sr = librosa.load(load_path)\n\n\t\t\t\tfor i in range(0, len(y) - sampling_rate, silence_stride):\n\t\t\t\t\tfile_path = ""silence/{}_{}.wav"".format(file[:-4], i)\n\t\t\t\t\ty_slice = y[i:i + sampling_rate]\n\t\t\t\t\tlibrosa.output.write_wav(os.path.join(root_dir, file_path), y_slice, sr)\n\t\t\t\t\tclass_list.append(file_path)\n\n\telse:\n\t\tfolder = os.path.join(root_dir, label)\n\n\t\tfor file in os.listdir(folder):\n\t\t\tfile_path = ""{}/{}"".format(label, file)\n\t\t\tclass_list.append(file_path)\n\n\tif min_samples_per_class is None or len(class_list) < min_samples_per_class:\n\t\tmin_samples_per_class = len(class_list)\n\n\trandom.shuffle(class_list)\n\tdata_list.append(class_list)\n\n\n# sample and write to files\ntest_part = int(test_split * min_samples_per_class)\ntest_part += eval_batch - (test_part % eval_batch)\nval_part = int(test_split * min_samples_per_class)\nval_part += eval_batch - (val_part % eval_batch)\n\ntrain_samples = []\ntest_samples = []\nval_samples = []\n\nfor i, class_list in enumerate(data_list):\n\t# take test and validation samples out\n\tfor sample in class_list[:test_part]:\n\t\ttest_samples.append(""{},{}"".format(i, sample))\n\tfor sample in class_list[test_part:test_part + val_part]:\n\t\tval_samples.append(""{},{}"".format(i, sample))\n\n\tsamples = class_list[test_part + val_part:]\n\tlength = len(samples)\n\n\twhile len(class_list) < max_samples_per_class:\n\t\tl = np.random.randint(0, length)\n\t\tclass_list.append(samples[l])\n\n\tfor sample in class_list[test_part + val_part:max_samples_per_class]:\n\t\ttrain_samples.append(""{},{}"".format(i, sample))\n\ntrain_file = open(os.path.join(root_dir, DATASET + ""-train.txt""), ""w"")\nfor line in train_samples:\n\ttrain_file.write(line + ""\\n"")\ntest_file = open(os.path.join(root_dir, DATASET + ""-test.txt""), ""w"")\nfor line in test_samples:\n\ttest_file.write(line + ""\\n"")\nval_file = open(os.path.join(root_dir, DATASET + ""-val.txt""), ""w"")\nfor line in val_samples:\n\tval_file.write(line + ""\\n"")\n\t'"
open_seq2seq/data/speech2text/speech_utils.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport math\nimport os\n\nimport h5py\nimport numpy as np\nimport resampy as rs\nimport scipy.io.wavfile as wave\nBACKENDS = []\ntry:\n  import python_speech_features as psf\n  BACKENDS.append(\'psf\')\nexcept ImportError:\n  pass\ntry:\n  import librosa\n  BACKENDS.append(\'librosa\')\nexcept ImportError:\n  pass\n\nWINDOWS_FNS = {""hanning"": np.hanning, ""hamming"": np.hamming, ""none"": None}\n\n\nclass PreprocessOnTheFlyException(Exception):\n  """""" Exception that is thrown to not load preprocessed features from disk;\n  recompute on-the-fly.\n  This saves disk space (if you\'re experimenting with data input\n  formats/preprocessing) but can be slower.\n  The slowdown is especially apparent for small, fast NNs.""""""\n  pass\n\n\nclass RegenerateCacheException(Exception):\n  """""" Exception that is thrown to force recomputation of (preprocessed) features\n  """"""\n  pass\n\n\ndef load_features(path, data_format):\n  """""" Function to load (preprocessed) features from disk\n\n  Args:\n      :param path:    the path where the features are stored\n      :param data_format:  the format in which the features are stored\n      :return:        tuple of (features, duration)\n      """"""\n  if data_format == \'hdf5\':\n    with h5py.File(path + \'.hdf5\', ""r"") as hf5_file:\n      features = hf5_file[""features""][:]\n      duration = hf5_file[""features""].attrs[""duration""]\n  elif data_format == \'npy\':\n    features, duration = np.load(path + \'.npy\')\n  elif data_format == \'npz\':\n    data = np.load(path + \'.npz\')\n    features = data[\'features\']\n    duration = data[\'duration\']\n  else:\n    raise ValueError(""Invalid data format for caching: "", data_format, ""!\\n"",\n                     ""options: hdf5, npy, npz"")\n  return features, duration\n\n\ndef save_features(features, duration, path, data_format, verbose=False):\n  """""" Function to save (preprocessed) features to disk\n\n  Args:\n      :param features:            features\n      :param duration:            metadata: duration in seconds of audio file\n      :param path:                path to store the data\n      :param data_format:              format to store the data in (\'npy\',\n      \'npz\',\n      \'hdf5\')\n  """"""\n  if verbose: print(""Saving to: "", path)\n\n  if data_format == \'hdf5\':\n    with h5py.File(path + \'.hdf5\', ""w"") as hf5_file:\n      dset = hf5_file.create_dataset(""features"", data=features)\n      dset.attrs[""duration""] = duration\n  elif data_format == \'npy\':\n    np.save(path + \'.npy\', [features, duration])\n  elif data_format == \'npz\':\n    np.savez(path + \'.npz\', features=features, duration=duration)\n  else:\n    raise ValueError(""Invalid data format for caching: "", data_format, ""!\\n"",\n                     ""options: hdf5, npy, npz"")\n\n\ndef get_preprocessed_data_path(filename, params):\n  """""" Function to convert the audio path into the path to the preprocessed\n  version of this audio\n  Args:\n      :param filename:    WAVE filename\n      :param params:      dictionary containing preprocessing parameters\n      :return:            path to new file (without extension). The path is\n      generated from the relevant preprocessing parameters.\n  """"""\n  if isinstance(filename, bytes):  # convert binary string to normal string\n    filename = filename.decode(\'ascii\')\n\n  filename = os.path.realpath(filename)  # decode symbolic links\n\n  ## filter relevant parameters # TODO is there a cleaner way of doing this?\n  # print(list(params.keys()))\n  ignored_params = [""cache_features"", ""cache_format"", ""cache_regenerate"",\n                    ""vocab_file"", ""dataset_files"", ""shuffle"", ""batch_size"",\n                    ""max_duration"",\n                    ""mode"", ""interactive"", ""autoregressive"", ""char2idx"",\n                    ""tgt_vocab_size"", ""idx2char"", ""dtype""]\n\n  def fix_kv(text):\n    """""" Helper function to shorten length of filenames to get around\n    filesystem path length limitations""""""\n    text = str(text)\n    text = text.replace(""speed_perturbation_ratio"", ""sp"") \\\n      .replace(""noise_level_min"", ""nlmin"", ) \\\n      .replace(""noise_level_max"", ""nlmax"") \\\n      .replace(""add_derivatives"", ""d"") \\\n      .replace(""add_second_derivatives"", ""dd"")\n    return text\n\n  # generate the identifier by simply concatenating preprocessing key-value\n  # pairs as strings.\n  preprocess_id = ""-"".join(\n      [fix_kv(k) + ""_"" + fix_kv(v) for k, v in params.items() if\n       k not in ignored_params])\n\n  preprocessed_dir = os.path.dirname(filename).replace(""wav"",\n                                                       ""preprocessed-"" +\n                                                       preprocess_id)\n  preprocessed_path = os.path.join(preprocessed_dir,\n                                   os.path.basename(filename).replace("".wav"",\n                                                                      """"))\n\n  # create dir if it doesn\'t exist yet\n  if not os.path.exists(preprocessed_dir):\n    os.makedirs(preprocessed_dir)\n\n  return preprocessed_path\n\n\ndef get_speech_features_from_file(filename, params):\n  """"""Function to get a numpy array of features, from an audio file.\n      if params[\'cache_features\']==True, try load preprocessed data from\n      disk, or store after preprocesseng.\n      else, perform preprocessing on-the-fly.\n\n  Args:\n    filename (string): WAVE filename.\n    params (dict): the following parameters\n      num_features (int): number of speech features in frequency domain.\n      features_type (string): \'mfcc\' or \'spectrogram\'.\n      window_size (float): size of analysis window in milli-seconds.\n      window_stride (float): stride of analysis window in milli-seconds.\n      augmentation (dict, optional): dictionary of augmentation parameters. See\n        :func:`augment_audio_signal` for specification and example.\n      window (str): window function to apply\n      dither (float): weight of Gaussian noise to apply to input signal for\n          dithering/preventing quantization noise\n      num_fft (int): size of fft window to use if features require fft,\n          defaults to smallest power of 2 larger than window size\n      norm_per_feature (bool): if True, the output features will be normalized\n          (whitened) individually. if False, a global mean/std over all features\n          will be used for normalization\n  Returns:\n    np.array: np.array of audio features with shape=[num_time_steps,\n    num_features].\n  """"""\n  cache_features = params.get(\'cache_features\', False)\n  cache_format = params.get(\'cache_format\', \'hdf5\')\n  cache_regenerate = params.get(\'cache_regenerate\', False)\n  try:\n    if not cache_features:\n      raise PreprocessOnTheFlyException(\n          ""on-the-fly preprocessing enforced with \'cache_features\'==True"")\n\n    if cache_regenerate:\n      raise RegenerateCacheException(""regenerating cache..."")\n\n    preprocessed_data_path = get_preprocessed_data_path(filename, params)\n    features, duration = load_features(preprocessed_data_path,\n                                       data_format=cache_format)\n\n  except PreprocessOnTheFlyException:\n    sample_freq, signal = wave.read(filename)\n    # check sample rate\n    if sample_freq != params[\'sample_freq\']:\n      raise ValueError(\n          (""The sampling frequency set in params {} does not match the ""\n           ""frequency {} read from file {}"").format(params[\'sample_freq\'],\n                                                    sample_freq, filename)\n      )\n    features, duration = get_speech_features(signal, sample_freq, params)\n\n  except (OSError, FileNotFoundError, RegenerateCacheException):\n    sample_freq, signal = wave.read(filename)\n    # check sample rate\n    if sample_freq != params[\'sample_freq\']:\n      raise ValueError(\n          (""The sampling frequency set in params {} does not match the ""\n           ""frequency {} read from file {}"").format(params[\'sample_freq\'],\n                                                    sample_freq, filename)\n      )\n    features, duration = get_speech_features(signal, sample_freq, params)\n\n    preprocessed_data_path = get_preprocessed_data_path(filename, params)\n    save_features(features, duration, preprocessed_data_path,\n                  data_format=cache_format)\n\n  return features, duration\n\n\ndef normalize_signal(signal, gain=None):\n  """"""\n  Normalize float32 signal to [-1, 1] range\n  """"""\n  if gain is None:\n    gain = 1.0 / (np.max(np.abs(signal)) + 1e-5)\n  return signal * gain\n\n\ndef augment_audio_signal(signal_float, sample_freq, augmentation):\n  """"""Function that performs audio signal augmentation.\n\n  Args:\n    signal_float (np.array): np.array containing raw audio signal.\n    sample_freq (float): frames per second.\n    augmentation (dict, optional): None or dictionary of augmentation parameters.\n        If not None, has to have \'speed_perturbation_ratio\',\n        \'noise_level_min\', or \'noise_level_max\' fields, e.g.::\n          augmentation={\n            \'speed_perturbation_ratio\': 0.2,\n            \'noise_level_min\': -90,\n            \'noise_level_max\': -46,\n          }\n        \'speed_perturbation_ratio\' can either be a list of possible speed\n        perturbation factors or a float. If float, a random value from \n        U[1-speed_perturbation_ratio, 1+speed_perturbation_ratio].\n  Returns:\n    np.array: np.array with augmented audio signal.\n  """"""\n  if \'speed_perturbation_ratio\' in augmentation:\n    stretch_amount = -1\n    if isinstance(augmentation[\'speed_perturbation_ratio\'], list):\n      stretch_amount = np.random.choice(augmentation[\'speed_perturbation_ratio\'])\n    elif augmentation[\'speed_perturbation_ratio\'] > 0:\n      # time stretch (might be slow)\n      stretch_amount = 1.0 + (2.0 * np.random.rand() - 1.0) * \\\n                       augmentation[\'speed_perturbation_ratio\']\n    if stretch_amount > 0:\n      signal_float = rs.resample(\n          signal_float,\n          sample_freq,\n          int(sample_freq * stretch_amount),\n          filter=\'kaiser_best\',\n      )\n\n  # noise\n  if \'noise_level_min\' in augmentation and \'noise_level_max\' in augmentation:\n    noise_level_db = np.random.randint(low=augmentation[\'noise_level_min\'],\n                                       high=augmentation[\'noise_level_max\'])\n    signal_float += np.random.randn(signal_float.shape[0]) * \\\n                    10.0 ** (noise_level_db / 20.0)\n\n  return signal_float\n\n\ndef preemphasis(signal, coeff=0.97):\n  return np.append(signal[0], signal[1:] - coeff * signal[:-1])\n\n\ndef get_speech_features(signal, sample_freq, params):\n  """"""\n  Get speech features using either librosa (recommended) or\n  python_speech_features\n  Args:\n    signal (np.array): np.array containing raw audio signal\n    sample_freq (float): sample rate of the signal\n    params (dict): parameters of pre-processing\n  Returns:\n    np.array: np.array of audio features with shape=[num_time_steps,\n    num_features].\n    audio_duration (float): duration of the signal in seconds\n  """"""\n\n  backend = params.get(\'backend\', \'psf\')\n\n  features_type = params.get(\'input_type\', \'spectrogram\')\n  num_features = params[\'num_audio_features\']\n  window_size = params.get(\'window_size\', 20e-3)\n  window_stride = params.get(\'window_stride\', 10e-3)\n  augmentation = params.get(\'augmentation\', None)\n\n  if backend == \'librosa\':\n    window_fn = WINDOWS_FNS[params.get(\'window\', ""hanning"")]\n    dither = params.get(\'dither\', 0.0)\n    num_fft = params.get(\'num_fft\', None)\n    norm_per_feature = params.get(\'norm_per_feature\', False)\n    mel_basis = params.get(\'mel_basis\', None)\n    gain = params.get(\'gain\')\n    mean = params.get(\'features_mean\')\n    std_dev = params.get(\'features_std_dev\')\n    features, duration = get_speech_features_librosa(\n        signal, sample_freq, num_features, features_type,\n        window_size, window_stride, augmentation, window_fn=window_fn,\n        dither=dither, norm_per_feature=norm_per_feature, num_fft=num_fft,\n        mel_basis=mel_basis, gain=gain, mean=mean, std_dev=std_dev\n    )\n  else:\n    pad_to = params.get(\'pad_to\', 8)\n    features, duration = get_speech_features_psf(\n        signal, sample_freq, num_features, pad_to, features_type,\n        window_size, window_stride, augmentation\n    )\n\n  return features, duration \n\n\ndef get_speech_features_librosa(signal, sample_freq, num_features,\n                                features_type=\'spectrogram\',\n                                window_size=20e-3,\n                                window_stride=10e-3,\n                                augmentation=None,\n                                window_fn=np.hanning,\n                                num_fft=None,\n                                dither=0.0,\n                                norm_per_feature=False,\n                                mel_basis=None,\n                                gain=None,\n                                mean=None,\n                                std_dev=None):\n  """"""Function to convert raw audio signal to numpy array of features.\n  Backend: librosa\n  Args:\n    signal (np.array): np.array containing raw audio signal.\n    sample_freq (float): frames per second.\n    num_features (int): number of speech features in frequency domain.\n    pad_to (int): if specified, the length will be padded to become divisible\n        by ``pad_to`` parameter.\n    features_type (string): \'mfcc\' or \'spectrogram\'.\n    window_size (float): size of analysis window in milli-seconds.\n    window_stride (float): stride of analysis window in milli-seconds.\n    augmentation (dict, optional): dictionary of augmentation parameters. See\n        :func:`augment_audio_signal` for specification and example.\n\n  Returns:\n    np.array: np.array of audio features with shape=[num_time_steps,\n    num_features].\n    audio_duration (float): duration of the signal in seconds\n  """"""\n  signal = normalize_signal(signal.astype(np.float32), gain)\n  if augmentation:\n    signal = augment_audio_signal(signal, sample_freq, augmentation)\n\n  audio_duration = len(signal) * 1.0 / sample_freq\n\n  n_window_size = int(sample_freq * window_size)\n  n_window_stride = int(sample_freq * window_stride)\n  num_fft = num_fft or 2**math.ceil(math.log2(window_size*sample_freq))\n\n  if dither > 0:\n    signal += dither*np.random.randn(*signal.shape)\n\n  if features_type == \'spectrogram\':\n    # ignore 1/n_fft multiplier, since there is a post-normalization\n    powspec = np.square(np.abs(librosa.core.stft(\n        signal, n_fft=n_window_size,\n        hop_length=n_window_stride, win_length=n_window_size, center=True,\n        window=window_fn)))\n    # remove small bins\n    powspec[powspec <= 1e-30] = 1e-30\n    features = 10 * np.log10(powspec.T)\n\n    assert num_features <= n_window_size // 2 + 1, \\\n      ""num_features for spectrogram should be <= (sample_freq * window_size // 2 + 1)""\n\n    # cut high frequency part\n    features = features[:, :num_features]\n\n  elif features_type == \'mfcc\':\n    signal = preemphasis(signal, coeff=0.97)\n    S = np.square(\n            np.abs(\n                librosa.core.stft(signal, n_fft=num_fft,\n                                  hop_length=int(window_stride * sample_freq),\n                                  win_length=int(window_size * sample_freq),\n                                  center=True, window=window_fn\n                )\n            )\n        )\n    features = librosa.feature.mfcc(sr=sample_freq, S=S,\n        n_mfcc=num_features, n_mels=2*num_features).T\n  elif features_type == \'logfbank\':\n    signal = preemphasis(signal,coeff=0.97)\n    S = np.abs(librosa.core.stft(signal, n_fft=num_fft,\n                                 hop_length=int(window_stride * sample_freq),\n                                 win_length=int(window_size * sample_freq),\n                                 center=True, window=window_fn))**2.0\n    if mel_basis is None:\n      # Build a Mel filter\n      mel_basis = librosa.filters.mel(sample_freq, num_fft, n_mels=num_features,\n                                      fmin=0, fmax=int(sample_freq/2))\n    features = np.log(np.dot(mel_basis, S) + 1e-20).T\n\n  else:\n    raise ValueError(\'Unknown features type: {}\'.format(features_type))\n\n  norm_axis = 0 if norm_per_feature else None\n  if mean is None:\n    mean = np.mean(features, axis=norm_axis)\n  if std_dev is None:\n    std_dev = np.std(features, axis=norm_axis)\n\n  features = (features - mean) / std_dev\n\n  if augmentation:\n    n_freq_mask = augmentation.get(\'n_freq_mask\', 0)\n    n_time_mask = augmentation.get(\'n_time_mask\', 0)\n    width_freq_mask = augmentation.get(\'width_freq_mask\', 10)\n    width_time_mask = augmentation.get(\'width_time_mask\', 50)\n\n    for idx in range(n_freq_mask):\n      freq_band = np.random.randint(width_freq_mask + 1)\n      freq_base = np.random.randint(0, features.shape[1] - freq_band)\n      features[:, freq_base:freq_base+freq_band] = 0\n    for idx in range(n_time_mask):\n      time_band = np.random.randint(width_time_mask + 1)\n      if features.shape[0] - time_band > 0:\n        time_base = np.random.randint(features.shape[0] - time_band)\n        features[time_base:time_base+time_band, :] = 0\n\n  # now it is safe to pad\n  # if pad_to > 0:\n  #   if features.shape[0] % pad_to != 0:\n  #     pad_size = pad_to - features.shape[0] % pad_to\n  #     if pad_size != 0:\n  #         features = np.pad(features, ((0,pad_size), (0,0)), mode=\'constant\')\n  return features, audio_duration\n\n\ndef get_speech_features_psf(signal, sample_freq, num_features,\n                            pad_to=8,\n                            features_type=\'spectrogram\',\n                            window_size=20e-3,\n                            window_stride=10e-3,\n                            augmentation=None):\n  """"""Function to convert raw audio signal to numpy array of features.\n  Backend: python_speech_features\n  Args:\n    signal (np.array): np.array containing raw audio signal.\n    sample_freq (float): frames per second.\n    num_features (int): number of speech features in frequency domain.\n    pad_to (int): if specified, the length will be padded to become divisible\n        by ``pad_to`` parameter.\n    features_type (string): \'mfcc\' or \'spectrogram\'.\n    window_size (float): size of analysis window in milli-seconds.\n    window_stride (float): stride of analysis window in milli-seconds.\n    augmentation (dict, optional): dictionary of augmentation parameters. See\n        :func:`augment_audio_signal` for specification and example.\n    apply_window (bool): whether to apply Hann window for mfcc and logfbank.\n        python_speech_features version should accept winfunc if it is True.\n  Returns:\n    np.array: np.array of audio features with shape=[num_time_steps,\n    num_features].\n    audio_duration (float): duration of the signal in seconds\n  """"""\n  if augmentation is not None:\n    signal = augment_audio_signal(signal.astype(np.float32), \n        sample_freq, augmentation)\n  signal = (normalize_signal(signal.astype(np.float32)) * 32767.0).astype(\n      np.int16)\n\n  audio_duration = len(signal) * 1.0 / sample_freq\n\n  n_window_size = int(sample_freq * window_size)\n  n_window_stride = int(sample_freq * window_stride)\n\n  # making sure length of the audio is divisible by 8 (fp16 optimization)\n  length = 1 + int(math.ceil(\n      (1.0 * signal.shape[0] - n_window_size) / n_window_stride\n  ))\n  if pad_to > 0:\n    if length % pad_to != 0:\n      pad_size = (pad_to - length % pad_to) * n_window_stride\n      signal = np.pad(signal, (0, pad_size), mode=\'constant\')\n\n  if features_type == \'spectrogram\':\n    frames = psf.sigproc.framesig(sig=signal,\n                                  frame_len=n_window_size,\n                                  frame_step=n_window_stride,\n                                  winfunc=np.hanning)\n\n    # features = np.log1p(psf.sigproc.powspec(frames, NFFT=N_window_size))\n    features = psf.sigproc.logpowspec(frames, NFFT=n_window_size)\n    assert num_features <= n_window_size // 2 + 1, \\\n      ""num_features for spectrogram should be <= (sample_freq * window_size // 2 + 1)""\n\n    # cut high frequency part\n    features = features[:, :num_features]\n\n  elif features_type == \'mfcc\':\n    features = psf.mfcc(signal=signal,\n                        samplerate=sample_freq,\n                        winlen=window_size,\n                        winstep=window_stride,\n                        numcep=num_features,\n                        nfilt=2 * num_features,\n                        nfft=512,\n                        lowfreq=0, highfreq=None,\n                        preemph=0.97,\n                        ceplifter=2 * num_features,\n                        appendEnergy=False)\n\n  elif features_type == \'logfbank\':\n    features = psf.logfbank(signal=signal,\n                            samplerate=sample_freq,\n                            winlen=window_size,\n                            winstep=window_stride,\n                            nfilt=num_features,\n                            nfft=512,\n                            lowfreq=0, highfreq=sample_freq / 2,\n                            preemph=0.97)\n  else:\n    raise ValueError(\'Unknown features type: {}\'.format(features_type))\n\n  if pad_to > 0:\n    assert features.shape[0] % pad_to == 0\n  mean = np.mean(features)\n  std_dev = np.std(features)\n  features = (features - mean) / std_dev\n\n  return features, audio_duration\n\n'"
open_seq2seq/data/speech2text/speech_utils_test.py,2,"b""# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport math\nimport os\n\nimport numpy as np\nimport numpy.testing as npt\nimport scipy.io.wavfile as wave\nimport tensorflow as tf\nfrom six.moves import range\n\nfrom .speech_utils import get_speech_features, get_speech_features_from_file, \\\n                          augment_audio_signal\n\n\nclass SpeechUtilsTests(tf.test.TestCase):\n\n  def test_augment_audio_signal(self):\n    filename = 'open_seq2seq/test_utils/toy_speech_data/wav_files/46gc040q.wav'\n    freq_s, signal = wave.read(filename)\n    signal = signal.astype(np.float32)\n    augmentation = {\n        'speed_perturbation_ratio': 0.2,\n        'noise_level_min': -90,\n        'noise_level_max': -46,\n    }\n    # just checking length requirements here for now\n    for _ in range(100):\n      signal_augm = augment_audio_signal(signal, freq_s, augmentation)\n      self.assertLessEqual(signal.shape[0] * 0.8, signal_augm.shape[0])\n      self.assertGreaterEqual(signal.shape[0] * 1.2, signal_augm.shape[0])\n    augmentation = {\n        'speed_perturbation_ratio': 0.5,\n        'noise_level_min': -90,\n        'noise_level_max': -46,\n    }\n    # just checking length requirements here for now\n    for _ in range(100):\n      signal_augm = augment_audio_signal(signal, freq_s, augmentation)\n      self.assertLessEqual(signal.shape[0] * 0.5, signal_augm.shape[0])\n      self.assertGreaterEqual(signal.shape[0] * 1.5, signal_augm.shape[0])\n\n  def test_get_speech_features_from_file(self):\n    dirname = 'open_seq2seq/test_utils/toy_speech_data/wav_files/'\n    for name in ['46gc040q.wav', '206o0103.wav', '48rc041b.wav']:\n      filename = os.path.join(dirname, name)\n      for num_features in [161, 120]:\n        for window_stride in [10e-3, 5e-3, 40e-3]:\n          for window_size in [20e-3, 30e-3]:\n            for features_type in ['spectrogram', 'mfcc', 'logfbank']:\n              freq_s, signal = wave.read(filename)\n              n_window_size = int(freq_s * window_size)\n              n_window_stride = int(freq_s * window_stride)\n              length = 1 + (signal.shape[0] - n_window_size)// n_window_stride\n              if length % 8 != 0:\n                length += 8 - length % 8\n              right_shape = (length, num_features)\n              params = {}\n              params['num_audio_features'] = num_features\n              params['input_type'] = features_type\n              params['window_size'] = window_size\n              params['window_stride'] = window_stride\n              params['sample_freq'] = 16000\n              input_features, _ = get_speech_features_from_file(\n                  filename,\n                  params\n              )\n              self.assertTrue(input_features.shape[0] % 8 == 0)\n              self.assertTupleEqual(right_shape, input_features.shape)\n              self.assertAlmostEqual(np.mean(input_features), 0.0, places=6)\n              self.assertAlmostEqual(np.std(input_features), 1.0, places=6)\n            # only for spectrogram\n            with self.assertRaises(AssertionError):\n              params = {}\n              params['num_audio_features'] = n_window_size // 2 + 2\n              params['input_type'] = 'spectrogram'\n              params['window_size'] = window_size\n              params['window_stride'] = window_stride\n              params['sample_freq'] = 16000\n              get_speech_features_from_file(\n                  filename,\n                  params\n              )\n\n  def test_get_speech_features_from_file_augmentation(self):\n    augmentation = {\n        'speed_perturbation_ratio': 0.0,\n        'noise_level_min': -90,\n        'noise_level_max': -46,\n    }\n    filename = 'open_seq2seq/test_utils/toy_speech_data/wav_files/46gc040q.wav'\n    num_features = 161\n    params = {}\n    params['sample_freq'] = 16000\n    params['num_audio_features'] = num_features\n    input_features_clean, _ = get_speech_features_from_file(\n        filename, params\n    )\n    params['augmentation'] = augmentation\n    input_features_augm, _ = get_speech_features_from_file(\n        filename, params\n    )\n    # just checking that result is different with and without augmentation\n    self.assertTrue(np.all(np.not_equal(input_features_clean,\n                                        input_features_augm)))\n\n    augmentation = {\n        'speed_perturbation_ratio': 0.2,\n        'noise_level_min': -90,\n        'noise_level_max': -46,\n    }\n    params['augmentation'] = augmentation\n    input_features_augm, _ = get_speech_features_from_file(\n        filename, params\n    )\n    self.assertNotEqual(\n        input_features_clean.shape[0],\n        input_features_augm.shape[0],\n    )\n    self.assertEqual(\n        input_features_clean.shape[1],\n        input_features_augm.shape[1],\n    )\n\n  def tst_get_speech_features_with_sine(self):\n    freq_s = 16000.0\n    t_s = np.arange(0, 0.5, 1.0 / freq_s)\n    signal = np.sin(2 * np.pi * 4000 * t_s)\n    features, _ = get_speech_features(signal, freq_s, 161)\n    npt.assert_allclose(\n        np.abs(features - features[0]),\n        np.zeros_like(features),\n        atol=1e-6,\n    )\n    for i in range(80):\n      npt.assert_allclose(features[:, 79 - i], features[:, 81 + i], atol=1e-6)\n      self.assertGreater(features[0, 80 - i], features[0, 80 - i - 1])\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
open_seq2seq/data/text2speech/__init__.py,0,b''
open_seq2seq/data/text2speech/speech_utils.py,0,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport librosa\nimport librosa.filters\nimport resampy as rs\n\ndef get_speech_features_from_file(\n    filename,\n    num_features,\n    features_type=\'magnitude\',\n    n_fft=1024,\n    hop_length=None,\n    mag_power=2,\n    feature_normalize=False,\n    mean=0.,\n    std=1.,\n    trim=False,\n    data_min=1e-5,\n    return_raw_audio=False,\n    return_audio_duration=False,\n    augmentation=None,\n    mel_basis=None\n):\n  """""" Helper function to retrieve spectrograms from wav files\n\n  Args:\n    filename (string): WAVE filename.\n    num_features (int): number of speech features in frequency domain.\n    features_type (string): \'magnitude\' or \'mel\'.\n    n_fft (int): size of analysis window in samples.\n    hop_length (int): stride of analysis window in samples.\n    mag_power (int): power to raise magnitude spectrograms (prior to dot product\n      with mel basis)\n      1 for energy spectrograms\n      2 fot power spectrograms\n    feature_normalize (bool): whether to normalize the data with mean and std\n    mean (float): if normalize is enabled, the mean to normalize to\n    std (float): if normalize is enabled, the deviation to normalize to\n    trim (bool): Whether to trim silence via librosa or not\n    data_min (float): min clip value prior to taking the log.\n\n  Returns:\n    np.array: np.array of audio features with shape=[num_time_steps,\n    num_features].\n  """"""\n  # load audio signal\n  signal, fs = librosa.core.load(filename, sr=None)\n  if hop_length is None:\n    hop_length = int(n_fft / 4)\n  if trim:\n    signal, _ = librosa.effects.trim(\n        signal,\n        frame_length=int(n_fft/2),\n        hop_length=int(hop_length/2)\n    )\n\n  if augmentation is not None:\n    if \'pitch_shift_steps\' in augmentation:\n      pitch_shift_steps = (2.0 * augmentation[\'pitch_shift_steps\'] * \\\n          np.random.rand()) - augmentation[\'pitch_shift_steps\']\n      signal = librosa.effects.pitch_shift(signal, fs, pitch_shift_steps)\n\n    if augmentation[\'time_stretch_ratio\'] > 0:\n      # time stretch\n      stretch_amount = 1.0 + (2.0 * np.random.rand() - 1.0) * \\\n          augmentation[\'time_stretch_ratio\']\n      signal = rs.resample(\n          signal,\n          fs,\n          int(fs * stretch_amount),\n          filter=\'kaiser_fast\',\n      )\n\n      # noise\n      noise_level_db = np.random.randint(\n          low=augmentation[\'noise_level_min\'],\n          high=augmentation[\'noise_level_max\']\n      )\n      signal += np.random.randn(signal.shape[0]) * \\\n          10.0 ** (noise_level_db / 20.0)\n\n  speech_features = get_speech_features(\n      signal, fs, num_features, features_type, n_fft,\n      hop_length, mag_power, feature_normalize, mean, std, data_min, mel_basis\n  )\n\n  if return_raw_audio:\n    return signal, speech_features\n  elif return_audio_duration:\n    return speech_features, len(signal) * 1.0 / fs\n  else:\n    return speech_features\n\n\ndef get_speech_features(\n    signal,\n    fs,\n    num_features,\n    features_type=\'magnitude\',\n    n_fft=1024,\n    hop_length=256,\n    mag_power=2,\n    feature_normalize=False,\n    mean=0.,\n    std=1.,\n    data_min=1e-5,\n    mel_basis=None\n):\n  """""" Helper function to retrieve spectrograms from loaded wav\n\n  Args:\n    signal: signal loaded with librosa.\n    fs (int): sampling frequency in Hz.\n    num_features (int): number of speech features in frequency domain.\n    features_type (string): \'magnitude\' or \'mel\'.\n    n_fft (int): size of analysis window in samples.\n    hop_length (int): stride of analysis window in samples.\n    mag_power (int): power to raise magnitude spectrograms (prior to dot product\n      with mel basis)\n      1 for energy spectrograms\n      2 fot power spectrograms\n    feature_normalize(bool): whether to normalize the data with mean and std\n    mean(float): if normalize is enabled, the mean to normalize to\n    std(float): if normalize is enabled, the deviation to normalize to\n    data_min (float): min clip value prior to taking the log.\n\n  Returns:\n    np.array: np.array of audio features with shape=[num_time_steps,\n    num_features].\n  """"""\n  if isinstance(data_min, dict):\n    data_min_mel = data_min[""mel""]\n    data_min_mag = data_min[""magnitude""]\n  else:\n    data_min_mel = data_min_mag = data_min\n\n  if isinstance(num_features, dict):\n    num_features_mel = num_features[""mel""]\n    num_features_mag = num_features[""magnitude""]\n  else:\n    num_features_mel = num_features_mag = num_features\n\n  complex_spec = librosa.stft(y=signal, n_fft=n_fft)\n  mag, _ = librosa.magphase(complex_spec, power=mag_power)\n\n  if features_type == \'magnitude\' or features_type == ""both"":\n    features = np.log(np.clip(mag, a_min=data_min_mag, a_max=None)).T\n    assert num_features_mag <= n_fft // 2 + 1, \\\n        ""num_features for spectrogram should be <= (fs * window_size // 2 + 1)""\n\n    # cut high frequency part\n    features = features[:, :num_features_mag]\n\n  if \'mel\' in features_type or features_type == ""both"":\n    if features_type == ""both"":\n      mag_features = features\n    if mel_basis is None:\n      htk = True\n      norm = None\n      if \'slaney\' in features_type:\n        htk = False\n        norm = 1\n      mel_basis = librosa.filters.mel(\n          sr=fs,\n          n_fft=n_fft,\n          n_mels=num_features_mel,\n          htk=htk,\n          norm=norm\n      )\n    features = np.dot(mel_basis, mag)\n    features = np.log(np.clip(features, a_min=data_min_mel, a_max=None)).T\n\n  if feature_normalize:\n    features = normalize(features, mean, std)\n\n  if features_type == ""both"":\n    return [features, mag_features]\n\n  return features\n\ndef get_mel(\n    log_mag_spec,\n    fs=22050,\n    n_fft=1024,\n    n_mels=80,\n    power=2.,\n    feature_normalize=False,\n    mean=0,\n    std=1,\n    mel_basis=None,\n    data_min=1e-5,\n    htk=True,\n    norm=None\n):\n  """"""\n  Method to get mel spectrograms from magnitude spectrograms\n\n  Args:\n    log_mag_spec (np.array): log of the magnitude spec\n    fs (int): sampling frequency in Hz\n    n_fft (int): size of fft window in samples\n    n_mels (int): number of mel features\n    power (float): power of the mag spectrogram\n    feature_normalize (bool): whether the mag spec was normalized\n    mean (float): normalization param of mag spec\n    std (float): normalization param of mag spec\n    mel_basis (np.array): optional pre-computed mel basis to save computational\n      time if passed. If not passed, it will call librosa to construct one\n    data_min (float): min clip value prior to taking the log.\n    htk (bool): whther to compute the mel spec with the htk or slaney algorithm\n    norm: Should be None for htk, and 1 for slaney\n\n  Returns:\n    np.array: mel_spec with shape [time, n_mels]\n  """"""\n  if mel_basis is None:\n    mel_basis = librosa.filters.mel(\n        fs,\n        n_fft,\n        n_mels=n_mels,\n        htk=htk,\n        norm=norm\n    )\n  log_mag_spec = log_mag_spec * power\n  mag_spec = np.exp(log_mag_spec)\n  mel_spec = np.dot(mag_spec, mel_basis.T)\n  mel_spec = np.log(np.clip(mel_spec, a_min=data_min, a_max=None))\n  if feature_normalize:\n    mel_spec = normalize(mel_spec, mean, std)\n  return mel_spec\n\n\ndef inverse_mel(\n    log_mel_spec,\n    fs=22050,\n    n_fft=1024,\n    n_mels=80,\n    power=2.,\n    feature_normalize=False,\n    mean=0,\n    std=1,\n    mel_basis=None,\n    htk=True,\n    norm=None\n):\n  """"""\n  Reconstructs magnitude spectrogram from a mel spectrogram by multiplying it\n  with the transposed mel basis.\n\n  Args:\n    log_mel_spec (np.array): log of the mel spec\n    fs (int): sampling frequency in Hz\n    n_fft (int): size of fft window in samples\n    n_mels (int): number of mel features\n    power (float): power of the mag spectrogram that was used to generate the\n      mel spec\n    feature_normalize (bool): whether the mel spec was normalized\n    mean (float): normalization param of mel spec\n    std (float): normalization param of mel spec\n    mel_basis (np.array): optional pre-computed mel basis to save computational\n      time if passed. If not passed, it will call librosa to construct one\n    htk (bool): whther to compute the mel spec with the htk or slaney algorithm\n    norm: Should be None for htk, and 1 for slaney\n\n  Returns:\n    np.array: mag_spec with shape [time, n_fft/2 + 1]\n  """"""\n  if mel_basis is None:\n    mel_basis = librosa.filters.mel(\n        fs,\n        n_fft,\n        n_mels=n_mels,\n        htk=htk,\n        norm=norm\n    )\n  if feature_normalize:\n    log_mel_spec = denormalize(log_mel_spec, mean, std)\n  mel_spec = np.exp(log_mel_spec)\n  mag_spec = np.dot(mel_spec, mel_basis)\n  mag_spec = np.power(mag_spec, 1. / power)\n  return mag_spec\n\n\ndef normalize(features, mean, std):\n  """"""\n  Normalizes features with the specificed mean and std\n  """"""\n  return (features - mean) / std\n\n\ndef denormalize(features, mean, std):\n  """"""\n  Normalizes features with the specificed mean and std\n  """"""\n  return features * std + mean\n'"
open_seq2seq/data/text2speech/text2speech.py,22,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport six\nimport librosa\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\n\nfrom six import string_types\n\nfrom open_seq2seq.data.data_layer import DataLayer\nfrom open_seq2seq.data.utils import load_pre_existing_vocabulary\nfrom .speech_utils import get_speech_features_from_file,\\\n                          inverse_mel, normalize, denormalize\n\nclass Text2SpeechDataLayer(DataLayer):\n  """"""\n  Text-to-speech data layer class\n  """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(\n        DataLayer.get_required_params(), **{\n            \'dataset_location\': str,\n            \'dataset\': [\'LJ\', \'MAILABS\'],\n            \'num_audio_features\': None,\n            \'output_type\': [\'magnitude\', \'mel\', \'both\'],\n            \'vocab_file\': str,\n            \'dataset_files\': list,\n            \'feature_normalize\': bool,\n        }\n    )\n\n  @staticmethod\n  def get_optional_params():\n    return dict(\n        DataLayer.get_optional_params(), **{\n            \'pad_to\': int,\n            \'mag_power\': int,\n            \'pad_EOS\': bool,\n            \'pad_value\': float,\n            \'feature_normalize_mean\': float,\n            \'feature_normalize_std\': float,\n            \'trim\': bool,\n            \'data_min\': None,\n            \'duration_min\': int,\n            \'duration_max\': int,\n            \'mel_type\': [\'slaney\', \'htk\'],\n            ""exp_mag"": bool,\n            \'style_input\': [None, \'wav\'],\n            \'n_samples_train\': int,\n            \'n_samples_eval\': int,\n            \'n_fft\': int,\n            \'fmax\': float,\n            \'max_normalization\': bool,\n            \'use_cache\': bool\n        }\n    )\n\n  def __init__(self, params, model, num_workers=None, worker_id=None):\n    """"""Text-to-speech data layer constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **dataset** (str) --- The dataset to use. Currently \'LJ\' for the LJSpeech\n      1.1 dataset is supported.\n    * **num_audio_features** (int) --- number of audio features to extract.\n    * **output_type** (str) --- could be either ""magnitude"", or ""mel"".\n    * **vocab_file** (str) --- path to vocabulary file.\n    * **dataset_files** (list) --- list with paths to all dataset .csv files.\n      File is assumed to be separated by ""|"".\n    * **dataset_location** (string) --- string with path to directory where wavs\n      are stored.\n    * **feature_normalize** (bool) --- whether to normlize the data with a\n      preset mean and std\n    * **feature_normalize_mean** (bool) --- used for feature normalize.\n      Defaults to 0.\n    * **feature_normalize_std** (bool) --- used for feature normalize.\n      Defaults to 1.\n    * **mag_power** (int) --- the power to which the magnitude spectrogram is\n      scaled to. Defaults to 1.\n      1 for energy spectrogram\n      2 for power spectrogram\n      Defaults to 2.\n    * **pad_EOS** (bool) --- whether to apply EOS tokens to both the text and\n      the speech signal. Will pad at least 1 token regardless of pad_to value.\n      Defaults to True.\n    * **pad_value** (float) --- The value we pad the spectrogram with. Defaults\n      to np.log(data_min).\n    * **pad_to** (int) --- we pad such that the resulting datapoint is a\n      multiple of pad_to.\n      Defaults to 8.\n    * **trim** (bool) --- Whether to trim silence via librosa or not. Defaults\n      to False.\n    * **data_min** (float) --- min clip value prior to taking the log. Defaults\n      to 1e-5. Please change to 1e-2 if using htk mels.\n    * **duration_min** (int) --- Minimum duration in steps for speech signal.\n      All signals less than this will be cut from the training set. Defaults to\n      0.\n    * **duration_max** (int) --- Maximum duration in steps for speech signal.\n      All signals greater than this will be cut from the training set. Defaults\n      to 4000.\n    * **mel_type** (str) --- One of [\'slaney\', \'htk\']. Decides which algorithm to\n      use to compute mel specs.\n      Defaults to htk.\n    * **style_input** (str) --- Can be either None or ""wav"". Must be set to ""wav""\n      for GST. Defaults to None.\n    * **n_samples_train** (int) --- number of the shortest examples to use for training.\n    * **n_samples_eval** (int) --- number of the shortest examples to use for evaluation.\n    * **n_fft** (int) --- FFT window size.\n    * **fmax** (float) --- highest frequency to use.\n    * **max_normalization** (bool) --- whether to divide the final audio signal\n      by its\' absolute maximum.\n    * **use_cache** (bool) --- whether to use cache.\n\n    """"""\n    super(Text2SpeechDataLayer, self).__init__(\n        params,\n        model,\n        num_workers,\n        worker_id\n    )\n\n    self.use_cache = self.params.get(\'use_cache\', False)\n    self._cache = {}\n\n    names = [\'wav_filename\', \'raw_transcript\', \'transcript\']\n    sep = \'\\x7c\'\n    header = None\n\n    if self.params[""dataset""] == ""LJ"":\n      self._sampling_rate = 22050\n      self._n_fft = self.params.get(""n_fft"", 1024)\n    elif self.params[""dataset""] == ""MAILABS"":\n      self._sampling_rate = 16000\n      self._n_fft = 800\n\n    # Character level vocab\n    self.params[\'char2idx\'] = load_pre_existing_vocabulary(\n        self.params[\'vocab_file\'],\n        min_idx=3,\n        read_chars=True,\n    )\n    # Add the pad, start, and end chars\n    self.params[\'char2idx\'][\'<p>\'] = 0\n    self.params[\'char2idx\'][\'<s>\'] = 1\n    self.params[\'char2idx\'][\'</s>\'] = 2\n    self.params[\'idx2char\'] = {i: w for w, i in self.params[\'char2idx\'].items()}\n    self.params[\'src_vocab_size\'] = len(self.params[\'char2idx\'])\n\n    self.max_normalization = self.params.get(\'max_normalization\', False)\n\n    n_feats = self.params[\'num_audio_features\']\n    if ""both"" in self.params[""output_type""]:\n      self._both = True\n      if self.params[""feature_normalize""]:\n        raise ValueError(\n            ""feature normalize is not currently enabled for both mode""\n        )\n      if not isinstance(n_feats, dict):\n        raise ValueError(\n            ""num_audio_features must be a dictionary for both mode""\n        )\n      else:\n        if (""mel"" not in n_feats and\n            ""magnitude"" not in n_feats):\n          raise ValueError(\n            ""num_audio_features must contain mel and magnitude keys""\n          )\n        elif (not isinstance(n_feats[""mel""], int) or\n              not isinstance(n_feats[""magnitude""], int)):\n            raise ValueError(\n                ""num_audio_features must be a int""\n            )\n      n_mels = n_feats[\'mel\']\n      data_min = self.params.get(""data_min"", None)\n      if data_min is not None:\n        if not isinstance(data_min, dict):\n          raise ValueError(\n              ""data_min must be a dictionary for both mode""\n          )\n        else:\n          if ""mel"" not in data_min and ""magnitude"" not in data_min:\n            raise ValueError(\n              ""data_min must contain mel and magnitude keys""\n            )\n          elif (not isinstance(data_min[""mel""], float) or \n                not isinstance(data_min[""magnitude""], float)):\n            raise ValueError(\n                ""data_min must be a float""\n            )\n      self._exp_mag = self.params.get(""exp_mag"", True)\n    else:\n      if not isinstance(n_feats, int):\n        raise ValueError(\n            ""num_audio_features must be a float for mel or magnitude mode""\n        )\n      if not isinstance(self.params.get(""data_min"",1.0), float):\n        raise ValueError(\n            ""data_min must be a float for mel or magnitude mode""\n        )\n      self._both = False\n      self._exp_mag = False\n      n_mels = n_feats\n\n    self._mel = ""mel"" in self.params[""output_type""]\n\n    if self._mel or self._both:\n      htk = True\n      norm = None\n      if self.params.get(\'mel_type\', \'htk\') == \'slaney\':\n        htk = False\n        norm = 1\n      self._mel_basis = librosa.filters.mel(\n          sr=self._sampling_rate,\n          n_fft=self._n_fft,\n          n_mels=n_mels,\n          htk=htk,\n          norm=norm,\n          fmax=self.params.get(\'fmax\', None)\n      )\n    else:\n      self._mel_basis = None\n\n    if self.params[""interactive""]:\n      return\n\n    # Load csv files\n    self._files = None\n    for csvs in params[\'dataset_files\']:\n      files = pd.read_csv(\n          csvs,\n          encoding=\'utf-8\',\n          sep=sep,\n          header=header,\n          names=names,\n          quoting=3\n      )\n      if self._files is None:\n        self._files = files\n      else:\n        self._files = self._files.append(files)\n\n    if self.params[\'mode\'] == \'train\' and \'n_samples_train\' in self.params:\n      indices = self._files[\'transcript\'].str.len().sort_values().index\n      self._files = self._files.reindex(indices)\n\n      n_samples = self.params.get(\'n_samples_train\')\n      print(\'Using just the {} shortest samples\'.format(n_samples))\n      self._files = self._files.iloc[:n_samples]\n\n    if self.params[\'mode\'] == \'eval\':\n      indices = self._files[\'transcript\'].str.len().sort_values().index\n      self._files = self._files.reindex(indices)\n\n      if \'n_samples_eval\' in self.params:\n        n_samples = self.params[\'n_samples_eval\']\n        self._files = self._files.iloc[:n_samples]\n\n    if (self.params[\'mode\'] != \'infer\'\n        or self.params.get(""style_input"", None) == ""wav""):\n      cols = [\'wav_filename\', \'transcript\']\n    else:\n      cols = \'transcript\'\n\n    all_files = self._files.loc[:, cols].values\n    self._files = self.split_data(all_files)\n\n    self._size = self.get_size_in_samples()\n    self._dataset = None\n    self._iterator = None\n    self._input_tensors = None\n\n  def split_data(self, data):\n    if self.params[\'mode\'] != \'train\' and self._num_workers is not None:\n      size = len(data)\n      start = size // self._num_workers * self._worker_id\n      if self._worker_id == self._num_workers - 1:\n        end = size\n      else:\n        end = size // self._num_workers * (self._worker_id + 1)\n      return data[start:end]\n    return data\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  def build_graph(self):\n    with tf.device(\'/cpu:0\'):\n      """"""Builds data reading graph.""""""\n\n      self._dataset = tf.data.Dataset.from_tensor_slices(self._files)\n      if self.params[\'shuffle\']:\n        self._dataset = self._dataset.shuffle(self._size)\n      self._dataset = self._dataset.repeat()\n\n      if self._both:\n        num_audio_features = self.params[\'num_audio_features\'][\'mel\']\n        num_audio_features += self.params[\'num_audio_features\'][\'magnitude\']\n      else:\n        num_audio_features = self.params[\'num_audio_features\']\n\n      if (self.params[\'mode\'] != \'infer\'\n          or self.params.get(""style_input"", None) == ""wav""):\n        self._dataset = self._dataset.map(\n            lambda line: tf.py_func(\n                self._parse_audio_transcript_element,\n                [line],\n                [tf.int32, tf.int32, self.params[\'dtype\'], self.params[\'dtype\'],\\\n                 tf.int32],\n                stateful=False,\n            ),\n            num_parallel_calls=8,\n        )\n        if (self.params.get(""duration_max"", None) or\n            self.params.get(""duration_max"", None)):\n          self._dataset = self._dataset.filter(\n              lambda txt, txt_len, spec, stop, spec_len:\n                  tf.logical_and(\n                      tf.less_equal(\n                          spec_len,\n                          self.params.get(""duration_max"", 4000)\n                      ),\n                      tf.greater_equal(\n                          spec_len,\n                          self.params.get(""duration_min"", 0)\n                      )\n                  )\n          )\n        if self._both:\n          default_pad_value = 0.\n        else:\n          default_pad_value = np.log(self.params.get(""data_min"", 1e-5))\n        pad_value = self.params.get(""pad_value"", default_pad_value)\n        if self.params[""feature_normalize""]:\n          pad_value = self._normalize(pad_value)\n        self._dataset = self._dataset.padded_batch(\n            self.params[\'batch_size\'],\n            padded_shapes=(\n                [None], 1, [None, num_audio_features], [None], 1\n            ),\n            padding_values=(\n                0, 0, tf.cast(pad_value, dtype=self.params[\'dtype\']),\n                tf.cast(1., dtype=self.params[\'dtype\']), 0\n            )\n        )\n      else:\n        self._dataset = self._dataset.map(\n            lambda line: tf.py_func(\n                self._parse_transcript_element,\n                [line],\n                [tf.int32, tf.int32],\n                stateful=False,\n            ),\n            num_parallel_calls=8,\n        )\n        self._dataset = self._dataset.padded_batch(\n            self.params[\'batch_size\'], padded_shapes=([None], 1)\n        )\n\n      self._iterator = self._dataset.prefetch(tf.contrib.data.AUTOTUNE)\\\n                                    .make_initializable_iterator()\n\n      if (self.params[\'mode\'] != \'infer\'\n          or self.params.get(""style_input"", None) == ""wav""):\n        text, text_length, spec, stop_token_target, spec_length = self._iterator\\\n                                                                      .get_next()\n        # need to explicitly set batch size dimension\n        # (it is employed in the model)\n        spec.set_shape(\n            [self.params[\'batch_size\'], None, num_audio_features]\n        )\n        stop_token_target.set_shape([self.params[\'batch_size\'], None])\n        spec_length = tf.reshape(spec_length, [self.params[\'batch_size\']])\n      else:\n        text, text_length = self._iterator.get_next()\n      text.set_shape([self.params[\'batch_size\'], None])\n      text_length = tf.reshape(text_length, [self.params[\'batch_size\']])\n\n      self._input_tensors = {}\n      self._input_tensors[""source_tensors""] = [text, text_length]\n      if self.params.get(""style_input"", None) == ""wav"":\n        # mag - not supported currently\n        if not self._mel and not self._both:\n          raise ValueError(\n              ""GST is currently only supported on mel and both output modes."")\n        # mel\n        mel_spec = spec\n        if self._both:\n          mel_spec, _ = tf.split(\n              mel_spec,\n              [self.params[\'num_audio_features\'][\'mel\'],\n               self.params[\'num_audio_features\'][\'magnitude\']],\n              axis=2\n          )\n        self._input_tensors[""source_tensors""].extend([mel_spec, spec_length])\n        # both\n      if self.params[\'mode\'] != \'infer\':\n        self._input_tensors[\'target_tensors\'] = [\n            spec, stop_token_target, spec_length\n        ]\n\n  def _parse_audio_transcript_element(self, element):\n    """"""Parses tf.data element from TextLineDataset into audio and text.\n\n    Args:\n      element: tf.data element from TextLineDataset.\n\n    Returns:\n      tuple: text_input text as `np.array` of ids, text_input length,\n      target audio features as `np.array`, stop token targets as `np.array`,\n      length of target sequence.\n\n    """"""\n    audio_filename, transcript = element\n    transcript = transcript.lower()\n    if six.PY2:\n      audio_filename = unicode(audio_filename, ""utf-8"")\n      transcript = unicode(transcript, ""utf-8"")\n    elif not isinstance(transcript, string_types):\n      audio_filename = str(audio_filename, ""utf-8"")\n      transcript = str(transcript, ""utf-8"")\n    text_input = np.array(\n        [self.params[\'char2idx\'][c] for c in transcript]\n    )\n    pad_to = self.params.get(\'pad_to\', 8)\n    if self.params.get(""pad_EOS"", True):\n      num_pad = pad_to - ((len(text_input) + 2) % pad_to)\n      text_input = np.pad(\n          text_input, ((1, 1)),\n          ""constant"",\n          constant_values=(\n              (self.params[\'char2idx\'][""<s>""], self.params[\'char2idx\'][""</s>""])\n          )\n      )\n      text_input = np.pad(\n          text_input, ((0, num_pad)),\n          ""constant"",\n          constant_values=self.params[\'char2idx\'][""<p>""]\n      )\n\n    # Mainly used for GST\n    if ""wavs"" in audio_filename:\n      file_path = os.path.join(\n          self.params[\'dataset_location\'], audio_filename + "".wav""\n      )\n    # Default path for LJ and MAILABS\n    else:\n      file_path = os.path.join(\n          self.params[\'dataset_location\'], ""wavs"", audio_filename + "".wav""\n      )\n    if self._mel:\n      features_type = ""mel_htk""\n      if self.params.get(\'mel_type\', \'htk\') == \'slaney\':\n        features_type = ""mel_slaney""\n    else:\n      features_type = self.params[\'output_type\']\n\n    if self.use_cache and audio_filename in self._cache:\n      spectrogram = self._cache[audio_filename]\n    else:\n      spectrogram = get_speech_features_from_file(\n          file_path,\n          self.params[\'num_audio_features\'],\n          features_type=features_type,\n          n_fft=self._n_fft,\n          mag_power=self.params.get(\'mag_power\', 2),\n          feature_normalize=self.params[""feature_normalize""],\n          mean=self.params.get(""feature_normalize_mean"", 0.),\n          std=self.params.get(""feature_normalize_std"", 1.),\n          trim=self.params.get(""trim"", False),\n          data_min=self.params.get(""data_min"", 1e-5),\n          mel_basis=self._mel_basis\n      )\n\n      if self.use_cache:\n        self._cache[audio_filename] = spectrogram\n\n    if self._both:\n      mel_spectrogram, spectrogram = spectrogram\n      if self._exp_mag:\n        spectrogram = np.exp(spectrogram)\n\n    stop_token_target = np.zeros(\n        [len(spectrogram)], dtype=self.params[\'dtype\'].as_numpy_dtype()\n    )\n    if self.params.get(""pad_EOS"", True):\n      num_pad = pad_to - ((len(spectrogram) + 1) % pad_to) + 1\n\n      data_min = self.params.get(""data_min"", 1e-5)\n      if isinstance(data_min, dict):\n        pad_value_mel = self.params.get(""pad_value"", np.log(data_min[""mel""]))\n        if self._exp_mag:\n          pad_value_mag = self.params.get(""pad_value"", data_min[""magnitude""])\n        else:\n          pad_value_mag = self.params.get(""pad_value"", np.log(data_min[""magnitude""]))\n      else:\n        pad_value = self.params.get(""pad_value"", np.log(data_min))\n        if self.params[""feature_normalize""]:\n          pad_value = self._normalize(pad_value)\n          pad_value_mel = pad_value_mag = pad_value\n\n      if self._both:\n        mel_spectrogram = np.pad(\n            mel_spectrogram,\n            # ((8, num_pad), (0, 0)),\n            ((0, num_pad), (0, 0)),\n            ""constant"",\n            constant_values=pad_value_mel\n        )\n        spectrogram = np.pad(\n            spectrogram,\n            # ((8, num_pad), (0, 0)),\n            ((0, num_pad), (0, 0)),\n            ""constant"",\n            constant_values=pad_value_mag\n        )\n        spectrogram = np.concatenate((mel_spectrogram, spectrogram), axis=1)\n      else:\n        spectrogram = np.pad(\n            spectrogram,\n            # ((8, num_pad), (0, 0)),\n            ((0, num_pad), (0, 0)),\n            ""constant"",\n            constant_values=pad_value\n        )\n      stop_token_target = np.pad(\n          stop_token_target, ((0, num_pad)), ""constant"", constant_values=1\n      )\n    else:\n      stop_token_target[-1] = 1.\n\n    assert len(text_input) % pad_to == 0\n    assert len(spectrogram) % pad_to == 0\n    return np.int32(text_input), \\\n           np.int32([len(text_input)]), \\\n           spectrogram.astype(self.params[\'dtype\'].as_numpy_dtype()), \\\n           stop_token_target.astype(self.params[\'dtype\'].as_numpy_dtype()), \\\n           np.int32([len(spectrogram)])\n\n  def _parse_transcript_element(self, transcript):\n    """"""Parses text from file and returns array of text features.\n\n    Args:\n      transcript: the string to parse.\n\n    Returns:\n      tuple: target text as `np.array` of ids, target text length.\n    """"""\n\n    if six.PY2:\n      transcript = unicode(transcript, ""utf-8"")\n    elif not isinstance(transcript, string_types):\n      transcript = str(transcript, ""utf-8"")\n    transcript = transcript.lower()\n\n    text_input = np.array(\n        [self.params[\'char2idx\'].get(c,3) for c in transcript]\n    )\n    pad_to = self.params.get(\'pad_to\', 8)\n    if self.params.get(""pad_EOS"", True):\n      num_pad = pad_to - ((len(text_input) + 2) % pad_to)\n      text_input = np.pad(\n          text_input, ((1, 1)),\n          ""constant"",\n          constant_values=(\n              (self.params[\'char2idx\'][""<s>""], self.params[\'char2idx\'][""</s>""])\n          )\n      )\n      text_input = np.pad(\n          text_input, ((0, num_pad)),\n          ""constant"",\n          constant_values=self.params[\'char2idx\'][""<p>""]\n      )\n\n    return np.int32(text_input), \\\n           np.int32([len(text_input)])\n\n  def parse_text_output(self, text):\n    text = """".join([self.params[\'idx2char\'][k] for k in text])\n    return text\n\n  def create_interactive_placeholders(self):\n    self._text = tf.placeholder(\n        dtype=tf.int32,\n        shape=[self.params[""batch_size""], None]\n    )\n    self._text_length = tf.placeholder(\n        dtype=tf.int32,\n        shape=[self.params[""batch_size""]]\n    )\n\n    self._input_tensors = {}\n    self._input_tensors[""source_tensors""] = [self._text, self._text_length]\n\n  def create_feed_dict(self, model_in):\n    """""" Creates the feed dict for interactive infer\n\n    Args:\n      model_in (str): The string to be spoken.\n\n    Returns:\n      feed_dict (dict): Dictionary with values for the placeholders.\n    """"""\n    text = []\n    text_length = []\n    for line in model_in:\n      if not isinstance(line, string_types):\n        raise ValueError(\n            ""Text2Speech\'s interactive inference mode only supports string."",\n            ""Got {}"". format(type(line))\n        )\n      text_a, text_length_a = self._parse_transcript_element(line)\n      text.append(text_a)\n      text_length.append(text_length_a)\n    max_len = np.max(text_length)\n    for i, line in enumerate(text):\n      line = np.pad(\n          line, ((0, max_len-len(line))),\n          ""constant"", constant_values=self.params[\'char2idx\'][""<p>""]\n      )\n      text[i] = line\n\n    text = np.reshape(text, [self.params[""batch_size""], -1])\n    text_length = np.reshape(text_length, [self.params[""batch_size""]])\n\n    feed_dict = {\n        self._text: text,\n        self._text_length: text_length,\n    }\n    return feed_dict\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n\n  @property\n  def sampling_rate(self):\n    return self._sampling_rate\n\n  @property\n  def n_fft(self):\n    return self._n_fft\n\n  def get_size_in_samples(self):\n    """"""Returns the number of audio files.""""""\n    return len(self._files)\n\n  def get_magnitude_spec(self, spectrogram, is_mel=False):\n    """"""Returns an energy magnitude spectrogram. The processing depends on the\n    data layer params.\n\n    Args:\n      spectrogram: output spec from model\n\n    Returns:\n      mag_spec: mag spec\n    """"""\n    spectrogram = spectrogram.astype(float)\n    if self._mel or (is_mel and self._both):\n      htk = True\n      norm = None\n      if self.params.get(\'mel_type\', \'htk\') == \'slaney\':\n        htk = False\n        norm = 1\n      n_feats = self.params[\'num_audio_features\']\n      if self._both:\n        n_feats = n_feats[""mel""]\n      return inverse_mel(\n          spectrogram,\n          fs=self._sampling_rate,\n          n_fft=self._n_fft,\n          n_mels=n_feats,\n          power=self.params.get(\'mag_power\', 2),\n          feature_normalize=self.params[""feature_normalize""],\n          mean=self.params.get(""feature_normalize_mean"", 0.),\n          std=self.params.get(""feature_normalize_std"", 1.),\n          mel_basis=self._mel_basis,\n          htk=htk,\n          norm=norm\n      )\n    # Else it is a mag spec\n    else:\n      if self.params[""feature_normalize""]:\n        spectrogram = self._denormalize(spectrogram)\n      n_feats = self.params[\'num_audio_features\']\n      data_min = self.params.get(""data_min"", 1e-5)\n      if self._both:\n        n_feats = n_feats[""magnitude""]\n        if isinstance(data_min, dict):\n          data_min = data_min[""magnitude""]\n        if not self._exp_mag:\n          data_min = np.log(data_min)\n      else:\n        data_min = np.log(data_min)\n      # Ensure that num_features is consistent with n_fft\n      if n_feats < self._n_fft // 2 + 1:\n        num_pad = (self._n_fft // 2 + 1) - spectrogram.shape[1]\n        spectrogram = np.pad(\n            spectrogram,\n            ((0, 0), (0, num_pad)),\n            ""constant"",\n            constant_values=data_min\n        )\n      mag_spec = spectrogram * 1.0 / self.params.get(\'mag_power\', 2)\n      if not self._both and not self._exp_mag:\n        mag_spec = np.exp(mag_spec)\n      return mag_spec\n\n  def _normalize(self, spectrogram):\n    return normalize(\n        spectrogram,\n        mean=self.params.get(""feature_normalize_mean"", 0.),\n        std=self.params.get(""feature_normalize_std"", 1.)\n    )\n\n  def _denormalize(self, spectrogram):\n    return denormalize(\n        spectrogram,\n        mean=self.params.get(""feature_normalize_mean"", 0.),\n        std=self.params.get(""feature_normalize_std"", 1.)\n    )\n'"
open_seq2seq/data/text2speech/text2speech_wavenet.py,15,"b'# Copyright (c) 2018 NVIDIA Corporation\nimport os\nimport six\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\n\nfrom open_seq2seq.data.data_layer import DataLayer\nfrom open_seq2seq.data.text2speech.speech_utils import \\\n  get_speech_features_from_file\n\nclass WavenetDataLayer(DataLayer):\n  """""" Text to speech data layer class for Wavenet """"""\n\n  @staticmethod\n  def get_required_params():\n    return dict(\n        DataLayer.get_required_params(), **{\n            ""num_audio_features"": int,\n            ""dataset_files"": list\n        }\n    )\n\n  @staticmethod\n  def get_optional_params():\n    return dict(\n        DataLayer.get_optional_params(), **{\n            ""dataset_location"": str\n        }\n    )\n\n  def __init__(self, params, model, num_workers=None, worker_id=None):\n    """"""\n    Wavenet data layer constructor.\n\n    See parent class for arguments description.\n\n    Config parameters:\n\n    * **num_audio_features** (int) --- number of spectrogram audio features\n    * **dataset_files** (list) --- list with paths to all dataset .csv files\n\n    * **dataset_location** (str) --- string with path to directory where wavs\n      are stored\n    """"""\n\n    super(WavenetDataLayer, self).__init__(\n        params,\n        model,\n        num_workers,\n        worker_id\n    )\n\n    if self.params.get(""dataset_location"", None) is None:\n      raise ValueError(\n          ""dataset_location must be specified when using LJSpeech""\n      )\n\n    names = [""wav_filename"", ""raw_transcript"", ""transcript""]\n    sep = ""\\x7c""\n    header = None\n\n    self.sampling_rate = 22050\n    self.n_fft = 1024\n\n    self._files = None\n    for csvs in params[""dataset_files""]:\n      files = pd.read_csv(\n          csvs,\n          encoding=""utf-8"",\n          sep=sep,\n          header=header,\n          names=names,\n          quoting=3\n      )\n\n      if self._files is None:\n        self._files = files\n      else:\n        self._files = self._files.append(files)\n\n    cols = ""wav_filename""\n    if self._files is not None:\n      all_files = self._files.loc[:, cols].values\n      self._files = self.split_data(all_files)\n\n    self._size = self.get_size_in_samples()\n    self._dataset = None\n    self._iterator = None\n    self._input_tensors = None\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n\n  def get_size_in_samples(self):\n    if self._files is not None:\n      return len(self._files)\n    else:\n      return 0\n\n  def split_data(self, data):\n    if self.params[\'mode\'] != \'train\' and self._num_workers is not None:\n      size = len(data)\n      start = size // self._num_workers * self._worker_id\n\n      if self._worker_id == self._num_workers - 1:\n        end = size\n      else:\n        end = size // self._num_workers * (self._worker_id + 1)\n\n      return data[start:end]\n\n    return data\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  def _parse_audio_element(self, element):\n    """"""Parses tf.data element from TextLineDataset into audio.""""""\n    audio_filename = element\n\n    if six.PY2:\n      audio_filename = unicode(audio_filename, ""utf-8"")\n    else:\n      audio_filename = str(audio_filename, ""utf-8"")\n\n    file_path = os.path.join(\n        self.params[""dataset_location""],\n        audio_filename + "".wav""\n    )\n\n    audio, spectrogram = get_speech_features_from_file(\n        file_path,\n        self.params[""num_audio_features""],\n        features_type=""mel"",\n        data_min=1e-5,\n        return_raw_audio=True\n    )\n\n    spectrogram = np.pad(\n        spectrogram,\n        ((0, 1), (0, 0)),\n        ""constant"",\n        constant_values=1e-5\n    )\n    assert len(audio) < len(spectrogram)*256, \\\n        ""audio len: {}, spec*256 len: {}"".format(len(audio), \\\n        len(spectrogram)*256)\n    num_pad = len(spectrogram)*256 - len(audio)\n    audio = np.pad(\n        audio,\n        (0, num_pad),\n        ""constant"",\n        constant_values=0\n    )\n\n    # upsample the spectrogram to match source length by repeating each value\n    spectrogram = np.repeat(spectrogram, 256, axis=0)\n\n    return audio.astype(self.params[""dtype""].as_numpy_dtype()), \\\n      np.int32([len(audio)]), \\\n      spectrogram.astype(self.params[""dtype""].as_numpy_dtype()), \\\n      np.int32([len(spectrogram)])\n\n  def _parse_spectrogram_element(self, element):\n    audio, au_length, spectrogram, spec_length = \\\n      self._parse_audio_element(element)\n    return spectrogram, spec_length\n\n  def create_interactive_placeholders(self):\n    self._source = tf.placeholder(\n        dtype=self.params[""dtype""],\n        shape=[self.params[""batch_size""], None]\n    )\n    self._src_length = tf.placeholder(\n        dtype=tf.int32,\n        shape=[self.params[""batch_size""]]\n    )\n\n    self._spec = tf.placeholder(\n        dtype=self.params[""dtype""],\n        shape=[self.params[""batch_size""], None,\n               self.params[""num_audio_features""]]\n    )\n    self._spec_length = tf.placeholder(\n        dtype=tf.int32,\n        shape=[self.params[""batch_size""]]\n    )\n    self._spec_offset = tf.placeholder(\n        dtype=tf.int32,\n        shape=()\n    )\n\n    self._input_tensors = {}\n    self._input_tensors[""source_tensors""] = [\n        self._source, self._src_length, self._spec, self._spec_length,\n        self._spec_offset\n    ]\n\n  def create_feed_dict(self, model_in):\n    """"""\n    Creates the feed dict for interactive infer using a spectrogram\n\n    Args:\n      model_in: tuple containing source audio, length of the source, \\\n      conditioning spectrogram, length of the spectrogram, index of \\\n      receptive field window\n    """"""\n\n    source, src_length, spec, spec_length, spec_offset = model_in\n\n    return {\n        self._source: source,\n        self._src_length: src_length,\n        self._spec: spec,\n        self._spec_length: spec_length,\n        self._spec_offset: spec_offset\n    }\n\n  def build_graph(self):\n    """""" builds data reading graph """"""\n    self._dataset = tf.data.Dataset.from_tensor_slices(self._files)\n\n    if self.params[""shuffle""]:\n      self._dataset = self._dataset.shuffle(self._size)\n    self._dataset = self._dataset.repeat()\n\n    num_audio_features = self.params[""num_audio_features""]\n\n    if self.params[""mode""] != ""infer"":\n      self._dataset = self._dataset.map(\n          lambda line: tf.py_func(\n              self._parse_audio_element,\n              [line],\n              [self.params[""dtype""], tf.int32, self.params[""dtype""], tf.int32],\n              stateful=False\n          ),\n          num_parallel_calls=8\n      )\n\n      self._dataset = self._dataset.padded_batch(\n          self.params[""batch_size""],\n          padded_shapes=([None], 1, [None, num_audio_features], 1)\n      )\n\n    else:\n      raise ValueError(""Non-interactive infer is not supported"")\n\n    self._iterator = self._dataset.prefetch(tf.contrib.data.AUTOTUNE) \\\n      .make_initializable_iterator()\n\n    if self.params[""mode""] != ""infer"":\n      source, src_length, spec, spec_length = self._iterator.get_next()\n      spec.set_shape([self.params[""batch_size""], None, num_audio_features])\n      spec_length = tf.reshape(spec_length, [self.params[""batch_size""]])\n\n      source.set_shape([self.params[""batch_size""], None])\n      src_length = tf.reshape(src_length, [self.params[""batch_size""]])\n\n      self._input_tensors = {}\n      self._input_tensors[""source_tensors""] = [\n          source, src_length, spec, spec_length\n      ]\n      self._input_tensors[""target_tensors""] = [source, src_length]\n\n    else:\n      raise ValueError(""Non-interactive infer is not supported"")\n'"
open_seq2seq/data/text2text/__init__.py,0,b''
open_seq2seq/data/text2text/parse_output.py,0,"b'# Copyright (c) 2017 NVIDIA Corporation\n""""""\nThis file takes output of the inference stage produced using\nTransformerDataLayer and converts it to simple tokenized text\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport argparse\n\nimport tokenizer\n\n\ndef main():\n  with open(FLAGS.input_file, \'r\') as in_file:\n    def trim(token):\n      return token[1:-1]\n\n    print(""******Reading from file: {}"".format(FLAGS.input_file))\n    with open(FLAGS.output_file, \'w\') as out_file:\n      print(""******Writing to file: {}"".format(FLAGS.output_file))\n      for line in in_file:\n        # merge and split by _\n        escaped_tokens = """".join([trim(t) for t in line.strip().split("" "")])\n        escaped_tokens = escaped_tokens.split(""_"")\n\n        # unescape\n        unescaped_tokens = []\n        for token in escaped_tokens:\n          if token:\n            unescaped_tokens.append(tokenizer.unescape_token(token))\n\n        # join and write\n        out_file.write(tokenizer.join_tokens_to_string(unescaped_tokens)+\'\\n\')\n  print(""******All done!"")\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--input_file"", ""-if"", type=str, default="""",\n      help=""output of the inference stage produced using model with ""\n           ""TransformerDataLayer"",\n      metavar=""<IF>"")\n  parser.add_argument(\n      ""--output_file"", ""-of"", type=str, default=""tokenized_output.txt"",\n      help=""where to save output"",\n      metavar=""<OF>"")\n  FLAGS, _ = parser.parse_known_args()\n  main()\n'"
open_seq2seq/data/text2text/process_data.py,36,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Download and preprocess WMT17 ende training and evaluation datasets.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport sys\nimport tarfile\nimport urllib\n\nimport six\nimport tensorflow as tf\nimport urllib.request\n\nimport tokenizer\n\n# Data sources for training/evaluating the transformer translation model.\n# If any of the training sources are changed, then either:\n#   1) use the flag `--search` to find the best min count or\n#   2) update the _TRAIN_DATA_MIN_COUNT constant.\n# min_count is the minimum number of times a token must appear in the data\n# before it is added to the vocabulary. ""Best min count"" refers to the value\n# that generates a vocabulary set that is closest in size to _TARGET_VOCAB_SIZE.\n_TRAIN_DATA_SOURCES = [\n    {\n        ""url"": ""http://data.statmt.org/wmt17/translation-task/""\n               ""training-parallel-nc-v12.tgz"",\n        ""input"": ""news-commentary-v12.de-en.en"",\n        ""target"": ""news-commentary-v12.de-en.de"",\n    },\n    {\n        ""url"": ""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n        ""input"": ""commoncrawl.de-en.en"",\n        ""target"": ""commoncrawl.de-en.de"",\n    },\n    {\n        ""url"": ""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n        ""input"": ""europarl-v7.de-en.en"",\n        ""target"": ""europarl-v7.de-en.de"",\n    },\n]\n# Use pre-defined minimum count to generate subtoken vocabulary.\n_TRAIN_DATA_MIN_COUNT = 6\n\n\n_EVAL_DATA_SOURCES = [\n    {\n        ""url"": ""http://data.statmt.org/wmt17/translation-task/dev.tgz"",\n        ""input"": ""newstest2013.en"",\n        ""target"": ""newstest2013.de"",\n    }\n]\n\n_TEST_DATA_SOURCES = [\n    {\n        ""url"": ""https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2014.en"",\n        ""input"": ""newstest2014.en"",\n        ""target"": ""newstest2014.en"",\n    }\n]\n\n# Vocabulary constants\n_TARGET_VOCAB_SIZE = 32768  # Number of subtokens in the vocabulary list.\n_TARGET_THRESHOLD = 327  # Accept vocabulary if size is within this threshold\n_VOCAB_FILE = ""vocab.ende.%d"" % _TARGET_VOCAB_SIZE\n\n# Strings to inclue in the generated files.\n_PREFIX = ""wmt32k""\n_TRAIN_TAG = ""train""\n_EVAL_TAG = ""dev""  # Following WMT and Tensor2Tensor conventions, in which the\n                   # evaluation datasets are tagged as ""dev"" for development.\n_TEST_TAG = ""test""\n\n# Number of files to split train and evaluation data\n_TRAIN_SHARDS = 100\n_EVAL_SHARDS = 1\n_TEST_SHARDS = 1\n\n\ndef find_file(path, filename, max_depth=5):\n  """"""Returns full filepath if the file is in path or a subdirectory.""""""\n  for root, dirs, files in os.walk(path):\n    if filename in files:\n      return os.path.join(root, filename)\n\n    # Don\'t search past max_depth\n    depth = root[len(path) + 1:].count(os.sep)\n    if depth > max_depth:\n      del dirs[:]  # Clear dirs\n  return None\n\n\n###############################################################################\n# Download and extraction functions\n###############################################################################\ndef get_raw_files(raw_dir, data_source):\n  """"""Return raw files from source. Downloads/extracts if needed.\n\n  Args:\n    raw_dir: string directory to store raw files\n    data_source: dictionary with\n      {""url"": url of compressed dataset containing input and target files\n       ""input"": file with data in input language\n       ""target"": file with data in target language}\n\n  Returns:\n    dictionary with\n      {""inputs"": list of files containing data in input language\n       ""targets"": list of files containing corresponding data in target language\n      }\n  """"""\n  raw_files = {\n      ""inputs"": [],\n      ""targets"": [],\n  }  # keys\n  for d in data_source:\n    input_file, target_file = download_and_extract(\n        raw_dir, d[""url""], d[""input""], d[""target""])\n    raw_files[""inputs""].append(input_file)\n    raw_files[""targets""].append(target_file)\n  return raw_files\n\n\ndef download_report_hook(count, block_size, total_size):\n  """"""Report hook for download progress.\n\n  Args:\n    count: current block number\n    block_size: block size\n    total_size: total size\n  """"""\n  percent = int(count * block_size * 100 / total_size)\n  print(""\\r%d%%"" % percent + "" completed"", end=""\\r"")\n\n\ndef download_from_url(path, url):\n  """"""Download content from a url.\n\n  Args:\n    path: string directory where file will be downloaded\n    url: string url\n\n  Returns:\n    Full path to downloaded file\n  """"""\n  filename = url.split(""/"")[-1]\n  found_file = find_file(path, filename, max_depth=0)\n  if found_file is None:\n    filename = os.path.join(path, filename)\n    tf.logging.info(""Downloading from %s to %s."" % (url, filename))\n    inprogress_filepath = filename + "".incomplete""\n    inprogress_filepath, _ = urllib.request.urlretrieve(\n        url, inprogress_filepath, reporthook=download_report_hook)\n    # Print newline to clear the carriage return from the download progress.\n    print()\n    tf.gfile.Rename(inprogress_filepath, filename)\n    return filename\n  else:\n    tf.logging.info(""Already downloaded: %s (at %s)."" % (url, found_file))\n    return found_file\n\n\ndef download_and_extract(path, url, input_filename, target_filename):\n  """"""Extract files from downloaded compressed archive file.\n\n  Args:\n    path: string directory where the files will be downloaded\n    url: url containing the compressed input and target files\n    input_filename: name of file containing data in source language\n    target_filename: name of file containing data in target language\n\n  Returns:\n    Full paths to extracted input and target files.\n\n  Raises:\n    OSError: if the the download/extraction fails.\n  """"""\n  # Check if extracted files already exist in path\n  input_file = find_file(path, input_filename)\n  target_file = find_file(path, target_filename)\n  if input_file and target_file:\n    tf.logging.info(""Already downloaded and extracted %s."" % url)\n    return input_file, target_file\n\n  # Download archive file if it doesn\'t already exist.\n  compressed_file = download_from_url(path, url)\n\n  # Extract compressed files\n  tf.logging.info(""Extracting %s."" % compressed_file)\n  with tarfile.open(compressed_file, ""r:gz"") as corpus_tar:\n    corpus_tar.extractall(path)\n\n  # Return filepaths of the requested files.\n  input_file = find_file(path, input_filename)\n  target_file = find_file(path, target_filename)\n\n  if input_file and target_file:\n    return input_file, target_file\n\n  raise OSError(""Download/extraction failed for url %s to path %s"" %\n                (url, path))\n\n\ndef txt_line_iterator(path):\n  """"""Iterate through lines of file.""""""\n  with tf.gfile.Open(path) as f:\n    for line in f:\n      yield line.strip()\n\n\ndef compile_files(raw_dir, raw_files, tag):\n  """"""Compile raw files into a single file for each language.\n\n  Args:\n    raw_dir: Directory containing downloaded raw files.\n    raw_files: Dict containing filenames of input and target data.\n      {""inputs"": list of files containing data in input language\n       ""targets"": list of files containing corresponding data in target language\n      }\n    tag: String to append to the compiled filename.\n\n  Returns:\n    Full path of compiled input and target files.\n  """"""\n  tf.logging.info(""Compiling files with tag %s."" % tag)\n  filename = ""%s-%s"" % (_PREFIX, tag)\n  input_compiled_file = os.path.join(raw_dir, filename + "".lang1"")\n  target_compiled_file = os.path.join(raw_dir, filename + "".lang2"")\n\n  with tf.gfile.Open(input_compiled_file, mode=""w"") as input_writer:\n    with tf.gfile.Open(target_compiled_file, mode=""w"") as target_writer:\n      for i in range(len(raw_files[""inputs""])):\n        input_file = raw_files[""inputs""][i]\n        target_file = raw_files[""targets""][i]\n\n        tf.logging.info(""Reading files %s and %s."" % (input_file, target_file))\n        write_file(input_writer, input_file)\n        write_file(target_writer, target_file)\n  return input_compiled_file, target_compiled_file\n\n\ndef write_file(writer, filename):\n  """"""Write all of lines from file using the writer.""""""\n  for line in txt_line_iterator(filename):\n    writer.write(line)\n    writer.write(""\\n"")\n\n\n###############################################################################\n# Data preprocessing\n###############################################################################\ndef encode_and_save_files(\n    subtokenizer, data_dir, raw_files, tag, total_shards):\n  """"""Save data from files as encoded Examples in TFrecord format.\n\n  Args:\n    subtokenizer: Subtokenizer object that will be used to encode the strings.\n    data_dir: The directory in which to write the examples\n    raw_files: A tuple of (input, target) data files. Each line in the input and\n      the corresponding line in target file will be saved in a tf.Example.\n    tag: String that will be added onto the file names.\n    total_shards: Number of files to divide the data into.\n\n  Returns:\n    List of all files produced.\n  """"""\n  # Create a file for each shard.\n  filepaths = [shard_filename(data_dir, tag, n + 1, total_shards)\n               for n in range(total_shards)]\n\n  if all_exist(filepaths):\n    tf.logging.info(""Files with tag %s already exist."" % tag)\n    return filepaths\n\n  tf.logging.info(""Saving files with tag %s."" % tag)\n  input_file = raw_files[0]\n  target_file = raw_files[1]\n\n  # Write examples to each shard in round robin order.\n  tmp_filepaths = [fname + "".incomplete"" for fname in filepaths]\n  writers = [tf.python_io.TFRecordWriter(fname) for fname in tmp_filepaths]\n  counter, shard = 0, 0\n  for counter, (input_line, target_line) in enumerate(zip(\n      txt_line_iterator(input_file), txt_line_iterator(target_file))):\n    if counter > 0 and counter % 100000 == 0:\n      tf.logging.info(""\\tSaving case %d."" % counter)\n    example = dict_to_example(\n        {""inputs"": subtokenizer.encode(input_line, add_eos=True),\n         ""targets"": subtokenizer.encode(target_line, add_eos=True)})\n    writers[shard].write(example.SerializeToString())\n    shard = (shard + 1) % total_shards\n  for writer in writers:\n    writer.close()\n\n  for tmp_name, final_name in zip(tmp_filepaths, filepaths):\n    tf.gfile.Rename(tmp_name, final_name)\n\n  tf.logging.info(""Saved %d Examples"", counter)\n  return filepaths\n\n\ndef shard_filename(path, tag, shard_num, total_shards):\n  """"""Create filename for data shard.""""""\n  return os.path.join(\n      path, ""%s-%s-%.5d-of-%.5d"" % (_PREFIX, tag, shard_num, total_shards))\n\n\ndef shuffle_records(fname):\n  """"""Shuffle records in a single file.""""""\n  tf.logging.info(""Shuffling records in file %s"" % fname)\n\n  # Rename file prior to shuffling\n  tmp_fname = fname + "".unshuffled""\n  tf.gfile.Rename(fname, tmp_fname)\n\n  reader = tf.python_io.tf_record_iterator(tmp_fname)\n  records = []\n  for record in reader:\n    records.append(record)\n    if len(records) % 100000 == 0:\n      tf.logging.info(""\\tRead: %d"", len(records))\n\n  random.shuffle(records)\n\n  # Write shuffled records to original file name\n  with tf.python_io.TFRecordWriter(fname) as w:\n    for count, record in enumerate(records):\n      w.write(record)\n      if count > 0 and count % 100000 == 0:\n        tf.logging.info(""\\tWriting record: %d"" % count)\n\n  tf.gfile.Remove(tmp_fname)\n\n\ndef dict_to_example(dictionary):\n  """"""Converts a dictionary of string->int to a tf.Example.""""""\n  features = {}\n  for k, v in six.iteritems(dictionary):\n    features[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n  return tf.train.Example(features=tf.train.Features(feature=features))\n\n\ndef all_exist(filepaths):\n  """"""Returns true if all files in the list exist.""""""\n  for fname in filepaths:\n    if not tf.gfile.Exists(fname):\n      return False\n  return True\n\n\ndef make_dir(path):\n  if not tf.gfile.Exists(path):\n    tf.logging.info(""Creating directory %s"" % path)\n    tf.gfile.MakeDirs(path)\n\n\ndef main(unused_argv):\n  """"""Obtain training and evaluation data for the Transformer model.""""""\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  make_dir(FLAGS.raw_dir)\n  make_dir(FLAGS.data_dir)\n\n  # Get paths of download/extracted training and evaluation files.\n  tf.logging.info(""Step 1/4: Downloading data from source"")\n  train_files = get_raw_files(FLAGS.raw_dir, _TRAIN_DATA_SOURCES)\n  eval_files = get_raw_files(FLAGS.raw_dir, _EVAL_DATA_SOURCES)\n  test_files = get_raw_files(FLAGS.raw_dir, _TEST_DATA_SOURCES)\n\n  # Create subtokenizer based on the training files.\n  tf.logging.info(""Step 2/4: Creating subtokenizer and building vocabulary"")\n  train_files_flat = train_files[""inputs""] + train_files[""targets""]\n  vocab_file = os.path.join(FLAGS.data_dir, _VOCAB_FILE)\n  subtokenizer = tokenizer.Subtokenizer.init_from_files(\n      vocab_file, train_files_flat, _TARGET_VOCAB_SIZE, _TARGET_THRESHOLD,\n      min_count=None if FLAGS.search else _TRAIN_DATA_MIN_COUNT)\n\n  tf.logging.info(""Step 3/4: Compiling training and evaluation data"")\n  compiled_train_files = compile_files(FLAGS.raw_dir, train_files, _TRAIN_TAG)\n  compiled_eval_files = compile_files(FLAGS.raw_dir, eval_files, _EVAL_TAG)\n  compiled_test_files = compile_files(FLAGS.raw_dir, test_files, _TEST_TAG)\n\n  # Tokenize and save data as Examples in the TFRecord format.\n  tf.logging.info(""Step 4/4: Preprocessing and saving data"")\n  train_tfrecord_files = encode_and_save_files(\n      subtokenizer, FLAGS.data_dir, compiled_train_files, _TRAIN_TAG,\n      _TRAIN_SHARDS)\n  encode_and_save_files(\n      subtokenizer, FLAGS.data_dir, compiled_eval_files, _EVAL_TAG,\n      _EVAL_SHARDS)\n  encode_and_save_files(\n    subtokenizer, FLAGS.data_dir, compiled_test_files, _TEST_TAG,\n    _TEST_SHARDS)\n\n  for fname in train_tfrecord_files:\n    shuffle_records(fname)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      ""--data_dir"", ""-dd"", type=str, default=""/tmp/translate_ende"",\n      help=""[default: %(default)s] Directory for where the ""\n           ""translate_ende_wmt32k dataset is saved."",\n      metavar=""<DD>"")\n  parser.add_argument(\n      ""--raw_dir"", ""-rd"", type=str, default=""/tmp/translate_ende_raw"",\n      help=""[default: %(default)s] Path where the raw data will be downloaded ""\n           ""and extracted."",\n      metavar=""<RD>"")\n  parser.add_argument(\n      ""--search"", action=""store_true"",\n      help=""If set, use binary search to find the vocabulary set with size""\n           ""closest to the target size (%d)."" % _TARGET_VOCAB_SIZE)\n\n  FLAGS, unparsed = parser.parse_known_args()\n  main(sys.argv)'"
open_seq2seq/data/text2text/t2t.py,29,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Input pipeline for the transformer model to read, filter, and batch examples.\n\nTwo things to note in the pipeline:\n\n1. Batching scheme\n\n   The examples encoded in the TFRecord files contain data in the format:\n     {""inputs"": [variable length array of integers],\n      ""targets"": [variable length array of integers]}\n   Where integers in the arrays refer to tokens in the English and German vocab\n   file (named `vocab.ende.32768`).\n\n   Prior to batching, elements in the dataset are grouped by length (max between\n   ""inputs"" and ""targets"" length). Each group is then batched such that:\n     group_batch_size * length <= batch_size.\n\n   Another way to view batch_size is the maximum number of tokens in each batch.\n\n   Once batched, each element in the dataset will have the shape:\n     {""inputs"": [group_batch_size, padded_input_length],\n      ""targets"": [group_batch_size, padded_target_length]}\n   Lengths are padded to the longest ""inputs"" or ""targets"" sequence in the batch\n   (padded_input_length and padded_target_length can be different).\n\n   This batching scheme decreases the fraction of padding tokens per training\n   batch, thus improving the training speed significantly.\n\n2. Shuffling\n\n   While training, the dataset is shuffled in two places in the code. The first\n   is the list of training files. Second, while reading records using\n   `parallel_interleave`, the `sloppy` argument is used to generate randomness\n   in the order of the examples.\n\n3. Modified slightly to fit OpenSeq2Seq needs\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# Use the number of training files as the shuffle buffer.\n_FILE_SHUFFLE_BUFFER = 100\n# Buffer size for reading records from a TFRecord file. Each training file is\n# 7.2 MB, so 8 MB allows an entire file to be kept in memory.\n_READ_RECORD_BUFFER = 8 * 1000 * 1000\n\n# Example grouping constants. Defines length boundaries for each group.\n# These values are the defaults used in Tensor2Tensor.\n_MIN_BOUNDARY = 8\n_BOUNDARY_SCALE = 1.1\n\n\ndef _load_records(filename):\n  """"""Read file and return a dataset of tf.Examples.""""""\n  return tf.data.TFRecordDataset(filename, buffer_size=_READ_RECORD_BUFFER)\n\n\ndef _parse_example(serialized_example, pad_2_eight=False):\n  """"""Return inputs and targets Tensors from a serialized tf.Example.""""""\n  data_fields = {\n      ""inputs"": tf.VarLenFeature(tf.int64),\n      ""targets"": tf.VarLenFeature(tf.int64)\n  }\n  parsed = tf.parse_single_example(serialized_example, data_fields)\n  inputs = tf.sparse_tensor_to_dense(parsed[""inputs""])\n  targets = tf.sparse_tensor_to_dense(parsed[""targets""])\n\n  if pad_2_eight:\n    inputs = tf.cond(\n        tf.equal(tf.shape(inputs)[0] % 8, 0),\n        true_fn=lambda: inputs,\n        false_fn=lambda: tf.pad(inputs,\n                                paddings=[[0, 8 - tf.shape(inputs)[0] % 8]])\n    )\n    targets = tf.cond(\n        tf.equal(tf.shape(targets)[0] % 8, 0),\n        true_fn=lambda: targets,\n        false_fn=lambda: tf.pad(targets,\n                                paddings=[[0, 8 - tf.shape(targets)[0] % 8]])\n    )\n\n  return inputs, targets\n\n\ndef _filter_max_length(example, max_length=256):\n  """"""Indicates whether the example\'s length is lower than the maximum length.""""""\n  return tf.logical_and(tf.size(example[0]) <= max_length,\n                        tf.size(example[1]) <= max_length)\n\n\ndef _get_example_length(example):\n  """"""Returns the maximum length between the example inputs and targets.""""""\n  length = tf.maximum(tf.shape(example[0])[0], tf.shape(example[1])[0])\n  return length\n\n\ndef _create_min_max_boundaries(\n    max_length, min_boundary=_MIN_BOUNDARY, boundary_scale=_BOUNDARY_SCALE):\n  """"""Create min and max boundary lists up to max_length.\n\n  For example, when max_length=24, min_boundary=4 and boundary_scale=2, the\n  returned values will be:\n    buckets_min = [0, 4, 8, 16, 24]\n    buckets_max = [4, 8, 16, 24, 25]\n\n  Args:\n    max_length: The maximum length of example in dataset.\n    min_boundary: Minimum length in boundary.\n    boundary_scale: Amount to scale consecutive boundaries in the list.\n\n  Returns:\n    min and max boundary lists\n\n  """"""\n  # Create bucket boundaries list by scaling the previous boundary or adding 1\n  # (to ensure increasing boundary sizes).\n  bucket_boundaries = []\n  x = min_boundary\n  while x < max_length:\n    bucket_boundaries.append(x)\n    x = max(x + 1, int(x * boundary_scale))\n\n  # Create min and max boundary lists from the initial list.\n  buckets_min = [0] + bucket_boundaries\n  buckets_max = bucket_boundaries + [max_length + 1]\n  return buckets_min, buckets_max\n\n\ndef _batch_examples(dataset, batch_size, max_length, pad_2_eight=True):\n  """"""Group examples by similar lengths, and return batched dataset.\n\n  Each batch of similar-length examples are padded to the same length, and may\n  have different number of elements in each batch, such that:\n    group_batch_size * padded_length <= batch_size.\n\n  This decreases the number of padding tokens per batch, which improves the\n  training speed.\n\n  Args:\n    dataset: Dataset of unbatched examples.\n    batch_size: Max number of tokens per batch of examples.\n    max_length: Max number of tokens in an example input or target sequence.\n\n  Returns:\n    Dataset of batched examples with similar lengths.\n  """"""\n  # Get min and max boundary lists for each example. These are used to calculate\n  # the `bucket_id`, which is the index at which:\n  # buckets_min[bucket_id] <= len(example) < buckets_max[bucket_id]\n  # Note that using both min and max lists improves the performance.\n  buckets_min, buckets_max = _create_min_max_boundaries(max_length)\n\n  # Create list of batch sizes for each bucket_id, so that\n  # bucket_batch_size[bucket_id] * buckets_max[bucket_id] <= batch_size\n  if pad_2_eight:  # pad to 8 for HMMA\n    bucket_batch_sizes = [\n        batch_size // x if batch_size // x % 8 == 0 else\n        batch_size // x + (8 - batch_size // x % 8)\n        for x in buckets_max\n    ]\n  else:\n    bucket_batch_sizes = [batch_size // x for x in buckets_max]\n  # bucket_id will be a tensor, so convert this list to a tensor as well.\n  bucket_batch_sizes = tf.constant(bucket_batch_sizes, dtype=tf.int64)\n\n  def example_to_bucket_id(example_input, example_target):\n    """"""Return int64 bucket id for this example, calculated based on length.""""""\n    seq_length = _get_example_length((example_input, example_target))\n\n    # TODO: investigate whether removing code branching improves performance.\n    conditions_c = tf.logical_and(\n        tf.less_equal(buckets_min, seq_length),\n        tf.less(seq_length, buckets_max))\n    bucket_id = tf.reduce_min(tf.where(conditions_c))\n    return bucket_id\n\n  def window_size_fn(bucket_id):\n    """"""Return number of examples to be grouped when given a bucket id.""""""\n    return bucket_batch_sizes[bucket_id]\n\n  def batching_fn(bucket_id, grouped_dataset):\n    """"""Batch and add padding to a dataset of elements with similar lengths.""""""\n    bucket_batch_size = window_size_fn(bucket_id)\n    # Batch the dataset and add padding so that all input sequences in the\n    # examples have the same length, and all target sequences have the same\n    # lengths as well. Resulting lengths of inputs and targets can differ.\n    return grouped_dataset.padded_batch(bucket_batch_size, ([None], [None]))\n\n  return dataset.apply(\n      tf.contrib.data.group_by_window(  # pylint: disable=no-member\n          key_func=example_to_bucket_id,\n          reduce_func=batching_fn,\n          window_size=None,\n          window_size_func=window_size_fn\n      )\n  )\n\n\ndef _read_and_batch_from_files(\n    file_pattern, batch_size, max_length, num_cpu_cores, shuffle, repeat,\n    num_workers, worker_id, batch_in_tokens, pad2eight=True):\n  """"""Create dataset where each item is a dict of ""inputs"" and ""targets"".\n\n  Args:\n    file_pattern: String used to match the input TFRecord files.\n    batch_size: Maximum number of tokens per batch of examples\n    max_length: Maximum number of tokens per example\n    num_cpu_cores: Number of cpu cores for parallel input processing.\n    shuffle: If true, randomizes order of elements.\n    repeat: Number of times to repeat the dataset. If None, the dataset is\n      repeated forever.\n    num_workers: Number of workers or number of Horovod workers\n    worker_id: Worker id or Horovod rank\n    batch_in_tokens: whether to batch_size means amounts in tokens or sentence\n    pairs. batching in tokens is more efficient as it reduces PADs. batching in\n    sentences should be used in inference mode since order of\n    sentences is important\n    pad2eight: if True, it will pad both dimensions to be divisible by 8\n\n  Returns:\n    tf.data.Dataset object containing examples loaded from the files.\n  """"""\n  dataset = tf.data.Dataset.list_files(file_pattern)\n  if num_workers > 1:\n    dataset = dataset.shard(num_shards=num_workers, index=worker_id)\n\n  if shuffle:\n    # Shuffle filenames\n    dataset = dataset.shuffle(buffer_size=_FILE_SHUFFLE_BUFFER)\n\n  # Read files and interleave results. When training, the order of the examples\n  # will be non-deterministic.\n  dataset = dataset.apply(\n      tf.contrib.data.parallel_interleave(  # pylint: disable=no-member\n          _load_records, sloppy=shuffle, cycle_length=num_cpu_cores))\n\n  # Parse each tf.Example into a dictionary\n  # TODO: Look into prefetch_input_elements for performance optimization.\n  dataset = dataset.map(lambda x: _parse_example(x, pad_2_eight=pad2eight),\n                        num_parallel_calls=num_cpu_cores)\n\n  # Remove examples where the input or target length exceeds the maximum length,\n  dataset = dataset.filter(lambda x, y: _filter_max_length((x, y), max_length))\n\n  if batch_in_tokens:\n    # Batch such that each batch has examples of similar length.\n    dataset = _batch_examples(dataset, batch_size, max_length,\n                              pad_2_eight=pad2eight)\n  else:\n    # Examples can have different lenghts\n    dataset = dataset.padded_batch(batch_size, ([None], [None]))\n  dataset = dataset.repeat(repeat)\n\n  # Prefetch the next element to improve speed of input pipeline.\n  dataset = dataset.prefetch(1)\n  return dataset\n'"
open_seq2seq/data/text2text/text2text.py,21,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom enum import Enum\nfrom open_seq2seq.data.data_layer import DataLayer\nfrom open_seq2seq.data.utils import load_pre_existing_vocabulary, pad_vocab_to_eight\nfrom open_seq2seq.data.text2text.t2t import _read_and_batch_from_files\nfrom open_seq2seq.data.text2text.tokenizer import PAD_ID\n\nclass SpecialTextTokens(Enum):\n  PAD_ID = 0  # special padding token\n  EOS_ID = 1  # special end of sentence token\n  S_ID = 2  # special start of sentence token\n  UNK_ID = 3  # out-of-vocabulary tokens will map there\n  OUT_OF_BUCKET = 1234567890\n  END_OF_CHOICE = -100\n\n  @staticmethod\n  def to_string(s_token):\n    if s_token == SpecialTextTokens.UNK_ID.value:\n      return \'<UNK>\'\n    elif s_token == SpecialTextTokens.S_ID.value:\n      return \'<S>\'\n    elif s_token == SpecialTextTokens.EOS_ID.value:\n      return \'</S>\'\n    elif s_token == SpecialTextTokens.PAD_ID.value:\n      return \'<PAD>\'\n    else:\n      raise ValueError(""Unknown Value in SpecialTokens"")\n\n\nclass ParallelTextDataLayer(DataLayer):\n  @staticmethod\n  def get_required_params():\n    return dict(DataLayer.get_required_params(), **{\n      \'source_file\': str,\n      \'src_vocab_file\': str,\n      \'tgt_vocab_file\': str,\n      \'max_length\': int,\n      \'shuffle\': bool,\n      \'repeat\': bool,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(DataLayer.get_optional_params(), **{\n      \'use_targets\': bool,\n      \'delimiter\': str,\n      \'target_file\': str,\n      \'map_parallel_calls\': int,\n      \'prefetch_buffer_size\': int,\n      \'pad_lengths_to_eight\': bool,\n      \'pad_vocab_to_eight\': bool,\n      \'shuffle_buffer_size\': int,\n      \'special_tokens_already_in_vocab\': bool,\n      \'use_start_token\': bool,\n    })\n\n  def __init__(self, params, model, num_workers=1, worker_id=0):\n    super(ParallelTextDataLayer, self).__init__(params, model,\n                                                num_workers, worker_id)\n    self._batch_size = self.params[\'batch_size\']\n    self.source_file = self.params[\'source_file\']\n    self._use_targets = self.params.get(\'use_targets\', True)\n    if not self._use_targets:\n      self.target_file = self.source_file\n      if \'target_file\' in self.params:\n        print(""WARNING: target file was specified but was ""\n              ""ignored by data layer because \'use_targets\'=False"")\n    else:\n      self.target_file = self.params[\'target_file\']\n    self.src_vocab_file = self.params[\'src_vocab_file\']\n    self.tgt_vocab_file = self.params[\'tgt_vocab_file\']\n    self.max_len = self.params[\'max_length\']\n    self._delimiter = self.params.get(\'delimiter\', \' \')\n    self._map_parallel_calls = self.params.get(\'map_parallel_calls\', 8)\n    self._pad_lengths_to_eight = self.params.get(\'pad_lengths_to_eight\', False)\n    self._prefetch_buffer_size = self.params.get(\'prefetch_buffer_size\',\n                                                 tf.contrib.data.AUTOTUNE)\n    self._shuffle_buffer_size = self.params.get(\'shuffle_buffer_size\', -1)\n    self._num_workers = num_workers\n    self._worker_id = worker_id\n    self._use_start_token = self.params.get(\'use_start_token\', True)\n    if self._pad_lengths_to_eight and not (self.params[\'max_length\'] % 8 == 0):\n      raise ValueError(""If padding to 8 in data layer, then ""\n                       ""max_length should be multiple of 8"")\n\n    def file_len(fname):\n      with open(fname) as f:\n        for i, l in enumerate(f):\n          pass\n      return i + 1\n\n    self.dataset_size = file_len(self.source_file)\n    special_tokens_already_in_vocab = self.params.get(\'special_tokens_already_in_vocab\', True)\n\n    # load source and target vocabularies to RAM\n    self.src_seq2idx = load_pre_existing_vocabulary(\n      self.src_vocab_file, min_idx=0 if special_tokens_already_in_vocab\n      else SpecialTextTokens.UNK_ID.value + 1)\n    self.tgt_seq2idx = load_pre_existing_vocabulary(\n      self.tgt_vocab_file, min_idx=0 if special_tokens_already_in_vocab\n      else SpecialTextTokens.UNK_ID.value + 1)\n\n    if not special_tokens_already_in_vocab:\n      # manually add special tokens\n      # unknown symbol\n      self.src_seq2idx[\n        SpecialTextTokens.to_string(SpecialTextTokens.UNK_ID.value)] = \\\n        SpecialTextTokens.UNK_ID.value\n      self.tgt_seq2idx[\n        SpecialTextTokens.to_string(SpecialTextTokens.UNK_ID.value)] = \\\n        SpecialTextTokens.UNK_ID.value\n      # sentence start\n      self.src_seq2idx[\n        SpecialTextTokens.to_string(SpecialTextTokens.S_ID.value)] = \\\n        SpecialTextTokens.S_ID.value\n      self.tgt_seq2idx[\n        SpecialTextTokens.to_string(SpecialTextTokens.S_ID.value)] = \\\n        SpecialTextTokens.S_ID.value\n      # sentence end\n      self.src_seq2idx[\n        SpecialTextTokens.to_string(SpecialTextTokens.EOS_ID.value)] = \\\n        SpecialTextTokens.EOS_ID.value\n      self.tgt_seq2idx[\n        SpecialTextTokens.to_string(SpecialTextTokens.EOS_ID.value)] = \\\n        SpecialTextTokens.EOS_ID.value\n      # padding\n      self.src_seq2idx[\n        SpecialTextTokens.to_string(SpecialTextTokens.PAD_ID.value)] = \\\n        SpecialTextTokens.PAD_ID.value\n      self.tgt_seq2idx[\n        SpecialTextTokens.to_string(SpecialTextTokens.PAD_ID.value)] = \\\n        SpecialTextTokens.PAD_ID.value\n\n    if self.params.get(\'pad_vocab_to_eight\', False):\n      self.src_seq2idx = pad_vocab_to_eight(self.src_seq2idx)\n      self.tgt_seq2idx = pad_vocab_to_eight(self.tgt_seq2idx)\n\n    self.src_idx2seq = {idx: w for w, idx in self.src_seq2idx.items()}\n    self.tgt_idx2seq = {idx: w for w, idx in self.tgt_seq2idx.items()}\n\n    self.params[\'src_vocab_size\'] = len(self.src_seq2idx)\n    self.params[\'tgt_vocab_size\'] = len(self.tgt_seq2idx)\n    self.params[\'target_seq2idx\'] = self.tgt_seq2idx\n    self.params[\'source_seq2idx\'] = self.src_seq2idx\n    self.params[\'target_idx2seq\'] = self.tgt_idx2seq\n    self.params[\'source_idx2seq\'] = self.src_idx2seq\n\n    self._input_tensors = {}\n\n  def _pad2eight(self, lst, do_pad_eight):\n    if len(lst) % 8 == 0 or not do_pad_eight:\n      return lst\n    else:\n      return lst + [SpecialTextTokens.PAD_ID.value] * (8 - len(lst) % 8)\n\n  def _src_token_to_id(self, line):\n    tokens = line.decode(""utf-8"").split(self._delimiter) #line.numpy().decode\n    if self._use_start_token:\n      return np.array(self._pad2eight([SpecialTextTokens.S_ID.value] + \\\n             [self.src_seq2idx.get(token, SpecialTextTokens.UNK_ID.value) for token in tokens[:self.max_len-2]] + \\\n             [SpecialTextTokens.EOS_ID.value], self._pad_lengths_to_eight), dtype=""int32"")\n    else:\n      return np.array(self._pad2eight([self.src_seq2idx.get(token, SpecialTextTokens.UNK_ID.value) for token in\n                                       tokens[:self.max_len - 2]] + \\\n                                      [SpecialTextTokens.EOS_ID.value], self._pad_lengths_to_eight), dtype=""int32"")\n\n  def _tgt_token_to_id(self, line):\n    tokens = line.decode(""utf-8"").split(self._delimiter) #line.numpy().decode\n    if self._use_start_token:\n      return np.array(self._pad2eight([SpecialTextTokens.S_ID.value] + \\\n             [self.tgt_seq2idx.get(token, SpecialTextTokens.UNK_ID.value) for token in tokens[:self.max_len-2]] + \\\n             [SpecialTextTokens.EOS_ID.value], self._pad_lengths_to_eight), dtype=""int32"")\n    else:\n      return np.array(self._pad2eight([self.tgt_seq2idx.get(token, SpecialTextTokens.UNK_ID.value) for token in\n                                       tokens[:self.max_len - 2]] + \\\n                                      [SpecialTextTokens.EOS_ID.value], self._pad_lengths_to_eight), dtype=""int32"")\n\n  def build_graph(self):\n    with tf.device(\'/cpu:0\'):\n\n      _sources = tf.data.TextLineDataset(self.source_file)\n      _targets = tf.data.TextLineDataset(self.target_file)\n\n      if self._num_workers > 1:\n        #_src_tgt_dataset = _src_tgt_dataset\\\n        #  .shard(num_shards=self._num_workers, index=self._worker_id)\n        _sources = _sources.shard(num_shards=self._num_workers,\n                                  index=self._worker_id)\n        _targets = _targets.shard(num_shards=self._num_workers,\n                                  index=self._worker_id)\n\n      _sources = _sources.map(lambda line: tf.py_func(func=self._src_token_to_id, inp=[line],\n                                     Tout=[tf.int32], stateful=False),\n             num_parallel_calls=self._map_parallel_calls) \\\n        .map(lambda tokens: (tokens, tf.size(tokens)),\n             num_parallel_calls=self._map_parallel_calls)\n\n      _targets = _targets.map(lambda line: tf.py_func(func=self._tgt_token_to_id, inp=[line],\n                                     Tout=[tf.int32], stateful=False),\n             num_parallel_calls=self._map_parallel_calls) \\\n        .map(lambda tokens: (tokens, tf.size(tokens)),\n             num_parallel_calls=self._map_parallel_calls)\n\n      _src_tgt_dataset = tf.data.Dataset.zip((_sources, _targets)).filter(\n        lambda t1, t2: tf.logical_and(tf.less_equal(t1[1], self.max_len),\n                                      tf.less_equal(t2[1], self.max_len))\n      ).cache()\n\n      if self.params[\'shuffle\']:\n        bf_size = self.get_size_in_samples() if self._shuffle_buffer_size == -1 \\\n                                             else self._shuffle_buffer_size\n        _src_tgt_dataset = _src_tgt_dataset.shuffle(buffer_size=bf_size)\n      else:\n        _src_tgt_dataset = _src_tgt_dataset\n\n      if self.params[\'repeat\']:\n        _src_tgt_dataset = _src_tgt_dataset.repeat()\n\n      self.batched_dataset = _src_tgt_dataset.padded_batch(\n        self._batch_size,\n        padded_shapes=((tf.TensorShape([None]),\n                        tf.TensorShape([])),\n                       (tf.TensorShape([None]),\n                        tf.TensorShape([]))),\n        padding_values=(\n        (SpecialTextTokens.PAD_ID.value,\n         0),\n        (SpecialTextTokens.PAD_ID.value,\n         0))).prefetch(buffer_size=self._prefetch_buffer_size)\n\n      self._iterator = self.batched_dataset.make_initializable_iterator()\n\n      if self.params[\'mode\'] == \'train\' or self.params[\'mode\'] == \'eval\':\n        t1, t2 = self.iterator.get_next()\n        x, x_length = t1[0], t1[1]\n        y, y_length = t2[0], t2[1]\n        self._input_tensors[\'source_tensors\'] = [x, x_length]\n        self._input_tensors[\'target_tensors\'] = [y, y_length]\n      else:\n        t1, _ = self.iterator.get_next()\n        self._input_tensors[\'source_tensors\'] = [t1[0], t1[1]]\n\n  def create_interactive_placeholders(self):\n    self._text = tf.placeholder(dtype=tf.int32, shape=[self._batch_size, None])\n    self._text_length = tf.placeholder(dtype=tf.int32, shape=[self._batch_size])\n\n    self._input_tensors = {}\n    self._input_tensors[\'source_tensors\'] = [self._text, self._text_length]\n\n  def create_feed_dict(self, model_in):\n    """""" Creates the feed dict for interactive infer\n\n    Args:\n      model_in (str): the string to be translated. Should be in bpe format.\n\n    Returns:\n      feed_dict (dict): Dictionary with values for the placeholders.\n    """"""\n    text = []\n    text_length = []\n    for line in model_in:\n      line = self._src_token_to_id(line)\n      text.append(line)\n      text_length.append(line.shape[0])\n    max_len = np.max(text_length)\n    for i,line in enumerate(text):\n      line = np.pad(\n          line, ((0, max_len-len(line))),\n          ""constant"", constant_values=SpecialTextTokens.PAD_ID.value\n      )\n      text[i] = line\n\n    text = np.reshape(text, [self._batch_size, -1])\n    text_length = np.reshape(text_length, [self._batch_size])\n\n    feed_dict = {\n        self._text: text,\n        self._text_length: text_length\n    }\n    return feed_dict\n\n  def get_size_in_samples(self):\n    return self.dataset_size\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n\nclass TransformerDataLayer(DataLayer):\n  """"""Wraps Transformers data pipeline into the form for OpenSeq2Seq""""""\n  @staticmethod\n  def get_required_params():\n    return dict(DataLayer.get_required_params(), **{\n      \'data_dir\': str,\n      \'file_pattern\': str,\n      \'src_vocab_file\': str,\n      \'batch_size\': int,\n      \'max_length\': int,\n      \'shuffle\': bool,\n      ""delimiter"": str,\n    })\n\n  @staticmethod\n  def get_optional_params():\n    return dict(DataLayer.get_optional_params(), **{\n      \'repeat\': int,\n      \'num_cpu_cores\': int,\n      \'tgt_vocab_file\': str,\n      \'pad_data_to_eight\': bool,\n      \'batch_in_tokens\': bool,\n    })\n\n  def __init__(self, params, model, num_workers=1, worker_id=0):\n    super(TransformerDataLayer, self).__init__(params, model,\n                                               num_workers, worker_id)\n    self.src_vocab_file = self.params[\'src_vocab_file\']\n    # if tgt vocab isn\'t specified - assume common vocab file\n    self.tgt_vocab_file = self.params.get(\'tgt_vocab_file\', self.src_vocab_file)\n\n    # load source and target vocabularies to RAM\n    # pre-processed vocab starts from PAD, EOS\n    self.src_seq2idx = load_pre_existing_vocabulary(\n      self.src_vocab_file,\n      min_idx=PAD_ID)\n    self.tgt_seq2idx = load_pre_existing_vocabulary(\n      self.tgt_vocab_file,\n      min_idx=PAD_ID)\n\n    self.src_idx2seq = {idx: w for w, idx in self.src_seq2idx.items()}\n    self.tgt_idx2seq = {idx: w for w, idx in self.tgt_seq2idx.items()}\n\n    self.params[\'src_vocab_size\'] = len(self.src_seq2idx)\n    self.params[\'tgt_vocab_size\'] = len(self.tgt_seq2idx)\n    self.params[\'target_seq2idx\'] = self.tgt_seq2idx\n    self.params[\'source_seq2idx\'] = self.src_seq2idx\n    self.params[\'target_idx2seq\'] = self.tgt_idx2seq\n    self.params[\'source_idx2seq\'] = self.src_idx2seq\n\n    self._num_workers = num_workers\n    self._worker_id = worker_id\n\n    self._input_tensors = {}\n    self._iterator = None\n    self.batched_dataset = None\n\n  def build_graph(self):\n    file_pattern = os.path.join(self.params[\'data_dir\'],\n                                self.params[\'file_pattern\'])\n    self.batched_dataset = _read_and_batch_from_files(\n      file_pattern=file_pattern,\n      batch_size=self.params[\'batch_size\'],\n      max_length=self.params[\'max_length\'],\n      num_cpu_cores=self.params.get(\'num_cpu_cores\', 2),\n      shuffle=self.params[\'shuffle\'],\n      repeat=self.params[\'repeat\'],\n      num_workers=self._num_workers,\n      worker_id=self._worker_id,\n      batch_in_tokens=self.params.get(\'batch_in_tokens\', True),\n      pad2eight=self.params.get(\'pad_data_to_eight\', False))\n\n    self._iterator = self.batched_dataset.make_initializable_iterator()\n    x, y = self.iterator.get_next()\n\n    len_x = tf.count_nonzero(x, axis=1, dtype=tf.int32)\n    len_y = tf.count_nonzero(y, axis=1, dtype=tf.int32)\n    if self.params[\'mode\'] == \'train\' or self.params[\'mode\'] == \'eval\':\n      self._input_tensors[\'source_tensors\'] = [x, len_x]\n      self._input_tensors[\'target_tensors\'] = [y, len_y]\n    else:\n      self._input_tensors[\'source_tensors\'] = [x, len_x]\n\n  @property\n  def iterator(self):\n    return self._iterator\n\n  @property\n  def input_tensors(self):\n    return self._input_tensors\n'"
open_seq2seq/data/text2text/text2text_test.py,2,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom open_seq2seq.data.text2text.text2text import ParallelTextDataLayer\nfrom open_seq2seq.test_utils.create_reversed_examples import create_data, \\\n                                                             remove_data\n\n\nclass ParallelTextDataLayerTests(tf.test.TestCase):\n  def setUp(self):\n    create_data(train_corpus_size=1000, data_path=""tmp1"")\n    batch_size = 2\n    self.params = {\n        \'src_vocab_file\': ""tmp1/vocab/source.txt"",\n        \'tgt_vocab_file\': ""tmp1/vocab/target.txt"",\n        \'source_file\': ""tmp1/train/source.txt"",\n        \'target_file\': ""tmp1/train/target.txt"",\n        \'shuffle\': True,\n        \'batch_size\': batch_size,\n        \'max_length\': 56,\n        \'repeat\': False,\n        \'delimiter\': \' \',\n        \'map_parallel_calls\': 1,\n        \'prefetch_buffer_size\': 1,\n        \'mode\': \'train\',\n    }\n\n  def tearDown(self):\n    remove_data(data_path=\'tmp1\')\n\n  def test_init_test4(self):\n    dl = ParallelTextDataLayer(params=self.params, model=None)\n    dl.build_graph()\n    print(len(dl.src_seq2idx))\n    print(len(dl.tgt_seq2idx))\n    with self.test_session(use_gpu=True) as sess:\n      sess.run(dl.iterator.initializer)\n      et = sess.run(dl.input_tensors)\n      self.assertIn(\'source_tensors\', et)\n      self.assertIn(\'target_tensors\', et)\n      self.assertEqual(et[\'source_tensors\'][0].shape[0],\n                       self.params[\'batch_size\'])\n      self.assertLessEqual(et[\'source_tensors\'][0].shape[1],\n                           self.params[\'max_length\'])\n      self.assertEqual(et[\'source_tensors\'][1].shape[0],\n                       self.params[\'batch_size\'])\n      self.assertEqual(et[\'target_tensors\'][0].shape[0],\n                       self.params[\'batch_size\'])\n      self.assertLessEqual(et[\'target_tensors\'][0].shape[1],\n                           self.params[\'max_length\'])\n      self.assertEqual(et[\'target_tensors\'][1].shape[0],\n                       self.params[\'batch_size\'])\n\n  def test_init_test2(self):\n    self.params[\'mode\'] = ""infer""  # in this case we do not yield targets\n    self.params[\'shuffle\'] = False  # in this case we do not yield targets\n    dl = ParallelTextDataLayer(params=self.params, model=None)\n    dl.build_graph()\n    print(len(dl.src_seq2idx))\n    print(len(dl.tgt_seq2idx))\n    with self.test_session(use_gpu=True) as sess:\n      sess.run(dl.iterator.initializer)\n      et = sess.run(dl.input_tensors)\n      self.assertIn(\'source_tensors\', et)\n      self.assertEqual(et[\'source_tensors\'][0].shape[0],\n                       self.params[\'batch_size\'])\n      self.assertLessEqual(et[\'source_tensors\'][0].shape[1],\n                           self.params[\'max_length\'])\n      self.assertEqual(et[\'source_tensors\'][1].shape[0],\n                       self.params[\'batch_size\'])\n\n  def test_pad8(self):\n    self.params[\'shuffle\'] = False  # in this case we do not yield targets\n    self.params[\'pad_lengths_to_eight\'] = True\n    dl = ParallelTextDataLayer(params=self.params, model=None)\n    dl.build_graph()\n    print(len(dl.src_seq2idx))\n    print(len(dl.tgt_seq2idx))\n    print(dl.src_seq2idx)\n    print(dl.src_idx2seq)\n    for i in range(len(dl.src_seq2idx)):\n      self.assertIn(i, dl.src_idx2seq)\n    with self.test_session(use_gpu=True) as sess:\n      sess.run(dl.iterator.initializer)\n      et = sess.run(dl.input_tensors)\n      self.assertIn(\'source_tensors\', et)\n      self.assertIn(\'target_tensors\', et)\n      self.assertEqual(et[\'source_tensors\'][0].shape[0],\n                       self.params[\'batch_size\'])\n      self.assertTrue(et[\'source_tensors\'][0].shape[1] % 8 == 0)\n      self.assertEqual(et[\'source_tensors\'][1].shape[0],\n                       self.params[\'batch_size\'])\n      self.assertEqual(et[\'target_tensors\'][0].shape[0],\n                       self.params[\'batch_size\'])\n      self.assertTrue(et[\'target_tensors\'][0].shape[1] % 8 == 0)\n      self.assertEqual(et[\'target_tensors\'][1].shape[0],\n                       self.params[\'batch_size\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
open_seq2seq/data/text2text/tokenizer.py,14,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Defines Subtokenizer class to encode and decode strings.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport sys\nimport unicodedata\n\nimport numpy as np\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nPAD = ""<pad>""\nPAD_ID = 0\nEOS = ""<EOS>""\nEOS_ID = 1\nRESERVED_TOKENS = [PAD, EOS]\n\n# Set of characters that will be used in the function _escape_token() (see func\n# docstring for more details).\n# This set is added to the alphabet list to ensure that all escaped tokens can\n# be encoded.\n_ESCAPE_CHARS = set(u""\\\\_u;0123456789"")\n# Regex for the function _unescape_token(), the inverse of _escape_token().\n# This is used to find ""\\u"", ""\\\\"", and ""\\###;"" substrings in the token.\n_UNESCAPE_REGEX = re.compile(r""\\\\u|\\\\\\\\|\\\\([0-9]+);"")\n\n_UNDEFINED_UNICODE = u""\\u3013""\n\n# Set contains all letter and number characters.\n_ALPHANUMERIC_CHAR_SET = set(\n    six.unichr(i) for i in xrange(sys.maxunicode)\n    if (unicodedata.category(six.unichr(i)).startswith(""L"") or\n        unicodedata.category(six.unichr(i)).startswith(""N"")))\n\n# min_count is the minimum number of times a subtoken must appear in the data\n# before before it is added to the vocabulary. The value is found using binary\n# search to obtain the target vocabulary size.\n_MIN_MIN_COUNT = 1     # min value to use when binary searching for min_count\n_MAX_MIN_COUNT = 1000  # max value to use when binary searching for min_count\n\n\nclass Subtokenizer(object):\n  """"""Encodes and decodes strings to/from integer IDs.""""""\n\n  def __init__(self, vocab_file, reserved_tokens=None):\n    """"""Initializes class, creating a vocab file if data_files is provided.""""""\n    tf.logging.info(""Initializing Subtokenizer from file %s."" % vocab_file)\n\n    if reserved_tokens is None:\n      reserved_tokens = RESERVED_TOKENS\n\n    self.subtoken_list = _load_vocab_file(vocab_file, reserved_tokens)\n    self.alphabet = _generate_alphabet_dict(self.subtoken_list)\n    self.subtoken_to_id_dict = _list_to_index_dict(self.subtoken_list)\n\n    self.max_subtoken_length = 0\n    for subtoken in self.subtoken_list:\n      self.max_subtoken_length = max(self.max_subtoken_length, len(subtoken))\n\n    # Create cache to speed up subtokenization\n    self._cache_size = 2 ** 20\n    self._cache = [(None, None)] * self._cache_size\n\n  @staticmethod\n  def init_from_files(\n      vocab_file, files, target_vocab_size, threshold, min_count=None,\n      file_byte_limit=1e6, reserved_tokens=None):\n    """"""Create subtoken vocabulary based on files, and save vocab to file.\n\n    Args:\n      vocab_file: String name of vocab file to store subtoken vocabulary.\n      files: List of file paths that will be used to generate vocabulary.\n      target_vocab_size: target vocabulary size to generate.\n      threshold: int threshold of vocabulary size to accept.\n      min_count: int minimum count to use for generating the vocabulary. The min\n        count is the minimum number of times a subtoken should appear in the\n        files before it is added to the vocabulary. If set to none, this value\n        is found using binary search.\n      file_byte_limit: (Default 1e6) Maximum number of bytes of sample text that\n        will be drawn from the files.\n      reserved_tokens: List of string tokens that are guaranteed to be at the\n        beginning of the subtoken vocabulary list.\n\n    Returns:\n      Subtokenizer object\n    """"""\n    if reserved_tokens is None:\n      reserved_tokens = RESERVED_TOKENS\n\n    if tf.gfile.Exists(vocab_file):\n      tf.logging.info(""Vocab file already exists (%s)"" % vocab_file)\n    else:\n      tf.logging.info(""Begin steps to create subtoken vocabulary..."")\n      token_counts = _count_tokens(files, file_byte_limit)\n      alphabet = _generate_alphabet_dict(token_counts)\n      subtoken_list = _generate_subtokens_with_target_vocab_size(\n          token_counts, alphabet, target_vocab_size, threshold, min_count,\n          reserved_tokens)\n      tf.logging.info(""Generated vocabulary with %d subtokens."" %\n                      len(subtoken_list))\n      _save_vocab_file(vocab_file, subtoken_list)\n    return Subtokenizer(vocab_file)\n\n  def encode(self, raw_string, add_eos=False):\n    """"""Encodes a string into a list of int subtoken ids.""""""\n    ret = []\n    tokens = _split_string_to_tokens(_native_to_unicode(raw_string))\n    for token in tokens:\n      ret.extend(self._token_to_subtoken_ids(token))\n    if add_eos:\n      ret.append(EOS_ID)\n    return ret\n\n  def _token_to_subtoken_ids(self, token):\n    """"""Encode a single token into a list of subtoken ids.""""""\n    cache_location = hash(token) % self._cache_size\n    cache_key, cache_value = self._cache[cache_location]\n    if cache_key == token:\n      return cache_value\n\n    ret = _split_token_to_subtokens(\n        _escape_token(token, self.alphabet), self.subtoken_to_id_dict,\n        self.max_subtoken_length)\n    ret = [self.subtoken_to_id_dict[subtoken_id] for subtoken_id in ret]\n\n    self._cache[cache_location] = (token, ret)\n    return ret\n\n  def decode(self, subtokens):\n    """"""Converts list of int subtokens ids into a string.""""""\n    if isinstance(subtokens, np.ndarray):\n      # Note that list(subtokens) converts subtokens to a python list, but the\n      # items remain as np.int32. This converts both the array and its items.\n      subtokens = subtokens.tolist()\n\n    if not subtokens:\n      return """"\n\n    assert isinstance(subtokens, list) and isinstance(subtokens[0], int), (\n        ""Subtokens argument passed into decode() must be a list of integers."")\n\n    return _unicode_to_native(\n        join_tokens_to_string(self._subtoken_ids_to_tokens(subtokens)))\n\n  def _subtoken_ids_to_tokens(self, subtokens):\n    """"""Convert list of int subtoken ids to a list of string tokens.""""""\n    escaped_tokens = """".join([\n        self.subtoken_list[s] for s in subtokens\n        if s < len(self.subtoken_list)])\n    escaped_tokens = escaped_tokens.split(""_"")\n\n    # All tokens in the vocabulary list have been escaped (see _escape_token())\n    # so each token must be unescaped when decoding.\n    ret = []\n    for token in escaped_tokens:\n      if token:\n        ret.append(unescape_token(token))\n    return ret\n\n\ndef _save_vocab_file(vocab_file, subtoken_list):\n  """"""Save subtokens to file.""""""\n  with tf.gfile.Open(vocab_file, mode=""w"") as f:\n    for subtoken in subtoken_list:\n      f.write(""\'%s\'\\n"" % _unicode_to_native(subtoken))\n\n\ndef _load_vocab_file(vocab_file, reserved_tokens=None):\n  """"""Load vocabulary while ensuring reserved tokens are at the top.""""""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n\n  subtoken_list = []\n  with tf.gfile.Open(vocab_file, mode=""r"") as f:\n    for line in f:\n      subtoken = _native_to_unicode(line.strip())\n      subtoken = subtoken[1:-1]  # Remove surrounding single-quotes\n      if subtoken in reserved_tokens:\n        continue\n      subtoken_list.append(_native_to_unicode(subtoken))\n  return reserved_tokens + subtoken_list\n\n\ndef _native_to_unicode(s):\n  """"""Convert string to unicode (required in Python 2).""""""\n  if six.PY2:\n    return s if isinstance(s, unicode) else s.decode(""utf-8"")\n  else:\n    return s\n\n\ndef _unicode_to_native(s):\n  """"""Convert string from unicode to native format (required in Python 2).""""""\n  if six.PY2:\n    return s.encode(""utf-8"") if isinstance(s, unicode) else s\n  else:\n    return s\n\n\ndef _split_string_to_tokens(text):\n  """"""Splits text to a list of string tokens.""""""\n  if not text:\n    return []\n  ret = []\n  token_start = 0\n  # Classify each character in the input string\n  is_alnum = [c in _ALPHANUMERIC_CHAR_SET for c in text]\n  for pos in xrange(1, len(text)):\n    if is_alnum[pos] != is_alnum[pos - 1]:\n      token = text[token_start:pos]\n      if token != u"" "" or token_start == 0:\n        ret.append(token)\n      token_start = pos\n  final_token = text[token_start:]\n  ret.append(final_token)\n  return ret\n\n\ndef join_tokens_to_string(tokens):\n  """"""Join a list of string tokens into a single string.""""""\n  token_is_alnum = [t[0] in _ALPHANUMERIC_CHAR_SET for t in tokens]\n  ret = []\n  for i, token in enumerate(tokens):\n    if i > 0 and token_is_alnum[i - 1] and token_is_alnum[i]:\n      ret.append(u"" "")\n    ret.append(token)\n  return """".join(ret)\n\n\ndef _escape_token(token, alphabet):\n  r""""""Replace characters that aren\'t in the alphabet and append ""_"" to token.\n\n  Apply three transformations to the token:\n    1. Replace underline character ""_"" with ""\\u"", and backslash ""\\"" with ""\\\\"".\n    2. Replace characters outside of the alphabet with ""\\###;"", where ### is the\n       character\'s Unicode code point.\n    3. Appends ""_"" to mark the end of a token.\n\n  Args:\n    token: unicode string to be escaped\n    alphabet: list of all known characters\n\n  Returns:\n    escaped string\n  """"""\n  token = token.replace(u""\\\\"", u""\\\\\\\\"").replace(u""_"", u""\\\\u"")\n  ret = [c if c in alphabet and c != u""\\n"" else r""\\%d;"" % ord(c) for c in token]\n  return u"""".join(ret) + ""_""\n\n\ndef unescape_token(token):\n  r""""""Replaces escaped characters in the token with their unescaped versions.\n\n  Applies inverse transformations as _escape_token():\n    1. Replace ""\\u"" with ""_"", and ""\\\\"" with ""\\"".\n    2. Replace ""\\###;"" with the unicode character the ### refers to.\n\n  Args:\n    token: escaped string\n\n  Returns:\n    unescaped string\n  """"""\n\n  def match(m):\n    r""""""Returns replacement string for matched object.\n\n    Matched objects contain one of the strings that matches the regex pattern:\n      r""\\\\u|\\\\\\\\|\\\\([0-9]+);""\n    The strings can be \'\\u\', \'\\\\\', or \'\\###;\' (### is any digit number).\n\n    m.group(0) refers to the entire matched string (\'\\u\', \'\\\\\', or \'\\###;\').\n    m.group(1) refers to the first parenthesized subgroup (\'###\').\n\n    m.group(0) exists for all match objects, while m.group(1) exists only for\n    the string \'\\###;\'.\n\n    This function looks to see if m.group(1) exists. If it doesn\'t, then the\n    matched string must be \'\\u\' or \'\\\\\' . In this case, the corresponding\n    replacement (\'_\' and \'\\\') are returned. Note that in python, a single\n    backslash is written as \'\\\\\', and double backslash as \'\\\\\\\\\'.\n\n    If m.goup(1) exists, then use the integer in m.group(1) to return a\n    unicode character.\n\n    Args:\n      m: match object\n\n    Returns:\n      String to replace matched object with.\n    """"""\n    # Check if the matched strings are \'\\u\' or \'\\\\\'.\n    if m.group(1) is None:\n      return u""_"" if m.group(0) == u""\\\\u"" else u""\\\\""\n\n    # If m.group(1) exists, try and return unicode character.\n    try:\n      return six.unichr(int(m.group(1)))\n    except (ValueError, OverflowError) as _:\n      return _UNDEFINED_UNICODE\n\n  # Use match function to replace escaped substrings in the token.\n  return _UNESCAPE_REGEX.sub(match, token)\n\n\ndef _count_tokens(files, file_byte_limit=1e6):\n  """"""Return token counts of words in the files.\n\n  Samples file_byte_limit bytes from each file, and counts the words that appear\n  in the samples. The samples are semi-evenly distributed across the file.\n\n  Args:\n    files: List of filepaths\n    file_byte_limit: Max number of bytes that will be read from each file.\n\n  Returns:\n    Dictionary mapping tokens to the number of times they appear in the sampled\n    lines from the files.\n  """"""\n  token_counts = collections.defaultdict(int)\n\n  for filepath in files:\n    with tf.gfile.Open(filepath, mode=""r"") as reader:\n      file_byte_budget = file_byte_limit\n      counter = 0\n      lines_to_skip = int(reader.size() / (file_byte_budget * 2))\n      for line in reader:\n        if counter < lines_to_skip:\n          counter += 1\n        else:\n          if file_byte_budget < 0:\n            break\n          line = line.strip()\n          file_byte_budget -= len(line)\n          counter = 0\n\n          # Add words to token counts\n          for token in _split_string_to_tokens(_native_to_unicode(line)):\n            token_counts[token] += 1\n  return token_counts\n\n\ndef _list_to_index_dict(lst):\n  """"""Create dictionary mapping list items to their indices in the list.""""""\n  return {item: n for n, item in enumerate(lst)}\n\n\ndef _split_token_to_subtokens(token, subtoken_dict, max_subtoken_length):\n  """"""Splits a token into subtokens defined in the subtoken dict.""""""\n  ret = []\n  start = 0\n  token_len = len(token)\n  while start < token_len:\n    # Find the longest subtoken, so iterate backwards.\n    for end in xrange(min(token_len, start + max_subtoken_length), start, -1):\n      subtoken = token[start:end]\n      if subtoken in subtoken_dict:\n        ret.append(subtoken)\n        start = end\n        break\n    else:  # Did not break\n      # If there is no possible encoding of the escaped token then one of the\n      # characters in the token is not in the alphabet. This should be\n      # impossible and would be indicative of a bug.\n      raise ValueError(""Was unable to split token \\""%s\\"" into subtokens."" %\n                       token)\n  return ret\n\n\ndef _generate_subtokens_with_target_vocab_size(\n    token_counts, alphabet, target_size, threshold, min_count=None,\n    reserved_tokens=None):\n  """"""Generate subtoken vocabulary close to the target size.""""""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n\n  if min_count is not None:\n    tf.logging.info(""Using min_count=%d to generate vocab with target size %d"" %\n                    (min_count, target_size))\n    return _generate_subtokens(\n        token_counts, alphabet, min_count, reserved_tokens=reserved_tokens)\n\n  def bisect(min_val, max_val):\n    """"""Recursive function to binary search for subtoken vocabulary.""""""\n    cur_count = (min_val + max_val) // 2\n    tf.logging.info(""Binary search: trying min_count=%d (%d %d)"" %\n                    (cur_count, min_val, max_val))\n    subtoken_list = _generate_subtokens(\n        token_counts, alphabet, cur_count, reserved_tokens=reserved_tokens)\n\n    val = len(subtoken_list)\n    tf.logging.info(""Binary search: min_count=%d resulted in %d tokens"" %\n                    (cur_count, val))\n\n    within_threshold = abs(val - target_size) < threshold\n    if within_threshold or min_val >= max_val or cur_count < 2:\n      return subtoken_list\n    if val > target_size:\n      other_subtoken_list = bisect(cur_count + 1, max_val)\n    else:\n      other_subtoken_list = bisect(min_val, cur_count - 1)\n\n    # Return vocabulary dictionary with the closest number of tokens.\n    other_val = len(other_subtoken_list)\n    if abs(other_val - target_size) < abs(val - target_size):\n      return other_subtoken_list\n    return subtoken_list\n\n  tf.logging.info(""Finding best min_count to get target size of %d"" %\n                  target_size)\n  return bisect(_MIN_MIN_COUNT, _MAX_MIN_COUNT)\n\n\ndef _generate_alphabet_dict(iterable, reserved_tokens=None):\n  """"""Create set of characters that appear in any element in the iterable.""""""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n  alphabet = {c for token in iterable for c in token}\n  alphabet |= {c for token in reserved_tokens for c in token}\n  alphabet |= _ESCAPE_CHARS  # Add escape characters to alphabet set.\n  return alphabet\n\n\ndef _count_and_gen_subtokens(\n    token_counts, alphabet, subtoken_dict, max_subtoken_length):\n  """"""Count number of times subtokens appear, and generate new subtokens.\n\n  Args:\n    token_counts: dict mapping tokens to the number of times they appear in the\n      original files.\n    alphabet: list of allowed characters. Used to escape the tokens, which\n      guarantees that all tokens can be split into subtokens.\n    subtoken_dict: dict mapping subtokens to ids.\n    max_subtoken_length: maximum length of subtoken in subtoken_dict.\n\n  Returns:\n    A defaultdict mapping subtokens to the number of times they appear in the\n    tokens. The dict may contain new subtokens.\n  """"""\n  subtoken_counts = collections.defaultdict(int)\n  for token, count in six.iteritems(token_counts):\n    token = _escape_token(token, alphabet)\n    subtokens = _split_token_to_subtokens(\n        token, subtoken_dict, max_subtoken_length)\n\n    # Generate new subtokens by taking substrings from token.\n    start = 0\n    for subtoken in subtokens:\n      for end in xrange(start + 1, len(token) + 1):\n        new_subtoken = token[start:end]\n        subtoken_counts[new_subtoken] += count\n      start += len(subtoken)\n\n  return subtoken_counts\n\n\ndef _filter_and_bucket_subtokens(subtoken_counts, min_count):\n  """"""Return a bucketed list of subtokens that are filtered by count.\n\n  Args:\n    subtoken_counts: defaultdict mapping subtokens to their counts\n    min_count: int count used to filter subtokens\n\n  Returns:\n    List of subtoken sets, where subtokens in set i have the same length=i.\n  """"""\n  # Create list of buckets, where subtokens in bucket i have length i.\n  subtoken_buckets = []\n  for subtoken, count in six.iteritems(subtoken_counts):\n    if count < min_count:  # Filter out subtokens that don\'t appear enough\n      continue\n    while len(subtoken_buckets) <= len(subtoken):\n      subtoken_buckets.append(set())\n    subtoken_buckets[len(subtoken)].add(subtoken)\n  return subtoken_buckets\n\n\ndef _gen_new_subtoken_list(\n    subtoken_counts, min_count, alphabet, reserved_tokens=None):\n  """"""Generate candidate subtokens ordered by count, and new max subtoken length.\n\n  Add subtokens to the candiate list in order of length (longest subtokens\n  first). When a subtoken is added, the counts of each of its prefixes are\n  decreased. Prefixes that don\'t appear much outside the subtoken are not added\n  to the candidate list.\n\n  For example:\n    subtoken being added to candidate list: \'translate\'\n    subtoken_counts: {\'translate\':10, \'t\':40, \'tr\':16, \'tra\':12, ...}\n    min_count: 5\n\n  When \'translate\' is added, subtoken_counts is updated to:\n    {\'translate\':0, \'t\':30, \'tr\':6, \'tra\': 2, ...}\n\n  The subtoken \'tra\' will not be added to the candidate list, because it appears\n  twice (less than min_count) outside of \'translate\'.\n\n  Args:\n    subtoken_counts: defaultdict mapping str subtokens to int counts\n    min_count: int minumum count requirement for subtokens\n    alphabet: set of characters. Each character is added to the subtoken list to\n      guarantee that all tokens can be encoded.\n    reserved_tokens: list of tokens that will be added to the beginning of the\n      returned subtoken list.\n\n  Returns:\n    List of candidate subtokens in decreasing count order, and maximum subtoken\n    length\n  """"""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n\n  # Create a list of (count, subtoken) for each candidate subtoken.\n  subtoken_candidates = []\n\n  # Use bucketted list to iterate through subtokens in order of length.\n  # subtoken_buckets[i] = set(subtokens), where each subtoken has length i.\n  subtoken_buckets = _filter_and_bucket_subtokens(subtoken_counts, min_count)\n  max_subtoken_length = len(subtoken_buckets) - 1\n\n  # Go through the list in reverse order to consider longer subtokens first.\n  for subtoken_len in xrange(max_subtoken_length, 0, -1):\n    for subtoken in subtoken_buckets[subtoken_len]:\n      count = subtoken_counts[subtoken]\n\n      # Possible if this subtoken is a prefix of another token.\n      if count < min_count:\n        continue\n\n      # Ignore alphabet/reserved tokens, which will be added manually later.\n      if subtoken not in alphabet and subtoken not in reserved_tokens:\n        subtoken_candidates.append((count, subtoken))\n\n      # Decrement count of the subtoken\'s prefixes (if a longer subtoken is\n      # added, its prefixes lose priority to be added).\n      for end in xrange(1, subtoken_len):\n        subtoken_counts[subtoken[:end]] -= count\n\n  # Add alphabet subtokens (guarantees that all strings are encodable).\n  subtoken_candidates.extend((subtoken_counts.get(a, 0), a) for a in alphabet)\n\n  # Order subtoken candidates by decreasing count.\n  subtoken_list = [t for _, t in sorted(subtoken_candidates, reverse=True)]\n\n  # Add reserved tokens to beginning of the list.\n  subtoken_list = reserved_tokens + subtoken_list\n  return subtoken_list, max_subtoken_length\n\n\ndef _generate_subtokens(\n    token_counts, alphabet, min_count, num_iterations=4,\n    reserved_tokens=None):\n  """"""Create a list of subtokens in decreasing order of frequency.\n\n  Args:\n    token_counts: dict mapping str tokens -> int count\n    alphabet: set of characters\n    min_count: int minimum number of times a subtoken must appear before it is\n      added to the vocabulary.\n    num_iterations: int number of iterations to generate new tokens.\n    reserved_tokens: list of tokens that will be added to the beginning to the\n      returned subtoken list.\n\n  Returns:\n    Sorted list of subtokens (most frequent first)\n  """"""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n\n  # Use alphabet set to create initial list of subtokens\n  subtoken_list = reserved_tokens + list(alphabet)\n  max_subtoken_length = 1\n\n  # On each iteration, segment all words using the subtokens defined in\n  # subtoken_dict, count how often the resulting subtokens appear, and update\n  # the dictionary with subtokens w/ high enough counts.\n  for i in xrange(num_iterations):\n    tf.logging.info(""\\tGenerating subtokens: iteration %d"" % i)\n    # Generate new subtoken->id dictionary using the new subtoken list.\n    subtoken_dict = _list_to_index_dict(subtoken_list)\n\n    # Create dict mapping subtoken->count, with additional subtokens created\n    # from substrings taken from the tokens.\n    subtoken_counts = _count_and_gen_subtokens(\n        token_counts, alphabet, subtoken_dict, max_subtoken_length)\n\n    # Generate new list of subtokens sorted by subtoken count.\n    subtoken_list, max_subtoken_length = _gen_new_subtoken_list(\n        subtoken_counts, min_count, alphabet, reserved_tokens)\n\n    tf.logging.info(""\\tVocab size: %d"" % len(subtoken_list))\n  return subtoken_list\n'"
open_seq2seq/parts/centaur/__init__.py,0,b'# Copyright (c) 2019 NVIDIA Corporation\nfrom .conv_block import ConvBlock\nfrom .attention import AttentionBlock\nfrom .batch_norm import BatchNorm1D\nfrom .prenet import Prenet\n'
open_seq2seq/parts/centaur/attention.py,4,"b'# Copyright (c) 2019 NVIDIA Corporation\nimport tensorflow as tf\n\nfrom open_seq2seq.parts.centaur import ConvBlock\nfrom open_seq2seq.parts.transformer import attention_layer\nfrom open_seq2seq.parts.transformer.common import PrePostProcessingWrapper\nfrom open_seq2seq.parts.transformer.ffn_layer import FeedFowardNetwork\n\n\nclass AttentionBlock:\n  """"""\n  Attention block for Centaur model.\n  """"""\n\n  def __init__(self,\n               hidden_size,\n               attention_dropout,\n               layer_postprocess_dropout,\n               training,\n               cnn_dropout_prob,\n               regularizer=None,\n               conv_params=None,\n               n_heads=1,\n               window_size=None,\n               back_step_size=None,\n               name=""attention_block""):\n    """"""\n    Attention block constructor.\n\n    Args:\n      hidden_size: dimensionality of hidden embeddings.\n      attention_dropout: dropout rate for attention layer.\n      layer_postprocess_dropout:  dropout rate for sublayer.\n      training: whether it is training mode.\n      cnn_dropout_prob: dropout probabilty for cnn layers.\n      regularizer: regularizer for the convolution kernel.\n      conv_params: description of convolutional layer.\n      n_heads: number of attention heads. Defaults to 1.\n      window_size: size of attention window for forcing\n        monotonic attention during the inference. Defaults to None.\n      back_step_size: number of steps attention is allowed to\n        go back during the inference. Defaults to 0.\n      name: name of the block.\n    """"""\n\n    self.name = name\n    self.conv = None\n\n    if conv_params:\n      self.conv = ConvBlock.create(\n          index=0,\n          conv_params=conv_params,\n          regularizer=regularizer,\n          bn_momentum=0.95,\n          bn_epsilon=1e-8,\n          cnn_dropout_prob=cnn_dropout_prob,\n          training=training\n      )\n      self.conv.name = ""conv""\n\n    attention = attention_layer.Attention(\n        hidden_size=hidden_size,\n        num_heads=n_heads,\n        attention_dropout=attention_dropout,\n        regularizer=regularizer,\n        train=training,\n        window_size=window_size,\n        back_step_size=back_step_size,\n    )\n\n    feed_forward = tf.layers.Dense(\n        units=hidden_size,\n        use_bias=True,\n        kernel_regularizer=regularizer\n    )\n\n    wrapper_params = {\n        ""hidden_size"": hidden_size,\n        ""layer_postprocess_dropout"": layer_postprocess_dropout\n    }\n\n    self.attention = PrePostProcessingWrapper(\n        layer=attention,\n        params=wrapper_params,\n        training=training\n    )\n\n    self.feed_forward = PrePostProcessingWrapper(\n        layer=feed_forward,\n        params=wrapper_params,\n        training=training\n    )\n\n  def __call__(self,\n               decoder_inputs,\n               encoder_outputs,\n               attention_bias,\n               positions=None):\n    with tf.variable_scope(self.name):\n      y = decoder_inputs\n\n      if self.conv:\n        y = self.conv(y)\n\n      with tf.variable_scope(""attention""):\n        y = self.attention(\n            y,\n            encoder_outputs,\n            attention_bias,\n            positions=positions\n        )\n\n      with tf.variable_scope(""feed_forward""):\n        y = self.feed_forward(y)\n\n      return y\n'"
open_seq2seq/parts/centaur/batch_norm.py,4,"b'# Copyright (c) 2019 NVIDIA Corporation\nimport tensorflow as tf\n\n\nclass BatchNorm1D:\n  """"""\n  1D batch normalization layer.\n  """"""\n\n  def __init__(self, *args, **kwargs):\n    super(BatchNorm1D, self).__init__()\n    self.norm = tf.layers.BatchNormalization(*args, **kwargs)\n\n  def __call__(self, x, training):\n    with tf.variable_scope(""batch_norm_1d""):\n      y = tf.expand_dims(x, axis=1)\n      y = self.norm(y, training=training)\n      y = tf.squeeze(y, axis=1)\n      return y\n'"
open_seq2seq/parts/centaur/conv_block.py,5,"b'# Copyright (c) 2019 NVIDIA Corporation\nimport tensorflow as tf\n\nfrom .batch_norm import BatchNorm1D\n\n\nclass ConvBlock:\n  """"""\n  Convolutional block for Centaur model.\n  """"""\n\n  def __init__(self,\n               name,\n               conv,\n               norm,\n               activation_fn,\n               dropout,\n               training,\n               is_residual,\n               is_causal):\n    """"""\n    Convolutional block constructor.\n\n    Args:\n      name: name of the block.\n      conv: convolutional layer.\n      norm: normalization layer to use after the convolutional layer.\n      activation_fn: activation function to use after the normalization.\n      dropout: dropout rate.\n      training: whether it is training mode.\n      is_residual: whether the block should contain a residual connection.\n      is_causal: whether the convolutional layer should be causal.\n    """"""\n\n    self.name = name\n    self.conv = conv\n    self.norm = norm\n    self.activation_fn = activation_fn\n    self.dropout = dropout\n    self.training = training\n    self.is_residual = is_residual\n    self.is_casual = is_causal\n\n  def __call__(self, x):\n    with tf.variable_scope(self.name):\n      if self.is_casual:\n        # Add padding from the left side to avoid looking to the future\n        pad_size = self.conv.kernel_size[0] - 1\n        y = tf.pad(x, [[0, 0], [pad_size, 0], [0, 0]])\n      else:\n        y = x\n\n      y = self.conv(y)\n\n      if self.norm is not None:\n        y = self.norm(y, training=self.training)\n\n      if self.activation_fn is not None:\n        y = self.activation_fn(y)\n\n      if self.dropout is not None:\n        y = self.dropout(y, training=self.training)\n\n      return x + y if self.is_residual else y\n\n  @staticmethod\n  def create(index,\n             conv_params,\n             regularizer,\n             bn_momentum,\n             bn_epsilon,\n             cnn_dropout_prob,\n             training,\n             is_residual=True,\n             is_causal=False):\n    activation_fn = conv_params.get(""activation_fn"", tf.nn.relu)\n\n    conv = tf.layers.Conv1D(\n        name=""conv_%d"" % index,\n        filters=conv_params[""num_channels""],\n        kernel_size=conv_params[""kernel_size""],\n        strides=conv_params[""stride""],\n        padding=conv_params[""padding""],\n        kernel_regularizer=regularizer\n    )\n\n    norm = BatchNorm1D(\n        name=""bn_%d"" % index,\n        gamma_regularizer=regularizer,\n        momentum=bn_momentum,\n        epsilon=bn_epsilon\n    )\n\n    dropout = tf.layers.Dropout(\n        name=""dropout_%d"" % index,\n        rate=cnn_dropout_prob\n    )\n\n    if ""is_causal"" in conv_params:\n      is_causal = conv_params[""is_causal""]\n\n    if ""is_residual"" in conv_params:\n      is_residual = conv_params[""is_residual""]\n\n    return ConvBlock(\n        name=""layer_%d"" % index,\n        conv=conv,\n        norm=norm,\n        activation_fn=activation_fn,\n        dropout=dropout,\n        training=training,\n        is_residual=is_residual,\n        is_causal=is_causal\n    )\n'"
open_seq2seq/parts/centaur/prenet.py,3,"b'# Copyright (c) 2019 NVIDIA Corporation\nimport tensorflow as tf\n\n\nclass Prenet:\n  """"""\n  Centaur decoder pre-net.\n  """"""\n\n  def __init__(self,\n               n_layers,\n               hidden_size,\n               activation_fn,\n               dropout=0.5,\n               regularizer=None,\n               training=True,\n               dtype=None,\n               name=""prenet""):\n    """"""\n    Pre-net constructor.\n\n    Args:\n      n_layers: number of fully-connected layers to use.\n      hidden_size: number of units in each pre-net layer.\n      activation_fn: activation function to use.\n      dropout: dropout rate. Defaults to 0.5.\n      regularizer: regularizer for the convolution kernel.\n        Defaults to None.\n      training: whether it is training mode. Defaults to None.\n      dtype: dtype of the layer\'s weights. Defaults to None.\n      name: name of the block.\n    """"""\n\n    self.name = name\n    self.layers = []\n    self.dropout = dropout\n    self.training = training\n\n    for i in range(n_layers):\n      layer = tf.layers.Dense(\n          name=""layer_%d"" % i,\n          units=hidden_size,\n          use_bias=True,\n          activation=activation_fn,\n          kernel_regularizer=regularizer,\n          dtype=dtype\n      )\n      self.layers.append(layer)\n\n  def __call__(self, x):\n    with tf.variable_scope(self.name):\n      for layer in self.layers:\n        x = tf.layers.dropout(\n            layer(x),\n            rate=self.dropout,\n            training=self.training\n        )\n\n      return x\n'"
open_seq2seq/parts/cnns/__init__.py,0,b'# Copyright (c) 2018 NVIDIA Corporation\n'
open_seq2seq/parts/cnns/conv_blocks.py,20,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nimport tensorflow as tf\nfrom .tcn import tcn\n\nlayers_dict = {\n    ""conv1d"": tf.layers.conv1d,\n    ""sep_conv1d"": tf.layers.separable_conv1d,\n    ""conv2d"": tf.layers.conv2d,\n    ""tcn"": tcn,\n}\n\n\ndef conv_actv(layer_type, name, inputs, filters, kernel_size, activation_fn,\n              strides, padding, regularizer, training, data_format, dilation=1):\n  """"""Helper function that applies convolution and activation.\n    Args:\n      layer_type: the following types are supported\n        \'conv1d\', \'conv2d\'\n  """"""\n  layer = layers_dict[layer_type]\n\n  if layer_type == \'sep_conv1d\':\n    conv = layer(\n        name=""{}"".format(name),\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        dilation_rate=dilation,\n        depthwise_regularizer=regularizer,\n        pointwise_regularizer=regularizer,\n        use_bias=False,\n        data_format=data_format,\n    )\n  else:\n    conv = layer(\n        name=""{}"".format(name),\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        dilation_rate=dilation,\n        kernel_regularizer=regularizer,\n        use_bias=False,\n        data_format=data_format,\n    )\n\n  output = conv\n  if activation_fn is not None:\n    output = activation_fn(output)\n  return output\n\ndef conv_bn_res_bn_actv(layer_type, name, inputs, res_inputs, filters,\n                        kernel_size, activation_fn, strides, padding,\n                        regularizer, training, data_format, bn_momentum,\n                        bn_epsilon, dilation=1,\n                        drop_block_prob=0.0, drop_block=False):\n  layer = layers_dict[layer_type]\n\n  if not isinstance(res_inputs, list):\n    res_inputs = [res_inputs]\n    # For backwards compatibiliaty with earlier models\n    res_name = ""{}/res""\n    res_bn_name = ""{}/res_bn""\n  else:\n    res_name = ""{}/res_{}""\n    res_bn_name = ""{}/res_bn_{}""\n\n  res_aggregation = 0\n  for i, res in enumerate(res_inputs):\n    res = layer(\n        res,\n        filters,\n        1,\n        name=res_name.format(name, i),\n        use_bias=False,\n    )\n    squeeze = False\n    if ""conv1d"" in layer_type:\n      axis = 1 if data_format == \'channels_last\' else 2\n      res = tf.expand_dims(res, axis=axis)  # NWC --> NHWC\n      squeeze = True\n    res = tf.layers.batch_normalization(\n        name=res_bn_name.format(name, i),\n        inputs=res,\n        gamma_regularizer=regularizer,\n        training=training,\n        axis=-1 if data_format == \'channels_last\' else 1,\n        momentum=bn_momentum,\n        epsilon=bn_epsilon,\n    )\n    if squeeze:\n      res = tf.squeeze(res, axis=axis)\n\n    res_aggregation += res\n\n  if layer_type == ""sep_conv1d"":\n    conv = layer(\n        name=""{}"".format(name),\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        dilation_rate=dilation,\n        depthwise_regularizer=regularizer,\n        pointwise_regularizer=regularizer,\n        use_bias=False,\n        data_format=data_format,\n    )\n  else:\n    conv = layer(\n        name=""{}"".format(name),\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        dilation_rate=dilation,\n        kernel_regularizer=regularizer,\n        use_bias=False,\n        data_format=data_format,\n    )\n\n  # trick to make batchnorm work for mixed precision training.\n  # To-Do check if batchnorm works smoothly for >4 dimensional tensors\n  squeeze = False\n  if ""conv1d"" in layer_type:\n    axis = 1 if data_format == \'channels_last\' else 2\n    conv = tf.expand_dims(conv, axis=axis)  # NWC --> NHWC\n    squeeze = True\n\n  bn = tf.layers.batch_normalization(\n      name=""{}/bn"".format(name),\n      inputs=conv,\n      gamma_regularizer=regularizer,\n      training=training,\n      axis=-1 if data_format == \'channels_last\' else 1,\n      momentum=bn_momentum,\n      epsilon=bn_epsilon,\n  )\n\n  if squeeze:\n    bn = tf.squeeze(bn, axis=axis)\n\n  output = bn + res_aggregation\n\n  if drop_block_prob > 0:\n    if training:\n      output = tf.cond(\n        tf.random_uniform(shape=[]) < drop_block_prob,\n        lambda: res_aggregation,\n        lambda: bn + res_aggregation\n        )\n    elif drop_block:\n      output = res_aggregation\n\n  if activation_fn is not None:\n    output = activation_fn(output)\n  return output\n\ndef conv_bn_actv(layer_type, name, inputs, filters, kernel_size, activation_fn,\n                 strides, padding, regularizer, training, data_format,\n                 bn_momentum, bn_epsilon, dilation=1):\n  """"""Helper function that applies convolution, batch norm and activation.\n    Args:\n      layer_type: the following types are supported\n        \'conv1d\', \'conv2d\'\n  """"""\n  layer = layers_dict[layer_type]\n\n  if layer_type == \'sep_conv1d\':\n    conv = layer(\n        name=""{}"".format(name),\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        dilation_rate=dilation,\n        depthwise_regularizer=regularizer,\n        pointwise_regularizer=regularizer,\n        use_bias=False,\n        data_format=data_format,\n    )\n  else:\n    conv = layer(\n        name=""{}"".format(name),\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        dilation_rate=dilation,\n        kernel_regularizer=regularizer,\n        use_bias=False,\n        data_format=data_format,\n    )\n\n  # trick to make batchnorm work for mixed precision training.\n  # To-Do check if batchnorm works smoothly for >4 dimensional tensors\n  squeeze = False\n  if ""conv1d"" in layer_type:\n    axis = 1 if data_format == \'channels_last\' else 2\n    conv = tf.expand_dims(conv, axis=axis)  # NWC --> NHWC\n    squeeze = True\n\n  bn = tf.layers.batch_normalization(\n      name=""{}/bn"".format(name),\n      inputs=conv,\n      gamma_regularizer=regularizer,\n      training=training,\n      axis=-1 if data_format == \'channels_last\' else 1,\n      momentum=bn_momentum,\n      epsilon=bn_epsilon,\n  )\n\n  if squeeze:\n    bn = tf.squeeze(bn, axis=axis)\n\n  output = bn\n  if activation_fn is not None:\n    output = activation_fn(output)\n  return output\n\n\ndef conv_ln_actv(layer_type, name, inputs, filters, kernel_size, activation_fn,\n                 strides, padding, regularizer, training, data_format,\n                 dilation=1):\n  """"""Helper function that applies convolution, layer norm and activation.\n    Args:\n      layer_type: the following types are supported\n        \'conv1d\', \'conv2d\'\n  """"""\n  layer = layers_dict[layer_type]\n\n  conv = layer(\n      name=""{}"".format(name),\n      inputs=inputs,\n      filters=filters,\n      kernel_size=kernel_size,\n      strides=strides,\n      padding=padding,\n      dilation_rate=dilation,\n      kernel_regularizer=regularizer,\n      use_bias=False,\n      data_format=data_format,\n  )\n\n  if data_format == \'channels_first\':\n    if layer_type == ""conv1d"":\n      conv = tf.transpose(conv, [0, 2, 1])\n    elif layer_type == ""conv2d"":\n      conv = tf.transpose(conv, [0, 2, 3, 1])\n  ln = tf.contrib.layers.layer_norm(\n      inputs=conv,\n  )\n  if data_format == \'channels_first\':\n    if layer_type == ""conv1d"":\n      ln = tf.transpose(ln, [0, 2, 1])\n    elif layer_type == ""conv2d"":\n      ln = tf.transpose(ln, [0, 3, 1, 2])\n\n  output = ln\n  if activation_fn is not None:\n    output = activation_fn(output)\n  return output\n\n\ndef conv_in_actv(layer_type, name, inputs, filters, kernel_size, activation_fn,\n                 strides, padding, regularizer, training, data_format,\n                 dilation=1):\n  """"""Helper function that applies convolution, instance norm and activation.\n    Args:\n      layer_type: the following types are supported\n        \'conv1d\', \'conv2d\'\n  """"""\n  layer = layers_dict[layer_type]\n\n  conv = layer(\n      name=""{}"".format(name),\n      inputs=inputs,\n      filters=filters,\n      kernel_size=kernel_size,\n      strides=strides,\n      padding=padding,\n      dilation_rate=dilation,\n      kernel_regularizer=regularizer,\n      use_bias=False,\n      data_format=data_format,\n  )\n\n  sn = tf.contrib.layers.instance_norm(\n      inputs=conv,\n      data_format=""NHWC"" if data_format == \'channels_last\' else ""NCHW""\n  )\n\n  output = sn\n  if activation_fn is not None:\n    output = activation_fn(output)\n  return output\n'"
open_seq2seq/parts/cnns/tcn.py,6,"b'# Copyright (c) 2018 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\n\nclass TemporalConvolutionalLayer(tf.layers.Conv1D):\n  """"""Temporal Convolutional layer\n  """"""\n\n  def __init__(\n      self,\n      filters,\n      kernel_size,\n      strides=1,\n      dilation_rate=1,\n      activation=None,\n      data_format=\'channels_last\',\n      name=""temporal_convolutional"",\n      use_bias=True,\n      kernel_initializer=None,\n      bias_initializer=tf.zeros_initializer(),\n      kernel_regularizer=None,\n      bias_regularizer=None,\n      activity_regularizer=None,\n      kernel_constraint=None,\n      bias_constraint=None,\n      trainable=True,\n      padding=\'valid\',\n      **kwargs\n  ):\n    super(TemporalConvolutionalLayer, self).__init__(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        dilation_rate=dilation_rate,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=kernel_initializer,\n        bias_initializer=bias_initializer,\n        kernel_regularizer=kernel_regularizer,\n        bias_regularizer=bias_regularizer,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint,\n        bias_constraint=bias_constraint,\n        trainable=trainable,\n        data_format=data_format,\n        name=name,\n        padding=\'valid\',\n        **kwargs\n    )\n\n  def call(self, inputs):\n    pads = (self.kernel_size[0] - 1) * self.dilation_rate[0]\n    padding = tf.fill([tf.shape(inputs)[0], pads, tf.shape(\n        inputs)[2]], tf.constant(0, dtype=inputs.dtype))\n    inputs = tf.concat([padding, inputs], 1)\n    return super(TemporalConvolutionalLayer, self).call(inputs)\n\n\ndef tcn(inputs,\n        filters,\n        kernel_size,\n        strides=1,\n        padding=\'valid\',\n        data_format=\'channels_last\',\n        dilation_rate=1,\n        activation=None,\n        use_bias=True,\n        kernel_initializer=None,\n        bias_initializer=tf.zeros_initializer(),\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        trainable=True,\n        name=None,\n        reuse=None):\n  """"""Functional interface for temporal convolution layer.\n  """"""\n  layer = TemporalConvolutionalLayer(\n      filters=filters,\n      kernel_size=kernel_size,\n      strides=strides,\n      padding=padding,\n      data_format=data_format,\n      dilation_rate=dilation_rate,\n      activation=activation,\n      use_bias=use_bias,\n      kernel_initializer=kernel_initializer,\n      bias_initializer=bias_initializer,\n      kernel_regularizer=kernel_regularizer,\n      bias_regularizer=bias_regularizer,\n      activity_regularizer=activity_regularizer,\n      kernel_constraint=kernel_constraint,\n      bias_constraint=bias_constraint,\n      trainable=trainable,\n      name=name,\n      _reuse=reuse,\n      _scope=name)\n  return layer.apply(inputs)\n'"
open_seq2seq/parts/convs2s/__init__.py,0,b'# Copyright (c) 2018 NVIDIA Corporation\n'
open_seq2seq/parts/convs2s/attention_wn_layer.py,9,"b'""""""Implementation of the attention layer for convs2s.\nInspired from https://github.com/tobyyouup/conv_seq2seq""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nimport math\nfrom open_seq2seq.parts.convs2s.ffn_wn_layer import FeedFowardNetworkNormalized\n\n\nclass AttentionLayerNormalized(tf.layers.Layer):\n  """"""Attention layer for convs2s with weight normalization""""""\n\n  def __init__(self, in_dim, embed_size, layer_id, add_res, mode,\n               scaling_factor=math.sqrt(0.5),\n               normalization_type=""weight_norm"",\n               regularizer=None,\n               init_var=None,\n               ):\n    """"""initializes the attention layer.\n    It uses weight normalization for linear projections\n    (Salimans & Kingma, 2016)  w = g * v/2-norm(v)\n\n    Args:\n      in_dim: int last dimension of the inputs\n      embed_size: int target embedding size\n      layer_id: int the id of current convolution layer\n      add_res: bool whether residual connection should be added or not\n      mode: str current mode\n    """"""\n    super(AttentionLayerNormalized, self).__init__()\n\n    self.add_res = add_res\n    self.scaling_factor = scaling_factor\n    self.regularizer = regularizer\n\n    with tf.variable_scope(""attention_layer_"" + str(layer_id)):\n\n      # linear projection layer to project the attention input to target space\n      self.tgt_embed_proj = FeedFowardNetworkNormalized(\n          in_dim,\n          embed_size,\n          dropout=1.0,\n          var_scope_name=""att_linear_mapping_tgt_embed"",\n          mode=mode,\n          normalization_type=normalization_type,\n          regularizer=self.regularizer,\n          init_var=init_var\n      )\n\n      # linear projection layer to project back to the input space\n      self.out_proj = FeedFowardNetworkNormalized(\n          embed_size,\n          in_dim,\n          dropout=1.0,\n          var_scope_name=""att_linear_mapping_out"",\n          mode=mode,\n          normalization_type=normalization_type,\n          regularizer=self.regularizer,\n          init_var=init_var\n      )\n\n  def call(self, input, target_embed, encoder_output_a, encoder_output_b,\n           input_attention_bias):\n    """"""Calculates the attention vectors.\n\n    Args:\n      input: A float32 tensor with shape [batch_size, length, in_dim]\n      target_embed: A float32 tensor with shape [batch_size, length, in_dim]\n                    containing the target embeddings\n      encoder_output_a: A float32 tensor with shape [batch_size, length, out_dim]\n                        containing the first encoder outputs, uses as the keys\n      encoder_output_b: A float32 tensor with shape [batch_size, length, src_emb_dim]\n                        containing the second encoder outputs, uses as the values\n      input_attention_bias: A float32 tensor with shape [batch_size, length, 1]\n                            containing the bias used to mask the paddings\n\n    Returns:\n      float32 tensor with shape [batch_size, length, out_dim].\n    """"""\n\n    h_proj = self.tgt_embed_proj(input)\n    d_proj = (h_proj + target_embed) * self.scaling_factor\n    att_score = tf.matmul(d_proj, encoder_output_a, transpose_b=True)\n\n    # Masking need to be done in float32. Added to support mixed-precision training.\n    att_score = tf.cast(x=att_score, dtype=tf.float32)\n\n    # mask out the paddings\n    if input_attention_bias is not None:\n      att_score = att_score + input_attention_bias\n\n    att_score = tf.nn.softmax(att_score)\n\n    # Cast back to original type\n    att_score = tf.cast(x=att_score, dtype=encoder_output_b.dtype)\n\n    length = tf.cast(tf.shape(encoder_output_b), encoder_output_b.dtype)\n    output = tf.matmul(att_score, encoder_output_b) * \\\n             length[1] * tf.cast(tf.sqrt(1.0 / length[1]), dtype=encoder_output_b.dtype)\n    output = self.out_proj(output)\n\n    if self.add_res:\n      output = (output + input) * self.scaling_factor\n\n    return output\n'"
open_seq2seq/parts/convs2s/conv_wn_layer.py,19,"b'""""""Implementation of a 1d convolutional layer with weight normalization.\nInspired from https://github.com/tobyyouup/conv_seq2seq""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nimport math\nfrom open_seq2seq.parts.convs2s.utils import gated_linear_units\nfrom open_seq2seq.parts.transformer.common import LayerNormalization\n\n\nclass Conv1DNetworkNormalized(tf.layers.Layer):\n  """"""1D convolutional layer with weight normalization""""""\n\n  def __init__(self,\n               in_dim,\n               out_dim,\n               kernel_width,\n               mode,\n               layer_id,\n               hidden_dropout,\n               conv_padding,\n               decode_padding,\n               activation=gated_linear_units,\n               normalization_type=""weight_norm"",\n               regularizer=None, # tf.contrib.layers.l2_regularizer(scale=1e-4)\n               init_var=None,\n               ):\n    """"""initializes the 1D convolution layer.\n    It uses weight normalization (Salimans & Kingma, 2016)  w = g * v/2-norm(v)\n\n    Args:\n      in_dim: int last dimension of the inputs\n      out_dim: int new dimension for the output\n      kernel_width: int width of kernel\n      mode: str the current mode\n      layer_id: int the id of current convolution layer\n      hidden_dropout: float the keep-dropout value used on the input.\n                      Give 1.0 if no dropout.\n                      It is used to initialize the weights of convolution.\n      conv_padding: str the type of padding done for convolution\n      decode_padding: bool specifies if this convolution layer is in decoder or not\n                          in decoder padding is done explicitly before convolution\n      activation: the activation function applies after the convolution\n      normalization_type: str specifies the normalization used for the layer.\n                    ""weight_norm"" for weight normalization or\n                    ""batch_norm"" for batch normalization or\n                    ""layer_norm"" for layer normalization\n      regularizer: the regularizer for the batch normalization\n\n    """"""\n\n    super(Conv1DNetworkNormalized, self).__init__()\n    self.mode = mode\n    self.conv_padding = conv_padding\n    self.decode_padding = decode_padding\n    self.hidden_dropout = hidden_dropout\n    self.kernel_width = kernel_width\n    self.layer_id = layer_id\n    self.act_func = activation\n    self.regularizer = regularizer\n\n    if normalization_type == ""batch_norm"":\n      self.apply_batch_norm = True\n      self.bias_enabled = False\n      self.wn_enabled = False\n      self.apply_layer_norm = False\n    elif normalization_type == ""weight_norm"":\n      self.apply_batch_norm = False\n      self.bias_enabled = True\n      self.wn_enabled = True\n      self.apply_layer_norm = False\n    elif normalization_type == ""layer_norm"":\n      self.apply_batch_norm = False\n      self.bias_enabled = False\n      self.wn_enabled = False\n      self.apply_layer_norm = True\n    elif normalization_type is None:\n      self.apply_batch_norm = False\n      self.bias_enabled = True\n      self.wn_enabled = False\n      self.apply_layer_norm = False\n    else:\n      raise ValueError(""Wrong normalization type: {}"".format(normalization_type))\n\n    if activation == gated_linear_units:\n      conv_out_size = 2 * out_dim\n    else:\n      conv_out_size = out_dim\n\n    with tf.variable_scope(""conv_layer_"" + str(layer_id)):\n      if init_var is None:\n        V_std = math.sqrt(4.0 * hidden_dropout / (kernel_width * in_dim))\n      else:\n        V_std = init_var\n      if self.wn_enabled:\n        self.V = tf.get_variable(\n            \'V\',\n            shape=[kernel_width, in_dim, conv_out_size],\n            initializer=tf.random_normal_initializer(mean=0, stddev=V_std),\n            trainable=True)\n        self.V_norm = tf.norm(self.V.initialized_value(), axis=[0, 1])\n        self.g = tf.get_variable(\'g\', initializer=self.V_norm, trainable=True)\n        self.W = tf.reshape(self.g, [1, 1, conv_out_size]) * tf.nn.l2_normalize(\n            self.V, [0, 1])\n      else:\n        self.W = tf.get_variable(\n            \'W\',\n            shape=[kernel_width, in_dim, conv_out_size],\n            initializer=tf.random_normal_initializer(mean=0, stddev=V_std),\n            trainable=True,\n            regularizer=self.regularizer)\n\n      if self.bias_enabled:\n        self.b = tf.get_variable(\n            \'b\',\n            shape=[conv_out_size],\n            initializer=tf.zeros_initializer(),\n            trainable=True)\n      else:\n        self.b = None\n\n      if self.apply_layer_norm:\n        self.layer_norm = LayerNormalization(out_dim)\n      else:\n        self.layer_norm = None\n\n\n  def call(self, input):\n    """"""Applies convolution with gated linear units on x.\n\n    Args:\n      x: A float32 tensor with shape [batch_size, length, in_dim]\n\n    Returns:\n      float32 tensor with shape [batch_size, length, out_dim].\n    """"""\n    output = input\n\n    if self.mode == ""train"":\n      output = tf.nn.dropout(output, self.hidden_dropout)\n\n    if self.decode_padding:\n      output = tf.pad(\n          output, [[0, 0], [self.kernel_width - 1, self.kernel_width - 1], [0, 0]],\n          ""CONSTANT"")\n\n    output = tf.nn.conv1d(\n        value=output, filters=self.W, stride=1, padding=self.conv_padding)\n\n    if self.decode_padding and self.kernel_width > 1:\n      output = output[:, 0:-self.kernel_width + 1, :]\n\n    if self.apply_batch_norm:\n      # trick to make batchnorm work for mixed precision training.\n      bn_input = tf.expand_dims(output, axis=1)\n      bn_output = tf.layers.batch_normalization(\n          name=""batch_norm_"" + str(self.layer_id),\n          inputs=bn_input,\n          training=self.mode == \'train\',\n          axis=-1,\n          momentum=0.95,\n          epsilon=1e-4\n      )\n      output = tf.squeeze(bn_output, axis=1)\n\n    if self.apply_layer_norm:\n      output = self.layer_norm(output)\n    if self.b is not None:\n      output = tf.nn.bias_add(output, self.b)\n\n    if self.act_func is not None:\n      output = self.act_func(output)\n    return output\n'"
open_seq2seq/parts/convs2s/ffn_wn_layer.py,20,"b'""""""Implementation of fully connected network with weight normalization.\nInspired from https://github.com/tobyyouup/conv_seq2seq""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nimport math\nfrom open_seq2seq.parts.transformer.common import LayerNormalization\n\n\nclass FeedFowardNetworkNormalized(tf.layers.Layer):\n  """"""Fully connected feedforward network with weight normalization""""""\n\n  def __init__(self,\n               in_dim,\n               out_dim,\n               dropout,\n               var_scope_name,\n               mode,\n               normalization_type=""weight_norm"",\n               regularizer=None,\n               init_var=None\n               ):\n    """"""initializes the linear layer.\n    This layer projects from in_dim-dimenstional space to out_dim-dimentional space.\n    It uses weight normalization (Salimans & Kingma, 2016)  w = g * v/2-norm(v)\n\n    Args:\n      in_dim: int last dimension of the inputs\n      out_dim: int new dimension for the output\n      dropout: float the keep-dropout value used in the previous layer.\n                  It is used to initialize the weights. Give 1.0 if no dropout.\n      var_scope_name: str the scope name for the weight variables\n      mode: str current mode\n      normalization_type: str specifies the normalization used for this layer.\n                          ""weight_norm"" for weight normalization or\n                          ""batch_norm"" for batch normalization\n    """"""\n    super(FeedFowardNetworkNormalized, self).__init__()\n    self.out_dim = out_dim\n    self.in_dim = in_dim\n    self.normalization_type = normalization_type\n    self.regularizer = regularizer\n    self.var_scope_name = var_scope_name\n    self.mode = mode\n\n    if normalization_type == ""batch_norm"":\n      self.apply_batch_norm = True\n      self.bias_enabled = False\n      self.wn_enabled = False\n      self.apply_layer_norm = False\n    elif normalization_type == ""weight_norm"":\n      self.apply_batch_norm = False\n      self.bias_enabled = True\n      self.wn_enabled = True\n      self.apply_layer_norm = False\n    elif normalization_type == ""layer_norm"":\n      self.apply_batch_norm = False\n      self.bias_enabled = False\n      self.wn_enabled = False\n      self.apply_layer_norm = True\n    elif normalization_type is None:\n      self.apply_batch_norm = False\n      self.bias_enabled = True\n      self.wn_enabled = False\n      self.apply_layer_norm = False\n    else:\n      raise ValueError(""Wrong normalization type: {}"".format(normalization_type))\n\n    with tf.variable_scope(var_scope_name):\n      if init_var is None:\n        V_std = math.sqrt(dropout * 1.0 / in_dim)\n      else:\n        V_std = init_var\n\n      if self.wn_enabled:\n        V_initializer = \\\n          tf.random_normal_initializer(mean=0, stddev=V_std)\n        self.V = tf.get_variable(\n            \'V\',\n            shape=[in_dim, out_dim],\n            initializer=V_initializer,\n            trainable=True)\n        self.V_norm = tf.norm(self.V.initialized_value(), axis=0)\n        self.g = tf.get_variable(\'g\', initializer=self.V_norm, trainable=True)\n      else:\n        self.V = tf.get_variable(\n            \'W\',\n            shape=[in_dim, out_dim],\n            initializer=tf.random_normal_initializer(mean=0, stddev=V_std),\n            trainable=True, regularizer=self.regularizer)\n\n      if self.bias_enabled:\n        self.b = tf.get_variable(\n            \'b\',\n            shape=[out_dim],\n            initializer=tf.zeros_initializer(),\n            trainable=True)\n      else:\n        self.b = None\n\n      if self.apply_layer_norm:\n        self.layer_norm = LayerNormalization(out_dim)\n      else:\n        self.layer_norm = None\n\n\n  def call(self, x):\n    """"""Projects x with its linear transformation.\n\n    Args:\n      x: A float32 tensor with shape [batch_size, length, in_dim]\n\n    Returns:\n      float32 tensor with shape [batch_size, length, out_dim].\n    """"""\n    batch_size = tf.shape(x)[0]\n\n    x = tf.reshape(x, [-1, self.in_dim])\n    y = tf.matmul(x, self.V)\n    y = tf.reshape(y, [batch_size, -1, self.out_dim])\n\n    if self.wn_enabled:\n      # x*(v*(g/2-norm(v)))\n      scaler = tf.div(self.g, tf.norm(self.V, axis=0))\n      output = tf.reshape(scaler, [1, self.out_dim]) * y\n\n    elif self.apply_batch_norm:\n      bn_input = tf.expand_dims(y, axis=1)\n      bn_output = tf.layers.batch_normalization(\n          name=self.var_scope_name + ""_batch_norm"",\n          inputs=bn_input,\n          training=self.mode == \'train\',\n          axis=-1,\n          momentum=0.95,\n          epsilon=1e-4\n      )\n      output = tf.squeeze(bn_output, axis=1)\n\n    elif self.apply_layer_norm:\n      output = self.layer_norm(y)\n    else:\n      output = y\n\n    if self.b is not None:\n      output = output + tf.reshape(self.b, [1, self.out_dim])\n\n    return output\n'"
open_seq2seq/parts/convs2s/utils.py,2,"b'""""""Implementation of a 1d convolutional layer with weight normalization.\nInspired from https://github.com/tobyyouup/conv_seq2seq""""""\n\nimport tensorflow as tf\n\n\ndef gated_linear_units(inputs):\n  """"""Gated Linear Units (GLU) on x.\n\n  Args:\n    x: A float32 tensor with shape [batch_size, length, 2*out_dim]\n  Returns:\n    float32 tensor with shape [batch_size, length, out_dim].\n  """"""\n  input_shape = inputs.get_shape().as_list()\n  assert len(input_shape) == 3\n  input_pass = inputs[:, :, 0:int(input_shape[2] / 2)]\n  input_gate = inputs[:, :, int(input_shape[2] / 2):]\n  input_gate = tf.sigmoid(input_gate)\n  return tf.multiply(input_pass, input_gate)\n'"
open_seq2seq/parts/rnns/__init__.py,0,b''
open_seq2seq/parts/rnns/attention_wrapper.py,20,"b'# pylint: skip-file\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A powerful dynamic attention wrapper object.\n\nModified by blisc to add support for LocationSensitiveAttention and changed\nthe AttentionWrapper class to output both the cell_output and attention context\nconcatenated together.\n\nNew classes:\n  LocationSensitiveAttention\n  LocationLayer\n\nNew functions:\n  _bahdanau_score_with_location\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nimport collections\nimport functools\nimport math\n\nimport numpy as np\n\nfrom tensorflow.contrib.framework.python.framework import tensor_util\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.layers import base as layers_base\nfrom tensorflow.python.layers import core as layers_core\nfrom tensorflow.python.layers.convolutional import Conv1D\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import functional_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.util import nest\n\n__all__ = [\n    ""AttentionMechanism"", ""AttentionWrapper"", ""AttentionWrapperState"",\n    ""LuongAttention"", ""BahdanauAttention"", ""hardmax"", ""safe_cumprod"",\n    ""monotonic_attention"", ""BahdanauMonotonicAttention"",\n    ""LuongMonotonicAttention"", ""LocationSensitiveAttention""\n]\n\n_zero_state_tensors = rnn_cell_impl._zero_state_tensors  # pylint: disable=protected-access\n\n\nclass AttentionMechanism(object):\n\n  @property\n  def alignments_size(self):\n    raise NotImplementedError\n\n  @property\n  def state_size(self):\n    raise NotImplementedError\n\n\ndef _prepare_memory(memory, memory_sequence_length, check_inner_dims_defined):\n  """"""Convert to tensor and possibly mask `memory`.\n\n  Args:\n    memory: `Tensor`, shaped `[batch_size, max_time, ...]`.\n    memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`.\n    check_inner_dims_defined: Python boolean.  If `True`, the `memory`\n      argument\'s shape is checked to ensure all but the two outermost\n      dimensions are fully defined.\n\n  Returns:\n    A (possibly masked), checked, new `memory`.\n\n  Raises:\n    ValueError: If `check_inner_dims_defined` is `True` and not\n      `memory.shape[2:].is_fully_defined()`.\n  """"""\n  memory = nest.map_structure(\n      lambda m: ops.convert_to_tensor(m, name=""memory""), memory\n  )\n  if memory_sequence_length is not None:\n    memory_sequence_length = ops.convert_to_tensor(\n        memory_sequence_length, name=""memory_sequence_length""\n    )\n  if check_inner_dims_defined:\n\n    def _check_dims(m):\n      if not m.get_shape()[2:].is_fully_defined():\n        raise ValueError(\n            ""Expected memory %s to have fully defined inner dims, ""\n            ""but saw shape: %s"" % (m.name, m.get_shape())\n        )\n\n    nest.map_structure(_check_dims, memory)\n  if memory_sequence_length is None:\n    seq_len_mask = None\n  else:\n    seq_len_mask = array_ops.sequence_mask(\n        memory_sequence_length,\n        maxlen=array_ops.shape(nest.flatten(memory)[0])[1],\n        dtype=nest.flatten(memory)[0].dtype\n    )\n    seq_len_batch_size = (\n        memory_sequence_length.shape[0].value or\n        array_ops.shape(memory_sequence_length)[0]\n    )\n\n  def _maybe_mask(m, seq_len_mask):\n    rank = m.get_shape().ndims\n    rank = rank if rank is not None else array_ops.rank(m)\n    extra_ones = array_ops.ones(rank - 2, dtype=dtypes.int32)\n    m_batch_size = m.shape[0].value or array_ops.shape(m)[0]\n    if memory_sequence_length is not None:\n      message = (\n          ""memory_sequence_length and memory tensor batch sizes do not ""\n          ""match.""\n      )\n      with ops.control_dependencies(\n          [\n              check_ops.assert_equal(\n                  seq_len_batch_size, m_batch_size, message=message\n              )\n          ]\n      ):\n        seq_len_mask = array_ops.reshape(\n            seq_len_mask,\n            array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0)\n        )\n        return m * seq_len_mask\n    else:\n      return m\n\n  return nest.map_structure(lambda m: _maybe_mask(m, seq_len_mask), memory)\n\n\ndef _maybe_mask_score(score, memory_sequence_length, score_mask_value):\n  if memory_sequence_length is None:\n    return score\n  message = (""All values in memory_sequence_length must greater than zero."")\n  with ops.control_dependencies(\n      [check_ops.assert_positive(memory_sequence_length, message=message)]\n  ):\n    score_mask = array_ops.sequence_mask(\n        memory_sequence_length, maxlen=array_ops.shape(score)[1]\n    )\n    score_mask_values = score_mask_value * array_ops.ones_like(score)\n    return array_ops.where(score_mask, score, score_mask_values)\n\n\nclass _BaseAttentionMechanism(AttentionMechanism):\n  """"""A base AttentionMechanism class providing common functionality.\n\n  Common functionality includes:\n    1. Storing the query and memory layers.\n    2. Preprocessing and storing the memory.\n  """"""\n\n  def __init__(\n      self,\n      query_layer,\n      memory,\n      probability_fn,\n      memory_sequence_length=None,\n      memory_layer=None,\n      check_inner_dims_defined=True,\n      score_mask_value=None,\n      name=None\n  ):\n    """"""Construct base AttentionMechanism class.\n\n    Args:\n      query_layer: Callable.  Instance of `tf.layers.Layer`.  The layer\'s depth\n        must match the depth of `memory_layer`.  If `query_layer` is not\n        provided, the shape of `query` must match that of `memory_layer`.\n      memory: The memory to query; usually the output of an RNN encoder.  This\n        tensor should be shaped `[batch_size, max_time, ...]`.\n      probability_fn: A `callable`.  Converts the score and previous alignments\n        to probabilities. Its signature should be:\n        `probabilities = probability_fn(score, state)`.\n      memory_sequence_length (optional): Sequence lengths for the batch entries\n        in memory.  If provided, the memory tensor rows are masked with zeros\n        for values past the respective sequence lengths.\n      memory_layer: Instance of `tf.layers.Layer` (may be None).  The layer\'s\n        depth must match the depth of `query_layer`.\n        If `memory_layer` is not provided, the shape of `memory` must match\n        that of `query_layer`.\n      check_inner_dims_defined: Python boolean.  If `True`, the `memory`\n        argument\'s shape is checked to ensure all but the two outermost\n        dimensions are fully defined.\n      score_mask_value: (optional): The mask value for score before passing into\n        `probability_fn`. The default is -inf. Only used if\n        `memory_sequence_length` is not None.\n      name: Name to use when creating ops.\n    """"""\n    if (\n        query_layer is not None and\n        not isinstance(query_layer, layers_base.Layer)\n    ):\n      raise TypeError(\n          ""query_layer is not a Layer: %s"" % type(query_layer).__name__\n      )\n    if (\n        memory_layer is not None and\n        not isinstance(memory_layer, layers_base.Layer)\n    ):\n      raise TypeError(\n          ""memory_layer is not a Layer: %s"" % type(memory_layer).__name__\n      )\n    self._query_layer = query_layer\n    self._memory_layer = memory_layer\n    self.dtype = memory_layer.dtype\n    if not callable(probability_fn):\n      raise TypeError(\n          ""probability_fn must be callable, saw type: %s"" %\n          type(probability_fn).__name__\n      )\n    if score_mask_value is None:\n      score_mask_value = dtypes.as_dtype(self._memory_layer.dtype\n                                         ).as_numpy_dtype(-np.inf)\n    self._probability_fn = lambda score, prev: (  # pylint:disable=g-long-lambda\n        probability_fn(\n            _maybe_mask_score(score, memory_sequence_length, score_mask_value),\n            prev))\n    with ops.name_scope(\n        name, ""BaseAttentionMechanismInit"", nest.flatten(memory)\n    ):\n      self._values = _prepare_memory(\n          memory,\n          memory_sequence_length,\n          check_inner_dims_defined=check_inner_dims_defined\n      )\n      self._keys = (\n          self.memory_layer(self._values) if self.memory_layer  # pylint: disable=not-callable\n          else self._values\n      )\n      self._batch_size = (\n          self._keys.shape[0].value or array_ops.shape(self._keys)[0]\n      )\n      self._alignments_size = (\n          self._keys.shape[1].value or array_ops.shape(self._keys)[1]\n      )\n\n  @property\n  def memory_layer(self):\n    return self._memory_layer\n\n  @property\n  def query_layer(self):\n    return self._query_layer\n\n  @property\n  def values(self):\n    return self._values\n\n  @property\n  def keys(self):\n    return self._keys\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def alignments_size(self):\n    return self._alignments_size\n\n  @property\n  def state_size(self):\n    return self._alignments_size\n\n  def initial_alignments(self, batch_size, dtype):\n    """"""Creates the initial alignment values for the `AttentionWrapper` class.\n\n    This is important for AttentionMechanisms that use the previous alignment\n    to calculate the alignment at the next time step (e.g. monotonic attention).\n\n    The default behavior is to return a tensor of all zeros.\n\n    Args:\n      batch_size: `int32` scalar, the batch_size.\n      dtype: The `dtype`.\n\n    Returns:\n      A `dtype` tensor shaped `[batch_size, alignments_size]`\n      (`alignments_size` is the values\' `max_time`).\n    """"""\n    max_time = self._alignments_size\n    return _zero_state_tensors(max_time, batch_size, dtype)\n\n  def initial_state(self, batch_size, dtype):\n    """"""Creates the initial state values for the `AttentionWrapper` class.\n\n    This is important for AttentionMechanisms that use the previous alignment\n    to calculate the alignment at the next time step (e.g. monotonic attention).\n\n    The default behavior is to return the same output as initial_alignments.\n\n    Args:\n      batch_size: `int32` scalar, the batch_size.\n      dtype: The `dtype`.\n\n    Returns:\n      A structure of all-zero tensors with shapes as described by `state_size`.\n    """"""\n    return self.initial_alignments(batch_size, dtype)\n\n\ndef _luong_score(query, keys, scale):\n  """"""Implements Luong-style (multiplicative) scoring function.\n\n  This attention has two forms.  The first is standard Luong attention,\n  as described in:\n\n  Minh-Thang Luong, Hieu Pham, Christopher D. Manning.\n  ""Effective Approaches to Attention-based Neural Machine Translation.""\n  EMNLP 2015.  https://arxiv.org/abs/1508.04025\n\n  The second is the scaled form inspired partly by the normalized form of\n  Bahdanau attention.\n\n  To enable the second form, call this function with `scale=True`.\n\n  Args:\n    query: Tensor, shape `[batch_size, num_units]` to compare to keys.\n    keys: Processed memory, shape `[batch_size, max_time, num_units]`.\n    scale: Whether to apply a scale to the score function.\n\n  Returns:\n    A `[batch_size, max_time]` tensor of unnormalized score values.\n\n  Raises:\n    ValueError: If `key` and `query` depths do not match.\n  """"""\n  depth = query.get_shape()[-1]\n  key_units = keys.get_shape()[-1]\n  if depth != key_units:\n    raise ValueError(\n        ""Incompatible or unknown inner dimensions between query and keys.  ""\n        ""Query (%s) has units: %s.  Keys (%s) have units: %s.  ""\n        ""Perhaps you need to set num_units to the keys\' dimension (%s)?"" %\n        (query, depth, keys, key_units, key_units)\n    )\n  dtype = query.dtype\n\n  # Reshape from [batch_size, depth] to [batch_size, 1, depth]\n  # for matmul.\n  query = array_ops.expand_dims(query, 1)\n\n  # Inner product along the query units dimension.\n  # matmul shapes: query is [batch_size, 1, depth] and\n  #                keys is [batch_size, max_time, depth].\n  # the inner product is asked to **transpose keys\' inner shape** to get a\n  # batched matmul on:\n  #   [batch_size, 1, depth] . [batch_size, depth, max_time]\n  # resulting in an output shape of:\n  #   [batch_size, 1, max_time].\n  # we then squeeze out the center singleton dimension.\n  score = math_ops.matmul(query, keys, transpose_b=True)\n  score = array_ops.squeeze(score, [1])\n\n  if scale:\n    # Scalar used in weight scaling\n    g = variable_scope.get_variable(""attention_g"", dtype=dtype, initializer=1.)\n    score = g * score\n  return score\n\n\nclass LuongAttention(_BaseAttentionMechanism):\n  """"""Implements Luong-style (multiplicative) attention scoring.\n\n  This attention has two forms.  The first is standard Luong attention,\n  as described in:\n\n  Minh-Thang Luong, Hieu Pham, Christopher D. Manning.\n  ""Effective Approaches to Attention-based Neural Machine Translation.""\n  EMNLP 2015.  https://arxiv.org/abs/1508.04025\n\n  The second is the scaled form inspired partly by the normalized form of\n  Bahdanau attention.\n\n  To enable the second form, construct the object with parameter\n  `scale=True`.\n  """"""\n\n  def __init__(\n      self,\n      num_units,\n      memory,\n      memory_sequence_length=None,\n      scale=False,\n      probability_fn=None,\n      score_mask_value=None,\n      dtype=None,\n      name=""LuongAttention""\n  ):\n    """"""Construct the AttentionMechanism mechanism.\n\n    Args:\n      num_units: The depth of the attention mechanism.\n      memory: The memory to query; usually the output of an RNN encoder.  This\n        tensor should be shaped `[batch_size, max_time, ...]`.\n      memory_sequence_length: (optional) Sequence lengths for the batch entries\n        in memory.  If provided, the memory tensor rows are masked with zeros\n        for values past the respective sequence lengths.\n      scale: Python boolean.  Whether to scale the energy term.\n      probability_fn: (optional) A `callable`.  Converts the score to\n        probabilities.  The default is @{tf.nn.softmax}. Other options include\n        @{tf.contrib.seq2seq.hardmax} and @{tf.contrib.sparsemax.sparsemax}.\n        Its signature should be: `probabilities = probability_fn(score)`.\n      score_mask_value: (optional) The mask value for score before passing into\n        `probability_fn`. The default is -inf. Only used if\n        `memory_sequence_length` is not None.\n      dtype: The data type for the memory layer of the attention mechanism.\n      name: Name to use when creating ops.\n    """"""\n    # For LuongAttention, we only transform the memory layer; thus\n    # num_units **must** match expected the query depth.\n    if probability_fn is None:\n      probability_fn = nn_ops.softmax\n    if dtype is None:\n      dtype = dtypes.float32\n    wrapped_probability_fn = lambda score, _: probability_fn(score)\n    super(LuongAttention, self).__init__(\n        query_layer=None,\n        memory_layer=layers_core.Dense(\n            num_units, name=""memory_layer"", use_bias=False, dtype=dtype\n        ),\n        memory=memory,\n        probability_fn=wrapped_probability_fn,\n        memory_sequence_length=memory_sequence_length,\n        score_mask_value=score_mask_value,\n        name=name\n    )\n    self._num_units = num_units\n    self._scale = scale\n    self._name = name\n\n  def __call__(self, query, state):\n    """"""Score the query based on the keys and values.\n\n    Args:\n      query: Tensor of dtype matching `self.values` and shape\n        `[batch_size, query_depth]`.\n      state: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]`\n        (`alignments_size` is memory\'s `max_time`).\n\n    Returns:\n      alignments: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n        `max_time`).\n    """"""\n    with variable_scope.variable_scope(None, ""luong_attention"", [query]):\n      score = _luong_score(query, self._keys, self._scale)\n    alignments = self._probability_fn(score, state)\n    next_state = alignments\n    return alignments, next_state\n\n\ndef _bahdanau_score(processed_query, keys, normalize):\n  """"""Implements Bahdanau-style (additive) scoring function.\n\n  This attention has two forms.  The first is Bhandanau attention,\n  as described in:\n\n  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.\n  ""Neural Machine Translation by Jointly Learning to Align and Translate.""\n  ICLR 2015. https://arxiv.org/abs/1409.0473\n\n  The second is the normalized form.  This form is inspired by the\n  weight normalization article:\n\n  Tim Salimans, Diederik P. Kingma.\n  ""Weight Normalization: A Simple Reparameterization to Accelerate\n   Training of Deep Neural Networks.""\n  https://arxiv.org/abs/1602.07868\n\n  To enable the second form, set `normalize=True`.\n\n  Args:\n    processed_query: Tensor, shape `[batch_size, num_units]` to compare to keys.\n    keys: Processed memory, shape `[batch_size, max_time, num_units]`.\n    normalize: Whether to normalize the score function.\n\n  Returns:\n    A `[batch_size, max_time]` tensor of unnormalized score values.\n  """"""\n  dtype = processed_query.dtype\n  # Get the number of hidden units from the trailing dimension of keys\n  num_units = keys.shape[2].value or array_ops.shape(keys)[2]\n  # Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting.\n  processed_query = array_ops.expand_dims(processed_query, 1)\n  v = variable_scope.get_variable(""attention_v"", [num_units], dtype=dtype)\n  if normalize:\n    # Scalar used in weight normalization\n    g = variable_scope.get_variable(\n        ""attention_g"",\n        dtype=dtype,\n        shape=[1],\n        # initializer=math.sqrt((1. / num_units)))\n        initializer=init_ops.constant_initializer(\n            math.sqrt(1. / num_units), dtype=dtype\n        )\n    )\n    # Bias added prior to the nonlinearity\n    b = variable_scope.get_variable(\n        ""attention_b"", [num_units],\n        dtype=dtype,\n        initializer=init_ops.zeros_initializer()\n    )\n    # normed_v = g * v / ||v||\n    normed_v = g * v * math_ops.rsqrt(math_ops.reduce_sum(math_ops.square(v)))\n    return math_ops.reduce_sum(\n        normed_v * math_ops.tanh(keys + processed_query + b), [2]\n    )\n  else:\n    return math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query), [2])\n\n\nclass BahdanauAttention(_BaseAttentionMechanism):\n  """"""Implements Bahdanau-style (additive) attention.\n\n  This attention has two forms.  The first is Bahdanau attention,\n  as described in:\n\n  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.\n  ""Neural Machine Translation by Jointly Learning to Align and Translate.""\n  ICLR 2015. https://arxiv.org/abs/1409.0473\n\n  The second is the normalized form.  This form is inspired by the\n  weight normalization article:\n\n  Tim Salimans, Diederik P. Kingma.\n  ""Weight Normalization: A Simple Reparameterization to Accelerate\n   Training of Deep Neural Networks.""\n  https://arxiv.org/abs/1602.07868\n\n  To enable the second form, construct the object with parameter\n  `normalize=True`.\n  """"""\n\n  def __init__(\n      self,\n      num_units,\n      memory,\n      memory_sequence_length=None,\n      normalize=False,\n      probability_fn=None,\n      score_mask_value=None,\n      dtype=None,\n      name=""BahdanauAttention""\n  ):\n    """"""Construct the Attention mechanism.\n\n    Args:\n      num_units: The depth of the query mechanism.\n      memory: The memory to query; usually the output of an RNN encoder.  This\n        tensor should be shaped `[batch_size, max_time, ...]`.\n      memory_sequence_length (optional): Sequence lengths for the batch entries\n        in memory.  If provided, the memory tensor rows are masked with zeros\n        for values past the respective sequence lengths.\n      normalize: Python boolean.  Whether to normalize the energy term.\n      probability_fn: (optional) A `callable`.  Converts the score to\n        probabilities.  The default is @{tf.nn.softmax}. Other options include\n        @{tf.contrib.seq2seq.hardmax} and @{tf.contrib.sparsemax.sparsemax}.\n        Its signature should be: `probabilities = probability_fn(score)`.\n      score_mask_value: (optional): The mask value for score before passing into\n        `probability_fn`. The default is -inf. Only used if\n        `memory_sequence_length` is not None.\n      dtype: The data type for the query and memory layers of the attention\n        mechanism.\n      name: Name to use when creating ops.\n    """"""\n    if probability_fn is None:\n      probability_fn = nn_ops.softmax\n    if dtype is None:\n      dtype = dtypes.float32\n    wrapped_probability_fn = lambda score, _: probability_fn(score)\n    super(BahdanauAttention, self).__init__(\n        query_layer=layers_core.Dense(\n            num_units, name=""query_layer"", use_bias=False, dtype=dtype\n        ),\n        memory_layer=layers_core.Dense(\n            num_units, name=""memory_layer"", use_bias=False, dtype=dtype\n        ),\n        memory=memory,\n        probability_fn=wrapped_probability_fn,\n        memory_sequence_length=memory_sequence_length,\n        score_mask_value=score_mask_value,\n        name=name\n    )\n    self._num_units = num_units\n    self._normalize = normalize\n    self._name = name\n\n  def __call__(self, query, state):\n    """"""Score the query based on the keys and values.\n\n    Args:\n      query: Tensor of dtype matching `self.values` and shape\n        `[batch_size, query_depth]`.\n      state: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]`\n        (`alignments_size` is memory\'s `max_time`).\n\n    Returns:\n      alignments: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n        `max_time`).\n    """"""\n    with variable_scope.variable_scope(None, ""bahdanau_attention"", [query]):\n      processed_query = self.query_layer(query) if self.query_layer else query\n      score = _bahdanau_score(processed_query, self._keys, self._normalize)\n    alignments = self._probability_fn(score, state)\n    next_state = alignments\n    return alignments, next_state\n\n\ndef _bahdanau_score_with_location(processed_query, keys, location, use_bias):\n  """"""Implements Bahdanau-style (additive) scoring function with location\n  information.\n\n  The implementation is described in\n\n  Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, Yoshua Bengio\n  ""Attention-Based Models for Speech Recognition""\n  https://arxiv.org/abs/1506.07503\n\n  Args:\n    processed_query: Tensor, shape `[batch_size, num_units]` to compare to keys.\n    keys: Processed memory, shape `[batch_size, max_time, num_units]`.\n    location: Tensor, shape `[batch_size, max_time, num_units]`\n    use_bias (bool): Whether to use a bias when computing alignments\n\n  Returns:\n    A `[batch_size, max_time]` tensor of unnormalized score values.\n  """"""\n  dtype = processed_query.dtype\n  # Get the number of hidden units from the trailing dimension of keys\n  num_units = keys.shape[2].value or array_ops.shape(keys)[2]\n  # Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting.\n  processed_query = array_ops.expand_dims(processed_query, 1)\n  v = variable_scope.get_variable(""attention_v"", [num_units], dtype=dtype)\n  if use_bias:\n    b = variable_scope.get_variable(""attention_bias"", [num_units], dtype=dtype)\n    return math_ops.reduce_sum(\n        v * math_ops.tanh(keys + processed_query + location + b), [2]\n    )\n  return math_ops.reduce_sum(\n      v * math_ops.tanh(keys + processed_query + location), [2]\n  )\n\n\nclass ChorowskiLocationLayer(layers_base.Layer):\n  """"""\n  The layer that processed the location information\n  """"""\n\n  def __init__(\n      self,\n      filters,\n      kernel_size,\n      attention_units,\n      strides=1,\n      data_format=""channels_last"",\n      name=""location"",\n      dtype=None,\n      **kwargs\n  ):\n    super(ChorowskiLocationLayer, self).__init__(name=name, **kwargs)\n    self.conv_layer = Conv1D(\n        name=""{}_conv"".format(name),\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=""SAME"",\n        use_bias=True,\n        data_format=data_format,\n    )\n    self.location_dense = Conv1D(\n        name=""{}_dense"".format(name),\n        filters=attention_units,\n        kernel_size=1,\n        strides=strides,\n        padding=""SAME"",\n        use_bias=False,\n        data_format=data_format,\n    )\n\n  def __call__(self, prev_attention, query=None):\n    location_attention = self.conv_layer(prev_attention)\n    location_attention = self.location_dense(location_attention)\n    return location_attention\n\n\nclass ZhaopengLocationLayer(layers_base.Layer):\n  """"""\n  The layer that processed the location information. \n  Similar to https://arxiv.org/abs/1805.03294 and https://arxiv.org/abs/1601.04811.\n  """"""\n\n  def __init__(\n      self,\n      attention_units,\n      query_dim,\n      name=""location"",\n      dtype=None,\n      **kwargs\n  ):\n    super(ZhaopengLocationLayer, self).__init__(name=name, **kwargs)\n    self.vbeta = variable_scope.get_variable(\n        ""location_attention_vbeta"", [query_dim], dtype=dtypes.float32)\n    self.location_dense = layers_core.Dense(\n        name=""{}_dense"".format(name), units=attention_units, use_bias=False\n    )\n\n  def __call__(self, prev_attention, query):\n    # To-Do add mixed precision support.\n    #query = math_ops.cast(query, dtypes.float32)\n    fertility = math_ops.sigmoid(math_ops.reduce_sum(\n        math_ops.multiply(self.vbeta, query)))\n    location_attention = fertility * prev_attention\n    location_attention = self.location_dense(location_attention)\n    return location_attention\n\n\nclass LocationSensitiveAttention(_BaseAttentionMechanism):\n  """"""Implements Bahdanau-style (additive) scoring function with cumulative\n  location information.\n\n  The implementation is described in:\n\n  Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, Yoshua Bengio\n  ""Attention-Based Models for Speech Recognition""\n  https://arxiv.org/abs/1506.07503\n\n  Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly,\n  Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan,\n  Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu\n  ""Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions""\n  https://arxiv.org/abs/1712.05884\n  """"""\n\n  def __init__(\n      self,\n      num_units,\n      memory,\n      query_dim=None,\n      memory_sequence_length=None,\n      probability_fn=None,\n      score_mask_value=None,\n      dtype=None,\n      use_bias=False,\n      use_coverage=True,\n      location_attn_type=""chorowski"",\n      location_attention_params=None,\n      name=""LocationSensitiveAttention"",\n  ):\n    """"""Construct the Attention mechanism.\n\n    Args:\n      num_units: The depth of the query mechanism.\n      memory: The memory to query; usually the output of an RNN encoder.  This\n        tensor should be shaped `[batch_size, max_time, ...]`.\n      memory_sequence_length (optional): Sequence lengths for the batch entries\n        in memory.  If provided, the memory tensor rows are masked with zeros\n        for values past the respective sequence lengths.\n      normalize: Python boolean.  Whether to normalize the energy term.\n      probability_fn: (optional) A `callable`.  Converts the score to\n        probabilities.  The default is @{tf.nn.softmax}. Other options include\n        @{tf.contrib.seq2seq.hardmax} and @{tf.contrib.sparsemax.sparsemax}.\n        Its signature should be: `probabilities = probability_fn(score)`.\n      score_mask_value: (optional): The mask value for score before passing into\n        `probability_fn`. The default is -inf. Only used if\n        `memory_sequence_length` is not None.\n      dtype: The data type for the query and memory layers of the attention\n        mechanism.\n      use_bias (bool): Whether to use a bias when computing alignments.\n      location_attn_type (String): Accepts [""chorowski"", ""zhaopeng""].\n      location_attention_params (dict): Params required for location attention.\n      name: Name to use when creating ops.\n    """"""\n    if probability_fn is None:\n      probability_fn = nn_ops.softmax\n    if dtype is None:\n      dtype = dtypes.float32\n    wrapped_probability_fn = lambda score, _: probability_fn(score)\n    super(LocationSensitiveAttention, self).__init__(\n        query_layer=layers_core.Dense(\n            num_units, name=""query_layer"", use_bias=False, dtype=dtype\n        ),\n        memory_layer = Conv1D(\n            name=""memory_layer"".format(name),\n            filters=num_units,\n            kernel_size=1,\n            strides=1,\n            padding=""SAME"",\n            use_bias=False,\n            data_format=""channels_last"",\n            dtype=dtype\n        ),\n        memory=memory,\n        probability_fn=wrapped_probability_fn,\n        memory_sequence_length=memory_sequence_length,\n        score_mask_value=score_mask_value,\n        name=name\n    )\n\n    self._num_units = num_units\n    self._name = name\n    self.use_bias = use_bias\n    self._use_coverage = use_coverage\n\n    if location_attn_type == ""chorowski"":\n      kernel_size = 32\n      filters = 32\n      if location_attention_params is not None:\n        kernel_size = location_attention_params[""kernel_size""]\n        filters = location_attention_params[""filters""]\n\n      self.location_layer = ChorowskiLocationLayer(\n          filters, kernel_size, num_units)\n    elif location_attn_type == ""zhaopeng"":\n      self.location_layer = ZhaopengLocationLayer(num_units, query_dim)\n      self._use_coverage = True\n\n  def __call__(self, query, state):\n    """"""Score the query based on the keys, values, and location.\n\n    Args:\n      query: Tensor of dtype matching `self.values` and shape\n        `[batch_size, query_depth]`.\n      state: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]`\n        (`alignments_size` is memory\'s `max_time`).\n\n    Returns:\n      alignments: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n        `max_time`).\n    """"""\n    with variable_scope.variable_scope(None, ""location_attention"", [query]):\n      processed_query = self.query_layer(query) if self.query_layer else query\n      location = array_ops.expand_dims(state, axis=-1)\n      processed_location = self.location_layer(location, query)\n      score = _bahdanau_score_with_location(\n          processed_query, self._keys, processed_location, self.use_bias\n      )\n    alignments = self._probability_fn(score, state)\n\n    if self._use_coverage:\n      next_state = alignments + state\n    else:\n      next_state = alignments\n\n    return alignments, next_state\n\n\ndef safe_cumprod(x, *args, **kwargs):\n  """"""Computes cumprod of x in logspace using cumsum to avoid underflow.\n\n  The cumprod function and its gradient can result in numerical instabilities\n  when its argument has very small and/or zero values.  As long as the argument\n  is all positive, we can instead compute the cumulative product as\n  exp(cumsum(log(x))).  This function can be called identically to tf.cumprod.\n\n  Args:\n    x: Tensor to take the cumulative product of.\n    *args: Passed on to cumsum; these are identical to those in cumprod.\n    **kwargs: Passed on to cumsum; these are identical to those in cumprod.\n  Returns:\n    Cumulative product of x.\n  """"""\n  with ops.name_scope(None, ""SafeCumprod"", [x]):\n    x = ops.convert_to_tensor(x, name=""x"")\n    tiny = np.finfo(x.dtype.as_numpy_dtype).tiny\n    return math_ops.exp(\n        math_ops.cumsum(\n            math_ops.log(clip_ops.clip_by_value(x, tiny, 1)), *args, **kwargs\n        )\n    )\n\n\ndef monotonic_attention(p_choose_i, previous_attention, mode):\n  """"""Compute monotonic attention distribution from choosing probabilities.\n\n  Monotonic attention implies that the input sequence is processed in an\n  explicitly left-to-right manner when generating the output sequence.  In\n  addition, once an input sequence element is attended to at a given output\n  timestep, elements occurring before it cannot be attended to at subsequent\n  output timesteps.  This function generates attention distributions according\n  to these assumptions.  For more information, see ``Online and Linear-Time\n  Attention by Enforcing Monotonic Alignments\'\'.\n\n  Args:\n    p_choose_i: Probability of choosing input sequence/memory element i.  Should\n      be of shape (batch_size, input_sequence_length), and should all be in the\n      range [0, 1].\n    previous_attention: The attention distribution from the previous output\n      timestep.  Should be of shape (batch_size, input_sequence_length).  For\n      the first output timestep, preevious_attention[n] should be [1, 0, 0, ...,\n      0] for all n in [0, ... batch_size - 1].\n    mode: How to compute the attention distribution.  Must be one of\n      \'recursive\', \'parallel\', or \'hard\'.\n        * \'recursive\' uses tf.scan to recursively compute the distribution.\n          This is slowest but is exact, general, and does not suffer from\n          numerical instabilities.\n        * \'parallel\' uses parallelized cumulative-sum and cumulative-product\n          operations to compute a closed-form solution to the recurrence\n          relation defining the attention distribution.  This makes it more\n          efficient than \'recursive\', but it requires numerical checks which\n          make the distribution non-exact.  This can be a problem in particular\n          when input_sequence_length is long and/or p_choose_i has entries very\n          close to 0 or 1.\n        * \'hard\' requires that the probabilities in p_choose_i are all either 0\n          or 1, and subsequently uses a more efficient and exact solution.\n\n  Returns:\n    A tensor of shape (batch_size, input_sequence_length) representing the\n    attention distributions for each sequence in the batch.\n\n  Raises:\n    ValueError: mode is not one of \'recursive\', \'parallel\', \'hard\'.\n  """"""\n  # Force things to be tensors\n  p_choose_i = ops.convert_to_tensor(p_choose_i, name=""p_choose_i"")\n  previous_attention = ops.convert_to_tensor(\n      previous_attention, name=""previous_attention""\n  )\n  if mode == ""recursive"":\n    # Use .shape[0].value when it\'s not None, or fall back on symbolic shape\n    batch_size = p_choose_i.shape[0].value or array_ops.shape(p_choose_i)[0]\n    # Compute [1, 1 - p_choose_i[0], 1 - p_choose_i[1], ..., 1 - p_choose_i[-2]]\n    shifted_1mp_choose_i = array_ops.concat(\n        [array_ops.ones((batch_size, 1)), 1 - p_choose_i[:, :-1]], 1\n    )\n    # Compute attention distribution recursively as\n    # q[i] = (1 - p_choose_i[i])*q[i - 1] + previous_attention[i]\n    # attention[i] = p_choose_i[i]*q[i]\n    attention = p_choose_i * array_ops.transpose(\n        functional_ops.scan(\n            # Need to use reshape to remind TF of the shape between loop\n            # iterations\n            lambda x, yz: array_ops.reshape(yz[0] * x + yz[1], (batch_size,)),\n            # Loop variables yz[0] and yz[1]\n            [\n                array_ops.transpose(shifted_1mp_choose_i),\n                array_ops.transpose(previous_attention)\n            ],\n            # Initial value of x is just zeros\n            array_ops.zeros((batch_size,))\n        )\n    )\n  elif mode == ""parallel"":\n    # safe_cumprod computes cumprod in logspace with numeric checks\n    cumprod_1mp_choose_i = safe_cumprod(1 - p_choose_i, axis=1, exclusive=True)\n    # Compute recurrence relation solution\n    attention = p_choose_i * cumprod_1mp_choose_i * math_ops.cumsum(\n        previous_attention /\n        # Clip cumprod_1mp to avoid divide-by-zero\n        clip_ops.clip_by_value(cumprod_1mp_choose_i, 1e-10, 1.),\n        axis=1\n    )\n  elif mode == ""hard"":\n    # Remove any probabilities before the index chosen last time step\n    p_choose_i *= math_ops.cumsum(previous_attention, axis=1)\n    # Now, use exclusive cumprod to remove probabilities after the first\n    # chosen index, like so:\n    # p_choose_i = [0, 0, 0, 1, 1, 0, 1, 1]\n    # cumprod(1 - p_choose_i, exclusive=True) = [1, 1, 1, 1, 0, 0, 0, 0]\n    # Product of above: [0, 0, 0, 1, 0, 0, 0, 0]\n    attention = p_choose_i * math_ops.cumprod(\n        1 - p_choose_i, axis=1, exclusive=True\n    )\n  else:\n    raise ValueError(""mode must be \'recursive\', \'parallel\', or \'hard\'."")\n  return attention\n\n\ndef _monotonic_probability_fn(\n    score, previous_alignments, sigmoid_noise, mode, seed=None\n):\n  """"""Attention probability function for monotonic attention.\n\n  Takes in unnormalized attention scores, adds pre-sigmoid noise to encourage\n  the model to make discrete attention decisions, passes them through a sigmoid\n  to obtain ""choosing"" probabilities, and then calls monotonic_attention to\n  obtain the attention distribution.  For more information, see\n\n  Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n  ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n  ICML 2017.  https://arxiv.org/abs/1704.00784\n\n  Args:\n    score: Unnormalized attention scores, shape `[batch_size, alignments_size]`\n    previous_alignments: Previous attention distribution, shape\n      `[batch_size, alignments_size]`\n    sigmoid_noise: Standard deviation of pre-sigmoid noise.  Setting this larger\n      than 0 will encourage the model to produce large attention scores,\n      effectively making the choosing probabilities discrete and the resulting\n      attention distribution one-hot.  It should be set to 0 at test-time, and\n      when hard attention is not desired.\n    mode: How to compute the attention distribution.  Must be one of\n      \'recursive\', \'parallel\', or \'hard\'.  See the docstring for\n      `tf.contrib.seq2seq.monotonic_attention` for more information.\n    seed: (optional) Random seed for pre-sigmoid noise.\n\n  Returns:\n    A `[batch_size, alignments_size]`-shape tensor corresponding to the\n    resulting attention distribution.\n  """"""\n  # Optionally add pre-sigmoid noise to the scores\n  if sigmoid_noise > 0:\n    noise = random_ops.random_normal(\n        array_ops.shape(score), dtype=score.dtype, seed=seed\n    )\n    score += sigmoid_noise * noise\n  # Compute ""choosing"" probabilities from the attention scores\n  if mode == ""hard"":\n    # When mode is hard, use a hard sigmoid\n    p_choose_i = math_ops.cast(score > 0, score.dtype)\n  else:\n    p_choose_i = math_ops.sigmoid(score)\n  # Convert from choosing probabilities to attention distribution\n  return monotonic_attention(p_choose_i, previous_alignments, mode)\n\n\nclass _BaseMonotonicAttentionMechanism(_BaseAttentionMechanism):\n  """"""Base attention mechanism for monotonic attention.\n\n  Simply overrides the initial_alignments function to provide a dirac\n  distribution,which is needed in order for the monotonic attention\n  distributions to have the correct behavior.\n  """"""\n\n  def initial_alignments(self, batch_size, dtype):\n    """"""Creates the initial alignment values for the monotonic attentions.\n\n    Initializes to dirac distributions, i.e. [1, 0, 0, ...memory length..., 0]\n    for all entries in the batch.\n\n    Args:\n      batch_size: `int32` scalar, the batch_size.\n      dtype: The `dtype`.\n\n    Returns:\n      A `dtype` tensor shaped `[batch_size, alignments_size]`\n      (`alignments_size` is the values\' `max_time`).\n    """"""\n    max_time = self._alignments_size\n    return array_ops.one_hot(\n        array_ops.zeros((batch_size,), dtype=dtypes.int32),\n        max_time,\n        dtype=dtype\n    )\n\n\nclass BahdanauMonotonicAttention(_BaseMonotonicAttentionMechanism):\n  """"""Monotonic attention mechanism with Bahadanau-style energy function.\n\n  This type of attention encorces a monotonic constraint on the attention\n  distributions; that is once the model attends to a given point in the memory\n  it can\'t attend to any prior points at subsequence output timesteps.  It\n  achieves this by using the _monotonic_probability_fn instead of softmax to\n  construct its attention distributions.  Since the attention scores are passed\n  through a sigmoid, a learnable scalar bias parameter is applied after the\n  score function and before the sigmoid.  Otherwise, it is equivalent to\n  BahdanauAttention.  This approach is proposed in\n\n  Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n  ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n  ICML 2017.  https://arxiv.org/abs/1704.00784\n  """"""\n\n  def __init__(\n      self,\n      num_units,\n      memory,\n      memory_sequence_length=None,\n      normalize=False,\n      score_mask_value=None,\n      sigmoid_noise=0.,\n      sigmoid_noise_seed=None,\n      score_bias_init=0.,\n      mode=""parallel"",\n      dtype=None,\n      name=""BahdanauMonotonicAttention""\n  ):\n    """"""Construct the Attention mechanism.\n\n    Args:\n      num_units: The depth of the query mechanism.\n      memory: The memory to query; usually the output of an RNN encoder.  This\n        tensor should be shaped `[batch_size, max_time, ...]`.\n      memory_sequence_length (optional): Sequence lengths for the batch entries\n        in memory.  If provided, the memory tensor rows are masked with zeros\n        for values past the respective sequence lengths.\n      normalize: Python boolean.  Whether to normalize the energy term.\n      score_mask_value: (optional): The mask value for score before passing into\n        `probability_fn`. The default is -inf. Only used if\n        `memory_sequence_length` is not None.\n      sigmoid_noise: Standard deviation of pre-sigmoid noise.  See the docstring\n        for `_monotonic_probability_fn` for more information.\n      sigmoid_noise_seed: (optional) Random seed for pre-sigmoid noise.\n      score_bias_init: Initial value for score bias scalar.  It\'s recommended to\n        initialize this to a negative value when the length of the memory is\n        large.\n      mode: How to compute the attention distribution.  Must be one of\n        \'recursive\', \'parallel\', or \'hard\'.  See the docstring for\n        `tf.contrib.seq2seq.monotonic_attention` for more information.\n      dtype: The data type for the query and memory layers of the attention\n        mechanism.\n      name: Name to use when creating ops.\n    """"""\n    # Set up the monotonic probability fn with supplied parameters\n    if dtype is None:\n      dtype = dtypes.float32\n    wrapped_probability_fn = functools.partial(\n        _monotonic_probability_fn,\n        sigmoid_noise=sigmoid_noise,\n        mode=mode,\n        seed=sigmoid_noise_seed\n    )\n    super(BahdanauMonotonicAttention, self).__init__(\n        query_layer=layers_core.Dense(\n            num_units, name=""query_layer"", use_bias=False, dtype=dtype\n        ),\n        memory_layer=layers_core.Dense(\n            num_units, name=""memory_layer"", use_bias=False, dtype=dtype\n        ),\n        memory=memory,\n        probability_fn=wrapped_probability_fn,\n        memory_sequence_length=memory_sequence_length,\n        score_mask_value=score_mask_value,\n        name=name\n    )\n    self._num_units = num_units\n    self._normalize = normalize\n    self._name = name\n    self._score_bias_init = score_bias_init\n\n  def __call__(self, query, state):\n    """"""Score the query based on the keys and values.\n\n    Args:\n      query: Tensor of dtype matching `self.values` and shape\n        `[batch_size, query_depth]`.\n      state: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]`\n        (`alignments_size` is memory\'s `max_time`).\n\n    Returns:\n      alignments: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n        `max_time`).\n    """"""\n    with variable_scope.variable_scope(\n        None, ""bahdanau_monotonic_attention"", [query]\n    ):\n      processed_query = self.query_layer(query) if self.query_layer else query\n      score = _bahdanau_score(processed_query, self._keys, self._normalize)\n      score_bias = variable_scope.get_variable(\n          ""attention_score_bias"",\n          dtype=processed_query.dtype,\n          initializer=self._score_bias_init\n      )\n      score += score_bias\n    alignments = self._probability_fn(score, state)\n    next_state = alignments\n    return alignments, next_state\n\n\nclass LuongMonotonicAttention(_BaseMonotonicAttentionMechanism):\n  """"""Monotonic attention mechanism with Luong-style energy function.\n\n  This type of attention encorces a monotonic constraint on the attention\n  distributions; that is once the model attends to a given point in the memory\n  it can\'t attend to any prior points at subsequence output timesteps.  It\n  achieves this by using the _monotonic_probability_fn instead of softmax to\n  construct its attention distributions.  Otherwise, it is equivalent to\n  LuongAttention.  This approach is proposed in\n\n  Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n  ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n  ICML 2017.  https://arxiv.org/abs/1704.00784\n  """"""\n\n  def __init__(\n      self,\n      num_units,\n      memory,\n      memory_sequence_length=None,\n      scale=False,\n      score_mask_value=None,\n      sigmoid_noise=0.,\n      sigmoid_noise_seed=None,\n      score_bias_init=0.,\n      mode=""parallel"",\n      dtype=None,\n      name=""LuongMonotonicAttention""\n  ):\n    """"""Construct the Attention mechanism.\n\n    Args:\n      num_units: The depth of the query mechanism.\n      memory: The memory to query; usually the output of an RNN encoder.  This\n        tensor should be shaped `[batch_size, max_time, ...]`.\n      memory_sequence_length (optional): Sequence lengths for the batch entries\n        in memory.  If provided, the memory tensor rows are masked with zeros\n        for values past the respective sequence lengths.\n      scale: Python boolean.  Whether to scale the energy term.\n      score_mask_value: (optional): The mask value for score before passing into\n        `probability_fn`. The default is -inf. Only used if\n        `memory_sequence_length` is not None.\n      sigmoid_noise: Standard deviation of pre-sigmoid noise.  See the docstring\n        for `_monotonic_probability_fn` for more information.\n      sigmoid_noise_seed: (optional) Random seed for pre-sigmoid noise.\n      score_bias_init: Initial value for score bias scalar.  It\'s recommended to\n        initialize this to a negative value when the length of the memory is\n        large.\n      mode: How to compute the attention distribution.  Must be one of\n        \'recursive\', \'parallel\', or \'hard\'.  See the docstring for\n        `tf.contrib.seq2seq.monotonic_attention` for more information.\n      dtype: The data type for the query and memory layers of the attention\n        mechanism.\n      name: Name to use when creating ops.\n    """"""\n    # Set up the monotonic probability fn with supplied parameters\n    if dtype is None:\n      dtype = dtypes.float32\n    wrapped_probability_fn = functools.partial(\n        _monotonic_probability_fn,\n        sigmoid_noise=sigmoid_noise,\n        mode=mode,\n        seed=sigmoid_noise_seed\n    )\n    super(LuongMonotonicAttention, self).__init__(\n        query_layer=None,\n        memory_layer=layers_core.Dense(\n            num_units, name=""memory_layer"", use_bias=False, dtype=dtype\n        ),\n        memory=memory,\n        probability_fn=wrapped_probability_fn,\n        memory_sequence_length=memory_sequence_length,\n        score_mask_value=score_mask_value,\n        name=name\n    )\n    self._num_units = num_units\n    self._scale = scale\n    self._score_bias_init = score_bias_init\n    self._name = name\n\n  def __call__(self, query, state):\n    """"""Score the query based on the keys and values.\n\n    Args:\n      query: Tensor of dtype matching `self.values` and shape\n        `[batch_size, query_depth]`.\n      state: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]`\n        (`alignments_size` is memory\'s `max_time`).\n\n    Returns:\n      alignments: Tensor of dtype matching `self.values` and shape\n        `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n        `max_time`).\n    """"""\n    with variable_scope.variable_scope(\n        None, ""luong_monotonic_attention"", [query]\n    ):\n      score = _luong_score(query, self._keys, self._scale)\n      score_bias = variable_scope.get_variable(\n          ""attention_score_bias"",\n          dtype=query.dtype,\n          initializer=self._score_bias_init\n      )\n      score += score_bias\n    alignments = self._probability_fn(score, state)\n    next_state = alignments\n    return alignments, next_state\n\n\nclass AttentionWrapperState(\n    collections.namedtuple(\n        ""AttentionWrapperState"", (\n            ""cell_state"", ""attention"", ""time"", ""alignments"",\n            ""alignment_history"", ""attention_state""\n        )\n    )\n):\n  """"""`namedtuple` storing the state of a `AttentionWrapper`.\n\n  Contains:\n\n    - `cell_state`: The state of the wrapped `RNNCell` at the previous time\n      step.\n    - `attention`: The attention emitted at the previous time step.\n    - `time`: int32 scalar containing the current time step.\n    - `alignments`: A single or tuple of `Tensor`(s) containing the alignments\n       emitted at the previous time step for each attention mechanism.\n    - `alignment_history`: (if enabled) a single or tuple of `TensorArray`(s)\n       containing alignment matrices from all time steps for each attention\n       mechanism. Call `stack()` on each to convert to a `Tensor`.\n    - `attention_state`: A single or tuple of nested objects\n       containing attention mechanism state for each attention mechanism.\n       The objects may contain Tensors or TensorArrays.\n  """"""\n\n  def clone(self, **kwargs):\n    """"""Clone this object, overriding components provided by kwargs.\n\n    The new state fields\' shape must match original state fields\' shape. This\n    will be validated, and original fields\' shape will be propagated to new\n    fields.\n\n    Example:\n\n    ```python\n    initial_state = attention_wrapper.zero_state(dtype=..., batch_size=...)\n    initial_state = initial_state.clone(cell_state=encoder_state)\n    ```\n\n    Args:\n      **kwargs: Any properties of the state object to replace in the returned\n        `AttentionWrapperState`.\n\n    Returns:\n      A new `AttentionWrapperState` whose properties are the same as\n      this one, except any overridden properties as provided in `kwargs`.\n    """"""\n\n    def with_same_shape(old, new):\n      """"""Check and set new tensor\'s shape.""""""\n      if isinstance(old, ops.Tensor) and isinstance(new, ops.Tensor):\n        return tensor_util.with_same_shape(old, new)\n      return new\n\n    return nest.map_structure(\n        with_same_shape, self,\n        super(AttentionWrapperState, self)._replace(**kwargs)\n    )\n\n\ndef hardmax(logits, name=None):\n  """"""Returns batched one-hot vectors.\n\n  The depth index containing the `1` is that of the maximum logit value.\n\n  Args:\n    logits: A batch tensor of logit values.\n    name: Name to use when creating ops.\n  Returns:\n    A batched one-hot tensor.\n  """"""\n  with ops.name_scope(name, ""Hardmax"", [logits]):\n    logits = ops.convert_to_tensor(logits, name=""logits"")\n    if logits.get_shape()[-1].value is not None:\n      depth = logits.get_shape()[-1].value\n    else:\n      depth = array_ops.shape(logits)[-1]\n    return array_ops.one_hot(\n        math_ops.argmax(logits, -1), depth, dtype=logits.dtype\n    )\n\n\ndef _compute_attention(\n    attention_mechanism, cell_output, attention_state, attention_layer\n):\n  """"""Computes the attention and alignments for a given attention_mechanism.""""""\n  alignments, next_attention_state = attention_mechanism(\n      cell_output, state=attention_state\n  )\n\n  # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\n  expanded_alignments = array_ops.expand_dims(alignments, 1)\n  # Context is the inner product of alignments and values along the\n  # memory time dimension.\n  # alignments shape is\n  #   [batch_size, 1, memory_time]\n  # attention_mechanism.values shape is\n  #   [batch_size, memory_time, memory_size]\n  # the batched matmul is over memory_time, so the output shape is\n  #   [batch_size, 1, memory_size].\n  # we then squeeze out the singleton dim.\n  context = math_ops.matmul(expanded_alignments, attention_mechanism.values)\n  context = array_ops.squeeze(context, [1])\n\n  if attention_layer is not None:\n    attention = attention_layer(array_ops.concat([cell_output, context], 1))\n  else:\n    attention = context\n\n  return attention, alignments, next_attention_state\n\n\nclass AttentionWrapper(rnn_cell_impl.RNNCell):\n  """"""Wraps another `RNNCell` with attention.\n  """"""\n\n  def __init__(\n      self,\n      cell,\n      attention_mechanism,\n      attention_layer_size=None,\n      alignment_history=False,\n      cell_input_fn=None,\n      output_attention=True,\n      initial_cell_state=None,\n      name=None\n  ):\n    """"""Construct the `AttentionWrapper`.\n\n    **NOTE** If you are using the `BeamSearchDecoder` with a cell wrapped in\n    `AttentionWrapper`, then you must ensure that:\n\n    - The encoder output has been tiled to `beam_width` via\n      @{tf.contrib.seq2seq.tile_batch} (NOT `tf.tile`).\n    - The `batch_size` argument passed to the `zero_state` method of this\n      wrapper is equal to `true_batch_size * beam_width`.\n    - The initial state created with `zero_state` above contains a\n      `cell_state` value containing properly tiled final state from the\n      encoder.\n\n    An example:\n\n    ```\n    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n        encoder_outputs, multiplier=beam_width)\n    tiled_encoder_final_state = tf.conrib.seq2seq.tile_batch(\n        encoder_final_state, multiplier=beam_width)\n    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(\n        sequence_length, multiplier=beam_width)\n    attention_mechanism = MyFavoriteAttentionMechanism(\n        num_units=attention_depth,\n        memory=tiled_inputs,\n        memory_sequence_length=tiled_sequence_length)\n    attention_cell = AttentionWrapper(cell, attention_mechanism, ...)\n    decoder_initial_state = attention_cell.zero_state(\n        dtype, batch_size=true_batch_size * beam_width)\n    decoder_initial_state = decoder_initial_state.clone(\n        cell_state=tiled_encoder_final_state)\n    ```\n\n    Args:\n      cell: An instance of `RNNCell`.\n      attention_mechanism: A list of `AttentionMechanism` instances or a single\n        instance.\n      attention_layer_size: A list of Python integers or a single Python\n        integer, the depth of the attention (output) layer(s). If None\n        (default), use the context as attention at each time step. Otherwise,\n        feed the context and cell output into the attention layer to generate\n        attention at each time step. If attention_mechanism is a list,\n        attention_layer_size must be a list of the same length.\n      alignment_history: Python boolean, whether to store alignment history\n        from all time steps in the final output state (currently stored as a\n        time major `TensorArray` on which you must call `stack()`).\n      cell_input_fn: (optional) A `callable`.  The default is:\n        `lambda inputs, attention: array_ops.concat([inputs, attention], -1)`.\n      output_attention: bool or ""both"".  If `True` (default), the output at each\n        time step is the attention value.  This is the behavior of Luong-style\n        attention mechanisms.  If `False`, the output at each time step is\n        the output of `cell`.  This is the beahvior of Bhadanau-style\n        attention mechanisms.  If ""both"", the attention value and cell output\n        are concatenated together and set as the output. In all cases, the\n        `attention` tensor is propagated to the next time step via the state and\n        is used there. This flag only controls whether the attention mechanism\n        is propagated up to the next cell in an RNN stack or to the top RNN\n        output.\n      initial_cell_state: The initial state value to use for the cell when\n        the user calls `zero_state()`.  Note that if this value is provided\n        now, and the user uses a `batch_size` argument of `zero_state` which\n        does not match the batch size of `initial_cell_state`, proper\n        behavior is not guaranteed.\n      name: Name to use when creating ops.\n\n    Raises:\n      TypeError: `attention_layer_size` is not None and (`attention_mechanism`\n        is a list but `attention_layer_size` is not; or vice versa).\n      ValueError: if `attention_layer_size` is not None, `attention_mechanism`\n        is a list, and its length does not match that of `attention_layer_size`.\n    """"""\n    super(AttentionWrapper, self).__init__(name=name)\n    rnn_cell_impl.assert_like_rnncell(""cell"", cell)\n    if isinstance(attention_mechanism, (list, tuple)):\n      self._is_multi = True\n      attention_mechanisms = attention_mechanism\n      for attention_mechanism in attention_mechanisms:\n        if not isinstance(attention_mechanism, AttentionMechanism):\n          raise TypeError(\n              ""attention_mechanism must contain only instances of ""\n              ""AttentionMechanism, saw type: %s"" %\n              type(attention_mechanism).__name__\n          )\n    else:\n      self._is_multi = False\n      if not isinstance(attention_mechanism, AttentionMechanism):\n        raise TypeError(\n            ""attention_mechanism must be an AttentionMechanism or list of ""\n            ""multiple AttentionMechanism instances, saw type: %s"" %\n            type(attention_mechanism).__name__\n        )\n      attention_mechanisms = (attention_mechanism,)\n\n    if cell_input_fn is None:\n      cell_input_fn = (\n          lambda inputs, attention: array_ops.concat([inputs, attention], -1)\n      )\n    else:\n      if not callable(cell_input_fn):\n        raise TypeError(\n            ""cell_input_fn must be callable, saw type: %s"" %\n            type(cell_input_fn).__name__\n        )\n\n    if attention_layer_size is not None:\n      attention_layer_sizes = tuple(\n          attention_layer_size\n          if isinstance(attention_layer_size, (list, tuple\n                                               )) else (attention_layer_size,)\n      )\n      if len(attention_layer_sizes) != len(attention_mechanisms):\n        raise ValueError(\n            ""If provided, attention_layer_size must contain exactly one ""\n            ""integer per attention_mechanism, saw: %d vs %d"" %\n            (len(attention_layer_sizes), len(attention_mechanisms))\n        )\n      self._attention_layers = tuple(\n          layers_core.Dense(\n              attention_layer_size,\n              name=""attention_layer"",\n              use_bias=False,\n              dtype=attention_mechanisms[i].dtype\n          ) for i, attention_layer_size in enumerate(attention_layer_sizes)\n      )\n      self._attention_layer_size = sum(attention_layer_sizes)\n    else:\n      self._attention_layers = None\n      self._attention_layer_size = sum(\n          attention_mechanism.values.get_shape()[-1].value\n          for attention_mechanism in attention_mechanisms\n      )\n\n    self._cell = cell\n    self._attention_mechanisms = attention_mechanisms\n    self._cell_input_fn = cell_input_fn\n    self._output_attention = output_attention\n    self._alignment_history = alignment_history\n    with ops.name_scope(name, ""AttentionWrapperInit""):\n      if initial_cell_state is None:\n        self._initial_cell_state = None\n      else:\n        final_state_tensor = nest.flatten(initial_cell_state)[-1]\n        state_batch_size = (\n            final_state_tensor.shape[0].value or\n            array_ops.shape(final_state_tensor)[0]\n        )\n        error_message = (\n            ""When constructing AttentionWrapper %s: "" % self._base_name +\n            ""Non-matching batch sizes between the memory ""\n            ""(encoder output) and initial_cell_state.  Are you using ""\n            ""the BeamSearchDecoder?  You may need to tile your initial state ""\n            ""via the tf.contrib.seq2seq.tile_batch function with argument ""\n            ""multiple=beam_width.""\n        )\n        with ops.control_dependencies(\n            self._batch_size_checks(state_batch_size, error_message)\n        ):\n          self._initial_cell_state = nest.map_structure(\n              lambda s: array_ops.identity(s, name=""check_initial_cell_state""),\n              initial_cell_state\n          )\n\n  def _batch_size_checks(self, batch_size, error_message):\n    return [\n        check_ops.assert_equal(\n            batch_size, attention_mechanism.batch_size, message=error_message\n        ) for attention_mechanism in self._attention_mechanisms\n    ]\n\n  def _item_or_tuple(self, seq):\n    """"""Returns `seq` as tuple or the singular element.\n\n    Which is returned is determined by how the AttentionMechanism(s) were passed\n    to the constructor.\n\n    Args:\n      seq: A non-empty sequence of items or generator.\n\n    Returns:\n       Either the values in the sequence as a tuple if AttentionMechanism(s)\n       were passed to the constructor as a sequence or the singular element.\n    """"""\n    t = tuple(seq)\n    if self._is_multi:\n      return t\n    else:\n      return t[0]\n\n  @property\n  def output_size(self):\n    if self._output_attention == True:\n      return self._attention_layer_size\n    elif self._output_attention == False:\n      return self._cell.output_size\n    elif self._output_attention == ""both"":\n      return self._attention_layer_size + self._cell.output_size\n    else:\n      raise ValueError(\n          ""output_attention: %s must be either True, False, or both"" %\n          self._output_attention\n      )\n\n  @property\n  def state_size(self):\n    """"""The `state_size` property of `AttentionWrapper`.\n\n    Returns:\n      An `AttentionWrapperState` tuple containing shapes used by this object.\n    """"""\n    return AttentionWrapperState(\n        cell_state=self._cell.state_size,\n        time=tensor_shape.TensorShape([]),\n        attention=self._attention_layer_size,\n        alignments=self._item_or_tuple(\n            a.alignments_size for a in self._attention_mechanisms\n        ),\n        attention_state=self._item_or_tuple(\n            a.state_size for a in self._attention_mechanisms\n        ),\n        alignment_history=self._item_or_tuple(\n            () for _ in self._attention_mechanisms\n        )\n    )  # sometimes a TensorArray\n\n  def zero_state(self, batch_size, dtype):\n    """"""Return an initial (zero) state tuple for this `AttentionWrapper`.\n\n    **NOTE** Please see the initializer documentation for details of how\n    to call `zero_state` if using an `AttentionWrapper` with a\n    `BeamSearchDecoder`.\n\n    Args:\n      batch_size: `0D` integer tensor: the batch size.\n      dtype: The internal state data type.\n\n    Returns:\n      An `AttentionWrapperState` tuple containing zeroed out tensors and,\n      possibly, empty `TensorArray` objects.\n\n    Raises:\n      ValueError: (or, possibly at runtime, InvalidArgument), if\n        `batch_size` does not match the output size of the encoder passed\n        to the wrapper object at initialization time.\n    """"""\n    with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n      if self._initial_cell_state is not None:\n        cell_state = self._initial_cell_state\n      else:\n        cell_state = self._cell.zero_state(batch_size, dtype)\n      error_message = (\n          ""When calling zero_state of AttentionWrapper %s: "" % self._base_name +\n          ""Non-matching batch sizes between the memory ""\n          ""(encoder output) and the requested batch size.  Are you using ""\n          ""the BeamSearchDecoder?  If so, make sure your encoder output has ""\n          ""been tiled to beam_width via tf.contrib.seq2seq.tile_batch, and ""\n          ""the batch_size= argument passed to zero_state is ""\n          ""batch_size * beam_width.""\n      )\n      with ops.control_dependencies(\n          self._batch_size_checks(batch_size, error_message)\n      ):\n        cell_state = nest.map_structure(\n            lambda s: array_ops.identity(s, name=""checked_cell_state""),\n            cell_state\n        )\n      return AttentionWrapperState(\n          cell_state=cell_state,\n          time=array_ops.zeros([], dtype=dtypes.int32),\n          attention=_zero_state_tensors(\n              self._attention_layer_size, batch_size, dtype\n          ),\n          alignments=self._item_or_tuple(\n              attention_mechanism.initial_alignments(batch_size, dtype)\n              for attention_mechanism in self._attention_mechanisms\n          ),\n          attention_state=self._item_or_tuple(\n              attention_mechanism.initial_state(batch_size, dtype)\n              for attention_mechanism in self._attention_mechanisms\n          ),\n          alignment_history=self._item_or_tuple(\n              tensor_array_ops.TensorArray(\n                  dtype=dtype, size=0, dynamic_size=True\n              ) if self._alignment_history else ()\n              for _ in self._attention_mechanisms\n          )\n      )\n\n  def call(self, inputs, state):\n    """"""Perform a step of attention-wrapped RNN.\n\n    - Step 1: Mix the `inputs` and previous step\'s `attention` output via\n      `cell_input_fn`.\n    - Step 2: Call the wrapped `cell` with this input and its previous state.\n    - Step 3: Score the cell\'s output with `attention_mechanism`.\n    - Step 4: Calculate the alignments by passing the score through the\n      `normalizer`.\n    - Step 5: Calculate the context vector as the inner product between the\n      alignments and the attention_mechanism\'s values (memory).\n    - Step 6: Calculate the attention output by concatenating the cell output\n      and context through the attention layer (a linear layer with\n      `attention_layer_size` outputs).\n\n    Args:\n      inputs: (Possibly nested tuple of) Tensor, the input at this time step.\n      state: An instance of `AttentionWrapperState` containing\n        tensors from the previous time step.\n\n    Returns:\n      A tuple `(attention_or_cell_output, next_state)`, where:\n\n      - `attention_or_cell_output` depending on `output_attention`.\n      - `next_state` is an instance of `AttentionWrapperState`\n         containing the state calculated at this time step.\n\n    Raises:\n      TypeError: If `state` is not an instance of `AttentionWrapperState`.\n    """"""\n    if not isinstance(state, AttentionWrapperState):\n      raise TypeError(\n          ""Expected state to be instance of AttentionWrapperState. ""\n          ""Received type %s instead."" % type(state)\n      )\n\n    # Step 1: Calculate the true inputs to the cell based on the\n    # previous attention value.\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\n    cell_state = state.cell_state\n    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\n\n    cell_batch_size = (\n        cell_output.shape[0].value or array_ops.shape(cell_output)[0]\n    )\n    error_message = (\n        ""When applying AttentionWrapper %s: "" % self.name +\n        ""Non-matching batch sizes between the memory ""\n        ""(encoder output) and the query (decoder output).  Are you using ""\n        ""the BeamSearchDecoder?  You may need to tile your memory input via ""\n        ""the tf.contrib.seq2seq.tile_batch function with argument ""\n        ""multiple=beam_width.""\n    )\n    with ops.control_dependencies(\n        self._batch_size_checks(cell_batch_size, error_message)\n    ):\n      cell_output = array_ops.identity(cell_output, name=""checked_cell_output"")\n\n    if self._is_multi:\n      previous_attention_state = state.attention_state\n      previous_alignment_history = state.alignment_history\n    else:\n      previous_attention_state = [state.attention_state]\n      previous_alignment_history = [state.alignment_history]\n\n    all_alignments = []\n    all_attentions = []\n    all_attention_states = []\n    maybe_all_histories = []\n    for i, attention_mechanism in enumerate(self._attention_mechanisms):\n      attention, alignments, next_attention_state = _compute_attention(\n          attention_mechanism, cell_output, previous_attention_state[i],\n          self._attention_layers[i] if self._attention_layers else None\n      )\n      alignment_history = previous_alignment_history[i].write(\n          state.time, alignments\n      ) if self._alignment_history else ()\n\n      all_attention_states.append(next_attention_state)\n      all_alignments.append(alignments)\n      all_attentions.append(attention)\n      maybe_all_histories.append(alignment_history)\n\n    attention = array_ops.concat(all_attentions, 1)\n    next_state = AttentionWrapperState(\n        time=state.time + 1,\n        cell_state=next_cell_state,\n        attention=attention,\n        attention_state=self._item_or_tuple(all_attention_states),\n        alignments=self._item_or_tuple(all_alignments),\n        alignment_history=self._item_or_tuple(maybe_all_histories)\n    )\n\n    if self._output_attention == True:\n      return attention, next_state\n    elif self._output_attention == False:\n      return cell_output, next_state\n    elif self._output_attention == ""both"":\n      return array_ops.concat((cell_output, attention), axis=-1), next_state\n    else:\n      raise ValueError(\n          ""output_attention: %s must be either True, False, or both"" %\n          self._output_attention\n      )\n'"
open_seq2seq/parts/rnns/flstm.py,0,"b'""""""Module for constructing RNN Cells.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nfrom tensorflow.contrib.rnn.python.ops import core_rnn_cell\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import variable_scope as vs\n\n# pylint: disable=protected-access\n_Linear = core_rnn_cell._Linear  # pylint: disable=invalid-name\n# pylint: enable=protected-access\n\n\n# TODO: must implement all abstract methods\nclass FLSTMCell(rnn_cell_impl.RNNCell):\n  """"""Group LSTM cell (G-LSTM).\n  The implementation is based on:\n    https://arxiv.org/abs/1703.10722\n  O. Kuchaiev and B. Ginsburg\n  ""Factorization Tricks for LSTM Networks"", ICLR 2017 workshop.\n  """"""\n\n  def __init__(self, num_units, fact_size, initializer=None, num_proj=None,\n               forget_bias=1.0, activation=math_ops.tanh, reuse=None):\n    """"""Initialize the parameters of G-LSTM cell.\n    Args:\n      num_units: int, The number of units in the G-LSTM cell\n      initializer: (optional) The initializer to use for the weight and\n        projection matrices.\n      num_proj: (optional) int, The output dimensionality for the projection\n        matrices.  If None, no projection is performed.\n      forget_bias: Biases of the forget gate are initialized by default to 1\n        in order to reduce the scale of forgetting at the beginning of\n        the training.\n      activation: Activation function of the inner states.\n      reuse: (optional) Python boolean describing whether to reuse variables\n        in an existing scope.  If not `True`, and the existing scope already\n        has the given variables, an error is raised.\n    Raises:\n      ValueError: If `num_units` or `num_proj` is not divisible by\n        `number_of_groups`.\n    """"""\n    super(FLSTMCell, self).__init__(_reuse=reuse)\n    self._num_units = num_units\n    self._initializer = initializer\n    self._fact_size = fact_size\n    self._forget_bias = forget_bias\n    self._activation = activation\n    self._num_proj = num_proj\n\n    if num_proj:\n      self._state_size = rnn_cell_impl.LSTMStateTuple(num_units, num_proj)\n      self._output_size = num_proj\n    else:\n      self._state_size = rnn_cell_impl.LSTMStateTuple(num_units, num_units)\n      self._output_size = num_units\n    self._linear1 = None\n    self._linear2 = None\n    self._linear3 = None\n\n  @property\n  def state_size(self):\n    return self._state_size\n\n  @property\n  def output_size(self):\n    return self._output_size\n\n  # TODO: does not match signature of the base method\n  def call(self, inputs, state):\n    """"""\n    """"""\n    (c_prev, m_prev) = state\n    self._batch_size = inputs.shape[0].value or array_ops.shape(inputs)[0]\n    scope = vs.get_variable_scope()\n    with vs.variable_scope(scope, initializer=self._initializer):\n      x = array_ops.concat([inputs, m_prev], axis=1)\n      with vs.variable_scope(""first_gemm""):\n        if self._linear1 is None:\n          # no bias for bottleneck\n          self._linear1 = _Linear(x, self._fact_size, False)\n        R_fact = self._linear1(x)\n      with vs.variable_scope(""second_gemm""):\n        if self._linear2 is None:\n          self._linear2 = _Linear(R_fact, 4*self._num_units, True)\n        R = self._linear2(R_fact)\n      i, j, f, o = array_ops.split(R, 4, 1)\n\n      c = (math_ops.sigmoid(f + self._forget_bias) * c_prev +\n           math_ops.sigmoid(i) * math_ops.tanh(j))\n      m = math_ops.sigmoid(o) * self._activation(c)\n\n    if self._num_proj is not None:\n      with vs.variable_scope(""projection""):\n        if self._linear3 is None:\n          self._linear3 = _Linear(m, self._num_proj, False)\n        m = self._linear3(m)\n\n    new_state = rnn_cell_impl.LSTMStateTuple(c, m)\n    return m, new_state\n'"
open_seq2seq/parts/rnns/glstm.py,0,"b'""""""Module for constructing RNN Cells.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nfrom tensorflow.contrib.rnn.python.ops import core_rnn_cell\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import variable_scope as vs\n\n# pylint: disable=protected-access\n_Linear = core_rnn_cell._Linear  # pylint: disable=invalid-name\n# pylint: enable=protected-access\n\n\n# TODO: must implement all abstract methods\nclass GLSTMCell(rnn_cell_impl.RNNCell):\n  """"""Group LSTM cell (G-LSTM).\n  The implementation is based on:\n    https://arxiv.org/abs/1703.10722\n  O. Kuchaiev and B. Ginsburg\n  ""Factorization Tricks for LSTM Networks"", ICLR 2017 workshop.\n  """"""\n\n  def __init__(self, num_units, initializer=None, num_proj=None,\n               number_of_groups=1, forget_bias=1.0, activation=math_ops.tanh,\n               reuse=None):\n    """"""Initialize the parameters of G-LSTM cell.\n    Args:\n      num_units: int, The number of units in the G-LSTM cell\n      initializer: (optional) The initializer to use for the weight and\n        projection matrices.\n      num_proj: (optional) int, The output dimensionality for the projection\n        matrices.  If None, no projection is performed.\n      number_of_groups: (optional) int, number of groups to use.\n        If `number_of_groups` is 1, then it should be equivalent to LSTM cell\n      forget_bias: Biases of the forget gate are initialized by default to 1\n        in order to reduce the scale of forgetting at the beginning of\n        the training.\n      activation: Activation function of the inner states.\n      reuse: (optional) Python boolean describing whether to reuse variables\n        in an existing scope.  If not `True`, and the existing scope already\n        has the given variables, an error is raised.\n    Raises:\n      ValueError: If `num_units` or `num_proj` is not divisible by\n        `number_of_groups`.\n    """"""\n    super(GLSTMCell, self).__init__(_reuse=reuse)\n    self._num_units = num_units\n    self._initializer = initializer\n    self._num_proj = num_proj\n    self._forget_bias = forget_bias\n    self._activation = activation\n    self._number_of_groups = number_of_groups\n\n    if self._num_units % self._number_of_groups != 0:\n      raise ValueError(""num_units must be divisible by number_of_groups"")\n    if self._num_proj:\n      if self._num_proj % self._number_of_groups != 0:\n        raise ValueError(""num_proj must be divisible by number_of_groups"")\n      self._group_shape = [int(self._num_proj / self._number_of_groups),\n                           int(self._num_units / self._number_of_groups)]\n    else:\n      self._group_shape = [int(self._num_units / self._number_of_groups),\n                           int(self._num_units / self._number_of_groups)]\n\n    if num_proj:\n      self._state_size = rnn_cell_impl.LSTMStateTuple(num_units, num_proj)\n      self._output_size = num_proj\n    else:\n      self._state_size = rnn_cell_impl.LSTMStateTuple(num_units, num_units)\n      self._output_size = num_units\n    self._linear1 = [None] * self._number_of_groups\n    self._linear2 = None\n\n  @property\n  def state_size(self):\n    return self._state_size\n\n  @property\n  def output_size(self):\n    return self._output_size\n\n  def _get_input_for_group(self, inputs, group_id, group_size):\n    """"""Slices inputs into groups to prepare for processing by cell\'s groups\n    Args:\n      inputs: cell input or it\'s previous state,\n              a Tensor, 2D, [batch x num_units]\n      group_id: group id, a Scalar, for which to prepare input\n      group_size: size of the group\n    Returns:\n      subset of inputs corresponding to group ""group_id"",\n      a Tensor, 2D, [batch x num_units/number_of_groups]\n    """"""\n    return array_ops.slice(input_=inputs,\n                           begin=[0, group_id * group_size],\n                           size=[self._batch_size, group_size],\n                           name=(""GLSTM_group%d_input_generation"" % group_id))\n\n  # TODO: does not match signature of the base method\n  def call(self, inputs, state):\n    """"""Run one step of G-LSTM.\n    Args:\n      inputs: input Tensor, 2D, [batch x num_units].\n      state: this must be a tuple of state Tensors, both `2-D`,\n      with column sizes `c_state` and `m_state`.\n    Returns:\n      A tuple containing:\n      - A `2-D, [batch x output_dim]`, Tensor representing the output of the\n        G-LSTM after reading `inputs` when previous state was `state`.\n        Here output_dim is:\n           num_proj if num_proj was set,\n           num_units otherwise.\n      - LSTMStateTuple representing the new state of G-LSTM cell\n        after reading `inputs` when the previous state was `state`.\n    Raises:\n      ValueError: If input size cannot be inferred from inputs via\n        static shape inference.\n    """"""\n    (c_prev, m_prev) = state\n\n    self._batch_size = inputs.shape[0].value or array_ops.shape(inputs)[0]\n    input_size = inputs.shape[-1].value or array_ops.shape(inputs)[-1]\n    dtype = inputs.dtype\n    scope = vs.get_variable_scope()\n    with vs.variable_scope(scope, initializer=self._initializer):\n      i_parts = []\n      j_parts = []\n      f_parts = []\n      o_parts = []\n\n      for group_id in range(self._number_of_groups):\n        with vs.variable_scope(""group%d"" % group_id):\n          x_g_id = array_ops.concat(\n            [self._get_input_for_group(inputs, group_id,\n                                       int(input_size / self._number_of_groups)),\n                                       # self._group_shape[0]), # this is only correct if inputs dim = num_units!!!\n             self._get_input_for_group(m_prev, group_id,\n                                       int(self._output_size / self._number_of_groups))], axis=1)\n                                       # self._group_shape[0])], axis=1)\n          if self._linear1[group_id] is None:\n            self._linear1[group_id] = _Linear(\n              x_g_id, 4 * self._group_shape[1],\n              False,\n            )\n          R_k = self._linear1[group_id](x_g_id)  # pylint: disable=invalid-name\n          i_k, j_k, f_k, o_k = array_ops.split(R_k, 4, 1)\n\n        i_parts.append(i_k)\n        j_parts.append(j_k)\n        f_parts.append(f_k)\n        o_parts.append(o_k)\n\n      bi = vs.get_variable(\n        name=""bias_i"",\n        shape=[self._num_units],\n        dtype=dtype,\n        initializer=init_ops.constant_initializer(0.0, dtype=dtype),\n      )\n      bj = vs.get_variable(\n        name=""bias_j"",\n        shape=[self._num_units],\n        dtype=dtype,\n        initializer=init_ops.constant_initializer(0.0, dtype=dtype),\n      )\n      bf = vs.get_variable(\n        name=""bias_f"",\n        shape=[self._num_units],\n        dtype=dtype,\n        initializer=init_ops.constant_initializer(0.0, dtype=dtype),\n      )\n      bo = vs.get_variable(\n        name=""bias_o"",\n        shape=[self._num_units],\n        dtype=dtype,\n        initializer=init_ops.constant_initializer(0.0, dtype=dtype),\n      )\n\n      i = nn_ops.bias_add(array_ops.concat(i_parts, axis=1), bi)\n      j = nn_ops.bias_add(array_ops.concat(j_parts, axis=1), bj)\n      f = nn_ops.bias_add(array_ops.concat(f_parts, axis=1), bf)\n      o = nn_ops.bias_add(array_ops.concat(o_parts, axis=1), bo)\n\n    c = (math_ops.sigmoid(f + self._forget_bias) * c_prev +\n         math_ops.sigmoid(i) * math_ops.tanh(j))\n    m = math_ops.sigmoid(o) * self._activation(c)\n\n    if self._num_proj is not None:\n      with vs.variable_scope(""projection""):\n        if self._linear2 is None:\n          self._linear2 = _Linear(m, self._num_proj, False)\n        m = self._linear2(m)\n\n    new_state = rnn_cell_impl.LSTMStateTuple(c, m)\n    return m, new_state\n'"
open_seq2seq/parts/rnns/gnmt.py,7,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# THIS CODE WAS TAKEN FROM:\n#   https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/gnmt_model.py\n\n""""""GNMT attention sequence-to-sequence model with dynamic RNN support.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nimport tensorflow as tf\n\nfrom tensorflow.python.util import nest\n\n\n# TODO: must implement all abstract methods\nclass GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):\n  """"""A MultiCell with GNMT attention style.""""""\n\n  def __init__(self, attention_cell, cells, use_new_attention=False):\n    """"""Creates a GNMTAttentionMultiCell.\n\n    Args:\n      attention_cell: An instance of AttentionWrapper.\n      cells: A list of RNNCell wrapped with AttentionInputWrapper.\n      use_new_attention: Whether to use the attention generated from current\n        step bottom layer\'s output. Default is False.\n    """"""\n    cells = [attention_cell] + cells\n    self.use_new_attention = use_new_attention\n    super(GNMTAttentionMultiCell, self).__init__(cells, state_is_tuple=True)\n\n  # TODO: does not match signature of the base method\n  def __call__(self, inputs, state, scope=None):\n    """"""Run the cell with bottom layer\'s attention copied to all upper layers.""""""\n    if not nest.is_sequence(state):\n      raise ValueError(\n          ""Expected state to be a tuple of length %d, but received: %s""\n          % (len(self.state_size), state))\n\n    with tf.variable_scope(scope or ""multi_rnn_cell""):\n      new_states = []\n\n      with tf.variable_scope(""cell_0_attention""):\n        attention_cell = self._cells[0]\n        attention_state = state[0]\n        cur_inp, new_attention_state = attention_cell(inputs, attention_state)\n        new_states.append(new_attention_state)\n\n      for i in range(1, len(self._cells)):\n        with tf.variable_scope(""cell_%d"" % i):\n\n          cell = self._cells[i]\n          cur_state = state[i]\n\n          if self.use_new_attention:\n            cur_inp = tf.concat([cur_inp, new_attention_state.attention], -1)\n          else:\n            cur_inp = tf.concat([cur_inp, attention_state.attention], -1)\n\n          cur_inp, new_state = cell(cur_inp, cur_state)\n          new_states.append(new_state)\n\n    return cur_inp, tuple(new_states)\n\n\ndef gnmt_residual_fn(inputs, outputs):\n  """"""Residual function that handles different inputs and outputs inner dims.\n\n  Args:\n    inputs: cell inputs, this is actual inputs concatenated with the attention\n      vector.\n    outputs: cell outputs\n\n  Returns:\n    outputs + actual inputs\n  """"""\n  def split_input(inp, out):\n    out_dim = out.get_shape().as_list()[-1]\n    inp_dim = inp.get_shape().as_list()[-1]\n    return tf.split(inp, [out_dim, inp_dim - out_dim], axis=-1)\n\n  actual_inputs, _ = nest.map_structure(split_input, inputs, outputs)\n\n  def assert_shape_match(inp, out):\n    inp.get_shape().assert_is_compatible_with(out.get_shape())\n\n  nest.assert_same_structure(actual_inputs, outputs)\n  nest.map_structure(assert_shape_match, actual_inputs, outputs)\n  return nest.map_structure(lambda inp, out: inp + out, actual_inputs, outputs)\n'"
open_seq2seq/parts/rnns/helper.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A library of helpers for use with SamplingDecoders.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nimport six\n\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops.distributions import bernoulli\nfrom tensorflow.python.ops.distributions import categorical\nfrom tensorflow.python.util import nest\nfrom tensorflow.contrib.seq2seq.python.ops.helper import Helper\n\n__all__ = [\n    ""Helper"",\n    ""TrainingHelper"",\n    ""GreedyEmbeddingHelper"",\n    ""SampleEmbeddingHelper"",\n    ""CustomHelper"",\n    ""ScheduledEmbeddingTrainingHelper"",\n    ""ScheduledOutputTrainingHelper"",\n    ""InferenceHelper"",\n]\n\n_transpose_batch_time = decoder._transpose_batch_time  # pylint: disable=protected-access\n\n\ndef _unstack_ta(inp):\n  return tensor_array_ops.TensorArray(\n      dtype=inp.dtype, size=array_ops.shape(inp)[0],\n      element_shape=inp.get_shape()[1:]).unstack(inp)\n\n\nclass CustomHelper(Helper):\n  """"""Base abstract class that allows the user to customize sampling.""""""\n\n  def __init__(self, initialize_fn, sample_fn, next_inputs_fn,\n               sample_ids_shape=None, sample_ids_dtype=None):\n    """"""Initializer.\n    Args:\n      initialize_fn: callable that returns `(finished, next_inputs)`\n        for the first iteration.\n      sample_fn: callable that takes `(time, outputs, state)`\n        and emits tensor `sample_ids`.\n      next_inputs_fn: callable that takes `(time, outputs, state, sample_ids)`\n        and emits `(finished, next_inputs, next_state)`.\n      sample_ids_shape: Either a list of integers, or a 1-D Tensor of type\n        `int32`, the shape of each value in the `sample_ids` batch. Defaults to\n        a scalar.\n      sample_ids_dtype: The dtype of the `sample_ids` tensor. Defaults to int32.\n    """"""\n    self._initialize_fn = initialize_fn\n    self._sample_fn = sample_fn\n    self._next_inputs_fn = next_inputs_fn\n    self._batch_size = None\n    self._sample_ids_shape = tensor_shape.TensorShape(sample_ids_shape or [])\n    self._sample_ids_dtype = sample_ids_dtype or dtypes.int32\n\n  @property\n  def batch_size(self):\n    if self._batch_size is None:\n      raise ValueError(""batch_size accessed before initialize was called"")\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return self._sample_ids_shape\n\n  @property\n  def sample_ids_dtype(self):\n    return self._sample_ids_dtype\n\n  def initialize(self, name=None):\n    with ops.name_scope(name, ""%sInitialize"" % type(self).__name__):\n      (finished, next_inputs) = self._initialize_fn()\n      if self._batch_size is None:\n        self._batch_size = array_ops.size(finished)\n    return (finished, next_inputs)\n\n  def sample(self, time, outputs, state, name=None):\n    with ops.name_scope(\n            name, ""%sSample"" % type(self).__name__, (time, outputs, state)):\n      return self._sample_fn(time=time, outputs=outputs, state=state)\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    with ops.name_scope(\n            name, ""%sNextInputs"" % type(self).__name__, (time, outputs, state)):\n      return self._next_inputs_fn(\n          time=time, outputs=outputs, state=state, sample_ids=sample_ids)\n\n\nclass TrainingHelper(Helper):\n  """"""A helper for use during training.  Only reads inputs.\n  Returned sample_ids are the argmax of the RNN output logits.\n  """"""\n\n  def __init__(self, inputs, sequence_length, time_major=False, name=None):\n    """"""Initializer.\n    Args:\n      inputs: A (structure of) input tensors.\n      sequence_length: An int32 vector tensor.\n      time_major: Python bool.  Whether the tensors in `inputs` are time major.\n        If `False` (default), they are assumed to be batch major.\n      name: Name scope for any created operations.\n    Raises:\n      ValueError: if `sequence_length` is not a 1D tensor.\n    """"""\n    with ops.name_scope(name, ""TrainingHelper"", [inputs, sequence_length]):\n      inputs = ops.convert_to_tensor(inputs, name=""inputs"")\n      self._inputs = inputs\n      if not time_major:\n        inputs = nest.map_structure(_transpose_batch_time, inputs)\n\n      self._input_tas = nest.map_structure(_unstack_ta, inputs)\n      self._sequence_length = ops.convert_to_tensor(\n          sequence_length, name=""sequence_length"")\n      if self._sequence_length.get_shape().ndims != 1:\n        raise ValueError(\n            ""Expected sequence_length to be a vector, but received shape: %s"" %\n            self._sequence_length.get_shape())\n\n      self._zero_inputs = nest.map_structure(\n          lambda inp: array_ops.zeros_like(inp[0, :]), inputs)\n\n      self._batch_size = array_ops.size(sequence_length)\n\n  @property\n  def inputs(self):\n    return self._inputs\n\n  @property\n  def sequence_length(self):\n    return self._sequence_length\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return tensor_shape.TensorShape([])\n\n  @property\n  def sample_ids_dtype(self):\n    return dtypes.int32\n\n  def initialize(self, name=None):\n    with ops.name_scope(name, ""TrainingHelperInitialize""):\n      finished = math_ops.equal(0, self._sequence_length)\n      all_finished = math_ops.reduce_all(finished)\n      next_inputs = control_flow_ops.cond(\n          all_finished, lambda: self._zero_inputs,\n          lambda: nest.map_structure(lambda inp: inp.read(0), self._input_tas))\n      return (finished, next_inputs)\n\n  def sample(self, time, outputs, name=None, **unused_kwargs):\n    with ops.name_scope(name, ""TrainingHelperSample"", [time, outputs]):\n      sample_ids = math_ops.cast(\n          math_ops.argmax(outputs, axis=-1), dtypes.int32)\n      return sample_ids\n\n  def next_inputs(self, time, outputs, state, name=None, **unused_kwargs):\n    """"""next_inputs_fn for TrainingHelper.""""""\n    with ops.name_scope(name, ""TrainingHelperNextInputs"",\n                        [time, outputs, state]):\n      next_time = time + 1\n      finished = (next_time >= self._sequence_length)\n      all_finished = math_ops.reduce_all(finished)\n\n      def read_from_ta(inp):\n        return inp.read(next_time)\n      next_inputs = control_flow_ops.cond(\n          all_finished, lambda: self._zero_inputs,\n          lambda: nest.map_structure(read_from_ta, self._input_tas))\n      return (finished, next_inputs, state)\n\n\nclass ScheduledEmbeddingTrainingHelper(TrainingHelper):\n  """"""A training helper that adds scheduled sampling.\n  Returns -1s for sample_ids where no sampling took place; valid sample id\n  values elsewhere.\n  """"""\n\n  def __init__(self, inputs, sequence_length, embedding, sampling_probability,\n               time_major=False, seed=None, scheduling_seed=None, name=None):\n    """"""Initializer.\n    Args:\n      inputs: A (structure of) input tensors.\n      sequence_length: An int32 vector tensor.\n      embedding: A callable that takes a vector tensor of `ids` (argmax ids),\n        or the `params` argument for `embedding_lookup`.\n      sampling_probability: A 0D `float32` tensor: the probability of sampling\n        categorically from the output ids instead of reading directly from the\n        inputs.\n      time_major: Python bool.  Whether the tensors in `inputs` are time major.\n        If `False` (default), they are assumed to be batch major.\n      seed: The sampling seed.\n      scheduling_seed: The schedule decision rule sampling seed.\n      name: Name scope for any created operations.\n    Raises:\n      ValueError: if `sampling_probability` is not a scalar or vector.\n    """"""\n    with ops.name_scope(name, ""ScheduledEmbeddingSamplingWrapper"",\n                        [embedding, sampling_probability]):\n      if callable(embedding):\n        self._embedding_fn = embedding\n      else:\n        self._embedding_fn = (\n            lambda ids: embedding_ops.embedding_lookup(embedding, ids))\n      self._sampling_probability = ops.convert_to_tensor(\n          sampling_probability, name=""sampling_probability"")\n      if self._sampling_probability.get_shape().ndims not in (0, 1):\n        raise ValueError(\n            ""sampling_probability must be either a scalar or a vector. ""\n            ""saw shape: %s"" % (self._sampling_probability.get_shape()))\n      self._seed = seed\n      self._scheduling_seed = scheduling_seed\n      super(ScheduledEmbeddingTrainingHelper, self).__init__(\n          inputs=inputs,\n          sequence_length=sequence_length,\n          time_major=time_major,\n          name=name)\n\n  def initialize(self, name=None):\n    return super(ScheduledEmbeddingTrainingHelper, self).initialize(name=name)\n\n  def sample(self, time, outputs, state, name=None):\n    with ops.name_scope(name, ""ScheduledEmbeddingTrainingHelperSample"",\n                        [time, outputs, state]):\n      # Return -1s where we did not sample, and sample_ids elsewhere\n      select_sampler = bernoulli.Bernoulli(\n          probs=self._sampling_probability, dtype=dtypes.bool)\n      select_sample = select_sampler.sample(\n          sample_shape=self.batch_size, seed=self._scheduling_seed)\n      sample_id_sampler = categorical.Categorical(logits=outputs)\n      return array_ops.where(\n          select_sample,\n          sample_id_sampler.sample(seed=self._seed),\n          gen_array_ops.fill([self.batch_size], -1))\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    with ops.name_scope(name, ""ScheduledEmbeddingTrainingHelperNextInputs"",\n                        [time, outputs, state, sample_ids]):\n      (finished, base_next_inputs, state) = (\n          super(ScheduledEmbeddingTrainingHelper, self).next_inputs(\n              time=time,\n              outputs=outputs,\n              state=state,\n              sample_ids=sample_ids,\n              name=name))\n\n      def maybe_sample():\n        """"""Perform scheduled sampling.""""""\n        where_sampling = math_ops.cast(\n            array_ops.where(sample_ids > -1), dtypes.int32)\n        where_not_sampling = math_ops.cast(\n            array_ops.where(sample_ids <= -1), dtypes.int32)\n        sample_ids_sampling = array_ops.gather_nd(sample_ids, where_sampling)\n        inputs_not_sampling = array_ops.gather_nd(\n            base_next_inputs, where_not_sampling)\n        sampled_next_inputs = self._embedding_fn(sample_ids_sampling)\n        base_shape = array_ops.shape(base_next_inputs)\n        return (array_ops.scatter_nd(indices=where_sampling,\n                                     updates=sampled_next_inputs,\n                                     shape=base_shape)\n                + array_ops.scatter_nd(indices=where_not_sampling,\n                                       updates=inputs_not_sampling,\n                                       shape=base_shape))\n\n      all_finished = math_ops.reduce_all(finished)\n      next_inputs = control_flow_ops.cond(\n          all_finished, lambda: base_next_inputs, maybe_sample)\n      return (finished, next_inputs, state)\n\n\nclass ScheduledOutputTrainingHelper(TrainingHelper):\n  """"""A training helper that adds scheduled sampling directly to outputs.\n  Returns False for sample_ids where no sampling took place; True elsewhere.\n  """"""\n\n  def __init__(self, inputs, sequence_length, sampling_probability,\n               time_major=False, seed=None, next_inputs_fn=None,\n               auxiliary_inputs=None, name=None):\n    """"""Initializer.\n    Args:\n      inputs: A (structure) of input tensors.\n      sequence_length: An int32 vector tensor.\n      sampling_probability: A 0D `float32` tensor: the probability of sampling\n        from the outputs instead of reading directly from the inputs.\n      time_major: Python bool.  Whether the tensors in `inputs` are time major.\n        If `False` (default), they are assumed to be batch major.\n      seed: The sampling seed.\n      next_inputs_fn: (Optional) callable to apply to the RNN outputs to create\n        the next input when sampling. If `None` (default), the RNN outputs will\n        be used as the next inputs.\n      auxiliary_inputs: An optional (structure of) auxiliary input tensors with\n        a shape that matches `inputs` in all but (potentially) the final\n        dimension. These tensors will be concatenated to the sampled output or\n        the `inputs` when not sampling for use as the next input.\n      name: Name scope for any created operations.\n    Raises:\n      ValueError: if `sampling_probability` is not a scalar or vector.\n    """"""\n    with ops.name_scope(name, ""ScheduledOutputTrainingHelper"",\n                        [inputs, auxiliary_inputs, sampling_probability]):\n      self._sampling_probability = ops.convert_to_tensor(\n          sampling_probability, name=""sampling_probability"")\n      if self._sampling_probability.get_shape().ndims not in (0, 1):\n        raise ValueError(\n            ""sampling_probability must be either a scalar or a vector. ""\n            ""saw shape: %s"" % (self._sampling_probability.get_shape()))\n\n      if auxiliary_inputs is None:\n        maybe_concatenated_inputs = inputs\n      else:\n        inputs = ops.convert_to_tensor(inputs, name=""inputs"")\n        auxiliary_inputs = ops.convert_to_tensor(\n            auxiliary_inputs, name=""auxiliary_inputs"")\n        maybe_concatenated_inputs = nest.map_structure(\n            lambda x, y: array_ops.concat((x, y), -1),\n            inputs, auxiliary_inputs)\n        if not time_major:\n          auxiliary_inputs = nest.map_structure(\n              _transpose_batch_time, auxiliary_inputs)\n\n      self._auxiliary_input_tas = (\n          nest.map_structure(_unstack_ta, auxiliary_inputs)\n          if auxiliary_inputs is not None else None)\n\n      self._seed = seed\n\n      self._next_inputs_fn = next_inputs_fn\n\n      super(ScheduledOutputTrainingHelper, self).__init__(\n          inputs=maybe_concatenated_inputs,\n          sequence_length=sequence_length,\n          time_major=time_major,\n          name=name)\n\n  def initialize(self, name=None):\n    return super(ScheduledOutputTrainingHelper, self).initialize(name=name)\n\n  def sample(self, time, outputs, state, name=None):\n    with ops.name_scope(name, ""ScheduledOutputTrainingHelperSample"",\n                        [time, outputs, state]):\n      sampler = bernoulli.Bernoulli(probs=self._sampling_probability)\n      return sampler.sample(sample_shape=self.batch_size, seed=self._seed)\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    with ops.name_scope(name, ""ScheduledOutputTrainingHelperNextInputs"",\n                        [time, outputs, state, sample_ids]):\n      (finished, base_next_inputs, state) = (\n          super(ScheduledOutputTrainingHelper, self).next_inputs(\n              time=time,\n              outputs=outputs,\n              state=state,\n              sample_ids=sample_ids,\n              name=name))\n      sample_ids = math_ops.cast(sample_ids, dtypes.bool)\n\n      def maybe_sample():\n        """"""Perform scheduled sampling.""""""\n\n        def maybe_concatenate_auxiliary_inputs(outputs_, indices=None):\n          """"""Concatenate outputs with auxiliary inputs, if they exist.""""""\n          if self._auxiliary_input_tas is None:\n            return outputs_\n\n          next_time = time + 1\n          auxiliary_inputs = nest.map_structure(\n              lambda ta: ta.read(next_time), self._auxiliary_input_tas)\n          if indices is not None:\n            auxiliary_inputs = array_ops.gather_nd(auxiliary_inputs, indices)\n          return nest.map_structure(\n              lambda x, y: array_ops.concat((x, y), -1),\n              outputs_, auxiliary_inputs)\n\n        if self._next_inputs_fn is None:\n          return array_ops.where(\n              sample_ids, maybe_concatenate_auxiliary_inputs(outputs),\n              base_next_inputs)\n\n        where_sampling = math_ops.cast(\n            array_ops.where(sample_ids), dtypes.int32)\n        where_not_sampling = math_ops.cast(\n            array_ops.where(math_ops.logical_not(sample_ids)), dtypes.int32)\n        outputs_sampling = array_ops.gather_nd(outputs, where_sampling)\n        inputs_not_sampling = array_ops.gather_nd(base_next_inputs,\n                                                  where_not_sampling)\n        sampled_next_inputs = maybe_concatenate_auxiliary_inputs(\n            self._next_inputs_fn(outputs_sampling), where_sampling)\n\n        base_shape = array_ops.shape(base_next_inputs)\n        return (array_ops.scatter_nd(indices=where_sampling,\n                                     updates=sampled_next_inputs,\n                                     shape=base_shape)\n                + array_ops.scatter_nd(indices=where_not_sampling,\n                                       updates=inputs_not_sampling,\n                                       shape=base_shape))\n\n      all_finished = math_ops.reduce_all(finished)\n      no_samples = math_ops.logical_not(math_ops.reduce_any(sample_ids))\n      next_inputs = control_flow_ops.cond(\n          math_ops.logical_or(all_finished, no_samples),\n          lambda: base_next_inputs, maybe_sample)\n      return (finished, next_inputs, state)\n\n\nclass GreedyEmbeddingHelper(Helper):\n  """"""A helper for use during inference.\n  Uses the argmax of the output (treated as logits) and passes the\n  result through an embedding layer to get the next input.\n  """"""\n\n  def __init__(self, embedding, start_tokens, end_token, positional_embedding=None):\n    """"""Initializer.\n    Args:\n      embedding: A callable that takes a vector tensor of `ids` (argmax ids),\n        or the `params` argument for `embedding_lookup`. The returned tensor\n        will be passed to the decoder input.\n      start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n      end_token: `int32` scalar, the token that marks end of decoding.\n    Raises:\n      ValueError: if `start_tokens` is not a 1D tensor or `end_token` is not a\n        scalar.\n    """"""\n    if callable(embedding):\n      self._embedding_fn = embedding\n    else:\n      self._embedding_fn = (\n          lambda ids: embedding_ops.embedding_lookup(embedding, ids))\n\n    self._use_pos_embedding = False\n    if positional_embedding is not None:\n      if callable(positional_embedding):\n        self._pos_embedding_fn = positional_embedding\n      else:\n        self._pos_embedding_fn = (\n            lambda ids: embedding_ops.embedding_lookup(positional_embedding, ids))\n      self._use_pos_embedding = True\n\n    self._start_tokens = ops.convert_to_tensor(\n        start_tokens, dtype=dtypes.int32, name=""start_tokens"")\n    self._end_token = ops.convert_to_tensor(\n        end_token, dtype=dtypes.int32, name=""end_token"")\n    if self._start_tokens.get_shape().ndims != 1:\n      raise ValueError(""start_tokens must be a vector"")\n    self._batch_size = array_ops.size(start_tokens)\n    if self._end_token.get_shape().ndims != 0:\n      raise ValueError(""end_token must be a scalar"")\n    self._start_inputs = self._embedding_fn(self._start_tokens)\n    if self._use_pos_embedding:\n      # change dtype for mixed precision\n      self._start_inputs += self._pos_embedding_fn(ops.convert_to_tensor(0))\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return tensor_shape.TensorShape([])\n\n  @property\n  def sample_ids_dtype(self):\n    return dtypes.int32\n\n  def initialize(self, name=None):\n    finished = array_ops.tile([False], [self._batch_size])\n    return (finished, self._start_inputs)\n\n  def sample(self, time, outputs, state, name=None):\n    """"""sample for GreedyEmbeddingHelper.""""""\n    del time, state  # unused by sample_fn\n    # Outputs are logits, use argmax to get the most probable id\n    if not isinstance(outputs, ops.Tensor):\n      raise TypeError(""Expected outputs to be a single Tensor, got: %s"" %\n                      type(outputs))\n    sample_ids = math_ops.argmax(outputs, axis=-1, output_type=dtypes.int32)\n    return sample_ids\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    """"""next_inputs_fn for GreedyEmbeddingHelper.""""""\n    del outputs  # unused by next_inputs_fn\n    finished = math_ops.equal(sample_ids, self._end_token)\n    all_finished = math_ops.reduce_all(finished)\n    next_inputs = control_flow_ops.cond(\n        all_finished,\n        # If we\'re finished, the next_inputs value doesn\'t matter\n        lambda: self._start_inputs,\n        lambda: self._embedding_fn(sample_ids))\n    if self._use_pos_embedding:\n      next_inputs += self._pos_embedding_fn(ops.convert_to_tensor(time))\n    return (finished, next_inputs, state)\n\n\nclass SampleEmbeddingHelper(GreedyEmbeddingHelper):\n  """"""A helper for use during inference.\n  Uses sampling (from a distribution) instead of argmax and passes the\n  result through an embedding layer to get the next input.\n  """"""\n\n  def __init__(self, embedding, start_tokens, end_token,\n               softmax_temperature=None, seed=None):\n    """"""Initializer.\n    Args:\n      embedding: A callable that takes a vector tensor of `ids` (argmax ids),\n        or the `params` argument for `embedding_lookup`. The returned tensor\n        will be passed to the decoder input.\n      start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n      end_token: `int32` scalar, the token that marks end of decoding.\n      softmax_temperature: (Optional) `float32` scalar, value to divide the\n        logits by before computing the softmax. Larger values (above 1.0) result\n        in more random samples, while smaller values push the sampling\n        distribution towards the argmax. Must be strictly greater than 0.\n        Defaults to 1.0.\n      seed: (Optional) The sampling seed.\n    Raises:\n      ValueError: if `start_tokens` is not a 1D tensor or `end_token` is not a\n        scalar.\n    """"""\n    super(SampleEmbeddingHelper, self).__init__(\n        embedding, start_tokens, end_token)\n    self._softmax_temperature = softmax_temperature\n    self._seed = seed\n\n  def sample(self, time, outputs, state, name=None):\n    """"""sample for SampleEmbeddingHelper.""""""\n    del time, state  # unused by sample_fn\n    # Outputs are logits, we sample instead of argmax (greedy).\n    if not isinstance(outputs, ops.Tensor):\n      raise TypeError(""Expected outputs to be a single Tensor, got: %s"" %\n                      type(outputs))\n    if self._softmax_temperature is None:\n      logits = outputs\n    else:\n      logits = outputs / self._softmax_temperature\n\n    sample_id_sampler = categorical.Categorical(logits=logits)\n    sample_ids = sample_id_sampler.sample(seed=self._seed)\n\n    return sample_ids\n\n\nclass InferenceHelper(Helper):\n  """"""A helper to use during inference with a custom sampling function.""""""\n\n  def __init__(self, sample_fn, sample_shape, sample_dtype,\n               start_inputs, end_fn, next_inputs_fn=None):\n    """"""Initializer.\n    Args:\n      sample_fn: A callable that takes `outputs` and emits tensor `sample_ids`.\n      sample_shape: Either a list of integers, or a 1-D Tensor of type `int32`,\n        the shape of the each sample in the batch returned by `sample_fn`.\n      sample_dtype: the dtype of the sample returned by `sample_fn`.\n      start_inputs: The initial batch of inputs.\n      end_fn: A callable that takes `sample_ids` and emits a `bool` vector\n        shaped `[batch_size]` indicating whether each sample is an end token.\n      next_inputs_fn: (Optional) A callable that takes `sample_ids` and returns\n        the next batch of inputs. If not provided, `sample_ids` is used as the\n        next batch of inputs.\n    """"""\n    self._sample_fn = sample_fn\n    self._end_fn = end_fn\n    self._sample_shape = tensor_shape.TensorShape(sample_shape)\n    self._sample_dtype = sample_dtype\n    self._next_inputs_fn = next_inputs_fn\n    self._batch_size = array_ops.shape(start_inputs)[0]\n    self._start_inputs = ops.convert_to_tensor(\n        start_inputs, name=""start_inputs"")\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return self._sample_shape\n\n  @property\n  def sample_ids_dtype(self):\n    return self._sample_dtype\n\n  def initialize(self, name=None):\n    finished = array_ops.tile([False], [self._batch_size])\n    return (finished, self._start_inputs)\n\n  def sample(self, time, outputs, state, name=None):\n    del time, state  # unused by sample\n    return self._sample_fn(outputs)\n\n  def next_inputs(self, time, outputs, state, sample_ids, name=None):\n    del time, outputs  # unused by next_inputs\n    if self._next_inputs_fn is None:\n      next_inputs = sample_ids\n    else:\n      next_inputs = self._next_inputs_fn(sample_ids)\n    finished = self._end_fn(sample_ids)\n    return (finished, next_inputs, state)\n'"
open_seq2seq/parts/rnns/rnn_beam_search_decoder.py,16,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A decoder that performs beam search.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nimport collections\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.contrib.seq2seq.python.ops import beam_search_ops\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.layers import base as layers_base\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.util import nest\n\n__all__ = [\n    ""BeamSearchDecoderOutput"",\n    ""BeamSearchDecoderState"",\n    ""BeamSearchDecoder"",\n    ""FinalBeamSearchDecoderOutput"",\n    ""tile_batch"",\n]\n\n\nclass BeamSearchDecoderState(\n    collections.namedtuple(""BeamSearchDecoderState"",\n                           (""cell_state"", ""log_probs"", ""finished"", ""lengths""))):\n  pass\n\n\nclass BeamSearchDecoderOutput(\n    collections.namedtuple(""BeamSearchDecoderOutput"",\n                           (""scores"", ""predicted_ids"", ""parent_ids""))):\n  pass\n\n\nclass FinalBeamSearchDecoderOutput(\n    collections.namedtuple(""FinalBeamDecoderOutput"",\n                           [""predicted_ids"", ""beam_search_decoder_output""])):\n  """"""Final outputs returned by the beam search after all decoding is finished.\n\n  Args:\n    predicted_ids: The final prediction. A tensor of shape\n      `[batch_size, T, beam_width]` (or `[T, batch_size, beam_width]` if\n      `output_time_major` is True). Beams are ordered from best to worst.\n    beam_search_decoder_output: An instance of `BeamSearchDecoderOutput` that\n      describes the state of the beam search.\n  """"""\n  pass\n\n\ndef _tile_batch(t, multiplier):\n  """"""Core single-tensor implementation of tile_batch.""""""\n  t = ops.convert_to_tensor(t, name=""t"")\n  shape_t = array_ops.shape(t)\n  if t.shape.ndims is None or t.shape.ndims < 1:\n    raise ValueError(""t must have statically known rank"")\n  tiling = [1] * (t.shape.ndims + 1)\n  tiling[1] = multiplier\n  tiled_static_batch_size = (\n      t.shape[0].value * multiplier if t.shape[0].value is not None else None)\n  tiled = array_ops.tile(array_ops.expand_dims(t, 1), tiling)\n  tiled = array_ops.reshape(tiled,\n                            array_ops.concat(\n                                ([shape_t[0] * multiplier], shape_t[1:]), 0))\n  tiled.set_shape(\n      tensor_shape.TensorShape([tiled_static_batch_size]).concatenate(\n          t.shape[1:]))\n  return tiled\n\n\ndef tile_batch(t, multiplier, name=None):\n  """"""Tile the batch dimension of a (possibly nested structure of) tensor(s) t.\n\n  For each tensor t in a (possibly nested structure) of tensors,\n  this function takes a tensor t shaped `[batch_size, s0, s1, ...]` composed of\n  minibatch entries `t[0], ..., t[batch_size - 1]` and tiles it to have a shape\n  `[batch_size * multiplier, s0, s1, ...]` composed of minibatch entries\n  `t[0], t[0], ..., t[1], t[1], ...` where each minibatch entry is repeated\n  `multiplier` times.\n\n  Args:\n    t: `Tensor` shaped `[batch_size, ...]`.\n    multiplier: Python int.\n    name: Name scope for any created operations.\n\n  Returns:\n    A (possibly nested structure of) `Tensor` shaped\n    `[batch_size * multiplier, ...]`.\n\n  Raises:\n    ValueError: if tensor(s) `t` do not have a statically known rank or\n    the rank is < 1.\n  """"""\n  flat_t = nest.flatten(t)\n  with ops.name_scope(name, ""tile_batch"", flat_t + [multiplier]):\n    return nest.map_structure(lambda t_: _tile_batch(t_, multiplier), t)\n\n\ndef _check_maybe(t):\n  if isinstance(t, tensor_array_ops.TensorArray):\n    raise TypeError(\n        ""TensorArray state is not supported by BeamSearchDecoder: %s"" % t.name)\n  if t.shape.ndims is None:\n    raise ValueError(\n        ""Expected tensor (%s) to have known rank, but ndims == None."" % t)\n\n\nclass BeamSearchDecoder(decoder.Decoder):\n  """"""BeamSearch sampling decoder.\n\n    **NOTE** If you are using the `BeamSearchDecoder` with a cell wrapped in\n    `AttentionWrapper`, then you must ensure that:\n\n    - The encoder output has been tiled to `beam_width` via\n      @{tf.contrib.seq2seq.tile_batch} (NOT `tf.tile`).\n    - The `batch_size` argument passed to the `zero_state` method of this\n      wrapper is equal to `true_batch_size * beam_width`.\n    - The initial state created with `zero_state` above contains a\n      `cell_state` value containing properly tiled final state from the\n      encoder.\n\n    An example:\n\n    ```\n    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n        encoder_outputs, multiplier=beam_width)\n    tiled_encoder_final_state = tf.conrib.seq2seq.tile_batch(\n        encoder_final_state, multiplier=beam_width)\n    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(\n        sequence_length, multiplier=beam_width)\n    attention_mechanism = MyFavoriteAttentionMechanism(\n        num_units=attention_depth,\n        memory=tiled_inputs,\n        memory_sequence_length=tiled_sequence_length)\n    attention_cell = AttentionWrapper(cell, attention_mechanism, ...)\n    decoder_initial_state = attention_cell.zero_state(\n        dtype, batch_size=true_batch_size * beam_width)\n    decoder_initial_state = decoder_initial_state.clone(\n        cell_state=tiled_encoder_final_state)\n    ```\n  """"""\n\n  def __init__(self,\n               cell,\n               embedding,\n               start_tokens,\n               end_token,\n               initial_state,\n               beam_width,\n               output_layer=None,\n               length_penalty_weight=0.0,\n               positional_embedding=None):\n    """"""Initialize the BeamSearchDecoder.\n\n    Args:\n      cell: An `RNNCell` instance.\n      embedding: A callable that takes a vector tensor of `ids` (argmax ids),\n        or the `params` argument for `embedding_lookup`.\n      start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n      end_token: `int32` scalar, the token that marks end of decoding.\n      initial_state: A (possibly nested tuple of...) tensors and TensorArrays.\n      beam_width:  Python integer, the number of beams.\n      output_layer: (Optional) An instance of `tf.layers.Layer`, i.e.,\n        `tf.layers.Dense`.  Optional layer to apply to the RNN output prior\n        to storing the result or sampling.\n      length_penalty_weight: Float weight to penalize length. Disabled with 0.0.\n      positional_embedding: A callable to use decoder positional embedding.\n      Default is None in which case positional embedding is disabled\n\n    Raises:\n      TypeError: if `cell` is not an instance of `RNNCell`,\n        or `output_layer` is not an instance of `tf.layers.Layer`.\n      ValueError: If `start_tokens` is not a vector or\n        `end_token` is not a scalar.\n    """"""\n    rnn_cell_impl.assert_like_rnncell(""cell"", cell)\n    if (output_layer is not None and\n            not isinstance(output_layer, layers_base.Layer)):\n      raise TypeError(\n          ""output_layer must be a Layer, received: %s"" % type(output_layer))\n    self._cell = cell\n    self._output_layer = output_layer\n\n    if callable(embedding):\n      self._embedding_fn = embedding\n    else:\n      self._embedding_fn = (\n          lambda ids: embedding_ops.embedding_lookup(embedding, ids))\n\n    self._use_pos_embedding = False\n    if positional_embedding is not None:\n      if callable(positional_embedding):\n        self._pos_embedding_fn = positional_embedding\n      else:\n        self._pos_embedding_fn = (\n            lambda ids: embedding_ops.embedding_lookup(positional_embedding, ids))\n      self._use_pos_embedding = True\n\n    self._start_tokens = ops.convert_to_tensor(\n        start_tokens, dtype=dtypes.int32, name=""start_tokens"")\n    if self._start_tokens.get_shape().ndims != 1:\n      raise ValueError(""start_tokens must be a vector"")\n    self._end_token = ops.convert_to_tensor(\n        end_token, dtype=dtypes.int32, name=""end_token"")\n    if self._end_token.get_shape().ndims != 0:\n      raise ValueError(""end_token must be a scalar"")\n\n    self._batch_size = array_ops.size(start_tokens)\n    self._beam_width = beam_width\n    self._length_penalty_weight = length_penalty_weight\n    self._initial_cell_state = nest.map_structure(\n        self._maybe_split_batch_beams, initial_state, self._cell.state_size)\n    self._start_tokens = array_ops.tile(\n        array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])\n    self._start_inputs = self._embedding_fn(self._start_tokens)\n\n    if self._use_pos_embedding:\n      self._start_inputs += self._pos_embedding_fn(ops.convert_to_tensor(0))\n\n    self._finished = array_ops.one_hot(\n        array_ops.zeros([self._batch_size], dtype=dtypes.int32),\n        depth=self._beam_width,\n        on_value=False,\n        off_value=True,\n        dtype=dtypes.bool)\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  def _rnn_output_size(self):\n    size = self._cell.output_size\n    if self._output_layer is None:\n      return size\n    else:\n      # To use layer\'s compute_output_shape, we need to convert the\n      # RNNCell\'s output_size entries into shapes with an unknown\n      # batch size.  We then pass this through the layer\'s\n      # compute_output_shape and read off all but the first (batch)\n      # dimensions to get the output size of the rnn with the layer\n      # applied to the top.\n      output_shape_with_unknown_batch = nest.map_structure(\n          lambda s: tensor_shape.TensorShape([None]).concatenate(s), size)\n      layer_output_shape = self._output_layer.compute_output_shape(\n          output_shape_with_unknown_batch)\n      return nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n  @property\n  def tracks_own_finished(self):\n    """"""The BeamSearchDecoder shuffles its beams and their finished state.\n\n    For this reason, it conflicts with the `dynamic_decode` function\'s\n    tracking of finished states.  Setting this property to true avoids\n    early stopping of decoding due to mismanagement of the finished state\n    in `dynamic_decode`.\n\n    Returns:\n      `True`.\n    """"""\n    return True\n\n  @property\n  def output_size(self):\n    # Return the cell output and the id\n    return BeamSearchDecoderOutput(\n        scores=tensor_shape.TensorShape([self._beam_width]),\n        predicted_ids=tensor_shape.TensorShape([self._beam_width]),\n        parent_ids=tensor_shape.TensorShape([self._beam_width]))\n\n  @property\n  def output_dtype(self):\n    # Assume the dtype of the cell is the output_size structure\n    # containing the input_state\'s first component\'s dtype.\n    # Return that structure and int32 (the id)\n    dtype = nest.flatten(self._initial_cell_state)[0].dtype\n    return BeamSearchDecoderOutput(\n        scores=nest.map_structure(lambda _: dtype, self._rnn_output_size()),\n        predicted_ids=dtypes.int32,\n        parent_ids=dtypes.int32)\n\n  def initialize(self, name=None):\n    """"""Initialize the decoder.\n\n    Args:\n      name: Name scope for any created operations.\n\n    Returns:\n      `(finished, start_inputs, initial_state)`.\n    """"""\n    finished, start_inputs = self._finished, self._start_inputs\n    dtype = nest.flatten(self._initial_cell_state)[0].dtype\n    log_probs = array_ops.one_hot(  # shape(batch_sz, beam_sz)\n        array_ops.zeros([self._batch_size], dtype=dtypes.int32),\n        depth=self._beam_width,\n        on_value=math_ops.cast(0.0, dtype),\n        off_value=-np.float16(\'inf\') if dtype == dtypes.float16 else -np.Inf,\n        dtype=dtype)\n\n    initial_state = BeamSearchDecoderState(\n        cell_state=self._initial_cell_state,\n        log_probs=log_probs,\n        finished=finished,\n        lengths=array_ops.zeros(\n            [self._batch_size, self._beam_width], dtype=dtypes.int64))\n\n    return (finished, start_inputs, initial_state)\n\n  def finalize(self, outputs, final_state, sequence_lengths):\n    """"""Finalize and return the predicted_ids.\n\n    Args:\n      outputs: An instance of BeamSearchDecoderOutput.\n      final_state: An instance of BeamSearchDecoderState. Passed through to the\n        output.\n      sequence_lengths: An `int64` tensor shaped `[batch_size, beam_width]`.\n        The sequence lengths determined for each beam during decode.\n        **NOTE** These are ignored; the updated sequence lengths are stored in\n        `final_state.lengths`.\n\n    Returns:\n      outputs: An instance of `FinalBeamSearchDecoderOutput` where the\n        predicted_ids are the result of calling _gather_tree.\n      final_state: The same input instance of `BeamSearchDecoderState`.\n    """"""\n    del sequence_lengths\n    # Get max_sequence_length across all beams for each batch.\n    max_sequence_lengths = tf.cast(\n        math_ops.reduce_max(final_state.lengths, axis=1),tf.int32)\n    predicted_ids = beam_search_ops.gather_tree(\n        outputs.predicted_ids,\n        outputs.parent_ids,\n        max_sequence_lengths=max_sequence_lengths,\n        end_token=self._end_token)\n    outputs = FinalBeamSearchDecoderOutput(\n        beam_search_decoder_output=outputs, predicted_ids=predicted_ids)\n    return outputs, final_state\n\n  def _merge_batch_beams(self, t, s=None):\n    """"""Merges the tensor from a batch of beams into a batch by beams.\n\n    More exactly, t is a tensor of dimension [batch_size, beam_width, s]. We\n    reshape this into [batch_size*beam_width, s]\n\n    Args:\n      t: Tensor of dimension [batch_size, beam_width, s]\n      s: (Possibly known) depth shape.\n\n    Returns:\n      A reshaped version of t with dimension [batch_size * beam_width, s].\n    """"""\n    if isinstance(s, ops.Tensor):\n      s = tensor_shape.as_shape(tensor_util.constant_value(s))\n    else:\n      s = tensor_shape.TensorShape(s)\n    t_shape = array_ops.shape(t)\n    static_batch_size = tensor_util.constant_value(self._batch_size)\n    batch_size_beam_width = (\n        None\n        if static_batch_size is None else static_batch_size * self._beam_width)\n    reshaped_t = array_ops.reshape(\n        t,\n        array_ops.concat(([self._batch_size * self._beam_width], t_shape[2:]),\n                         0))\n    reshaped_t.set_shape(\n        (tensor_shape.TensorShape([batch_size_beam_width]).concatenate(s)))\n    return reshaped_t\n\n  def _split_batch_beams(self, t, s=None):\n    """"""Splits the tensor from a batch by beams into a batch of beams.\n\n    More exactly, t is a tensor of dimension [batch_size*beam_width, s]. We\n    reshape this into [batch_size, beam_width, s]\n\n    Args:\n      t: Tensor of dimension [batch_size*beam_width, s].\n      s: (Possibly known) depth shape.\n\n    Returns:\n      A reshaped version of t with dimension [batch_size, beam_width, s].\n\n    Raises:\n      ValueError: If, after reshaping, the new tensor is not shaped\n        `[batch_size, beam_width, s]` (assuming batch_size and beam_width\n        are known statically).\n    """"""\n    if isinstance(s, ops.Tensor):\n      s = tensor_shape.TensorShape(tensor_util.constant_value(s))\n    else:\n      s = tensor_shape.TensorShape(s)\n    t_shape = array_ops.shape(t)\n    reshaped_t = array_ops.reshape(\n        t,\n        array_ops.concat(([self._batch_size, self._beam_width], t_shape[1:]),\n                         0))\n    static_batch_size = tensor_util.constant_value(self._batch_size)\n    expected_reshaped_shape = tensor_shape.TensorShape(\n        [static_batch_size, self._beam_width]).concatenate(s)\n    if not reshaped_t.shape.is_compatible_with(expected_reshaped_shape):\n      raise ValueError(""Unexpected behavior when reshaping between beam width ""\n                       ""and batch size.  The reshaped tensor has shape: %s.  ""\n                       ""We expected it to have shape ""\n                       ""(batch_size, beam_width, depth) == %s.  Perhaps you ""\n                       ""forgot to create a zero_state with ""\n                       ""batch_size=encoder_batch_size * beam_width?"" %\n                       (reshaped_t.shape, expected_reshaped_shape))\n    reshaped_t.set_shape(expected_reshaped_shape)\n    return reshaped_t\n\n  def _maybe_split_batch_beams(self, t, s):\n    """"""Maybe splits the tensor from a batch by beams into a batch of beams.\n\n    We do this so that we can use nest and not run into problems with shapes.\n\n    Args:\n      t: `Tensor`, either scalar or shaped `[batch_size * beam_width] + s`.\n      s: `Tensor`, Python int, or `TensorShape`.\n\n    Returns:\n      If `t` is a matrix or higher order tensor, then the return value is\n      `t` reshaped to `[batch_size, beam_width] + s`.  Otherwise `t` is\n      returned unchanged.\n\n    Raises:\n      TypeError: If `t` is an instance of `TensorArray`.\n      ValueError: If the rank of `t` is not statically known.\n    """"""\n    _check_maybe(t)\n    if t.shape.ndims >= 1:\n      return self._split_batch_beams(t, s)\n    else:\n      return t\n\n  def _maybe_merge_batch_beams(self, t, s):\n    """"""Splits the tensor from a batch by beams into a batch of beams.\n\n    More exactly, `t` is a tensor of dimension `[batch_size * beam_width] + s`,\n    then we reshape it to `[batch_size, beam_width] + s`.\n\n    Args:\n      t: `Tensor` of dimension `[batch_size * beam_width] + s`.\n      s: `Tensor`, Python int, or `TensorShape`.\n\n    Returns:\n      A reshaped version of t with shape `[batch_size, beam_width] + s`.\n\n    Raises:\n      TypeError: If `t` is an instance of `TensorArray`.\n      ValueError:  If the rank of `t` is not statically known.\n    """"""\n    _check_maybe(t)\n    if t.shape.ndims >= 2:\n      return self._merge_batch_beams(t, s)\n    else:\n      return t\n\n  def step(self, time, inputs, state, name=None):\n    """"""Perform a decoding step.\n\n    Args:\n      time: scalar `int32` tensor.\n      inputs: A (structure of) input tensors.\n      state: A (structure of) state tensors and TensorArrays.\n      name: Name scope for any created operations.\n\n    Returns:\n      `(outputs, next_state, next_inputs, finished)`.\n    """"""\n    batch_size = self._batch_size\n    beam_width = self._beam_width\n    end_token = self._end_token\n    length_penalty_weight = self._length_penalty_weight\n\n    with ops.name_scope(name, ""BeamSearchDecoderStep"", (time, inputs, state)):\n      cell_state = state.cell_state\n      inputs = nest.map_structure(\n          lambda inp: self._merge_batch_beams(inp, s=inp.shape[2:]), inputs)\n      cell_state = nest.map_structure(self._maybe_merge_batch_beams, cell_state,\n                                      self._cell.state_size)\n      cell_outputs, next_cell_state = self._cell(inputs, cell_state)\n      cell_outputs = nest.map_structure(\n          lambda out: self._split_batch_beams(out, out.shape[1:]), cell_outputs)\n      next_cell_state = nest.map_structure(\n          self._maybe_split_batch_beams, next_cell_state, self._cell.state_size)\n\n      if self._output_layer is not None:\n        cell_outputs = self._output_layer(cell_outputs)\n\n      beam_search_output, beam_search_state = _beam_search_step(\n          time=time,\n          logits=cell_outputs,\n          next_cell_state=next_cell_state,\n          beam_state=state,\n          batch_size=batch_size,\n          beam_width=beam_width,\n          end_token=end_token,\n          length_penalty_weight=length_penalty_weight)\n\n      finished = beam_search_state.finished\n      sample_ids = beam_search_output.predicted_ids\n      next_inputs = control_flow_ops.cond(\n          math_ops.reduce_all(finished), lambda: self._start_inputs,\n          lambda: self._embedding_fn(sample_ids))\n      if self._use_pos_embedding:\n        next_inputs += self._pos_embedding_fn(ops.convert_to_tensor(time))\n\n    return (beam_search_output, beam_search_state, next_inputs, finished)\n\n\ndef _beam_search_step(time, logits, next_cell_state, beam_state, batch_size,\n                      beam_width, end_token, length_penalty_weight):\n  """"""Performs a single step of Beam Search Decoding.\n\n  Args:\n    time: Beam search time step, should start at 0. At time 0 we assume\n      that all beams are equal and consider only the first beam for\n      continuations.\n    logits: Logits at the current time step. A tensor of shape\n      `[batch_size, beam_width, vocab_size]`\n    next_cell_state: The next state from the cell, e.g. an instance of\n      AttentionWrapperState if the cell is attentional.\n    beam_state: Current state of the beam search.\n      An instance of `BeamSearchDecoderState`.\n    batch_size: The batch size for this input.\n    beam_width: Python int.  The size of the beams.\n    end_token: The int32 end token.\n    length_penalty_weight: Float weight to penalize length. Disabled with 0.0.\n\n  Returns:\n    A new beam state.\n  """"""\n  static_batch_size = tensor_util.constant_value(batch_size)\n\n  # Calculate the current lengths of the predictions\n  prediction_lengths = beam_state.lengths\n  previously_finished = beam_state.finished\n\n  # Calculate the total log probs for the new hypotheses\n  # Final Shape: [batch_size, beam_width, vocab_size]\n  step_log_probs = nn_ops.log_softmax(logits)\n  step_log_probs = _mask_probs(step_log_probs, end_token, previously_finished)\n  total_probs = array_ops.expand_dims(beam_state.log_probs, 2) + step_log_probs\n\n  # Calculate the continuation lengths by adding to all continuing beams.\n  vocab_size = logits.shape[-1].value or array_ops.shape(logits)[-1]\n  lengths_to_add = array_ops.one_hot(\n      indices=array_ops.fill([batch_size, beam_width], end_token),\n      depth=vocab_size,\n      on_value=np.int64(0),\n      off_value=np.int64(1),\n      dtype=dtypes.int64)\n  add_mask = tf.cast(math_ops.logical_not(previously_finished), tf.int64)\n  lengths_to_add *= array_ops.expand_dims(add_mask, 2)\n  new_prediction_lengths = (\n      lengths_to_add + array_ops.expand_dims(prediction_lengths, 2))\n\n  # Calculate the scores for each beam\n  scores = _get_scores(\n      log_probs=total_probs,\n      sequence_lengths=new_prediction_lengths,\n      length_penalty_weight=length_penalty_weight,\n      dtype=logits.dtype)\n\n  time = ops.convert_to_tensor(time, name=""time"")\n  # During the first time step we only consider the initial beam\n  scores_shape = array_ops.shape(scores)\n  scores_flat = array_ops.reshape(scores, [batch_size, -1])\n\n  # Pick the next beams according to the specified successors function\n  next_beam_size = ops.convert_to_tensor(\n      beam_width, dtype=dtypes.int32, name=""beam_width"")\n  next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=next_beam_size)\n\n  next_beam_scores.set_shape([static_batch_size, beam_width])\n  word_indices.set_shape([static_batch_size, beam_width])\n\n  # Pick out the probs, beam_ids, and states according to the chosen\n  # predictions\n  next_beam_probs = _tensor_gather_helper(\n      gather_indices=word_indices,\n      gather_from=total_probs,\n      batch_size=batch_size,\n      range_size=beam_width * vocab_size,\n      gather_shape=[-1],\n      name=""next_beam_probs"")\n  # Note: just doing the following\n  #   math_ops.to_int32(word_indices % vocab_size,\n  #       name=""next_beam_word_ids"")\n  # would be a lot cleaner but for reasons unclear, that hides the results of\n  # the op which prevents capturing it with tfdbg debug ops.\n  raw_next_word_ids = math_ops.mod(\n      word_indices, vocab_size, name=""next_beam_word_ids"")\n  next_word_ids = tf.cast(raw_next_word_ids, tf.int32)\n  next_beam_ids = tf.cast(word_indices / vocab_size,\n                          name=""next_beam_parent_ids"", dtype=tf.int32)\n  # Append new ids to current predictions\n  previously_finished = _tensor_gather_helper(\n      gather_indices=next_beam_ids,\n      gather_from=previously_finished,\n      batch_size=batch_size,\n      range_size=beam_width,\n      gather_shape=[-1])\n  next_finished = math_ops.logical_or(\n      previously_finished,\n      math_ops.equal(next_word_ids, end_token),\n      name=""next_beam_finished"")\n\n  # Calculate the length of the next predictions.\n  # 1. Finished beams remain unchanged.\n  # 2. Beams that are now finished (EOS predicted) have their length\n  #    increased by 1.\n  # 3. Beams that are not yet finished have their length increased by 1.\n  lengths_to_add = tf.cast(math_ops.logical_not(previously_finished), tf.int64)\n  next_prediction_len = _tensor_gather_helper(\n      gather_indices=next_beam_ids,\n      gather_from=beam_state.lengths,\n      batch_size=batch_size,\n      range_size=beam_width,\n      gather_shape=[-1])\n  next_prediction_len += lengths_to_add\n\n  # Pick out the cell_states according to the next_beam_ids. We use a\n  # different gather_shape here because the cell_state tensors, i.e.\n  # the tensors that would be gathered from, all have dimension\n  # greater than two and we need to preserve those dimensions.\n  # pylint: disable=g-long-lambda\n  next_cell_state = nest.map_structure(\n      lambda gather_from: _maybe_tensor_gather_helper(\n          gather_indices=next_beam_ids,\n          gather_from=gather_from,\n          batch_size=batch_size,\n          range_size=beam_width,\n          gather_shape=[batch_size * beam_width, -1]),\n      next_cell_state)\n  # pylint: enable=g-long-lambda\n\n  next_state = BeamSearchDecoderState(\n      cell_state=next_cell_state,\n      log_probs=next_beam_probs,\n      lengths=next_prediction_len,\n      finished=next_finished)\n\n  output = BeamSearchDecoderOutput(\n      scores=next_beam_scores,\n      predicted_ids=next_word_ids,\n      parent_ids=next_beam_ids)\n\n  return output, next_state\n\n\ndef _get_scores(log_probs, sequence_lengths, length_penalty_weight,\n                dtype=dtypes.float32):\n  """"""Calculates scores for beam search hypotheses.\n\n  Args:\n    log_probs: The log probabilities with shape\n      `[batch_size, beam_width, vocab_size]`.\n    sequence_lengths: The array of sequence lengths.\n    length_penalty_weight: Float weight to penalize length. Disabled with 0.0.\n\n  Returns:\n    The scores normalized by the length_penalty.\n  """"""\n  length_penality_ = _length_penalty(\n      sequence_lengths=sequence_lengths, penalty_factor=length_penalty_weight)\n  return log_probs / math_ops.cast(length_penality_, dtype)\n\n\ndef _length_penalty(sequence_lengths, penalty_factor):\n  """"""Calculates the length penalty. See https://arxiv.org/abs/1609.08144.\n\n  Returns the length penalty tensor:\n  ```\n  [(5+sequence_lengths)/6]**penalty_factor\n  ```\n  where all operations are performed element-wise.\n\n  Args:\n    sequence_lengths: `Tensor`, the sequence lengths of each hypotheses.\n    penalty_factor: A scalar that weights the length penalty.\n\n  Returns:\n    If the penalty is `0`, returns the scalar `1.0`.  Otherwise returns\n    the length penalty factor, a tensor with the same shape as\n    `sequence_lengths`.\n  """"""\n  penalty_factor = ops.convert_to_tensor(penalty_factor, name=""penalty_factor"")\n  penalty_factor.set_shape(())  # penalty should be a scalar.\n  static_penalty = tensor_util.constant_value(penalty_factor)\n  if static_penalty is not None and static_penalty == 0:\n    return 1.0\n  return math_ops.div((5. + math_ops.to_float(sequence_lengths))\n                      ** penalty_factor, (5. + 1.)**penalty_factor)\n\n\ndef _mask_probs(probs, eos_token, finished):\n  """"""Masks log probabilities.\n\n  The result is that finished beams allocate all probability mass to eos and\n  unfinished beams remain unchanged.\n\n  Args:\n    probs: Log probabiltiies of shape `[batch_size, beam_width, vocab_size]`\n    eos_token: An int32 id corresponding to the EOS token to allocate\n      probability to.\n    finished: A boolean tensor of shape `[batch_size, beam_width]` that\n      specifies which elements in the beam are finished already.\n\n  Returns:\n    A tensor of shape `[batch_size, beam_width, vocab_size]`, where unfinished\n    beams stay unchanged and finished beams are replaced with a tensor with all\n    probability on the EOS token.\n  """"""\n  vocab_size = array_ops.shape(probs)[2]\n  # All finished examples are replaced with a vector that has all\n  # probability on EOS\n  finished_row = array_ops.one_hot(\n      eos_token,\n      vocab_size,\n      dtype=probs.dtype,\n      on_value=ops.convert_to_tensor(0., dtype=probs.dtype),\n      off_value=probs.dtype.min)\n  finished_probs = array_ops.tile(\n      array_ops.reshape(finished_row, [1, 1, -1]),\n      array_ops.concat([array_ops.shape(finished), [1]], 0))\n  finished_mask = array_ops.tile(\n      array_ops.expand_dims(finished, 2), [1, 1, vocab_size])\n\n  return array_ops.where(finished_mask, finished_probs, probs)\n\n\ndef _maybe_tensor_gather_helper(gather_indices, gather_from, batch_size,\n                                range_size, gather_shape):\n  """"""Maybe applies _tensor_gather_helper.\n\n  This applies _tensor_gather_helper when the gather_from dims is at least as\n  big as the length of gather_shape. This is used in conjunction with nest so\n  that we don\'t apply _tensor_gather_helper to inapplicable values like scalars.\n\n  Args:\n    gather_indices: The tensor indices that we use to gather.\n    gather_from: The tensor that we are gathering from.\n    batch_size: The batch size.\n    range_size: The number of values in each range. Likely equal to beam_width.\n    gather_shape: What we should reshape gather_from to in order to preserve the\n      correct values. An example is when gather_from is the attention from an\n      AttentionWrapperState with shape [batch_size, beam_width, attention_size].\n      There, we want to preserve the attention_size elements, so gather_shape is\n      [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n      attention_size as desired.\n\n  Returns:\n    output: Gathered tensor of shape tf.shape(gather_from)[:1+len(gather_shape)]\n      or the original tensor if its dimensions are too small.\n  """"""\n  _check_maybe(gather_from)\n  if gather_from.shape.ndims >= len(gather_shape):\n    return _tensor_gather_helper(\n        gather_indices=gather_indices,\n        gather_from=gather_from,\n        batch_size=batch_size,\n        range_size=range_size,\n        gather_shape=gather_shape)\n  else:\n    return gather_from\n\n\ndef _tensor_gather_helper(gather_indices,\n                          gather_from,\n                          batch_size,\n                          range_size,\n                          gather_shape,\n                          name=None):\n  """"""Helper for gathering the right indices from the tensor.\n\n  This works by reshaping gather_from to gather_shape (e.g. [-1]) and then\n  gathering from that according to the gather_indices, which are offset by\n  the right amounts in order to preserve the batch order.\n\n  Args:\n    gather_indices: The tensor indices that we use to gather.\n    gather_from: The tensor that we are gathering from.\n    batch_size: The input batch size.\n    range_size: The number of values in each range. Likely equal to beam_width.\n    gather_shape: What we should reshape gather_from to in order to preserve the\n      correct values. An example is when gather_from is the attention from an\n      AttentionWrapperState with shape [batch_size, beam_width, attention_size].\n      There, we want to preserve the attention_size elements, so gather_shape is\n      [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n      attention_size as desired.\n    name: The tensor name for set of operations. By default this is\n      \'tensor_gather_helper\'. The final output is named \'output\'.\n\n  Returns:\n    output: Gathered tensor of shape tf.shape(gather_from)[:1+len(gather_shape)]\n  """"""\n  with ops.name_scope(name, ""tensor_gather_helper""):\n    range_ = array_ops.expand_dims(math_ops.range(batch_size) * range_size, 1)\n    gather_indices = array_ops.reshape(gather_indices + range_, [-1])\n    output = array_ops.gather(\n        array_ops.reshape(gather_from, gather_shape), gather_indices)\n    final_shape = array_ops.shape(gather_from)[:1 + len(gather_shape)]\n    static_batch_size = tensor_util.constant_value(batch_size)\n    final_static_shape = (\n        tensor_shape.TensorShape([static_batch_size]).concatenate(\n            gather_from.shape[1:1 + len(gather_shape)]))\n    output = array_ops.reshape(output, final_shape, name=""output"")\n    output.set_shape(final_static_shape)\n    return output\n'"
open_seq2seq/parts/rnns/slstm.py,2,"b'""""""Implement https://arxiv.org/abs/1709.02755\n\nCopy from LSTM, and make it functionally correct with minimum code change\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n_BIAS_VARIABLE_NAME = ""biases"" if tf.__version__ < ""1.2.0"" else ""bias""\n_WEIGHTS_VARIABLE_NAME = ""weights"" if tf.__version__ < ""1.2.0"" else ""kernel""\n\n\n# TODO: must implement all abstract methods\nclass BasicSLSTMCell(rnn_cell.RNNCell):\n  """"""Basic SLSTM recurrent network cell.\n\n  The implementation is based on: https://arxiv.org/abs/1709.02755.\n\n  """"""\n\n  def __init__(self, num_units, forget_bias=1.0,\n               state_is_tuple=True, activation=None, reuse=None):\n    """"""Initialize the basic SLSTM cell.\n\n    Args:\n      num_units: int, The number of units in the SLSTM cell.\n      forget_bias: float, The bias added to forget gates (see above).\n        Must set to `0.0` manually when restoring from CudnnLSTM-trained\n        checkpoints.\n      state_is_tuple: If True, accepted and returned states are 2-tuples of\n        the `c_state` and `m_state`.  If False, they are concatenated\n        along the column axis.  The latter behavior will soon be deprecated.\n      activation: Activation function of the inner states.  Default: `tanh`.\n      reuse: (optional) Python boolean describing whether to reuse variables\n        in an existing scope.  If not `True`, and the existing scope already has\n        the given variables, an error is raised.\n\n    """"""\n    super(BasicSLSTMCell, self).__init__(_reuse=reuse)\n    if not state_is_tuple:\n      logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                   ""deprecated.  Use state_is_tuple=True."", self)\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._state_is_tuple = state_is_tuple\n    self._activation = activation or math_ops.tanh\n\n  @property\n  def state_size(self):\n    return (rnn_cell.LSTMStateTuple(self._num_units, self._num_units)\n            if self._state_is_tuple else 2 * self._num_units)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  # TODO: does not match signature of the base method\n  def call(self, inputs, state):\n    """"""Long short-term memory cell (LSTM).\n\n    Args:\n      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n      state: An `LSTMStateTuple` of state tensors, each shaped\n        `[batch_size x self.state_size]`, if `state_is_tuple` has been set to\n        `True`.  Otherwise, a `Tensor` shaped\n        `[batch_size x 2 * self.state_size]`.\n\n    Returns:\n      A pair containing the new hidden state, and the new state (either a\n        `LSTMStateTuple` or a concatenated state, depending on\n        `state_is_tuple`).\n    """"""\n    sigmoid = math_ops.sigmoid\n    # Parameters of gates are concatenated into one multiply for efficiency.\n    if self._state_is_tuple:\n      c, h = state\n    else:\n      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n\n    # concat = _linear([inputs, h], 4 * self._num_units, True)\n    concat = _linear(inputs, 4 * self._num_units, True)\n\n    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1)\n\n    new_c = (\n        c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))\n    new_h = self._activation(new_c) * sigmoid(o)\n\n    if self._state_is_tuple:\n      new_state = rnn_cell.LSTMStateTuple(new_c, new_h)\n    else:\n      new_state = array_ops.concat([new_c, new_h], 1)\n    return new_h, new_state\n\n\ndef _linear(args,\n            output_size,\n            bias,\n            bias_initializer=None,\n            kernel_initializer=None):\n  """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_initializer: starting value to initialize the bias\n      (default is all zeros).\n    kernel_initializer: starting value to initialize the weight.\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  """"""\n  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(""`args` must be specified"")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape() for a in args]\n  for shape in shapes:\n    if shape.ndims != 2:\n      raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)\n    if shape[1].value is None:\n      raise ValueError(""linear expects shape[1] to be provided for shape %s, ""\n                       ""but saw %s"" % (shape, shape[1]))\n    else:\n      total_arg_size += shape[1].value\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  scope = vs.get_variable_scope()\n  with vs.variable_scope(scope) as outer_scope:\n    weights = vs.get_variable(\n        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n        dtype=dtype,\n        initializer=kernel_initializer)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], weights)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n    if not bias:\n      return res\n    with vs.variable_scope(outer_scope) as inner_scope:\n      inner_scope.set_partitioner(None)\n      if bias_initializer is None:\n        bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n      biases = vs.get_variable(\n          _BIAS_VARIABLE_NAME, [output_size],\n          dtype=dtype,\n          initializer=bias_initializer)\n    return nn_ops.bias_add(res, biases)\n'"
open_seq2seq/parts/rnns/utils.py,1,"b'# Copyright (c) 2017 NVIDIA Corporation\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport math\n\nfrom six.moves import range\nimport tensorflow as tf\n\nfrom tensorflow.python.ops.rnn_cell import ResidualWrapper, DropoutWrapper\nfrom open_seq2seq.parts.rnns.weight_drop import WeightDropLayerNormBasicLSTMCell\nfrom open_seq2seq.parts.rnns.slstm import BasicSLSTMCell\nfrom open_seq2seq.parts.rnns.glstm import GLSTMCell\nfrom open_seq2seq.parts.rnns.zoneout import ZoneoutWrapper\n\n\ndef single_cell(\n    cell_class,\n    cell_params,\n    dp_input_keep_prob=1.0,\n    dp_output_keep_prob=1.0,\n    recurrent_keep_prob=1.0,\n    input_weight_keep_prob=1.0,\n    recurrent_weight_keep_prob=1.0,\n    weight_variational=False,\n    dropout_seed=None,\n    zoneout_prob=0.,\n    training=True,\n    residual_connections=False,\n    awd_initializer=False,\n    variational_recurrent=False, # in case they want to use DropoutWrapper\n    dtype=None,\n):\n  """"""Creates an instance of the rnn cell.\n     Such cell describes one step one layer and can include residual connection\n     and/or dropout\n\n     Args:\n      cell_class: Tensorflow RNN cell class\n      cell_params (dict): cell parameters\n      dp_input_keep_prob (float): (default: 1.0) input dropout keep\n        probability.\n      dp_output_keep_prob (float): (default: 1.0) output dropout keep\n        probability.\n      zoneout_prob(float): zoneout probability. Applying both zoneout and\n        droupout is currently not supported\n      residual_connections (bool): whether to add residual connection\n\n     Returns:\n       TF RNN instance\n  """"""\n  if awd_initializer:\n    val = 1.0/math.sqrt(cell_params[\'num_units\'])\n    cell_params[\'initializer\'] = tf.random_uniform_initializer(minval=-val, maxval=val)\n  if \'WeightDropLayerNormBasicLSTMCell\' in str(cell_class):\n    if recurrent_keep_prob < 1.0:\n      cell_params[\'recurrent_keep_prob\'] = recurrent_keep_prob\n    if input_weight_keep_prob < 1.0:\n      cell_params[\'input_weight_keep_prob\'] = input_weight_keep_prob\n    if recurrent_weight_keep_prob < 1.0:\n      cell_params[\'recurrent_weight_keep_prob\'] = recurrent_weight_keep_prob\n    if weight_variational:\n      cell_params[\'weight_variational\'] = weight_variational # which is basically True\n    if dropout_seed:\n      cell_params[\'dropout_seed\'] = dropout_seed\n\n\n  cell = cell_class(**cell_params)\n  if residual_connections:\n    cell = ResidualWrapper(cell)\n  if zoneout_prob > 0. and (\n      dp_input_keep_prob < 1.0 or dp_output_keep_prob < 1.0\n  ):\n    raise ValueError(\n        ""Currently applying both dropout and zoneout on the same cell.""\n        ""This is currently not supported.""\n    )\n  if dp_input_keep_prob != 1.0 or dp_output_keep_prob != 1.0 and training:\n    cell = DropoutWrapper(\n        cell,\n        input_keep_prob=dp_input_keep_prob,\n        output_keep_prob=dp_output_keep_prob,\n        variational_recurrent=variational_recurrent,\n        dtype=dtype,\n        seed=dropout_seed\n    )\n  if zoneout_prob > 0.:\n    cell = ZoneoutWrapper(cell, zoneout_prob, is_training=training)\n  return cell\n'"
open_seq2seq/parts/rnns/weight_drop.py,24,"b'import tensorflow as tf\n\nclass WeightDropLayerNormBasicLSTMCell(tf.contrib.rnn.RNNCell):\n  """"""LSTM unit with layer normalization, weight dropout, and recurrent dropout.\n  This is based on LSTM\'s standard implementation of LayerNormBasicLSTMCell.\n  This class adds layer normalization and recurrent dropout to a\n  basic LSTM unit. Layer normalization implementation is based on:\n    https://arxiv.org/abs/1607.06450.\n  ""Layer Normalization""\n  Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n  and is applied before the internal nonlinearities.\n  Recurrent dropout is base on:\n    https://arxiv.org/abs/1603.05118\n  ""Recurrent Dropout without Memory Loss""\n  Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.\n\n  Code is basd on TensorFlow\'s LayerNormBasicLSTMCell\n  """"""\n\n  def __init__(self,\n               num_units,\n               forget_bias=1.0,\n               activation=tf.tanh,\n               layer_norm=True,\n               norm_gain=1.0,\n               norm_shift=0.0,\n               recurrent_keep_prob=1.0,\n               input_weight_keep_prob=1.0,\n               recurrent_weight_keep_prob=1.0,\n               dropout_seed=None,\n               weight_variational=False,\n               reuse=None,\n               dtype=None):\n    """"""Initializes the basic LSTM cell.\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (see above).\n      input_size: Deprecated and unused.\n      activation: Activation function of the inner states.\n      layer_norm: If `True`, layer normalization will be applied.\n      norm_gain: float, The layer normalization gain initial value. If\n        `layer_norm` has been set to `False`, this argument will be ignored.\n      norm_shift: float, The layer normalization shift initial value. If\n        `layer_norm` has been set to `False`, this argument will be ignored.\n      input_weight_keep_prob: keep probablility for dropout of W \n                              (kernel used to multiply with the input tensor)\n      recurrent_weight_keep_prob: keep probablility for dropout of U\n                                 (kernel used to multiply with last hidden state tensor)\n      recurrent_keep_prob: keep probability for dropout\n                           when applying tanh for the input transform step\n      weight_variational: whether to keep the same weight dropout mask\n                          at every timestep. This feature is not yet implemented.\n      dropout_prob_seed: (optional) integer, the randomness seed.\n      reuse: (optional) Python boolean describing whether to reuse variables\n        in an existing scope.  If not `True`, and the existing scope already has\n        the given variables, an error is raised.\n    """"""\n    super(WeightDropLayerNormBasicLSTMCell, self).__init__(_reuse=reuse)\n\n    self._num_units = num_units\n    self._activation = activation\n    self._forget_bias = forget_bias\n    self._recurrent_keep_prob = recurrent_keep_prob\n    self._input_weight_keep_prob = input_weight_keep_prob\n    self._recurrent_weight_keep_prob = recurrent_weight_keep_prob\n    self._dropout_seed = dropout_seed\n    self._layer_norm = layer_norm\n    self._norm_gain = norm_gain\n    self._norm_shift = norm_shift\n    self._reuse = reuse\n    self._weight_variational = weight_variational\n    self._dtype = dtype\n\n    self._input_weight_noise = None\n    self._recurrent_weight_noise = None\n\n\n    if self._weight_variational:\n      if dtype is None:\n        raise ValueError(\n            ""When weight_variational=True, dtype must be provided"")\n\n  @property\n  def state_size(self):\n    return tf.contrib.rnn.LSTMStateTuple(self._num_units, self._num_units)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def _norm(self, inp, scope, dtype=tf.float32):\n    shape = inp.get_shape()[-1:]\n    gamma_init = tf.constant_initializer(self._norm_gain)\n    beta_init = tf.constant_initializer(self._norm_shift)\n    with tf.variable_scope(scope):\n      # Initialize beta and gamma for use by layer_norm.\n      tf.get_variable(""gamma"", shape=shape, initializer=gamma_init, dtype=dtype)\n      tf.get_variable(""beta"", shape=shape, initializer=beta_init, dtype=dtype)\n    normalized = tf.contrib.layers.layer_norm(inp, reuse=True, scope=scope)\n    return normalized\n\n  def _linear(self, args, inputs_shape, h_shape):\n    out_size = 4 * self._num_units\n    proj_size = args.get_shape()[-1]\n    dtype = args.dtype\n    weights = tf.get_variable(""kernel"", [proj_size, out_size], dtype=dtype)\n\n    w, u = tf.split(weights, [inputs_shape, h_shape], axis=0)\n\n    if self._should_drop(self._input_weight_keep_prob):\n      w = self._dropout(w, self._input_weight_noise, self._input_weight_keep_prob)\n    if self._should_drop(self._recurrent_weight_keep_prob):\n      u = self._dropout(u, self._recurrent_weight_noise, self._recurrent_weight_keep_prob)\n\n    weights = tf.concat([w, u], 0)\n\n    out = tf.matmul(args, weights)\n    if not self._layer_norm:\n      bias = tf.get_variable(""bias"", [out_size], dtype=dtype)\n      out = tf.nn.bias_add(out, bias)\n    return out\n\n  def _variational_dropout(self, values, noise, keep_prob):\n    \'\'\'\n    TODO: Implement variational dropout for weight dropout\n    \'\'\'\n    return tf.nn.dropout(values, keep_prob, seed=self._dropout_seed)\n\n  def _dropout(self, values, dropout_noise, keep_prob):\n    # when it gets in here, keep_prob < 1.0\n    if not self._weight_variational:\n      return tf.nn.dropout(values, keep_prob, seed=self._dropout_seed)\n    else:\n      return self._variational_dropout(values, dropout_noise, keep_prob)\n\n\n  def _should_drop(self, p):\n    return (not isinstance(p, float)) or p < 1\n\n  def call(self, inputs, state):\n    """"""LSTM cell with layer normalization and recurrent dropout.""""""\n    c, h = state\n    args = tf.concat([inputs, h], 1)\n    concat = self._linear(args, inputs.get_shape().as_list()[-1], h.get_shape().as_list()[-1])\n    dtype = args.dtype\n\n    i, j, f, o = tf.split(value=concat, num_or_size_splits=4, axis=1)\n\n    if self._layer_norm:\n      i = self._norm(i, ""input"", dtype=dtype)\n      j = self._norm(j, ""transform"", dtype=dtype)\n      f = self._norm(f, ""forget"", dtype=dtype)\n      o = self._norm(o, ""output"", dtype=dtype)\n\n    g = self._activation(j)\n    if self._should_drop(self._recurrent_keep_prob):\n      g = tf.nn.dropout(g, self._recurrent_keep_prob, seed=self._dropout_seed)      \n\n    new_c = (\n        c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) * g)\n    if self._layer_norm:\n      new_c = self._norm(new_c, ""state"", dtype=dtype)\n    new_h = self._activation(new_c) * tf.sigmoid(o)\n\n    new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n    return new_h, new_state'"
open_seq2seq/parts/rnns/zoneout.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom six.moves import range\n\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops.nn_ops import dropout\n\n\nclass ZoneoutWrapper(rnn_cell_impl.RNNCell):\n  """"""Operator adding zoneout to all states (states+cells) of the given cell.\n  Code taken from https://github.com/teganmaharaj/zoneout\n  applying zoneout as described in https://arxiv.org/pdf/1606.01305.pdf""""""\n\n  def __init__(self, cell, zoneout_prob, is_training=True, seed=None):\n    if not isinstance(cell, rnn_cell_impl.RNNCell):\n      raise TypeError(""The parameter cell is not an RNNCell."")\n    if (\n        isinstance(zoneout_prob, float) and\n        not (zoneout_prob >= 0.0 and zoneout_prob <= 1.0)\n    ):\n      raise ValueError(\n          ""Parameter zoneout_prob must be between 0 and 1: %d"" % zoneout_prob\n      )\n    self._cell = cell\n    self._zoneout_prob = (zoneout_prob, zoneout_prob)\n    self._seed = seed\n    self._is_training = is_training\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def __call__(self, inputs, state, scope=None):\n    if isinstance(self.state_size,\n                  tuple) != isinstance(self._zoneout_prob, tuple):\n      raise TypeError(""Subdivided states need subdivided zoneouts."")\n    if isinstance(self.state_size,\n                  tuple) and len(tuple(self.state_size)\n                                ) != len(tuple(self._zoneout_prob)):\n      raise ValueError(""State and zoneout need equally many parts."")\n    output, new_state = self._cell(inputs, state, scope)\n    if isinstance(self.state_size, tuple):\n      if self._is_training:\n        new_state = tuple(\n            (1 - state_part_zoneout_prob) * dropout(\n                new_state_part - state_part, (1 - state_part_zoneout_prob),\n                seed=self._seed\n            ) + state_part\n            for new_state_part, state_part, state_part_zoneout_prob in\n            zip(new_state, state, self._zoneout_prob)\n        )\n      else:\n        new_state = tuple(\n            state_part_zoneout_prob * state_part +\n            (1 - state_part_zoneout_prob) * new_state_part\n            for new_state_part, state_part, state_part_zoneout_prob in\n            zip(new_state, state, self._zoneout_prob)\n        )\n      new_state = rnn_cell_impl.LSTMStateTuple(new_state[0], new_state[1])\n    else:\n      raise ValueError(""Only states that are tuples are supported"")\n    return output, new_state\n'"
open_seq2seq/parts/tacotron/__init__.py,0,b''
open_seq2seq/parts/tacotron/tacotron_decoder.py,4,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""\nModified by blisc to enable support for tacotron models, specfically enables\nthe prenet\n""""""\n\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport collections\n\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.contrib.seq2seq.python.ops import helper as helper_py\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.layers import base as layers_base\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.util import nest\n\nclass BasicDecoderOutput(\n    collections.namedtuple(\n        ""BasicDecoderOutput"", (""rnn_output"", ""stop_token_output"")\n    )\n):\n  pass\n\n\nclass TacotronDecoder(decoder.Decoder):\n  """"""Basic sampling decoder.""""""\n\n  def __init__(\n      self,\n      decoder_cell,\n      helper,\n      initial_decoder_state,\n      attention_type,\n      spec_layer,\n      stop_token_layer,\n      prenet=None,\n      dtype=dtypes.float32,\n      train=True\n  ):\n    """"""Initialize TacotronDecoder.\n\n    Args:\n      decoder_cell: An `RNNCell` instance.\n      helper: A `Helper` instance.\n      initial_decoder_state: A (possibly nested tuple of...) tensors and\n        TensorArrays. The initial state of the RNNCell.\n      attention_type: The type of attention used\n      stop_token_layer: An instance of `tf.layers.Layer`, i.e.,\n        `tf.layers.Dense`. Stop token layer to apply to the RNN output to\n        predict when to stop the decoder\n      spec_layer: An instance of `tf.layers.Layer`, i.e.,\n        `tf.layers.Dense`. Output layer to apply to the RNN output to map\n        the ressult to a spectrogram\n      prenet: The prenet to apply to inputs\n\n    Raises:\n      TypeError: if `cell`, `helper` or `output_layer` have an incorrect type.\n    """"""\n    rnn_cell_impl.assert_like_rnncell(""cell"", decoder_cell)\n    if not isinstance(helper, helper_py.Helper):\n      raise TypeError(""helper must be a Helper, received: %s"" % type(helper))\n    if (\n        spec_layer is not None and\n        not isinstance(spec_layer, layers_base.Layer)\n    ):\n      raise TypeError(\n          ""spec_layer must be a Layer, received: %s"" % type(spec_layer)\n      )\n    self._decoder_cell = decoder_cell\n    self._helper = helper\n    self._decoder_initial_state = initial_decoder_state\n    self._spec_layer = spec_layer\n    self._stop_token_layer = stop_token_layer\n    self._attention_type = attention_type\n    self._dtype = dtype\n    self._prenet = prenet\n\n    if train:\n      self._spec_layer = None\n      self._stop_token_layer = None\n\n  @property\n  def batch_size(self):\n    return self._helper.batch_size\n\n  def _rnn_output_size(self):\n    size = self._decoder_cell.output_size\n    if self._spec_layer is None:\n      return size\n\n    output_shape_with_unknown_batch = nest.map_structure(\n        lambda s: tensor_shape.TensorShape([None]).concatenate(s), size\n    )\n    layer_output_shape = self._spec_layer.compute_output_shape(\n        output_shape_with_unknown_batch\n    )\n    return nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n  def _stop_token_output_size(self):\n    size = self._decoder_cell.output_size\n    if self._stop_token_layer is None:\n      return size\n\n    output_shape_with_unknown_batch = nest.map_structure(\n        lambda s: tensor_shape.TensorShape([None]).concatenate(s), size\n    )\n    layer_output_shape = self._stop_token_layer.compute_output_shape(\n        output_shape_with_unknown_batch\n    )\n    return nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n  @property\n  def output_size(self):\n    return BasicDecoderOutput(\n        rnn_output=self._rnn_output_size(),\n        stop_token_output=self._stop_token_output_size(),\n    )\n\n  @property\n  def output_dtype(self):\n    # dtype = nest.flatten(self._decoder_initial_state)[0].dtype\n    return BasicDecoderOutput(\n        nest.map_structure(lambda _: self._dtype, self._rnn_output_size()),\n        nest.map_structure(lambda _: self._dtype, self._stop_token_output_size()),\n    )\n\n  def initialize(self, name=None):\n    """"""Initialize the decoder.\n\n    Args:\n      name: Name scope for any created operations.\n    """"""\n    state = (self._decoder_initial_state, )\n    return self._helper.initialize() + state\n\n  def step(self, time, inputs, state, name=None):\n    """"""Perform a decoding step.\n\n    Args:\n      time: scalar `int32` tensor.\n      inputs: A (structure of) input tensors.\n      state: A (structure of) state tensors and TensorArrays.\n      name: Name scope for any created operations.\n\n    Returns:\n      `(outputs, next_state, next_inputs, finished)`.\n    """"""\n    with ops.name_scope(name, ""BasicDecoderStep"", (time, inputs, state)):\n      if self._prenet is not None:\n        inputs = self._prenet(inputs)\n\n      cell_outputs, cell_state = self._decoder_cell(inputs, state)\n\n      # If we are training and not using scheduled sampling, we can move\n      # all projection layers outside decoder,\n      # else we must project inside decoder\n      if self._spec_layer is not None:\n        spec_outputs = self._spec_layer(cell_outputs)\n      else:\n        spec_outputs = cell_outputs\n      if self._stop_token_layer is not None:\n        stop_token_output = self._stop_token_layer(spec_outputs)\n      else:\n        stop_token_output = cell_outputs\n\n      (finished, next_inputs, next_state) = self._helper.next_inputs(\n          time=time,\n          outputs=spec_outputs,\n          state=cell_state,\n          stop_token_predictions=stop_token_output\n      )\n    outputs = BasicDecoderOutput(spec_outputs, stop_token_output)\n    return (outputs, next_state, next_inputs, finished)\n'"
open_seq2seq/parts/tacotron/tacotron_helper.py,4,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""\nModified by blisc to enable support for tacotron models\nCustom Helper class that implements the tacotron decoder pre and post nets\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.contrib.seq2seq.python.ops.helper import Helper\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.util import nest\n\n_transpose_batch_time = decoder._transpose_batch_time\n\n\ndef _unstack_ta(inp):\n  return tensor_array_ops.TensorArray(\n      dtype=inp.dtype,\n      size=array_ops.shape(inp)[0],\n      element_shape=inp.get_shape()[1:]\n  ).unstack(inp)\n\n\nclass TacotronTrainingHelper(Helper):\n  """"""Helper funciton for training. Can be used for teacher forcing or scheduled\n  sampling""""""\n\n  def __init__(\n      self,\n      inputs,\n      sequence_length,\n      prenet=None,\n      time_major=False,\n      sample_ids_shape=None,\n      sample_ids_dtype=None,\n      model_dtype=tf.float32,\n      mask_decoder_sequence=None\n  ):\n    """"""Initializer.\n\n    Args:\n      inputs (Tensor): inputs of shape [batch, time, n_feats]\n      sequence_length (Tensor): length of each input. shape [batch]\n      prenet: prenet to use, currently disabled and used in tacotron decoder\n        instead.\n      sampling_prob (float): see tacotron 2 decoder\n      time_major (bool): (float): see tacotron 2 decoder\n      mask_decoder_sequence (bool): whether to pass finished when the decoder\n        passed the sequence_length input or to pass unfinished to dynamic_decode\n    """"""\n    self._sample_ids_shape = tensor_shape.TensorShape(sample_ids_shape or [])\n    self._sample_ids_dtype = sample_ids_dtype or dtypes.int32\n\n    if not time_major:\n      inputs = nest.map_structure(_transpose_batch_time, inputs)\n    self._input_tas = nest.map_structure(_unstack_ta, inputs)\n    self._sequence_length = sequence_length\n    self._batch_size = array_ops.size(sequence_length)\n    self._seed = None\n    self._mask_decoder_sequence = mask_decoder_sequence\n    self._prenet = prenet\n    self._zero_inputs = nest.map_structure(\n        lambda inp: array_ops.zeros_like(inp[0, :]), inputs\n    )\n    self._start_inputs = self._zero_inputs\n    if prenet is not None:\n      self._start_inputs = self._prenet(self._zero_inputs)\n    self._last_dim = self._start_inputs.get_shape()[-1]\n    self._dtype = model_dtype\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return self._sample_ids_shape\n\n  @property\n  def sample_ids_dtype(self):\n    return self._sample_ids_dtype\n\n  def initialize(self, name=None):\n    finished = array_ops.tile([False], [self._batch_size])\n    return (finished, self._start_inputs)\n\n  def sample(self, time, outputs, state, name=None):\n    # Fully deterministic, output should already be projected\n    pass\n\n  def next_inputs(self, time, outputs, state, name=None, **unused_kwargs):\n    # Applies the fully connected pre-net to the decoder\n    # Also decides whether the decoder is finished\n    next_time = time + 1\n    if self._mask_decoder_sequence:\n      finished = (next_time >= self._sequence_length)\n    else:\n      finished = array_ops.tile([False], [self._batch_size])\n    all_finished = math_ops.reduce_all(finished)\n\n    def get_next_input(inp, out):\n      next_input = inp.read(time)\n      if self._prenet is not None:\n        next_input = self._prenet(next_input)\n        out = self._prenet(out)\n      return next_input\n\n    next_inputs = control_flow_ops.cond(\n        all_finished, lambda: self._start_inputs,\n        lambda: get_next_input(self._input_tas, outputs)\n    )\n\n    return (finished, next_inputs, state)\n\n\nclass TacotronHelper(Helper):\n  """"""Helper for use during eval and infer. Does not use teacher forcing""""""\n\n  def __init__(\n      self,\n      inputs,\n      prenet=None,\n      time_major=False,\n      sample_ids_shape=None,\n      sample_ids_dtype=None,\n      mask_decoder_sequence=None\n  ):\n    """"""Initializer.\n\n    Args:\n      inputs (Tensor): inputs of shape [batch, time, n_feats]\n      prenet: prenet to use, currently disabled and used in tacotron decoder\n        instead.\n      sampling_prob (float): see tacotron 2 decoder\n      anneal_teacher_forcing (float): see tacotron 2 decoder\n      stop_gradient (float): see tacotron 2 decoder\n      time_major (bool): (float): see tacotron 2 decoder\n      mask_decoder_sequence (bool): whether to pass finished when the decoder\n        passed the sequence_length input or to pass unfinished to dynamic_decode\n    """"""\n    self._sample_ids_shape = tensor_shape.TensorShape(sample_ids_shape or [])\n    self._sample_ids_dtype = sample_ids_dtype or dtypes.int32\n    self._batch_size = inputs.get_shape()[0]\n    self._mask_decoder_sequence = mask_decoder_sequence\n\n    if not time_major:\n      inputs = nest.map_structure(_transpose_batch_time, inputs)\n\n    inputs = inputs[0, :, :]\n    self._prenet = prenet\n    if prenet is None:\n      self._start_inputs = inputs\n    else:\n      self._start_inputs = self._prenet(inputs)\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def sample_ids_shape(self):\n    return self._sample_ids_shape\n\n  @property\n  def sample_ids_dtype(self):\n    return self._sample_ids_dtype\n\n  def initialize(self, name=None):\n    finished = array_ops.tile([False], [self._batch_size])\n    return (finished, self._start_inputs)\n\n  def sample(self, time, outputs, state, name=None):\n    # Fully deterministic, output should already be projected\n    pass\n  def next_inputs(\n      self,\n      time,\n      outputs,\n      state,\n      stop_token_predictions,\n      name=None,\n      **unused_kwargs\n  ):\n    # Applies the fully connected pre-net to the decoder\n    # Also decides whether the decoder is finished\n    next_time = time + 1\n    if self._mask_decoder_sequence:\n      stop_token_predictions = tf.sigmoid(stop_token_predictions)\n      finished = tf.cast(tf.round(stop_token_predictions), tf.bool)\n      finished = tf.squeeze(finished)\n    else:\n      finished = array_ops.tile([False], [self._batch_size])\n    all_finished = math_ops.reduce_all(finished)\n\n    def get_next_input(out):\n      if self._prenet is not None:\n        out = self._prenet(out)\n      return out\n\n    next_inputs = control_flow_ops.cond(\n        all_finished, lambda: self._start_inputs,\n        lambda: get_next_input(outputs)\n    )\n    return (finished, next_inputs, state)\n'"
open_seq2seq/parts/transformer/__init__.py,0,b''
open_seq2seq/parts/transformer/attention_layer.py,41,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of multiheaded attention and self-attention layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass Attention(tf.layers.Layer):\n  """"""Multi-headed attention layer.""""""\n\n  def __init__(\n          self,\n          hidden_size,\n          num_heads,\n          attention_dropout,\n          train,\n          mode=""loung"",\n          regularizer=None,\n          window_size=None,\n          back_step_size=None\n  ):\n    if hidden_size % num_heads != 0:\n      raise ValueError(""Hidden size must be evenly divisible by the number of ""\n                       ""heads."")\n\n    super(Attention, self).__init__()\n    self.hidden_size = hidden_size\n    self.num_heads = num_heads\n    self.attention_dropout = attention_dropout\n    self.train = train\n    self.mode = mode\n\n    # Parameters for monotonic attention forcing during inference\n    self.window_size = window_size\n    self.back_step_size = back_step_size\n\n    # Layers for linearly projecting the queries, keys, and values.\n    self.q_dense_layer = tf.layers.Dense(hidden_size, use_bias=False, name=""q"",\n                                         kernel_regularizer=regularizer)\n    self.k_dense_layer = tf.layers.Dense(hidden_size, use_bias=False, name=""k"",\n                                         kernel_regularizer=regularizer)\n    self.v_dense_layer = tf.layers.Dense(hidden_size, use_bias=False, name=""v"",\n                                         kernel_regularizer=regularizer)\n    self.output_dense_layer = tf.layers.Dense(hidden_size, use_bias=False,\n                                              name=""output_transform"",\n                                              kernel_regularizer=regularizer)\n\n  def split_heads(self, x):\n    """"""Split x into different heads, and transpose the resulting value.\n\n    The tensor is transposed to insure the inner dimensions hold the correct\n    values during the matrix multiplication.\n\n    Args:\n      x: A tensor with shape [batch_size, length, hidden_size]\n\n    Returns:\n      A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n    """"""\n    with tf.name_scope(""split_heads""):\n      batch_size = tf.shape(x)[0]\n      length = tf.shape(x)[1]\n\n      # Calculate depth of last dimension after it has been split.\n      depth = (self.hidden_size // self.num_heads)\n\n      # Split the last dimension\n      x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n\n      # Transpose the result\n      return tf.transpose(x, [0, 2, 1, 3])\n\n  def combine_heads(self, x):\n    """"""Combine tensor that has been split.\n\n    Args:\n      x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n\n    Returns:\n      A tensor with shape [batch_size, length, hidden_size]\n    """"""\n    with tf.name_scope(""combine_heads""):\n      batch_size = tf.shape(x)[0]\n      length = tf.shape(x)[2]\n      x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n      return tf.reshape(x, [batch_size, length, self.hidden_size])\n\n  def call(self, x, y, bias, cache=None, positions=None):\n    """"""Apply attention mechanism to x and y.\n\n    Args:\n      x: a tensor with shape [batch_size, length_x, hidden_size]\n      y: a tensor with shape [batch_size, length_y, hidden_size]\n      bias: attention bias that will be added to the result of the dot product.\n      cache: (Used during prediction) dictionary with tensors containing results\n        of previous attentions. The dictionary must have the items:\n            {""k"": tensor with shape [batch_size, i, key_channels],\n             ""v"": tensor with shape [batch_size, i, value_channels]}\n        where i is the current decoded length.\n      positions: decoder-encoder alignment for previous steps [batch_size, n_heads, length_x]\n\n    Returns:\n      Attention layer output with shape [batch_size, length_x, hidden_size]\n    """"""\n    # Linearly project the query (q), key (k) and value (v) using different\n    # learned projections. This is in preparation of splitting them into\n    # multiple heads. Multi-head attention uses multiple queries, keys, and\n    # values rather than regular attention (which uses a single q, k, v).\n    q = self.q_dense_layer(x)\n    k = self.k_dense_layer(y)\n    v = self.v_dense_layer(y)\n\n    if cache is not None:\n      # Combine cached keys and values with new keys and values.\n      k = tf.concat([cache[""k""], k], axis=1)\n      v = tf.concat([cache[""v""], v], axis=1)\n\n      # Update cache\n      cache[""k""] = k\n      cache[""v""] = v\n\n    # Split q, k, v into heads.\n    q = self.split_heads(q)\n    k = self.split_heads(k)\n    v = self.split_heads(v)\n\n    if self.mode == ""loung"":\n      # Scale q to prevent the dot product between q and k from growing too large.\n      depth = (self.hidden_size // self.num_heads)\n      q *= depth ** -0.5\n\n      # Calculate dot product attention\n      # logits = tf.matmul(q, k, transpose_b=True)\n      # logits += bias\n      # weights = tf.nn.softmax(logits, name=""attention_weights"")\n      logits = tf.matmul(q, k, transpose_b=True)\n      dtype = logits.dtype\n      if dtype != tf.float32:\n        # upcast softmax inputs\n        logits = tf.cast(x=logits, dtype=tf.float32)\n        logits += bias\n        weights = tf.nn.softmax(logits, name=""attention_weights"")\n        # downcast softmax output\n        weights = tf.cast(weights, dtype=dtype)\n      else:\n        # Logits shape: [batch, head, decoder, encoder]\n        # Bias shape:   [batch, 1, 1, encoder]\n\n        # Force monotonic attention during inference\n        if positions is not None and self.window_size is not None:\n          assert self.back_step_size is not None\n\n          max_length = tf.shape(logits)[-1]\n\n          # Allow to make back_step_size steps back\n          window_pos = tf.maximum(positions - self.back_step_size, tf.zeros_like(positions))\n\n          # Create attention mask\n          mask_large = tf.sequence_mask(window_pos + self.window_size, maxlen=max_length)\n          mask_large = tf.cast(mask_large, tf.float32)\n          mask_small = tf.sequence_mask(window_pos, maxlen=max_length)\n          mask_small = tf.cast(mask_small, tf.float32)\n          mask = mask_large - mask_small\n          mask = -1e9 * (1 - mask)\n\n          bias = mask + bias\n\n          # Clipping\n          bias = tf.maximum(bias, -1e9)\n\n        logits += bias\n        weights = tf.nn.softmax(logits, name=""attention_weights"")\n    elif self.mode == ""bahdanau"":\n      att_v = tf.get_variable(\n        ""attention_v"", [self.hidden_size // self.num_heads], dtype=q.dtype\n      )\n\n      # Compute the attention score\n      if bias is not None:\n        weights = tf.reduce_sum(\n          tf.nn.tanh(att_v * tf.nn.tanh(k + q + bias)), 3\n        )\n      else:\n        weights = tf.reduce_sum(\n          tf.nn.tanh(att_v * tf.nn.tanh(k + q)), 3\n        )\n      weights = tf.nn.softmax(weights)\n      weights = tf.expand_dims(weights, 2)\n    else:\n      raise ValueError(\n        ""Mode for multi-head attention must be either loung for dot-product"",\n        ""attention, or bahdanau for content-based/additive/mlp-base attention""\n      )\n\n    if self.train:\n      weights = tf.nn.dropout(weights, keep_prob=1 - self.attention_dropout)\n    attention_output = tf.matmul(weights, v)\n\n    # Recombine heads --> [batch_size, length, hidden_size]\n    attention_output = self.combine_heads(attention_output)\n\n    # Run the combined outputs through another linear projection layer.\n    attention_output = self.output_dense_layer(attention_output)\n    return attention_output\n\n\nclass SelfAttention(Attention):\n  """"""Multiheaded self-attention layer.""""""\n\n  def call(self, x, bias, cache=None):\n    return super(SelfAttention, self).call(x, x, bias, cache)\n'"
open_seq2seq/parts/transformer/beam_search.py,57,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Beam search to find the translated sequence with the highest probability.\n\nSource implementation from Tensor2Tensor:\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/beam_search.py\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\n# Default value for INF\n#INF = 1. * 1e7\nINF = 32768.0\n\n\nclass _StateKeys(object):\n  """"""Keys to dictionary storing the state of the beam search loop.""""""\n\n  # Variable storing the loop index.\n  CUR_INDEX = ""CUR_INDEX""\n\n  # Top sequences that are alive for each batch item. Alive sequences are ones\n  # that have not generated an EOS token. Sequences that reach EOS are marked as\n  # finished and moved to the FINISHED_SEQ tensor.\n  # Has shape [batch_size, beam_size, CUR_INDEX + 1]\n  ALIVE_SEQ = ""ALIVE_SEQ""\n  # Log probabilities of each alive sequence. Shape [batch_size, beam_size]\n  ALIVE_LOG_PROBS = ""ALIVE_LOG_PROBS""\n  # Dictionary of cached values for each alive sequence. The cache stores\n  # the encoder output, attention bias, and the decoder attention output from\n  # the previous iteration.\n  ALIVE_CACHE = ""ALIVE_CACHE""\n\n  # Top finished sequences for each batch item.\n  # Has shape [batch_size, beam_size, CUR_INDEX + 1]. Sequences that are\n  # shorter than CUR_INDEX + 1 are padded with 0s.\n  FINISHED_SEQ = ""FINISHED_SEQ""\n  # Scores for each finished sequence. Score = log probability / length norm\n  # Shape [batch_size, beam_size]\n  FINISHED_SCORES = ""FINISHED_SCORES""\n  # Flags indicating which sequences in the finished sequences are finished.\n  # At the beginning, all of the sequences in FINISHED_SEQ are filler values.\n  # True -> finished sequence, False -> filler. Shape [batch_size, beam_size]\n  FINISHED_FLAGS = ""FINISHED_FLAGS""\n\n\nclass SequenceBeamSearch(object):\n  """"""Implementation of beam search loop.""""""\n\n  def __init__(self, symbols_to_logits_fn, vocab_size, batch_size,\n               beam_size, alpha, max_decode_length, eos_id):\n    self.symbols_to_logits_fn = symbols_to_logits_fn\n    self.vocab_size = vocab_size\n    self.batch_size = batch_size\n    self.beam_size = beam_size\n    self.alpha = alpha\n    self.max_decode_length = max_decode_length\n    self.eos_id = eos_id\n\n  def search(self, initial_ids, initial_cache):\n    """"""Beam search for sequences with highest scores.""""""\n    state, state_shapes = self._create_initial_state(initial_ids, initial_cache)\n\n    finished_state = tf.while_loop(\n        self._continue_search, self._search_step, loop_vars=[state],\n        shape_invariants=[state_shapes], parallel_iterations=1, back_prop=False)\n    finished_state = finished_state[0]\n\n    alive_seq = finished_state[_StateKeys.ALIVE_SEQ]\n    alive_log_probs = finished_state[_StateKeys.ALIVE_LOG_PROBS]\n    finished_seq = finished_state[_StateKeys.FINISHED_SEQ]\n    finished_scores = finished_state[_StateKeys.FINISHED_SCORES]\n    finished_flags = finished_state[_StateKeys.FINISHED_FLAGS]\n\n    # Account for corner case where there are no finished sequences for a\n    # particular batch item. In that case, return alive sequences for that batch\n    # item.\n    finished_seq = tf.where(\n        tf.reduce_any(finished_flags, 1), finished_seq, alive_seq)\n    finished_scores = tf.where(\n        tf.reduce_any(finished_flags, 1), finished_scores, alive_log_probs)\n    return finished_seq, finished_scores\n\n  def _create_initial_state(self, initial_ids, initial_cache):\n    """"""Return initial state dictionary and its shape invariants.\n\n    Args:\n      initial_ids: initial ids to pass into the symbols_to_logits_fn.\n        int tensor with shape [batch_size, 1]\n      initial_cache: dictionary storing values to be passed into the\n        symbols_to_logits_fn.\n\n    Returns:\n        state and shape invariant dictionaries with keys from _StateKeys\n    """"""\n    # Current loop index (starts at 0)\n    cur_index = tf.constant(0)\n\n    # Create alive sequence with shape [batch_size, beam_size, 1]\n    alive_seq = _expand_to_beam_size(initial_ids, self.beam_size)\n    alive_seq = tf.expand_dims(alive_seq, axis=2)\n\n    # Create tensor for storing initial log probabilities.\n    # Assume initial_ids are prob 1.0\n    initial_log_probs = tf.constant(\n        [[0.] + [-float(""inf"")] * (self.beam_size - 1)])\n    alive_log_probs = tf.tile(initial_log_probs, [self.batch_size, 1])\n\n    # Expand all values stored in the dictionary to the beam size, so that each\n    # beam has a separate cache.\n    alive_cache = nest.map_structure(\n        lambda t: _expand_to_beam_size(t, self.beam_size), initial_cache)\n\n    # Initialize tensor storing finished sequences with filler values.\n    finished_seq = tf.zeros(tf.shape(alive_seq), tf.int32)\n\n    # Set scores of the initial finished seqs to negative infinity.\n    finished_scores = tf.ones([self.batch_size, self.beam_size]) * -INF\n\n    # Initialize finished flags with all False values.\n    finished_flags = tf.zeros([self.batch_size, self.beam_size], tf.bool)\n\n    # Create state dictionary\n    state = {\n        _StateKeys.CUR_INDEX: cur_index,\n        _StateKeys.ALIVE_SEQ: alive_seq,\n        _StateKeys.ALIVE_LOG_PROBS: alive_log_probs,\n        _StateKeys.ALIVE_CACHE: alive_cache,\n        _StateKeys.FINISHED_SEQ: finished_seq,\n        _StateKeys.FINISHED_SCORES: finished_scores,\n        _StateKeys.FINISHED_FLAGS: finished_flags\n    }\n\n    # Create state invariants for each value in the state dictionary. Each\n    # dimension must be a constant or None. A None dimension means either:\n    #   1) the dimension\'s value is a tensor that remains the same but may\n    #      depend on the input sequence to the model (e.g. batch size).\n    #   2) the dimension may have different values on different iterations.\n    state_shape_invariants = {\n        _StateKeys.CUR_INDEX: tf.TensorShape([]),\n        _StateKeys.ALIVE_SEQ: tf.TensorShape([None, self.beam_size, None]),\n        _StateKeys.ALIVE_LOG_PROBS: tf.TensorShape([None, self.beam_size]),\n        _StateKeys.ALIVE_CACHE: nest.map_structure(\n            _get_shape_keep_last_dim, alive_cache),\n        _StateKeys.FINISHED_SEQ: tf.TensorShape([None, self.beam_size, None]),\n        _StateKeys.FINISHED_SCORES: tf.TensorShape([None, self.beam_size]),\n        _StateKeys.FINISHED_FLAGS: tf.TensorShape([None, self.beam_size])\n    }\n\n    return state, state_shape_invariants\n\n  def _continue_search(self, state):\n    """"""Return whether to continue the search loop.\n\n    The loops should terminate when\n      1) when decode length has been reached, or\n      2) when the worst score in the finished sequences is better than the best\n         score in the alive sequences (i.e. the finished sequences are provably\n         unchanging)\n\n    Args:\n      state: A dictionary with the current loop state.\n\n    Returns:\n      Bool tensor with value True if loop should continue, False if loop should\n      terminate.\n    """"""\n    i = state[_StateKeys.CUR_INDEX]\n    alive_log_probs = state[_StateKeys.ALIVE_LOG_PROBS]\n    finished_scores = state[_StateKeys.FINISHED_SCORES]\n    finished_flags = state[_StateKeys.FINISHED_FLAGS]\n\n    not_at_max_decode_length = tf.less(i, self.max_decode_length)\n\n    # Calculate largest length penalty (the larger penalty, the better score).\n    max_length_norm = _length_normalization(self.alpha, self.max_decode_length)\n    # Get the best possible scores from alive sequences.\n    best_alive_scores = alive_log_probs[:, 0] / max_length_norm\n\n    # Compute worst score in finished sequences for each batch element\n    finished_scores *= tf.to_float(finished_flags)  # set filler scores to zero\n    lowest_finished_scores = tf.reduce_min(finished_scores, axis=1)\n\n    # If there are no finished sequences in a batch element, then set the lowest\n    # finished score to -INF for that element.\n    finished_batches = tf.reduce_any(finished_flags, 1)\n    lowest_finished_scores += (1. - tf.to_float(finished_batches)) * -INF\n\n    worst_finished_score_better_than_best_alive_score = tf.reduce_all(\n        tf.greater(lowest_finished_scores, best_alive_scores)\n    )\n\n    return tf.logical_and(\n        not_at_max_decode_length,\n        tf.logical_not(worst_finished_score_better_than_best_alive_score)\n    )\n\n  def _search_step(self, state):\n    """"""Beam search loop body.\n\n    Grow alive sequences by a single ID. Sequences that have reached the EOS\n    token are marked as finished. The alive and finished sequences with the\n    highest log probabilities and scores are returned.\n\n    A sequence\'s finished score is calculating by dividing the log probability\n    by the length normalization factor. Without length normalization, the\n    search is more likely to return shorter sequences.\n\n    Args:\n      state: A dictionary with the current loop state.\n\n    Returns:\n      new state dictionary.\n    """"""\n    # Grow alive sequences by one token.\n    new_seq, new_log_probs, new_cache = self._grow_alive_seq(state)\n    # Collect top beam_size alive sequences\n    alive_state = self._get_new_alive_state(new_seq, new_log_probs, new_cache)\n\n    # Combine newly finished sequences with existing finished sequences, and\n    # collect the top k scoring sequences.\n    finished_state = self._get_new_finished_state(state, new_seq, new_log_probs)\n\n    # Increment loop index and create new state dictionary\n    new_state = {_StateKeys.CUR_INDEX: state[_StateKeys.CUR_INDEX] + 1}\n    new_state.update(alive_state)\n    new_state.update(finished_state)\n    return [new_state]\n\n  def _grow_alive_seq(self, state):\n    """"""Grow alive sequences by one token, and collect top 2*beam_size sequences.\n\n    2*beam_size sequences are collected because some sequences may have reached\n    the EOS token. 2*beam_size ensures that at least beam_size sequences are\n    still alive.\n\n    Args:\n      state: A dictionary with the current loop state.\n    Returns:\n      Tuple of\n      (Top 2*beam_size sequences [batch_size, 2 * beam_size, cur_index + 1],\n       Scores of returned sequences [batch_size, 2 * beam_size],\n       New alive cache, for each of the 2 * beam_size sequences)\n    """"""\n    i = state[_StateKeys.CUR_INDEX]\n    alive_seq = state[_StateKeys.ALIVE_SEQ]\n    alive_log_probs = state[_StateKeys.ALIVE_LOG_PROBS]\n    alive_cache = state[_StateKeys.ALIVE_CACHE]\n\n    beams_to_keep = 2 * self.beam_size\n\n    # Get logits for the next candidate IDs for the alive sequences. Get the new\n    # cache values at the same time.\n    flat_ids = _flatten_beam_dim(alive_seq)  # [batch_size * beam_size]\n    flat_cache = nest.map_structure(_flatten_beam_dim, alive_cache)\n\n    flat_logits, flat_cache = self.symbols_to_logits_fn(flat_ids, i, flat_cache)\n\n    # Unflatten logits to shape [batch_size, beam_size, vocab_size]\n    logits = _unflatten_beam_dim(flat_logits, self.batch_size, self.beam_size)\n    new_cache = nest.map_structure(\n        lambda t: _unflatten_beam_dim(t, self.batch_size, self.beam_size),\n        flat_cache)\n\n    # Convert logits to normalized log probs\n    candidate_log_probs = _log_prob_from_logits(logits)\n\n    # Calculate new log probabilities if each of the alive sequences were\n    # extended # by the the candidate IDs.\n    # Shape [batch_size, beam_size, vocab_size]\n    log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)\n\n    # Each batch item has beam_size * vocab_size candidate sequences. For each\n    # batch item, get the k candidates with the highest log probabilities.\n    flat_log_probs = tf.reshape(log_probs,\n                                [-1, self.beam_size * self.vocab_size])\n    topk_log_probs, topk_indices = tf.nn.top_k(flat_log_probs, k=beams_to_keep)\n\n    # Extract the alive sequences that generate the highest log probabilities\n    # after being extended.\n    topk_beam_indices = topk_indices // self.vocab_size\n    topk_seq, new_cache = _gather_beams(\n        [alive_seq, new_cache], topk_beam_indices, self.batch_size,\n        beams_to_keep)\n\n    # Append the most probable IDs to the topk sequences\n    topk_ids = topk_indices % self.vocab_size\n    topk_ids = tf.expand_dims(topk_ids, axis=2)\n    topk_seq = tf.concat([topk_seq, topk_ids], axis=2)\n    return topk_seq, topk_log_probs, new_cache\n\n  def _get_new_alive_state(self, new_seq, new_log_probs, new_cache):\n    """"""Gather the top k sequences that are still alive.\n\n    Args:\n      new_seq: New sequences generated by growing the current alive sequences\n        int32 tensor with shape [batch_size, 2 * beam_size, cur_index + 1]\n      new_log_probs: Log probabilities of new sequences\n        float32 tensor with shape [batch_size, beam_size]\n      new_cache: Dict of cached values for each sequence.\n\n    Returns:\n      Dictionary with alive keys from _StateKeys:\n        {Top beam_size sequences that are still alive (don\'t end with eos_id)\n         Log probabilities of top alive sequences\n         Dict cache storing decoder states for top alive sequences}\n    """"""\n    # To prevent finished sequences from being considered, set log probs to -INF\n    new_finished_flags = tf.equal(new_seq[:, :, -1], self.eos_id)\n    new_log_probs += tf.to_float(new_finished_flags) * -INF\n\n    top_alive_seq, top_alive_log_probs, top_alive_cache = _gather_topk_beams(\n        [new_seq, new_log_probs, new_cache], new_log_probs, self.batch_size,\n        self.beam_size)\n\n    return {\n        _StateKeys.ALIVE_SEQ: top_alive_seq,\n        _StateKeys.ALIVE_LOG_PROBS: top_alive_log_probs,\n        _StateKeys.ALIVE_CACHE: top_alive_cache\n    }\n\n  def _get_new_finished_state(self, state, new_seq, new_log_probs):\n    """"""Combine new and old finished sequences, and gather the top k sequences.\n\n    Args:\n      state: A dictionary with the current loop state.\n      new_seq: New sequences generated by growing the current alive sequences\n        int32 tensor with shape [batch_size, beam_size, i + 1]\n      new_log_probs: Log probabilities of new sequences\n        float32 tensor with shape [batch_size, beam_size]\n\n    Returns:\n      Dictionary with finished keys from _StateKeys:\n        {Top beam_size finished sequences based on score,\n         Scores of finished sequences,\n         Finished flags of finished sequences}\n    """"""\n    i = state[_StateKeys.CUR_INDEX]\n    finished_seq = state[_StateKeys.FINISHED_SEQ]\n    finished_scores = state[_StateKeys.FINISHED_SCORES]\n    finished_flags = state[_StateKeys.FINISHED_FLAGS]\n\n    # First append a column of 0-ids to finished_seq to increment the length.\n    # New shape of finished_seq: [batch_size, beam_size, i + 1]\n    finished_seq = tf.concat(\n        [finished_seq,\n         tf.zeros([self.batch_size, self.beam_size, 1], tf.int32)], axis=2)\n\n    # Calculate new seq scores from log probabilities.\n    length_norm = _length_normalization(self.alpha, i + 1)\n    new_scores = new_log_probs / length_norm\n\n    # Set the scores of the still-alive seq in new_seq to large negative values.\n    new_finished_flags = tf.equal(new_seq[:, :, -1], self.eos_id)\n    new_scores += (1. - tf.to_float(new_finished_flags)) * -INF\n\n    # Combine sequences, scores, and flags.\n    finished_seq = tf.concat([finished_seq, new_seq], axis=1)\n    finished_scores = tf.concat([finished_scores, new_scores], axis=1)\n    finished_flags = tf.concat([finished_flags, new_finished_flags], axis=1)\n\n    # Return the finished sequences with the best scores.\n    top_finished_seq, top_finished_scores, top_finished_flags = (\n        _gather_topk_beams([finished_seq, finished_scores, finished_flags],\n                           finished_scores, self.batch_size, self.beam_size))\n\n    return {\n        _StateKeys.FINISHED_SEQ: top_finished_seq,\n        _StateKeys.FINISHED_SCORES: top_finished_scores,\n        _StateKeys.FINISHED_FLAGS: top_finished_flags\n    }\n\n\ndef sequence_beam_search(\n    symbols_to_logits_fn, initial_ids, initial_cache, vocab_size, beam_size,\n    alpha, max_decode_length, eos_id):\n  """"""Search for sequence of subtoken ids with the largest probability.\n\n  Args:\n    symbols_to_logits_fn: A function that takes in ids, index, and cache as\n      arguments. The passed in arguments will have shape:\n        ids -> [batch_size * beam_size, index]\n        index -> [] (scalar)\n        cache -> nested dictionary of tensors [batch_size * beam_size, ...]\n      The function must return logits and new cache.\n        logits -> [batch * beam_size, vocab_size]\n        new cache -> same shape/structure as inputted cache\n    initial_ids: Starting ids for each batch item.\n      int32 tensor with shape [batch_size]\n    initial_cache: dict containing starting decoder variables information\n    vocab_size: int size of tokens\n    beam_size: int number of beams\n    alpha: float defining the strength of length normalization\n    max_decode_length: maximum length to decoded sequence\n    eos_id: int id of eos token, used to determine when a sequence has finished\n\n  Returns:\n    Top decoded sequences [batch_size, beam_size, max_decode_length]\n    sequence scores [batch_size, beam_size]\n  """"""\n  batch_size = tf.shape(initial_ids)[0]\n  sbs = SequenceBeamSearch(symbols_to_logits_fn, vocab_size, batch_size,\n                           beam_size, alpha, max_decode_length, eos_id)\n  return sbs.search(initial_ids, initial_cache)\n\n\ndef _log_prob_from_logits(logits):\n  return logits - tf.reduce_logsumexp(logits, axis=2, keep_dims=True)\n\n\ndef _length_normalization(alpha, length):\n  """"""Return length normalization factor.""""""\n  return tf.pow(((5. + tf.to_float(length)) / 6.), alpha)\n\n\ndef _expand_to_beam_size(tensor, beam_size):\n  """"""Tiles a given tensor by beam_size.\n\n  Args:\n    tensor: tensor to tile [batch_size, ...]\n    beam_size: How much to tile the tensor by.\n\n  Returns:\n    Tiled tensor [batch_size, beam_size, ...]\n  """"""\n  tensor = tf.expand_dims(tensor, axis=1)\n  tile_dims = [1] * tensor.shape.ndims\n  tile_dims[1] = beam_size\n\n  return tf.tile(tensor, tile_dims)\n\n\ndef _shape_list(tensor):\n  """"""Return a list of the tensor\'s shape, and ensure no None values in list.""""""\n  # Get statically known shape (may contain None\'s for unknown dimensions)\n  shape = tensor.get_shape().as_list()\n\n  # Ensure that the shape values are not None\n  dynamic_shape = tf.shape(tensor)\n  for i in range(len(shape)):\n    if shape[i] is None:\n      shape[i] = dynamic_shape[i]\n  return shape\n\n\ndef _get_shape_keep_last_dim(tensor):\n  shape_list = _shape_list(tensor)\n\n  # Only the last\n  for i in range(len(shape_list) - 1):\n    shape_list[i] = None\n\n  if isinstance(shape_list[-1], tf.Tensor):\n    shape_list[-1] = None\n  return tf.TensorShape(shape_list)\n\n\ndef _flatten_beam_dim(tensor):\n  """"""Reshapes first two dimensions in to single dimension.\n\n  Args:\n    tensor: Tensor to reshape of shape [A, B, ...]\n\n  Returns:\n    Reshaped tensor of shape [A*B, ...]\n  """"""\n  shape = _shape_list(tensor)\n  shape[0] *= shape[1]\n  shape.pop(1)  # Remove beam dim\n  return tf.reshape(tensor, shape)\n\n\ndef _unflatten_beam_dim(tensor, batch_size, beam_size):\n  """"""Reshapes first dimension back to [batch_size, beam_size].\n\n  Args:\n    tensor: Tensor to reshape of shape [batch_size*beam_size, ...]\n    batch_size: Tensor, original batch size.\n    beam_size: int, original beam size.\n\n  Returns:\n    Reshaped tensor of shape [batch_size, beam_size, ...]\n  """"""\n  shape = _shape_list(tensor)\n  new_shape = [batch_size, beam_size] + shape[1:]\n  return tf.reshape(tensor, new_shape)\n\n\ndef _gather_beams(nested, beam_indices, batch_size, new_beam_size):\n  """"""Gather beams from nested structure of tensors.\n\n  Each tensor in nested represents a batch of beams, where beam refers to a\n  single search state (beam search involves searching through multiple states\n  in parallel).\n\n  This function is used to gather the top beams, specified by\n  beam_indices, from the nested tensors.\n\n  Args:\n    nested: Nested structure (tensor, list, tuple or dict) containing tensors\n      with shape [batch_size, beam_size, ...].\n    beam_indices: int32 tensor with shape [batch_size, new_beam_size]. Each\n     value in beam_indices must be between [0, beam_size), and are not\n     necessarily unique.\n    batch_size: int size of batch\n    new_beam_size: int number of beams to be pulled from the nested tensors.\n\n  Returns:\n    Nested structure containing tensors with shape\n      [batch_size, new_beam_size, ...]\n  """"""\n  # Computes the i\'th coodinate that contains the batch index for gather_nd.\n  # Batch pos is a tensor like [[0,0,0,0,],[1,1,1,1],..].\n  batch_pos = tf.range(batch_size * new_beam_size) // new_beam_size\n  batch_pos = tf.reshape(batch_pos, [batch_size, new_beam_size])\n\n  # Create coordinates to be passed to tf.gather_nd. Stacking creates a tensor\n  # with shape [batch_size, beam_size, 2], where the last dimension contains\n  # the (i, j) gathering coordinates.\n  coordinates = tf.stack([batch_pos, beam_indices], axis=2)\n\n  return nest.map_structure(\n      lambda state: tf.gather_nd(state, coordinates), nested)\n\n\ndef _gather_topk_beams(nested, score_or_log_prob, batch_size, beam_size):\n  """"""Gather top beams from nested structure.""""""\n  _, topk_indexes = tf.nn.top_k(score_or_log_prob, k=beam_size)\n  return _gather_beams(nested, topk_indexes, batch_size, beam_size)'"
open_seq2seq/parts/transformer/beam_search_test.py,15,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test beam search helper methods.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom . import beam_search\n#import beam_search\n\n\nclass BeamSearchHelperTests(tf.test.TestCase):\n\n  def test_expand_to_beam_size(self):\n    x = tf.ones([7, 4, 2, 5])\n    x = beam_search._expand_to_beam_size(x, 3)\n    with self.test_session() as sess:\n      shape = sess.run(tf.shape(x))\n    self.assertAllEqual([7, 3, 4, 2, 5], shape)\n\n  def test_shape_list(self):\n    y = tf.constant(4.0)\n    x = tf.ones([7, tf.cast(tf.sqrt(y),tf.int32), 2, 5])\n    shape = beam_search._shape_list(x)\n    self.assertIsInstance(shape[0], int)\n    #self.assertIsInstance(shape[1], tf.Tensor)\n    self.assertIsNotNone(shape[1])\n    self.assertIsInstance(shape[2], int)\n    self.assertIsInstance(shape[3], int)\n\n  def test_get_shape_keep_last_dim(self):\n    y = tf.constant(4.0)\n    x = tf.ones([7, tf.cast(tf.sqrt(y),tf.int32), 2, 5])\n    shape = beam_search._get_shape_keep_last_dim(x)\n    self.assertAllEqual([None, None, None, 5],\n                        shape.as_list())\n\n  def test_flatten_beam_dim(self):\n    x = tf.ones([7, 4, 2, 5])\n    x = beam_search._flatten_beam_dim(x)\n    with self.test_session() as sess:\n      shape = sess.run(tf.shape(x))\n    self.assertAllEqual([28, 2, 5], shape)\n\n  def test_unflatten_beam_dim(self):\n    x = tf.ones([28, 2, 5])\n    x = beam_search._unflatten_beam_dim(x, 7, 4)\n    with self.test_session() as sess:\n      shape = sess.run(tf.shape(x))\n    self.assertAllEqual([7, 4, 2, 5], shape)\n\n  def test_gather_beams(self):\n    x = tf.reshape(tf.range(24), [2, 3, 4])\n    # x looks like:  [[[ 0  1  2  3]\n    #                  [ 4  5  6  7]\n    #                  [ 8  9 10 11]]\n    #\n    #                 [[12 13 14 15]\n    #                  [16 17 18 19]\n    #                  [20 21 22 23]]]\n\n    y = beam_search._gather_beams(x, [[1, 2], [0, 2]], 2, 2)\n    with self.test_session() as sess:\n      y = sess.run(y)\n\n    self.assertAllEqual([[[4, 5, 6, 7],\n                          [8, 9, 10, 11]],\n                         [[12, 13, 14, 15],\n                          [20, 21, 22, 23]]],\n                        y)\n\n  def test_gather_topk_beams(self):\n    x = tf.reshape(tf.range(24), [2, 3, 4])\n    x_scores = [[0, 1, 1], [1, 0, 1]]\n\n    y = beam_search._gather_topk_beams(x, x_scores, 2, 2)\n    with self.test_session() as sess:\n      y = sess.run(y)\n\n    self.assertAllEqual([[[4, 5, 6, 7],\n                          [8, 9, 10, 11]],\n                         [[12, 13, 14, 15],\n                          [20, 21, 22, 23]]],\n                        y)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
open_seq2seq/parts/transformer/common.py,24,"b'# This code is heavily based on the code from MLPerf\n# https://github.com/mlperf/reference/tree/master/translation/tensorflow/transformer\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass Transformer_BatchNorm(tf.layers.Layer):\n  """"""Transformer batch norn: supports [BTC](default) and [BCT] formats. """"""\n\n  def __init__(self, training, params={}):\n    super(Transformer_BatchNorm, self).__init__()\n    self.training = training\n    self.data_format=params.get(\'data_format\',\'channels_last\')\n    self.momentum = params.get(\'momentum\',0.95)\n    self.epsilon  = params.get(\'epsilon\',0.0001)\n    self.center_scale = params.get(\'center_scale\', True)\n    self.regularizer = params.get(\'regularizer\', None) if self.center_scale else None\n    if self.regularizer != None:\n      self.regularizer_params = params.get(""regularizer_params"", {\'scale\': 0.0})\n      self.regularizer=self.regularizer(self.regularizer_params[\'scale\']) \\\n        if self.regularizer_params[\'scale\'] > 0.0 else None\n\n    #print(""Batch norm, training="", training, params)\n\n  def call(self, x):\n    x = tf.expand_dims(x, axis=2)\n    axis = -1 if (self.data_format==\'channels_last\') else 1\n    y = tf.layers.batch_normalization(inputs=x, axis=axis,\n      momentum=self.momentum, epsilon=self.epsilon,\n      center=self.center_scale, scale=self.center_scale,\n      beta_regularizer=self.regularizer, gamma_regularizer=self.regularizer,\n      training=self.training,\n    )\n    y = tf.squeeze(y, axis=[2])\n    return y\n\nclass LayerNormalization(tf.layers.Layer):\n  """"""Layer normalization for BTC format: supports L2(default) and L1 modes""""""\n\n  def __init__(self, hidden_size, params={}):\n    super(LayerNormalization, self).__init__()\n    self.hidden_size = hidden_size\n    self.norm_type = params.get(""type"", ""layernorm_L2"")\n    self.epsilon = params.get(""epsilon"", 1e-6)\n\n  def build(self, _):\n    self.scale = tf.get_variable(""layer_norm_scale"", [self.hidden_size],\n                                 initializer= tf.keras.initializers.Ones(),\n                                 dtype=tf.float32)\n    self.bias = tf.get_variable(""layer_norm_bias"", [self.hidden_size],\n                                 initializer=tf.keras.initializers.Zeros(),\n                                 dtype=tf.float32)\n    self.built = True\n\n  def call(self, x):\n    if self.norm_type==""layernorm_L2"":\n      epsilon = self.epsilon\n      dtype = x.dtype\n      x = tf.cast(x=x, dtype=tf.float32)\n      mean = tf.reduce_mean(x, axis=[-1], keepdims=True)\n      variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keepdims=True)\n      norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n      result = norm_x * self.scale + self.bias\n      return tf.cast(x=result, dtype=dtype)\n\n    else:\n      dtype = x.dtype\n      if dtype==tf.float16:\n        x = tf.cast(x, dtype=tf.float32)\n      mean = tf.reduce_mean(x, axis=[-1], keepdims=True)\n      x = x - mean\n      variance = tf.reduce_mean(tf.abs(x), axis=[-1], keepdims=True)\n      norm_x = tf.div(x , variance + self.epsilon)\n      y = norm_x * self.scale + self.bias\n      if dtype == tf.float16:\n        y = tf.saturate_cast(y, dtype)\n      return y\n\nclass PrePostProcessingWrapper(object):\n  """"""Wrapper around layer, that applies pre-processing and post-processing.""""""\n\n  def __init__(self, layer, params, training):\n    self.layer = layer\n    self.postprocess_dropout = params[""layer_postprocess_dropout""]\n    self.training = training\n    self.norm_params = params.get(""norm_params"", {""type"": ""layernorm_L2""})\n    # Create normalization layer\n    if self.norm_params[""type""]==""batch_norm"":\n      self.norm = Transformer_BatchNorm(training=training,\n                                        params=self.norm_params)\n    else:\n      self.norm = LayerNormalization(hidden_size=params[""hidden_size""],\n                                     params=self.norm_params)\n\n  def __call__(self, x, *args, **kwargs):\n    # Preprocessing: normalization\n    y = self.norm(x)\n    y = self.layer(y, *args, **kwargs)\n    # Postprocessing: dropout and residual connection\n    if self.training:\n      y = tf.nn.dropout(y, keep_prob=1 - self.postprocess_dropout)\n    return x + y\n'"
open_seq2seq/parts/transformer/embedding_layer.py,16,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of embedding layer with shared weights.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom . import utils as model_utils\n\n\nclass EmbeddingSharedWeights(tf.layers.Layer):\n  """"""Calculates input embeddings and pre-softmax linear with shared weights.""""""\n\n  def __init__(self, vocab_size, hidden_size, pad_vocab_to_eight=False, init_var=None,\n               embed_scale=True, pad_sym=0, mask_paddings=True, regularizer=None):\n    super(EmbeddingSharedWeights, self).__init__()\n    self.hidden_size = hidden_size\n    self.embed_scale = embed_scale\n    self.pad_sym = pad_sym\n    self.mask_paddings = mask_paddings\n    self.regularizer = regularizer\n\n    padf = lambda x: x if x % 8 == 0 else x + 8 - x % 8\n    if pad_vocab_to_eight:\n      self.vocab_size = padf(vocab_size)\n    else:\n      self.vocab_size = vocab_size\n\n    if init_var is None:\n      self.init_var = hidden_size ** -0.5\n    else:\n      self.init_var = init_var\n\n  def build(self, _):\n    with tf.variable_scope(""embedding_and_softmax"", reuse=tf.AUTO_REUSE):\n      # Create and initialize weights. The random normal initializer was chosen\n      # randomly, and works well.\n      self.shared_weights = tf.get_variable(""weights"", [self.vocab_size, self.hidden_size],\n                                            initializer=tf.random_normal_initializer(0., self.init_var), \\\n                                            regularizer=self.regularizer)\n\n    self.built = True\n\n  def call(self, x):\n    """"""Get token embeddings of x.\n\n    Args:\n      x: An int64 tensor with shape [batch_size, length]\n    Returns:\n      embeddings: float32 tensor with shape [batch_size, length, embedding_size]\n      padding: float32 tensor with shape [batch_size, length] indicating the\n        locations of the padding tokens in x.\n    """"""\n    with tf.name_scope(""embedding""):\n      # fills out of bound values with padding symbol\n      out_bound_mask = tf.cast(x > (self.vocab_size - 1), dtype=tf.int32)\n      x *= 1 - out_bound_mask\n      x += out_bound_mask * tf.cast(self.pad_sym, dtype=tf.int32)\n\n      embeddings = tf.gather(self.shared_weights, x)\n      if self.embed_scale:\n        # Scale embedding by the sqrt of the hidden size\n        embeddings *= self.hidden_size ** 0.5\n\n      if self.mask_paddings:\n        # Create binary array of size [batch_size, length]\n        # where 1 = padding, 0 = not padding\n        padding = model_utils.get_padding(x, padding_value=self.pad_sym)\n\n        # Set all padding embedding values to 0\n        #embeddings *= tf.expand_dims(1 - padding, -1)\n        embeddings *= tf.cast(tf.expand_dims(1.0 - padding, -1), dtype=embeddings.dtype)\n      return embeddings\n\n  def linear(self, x):\n    """"""Computes logits by running x through a linear layer.\n\n    Args:\n      x: A float32 tensor with shape [batch_size, length, hidden_size]\n    Returns:\n      float32 tensor with shape [batch_size, length, vocab_size].\n    """"""\n    with tf.name_scope(""presoftmax_linear""):\n      batch_size = tf.shape(x)[0]\n      length = tf.shape(x)[1]\n\n      x = tf.reshape(x, [-1, self.hidden_size])\n      logits = tf.matmul(x, self.shared_weights, transpose_b=True)\n\n      return tf.reshape(logits, [batch_size, length, self.vocab_size])\n'"
open_seq2seq/parts/transformer/ffn_layer.py,18,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of fully connected network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass FeedFowardNetwork(tf.layers.Layer):\n  """"""Fully connected feedforward network.""""""\n\n  def __init__(self, hidden_size, filter_size, relu_dropout, train, regularizer=None):\n    super(FeedFowardNetwork, self).__init__()\n    self.hidden_size = hidden_size\n    self.filter_size = filter_size\n    self.relu_dropout = relu_dropout\n    self.train = train\n\n    # regularizer = tf.contrib.layers.l2_regularizer(0.0005)\n\n    self.filter_dense_layer = tf.layers.Dense(\n        filter_size,\n        use_bias=True,\n        activation=tf.nn.relu,\n        name=""filter_layer"",\n        kernel_regularizer=regularizer,\n        bias_regularizer=regularizer\n    )\n    self.output_dense_layer = tf.layers.Dense(\n        hidden_size,\n        use_bias=True,\n        name=""output_layer"",\n        kernel_regularizer=regularizer,\n        bias_regularizer=regularizer )\n\n  def call(self, x, padding=None):\n    # Retrieve dynamically known shapes\n    batch_size = tf.shape(x)[0]\n    length = tf.shape(x)[1]\n\n    if padding is not None:\n      with tf.name_scope(""remove_padding""):\n        # Flatten padding to [batch_size*length]\n        pad_mask = tf.reshape(padding, [-1])\n\n        nonpad_ids = tf.cast(tf.where(pad_mask < 1e-9), dtype=tf.int32)\n\n        # Reshape x to [batch_size*length, hidden_size] to remove padding\n        x = tf.reshape(x, [-1, self.hidden_size])\n        x = tf.gather_nd(x, indices=nonpad_ids)\n\n        # Reshape x from 2 dimensions to 3 dimensions.\n        x.set_shape([None, self.hidden_size])\n        x = tf.expand_dims(x, axis=0)\n\n    output = self.filter_dense_layer(x)\n    if self.train:\n      output = tf.nn.dropout(output, keep_prob = 1 - self.relu_dropout)\n    output = self.output_dense_layer(output)\n\n    if padding is not None:\n      with tf.name_scope(""re_add_padding""):\n        output = tf.squeeze(output, axis=0)\n        output = tf.scatter_nd(\n            indices=nonpad_ids,\n            updates=output,\n            shape=[batch_size * length, self.hidden_size]\n        )\n        output = tf.reshape(output, [batch_size, length, self.hidden_size])\n    return output\n\n'"
open_seq2seq/parts/transformer/utils.py,22,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Transformer model helper methods.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport tensorflow as tf\n\n_NEG_INF = -1e9\n#_NEG_INF_FP16 = -1e4\n\ndef get_position_encoding(\n    length, hidden_size, min_timescale=1.0, max_timescale=1.0e4):\n  """"""Return positional encoding.\n\n  Calculates the position encoding as a mix of sine and cosine functions with\n  geometrically increasing wavelengths.\n  Defined and formulized in Attention is All You Need, section 3.5.\n\n  Args:\n    length: Sequence length.\n    hidden_size: Size of the\n    min_timescale: Minimum scale that will be applied at each position\n    max_timescale: Maximum scale that will be applied at each position\n\n  Returns:\n    Tensor with shape [length, hidden_size]\n  """"""\n  position = tf.cast(tf.range(length),dtype=tf.float32)\n  num_timescales = hidden_size // 2\n  log_timescale_increment = (\n      math.log(float(max_timescale) / float(min_timescale)) /\n      (tf.cast((num_timescales) - 1, dtype=tf.float32)))\n  inv_timescales = min_timescale * tf.exp(\n      tf.cast(tf.range(num_timescales),dtype=tf.float32 ) * -log_timescale_increment)\n  scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n  return signal\n\n\ndef get_decoder_self_attention_bias(length, dtype=tf.float32):\n  """"""Calculate bias for decoder that maintains model\'s autoregressive property.\n\n  Creates a tensor that masks out locations that correspond to illegal\n  connections, so prediction at position i cannot draw information from future\n  positions.\n\n  Args:\n    length: int length of sequences in batch.\n\n  Returns:\n    float tensor of shape [1, 1, length, length]\n  """"""\n  #print(""get_decoder_self_attention_bias"", dtype)\n\n  with tf.name_scope(""decoder_self_attention_bias""):\n    #valid_locs = tf.matrix_band_part(tf.ones([length, length], dtype=dtype), -1, 0)\n    valid_locs = tf.matrix_band_part(tf.ones([length, length], dtype=tf.float32), -1, 0)\n    valid_locs = tf.reshape(valid_locs, [1, 1, length, length])\n    neg_inf=_NEG_INF #if (dtype==tf.float32) else _NEG_INF_FP16\n    bias = neg_inf * (1.0 - valid_locs)\n    #bias=tf.saturate_cast(bias, dtype=dtype)\n  return bias\n\n\ndef get_padding(x, padding_value=0, dtype=tf.float32):\n  """"""Return float tensor representing the padding values in x.\n\n  Args:\n    x: int tensor with any shape\n    padding_value: int value that\n    dtype: type of the output\n\n  Returns:\n    float tensor with same shape as x containing values 0 or 1.\n      0 -> non-padding, 1 -> padding\n  """"""\n  #print(""get_padding"", dtype)\n  with tf.name_scope(""padding""):\n    return tf.cast(tf.equal(x, padding_value), dtype=dtype)\n\n\ndef get_padding_bias(x, res_rank=4, pad_sym=0, dtype=tf.float32):\n  """"""Calculate bias tensor from padding values in tensor.\n\n  Bias tensor that is added to the pre-softmax multi-headed attention logits,\n  which has shape [batch_size, num_heads, length, length]. The tensor is zero at\n  non-padding locations, and -1e9 (negative infinity) at padding locations.\n\n  Args:\n    x: int tensor with shape [batch_size, length]\n    res_rank: int indicates the rank of attention_bias.\n    dtype: type of the output attention_bias\n    pad_sym: int the symbol used for padding\n\n  Returns:\n    Attention bias tensor of shape\n    [batch_size, 1, 1, length] if  res_rank = 4 - for Transformer\n    or [batch_size, 1, length] if res_rank = 3 - for ConvS2S\n  """"""\n  #print(""get_padding_bias"", dtype)\n  with tf.name_scope(""attention_bias""):\n    padding = get_padding(x, padding_value=pad_sym, dtype=tf.float32)\n    # padding = get_padding(x, padding_value=pad_sym, dtype=dtype)\n    neg_inf=_NEG_INF #if dtype==tf.float32 else _NEG_INF_FP16\n    attention_bias = padding * neg_inf\n    if res_rank == 4:\n      attention_bias = tf.expand_dims(tf.expand_dims(attention_bias, axis=1), axis=1)\n    elif res_rank == 3:\n      attention_bias = tf.expand_dims(attention_bias, axis=1)\n    else:\n      raise ValueError(""res_rank should be 3 or 4 but got {}"".format(res_rank))\n  return attention_bias\n'"
open_seq2seq/parts/transformer/utils_test.py,6,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test Transformer model helper methods.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport open_seq2seq.parts.transformer.utils as model_utils\nfrom open_seq2seq.parts.transformer.utils import _NEG_INF as NEG_INF\n\nclass ModelUtilsTest(tf.test.TestCase):\n\n  def test_get_padding(self):\n    x = tf.constant([[1, 0, 0, 0, 2], [3, 4, 0, 0, 0], [0, 5, 6, 0, 7]])\n    padding = model_utils.get_padding(x, padding_value=0)\n    with self.test_session() as sess:\n      padding = sess.run(padding)\n\n    self.assertAllEqual([[0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [1, 0, 0, 1, 0]],\n                        padding)\n\n  def test_get_padding_bias(self):\n    x = tf.constant([[1, 0, 0, 0, 2], [3, 4, 0, 0, 0], [0, 5, 6, 0, 7]])\n    bias = model_utils.get_padding_bias(x)\n    bias_shape = tf.shape(bias)\n    flattened_bias = tf.reshape(bias, [3, 5])\n    with self.test_session() as sess:\n      flattened_bias, bias_shape = sess.run((flattened_bias, bias_shape))\n\n    self.assertAllEqual([[0, NEG_INF, NEG_INF, NEG_INF, 0],\n                         [0, 0, NEG_INF, NEG_INF, NEG_INF],\n                         [NEG_INF, 0, 0, NEG_INF, 0]],\n                        flattened_bias)\n    self.assertAllEqual([3, 1, 1, 5], bias_shape)\n\n  def test_get_decoder_self_attention_bias(self):\n    length = 5\n    bias = model_utils.get_decoder_self_attention_bias(length)\n    with self.test_session() as sess:\n      bias = sess.run(bias)\n\n    self.assertAllEqual([[[[0, NEG_INF, NEG_INF, NEG_INF, NEG_INF],\n                           [0, 0, NEG_INF, NEG_INF, NEG_INF],\n                           [0, 0, 0, NEG_INF, NEG_INF],\n                           [0, 0, 0, 0, NEG_INF],\n                           [0, 0, 0, 0, 0]]]],\n                        bias)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
open_seq2seq/test_utils/test_speech_configs/__init__.py,0,b''
open_seq2seq/test_utils/test_speech_configs/ds2_test_config.py,4,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import DeepSpeech2Encoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""use_horovod"": False,\n    ""num_epochs"": 150,\n\n    ""num_gpus"": 1,\n    ""batch_size_per_gpu"": 10,\n    ""save_summaries_steps"": 10,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 20,\n    ""eval_steps"": 50,\n    ""save_checkpoint_steps"": 50,\n    ""logdir"": ""tmp_log_folder"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.90,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.01,\n        ""power"": 2,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n    ""dtype"": tf.float32,\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": DeepSpeech2Encoder,\n    ""encoder_params"": {\n        ""conv_layers"": [\n            {\n                ""kernel_size"": [5, 11], ""stride"": [2, 2],\n                ""num_channels"": 32, ""padding"": ""SAME""\n            },\n            {\n                ""kernel_size"": [5, 11], ""stride"": [1, 2],\n                ""num_channels"": 64, ""padding"": ""SAME""\n            },\n        ],\n        ""n_hidden"": 128,\n\n        ""rnn_cell_dim"": 128,\n        ""rnn_type"": ""gru"",\n        ""num_rnn_layers"": 1,\n        ""rnn_unidirectional"": False,\n        ""row_conv"": True,\n        ""row_conv_width"": 8,\n        ""use_cudnn_rnn"": True,\n\n        ""dropout_keep_prob"": 0.9,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n        ""data_format"": ""channels_first"",\n        ""bn_momentum"": 0.001,\n    },\n\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 160,\n        ""input_type"": ""spectrogram"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n        ],\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 160,\n        ""input_type"": ""spectrogram"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
open_seq2seq/test_utils/test_speech_configs/jasper_res_blockout_test_config.py,4,"b'# pylint: skip-file\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data.speech2text.speech2text import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n### If training with synthetic data, don\'t forget to add your synthetic csv\n### to dataset files\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""use_horovod"": False,\n    ""num_epochs"": 500,\n\n    ""num_gpus"": 1,\n    ""batch_size_per_gpu"": 10,\n    ""save_summaries_steps"": 10,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 20,\n    ""eval_steps"": 50,\n    ""save_checkpoint_steps"": 50,\n    ""logdir"": ""tmp_log_folder"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.90,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.01,\n        ""power"": 2,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n    ""dtype"": tf.float32,\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [7], ""stride"": [1],\n                ""num_channels"": 128, ""padding"": ""SAME"",\n                ""dilation"":[1]\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 2,\n                ""kernel_size"": [7], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 2,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 256, ""padding"": ""SAME"",\n                ""dilation"":[1],\n                ""residual"": True\n            },\n        ],\n\n        ""dropout_keep_prob"": 0.9,\n\n        ""drop_block_prob"": 0.2,\n        ""drop_block_index"": -1,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""normalization"": ""batch_norm"",\n        ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n        ""data_format"": ""channels_last"",\n    },\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 64,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""/data/librispeech/librivox-train-clean-100.csv"",\n            ""/data/librispeech/librivox-train-clean-360.csv"",\n            ""/data/librispeech/librivox-train-other-500.csv"",\n            # Add synthetic csv here\n        ],\n        ""syn_enable"": False, # Change to True if using synthetic data\n        ""syn_subdirs"": [], # Add subdirs of synthetic data\n        ""max_duration"": 16.7,\n        ""shuffle"": True,\n    },\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 40,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n        ],\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 40,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
open_seq2seq/test_utils/test_speech_configs/w2l_test_config.py,4,"b'# pylint: skip-file\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nfrom open_seq2seq.models import Speech2Text\nfrom open_seq2seq.encoders import TDNNEncoder\nfrom open_seq2seq.decoders import FullyConnectedCTCDecoder\nfrom open_seq2seq.data import Speech2TextDataLayer\nfrom open_seq2seq.losses import CTCLoss\nfrom open_seq2seq.optimizers.lr_policies import poly_decay\n\n\nbase_model = Speech2Text\n\nbase_params = {\n    ""use_horovod"": False,\n    ""num_epochs"": 500,\n\n    ""num_gpus"": 1,\n    ""batch_size_per_gpu"": 10,\n    ""save_summaries_steps"": 10,\n    ""print_loss_steps"": 10,\n    ""print_samples_steps"": 20,\n    ""eval_steps"": 50,\n    ""save_checkpoint_steps"": 50,\n    ""logdir"": ""tmp_log_folder"",\n\n    ""optimizer"": ""Momentum"",\n    ""optimizer_params"": {\n        ""momentum"": 0.90,\n    },\n    ""lr_policy"": poly_decay,\n    ""lr_policy_params"": {\n        ""learning_rate"": 0.01,\n        ""power"": 2,\n    },\n    ""larc_params"": {\n        ""larc_eta"": 0.001,\n    },\n    ""dtype"": tf.float32,\n    ""summaries"": [\'learning_rate\', \'variables\', \'gradients\', \'larc_summaries\',\n                  \'variable_norm\', \'gradient_norm\', \'global_gradient_norm\'],\n\n    ""encoder"": TDNNEncoder,\n    ""encoder_params"": {\n        ""convnet_layers"": [\n            {\n                ""type"": ""conv1d"", ""repeat"": 3,\n                ""kernel_size"": [7], ""stride"": [1],\n                ""num_channels"": 200, ""padding"": ""SAME"",\n                ""dilation"":[1]\n            },\n            {\n                ""type"": ""conv1d"", ""repeat"": 1,\n                ""kernel_size"": [1], ""stride"": [1],\n                ""num_channels"": 400, ""padding"": ""SAME"",\n                ""dilation"":[1]\n            },\n        ],\n\n        ""dropout_keep_prob"": 0.9,\n\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""initializer_params"": {\n            \'uniform\': False,\n        },\n        ""activation_fn"": lambda x: tf.minimum(tf.nn.relu(x), 20.0),\n        ""data_format"": ""channels_last"",\n        ""bn_momentum"": 0.001,\n    },\n    ""decoder"": FullyConnectedCTCDecoder,\n    ""decoder_params"": {\n        ""initializer"": tf.contrib.layers.xavier_initializer,\n        ""use_language_model"": False,\n    },\n    ""loss"": CTCLoss,\n    ""loss_params"": {},\n}\n\ntrain_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 40,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n        ],\n        ""shuffle"": True,\n    },\n}\n\neval_params = {\n    ""data_layer"": Speech2TextDataLayer,\n    ""data_layer_params"": {\n        ""num_audio_features"": 40,\n        ""input_type"": ""logfbank"",\n        ""vocab_file"": ""open_seq2seq/test_utils/toy_speech_data/vocab.txt"",\n        ""dataset_files"": [\n            ""open_seq2seq/test_utils/toy_speech_data/toy_data.csv"",\n        ],\n        ""shuffle"": False,\n    },\n}\n'"
