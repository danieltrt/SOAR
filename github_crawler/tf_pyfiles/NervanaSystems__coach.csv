file_path,api_count,code
__init__.py,0,b''
setup.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom codecs import open\nfrom os import path\n\nfrom setuptools import setup, find_packages\nimport subprocess\n\n# Creating the pip package involves the following steps:\n# - Define the pip package related files - setup.py (this file) and MANIFEST.in by:\n# 1. Make sure all the requirements in install_requires are defined correctly and that their version is the correct one\n# 2. Add all the non .py files to the package_data and to the MANIFEST.in file\n# 3. Make sure that all the python directories have an __init__.py file\n\n# - Check that everything works fine by:\n# 1. Create a new virtual environment using `virtualenv coach_env -p python3`\n# 2. Run `pip install -e .`\n# 3. Run `coach -p CartPole_DQN` and make sure it works\n# 4. Run `dashboard` and make sure it works\n\n# - If everything works fine, build and upload the package to PyPi:\n# 1. Update the version of Coach in the call to setup()\n# 2. Remove the directories build, dist and rl_coach.egg-info if they exist\n# 3. Run `python setup.py sdist`\n# 4. Run `twine upload dist/*`\n\nslim_package = False  # if true build aws package with partial dependencies, otherwise, build full package\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\ninstall_requires = list()\nextras = dict()\nexcluded_packages = [\'kubernetes\', \'tensorflow\'] if slim_package else []\n\nwith open(path.join(here, \'requirements.txt\'), \'r\') as f:\n    for line in f:\n        package = line.strip()\n        if any(p in package for p in excluded_packages):\n            continue\n        install_requires.append(package)\n\n# check if system has CUDA enabled GPU\np = subprocess.Popen([\'command -v nvidia-smi\'], stdout=subprocess.PIPE, shell=True)\nout = p.communicate()[0].decode(\'UTF-8\')\nusing_GPU = out != \'\'\n\nif not using_GPU:\n    if not slim_package:\n        # For linux wth no GPU, we install the Intel optimized version of TensorFlow\n        if sys.platform == ""linux"" or sys.platform == ""linux2"":\n            # CI: limiting version to 1.13.1 due to\n            # https://github.com/tensorflow/tensorflow/issues/29617\n            # (reproduced with intel-tensorflow 1.14.0 but not with 1.13.1)\n            install_requires.append(\'intel-tensorflow==1.13.1\')\n        else:\n            install_requires.append(\'tensorflow>=1.9.0,<=1.14.0\')\n    extras[\'mxnet\'] = [\'mxnet-mkl>=1.3.0\']\nelse:\n    if not slim_package:\n        install_requires.append(\'tensorflow-gpu>=1.9.0,<=1.14.0\')\n    extras[\'mxnet\'] = [\'mxnet-cu90mkl>=1.3.0\']\n\nall_deps = []\nfor group_name in extras:\n    all_deps += extras[group_name]\nextras[\'all\'] = all_deps\n\n\nsetup(\n    name=\'rl-coach\' if not slim_package else \'rl-coach-slim\',\n    version=\'1.0.1\',\n    description=\'Reinforcement Learning Coach enables easy experimentation with state of the art Reinforcement Learning algorithms.\',\n    url=\'https://github.com/NervanaSystems/coach\',\n    author=\'Intel AI Lab\',\n    author_email=\'coach@intel.com\',\n    packages=find_packages(),\n    python_requires="">=3.5.*"",\n    install_requires=install_requires,\n    extras_require=extras,\n    package_data={\'rl_coach\': [\'dashboard_components/*.css\',\n                               \'environments/doom/*.cfg\',\n                               \'environments/doom/*.wad\',\n                               \'environments/mujoco/common/*.xml\',\n                               \'environments/mujoco/*.xml\',\n                               \'environments/*.ini\',\n                               \'tests/*.ini\']},\n    entry_points={\n        \'console_scripts\': [\n            \'coach=rl_coach.coach:main\',\n            \'dashboard=rl_coach.dashboard:main\'\n        ],\n    }\n)\n'"
docs_raw/__init__.py,0,b''
rl_coach/__init__.py,0,b''
rl_coach/base_parameters.py,0,"b'#\n#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport inspect\nimport json\nimport os\nimport sys\nimport types\nfrom collections import OrderedDict\nfrom enum import Enum\nfrom typing import Dict, List, Union\n\nfrom rl_coach.core_types import TrainingSteps, EnvironmentSteps, GradientClippingMethod, RunPhase, \\\n    SelectedPhaseOnlyDumpFilter, MaxDumpFilter\nfrom rl_coach.filters.filter import NoInputFilter\nfrom rl_coach.logger import screen\n\n\nclass Frameworks(Enum):\n    tensorflow = ""TensorFlow""\n    mxnet = ""MXNet""\n\n\nclass EmbedderScheme(Enum):\n    Empty = ""Empty""\n    Shallow = ""Shallow""\n    Medium = ""Medium""\n    Deep = ""Deep""\n\n\nclass MiddlewareScheme(Enum):\n    Empty = ""Empty""\n    Shallow = ""Shallow""\n    Medium = ""Medium""\n    Deep = ""Deep""\n\n\nclass EmbeddingMergerType(Enum):\n    Concat = 0\n    Sum = 1\n    #ConcatDepthWise = 2\n    #Multiply = 3\n\nclass RunType(Enum):\n    ORCHESTRATOR = ""orchestrator""\n    TRAINER = ""trainer""\n    ROLLOUT_WORKER = ""rollout-worker""\n\n    def __str__(self):\n        return self.value\n\n\nclass DeviceType(Enum):\n    CPU = \'cpu\'\n    GPU = \'gpu\'\n\n\nclass Device(object):\n    def __init__(self, device_type: DeviceType, index: int=0):\n        """"""\n        :param device_type: type of device (CPU/GPU)\n        :param index: index of device (only used if device type is GPU)\n        """"""\n        self._device_type = device_type\n        self._index = index\n\n    @property\n    def device_type(self):\n        return self._device_type\n\n    @property\n    def index(self):\n        return self._index\n\n    def __str__(self):\n        return ""{}{}"".format(self._device_type, self._index)\n\n    def __repr__(self):\n        return str(self)\n\n\n# DistributedCoachSynchronizationType provides the synchronization type for distributed Coach.\n# The default value is None, which means the algorithm or preset cannot be used with distributed Coach.\nclass DistributedCoachSynchronizationType(Enum):\n    # In SYNC mode, the trainer waits for all the experiences to be gathered from distributed rollout workers before\n    # training a new policy and the rollout workers wait for a new policy before gathering experiences.\n    SYNC = ""sync""\n\n    # In ASYNC mode, the trainer doesn\'t wait for any set of experiences to be gathered from distributed rollout workers\n    # and the rollout workers continously gather experiences loading new policies, whenever they become available.\n    ASYNC = ""async""\n\n\ndef iterable_to_items(obj):\n    if isinstance(obj, dict) or isinstance(obj, OrderedDict) or isinstance(obj, types.MappingProxyType):\n        items = obj.items()\n    elif isinstance(obj, list):\n        items = enumerate(obj)\n    else:\n        raise ValueError(""The given object is not a dict or a list"")\n    return items\n\n\ndef unfold_dict_or_list(obj: Union[Dict, List, OrderedDict]):\n    """"""\n    Recursively unfolds all the parameters in dictionaries and lists\n    :param obj: a dictionary or list to unfold\n    :return: the unfolded parameters dictionary\n    """"""\n    parameters = OrderedDict()\n    items = iterable_to_items(obj)\n    for k, v in items:\n        if isinstance(v, dict) or isinstance(v, list) or isinstance(v, OrderedDict):\n            if \'tensorflow.\' not in str(v.__class__):\n                parameters[k] = unfold_dict_or_list(v)\n        elif \'tensorflow.\' in str(v.__class__):\n            parameters[k] = v\n        elif hasattr(v, \'__dict__\'):\n            sub_params = v.__dict__\n            if \'__objclass__\' not in sub_params.keys():\n                try:\n                    parameters[k] = unfold_dict_or_list(sub_params)\n                except RecursionError:\n                    parameters[k] = sub_params\n                parameters[k][\'__class__\'] = v.__class__.__name__\n            else:\n                # unfolding this type of object will result in infinite recursion\n                parameters[k] = sub_params\n        else:\n            parameters[k] = v\n    if not isinstance(obj, OrderedDict) and not isinstance(obj, list):\n        parameters = OrderedDict(sorted(parameters.items()))\n    return parameters\n\n\nclass Parameters(object):\n    def __setattr__(self, key, value):\n        caller_name = sys._getframe(1).f_code.co_name\n\n        if caller_name != \'__init__\' and not hasattr(self, key):\n            raise TypeError(""Parameter \'{}\' does not exist in {}. Parameters are only to be defined in a constructor of""\n                            "" a class inheriting from Parameters. In order to explicitly register a new parameter ""\n                            ""outside of a constructor use register_var()."".\n                            format(key, self.__class__))\n        object.__setattr__(self, key, value)\n\n    @property\n    def path(self):\n        if hasattr(self, \'parameterized_class_name\'):\n            module_path = os.path.relpath(inspect.getfile(self.__class__), os.getcwd())[:-3] + \'.py\'\n\n            return \':\'.join([module_path, self.parameterized_class_name])\n        else:\n            raise ValueError(""The parameters class does not have an attached class it parameterizes. ""\n                             ""The self.parameterized_class_name should be set to the parameterized class."")\n\n    def register_var(self, key, value):\n        if hasattr(self, key):\n            raise TypeError(""Cannot register an already existing parameter \'{}\'. "".format(key))\n        object.__setattr__(self, key, value)\n\n    def __str__(self):\n        result = ""\\""{}\\"" {}\\n"".format(self.__class__.__name__,\n                                   json.dumps(unfold_dict_or_list(self.__dict__), indent=4, default=repr))\n        return result\n\n\nclass AlgorithmParameters(Parameters):\n    def __init__(self):\n        # Architecture parameters\n        self.use_accumulated_reward_as_measurement = False\n\n        # Agent parameters\n        self.num_consecutive_playing_steps = EnvironmentSteps(1)\n        self.num_consecutive_training_steps = 1  # TODO: update this to TrainingSteps\n\n        self.heatup_using_network_decisions = False\n        self.discount = 0.99\n        self.apply_gradients_every_x_episodes = 5\n        self.num_steps_between_copying_online_weights_to_target = TrainingSteps(0)\n        self.rate_for_copying_weights_to_target = 1.0\n        self.load_memory_from_file_path = None\n        self.store_transitions_only_when_episodes_are_terminated = False\n\n        # HRL / HER related params\n        self.in_action_space = None\n\n        # distributed agents params\n        self.share_statistics_between_workers = True\n\n        # n-step returns\n        self.n_step = -1  # calculate the total return (no bootstrap, by default)\n\n        # Distributed Coach params\n        self.distributed_coach_synchronization_type = None\n\n        # Should the workers wait for full episode\n        self.act_for_full_episodes = False\n\n        # Support for parameter noise\n        self.supports_parameter_noise = False\n\n        # Override, in retrospective, all the episode rewards with the last reward in the episode\n        # (sometimes useful for sparse, end of the episode, rewards problems)\n        self.override_episode_rewards_with_the_last_transition_reward = False\n\n        # Filters - TODO consider creating a FilterParameters class and initialize the filters with it\n        self.update_pre_network_filters_state_on_train = False\n        self.update_pre_network_filters_state_on_inference = True\n        \n\nclass PresetValidationParameters(Parameters):\n    def __init__(self,\n                 test=False,\n                 min_reward_threshold=0,\n                 max_episodes_to_achieve_reward=1,\n                 num_workers=1,\n                 reward_test_level=None,\n                 test_using_a_trace_test=True,\n                 trace_test_levels=None,\n                 trace_max_env_steps=5000,\n                 read_csv_tries=200):\n        """"""\n        :param test:\n            A flag which specifies if the preset should be tested as part of the validation process.\n        :param min_reward_threshold:\n            The minimum reward that the agent should pass after max_episodes_to_achieve_reward episodes when the\n            preset is run.\n        :param max_episodes_to_achieve_reward:\n            The maximum number of episodes that the agent should train using the preset in order to achieve the\n            reward specified by min_reward_threshold.\n        :param num_workers:\n            The number of workers that should be used when running this preset in the test suite for validation.\n        :param reward_test_level:\n            The environment level or levels, given by a list of strings, that should be tested as part of the\n            reward tests suite.\n        :param test_using_a_trace_test:\n            A flag that specifies if the preset should be run as part of the trace tests suite.\n        :param trace_test_levels:\n            The environment level or levels, given by a list of strings, that should be tested as part of the\n            trace tests suite.\n        :param trace_max_env_steps:\n            An integer representing the maximum number of environment steps to run when running this preset as part\n            of the trace tests suite.\n        :param read_csv_tries:\n            The number of retries to attempt for reading the experiment csv file, before declaring failure.\n        """"""\n        super().__init__()\n\n        # setting a seed will only work for non-parallel algorithms. Parallel algorithms add uncontrollable noise in\n        # the form of different workers starting at different times, and getting different assignments of CPU\n        # time from the OS.\n\n        # Testing parameters\n        self.test = test\n        self.min_reward_threshold = min_reward_threshold\n        self.max_episodes_to_achieve_reward = max_episodes_to_achieve_reward\n        self.num_workers = num_workers\n        self.reward_test_level = reward_test_level\n        self.test_using_a_trace_test = test_using_a_trace_test\n        self.trace_test_levels = trace_test_levels\n        self.trace_max_env_steps = trace_max_env_steps\n        self.read_csv_tries = read_csv_tries\n\n\nclass NetworkParameters(Parameters):\n    def __init__(self,\n                 force_cpu=False,\n                 async_training=False,\n                 shared_optimizer=True,\n                 scale_down_gradients_by_number_of_workers_for_sync_training=True,\n                 clip_gradients=None,\n                 gradients_clipping_method=GradientClippingMethod.ClipByGlobalNorm,\n                 l2_regularization=0,\n                 learning_rate=0.00025,\n                 learning_rate_decay_rate=0,\n                 learning_rate_decay_steps=0,\n                 input_embedders_parameters={},\n                 embedding_merger_type=EmbeddingMergerType.Concat,\n                 middleware_parameters=None,\n                 heads_parameters=[],\n                 use_separate_networks_per_head=False,\n                 optimizer_type=\'Adam\',\n                 optimizer_epsilon=0.0001,\n                 adam_optimizer_beta1=0.9,\n                 adam_optimizer_beta2=0.99,\n                 rms_prop_optimizer_decay=0.9,\n                 batch_size=32,\n                 replace_mse_with_huber_loss=False,\n                 create_target_network=False,\n                 tensorflow_support=True,\n                 softmax_temperature=1):\n        """"""\n        :param force_cpu:\n            Force the neural networks to run on the CPU even if a GPU is available\n        :param async_training:\n            If set to True, asynchronous training will be used, meaning that each workers will progress in its own\n            speed, while not waiting for the rest of the workers to calculate their gradients.\n        :param shared_optimizer:\n            If set to True, a central optimizer which will be shared with all the workers will be used for applying\n            gradients to the network. Otherwise, each worker will have its own optimizer with its own internal\n            parameters that will only be affected by the gradients calculated by that worker\n        :param scale_down_gradients_by_number_of_workers_for_sync_training:\n            If set to True, in synchronous training, the gradients of each worker will be scaled down by the\n            number of workers. This essentially means that the gradients applied to the network are the average\n            of the gradients over all the workers.\n        :param clip_gradients:\n            A value that will be used for clipping the gradients of the network. If set to None, no gradient clipping\n            will be applied. Otherwise, the gradients will be clipped according to the gradients_clipping_method.\n        :param gradients_clipping_method:\n            A gradient clipping method, defined by a GradientClippingMethod enum, and that will be used to clip the\n            gradients of the network. This will only be used if the clip_gradients value is defined as a value other\n            than None.\n        :param l2_regularization:\n            A L2 regularization weight that will be applied to the network weights while calculating the loss function\n        :param learning_rate:\n            The learning rate for the network\n        :param learning_rate_decay_rate:\n            If this value is larger than 0, an exponential decay will be applied to the network learning rate.\n            The rate of the decay is defined by this parameter, and the number of training steps the decay will be\n            applied is defined by learning_rate_decay_steps. Notice that both parameters should be defined in order\n            for this to work correctly.\n        :param learning_rate_decay_steps:\n            If the learning_rate_decay_rate of the network is larger than 0, an exponential decay will be applied to\n            the network learning rate. The number of steps the decay will be applied is defined by this parameter.\n            Notice that both this parameter, as well as learning_rate_decay_rate should be defined in order for the\n            learning rate decay to work correctly.\n        :param input_embedders_parameters:\n            A dictionary mapping between input names and input embedders (InputEmbedderParameters) to use for the\n            network. Each of the keys is an input name as returned from the environment in the state.\n            For example, if the environment returns a state containing \'observation\' and \'measurements\', then\n            the keys for the input embedders dictionary can be either \'observation\' to use the observation as input,\n            \'measurements\' to use the measurements as input, or both.\n            The embedder type will be automatically selected according to the input type. Vector inputs will\n            produce a fully connected embedder, and image inputs will produce a convolutional embedder.\n        :param embedding_merger_type:\n            The type of embedding merging to use, given by one of the EmbeddingMergerType enum values.\n            This will be used to merge the outputs of all the input embedders into a single embbeding.\n        :param middleware_parameters:\n            The parameters of the middleware to use, given by a MiddlewareParameters object.\n            Each network will have only a single middleware embedder which will take the merged embeddings from the\n            input embedders and pass them through more neural network layers.\n        :param heads_parameters:\n            A list of heads for the network given by their corresponding HeadParameters.\n            Each network can have one or multiple network heads, where each one will take the output of the middleware\n            and make some additional computation on top of it. Additionally, each head calculates a weighted loss value,\n            and the loss values from all the heads will be summed later on.\n        :param use_separate_networks_per_head:\n            A flag that allows using different copies of the input embedders and middleware for each one of the heads.\n            Regularly, the heads will have a shared input, but in the case where use_separate_networks_per_head is set\n            to True, each one of the heads will get a different input.\n        :param optimizer_type:\n            A string specifying the optimizer type to use for updating the network. The available optimizers are\n            Adam, RMSProp and LBFGS.\n        :param optimizer_epsilon:\n            An internal optimizer parameter used for Adam and RMSProp.\n        :param adam_optimizer_beta1:\n            An beta1 internal optimizer parameter used for Adam. It will be used only if Adam was selected as the\n            optimizer for the network.\n        :param adam_optimizer_beta2:\n            An beta2 internal optimizer parameter used for Adam. It will be used only if Adam was selected as the\n            optimizer for the network.\n        :param rms_prop_optimizer_decay:\n            The decay value for the RMSProp optimizer, which will be used only in case the RMSProp optimizer was\n            selected for this network.\n        :param batch_size:\n            The batch size to use when updating the network.\n        :param replace_mse_with_huber_loss:\n        :param create_target_network:\n            If this flag is set to True, an additional copy of the network will be created and initialized with the\n            same weights as the online network. It can then be queried, and its weights can be synced from the\n            online network at will.\n        :param tensorflow_support:\n            A flag which specifies if the network is supported by the TensorFlow framework.\n        :param softmax_temperature:\n            If a softmax is present in the network head output, use this temperature\n        """"""\n        super().__init__()\n        self.framework = Frameworks.tensorflow\n        self.sess = None\n\n        # hardware parameters\n        self.force_cpu = force_cpu\n\n        # distributed training options\n        self.async_training = async_training\n        self.shared_optimizer = shared_optimizer\n        self.scale_down_gradients_by_number_of_workers_for_sync_training = scale_down_gradients_by_number_of_workers_for_sync_training\n\n        # regularization\n        self.clip_gradients = clip_gradients\n        self.gradients_clipping_method = gradients_clipping_method\n        self.l2_regularization = l2_regularization\n\n        # learning rate\n        self.learning_rate = learning_rate\n        self.learning_rate_decay_rate = learning_rate_decay_rate\n        self.learning_rate_decay_steps = learning_rate_decay_steps\n\n        # structure\n        self.input_embedders_parameters = input_embedders_parameters\n        self.embedding_merger_type = embedding_merger_type\n        self.middleware_parameters = middleware_parameters\n        self.heads_parameters = heads_parameters\n        self.use_separate_networks_per_head = use_separate_networks_per_head\n        self.optimizer_type = optimizer_type\n        self.replace_mse_with_huber_loss = replace_mse_with_huber_loss\n        self.create_target_network = create_target_network\n\n        # Framework support\n        self.tensorflow_support = tensorflow_support\n\n        # Hyper-Parameter values\n        self.optimizer_epsilon = optimizer_epsilon\n        self.adam_optimizer_beta1 = adam_optimizer_beta1\n        self.adam_optimizer_beta2 = adam_optimizer_beta2\n        self.rms_prop_optimizer_decay = rms_prop_optimizer_decay\n        self.batch_size = batch_size\n        self.softmax_temperature = softmax_temperature\n\n\nclass NetworkComponentParameters(Parameters):\n    def __init__(self, dense_layer):\n        self.dense_layer = dense_layer\n\n\nclass VisualizationParameters(Parameters):\n    def __init__(self,\n                 print_networks_summary=False,\n                 dump_csv=True,\n                 dump_signals_to_csv_every_x_episodes=5,\n                 dump_gifs=False,\n                 dump_mp4=False,\n                 video_dump_methods=None,\n                 dump_in_episode_signals=False,\n                 dump_parameters_documentation=True,\n                 render=False,\n                 native_rendering=False,\n                 max_fps_for_human_control=10,\n                 tensorboard=False,\n                 add_rendered_image_to_env_response=False):\n        """"""\n        :param print_networks_summary:\n            If set to True, a summary of all the networks structure will be printed at the beginning of the experiment\n        :param dump_csv:\n            If set to True, the logger will dump logs to a csv file once in every dump_signals_to_csv_every_x_episodes\n            episodes. The logs can be later used to visualize the training process using Coach Dashboard.\n        :param dump_signals_to_csv_every_x_episodes:\n            Defines the number of episodes between writing new data to the csv log files. Lower values can affect\n            performance, as writing to disk may take time, and it is done synchronously.\n        :param dump_gifs:\n            If set to True, GIF videos of the environment will be stored into the experiment directory according to\n            the filters defined in video_dump_methods.\n        :param dump_mp4:\n            If set to True, MP4 videos of the environment will be stored into the experiment directory according to\n            the filters defined in video_dump_methods.\n        :param dump_in_episode_signals:\n            If set to True, csv files will be dumped for each episode for inspecting different metrics within the\n            episode. This means that for each step in each episode, different metrics such as the reward, the\n            future return, etc. will be saved. Setting this to True may affect performance severely, and therefore\n            this should be used only for debugging purposes.\n        :param dump_parameters_documentation:\n            If set to True, a json file containing all the agent parameters will be saved in the experiment directory.\n            This may be very useful for inspecting the values defined for each parameters and making sure that all\n            the parameters are defined as expected.\n        :param render:\n            If set to True, the environment render function will be called for each step, rendering the image of the\n            environment. This may affect the performance of training, and is highly dependent on the environment.\n            By default, Coach uses PyGame to render the environment image instead of the environment specific rendered.\n            To change this, use the native_rendering flag.\n        :param native_rendering:\n            If set to True, the environment native renderer will be used for rendering the environment image.\n            In some cases this can be slower than rendering using PyGame through Coach, but in other cases the\n            environment opens its native renderer by default, so rendering with PyGame is an unnecessary overhead.\n        :param max_fps_for_human_control:\n            The maximum number of frames per second used while playing the environment as a human. This only has\n            effect while using the --play flag for Coach.\n        :param tensorboard:\n            If set to True, TensorBoard summaries will be stored in the experiment directory. This can later be\n            loaded in TensorBoard in order to visualize the training process.\n        :param video_dump_methods:\n            A list of dump methods that will be used as filters for deciding when to save videos.\n            The filters in the list will be checked one after the other until the first dump method that returns\n            false for should_dump() in the environment class. This list will only be used if dump_mp4 or dump_gif are\n            set to True.\n        :param add_rendered_image_to_env_response:\n            Some environments have a different observation compared to the one displayed while rendering.\n            For some cases it can be useful to pass the rendered image to the agent for visualization purposes.\n            If this flag is set to True, the rendered image will be added to the environment EnvResponse object,\n            which will be passed to the agent and allow using those images.\n        """"""\n        super().__init__()\n        if video_dump_methods is None:\n            video_dump_methods = [SelectedPhaseOnlyDumpFilter(RunPhase.TEST), MaxDumpFilter()]\n        self.print_networks_summary = print_networks_summary\n        self.dump_csv = dump_csv\n        self.dump_gifs = dump_gifs\n        self.dump_mp4 = dump_mp4\n        self.dump_signals_to_csv_every_x_episodes = dump_signals_to_csv_every_x_episodes\n        self.dump_in_episode_signals = dump_in_episode_signals\n        self.dump_parameters_documentation = dump_parameters_documentation\n        self.render = render\n        self.native_rendering = native_rendering\n        self.max_fps_for_human_control = max_fps_for_human_control\n        self.tensorboard = tensorboard\n        self.video_dump_filters = video_dump_methods\n        self.add_rendered_image_to_env_response = add_rendered_image_to_env_response\n\n\nclass AgentParameters(Parameters):\n    def __init__(self, algorithm: AlgorithmParameters, exploration: \'ExplorationParameters\', memory: \'MemoryParameters\',\n                 networks: Dict[str, NetworkParameters], visualization: VisualizationParameters=VisualizationParameters()):\n        """"""\n        :param algorithm:\n            A class inheriting AlgorithmParameters.\n            The parameters used for the specific algorithm used by the agent.\n            These parameters can be later referenced in the agent implementation through self.ap.algorithm.\n        :param exploration:\n            Either a class inheriting ExplorationParameters or a dictionary mapping between action\n            space types and their corresponding ExplorationParameters. If a dictionary was used,\n            when the agent will be instantiated, the correct exploration policy parameters will be used\n            according to the real type of the environment action space.\n            These parameters will be used to instantiate the exporation policy.\n        :param memory:\n            A class inheriting MemoryParameters. It defines all the parameters used by the memory module.\n        :param networks:\n            A dictionary mapping between network names and their corresponding network parmeters, defined\n            as a class inheriting NetworkParameters. Each element will be used in order to instantiate\n            a NetworkWrapper class, and all the network wrappers will be stored in the agent under\n            self.network_wrappers. self.network_wrappers is a dict mapping between the network name that\n            was given in the networks dict, and the instantiated network wrapper.\n        :param visualization:\n            A class inheriting VisualizationParameters and defining various parameters that can be\n            used for visualization purposes, such as printing to the screen, rendering, and saving videos.\n        """"""\n        super().__init__()\n        self.visualization = visualization\n        self.algorithm = algorithm\n        self.exploration = exploration\n        self.memory = memory\n        self.network_wrappers = networks\n        self.input_filter = None\n        self.output_filter = None\n        self.pre_network_filter = NoInputFilter()\n        self.full_name_id = None\n        self.name = None\n        self.is_a_highest_level_agent = True\n        self.is_a_lowest_level_agent = True\n        self.task_parameters = None\n        self.is_batch_rl_training = False\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.agent:Agent\'\n\n\nclass TaskParameters(Parameters):\n    def __init__(self, framework_type: Frameworks=Frameworks.tensorflow, evaluate_only: int=None, use_cpu: bool=False,\n                 experiment_path=\'/tmp\', seed=None, checkpoint_save_secs=None, checkpoint_restore_dir=None,\n                 checkpoint_restore_path=None, checkpoint_save_dir=None, export_onnx_graph: bool=False,\n                 apply_stop_condition: bool=False, num_gpu: int=1):\n        """"""\n        :param framework_type: deep learning framework type. currently only tensorflow is supported\n        :param evaluate_only: if not None, the task will be used only for evaluating the model for the given number of steps.\n                                A value of 0 means that task will be evaluated for an infinite number of steps.\n        :param use_cpu: use the cpu for this task\n        :param experiment_path: the path to the directory which will store all the experiment outputs\n        :param seed: a seed to use for the random numbers generator\n        :param checkpoint_save_secs: the number of seconds between each checkpoint saving\n        :param checkpoint_restore_dir:\n                [DEPECRATED - will be removed in one of the next releases - switch to checkpoint_restore_path]\n                the dir to restore the checkpoints from\n        :param checkpoint_restore_path: the path to restore the checkpoints from\n        :param checkpoint_save_dir: the directory to store the checkpoints in\n        :param export_onnx_graph: If set to True, this will export an onnx graph each time a checkpoint is saved\n        :param apply_stop_condition: If set to True, this will apply the stop condition defined by reaching a target success rate\n        :param num_gpu: number of GPUs to use\n        """"""\n        self.framework_type = framework_type\n        self.task_index = 0  # TODO: not really needed\n        self.evaluate_only = evaluate_only\n        self.use_cpu = use_cpu\n        self.experiment_path = experiment_path\n        self.checkpoint_save_secs = checkpoint_save_secs\n        if checkpoint_restore_dir:\n            screen.warning(\'TaskParameters.checkpoint_restore_dir is DEPECRATED and will be removed in one of the next \'\n                           \'releases. Please switch to using TaskParameters.checkpoint_restore_path, with your \'\n                           \'directory path. \')\n            self.checkpoint_restore_path = checkpoint_restore_dir\n        else:\n            self.checkpoint_restore_path = checkpoint_restore_path\n        self.checkpoint_save_dir = checkpoint_save_dir\n        self.seed = seed\n        self.export_onnx_graph = export_onnx_graph\n        self.apply_stop_condition = apply_stop_condition\n        self.num_gpu = num_gpu\n\n\nclass DistributedTaskParameters(TaskParameters):\n    def __init__(self, framework_type: Frameworks, parameters_server_hosts: str, worker_hosts: str, job_type: str,\n                 task_index: int, evaluate_only: int=None, num_tasks: int=None,\n                 num_training_tasks: int=None, use_cpu: bool=False, experiment_path=None, dnd=None,\n                 shared_memory_scratchpad=None, seed=None, checkpoint_save_secs=None, checkpoint_restore_path=None,\n                 checkpoint_save_dir=None, export_onnx_graph: bool=False, apply_stop_condition: bool=False):\n        """"""\n        :param framework_type: deep learning framework type. currently only tensorflow is supported\n        :param evaluate_only: if not None, the task will be used only for evaluating the model for the given number of steps.\n                                A value of 0 means that task will be evaluated for an infinite number of steps.\n        :param parameters_server_hosts: comma-separated list of hostname:port pairs to which the parameter servers are\n                                        assigned\n        :param worker_hosts: comma-separated list of hostname:port pairs to which the workers are assigned\n        :param job_type: the job type - either ps (short for parameters server) or worker\n        :param task_index: the index of the process\n        :param num_tasks: the number of total tasks that are running (not including the parameters server)\n        :param num_training_tasks: the number of tasks that are training (not including the parameters server)\n        :param use_cpu: use the cpu for this task\n        :param experiment_path: the path to the directory which will store all the experiment outputs\n        :param dnd: an external DND to use for NEC. This is a workaround needed for a shared DND not using the scratchpad.\n        :param seed: a seed to use for the random numbers generator\n        :param checkpoint_save_secs: the number of seconds between each checkpoint saving\n        :param checkpoint_restore_path: the path to restore the checkpoints from\n        :param checkpoint_save_dir: the directory to store the checkpoints in\n        :param export_onnx_graph: If set to True, this will export an onnx graph each time a checkpoint is saved\n        :param apply_stop_condition: If set to True, this will apply the stop condition defined by reaching a target success rate\n\n        """"""\n        super().__init__(framework_type=framework_type, evaluate_only=evaluate_only, use_cpu=use_cpu,\n                         experiment_path=experiment_path, seed=seed, checkpoint_save_secs=checkpoint_save_secs,\n                         checkpoint_restore_path=checkpoint_restore_path, checkpoint_save_dir=checkpoint_save_dir,\n                         export_onnx_graph=export_onnx_graph, apply_stop_condition=apply_stop_condition)\n        self.parameters_server_hosts = parameters_server_hosts\n        self.worker_hosts = worker_hosts\n        self.job_type = job_type\n        self.task_index = task_index\n        self.num_tasks = num_tasks\n        self.num_training_tasks = num_training_tasks\n        self.device = None  # the replicated device which will be used for the global parameters\n        self.worker_target = None\n        self.dnd = dnd\n        self.shared_memory_scratchpad = shared_memory_scratchpad\n'"
rl_coach/checkpoint.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n""""""\nModule providing helper classes and functions for reading/writing checkpoint state\n""""""\n\nimport os\nimport re\nfrom typing import List, Union, Tuple\n\n\nclass SingleCheckpoint(object):\n    """"""\n    Helper class for storing checkpoint name and number\n    """"""\n    def __init__(self, num: int, name: str):\n        """"""\n        :param num: checkpoint number\n        :param name: checkpoint name (i.e. the prefix for all checkpoint files)\n        """"""\n        self._num = num\n        self._name = name\n\n    @property\n    def num(self) -> int:\n        return self._num\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    def __str__(self):\n        return self._name\n\n    def __repr__(self):\n        return str(self)\n\n    def __eq__(self, other: \'SingleCheckpoint\'):\n        if not isinstance(other, SingleCheckpoint):\n            return False\n        return self._name == other._name and self._num == other._num\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n\nclass CheckpointState(object):\n    """"""\n    Helper class for checkpoint directory information. It replicates\n    the CheckpointState protobuf class in tensorflow with addition of\n    two new functions: last_checkpoint() and all_checkpoints()\n    """"""\n    def __init__(self, checkpoints: List[SingleCheckpoint], checkpoint_dir: str):\n        """"""\n        :param checkpoints: sorted list of checkpoints from oldest to newest. checkpoint[-1] is\n            considered to be the most recent checkpoint.\n        :param checkpoint_dir: checkpoint directory which is added to the paths\n        """"""\n        self._checkpoints = checkpoints\n        self._checkpoin_dir = checkpoint_dir\n\n    @property\n    def all_checkpoints(self) -> List[SingleCheckpoint]:\n        """"""\n        :return: list of all checkpoints\n        """"""\n        return self._checkpoints\n\n    @property\n    def last_checkpoint(self) -> SingleCheckpoint:\n        """"""\n        :return: the most recent checkpoint\n        """"""\n        return self._checkpoints[-1]\n\n    @property\n    def all_model_checkpoint_paths(self) -> List[str]:\n        """"""\n        TF compatible function call to get all checkpoints\n        :return: list of all available model checkpoint paths\n        """"""\n        return [os.path.join(self._checkpoin_dir, c.name) for c in self._checkpoints]\n\n    @property\n    def model_checkpoint_path(self) -> str:\n        """"""\n        TF compatible call to get most recent checkpoint\n        :return: path of the most recent model checkpoint\n        """"""\n        return os.path.join(self._checkpoin_dir, self._checkpoints[-1].name)\n\n    def __str__(self):\n        out_str = \'model_checkpoint_path: {}\\n\'.format(self.model_checkpoint_path)\n        for c in self.all_model_checkpoint_paths:\n            out_str += \'all_model_checkpoint_paths: {}\\n\'.format(c)\n        return out_str\n\n    def __repr__(self):\n        return str(self._checkpoints)\n\n\nclass CheckpointStateFile(object):\n    """"""\n    Helper class for reading from and writing to the checkpoint state file\n    """"""\n    checkpoint_state_filename = \'.coach_checkpoint\'\n\n    def __init__(self, checkpoint_dir: str):\n        self._checkpoint_state_path = os.path.join(checkpoint_dir, self.checkpoint_state_filename)\n\n    def exists(self) -> bool:\n        """"""\n        :return: True if checkpoint state file exists, false otherwise\n        """"""\n        return os.path.exists(self._checkpoint_state_path)\n\n    def read(self) -> Union[None, SingleCheckpoint]:\n        """"""\n        Read checkpoint state file and interpret its content\n        :return:\n        """"""\n        if not self.exists():\n            return None\n        with open(self._checkpoint_state_path, \'r\') as fd:\n            return CheckpointFilenameParser().parse(fd.read(256))\n\n    def write(self, data: SingleCheckpoint) -> None:\n        """"""\n        Writes data to checkpoint state file\n        :param data: string data\n        """"""\n        with open(self._checkpoint_state_path, \'w\') as fd:\n            fd.write(data.name)\n\n    @property\n    def filename(self) -> str:\n        return self.checkpoint_state_filename\n\n    @property\n    def path(self) -> str:\n        return self._checkpoint_state_path\n\n\nclass CheckpointStateReader(object):\n    """"""\n    Class for scanning checkpoint directory and updating the checkpoint state\n    """"""\n    def __init__(self, checkpoint_dir: str, checkpoint_state_optional: bool=True):\n        """"""\n        :param checkpoint_dir: path to checkpoint directory\n        :param checkpoint_state_optional: If True, checkpoint state file is optional and if not found,\n            directory is scanned to find the latest checkpoint. Default is True for backward compatibility\n        """"""\n        self._checkpoint_dir = checkpoint_dir\n        self._checkpoint_state_file = CheckpointStateFile(self._checkpoint_dir)\n        self._checkpoint_state_optional = checkpoint_state_optional\n\n    def get_latest(self) -> SingleCheckpoint:\n        """"""\n        Tries to read the checkpoint state file. If that fails, discovers latest by reading the entire directory.\n        :return: checkpoint object representing the latest checkpoint\n        """"""\n        latest = self._checkpoint_state_file.read()\n        if latest is None and self._checkpoint_state_optional:\n            all_checkpoints = _filter_checkpoint_files(os.listdir(self._checkpoint_dir))\n            if len(all_checkpoints) > 0:\n                latest = all_checkpoints[-1]\n        return latest\n\n    def get_all(self) -> List[SingleCheckpoint]:\n        """"""\n        Reads both the checkpoint state file as well as contents of the directory and merges them into one list.\n        :return: list of checkpoint objects\n        """"""\n        # discover all checkpoint files in directory if requested or if a valid checkpoint-state file doesn\'t exist\n        all_checkpoints = _filter_checkpoint_files(os.listdir(self._checkpoint_dir))\n        last_checkpoint = self._checkpoint_state_file.read()\n        if last_checkpoint is not None:\n            # remove excess checkpoints: higher checkpoint number, but not recent (e.g. from a previous run)\n            all_checkpoints = all_checkpoints[: all_checkpoints.index(last_checkpoint) + 1]\n        elif not self._checkpoint_state_optional:\n            # if last_checkpoint is not discovered from the checkpoint-state file and it isn\'t optional, then\n            # all checkpoint files discovered must be partial or invalid, so don\'t return anything\n            all_checkpoints.clear()\n        return all_checkpoints\n\n\nclass CheckpointStateUpdater(object):\n    """"""\n    Class for scanning checkpoint directory and updating the checkpoint state\n    """"""\n    def __init__(self, checkpoint_dir: str, read_all: bool=False):\n        """"""\n        :param checkpoint_dir: path to checkpoint directory\n        :param read_all: whether to scan the directory for existing checkpoints\n        """"""\n        self._checkpoint_dir = checkpoint_dir\n        self._checkpoint_state_file = CheckpointStateFile(checkpoint_dir)\n        self._all_checkpoints = list()\n        # Read checkpoint state and initialize\n        state_reader = CheckpointStateReader(checkpoint_dir)\n        if read_all:\n            self._all_checkpoints = state_reader.get_all()\n        else:\n            latest = state_reader.get_latest()\n            if latest is not None:\n                self._all_checkpoints = [latest]\n\n    def update(self, checkpoint: SingleCheckpoint) -> None:\n        """"""\n        Update the checkpoint state with the latest checkpoint.\n        :param checkpoint: SingleCheckpoint object containing name and number of checkpoint\n        """"""\n        self._all_checkpoints.append(checkpoint)\n        # Simply write checkpoint_name to checkpoint-state file\n        self._checkpoint_state_file.write(checkpoint)\n\n    @property\n    def last_checkpoint(self) -> Union[None, SingleCheckpoint]:\n        if len(self._all_checkpoints) == 0:\n            return None\n        return self._all_checkpoints[-1]\n\n    @property\n    def all_checkpoints(self) -> List[SingleCheckpoint]:\n        return self._all_checkpoints\n\n    def get_checkpoint_state(self) -> Union[None, CheckpointState]:\n        """"""\n        :return: The most recent checkpoint state\n        """"""\n        if len(self._all_checkpoints) == 0:\n            return None\n        return CheckpointState(self._all_checkpoints, self._checkpoint_dir)\n\n\nclass CheckpointFilenameParser(object):\n    """"""\n    Helper object for parsing filenames that are potentially checkpoints\n    """"""\n    coach_checkpoint_filename_pattern = r\'\\A(([0-9]+)[^0-9])?.*?\\.ckpt(-([0-9]+))?\'\n\n    def __init__(self):\n        self._prog = re.compile(self.coach_checkpoint_filename_pattern)\n\n    def parse(self, filename: str) -> Union[None, SingleCheckpoint]:\n        """"""\n        Tries to parse the filename using the checkpoint filename pattern. If successful,\n        it returns tuple of (checkpoint-number, checkpoint-name). Otherwise it returns None.\n        :param filename: filename to be parsed\n        :return: None or (checkpoint-number, checkpoint-name)\n        """"""\n        m = self._prog.search(filename)\n        if m is not None and (m.group(2) is not None or m.group(4) is not None):\n            assert m.group(2) is None or m.group(4) is None  # Only one group must be valid\n            checkpoint_num = int(m.group(2) if m.group(2) is not None else m.group(4))\n            return SingleCheckpoint(checkpoint_num, m.group(0))\n        return None\n\n\ndef _filter_checkpoint_files(filenames: List[str], sort_by_num: bool=True) -> List[SingleCheckpoint]:\n    """"""\n    Given a list of potential file names, return the ones that match checkpoint pattern along with\n    the checkpoint number of each file name.\n    :param filenames: list of all filenames\n    :param sort_by_num: whether to sort the output result by checkpoint number\n    :return: list of (checkpoint-number, checkpoint-filename) tuples\n    """"""\n    parser = CheckpointFilenameParser()\n    checkpoints = [ckp for ckp in [parser.parse(fn) for fn in filenames] if ckp is not None]\n    if sort_by_num:\n        checkpoints.sort(key=lambda x: x.num)\n    return checkpoints\n\n\ndef get_checkpoint_state(checkpoint_dir: str, all_checkpoints=False) ->Union[CheckpointState, None]:\n    """"""\n    Scan checkpoint directory and find the list of checkpoint files.\n    :param checkpoint_dir: directory where checkpoints are saved\n    :param all_checkpoints: if True, scan the directory and return list of all checkpoints\n        as well as the most recent one\n    :return: a CheckpointState for checkpoint_dir containing a sorted list of checkpoints by checkpoint-number.\n        If no matching files are found, returns None.\n    """"""\n    return CheckpointStateUpdater(checkpoint_dir, read_all=all_checkpoints).get_checkpoint_state()\n'"
rl_coach/coach.py,0,"b'# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nsys.path.append(\'.\')\n\nimport copy\nfrom configparser import ConfigParser, Error\nimport os\nfrom rl_coach import logger\nimport traceback\nfrom rl_coach.logger import screen, failed_imports\nimport argparse\nimport atexit\nimport time\nimport sys\nimport json\nfrom rl_coach.base_parameters import Frameworks, VisualizationParameters, TaskParameters, DistributedTaskParameters, \\\n    RunType, DistributedCoachSynchronizationType\nfrom rl_coach.core_types import TotalStepsCounter, RunPhase, PlayingStepsType, TrainingSteps, EnvironmentEpisodes, \\\n    EnvironmentSteps, StepMethod, Transition\nfrom multiprocessing import Process\nfrom multiprocessing.managers import BaseManager\nimport subprocess\nfrom glob import glob\n\nfrom rl_coach.graph_managers.graph_manager import HumanPlayScheduleParameters, GraphManager\nfrom rl_coach.utils import list_all_presets, short_dynamic_import, get_open_port, SharedMemoryScratchPad, get_base_dir\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.memories.backend.redis import RedisPubSubMemoryBackendParameters\nfrom rl_coach.memories.backend.memory_impl import construct_memory_params\nfrom rl_coach.data_stores.data_store import DataStoreParameters\nfrom rl_coach.data_stores.s3_data_store import S3DataStoreParameters\nfrom rl_coach.data_stores.nfs_data_store import NFSDataStoreParameters\nfrom rl_coach.data_stores.redis_data_store import RedisDataStoreParameters\nfrom rl_coach.data_stores.data_store_impl import get_data_store, construct_data_store_params\nfrom rl_coach.training_worker import training_worker\nfrom rl_coach.rollout_worker import rollout_worker\n\n\nif len(set(failed_imports)) > 0:\n    screen.warning(""Warning: failed to import the following packages - {}"".format(\', \'.join(set(failed_imports))))\n\n\ndef add_items_to_dict(target_dict, source_dict):\n    updated_task_parameters = copy.copy(source_dict)\n    updated_task_parameters.update(target_dict)\n    return updated_task_parameters\n\n\ndef open_dashboard(experiment_path):\n    """"""\n    open X11 based dashboard in a new process (nonblocking)\n    """"""\n    dashboard_path = \'python {}/dashboard.py\'.format(get_base_dir())\n    cmd = ""{} --experiment_dir {}"".format(dashboard_path, experiment_path)\n    screen.log_title(""Opening dashboard - experiment path: {}"".format(experiment_path))\n    # subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True, executable=""bash"")\n    subprocess.Popen(cmd, shell=True, executable=""bash"")\n\n\ndef start_graph(graph_manager: \'GraphManager\', task_parameters: \'TaskParameters\'):\n    """"""\n    Runs the graph_manager using the configured task_parameters.\n    This stand-alone method is a convenience for multiprocessing.\n    """"""\n    graph_manager.create_graph(task_parameters)\n\n    # let the adventure begin\n    if task_parameters.evaluate_only is not None:\n        steps_to_evaluate = task_parameters.evaluate_only if task_parameters.evaluate_only > 0 \\\n            else sys.maxsize\n        graph_manager.evaluate(EnvironmentSteps(steps_to_evaluate))\n    else:\n        graph_manager.improve()\n    graph_manager.close()\n\n\ndef handle_distributed_coach_tasks(graph_manager, args, task_parameters):\n    ckpt_inside_container = ""/checkpoint""\n\n    memory_backend_params = None\n    if args.memory_backend_params:\n        memory_backend_params = json.loads(args.memory_backend_params)\n        memory_backend_params[\'run_type\'] = str(args.distributed_coach_run_type)\n        graph_manager.agent_params.memory.register_var(\'memory_backend_params\', construct_memory_params(memory_backend_params))\n\n    data_store = None\n    data_store_params = None\n    if args.data_store_params:\n        data_store_params = construct_data_store_params(json.loads(args.data_store_params))\n        data_store_params.expt_dir = args.experiment_path\n        data_store_params.checkpoint_dir = ckpt_inside_container\n        graph_manager.data_store_params = data_store_params\n        data_store = get_data_store(data_store_params)\n\n    if args.distributed_coach_run_type == RunType.TRAINER:\n        task_parameters.checkpoint_save_dir = ckpt_inside_container\n        training_worker(\n            graph_manager=graph_manager,\n            data_store=data_store,\n            task_parameters=task_parameters,\n            is_multi_node_test=args.is_multi_node_test\n        )\n\n    if args.distributed_coach_run_type == RunType.ROLLOUT_WORKER:\n        rollout_worker(\n            graph_manager=graph_manager,\n            data_store=data_store,\n            num_workers=args.num_workers,\n            task_parameters=task_parameters\n        )\n\n\ndef handle_distributed_coach_orchestrator(args):\n    from rl_coach.orchestrators.kubernetes_orchestrator import KubernetesParameters, Kubernetes, \\\n        RunTypeParameters\n\n    ckpt_inside_container = ""/checkpoint""\n    arg_list = sys.argv[1:]\n    try:\n        i = arg_list.index(\'--distributed_coach_run_type\')\n        arg_list.pop(i)\n        arg_list.pop(i)\n    except ValueError:\n        pass\n\n    trainer_command = [\'python3\', \'rl_coach/coach.py\', \'--distributed_coach_run_type\', str(RunType.TRAINER)] + arg_list\n    rollout_command = [\'python3\', \'rl_coach/coach.py\', \'--distributed_coach_run_type\', str(RunType.ROLLOUT_WORKER)] + arg_list\n\n    if \'--experiment_name\' not in rollout_command:\n        rollout_command = rollout_command + [\'--experiment_name\', args.experiment_name]\n\n    if \'--experiment_name\' not in trainer_command:\n        trainer_command = trainer_command + [\'--experiment_name\', args.experiment_name]\n\n    memory_backend_params = None\n    if args.memory_backend == ""redispubsub"":\n        memory_backend_params = RedisPubSubMemoryBackendParameters()\n\n    ds_params_instance = None\n    if args.data_store == ""s3"":\n        ds_params = DataStoreParameters(""s3"", """", """")\n        ds_params_instance = S3DataStoreParameters(ds_params=ds_params, end_point=args.s3_end_point, bucket_name=args.s3_bucket_name,\n                                                   creds_file=args.s3_creds_file, checkpoint_dir=ckpt_inside_container, expt_dir=args.experiment_path)\n    elif args.data_store == ""nfs"":\n        ds_params = DataStoreParameters(""nfs"", ""kubernetes"", """")\n        ds_params_instance = NFSDataStoreParameters(ds_params)\n    elif args.data_store == ""redis"":\n        ds_params = DataStoreParameters(""redis"", ""kubernetes"", """")\n        ds_params_instance = RedisDataStoreParameters(ds_params)\n    else:\n        raise ValueError(""data_store {} found. Expected \'s3\' or \'nfs\'"".format(args.data_store))\n\n    worker_run_type_params = RunTypeParameters(args.image, rollout_command, run_type=str(RunType.ROLLOUT_WORKER), num_replicas=args.num_workers)\n    trainer_run_type_params = RunTypeParameters(args.image, trainer_command, run_type=str(RunType.TRAINER))\n\n    orchestration_params = KubernetesParameters([worker_run_type_params, trainer_run_type_params],\n                                                kubeconfig=\'~/.kube/config\',\n                                                memory_backend_parameters=memory_backend_params,\n                                                data_store_params=ds_params_instance)\n    orchestrator = Kubernetes(orchestration_params)\n    if not orchestrator.setup(args.checkpoint_restore_dir):\n        print(""Could not setup."")\n        return 1\n\n    if orchestrator.deploy_trainer():\n        print(""Successfully deployed trainer."")\n    else:\n        print(""Could not deploy trainer."")\n        return 1\n\n    if orchestrator.deploy_worker():\n        print(""Successfully deployed rollout worker(s)."")\n    else:\n        print(""Could not deploy rollout worker(s)."")\n        return 1\n\n    if args.dump_worker_logs:\n        screen.log_title(""Dumping rollout worker logs in: {}"".format(args.experiment_path))\n        orchestrator.worker_logs(path=args.experiment_path)\n\n    exit_code = 1\n    try:\n        exit_code = orchestrator.trainer_logs()\n    except KeyboardInterrupt:\n        pass\n\n    orchestrator.undeploy()\n    return exit_code\n\n\nclass CoachLauncher(object):\n    """"""\n    This class is responsible for gathering all user-specified configuration options, parsing them,\n    instantiating a GraphManager and then starting that GraphManager with either improve() or evaluate().\n    This class is also responsible for launching multiple processes.\n    It is structured so that it can be sub-classed to provide alternate mechanisms to configure and launch\n    Coach jobs.\n\n    The key entry-point for this class is the .launch() method which is expected to be called from __main__\n    and handle absolutely everything for a job.\n    """"""\n\n    def launch(self):\n        """"""\n        Main entry point for the class, and the standard way to run coach from the command line.\n        Parses command-line arguments through argparse, instantiates a GraphManager and then runs it.\n        """"""\n        parser = self.get_argument_parser()\n        args = self.get_config_args(parser)\n        graph_manager = self.get_graph_manager_from_args(args)\n        self.run_graph_manager(graph_manager, args)\n\n    def get_graph_manager_from_args(self, args: argparse.Namespace) -> \'GraphManager\':\n        """"""\n        Return the graph manager according to the command line arguments given by the user.\n        :param args: the arguments given by the user\n        :return: the graph manager, not bound to task_parameters yet.\n        """"""\n        graph_manager = None\n\n        # if a preset was given we will load the graph manager for the preset\n        if args.preset is not None:\n            graph_manager = short_dynamic_import(args.preset, ignore_module_case=True)\n\n        # for human play we need to create a custom graph manager\n        if args.play:\n            from rl_coach.agents.human_agent import HumanAgentParameters\n\n            env_params = short_dynamic_import(args.environment_type, ignore_module_case=True)()\n            env_params.human_control = True\n            schedule_params = HumanPlayScheduleParameters()\n            graph_manager = BasicRLGraphManager(HumanAgentParameters(), env_params, schedule_params, VisualizationParameters())\n\n        # Set framework\n        # Note: Some graph managers (e.g. HAC preset) create multiple agents and the attribute is called agents_params\n        if hasattr(graph_manager, \'agent_params\'):\n            for network_parameters in graph_manager.agent_params.network_wrappers.values():\n                network_parameters.framework = args.framework\n        elif hasattr(graph_manager, \'agents_params\'):\n            for ap in graph_manager.agents_params:\n                for network_parameters in ap.network_wrappers.values():\n                    network_parameters.framework = args.framework\n\n        if args.level:\n            if isinstance(graph_manager.env_params.level, SingleLevelSelection):\n                graph_manager.env_params.level.select(args.level)\n            else:\n                graph_manager.env_params.level = args.level\n\n        # set the seed for the environment\n        if args.seed is not None and graph_manager.env_params is not None:\n            graph_manager.env_params.seed = args.seed\n\n        # visualization\n        graph_manager.visualization_parameters.dump_gifs = graph_manager.visualization_parameters.dump_gifs or args.dump_gifs\n        graph_manager.visualization_parameters.dump_mp4 = graph_manager.visualization_parameters.dump_mp4 or args.dump_mp4\n        graph_manager.visualization_parameters.render = args.render\n        graph_manager.visualization_parameters.tensorboard = args.tensorboard\n        graph_manager.visualization_parameters.print_networks_summary = args.print_networks_summary\n\n        # update the custom parameters\n        if args.custom_parameter is not None:\n            unstripped_key_value_pairs = [pair.split(\'=\') for pair in args.custom_parameter.split(\';\')]\n            stripped_key_value_pairs = [tuple([pair[0].strip(), pair[1].strip()]) for pair in\n                                        unstripped_key_value_pairs if len(pair) == 2]\n\n            # load custom parameters into run_dict\n            for key, value in stripped_key_value_pairs:\n                exec(""graph_manager.{}={}"".format(key, value))\n\n        return graph_manager\n\n    def display_all_presets_and_exit(self):\n        # list available presets\n        screen.log_title(""Available Presets:"")\n        for preset in sorted(list_all_presets()):\n            print(preset)\n        sys.exit(0)\n\n    def expand_preset(self, preset):\n        """"""\n        Replace a short preset name with the full python path, and verify that it can be imported.\n        """"""\n        if preset.lower() in [p.lower() for p in list_all_presets()]:\n            preset = ""{}.py:graph_manager"".format(os.path.join(get_base_dir(), \'presets\', preset))\n        else:\n            preset = ""{}"".format(preset)\n            # if a graph manager variable was not specified, try the default of :graph_manager\n            if len(preset.split("":"")) == 1:\n                preset += "":graph_manager""\n\n        # verify that the preset exists\n        preset_path = preset.split("":"")[0]\n        if not os.path.exists(preset_path):\n            screen.error(""The given preset ({}) cannot be found."".format(preset))\n\n        # verify that the preset can be instantiated\n        try:\n            short_dynamic_import(preset, ignore_module_case=True)\n        except TypeError as e:\n            traceback.print_exc()\n            screen.error(\'Internal Error: \' + str(e) + ""\\n\\nThe given preset ({}) cannot be instantiated.""\n                         .format(preset))\n\n        return preset\n\n    def get_config_args(self, parser: argparse.ArgumentParser, arguments=None) -> argparse.Namespace:\n        """"""\n        Returns a Namespace object with all the user-specified configuration options needed to launch.\n        This implementation uses argparse to take arguments from the CLI, but this can be over-ridden by\n        another method that gets its configuration from elsewhere.  An equivalent method however must\n        return an identically structured Namespace object, which conforms to the structure defined by\n        get_argument_parser.\n\n        This method parses the arguments that the user entered, does some basic validation, and\n        modification of user-specified values in short form to be more explicit.\n\n        :param parser: a parser object which implicitly defines the format of the Namespace that\n                       is expected to be returned.\n        :param arguments: command line arguments\n        :return: the parsed arguments as a Namespace\n        """"""\n        if arguments is None:\n            args = parser.parse_args()\n        else:\n            args = parser.parse_args(arguments)\n\n        if args.nocolor:\n            screen.set_use_colors(False)\n\n        # if no arg is given\n        if (len(sys.argv) == 1 and arguments is None) or (arguments is not None and len(arguments) <= 2):\n            parser.print_help()\n            sys.exit(1)\n\n        # list available presets\n        if args.list:\n            self.display_all_presets_and_exit()\n\n        # Read args from config file for distributed Coach.\n        if args.distributed_coach and args.distributed_coach_run_type == RunType.ORCHESTRATOR:\n            coach_config = ConfigParser({\n                \'image\': \'\',\n                \'memory_backend\': \'redispubsub\',\n                \'data_store\': \'s3\',\n                \'s3_end_point\': \'s3.amazonaws.com\',\n                \'s3_bucket_name\': \'\',\n                \'s3_creds_file\': \'\'\n            })\n            try:\n                coach_config.read(args.distributed_coach_config_path)\n                args.image = coach_config.get(\'coach\', \'image\')\n                args.memory_backend = coach_config.get(\'coach\', \'memory_backend\')\n                args.data_store = coach_config.get(\'coach\', \'data_store\')\n                if args.data_store == \'s3\':\n                    args.s3_end_point = coach_config.get(\'coach\', \'s3_end_point\')\n                    args.s3_bucket_name = coach_config.get(\'coach\', \'s3_bucket_name\')\n                    args.s3_creds_file = coach_config.get(\'coach\', \'s3_creds_file\')\n            except Error as e:\n                screen.error(""Error when reading distributed Coach config file: {}"".format(e))\n\n            if args.image == \'\':\n                screen.error(""Image cannot be empty."")\n\n            data_store_choices = [\'s3\', \'nfs\', \'redis\']\n            if args.data_store not in data_store_choices:\n                screen.warning(""{} data store is unsupported."".format(args.data_store))\n                screen.error(""Supported data stores are {}."".format(data_store_choices))\n\n            memory_backend_choices = [\'redispubsub\']\n            if args.memory_backend not in memory_backend_choices:\n                screen.warning(""{} memory backend is not supported."".format(args.memory_backend))\n                screen.error(""Supported memory backends are {}."".format(memory_backend_choices))\n\n            if args.data_store == \'s3\':\n                if args.s3_bucket_name == \'\':\n                    screen.error(""S3 bucket name cannot be empty."")\n                if args.s3_creds_file == \'\':\n                    args.s3_creds_file = None\n\n        if args.play and args.distributed_coach:\n            screen.error(""Playing is not supported in distributed Coach."")\n\n        # replace a short preset name with the full path\n        if args.preset is not None:\n            args.preset = self.expand_preset(args.preset)\n\n        # validate the checkpoints args\n        if args.checkpoint_restore_dir is not None and not os.path.exists(args.checkpoint_restore_dir):\n            # If distributed trainer, the checkpoint dir is not yet available so skipping the check in that case.\n            if not (args.distributed_coach and args.distributed_coach_run_type in [RunType.TRAINER, RunType.ROLLOUT_WORKER]):\n                screen.error(""The requested checkpoint folder to load from does not exist."")\n\n        # validate the checkpoints args\n        if args.checkpoint_restore_file is not None and not glob(args.checkpoint_restore_file + \'*\'):\n            screen.error(""The requested checkpoint file to load from does not exist."")\n\n        # no preset was given. check if the user requested to play some environment on its own\n        if args.preset is None and args.play and not args.environment_type:\n            screen.error(\'When no preset is given for Coach to run, and the user requests human control over \'\n                         \'the environment, the user is expected to input the desired environment_type and level.\'\n                         \'\\nAt least one of these parameters was not given.\')\n        elif args.preset and args.play:\n            screen.error(""Both the --preset and the --play flags were set. These flags can not be used together. ""\n                         ""For human control, please use the --play flag together with the environment type flag (-et)"")\n        elif args.preset is None and not args.play:\n            screen.error(""Please choose a preset using the -p flag or use the --play flag together with choosing an ""\n                         ""environment type (-et) in order to play the game."")\n\n        # get experiment name and path\n        args.experiment_name = logger.get_experiment_name(args.experiment_name)\n        args.experiment_path = logger.get_experiment_path(args.experiment_name, args.experiment_path)\n\n        if args.play and args.num_workers > 1:\n            screen.warning(""Playing the game as a human is only available with a single worker. ""\n                           ""The number of workers will be reduced to 1"")\n            args.num_workers = 1\n\n        args.framework = Frameworks[args.framework.lower()]\n\n        # checkpoints\n        args.checkpoint_save_dir = os.path.join(args.experiment_path, \'checkpoint\') if args.checkpoint_save_secs is not None else None\n\n        if args.export_onnx_graph and not args.checkpoint_save_secs:\n            screen.warning(""Exporting ONNX graphs requires setting the --checkpoint_save_secs flag. ""\n                           ""The --export_onnx_graph will have no effect."")\n\n        return args\n\n    def get_argument_parser(self) -> argparse.ArgumentParser:\n        """"""\n        This returns an ArgumentParser object which defines the set of options that customers are expected to supply in order\n        to launch a coach job.\n        """"""\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\'-p\', \'--preset\',\n                            help=""(string) Name of a preset to run (class name from the \'presets\' directory.)"",\n                            default=None,\n                            type=str)\n        parser.add_argument(\'-l\', \'--list\',\n                            help=""(flag) List all available presets"",\n                            action=\'store_true\')\n        parser.add_argument(\'-e\', \'--experiment_name\',\n                            help=""(string) Experiment name to be used to store the results."",\n                            default=None,\n                            type=str)\n        parser.add_argument(\'-ep\', \'--experiment_path\',\n                            help=""(string) Path to experiments folder."",\n                            default=None,\n                            type=str)\n        parser.add_argument(\'-r\', \'--render\',\n                            help=""(flag) Render environment"",\n                            action=\'store_true\')\n        parser.add_argument(\'-f\', \'--framework\',\n                            help=""(string) Neural network framework. Available values: tensorflow, mxnet"",\n                            default=\'tensorflow\',\n                            type=str)\n        parser.add_argument(\'-n\', \'--num_workers\',\n                            help=""(int) Number of workers for multi-process based agents, e.g. A3C"",\n                            default=1,\n                            type=int)\n        parser.add_argument(\'-c\', \'--use_cpu\',\n                            help=""(flag) Use only the cpu for training. If a GPU is not available, this flag will have no ""\n                                 ""effect and the CPU will be used either way."",\n                            action=\'store_true\')\n        parser.add_argument(\'-ew\', \'--evaluation_worker\',\n                            help=""(flag) If multiple workers are used, add an evaluation worker as well which will ""\n                                 ""evaluate asynchronously and independently during the training. NOTE: this worker will ""\n                                 ""ignore the evaluation settings in the preset\'s ScheduleParams."",\n                            action=\'store_true\')\n        parser.add_argument(\'--play\',\n                            help=""(flag) Play as a human by controlling the game with the keyboard. ""\n                                 ""This option will save a replay buffer with the game play."",\n                            action=\'store_true\')\n        parser.add_argument(\'--evaluate\',\n                            help=""(int) Run evaluation only, for at least the given number of steps (note that complete ""\n                                ""episodes are evaluated). This is a convenient way to disable training in order ""\n                                ""to evaluate an existing checkpoint. If value is 0, or no value is provided, ""\n                                ""evaluation will run for an infinite number of steps."",\n                            nargs=\'?\',\n                            const=0,\n                            type=int)\n        parser.add_argument(\'-v\', \'--verbosity\',\n                            help=""(flag) Sets the verbosity level of Coach print outs. Can be either low or high."",\n                            default=""low"",\n                            type=str)\n        parser.add_argument(\'-tfv\', \'--tf_verbosity\',\n                            help=""(flag) TensorFlow verbosity level"",\n                            default=3,\n                            type=int)\n        parser.add_argument(\'--nocolor\',\n                            help=""(flag) Turn off color-codes in screen logging.  Ascii text only"",\n                            action=\'store_true\')\n        parser.add_argument(\'-s\', \'--checkpoint_save_secs\',\n                            help=""(int) Time in seconds between saving checkpoints of the model."",\n                            default=None,\n                            type=int)\n        parser.add_argument(\'-crd\', \'--checkpoint_restore_dir\',\n                            help=\'(string) Path to a folder containing a checkpoint to restore the model from.\',\n                            type=str)\n        parser.add_argument(\'-crf\', \'--checkpoint_restore_file\',\n                            help=\'(string) Path to a checkpoint file to restore the model from.\',\n                            type=str)\n        parser.add_argument(\'-dg\', \'--dump_gifs\',\n                            help=""(flag) Enable the gif saving functionality."",\n                            action=\'store_true\')\n        parser.add_argument(\'-dm\', \'--dump_mp4\',\n                            help=""(flag) Enable the mp4 saving functionality."",\n                            action=\'store_true\')\n        parser.add_argument(\'-et\', \'--environment_type\',\n                            help=""(string) Choose an environment type class to override on top of the selected preset."",\n                            default=None,\n                            type=str)\n        parser.add_argument(\'-lvl\', \'--level\',\n                            help=""(string) Choose the level that will be played in the environment that was selected.""\n                                 ""This value will override the level parameter in the environment class.""\n                            ,\n                            default=None,\n                            type=str)\n        parser.add_argument(\'-cp\', \'--custom_parameter\',\n                            help=""(string) Semicolon separated parameters used to override specific parameters on top of""\n                                 "" the selected preset (or on top of the command-line assembled one). ""\n                                 ""Whenever a parameter value is a string, it should be inputted as \'\\\\\\""string\\\\\\""\'. ""\n                                 ""For ex.: ""\n                                 ""\\""visualization_parameters.render=False; heatup_steps=EnvironmentSteps(1000);""\n                                 ""improve_steps=TrainingSteps(100000); optimizer=\'rmsprop\'\\"""",\n                            default=None,\n                            type=str)\n        parser.add_argument(\'--print_networks_summary\',\n                            help=""(flag) Print network summary to stdout"",\n                            action=\'store_true\')\n        parser.add_argument(\'-tb\', \'--tensorboard\',\n                            help=""(flag) When using the TensorFlow backend, enable TensorBoard log dumps. "",\n                            action=\'store_true\')\n        parser.add_argument(\'-ns\', \'--no_summary\',\n                            help=""(flag) Prevent Coach from printing a summary and asking questions at the end of runs"",\n                            action=\'store_true\')\n        parser.add_argument(\'-d\', \'--open_dashboard\',\n                            help=""(flag) Open dashboard with the experiment when the run starts"",\n                            action=\'store_true\')\n        parser.add_argument(\'--seed\',\n                            help=""(int) A seed to use for running the experiment"",\n                            default=None,\n                            type=int)\n        parser.add_argument(\'-onnx\', \'--export_onnx_graph\',\n                            help=""(flag) Export the ONNX graph to the experiment directory. ""\n                                 ""This will have effect only if the --checkpoint_save_secs flag is used in order to store ""\n                                 ""checkpoints, since the weights checkpoint are needed for the ONNX graph. ""\n                                 ""Keep in mind that this can cause major overhead on the experiment. ""\n                                 ""Exporting ONNX graphs requires manually installing the tf2onnx package ""\n                                 ""(https://github.com/onnx/tensorflow-onnx)."",\n                            action=\'store_true\')\n        parser.add_argument(\'-dc\', \'--distributed_coach\',\n                            help=""(flag) Use distributed Coach."",\n                            action=\'store_true\')\n        parser.add_argument(\'-dcp\', \'--distributed_coach_config_path\',\n                            help=""(string) Path to config file when using distributed rollout workers.""\n                                 ""Only distributed Coach parameters should be provided through this config file.""\n                                 ""Rest of the parameters are provided using Coach command line options.""\n                                 ""Used only with --distributed_coach flag.""\n                                 ""Ignored if --distributed_coach flag is not used."",\n                            type=str)\n        parser.add_argument(\'--memory_backend_params\',\n                            help=argparse.SUPPRESS,\n                            type=str)\n        parser.add_argument(\'--data_store_params\',\n                            help=argparse.SUPPRESS,\n                            type=str)\n        parser.add_argument(\'--distributed_coach_run_type\',\n                            help=argparse.SUPPRESS,\n                            type=RunType,\n                            default=RunType.ORCHESTRATOR,\n                            choices=list(RunType))\n        parser.add_argument(\'-asc\', \'--apply_stop_condition\',\n                            help=""(flag) If set, this will apply a stop condition on the run, defined by reaching a""\n                                 ""target success rate as set by the environment or a custom success rate as defined ""\n                                 ""in the preset. "",\n                            action=\'store_true\')\n        parser.add_argument(\'--dump_worker_logs\',\n                            help=""(flag) Only used in distributed coach. If set, the worker logs are saved in the experiment dir"",\n                            action=\'store_true\')\n        parser.add_argument(\'--is_multi_node_test\',\n                            help=argparse.SUPPRESS,\n                            action=\'store_true\')\n\n        return parser\n\n    def run_graph_manager(self, graph_manager: \'GraphManager\', args: argparse.Namespace):\n        task_parameters = self.create_task_parameters(graph_manager, args)\n\n        if args.distributed_coach and args.distributed_coach_run_type != RunType.ORCHESTRATOR:\n            handle_distributed_coach_tasks(graph_manager, args, task_parameters)\n            return\n\n        # Single-threaded runs\n        if args.num_workers == 1:\n            self.start_single_threaded(task_parameters, graph_manager, args)\n        else:\n            self.start_multi_threaded(graph_manager, args)\n\n    @staticmethod\n    def create_task_parameters(graph_manager: \'GraphManager\', args: argparse.Namespace):\n        if args.distributed_coach and not graph_manager.agent_params.algorithm.distributed_coach_synchronization_type:\n            screen.error(\n                ""{} algorithm is not supported using distributed Coach."".format(graph_manager.agent_params.algorithm))\n\n        if args.distributed_coach and args.checkpoint_save_secs and graph_manager.agent_params.algorithm.distributed_coach_synchronization_type == DistributedCoachSynchronizationType.SYNC:\n            screen.warning(\n                ""The --checkpoint_save_secs or -s argument will be ignored as SYNC distributed coach sync type is used. Checkpoint will be saved every training iteration."")\n\n        if args.distributed_coach and not args.checkpoint_save_secs and graph_manager.agent_params.algorithm.distributed_coach_synchronization_type == DistributedCoachSynchronizationType.ASYNC:\n            screen.error(\n                ""Distributed coach with ASYNC distributed coach sync type requires --checkpoint_save_secs or -s."")\n\n        # Intel optimized TF seems to run significantly faster when limiting to a single OMP thread.\n        # This will not affect GPU runs.\n        os.environ[""OMP_NUM_THREADS""] = ""1""\n\n        # turn TF debug prints off\n        if args.framework == Frameworks.tensorflow:\n            os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = str(args.tf_verbosity)\n\n        # turn off the summary at the end of the run if necessary\n        if not args.no_summary and not args.distributed_coach:\n            atexit.register(logger.summarize_experiment)\n            screen.change_terminal_title(args.experiment_name)\n\n        if args.checkpoint_restore_dir is not None and args.checkpoint_restore_file is not None:\n            raise ValueError(""Only one of the checkpoint_restore_dir and checkpoint_restore_file arguments can be used""\n                             "" simulatenously."")\n        checkpoint_restore_path = args.checkpoint_restore_dir if args.checkpoint_restore_dir \\\n            else args.checkpoint_restore_file\n\n        # open dashboard\n        if args.open_dashboard:\n            open_dashboard(args.experiment_path)\n\n        if args.distributed_coach and args.distributed_coach_run_type == RunType.ORCHESTRATOR:\n            exit(handle_distributed_coach_orchestrator(args))\n\n        task_parameters = TaskParameters(\n            framework_type=args.framework,\n            evaluate_only=args.evaluate,\n            experiment_path=args.experiment_path,\n            seed=args.seed,\n            use_cpu=args.use_cpu,\n            checkpoint_save_secs=args.checkpoint_save_secs,\n            checkpoint_restore_path=checkpoint_restore_path,\n            checkpoint_save_dir=args.checkpoint_save_dir,\n            export_onnx_graph=args.export_onnx_graph,\n            apply_stop_condition=args.apply_stop_condition\n        )\n\n        return task_parameters\n\n    @staticmethod\n    def start_single_threaded(task_parameters, graph_manager: \'GraphManager\', args: argparse.Namespace):\n        # Start the training or evaluation\n        start_graph(graph_manager=graph_manager, task_parameters=task_parameters)\n\n    @staticmethod\n    def start_multi_threaded(graph_manager: \'GraphManager\', args: argparse.Namespace):\n        total_tasks = args.num_workers\n        if args.evaluation_worker:\n            total_tasks += 1\n\n        ps_hosts = ""localhost:{}"".format(get_open_port())\n        worker_hosts = "","".join([""localhost:{}"".format(get_open_port()) for i in range(total_tasks)])\n\n        # Shared memory\n        class CommManager(BaseManager):\n            pass\n        CommManager.register(\'SharedMemoryScratchPad\', SharedMemoryScratchPad, exposed=[\'add\', \'get\', \'internal_call\'])\n        comm_manager = CommManager()\n        comm_manager.start()\n        shared_memory_scratchpad = comm_manager.SharedMemoryScratchPad()\n\n        if args.checkpoint_restore_file:\n            raise ValueError(""Multi-Process runs only support restoring checkpoints from a directory, ""\n                             ""and not from a file. "")\n\n        def start_distributed_task(job_type, task_index, evaluation_worker=False,\n                                   shared_memory_scratchpad=shared_memory_scratchpad):\n            task_parameters = DistributedTaskParameters(\n                framework_type=args.framework,\n                parameters_server_hosts=ps_hosts,\n                worker_hosts=worker_hosts,\n                job_type=job_type,\n                task_index=task_index,\n                evaluate_only=0 if evaluation_worker else None, # 0 value for evaluation worker as it should run infinitely\n                use_cpu=args.use_cpu,\n                num_tasks=total_tasks,  # training tasks + 1 evaluation task\n                num_training_tasks=args.num_workers,\n                experiment_path=args.experiment_path,\n                shared_memory_scratchpad=shared_memory_scratchpad,\n                seed=args.seed+task_index if args.seed is not None else None,  # each worker gets a different seed\n                checkpoint_save_secs=args.checkpoint_save_secs,\n                checkpoint_restore_path=args.checkpoint_restore_dir,  # MonitoredTrainingSession only supports a dir\n                checkpoint_save_dir=args.checkpoint_save_dir,\n                export_onnx_graph=args.export_onnx_graph,\n                apply_stop_condition=args.apply_stop_condition\n            )\n            # we assume that only the evaluation workers are rendering\n            graph_manager.visualization_parameters.render = args.render and evaluation_worker\n            p = Process(target=start_graph, args=(graph_manager, task_parameters))\n            # p.daemon = True\n            p.start()\n            return p\n\n        # parameter server\n        parameter_server = start_distributed_task(""ps"", 0)\n\n        # training workers\n        # wait a bit before spawning the non chief workers in order to make sure the session is already created\n        workers = []\n        workers.append(start_distributed_task(""worker"", 0))\n\n        time.sleep(2)\n        for task_index in range(1, args.num_workers):\n            workers.append(start_distributed_task(""worker"", task_index))\n\n        # evaluation worker\n        if args.evaluation_worker or args.render:\n            evaluation_worker = start_distributed_task(""worker"", args.num_workers, evaluation_worker=True)\n\n        # wait for all workers\n        [w.join() for w in workers]\n        if args.evaluation_worker:\n            evaluation_worker.terminate()\n\n\nclass CoachInterface(CoachLauncher):\n    """"""\n        This class is used as an interface to use coach as library. It can take any of the command line arguments\n        (with the respective names) as arguments to the class.\n    """"""\n    def __init__(self, **kwargs):\n        parser = self.get_argument_parser()\n\n        arguments = []\n        for key in kwargs:\n            arguments.append(\'--\' + key)\n            arguments.append(str(kwargs[key]))\n\n        if \'--experiment_name\' not in arguments:\n            arguments.append(\'--experiment_name\')\n            arguments.append(\'\')\n        self.args = self.get_config_args(parser, arguments)\n\n        self.graph_manager = self.get_graph_manager_from_args(self.args)\n\n        if self.args.num_workers == 1:\n            task_parameters = self.create_task_parameters(self.graph_manager, self.args)\n            self.graph_manager.create_graph(task_parameters)\n\n    def run(self):\n        self.run_graph_manager(self.graph_manager, self.args)\n\n\ndef main():\n    launcher = CoachLauncher()\n    launcher.launch()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
rl_coach/core_types.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom collections import namedtuple\n\nimport copy\nimport math\nfrom enum import Enum\nfrom random import shuffle\nfrom typing import List, Union, Dict, Any, Type\n\nimport numpy as np\n\nfrom rl_coach.utils import force_list\n\nActionType = Union[int, float, np.ndarray, List]\nGoalType = Union[None, np.ndarray]\nObservationType = np.ndarray\nRewardType = Union[int, float, np.ndarray]\nStateType = Dict[str, np.ndarray]\n\n\nclass GoalTypes(Enum):\n    Embedding = 1\n    EmbeddingChange = 2\n    Observation = 3\n    Measurements = 4\n\n\nRecord = namedtuple(\'Record\', [\'name\', \'label\'])\n\n\nclass TimeTypes(Enum):\n    EpisodeNumber = Record(name=\'Episode #\', label=\'Episode #\')\n    TrainingIteration = Record(name=\'Training Iter\', label=\'Training Iteration\')\n    EnvironmentSteps = Record(name=\'Total steps\', label=\'Total steps (per worker)\')\n    WallClockTime = Record(name=\'Wall-Clock Time\', label=\'Wall-Clock Time (minutes)\')\n    Epoch = Record(name=\'Epoch\', label=\'Epoch #\')\n\n\n# step methods\n\nclass StepMethod(object):\n    def __init__(self, num_steps: int):\n        self._num_steps = self.num_steps = num_steps\n\n    @property\n    def num_steps(self) -> int:\n        return self._num_steps\n\n    @num_steps.setter\n    def num_steps(self, val: int) -> None:\n        self._num_steps = val\n\n    def __eq__(self, other):\n        return self.num_steps == other.num_steps\n\n    def __truediv__(self, other):\n        """"""\n        divide this step method with other. If other is an integer, returns an object of the same\n        type as self. If other is the same type of self, returns an integer. In either case, any\n        floating point value is rounded up under the assumption that if we are dividing Steps, we\n        would rather overestimate than underestimate.\n        """"""\n        if isinstance(other, type(self)):\n            return math.ceil(self.num_steps / other.num_steps)\n        elif isinstance(other, int):\n            return type(self)(math.ceil(self.num_steps / other))\n        else:\n            raise TypeError(""cannot divide {} by {}"".format(type(self), type(other)))\n\n    def __rtruediv__(self, other):\n        """"""\n        divide this step method with other. If other is an integer, returns an object of the same\n        type as self. If other is the same type of self, returns an integer. In either case, any\n        floating point value is rounded up under the assumption that if we are dividing Steps, we\n        would rather overestimate than underestimate.\n        """"""\n        if isinstance(other, type(self)):\n            return math.ceil(other.num_steps / self.num_steps)\n        elif isinstance(other, int):\n            return type(self)(math.ceil(other / self.num_steps))\n        else:\n            raise TypeError(""cannot divide {} by {}"".format(type(other), type(self)))\n\n\nclass Frames(StepMethod):\n    def __init__(self, num_steps):\n        super().__init__(num_steps)\n\n\nclass EnvironmentSteps(StepMethod):\n    def __init__(self, num_steps):\n        super().__init__(num_steps)\n\n\nclass EnvironmentEpisodes(StepMethod):\n    def __init__(self, num_steps):\n        super().__init__(num_steps)\n\n\nclass TrainingSteps(StepMethod):\n    def __init__(self, num_steps):\n        super().__init__(num_steps)\n\n    def __truediv__(self, other):\n        if isinstance(other, EnvironmentSteps):\n            return math.ceil(self.num_steps / other.num_steps)\n        else:\n            super().__truediv__(self, other)\n\n\nclass Time(StepMethod):\n    def __init__(self, num_steps):\n        super().__init__(num_steps)\n\n\nclass PredictionType(object):\n    pass\n\n\nclass VStateValue(PredictionType):\n    pass\n\n\nclass QActionStateValue(PredictionType):\n    pass\n\n\nclass ActionProbabilities(PredictionType):\n    pass\n\n\nclass Embedding(PredictionType):\n    pass\n\n\nclass Measurements(PredictionType):\n    pass\n\n\nclass InputEmbedding(Embedding):\n    pass\n\n\nclass MiddlewareEmbedding(Embedding):\n    pass\n\n\nclass InputImageEmbedding(InputEmbedding):\n    pass\n\n\nclass InputVectorEmbedding(InputEmbedding):\n    pass\n\n\nclass InputTensorEmbedding(InputEmbedding):\n    pass\n\n\nclass Middleware_FC_Embedding(MiddlewareEmbedding):\n    pass\n\n\nclass Middleware_LSTM_Embedding(MiddlewareEmbedding):\n    pass\n\n\nPlayingStepsType = Union[EnvironmentSteps, EnvironmentEpisodes, Frames]\n\n\n# run phases\nclass RunPhase(Enum):\n    HEATUP = ""Heatup""\n    TRAIN = ""Training""\n    TEST = ""Testing""\n    UNDEFINED = ""Undefined""\n\n\n# transitions\n\nclass Transition(object):\n    def __init__(self, state: Dict[str, np.ndarray]=None, action: ActionType=None, reward: RewardType=None,\n                 next_state: Dict[str, np.ndarray]=None, game_over: bool=None, info: Dict=None):\n        """"""\n        A transition is a tuple containing the information of a single step of interaction\n        between the agent and the environment. The most basic version should contain the following values:\n        (current state, action, reward, next state, game over)\n        For imitation learning algorithms, if the reward, next state or game over is not known,\n        it is sufficient to store the current state and action taken by the expert.\n\n        :param state: The current state. Assumed to be a dictionary where the observation\n                      is located at state[\'observation\']\n        :param action: The current action that was taken\n        :param reward: The reward received from the environment\n        :param next_state: The next state of the environment after applying the action.\n                           The next state should be similar to the state in its structure.\n        :param game_over: A boolean which should be True if the episode terminated after\n                          the execution of the action.\n        :param info: A dictionary containing any additional information to be stored in the transition\n        """"""\n\n        self._state = self.state = state\n        self._action = self.action = action\n        self._reward = self.reward = reward\n        self._n_step_discounted_rewards = self.n_step_discounted_rewards = None\n        if not next_state:\n            next_state = state\n        self._next_state = self._next_state = next_state\n        self._game_over = self.game_over = game_over\n        if info is None:\n            self.info = {}\n        else:\n            self.info = info\n\n    def __repr__(self):\n        return str(self.__dict__)\n\n    @property\n    def state(self):\n        if self._state is None:\n            raise Exception(""The state was not filled by any of the modules between the environment and the agent"")\n        return self._state\n\n    @state.setter\n    def state(self, val):\n        self._state = val\n\n    @property\n    def action(self):\n        if self._action is None:\n            raise Exception(""The action was not filled by any of the modules between the environment and the agent"")\n        return self._action\n\n    @action.setter\n    def action(self, val):\n        self._action = val\n\n    @property\n    def reward(self):\n\n        if self._reward is None:\n            raise Exception(""The reward was not filled by any of the modules between the environment and the agent"")\n        return self._reward\n\n    @reward.setter\n    def reward(self, val):\n        self._reward = val\n\n    @property\n    def n_step_discounted_rewards(self):\n        if self._n_step_discounted_rewards is None:\n            raise Exception(""The n_step_discounted_rewards were not filled by any of the modules between the ""\n                            ""environment and the agent.  Make sure that you are using an episodic experience replay."")\n        return self._n_step_discounted_rewards\n\n    @n_step_discounted_rewards.setter\n    def n_step_discounted_rewards(self, val):\n        self._n_step_discounted_rewards = val\n\n    @property\n    def game_over(self):\n        if self._game_over is None:\n            raise Exception(""The done flag was not filled by any of the modules between the environment and the agent"")\n        return self._game_over\n\n    @game_over.setter\n    def game_over(self, val):\n        self._game_over = val\n\n    @property\n    def next_state(self):\n        if self._next_state is None:\n            raise Exception(""The next state was not filled by any of the modules between the environment and the agent"")\n        return self._next_state\n\n    @next_state.setter\n    def next_state(self, val):\n        self._next_state = val\n\n    def add_info(self, new_info: Dict[str, Any]) -> None:\n        if not new_info.keys().isdisjoint(self.info.keys()):\n            raise ValueError(""The new info dictionary can not be appended to the existing info dictionary since there ""\n                             ""are overlapping keys between the two. old keys: {}, new keys: {}""\n                             .format(self.info.keys(), new_info.keys()))\n        self.info.update(new_info)\n\n    def update_info(self, new_info: Dict[str, Any]) -> None:\n        self.info.update(new_info)\n\n    def __copy__(self):\n        new_transition = type(self)()\n        new_transition.__dict__.update(self.__dict__)\n        new_transition.state = copy.copy(new_transition.state)\n        new_transition.next_state = copy.copy(new_transition.next_state)\n        new_transition.info = copy.copy(new_transition.info)\n        return new_transition\n\n\nclass EnvResponse(object):\n    def __init__(self, next_state: Dict[str, ObservationType], reward: RewardType, game_over: bool, info: Dict=None,\n                 goal: ObservationType=None):\n        """"""\n        An env response is a collection containing the information returning from the environment after a single action\n        has been performed on it.\n\n        :param next_state: The new state that the environment has transitioned into. Assumed to be a dictionary where the\n                          observation is located at state[\'observation\']\n        :param reward: The reward received from the environment\n        :param game_over: A boolean which should be True if the episode terminated after\n                          the execution of the action.\n        :param info: any additional info from the environment\n        :param goal: a goal defined by the environment\n        """"""\n        self._next_state = self.next_state = next_state\n        self._reward = self.reward = reward\n        self._game_over = self.game_over = game_over\n        self._goal = self.goal = goal\n        if info is None:\n            self.info = {}\n        else:\n            self.info = info\n\n    def __repr__(self):\n        return str(self.__dict__)\n\n    @property\n    def next_state(self):\n        return self._next_state\n\n    @next_state.setter\n    def next_state(self, val):\n        self._next_state = val\n\n    @property\n    def reward(self):\n        return self._reward\n\n    @reward.setter\n    def reward(self, val):\n        self._reward = val\n\n    @property\n    def game_over(self):\n        return self._game_over\n\n    @game_over.setter\n    def game_over(self, val):\n        self._game_over = val\n\n    @property\n    def goal(self):\n        return self._goal\n\n    @goal.setter\n    def goal(self, val):\n        self._goal = val\n\n    def add_info(self, info: Dict[str, Any]) -> None:\n        if info.keys().isdisjoint(self.info.keys()):\n            raise ValueError(""The new info dictionary can not be appended to the existing info dictionary since there""\n                             ""are overlapping keys between the two"")\n        self.info.update(info)\n\n\nclass ActionInfo(object):\n    """"""\n    Action info is a class that holds an action and various additional information details about it\n    """"""\n\n    def __init__(self, action: ActionType, all_action_probabilities: float=0,\n                 action_value: float=0., state_value: float=0., max_action_value: float=None):\n        """"""\n        :param action: the action\n        :param all_action_probabilities: the probability that the action was given when selecting it\n        :param action_value: the state-action value (Q value) of the action\n        :param state_value: the state value (V value) of the state where the action was taken\n        :param max_action_value: in case this is an action that was selected randomly, this is the value of the action\n                                 that received the maximum value. if no value is given, the action is assumed to be the\n                                 action with the maximum value\n        """"""\n        self.action = action\n        self.all_action_probabilities = all_action_probabilities\n        self.action_value = action_value\n        self.state_value = state_value\n        if not max_action_value:\n            self.max_action_value = action_value\n        else:\n            self.max_action_value = max_action_value\n\n\nclass Batch(object):\n    """"""\n    A wrapper around a list of transitions that helps extracting batches of parameters from it.\n    For example, one can extract a list of states corresponding to the list of transitions.\n    The class uses lazy evaluation in order to return each of the available parameters.\n    """"""\n    def __init__(self, transitions: List[Transition]):\n        """"""\n        :param transitions: a list of transitions to extract the batch from\n        """"""\n        self.transitions = transitions\n        self._states = {}\n        self._actions = None\n        self._rewards = None\n        self._n_step_discounted_rewards = None\n        self._game_overs = None\n        self._next_states = {}\n        self._goals = None\n        self._info = {}\n\n    def slice(self, start, end) -> None:\n        """"""\n        Keep a slice from the batch and discard the rest of the batch\n\n        :param start: the start index in the slice\n        :param end: the end index in the slice\n        :return: None\n        """"""\n\n        self.transitions = self.transitions[start:end]\n        for k, v in self._states.items():\n            self._states[k] = v[start:end]\n        if self._actions is not None:\n            self._actions = self._actions[start:end]\n        if self._rewards is not None:\n            self._rewards = self._rewards[start:end]\n        if self._n_step_discounted_rewards is not None:\n            self._n_step_discounted_rewards = self._n_step_discounted_rewards[start:end]\n        if self._game_overs is not None:\n            self._game_overs = self._game_overs[start:end]\n        for k, v in self._next_states.items():\n            self._next_states[k] = v[start:end]\n        if self._goals is not None:\n            self._goals = self._goals[start:end]\n        for k, v in self._info.items():\n            self._info[k] = v[start:end]\n\n    def shuffle(self) -> None:\n        """"""\n        Shuffle all the transitions in the batch\n\n        :return: None\n        """"""\n        batch_order = list(range(self.size))\n        shuffle(batch_order)\n        self.transitions = [self.transitions[i] for i in batch_order]\n        self._states = {}\n        self._actions = None\n        self._rewards = None\n        self._n_step_discounted_rewards = None\n        self._game_overs = None\n        self._next_states = {}\n        self._goals = None\n        self._info = {}\n\n        # This seems to be slower\n        # for k, v in self._states.items():\n        #     self._states[k] = [v[i] for i in batch_order]\n        # if self._actions is not None:\n        #     self._actions = [self._actions[i] for i in batch_order]\n        # if self._rewards is not None:\n        #     self._rewards = [self._rewards[i] for i in batch_order]\n        # if self._total_returns is not None:\n        #     self._total_returns = [self._total_returns[i] for i in batch_order]\n        # if self._game_overs is not None:\n        #     self._game_overs = [self._game_overs[i] for i in batch_order]\n        # for k, v in self._next_states.items():\n        #     self._next_states[k] = [v[i] for i in batch_order]\n        # if self._goals is not None:\n        #     self._goals = [self._goals[i] for i in batch_order]\n        # for k, v in self._info.items():\n        #     self._info[k] = [v[i] for i in batch_order]\n\n    def states(self, fetches: List[str], expand_dims=False) -> Dict[str, np.ndarray]:\n        """"""\n        follow the keys in fetches to extract the corresponding items from the states in the batch\n        if these keys were not already extracted before. return only the values corresponding to those keys\n\n        :param fetches: the keys of the state dictionary to extract\n        :param expand_dims: add an extra dimension to each of the value batches\n        :return: a dictionary containing a batch of values correponding to each of the given fetches keys\n        """"""\n        current_states = {}\n        # there are cases (e.g. ddpg) where the state does not contain all the information needed for running\n        # through the network and this has to be added externally (e.g. ddpg where the action needs to be given in\n        # addition to the current_state, so that all the inputs of the network will be filled)\n        for key in set(fetches).intersection(self.transitions[0].state.keys()):\n            if key not in self._states.keys():\n                self._states[key] = np.array([np.array(transition.state[key]) for transition in self.transitions])\n            if expand_dims:\n                current_states[key] = np.expand_dims(self._states[key], -1)\n            else:\n                current_states[key] = self._states[key]\n        return current_states\n\n    def actions(self, expand_dims=False) -> np.ndarray:\n        """"""\n        if the actions were not converted to a batch before, extract them to a batch and then return the batch\n\n        :param expand_dims: add an extra dimension to the actions batch\n        :return: a numpy array containing all the actions of the batch\n        """"""\n        if self._actions is None:\n            self._actions = np.array([transition.action for transition in self.transitions])\n        if expand_dims:\n            return np.expand_dims(self._actions, -1)\n        return self._actions\n\n    def rewards(self, expand_dims=False) -> np.ndarray:\n        """"""\n        if the rewards were not converted to a batch before, extract them to a batch and then return the batch\n\n        :param expand_dims: add an extra dimension to the rewards batch\n        :return: a numpy array containing all the rewards of the batch\n        """"""\n        if self._rewards is None:\n            self._rewards = np.array([transition.reward for transition in self.transitions])\n        if expand_dims:\n            return np.expand_dims(self._rewards, -1)\n        return self._rewards\n\n    def n_step_discounted_rewards(self, expand_dims=False) -> np.ndarray:\n        """"""\n        if the n_step_discounted_rewards were not converted to a batch before, extract them to a batch and then return\n         the batch\n        if the n step discounted rewards were not filled, this will raise an exception\n        :param expand_dims: add an extra dimension to the total_returns batch\n        :return: a numpy array containing all the total return values of the batch\n        """"""\n        if self._n_step_discounted_rewards is None:\n            self._n_step_discounted_rewards = np.array([transition.n_step_discounted_rewards for transition in\n                                                        self.transitions])\n        if expand_dims:\n            return np.expand_dims(self._n_step_discounted_rewards, -1)\n        return self._n_step_discounted_rewards\n\n    def game_overs(self, expand_dims=False) -> np.ndarray:\n        """"""\n        if the game_overs were not converted to a batch before, extract them to a batch and then return the batch\n\n        :param expand_dims: add an extra dimension to the game_overs batch\n        :return: a numpy array containing all the game over flags of the batch\n        """"""\n        if self._game_overs is None:\n            self._game_overs = np.array([transition.game_over for transition in self.transitions])\n        if expand_dims:\n            return np.expand_dims(self._game_overs, -1)\n        return self._game_overs\n\n    def next_states(self, fetches: List[str], expand_dims=False) -> Dict[str, np.ndarray]:\n        """"""\n        follow the keys in fetches to extract the corresponding items from the next states in the batch\n        if these keys were not already extracted before. return only the values corresponding to those keys\n\n        :param fetches: the keys of the state dictionary to extract\n        :param expand_dims: add an extra dimension to each of the value batches\n        :return: a dictionary containing a batch of values correponding to each of the given fetches keys\n        """"""\n        next_states = {}\n        # there are cases (e.g. ddpg) where the state does not contain all the information needed for running\n        # through the network and this has to be added externally (e.g. ddpg where the action needs to be given in\n        # addition to the current_state, so that all the inputs of the network will be filled)\n        for key in set(fetches).intersection(self.transitions[0].next_state.keys()):\n            if key not in self._next_states.keys():\n                self._next_states[key] = np.array(\n                    [np.array(transition.next_state[key]) for transition in self.transitions])\n            if expand_dims:\n                next_states[key] = np.expand_dims(self._next_states[key], -1)\n            else:\n                next_states[key] = self._next_states[key]\n        return next_states\n\n    def goals(self, expand_dims=False) -> np.ndarray:\n        """"""\n        if the goals were not converted to a batch before, extract them to a batch and then return the batch\n        if the goal was not filled, this will raise an exception\n\n        :param expand_dims: add an extra dimension to the goals batch\n        :return: a numpy array containing all the goals of the batch\n        """"""\n        if self._goals is None:\n            self._goals = np.array([transition.goal for transition in self.transitions])\n        if expand_dims:\n            return np.expand_dims(self._goals, -1)\n        return self._goals\n\n    def info_as_list(self, key) -> list:\n        """"""\n        get the info and store it internally as a list, if wasn\'t stored before. return it as a list\n        :param expand_dims: add an extra dimension to the info batch\n        :return: a list containing all the info values of the batch corresponding to the given key\n        """"""\n        if key not in self._info.keys():\n            self._info[key] = [transition.info[key] for transition in self.transitions]\n        return self._info[key]\n\n    def info(self, key, expand_dims=False) -> np.ndarray:\n        """"""\n        if the given info dictionary key was not converted to a batch before, extract it to a batch and then return the\n        batch. if the key is not part of the keys in the info dictionary, this will raise an exception\n\n        :param expand_dims: add an extra dimension to the info batch\n        :return: a numpy array containing all the info values of the batch corresponding to the given key\n        """"""\n        info_list = self.info_as_list(key)\n\n        if expand_dims:\n            return np.expand_dims(info_list, -1)\n        return np.array(info_list)\n\n    @property\n    def size(self) -> int:\n        """"""\n        :return: the size of the batch\n        """"""\n        return len(self.transitions)\n\n    def __getitem__(self, key):\n        """"""\n        get an item from the transitions list\n\n        :param key: index of the transition in the batch\n        :return: the transition corresponding to the given index\n        """"""\n        return self.transitions[key]\n\n    def __setitem__(self, key, item):\n        """"""\n        set an item in the transition list\n\n        :param key: index of the transition in the batch\n        :param item: the transition to place in the given index\n        :return: None\n        """"""\n        self.transitions[key] = item\n\n\nclass TotalStepsCounter(object):\n    """"""\n    A wrapper around a dictionary counting different StepMethods steps done.\n    """"""\n\n    def __init__(self):\n        self.counters = {\n            EnvironmentEpisodes: 0,\n            EnvironmentSteps: 0,\n            TrainingSteps: 0\n        }\n\n    def __getitem__(self, key: Type[StepMethod]) -> int:\n        """"""\n        get counter value\n\n        :param key: counter type\n        :return: the counter value\n        """"""\n        return self.counters[key]\n\n    def __setitem__(self, key: StepMethod, item: int) -> None:\n        """"""\n        set an item in the transition list\n\n        :param key: counter type\n        :param item: an integer representing the new counter value\n        :return: None\n        """"""\n        self.counters[key] = item\n\n    def __add__(self, other: Type[StepMethod]) -> Type[StepMethod]:\n        return other.__class__(self.counters[other.__class__] + other.num_steps)\n\n    def __lt__(self, other: Type[StepMethod]):\n        return self.counters[other.__class__] < other.num_steps\n\n\nclass GradientClippingMethod(Enum):\n    ClipByGlobalNorm = 0\n    ClipByNorm = 1\n    ClipByValue = 2\n\n\nclass Episode(object):\n    """"""\n    An Episode represents a set of sequential transitions, that end with a terminal state.\n    """"""\n    def __init__(self, discount: float=0.99, bootstrap_total_return_from_old_policy: bool=False, n_step: int=-1):\n        """"""\n        :param discount: the discount factor to use when calculating total returns\n        :param bootstrap_total_return_from_old_policy: should the total return be bootstrapped from the values in the\n                                                       memory\n        :param n_step: the number of future steps to sum the reward over before bootstrapping\n        """"""\n        self.transitions = []\n        self._length = 0\n        self.discount = discount\n        self.bootstrap_total_return_from_old_policy = bootstrap_total_return_from_old_policy\n        self.n_step = n_step\n        self.is_complete = False\n\n    def insert(self, transition: Transition) -> None:\n        """"""\n        Insert a new transition to the episode. If the game_over flag in the transition is set to True,\n        the episode will be marked as complete.\n\n        :param transition: The new transition to insert to the episode\n        :return: None\n        """"""\n        self.transitions.append(transition)\n        self._length += 1\n        if transition.game_over:\n            self.is_complete = True\n\n    def is_empty(self) -> bool:\n        """"""\n        Check if the episode is empty\n\n        :return: A boolean value determining if the episode is empty or not\n        """"""\n        return self.length() == 0\n\n    def length(self) -> int:\n        """"""\n        Return the length of the episode, which is the number of transitions it holds.\n\n        :return: The number of transitions in the episode\n        """"""\n        return self._length\n\n    def __len__(self):\n        return self.length()\n\n    def get_transition(self, transition_idx: int) -> Transition:\n        """"""\n        Get a specific transition by its index.\n\n        :param transition_idx: The index of the transition to get\n        :return: The transition which is stored in the given index\n        """"""\n        return self.transitions[transition_idx]\n\n    def get_last_transition(self) -> Transition:\n        """"""\n        Get the last transition in the episode, or None if there are no transition available\n\n        :return: The last transition in the episode\n        """"""\n        return self.get_transition(-1) if self.length() > 0 else None\n\n    def get_first_transition(self) -> Transition:\n        """"""\n        Get the first transition in the episode, or None if there are no transitions available\n\n        :return: The first transition in the episode\n        """"""\n        return self.get_transition(0) if self.length() > 0 else None\n\n    def update_discounted_rewards(self):\n        """"""\n        Update the discounted returns for all the transitions in the episode.\n        The returns will be calculated according to the rewards of each transition, together with the number of steps\n        to bootstrap from and the discount factor, as defined by n_step and discount respectively when initializing\n        the episode.\n\n        :return: None\n        """"""\n        if self.n_step == -1 or self.n_step > self.length():\n            curr_n_step = self.length()\n        else:\n            curr_n_step = self.n_step\n\n        rewards = np.array([t.reward for t in self.transitions])\n        rewards = rewards.astype(\'float\')\n        discounted_rewards = rewards.copy()\n        current_discount = self.discount\n        for i in range(1, curr_n_step):\n            discounted_rewards += current_discount * np.pad(rewards[i:], (0, i), \'constant\', constant_values=0)\n            current_discount *= self.discount\n\n        # calculate the bootstrapped returns\n        if self.bootstrap_total_return_from_old_policy:\n            bootstraps = np.array([np.squeeze(t.info[\'max_action_value\']) for t in self.transitions[curr_n_step:]])\n            bootstrapped_return = discounted_rewards + current_discount * np.pad(bootstraps, (0, curr_n_step),\n                                                                                 \'constant\', constant_values=0)\n            discounted_rewards = bootstrapped_return\n\n        for transition_idx in range(self.length()):\n            self.transitions[transition_idx].n_step_discounted_rewards = discounted_rewards[transition_idx]\n\n    def update_transitions_rewards_and_bootstrap_data(self):\n        if not isinstance(self.n_step, int) or (self.n_step < 1 and self.n_step != -1):\n            raise ValueError(""n-step should be an integer with value >= 1, or set to -1 for always setting to episode""\n                             "" length."")\n        elif self.n_step > 1:\n            curr_n_step = self.n_step if self.n_step < self.length() else self.length()\n\n            for idx, transition in enumerate(self.transitions):\n                next_n_step_transition_idx = (idx + curr_n_step)\n                if next_n_step_transition_idx < len(self.transitions):\n                    # next state will now point to the n-step next state\n                    transition.next_state = self.transitions[next_n_step_transition_idx].state\n                    transition.info[\'should_bootstrap_next_state\'] = True\n                else:\n                    transition.next_state = self.transitions[-1].next_state\n                    transition.info[\'should_bootstrap_next_state\'] = False\n\n        self.update_discounted_rewards()\n\n\n\n    def get_transitions_attribute(self, attribute_name: str) -> List[Any]:\n        """"""\n        Get the values for some transition attribute from all the transitions in the episode.\n        For example, this allows getting the rewards for all the transitions as a list by calling\n        get_transitions_attribute(\'reward\')\n\n        :param attribute_name: The name of the attribute to extract from all the transitions\n        :return: A list of values from all the transitions according to the attribute given in attribute_name\n        """"""\n        if len(self.transitions) > 0 and hasattr(self.transitions[0], attribute_name):\n            return [getattr(t, attribute_name) for t in self.transitions]\n        elif len(self.transitions) == 0:\n            return []\n        else:\n            raise ValueError(""The transitions have no such attribute name"")\n\n    def __getitem__(self, sliced):\n        return self.transitions[sliced]\n\n\n""""""\nVideo Dumping Methods\n""""""\n\n\nclass VideoDumpFilter(object):\n    """"""\n    Method used to decide when to dump videos\n    """"""\n    def should_dump(self, episode_terminated=False, **kwargs):\n        raise NotImplementedError("""")\n\n\nclass AlwaysDumpFilter(VideoDumpFilter):\n    """"""\n    Dump video for every episode\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def should_dump(self, episode_terminated=False, **kwargs):\n        return True\n\n\nclass MaxDumpFilter(VideoDumpFilter):\n    """"""\n    Dump video every time a new max total reward has been achieved\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.max_reward_achieved = -np.inf\n\n    def should_dump(self, episode_terminated=False, **kwargs):\n        # if the episode has not finished yet we want to be prepared for dumping a video\n        if not episode_terminated:\n            return True\n        if kwargs[\'total_reward_in_current_episode\'] > self.max_reward_achieved:\n            self.max_reward_achieved = kwargs[\'total_reward_in_current_episode\']\n            return True\n        else:\n            return False\n\n\nclass EveryNEpisodesDumpFilter(object):\n    """"""\n    Dump videos once in every N episodes\n    """"""\n    def __init__(self, num_episodes_between_dumps: int):\n        super().__init__()\n        self.num_episodes_between_dumps = num_episodes_between_dumps\n        self.last_dumped_episode = 0\n        if num_episodes_between_dumps < 1:\n            raise ValueError(""the number of episodes between dumps should be a positive number"")\n\n    def should_dump(self, episode_terminated=False, **kwargs):\n        if kwargs[\'episode_idx\'] >= self.last_dumped_episode + self.num_episodes_between_dumps - 1:\n            self.last_dumped_episode = kwargs[\'episode_idx\']\n            return True\n        else:\n            return False\n\n\nclass SelectedPhaseOnlyDumpFilter(object):\n    """"""\n    Dump videos when the phase of the environment matches a predefined phase\n    """"""\n    def __init__(self, run_phases: Union[RunPhase, List[RunPhase]]):\n        self.run_phases = force_list(run_phases)\n\n    def should_dump(self, episode_terminated=False, **kwargs):\n        if kwargs[\'_phase\'] in self.run_phases:\n            return True\n        else:\n            return False\n\n\n# TODO move to a NamedTuple, once we move to Python3.6\n#        https://stackoverflow.com/questions/34269772/type-hints-in-namedtuple/34269877\nclass CsvDataset(object):\n    def __init__(self, filepath: str, is_episodic: bool = True):\n        self.filepath = filepath\n        self.is_episodic = is_episodic\n\n\nclass PickledReplayBuffer(object):\n    def __init__(self, filepath: str):\n        self.filepath = filepath\n'"
rl_coach/dashboard.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n""""""\nTo run Coach Dashboard, run the following command:\npython3 dashboard.py\n""""""\n\nimport sys\nsys.path.append(\'.\')\n\nimport os\n\nfrom rl_coach.dashboard_components.experiment_board import display_directory_group, display_files\nfrom rl_coach.dashboard_components.globals import doc\nimport rl_coach.dashboard_components.boards\nfrom rl_coach.dashboard_components.landing_page import landing_page\n\ndoc.add_root(landing_page)\n\nimport argparse\nimport glob\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-d\', \'--experiment_dir\',\n                    help=""(string) The path of an experiment dir to open"",\n                    default=None,\n                    type=str)\nparser.add_argument(\'-f\', \'--experiment_files\',\n                    help=""(string) The path of an experiment file to open"",\n                    default=None,\n                    type=str)\nparser.add_argument(\'-rc\', \'--allow_remote_connection\',\n                    help=""(flag) Allow remote connection to view dashboard results from a remote machine. ""\n                         ""Note this opens up the connection and allows anyone who tries to connect to the ""\n                         ""ip address/port to connect and view the bokeh results via their browser."",\n                    action=\'store_true\')\n\nargs = parser.parse_args()\n\nif args.experiment_dir:\n    doc.add_timeout_callback(lambda: display_directory_group(args.experiment_dir), 1000)\nelif args.experiment_files:\n    files = []\n    for file_pattern in args.experiment_files:\n        files.extend(glob.glob(args.experiment_files))\n    doc.add_timeout_callback(lambda: display_files(files), 1000)\n\n\ndef main():\n    from rl_coach.utils import get_open_port\n\n    dashboard_path = os.path.realpath(__file__)\n    port = get_open_port()\n    command = \'bokeh serve --show {path} --port {port}\'.format(path=dashboard_path, port=port)\n\n    if args.allow_remote_connection:\n        # when allowing remote connection, selecting an experiment or a file via the GUI buttons do not seem to work\n        # well from remote. Instead, we only allow entering an experiment dir from command line.\n        if not args.experiment_dir and not args.experiment_files:\n            raise ValueError(""The allow_remote_connection flag only works in conjunction with either the experiment_dir""\n                             "" or the experiment_files flag. "")\n\n        # allow-websocket-origin = * allows connections from a remote machine.\n        command += \' --allow-websocket-origin=*\'\n\n    if args.experiment_dir or args.experiment_files:\n        command += \' --args\'\n        if args.experiment_dir:\n            command += \' --experiment_dir {}\'.format(args.experiment_dir)\n        if args.experiment_files:\n            command += \' --experiment_files {}\'.format(args.experiment_files)\n\n    os.system(command)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
rl_coach/debug_utils.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport math\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_stacking_filter import LazyStack\n\n\ndef show_observation_stack(stack, channels_last=True, show=True, force_num_rows=None, row_to_update=0):\n    if isinstance(stack, LazyStack):\n        stack = np.array(stack)\n    if isinstance(stack, list):  # is list\n        stack_size = len(stack)\n    elif len(stack.shape) == 3:\n        stack_size = stack.shape[0]  # is numpy array\n    elif len(stack.shape) == 4:\n        stack_size = stack.shape[1]  # ignore batch dimension\n        stack = stack[0]\n    else:\n        raise ValueError(""The observation stack must be a list, a numpy array or a LazyStack object"")\n\n    if channels_last:\n        stack = np.transpose(stack, (2, 0, 1))\n        stack_size = stack.shape[0]\n\n    max_cols = 10\n    if force_num_rows:\n        rows = force_num_rows\n    else:\n        rows = math.ceil(stack_size / max_cols)\n    cols = max_cols if stack_size > max_cols else stack_size\n\n    for i in range(stack_size):\n        plt.subplot(rows, cols, row_to_update * cols + i + 1)\n        plt.imshow(stack[i], cmap=\'gray\')\n\n    if show:\n        plt.show()\n\n\ndef show_diff_between_two_observations(observation1, observation2):\n    plt.imshow(observation1 - observation2, cmap=\'gray\')\n    plt.show()\n\n\ndef plot_grayscale_observation(observation):\n    plt.imshow(observation, cmap=\'gray\')\n    plt.show()\n\n\ndef plot_episode_states(episode_transitions, state_variable: str=\'state\', observation_index_in_stack: int=0):\n    observations = []\n    for transition in episode_transitions:\n        observations.append(np.array(getattr(transition, state_variable)[\'observation\'])[..., observation_index_in_stack])\n    show_observation_stack(observations, False)\n\n\ndef plot_list_of_observation_stacks(observation_stacks):\n    for idx, stack in enumerate(observation_stacks):\n        show_observation_stack(stack[\'observation\'], True, False,\n                               force_num_rows=len(observation_stacks), row_to_update=idx)\n    plt.show()\n'"
rl_coach/level_manager.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport copy\nfrom typing import Union, Dict\n\nfrom rl_coach.agents.composite_agent import CompositeAgent\nfrom rl_coach.agents.agent_interface import AgentInterface\nfrom rl_coach.core_types import EnvResponse, ActionInfo, RunPhase, ActionType, EnvironmentSteps, Transition\nfrom rl_coach.environments.environment import Environment\nfrom rl_coach.environments.environment_interface import EnvironmentInterface\nfrom rl_coach.saver import SaverCollection\nfrom rl_coach.spaces import ActionSpace, SpacesDefinition\n\n\nclass LevelManager(EnvironmentInterface):\n    """"""\n    The LevelManager is in charge of managing a level in the hierarchy of control. Each level can have one or more\n    CompositeAgents and an environment to control. Its API is double-folded:\n        1. Expose services of a LevelManager such as training the level, or stepping it (while behaving according to a\n           LevelBehaviorScheme, e.g. as SelfPlay between two identical agents). These methods are implemented in the\n           LevelManagerLogic class.\n        2. Disguise as appearing as an environment to the upper level control so it will believe it is interacting with\n           an environment. This includes stepping through what appears to be a regular environment, setting its phase\n           or resetting it. These methods are implemented directly in LevelManager as it inherits from\n           EnvironmentInterface.\n    """"""\n    def __init__(self,\n                 name: str,\n                 agents: Union[AgentInterface, Dict[str, AgentInterface]],\n                 environment: Union[\'LevelManager\', Environment],\n                 real_environment: Environment = None,\n                 steps_limit: EnvironmentSteps = EnvironmentSteps(1),\n                 should_reset_agent_state_after_time_limit_passes: bool = False,\n                 spaces_definition: SpacesDefinition = None\n                 ):\n        """"""\n        A level manager controls a single or multiple composite agents and a single environment.\n        The environment can be either a real environment or another level manager behaving as an environment.\n        :param agents: a single agent or a dictionary of agents or composite agents to control\n        :param environment: an environment or level manager to control\n        :param real_environment: the real environment that is is acted upon. if this is None (which it should be for\n         the most bottom level), it will be replaced by the environment parameter. For simple RL schemes, where there\n         is only a single level of hierarchy, this removes the requirement of defining both the environment and the\n         real environment, as they are the same.\n        :param steps_limit: the number of time steps to run when stepping the internal components\n        :param should_reset_agent_state_after_time_limit_passes: reset the agent after stepping for steps_limit\n        :param name: the level\'s name\n        :param spaces_definition: external definition of spaces for when we don\'t have an environment (e.g. batch-rl)\n        """"""\n        super().__init__()\n\n        if not isinstance(agents, dict):\n            # insert the single composite agent to a dictionary for compatibility\n            agents = {agents.name: agents}\n        if real_environment is None:\n            self._real_environment = real_environment = environment\n        self.agents = agents\n        self.environment = environment\n        self.real_environment = real_environment\n        self.steps_limit = steps_limit\n        self.should_reset_agent_state_after_time_limit_passes = should_reset_agent_state_after_time_limit_passes\n        self.full_name_id = self.name = name\n        self._phase = RunPhase.HEATUP\n        self.reset_required = False\n\n        # set self as the parent for all the composite agents\n        for agent in self.agents.values():\n            agent.parent = self\n            agent.parent_level_manager = self\n\n        # create all agents in all composite_agents - we do it here so agents will have access to their level manager\n        for agent in self.agents.values():\n            if isinstance(agent, CompositeAgent):\n                agent.create_agents()\n\n        if not isinstance(self.steps_limit, EnvironmentSteps):\n            raise ValueError(""The num consecutive steps for acting must be defined in terms of environment steps"")\n        self.build(spaces_definition)\n\n        # there are cases where we don\'t have an environment. e.g. in batch-rl or in imitation learning.\n        self.last_env_response = self.real_environment.last_env_response if self.real_environment else None\n\n        self.parent_graph_manager = None\n\n    def handle_episode_ended(self) -> None:\n        """"""\n        End the environment episode\n        :return: None\n        """"""\n        [agent.handle_episode_ended() for agent in self.agents.values()]\n\n    def reset_internal_state(self, force_environment_reset: bool = False) -> EnvResponse:\n        """"""\n        Reset the environment episode parameters\n        :param force_environment_reset: in some cases, resetting the environment can be suppressed by the environment\n                                        itself. This flag allows force the reset.\n        :return: the environment response as returned in get_last_env_response\n        """"""\n        [agent.reset_internal_state() for agent in self.agents.values()]\n        self.reset_required = False\n        if self.real_environment and self.real_environment.current_episode_steps_counter == 0:\n            self.last_env_response = self.real_environment.last_env_response\n        return self.last_env_response\n\n    @property\n    def action_space(self) -> Dict[str, ActionSpace]:\n        """"""\n        Get the action space of each of the agents wrapped in this environment.\n        :return: the action space\n        """"""\n        cagents_dict = self.agents\n        cagents_names = cagents_dict.keys()\n\n        return {name: cagents_dict[name].in_action_space for name in cagents_names}\n\n    def get_random_action(self) -> Dict[str, ActionType]:\n        """"""\n        Get a random action from the environment action space\n        :return: An action that follows the definition of the action space.\n        """"""\n        action_spaces = self.action_space  # The action spaces of the abstracted composite agents in this level\n        return {name: action_space.sample() for name, action_space in action_spaces.items()}\n\n    def get_random_action_with_info(self) -> Dict[str, ActionInfo]:\n        """"""\n        Get a random action from the environment action space and wrap it with additional info\n        :return: An action that follows the definition of the action space with additional generated info.\n        """"""\n        return {k: ActionInfo(v) for k, v in self.get_random_action().items()}\n\n    def build(self, spaces_definition: SpacesDefinition = None) -> None:\n        """"""\n        Build all the internal components of the level manager (composite agents and environment).\n        :param spaces_definition: external definition of spaces for when we don\'t have an environment (e.g. batch-rl)\n        :return: None\n        """"""\n        if spaces_definition is None:\n            # normally the spaces are defined by the environment, and we only gather these here\n            action_space = self.environment.action_space\n\n            if isinstance(action_space, dict):  # TODO: shouldn\'t be a dict\n                action_space = list(action_space.values())[0]\n\n            spaces = SpacesDefinition(state=self.real_environment.state_space,\n                                      goal=self.real_environment.goal_space,\n                                      # in HRL the agent might want to override this\n                                      action=action_space,\n                                      reward=self.real_environment.reward_space)\n        else:\n            spaces = spaces_definition\n\n        [agent.set_environment_parameters(spaces) for agent in self.agents.values()]\n\n    def setup_logger(self) -> None:\n        """"""\n        Setup the logger for all the agents in the level\n        :return: None\n        """"""\n        [agent.setup_logger() for agent in self.agents.values()]\n\n    def set_session(self, sess) -> None:\n        """"""\n        Set the deep learning framework session for all the composite agents in the level manager\n        :return: None\n        """"""\n        [agent.set_session(sess) for agent in self.agents.values()]\n\n    def train(self) -> None:\n        """"""\n        Make a training step for all the composite agents in this level manager\n        :return: the loss?\n        """"""\n        # both to screen and to csv\n        [agent.train() for agent in self.agents.values()]\n\n    @property\n    def phase(self) -> RunPhase:\n        """"""\n        Get the phase of the level manager\n        :return: the current phase\n        """"""\n        return self._phase\n\n    @phase.setter\n    def phase(self, val: RunPhase):\n        """"""\n        Change the phase of the level manager and all the hierarchy levels below it\n        :param val: the new phase\n        :return: None\n        """"""\n        self._phase = val\n        for agent in self.agents.values():\n            agent.phase = val\n\n    def acting_agent(self) -> AgentInterface:\n        """"""\n        Return the agent in this level that gets to act in the environment\n        :return: Agent\n        """"""\n        return list(self.agents.values())[0]\n\n    def step(self, action: Union[None, Dict[str, ActionType]]) -> EnvResponse:\n        """"""\n        Run a single step of following the behavioral scheme set for this environment.\n        :param action: the action to apply to the agents held in this level, before beginning following\n                       the scheme.\n        :return: None\n        """"""\n        # set the incoming directive for the sub-agent (goal / skill selection / etc.)\n        if action is not None:\n            for agent_name, agent in self.agents.items():\n                agent.set_incoming_directive(action)\n\n        if self.reset_required:\n            self.reset_internal_state()\n\n        # get last response or initial response from the environment\n        env_response = copy.copy(self.environment.last_env_response)\n\n        # step for several time steps\n        accumulated_reward = 0\n        acting_agent = self.acting_agent()\n\n        for i in range(self.steps_limit.num_steps):\n            # let the agent observe the result and decide if it wants to terminate the episode\n            done = acting_agent.observe(env_response)\n\n            if done:\n                break\n            else:\n                # get action\n                action_info = acting_agent.act()\n\n                # imitation agents will return no action since they don\'t play during training\n                if action_info:\n                    # step environment\n                    env_response = self.environment.step(action_info.action)\n\n                    # accumulate rewards such that the master policy will see the total reward during the step phase\n                    accumulated_reward += env_response.reward\n\n        # update the env response that will be exposed to the parent agent\n        env_response_for_upper_level = copy.copy(env_response)\n        env_response_for_upper_level.reward = accumulated_reward\n        self.last_env_response = env_response_for_upper_level\n\n        # if the environment terminated the episode -> let the agent observe the last response\n        # in HRL,excluding top level one, we will always enter the below if clause\n        # (because should_reset_agent_state_after_time_limit_passes is set to True)\n        if env_response.game_over or self.should_reset_agent_state_after_time_limit_passes:\n            # this is the agent\'s only opportunity to observe this transition - he will not get another one\n            acting_agent.observe(env_response)  # TODO: acting agent? maybe all of the agents in the layer?\n            self.handle_episode_ended()\n            self.reset_required = True\n\n        return env_response_for_upper_level\n\n    def save_checkpoint(self, checkpoint_prefix: str) -> None:\n        """"""\n        Save checkpoints of the networks of all agents\n        :param: checkpoint_prefix: The prefix of the checkpoint file to save\n        :return: None\n        """"""\n        [agent.save_checkpoint(checkpoint_prefix) for agent in self.agents.values()]\n\n    def restore_checkpoint(self, checkpoint_dir: str) -> None:\n        """"""\n        Restores checkpoints of the networks of all agents\n        :return: None\n        """"""\n        [agent.restore_checkpoint(checkpoint_dir) for agent in self.agents.values()]\n\n    def sync(self) -> None:\n        """"""\n        Sync the networks of the agents with the global network parameters\n        :return:\n        """"""\n        [agent.sync() for agent in self.agents.values()]\n\n    def should_train(self) -> bool:\n        return any([agent._should_update() for agent in self.agents.values()])\n\n    # TODO-remove - this is a temporary flow, used by the trainer worker, duplicated from observe() - need to create\n    #               an external trainer flow reusing the existing flow and methods [e.g. observe(), step(), act()]\n    def emulate_step_on_trainer(self, transition: Transition) -> None:\n        """"""\n        This emulates a step using the transition obtained from the rollout worker on the training worker\n        in case of distributed training.\n        Run a single step of following the behavioral scheme set for this environment.\n        :param action: the action to apply to the agents held in this level, before beginning following\n                       the scheme.\n        :return: None\n        """"""\n\n        if self.reset_required:\n            self.reset_internal_state()\n\n        acting_agent = self.acting_agent()\n\n        # for i in range(self.steps_limit.num_steps):\n        # let the agent observe the result and decide if it wants to terminate the episode\n        done = acting_agent.observe_transition(transition)\n        acting_agent.act(transition.action)\n\n        if done:\n            self.handle_episode_ended()\n            self.reset_required = True\n\n    def should_stop(self) -> bool:\n        return all([agent.get_success_rate() >= self.environment.get_target_success_rate() for agent in self.agents.values()])\n\n    def collect_savers(self) -> SaverCollection:\n        """"""\n        Calls collect_savers() on all agents and combines the results to a single collection\n        :return: saver collection of all agent savers\n        """"""\n        savers = SaverCollection()\n        for agent in self.agents.values():\n            savers.update(agent.collect_savers(parent_path_suffix=self.name))\n        return savers\n'"
rl_coach/logger.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport atexit\nimport datetime\nimport os\nimport re\nimport shutil\nimport signal\nimport time\nimport uuid\nfrom subprocess import Popen, PIPE\nfrom typing import Union\n\nfrom PIL import Image\nfrom pandas import DataFrame\nfrom six.moves import input\n\nglobal failed_imports\nfailed_imports = []\n\n\nclass Colors(object):\n    PURPLE = \'\\033[95m\'\n    CYAN = \'\\033[96m\'\n    DARKCYAN = \'\\033[36m\'\n    BLUE = \'\\033[94m\'\n    GREEN = \'\\033[92m\'\n    YELLOW = \'\\033[93m\'\n    RED = \'\\033[91m\'\n    WHITE = \'\\033[37m\'\n    BG_RED = \'\\033[41m\'\n    BG_GREEN = \'\\033[42m\'\n    BG_YELLOW = \'\\033[43m\'\n    BG_BLUE = \'\\033[44m\'\n    BG_PURPLE = \'\\033[45m\'\n    BG_CYAN = \'\\033[30;46m\'\n    BG_WHITE = \'\\x1b[30;47m\'\n    BG_RESET = \'\\033[49m\'\n    BOLD = \'\\033[1m\'\n    UNDERLINE_ON = \'\\033[4m\'\n    UNDERLINE_OFF = \'\\033[24m\'\n    END = \'\\033[0m\'\n\n\n# prints to screen with a prefix identifying the origin of the print\nclass ScreenLogger(object):\n    def __init__(self, name, use_colors=True):\n        self.name = name\n        self.set_use_colors(use_colors)\n\n    def set_use_colors(self, use_colors):\n        self._use_colors = use_colors\n        if use_colors:\n            self._prefix_success = Colors.GREEN\n            self._prefix_warning = Colors.YELLOW\n            self._prefix_error = Colors.RED\n            self._prefix_title = Colors.BG_CYAN\n            self._prefix_ask = Colors.BG_CYAN\n            self._suffix = Colors.END\n        else:\n            self._prefix_success = """"\n            self._prefix_warning = ""!! ""\n            self._prefix_error = ""!!!! ""\n            self._prefix_title = ""## ""\n            self._prefix_ask = """"\n            self._suffix = """"\n\n    def separator(self):\n        print("""")\n        print(""--------------------------------"")\n        print("""")\n\n    def log(self, data):\n        print(data)\n\n    def log_dict(self, data, prefix=""""):\n        timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d-%H:%M:%S.%f\') + \' \'\n        if self._use_colors:\n            str = timestamp\n            str += ""{}{}{} - "".format(Colors.PURPLE, prefix, Colors.END)\n            for k, v in data.items():\n                str += ""{}{}: {}{} "".format(Colors.BLUE, k, Colors.END, v)\n            print(str)\n        else:\n            logentries = [timestamp]\n            for k, v in data.items():\n                logentries.append(""{}={}"".format(k, v))\n            logline = ""{}> {}"".format(prefix, "", "".join(logentries))\n            print(logline)\n\n    def log_title(self, title):\n        print(""{}{}{}"".format(self._prefix_title, title, self._suffix))\n\n    def success(self, text):\n        print(""{}{}{}"".format(self._prefix_success, text, self._suffix))\n\n    def warning(self, text):\n        print(""{}{}{}"".format(self._prefix_warning, text, self._suffix))\n\n    def error(self, text, crash=True):\n        print(""{}{}{}"".format(self._prefix_error, text, self._suffix))\n        if crash:\n            exit(1)\n\n    def ask_input(self, title):\n        return input(""{}{}{}"".format(self._prefix_ask, title, self._suffix))\n\n    def ask_input_with_timeout(self, title, timeout, msg_if_timeout=\'Timeout expired.\'):\n        class TimeoutExpired(Exception):\n            pass\n\n        def timeout_alarm_handler(signum, frame):\n            raise TimeoutExpired\n\n        signal.signal(signal.SIGALRM, timeout_alarm_handler)\n        signal.alarm(timeout)\n\n        try:\n            return input(""{}{}{}"".format(Colors.BG_CYAN, title, Colors.END))\n        except TimeoutExpired:\n            self.warning(msg_if_timeout)\n        finally:\n            signal.alarm(0)\n\n    def ask_yes_no(self, title: str, default: Union[None, bool] = None):\n        """"""\n        Ask the user for a yes / no question and return True if the answer is yes and False otherwise.\n        The function will keep asking the user for an answer until he answers one of the possible responses.\n        A default answer can be passed and will be selected if the user presses enter\n        :param title: The question to ask the user\n        :param default: the default answer\n        :return: True / False according to the users answer\n        """"""\n        default_answer = \'y/n\'\n        if default == True:\n            default_answer = \'Y/n\'\n        elif default == False:\n            default_answer = \'y/N\'\n\n        while True:\n            answer = input(""{}{}{} ({})"".format(self._prefix_ask, title, self._suffix, default_answer))\n            if answer == ""yes"" or answer == ""YES"" or answer == ""y"" or answer == ""Y"":\n                return True\n            elif answer == ""no"" or answer == ""NO"" or answer == ""n"" or answer == ""N"":\n                return False\n            elif answer == """":\n                if default is not None:\n                    return default\n\n    def change_terminal_title(self, title: str):\n        """"""\n        Changes the title of the terminal window\n        :param title: The new title\n        :return: None\n        """"""\n        if self._use_colors:\n            print(""\\x1b]2;{}\\x07"".format(title))\n        else:\n            print(""Title: %s"" % title)\n\n\nclass BaseLogger(object):\n    def __init__(self):\n        self.data = DataFrame()\n        self.csv_path = \'\'\n        self.start_time = None\n        self.time = None\n        self.experiments_path = """"\n        self.last_line_idx_written_to_csv = 0\n        self.experiment_name = """"\n        self.index_name = ""Index""\n\n    def set_current_time(self, time):\n        self.time = time\n\n    def create_signal_value(self, signal_name, value, overwrite=True, time=None):\n        if self.index_name == signal_name:\n            return False  # make sure that we don\'t create duplicate signals\n\n        if self.last_line_idx_written_to_csv != 0:\n            assert signal_name in self.data.columns\n\n        if not time:\n            time = self.time\n        # create only if it doesn\'t already exist\n        if overwrite or not self.signal_value_exists(time, signal_name):\n            self.data.loc[time, signal_name] = value\n            return True\n        return False\n\n    def change_signal_value(self, signal_name, time, value):\n        # change only if it already exists\n        if self.signal_value_exists(time, signal_name):\n            self.data.loc[time, signal_name] = value\n            return True\n        return False\n\n    def signal_value_exists(self, signal_name, time):\n        try:\n            value = self.get_signal_value(time, signal_name)\n            if value != value:  # value is nan\n                return False\n        except:\n            return False\n        return True\n\n    def get_signal_value(self, signal_name, time=None):\n        if not time:\n            time = self.time\n        return self.data.loc[time, signal_name]\n\n    def dump_output_csv(self, append=True):\n        self.data.index.name = self.index_name\n        if len(self.data.index) == 1:\n            self.start_time = time.time()\n\n        if os.path.exists(self.csv_path) and append:\n            self.data[self.last_line_idx_written_to_csv:].to_csv(self.csv_path, mode=\'a\', header=False)\n        else:\n            self.data.to_csv(self.csv_path)\n\n        self.last_line_idx_written_to_csv = len(self.data.index)\n\n    def get_current_wall_clock_time(self):\n        if self.start_time:\n            return time.time() - self.start_time\n        else:\n            self.start_time = time.time()\n            return 0\n\n    def update_wall_clock_time(self, index):\n        self.create_signal_value(\'Wall-Clock Time\', self.get_current_wall_clock_time(), time=index)\n\n\nclass EpisodeLogger(BaseLogger):\n    def __init__(self):\n        super().__init__()\n        self.worker_dir_path = \'\'\n        self.index_name = ""Episode Steps""\n\n    def set_logger_filenames(self, _experiments_path, logger_prefix=\'\', task_id=None, add_timestamp=False, filename=\'\'):\n        self.experiments_path = _experiments_path\n\n        # set file names\n        if task_id is not None:\n            filename += ""worker_{}."".format(task_id)\n\n        # add timestamp\n        if add_timestamp:\n            filename += logger_prefix\n\n        self.worker_dir_path = os.path.join(_experiments_path, \'{}\'.format(filename))\n        if not os.path.exists(self.worker_dir_path):\n            os.makedirs(self.worker_dir_path)\n\n    def set_episode_idx(self, episode_idx):\n        self.data = DataFrame()\n        self.csv_path = os.path.join(self.worker_dir_path, \'episode_{}.csv\'.format(episode_idx))\n        self.last_line_idx_written_to_csv = 0\n\n\nclass Logger(BaseLogger):\n    def __init__(self, index_name=\'Episode #\'):\n        super().__init__()\n        self.doc_path = \'\'\n        self.index_name = index_name\n\n    def set_index_name(self, index_name):\n        self.index_name = index_name\n\n    def set_logger_filenames(self, _experiments_path, logger_prefix=\'\', task_id=None, add_timestamp=False, filename=\'\'):\n        self.experiments_path = _experiments_path\n\n        # set file names\n        if task_id is not None:\n            filename += ""worker_{}."".format(task_id)\n\n        # add timestamp\n        if add_timestamp:\n            filename += logger_prefix\n\n        # add an index to the file in case there is already an experiment running with the same timestamp\n        path_exists = True\n        idx = 0\n        while path_exists:\n            self.csv_path = os.path.join(_experiments_path, \'{}_{}.csv\'.format(filename, idx))\n            self.doc_path = os.path.join(_experiments_path, \'{}_{}.json\'.format(filename, idx))\n            path_exists = os.path.exists(self.csv_path) or os.path.exists(self.doc_path)\n            idx += 1\n\n    def dump_documentation(self, parameters):\n        if not os.path.exists(os.path.dirname(self.doc_path)):\n            os.makedirs(self.experiments_path)\n        with open(self.doc_path, \'w\') as outfile:\n            outfile.write(parameters)\n\n\n#######################################################################################################################\n#################################### Module Related Methods/Vars ######################################################\n#######################################################################################################################\n\nglobal experiment_path\nexperiment_path = """"\n\nglobal experiment_name\nexperiment_name = None\ntime_started = datetime.datetime.now()\n\n\ndef two_digits(num):\n    return \'%02d\' % num\n\n\ndef create_gif(images, fps=10, name=""Gif""):\n    global experiment_path\n\n    output_file = \'{}_{}.gif\'.format(datetime.datetime.now().strftime(\'%Y-%m-%d-%H-%M-%S\'), name)\n    output_dir = os.path.join(experiment_path, \'gifs\')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    output_path = os.path.join(output_dir, output_file)\n    pil_images = [Image.fromarray(image) for image in images]\n    pil_images[0].save(output_path, save_all=True, append_images=pil_images[1:], duration=1.0 / fps, loop=0)\n\n\ndef create_mp4(images, fps=10, name=""mp4""):\n    global experiment_path\n\n    output_file = \'{}_{}.mp4\'.format(datetime.datetime.now().strftime(\'%Y-%m-%d-%H-%M-%S\'), name)\n    output_dir = os.path.join(experiment_path, \'videos\')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    output_path = os.path.join(output_dir, output_file)\n    shape = \'x\'.join([str(d) for d in images[0].shape[:2][::-1]])\n    command = [\'ffmpeg\',\n               \'-y\',\n               \'-f\', \'rawvideo\',\n               \'-s\', shape,\n               \'-pix_fmt\', \'rgb24\',\n               \'-r\', str(fps),\n               \'-i\', \'-\',\n               \'-vcodec\', \'libx264\',\n               \'-pix_fmt\', \'yuv420p\',\n               output_path]\n\n    p = Popen(command, stdin=PIPE, stderr=PIPE)\n    for image in images:\n        p.stdin.write(image.tostring())\n    p.stdin.close()\n    p.wait()\n\n\ndef remove_experiment_dir():\n    shutil.rmtree(experiment_path)\n\n\ndef summarize_experiment():\n    screen.separator()\n    screen.log_title(""Results stored at: {}"".format(experiment_path))\n    screen.log_title(""Total runtime: {}"".format(datetime.datetime.now() - time_started))\n    # TODO: reimplement the following code to print out the max reward during the training\n    # if \'Training Reward\' in self.data.keys() and \'Evaluation Reward\' in self.data.keys():\n    #     screen.log_title(""Max training reward: {}, max evaluation reward: {}"".format(\n    # self.data[\'Training Reward\'].max(), self.data[\'Evaluation Reward\'].max()))\n    screen.separator()\n    if screen.ask_yes_no(""Do you want to discard the experiment results (Warning: this cannot be undone)?"", False):\n        remove_experiment_dir()\n    elif screen.ask_yes_no(""Do you want to specify a different experiment name to save to?"", False):\n        new_name = get_experiment_name()\n        old_path = experiment_path\n        new_path = get_experiment_path(new_name, create_path=False)\n        shutil.move(old_path, new_path)\n        screen.log_title(""Results moved to: {}"".format(new_path))\n\n\ndef get_experiment_name(initial_experiment_name=None):\n    global experiment_name\n\n    match = None\n    while match is None:\n        if initial_experiment_name is None:\n            msg_if_timeout = ""Timeout waiting for experiement name.""\n            experiment_name = screen.ask_input_with_timeout(""Please enter an experiment name: "", 60, msg_if_timeout)\n        else:\n            experiment_name = initial_experiment_name\n\n        if not experiment_name:\n            experiment_name = \'\'\n\n        experiment_name = experiment_name.replace("" "", ""_"")\n        match = re.match(""^$|^[\\w -/]{1,1000}$"", experiment_name)\n\n        if match is None:\n            screen.error(\'Experiment name must be composed only of alphanumeric letters, \'\n                         \'underscores and dashes and should not be longer than 1000 characters.\')\n\n    experiment_name = match.group(0)\n    return experiment_name\n\n\ndef get_experiment_path(experiment_name, initial_experiment_path=None, create_path=True):\n    global experiment_path\n\n    if not initial_experiment_path:\n        initial_experiment_path = \'./experiments/\'\n    general_experiments_path = os.path.join(initial_experiment_path, experiment_name)\n\n    cur_date = time_started.date()\n    cur_time = time_started.time()\n\n    if not os.path.exists(general_experiments_path) and create_path:\n        os.makedirs(general_experiments_path)\n    experiment_path = os.path.join(general_experiments_path, \'{}_{}_{}-{}_{}\'\n                                   .format(two_digits(cur_date.day), two_digits(cur_date.month),\n                                           cur_date.year, two_digits(cur_time.hour),\n                                           two_digits(cur_time.minute)))\n    i = 0\n    while True:\n        if os.path.exists(experiment_path):\n            experiment_path = os.path.join(general_experiments_path, \'{}_{}_{}-{}_{}_{}\'\n                                           .format(cur_date.day, cur_date.month, cur_date.year, cur_time.hour,\n                                                   cur_time.minute, i))\n            i += 1\n        else:\n            if create_path:\n                os.makedirs(experiment_path)\n            return experiment_path\n\n\nglobal screen\nscreen = ScreenLogger("""")\n'"
rl_coach/plot_atari.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport argparse\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom rl_coach.dashboard_components.signals_file import SignalsFile\n\n\nclass FigureMaker(object):\n    def __init__(self, path, cols, smoothness, signal_to_plot, x_axis, color):\n        self.experiments_path = path\n        self.environments = self.list_environments()\n        self.cols = cols\n        self.rows = int((len(self.environments) + cols - 1) / cols)\n        self.smoothness = smoothness\n        self.signal_to_plot = signal_to_plot\n        self.x_axis = x_axis\n        self.color = color\n\n        params = {\n            \'axes.labelsize\': 8,\n            \'font.size\': 10,\n            \'legend.fontsize\': 14,\n            \'xtick.labelsize\': 8,\n            \'ytick.labelsize\': 8,\n            \'text.usetex\': False,\n            \'figure.figsize\': [16, 30]\n        }\n        matplotlib.rcParams.update(params)\n\n    def list_environments(self):\n        environments = sorted([e.name for e in os.scandir(self.experiments_path) if e.is_dir()])\n        filtered_environments = self.filter_environments(environments)\n        return filtered_environments\n\n    def filter_environments(self, environments):\n        filtered_environments = []\n        for idx, environment in enumerate(environments):\n            path = os.path.join(self.experiments_path, environment)\n            experiments = [e.name for e in os.scandir(path) if e.is_dir()]\n\n            # take only the last updated experiment directory\n            last_experiment_dir = max([os.path.join(path, root) for root in experiments], key=os.path.getctime)\n\n            # make sure there is a csv file inside it\n            for file_path in os.listdir(last_experiment_dir):\n                full_file_path = os.path.join(last_experiment_dir, file_path)\n                if os.path.isfile(full_file_path) and file_path.endswith(\'.csv\'):\n                    filtered_environments.append((environment, full_file_path))\n\n        return filtered_environments\n\n    def plot_figures(self, prev_subplot_map=None):\n        subplot_map = {}\n        for idx, (environment, full_file_path) in enumerate(self.environments):\n            environment = environment.split(\'level\')[1].split(\'-\')[1].split(\'Deterministic\')[0][1:]\n            if prev_subplot_map:\n                # skip on environments which were not plotted before\n                if environment not in prev_subplot_map.keys():\n                    continue\n                subplot_idx = prev_subplot_map[environment]\n            else:\n                subplot_idx = idx + 1\n            print(environment)\n            axis = plt.subplot(self.rows, self.cols, subplot_idx)\n            subplot_map[environment] = subplot_idx\n            signals = SignalsFile(full_file_path)\n            signals.change_averaging_window(self.smoothness, force=True, signals=[self.signal_to_plot])\n            steps = signals.bokeh_source.data[self.x_axis]\n            rewards = signals.bokeh_source.data[self.signal_to_plot]\n\n            yloc = plt.MaxNLocator(4)\n            axis.yaxis.set_major_locator(yloc)\n            axis.ticklabel_format(style=\'sci\', axis=\'x\', scilimits=(0, 0))\n            plt.title(environment, fontsize=10, y=1.08)\n            plt.plot(steps, rewards, self.color, linewidth=0.8)\n            plt.subplots_adjust(hspace=2.0, wspace=0.4)\n\n        return subplot_map\n\n    def save_pdf(self, name):\n        plt.savefig(name + "".pdf"", bbox_inches=\'tight\')\n\n    def show_figures(self):\n        plt.show()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-p\', \'--paths\',\n                        help=""(string) Root directory of the experiments"",\n                        default=None,\n                        type=str)\n    parser.add_argument(\'-c\', \'--cols\',\n                        help=""(int) Number of plot columns"",\n                        default=6,\n                        type=int)\n    parser.add_argument(\'-s\', \'--smoothness\',\n                        help=""(int) Number of consequent episodes to average over"",\n                        default=100,\n                        type=int)\n    parser.add_argument(\'-sig\', \'--signal\',\n                        help=""(str) The name of the signal to plot"",\n                        default=\'Evaluation Reward\',\n                        type=str)\n    parser.add_argument(\'-x\', \'--x_axis\',\n                        help=""(str) The meaning of the x axis"",\n                        default=\'Total steps\',\n                        type=str)\n    parser.add_argument(\'-pdf\', \'--pdf\',\n                        help=""(str) A name of a pdf to save to"",\n                        default=\'atari\',\n                        type=str)\n    args = parser.parse_args()\n\n    paths = args.paths.split("","")\n    subplot_map = None\n    for idx, path in enumerate(paths):\n        maker = FigureMaker(path, cols=args.cols, smoothness=args.smoothness, signal_to_plot=args.signal, x_axis=args.x_axis, color=\'C{}\'.format(idx))\n        subplot_map = maker.plot_figures(subplot_map)\n    plt.legend(paths)\n    maker.save_pdf(args.pdf)\n    maker.show_figures()\n'"
rl_coach/renderer.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\nimport contextlib\nwith contextlib.redirect_stdout(None):\n    import pygame\nfrom pygame.locals import HWSURFACE, DOUBLEBUF\n\n\nclass Renderer(object):\n    def __init__(self):\n        self.size = (1, 1)\n        self.screen = None\n        self.clock = pygame.time.Clock()\n        self.display = pygame.display\n        self.fps = 30\n        self.pressed_keys = []\n        self.is_open = False\n\n    def create_screen(self, width, height):\n        """"""\n        Creates a pygame window\n        :param width: the width of the window\n        :param height: the height of the window\n        :return: None\n        """"""\n        self.size = (width, height)\n        self.screen = self.display.set_mode(self.size, HWSURFACE | DOUBLEBUF)\n        self.display.set_caption(""Coach"")\n        self.is_open = True\n\n    def normalize_image(self, image):\n        """"""\n        Normalize image values to be between 0 and 255\n        :param image: 2D/3D array containing an image with arbitrary values\n        :return: the input image with values rescaled to 0-255\n        """"""\n        image_min, image_max = image.min(), image.max()\n        return 255.0 * (image - image_min) / (image_max - image_min)\n\n    def render_image(self, image):\n        """"""\n        Render the given image to the pygame window\n        :param image: a grayscale or color image in an arbitrary size. assumes that the channels are the last axis\n        :return: None\n        """"""\n        if self.is_open:\n            if len(image.shape) == 2:\n                image = np.stack([image] * 3)\n            if len(image.shape) == 3:\n                if image.shape[0] == 3 or image.shape[0] == 1:\n                    image = np.transpose(image, (1, 2, 0))\n            surface = pygame.surfarray.make_surface(image.swapaxes(0, 1))\n            surface = pygame.transform.scale(surface, self.size)\n            self.screen.blit(surface, (0, 0))\n            self.display.flip()\n            self.clock.tick()\n            self.get_events()\n\n    def get_events(self):\n        """"""\n        Get all the window events in the last tick and reponse accordingly\n        :return: None\n        """"""\n        for event in pygame.event.get():\n            if event.type == pygame.KEYDOWN:\n                self.pressed_keys.append(event.key)\n                # esc pressed\n                if event.key == pygame.K_ESCAPE:\n                    self.close()\n            elif event.type == pygame.KEYUP:\n                if event.key in self.pressed_keys:\n                    self.pressed_keys.remove(event.key)\n            elif event.type == pygame.QUIT:\n                self.close()\n\n    def get_key_names(self, key_ids):\n        """"""\n        Get the key name for each key index in the list\n        :param key_ids: a list of key id\'s\n        :return: a list of key names corresponding to the key id\'s\n        """"""\n        return [pygame.key.name(key_id) for key_id in key_ids]\n\n    def close(self):\n        """"""\n        Close the pygame window\n        :return: None\n        """"""\n        self.is_open = False\n        pygame.quit()\n'"
rl_coach/rollout_worker.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n""""""\nthis rollout worker:\n\n- restores a model from disk\n- evaluates a predefined number of episodes\n- contributes them to a distributed memory\n- exits\n""""""\n\n\n\nimport time\nimport os\n\nfrom rl_coach.base_parameters import TaskParameters, DistributedCoachSynchronizationType\nfrom rl_coach.checkpoint import CheckpointStateFile, CheckpointStateReader\nfrom rl_coach.data_stores.data_store import SyncFiles\nfrom rl_coach.core_types import RunPhase\n\n\ndef wait_for(wait_func, data_store=None, timeout=10):\n    """"""\n    block until wait_func is true\n    """"""\n    for i in range(timeout):\n        if data_store:\n            data_store.load_from_store()\n\n        if wait_func():\n            return\n        time.sleep(10)\n\n    # one last time\n    if wait_func():\n        return\n\n    raise ValueError((\n        \'Waited {timeout} seconds, but condition timed out\'\n    ).format(\n        timeout=timeout,\n    ))\n\n\ndef wait_for_trainer_ready(checkpoint_dir, data_store=None, timeout=10):\n    """"""\n    Block until trainer is ready\n    """"""\n\n    def wait():\n        return os.path.exists(os.path.join(checkpoint_dir, SyncFiles.TRAINER_READY.value))\n\n    wait_for(wait, data_store, timeout)\n\n\ndef rollout_worker(graph_manager, data_store, num_workers, task_parameters):\n    """"""\n    wait for first checkpoint then perform rollouts using the model\n    """"""\n    if (\n        graph_manager.agent_params.algorithm.distributed_coach_synchronization_type\n        == DistributedCoachSynchronizationType.SYNC\n    ):\n        timeout = float(""inf"")\n    else:\n        timeout = None\n\n    # this could probably be moved up into coach.py\n    graph_manager.create_graph(task_parameters)\n\n    data_store.load_policy(graph_manager, require_new_policy=False, timeout=60)\n\n    with graph_manager.phase_context(RunPhase.TRAIN):\n        # this worker should play a fraction of the total playing steps per rollout\n        graph_manager.reset_internal_state(force_environment_reset=True)\n\n        act_steps = (\n            graph_manager.agent_params.algorithm.num_consecutive_playing_steps\n            / num_workers\n        )\n        for i in range(graph_manager.improve_steps / act_steps):\n            if data_store.end_of_policies():\n                break\n\n            graph_manager.act(\n                act_steps,\n                wait_for_full_episodes=graph_manager.agent_params.algorithm.act_for_full_episodes,\n            )\n\n            data_store.load_policy(graph_manager, require_new_policy=True, timeout=timeout)\n'"
rl_coach/run_multiple_seeds.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nsys.path.append(\'.\')\nfrom subprocess import Popen\nimport argparse\nfrom rl_coach.utils import set_gpu, force_list\n\n""""""\nThis script makes it easier to run multiple instances of a given preset.\nEach instance uses a different seed, and optionally, multiple environment levels can be configured as well.\n""""""\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-p\', \'--preset\',\n                        help=""(string) The preset to run"",\n                        default=None,\n                        type=str)\n    parser.add_argument(\'-s\', \'--seeds\',\n                        help=""(int) Number of seeds to run"",\n                        default=5,\n                        type=int)\n    parser.add_argument(\'-lvl\', \'--level\',\n                        help=""(string) Environment level to use. This can be defined as a comma separated list."",\n                        default=None,\n                        type=str)\n    parser.add_argument(\'-g\', \'--gpu\',\n                        help=""(int) The gpu to use. This can be defined as a comma separated list. For example,""\n                             "" 0,1 will use both gpu\'s 0 and 1, by switching between them for each run instance"",\n                        default=\'0\',\n                        type=str)\n    parser.add_argument(\'-n\', \'--num_workers\',\n                        help=""(int) The number of workers to use for each run"",\n                        default=1,\n                        type=int)\n    parser.add_argument(\'-d\', \'--dir_prefix\',\n                        help=""(str) A prefix for the directory name. If not given, the directory name will match ""\n                             ""the preset name, followed by the environment level"",\n                        default=\'\',\n                        type=str)\n    parser.add_argument(\'-lsd\', \'--level_as_sub_dir\',\n                        help=""(flag) Store each level in it\'s own sub directory where the root directory name matches ""\n                             ""the preset name"",\n                        action=\'store_true\')\n    parser.add_argument(\'-ssd\', \'--seed_as_sub_dir\',\n                        help=""(flag) Store each seed in it\'s own sub directory where the root directory name matches ""\n                             ""the preset name"",\n                        action=\'store_true\')\n    parser.add_argument(\'-ew\', \'--evaluation_worker\',\n                        help=""(flag) Start an additional worker that will only do evaluation"",\n                        action=\'store_true\')\n    parser.add_argument(\'-c\', \'--use_cpu\',\n                        help=""(flag) Use the cpu instead of the gpu"",\n                        action=\'store_true\')\n    parser.add_argument(\'-f\', \'--framework\',\n                        help=""(string) Neural network framework. Available values: tensorflow, mxnet"",\n                        default=\'tensorflow\',\n                        type=str)\n    args = parser.parse_args()\n\n    # dir_prefix = ""benchmark_""\n    # preset = \'Mujoco_DDPG\'  # \'Mujoco_DDPG\'\n    # levels = [""inverted_pendulum""]\n    # num_seeds = 5\n    # num_workers = 1\n    # gpu = [0, 1]\n    #\n\n    # if no arg is given\n    if len(sys.argv) == 1:\n        parser.print_help()\n        exit(0)\n\n    dir_prefix = args.dir_prefix\n    preset = args.preset\n    levels = args.level.split(\',\') if args.level is not None else [None]\n    num_seeds = args.seeds\n    num_workers = args.num_workers\n    framework = args.framework\n    gpu = [int(gpu) for gpu in args.gpu.split(\',\')]\n    level_as_sub_dir = args.level_as_sub_dir\n\n    processes = []\n    gpu_list = force_list(gpu)\n    curr_gpu_idx = 0\n    for level in levels:\n        if dir_prefix != """":\n            dir_prefix += ""_""\n        for seed in range(num_seeds):\n            # select the next gpu for this run\n            set_gpu(gpu_list[curr_gpu_idx])\n\n            command = [\'python3\', \'rl_coach/coach.py\', \'-ns\', \'-p\', \'{}\'.format(preset),\n                       \'--seed\', \'{}\'.format(seed), \'-n\', \'{}\'.format(num_workers),\n                       \'--framework\', framework]\n            if args.use_cpu:\n                command.append(""-c"")\n            if args.evaluation_worker:\n                command.append(""-ew"")\n            if args.seed_as_sub_dir:\n                seed = \'\'\n            if level is not None:\n                command.extend([\'-lvl\', \'{}\'.format(level)])\n                if level_as_sub_dir:\n                    separator = ""/""\n                else:\n                    separator = ""_""\n                command.extend([\'-e\', \'{dir_prefix}{preset}_{seed}_{separator}{level}_{num_workers}_workers\'.format(\n                    dir_prefix=dir_prefix, preset=preset, seed=seed, level=level, separator=separator,\n                    num_workers=args.num_workers)])\n            else:\n                command.extend([\'-e\', \'{dir_prefix}{preset}_{seed}_{num_workers}_workers\'.format(\n                    dir_prefix=dir_prefix, preset=preset, seed=seed, num_workers=args.num_workers)])\n            print(command)\n\n            p = Popen(command)\n            processes.append(p)\n\n            # for each run, select the next gpu from the available gpus\n            curr_gpu_idx = (curr_gpu_idx + 1) % len(gpu_list)\n\n    for p in processes:\n        p.wait()\n'"
rl_coach/saver.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\n""""""\nModule for abstract base class for checkpoint object and checkpoint collection\n""""""\nfrom typing import Any, Dict, List\n\n\nclass Saver(object):\n    """"""\n    ABC for saver objects that implement saving/restoring to/from path, and merging two savers.\n    """"""\n    @property\n    def path(self):\n        """"""\n        Relative path for save/load. If two saver objects return the same path, they must be merge-able.\n        """"""\n        raise NotImplementedError\n\n    def save(self, sess: Any, save_path: str) -> List[str]:\n        """"""\n        Save to save_path\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param save_path: full path to save checkpoint (typically directory plus self.path plus checkpoint count).\n        :return: list of all saved paths\n        """"""\n        raise NotImplementedError\n\n    def restore(self, sess: Any, restore_path: str) -> None:\n        """"""\n        Restore from restore_path\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param restore_path: full path to load checkpoint from.\n        """"""\n        raise NotImplementedError\n\n    def merge(self, other: \'Saver\') -> None:\n        """"""\n        Merge other saver into this saver\n        :param other: saver to be merged into self\n        """"""\n        raise NotImplementedError\n\n\nclass SaverCollection(object):\n    """"""\n    Object for storing a collection of saver objects. It takes care of ensuring uniqueness of saver paths\n    and merging savers if they have the same path. For example, if a saver handles saving a generic key/value\n    file for all networks in a single file, it can use a more generic path and all savers of all networks would be\n    merged into a single saver that saves/restores parameters for all networks.\n    NOTE: If two savers have the same path, the respective saver class must support merging them\n    into a single saver that saves/restores all merged parameters.\n    """"""\n    def __init__(self, saver: Saver = None):\n        """"""\n        :param saver: optional initial saver for the collection\n        """"""\n        self._saver_dict = dict()  # type: Dict[str, Saver]\n        if saver is not None:\n            self._saver_dict[saver.path] = saver\n\n    def add(self, saver: Saver):\n        """"""\n        Add a new saver to the collection. If saver.path is already in the collection, merge\n        the new saver with the existing saver.\n        :param saver: new saver to be added to collection\n        """"""\n        if saver.path in self._saver_dict:\n            self._saver_dict[saver.path].merge(saver)\n        else:\n            self._saver_dict[saver.path] = saver\n\n    def update(self, other: \'SaverCollection\'):\n        """"""\n        Merge savers from other collection into self\n        :param other: saver collection to update self with.\n        """"""\n        for c in other:\n            self.add(c)\n\n    def save(self, sess: Any, save_path: str) -> List[str]:\n        """"""\n        Call save on all savers in the collection\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param save_path: path for saving checkpoints using savers. All saved file paths must\n        start with this path in their full path. For example if save_path is \'/home/checkpoints/checkpoint-01\',\n        then saved file paths can be \'/home/checkpoints/checkpoint-01.main-network\' but not\n        \'/home/checkpoints/main-network\'\n        :return: list of all saved paths\n        """"""\n        paths = list()\n        for saver in self:\n            paths.extend(saver.save(sess, self._full_path(save_path, saver)))\n        return paths\n\n    def restore(self, sess: Any, restore_path: str) -> None:\n        """"""\n        Call restore on all savers in the collection\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param restore_path: path for restoring checkpoint using savers.\n        """"""\n        for saver in self:\n            saver.restore(sess, self._full_path(restore_path, saver))\n\n    def __iter__(self):\n        """"""\n        Return an iterator for savers in the collection\n        :return: saver iterator\n        """"""\n        return (v for v in self._saver_dict.values())\n\n    @staticmethod\n    def _full_path(path_prefix: str, saver: Saver) -> str:\n        """"""\n        Concatenates path of the saver to parent prefix to create full save path\n        :param path_prefix: prefix of the path\n        :param saver: saver object to get unique path extension from\n        :return: full path\n        """"""\n        if saver.path == """":\n            return path_prefix\n        return ""{}.{}"".format(path_prefix, saver.path)\n\n\n'"
rl_coach/schedules.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom rl_coach.core_types import EnvironmentSteps\n\n\nclass Schedule(object):\n    def __init__(self, initial_value: float):\n        self.initial_value = initial_value\n        self.current_value = initial_value\n\n    def step(self):\n        raise NotImplementedError("""")\n\n\nclass ConstantSchedule(Schedule):\n    def __init__(self, initial_value: float):\n        super().__init__(initial_value)\n\n    def step(self):\n        pass\n\n\nclass LinearSchedule(Schedule):\n    """"""\n    A simple linear schedule which decreases or increases over time from an initial to a final value\n    """"""\n    def __init__(self, initial_value: float, final_value: float, decay_steps: int):\n        """"""\n        :param initial_value: the initial value\n        :param final_value: the final value\n        :param decay_steps: the number of steps that are required to decay the initial value to the final value\n        """"""\n        super().__init__(initial_value)\n        self.final_value = final_value\n        self.decay_steps = decay_steps\n        self.decay_delta = (initial_value - final_value) / float(decay_steps)\n\n    def step(self):\n        self.current_value -= self.decay_delta\n        # decreasing schedule\n        if self.final_value < self.initial_value:\n            self.current_value = np.clip(self.current_value, self.final_value, self.initial_value)\n        # increasing schedule\n        if self.final_value > self.initial_value:\n            self.current_value = np.clip(self.current_value, self.initial_value, self.final_value)\n\n\nclass PieceWiseSchedule(Schedule):\n    """"""\n    A schedule which consists of multiple sub-schedules, where each one is used for a defined number of steps\n    """"""\n    def __init__(self, schedules: List[Tuple[Schedule, EnvironmentSteps]]):\n        """"""\n        :param schedules: a list of schedules to apply serially. Each element of the list should be a tuple of\n                          2 elements - a schedule and the number of steps to run it in terms of EnvironmentSteps\n        """"""\n        super().__init__(schedules[0][0].initial_value)\n        self.schedules = schedules\n        self.current_schedule = schedules[0]\n        self.current_schedule_idx = 0\n        self.current_schedule_step_count = 0\n\n    def step(self):\n        self.current_schedule[0].step()\n\n        if self.current_schedule_idx < len(self.schedules) - 1 \\\n                and self.current_schedule_step_count >= self.current_schedule[1].num_steps:\n            self.current_schedule_idx += 1\n            self.current_schedule = self.schedules[self.current_schedule_idx]\n            self.current_schedule_step_count = 0\n\n        self.current_value = self.current_schedule[0].current_value\n        self.current_schedule_step_count += 1\n\n\nclass ExponentialSchedule(Schedule):\n    """"""\n    A simple exponential schedule which decreases or increases over time from an initial to a final value\n    """"""\n    def __init__(self, initial_value: float, final_value: float, decay_coefficient: float):\n        """"""\n        :param initial_value: the initial value\n        :param final_value: the final value\n        :param decay_coefficient: the exponential decay coefficient\n        """"""\n        super().__init__(initial_value)\n        self.initial_value = initial_value\n        self.final_value = final_value\n        self.decay_coefficient = decay_coefficient\n        self.current_step = 0\n        self.current_value = self.initial_value\n        if decay_coefficient < 1 and final_value > initial_value:\n            raise ValueError(""The final value should be lower than the initial value when the decay coefficient < 1"")\n        if decay_coefficient > 1 and initial_value > final_value:\n            raise ValueError(""The final value should be higher than the initial value when the decay coefficient > 1"")\n\n    def step(self):\n        self.current_value *= self.decay_coefficient\n\n        # decreasing schedule\n        if self.final_value < self.initial_value:\n            self.current_value = np.clip(self.current_value, self.final_value, self.initial_value)\n        # increasing schedule\n        if self.final_value > self.initial_value:\n            self.current_value = np.clip(self.current_value, self.initial_value, self.final_value)\n\n        self.current_step += 1\n'"
rl_coach/spaces.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport random\nfrom enum import Enum\nfrom itertools import product\nfrom typing import Union, List, Dict, Tuple, Callable\n\nimport numpy as np\nimport scipy\nimport scipy.spatial\n\nfrom rl_coach.core_types import ActionType, ActionInfo\nfrom rl_coach.utils import eps\n\n\nclass Space(object):\n    """"""\n    A space defines a set of valid values\n    """"""\n    def __init__(self, shape: Union[int, tuple, list, np.ndarray], low: Union[None, int, float, np.ndarray]=-np.inf,\n                 high: Union[None, int, float, np.ndarray]=np.inf):\n        """"""\n        :param shape: the shape of the space\n        :param low: the lowest values possible in the space. can be an array defining the lowest values per point,\n                    or a single value defining the general lowest values\n        :param high: the highest values possible in the space. can be an array defining the highest values per point,\n                    or a single value defining the general highest values\n        """"""\n\n        # the number of dimensions is the number of axes in the shape. it will be set in the shape setter\n        self.num_dimensions = 0\n\n        # the number of elements is the number of possible actions if the action space was discrete.\n        # it will be set in the shape setter\n        self.num_elements = 0\n\n        self._low = self._high = None\n        self._shape = self.shape = shape\n        self._low = self.low = low\n        self._high = self.high = high\n\n        # we allow zero sized spaces which means that the space is empty. this is useful for environments with no\n        # measurements for example.\n        if type(shape) == int and shape < 0:\n            raise ValueError(""The shape of the space must be a non-negative number"")\n\n    @property\n    def shape(self):\n        return self._shape\n\n    @shape.setter\n    def shape(self, val: Union[int, tuple, list, np.ndarray]):\n        # convert the shape to an np.ndarray\n        self._shape = val\n        if type(self._shape) == int:\n            self._shape = np.array([self._shape])\n        if type(self._shape) == tuple or type(self._shape) == list:\n            self._shape = np.array(self._shape)\n\n        # the shape is now an np.ndarray\n        self.num_dimensions = len(self._shape)\n        self.num_elements = int(np.prod(self._shape))\n\n    @property\n    def low(self):\n        if hasattr(self, \'_low\'):\n            return self._low\n        else:\n            return None\n\n    @low.setter\n    def low(self, val: Union[None, int, float, np.ndarray]):\n        if type(val) == np.ndarray and type(self.shape) == np.ndarray and np.all(val.shape != self.shape):\n            raise ValueError(""The low values shape don\'t match the shape of the space"")\n        elif self.high is not None and not np.all(self.high >= val):\n            raise ValueError(""At least one of the axes-parallel lines defining the space has high values which ""\n                             ""are lower than the given low values"")\n        else:\n            self._low = val\n            # we allow using a number to define the low values, but we immediately convert it to an array which defines\n            # the low values for all the space dimensions in order to expose a consistent value type\n            if type(self._low) == int or type(self._low) == float:\n                self._low = np.ones(self.shape)*self._low\n\n    @property\n    def high(self):\n        if hasattr(self, \'_high\'):\n            return self._high\n        else:\n            return None\n\n    @high.setter\n    def high(self, val: Union[None, int, float, np.ndarray]):\n        if type(val) == np.ndarray and type(self.shape) == np.ndarray and np.all(val.shape != self.shape):\n            raise ValueError(""The high values shape don\'t match the shape of the space"")\n        elif self.low is not None and not np.all(self.low <= val):\n            raise ValueError(""At least one of the axes-parallel lines defining the space has low values which ""\n                             ""are higher than the given high values"")\n        else:\n            self._high = val\n            # we allow using a number to define the high values, but we immediately convert it to an array which defines\n            # the high values for all the space dimensions in order to expose a consistent value type\n            if type(self._high) == int or type(self._high) == float:\n                self._high = np.ones(self.shape)*self._high\n\n    def contains(self, val: Union[int, float, np.ndarray]) -> bool:\n        """"""\n        Checks if value is contained by this space. The shape must match and\n        all of the values must be within the low and high bounds.\n\n        :param val: a value to check\n        :return: True / False depending on if the val matches the space definition\n        """"""\n        if (type(val) == int or type(val) == float) and not np.all(self.shape == np.ones(1)):\n            return False\n        if type(val) == np.ndarray and not np.all(val.shape == self.shape):\n            return False\n        if (self.low is not None and not np.all(val >= self.low)) \\\n                or (self.high is not None and not np.all(val <= self.high)):\n            # TODO: check the performance overhead this causes\n            return False\n        return True\n\n    def is_valid_index(self, index: np.ndarray) -> bool:\n        """"""\n        Checks if a given multidimensional index is within the bounds of the shape of the space\n\n        :param index: a multidimensional index\n        :return: True if the index is within the shape of the space. False otherwise\n        """"""\n        if len(index) != self.num_dimensions:\n            return False\n        if np.any(index < np.zeros(self.num_dimensions)) or np.any(index >= self.shape):\n            return False\n        return True\n\n    def sample(self) -> np.ndarray:\n        """"""\n        Sample the defined space, either uniformly, if space bounds are defined, or Normal distributed if no\n        bounds are defined\n\n        :return: A numpy array sampled from the space\n        """"""\n        # if there are infinite bounds, we sample using gaussian noise with mean 0 and std 1\n        if np.any(self.low == -np.inf) or np.any(self.high == np.inf):\n            return np.random.normal(0, 1, self.shape)\n        else:\n            return np.random.uniform(self.low, self.high, self.shape)\n\n    def val_matches_space_definition(self, val: Union[int, float, np.ndarray]) -> bool:\n        screen.warning(\n            ""Space.val_matches_space_definition will be deprecated soon. Use ""\n            ""contains instead.""\n        )\n        return self.contains(val)\n\n    def is_point_in_space_shape(self, point: np.ndarray) -> bool:\n        screen.warning(\n            ""Space.is_point_in_space_shape will be deprecated soon. Use ""\n            ""is_valid_index instead.""\n        )\n        return self.is_valid_index(point)\n\n\nclass RewardSpace(Space):\n    def __init__(self, shape: Union[int, np.ndarray], low: Union[None, int, float, np.ndarray]=-np.inf,\n                 high: Union[None, int, float, np.ndarray]=np.inf,\n                 reward_success_threshold: Union[None, int, float]=None):\n        super().__init__(shape, low, high)\n        self.reward_success_threshold = reward_success_threshold\n\n\n""""""\nObservation Spaces\n""""""\n\n\nclass ObservationSpace(Space):\n    def __init__(self, shape: Union[int, np.ndarray], low: Union[None, int, float, np.ndarray]=-np.inf,\n                 high: Union[None, int, float, np.ndarray]=np.inf):\n        super().__init__(shape, low, high)\n\n\nclass VectorObservationSpace(ObservationSpace):\n    """"""\n    An observation space which is defined as a vector of elements. This can be particularly useful for environments\n    which return measurements, such as in robotic environments.\n    """"""\n    def __init__(self, shape: int, low: Union[None, int, float, np.ndarray]=-np.inf,\n                 high: Union[None, int, float, np.ndarray]=np.inf, measurements_names: List[str]=None):\n        if measurements_names is None:\n            measurements_names = []\n        if len(measurements_names) > shape:\n            raise ValueError(""measurement_names size {} is larger than shape {}."".format(\n                len(measurements_names), shape))\n\n        self.measurements_names = measurements_names\n        super().__init__(shape, low, high)\n\n\nclass TensorObservationSpace(ObservationSpace):\n    """"""\n    An observation space which defines observations with arbitrary shape. This can be particularly useful for\n    environments with non image input.\n    """"""\n    def __init__(self, shape: np.ndarray, low: -np.inf,\n                 high: np.inf):\n        super().__init__(shape, low, high)\n\n\nclass PlanarMapsObservationSpace(ObservationSpace):\n    """"""\n    An observation space which defines a stack of 2D observations. For example, an environment which returns\n    a stack of segmentation maps like in Starcraft.\n    """"""\n    def __init__(self, shape: Union[np.ndarray], low: int, high: int, channels_axis: int=-1):\n        super().__init__(shape, low, high)\n        self.channels_axis = channels_axis\n\n        if not 2 <= len(shape) <= 3:\n            raise ValueError(""Planar maps observations must have 3 dimensions - a channels dimension and 2 maps ""\n                             ""dimensions, not {}"".format(len(shape)))\n        if len(shape) == 2:\n            self.channels = 1\n        else:\n            self.channels = shape[channels_axis]\n\n\nclass ImageObservationSpace(PlanarMapsObservationSpace):\n    """"""\n    An observation space which is a private case of the PlanarMapsObservationSpace, where the stack of 2D observations\n    represent a RGB image, or a grayscale image.\n    """"""\n    def __init__(self, shape: Union[np.ndarray], high: int, channels_axis: int=-1):\n        # TODO: consider allowing arbitrary low values for images\n        super().__init__(shape, 0, high, channels_axis)\n        self.has_colors = self.channels == 3\n        if not self.channels == 3 and not self.channels == 1:\n            raise ValueError(""Image observations must have 1 or 3 channels, not {}"".format(self.channels))\n\n\n# TODO: mixed observation spaces (image + measurements, image + segmentation + depth map, etc.)\nclass StateSpace(object):\n    def __init__(self, sub_spaces: Dict[str, Space]):\n        self.sub_spaces = sub_spaces\n\n    def __getitem__(self, item):\n        return self.sub_spaces[item]\n\n    def __setitem__(self, key, value):\n        self.sub_spaces[key] = value\n\n\n""""""\nAction Spaces\n""""""\n\n\nclass ActionSpace(Space):\n    def __init__(self, shape: Union[int, np.ndarray], low: Union[None, int, float, np.ndarray]=-np.inf,\n                 high: Union[None, int, float, np.ndarray]=np.inf, descriptions: Union[None, List, Dict]=None,\n                 default_action: ActionType=None):\n        super().__init__(shape, low, high)\n        # we allow a mismatch between the number of descriptions and the number of actions.\n        # in this case the descriptions for the actions that were not given will be the action index\n        if descriptions is not None:\n            self.descriptions = descriptions\n        else:\n            self.descriptions = {}\n        self.default_action = default_action\n\n    @property\n    def actions(self) -> List[ActionType]:\n        raise NotImplementedError(""The action space does not have an explicit actions list"")\n\n    def sample_with_info(self) -> ActionInfo:\n        """"""\n        Get a random action with additional ""fake"" info\n\n        :return: An action info instance\n        """"""\n        return ActionInfo(self.sample())\n\n    def clip_action_to_space(self, action: ActionType) -> ActionType:\n        """"""\n        Given an action, clip its values to fit to the action space ranges\n\n        :param action: a given action\n        :return: the clipped action\n        """"""\n        return action\n\n    def get_description(self, action: np.ndarray) -> str:\n        raise NotImplementedError("""")\n\n    def __str__(self):\n        return ""{}: shape = {}, low = {}, high = {}"".format(self.__class__.__name__, self.shape, self.low, self.high)\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass AttentionActionSpace(ActionSpace):\n    """"""\n    A box selection continuous action space, meaning that the actions are defined as selecting a multidimensional box\n    from a given range.\n    The actions will be in the form:\n    [[low_x, low_y, ...], [high_x, high_y, ...]]\n    """"""\n    def __init__(self, shape: int, low: Union[None, int, float, np.ndarray]=-np.inf,\n                 high: Union[None, int, float, np.ndarray]=np.inf, descriptions: Union[None, List, Dict]=None,\n                 default_action: np.ndarray = None, forced_attention_size: Union[None, int, float, np.ndarray]=None):\n        super().__init__(shape, low, high, descriptions)\n\n        self.forced_attention_size = forced_attention_size\n        if isinstance(self.forced_attention_size, int) or isinstance(self.forced_attention_size, float):\n            self.forced_attention_size = np.ones(self.shape) * self.forced_attention_size\n\n        if self.forced_attention_size is not None and np.all(self.forced_attention_size > (self.high - self.low)):\n            raise ValueError(""The forced attention size is larger than the action space"")\n\n        # default action\n        if default_action is None:\n            if self.forced_attention_size is not None:\n                self.default_action = [self.low*np.ones(self.shape),\n                                       (self.low+self.forced_attention_size)*np.ones(self.shape)]\n            else:\n                self.default_action = [self.low*np.ones(self.shape), self.high*np.ones(self.shape)]\n        else:\n            self.default_action = default_action\n\n    def sample(self) -> List:\n        if self.forced_attention_size is not None:\n            sampled_low = np.random.uniform(self.low, self.high-self.forced_attention_size, self.shape)\n            sampled_high = sampled_low + self.forced_attention_size\n        else:\n            sampled_low = np.random.uniform(self.low, self.high, self.shape)\n            sampled_high = np.random.uniform(sampled_low, self.high, self.shape)\n        return [sampled_low, sampled_high]\n\n    def clip_action_to_space(self, action: ActionType) -> ActionType:\n        action = [np.clip(action[0], self.low, self.high), np.clip(action[1], self.low, self.high)]\n        return action\n\n\nclass BoxActionSpace(ActionSpace):\n    """"""\n    A multidimensional bounded or unbounded continuous action space\n    """"""\n    def __init__(self, shape: Union[int, np.ndarray], low: Union[None, int, float, np.ndarray]=-np.inf,\n                 high: Union[None, int, float, np.ndarray]=np.inf, descriptions: Union[None, List, Dict]=None,\n                 default_action: np.ndarray=None):\n        super().__init__(shape, low, high, descriptions)\n        self.max_abs_range = np.maximum(np.abs(self.low), np.abs(self.high))\n\n        # default action\n        if default_action is None:\n            if np.any(np.isinf(self.low)) or np.any(np.isinf(self.high)):\n                self.default_action = np.zeros(shape)\n            else:\n                self.default_action = self.low + (self.high - self.low) / 2\n        else:\n            self.default_action = default_action\n\n    def clip_action_to_space(self, action: ActionType) -> ActionType:\n        action = np.clip(action, self.low, self.high)\n        return action\n\n\nclass DiscreteActionSpace(ActionSpace):\n    """"""\n    A discrete action space with action indices as actions\n    """"""\n    def __init__(self, num_actions: int, descriptions: Union[None, List, Dict]=None, default_action: np.ndarray=None,\n                 filtered_action_space=None):\n        super().__init__(1, low=0, high=num_actions-1, descriptions=descriptions)\n        # the number of actions is mapped to high\n\n        # default action\n        if default_action is None:\n            self.default_action = 0\n        else:\n            self.default_action = default_action\n\n        if filtered_action_space is not None:\n            self.filtered_action_space = filtered_action_space\n\n    @property\n    def actions(self) -> List[ActionType]:\n        return list(range(0, int(self.high[0]) + 1))\n\n    def sample(self) -> int:\n        return np.random.choice(self.actions)\n\n    def sample_with_info(self) -> ActionInfo:\n        return ActionInfo(self.sample(),\n                          all_action_probabilities=np.full(len(self.actions), 1. / (self.high[0] - self.low[0] + 1)))\n\n    def get_description(self, action: int) -> str:\n        if type(self.descriptions) == list and 0 <= action < len(self.descriptions):\n            return self.descriptions[action]\n        elif type(self.descriptions) == dict and action in self.descriptions.keys():\n            return self.descriptions[action]\n        elif 0 <= action < self.shape:\n            return str(action)\n        else:\n            raise ValueError(""The given action is outside of the action space"")\n\n\nclass MultiSelectActionSpace(ActionSpace):\n    """"""\n    A discrete action space where multiple actions can be selected at once. The actions are encoded as multi-hot vectors\n    """"""\n    def __init__(self, size: int, max_simultaneous_selected_actions: int=1, descriptions: Union[None, List, Dict]=None,\n                 default_action: np.ndarray=None, allow_no_action_to_be_selected=True):\n        super().__init__(size, low=None, high=None, descriptions=descriptions)\n        self.max_simultaneous_selected_actions = max_simultaneous_selected_actions\n\n        if max_simultaneous_selected_actions > size:\n            raise ValueError(""The maximum simultaneous selected actions can\'t be larger the max number of actions"")\n\n        # create all combinations of actions as a list of actions\n        I = [np.eye(size)]*self.max_simultaneous_selected_actions\n        self._actions = []\n        if allow_no_action_to_be_selected:\n            self._actions.append(np.zeros(size))\n        self._actions.extend(list(np.unique([np.clip(np.sum(t, axis=0), 0, 1) for t in product(*I)], axis=0)))\n\n        # default action\n        if default_action is None:\n            self.default_action = self._actions[0]\n        else:\n            self.default_action = default_action\n\n    @property\n    def actions(self) -> List[ActionType]:\n        return self._actions\n\n    def sample(self) -> np.ndarray:\n        # samples a multi-hot vector\n        return random.choice(self.actions)\n\n    def sample_with_info(self) -> ActionInfo:\n        return ActionInfo(self.sample(), all_action_probabilities=np.full(len(self.actions), 1. / len(self.actions)))\n\n    def get_description(self, action: np.ndarray) -> str:\n        if np.sum(len(np.where(action == 0)[0])) + np.sum(len(np.where(action == 1)[0])) != self.shape or \\\n                        np.sum(len(np.where(action == 1)[0])) > self.max_simultaneous_selected_actions:\n            raise ValueError(""The given action is not in the action space"")\n        selected_actions = np.where(action == 1)[0]\n        description = [self.descriptions[a] for a in selected_actions]\n        if len(description) == 0:\n            description = [\'no-op\']\n        return \' + \'.join(description)\n\n\nclass CompoundActionSpace(ActionSpace):\n    """"""\n    An action space which consists of multiple sub-action spaces.\n    For example, in Starcraft the agent should choose an action identifier from ~550 options (Discrete(550)),\n    but it also needs to choose 13 different arguments for the selected action identifier, where each argument is\n    by itself an action space. In Starcraft, the arguments are Discrete action spaces as well, but this is not mandatory.\n    """"""\n    def __init__(self, sub_spaces: List[ActionSpace]):\n        super().__init__(0)\n        self.sub_action_spaces = sub_spaces\n        # TODO: define the shape, low and high value in a better way\n\n    @property\n    def actions(self) -> List[ActionType]:\n        return [action_space.actions for action_space in self.sub_action_spaces]\n\n    def sample(self) -> ActionType:\n        return [action_space.sample() for action_space in self.sub_action_spaces]\n\n    def clip_action_to_space(self, actions: List[ActionType]) -> ActionType:\n        if not isinstance(actions, list) or len(actions) != len(self.sub_action_spaces):\n            raise ValueError(""The actions to be clipped must be a list with the same number of sub-actions as ""\n                             ""defined in the compound action space."")\n        for idx in range(len(self.sub_action_spaces)):\n            actions[idx] = self.sub_action_spaces[idx].clip_action_to_space(actions[idx])\n        return actions\n\n    def get_description(self, actions: np.ndarray) -> str:\n        description = [action_space.get_description(action) for action_space, action in zip(self.sub_action_spaces, actions)]\n        return \' + \'.join(description)\n\n\n""""""\nGoals\n""""""\n\n\nclass GoalToRewardConversion(object):\n    def __init__(self, goal_reaching_reward: float=0):\n        self.goal_reaching_reward = goal_reaching_reward\n\n    def convert_distance_to_reward(self, distance: Union[float, np.ndarray]) -> Tuple[float, bool]:\n        """"""\n        Given a distance from the goal, return a reward and a flag representing if the goal was reached\n\n        :param distance: the distance from the goal\n        :return:\n        """"""\n        raise NotImplementedError("""")\n\n\nclass ReachingGoal(GoalToRewardConversion):\n    """"""\n    get a reward if the goal was reached and 0 otherwise\n    """"""\n    def __init__(self, distance_from_goal_threshold: Union[float, np.ndarray], goal_reaching_reward: float=0,\n                 default_reward: float=-1):\n        """"""\n        :param distance_from_goal_threshold: consider getting to this distance from the goal the same as getting\n                                             to the goal\n        :param goal_reaching_reward: the reward the agent will get when reaching the goal\n        :param default_reward: the reward the agent will get until it reaches the goal\n        """"""\n        super().__init__(goal_reaching_reward)\n        self.distance_from_goal_threshold = distance_from_goal_threshold\n        self.default_reward = default_reward\n\n    def convert_distance_to_reward(self, distance: Union[float, np.ndarray]) -> Tuple[float, bool]:\n        if np.all(distance <= self.distance_from_goal_threshold):\n            return self.goal_reaching_reward, True\n        else:\n            return self.default_reward, False\n\n\nclass InverseDistanceFromGoal(GoalToRewardConversion):\n    """"""\n    get a reward inversely proportional to the distance from the goal\n    """"""\n    def __init__(self, distance_from_goal_threshold: Union[float, np.ndarray], max_reward: float=1):\n        """"""\n        :param distance_from_goal_threshold: consider getting to this distance from the goal the same as getting\n                                             to the goal\n        :param max_reward: the max reward the agent can get\n        """"""\n        super().__init__(goal_reaching_reward=max_reward)\n        self.distance_from_goal_threshold = distance_from_goal_threshold\n        self.max_reward = max_reward\n\n    def convert_distance_to_reward(self, distance: Union[float, np.ndarray]) -> Tuple[float, bool]:\n        return min(self.max_reward, 1 / (distance + eps)), distance <= self.distance_from_goal_threshold\n\n\nclass GoalsSpace(VectorObservationSpace, ActionSpace):\n    """"""\n    A multidimensional space with a goal type definition. It also behaves as an action space, so that hierarchical\n    agents can use it as an output action space.\n    The class acts as a wrapper to the target space. So after setting the target space, all the values of the class\n    will match the values of the target space (the shape, low, high, etc.)\n    """"""\n    class DistanceMetric(Enum):\n        Euclidean = 0\n        Cosine = 1\n        Manhattan = 2\n\n    def __init__(self, goal_name: str, reward_type: GoalToRewardConversion,\n                 distance_metric: Union[DistanceMetric, Callable]):\n        """"""\n        :param goal_name: the name of the observation space to use as the achieved goal.\n        :param reward_type: the reward type to use for converting distances from goal to rewards\n        :param distance_metric: the distance metric to use. could be either one of the distances in the\n                                DistanceMetric enum, or a custom function that gets two vectors as input and\n                                returns the distance between them\n        """"""\n        super().__init__(0)\n        self.goal_name = goal_name\n        self.distance_metric = distance_metric\n        self.reward_type = reward_type\n        self.target_space = None\n        self.max_abs_range = None\n\n    def set_target_space(self, target_space: Space) -> None:\n        self.target_space = target_space\n        super().__init__(self.target_space.shape, self.target_space.low, self.target_space.high)\n        self.max_abs_range = np.maximum(np.abs(self.low), np.abs(self.high))\n\n    def goal_from_state(self, state: Dict):\n        """"""\n        Given a state, extract an observation according to the goal_name\n\n        :param state: a dictionary of observations\n        :return: the observation corresponding to the goal_name\n        """"""\n        return state[self.goal_name]\n\n    def distance_from_goal(self, goal: np.ndarray, state: dict) -> float:\n        """"""\n        Given a state, check its distance from the goal\n\n        :param goal: a numpy array representing the goal\n        :param state: a dict representing the state\n        :return: the distance from the goal\n        """"""\n        state_value = self.goal_from_state(state)\n\n        # calculate distance\n        if self.distance_metric == self.DistanceMetric.Cosine:\n            dist = scipy.spatial.distance.cosine(goal, state_value)\n        elif self.distance_metric == self.DistanceMetric.Euclidean:\n            dist = scipy.spatial.distance.euclidean(goal, state_value)\n        elif self.distance_metric == self.DistanceMetric.Manhattan:\n            dist = scipy.spatial.distance.cityblock(goal, state_value)\n        elif callable(self.distance_metric):\n            dist = self.distance_metric(goal, state_value)\n        else:\n            raise ValueError(""The given distance metric for the goal is not valid."")\n\n        return dist\n\n    def get_reward_for_goal_and_state(self, goal: np.ndarray, state: dict) -> Tuple[float, bool]:\n        """"""\n        Given a state, check if the goal was reached and return a reward accordingly\n\n        :param goal: a numpy array representing the goal\n        :param state: a dict representing the state\n        :return: the reward for the current goal and state pair and a boolean representing if the goal was reached\n        """"""\n        dist = self.distance_from_goal(goal, state)\n        return self.reward_type.convert_distance_to_reward(dist)\n\n\nclass AgentSelection(DiscreteActionSpace):\n    """"""\n    An discrete action space which is bounded by the number of agents to select from\n    """"""\n    def __init__(self, num_agents: int):\n        super().__init__(num_agents)\n\n\nclass SpacesDefinition(object):\n    """"""\n    A container class that allows passing the definitions of all the spaces at once\n    """"""\n    def __init__(self,\n                 state: StateSpace,\n                 goal: Union[ObservationSpace, None],\n                 action: ActionSpace,\n                 reward: RewardSpace):\n        self.state = state\n        self.goal = goal\n        self.action = action\n        self.reward = reward\n'"
rl_coach/training_worker.py,0,"b' #\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom rl_coach.base_parameters import DistributedCoachSynchronizationType\nfrom rl_coach.logger import screen\n\n\ndef data_store_ckpt_load(data_store):\n    if data_store:\n        data_store.load_from_store()\n\n\ndef training_worker(graph_manager, task_parameters, data_store, is_multi_node_test):\n    """"""\n    restore a checkpoint then perform rollouts using the restored model\n\n    :param graph_manager: An instance of the graph manager\n    :param data_store: An instance of DataStore which can be used to communicate policies to roll out workers\n    :param task_parameters: An instance of task parameters\n    :param is_multi_node_test: If this is a multi node test insted of a normal run.\n    """"""\n    # Load checkpoint if provided\n    if task_parameters.checkpoint_restore_path:\n        data_store_ckpt_load(data_store)\n        \n        # initialize graph\n        graph_manager.create_graph(task_parameters)\n        \n    else:\n        # initialize graph\n        graph_manager.create_graph(task_parameters)\n\n        # save randomly initialized graph\n        data_store.save_policy(graph_manager)\n\n\n    # training loop\n    steps = 0\n\n    # evaluation offset\n    eval_offset = 1\n\n    graph_manager.setup_memory_backend()\n    graph_manager.signal_ready()\n\n    while steps < graph_manager.improve_steps.num_steps:\n\n        if is_multi_node_test and graph_manager.get_current_episodes_count() > graph_manager.preset_validation_params.max_episodes_to_achieve_reward:\n            # Test failed as it has not reached the required success rate\n            graph_manager.flush_finished()\n            screen.error(""Could not reach required success by {} episodes."".format(graph_manager.preset_validation_params.max_episodes_to_achieve_reward), crash=True)\n\n        graph_manager.fetch_from_worker(graph_manager.agent_params.algorithm.num_consecutive_playing_steps)\n\n        if graph_manager.should_train():\n            steps += 1\n\n            graph_manager.train()\n\n            if steps * graph_manager.agent_params.algorithm.num_consecutive_playing_steps.num_steps > graph_manager.steps_between_evaluation_periods.num_steps * eval_offset:\n                eval_offset += 1\n                if graph_manager.evaluate(graph_manager.evaluation_steps):\n                    break\n\n            if graph_manager.agent_params.algorithm.distributed_coach_synchronization_type == DistributedCoachSynchronizationType.SYNC:\n                data_store.save_policy(graph_manager)\n            else:\n                # NOTE: this implementation conflated occasionally saving checkpoints for later use\n                # in production with checkpoints saved for communication to rollout workers.\n                # TODO: this should be implemented with a new parameter: distributed_coach_synchronization_frequency or similar\n                # graph_manager.occasionally_save_checkpoint()\n                raise NotImplementedError()\n'"
rl_coach/utils.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport importlib\nimport importlib.util\nimport inspect\nimport os\nimport re\nimport signal\nimport sys\nimport threading\nimport time\nimport traceback\nfrom multiprocessing import Manager\nfrom subprocess import Popen\nfrom typing import List, Tuple, Union\n\nimport atexit\nimport numpy as np\n\nfrom rl_coach.logger import screen\n\nkilled_processes = []\n\neps = np.finfo(np.float32).eps\n\n\ndef lower_under_to_upper(s):\n    s = s.replace(\'_\', \' \')\n    s = s.title()\n    return s.replace(\' \', \'\')\n\n\ndef get_base_dir():\n    return os.path.dirname(os.path.realpath(__file__))\n\n\ndef list_all_presets():\n    presets_path = os.path.join(get_base_dir(), \'presets\')\n    return [f.split(\'.\')[0] for f in os.listdir(presets_path) if f.endswith(\'.py\') and f != \'__init__.py\']\n\n\ndef list_all_classes_in_module(module):\n    return [k for k, v in inspect.getmembers(module, inspect.isclass) if v.__module__ == module.__name__]\n\n\ndef parse_bool(value):\n    return {\'true\': True, \'false\': False}.get(value.strip().lower(), value)\n\n\ndef convert_to_ascii(data):\n    import collections\n    if isinstance(data, basestring):\n        return parse_bool(str(data))\n    elif isinstance(data, collections.Mapping):\n        return dict(map(convert_to_ascii, data.iteritems()))\n    elif isinstance(data, collections.Iterable):\n        return type(data)(map(convert_to_ascii, data))\n    else:\n        return data\n\n\ndef break_file_path(path):\n    base = os.path.splitext(os.path.basename(path))[0]\n    extension = os.path.splitext(os.path.basename(path))[1]\n    dir = os.path.dirname(path)\n    return dir, base, extension\n\n\ndef is_empty(str):\n    return str == 0 or len(str.replace(""\'"", """").replace(""\\"""", """")) == 0\n\n\ndef path_is_valid_dir(path):\n    return os.path.isdir(path)\n\n\ndef remove_suffix(name, suffix_start):\n    for s in suffix_start:\n        split = name.find(s)\n        if split != -1:\n            name = name[:split]\n            return name\n\n\ndef parse_int(value):\n    import ast\n    try:\n        int_value = int(value)\n        return int_value if int_value == value else value\n    except:\n        pass\n\n    try:\n        return ast.literal_eval(value)\n    except:\n        return value\n\n\ndef set_gpu(gpu_id):\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(gpu_id)\n\n\ndef set_cpu():\n    set_gpu("""")\n\n\n# dictionary to class\nclass DictToClass(object):\n    def __init__(self, data):\n        for name, value in data.iteritems():\n            setattr(self, name, self._wrap(value))\n\n    def _wrap(self, value):\n        if isinstance(value, (tuple, list, set, frozenset)):\n            return type(value)([self._wrap(v) for v in value])\n        else:\n            return DictToClass(value) if isinstance(value, dict) else value\n\n\n# class to dictionary\ndef ClassToDict(x):\n    # return dict((key, getattr(x, key)) for key in dir(x) if key not in dir(x.__class__))\n    dictionary = x.__dict__\n    return {key: dictionary[key] for key in dictionary.keys() if not key.startswith(\'__\')}\n\n\ndef cmd_line_run(result, run_cmd, id=-1):\n    p = Popen(run_cmd, shell=True, executable=""bash"")\n    while result[0] is None or result[0] == [None]:\n        if id in killed_processes:\n            p.kill()\n        result[0] = p.poll()\n\n\ndef threaded_cmd_line_run(run_cmd, id=-1):\n    runThread = []\n    result = [[None]]\n    try:\n        params = (result, run_cmd, id)\n        runThread = threading.Thread(name=\'runThread\', target=cmd_line_run, args=params)\n        runThread.daemon = True\n        runThread.start()\n    except:\n        runThread.join()\n    return result\n\n\nclass Signal(object):\n    """"""\n    Stores a stream of values and provides methods like get_mean and get_max\n    which returns the statistics about accumulated values.\n    """"""\n    def __init__(self, name):\n        self.name = name\n        self.sample_count = 0\n        self.values = []\n\n    def reset(self):\n        self.sample_count = 0\n        self.values = []\n\n    def add_sample(self, sample):\n        """"""\n        :param sample: either a single value or an array of values\n        """"""\n        self.values.append(sample)\n\n    def _get_values(self):\n        if type(self.values[0]) == np.ndarray:\n            return np.concatenate(self.values)\n        else:\n            return self.values\n\n    def get_last_value(self):\n        if len(self.values) == 0:\n            return np.nan\n        else:\n            return self._get_values()[-1]\n\n    def get_mean(self):\n        if len(self.values) == 0:\n            return \'\'\n        return np.mean(self._get_values())\n\n    def get_max(self):\n        if len(self.values) == 0:\n            return \'\'\n        return np.max(self._get_values())\n\n    def get_min(self):\n        if len(self.values) == 0:\n            return \'\'\n        return np.min(self._get_values())\n\n    def get_stdev(self):\n        if len(self.values) == 0:\n            return \'\'\n        return np.std(self._get_values())\n\n\ndef force_list(var):\n    if isinstance(var, list):\n        return var\n    else:\n        return [var]\n\n\ndef squeeze_list(var):\n    if type(var) == list and len(var) == 1:\n        return var[0]\n    else:\n        return var\n\n\ndef get_open_port():\n    import socket\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind(("""", 0))\n    s.listen(1)\n    port = s.getsockname()[1]\n    s.close()\n    return port\n\n\nclass timeout:\n    def __init__(self, seconds=1, error_message=\'Timeout\'):\n        self.seconds = seconds\n        self.error_message = error_message\n\n    def _handle_timeout(self, signum, frame):\n        raise TimeoutError(self.error_message)\n\n    def __enter__(self):\n        signal.signal(signal.SIGALRM, self._handle_timeout)\n        signal.alarm(self.seconds)\n\n    def __exit__(self, type, value, traceback):\n        signal.alarm(0)\n\n\ndef switch_axes_order(observation, from_type=\'channels_first\', to_type=\'channels_last\'):\n    """"""\n    transpose an observation axes from channels_first to channels_last or vice versa\n    :param observation: a numpy array\n    :param from_type: can be \'channels_first\' or \'channels_last\'\n    :param to_type: can be \'channels_first\' or \'channels_last\'\n    :return: a new observation with the requested axes order\n    """"""\n    if from_type == to_type or len(observation.shape) == 1:\n        return observation\n    assert 2 <= len(observation.shape) <= 3, \'num axes of an observation must be 2 for a vector or 3 for an image\'\n    assert type(observation) == np.ndarray, \'observation must be a numpy array\'\n    if len(observation.shape) == 3:\n        if from_type == \'channels_first\' and to_type == \'channels_last\':\n            return np.transpose(observation, (1, 2, 0))\n        elif from_type == \'channels_last\' and to_type == \'channels_first\':\n            return np.transpose(observation, (2, 0, 1))\n    else:\n        return np.transpose(observation, (1, 0))\n\n\ndef stack_observation(curr_stack, observation, stack_size):\n    """"""\n    Adds a new observation to an existing stack of observations from previous time-steps.\n    :param curr_stack: The current observations stack.\n    :param observation: The new observation\n    :param stack_size: The required stack size\n    :return: The updated observation stack\n    """"""\n\n    if curr_stack == []:\n        # starting an episode\n        curr_stack = np.vstack(np.expand_dims([observation] * stack_size, 0))\n        curr_stack = switch_axes_order(curr_stack, from_type=\'channels_first\', to_type=\'channels_last\')\n    else:\n        curr_stack = np.append(curr_stack, np.expand_dims(np.squeeze(observation), axis=-1), axis=-1)\n        curr_stack = np.delete(curr_stack, 0, -1)\n\n    return curr_stack\n\n\ndef call_method_for_all(instances: List, method: str, args=[], kwargs={}) -> List:\n    """"""\n    Calls the same function for all the class instances in the group\n    :param instances: a list of class instances to apply the method on\n    :param method: the name of the function to be called\n    :param args: the positional parameters of the method\n    :param kwargs: the named parameters of the method\n    :return: a list of the returns values for all the instances\n    """"""\n    result = []\n    if not isinstance(args, list):\n        args = [args]\n    sub_methods = method.split(\'.\')  # we allow calling an internal method such as ""as_level_manager.train""\n    for instance in instances:\n        sub_instance = instance\n        for sub_method in sub_methods:\n            if not hasattr(sub_instance, sub_method):\n                raise ValueError(""The requested instance method {} does not exist for {}""\n                                 .format(sub_method, \'.\'.join([str(instance.__class__.__name__)] + sub_methods)))\n            sub_instance = getattr(sub_instance, sub_method)\n        result.append(sub_instance(*args, **kwargs))\n    return result\n\n\ndef set_member_values_for_all(instances: List, member: str, val) -> None:\n    """"""\n    Calls the same function for all the class instances in the group\n    :param instances: a list of class instances to apply the method on\n    :param member: the name of the member to be changed\n    :param val: the new value to assign\n    :return: None\n    """"""\n    for instance in instances:\n        if not hasattr(instance, member):\n            raise ValueError(""The requested instance member does not exist"")\n        setattr(instance, member, val)\n\n\ndef short_dynamic_import(module_path_and_attribute: str, ignore_module_case: bool=False):\n    """"""\n    Import by ""path:attribute""\n    :param module_path_and_attribute: a path to a python file (using dots to separate dirs), followed by a "":"" and\n                                      an attribute name to import from the path\n    :return: the requested attribute\n    """"""\n    if \'/\' in module_path_and_attribute:\n        """"""\n        Imports a class from a module using the full path of the module. The path should be given as:\n        <full absolute module path with / including .py>:<class name to import>\n        And this will be the same as doing ""from <full absolute module path> import <class name to import>""\n        """"""\n        return dynamic_import_from_full_path(*module_path_and_attribute.split(\':\'),\n                                             ignore_module_case=ignore_module_case)\n    else:\n        """"""\n        Imports a class from a module using the relative path of the module. The path should be given as:\n        <full absolute module path with . and not including .py>:<class name to import>\n        And this will be the same as doing ""from <full relative module path> import <class name to import>""\n        """"""\n        return dynamic_import(*module_path_and_attribute.split(\':\'),\n                              ignore_module_case=ignore_module_case)\n\n\ndef dynamic_import(module_path: str, class_name: str, ignore_module_case: bool=False):\n    if ignore_module_case:\n        module_name = module_path.split(""."")[-1]\n        available_modules = os.listdir(os.path.dirname(module_path.replace(\'.\', \'/\')))\n        for module in available_modules:\n            curr_module_ext = module.split(\'.\')[-1].lower()\n            curr_module_name = module.split(\'.\')[0]\n            if curr_module_ext == ""py"" and curr_module_name.lower() == module_name.lower():\n                module_path = \'.\'.join(module_path.split(""."")[:-1] + [curr_module_name])\n    module = importlib.import_module(module_path)\n    class_ref = getattr(module, class_name)\n    return class_ref\n\n\ndef dynamic_import_from_full_path(module_path: str, class_name: str, ignore_module_case: bool=False):\n    if ignore_module_case:\n        module_name = module_path.split(""/"")[-1]\n        available_modules = os.listdir(os.path.dirname(module_path))\n        for module in available_modules:\n            curr_module_ext = module.split(\'.\')[-1].lower()\n            curr_module_name = module.split(\'.\')[0]\n            if curr_module_ext == ""py"" and curr_module_name.lower() == module_name.lower():\n                module_path = \'.\'.join(module_path.split(""/"")[:-1] + [curr_module_name])\n    spec = importlib.util.spec_from_file_location(""module"", module_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    class_ref = getattr(module, class_name)\n    return class_ref\n\n\ndef dynamic_import_and_instantiate_module_from_params(module_parameters, path=None, positional_args=[],\n                                                      extra_kwargs={}):\n    """"""\n    A function dedicated for coach modules like memory, exploration policy, etc.\n    Given the module parameters, it imports it and instantiates it.\n    :param module_parameters:\n    :return:\n    """"""\n    import inspect\n    if path is None:\n        path = module_parameters.path\n    module = short_dynamic_import(path)\n    args = set(inspect.getfullargspec(module).args).intersection(module_parameters.__dict__)\n    args = {k: module_parameters.__dict__[k] for k in args}\n    args = {**args, **extra_kwargs}\n    return short_dynamic_import(path)(*positional_args, **args)\n\n\ndef last_sample(state):\n    """"""\n    given a batch of states, return the last sample of the batch with length 1\n    batch axis.\n    """"""\n    return {\n        k: np.expand_dims(v[-1], 0)\n        for k, v in state.items()\n    }\n\n\ndef get_all_subclasses(cls):\n    if len(cls.__subclasses__()) == 0:\n        return []\n    ret = []\n    for drv in cls.__subclasses__():\n        ret.append(drv)\n        ret.extend(get_all_subclasses(drv))\n\n    return ret\n\n\nclass SharedMemoryScratchPad(object):\n    def __init__(self):\n        self.dict = {}\n\n    def add(self, key, value):\n        self.dict[key] = value\n\n    def get(self, key, timeout=30):\n        start_time = time.time()\n        timeout_passed = False\n        while key not in self.dict and not timeout_passed:\n            time.sleep(0.1)\n            timeout_passed = (time.time() - start_time) > timeout\n\n        if timeout_passed:\n            return None\n        return self.dict[key]\n\n    def internal_call(self, key, func, args: Tuple):\n        if type(args) != tuple:\n            args = (args,)\n        return getattr(self.dict[key], func)(*args)\n\n\nclass Timer(object):\n    def __init__(self, prefix):\n        self.prefix = prefix\n\n    def __enter__(self):\n        self.start = time.time()\n\n    def __exit__(self, type, value, traceback):\n        print(self.prefix, time.time() - self.start)\n\n\nclass ReaderWriterLock(object):\n    def __init__(self):\n        self.num_readers_lock = Manager().Lock()\n        self.writers_lock = Manager().Lock()\n        self.num_readers = 0\n        self.now_writing = False\n\n    def some_worker_is_reading(self):\n        return self.num_readers > 0\n\n    def some_worker_is_writing(self):\n        return self.now_writing is True\n\n    def lock_writing_and_reading(self):\n        self.writers_lock.acquire()  # first things first - block all other writers\n        self.now_writing = True  # block new readers who haven\'t started reading yet\n        while self.some_worker_is_reading():  # let existing readers finish their homework\n            time.sleep(0.05)\n\n    def release_writing_and_reading(self):\n        self.now_writing = False  # release readers - guarantee no readers starvation\n        self.writers_lock.release()  # release writers\n\n    def lock_writing(self):\n        while self.now_writing:\n            time.sleep(0.05)\n\n        self.num_readers_lock.acquire()\n        self.num_readers += 1\n        self.num_readers_lock.release()\n\n    def release_writing(self):\n        self.num_readers_lock.acquire()\n        self.num_readers -= 1\n        self.num_readers_lock.release()\n\n\nclass ProgressBar(object):\n    def __init__(self, max_value):\n        self.start_time = time.time()\n        self.max_value = max_value\n        self.current_value = 0\n\n    def update(self, current_value, additional_info=""""):\n        self.current_value = current_value\n        percentage = int((100 * current_value) / self.max_value)\n        sys.stdout.write(""\\rProgress: ({}/{}) Time: {} sec {}%|{}{}|  {}""\n                         .format(current_value, self.max_value,\n                                 round(time.time() - self.start_time, 2),\n                                 percentage, \'#\' * int(percentage / 10),\n                                 \' \' * (10 - int(percentage / 10)),\n                                 additional_info))\n        sys.stdout.flush()\n\n    def close(self):\n        print("""")\n\n\ndef start_shell_command_and_wait(command):\n    p = Popen(command, shell=True, preexec_fn=os.setsid)\n\n    def cleanup():\n        os.killpg(os.getpgid(p.pid), 15)\n\n    atexit.register(cleanup)\n    p.wait()\n    atexit.unregister(cleanup)\n\n\ndef indent_string(string):\n    return \'\\t\' + string.replace(\'\\n\', \'\\n\\t\')\n\ndef get_latest_checkpoint(checkpoint_dir: str, checkpoint_prefix: str, checkpoint_file_extension: str) -> str:\n    latest_checkpoint_id = -1\n    latest_checkpoint = \'\'\n    # get all checkpoint files\n    for fname in os.listdir(checkpoint_dir):\n        path = os.path.join(checkpoint_dir, fname)\n        if os.path.isdir(path) or fname.split(\'.\')[-1] != checkpoint_file_extension or checkpoint_prefix not in fname:\n            continue\n        checkpoint_id = int(fname.split(\'_\')[0])\n        if checkpoint_id > latest_checkpoint_id:\n            latest_checkpoint = fname\n            latest_checkpoint_id = checkpoint_id\n\n    return latest_checkpoint\n'"
tutorials/python_invocation_example.py,0,"b""from rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import SimpleSchedule\n\ngraph_manager = BasicRLGraphManager(\n    agent_params=ClippedPPOAgentParameters(),\n    env_params=GymVectorEnvironment(level='CartPole-v0'),\n    schedule_params=SimpleSchedule()\n)\n\ngraph_manager.heatup(EnvironmentSteps(100))\ngraph_manager.train_and_act(EnvironmentSteps(100))"""
docs_raw/source/__init__.py,0,b''
docs_raw/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(\'.\'))))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Reinforcement Learning Coach\'\ncopyright = \'2018-2019, Intel AI Lab\'\nauthor = \'Intel AI Lab\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.12.0\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinxarg.ext\'\n]\n\n# add the thid-party modules to be mocked to the autodoc_mod_imports configuration value\nautodoc_mock_imports = [\'carla\', \'suite\', \'flags\', \'vizdoom\', \'dm_control\', \'pybullet\',\n                        \'roboschool\', \'pysc2\', \'gymextensions\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\nsource_parsers = {\n   \'.md\': \'recommonmark.parser.CommonMarkParser\',\n}\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n# source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\nautoclass_content = \'both\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\nhtml_logo = \'./_static/img/dark_logo.png\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n# html_theme_options = {\n#     \'canonical_url\': \'\',\n#     \'analytics_id\': \'\',\n#     \'logo_only\': True,\n#     \'display_version\': True,\n#     \'prev_next_buttons_location\': \'bottom\',\n#     \'style_external_links\': False,\n#     # Toc options\n#     \'collapse_navigation\': True,\n#     \'sticky_navigation\': True,\n#     \'navigation_depth\': 1,\n#     \'includehidden\': True,\n#     \'titles_only\': False\n# }\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\ndef setup(app):\n    app.add_stylesheet(\'css/custom.css\')\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'ReinforcementLearningCoachdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'ReinforcementLearningCoach.tex\', \'Reinforcement Learning Coach Documentation\',\n     \'Intel AI Lab\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'reinforcementlearningcoach\', \'Reinforcement Learning Coach Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'ReinforcementLearningCoach\', \'Reinforcement Learning Coach Documentation\',\n     author, \'ReinforcementLearningCoach\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True'"
rl_coach/agents/__init__.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
rl_coach/agents/acer_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\nimport numpy as np\n\nfrom rl_coach.agents.policy_optimization_agent import PolicyOptimizationAgent\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import ACERPolicyHeadParameters, QHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, NetworkParameters, AgentParameters\nfrom rl_coach.core_types import Batch\nfrom rl_coach.exploration_policies.categorical import CategoricalParameters\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.spaces import DiscreteActionSpace\nfrom rl_coach.utils import eps, last_sample\n\n\nclass ACERAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param num_steps_between_gradient_updates: (int)\n        Every num_steps_between_gradient_updates transitions will be considered as a single batch and use for\n        accumulating gradients. This is also the number of steps used for bootstrapping according to the n-step formulation.\n\n    :param ratio_of_replay: (int)\n        The number of off-policy training iterations in each ACER iteration.\n\n    :param num_transitions_to_start_replay: (int)\n        Number of environment steps until ACER starts to train off-policy from the experience replay.\n        This emulates a heat-up phase where the agents learns only on-policy until there are enough transitions in\n        the experience replay to start the off-policy training.\n\n    :param rate_for_copying_weights_to_target: (float)\n        The rate of the exponential moving average for the average policy which is used for the trust region optimization.\n        The target network in this algorithm is used as the average policy.\n\n    :param importance_weight_truncation: (float)\n        The clipping constant for the importance weight truncation (not used in the Q-retrace calculation).\n\n    :param use_trust_region_optimization: (bool)\n        If set to True, the gradients of the network will be modified with a term dependant on the KL divergence between\n        the average policy and the current one, to bound the change of the policy during the network update.\n\n    :param max_KL_divergence: (float)\n        The upper bound parameter for the trust region optimization, use_trust_region_optimization needs to be set true\n        for this parameter to have an effect.\n\n    :param beta_entropy: (float)\n        An entropy regulaization term can be added to the loss function in order to control exploration. This term\n        is weighted using the beta value defined by beta_entropy.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.apply_gradients_every_x_episodes = 5\n        self.num_steps_between_gradient_updates = 5000\n        self.ratio_of_replay = 4\n        self.num_transitions_to_start_replay = 10000\n        self.rate_for_copying_weights_to_target = 0.01\n        self.importance_weight_truncation = 10.0\n        self.use_trust_region_optimization = True\n        self.max_KL_divergence = 1.0\n        self.beta_entropy = 0\n\n\nclass ACERNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.heads_parameters = [QHeadParameters(loss_weight=0.5), ACERPolicyHeadParameters(loss_weight=1.0)]\n        self.optimizer_type = \'Adam\'\n        self.async_training = True\n        self.clip_gradients = 40.0\n        self.create_target_network = True\n\n\nclass ACERAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=ACERAlgorithmParameters(),\n                         exploration={DiscreteActionSpace: CategoricalParameters()},\n                         memory=EpisodicExperienceReplayParameters(),\n                         networks={""main"": ACERNetworkParameters()})\n    @property\n    def path(self):\n        return \'rl_coach.agents.acer_agent:ACERAgent\'\n\n\n# Actor-Critic with Experience Replay - https://arxiv.org/abs/1611.01224\nclass ACERAgent(PolicyOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        # signals definition\n        self.q_loss = self.register_signal(\'Q Loss\')\n        self.policy_loss = self.register_signal(\'Policy Loss\')\n        self.probability_loss = self.register_signal(\'Probability Loss\')\n        self.bias_correction_loss = self.register_signal(\'Bias Correction Loss\')\n        self.unclipped_grads = self.register_signal(\'Grads (unclipped)\')\n        self.V_Values = self.register_signal(\'Values\')\n        self.kl_divergence = self.register_signal(\'KL Divergence\')\n\n    def _learn_from_batch(self, batch):\n\n        fetches = [self.networks[\'main\'].online_network.output_heads[1].probability_loss,\n                   self.networks[\'main\'].online_network.output_heads[1].bias_correction_loss,\n                   self.networks[\'main\'].online_network.output_heads[1].kl_divergence]\n\n        # batch contains a list of transitions to learn from\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # get the values for the current states\n        Q_values, policy_prob = self.networks[\'main\'].online_network.predict(batch.states(network_keys))\n        avg_policy_prob = self.networks[\'main\'].target_network.predict(batch.states(network_keys))[1]\n        current_state_values = np.sum(policy_prob * Q_values, axis=1)\n\n        actions = batch.actions()\n        num_transitions = batch.size\n        Q_head_targets = Q_values\n\n        Q_i = Q_values[np.arange(num_transitions), actions]\n\n        mu = batch.info(\'all_action_probabilities\')\n        rho = policy_prob / (mu + eps)\n        rho_i = rho[np.arange(batch.size), actions]\n\n        rho_bar = np.minimum(1.0, rho_i)\n\n        if batch.game_overs()[-1]:\n            Qret = 0\n        else:\n            result = self.networks[\'main\'].online_network.predict(last_sample(batch.next_states(network_keys)))\n            Qret = np.sum(result[0] * result[1], axis=1)[0]\n\n        for i in reversed(range(num_transitions)):\n            Qret = batch.rewards()[i] + self.ap.algorithm.discount * Qret\n            Q_head_targets[i, actions[i]] = Qret\n            Qret = rho_bar[i] * (Qret - Q_i[i]) + current_state_values[i]\n\n        Q_retrace = Q_head_targets[np.arange(num_transitions), actions]\n\n        # train\n        result = self.networks[\'main\'].train_and_sync_networks({**batch.states(network_keys),\n                                                                \'output_1_0\': actions,\n                                                                \'output_1_1\': rho,\n                                                                \'output_1_2\': rho_i,\n                                                                \'output_1_3\': Q_values,\n                                                                \'output_1_4\': Q_retrace,\n                                                                \'output_1_5\': avg_policy_prob},\n                                                               [Q_head_targets, current_state_values],\n                                                               additional_fetches=fetches)\n\n        for network in self.networks.values():\n            network.update_target_network(self.ap.algorithm.rate_for_copying_weights_to_target)\n\n        # logging\n        total_loss, losses, unclipped_grads, fetch_result = result[:4]\n        self.q_loss.add_sample(losses[0])\n        self.policy_loss.add_sample(losses[1])\n        self.probability_loss.add_sample(fetch_result[0])\n        self.bias_correction_loss.add_sample(fetch_result[1])\n        self.unclipped_grads.add_sample(unclipped_grads)\n        self.V_Values.add_sample(current_state_values)\n        self.kl_divergence.add_sample(fetch_result[2])\n\n        return total_loss, losses, unclipped_grads\n\n    def learn_from_batch(self, batch):\n        # perform on-policy training iteration\n        total_loss, losses, unclipped_grads = self._learn_from_batch(batch)\n\n        if self.ap.algorithm.ratio_of_replay > 0 \\\n                and self.memory.num_transitions() > self.ap.algorithm.num_transitions_to_start_replay:\n            n = np.random.poisson(self.ap.algorithm.ratio_of_replay)\n            # perform n off-policy training iterations\n            for _ in range(n):\n                new_batch = Batch(self.call_memory(\'sample\', (self.ap.algorithm.num_steps_between_gradient_updates, True)))\n                result = self._learn_from_batch(new_batch)\n                total_loss += result[0]\n                losses += result[1]\n                unclipped_grads += result[2]\n\n        return total_loss, losses, unclipped_grads\n\n    def get_prediction(self, states):\n        tf_input_state = self.prepare_batch_for_inference(states, ""main"")\n        return self.networks[\'main\'].online_network.predict(tf_input_state)[1:]  # index 0 is the state value\n'"
rl_coach/agents/actor_critic_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\nimport scipy.signal\n\nfrom rl_coach.agents.policy_optimization_agent import PolicyOptimizationAgent, PolicyGradientRescaler\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import PolicyHeadParameters, VHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, NetworkParameters, \\\n    AgentParameters\nfrom rl_coach.exploration_policies.categorical import CategoricalParameters\nfrom rl_coach.exploration_policies.continuous_entropy import ContinuousEntropyParameters\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.episodic.single_episode_buffer import SingleEpisodeBufferParameters\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\nfrom rl_coach.utils import last_sample\n\n\nclass ActorCriticAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param policy_gradient_rescaler: (PolicyGradientRescaler)\n        The value that will be used to rescale the policy gradient\n\n    :param apply_gradients_every_x_episodes: (int)\n        The number of episodes to wait before applying the accumulated gradients to the network.\n        The training iterations only accumulate gradients without actually applying them.\n\n    :param beta_entropy: (float)\n        The weight that will be given to the entropy regularization which is used in order to improve exploration.\n\n    :param num_steps_between_gradient_updates: (int)\n        Every num_steps_between_gradient_updates transitions will be considered as a single batch and use for\n        accumulating gradients. This is also the number of steps used for bootstrapping according to the n-step formulation.\n\n    :param gae_lambda: (float)\n        If the policy gradient rescaler was defined as PolicyGradientRescaler.GAE, the generalized advantage estimation\n        scheme will be used, in which case the lambda value controls the decay for the different n-step lengths.\n\n    :param estimate_state_value_using_gae: (bool)\n        If set to True, the state value targets for the V head will be estimated using the GAE scheme.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.policy_gradient_rescaler = PolicyGradientRescaler.A_VALUE\n        self.apply_gradients_every_x_episodes = 5\n        self.beta_entropy = 0\n        self.num_steps_between_gradient_updates = 5000  # this is called t_max in all the papers\n        self.gae_lambda = 0.96\n        self.estimate_state_value_using_gae = False\n\n\nclass ActorCriticNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.heads_parameters = [VHeadParameters(loss_weight=0.5), PolicyHeadParameters(loss_weight=1.0)]\n        self.optimizer_type = \'Adam\'\n        self.clip_gradients = 40.0\n        self.async_training = True\n\n\nclass ActorCriticAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=ActorCriticAlgorithmParameters(),\n                         exploration={DiscreteActionSpace: CategoricalParameters(),\n                                      BoxActionSpace: ContinuousEntropyParameters()},\n                         memory=SingleEpisodeBufferParameters(),\n                         networks={""main"": ActorCriticNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.actor_critic_agent:ActorCriticAgent\'\n\n\n# Actor Critic - https://arxiv.org/abs/1602.01783\nclass ActorCriticAgent(PolicyOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.last_gradient_update_step_idx = 0\n        self.action_advantages = self.register_signal(\'Advantages\')\n        self.state_values = self.register_signal(\'Values\')\n        self.value_loss = self.register_signal(\'Value Loss\')\n        self.policy_loss = self.register_signal(\'Policy Loss\')\n\n    # Discounting function used to calculate discounted returns.\n    def discount(self, x, gamma):\n        return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n\n    def get_general_advantage_estimation_values(self, rewards, values):\n        # values contain n+1 elements (t ... t+n+1), rewards contain n elements (t ... t + n)\n        bootstrap_extended_rewards = np.array(rewards.tolist() + [values[-1]])\n\n        # Approximation based calculation of GAE (mathematically correct only when Tmax = inf,\n        # although in practice works even in much smaller Tmax values, e.g. 20)\n        deltas = rewards + self.ap.algorithm.discount * values[1:] - values[:-1]\n        gae = self.discount(deltas, self.ap.algorithm.discount * self.ap.algorithm.gae_lambda)\n\n        if self.ap.algorithm.estimate_state_value_using_gae:\n            discounted_returns = np.expand_dims(gae + values[:-1], -1)\n        else:\n            discounted_returns = np.expand_dims(np.array(self.discount(bootstrap_extended_rewards,\n                                                                       self.ap.algorithm.discount)), 1)[:-1]\n        return gae, discounted_returns\n\n    def learn_from_batch(self, batch):\n        # batch contains a list of episodes to learn from\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # get the values for the current states\n\n        result = self.networks[\'main\'].online_network.predict(batch.states(network_keys))\n        current_state_values = result[0]\n\n        self.state_values.add_sample(current_state_values)\n\n        # the targets for the state value estimator\n        num_transitions = batch.size\n        state_value_head_targets = np.zeros((num_transitions, 1))\n\n        # estimate the advantage function\n        action_advantages = np.zeros((num_transitions, 1))\n\n        if self.policy_gradient_rescaler == PolicyGradientRescaler.A_VALUE:\n            if batch.game_overs()[-1]:\n                R = 0\n            else:\n                R = self.networks[\'main\'].online_network.predict(last_sample(batch.next_states(network_keys)))[0]\n\n            for i in reversed(range(num_transitions)):\n                R = batch.rewards()[i] + self.ap.algorithm.discount * R\n                state_value_head_targets[i] = R\n                action_advantages[i] = R - current_state_values[i]\n\n        elif self.policy_gradient_rescaler == PolicyGradientRescaler.GAE:\n            # get bootstraps\n            bootstrapped_value = self.networks[\'main\'].online_network.predict(last_sample(batch.next_states(network_keys)))[0]\n            values = np.append(current_state_values, bootstrapped_value)\n            if batch.game_overs()[-1]:\n                values[-1] = 0\n\n            # get general discounted returns table\n            gae_values, state_value_head_targets = self.get_general_advantage_estimation_values(batch.rewards(), values)\n            action_advantages = np.vstack(gae_values)\n        else:\n            screen.warning(""WARNING: The requested policy gradient rescaler is not available"")\n\n        action_advantages = action_advantages.squeeze(axis=-1)\n        actions = batch.actions()\n        if not isinstance(self.spaces.action, DiscreteActionSpace) and len(actions.shape) < 2:\n            actions = np.expand_dims(actions, -1)\n\n        # train\n        result = self.networks[\'main\'].online_network.accumulate_gradients({**batch.states(network_keys),\n                                                                            \'output_1_0\': actions},\n                                                                       [state_value_head_targets, action_advantages])\n\n        # logging\n        total_loss, losses, unclipped_grads = result[:3]\n        self.action_advantages.add_sample(action_advantages)\n        self.unclipped_grads.add_sample(unclipped_grads)\n        self.value_loss.add_sample(losses[0])\n        self.policy_loss.add_sample(losses[1])\n\n        return total_loss, losses, unclipped_grads\n\n    def get_prediction(self, states):\n        tf_input_state = self.prepare_batch_for_inference(states, ""main"")\n        return self.networks[\'main\'].online_network.predict(tf_input_state)[1:]  # index 0 is the state value\n'"
rl_coach/agents/agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nimport random\nfrom collections import OrderedDict\nfrom typing import Dict, List, Union, Tuple\n\nimport numpy as np\nfrom six.moves import range\n\nfrom rl_coach.agents.agent_interface import AgentInterface\nfrom rl_coach.architectures.network_wrapper import NetworkWrapper\nfrom rl_coach.base_parameters import AgentParameters, Device, DeviceType, DistributedTaskParameters, Frameworks\nfrom rl_coach.core_types import RunPhase, PredictionType, EnvironmentEpisodes, ActionType, Batch, Episode, StateType\nfrom rl_coach.core_types import Transition, ActionInfo, TrainingSteps, EnvironmentSteps, EnvResponse\nfrom rl_coach.logger import screen, Logger, EpisodeLogger\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplay\nfrom rl_coach.saver import SaverCollection\nfrom rl_coach.spaces import SpacesDefinition, VectorObservationSpace, GoalsSpace, AttentionActionSpace\nfrom rl_coach.utils import Signal, force_list\nfrom rl_coach.utils import dynamic_import_and_instantiate_module_from_params\nfrom rl_coach.memories.backend.memory_impl import get_memory_backend\nfrom rl_coach.core_types import TimeTypes\nfrom rl_coach.off_policy_evaluators.ope_manager import OpeManager\nfrom rl_coach.core_types import PickledReplayBuffer, CsvDataset\n\n\nclass Agent(AgentInterface):\n    def __init__(self, agent_parameters: AgentParameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        """"""\n        :param agent_parameters: A AgentParameters class instance with all the agent parameters\n        """"""\n        super().__init__()\n        # use seed\n        if agent_parameters.task_parameters.seed is not None:\n            random.seed(agent_parameters.task_parameters.seed)\n            np.random.seed(agent_parameters.task_parameters.seed)\n        else:\n            # we need to seed the RNG since the different processes are initialized with the same parent seed\n            random.seed()\n            np.random.seed()\n\n        self.ap = agent_parameters\n        self.task_id = self.ap.task_parameters.task_index\n        self.is_chief = self.task_id == 0\n        self.shared_memory = type(agent_parameters.task_parameters) == DistributedTaskParameters \\\n                             and self.ap.memory.shared_memory\n        if self.shared_memory:\n            self.shared_memory_scratchpad = self.ap.task_parameters.shared_memory_scratchpad\n        self.parent = parent\n        self.parent_level_manager = None\n        # TODO this needs to be sorted out. Why the duplicates for the agent\'s name?\n        self.full_name_id = agent_parameters.full_name_id = self.name = agent_parameters.name\n\n        if type(agent_parameters.task_parameters) == DistributedTaskParameters:\n            screen.log_title(""Creating agent - name: {} task id: {} (may take up to 30 seconds due to ""\n                             ""tensorflow wake up time)"".format(self.full_name_id, self.task_id))\n        else:\n            screen.log_title(""Creating agent - name: {}"".format(self.full_name_id))\n        self.imitation = False\n        self.agent_logger = Logger()\n        self.agent_episode_logger = EpisodeLogger()\n\n        # get the memory\n        # - distributed training + shared memory:\n        #   * is chief?  -> create the memory and add it to the scratchpad\n        #   * not chief? -> wait for the chief to create the memory and then fetch it\n        # - non distributed training / not shared memory:\n        #   * create memory\n        memory_name = self.ap.memory.path.split(\':\')[1]\n        self.memory_lookup_name = self.full_name_id + \'.\' + memory_name\n        if self.shared_memory and not self.is_chief:\n            self.memory = self.shared_memory_scratchpad.get(self.memory_lookup_name)\n        else:\n            # modules\n            self.memory = dynamic_import_and_instantiate_module_from_params(self.ap.memory)\n\n            if hasattr(self.ap.memory, \'memory_backend_params\'):\n                self.memory_backend = get_memory_backend(self.ap.memory.memory_backend_params)\n\n                if self.ap.memory.memory_backend_params.run_type != \'trainer\':\n                    self.memory.set_memory_backend(self.memory_backend)\n\n            if self.shared_memory and self.is_chief:\n                self.shared_memory_scratchpad.add(self.memory_lookup_name, self.memory)\n\n        # set devices\n        if type(agent_parameters.task_parameters) == DistributedTaskParameters:\n            self.has_global = True\n            self.replicated_device = agent_parameters.task_parameters.device\n            self.worker_device = ""/job:worker/task:{}"".format(self.task_id)\n            if agent_parameters.task_parameters.use_cpu:\n                self.worker_device += ""/cpu:0""\n            else:\n                self.worker_device += ""/device:GPU:0""\n        else:\n            self.has_global = False\n            self.replicated_device = None\n            if agent_parameters.task_parameters.use_cpu:\n                self.worker_device = Device(DeviceType.CPU)\n            else:\n                self.worker_device = [Device(DeviceType.GPU, i)\n                                      for i in range(agent_parameters.task_parameters.num_gpu)]\n\n        # filters\n        self.input_filter = self.ap.input_filter\n        self.input_filter.set_name(\'input_filter\')\n        self.output_filter = self.ap.output_filter\n        self.output_filter.set_name(\'output_filter\')\n        self.pre_network_filter = self.ap.pre_network_filter\n        self.pre_network_filter.set_name(\'pre_network_filter\')\n\n        device = self.replicated_device if self.replicated_device else self.worker_device\n\n        # TODO-REMOVE This is a temporary flow dividing to 3 modes. To be converged to a single flow once distributed tf\n        #  is removed, and Redis is used for sharing data between local workers.\n        # Filters MoW will be split between different configurations\n        # 1. Distributed coach synchrnization type (=distributed across multiple nodes) - Redis based data sharing + numpy arithmetic backend\n        # 2. Distributed TF (=distributed on a single node, using distributed TF) - TF for both data sharing and arithmetic backend\n        # 3. Single worker (=both TF and Mxnet) - no data sharing needed + numpy arithmetic backend\n\n        if hasattr(self.ap.memory, \'memory_backend_params\') and self.ap.algorithm.distributed_coach_synchronization_type:\n            self.input_filter.set_device(device, memory_backend_params=self.ap.memory.memory_backend_params, mode=\'numpy\')\n            self.output_filter.set_device(device, memory_backend_params=self.ap.memory.memory_backend_params, mode=\'numpy\')\n            self.pre_network_filter.set_device(device, memory_backend_params=self.ap.memory.memory_backend_params, mode=\'numpy\')\n        elif (type(agent_parameters.task_parameters) == DistributedTaskParameters and\n              agent_parameters.task_parameters.framework_type == Frameworks.tensorflow):\n            self.input_filter.set_device(device, mode=\'tf\')\n            self.output_filter.set_device(device, mode=\'tf\')\n            self.pre_network_filter.set_device(device, mode=\'tf\')\n        else:\n            self.input_filter.set_device(device, mode=\'numpy\')\n            self.output_filter.set_device(device, mode=\'numpy\')\n            self.pre_network_filter.set_device(device, mode=\'numpy\')\n\n        # initialize all internal variables\n        self._phase = RunPhase.HEATUP\n        self.total_shaped_reward_in_current_episode = 0\n        self.total_reward_in_current_episode = 0\n        self.total_steps_counter = 0\n        self.running_reward = None\n        self.training_iteration = 0\n        self.training_epoch = 0\n        self.last_target_network_update_step = 0\n        self.last_training_phase_step = 0\n        self.current_episode = self.ap.current_episode = 0\n        self.curr_state = {}\n        self.current_hrl_goal = None\n        self.current_episode_steps_counter = 0\n        self.episode_running_info = {}\n        self.last_episode_evaluation_ran = 0\n        self.running_observations = []\n        self.agent_logger.set_current_time(self.current_episode)\n        self.exploration_policy = None\n        self.networks = {}\n        self.last_action_info = None\n        self.running_observation_stats = None\n        self.running_reward_stats = None\n        self.accumulated_rewards_across_evaluation_episodes = 0\n        self.accumulated_shaped_rewards_across_evaluation_episodes = 0\n        self.num_successes_across_evaluation_episodes = 0\n        self.num_evaluation_episodes_completed = 0\n        self.current_episode_buffer = Episode(discount=self.ap.algorithm.discount, n_step=self.ap.algorithm.n_step)\n        # TODO: add agents observation rendering for debugging purposes (not the same as the environment rendering)\n\n        # environment parameters\n        self.spaces = None\n        self.in_action_space = self.ap.algorithm.in_action_space\n\n        # signals\n        self.episode_signals = []\n        self.step_signals = []\n        self.loss = self.register_signal(\'Loss\')\n        self.curr_learning_rate = self.register_signal(\'Learning Rate\')\n        self.unclipped_grads = self.register_signal(\'Grads (unclipped)\')\n        self.reward = self.register_signal(\'Reward\', dump_one_value_per_episode=False, dump_one_value_per_step=True)\n        self.shaped_reward = self.register_signal(\'Shaped Reward\', dump_one_value_per_episode=False, dump_one_value_per_step=True)\n        self.discounted_return = self.register_signal(\'Discounted Return\')\n        if isinstance(self.in_action_space, GoalsSpace):\n            self.distance_from_goal = self.register_signal(\'Distance From Goal\', dump_one_value_per_step=True)\n\n        # batch rl\n        self.ope_manager = OpeManager() if self.ap.is_batch_rl_training else None\n\n    @property\n    def parent(self) -> \'LevelManager\':\n        """"""\n        Get the parent class of the agent\n\n        :return: the current phase\n        """"""\n        return self._parent\n\n    @parent.setter\n    def parent(self, val) -> None:\n        """"""\n        Change the parent class of the agent.\n        Additionally, updates the full name of the agent\n\n        :param val: the new parent\n        :return: None\n        """"""\n        self._parent = val\n        if self._parent is not None:\n            if not hasattr(self._parent, \'name\'):\n                raise ValueError(""The parent of an agent must have a name"")\n            self.full_name_id = self.ap.full_name_id = ""{}/{}"".format(self._parent.name, self.name)\n\n    def setup_logger(self) -> None:\n        """"""\n        Setup the logger for the agent\n\n        :return: None\n        """"""\n        # dump documentation\n        logger_prefix = ""{graph_name}.{level_name}.{agent_full_id}"".\\\n            format(graph_name=self.parent_level_manager.parent_graph_manager.name,\n                   level_name=self.parent_level_manager.name,\n                   agent_full_id=\'.\'.join(self.full_name_id.split(\'/\')))\n        self.agent_logger.set_index_name(self.parent_level_manager.parent_graph_manager.time_metric.value.name)\n        self.agent_logger.set_logger_filenames(self.ap.task_parameters.experiment_path, logger_prefix=logger_prefix,\n                                               add_timestamp=True, task_id=self.task_id)\n        if self.ap.visualization.dump_in_episode_signals:\n            self.agent_episode_logger.set_logger_filenames(self.ap.task_parameters.experiment_path,\n                                                           logger_prefix=logger_prefix,\n                                                           add_timestamp=True, task_id=self.task_id)\n\n    def set_session(self, sess) -> None:\n        """"""\n        Set the deep learning framework session for all the agents in the composite agent\n\n        :return: None\n        """"""\n        self.input_filter.set_session(sess)\n        self.output_filter.set_session(sess)\n        self.pre_network_filter.set_session(sess)\n        [network.set_session(sess) for network in self.networks.values()]\n        self.initialize_session_dependent_components()\n\n    def initialize_session_dependent_components(self):\n        """"""\n        Initialize components which require a session as part of their initialization.\n\n        :return: None\n        """"""\n\n        # Loading a memory from a CSV file, requires an input filter to filter through the data.\n        # The filter needs a session before it can be used.\n        if self.ap.memory.load_memory_from_file_path:\n            self.load_memory_from_file()\n\n    def load_memory_from_file(self):\n        """"""\n        Load memory transitions from a file.\n\n        :return: None\n        """"""\n\n        if isinstance(self.ap.memory.load_memory_from_file_path, PickledReplayBuffer):\n            screen.log_title(""Loading a pickled replay buffer. Pickled file path: {}""\n                             .format(self.ap.memory.load_memory_from_file_path.filepath))\n            self.memory.load_pickled(self.ap.memory.load_memory_from_file_path.filepath)\n        elif isinstance(self.ap.memory.load_memory_from_file_path, CsvDataset):\n            screen.log_title(""Loading a replay buffer from a CSV file. CSV file path: {}""\n                             .format(self.ap.memory.load_memory_from_file_path.filepath))\n            self.memory.load_csv(self.ap.memory.load_memory_from_file_path, self.input_filter)\n        else:\n            raise ValueError(\'Trying to load a replay buffer using an unsupported method - {}. \'\n                             .format(self.ap.memory.load_memory_from_file_path))\n\n    def register_signal(self, signal_name: str, dump_one_value_per_episode: bool=True,\n                        dump_one_value_per_step: bool=False) -> Signal:\n        """"""\n        Register a signal such that its statistics will be dumped and be viewable through dashboard\n\n        :param signal_name: the name of the signal as it will appear in dashboard\n        :param dump_one_value_per_episode: should the signal value be written for each episode?\n        :param dump_one_value_per_step: should the signal value be written for each step?\n        :return: the created signal\n        """"""\n        signal = Signal(signal_name)\n        if dump_one_value_per_episode:\n            self.episode_signals.append(signal)\n        if dump_one_value_per_step:\n            self.step_signals.append(signal)\n        return signal\n\n    def set_environment_parameters(self, spaces: SpacesDefinition):\n        """"""\n        Sets the parameters that are environment dependent. As a side effect, initializes all the components that are\n        dependent on those values, by calling init_environment_dependent_modules\n\n        :param spaces: the environment spaces definition\n        :return: None\n        """"""\n        self.spaces = copy.deepcopy(spaces)\n\n        if self.ap.algorithm.use_accumulated_reward_as_measurement:\n            if \'measurements\' in self.spaces.state.sub_spaces:\n                self.spaces.state[\'measurements\'].shape += 1\n                self.spaces.state[\'measurements\'].measurements_names += [\'accumulated_reward\']\n            else:\n                self.spaces.state[\'measurements\'] = VectorObservationSpace(1, measurements_names=[\'accumulated_reward\'])\n\n        for observation_name in self.spaces.state.sub_spaces.keys():\n            self.spaces.state[observation_name] = \\\n                self.pre_network_filter.get_filtered_observation_space(observation_name,\n                    self.input_filter.get_filtered_observation_space(observation_name,\n                                                                     self.spaces.state[observation_name]))\n\n        self.spaces.reward = self.pre_network_filter.get_filtered_reward_space(\n            self.input_filter.get_filtered_reward_space(self.spaces.reward))\n\n        self.spaces.action = self.output_filter.get_unfiltered_action_space(self.spaces.action)\n\n        if isinstance(self.in_action_space, GoalsSpace):\n            # TODO: what if the goal type is an embedding / embedding change?\n            self.spaces.goal = self.in_action_space\n            self.spaces.goal.set_target_space(self.spaces.state[self.spaces.goal.goal_name])\n\n        self.init_environment_dependent_modules()\n\n    def create_networks(self) -> Dict[str, NetworkWrapper]:\n        """"""\n        Create all the networks of the agent.\n        The network creation will be done after setting the environment parameters for the agent, since they are needed\n        for creating the network.\n\n        :return: A list containing all the networks\n        """"""\n        networks = {}\n        for network_name in sorted(self.ap.network_wrappers.keys()):\n            networks[network_name] = NetworkWrapper(name=network_name,\n                                                    agent_parameters=self.ap,\n                                                    has_target=self.ap.network_wrappers[network_name].create_target_network,\n                                                    has_global=self.has_global,\n                                                    spaces=self.spaces,\n                                                    replicated_device=self.replicated_device,\n                                                    worker_device=self.worker_device)\n\n            if self.ap.visualization.print_networks_summary:\n                print(networks[network_name])\n\n        return networks\n\n    def init_environment_dependent_modules(self) -> None:\n        """"""\n        Initialize any modules that depend on knowing information about the environment such as the action space or\n        the observation space\n\n        :return: None\n        """"""\n        # initialize exploration policy\n        if isinstance(self.ap.exploration, dict):\n            if self.spaces.action.__class__ in self.ap.exploration.keys():\n                self.ap.exploration = self.ap.exploration[self.spaces.action.__class__]\n            else:\n                raise ValueError(""The exploration parameters were defined as a mapping between action space types and ""\n                                 ""exploration types, but the action space used by the environment ({}) was not part of ""\n                                 ""the exploration parameters dictionary keys ({})""\n                                 .format(self.spaces.action.__class__, list(self.ap.exploration.keys())))\n        self.ap.exploration.action_space = self.spaces.action\n        self.exploration_policy = dynamic_import_and_instantiate_module_from_params(self.ap.exploration)\n\n        # create all the networks of the agent\n        self.networks = self.create_networks()\n\n    @property\n    def phase(self) -> RunPhase:\n        """"""\n        The current running phase of the agent\n\n        :return: RunPhase\n        """"""\n        return self._phase\n\n    @phase.setter\n    def phase(self, val: RunPhase) -> None:\n        """"""\n        Change the phase of the run for the agent and all the sub components\n\n        :param val: the new run phase (TRAIN, TEST, etc.)\n        :return: None\n        """"""\n        self.reset_evaluation_state(val)\n        self._phase = val\n        self.exploration_policy.change_phase(val)\n\n    def reset_evaluation_state(self, val: RunPhase) -> None:\n        """"""\n        Perform accumulators initialization when entering an evaluation phase, and signal dumping when exiting an\n        evaluation phase. Entering or exiting the evaluation phase is determined according to the new phase given\n        by val, and by the current phase set in self.phase.\n\n        :param val: The new phase to change to\n        :return: None\n        """"""\n        starting_evaluation = (val == RunPhase.TEST)\n        ending_evaluation = (self.phase == RunPhase.TEST)\n\n        if starting_evaluation:\n            self.accumulated_rewards_across_evaluation_episodes = 0\n            self.accumulated_shaped_rewards_across_evaluation_episodes = 0\n            self.num_successes_across_evaluation_episodes = 0\n            self.num_evaluation_episodes_completed = 0\n\n            # TODO verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back\n            # if self.ap.is_a_highest_level_agent or self.ap.task_parameters.verbosity == ""high"":\n            if self.ap.is_a_highest_level_agent:\n                screen.log_title(""{}: Starting evaluation phase"".format(self.name))\n\n        elif ending_evaluation:\n            # we write to the next episode, because it could be that the current episode was already written\n            # to disk and then we won\'t write it again\n            self.agent_logger.set_current_time(self.get_current_time() + 1)\n\n            evaluation_reward = self.accumulated_rewards_across_evaluation_episodes / self.num_evaluation_episodes_completed\n            self.agent_logger.create_signal_value(\n                \'Evaluation Reward\', evaluation_reward)\n            self.agent_logger.create_signal_value(\n                \'Shaped Evaluation Reward\',\n                self.accumulated_shaped_rewards_across_evaluation_episodes / self.num_evaluation_episodes_completed)\n            success_rate = self.num_successes_across_evaluation_episodes / self.num_evaluation_episodes_completed\n            self.agent_logger.create_signal_value(\n                ""Success Rate"",\n                success_rate)\n\n            # TODO verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back\n            # if self.ap.is_a_highest_level_agent or self.ap.task_parameters.verbosity == ""high"":\n            if self.ap.is_a_highest_level_agent:\n                screen.log_title(""{}: Finished evaluation phase. Success rate = {}, Avg Total Reward = {}""\n                                 .format(self.name, np.round(success_rate, 2), np.round(evaluation_reward, 2)))\n\n    def call_memory(self, func, args=()):\n        """"""\n        This function is a wrapper to allow having the same calls for shared or unshared memories.\n        It should be used instead of calling the memory directly in order to allow different algorithms to work\n        both with a shared and a local memory.\n\n        :param func: the name of the memory function to call\n        :param args: the arguments to supply to the function\n        :return: the return value of the function\n        """"""\n        if self.shared_memory:\n            result = self.shared_memory_scratchpad.internal_call(self.memory_lookup_name, func, args)\n        else:\n            if type(args) != tuple:\n                args = (args,)\n            result = getattr(self.memory, func)(*args)\n        return result\n\n    def log_to_screen(self) -> None:\n        """"""\n        Write an episode summary line to the terminal\n\n        :return: None\n        """"""\n        # log to screen\n        log = OrderedDict()\n        log[""Name""] = self.full_name_id\n        if self.task_id is not None:\n            log[""Worker""] = self.task_id\n        log[""Episode""] = self.current_episode\n        log[""Total reward""] = np.round(self.total_reward_in_current_episode, 2)\n        log[""Exploration""] = np.round(self.exploration_policy.get_control_param(), 2)\n        log[""Steps""] = self.total_steps_counter\n        log[""Training iteration""] = self.training_iteration\n        screen.log_dict(log, prefix=self.phase.value)\n\n    def update_step_in_episode_log(self) -> None:\n        """"""\n        Updates the in-episode log file with all the signal values from the most recent step.\n\n        :return: None\n        """"""\n        # log all the signals to file\n        self.agent_episode_logger.set_current_time(self.current_episode_steps_counter)\n        self.agent_episode_logger.create_signal_value(\'Training Iter\', self.training_iteration)\n        self.agent_episode_logger.create_signal_value(\'In Heatup\', int(self._phase == RunPhase.HEATUP))\n        self.agent_episode_logger.create_signal_value(\'ER #Transitions\', self.call_memory(\'num_transitions\'))\n        self.agent_episode_logger.create_signal_value(\'ER #Episodes\', self.call_memory(\'length\'))\n        self.agent_episode_logger.create_signal_value(\'Total steps\', self.total_steps_counter)\n        self.agent_episode_logger.create_signal_value(""Epsilon"", self.exploration_policy.get_control_param())\n        self.agent_episode_logger.create_signal_value(""Shaped Accumulated Reward"", self.total_shaped_reward_in_current_episode)\n        self.agent_episode_logger.create_signal_value(\'Update Target Network\', 0, overwrite=False)\n        self.agent_episode_logger.update_wall_clock_time(self.current_episode_steps_counter)\n\n        for signal in self.step_signals:\n            self.agent_episode_logger.create_signal_value(signal.name, signal.get_last_value())\n\n        # dump\n        self.agent_episode_logger.dump_output_csv()\n\n    def update_log(self) -> None:\n        """"""\n        Updates the episodic log file with all the signal values from the most recent episode.\n        Additional signals for logging can be set by the creating a new signal using self.register_signal,\n        and then updating it with some internal agent values.\n\n        :return: None\n        """"""\n        # log all the signals to file\n        current_time = self.get_current_time()\n        self.agent_logger.set_current_time(current_time)\n        self.agent_logger.create_signal_value(\'Training Iter\', self.training_iteration)\n        self.agent_logger.create_signal_value(\'Episode #\', self.current_episode)\n        self.agent_logger.create_signal_value(\'Epoch\', self.training_epoch)\n        self.agent_logger.create_signal_value(\'In Heatup\', int(self._phase == RunPhase.HEATUP))\n        self.agent_logger.create_signal_value(\'ER #Transitions\', self.call_memory(\'num_transitions\'))\n        self.agent_logger.create_signal_value(\'ER #Episodes\', self.call_memory(\'length\'))\n        self.agent_logger.create_signal_value(\'Episode Length\', self.current_episode_steps_counter)\n        self.agent_logger.create_signal_value(\'Total steps\', self.total_steps_counter)\n        self.agent_logger.create_signal_value(""Epsilon"", np.mean(self.exploration_policy.get_control_param()))\n        self.agent_logger.create_signal_value(""Shaped Training Reward"", self.total_shaped_reward_in_current_episode\n                                   if self._phase == RunPhase.TRAIN else np.nan)\n        self.agent_logger.create_signal_value(""Training Reward"", self.total_reward_in_current_episode\n                                   if self._phase == RunPhase.TRAIN else np.nan)\n\n        self.agent_logger.create_signal_value(\'Update Target Network\', 0, overwrite=False)\n        self.agent_logger.update_wall_clock_time(current_time)\n\n        # The following signals are created with meaningful values only when an evaluation phase is completed.\n        # Creating with default NaNs for any HEATUP/TRAIN/TEST episode which is not the last in an evaluation phase\n        self.agent_logger.create_signal_value(\'Evaluation Reward\', np.nan, overwrite=False)\n        self.agent_logger.create_signal_value(\'Shaped Evaluation Reward\', np.nan, overwrite=False)\n        self.agent_logger.create_signal_value(\'Success Rate\', np.nan, overwrite=False)\n        self.agent_logger.create_signal_value(\'Inverse Propensity Score\', np.nan, overwrite=False)\n        self.agent_logger.create_signal_value(\'Direct Method Reward\', np.nan, overwrite=False)\n        self.agent_logger.create_signal_value(\'Doubly Robust\', np.nan, overwrite=False)\n        self.agent_logger.create_signal_value(\'Weighted Importance Sampling\', np.nan, overwrite=False)\n        self.agent_logger.create_signal_value(\'Sequential Doubly Robust\', np.nan, overwrite=False)\n\n        for signal in self.episode_signals:\n            self.agent_logger.create_signal_value(""{}/Mean"".format(signal.name), signal.get_mean())\n            self.agent_logger.create_signal_value(""{}/Stdev"".format(signal.name), signal.get_stdev())\n            self.agent_logger.create_signal_value(""{}/Max"".format(signal.name), signal.get_max())\n            self.agent_logger.create_signal_value(""{}/Min"".format(signal.name), signal.get_min())\n\n        # dump\n        if self.current_episode % self.ap.visualization.dump_signals_to_csv_every_x_episodes == 0:\n            self.agent_logger.dump_output_csv()\n\n    def handle_episode_ended(self) -> None:\n        """"""\n        Make any changes needed when each episode is ended.\n        This includes incrementing counters, updating full episode dependent values, updating logs, etc.\n        This function is called right after each episode is ended.\n\n        :return: None\n        """"""\n        self.current_episode_buffer.is_complete = True\n        self.current_episode_buffer.update_transitions_rewards_and_bootstrap_data()\n\n        for transition in self.current_episode_buffer.transitions:\n            self.discounted_return.add_sample(transition.n_step_discounted_rewards)\n\n        if self.phase != RunPhase.TEST or self.ap.task_parameters.evaluate_only:\n            self.current_episode += 1\n\n        if self.phase != RunPhase.TEST:\n            if isinstance(self.memory, EpisodicExperienceReplay):\n                if self.ap.algorithm.override_episode_rewards_with_the_last_transition_reward:\n                    for t in self.current_episode_buffer.transitions:\n                        t.reward = self.current_episode_buffer.transitions[-1].reward\n                self.call_memory(\'store_episode\', self.current_episode_buffer)\n            elif self.ap.algorithm.store_transitions_only_when_episodes_are_terminated:\n                for transition in self.current_episode_buffer.transitions:\n                    self.call_memory(\'store\', transition)\n\n        if self.phase == RunPhase.TEST:\n            self.accumulated_rewards_across_evaluation_episodes += self.total_reward_in_current_episode\n            self.accumulated_shaped_rewards_across_evaluation_episodes += self.total_shaped_reward_in_current_episode\n            self.num_evaluation_episodes_completed += 1\n\n            if self.spaces.reward.reward_success_threshold and \\\n                    self.total_reward_in_current_episode >= self.spaces.reward.reward_success_threshold:\n                self.num_successes_across_evaluation_episodes += 1\n\n        if self.ap.visualization.dump_csv and \\\n                self.parent_level_manager.parent_graph_manager.time_metric == TimeTypes.EpisodeNumber:\n            self.update_log()\n\n        # TODO verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back\n        # if self.ap.is_a_highest_level_agent or self.ap.task_parameters.verbosity == ""high"":\n        if self.ap.is_a_highest_level_agent:\n            self.log_to_screen()\n\n    def reset_internal_state(self) -> None:\n        """"""\n        Reset all the episodic parameters. This function is called right before each episode starts.\n\n        :return: None\n        """"""\n        for signal in self.episode_signals:\n            signal.reset()\n        for signal in self.step_signals:\n            signal.reset()\n        self.agent_episode_logger.set_episode_idx(self.current_episode)\n        self.total_shaped_reward_in_current_episode = 0\n        self.total_reward_in_current_episode = 0\n        self.curr_state = {}\n        self.current_episode_steps_counter = 0\n        self.episode_running_info = {}\n        self.current_episode_buffer = Episode(discount=self.ap.algorithm.discount, n_step=self.ap.algorithm.n_step)\n        if self.exploration_policy:\n            self.exploration_policy.reset()\n        self.input_filter.reset()\n        self.output_filter.reset()\n        self.pre_network_filter.reset()\n        if isinstance(self.memory, EpisodicExperienceReplay):\n            self.call_memory(\'verify_last_episode_is_closed\')\n\n        for network in self.networks.values():\n            network.online_network.reset_internal_memory()\n\n    def learn_from_batch(self, batch) -> Tuple[float, List, List]:\n        """"""\n        Given a batch of transitions, calculates their target values and updates the network.\n\n        :param batch: A list of transitions\n        :return: The total loss of the training, the loss per head and the unclipped gradients\n        """"""\n        return 0, [], []\n\n    def _should_update_online_weights_to_target(self):\n        """"""\n        Determine if online weights should be copied to the target.\n\n        :return: boolean: True if the online weights should be copied to the target.\n        """"""\n\n        # update the target network of every network that has a target network\n        step_method = self.ap.algorithm.num_steps_between_copying_online_weights_to_target\n        if step_method.__class__ == TrainingSteps:\n            should_update = (self.training_iteration - self.last_target_network_update_step) >= step_method.num_steps\n            if should_update:\n                self.last_target_network_update_step = self.training_iteration\n        elif step_method.__class__ == EnvironmentSteps:\n            should_update = (self.total_steps_counter - self.last_target_network_update_step) >= step_method.num_steps\n            if should_update:\n                self.last_target_network_update_step = self.total_steps_counter\n        else:\n            raise ValueError(""The num_steps_between_copying_online_weights_to_target parameter should be either ""\n                             ""EnvironmentSteps or TrainingSteps. Instead it is {}"".format(step_method.__class__))\n        return should_update\n\n    def _should_train(self):\n        """"""\n        Determine if we should start a training phase according to the number of steps passed since the last training\n\n        :return:  boolean: True if we should start a training phase\n        """"""\n\n        should_update = self._should_update()\n\n        steps = self.ap.algorithm.num_consecutive_playing_steps\n\n        if should_update:\n            if steps.__class__ == EnvironmentEpisodes:\n                self.last_training_phase_step = self.current_episode\n            if steps.__class__ == EnvironmentSteps:\n                self.last_training_phase_step = self.total_steps_counter\n\n        return should_update\n\n    def _should_update(self):\n        wait_for_full_episode = self.ap.algorithm.act_for_full_episodes\n        steps = self.ap.algorithm.num_consecutive_playing_steps\n\n        if steps.__class__ == EnvironmentEpisodes:\n            should_update = (self.current_episode - self.last_training_phase_step) >= steps.num_steps\n            should_update = should_update and self.call_memory(\'length\') > 0\n\n        elif steps.__class__ == EnvironmentSteps:\n            should_update = (self.total_steps_counter - self.last_training_phase_step) >= steps.num_steps\n            should_update = should_update and self.call_memory(\'num_transitions\') > 0\n\n            if wait_for_full_episode:\n                should_update = should_update and self.current_episode_buffer.is_complete\n        else:\n            raise ValueError(""The num_consecutive_playing_steps parameter should be either ""\n                             ""EnvironmentSteps or Episodes. Instead it is {}"".format(steps.__class__))\n\n        return should_update\n\n    def train(self) -> float:\n        """"""\n        Check if a training phase should be done as configured by num_consecutive_playing_steps.\n        If it should, then do several training steps as configured by num_consecutive_training_steps.\n        A single training iteration: Sample a batch, train on it and update target networks.\n\n        :return: The total training loss during the training iterations.\n        """"""\n        loss = 0\n        if self._should_train():\n            if self.ap.is_batch_rl_training:\n                # when training an agent for generating a dataset in batch-rl, we don\'t want it to be counted as part of\n                # the training epochs. we only care for training epochs in batch-rl anyway.\n                self.training_epoch += 1\n            for network in self.networks.values():\n                network.set_is_training(True)\n\n            # At the moment we only support a single batch size for all the networks\n            networks_parameters = list(self.ap.network_wrappers.values())\n            assert all(net.batch_size == networks_parameters[0].batch_size for net in networks_parameters)\n\n            batch_size = networks_parameters[0].batch_size\n\n            # we either go sequentially through the entire replay buffer in the batch RL mode,\n            # or sample randomly for the basic RL case.\n            training_schedule = self.call_memory(\'get_shuffled_training_data_generator\', batch_size) if \\\n                self.ap.is_batch_rl_training else [self.call_memory(\'sample\', batch_size) for _ in\n                                      range(self.ap.algorithm.num_consecutive_training_steps)]\n\n            for batch in training_schedule:\n                # update counters\n                self.training_iteration += 1\n                if self.pre_network_filter is not None:\n                    update_internal_state = self.ap.algorithm.update_pre_network_filters_state_on_train\n                    batch = self.pre_network_filter.filter(batch, update_internal_state=update_internal_state, deep_copy=False)\n\n                # if the batch returned empty then there are not enough samples in the replay buffer -> skip\n                # training step\n                if len(batch) > 0:\n                    # train\n                    batch = Batch(batch)\n                    total_loss, losses, unclipped_grads = self.learn_from_batch(batch)\n                    loss += total_loss\n\n                    self.unclipped_grads.add_sample(unclipped_grads)\n\n                    # TODO: this only deals with the main network (if exists), need to do the same for other networks\n                    #  for instance, for DDPG, the LR signal is currently not shown. Probably should be done through the\n                    #  network directly instead of here\n                    # decay learning rate\n                    if \'main\' in self.ap.network_wrappers and \\\n                            self.ap.network_wrappers[\'main\'].learning_rate_decay_rate != 0:\n                        self.curr_learning_rate.add_sample(self.networks[\'main\'].sess.run(\n                            self.networks[\'main\'].online_network.current_learning_rate))\n                    else:\n                        self.curr_learning_rate.add_sample(networks_parameters[0].learning_rate)\n\n                    if any([network.has_target for network in self.networks.values()]) \\\n                            and self._should_update_online_weights_to_target():\n                        for network in self.networks.values():\n                            network.update_target_network(self.ap.algorithm.rate_for_copying_weights_to_target)\n\n                        self.agent_logger.create_signal_value(\'Update Target Network\', 1)\n                    else:\n                        self.agent_logger.create_signal_value(\'Update Target Network\', 0, overwrite=False)\n\n                    self.loss.add_sample(loss)\n\n                    if self.imitation:\n                        self.log_to_screen()\n\n            if self.ap.visualization.dump_csv and \\\n                    self.parent_level_manager.parent_graph_manager.time_metric == TimeTypes.Epoch:\n                # in BatchRL, or imitation learning, the agent never acts, so we have to get the stats out here.\n                # we dump the data out every epoch\n                self.update_log()\n\n            for network in self.networks.values():\n                network.set_is_training(False)\n\n            # run additional commands after the training is done\n            self.post_training_commands()\n\n        return loss\n\n    def choose_action(self, curr_state):\n        """"""\n        choose an action to act with in the current episode being played. Different behavior might be exhibited when\n        training or testing.\n\n        :param curr_state: the current state to act upon.\n        :return: chosen action, some action value describing the action (q-value, probability, etc)\n        """"""\n        pass\n\n    def prepare_batch_for_inference(self, states: Union[Dict[str, np.ndarray], List[Dict[str, np.ndarray]]],\n                                    network_name: str) -> Dict[str, np.array]:\n        """"""\n        Convert curr_state into input tensors tensorflow is expecting. i.e. if we have several inputs states, stack all\n        observations together, measurements together, etc.\n\n        :param states: A list of environment states, where each one is a dict mapping from an observation name to its\n                       corresponding observation\n        :param network_name: The agent network name to prepare the batch for. this is needed in order to extract only\n                             the observation relevant for the network from the states.\n        :return: A dictionary containing a list of values from all the given states for each of the observations\n        """"""\n        # convert to batch so we can run it through the network\n        states = force_list(states)\n        batches_dict = {}\n        for key in self.ap.network_wrappers[network_name].input_embedders_parameters.keys():\n            # there are cases (e.g. ddpg) where the state does not contain all the information needed for running\n            # through the network and this has to be added externally (e.g. ddpg where the action needs to be given in\n            # addition to the current_state, so that all the inputs of the network will be filled)\n            if key in states[0].keys():\n                batches_dict[key] = np.array([np.array(state[key]) for state in states])\n\n        return batches_dict\n\n    def act(self, action: Union[None, ActionType]=None) -> ActionInfo:\n        """"""\n        Given the agents current knowledge, decide on the next action to apply to the environment\n\n        :param action: An action to take, overriding whatever the current policy is\n        :return: An ActionInfo object, which contains the action and any additional info from the action decision process\n        """"""\n        if self.phase == RunPhase.TRAIN and self.ap.algorithm.num_consecutive_playing_steps.num_steps == 0:\n            # This agent never plays  while training (e.g. behavioral cloning)\n            return None\n\n        # count steps (only when training or if we are in the evaluation worker)\n        if self.phase != RunPhase.TEST or self.ap.task_parameters.evaluate_only:\n            self.total_steps_counter += 1\n        self.current_episode_steps_counter += 1\n\n        # decide on the action\n        if action is None:\n            if self.phase == RunPhase.HEATUP and not self.ap.algorithm.heatup_using_network_decisions:\n                # random action\n                action = self.spaces.action.sample_with_info()\n            else:\n                # informed action\n                if self.pre_network_filter is not None:\n                    # before choosing an action, first use the pre_network_filter to filter out the current state\n                    update_filter_internal_state = self.ap.algorithm.update_pre_network_filters_state_on_inference and \\\n                                                   self.phase is not RunPhase.TEST\n                    curr_state = self.run_pre_network_filter_for_inference(self.curr_state, update_filter_internal_state)\n\n                else:\n                    curr_state = self.curr_state\n                action = self.choose_action(curr_state)\n                assert isinstance(action, ActionInfo)\n\n        self.last_action_info = action\n\n        # output filters are explicitly applied after recording self.last_action_info. This is\n        # because the output filters may change the representation of the action so that the agent\n        # can no longer use the transition in it\'s replay buffer. It is possible that these filters\n        # could be moved to the environment instead, but they are here now for historical reasons.\n        filtered_action_info = self.output_filter.filter(self.last_action_info)\n\n        return filtered_action_info\n\n    def run_pre_network_filter_for_inference(self, state: StateType, update_filter_internal_state: bool=True)\\\n            -> StateType:\n        """"""\n        Run filters which where defined for being applied right before using the state for inference.\n\n        :param state: The state to run the filters on\n        :param update_filter_internal_state: Should update the filter\'s internal state - should not update when evaluating\n        :return: The filtered state\n        """"""\n        dummy_env_response = EnvResponse(next_state=state, reward=0, game_over=False)\n\n        # TODO actually we only want to run the observation filters. No point in running the reward filters as the\n        #  filtered reward is being ignored anyway (and it might unncecessarily affect the reward filters\' internal\n        #  state).\n        return self.pre_network_filter.filter(dummy_env_response,\n                                              update_internal_state=update_filter_internal_state)[0].next_state\n\n    def get_state_embedding(self, state: dict) -> np.ndarray:\n        """"""\n        Given a state, get the corresponding state embedding  from the main network\n\n        :param state: a state dict\n        :return: a numpy embedding vector\n        """"""\n        # TODO: this won\'t work anymore\n        # TODO: instead of the state embedding (which contains the goal) we should use the observation embedding\n        embedding = self.networks[\'main\'].online_network.predict(\n            self.prepare_batch_for_inference(state, ""main""),\n            outputs=self.networks[\'main\'].online_network.state_embedding)\n        return embedding\n\n    def update_transition_before_adding_to_replay_buffer(self, transition: Transition) -> Transition:\n        """"""\n        Allows agents to update the transition just before adding it to the replay buffer.\n        Can be useful for agents that want to tweak the reward, termination signal, etc.\n\n        :param transition: the transition to update\n        :return: the updated transition\n        """"""\n        return transition\n\n    def observe(self, env_response: EnvResponse) -> bool:\n        """"""\n        Given a response from the environment, distill the observation from it and store it for later use.\n        The response should be a dictionary containing the performed action, the new observation and measurements,\n        the reward, a game over flag and any additional information necessary.\n\n        :param env_response: result of call from environment.step(action)\n        :return: a boolean value which determines if the agent has decided to terminate the episode after seeing the\n                 given observation\n        """"""\n\n        # filter the env_response\n        filtered_env_response = self.input_filter.filter(env_response)[0]\n\n        # inject agent collected statistics, if required\n        if self.ap.algorithm.use_accumulated_reward_as_measurement:\n            if \'measurements\' in filtered_env_response.next_state:\n                filtered_env_response.next_state[\'measurements\'] = np.append(filtered_env_response.next_state[\'measurements\'],\n                                                                             self.total_shaped_reward_in_current_episode)\n            else:\n                filtered_env_response.next_state[\'measurements\'] = np.array([self.total_shaped_reward_in_current_episode])\n\n        # if we are in the first step in the episode, then we don\'t have a a next state and a reward and thus no\n        # transition yet, and therefore we don\'t need to store anything in the memory.\n        # also we did not reach the goal yet.\n        if self.current_episode_steps_counter == 0:\n            # initialize the current state\n            self.curr_state = filtered_env_response.next_state\n            return env_response.game_over\n        else:\n            transition = Transition(state=copy.copy(self.curr_state), action=self.last_action_info.action,\n                                    reward=filtered_env_response.reward, next_state=filtered_env_response.next_state,\n                                    game_over=filtered_env_response.game_over, info=filtered_env_response.info)\n\n            # now that we have formed a basic transition - the next state progresses to be the current state\n            self.curr_state = filtered_env_response.next_state\n\n            # make agent specific changes to the transition if needed\n            transition = self.update_transition_before_adding_to_replay_buffer(transition)\n\n            # add action info to transition\n            if type(self.parent).__name__ == \'CompositeAgent\':\n                transition.add_info(self.parent.last_action_info.__dict__)\n            else:\n                transition.add_info(self.last_action_info.__dict__)\n\n            self.total_reward_in_current_episode += env_response.reward\n            self.reward.add_sample(env_response.reward)\n\n            return self.observe_transition(transition)\n\n    def observe_transition(self, transition):\n        # sum up the total shaped reward\n        self.total_shaped_reward_in_current_episode += transition.reward\n        self.shaped_reward.add_sample(transition.reward)\n\n        # create and store the transition\n        if self.phase in [RunPhase.TRAIN, RunPhase.HEATUP]:\n            # for episodic memories we keep the transitions in a local buffer until the episode is ended.\n            # for regular memories we insert the transitions directly to the memory\n            self.current_episode_buffer.insert(transition)\n            if not isinstance(self.memory, EpisodicExperienceReplay) \\\n                    and not self.ap.algorithm.store_transitions_only_when_episodes_are_terminated:\n                self.call_memory(\'store\', transition)\n\n        if self.ap.visualization.dump_in_episode_signals:\n            self.update_step_in_episode_log()\n\n        return transition.game_over\n\n    def post_training_commands(self) -> None:\n        """"""\n        A function which allows adding any functionality that is required to run right after the training phase ends.\n\n        :return: None\n        """"""\n        pass\n\n    def get_predictions(self, states: List[Dict[str, np.ndarray]], prediction_type: PredictionType):\n        """"""\n        Get a prediction from the agent with regard to the requested prediction_type.\n        If the agent cannot predict this type of prediction_type, or if there is more than possible way to do so,\n        raise a ValueException.\n\n        :param states: The states to get a prediction for\n        :param prediction_type: The type of prediction to get for the states. For example, the state-value prediction.\n        :return: the predicted values\n        """"""\n\n        predictions = self.networks[\'main\'].online_network.predict_with_prediction_type(\n            # states=self.dict_state_to_batches_dict(states, \'main\'), prediction_type=prediction_type)\n            states=states, prediction_type=prediction_type)\n\n        if len(predictions.keys()) != 1:\n            raise ValueError(""The network has more than one component {} matching the requested prediction_type {}. "".\n                             format(list(predictions.keys()), prediction_type))\n        return list(predictions.values())[0]\n\n    def set_incoming_directive(self, action: ActionType) -> None:\n        """"""\n        Allows setting a directive for the agent to follow. This is useful in hierarchy structures, where the agent\n        has another master agent that is controlling it. In such cases, the master agent can define the goals for the\n        slave agent, define its observation, possible actions, etc. The directive type is defined by the agent\n        in-action-space.\n\n        :param action: The action that should be set as the directive\n        :return:\n        """"""\n        if isinstance(self.in_action_space, GoalsSpace):\n            self.current_hrl_goal = action\n        elif isinstance(self.in_action_space, AttentionActionSpace):\n            self.input_filter.observation_filters[\'attention\'].crop_low = action[0]\n            self.input_filter.observation_filters[\'attention\'].crop_high = action[1]\n            self.output_filter.action_filters[\'masking\'].set_masking(action[0], action[1])\n\n    def save_checkpoint(self, checkpoint_prefix: str) -> None:\n        """"""\n        Allows agents to store additional information when saving checkpoints.\n\n        :param checkpoint_prefix: The prefix of the checkpoint file to save\n        :return: None\n        """"""\n        checkpoint_dir = self.ap.task_parameters.checkpoint_save_dir\n\n        checkpoint_prefix = \'.\'.join([checkpoint_prefix] + self.full_name_id.split(\'/\'))  # adds both level name and agent name\n\n        self.input_filter.save_state_to_checkpoint(checkpoint_dir, checkpoint_prefix)\n        self.output_filter.save_state_to_checkpoint(checkpoint_dir, checkpoint_prefix)\n        self.pre_network_filter.save_state_to_checkpoint(checkpoint_dir, checkpoint_prefix)\n\n    def restore_checkpoint(self, checkpoint_dir: str) -> None:\n        """"""\n        Allows agents to store additional information when saving checkpoints.\n\n        :param checkpoint_dir: The checkpoint dir to restore from\n        :return: None\n        """"""\n        checkpoint_prefix = \'.\'.join(self.full_name_id.split(\'/\'))  # adds both level name and agent name\n        self.input_filter.restore_state_from_checkpoint(checkpoint_dir, checkpoint_prefix)\n        self.pre_network_filter.restore_state_from_checkpoint(checkpoint_dir, checkpoint_prefix)\n\n        # no output filters currently have an internal state to restore\n        # self.output_filter.restore_state_from_checkpoint(checkpoint_dir)\n\n    def sync(self) -> None:\n        """"""\n        Sync the global network parameters to local networks\n\n        :return: None\n        """"""\n        for network in self.networks.values():\n            network.sync()\n\n    def get_success_rate(self) -> float:\n        return self.num_successes_across_evaluation_episodes / self.num_evaluation_episodes_completed\n\n    def collect_savers(self, parent_path_suffix: str) -> SaverCollection:\n        """"""\n        Collect all of agent\'s network savers\n        :param parent_path_suffix: path suffix of the parent of the agent\n        (could be name of level manager or composite agent)\n        :return: collection of all agent savers\n        """"""\n        parent_path_suffix = ""{}.{}"".format(parent_path_suffix, self.name)\n        savers = SaverCollection()\n        for network in self.networks.values():\n            savers.update(network.collect_savers(parent_path_suffix))\n        return savers\n\n    def get_current_time(self):\n        pass\n        return {\n                TimeTypes.EpisodeNumber: self.current_episode,\n                TimeTypes.TrainingIteration: self.training_iteration,\n                TimeTypes.EnvironmentSteps: self.total_steps_counter,\n                TimeTypes.WallClockTime: self.agent_logger.get_current_wall_clock_time(),\n                TimeTypes.Epoch: self.training_epoch}[self.parent_level_manager.parent_graph_manager.time_metric]\n\n    def freeze_memory(self):\n        """"""\n        Shuffle episodes in the memory and freeze it to make sure that no extra data is being pushed anymore.\n        :return: None\n        """"""\n        self.call_memory(\'shuffle_episodes\')\n        self.call_memory(\'freeze\')\n'"
rl_coach/agents/agent_interface.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union, List, Dict\n\nimport numpy as np\n\nfrom rl_coach.core_types import EnvResponse, ActionInfo, RunPhase, PredictionType, ActionType, Transition\nfrom rl_coach.saver import SaverCollection\n\n\nclass AgentInterface(object):\n    def __init__(self):\n        self._phase = RunPhase.HEATUP\n        self._parent = None\n        self.spaces = None\n\n    @property\n    def parent(self):\n        """"""\n        Get the parent class of the agent\n        :return: the current phase\n        """"""\n        return self._parent\n\n    @parent.setter\n    def parent(self, val):\n        """"""\n        Change the parent class of the agent\n        :param val: the new parent\n        :return: None\n        """"""\n        self._parent = val\n\n    @property\n    def phase(self) -> RunPhase:\n        """"""\n        Get the phase of the agent\n        :return: the current phase\n        """"""\n        return self._phase\n\n    @phase.setter\n    def phase(self, val: RunPhase):\n        """"""\n        Change the phase of the agent\n        :param val: the new phase\n        :return: None\n        """"""\n        self._phase = val\n\n    def reset_internal_state(self) -> None:\n        """"""\n        Reset the episode parameters for the agent\n        :return: None\n        """"""\n        raise NotImplementedError("""")\n\n    def train(self) -> Union[float, List]:\n        """"""\n        Train the agents network\n        :return: The loss of the training\n        """"""\n        raise NotImplementedError("""")\n\n    def act(self) -> ActionInfo:\n        """"""\n        Get a decision of the next action to take.\n        The action is dependent on the current state which the agent holds from resetting the environment or from\n        the observe function.\n        :return: A tuple containing the actual action and additional info on the action\n        """"""\n        raise NotImplementedError("""")\n\n    def observe(self, env_response: EnvResponse) -> bool:\n        """"""\n        Gets a response from the environment.\n        Processes this information for later use. For example, create a transition and store it in memory.\n        The action info (a class containing any info the agent wants to store regarding its action decision process) is\n        stored by the agent itself when deciding on the action.\n        :param env_response: a EnvResponse containing the response from the environment\n        :return: a done signal which is based on the agent knowledge. This can be different from the done signal from\n                 the environment. For example, an agent can decide to finish the episode each time it gets some\n                 intrinsic reward\n        """"""\n        raise NotImplementedError("""")\n\n    def save_checkpoint(self, checkpoint_prefix: str) -> None:\n        """"""\n        Save the model of the agent to the disk. This can contain the network parameters, the memory of the agent, etc.\n        :param checkpoint_prefix: The prefix of the checkpoint file to save\n        :return: None\n        """"""\n        raise NotImplementedError("""")\n\n    def get_predictions(self, states: Dict, prediction_type: PredictionType) -> np.ndarray:\n        """"""\n        Get a prediction from the agent with regard to the requested prediction_type. If the agent cannot predict this\n        type of prediction_type, or if there is more than possible way to do so, raise a ValueException.\n        :param states:\n        :param prediction_type:\n        :return: the agent\'s prediction\n        """"""\n        raise NotImplementedError("""")\n\n    def set_incoming_directive(self, action: ActionType) -> None:\n        """"""\n        Pass a higher level command (directive) to the agent.\n        For example, a higher level agent can set the goal of the agent.\n        :param action: the directive to pass to the agent\n        :return: None\n        """"""\n        raise NotImplementedError("""")\n\n    def collect_savers(self, parent_path_suffix: str) -> SaverCollection:\n        """"""\n        Collect all of agent savers\n        :param parent_path_suffix: path suffix of the parent of the agent\n            (could be name of level manager or composite agent)\n        :return: collection of all agent savers\n        """"""\n        raise NotImplementedError("""")\n\n    def handle_episode_ended(self) -> None:\n        """"""\n        Make any changes needed when each episode is ended.\n        This includes incrementing counters, updating full episode dependent values, updating logs, etc.\n        This function is called right after each episode is ended.\n\n        :return: None\n        """"""\n        raise NotImplementedError("""")\n\n    def run_off_policy_evaluation(self) -> None:\n        """"""\n        Run off-policy evaluation estimators to evaluate the trained policy performance against a dataset.\n        Should only be implemented for off-policy RL algorithms.\n\n        :return: None\n        """"""\n        raise NotImplementedError("""")\n'"
rl_coach/agents/bc_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.imitation_agent import ImitationAgent\nfrom rl_coach.architectures.head_parameters import PolicyHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.base_parameters import AgentParameters, AlgorithmParameters, NetworkParameters, \\\n    MiddlewareScheme\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters\n\n\nclass BCAlgorithmParameters(AlgorithmParameters):\n    def __init__(self):\n        super().__init__()\n\n\nclass BCNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters(scheme=MiddlewareScheme.Medium)\n        self.heads_parameters = [PolicyHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 32\n        self.replace_mse_with_huber_loss = False\n        self.create_target_network = False\n\n\nclass BCAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=BCAlgorithmParameters(),\n                         exploration=EGreedyParameters(),\n                         memory=ExperienceReplayParameters(),\n                         networks={""main"": BCNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.bc_agent:BCAgent\'\n\n\n# Behavioral Cloning Agent\nclass BCAgent(ImitationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # When using a policy head, the targets refer to the advantages that we are normally feeding the head with.\n        # In this case, we need the policy head to just predict probabilities, so while we usually train the network\n        # with log(Pi)*Advantages, in this specific case we will train it to log(Pi), which after the softmax will\n        # predict Pi (=probabilities)\n        targets = np.ones(batch.actions().shape[0])\n\n        result = self.networks[\'main\'].train_and_sync_networks({**batch.states(network_keys),\n                                                                \'output_0_0\': batch.actions()},\n                                                               targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n\n'"
rl_coach/agents/bootstrapped_dqn_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.dqn_agent import DQNNetworkParameters, DQNAgentParameters\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.exploration_policies.bootstrapped import BootstrappedParameters\n\n\nclass BootstrappedDQNNetworkParameters(DQNNetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.heads_parameters[0].num_output_head_copies = 10\n        self.heads_parameters[0].rescale_gradient_from_head_by_factor = 1.0/self.heads_parameters[0].num_output_head_copies\n\n\nclass BootstrappedDQNAgentParameters(DQNAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.exploration = BootstrappedParameters()\n        self.network_wrappers = {""main"": BootstrappedDQNNetworkParameters()}\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.bootstrapped_dqn_agent:BootstrappedDQNAgent\'\n\n\n# Bootstrapped DQN - https://arxiv.org/pdf/1602.04621.pdf\nclass BootstrappedDQNAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n    def reset_internal_state(self):\n        super().reset_internal_state()\n        self.exploration_policy.select_head()\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        next_states_online_values = self.networks[\'main\'].online_network.predict(batch.next_states(network_keys))\n        result = self.networks[\'main\'].parallel_prediction([\n            (self.networks[\'main\'].target_network, batch.next_states(network_keys)),\n            (self.networks[\'main\'].online_network, batch.states(network_keys))\n        ])\n        q_st_plus_1 = result[:self.ap.exploration.architecture_num_q_heads]\n        TD_targets = result[self.ap.exploration.architecture_num_q_heads:]\n\n        # add Q value samples for logging\n\n        # initialize with the current prediction so that we will\n        #  only update the action that we have actually done in this transition\n        for i in range(batch.size):\n            mask = batch[i].info[\'mask\']\n            for head_idx in range(self.ap.exploration.architecture_num_q_heads):\n                self.q_values.add_sample(TD_targets[head_idx])\n\n                if mask[head_idx] == 1:\n                    selected_action = np.argmax(next_states_online_values[head_idx][i], 0)\n                    TD_targets[head_idx][i, batch.actions()[i]] = \\\n                        batch.rewards()[i] + (1.0 - batch.game_overs()[i]) * self.ap.algorithm.discount \\\n                                     * q_st_plus_1[head_idx][i][selected_action]\n\n        result = self.networks[\'main\'].train_and_sync_networks(batch.states(network_keys), TD_targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n\n    def observe(self, env_response):\n        mask = np.random.binomial(1, self.ap.exploration.bootstrapped_data_sharing_probability,\n                                  self.ap.exploration.architecture_num_q_heads)\n        env_response.info[\'mask\'] = mask\n        return super().observe(env_response)\n'"
rl_coach/agents/categorical_dqn_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom typing import Union\n\nimport numpy as np\nfrom rl_coach.agents.dqn_agent import DQNNetworkParameters, DQNAlgorithmParameters, DQNAgentParameters\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.architectures.head_parameters import CategoricalQHeadParameters\nfrom rl_coach.core_types import StateType\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.memories.non_episodic.prioritized_experience_replay import PrioritizedExperienceReplay\nfrom rl_coach.schedules import LinearSchedule\n\n\nclass CategoricalDQNNetworkParameters(DQNNetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.heads_parameters = [CategoricalQHeadParameters()]\n\n\nclass CategoricalDQNAlgorithmParameters(DQNAlgorithmParameters):\n    """"""\n    :param v_min: (float)\n        The minimal value that will be represented in the network output for predicting the Q value.\n        Corresponds to :math:`v_{min}` in the paper.\n\n    :param v_max: (float)\n        The maximum value that will be represented in the network output for predicting the Q value.\n        Corresponds to :math:`v_{max}` in the paper.\n\n    :param atoms: (int)\n        The number of atoms that will be used to discretize the range between v_min and v_max.\n        For the C51 algorithm described in the paper, the number of atoms is 51.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.v_min = -10.0\n        self.v_max = 10.0\n        self.atoms = 51\n\n\nclass CategoricalDQNExplorationParameters(EGreedyParameters):\n    def __init__(self):\n        super().__init__()\n        self.epsilon_schedule = LinearSchedule(1, 0.01, 1000000)\n        self.evaluation_epsilon = 0.001\n\n\nclass CategoricalDQNAgentParameters(DQNAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm = CategoricalDQNAlgorithmParameters()\n        self.exploration = CategoricalDQNExplorationParameters()\n        self.network_wrappers = {""main"": CategoricalDQNNetworkParameters()}\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.categorical_dqn_agent:CategoricalDQNAgent\'\n\n\n# Categorical Deep Q Network - https://arxiv.org/pdf/1707.06887.pdf\nclass CategoricalDQNAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.z_values = np.linspace(self.ap.algorithm.v_min, self.ap.algorithm.v_max, self.ap.algorithm.atoms)\n\n    def distribution_prediction_to_q_values(self, prediction):\n        return np.dot(prediction, self.z_values)\n\n    # prediction\'s format is (batch,actions,atoms)\n    def get_all_q_values_for_states(self, states: StateType):\n        q_values = None\n        if self.exploration_policy.requires_action_values():\n            q_values = self.get_prediction(states,\n                                           outputs=[self.networks[\'main\'].online_network.output_heads[0].q_values])\n\n        return q_values\n\n    def get_all_q_values_for_states_and_softmax_probabilities(self, states: StateType):\n        actions_q_values, softmax_probabilities = None, None\n        if self.exploration_policy.requires_action_values():\n            outputs = [self.networks[\'main\'].online_network.output_heads[0].q_values,\n                       self.networks[\'main\'].online_network.output_heads[0].softmax]\n            actions_q_values, softmax_probabilities = self.get_prediction(states, outputs=outputs)\n\n        return actions_q_values, softmax_probabilities\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # for the action we actually took, the error is calculated by the atoms distribution\n        # for all other actions, the error is 0\n        distributional_q_st_plus_1, TD_targets = self.networks[\'main\'].parallel_prediction([\n            (self.networks[\'main\'].target_network, batch.next_states(network_keys)),\n            (self.networks[\'main\'].online_network, batch.states(network_keys))\n        ])\n\n        # add Q value samples for logging\n        self.q_values.add_sample(self.distribution_prediction_to_q_values(TD_targets))\n\n        # select the optimal actions for the next state\n        target_actions = np.argmax(self.distribution_prediction_to_q_values(distributional_q_st_plus_1), axis=1)\n        m = np.zeros((batch.size, self.z_values.size))\n\n        batches = np.arange(batch.size)\n\n        # an alternative to the for loop. 3.7x perf improvement vs. the same code done with for looping.\n        # only 10% speedup overall - leaving commented out as the code is not as clear.\n\n        # tzj_ = np.fmax(np.fmin(batch.rewards() + (1.0 - batch.game_overs()) * self.ap.algorithm.discount *\n        #                        np.transpose(np.repeat(self.z_values[np.newaxis, :], batch.size, axis=0), (1, 0)),\n        #                     self.z_values[-1]),\n        #             self.z_values[0])\n        #\n        # bj_ = (tzj_ - self.z_values[0]) / (self.z_values[1] - self.z_values[0])\n        # u_ = (np.ceil(bj_)).astype(int)\n        # l_ = (np.floor(bj_)).astype(int)\n        # m_ = np.zeros((batch.size, self.z_values.size))\n        # np.add.at(m_, [batches, l_],\n        #           np.transpose(distributional_q_st_plus_1[batches, target_actions], (1, 0)) * (u_ - bj_))\n        # np.add.at(m_, [batches, u_],\n        #           np.transpose(distributional_q_st_plus_1[batches, target_actions], (1, 0)) * (bj_ - l_))\n\n        for j in range(self.z_values.size):\n            tzj = np.fmax(np.fmin(batch.rewards() +\n                                  (1.0 - batch.game_overs()) * self.ap.algorithm.discount * self.z_values[j],\n                                  self.z_values[-1]),\n                          self.z_values[0])\n            bj = (tzj - self.z_values[0])/(self.z_values[1] - self.z_values[0])\n            u = (np.ceil(bj)).astype(int)\n            l = (np.floor(bj)).astype(int)\n            m[batches, l] += (distributional_q_st_plus_1[batches, target_actions, j] * (u - bj))\n            m[batches, u] += (distributional_q_st_plus_1[batches, target_actions, j] * (bj - l))\n\n        # total_loss = cross entropy between actual result above and predicted result for the given action\n        # only update the action that we have actually done in this transition\n        TD_targets[batches, batch.actions()] = m\n\n        # update errors in prioritized replay buffer\n        importance_weights = batch.info(\'weight\') if isinstance(self.memory, PrioritizedExperienceReplay) else None\n\n        result = self.networks[\'main\'].train_and_sync_networks(batch.states(network_keys), TD_targets,\n                                                               importance_weights=importance_weights)\n\n        total_loss, losses, unclipped_grads = result[:3]\n\n        # TODO: fix this spaghetti code\n        if isinstance(self.memory, PrioritizedExperienceReplay):\n            errors = losses[0][np.arange(batch.size), batch.actions()]\n            self.call_memory(\'update_priorities\', (batch.info(\'idx\'), errors))\n\n        return total_loss, losses, unclipped_grads\n\n'"
rl_coach/agents/cil_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nfrom rl_coach.agents.imitation_agent import ImitationAgent\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import RegressionHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AgentParameters, MiddlewareScheme, NetworkParameters, AlgorithmParameters\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.memories.non_episodic.balanced_experience_replay import BalancedExperienceReplayParameters\n\n\nclass CILAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param state_key_with_the_class_index: (str)\n        The key of the state dictionary which corresponds to the value that will be used to control the class index.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.state_key_with_the_class_index = \'high_level_command\'\n\n\nclass CILNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters(scheme=MiddlewareScheme.Medium)\n        self.heads_parameters = [RegressionHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 32\n        self.replace_mse_with_huber_loss = False\n        self.create_target_network = False\n\n\nclass CILAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=CILAlgorithmParameters(),\n                         exploration=EGreedyParameters(),\n                         memory=BalancedExperienceReplayParameters(),\n                         networks={""main"": CILNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.cil_agent:CILAgent\'\n\n\n# Conditional Imitation Learning Agent: https://arxiv.org/abs/1710.02410\nclass CILAgent(ImitationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.current_high_level_control = 0\n\n    def choose_action(self, curr_state):\n        self.current_high_level_control = curr_state[self.ap.algorithm.state_key_with_the_class_index]\n        return super().choose_action(curr_state)\n\n    def extract_action_values(self, prediction):\n        return prediction[self.current_high_level_control].squeeze()\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        target_values = self.networks[\'main\'].online_network.predict({**batch.states(network_keys)})\n\n        branch_to_update = batch.states([self.ap.algorithm.state_key_with_the_class_index])[self.ap.algorithm.state_key_with_the_class_index]\n        for idx, branch in enumerate(branch_to_update):\n            target_values[branch][idx] = batch.actions()[idx]\n\n        result = self.networks[\'main\'].train_and_sync_networks({**batch.states(network_keys)}, target_values)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n'"
rl_coach/agents/clipped_ppo_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom collections import OrderedDict\nfrom random import shuffle\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.actor_critic_agent import ActorCriticAgent\nfrom rl_coach.agents.policy_optimization_agent import PolicyGradientRescaler\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import PPOHeadParameters, VHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, NetworkParameters, \\\n    AgentParameters\nfrom rl_coach.core_types import EnvironmentSteps, Batch, EnvResponse, StateType\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.exploration_policies.categorical import CategoricalParameters\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.schedules import ConstantSchedule\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\n\n\nclass ClippedPPONetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(activation_function=\'tanh\')}\n        self.middleware_parameters = FCMiddlewareParameters(activation_function=\'tanh\')\n        self.heads_parameters = [VHeadParameters(), PPOHeadParameters()]\n        self.batch_size = 64\n        self.optimizer_type = \'Adam\'\n        self.clip_gradients = None\n        self.use_separate_networks_per_head = True\n        self.async_training = False\n        self.l2_regularization = 0\n\n        # The target network is used in order to freeze the old policy, while making updates to the new one\n        # in train_network()\n        self.create_target_network = True\n        self.shared_optimizer = True\n        self.scale_down_gradients_by_number_of_workers_for_sync_training = True\n\n\nclass ClippedPPOAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param policy_gradient_rescaler: (PolicyGradientRescaler)\n        This represents how the critic will be used to update the actor. The critic value function is typically used\n        to rescale the gradients calculated by the actor. There are several ways for doing this, such as using the\n        advantage of the action, or the generalized advantage estimation (GAE) value.\n\n    :param gae_lambda: (float)\n        The :math:`\\lambda` value is used within the GAE function in order to weight different bootstrap length\n        estimations. Typical values are in the range 0.9-1, and define an exponential decay over the different\n        n-step estimations.\n\n    :param clip_likelihood_ratio_using_epsilon: (float)\n        If not None, the likelihood ratio between the current and new policy in the PPO loss function will be\n        clipped to the range [1-clip_likelihood_ratio_using_epsilon, 1+clip_likelihood_ratio_using_epsilon].\n        This is typically used in the Clipped PPO version of PPO, and should be set to None in regular PPO\n        implementations.\n\n    :param value_targets_mix_fraction: (float)\n        The targets for the value network are an exponential weighted moving average which uses this mix fraction to\n        define how much of the new targets will be taken into account when calculating the loss.\n        This value should be set to the range (0,1], where 1 means that only the new targets will be taken into account.\n\n    :param estimate_state_value_using_gae: (bool)\n        If set to True, the state value will be estimated using the GAE technique.\n\n    :param use_kl_regularization: (bool)\n        If set to True, the loss function will be regularized using the KL diveregence between the current and new\n        policy, to bound the change of the policy during the network update.\n\n    :param beta_entropy: (float)\n        An entropy regulaization term can be added to the loss function in order to control exploration. This term\n        is weighted using the :math:`\\beta` value defined by beta_entropy.\n\n    :param optimization_epochs: (int)\n        For each training phase, the collected dataset will be used for multiple epochs, which are defined by the\n        optimization_epochs value.\n\n    :param optimization_epochs: (Schedule)\n        Can be used to define a schedule over the clipping of the likelihood ratio.\n\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.num_episodes_in_experience_replay = 1000000\n        self.policy_gradient_rescaler = PolicyGradientRescaler.GAE\n        self.gae_lambda = 0.95\n        self.use_kl_regularization = False\n        self.clip_likelihood_ratio_using_epsilon = 0.2\n        self.estimate_state_value_using_gae = True\n        self.beta_entropy = 0.01  # should be 0 for mujoco\n        self.num_consecutive_playing_steps = EnvironmentSteps(2048)\n        self.optimization_epochs = 10\n        self.normalization_stats = None\n        self.clipping_decay_schedule = ConstantSchedule(1)\n        self.act_for_full_episodes = True\n        self.update_pre_network_filters_state_on_train = True\n        self.update_pre_network_filters_state_on_inference = False\n\n\nclass ClippedPPOAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=ClippedPPOAlgorithmParameters(),\n                         exploration={DiscreteActionSpace: CategoricalParameters(),\n                                      BoxActionSpace: AdditiveNoiseParameters()},\n                         memory=EpisodicExperienceReplayParameters(),\n                         networks={""main"": ClippedPPONetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.clipped_ppo_agent:ClippedPPOAgent\'\n\n\n# Clipped Proximal Policy Optimization - https://arxiv.org/abs/1707.06347\nclass ClippedPPOAgent(ActorCriticAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        # signals definition\n        self.value_loss = self.register_signal(\'Value Loss\')\n        self.policy_loss = self.register_signal(\'Policy Loss\')\n        self.total_kl_divergence_during_training_process = 0.0\n        self.unclipped_grads = self.register_signal(\'Grads (unclipped)\')\n        self.value_targets = self.register_signal(\'Value Targets\')\n        self.kl_divergence = self.register_signal(\'KL Divergence\')\n        self.likelihood_ratio = self.register_signal(\'Likelihood Ratio\')\n        self.clipped_likelihood_ratio = self.register_signal(\'Clipped Likelihood Ratio\')\n\n    def set_session(self, sess):\n        super().set_session(sess)\n        if self.ap.algorithm.normalization_stats is not None:\n            self.ap.algorithm.normalization_stats.set_session(sess)\n\n    def fill_advantages(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        current_state_values = self.networks[\'main\'].online_network.predict(batch.states(network_keys))[0]\n        current_state_values = current_state_values.squeeze()\n        self.state_values.add_sample(current_state_values)\n\n        # calculate advantages\n        advantages = []\n        value_targets = []\n        total_returns = batch.n_step_discounted_rewards()\n\n        if self.policy_gradient_rescaler == PolicyGradientRescaler.A_VALUE:\n            advantages = total_returns - current_state_values\n        elif self.policy_gradient_rescaler == PolicyGradientRescaler.GAE:\n            # get bootstraps\n            episode_start_idx = 0\n            advantages = np.array([])\n            value_targets = np.array([])\n            for idx, game_over in enumerate(batch.game_overs()):\n                if game_over:\n                    # get advantages for the rollout\n                    value_bootstrapping = np.zeros((1,))\n                    rollout_state_values = np.append(current_state_values[episode_start_idx:idx+1], value_bootstrapping)\n\n                    rollout_advantages, gae_based_value_targets = \\\n                        self.get_general_advantage_estimation_values(batch.rewards()[episode_start_idx:idx+1],\n                                                                     rollout_state_values)\n                    episode_start_idx = idx + 1\n                    advantages = np.append(advantages, rollout_advantages)\n                    value_targets = np.append(value_targets, gae_based_value_targets)\n        else:\n            screen.warning(""WARNING: The requested policy gradient rescaler is not available"")\n\n        # standardize\n        advantages = (advantages - np.mean(advantages)) / np.std(advantages)\n\n        for transition, advantage, value_target in zip(batch.transitions, advantages, value_targets):\n            transition.info[\'advantage\'] = advantage\n            transition.info[\'gae_based_value_target\'] = value_target\n\n        self.action_advantages.add_sample(advantages)\n\n    def train_network(self, batch, epochs):\n        batch_results = []\n        for j in range(epochs):\n            batch.shuffle()\n            batch_results = {\n                \'total_loss\': [],\n                \'losses\': [],\n                \'unclipped_grads\': [],\n                \'kl_divergence\': [],\n                \'entropy\': []\n            }\n\n            fetches = [self.networks[\'main\'].online_network.output_heads[1].kl_divergence,\n                       self.networks[\'main\'].online_network.output_heads[1].entropy,\n                       self.networks[\'main\'].online_network.output_heads[1].likelihood_ratio,\n                       self.networks[\'main\'].online_network.output_heads[1].clipped_likelihood_ratio]\n\n            # TODO-fixme if batch.size / self.ap.network_wrappers[\'main\'].batch_size is not an integer, we do not train on\n            #  some of the data\n            for i in range(int(batch.size / self.ap.network_wrappers[\'main\'].batch_size)):\n                start = i * self.ap.network_wrappers[\'main\'].batch_size\n                end = (i + 1) * self.ap.network_wrappers[\'main\'].batch_size\n\n                network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n                actions = batch.actions()[start:end]\n                gae_based_value_targets = batch.info(\'gae_based_value_target\')[start:end]\n                if not isinstance(self.spaces.action, DiscreteActionSpace) and len(actions.shape) == 1:\n                    actions = np.expand_dims(actions, -1)\n\n                # get old policy probabilities and distribution\n\n                # TODO-perf - the target network (""old_policy"") is not changing. this can be calculated once for all epochs.\n                # the shuffling being done, should only be performed on the indices.\n                result = self.networks[\'main\'].target_network.predict({k: v[start:end] for k, v in batch.states(network_keys).items()})\n                old_policy_distribution = result[1:]\n\n                total_returns = batch.n_step_discounted_rewards(expand_dims=True)\n\n                # calculate gradients and apply on both the local policy network and on the global policy network\n                if self.ap.algorithm.estimate_state_value_using_gae:\n                    value_targets = np.expand_dims(gae_based_value_targets, -1)\n                else:\n                    value_targets = total_returns[start:end]\n\n                inputs = copy.copy({k: v[start:end] for k, v in batch.states(network_keys).items()})\n                inputs[\'output_1_0\'] = actions\n\n                # The old_policy_distribution needs to be represented as a list, because in the event of\n                # discrete controls, it has just a mean. otherwise, it has both a mean and standard deviation\n                for input_index, input in enumerate(old_policy_distribution):\n                    inputs[\'output_1_{}\'.format(input_index + 1)] = input\n\n                # update the clipping decay schedule value\n                inputs[\'output_1_{}\'.format(len(old_policy_distribution)+1)] = \\\n                    self.ap.algorithm.clipping_decay_schedule.current_value\n\n                total_loss, losses, unclipped_grads, fetch_result = \\\n                    self.networks[\'main\'].train_and_sync_networks(\n                        inputs, [value_targets, batch.info(\'advantage\')[start:end]], additional_fetches=fetches\n                    )\n\n                batch_results[\'total_loss\'].append(total_loss)\n                batch_results[\'losses\'].append(losses)\n                batch_results[\'unclipped_grads\'].append(unclipped_grads)\n                batch_results[\'kl_divergence\'].append(fetch_result[0])\n                batch_results[\'entropy\'].append(fetch_result[1])\n\n                self.unclipped_grads.add_sample(unclipped_grads)\n                self.value_targets.add_sample(value_targets)\n                self.likelihood_ratio.add_sample(fetch_result[2])\n                self.clipped_likelihood_ratio.add_sample(fetch_result[3])\n\n            for key in batch_results.keys():\n                batch_results[key] = np.mean(batch_results[key], 0)\n\n            self.value_loss.add_sample(batch_results[\'losses\'][0])\n            self.policy_loss.add_sample(batch_results[\'losses\'][1])\n            self.loss.add_sample(batch_results[\'total_loss\'])\n\n            if self.ap.network_wrappers[\'main\'].learning_rate_decay_rate != 0:\n                curr_learning_rate = self.networks[\'main\'].online_network.get_variable_value(\n                    self.networks[\'main\'].online_network.adaptive_learning_rate_scheme)\n                self.curr_learning_rate.add_sample(curr_learning_rate)\n            else:\n                curr_learning_rate = self.ap.network_wrappers[\'main\'].learning_rate\n\n            # log training parameters\n            screen.log_dict(\n                OrderedDict([\n                    (""Surrogate loss"", batch_results[\'losses\'][1]),\n                    (""KL divergence"", batch_results[\'kl_divergence\']),\n                    (""Entropy"", batch_results[\'entropy\']),\n                    (""training epoch"", j),\n                    (""learning_rate"", curr_learning_rate)\n                ]),\n                prefix=""Policy training""\n            )\n\n        self.total_kl_divergence_during_training_process = batch_results[\'kl_divergence\']\n        self.entropy.add_sample(batch_results[\'entropy\'])\n        self.kl_divergence.add_sample(batch_results[\'kl_divergence\'])\n        return batch_results[\'losses\']\n\n    def post_training_commands(self):\n        # clean memory\n        self.call_memory(\'clean\')\n\n    def train(self):\n        if self._should_train():\n            for network in self.networks.values():\n                network.set_is_training(True)\n\n            dataset = self.memory.transitions\n            update_internal_state = self.ap.algorithm.update_pre_network_filters_state_on_train\n            dataset = self.pre_network_filter.filter(dataset, deep_copy=False,\n                                                     update_internal_state=update_internal_state)\n            batch = Batch(dataset)\n\n            for training_step in range(self.ap.algorithm.num_consecutive_training_steps):\n                self.networks[\'main\'].sync()\n                self.fill_advantages(batch)\n\n                # take only the requested number of steps\n                if isinstance(self.ap.algorithm.num_consecutive_playing_steps, EnvironmentSteps):\n                    dataset = dataset[:self.ap.algorithm.num_consecutive_playing_steps.num_steps]\n                shuffle(dataset)\n                batch = Batch(dataset)\n\n                self.train_network(batch, self.ap.algorithm.optimization_epochs)\n\n            for network in self.networks.values():\n                network.set_is_training(False)\n\n            self.post_training_commands()\n            self.training_iteration += 1\n            # should be done in order to update the data that has been accumulated * while not playing *\n            self.update_log()\n            return None\n\n    def run_pre_network_filter_for_inference(self, state: StateType, update_internal_state: bool=False):\n        dummy_env_response = EnvResponse(next_state=state, reward=0, game_over=False)\n        update_internal_state = self.ap.algorithm.update_pre_network_filters_state_on_inference\n        return self.pre_network_filter.filter(\n            dummy_env_response, update_internal_state=update_internal_state)[0].next_state\n\n    def choose_action(self, curr_state):\n        self.ap.algorithm.clipping_decay_schedule.step()\n        return super().choose_action(curr_state)\n\n'"
rl_coach/agents/composite_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nimport itertools\nfrom enum import Enum\nfrom typing import Union, List, Dict\n\nimport numpy as np\n\nfrom rl_coach.agents.agent_interface import AgentInterface\nfrom rl_coach.base_parameters import AgentParameters, VisualizationParameters\nfrom rl_coach.core_types import ActionInfo, EnvResponse, ActionType, RunPhase\nfrom rl_coach.filters.observation.observation_crop_filter import ObservationCropFilter\nfrom rl_coach.saver import SaverCollection\nfrom rl_coach.spaces import ActionSpace\nfrom rl_coach.spaces import AgentSelection, AttentionActionSpace, SpacesDefinition\nfrom rl_coach.utils import short_dynamic_import\n\n\nclass DecisionPolicy(object):\n    def choose_action(self, actions_info: Dict[str, ActionInfo]) -> ActionInfo:\n        """"""\n        Given a list of actions from multiple agents, decide on a single action to take.\n        :param actions_info: a dictionary of agent names and their corresponding\n                             ActionInfo instances containing information for each agents action\n        :return: a single action and the corresponding action info\n        """"""\n        raise NotImplementedError("""")\n\n\nclass SingleDecider(DecisionPolicy):\n    """"""\n    A decision policy that chooses the action according to the agent that is currently in control.\n    """"""\n    def __init__(self, default_decision_maker: str):\n        super().__init__()\n        self._decision_maker = default_decision_maker\n\n    @property\n    def decision_maker(self):\n        """"""\n        Get the decision maker that was set by the upper level control.\n        """"""\n        return self._decision_maker\n\n    @decision_maker.setter\n    def decision_maker(self, decision_maker: str):\n        """"""\n        Set the decision maker by the upper level control.\n        :param action: the incoming action from the upper level control.\n        """"""\n        self._decision_maker = decision_maker\n\n    def choose_action(self, actions_info: Dict[str, ActionInfo]) -> ActionInfo:\n        """"""\n        Given a list of actions from multiple agents, take the action of the current decision maker\n        :param actions_info: a list of ActionInfo instances containing the information for each agents action\n        :return: a single action\n        """"""\n        if self.decision_maker not in actions_info.keys():\n            raise ValueError(""The current decision maker ({}) does not exist in the given actions ({})""\n                             .format(self.decision_maker, actions_info.keys()))\n        return actions_info[self.decision_maker]\n\n\nclass RoundRobin(DecisionPolicy):\n    """"""\n    A decision policy that chooses the action according to agents selected in a circular order.\n    """"""\n    def __init__(self, num_agents: int):\n        super().__init__()\n        self.round_robin = itertools.cycle(range(num_agents))\n\n    def choose_action(self, actions_info: Dict[str, ActionInfo]) -> ActionInfo:\n        """"""\n        Given a list of actions from multiple agents, take the action of the current decision maker, which is set in a\n         circular order\n        :param actions_info: a list of ActionInfo instances containing the information for each agents action\n        :return: a single action\n        """"""\n        decision_maker = self.round_robin.__next__()\n        if decision_maker not in range(len(actions_info.keys())):\n            raise ValueError(""The size of action_info does not match the number of agents set to RoundRobin decision""\n                             "" policy."")\n        return actions_info.items()[decision_maker]\n\n\nclass MajorityVote(DecisionPolicy):\n    """"""\n    A decision policy that chooses the action that most of the agents chose.\n    This policy is only useful for discrete control.\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def choose_action(self, actions_info: Dict[str, ActionInfo]) -> ActionInfo:\n        """"""\n        Given a list of actions from multiple agents, take the action that most agents agree on\n        :param actions_info: a list of ActionInfo instances containing the information for each agents action\n        :return: a single action\n        """"""\n        # TODO: enforce discrete action spaces\n        if len(actions_info.keys()) == 0:\n            raise ValueError(""The given list of actions is empty"")\n        vote_count = np.bincount([action_info.action for action_info in actions_info.values()])\n        majority_vote = np.argmax(vote_count)\n        return actions_info.items()[majority_vote]\n\n\nclass MeanDecision(DecisionPolicy):\n    """"""\n    A decision policy that takes the mean action given the actions of all the agents.\n    This policy is only useful for continuous control.\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def choose_action(self, actions_info: Dict[str, ActionInfo]) -> ActionInfo:\n        """"""\n        Given a list of actions from multiple agents, take the mean action\n        :param actions_info: a list of ActionInfo instances containing the information for each agents action\n        :return: a single action\n        """"""\n        # TODO: enforce continuous action spaces\n        if len(actions_info.keys()) == 0:\n            raise ValueError(""The given list of actions is empty"")\n        mean = np.mean([action_info.action for action_info in actions_info.values()], axis=0)\n        return ActionInfo(mean)\n\n\nclass RewardPolicy(Enum):\n    ReachingGoal = 0\n    NativeEnvironmentReward = 1\n    AccumulatedEnvironmentRewards = 2\n\n\nclass CompositeAgent(AgentInterface):\n    """"""\n    A CompositeAgent is a group of agents in the same hierarchy level.\n    In a CompositeAgent, each agent may take the role of either a controller or an observer.\n    Each agent that is defined as observer, gets observations from the environment.\n    Each agent that is defined as controller, can potentially also control the environment, in addition to observing it.\n    There are several ways to decide on the action from different controller agents:\n    1. Ensemble -\n        - Take the majority vote (discrete controls)\n        - Take the mean action (continuous controls)\n        - Round robin between the agents (discrete/continuous)\n    2. Skills -\n        - At each step a single agent decides (Chosen by the uppoer hierarchy controlling agent)\n\n    A CompositeAgent can be controlled using one of the following methods (ActionSpaces):\n    1. Goals (in terms of measurements, observation, embedding or a change in those values)\n    2. Agent Selection (skills) / Discrete action space.\n    3. Attention (a subset of the real environment observation / action space)\n    """"""\n    def __init__(self,\n                 agents_parameters: Union[AgentParameters, Dict[str, AgentParameters]],\n                 visualization_parameters: VisualizationParameters,\n                 decision_policy: DecisionPolicy,\n                 out_action_space: ActionSpace,\n                 in_action_space: Union[None, ActionSpace]=None,\n                 decision_makers: Union[bool, Dict[str, bool]]=True,\n                 reward_policy: RewardPolicy=RewardPolicy.NativeEnvironmentReward,\n                 name=""CompositeAgent""):\n        """"""\n        Construct an agent group\n        :param agents_parameters: a list of presets describing each one of the agents in the group\n        :param decision_policy: the decision policy of the group which describes how actions are consolidated\n        :param out_action_space: the type of action space that is used by this composite agent in order to control the\n                                 underlying environment\n        :param in_action_space: the type of action space that is used by the upper level agent in order to control this\n                                group\n        :param decision_makers: a list of booleans representing for each corresponding agent if it has a decision\n                                privilege or if it is just an observer\n        :param reward_policy: the type of the reward that the group receives\n        """"""\n        super().__init__()\n\n        if isinstance(agents_parameters, AgentParameters):\n            decision_makers = {agents_parameters.name: True}\n            agents_parameters = {agents_parameters.name: agents_parameters}\n        self.agents_parameters = agents_parameters\n        self.visualization_parameters = visualization_parameters\n        self.decision_makers = decision_makers\n        self.decision_policy = decision_policy\n        self.in_action_space = in_action_space\n        self.out_action_space = out_action_space  # TODO: this is not being used\n        self.reward_policy = reward_policy\n        self.full_name_id = self.name = name\n        self.current_decision_maker = 0\n        self.environment = None\n        self.agents = {}  # key = agent_name, value = agent\n        self.incoming_action = None\n        self.last_state = None\n        self._phase = RunPhase.HEATUP\n        self.last_action_info = None\n        self.current_episode = 0\n        self.parent_level_manager = None\n\n        # environment spaces\n        self.spaces = None\n\n        # counters for logging\n        self.total_steps_counter = 0\n        self.current_episode_steps_counter = 0\n        self.total_reward_in_current_episode = 0\n\n        # validate input\n        if set(self.decision_makers) != set(self.agents_parameters):\n            raise ValueError(""The decision_makers dictionary keys does not match the names of the given agents"")\n        if sum(self.decision_makers.values()) > 1 and type(self.decision_policy) == SingleDecider \\\n                and type(self.in_action_space) != AgentSelection:\n            raise ValueError(""When the control policy is set to single decider, the master policy should control the""\n                             ""agent group via agent selection (ControlType.AgentSelection)"")\n\n    @property\n    def parent(self):\n        """"""\n        Get the parent class of the composite agent\n        :return: the current phase\n        """"""\n        return self._parent\n\n    @parent.setter\n    def parent(self, val):\n        """"""\n        Change the parent class of the composite agent.\n        Additionally, updates the full name of the agent\n        :param val: the new parent\n        :return: None\n        """"""\n        self._parent = val\n        if not hasattr(self._parent, \'name\'):\n            raise ValueError(""The parent of a composite agent must have a name"")\n        self.full_name_id = ""{}/{}"".format(self._parent.name, self.name)\n\n    def create_agents(self):\n        for agent_name, agent_parameters in self.agents_parameters.items():\n            agent_parameters.name = agent_name\n\n            # create agent\n            self.agents[agent_parameters.name] = short_dynamic_import(agent_parameters.path)(agent_parameters,\n                                                                                             parent=self)\n            self.agents[agent_parameters.name].parent_level_manager = self.parent_level_manager\n\n        # TODO: this is a bit too specific to be defined here\n        # add an attention cropping filter if the incoming directives are attention boxes\n        if isinstance(self.in_action_space, AttentionActionSpace):\n            attention_size = self.in_action_space.forced_attention_size\n            for agent in self.agents.values():\n                agent.input_filter.observation_filters[\'attention\'] = \\\n                    ObservationCropFilter(crop_low=np.zeros_like(attention_size), crop_high=attention_size)\n                agent.input_filter.observation_filters.move_to_end(\'attention\', last=False)  # add the cropping at the beginning\n\n    def setup_logger(self) -> None:\n        """"""\n        Setup the logger for all the agents in the composite agent\n        :return: None\n        """"""\n        [agent.setup_logger() for agent in self.agents.values()]\n\n    def set_session(self, sess) -> None:\n        """"""\n        Set the deep learning framework session for all the agents in the composite agent\n        :return: None\n        """"""\n        [agent.set_session(sess) for agent in self.agents.values()]\n\n    def set_environment_parameters(self, spaces: SpacesDefinition):\n        """"""\n        Sets the parameters that are environment dependent. As a side effect, initializes all the components that are\n        dependent on those values, by calling init_environment_dependent_modules\n        :param spaces: the definitions of all the spaces of the environment\n        :return: None\n        """"""\n        self.spaces = copy.deepcopy(spaces)\n        [agent.set_environment_parameters(self.spaces) for agent in self.agents.values()]\n\n    @property\n    def phase(self):\n        return self._phase\n\n    @phase.setter\n    def phase(self, val: RunPhase) -> None:\n        """"""\n        Change the current phase of all the agents in the group\n        :param phase: the new phase\n        :return: None\n        """"""\n        self._phase = val\n        for agent in self.agents.values():\n            agent.phase = val\n\n    def handle_episode_ended(self) -> None:\n        """"""\n        Make any changes needed when each episode is ended.\n        This includes incrementing counters, updating full episode dependent values, updating logs, etc.\n        This function is called right after each episode is ended.\n\n        :return: None\n        """"""\n        self.current_episode += 1\n        [agent.handle_episode_ended() for agent in self.agents.values()]\n\n    def reset_internal_state(self) -> None:\n        """"""\n        Reset the episode for all the agents in the group\n        :return: None\n        """"""\n        # update counters\n        self.total_steps_counter = 0\n        self.current_episode_steps_counter = 0\n        self.total_reward_in_current_episode = 0\n\n        # reset all sub modules\n        [agent.reset_internal_state() for agent in self.agents.values()]\n\n    def train(self) -> Union[float, List]:\n        """"""\n        Make a single training step for all the agents of the group\n        :return: a list of loss values from the training step\n        """"""\n        return [agent.train() for agent in self.agents.values()]\n\n    def act(self) -> ActionInfo:\n        """"""\n        Get the actions from all the agents in the group. Then use the decision policy in order to\n        extract a single action out of the list of actions.\n        :return: the chosen action and its corresponding information\n        """"""\n\n        # update counters\n        self.total_steps_counter += 1\n        self.current_episode_steps_counter += 1\n\n        # get the actions info from all the agents\n        actions_info = {}\n        for agent_name, agent in self.agents.items():\n            action_info = agent.act()\n            actions_info[agent_name] = action_info\n\n        # decide on a single action to apply to the environment\n        action_info = self.decision_policy.choose_action(actions_info)\n\n        # TODO: make the last action info a property?\n        # pass the action info to all the observers\n        for agent_name, is_decision_maker in self.decision_makers.items():\n            if not is_decision_maker:\n                self.agents[agent_name].last_action_info = action_info\n        self.last_action_info = action_info\n\n        return self.last_action_info\n\n    def observe(self, env_response: EnvResponse) -> bool:\n        """"""\n        Given a response from the environment as a env_response, filter it and pass it to the agents.\n        This method has two main jobs:\n        1. Wrap the previous transition, ending with the new observation coming from EnvResponse.\n        2. Save the next_state as the current_state to take action upon for the next call to act().\n\n        :param env_response:\n        :param action_info: additional info about the chosen action\n        :return:\n        """"""\n\n        # accumulate the unfiltered rewards for visualization\n        self.total_reward_in_current_episode += env_response.reward\n\n        episode_ended = env_response.game_over\n\n        # pass the env_response to all the sub-agents\n        # TODO: what if one agent decides to end the episode but the others don\'t? who decides?\n        for agent_name, agent in self.agents.items():\n            goal_reached = agent.observe(env_response)\n            episode_ended = episode_ended or goal_reached\n\n        # TODO: unlike for a single agent, here we also treat a game over by the environment.\n        # probably better to only return the agents\' goal_reached decisions.\n        return episode_ended\n\n    def save_checkpoint(self, checkpoint_prefix: str) -> None:\n        [agent.save_checkpoint(checkpoint_prefix) for agent in self.agents.values()]\n\n    def restore_checkpoint(self, checkpoint_dir: str) -> None:\n        [agent.restore_checkpoint(checkpoint_dir) for agent in self.agents.values()]\n\n    def set_incoming_directive(self, action: ActionType) -> None:\n        self.incoming_action = action\n        if isinstance(self.decision_policy, SingleDecider) and isinstance(self.in_action_space, AgentSelection):\n            self.decision_policy.decision_maker = list(self.agents.keys())[action]\n        if isinstance(self.in_action_space, AttentionActionSpace):\n            # TODO: redesign to be more modular\n            for agent in self.agents.values():\n                agent.input_filter.observation_filters[\'attention\'].crop_low = action[0]\n                agent.input_filter.observation_filters[\'attention\'].crop_high = action[1]\n                agent.output_filter.action_filters[\'masking\'].set_masking(action[0], action[1])\n\n        # TODO  rethink this scheme. we don\'t want so many if else clauses lying around here. \xc2\xa0\n        # TODO - for incoming actions which do not involve setting the acting agent we should change the\n        #  observation_space, goal to pursue, etc accordingly to the incoming action.\n\n    def sync(self) -> None:\n        """"""\n        Sync the agent networks with the global network\n        :return:\n        """"""\n        [agent.sync() for agent in self.agents.values()]\n\n    def collect_savers(self, parent_path_suffix: str) -> SaverCollection:\n        """"""\n        Collect all of agent\'s network savers\n        :param parent_path_suffix: path suffix of the parent of the agent\n            (could be name of level manager or composite agent)\n        :return: collection of all agent savers\n        """"""\n        savers = SaverCollection()\n        for agent in self.agents.values():\n            savers.update(agent.collect_savers(\n                parent_path_suffix=""{}.{}"".format(parent_path_suffix, self.name)))\n        return savers\n\n'"
rl_coach/agents/ddpg_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom typing import Union\nfrom collections import OrderedDict\n\nimport numpy as np\n\nfrom rl_coach.agents.actor_critic_agent import ActorCriticAgent\nfrom rl_coach.agents.agent import Agent\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import DDPGActorHeadParameters, DDPGVHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import NetworkParameters, AlgorithmParameters, \\\n    AgentParameters, EmbedderScheme\nfrom rl_coach.core_types import ActionInfo, EnvironmentSteps\nfrom rl_coach.exploration_policies.ou_process import OUProcessParameters\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.spaces import BoxActionSpace, GoalsSpace\n\n\nclass DDPGCriticNetworkParameters(NetworkParameters):\n    def __init__(self, use_batchnorm=False):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(batchnorm=use_batchnorm),\n                                            \'action\': InputEmbedderParameters(scheme=EmbedderScheme.Shallow)}\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.heads_parameters = [DDPGVHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 64\n        self.async_training = False\n        self.learning_rate = 0.001\n        self.adam_optimizer_beta2 = 0.999\n        self.optimizer_epsilon = 1e-8\n        self.create_target_network = True\n        self.shared_optimizer = True\n        self.scale_down_gradients_by_number_of_workers_for_sync_training = False\n        # self.l2_regularization = 1e-2\n\n\nclass DDPGActorNetworkParameters(NetworkParameters):\n    def __init__(self, use_batchnorm=False):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(batchnorm=use_batchnorm)}\n        self.middleware_parameters = FCMiddlewareParameters(batchnorm=use_batchnorm)\n        self.heads_parameters = [DDPGActorHeadParameters(batchnorm=use_batchnorm)]\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 64\n        self.adam_optimizer_beta2 = 0.999\n        self.optimizer_epsilon = 1e-8\n        self.async_training = False\n        self.learning_rate = 0.0001\n        self.create_target_network = True\n        self.shared_optimizer = True\n        self.scale_down_gradients_by_number_of_workers_for_sync_training = False\n\n\nclass DDPGAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param num_steps_between_copying_online_weights_to_target: (StepMethod)\n        The number of steps between copying the online network weights to the target network weights.\n\n    :param rate_for_copying_weights_to_target: (float)\n        When copying the online network weights to the target network weights, a soft update will be used, which\n        weight the new online network weights by rate_for_copying_weights_to_target\n\n    :param num_consecutive_playing_steps: (StepMethod)\n        The number of consecutive steps to act between every two training iterations\n\n    :param use_target_network_for_evaluation: (bool)\n        If set to True, the target network will be used for predicting the actions when choosing actions to act.\n        Since the target network weights change more slowly, the predicted actions will be more consistent.\n\n    :param action_penalty: (float)\n        The amount by which to penalize the network on high action feature (pre-activation) values.\n        This can prevent the actions features from saturating the TanH activation function, and therefore prevent the\n        gradients from becoming very low.\n\n    :param clip_critic_targets: (Tuple[float, float] or None)\n        The range to clip the critic target to in order to prevent overestimation of the action values.\n\n    :param use_non_zero_discount_for_terminal_states: (bool)\n        If set to True, the discount factor will be used for terminal states to bootstrap the next predicted state\n        values. If set to False, the terminal states reward will be taken as the target return for the network.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(1)\n        self.rate_for_copying_weights_to_target = 0.001\n        self.num_consecutive_playing_steps = EnvironmentSteps(1)\n        self.use_target_network_for_evaluation = False\n        self.action_penalty = 0\n        self.clip_critic_targets = None  # expected to be a tuple of the form (min_clip_value, max_clip_value) or None\n        self.use_non_zero_discount_for_terminal_states = False\n\n\nclass DDPGAgentParameters(AgentParameters):\n    def __init__(self, use_batchnorm=False):\n        super().__init__(algorithm=DDPGAlgorithmParameters(),\n                         exploration=OUProcessParameters(),\n                         memory=EpisodicExperienceReplayParameters(),\n                         networks=OrderedDict([(""actor"", DDPGActorNetworkParameters(use_batchnorm=use_batchnorm)),\n                                               (""critic"", DDPGCriticNetworkParameters(use_batchnorm=use_batchnorm))]))\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.ddpg_agent:DDPGAgent\'\n\n\n# Deep Deterministic Policy Gradients Network - https://arxiv.org/pdf/1509.02971.pdf\nclass DDPGAgent(ActorCriticAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n        self.q_values = self.register_signal(""Q"")\n        self.TD_targets_signal = self.register_signal(""TD targets"")\n        self.action_signal = self.register_signal(""actions"")\n\n    def learn_from_batch(self, batch):\n        actor = self.networks[\'actor\']\n        critic = self.networks[\'critic\']\n\n        actor_keys = self.ap.network_wrappers[\'actor\'].input_embedders_parameters.keys()\n        critic_keys = self.ap.network_wrappers[\'critic\'].input_embedders_parameters.keys()\n\n        # TD error = r + discount*max(q_st_plus_1) - q_st\n        next_actions, actions_mean = actor.parallel_prediction([\n            (actor.target_network, batch.next_states(actor_keys)),\n            (actor.online_network, batch.states(actor_keys))\n        ])\n\n        critic_inputs = copy.copy(batch.next_states(critic_keys))\n        critic_inputs[\'action\'] = next_actions\n        q_st_plus_1 = critic.target_network.predict(critic_inputs)[0]\n\n        # calculate the bootstrapped TD targets while discounting terminal states according to\n        # use_non_zero_discount_for_terminal_states\n        if self.ap.algorithm.use_non_zero_discount_for_terminal_states:\n            TD_targets = batch.rewards(expand_dims=True) + self.ap.algorithm.discount * q_st_plus_1\n        else:\n            TD_targets = batch.rewards(expand_dims=True) + \\\n                         (1.0 - batch.game_overs(expand_dims=True)) * self.ap.algorithm.discount * q_st_plus_1\n\n        # clip the TD targets to prevent overestimation errors\n        if self.ap.algorithm.clip_critic_targets:\n            TD_targets = np.clip(TD_targets, *self.ap.algorithm.clip_critic_targets)\n\n        self.TD_targets_signal.add_sample(TD_targets)\n\n        # get the gradients of the critic output with respect to the action\n        critic_inputs = copy.copy(batch.states(critic_keys))\n        critic_inputs[\'action\'] = actions_mean\n        action_gradients = critic.online_network.predict(critic_inputs,\n                                                         outputs=critic.online_network.gradients_wrt_inputs[1][\'action\'])\n\n        # train the critic\n        critic_inputs = copy.copy(batch.states(critic_keys))\n        critic_inputs[\'action\'] = batch.actions(len(batch.actions().shape) == 1)\n\n        # also need the inputs for when applying gradients so batchnorm\'s update of running mean and stddev will work\n        result = critic.train_and_sync_networks(critic_inputs, TD_targets, use_inputs_for_apply_gradients=True)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        # apply the gradients from the critic to the actor\n        initial_feed_dict = {actor.online_network.gradients_weights_ph[0]: -action_gradients}\n        gradients = actor.online_network.predict(batch.states(actor_keys),\n                                                 outputs=actor.online_network.weighted_gradients[0],\n                                                 initial_feed_dict=initial_feed_dict)\n\n        # also need the inputs for when applying gradients so batchnorm\'s update of running mean and stddev will work\n        if actor.has_global:\n            actor.apply_gradients_to_global_network(gradients, additional_inputs=copy.copy(batch.states(critic_keys)))\n            actor.update_online_network()\n        else:\n            actor.apply_gradients_to_online_network(gradients, additional_inputs=copy.copy(batch.states(critic_keys)))\n\n        return total_loss, losses, unclipped_grads\n\n    def train(self):\n        return Agent.train(self)\n\n    def choose_action(self, curr_state):\n        if not (isinstance(self.spaces.action, BoxActionSpace) or isinstance(self.spaces.action, GoalsSpace)):\n            raise ValueError(""DDPG works only for continuous control problems"")\n        # convert to batch so we can run it through the network\n        tf_input_state = self.prepare_batch_for_inference(curr_state, \'actor\')\n        if self.ap.algorithm.use_target_network_for_evaluation:\n            actor_network = self.networks[\'actor\'].target_network\n        else:\n            actor_network = self.networks[\'actor\'].online_network\n\n        action_values = actor_network.predict(tf_input_state).squeeze()\n\n        action = self.exploration_policy.get_action(action_values)\n\n        self.action_signal.add_sample(action)\n\n        # get q value\n        tf_input_state = self.prepare_batch_for_inference(curr_state, \'critic\')\n        action_batch = np.expand_dims(action, 0)\n        if type(action) != np.ndarray:\n            action_batch = np.array([[action]])\n        tf_input_state[\'action\'] = action_batch\n        q_value = self.networks[\'critic\'].online_network.predict(tf_input_state)[0]\n        self.q_values.add_sample(q_value)\n\n        action_info = ActionInfo(action=action,\n                                 action_value=q_value)\n\n        return action_info'"
rl_coach/agents/ddqn_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\nfrom rl_coach.agents.dqn_agent import DQNAgent, DQNAgentParameters\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.schedules import LinearSchedule\n\n\nclass DDQNAgentParameters(DQNAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(30000)\n        self.exploration.epsilon_schedule = LinearSchedule(1, 0.01, 1000000)\n        self.exploration.evaluation_epsilon = 0.001\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.ddqn_agent:DDQNAgent\'\n\n\n# Double DQN - https://arxiv.org/abs/1509.06461\nclass DDQNAgent(DQNAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n    def select_actions(self, next_states, q_st_plus_1):\n        return np.argmax(self.networks[\'main\'].online_network.predict(next_states), 1)\n\n'"
rl_coach/agents/ddqn_bcq_agent.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom collections import OrderedDict\n\nfrom copy import deepcopy\nfrom typing import Union, List, Dict\nimport numpy as np\n\nfrom rl_coach.agents.dqn_agent import DQNAgentParameters, DQNAlgorithmParameters, DQNAgent\nfrom rl_coach.base_parameters import Parameters\nfrom rl_coach.core_types import EnvironmentSteps, Batch, StateType\nfrom rl_coach.graph_managers.batch_rl_graph_manager import BatchRLGraphManager\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.non_episodic.differentiable_neural_dictionary import AnnoyDictionary\nfrom rl_coach.schedules import LinearSchedule\n\n\nclass NNImitationModelParameters(Parameters):\n    """"""\n    A parameters module grouping together parameters related to a neural network based action selection.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.imitation_model_num_epochs = 100\n        self.mask_out_actions_threshold = 0.35\n\n\nclass KNNParameters(Parameters):\n    """"""\n    A parameters module grouping together parameters related to a k-Nearest Neighbor based action selection.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.average_dist_coefficient = 1\n        self.knn_size = 50000\n        self.use_state_embedding_instead_of_state = True  # useful when the state is too big to be used for kNN\n\n\nclass DDQNBCQAlgorithmParameters(DQNAlgorithmParameters):\n    """"""\n    :param action_drop_method_parameters: (Parameters)\n        Defines the mode and related parameters according to which low confidence actions will be filtered out\n    :param num_steps_between_copying_online_weights_to_target (StepMethod)\n        Defines the number of steps between every phase of copying online network\'s weights to the target network\'s weights\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.action_drop_method_parameters = KNNParameters()\n        self.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(30000)\n\n\nclass DDQNBCQAgentParameters(DQNAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm = DDQNBCQAlgorithmParameters()\n        self.exploration.epsilon_schedule = LinearSchedule(1, 0.01, 1000000)\n        self.exploration.evaluation_epsilon = 0.001\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.ddqn_bcq_agent:DDQNBCQAgent\'\n\n\n# Double DQN - https://arxiv.org/abs/1509.06461\n# (a variant on) BCQ - https://arxiv.org/pdf/1812.02900v2.pdf\nclass DDQNBCQAgent(DQNAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n        if isinstance(self.ap.algorithm.action_drop_method_parameters, KNNParameters):\n            self.knn_trees = []  # will be filled out later, as we don\'t have the action space size yet\n            self.average_dist = 0\n\n            def to_embedding(states: Union[List[StateType], Dict]):\n                if isinstance(states, list):\n                    states = self.prepare_batch_for_inference(states, \'reward_model\')\n                if self.ap.algorithm.action_drop_method_parameters.use_state_embedding_instead_of_state:\n                    return self.networks[\'reward_model\'].online_network.predict(\n                        states,\n                        outputs=[self.networks[\'reward_model\'].online_network.state_embedding[0]])\n                else:\n                    return states[\'observation\']\n            self.embedding = to_embedding\n\n        elif isinstance(self.ap.algorithm.action_drop_method_parameters, NNImitationModelParameters):\n            if \'imitation_model\' not in self.ap.network_wrappers:\n                # user hasn\'t defined params for the reward model. we will use the same params as used for the \'main\'\n                # network.\n                self.ap.network_wrappers[\'imitation_model\'] = deepcopy(self.ap.network_wrappers[\'reward_model\'])\n        else:\n            raise ValueError(\'Unsupported action drop method {} for DDQNBCQAgent\'.format(\n                type(self.ap.algorithm.action_drop_method_parameters)))\n\n    def select_actions(self, next_states, q_st_plus_1):\n        if isinstance(self.ap.algorithm.action_drop_method_parameters, KNNParameters):\n            familiarity = np.array([[distance[0] for distance in\n                            knn_tree._get_k_nearest_neighbors_indices(self.embedding(next_states), 1)[0]]\n                                    for knn_tree in self.knn_trees]).transpose()\n            actions_to_mask_out = familiarity > self.ap.algorithm.action_drop_method_parameters.average_dist_coefficient \\\n                                  * self.average_dist\n\n        elif isinstance(self.ap.algorithm.action_drop_method_parameters, NNImitationModelParameters):\n            familiarity = self.networks[\'imitation_model\'].online_network.predict(next_states)\n            actions_to_mask_out = familiarity < \\\n                                    self.ap.algorithm.action_drop_method_parameters.mask_out_actions_threshold\n        else:\n            raise ValueError(\'Unsupported action drop method {} for DDQNBCQAgent\'.format(\n                type(self.ap.algorithm.action_drop_method_parameters)))\n\n        masked_next_q_values = self.networks[\'main\'].online_network.predict(next_states)\n        masked_next_q_values[actions_to_mask_out] = -np.inf\n\n        # occassionaly there are states in the batch for which our model shows no confidence for either of the actions\n        # in that case, we will just randomly assign q_values to actions, since otherwise argmax will always return\n        # the first action\n        zero_confidence_rows = (masked_next_q_values.max(axis=1) == -np.inf)\n        masked_next_q_values[zero_confidence_rows] = np.random.rand(np.sum(zero_confidence_rows),\n                                                                    masked_next_q_values.shape[1])\n\n        return np.argmax(masked_next_q_values, 1)\n\n    def improve_reward_model(self, epochs: int):\n        """"""\n        Train both a reward model to be used by the doubly-robust estimator, and some model to be used for BCQ\n\n        :param epochs: The total number of epochs to use for training a reward model\n        :return: None\n        """"""\n\n        # we\'ll be assuming that these gets drawn from the reward model parameters\n        batch_size = self.ap.network_wrappers[\'reward_model\'].batch_size\n        network_keys = self.ap.network_wrappers[\'reward_model\'].input_embedders_parameters.keys()\n\n        # if using a NN to decide which actions to drop, we\'ll train the NN here\n        if isinstance(self.ap.algorithm.action_drop_method_parameters, NNImitationModelParameters):\n            total_epochs = max(epochs, self.ap.algorithm.action_drop_method_parameters.imitation_model_num_epochs)\n        else:\n            total_epochs = epochs\n\n        for epoch in range(total_epochs):\n            # this is fitted from the training dataset\n            reward_model_loss = 0\n            imitation_model_loss = 0\n            total_transitions_processed = 0\n            for i, batch in enumerate(self.call_memory(\'get_shuffled_training_data_generator\', batch_size)):\n                batch = Batch(batch)\n\n                # reward model\n                if epoch < epochs:\n                    reward_model_loss += self.get_reward_model_loss(batch)\n\n                # imitation model\n                if isinstance(self.ap.algorithm.action_drop_method_parameters, NNImitationModelParameters) and \\\n                        epoch < self.ap.algorithm.action_drop_method_parameters.imitation_model_num_epochs:\n                    target_actions = np.zeros((batch.size, len(self.spaces.action.actions)))\n                    target_actions[range(batch.size), batch.actions()] = 1\n                    imitation_model_loss += self.networks[\'imitation_model\'].train_and_sync_networks(\n                        batch.states(network_keys), target_actions)[0]\n\n                total_transitions_processed += batch.size\n\n            log = OrderedDict()\n            log[\'Epoch\'] = epoch\n\n            if reward_model_loss:\n                log[\'Reward Model Loss\'] = reward_model_loss / total_transitions_processed\n            if imitation_model_loss:\n                log[\'Imitation Model Loss\'] = imitation_model_loss / total_transitions_processed\n\n            screen.log_dict(log, prefix=\'Training Batch RL Models\')\n\n        # if using a kNN based model, we\'ll initialize and build it here.\n        # initialization cannot be moved to the constructor as we don\'t have the agent\'s spaces initialized yet.\n        if isinstance(self.ap.algorithm.action_drop_method_parameters, KNNParameters):\n            knn_size = self.ap.algorithm.action_drop_method_parameters.knn_size\n            if self.ap.algorithm.action_drop_method_parameters.use_state_embedding_instead_of_state:\n                self.knn_trees = [AnnoyDictionary(\n                    dict_size=knn_size,\n                    key_width=int(self.networks[\'reward_model\'].online_network.state_embedding[0].shape[-1]),\n                    batch_size=knn_size)\n                    for _ in range(len(self.spaces.action.actions))]\n            else:\n                self.knn_trees = [AnnoyDictionary(\n                    dict_size=knn_size,\n                    key_width=self.spaces.state[\'observation\'].shape[0],\n                    batch_size=knn_size)\n                    for _ in range(len(self.spaces.action.actions))]\n\n            for i, knn_tree in enumerate(self.knn_trees):\n                state_embeddings = self.embedding([transition.state for transition in self.memory.transitions\n                                if transition.action == i])\n                knn_tree.add(\n                    keys=state_embeddings,\n                    values=np.expand_dims(np.zeros(state_embeddings.shape[0]), axis=1))\n\n            for knn_tree in self.knn_trees:\n                knn_tree._rebuild_index()\n\n            self.average_dist = [[dist[0] for dist in knn_tree._get_k_nearest_neighbors_indices(\n                keys=self.embedding([transition.state for transition in self.memory.transitions]),\n                k=1)[0]] for knn_tree in self.knn_trees]\n            self.average_dist = sum([x for l in self.average_dist for x in l])  # flatten and sum\n            self.average_dist /= len(self.memory.transitions)\n\n    def set_session(self, sess) -> None:\n        super().set_session(sess)\n\n        # we check here if we are in batch-rl, since this is the only place where we have a graph_manager to question\n        assert isinstance(self.parent_level_manager.parent_graph_manager, BatchRLGraphManager),\\\n            \'DDQNBCQ agent can only be used in BatchRL\'\n'"
rl_coach/agents/dfp_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom enum import Enum\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.agent import Agent\nfrom rl_coach.architectures.head_parameters import MeasurementsPredictionHeadParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.architectures.tensorflow_components.layers import Conv2d, Dense\nfrom rl_coach.base_parameters import AlgorithmParameters, AgentParameters, NetworkParameters, \\\n     MiddlewareScheme\nfrom rl_coach.core_types import ActionInfo, EnvironmentSteps, RunPhase\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.spaces import SpacesDefinition, VectorObservationSpace\n\n\nclass HandlingTargetsAfterEpisodeEnd(Enum):\n    LastStep = 0\n    NAN = 1\n\n\nclass DFPNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(activation_function=\'leaky_relu\'),\n                                            \'measurements\': InputEmbedderParameters(activation_function=\'leaky_relu\'),\n                                            \'goal\': InputEmbedderParameters(activation_function=\'leaky_relu\')}\n\n        self.input_embedders_parameters[\'observation\'].scheme = [\n            Conv2d(32, 8, 4),\n            Conv2d(64, 4, 2),\n            Conv2d(64, 3, 1),\n            Dense(512),\n        ]\n\n        self.input_embedders_parameters[\'measurements\'].scheme = [\n            Dense(128),\n            Dense(128),\n            Dense(128),\n        ]\n\n        self.input_embedders_parameters[\'goal\'].scheme = [\n            Dense(128),\n            Dense(128),\n            Dense(128),\n        ]\n\n        self.middleware_parameters = FCMiddlewareParameters(activation_function=\'leaky_relu\',\n                                                            scheme=MiddlewareScheme.Empty)\n        self.heads_parameters = [MeasurementsPredictionHeadParameters(activation_function=\'leaky_relu\')]\n        self.async_training = False\n        self.batch_size = 64\n        self.adam_optimizer_beta1 = 0.95\n\n\nclass DFPMemoryParameters(EpisodicExperienceReplayParameters):\n    def __init__(self):\n        self.max_size = (MemoryGranularity.Transitions, 20000)\n        self.shared_memory = True\n        super().__init__()\n\n\nclass DFPAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param num_predicted_steps_ahead: (int)\n        Number of future steps to predict measurements for. The future steps won\'t be sequential, but rather jump\n        in multiples of 2. For example, if num_predicted_steps_ahead = 3, then the steps will be: t+1, t+2, t+4.\n        The predicted steps will be [t + 2**i for i in range(num_predicted_steps_ahead)]\n\n    :param goal_vector: (List[float])\n        The goal vector will weight each of the measurements to form an optimization goal. The vector should have\n        the same length as the number of measurements, and it will be vector multiplied by the measurements.\n        Positive values correspond to trying to maximize the particular measurement, and negative values\n        correspond to trying to minimize the particular measurement.\n\n    :param future_measurements_weights: (List[float])\n        The future_measurements_weights weight the contribution of each of the predicted timesteps to the optimization\n        goal. For example, if there are 6 steps predicted ahead, and a future_measurements_weights vector with 3 values,\n        then only the 3 last timesteps will be taken into account, according to the weights in the\n        future_measurements_weights vector.\n\n    :param use_accumulated_reward_as_measurement: (bool)\n        If set to True, the accumulated reward from the beginning of the episode will be added as a measurement to\n        the measurements vector in the state. This van be useful in environments where the given measurements don\'t\n        include enough information for the particular goal the agent should achieve.\n\n    :param handling_targets_after_episode_end: (HandlingTargetsAfterEpisodeEnd)\n        Dictates how to handle measurements that are outside the episode length.\n\n    :param scale_measurements_targets: (Dict[str, float])\n        Allows rescaling the values of each of the measurements available. This van be useful when the measurements\n        have a different scale and you want to normalize them to the same scale.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.num_predicted_steps_ahead = 6\n        self.goal_vector = [1.0, 1.0]\n        self.future_measurements_weights = [0.5, 0.5, 1.0]\n        self.use_accumulated_reward_as_measurement = False\n        self.handling_targets_after_episode_end = HandlingTargetsAfterEpisodeEnd.NAN\n        self.scale_measurements_targets = {}\n        self.num_consecutive_playing_steps = EnvironmentSteps(8)\n\n\nclass DFPAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=DFPAlgorithmParameters(),\n                         exploration=EGreedyParameters(),\n                         memory=DFPMemoryParameters(),\n                         networks={""main"": DFPNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.dfp_agent:DFPAgent\'\n\n\n# Direct Future Prediction Agent - http://vladlen.info/papers/learning-to-act.pdf\nclass DFPAgent(Agent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.current_goal = self.ap.algorithm.goal_vector\n        self.target_measurements_scale_factors = None\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        network_inputs = batch.states(network_keys)\n        network_inputs[\'goal\'] = np.repeat(np.expand_dims(self.current_goal, 0),\n                                           batch.size, axis=0)\n\n        # get the current outputs of the network\n        targets = self.networks[\'main\'].online_network.predict(network_inputs)\n\n        # change the targets for the taken actions\n        for i in range(batch.size):\n            targets[i, batch.actions()[i]] = batch[i].info[\'future_measurements\'].flatten()\n\n        result = self.networks[\'main\'].train_and_sync_networks(network_inputs, targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n\n    def choose_action(self, curr_state):\n        if self.exploration_policy.requires_action_values():\n            # predict the future measurements\n            tf_input_state = self.prepare_batch_for_inference(curr_state, \'main\')\n            tf_input_state[\'goal\'] = np.expand_dims(self.current_goal, 0)\n            measurements_future_prediction = self.networks[\'main\'].online_network.predict(tf_input_state)[0]\n            action_values = np.zeros(len(self.spaces.action.actions))\n            num_steps_used_for_objective = len(self.ap.algorithm.future_measurements_weights)\n\n            # calculate the score of each action by multiplying it\'s future measurements with the goal vector\n            for action_idx in range(len(self.spaces.action.actions)):\n                action_measurements = measurements_future_prediction[action_idx]\n                action_measurements = np.reshape(action_measurements,\n                                                 (self.ap.algorithm.num_predicted_steps_ahead,\n                                                  self.spaces.state[\'measurements\'].shape[0]))\n                future_steps_values = np.dot(action_measurements, self.current_goal)\n                action_values[action_idx] = np.dot(future_steps_values[-num_steps_used_for_objective:],\n                                                   self.ap.algorithm.future_measurements_weights)\n        else:\n            action_values = None\n\n        # choose action according to the exploration policy and the current phase (evaluating or training the agent)\n        action, _ = self.exploration_policy.get_action(action_values)\n\n        if action_values is not None:\n            action_values = action_values.squeeze()\n            action_info = ActionInfo(action=action, action_value=action_values[action])\n        else:\n            action_info = ActionInfo(action=action)\n\n        return action_info\n\n    def set_environment_parameters(self, spaces: SpacesDefinition):\n        self.spaces = copy.deepcopy(spaces)\n        self.spaces.goal = VectorObservationSpace(shape=self.spaces.state[\'measurements\'].shape,\n                                                  measurements_names=\n                                                  self.spaces.state[\'measurements\'].measurements_names)\n\n        # if the user has filled some scale values, check that he got the names right\n        if set(self.spaces.state[\'measurements\'].measurements_names).intersection(\n                self.ap.algorithm.scale_measurements_targets.keys()) !=\\\n                set(self.ap.algorithm.scale_measurements_targets.keys()):\n            raise ValueError(""Some of the keys in parameter scale_measurements_targets ({})  are not defined in ""\n                             ""the measurements space {}"".format(self.ap.algorithm.scale_measurements_targets.keys(),\n                                                                self.spaces.state[\'measurements\'].measurements_names))\n\n        super().set_environment_parameters(self.spaces)\n\n        # the below is done after calling the base class method, as it might add accumulated reward as a measurement\n\n        # fill out the missing measurements scale factors\n        for measurement_name in self.spaces.state[\'measurements\'].measurements_names:\n            if measurement_name not in self.ap.algorithm.scale_measurements_targets:\n                self.ap.algorithm.scale_measurements_targets[measurement_name] = 1\n\n        self.target_measurements_scale_factors = \\\n            np.array([self.ap.algorithm.scale_measurements_targets[measurement_name] for measurement_name in\n                      self.spaces.state[\'measurements\'].measurements_names])\n\n    def handle_episode_ended(self):\n        last_episode = self.current_episode_buffer\n        if self.phase in [RunPhase.TRAIN, RunPhase.HEATUP] and last_episode:\n            self._update_measurements_targets(last_episode,\n                                              self.ap.algorithm.num_predicted_steps_ahead)\n        super().handle_episode_ended()\n\n    def _update_measurements_targets(self, episode, num_steps):\n        if \'measurements\' not in episode.transitions[0].state or episode.transitions[0].state[\'measurements\'] == []:\n            raise ValueError(""Measurements are not present in the transitions of the last episode played. "")\n        measurements_size = self.spaces.state[\'measurements\'].shape[0]\n        for transition_idx, transition in enumerate(episode.transitions):\n            transition.info[\'future_measurements\'] = np.zeros((num_steps, measurements_size))\n            for step in range(num_steps):\n                offset_idx = transition_idx + 2 ** step\n\n                if offset_idx >= episode.length():\n                    if self.ap.algorithm.handling_targets_after_episode_end == HandlingTargetsAfterEpisodeEnd.NAN:\n                        # the special MSE loss will ignore those entries so that the gradient will be 0 for these\n                        transition.info[\'future_measurements\'][step] = np.nan\n                        continue\n\n                    elif self.ap.algorithm.handling_targets_after_episode_end == HandlingTargetsAfterEpisodeEnd.LastStep:\n                        offset_idx = - 1\n\n                transition.info[\'future_measurements\'][step] = \\\n                    self.target_measurements_scale_factors * \\\n                    (episode.transitions[offset_idx].state[\'measurements\'] - transition.state[\'measurements\'])\n'"
rl_coach/agents/dqn_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import QHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, NetworkParameters, AgentParameters, \\\n    MiddlewareScheme\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters\nfrom rl_coach.schedules import LinearSchedule\n\n\nclass DQNAlgorithmParameters(AlgorithmParameters):\n    def __init__(self):\n        super().__init__()\n        self.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(10000)\n        self.num_consecutive_playing_steps = EnvironmentSteps(4)\n        self.discount = 0.99\n        self.supports_parameter_noise = True\n\n\nclass DQNNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters(scheme=MiddlewareScheme.Medium)\n        self.heads_parameters = [QHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 32\n        self.replace_mse_with_huber_loss = True\n        self.create_target_network = True\n        self.should_get_softmax_probabilities = False\n\n\nclass DQNAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=DQNAlgorithmParameters(),\n                         exploration=EGreedyParameters(),\n                         memory=ExperienceReplayParameters(),\n                         networks={""main"": DQNNetworkParameters()})\n        self.exploration.epsilon_schedule = LinearSchedule(1, 0.1, 1000000)\n        self.exploration.evaluation_epsilon = 0.05\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.dqn_agent:DQNAgent\'\n\n\n# Deep Q Network - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\nclass DQNAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n    def select_actions(self, next_states, q_st_plus_1):\n        return np.argmax(q_st_plus_1, 1)\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # for the action we actually took, the error is:\n        # TD error = r + discount*max(q_st_plus_1) - q_st\n        # # for all other actions, the error is 0\n        q_st_plus_1, TD_targets = self.networks[\'main\'].parallel_prediction([\n            (self.networks[\'main\'].target_network, batch.next_states(network_keys)),\n            (self.networks[\'main\'].online_network, batch.states(network_keys))\n        ])\n\n        selected_actions = self.select_actions(batch.next_states(network_keys), q_st_plus_1)\n\n        # add Q value samples for logging\n        self.q_values.add_sample(TD_targets)\n\n        #  only update the action that we have actually done in this transition\n        TD_errors = []\n        for i in range(batch.size):\n            new_target = batch.rewards()[i] +\\\n                         (1.0 - batch.game_overs()[i]) * self.ap.algorithm.discount * q_st_plus_1[i][selected_actions[i]]\n            TD_errors.append(np.abs(new_target - TD_targets[i, batch.actions()[i]]))\n            TD_targets[i, batch.actions()[i]] = new_target\n\n        # update errors in prioritized replay buffer\n        importance_weights = self.update_transition_priorities_and_get_weights(TD_errors, batch)\n\n        result = self.networks[\'main\'].train_and_sync_networks(batch.states(network_keys), TD_targets,\n                                                               importance_weights=importance_weights)\n\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n'"
rl_coach/agents/hac_ddpg_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.ddpg_agent import DDPGAgent, DDPGAgentParameters, DDPGAlgorithmParameters\nfrom rl_coach.core_types import RunPhase\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass HACDDPGAlgorithmParameters(DDPGAlgorithmParameters):\n    """"""\n    :param time_limit: (int)\n        The number of steps the agent is allowed to act for while trying to achieve its goal\n\n    :param sub_goal_testing_rate: (float)\n        The percent of episodes that will be used for testing the sub goals generated by the upper level agents.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.time_limit = 40\n        self.sub_goal_testing_rate = 0.5\n\n\nclass HACDDPGAgentParameters(DDPGAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm = HACDDPGAlgorithmParameters()\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.hac_ddpg_agent:HACDDPGAgent\'\n\n\n# Hierarchical Actor Critic Generating Subgoals DDPG Agent - https://arxiv.org/pdf/1712.00948.pdf\nclass HACDDPGAgent(DDPGAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.sub_goal_testing_rate = self.ap.algorithm.sub_goal_testing_rate\n        self.graph_manager = None\n\n    def choose_action(self, curr_state):\n        # top level decides, for each of his generated sub-goals, for all the layers beneath him if this is a sub-goal\n        # testing phase\n\n        graph_manager = self.parent_level_manager.parent_graph_manager\n        if self.ap.is_a_highest_level_agent:\n            graph_manager.should_test_current_sub_goal = np.random.rand() < self.sub_goal_testing_rate\n\n        if self.phase == RunPhase.TRAIN:\n            if graph_manager.should_test_current_sub_goal:\n                self.exploration_policy.change_phase(RunPhase.TEST)\n            else:\n                self.exploration_policy.change_phase(self.phase)\n\n        action_info = super().choose_action(curr_state)\n        return action_info\n\n    def update_transition_before_adding_to_replay_buffer(self, transition):\n        graph_manager = self.parent_level_manager.parent_graph_manager\n\n        # deal with goals given from a higher level agent\n        if not self.ap.is_a_highest_level_agent:\n            transition.state[\'desired_goal\'] = self.current_hrl_goal\n            transition.next_state[\'desired_goal\'] = self.current_hrl_goal\n            # TODO: allow setting goals which are not part of the state. e.g. state-embedding using get_prediction\n            self.distance_from_goal.add_sample(self.spaces.goal.distance_from_goal(\n                self.current_hrl_goal, transition.next_state))\n            goal_reward, sub_goal_reached = self.spaces.goal.get_reward_for_goal_and_state(\n                self.current_hrl_goal, transition.next_state)\n            transition.reward = goal_reward\n            transition.game_over = transition.game_over or sub_goal_reached\n\n        # each level tests its own generated sub goals\n        if not self.ap.is_a_lowest_level_agent and graph_manager.should_test_current_sub_goal:\n            #TODO-fixme\n            # _, sub_goal_reached = self.parent_level_manager.environment.agents[\'agent_1\'].spaces.goal.\\\n            # get_reward_for_goal_and_state(transition.action, transition.next_state)\n\n            _, sub_goal_reached = self.spaces.goal.get_reward_for_goal_and_state(\n                transition.action, transition.next_state)\n\n            sub_goal_is_missed = not sub_goal_reached\n\n            if sub_goal_is_missed:\n                transition.reward = -self.ap.algorithm.time_limit\n        return transition\n\n    def set_environment_parameters(self, spaces: SpacesDefinition):\n        super().set_environment_parameters(spaces)\n\n        if self.ap.is_a_highest_level_agent:\n            # the rest of the levels already have an in_action_space set to be of type GoalsSpace, thus they will have\n            # their GoalsSpace set to the in_action_space in agent.set_environment_parameters()\n            self.spaces.goal = self.spaces.action\n            self.spaces.goal.set_target_space(self.spaces.state[self.spaces.goal.goal_name])\n\n        if not self.ap.is_a_highest_level_agent:\n            self.spaces.reward.reward_success_threshold = self.spaces.goal.reward_type.goal_reaching_reward\n'"
rl_coach/agents/human_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nfrom collections import OrderedDict\nfrom typing import Union\n\nimport contextlib\nwith contextlib.redirect_stdout(None):\n    import pygame\nfrom pandas import to_pickle\n\nfrom rl_coach.agents.agent import Agent\nfrom rl_coach.agents.bc_agent import BCNetworkParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import PolicyHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, NetworkParameters, EmbedderScheme, \\\n    AgentParameters\nfrom rl_coach.core_types import ActionInfo\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters\n\n\nclass HumanAlgorithmParameters(AlgorithmParameters):\n    def __init__(self):\n        super().__init__()\n\n\nclass HumanNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.input_embedders_parameters[\'observation\'].scheme = EmbedderScheme.Medium\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 32\n        self.replace_mse_with_huber_loss = False\n        self.create_target_network = False\n\n\nclass HumanAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=HumanAlgorithmParameters(),\n                         exploration=EGreedyParameters(),\n                         memory=ExperienceReplayParameters(),\n                         networks={""main"": BCNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.human_agent:HumanAgent\'\n\n\nclass HumanAgent(Agent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n        self.clock = pygame.time.Clock()\n        self.max_fps = int(self.ap.visualization.max_fps_for_human_control)\n        self.env = None\n\n    def init_environment_dependent_modules(self):\n        super().init_environment_dependent_modules()\n        self.env = self.parent_level_manager._real_environment\n        screen.log_title(""Human Control Mode"")\n        available_keys = self.env.get_available_keys()\n        if available_keys:\n            screen.log(""Use keyboard keys to move. Press escape to quit. Available keys:"")\n            screen.log("""")\n            for action, key in self.env.get_available_keys():\n                screen.log(""\\t- {}: {}"".format(action, key))\n            screen.separator()\n\n    def train(self):\n        return 0\n\n    def choose_action(self, curr_state):\n        action = ActionInfo(self.env.get_action_from_user(), action_value=0)\n        action = self.output_filter.reverse_filter(action)\n\n        # keep constant fps\n        self.clock.tick(self.max_fps)\n\n        if not self.env.renderer.is_open:\n            self.save_replay_buffer_and_exit()\n\n        return action\n\n    def save_replay_buffer_and_exit(self):\n        replay_buffer_path = os.path.join(self.agent_logger.experiments_path, \'replay_buffer.p\')\n        self.memory.tp = None\n        self.memory.save(replay_buffer_path)\n        screen.log_title(""Replay buffer was stored in {}"".format(replay_buffer_path))\n        exit()\n\n    def log_to_screen(self):\n        # log to screen\n        log = OrderedDict()\n        log[""Episode""] = self.current_episode\n        log[""Total reward""] = round(self.total_reward_in_current_episode, 2)\n        log[""Steps""] = self.total_steps_counter\n        screen.log_dict(log, prefix=""Recording"")\n'"
rl_coach/agents/imitation_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom collections import OrderedDict\nfrom typing import Union\n\nfrom rl_coach.agents.agent import Agent\nfrom rl_coach.core_types import RunPhase, ActionInfo\nfrom rl_coach.logger import screen\nfrom rl_coach.spaces import DiscreteActionSpace\n\n\n## This is an abstract agent - there is no learn_from_batch method ##\n\n# Imitation Agent\nclass ImitationAgent(Agent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.imitation = True\n\n    def extract_action_values(self, prediction):\n        return prediction.squeeze()\n\n    def choose_action(self, curr_state):\n        # convert to batch so we can run it through the network\n        prediction = self.networks[\'main\'].online_network.predict(self.prepare_batch_for_inference(curr_state, \'main\'))\n\n        # get action values and extract the best action from it\n        action_values = self.extract_action_values(prediction)\n        self.exploration_policy.change_phase(RunPhase.TEST)\n        action = self.exploration_policy.get_action(action_values)\n        action_info = ActionInfo(action=action)\n\n        return action_info\n\n    def log_to_screen(self):\n        # log to screen\n        if self.phase == RunPhase.TRAIN:\n            # for the training phase - we log during the episode to visualize the progress in training\n            log = OrderedDict()\n            if self.task_id is not None:\n                log[""Worker""] = self.task_id\n            log[""Episode""] = self.current_episode\n            log[""Loss""] = self.loss.values[-1]\n            log[""Training iteration""] = self.training_iteration\n            screen.log_dict(log, prefix=""Training"")\n        else:\n            # for the evaluation phase - logging as in regular RL\n            super().log_to_screen()\n\n    def learn_from_batch(self, batch):\n        raise NotImplementedError(""ImitationAgent is an abstract agent. Not to be used directly."")\n'"
rl_coach/agents/mmc_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.dqn_agent import DQNAgentParameters, DQNAlgorithmParameters\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\n\n\nclass MixedMonteCarloAlgorithmParameters(DQNAlgorithmParameters):\n    """"""\n    :param monte_carlo_mixing_rate: (float)\n        The mixing rate is used for setting the amount of monte carlo estimate (full return) that will be mixes into\n        the single-step bootstrapped targets.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.monte_carlo_mixing_rate = 0.1\n\n\nclass MixedMonteCarloAgentParameters(DQNAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm = MixedMonteCarloAlgorithmParameters()\n        self.memory = EpisodicExperienceReplayParameters()\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.mmc_agent:MixedMonteCarloAgent\'\n\n\nclass MixedMonteCarloAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.mixing_rate = agent_parameters.algorithm.monte_carlo_mixing_rate\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # for the 1-step, we use the double-dqn target. hence actions are taken greedily according to the online network\n        selected_actions = np.argmax(self.networks[\'main\'].online_network.predict(batch.next_states(network_keys)), 1)\n\n        # TD_targets are initialized with the current prediction so that we will\n        #  only update the action that we have actually done in this transition\n        q_st_plus_1, TD_targets = self.networks[\'main\'].parallel_prediction([\n            (self.networks[\'main\'].target_network, batch.next_states(network_keys)),\n            (self.networks[\'main\'].online_network, batch.states(network_keys))\n        ])\n\n        total_returns = batch.n_step_discounted_rewards()\n\n        for i in range(batch.size):\n            one_step_target = batch.rewards()[i] + \\\n                              (1.0 - batch.game_overs()[i]) * self.ap.algorithm.discount * \\\n                              q_st_plus_1[i][selected_actions[i]]\n            monte_carlo_target = total_returns[i]\n            TD_targets[i, batch.actions()[i]] = (1 - self.mixing_rate) * one_step_target + \\\n                                                self.mixing_rate * monte_carlo_target\n\n        result = self.networks[\'main\'].train_and_sync_networks(batch.states(network_keys), TD_targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n'"
rl_coach/agents/n_step_q_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.policy_optimization_agent import PolicyOptimizationAgent\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import QHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, AgentParameters, NetworkParameters\n\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.memories.episodic.single_episode_buffer import SingleEpisodeBufferParameters\nfrom rl_coach.utils import last_sample\n\n\nclass NStepQNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.heads_parameters = [QHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.async_training = True\n        self.shared_optimizer = True\n        self.create_target_network = True\n\n\nclass NStepQAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param num_steps_between_copying_online_weights_to_target: (StepMethod)\n        The number of steps between copying the online network weights to the target network weights.\n\n    :param apply_gradients_every_x_episodes: (int)\n        The number of episodes between applying the accumulated gradients to the network. After every\n        num_steps_between_gradient_updates steps, the agent will calculate the gradients for the collected data,\n        it will then accumulate it in internal accumulators, and will only apply them to the network once in every\n        apply_gradients_every_x_episodes episodes.\n\n    :param num_steps_between_gradient_updates: (int)\n        The number of steps between calculating gradients for the collected data. In the A3C paper, this parameter is\n        called t_max. Since this algorithm is on-policy, only the steps collected between each two gradient calculations\n        are used in the batch.\n\n    :param targets_horizon: (str)\n        Should be either \'N-Step\' or \'1-Step\', and defines the length for which to bootstrap the network values over.\n        Essentially, 1-Step follows the regular 1 step bootstrapping Q learning update. For more information,\n        please refer to the original paper (https://arxiv.org/abs/1602.01783)\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(10000)\n        self.apply_gradients_every_x_episodes = 1\n        self.num_steps_between_gradient_updates = 5  # this is called t_max in all the papers\n        self.targets_horizon = \'N-Step\'\n\n\nclass NStepQAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=NStepQAlgorithmParameters(),\n                         exploration=EGreedyParameters(),\n                         memory=SingleEpisodeBufferParameters(),\n                         networks={""main"": NStepQNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.n_step_q_agent:NStepQAgent\'\n\n\n# N Step Q Learning Agent - https://arxiv.org/abs/1602.01783\nclass NStepQAgent(ValueOptimizationAgent, PolicyOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.last_gradient_update_step_idx = 0\n        self.q_values = self.register_signal(\'Q Values\')\n        self.value_loss = self.register_signal(\'Value Loss\')\n\n    def learn_from_batch(self, batch):\n        # batch contains a list of episodes to learn from\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # get the values for the current states\n        state_value_head_targets = self.networks[\'main\'].online_network.predict(batch.states(network_keys))\n\n        # the targets for the state value estimator\n        if self.ap.algorithm.targets_horizon == \'1-Step\':\n            # 1-Step Q learning\n            q_st_plus_1 = self.networks[\'main\'].target_network.predict(batch.next_states(network_keys))\n\n            for i in reversed(range(batch.size)):\n                state_value_head_targets[i][batch.actions()[i]] = \\\n                    batch.rewards()[i] \\\n                    + (1.0 - batch.game_overs()[i]) * self.ap.algorithm.discount * np.max(q_st_plus_1[i], 0)\n\n        elif self.ap.algorithm.targets_horizon == \'N-Step\':\n            # N-Step Q learning\n            if batch.game_overs()[-1]:\n                R = 0\n            else:\n                R = np.max(self.networks[\'main\'].target_network.predict(last_sample(batch.next_states(network_keys))))\n\n            for i in reversed(range(batch.size)):\n                R = batch.rewards()[i] + self.ap.algorithm.discount * R\n                state_value_head_targets[i][batch.actions()[i]] = R\n\n        else:\n            assert True, \'The available values for targets_horizon are: 1-Step, N-Step\'\n\n        # add Q value samples for logging\n        self.q_values.add_sample(state_value_head_targets)\n\n        # train\n        result = self.networks[\'main\'].online_network.accumulate_gradients(batch.states(network_keys), [state_value_head_targets])\n\n        # logging\n        total_loss, losses, unclipped_grads = result[:3]\n        self.value_loss.add_sample(losses[0])\n\n        return total_loss, losses, unclipped_grads\n\n    def train(self):\n        # update the target network of every network that has a target network\n        if any([network.has_target for network in self.networks.values()]) \\\n                and self._should_update_online_weights_to_target():\n            for network in self.networks.values():\n                network.update_target_network(self.ap.algorithm.rate_for_copying_weights_to_target)\n\n            self.agent_logger.create_signal_value(\'Update Target Network\', 1)\n        else:\n            self.agent_logger.create_signal_value(\'Update Target Network\', 0, overwrite=False)\n\n        return PolicyOptimizationAgent.train(self)\n'"
rl_coach/agents/naf_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import NAFHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, AgentParameters, \\\n    NetworkParameters\n\nfrom rl_coach.core_types import ActionInfo, EnvironmentSteps\nfrom rl_coach.exploration_policies.ou_process import OUProcessParameters\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.spaces import BoxActionSpace\n\n\nclass NAFNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.heads_parameters = [NAFHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.learning_rate = 0.001\n        self.async_training = True\n        self.create_target_network = True\n\n\nclass NAFAlgorithmParameters(AlgorithmParameters):\n    def __init__(self):\n        super().__init__()\n        self.num_consecutive_training_steps = 5\n        self.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(1)\n        self.rate_for_copying_weights_to_target = 0.001\n\n\nclass NAFAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=NAFAlgorithmParameters(),\n                         exploration=OUProcessParameters(),\n                         memory=EpisodicExperienceReplayParameters(),\n                         networks={""main"": NAFNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.naf_agent:NAFAgent\'\n\n\n# Normalized Advantage Functions - https://arxiv.org/pdf/1603.00748.pdf\nclass NAFAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.l_values = self.register_signal(""L"")\n        self.a_values = self.register_signal(""Advantage"")\n        self.mu_values = self.register_signal(""Action"")\n        self.v_values = self.register_signal(""V"")\n        self.TD_targets = self.register_signal(""TD targets"")\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # TD error = r + discount*v_st_plus_1 - q_st\n        v_st_plus_1 = self.networks[\'main\'].target_network.predict(\n            batch.next_states(network_keys),\n            self.networks[\'main\'].target_network.output_heads[0].V,\n            squeeze_output=False,\n        )\n        TD_targets = np.expand_dims(batch.rewards(), -1) + \\\n                     (1.0 - np.expand_dims(batch.game_overs(), -1)) * self.ap.algorithm.discount * v_st_plus_1\n\n        self.TD_targets.add_sample(TD_targets)\n\n        result = self.networks[\'main\'].train_and_sync_networks({**batch.states(network_keys),\n                                                                \'output_0_0\': batch.actions(len(batch.actions().shape) == 1)\n                                                                }, TD_targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n\n    def choose_action(self, curr_state):\n        if type(self.spaces.action) != BoxActionSpace:\n            raise ValueError(\'NAF works only for continuous control problems\')\n\n        # convert to batch so we can run it through the network\n        tf_input_state = self.prepare_batch_for_inference(curr_state, \'main\')\n        naf_head = self.networks[\'main\'].online_network.output_heads[0]\n        action_values = self.networks[\'main\'].online_network.predict(tf_input_state, outputs=naf_head.mu,\n                                                                     squeeze_output=False)\n\n        # get the actual action to use\n        action = self.exploration_policy.get_action(action_values)\n\n        # get the internal values for logging\n        outputs = [naf_head.mu, naf_head.Q, naf_head.L, naf_head.A, naf_head.V]\n        result = self.networks[\'main\'].online_network.predict(\n            {**tf_input_state, \'output_0_0\': action_values},\n            outputs=outputs\n        )\n        mu, Q, L, A, V = result\n\n        # store the q values statistics for logging\n        self.q_values.add_sample(Q)\n        self.l_values.add_sample(L)\n        self.a_values.add_sample(A)\n        self.mu_values.add_sample(mu)\n        self.v_values.add_sample(V)\n\n        action_info = ActionInfo(action=action, action_value=Q)\n        \n        return action_info\n'"
rl_coach/agents/nec_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport pickle\nfrom typing import Union, List\n\nimport numpy as np\n\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import DNDQHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, NetworkParameters, AgentParameters\n\nfrom rl_coach.core_types import RunPhase, EnvironmentSteps, Episode, StateType\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters, MemoryGranularity\nfrom rl_coach.schedules import ConstantSchedule\n\n\nclass NECNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.heads_parameters = [DNDQHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.should_get_softmax_probabilities = False\n\n\nclass NECAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param dnd_size: (int)\n        Defines the number of transitions that will be stored in each one of the DNDs. Note that the total number\n        of transitions that will be stored is dnd_size x num_actions.\n\n    :param l2_norm_added_delta: (float)\n        A small value that will be added when calculating the weight of each of the DND entries. This follows the\n        :math:`\\delta` patameter defined in the paper.\n\n    :param new_value_shift_coefficient: (float)\n        In the case where a ew embedding that was added to the DND was already present, the value that will be stored\n        in the DND is a mix between the existing value and the new value. The mix rate is defined by\n        new_value_shift_coefficient.\n\n    :param number_of_knn: (int)\n        The number of neighbors that will be retrieved for each DND query.\n\n    :param DND_key_error_threshold: (float)\n        When the DND is queried for a specific embedding, this threshold will be used to determine if the embedding\n        exists in the DND, since exact matches of embeddings are very rare.\n\n    :param propagate_updates_to_DND: (bool)\n        If set to True, when the gradients of the network will be calculated, the gradients will also be\n        backpropagated through the keys of the DND. The keys will then be updated as well, as if they were regular\n        network weights.\n\n    :param n_step: (int)\n        The bootstrap length that will be used when calculating the state values to store in the DND.\n\n    :param bootstrap_total_return_from_old_policy: (bool)\n        If set to True, the bootstrap that will be used to calculate each state-action value, is the network value\n        when the state was first seen, and not the latest, most up-to-date network value.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.dnd_size = 500000\n        self.l2_norm_added_delta = 0.001\n        self.new_value_shift_coefficient = 0.1\n        self.number_of_knn = 50\n        self.DND_key_error_threshold = 0\n        self.num_consecutive_playing_steps = EnvironmentSteps(4)\n        self.propagate_updates_to_DND = False\n        self.n_step = 100\n        self.bootstrap_total_return_from_old_policy = True\n\n\nclass NECMemoryParameters(EpisodicExperienceReplayParameters):\n    def __init__(self):\n        super().__init__()\n        self.max_size = (MemoryGranularity.Transitions, 100000)\n\n\nclass NECAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=NECAlgorithmParameters(),\n                         exploration=EGreedyParameters(),\n                         memory=NECMemoryParameters(),\n                         networks={""main"": NECNetworkParameters()})\n        self.exploration.epsilon_schedule = ConstantSchedule(0.1)\n        self.exploration.evaluation_epsilon = 0.01\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.nec_agent:NECAgent\'\n\n\n# Neural Episodic Control - https://arxiv.org/pdf/1703.01988.pdf\nclass NECAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.current_episode_state_embeddings = []\n        self.training_started = False\n        self.current_episode_buffer = \\\n            Episode(discount=self.ap.algorithm.discount,\n                    n_step=self.ap.algorithm.n_step,\n                    bootstrap_total_return_from_old_policy=self.ap.algorithm.bootstrap_total_return_from_old_policy)\n\n    def learn_from_batch(self, batch):\n        if not self.networks[\'main\'].online_network.output_heads[0].DND.has_enough_entries(self.ap.algorithm.number_of_knn):\n            return 0, [], 0\n        else:\n            if not self.training_started:\n                self.training_started = True\n                screen.log_title(""Finished collecting initial entries in DND. Starting to train network..."")\n\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        TD_targets = self.networks[\'main\'].online_network.predict(batch.states(network_keys))\n        bootstrapped_return_from_old_policy = batch.n_step_discounted_rewards()\n        #  only update the action that we have actually done in this transition\n        for i in range(batch.size):\n            TD_targets[i, batch.actions()[i]] = bootstrapped_return_from_old_policy[i]\n\n        # set the gradients to fetch for the DND update\n        fetches = []\n        head = self.networks[\'main\'].online_network.output_heads[0]\n        if self.ap.algorithm.propagate_updates_to_DND:\n            fetches = [head.dnd_embeddings_grad, head.dnd_values_grad, head.dnd_indices]\n\n        # train the neural network\n        result = self.networks[\'main\'].train_and_sync_networks(batch.states(network_keys), TD_targets, fetches)\n\n        total_loss, losses, unclipped_grads = result[:3]\n\n        # update the DND keys and values using the extracted gradients\n        if self.ap.algorithm.propagate_updates_to_DND:\n            embedding_gradients = np.swapaxes(result[-1][0], 0, 1)\n            value_gradients = np.swapaxes(result[-1][1], 0, 1)\n            indices = np.swapaxes(result[-1][2], 0, 1)\n            head.DND.update_keys_and_values(batch.actions(), embedding_gradients, value_gradients, indices)\n\n        return total_loss, losses, unclipped_grads\n\n    def act(self):\n        if self.phase == RunPhase.HEATUP:\n            # get embedding in heatup (otherwise we get it through get_prediction)\n            embedding = self.networks[\'main\'].online_network.predict(\n                self.prepare_batch_for_inference(self.curr_state, \'main\'),\n                outputs=self.networks[\'main\'].online_network.state_embedding)\n            self.current_episode_state_embeddings.append(embedding.squeeze())\n\n        return super().act()\n\n    def get_all_q_values_for_states(self, states: StateType, additional_outputs: List = None):\n        # we need to store the state embeddings regardless if the action is random or not\n        return self.get_prediction_and_update_embeddings(states)\n\n    def get_all_q_values_for_states_and_softmax_probabilities(self, states: StateType):\n        # get the actions q values and the state embedding\n        embedding, actions_q_values, softmax_probabilities = self.networks[\'main\'].online_network.predict(\n            self.prepare_batch_for_inference(states, \'main\'),\n            outputs=[self.networks[\'main\'].online_network.state_embedding,\n                     self.networks[\'main\'].online_network.output_heads[0].output,\n                     self.networks[\'main\'].online_network.output_heads[0].softmax]\n        )\n        if self.phase != RunPhase.TEST:\n            # store the state embedding for inserting it to the DND later\n            self.current_episode_state_embeddings.append(embedding.squeeze())\n        actions_q_values = actions_q_values[0][0]\n        return actions_q_values, softmax_probabilities\n\n    def get_prediction_and_update_embeddings(self, states):\n        # get the actions q values and the state embedding\n        embedding, actions_q_values = self.networks[\'main\'].online_network.predict(\n            self.prepare_batch_for_inference(states, \'main\'),\n            outputs=[self.networks[\'main\'].online_network.state_embedding,\n                     self.networks[\'main\'].online_network.output_heads[0].output]\n        )\n        if self.phase != RunPhase.TEST:\n            # store the state embedding for inserting it to the DND later\n            self.current_episode_state_embeddings.append(embedding[0].squeeze())\n        actions_q_values = actions_q_values[0][0]\n        return actions_q_values\n\n    def reset_internal_state(self):\n        super().reset_internal_state()\n        self.current_episode_state_embeddings = []\n        self.current_episode_buffer = \\\n            Episode(discount=self.ap.algorithm.discount,\n                    n_step=self.ap.algorithm.n_step,\n                    bootstrap_total_return_from_old_policy=self.ap.algorithm.bootstrap_total_return_from_old_policy)\n\n    def handle_episode_ended(self):\n        super().handle_episode_ended()\n\n        # get the last full episode that we have collected\n        episode = self.call_memory(\'get_last_complete_episode\')\n        if episode is not None and self.phase != RunPhase.TEST:\n            assert len(self.current_episode_state_embeddings) == episode.length()\n            discounted_rewards = episode.get_transitions_attribute(\'n_step_discounted_rewards\')\n            actions = episode.get_transitions_attribute(\'action\')\n            self.networks[\'main\'].online_network.output_heads[0].DND.add(self.current_episode_state_embeddings,\n                                                                         actions, discounted_rewards)\n\n    def save_checkpoint(self, checkpoint_prefix):\n        super().save_checkpoint(checkpoint_prefix)\n        with open(os.path.join(self.ap.task_parameters.checkpoint_save_dir, str(checkpoint_prefix) + \'.dnd\'), \'wb\') as f:\n            pickle.dump(self.networks[\'main\'].online_network.output_heads[0].DND, f, pickle.HIGHEST_PROTOCOL)\n'"
rl_coach/agents/pal_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.dqn_agent import DQNAgentParameters, DQNAlgorithmParameters\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\n\n\nclass PALAlgorithmParameters(DQNAlgorithmParameters):\n    """"""\n    :param pal_alpha: (float)\n        A factor that weights the amount by which the advantage learning update will be taken into account.\n\n    :param persistent_advantage_learning: (bool)\n        If set to True, the persistent mode of advantage learning will be used, which encourages the agent to take\n        the same actions one after the other instead of changing actions.\n\n    :param monte_carlo_mixing_rate: (float)\n        The amount of monte carlo values to mix into the targets of the network. The monte carlo values are just the\n        total discounted returns, and they can help reduce the time it takes for the network to update to the newly\n        seen values, since it is not based on bootstrapping the current network values.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.pal_alpha = 0.9\n        self.persistent_advantage_learning = False\n        self.monte_carlo_mixing_rate = 0.1\n\n\nclass PALAgentParameters(DQNAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm = PALAlgorithmParameters()\n        self.memory = EpisodicExperienceReplayParameters()\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.pal_agent:PALAgent\'\n\n\n# Persistent Advantage Learning - https://arxiv.org/pdf/1512.04860.pdf\nclass PALAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.alpha = agent_parameters.algorithm.pal_alpha\n        self.persistent = agent_parameters.algorithm.persistent_advantage_learning\n        self.monte_carlo_mixing_rate = agent_parameters.algorithm.monte_carlo_mixing_rate\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # next state values\n        q_st_plus_1_target, q_st_plus_1_online = self.networks[\'main\'].parallel_prediction([\n            (self.networks[\'main\'].target_network, batch.next_states(network_keys)),\n            (self.networks[\'main\'].online_network, batch.next_states(network_keys))\n        ])\n        selected_actions = np.argmax(q_st_plus_1_online, 1)\n        v_st_plus_1_target = np.max(q_st_plus_1_target, 1)\n\n        # current state values\n        q_st_target, q_st_online = self.networks[\'main\'].parallel_prediction([\n            (self.networks[\'main\'].target_network, batch.states(network_keys)),\n            (self.networks[\'main\'].online_network, batch.states(network_keys))\n        ])\n        v_st_target = np.max(q_st_target, 1)\n\n        # calculate TD error\n        TD_targets = np.copy(q_st_online)\n        total_returns = batch.n_step_discounted_rewards()\n        for i in range(batch.size):\n            TD_targets[i, batch.actions()[i]] = batch.rewards()[i] + \\\n                                        (1.0 - batch.game_overs()[i]) * self.ap.algorithm.discount * \\\n                                                     q_st_plus_1_target[i][selected_actions[i]]\n            advantage_learning_update = v_st_target[i] - q_st_target[i, batch.actions()[i]]\n            next_advantage_learning_update = v_st_plus_1_target[i] - q_st_plus_1_target[i, selected_actions[i]]\n            # Persistent Advantage Learning or Regular Advantage Learning\n            if self.persistent:\n                TD_targets[i, batch.actions()[i]] -= self.alpha * min(advantage_learning_update, next_advantage_learning_update)\n            else:\n                TD_targets[i, batch.actions()[i]] -= self.alpha * advantage_learning_update\n\n            # mixing monte carlo updates\n            monte_carlo_target = total_returns[i]\n            TD_targets[i, batch.actions()[i]] = (1 - self.monte_carlo_mixing_rate) * TD_targets[i, batch.actions()[i]] \\\n                                        + self.monte_carlo_mixing_rate * monte_carlo_target\n\n        result = self.networks[\'main\'].train_and_sync_networks(batch.states(network_keys), TD_targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n'"
rl_coach/agents/policy_gradients_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.policy_optimization_agent import PolicyOptimizationAgent, PolicyGradientRescaler\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import PolicyHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import NetworkParameters, AlgorithmParameters, \\\n    AgentParameters\n\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.exploration_policies.categorical import CategoricalParameters\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.episodic.single_episode_buffer import SingleEpisodeBufferParameters\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\n\n\nclass PolicyGradientNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.heads_parameters = [PolicyHeadParameters()]\n        self.async_training = True\n\n\nclass PolicyGradientAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param policy_gradient_rescaler: (PolicyGradientRescaler)\n        The rescaler type to use for the policy gradient loss. For policy gradients, we calculate log probability of\n        the action and then multiply it by the policy gradient rescaler. The most basic rescaler is the discounter\n        return, but there are other rescalers that are intended for reducing the variance of the updates.\n\n    :param apply_gradients_every_x_episodes: (int)\n        The number of episodes between applying the accumulated gradients to the network. After every\n        num_steps_between_gradient_updates steps, the agent will calculate the gradients for the collected data,\n        it will then accumulate it in internal accumulators, and will only apply them to the network once in every\n        apply_gradients_every_x_episodes episodes.\n\n    :param beta_entropy: (float)\n        A factor which defines the amount of entropy regularization to apply to the network. The entropy of the actions\n        will be added to the loss and scaled by the given beta factor.\n\n    :param num_steps_between_gradient_updates: (int)\n        The number of steps between calculating gradients for the collected data. In the A3C paper, this parameter is\n        called t_max. Since this algorithm is on-policy, only the steps collected between each two gradient calculations\n        are used in the batch.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.policy_gradient_rescaler = PolicyGradientRescaler.FUTURE_RETURN_NORMALIZED_BY_TIMESTEP\n        self.apply_gradients_every_x_episodes = 5\n        self.beta_entropy = 0\n        self.num_steps_between_gradient_updates = 20000  # this is called t_max in all the papers\n\n\nclass PolicyGradientsAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=PolicyGradientAlgorithmParameters(),\n                         exploration={DiscreteActionSpace: CategoricalParameters(),\n                                      BoxActionSpace: AdditiveNoiseParameters()},\n                         memory=SingleEpisodeBufferParameters(),\n                         networks={""main"": PolicyGradientNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.policy_gradients_agent:PolicyGradientsAgent\'\n\n\nclass PolicyGradientsAgent(PolicyOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.returns_mean = self.register_signal(\'Returns Mean\')\n        self.returns_variance = self.register_signal(\'Returns Variance\')\n        self.last_gradient_update_step_idx = 0\n\n    def learn_from_batch(self, batch):\n        # batch contains a list of episodes to learn from\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        total_returns = batch.n_step_discounted_rewards()\n        for i in reversed(range(batch.size)):\n            if self.policy_gradient_rescaler == PolicyGradientRescaler.TOTAL_RETURN:\n                total_returns[i] = total_returns[0]\n            elif self.policy_gradient_rescaler == PolicyGradientRescaler.FUTURE_RETURN:\n                # just take the total return as it is\n                pass\n            elif self.policy_gradient_rescaler == PolicyGradientRescaler.FUTURE_RETURN_NORMALIZED_BY_EPISODE:\n                # we can get a single transition episode while playing Doom Basic, causing the std to be 0\n                if self.std_discounted_return != 0:\n                    total_returns[i] = (total_returns[i] - self.mean_discounted_return) / self.std_discounted_return\n                else:\n                    total_returns[i] = 0\n            elif self.policy_gradient_rescaler == PolicyGradientRescaler.FUTURE_RETURN_NORMALIZED_BY_TIMESTEP:\n                total_returns[i] -= self.mean_return_over_multiple_episodes[i]\n            else:\n                screen.warning(""WARNING: The requested policy gradient rescaler is not available"")\n\n        targets = total_returns\n        actions = batch.actions()\n        if type(self.spaces.action) != DiscreteActionSpace and len(actions.shape) < 2:\n            actions = np.expand_dims(actions, -1)\n\n        self.returns_mean.add_sample(np.mean(total_returns))\n        self.returns_variance.add_sample(np.std(total_returns))\n\n        result = self.networks[\'main\'].online_network.accumulate_gradients(\n            {**batch.states(network_keys), \'output_0_0\': actions}, targets\n        )\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n'"
rl_coach/agents/policy_optimization_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom collections import OrderedDict\nfrom enum import Enum\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.agent import Agent\nfrom rl_coach.core_types import Batch, ActionInfo\nfrom rl_coach.logger import screen\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\nfrom rl_coach.utils import eps\n\n\nclass PolicyGradientRescaler(Enum):\n    TOTAL_RETURN = 0\n    FUTURE_RETURN = 1\n    FUTURE_RETURN_NORMALIZED_BY_EPISODE = 2\n    FUTURE_RETURN_NORMALIZED_BY_TIMESTEP = 3  # baselined\n    Q_VALUE = 4\n    A_VALUE = 5\n    TD_RESIDUAL = 6\n    DISCOUNTED_TD_RESIDUAL = 7\n    GAE = 8\n\n\n## This is an abstract agent - there is no learn_from_batch method ##\n\n\nclass PolicyOptimizationAgent(Agent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n        self.policy_gradient_rescaler = None\n        if hasattr(self.ap.algorithm, \'policy_gradient_rescaler\'):\n            self.policy_gradient_rescaler = self.ap.algorithm.policy_gradient_rescaler\n\n        # statistics for variance reduction\n        self.last_gradient_update_step_idx = 0\n        self.max_episode_length = 100000\n        self.mean_return_over_multiple_episodes = np.zeros(self.max_episode_length)\n        self.num_episodes_where_step_has_been_seen = np.zeros(self.max_episode_length)\n        self.entropy = self.register_signal(\'Entropy\')\n\n    def log_to_screen(self):\n        # log to screen\n        log = OrderedDict()\n        log[""Name""] = self.full_name_id\n        if self.task_id is not None:\n            log[""Worker""] = self.task_id\n        log[""Episode""] = self.current_episode\n        log[""Total reward""] = round(self.total_reward_in_current_episode, 2)\n        log[""Steps""] = self.total_steps_counter\n        log[""Training iteration""] = self.training_iteration\n        screen.log_dict(log, prefix=self.phase.value)\n\n    def update_episode_statistics(self, episode):\n        episode_discounted_returns = []\n        for i in range(episode.length()):\n            transition = episode.get_transition(i)\n            episode_discounted_returns.append(transition.n_step_discounted_rewards)\n            self.num_episodes_where_step_has_been_seen[i] += 1\n            self.mean_return_over_multiple_episodes[i] -= self.mean_return_over_multiple_episodes[i] / \\\n                                                          self.num_episodes_where_step_has_been_seen[i]\n            self.mean_return_over_multiple_episodes[i] += transition.n_step_discounted_rewards / \\\n                                                          self.num_episodes_where_step_has_been_seen[i]\n        self.mean_discounted_return = np.mean(episode_discounted_returns)\n        self.std_discounted_return = np.std(episode_discounted_returns)\n\n    def train(self):\n        episode = self.current_episode_buffer\n\n        # check if we should calculate gradients or skip\n        num_steps_passed_since_last_update = episode.length() - self.last_gradient_update_step_idx\n        is_t_max_steps_passed = num_steps_passed_since_last_update >= self.ap.algorithm.num_steps_between_gradient_updates\n        if not (is_t_max_steps_passed or episode.is_complete):\n            return 0\n\n        total_loss = 0\n        if num_steps_passed_since_last_update > 0:\n            for network in self.networks.values():\n                network.set_is_training(True)\n\n            # we need to update the returns of the episode until now\n            episode.update_transitions_rewards_and_bootstrap_data()\n\n            # get t_max transitions or less if the we got to a terminal state\n            # will be used for both actor-critic and vanilla PG.\n            # In order to get full episodes, Vanilla PG will set the end_idx to a very big value.\n            transitions = episode[self.last_gradient_update_step_idx:]\n            batch = Batch(transitions)\n\n            # move the pointer for the last update step\n            if episode.is_complete:\n                self.last_gradient_update_step_idx = 0\n            else:\n                self.last_gradient_update_step_idx = episode.length()\n\n            # update the statistics for the variance reduction techniques\n            if self.policy_gradient_rescaler in \\\n                    [PolicyGradientRescaler.FUTURE_RETURN_NORMALIZED_BY_EPISODE,\n                     PolicyGradientRescaler.FUTURE_RETURN_NORMALIZED_BY_TIMESTEP]:\n                self.update_episode_statistics(episode)\n\n            # accumulate the gradients\n            total_loss, losses, unclipped_grads = self.learn_from_batch(batch)\n\n            # apply the gradients once in every apply_gradients_every_x_episodes episodes\n            if self.current_episode % self.ap.algorithm.apply_gradients_every_x_episodes == 0:\n                for network in self.networks.values():\n                    network.apply_gradients_and_sync_networks()\n            self.training_iteration += 1\n\n            for network in self.networks.values():\n                network.set_is_training(False)\n\n            # run additional commands after the training is done\n            self.post_training_commands()\n\n        return total_loss\n\n    def learn_from_batch(self, batch):\n        raise NotImplementedError(""PolicyOptimizationAgent is an abstract agent. Not to be used directly."")\n\n    def get_prediction(self, states):\n        tf_input_state = self.prepare_batch_for_inference(states, ""main"")\n        return self.networks[\'main\'].online_network.predict(tf_input_state)\n\n    def choose_action(self, curr_state):\n        # convert to batch so we can run it through the network\n        action_values = self.get_prediction(curr_state)\n        if isinstance(self.spaces.action, DiscreteActionSpace):\n            # DISCRETE\n            action_probabilities = np.array(action_values).squeeze()\n            action, _ = self.exploration_policy.get_action(action_probabilities)\n            action_info = ActionInfo(action=action,\n                                     all_action_probabilities=action_probabilities)\n\n            self.entropy.add_sample(-np.sum(action_probabilities * np.log(action_probabilities + eps)))\n        elif isinstance(self.spaces.action, BoxActionSpace):\n            # CONTINUOUS\n            action = self.exploration_policy.get_action(action_values)\n\n            action_info = ActionInfo(action=action)\n        else:\n            raise ValueError(""The action space of the environment is not compatible with the algorithm"")\n        return action_info\n'"
rl_coach/agents/ppo_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom collections import OrderedDict\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.actor_critic_agent import ActorCriticAgent\nfrom rl_coach.agents.policy_optimization_agent import PolicyGradientRescaler\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import PPOHeadParameters, VHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, NetworkParameters, \\\n    AgentParameters, DistributedTaskParameters\n\nfrom rl_coach.core_types import EnvironmentSteps, Batch\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.exploration_policies.categorical import CategoricalParameters\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\nfrom rl_coach.utils import force_list\n\n\nclass PPOCriticNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(activation_function=\'tanh\')}\n        self.middleware_parameters = FCMiddlewareParameters(activation_function=\'tanh\')\n        self.heads_parameters = [VHeadParameters()]\n        self.async_training = True\n        self.l2_regularization = 0\n        self.create_target_network = True\n        self.batch_size = 128\n\n\nclass PPOActorNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(activation_function=\'tanh\')}\n        self.middleware_parameters = FCMiddlewareParameters(activation_function=\'tanh\')\n        self.heads_parameters = [PPOHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.async_training = True\n        self.l2_regularization = 0\n        self.create_target_network = True\n        self.batch_size = 128\n\n\nclass PPOAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param policy_gradient_rescaler: (PolicyGradientRescaler)\n        This represents how the critic will be used to update the actor. The critic value function is typically used\n        to rescale the gradients calculated by the actor. There are several ways for doing this, such as using the\n        advantage of the action, or the generalized advantage estimation (GAE) value.\n\n    :param gae_lambda: (float)\n        The :math:`\\lambda` value is used within the GAE function in order to weight different bootstrap length\n        estimations. Typical values are in the range 0.9-1, and define an exponential decay over the different\n        n-step estimations.\n\n    :param target_kl_divergence: (float)\n        The target kl divergence between the current policy distribution and the new policy. PPO uses a heuristic to\n        bring the KL divergence to this value, by adding a penalty if the kl divergence is higher.\n\n    :param initial_kl_coefficient: (float)\n        The initial weight that will be given to the KL divergence between the current and the new policy in the\n        regularization factor.\n\n    :param high_kl_penalty_coefficient: (float)\n        The penalty that will be given for KL divergence values which are highes than what was defined as the target.\n\n    :param clip_likelihood_ratio_using_epsilon: (float)\n        If not None, the likelihood ratio between the current and new policy in the PPO loss function will be\n        clipped to the range [1-clip_likelihood_ratio_using_epsilon, 1+clip_likelihood_ratio_using_epsilon].\n        This is typically used in the Clipped PPO version of PPO, and should be set to None in regular PPO\n        implementations.\n\n    :param value_targets_mix_fraction: (float)\n        The targets for the value network are an exponential weighted moving average which uses this mix fraction to\n        define how much of the new targets will be taken into account when calculating the loss.\n        This value should be set to the range (0,1], where 1 means that only the new targets will be taken into account.\n\n    :param estimate_state_value_using_gae: (bool)\n        If set to True, the state value will be estimated using the GAE technique.\n\n    :param use_kl_regularization: (bool)\n        If set to True, the loss function will be regularized using the KL diveregence between the current and new\n        policy, to bound the change of the policy during the network update.\n\n    :param beta_entropy: (float)\n        An entropy regulaization term can be added to the loss function in order to control exploration. This term\n        is weighted using the :math:`\\beta` value defined by beta_entropy.\n\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.policy_gradient_rescaler = PolicyGradientRescaler.GAE\n        self.gae_lambda = 0.96\n        self.target_kl_divergence = 0.01\n        self.initial_kl_coefficient = 1.0\n        self.high_kl_penalty_coefficient = 1000\n        self.clip_likelihood_ratio_using_epsilon = None\n        self.value_targets_mix_fraction = 0.1\n        self.estimate_state_value_using_gae = True\n        self.use_kl_regularization = True\n        self.beta_entropy = 0.01\n        self.num_consecutive_playing_steps = EnvironmentSteps(5000)\n        self.act_for_full_episodes = True\n\n\nclass PPOAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=PPOAlgorithmParameters(),\n                         exploration={DiscreteActionSpace: CategoricalParameters(),\n                                      BoxActionSpace: AdditiveNoiseParameters()},\n                         memory=EpisodicExperienceReplayParameters(),\n                         networks={""critic"": PPOCriticNetworkParameters(), ""actor"": PPOActorNetworkParameters()})\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.ppo_agent:PPOAgent\'\n\n\n# Proximal Policy Optimization - https://arxiv.org/pdf/1707.06347.pdf\nclass PPOAgent(ActorCriticAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n        # signals definition\n        self.value_loss = self.register_signal(\'Value Loss\')\n        self.policy_loss = self.register_signal(\'Policy Loss\')\n        self.kl_divergence = self.register_signal(\'KL Divergence\')\n        self.total_kl_divergence_during_training_process = 0.0\n        self.unclipped_grads = self.register_signal(\'Grads (unclipped)\')\n\n    def fill_advantages(self, batch):\n        batch = Batch(batch)\n        network_keys = self.ap.network_wrappers[\'critic\'].input_embedders_parameters.keys()\n\n        # * Found not to have any impact *\n        # current_states_with_timestep = self.concat_state_and_timestep(batch)\n\n        current_state_values = self.networks[\'critic\'].online_network.predict(batch.states(network_keys)).squeeze()\n        total_returns = batch.n_step_discounted_rewards()\n        # calculate advantages\n        advantages = []\n        if self.policy_gradient_rescaler == PolicyGradientRescaler.A_VALUE:\n            advantages = total_returns - current_state_values\n        elif self.policy_gradient_rescaler == PolicyGradientRescaler.GAE:\n            # get bootstraps\n            episode_start_idx = 0\n            advantages = np.array([])\n            # current_state_values[batch.game_overs()] = 0\n            for idx, game_over in enumerate(batch.game_overs()):\n                if game_over:\n                    # get advantages for the rollout\n                    value_bootstrapping = np.zeros((1,))\n                    rollout_state_values = np.append(current_state_values[episode_start_idx:idx+1], value_bootstrapping)\n\n                    rollout_advantages, _ = \\\n                        self.get_general_advantage_estimation_values(batch.rewards()[episode_start_idx:idx+1],\n                                                                     rollout_state_values)\n                    episode_start_idx = idx + 1\n                    advantages = np.append(advantages, rollout_advantages)\n        else:\n            screen.warning(""WARNING: The requested policy gradient rescaler is not available"")\n\n        # standardize\n        advantages = (advantages - np.mean(advantages)) / np.std(advantages)\n\n        # TODO: this will be problematic with a shared memory\n        for transition, advantage in zip(self.memory.transitions, advantages):\n            transition.info[\'advantage\'] = advantage\n\n        self.action_advantages.add_sample(advantages)\n\n    def train_value_network(self, dataset, epochs):\n        loss = []\n        batch = Batch(dataset)\n        network_keys = self.ap.network_wrappers[\'critic\'].input_embedders_parameters.keys()\n\n        # * Found not to have any impact *\n        # add a timestep to the observation\n        # current_states_with_timestep = self.concat_state_and_timestep(dataset)\n\n        mix_fraction = self.ap.algorithm.value_targets_mix_fraction\n        total_returns = batch.n_step_discounted_rewards(True)\n        for j in range(epochs):\n            curr_batch_size = batch.size\n            if self.networks[\'critic\'].online_network.optimizer_type != \'LBFGS\':\n                curr_batch_size = self.ap.network_wrappers[\'critic\'].batch_size\n            for i in range(batch.size // curr_batch_size):\n                # split to batches for first order optimization techniques\n                current_states_batch = {\n                    k: v[i * curr_batch_size:(i + 1) * curr_batch_size]\n                    for k, v in batch.states(network_keys).items()\n                }\n                total_return_batch = total_returns[i * curr_batch_size:(i + 1) * curr_batch_size]\n                old_policy_values = force_list(self.networks[\'critic\'].target_network.predict(\n                    current_states_batch).squeeze())\n                if self.networks[\'critic\'].online_network.optimizer_type != \'LBFGS\':\n                    targets = total_return_batch\n                else:\n                    current_values = self.networks[\'critic\'].online_network.predict(current_states_batch)\n                    targets = current_values * (1 - mix_fraction) + total_return_batch * mix_fraction\n\n                inputs = copy.copy(current_states_batch)\n                for input_index, input in enumerate(old_policy_values):\n                    name = \'output_0_{}\'.format(input_index)\n                    if name in self.networks[\'critic\'].online_network.inputs:\n                        inputs[name] = input\n\n                value_loss = self.networks[\'critic\'].online_network.accumulate_gradients(inputs, targets)\n\n                self.networks[\'critic\'].apply_gradients_to_online_network()\n                if isinstance(self.ap.task_parameters, DistributedTaskParameters):\n                    self.networks[\'critic\'].apply_gradients_to_global_network()\n                self.networks[\'critic\'].online_network.reset_accumulated_gradients()\n\n                loss.append([value_loss[0]])\n        loss = np.mean(loss, 0)\n        return loss\n\n    def concat_state_and_timestep(self, dataset):\n        current_states_with_timestep = [np.append(transition.state[\'observation\'], transition.info[\'timestep\'])\n                                        for transition in dataset]\n        current_states_with_timestep = np.expand_dims(current_states_with_timestep, -1)\n        return current_states_with_timestep\n\n    def train_policy_network(self, dataset, epochs):\n        loss = []\n        for j in range(epochs):\n            loss = {\n                \'total_loss\': [],\n                \'policy_losses\': [],\n                \'unclipped_grads\': [],\n                \'fetch_result\': []\n            }\n            #shuffle(dataset)\n            for i in range(len(dataset) // self.ap.network_wrappers[\'actor\'].batch_size):\n                batch = Batch(dataset[i * self.ap.network_wrappers[\'actor\'].batch_size:\n                                      (i + 1) * self.ap.network_wrappers[\'actor\'].batch_size])\n\n                network_keys = self.ap.network_wrappers[\'actor\'].input_embedders_parameters.keys()\n\n                advantages = batch.info(\'advantage\')\n                actions = batch.actions()\n                if not isinstance(self.spaces.action, DiscreteActionSpace) and len(actions.shape) == 1:\n                    actions = np.expand_dims(actions, -1)\n\n                # get old policy probabilities and distribution\n                old_policy = force_list(self.networks[\'actor\'].target_network.predict(batch.states(network_keys)))\n\n                # calculate gradients and apply on both the local policy network and on the global policy network\n                fetches = [self.networks[\'actor\'].online_network.output_heads[0].kl_divergence,\n                           self.networks[\'actor\'].online_network.output_heads[0].entropy]\n\n                inputs = copy.copy(batch.states(network_keys))\n                inputs[\'output_0_0\'] = actions\n\n                # old_policy_distribution needs to be represented as a list, because in the event of discrete controls,\n                # it has just a mean. otherwise, it has both a mean and standard deviation\n                for input_index, input in enumerate(old_policy):\n                    inputs[\'output_0_{}\'.format(input_index + 1)] = input\n\n                total_loss, policy_losses, unclipped_grads, fetch_result =\\\n                    self.networks[\'actor\'].online_network.accumulate_gradients(\n                        inputs, [advantages], additional_fetches=fetches)\n\n                self.networks[\'actor\'].apply_gradients_to_online_network()\n                if isinstance(self.ap.task_parameters, DistributedTaskParameters):\n                    self.networks[\'actor\'].apply_gradients_to_global_network()\n\n                self.networks[\'actor\'].online_network.reset_accumulated_gradients()\n\n                loss[\'total_loss\'].append(total_loss)\n                loss[\'policy_losses\'].append(policy_losses)\n                loss[\'unclipped_grads\'].append(unclipped_grads)\n                loss[\'fetch_result\'].append(fetch_result)\n\n                self.unclipped_grads.add_sample(unclipped_grads)\n\n            for key in loss.keys():\n                loss[key] = np.mean(loss[key], 0)\n\n            if self.ap.network_wrappers[\'critic\'].learning_rate_decay_rate != 0:\n                curr_learning_rate = self.networks[\'critic\'].online_network.get_variable_value(self.ap.learning_rate)\n                self.curr_learning_rate.add_sample(curr_learning_rate)\n            else:\n                curr_learning_rate = self.ap.network_wrappers[\'critic\'].learning_rate\n\n            # log training parameters\n            screen.log_dict(\n                OrderedDict([\n                    (""Surrogate loss"", loss[\'policy_losses\'][0]),\n                    (""KL divergence"", loss[\'fetch_result\'][0]),\n                    (""Entropy"", loss[\'fetch_result\'][1]),\n                    (""training epoch"", j),\n                    (""learning_rate"", curr_learning_rate)\n                ]),\n                prefix=""Policy training""\n            )\n\n        self.total_kl_divergence_during_training_process = loss[\'fetch_result\'][0]\n        self.entropy.add_sample(loss[\'fetch_result\'][1])\n        self.kl_divergence.add_sample(loss[\'fetch_result\'][0])\n        return loss[\'total_loss\']\n\n    def update_kl_coefficient(self):\n        # John Schulman takes the mean kl divergence only over the last epoch which is strange but we will follow\n        # his implementation for now because we know it works well\n        screen.log_title(""KL = {}"".format(self.total_kl_divergence_during_training_process))\n\n        # update kl coefficient\n        kl_target = self.ap.algorithm.target_kl_divergence\n        kl_coefficient = self.networks[\'actor\'].online_network.get_variable_value(\n            self.networks[\'actor\'].online_network.output_heads[0].kl_coefficient)\n        new_kl_coefficient = kl_coefficient\n        if self.total_kl_divergence_during_training_process > 1.3 * kl_target:\n            # kl too high => increase regularization\n            new_kl_coefficient *= 1.5\n        elif self.total_kl_divergence_during_training_process < 0.7 * kl_target:\n            # kl too low => decrease regularization\n            new_kl_coefficient /= 1.5\n\n        # update the kl coefficient variable\n        if kl_coefficient != new_kl_coefficient:\n            self.networks[\'actor\'].online_network.set_variable_value(\n                self.networks[\'actor\'].online_network.output_heads[0].assign_kl_coefficient,\n                new_kl_coefficient,\n                self.networks[\'actor\'].online_network.output_heads[0].kl_coefficient_ph)\n\n        screen.log_title(""KL penalty coefficient change = {} -> {}"".format(kl_coefficient, new_kl_coefficient))\n\n    def post_training_commands(self):\n        if self.ap.algorithm.use_kl_regularization:\n            self.update_kl_coefficient()\n\n        # clean memory\n        self.call_memory(\'clean\')\n\n    def train(self):\n        loss = 0\n        if self._should_train():\n            for network in self.networks.values():\n                network.set_is_training(True)\n\n            for training_step in range(self.ap.algorithm.num_consecutive_training_steps):\n                self.networks[\'actor\'].sync()\n                self.networks[\'critic\'].sync()\n\n                dataset = self.memory.transitions\n\n                self.fill_advantages(dataset)\n\n                # take only the requested number of steps\n                dataset = dataset[:self.ap.algorithm.num_consecutive_playing_steps.num_steps]\n\n                value_loss = self.train_value_network(dataset, 1)\n                policy_loss = self.train_policy_network(dataset, 10)\n\n                self.value_loss.add_sample(value_loss)\n                self.policy_loss.add_sample(policy_loss)\n\n            for network in self.networks.values():\n                network.set_is_training(False)\n\n            self.post_training_commands()\n            self.training_iteration += 1\n            self.update_log()  # should be done in order to update the data that has been accumulated * while not playing *\n            return np.append(value_loss, policy_loss)\n\n    def get_prediction(self, states):\n        tf_input_state = self.prepare_batch_for_inference(states, ""actor"")\n        return self.networks[\'actor\'].online_network.predict(tf_input_state)\n\n'"
rl_coach/agents/qr_dqn_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom copy import copy\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.dqn_agent import DQNAgentParameters, DQNNetworkParameters, DQNAlgorithmParameters\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.architectures.head_parameters import QuantileRegressionQHeadParameters\nfrom rl_coach.core_types import StateType\nfrom rl_coach.schedules import LinearSchedule\n\n\nclass QuantileRegressionDQNNetworkParameters(DQNNetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.heads_parameters = [QuantileRegressionQHeadParameters()]\n        self.learning_rate = 0.00005\n        self.optimizer_epsilon = 0.01 / 32\n\n\nclass QuantileRegressionDQNAlgorithmParameters(DQNAlgorithmParameters):\n    """"""\n    :param atoms: (int)\n        the number of atoms to predict for each action\n\n    :param huber_loss_interval: (float)\n        One of the huber loss parameters, and is referred to as :math:`\\kapa` in the paper.\n        It describes the interval [-k, k] in which the huber loss acts as a MSE loss.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.atoms = 200\n        self.huber_loss_interval = 1  # called k in the paper\n\n\nclass QuantileRegressionDQNAgentParameters(DQNAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm = QuantileRegressionDQNAlgorithmParameters()\n        self.network_wrappers = {""main"": QuantileRegressionDQNNetworkParameters()}\n        self.exploration.epsilon_schedule = LinearSchedule(1, 0.01, 1000000)\n        self.exploration.evaluation_epsilon = 0.001\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.qr_dqn_agent:QuantileRegressionDQNAgent\'\n\n\n# Quantile Regression Deep Q Network - https://arxiv.org/pdf/1710.10044v1.pdf\nclass QuantileRegressionDQNAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.quantile_probabilities = np.ones(self.ap.algorithm.atoms) / float(self.ap.algorithm.atoms)\n\n    def get_q_values(self, quantile_values):\n        return np.dot(quantile_values, self.quantile_probabilities)\n\n    # prediction\'s format is (batch,actions,atoms)\n    def get_all_q_values_for_states(self, states: StateType):\n        if self.exploration_policy.requires_action_values():\n            quantile_values = self.get_prediction(states)\n            actions_q_values = self.get_q_values(quantile_values)\n        else:\n            actions_q_values = None\n        return actions_q_values\n\n    # prediction\'s format is (batch,actions,atoms)\n    def get_all_q_values_for_states_and_softmax_probabilities(self, states: StateType):\n        actions_q_values, softmax_probabilities = None, None\n        if self.exploration_policy.requires_action_values():\n            outputs = copy(self.networks[\'main\'].online_network.outputs)\n            outputs.append(self.networks[\'main\'].online_network.output_heads[0].softmax)\n            quantile_values, softmax_probabilities = self.get_prediction(states, outputs)\n            actions_q_values = self.get_q_values(quantile_values)\n\n        return actions_q_values, softmax_probabilities\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        # get the quantiles of the next states and current states\n        next_state_quantiles, current_quantiles = self.networks[\'main\'].parallel_prediction([\n            (self.networks[\'main\'].target_network, batch.next_states(network_keys)),\n            (self.networks[\'main\'].online_network, batch.states(network_keys))\n        ])\n\n        # add Q value samples for logging\n        self.q_values.add_sample(self.get_q_values(current_quantiles))\n\n        # get the optimal actions to take for the next states\n        target_actions = np.argmax(self.get_q_values(next_state_quantiles), axis=1)\n\n        # calculate the Bellman update\n        batch_idx = list(range(batch.size))\n\n        TD_targets = batch.rewards(True) + (1.0 - batch.game_overs(True)) * self.ap.algorithm.discount \\\n                               * next_state_quantiles[batch_idx, target_actions]\n\n        # get the locations of the selected actions within the batch for indexing purposes\n        actions_locations = [[b, a] for b, a in zip(batch_idx, batch.actions())]\n\n        # calculate the cumulative quantile probabilities and reorder them to fit the sorted quantiles order\n        cumulative_probabilities = np.array(range(self.ap.algorithm.atoms + 1)) / float(self.ap.algorithm.atoms) # tau_i\n        quantile_midpoints = 0.5*(cumulative_probabilities[1:] + cumulative_probabilities[:-1])  # tau^hat_i\n        quantile_midpoints = np.tile(quantile_midpoints, (batch.size, 1))\n        sorted_quantiles = np.argsort(current_quantiles[batch_idx, batch.actions()])\n        for idx in range(batch.size):\n            quantile_midpoints[idx, :] = quantile_midpoints[idx, sorted_quantiles[idx]]\n\n        # train\n        result = self.networks[\'main\'].train_and_sync_networks({\n            **batch.states(network_keys),\n            \'output_0_0\': actions_locations,\n            \'output_0_1\': quantile_midpoints,\n        }, TD_targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n\n'"
rl_coach/agents/rainbow_dqn_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.agents.categorical_dqn_agent import CategoricalDQNAlgorithmParameters, \\\n    CategoricalDQNAgent, CategoricalDQNAgentParameters\nfrom rl_coach.agents.dqn_agent import DQNNetworkParameters\nfrom rl_coach.architectures.head_parameters import RainbowQHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import MiddlewareScheme\nfrom rl_coach.exploration_policies.parameter_noise import ParameterNoiseParameters\nfrom rl_coach.memories.non_episodic.prioritized_experience_replay import PrioritizedExperienceReplayParameters, \\\n    PrioritizedExperienceReplay\n\n\nclass RainbowDQNNetworkParameters(DQNNetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.heads_parameters = [RainbowQHeadParameters()]\n        self.middleware_parameters = FCMiddlewareParameters(scheme=MiddlewareScheme.Empty)\n\n\nclass RainbowDQNAlgorithmParameters(CategoricalDQNAlgorithmParameters):\n    """"""\n    :param n_step: (int)\n        The number of steps to bootstrap the network over. The first N-1 steps actual rewards will be accumulated\n        using an exponentially growing discount factor, and the Nth step will be bootstrapped from the network\n        prediction.\n\n    :param store_transitions_only_when_episodes_are_terminated: (bool)\n        If set to True, the transitions will be stored in an Episode object until the episode ends, and just then\n        written to the memory. This is useful since we want to calculate the N-step discounted rewards before saving the\n        transitions into the memory, and to do so we need the entire episode first.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.n_step = 3\n\n        # needed for n-step updates to work. i.e. waiting for a full episode to be closed before storing each transition\n        self.store_transitions_only_when_episodes_are_terminated = True\n\n\nclass RainbowDQNAgentParameters(CategoricalDQNAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm = RainbowDQNAlgorithmParameters()\n\n        # ParameterNoiseParameters is changing the network wrapper parameters. This line needs to be done first.\n        self.network_wrappers = {""main"": RainbowDQNNetworkParameters()}\n\n        self.exploration = ParameterNoiseParameters(self)\n        self.memory = PrioritizedExperienceReplayParameters()\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.rainbow_dqn_agent:RainbowDQNAgent\'\n\n\n# Rainbow Deep Q Network - https://arxiv.org/abs/1710.02298\n# Agent implementation is composed of:\n# 1. NoisyNets\n# 2. C51\n# 3. Prioritized ER\n# 4. DDQN\n# 5. Dueling DQN\n# 6. N-step returns\n\nclass RainbowDQNAgent(CategoricalDQNAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()\n\n        ddqn_selected_actions = np.argmax(self.distribution_prediction_to_q_values(\n            self.networks[\'main\'].online_network.predict(batch.next_states(network_keys))), axis=1)\n\n        # for the action we actually took, the error is calculated by the atoms distribution\n        # for all other actions, the error is 0\n        distributional_q_st_plus_n, TD_targets = self.networks[\'main\'].parallel_prediction([\n            (self.networks[\'main\'].target_network, batch.next_states(network_keys)),\n            (self.networks[\'main\'].online_network, batch.states(network_keys))\n        ])\n\n        # add Q value samples for logging\n        self.q_values.add_sample(self.distribution_prediction_to_q_values(TD_targets))\n\n        # only update the action that we have actually done in this transition (using the Double-DQN selected actions)\n        target_actions = ddqn_selected_actions\n        m = np.zeros((batch.size, self.z_values.size))\n\n        batches = np.arange(batch.size)\n        for j in range(self.z_values.size):\n            # we use batch.info(\'should_bootstrap_next_state\') instead of (1 - batch.game_overs()) since with n-step,\n            # we will not bootstrap for the last n-step transitions in the episode\n            tzj = np.fmax(np.fmin(batch.n_step_discounted_rewards() + batch.info(\'should_bootstrap_next_state\') *\n                                  (self.ap.algorithm.discount ** self.ap.algorithm.n_step) * self.z_values[j],\n                                  self.z_values[-1]), self.z_values[0])\n            bj = (tzj - self.z_values[0])/(self.z_values[1] - self.z_values[0])\n            u = (np.ceil(bj)).astype(int)\n            l = (np.floor(bj)).astype(int)\n            m[batches, l] += (distributional_q_st_plus_n[batches, target_actions, j] * (u - bj))\n            m[batches, u] += (distributional_q_st_plus_n[batches, target_actions, j] * (bj - l))\n\n        # total_loss = cross entropy between actual result above and predicted result for the given action\n        TD_targets[batches, batch.actions()] = m\n\n        # update errors in prioritized replay buffer\n        importance_weights = batch.info(\'weight\') if isinstance(self.memory, PrioritizedExperienceReplay) else None\n\n        result = self.networks[\'main\'].train_and_sync_networks(batch.states(network_keys), TD_targets,\n                                                               importance_weights=importance_weights)\n\n        total_loss, losses, unclipped_grads = result[:3]\n\n        # TODO: fix this spaghetti code\n        if isinstance(self.memory, PrioritizedExperienceReplay):\n            errors = losses[0][np.arange(batch.size), batch.actions()]\n            self.call_memory(\'update_priorities\', (batch.info(\'idx\'), errors))\n\n        return total_loss, losses, unclipped_grads\n\n'"
rl_coach/agents/soft_actor_critic_agent.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\nimport copy\nimport numpy as np\nfrom collections import OrderedDict\n\nfrom rl_coach.agents.agent import Agent\nfrom rl_coach.agents.policy_optimization_agent import PolicyOptimizationAgent\n\nfrom rl_coach.architectures.head_parameters import SACQHeadParameters,SACPolicyHeadParameters,VHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import AlgorithmParameters, NetworkParameters, AgentParameters, EmbedderScheme, MiddlewareScheme\nfrom rl_coach.core_types import ActionInfo, EnvironmentSteps, RunPhase\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.spaces import BoxActionSpace\n\n\n# There are 3 networks in SAC implementation. All have the same topology but parameters are not shared.\n# The networks are:\n# 1. State Value Network - SACValueNetwork\n# 2. Soft Q Value Network - SACCriticNetwork\n# 3. Policy Network - SACPolicyNetwork - currently supporting only Gaussian Policy\n\n\n# 1. State Value Network - SACValueNetwork\n# this is the state value network in SAC.\n# The network is trained to predict (regression) the state value in the max-entropy settings\n# The objective to be minimized is given in equation (5) in the paper:\n#\n# J(psi)= E_(s~D)[0.5*(V_psi(s)-y(s))^2]\n# where y(s) = E_(a~pi)[Q_theta(s,a)-log(pi(a|s))]\n\n\n# Default parameters for value network:\n# topology :\n#   input embedder : EmbedderScheme.Medium (Dense(256)) , relu activation\n#   middleware : EmbedderScheme.Medium (Dense(256)) , relu activation\n\n\nclass SACValueNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(activation_function=\'relu\')}\n        self.middleware_parameters = FCMiddlewareParameters(activation_function=\'relu\')\n        self.heads_parameters = [VHeadParameters(initializer=\'xavier\')]\n        self.rescale_gradient_from_head_by_factor = [1]\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 256\n        self.async_training = False\n        self.learning_rate = 0.0003     # 3e-4 see appendix D in the paper\n        self.create_target_network = True   # tau is set in SoftActorCriticAlgorithmParameters.rate_for_copying_weights_to_target\n\n\n# 2. Soft Q Value Network - SACCriticNetwork\n# the whole network is built in the SACQHeadParameters. we use empty input embedder and middleware\nclass SACCriticNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\n        self.middleware_parameters = FCMiddlewareParameters(scheme=MiddlewareScheme.Empty)\n        self.heads_parameters = [SACQHeadParameters()]      # SACQHeadParameters includes the topology of the head\n        self.rescale_gradient_from_head_by_factor = [1]\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 256\n        self.async_training = False\n        self.learning_rate = 0.0003\n        self.create_target_network = False\n\n\n# 3. policy Network\n# Default parameters for policy network:\n# topology :\n#   input embedder : EmbedderScheme.Medium (Dense(256)) , relu activation\n#   middleware : EmbedderScheme = [Dense(256)] , relu activation --> scheme should be overridden in preset\nclass SACPolicyNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(activation_function=\'relu\')}\n        self.middleware_parameters = FCMiddlewareParameters(activation_function=\'relu\')\n        self.heads_parameters = [SACPolicyHeadParameters()]\n        self.rescale_gradient_from_head_by_factor = [1]\n        self.optimizer_type = \'Adam\'\n        self.batch_size = 256\n        self.async_training = False\n        self.learning_rate = 0.0003\n        self.create_target_network = False\n        self.l2_regularization = 0      # weight decay regularization. not used in the original paper\n\n\n# Algorithm Parameters\n\nclass SoftActorCriticAlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param num_steps_between_copying_online_weights_to_target: (StepMethod)\n        The number of steps between copying the online network weights to the target network weights.\n\n    :param rate_for_copying_weights_to_target: (float)\n        When copying the online network weights to the target network weights, a soft update will be used, which\n        weight the new online network weights by rate_for_copying_weights_to_target. (Tau as defined in the paper)\n\n    :param use_deterministic_for_evaluation: (bool)\n        If True, during the evaluation phase, action are chosen deterministically according to the policy mean\n        and not sampled from the policy distribution.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(1)\n        self.rate_for_copying_weights_to_target = 0.005\n        self.use_deterministic_for_evaluation = True    # evaluate agent using deterministic policy (i.e. take the mean value)\n\n\nclass SoftActorCriticAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=SoftActorCriticAlgorithmParameters(),\n                         exploration=AdditiveNoiseParameters(),\n                         memory=ExperienceReplayParameters(),   # SAC doesnt use episodic related data\n                         # network wrappers:\n                         networks=OrderedDict([(""policy"", SACPolicyNetworkParameters()),\n                                               (""q"", SACCriticNetworkParameters()),\n                                               (""v"", SACValueNetworkParameters())]))\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.soft_actor_critic_agent:SoftActorCriticAgent\'\n\n\n# Soft Actor Critic - https://arxiv.org/abs/1801.01290\nclass SoftActorCriticAgent(PolicyOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.last_gradient_update_step_idx = 0\n\n        # register signals to track (in learn_from_batch)\n        self.policy_means = self.register_signal(\'Policy_mu_avg\')\n        self.policy_logsig = self.register_signal(\'Policy_logsig\')\n        self.policy_logprob_sampled = self.register_signal(\'Policy_logp_sampled\')\n        self.policy_grads = self.register_signal(\'Policy_grads_sumabs\')\n\n        self.q1_values = self.register_signal(""Q1"")\n        self.TD_err1 = self.register_signal(""TD err1"")\n        self.q2_values = self.register_signal(""Q2"")\n        self.TD_err2 = self.register_signal(""TD err2"")\n        self.v_tgt_ns = self.register_signal(\'V_tgt_ns\')\n        self.v_onl_ys = self.register_signal(\'V_onl_ys\')\n        self.action_signal = self.register_signal(""actions"")\n\n    def learn_from_batch(self, batch):\n        #########################################\n        # need to update the following networks:\n        # 1. actor (policy)\n        # 2. state value (v)\n        # 3. critic (q1 and q2)\n        # 4. target network - probably already handled by V\n\n        #########################################\n        # define the networks to be used\n\n        # State Value Network\n        value_network = self.networks[\'v\']\n        value_network_keys = self.ap.network_wrappers[\'v\'].input_embedders_parameters.keys()\n\n        # Critic Network\n        q_network = self.networks[\'q\'].online_network\n        q_head = q_network.output_heads[0]\n        q_network_keys = self.ap.network_wrappers[\'q\'].input_embedders_parameters.keys()\n\n        # Actor (policy) Network\n        policy_network = self.networks[\'policy\'].online_network\n        policy_network_keys = self.ap.network_wrappers[\'policy\'].input_embedders_parameters.keys()\n\n        ##########################################\n        # 1. updating the actor - according to (13) in the paper\n        policy_inputs = copy.copy(batch.states(policy_network_keys))\n        policy_results = policy_network.predict(policy_inputs)\n\n        policy_mu, policy_std, sampled_raw_actions, sampled_actions, sampled_actions_logprob, \\\n        sampled_actions_logprob_mean = policy_results\n\n        self.policy_means.add_sample(policy_mu)\n        self.policy_logsig.add_sample(policy_std)\n        self.policy_logprob_sampled.add_sample(sampled_actions_logprob_mean)\n\n        # get the state-action values for the replayed states and their corresponding actions from the policy\n        q_inputs = copy.copy(batch.states(q_network_keys))\n        q_inputs[\'output_0_0\'] = sampled_actions\n        log_target = q_network.predict(q_inputs)[0].squeeze()\n\n        # log internal q values\n        q1_vals, q2_vals = q_network.predict(q_inputs, outputs=[q_head.q1_output, q_head.q2_output])\n        self.q1_values.add_sample(q1_vals)\n        self.q2_values.add_sample(q2_vals)\n\n        # calculate the gradients according to (13)\n        # get the gradients of log_prob w.r.t the weights (parameters) - indicated as phi in the paper\n        initial_feed_dict = {policy_network.gradients_weights_ph[5]: np.array(1.0)}\n        dlogp_dphi = policy_network.predict(policy_inputs,\n                                            outputs=policy_network.weighted_gradients[5],\n                                            initial_feed_dict=initial_feed_dict)\n\n        # calculate dq_da\n        dq_da = q_network.predict(q_inputs,\n                                  outputs=q_network.gradients_wrt_inputs[1][\'output_0_0\'])\n\n        # calculate da_dphi\n        initial_feed_dict = {policy_network.gradients_weights_ph[3]: dq_da}\n        dq_dphi = policy_network.predict(policy_inputs,\n                                         outputs=policy_network.weighted_gradients[3],\n                                         initial_feed_dict=initial_feed_dict)\n\n        # now given dlogp_dphi, dq_dphi we need to calculate the policy gradients according to (13)\n        policy_grads = [dlogp_dphi[l] - dq_dphi[l] for l in range(len(dlogp_dphi))]\n\n        # apply the gradients to policy networks\n        policy_network.apply_gradients(policy_grads)\n        grads_sumabs = np.sum([np.sum(np.abs(policy_grads[l])) for l in range(len(policy_grads))])\n        self.policy_grads.add_sample(grads_sumabs)\n\n        ##########################################\n        # 2. updating the state value online network weights\n        # done by calculating the targets for the v head according to (5) in the paper\n        # value_targets = log_targets-sampled_actions_logprob\n        value_inputs = copy.copy(batch.states(value_network_keys))\n        value_targets = log_target - sampled_actions_logprob\n\n        self.v_onl_ys.add_sample(value_targets)\n\n        # call value_network apply gradients with this target\n        value_loss = value_network.online_network.train_on_batch(value_inputs, value_targets[:,None])[0]\n\n        ##########################################\n        # 3. updating the critic (q networks)\n        # updating q networks according to (7) in the paper\n\n        # define the input to the q network: state has been already updated previously, but now we need\n        # the actions from the batch (and not those sampled by the policy)\n        q_inputs[\'output_0_0\'] = batch.actions(len(batch.actions().shape) == 1)\n\n        # define the targets : scale_reward * reward + (1-terminal)*discount*v_target_next_state\n        # define v_target_next_state\n        value_inputs = copy.copy(batch.next_states(value_network_keys))\n        v_target_next_state = value_network.target_network.predict(value_inputs)\n        self.v_tgt_ns.add_sample(v_target_next_state)\n        # Note: reward is assumed to be rescaled by RewardRescaleFilter in the preset parameters\n        TD_targets = batch.rewards(expand_dims=True) + \\\n                     (1.0 - batch.game_overs(expand_dims=True)) * self.ap.algorithm.discount * v_target_next_state\n\n        # call critic network update\n        result = q_network.train_on_batch(q_inputs, TD_targets, additional_fetches=[q_head.q1_loss, q_head.q2_loss])\n        total_loss, losses, unclipped_grads = result[:3]\n        q1_loss, q2_loss = result[3]\n        self.TD_err1.add_sample(q1_loss)\n        self.TD_err2.add_sample(q2_loss)\n\n        ##########################################\n        # 4. updating the value target network\n        # I just need to set the parameter rate_for_copying_weights_to_target in the agent parameters to be 1-tau\n        # where tau is the hyper parameter as defined in sac original implementation\n\n        return total_loss, losses, unclipped_grads\n\n    def get_prediction(self, states):\n        """"""\n        get the mean and stdev of the policy distribution given \'states\'\n        :param states: the states for which we need to sample actions from the policy\n        :return: mean and stdev\n        """"""\n        tf_input_state = self.prepare_batch_for_inference(states, \'policy\')\n        return self.networks[\'policy\'].online_network.predict(tf_input_state)\n\n    def train(self):\n        # since the algorithm works with experience replay buffer (non-episodic),\n        # we cant use the policy optimization train method. we need Agent.train\n        # note that since in Agent.train there is no apply_gradients, we need to do it in learn from batch\n        return Agent.train(self)\n\n    def choose_action(self, curr_state):\n        """"""\n        choose_action - chooses the most likely action\n        if \'deterministic\' - take the mean of the policy which is the prediction of the policy network.\n        else - use the exploration policy\n        :param curr_state:\n        :return: action wrapped in ActionInfo\n        """"""\n        if not isinstance(self.spaces.action, BoxActionSpace):\n            raise ValueError(""SAC works only for continuous control problems"")\n        # convert to batch so we can run it through the network\n        tf_input_state = self.prepare_batch_for_inference(curr_state, \'policy\')\n        # use the online network for prediction\n        policy_network = self.networks[\'policy\'].online_network\n        policy_head = policy_network.output_heads[0]\n        result = policy_network.predict(tf_input_state,\n                                        outputs=[policy_head.policy_mean, policy_head.actions])\n        action_mean, action_sample = result\n\n        # if using deterministic policy, take the mean values. else, use exploration policy to sample from the pdf\n        if self.phase == RunPhase.TEST and self.ap.algorithm.use_deterministic_for_evaluation:\n            action = action_mean[0]\n        else:\n            action = action_sample[0]\n\n        self.action_signal.add_sample(action)\n\n        action_info = ActionInfo(action=action)\n        return action_info\n'"
rl_coach/agents/td3_agent.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom typing import Union\nfrom collections import OrderedDict\n\nimport numpy as np\n\nfrom rl_coach.agents.agent import Agent\nfrom rl_coach.agents.ddpg_agent import DDPGAgent\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import DDPGActorHeadParameters, TD3VHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import NetworkParameters, AlgorithmParameters, \\\n    AgentParameters, EmbedderScheme\nfrom rl_coach.core_types import ActionInfo, TrainingSteps, Transition\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.spaces import BoxActionSpace, GoalsSpace\n\n\nclass TD3CriticNetworkParameters(NetworkParameters):\n    def __init__(self, num_q_networks):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters(),\n                                            \'action\': InputEmbedderParameters(scheme=EmbedderScheme.Shallow)}\n        self.middleware_parameters = FCMiddlewareParameters(num_streams=num_q_networks)\n        self.heads_parameters = [TD3VHeadParameters()]\n        self.optimizer_type = \'Adam\'\n        self.adam_optimizer_beta2 = 0.999\n        self.optimizer_epsilon = 1e-8\n        self.batch_size = 100\n        self.async_training = False\n        self.learning_rate = 0.001\n        self.create_target_network = True\n        self.shared_optimizer = True\n        self.scale_down_gradients_by_number_of_workers_for_sync_training = False\n\n\nclass TD3ActorNetworkParameters(NetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.input_embedders_parameters = {\'observation\': InputEmbedderParameters()}\n        self.middleware_parameters = FCMiddlewareParameters()\n        self.heads_parameters = [DDPGActorHeadParameters(batchnorm=False)]\n        self.optimizer_type = \'Adam\'\n        self.adam_optimizer_beta2 = 0.999\n        self.optimizer_epsilon = 1e-8\n        self.batch_size = 100\n        self.async_training = False\n        self.learning_rate = 0.001\n        self.create_target_network = True\n        self.shared_optimizer = True\n        self.scale_down_gradients_by_number_of_workers_for_sync_training = False\n\n\nclass TD3AlgorithmParameters(AlgorithmParameters):\n    """"""\n    :param num_steps_between_copying_online_weights_to_target: (StepMethod)\n        The number of steps between copying the online network weights to the target network weights.\n\n    :param rate_for_copying_weights_to_target: (float)\n        When copying the online network weights to the target network weights, a soft update will be used, which\n        weight the new online network weights by rate_for_copying_weights_to_target\n\n    :param num_consecutive_playing_steps: (StepMethod)\n        The number of consecutive steps to act between every two training iterations\n\n    :param use_target_network_for_evaluation: (bool)\n        If set to True, the target network will be used for predicting the actions when choosing actions to act.\n        Since the target network weights change more slowly, the predicted actions will be more consistent.\n\n    :param action_penalty: (float)\n        The amount by which to penalize the network on high action feature (pre-activation) values.\n        This can prevent the actions features from saturating the TanH activation function, and therefore prevent the\n        gradients from becoming very low.\n\n    :param clip_critic_targets: (Tuple[float, float] or None)\n        The range to clip the critic target to in order to prevent overestimation of the action values.\n\n    :param use_non_zero_discount_for_terminal_states: (bool)\n        If set to True, the discount factor will be used for terminal states to bootstrap the next predicted state\n        values. If set to False, the terminal states reward will be taken as the target return for the network.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self.rate_for_copying_weights_to_target = 0.005\n        self.use_target_network_for_evaluation = False\n        self.action_penalty = 0\n        self.clip_critic_targets = None  # expected to be a tuple of the form (min_clip_value, max_clip_value) or None\n        self.use_non_zero_discount_for_terminal_states = False\n        self.act_for_full_episodes = True\n        self.update_policy_every_x_episode_steps = 2\n        self.num_steps_between_copying_online_weights_to_target = TrainingSteps(self.update_policy_every_x_episode_steps)\n        self.policy_noise = 0.2\n        self.noise_clipping = 0.5\n        self.num_q_networks = 2\n\n\nclass TD3AgentExplorationParameters(AdditiveNoiseParameters):\n    def __init__(self):\n        super().__init__()\n        self.noise_as_percentage_from_action_space = False\n\n\nclass TD3AgentParameters(AgentParameters):\n    def __init__(self):\n        td3_algorithm_params = TD3AlgorithmParameters()\n        super().__init__(algorithm=td3_algorithm_params,\n                         exploration=TD3AgentExplorationParameters(),\n                         memory=EpisodicExperienceReplayParameters(),\n                         networks=OrderedDict([(""actor"", TD3ActorNetworkParameters()),\n                                               (""critic"",\n                                                TD3CriticNetworkParameters(td3_algorithm_params.num_q_networks))]))\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.td3_agent:TD3Agent\'\n\n\n# Twin Delayed DDPG - https://arxiv.org/pdf/1802.09477.pdf\nclass TD3Agent(DDPGAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n\n        self.q_values = self.register_signal(""Q"")\n        self.TD_targets_signal = self.register_signal(""TD targets"")\n        self.action_signal = self.register_signal(""actions"")\n\n    def learn_from_batch(self, batch):\n        actor = self.networks[\'actor\']\n        critic = self.networks[\'critic\']\n\n        actor_keys = self.ap.network_wrappers[\'actor\'].input_embedders_parameters.keys()\n        critic_keys = self.ap.network_wrappers[\'critic\'].input_embedders_parameters.keys()\n\n        # TD error = r + discount*max(q_st_plus_1) - q_st\n        next_actions, actions_mean = actor.parallel_prediction([\n            (actor.target_network, batch.next_states(actor_keys)),\n            (actor.online_network, batch.states(actor_keys))\n        ])\n\n        # add noise to the next_actions\n        noise = np.random.normal(0, self.ap.algorithm.policy_noise, next_actions.shape).clip(\n            -self.ap.algorithm.noise_clipping, self.ap.algorithm.noise_clipping)\n        next_actions = self.spaces.action.clip_action_to_space(next_actions + noise)\n\n        critic_inputs = copy.copy(batch.next_states(critic_keys))\n        critic_inputs[\'action\'] = next_actions\n        q_st_plus_1 = critic.target_network.predict(critic_inputs)[2]  # output #2 is the min (Q1, Q2)\n\n        # calculate the bootstrapped TD targets while discounting terminal states according to\n        # use_non_zero_discount_for_terminal_states\n        if self.ap.algorithm.use_non_zero_discount_for_terminal_states:\n            TD_targets = batch.rewards(expand_dims=True) + self.ap.algorithm.discount * q_st_plus_1\n        else:\n            TD_targets = batch.rewards(expand_dims=True) + \\\n                         (1.0 - batch.game_overs(expand_dims=True)) * self.ap.algorithm.discount * q_st_plus_1\n\n        # clip the TD targets to prevent overestimation errors\n        if self.ap.algorithm.clip_critic_targets:\n            TD_targets = np.clip(TD_targets, *self.ap.algorithm.clip_critic_targets)\n\n        self.TD_targets_signal.add_sample(TD_targets)\n\n        # train the critic\n        critic_inputs = copy.copy(batch.states(critic_keys))\n        critic_inputs[\'action\'] = batch.actions(len(batch.actions().shape) == 1)\n        result = critic.train_and_sync_networks(critic_inputs, TD_targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        if self.training_iteration % self.ap.algorithm.update_policy_every_x_episode_steps == 0:\n            # get the gradients of output #3 (=mean of Q1 network) w.r.t the action\n            critic_inputs = copy.copy(batch.states(critic_keys))\n            critic_inputs[\'action\'] = actions_mean\n            action_gradients = critic.online_network.predict(critic_inputs,\n                                                             outputs=critic.online_network.gradients_wrt_inputs[3][\'action\'])\n\n            # apply the gradients from the critic to the actor\n            initial_feed_dict = {actor.online_network.gradients_weights_ph[0]: -action_gradients}\n            gradients = actor.online_network.predict(batch.states(actor_keys),\n                                                     outputs=actor.online_network.weighted_gradients[0],\n                                                     initial_feed_dict=initial_feed_dict)\n\n            if actor.has_global:\n                actor.apply_gradients_to_global_network(gradients)\n                actor.update_online_network()\n            else:\n                actor.apply_gradients_to_online_network(gradients)\n\n        return total_loss, losses, unclipped_grads\n\n    def train(self):\n        self.ap.algorithm.num_consecutive_training_steps = self.current_episode_steps_counter\n        return Agent.train(self)\n\n    def update_transition_before_adding_to_replay_buffer(self, transition: Transition) -> Transition:\n        """"""\n        Allows agents to update the transition just before adding it to the replay buffer.\n        Can be useful for agents that want to tweak the reward, termination signal, etc.\n\n        :param transition: the transition to update\n        :return: the updated transition\n        """"""\n        transition.game_over = False if self.current_episode_steps_counter ==\\\n                                        self.parent_level_manager.environment.env._max_episode_steps\\\n            else transition.game_over\n\n        return transition'"
rl_coach/agents/value_optimization_agent.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom collections import OrderedDict\nfrom typing import Union, List\n\nimport numpy as np\n\nfrom rl_coach.agents.agent import Agent\nfrom rl_coach.core_types import ActionInfo, StateType, Batch\nfrom rl_coach.filters.filter import NoInputFilter\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.non_episodic.prioritized_experience_replay import PrioritizedExperienceReplay\nfrom rl_coach.spaces import DiscreteActionSpace\nfrom copy import deepcopy, copy\n\n\n## This is an abstract agent - there is no learn_from_batch method ##\n\n\nclass ValueOptimizationAgent(Agent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\']=None):\n        super().__init__(agent_parameters, parent)\n        self.q_values = self.register_signal(""Q"")\n        self.q_value_for_action = {}\n\n        # currently we use softmax action probabilities only in batch-rl,\n        # but we might want to extend this later at some point.\n        self.should_get_softmax_probabilities = \\\n            hasattr(self.ap.network_wrappers[\'main\'], \'should_get_softmax_probabilities\') and  \\\n            self.ap.network_wrappers[\'main\'].should_get_softmax_probabilities\n\n    def init_environment_dependent_modules(self):\n        super().init_environment_dependent_modules()\n        if isinstance(self.spaces.action, DiscreteActionSpace):\n            for i in range(len(self.spaces.action.actions)):\n                self.q_value_for_action[i] = self.register_signal(""Q for action {}"".format(i),\n                                                                  dump_one_value_per_episode=False,\n                                                                  dump_one_value_per_step=True)\n\n    # Algorithms for which q_values are calculated from predictions will override this function\n    def get_all_q_values_for_states(self, states: StateType):\n        actions_q_values = None\n        if self.exploration_policy.requires_action_values():\n            actions_q_values = self.get_prediction(states)\n\n        return actions_q_values\n\n    def get_all_q_values_for_states_and_softmax_probabilities(self, states: StateType):\n        actions_q_values, softmax_probabilities = None, None\n        if self.exploration_policy.requires_action_values():\n            outputs = copy(self.networks[\'main\'].online_network.outputs)\n            outputs.append(self.networks[\'main\'].online_network.output_heads[0].softmax)\n\n            actions_q_values, softmax_probabilities = self.get_prediction(states, outputs=outputs)\n        return actions_q_values, softmax_probabilities\n\n    def get_prediction(self, states, outputs=None):\n        return self.networks[\'main\'].online_network.predict(self.prepare_batch_for_inference(states, \'main\'),\n                                                            outputs=outputs)\n\n    def update_transition_priorities_and_get_weights(self, TD_errors, batch):\n        # update errors in prioritized replay buffer\n        importance_weights = None\n        if isinstance(self.memory, PrioritizedExperienceReplay):\n            self.call_memory(\'update_priorities\', (batch.info(\'idx\'), TD_errors))\n            importance_weights = batch.info(\'weight\')\n        return importance_weights\n\n    def _validate_action(self, policy, action):\n        if np.array(action).shape != ():\n            raise ValueError((\n                \'The exploration_policy {} returned a vector of actions \'\n                \'instead of a single action. ValueOptimizationAgents \'\n                \'require exploration policies which return a single action.\'\n            ).format(policy.__class__.__name__))\n\n    def choose_action(self, curr_state):\n        if self.should_get_softmax_probabilities:\n            actions_q_values, softmax_probabilities = \\\n                self.get_all_q_values_for_states_and_softmax_probabilities(curr_state)\n        else:\n            actions_q_values = self.get_all_q_values_for_states(curr_state)\n\n        # choose action according to the exploration policy and the current phase (evaluating or training the agent)\n        action, action_probabilities = self.exploration_policy.get_action(actions_q_values)\n        if self.should_get_softmax_probabilities and softmax_probabilities is not None:\n            # override the exploration policy\'s generated probabilities when an action was taken\n            # with the agent\'s actual policy\n            action_probabilities = softmax_probabilities\n\n        self._validate_action(self.exploration_policy, action)\n\n        if actions_q_values is not None:\n            # this is for bootstrapped dqn\n            if type(actions_q_values) == list and len(actions_q_values) > 0:\n                actions_q_values = self.exploration_policy.last_action_values\n\n            # store the q values statistics for logging\n            self.q_values.add_sample(actions_q_values)\n\n            actions_q_values = actions_q_values.squeeze()\n            action_probabilities = action_probabilities.squeeze()\n\n            for i, q_value in enumerate(actions_q_values):\n                self.q_value_for_action[i].add_sample(q_value)\n\n            action_info = ActionInfo(action=action,\n                                     action_value=actions_q_values[action],\n                                     max_action_value=np.max(actions_q_values),\n                                     all_action_probabilities=action_probabilities)\n\n        else:\n            action_info = ActionInfo(action=action, all_action_probabilities=action_probabilities)\n\n        return action_info\n\n    def learn_from_batch(self, batch):\n        raise NotImplementedError(""ValueOptimizationAgent is an abstract agent. Not to be used directly."")\n\n    def run_off_policy_evaluation(self):\n        """"""\n        Run the off-policy evaluation estimators to get a prediction for the performance of the current policy based on\n        an evaluation dataset, which was collected by another policy(ies).\n        :return: None\n        """"""\n        assert self.ope_manager\n\n        if not isinstance(self.pre_network_filter, NoInputFilter) and len(self.pre_network_filter.reward_filters) != 0:\n            raise ValueError(""Defining a pre-network reward filter when OPEs are calculated will result in a mismatch ""\n                             ""between q values (which are scaled), and actual rewards, which are not. It is advisable ""\n                             ""to use an input_filter, if possible, instead, which will filter the transitions directly ""\n                             ""in the replay buffer, affecting both the q_values and the rewards themselves. "")\n\n        ips, dm, dr, seq_dr, wis = self.ope_manager.evaluate(\n                                  evaluation_dataset_as_episodes=self.memory.evaluation_dataset_as_episodes,\n                                  evaluation_dataset_as_transitions=self.memory.evaluation_dataset_as_transitions,\n                                  batch_size=self.ap.network_wrappers[\'main\'].batch_size,\n                                  discount_factor=self.ap.algorithm.discount,\n                                  q_network=self.networks[\'main\'].online_network,\n                                  network_keys=list(self.ap.network_wrappers[\'main\'].input_embedders_parameters.keys()))\n\n        # get the estimators out to the screen\n        log = OrderedDict()\n        log[\'Epoch\'] = self.training_epoch\n        log[\'IPS\'] = ips\n        log[\'DM\'] = dm\n        log[\'DR\'] = dr\n        log[\'WIS\'] = wis\n        log[\'Sequential-DR\'] = seq_dr\n        screen.log_dict(log, prefix=\'Off-Policy Evaluation\')\n\n        # get the estimators out to dashboard\n        self.agent_logger.set_current_time(self.get_current_time() + 1)\n        self.agent_logger.create_signal_value(\'Inverse Propensity Score\', ips)\n        self.agent_logger.create_signal_value(\'Direct Method Reward\', dm)\n        self.agent_logger.create_signal_value(\'Doubly Robust\', dr)\n        self.agent_logger.create_signal_value(\'Sequential Doubly Robust\', seq_dr)\n        self.agent_logger.create_signal_value(\'Weighted Importance Sampling\', wis)\n\n    def get_reward_model_loss(self, batch: Batch):\n        network_keys = self.ap.network_wrappers[\'reward_model\'].input_embedders_parameters.keys()\n        current_rewards_prediction_for_all_actions = self.networks[\'reward_model\'].online_network.predict(\n            batch.states(network_keys))\n        current_rewards_prediction_for_all_actions[range(batch.size), batch.actions()] = batch.rewards()\n\n        return self.networks[\'reward_model\'].train_and_sync_networks(\n            batch.states(network_keys), current_rewards_prediction_for_all_actions)[0]\n\n    def improve_reward_model(self, epochs: int):\n        """"""\n        Train a reward model to be used by the doubly-robust estimator\n\n        :param epochs: The total number of epochs to use for training a reward model\n        :return: None\n        """"""\n        batch_size = self.ap.network_wrappers[\'reward_model\'].batch_size\n\n        # this is fitted from the training dataset\n        for epoch in range(epochs):\n            loss = 0\n            total_transitions_processed = 0\n            for i, batch in enumerate(self.call_memory(\'get_shuffled_training_data_generator\', batch_size)):\n                batch = Batch(batch)\n                loss += self.get_reward_model_loss(batch)\n                total_transitions_processed += batch.size\n\n            log = OrderedDict()\n            log[\'Epoch\'] = epoch\n            log[\'loss\'] = loss / total_transitions_processed\n            screen.log_dict(log, prefix=\'Training Reward Model\')\n\n'"
rl_coach/agents/wolpertinger_agent.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom typing import Union\nfrom collections import OrderedDict\nimport numpy as np\n\nfrom rl_coach.agents.ddpg_agent import DDPGAlgorithmParameters, DDPGActorNetworkParameters, \\\n    DDPGCriticNetworkParameters, DDPGAgent\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import ActionInfo\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters\nfrom rl_coach.memories.non_episodic.differentiable_neural_dictionary import AnnoyDictionary\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\nfrom rl_coach.architectures.head_parameters import WolpertingerActorHeadParameters\n\n\nclass WolpertingerCriticNetworkParameters(DDPGCriticNetworkParameters):\n    def __init__(self, use_batchnorm=False):\n        super().__init__(use_batchnorm=use_batchnorm)\n\n\nclass WolpertingerActorNetworkParameters(DDPGActorNetworkParameters):\n    def __init__(self, use_batchnorm=False):\n        super().__init__()\n        self.heads_parameters = [WolpertingerActorHeadParameters(batchnorm=use_batchnorm)]\n\n\nclass WolpertingerAlgorithmParameters(DDPGAlgorithmParameters):\n    def __init__(self):\n        super().__init__()\n        self.action_embedding_width = 1\n        self.k = 1\n        \n\nclass WolpertingerAgentParameters(AgentParameters):\n    def __init__(self, use_batchnorm=False):\n        exploration_params = AdditiveNoiseParameters()\n        exploration_params.noise_as_percentage_from_action_space = False\n\n        super().__init__(algorithm=WolpertingerAlgorithmParameters(),\n                         exploration=exploration_params,\n                         memory=EpisodicExperienceReplayParameters(),\n                         networks=OrderedDict(\n                             [(""actor"", WolpertingerActorNetworkParameters(use_batchnorm=use_batchnorm)),\n                              (""critic"", WolpertingerCriticNetworkParameters(use_batchnorm=use_batchnorm))]))\n\n    @property\n    def path(self):\n        return \'rl_coach.agents.wolpertinger_agent:WolpertingerAgent\'\n\n\n# Deep Reinforcement Learning in Large Discrete Action Spaces - https://arxiv.org/pdf/1512.07679.pdf\nclass WolpertingerAgent(DDPGAgent):\n    def __init__(self, agent_parameters, parent: Union[\'LevelManager\', \'CompositeAgent\'] = None):\n        super().__init__(agent_parameters, parent)\n\n    def learn_from_batch(self, batch):\n        # replay buffer holds the actions in the discrete manner, as the agent is expected to act with discrete actions\n        # with the BoxDiscretization output filter. But DDPG needs to work on continuous actions, thus converting to\n        # continuous actions. This is actually a duplicate since this filtering is also done before applying actions on\n        # the environment. So might want to somehow reuse that conversion. Maybe can hold this information in the info\n        # dictionary of the transition.\n\n        output_action_filter = \\\n            list(self.output_filter.action_filters.values())[0]\n        continuous_actions = []\n        for action in batch.actions():\n            continuous_actions.append(output_action_filter.filter(action))\n        batch._actions = np.array(continuous_actions).squeeze()\n\n        return super().learn_from_batch(batch)\n\n    def train(self):\n        return super().train()\n\n    def choose_action(self, curr_state):\n        if not isinstance(self.spaces.action, DiscreteActionSpace):\n            raise ValueError(""WolpertingerAgent works only for discrete control problems"")\n\n        # convert to batch so we can run it through the network\n        tf_input_state = self.prepare_batch_for_inference(curr_state, \'actor\')\n        actor_network = self.networks[\'actor\'].online_network\n        critic_network = self.networks[\'critic\'].online_network\n        proto_action = actor_network.predict(tf_input_state)\n        proto_action = np.expand_dims(self.exploration_policy.get_action(proto_action), 0)\n\n        nn_action_embeddings, indices, _, _ = self.knn_tree.query(keys=proto_action, k=self.ap.algorithm.k)\n\n        # now move the actions through the critic and choose the one with the highest q value\n        critic_inputs = copy.copy(tf_input_state)\n        critic_inputs[\'observation\'] = np.tile(critic_inputs[\'observation\'], (self.ap.algorithm.k, 1))\n        critic_inputs[\'action\'] = nn_action_embeddings[0]\n        q_values = critic_network.predict(critic_inputs)[0]\n        action = int(indices[0][np.argmax(q_values)])\n        self.action_signal.add_sample(action)\n        return ActionInfo(action=action, action_value=0)\n\n    def init_environment_dependent_modules(self):\n        super().init_environment_dependent_modules()\n        self.knn_tree = self.get_initialized_knn()\n\n    # TODO - ideally the knn should not be defined here, but somehow be defined by the user in the preset\n    def get_initialized_knn(self):\n        num_actions = len(self.spaces.action.actions)\n        action_max_abs_range = self.spaces.action.filtered_action_space.max_abs_range if \\\n            (hasattr(self.spaces.action, \'filtered_action_space\') and\n             isinstance(self.spaces.action.filtered_action_space, BoxActionSpace)) \\\n            else 1.0\n        keys = np.expand_dims((np.arange(num_actions) / (num_actions - 1) - 0.5) * 2, 1) * action_max_abs_range\n        values = np.expand_dims(np.arange(num_actions), 1)\n        knn_tree = AnnoyDictionary(dict_size=num_actions, key_width=self.ap.algorithm.action_embedding_width)\n        knn_tree.add(keys, values, force_rebuild_tree=True)\n\n        return knn_tree\n\n'"
rl_coach/architectures/__init__.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
rl_coach/architectures/architecture.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\n\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.saver import SaverCollection\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass Architecture(object):\n    @staticmethod\n    def construct(variable_scope: str, devices: List[str], *args, **kwargs) -> \'Architecture\':\n        """"""\n        Construct a network class using the provided variable scope and on requested devices\n        :param variable_scope: string specifying variable scope under which to create network variables\n        :param devices: list of devices (can be list of Device objects, or string for TF distributed)\n        :param args: all other arguments for class initializer\n        :param kwargs: all other keyword arguments for class initializer\n        :return: an object which is a child of Architecture\n        """"""\n        raise NotImplementedError\n\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, name: str= """"):\n        """"""\n        Creates a neural network \'architecture\', that can be trained and used for inference.\n\n        :param agent_parameters: the agent parameters\n        :param spaces: the spaces (observation, action, etc.) definition of the agent\n        :param name: the name of the network\n        """"""\n        self.spaces = spaces\n        self.name = name\n        self.network_wrapper_name = self.name.split(\'/\')[0]  # e.g. \'main/online\' --> \'main\'\n        self.full_name = ""{}/{}"".format(agent_parameters.full_name_id, name)\n        self.network_parameters = agent_parameters.network_wrappers[self.network_wrapper_name]\n        self.batch_size = self.network_parameters.batch_size\n        self.learning_rate = self.network_parameters.learning_rate\n        self.optimizer = None\n        self.ap = agent_parameters\n\n    def predict(self,\n                inputs: Dict[str, np.ndarray],\n                outputs: List[Any] = None,\n                squeeze_output: bool = True,\n                initial_feed_dict: Dict[Any, np.ndarray] = None) -> Tuple[np.ndarray, ...]:\n        """"""\n        Given input observations, use the model to make predictions (e.g. action or value).\n\n        :param inputs: current state (i.e. observations, measurements, goals, etc.)\n            (e.g. `{\'observation\': numpy.ndarray}` of shape (batch_size, observation_space_size))\n        :param outputs: list of outputs to return. Return all outputs if unspecified. Type of the list elements\n            depends on the framework backend.\n        :param squeeze_output: call squeeze_list on output before returning if True\n        :param initial_feed_dict: a dictionary of extra inputs for forward pass.\n        :return: predictions of action or value of shape (batch_size, action_space_size) for action predictions)\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def parallel_predict(sess: Any,\n                         network_input_tuples: List[Tuple[\'Architecture\', Dict[str, np.ndarray]]]) -> \\\n            Tuple[np.ndarray, ...]:\n        """"""\n        :param sess: active session to use for prediction\n        :param network_input_tuples: tuple of network and corresponding input\n        :return: list or tuple of outputs from all networks\n        """"""\n        raise NotImplementedError\n\n    def train_on_batch(self,\n                       inputs: Dict[str, np.ndarray],\n                       targets: List[np.ndarray],\n                       scaler: float=1.,\n                       additional_fetches: list=None,\n                       importance_weights: np.ndarray=None) -> Tuple[float, List[float], float, list]:\n        """"""\n        Given a batch of inputs (e.g. states) and targets (e.g. discounted rewards), takes a training step: i.e. runs a\n        forward pass and backward pass of the network, accumulates the gradients and applies an optimization step to\n        update the weights.\n        Calls `accumulate_gradients` followed by `apply_and_reset_gradients`.\n        Note: Currently an unused method.\n\n        :param inputs: typically the environment states (but can also contain other data necessary for loss).\n            (e.g. `{\'observation\': numpy.ndarray}` with `observation` of shape (batch_size, observation_space_size) or\n            (batch_size, observation_space_size, stack_size) or\n            `{\'observation\': numpy.ndarray, \'output_0_0\': numpy.ndarray}` with `output_0_0` of shape (batch_size,))\n        :param targets: target values of shape (batch_size, ). For example discounted rewards for value network\n            for calculating the value-network loss would be a target. Length of list and order of arrays in\n            the list matches that of network losses which are defined by network parameters\n        :param scaler: value to scale gradients by before optimizing network weights\n        :param additional_fetches: list of additional values to fetch and return. The type of each list\n            element is framework dependent.\n        :param importance_weights: ndarray of shape (batch_size,) to multiply with batch loss.\n        :return: tuple of total_loss, losses, norm_unclipped_grads, fetched_tensors\n            total_loss (float): sum of all head losses\n            losses (list of float): list of all losses. The order is list of target losses followed by list\n                of regularization losses. The specifics of losses is dependant on the network parameters\n                (number of heads, etc.)\n            norm_unclippsed_grads (float): global norm of all gradients before any gradient clipping is applied\n            fetched_tensors: all values for additional_fetches\n        """"""\n        raise NotImplementedError\n\n    def get_weights(self) -> List[np.ndarray]:\n        """"""\n        Gets model weights as a list of ndarrays. It is used for synchronizing weight between two identical networks.\n\n        :return: list weights as ndarray\n        """"""\n        raise NotImplementedError\n\n    def set_weights(self, weights: List[np.ndarray], rate: float=1.0) -> None:\n        """"""\n        Sets model weights for provided layer parameters.\n\n        :param weights: list of model weights in the same order as received in get_weights\n        :param rate: controls the mixture of given weight values versus old weight values.\n            i.e. new_weight = rate * given_weight + (1 - rate) * old_weight\n        :return: None\n        """"""\n        raise NotImplementedError\n\n    def reset_accumulated_gradients(self) -> None:\n        """"""\n        Sets gradient of all parameters to 0.\n\n        Once gradients are reset, they must be accessible by `accumulated_gradients` property of this class,\n        which must return a list of numpy ndarrays. Child class must ensure that `accumulated_gradients` is set.\n        """"""\n        raise NotImplementedError\n\n    def accumulate_gradients(self,\n                             inputs: Dict[str, np.ndarray],\n                             targets: List[np.ndarray],\n                             additional_fetches: list=None,\n                             importance_weights: np.ndarray=None,\n                             no_accumulation: bool=False) -> Tuple[float, List[float], float, list]:\n        """"""\n        Given a batch of inputs (i.e. states) and targets (e.g. discounted rewards), computes and accumulates the\n        gradients for model parameters. Will run forward and backward pass to compute gradients, clip the gradient\n        values if required and then accumulate gradients from all learners. It does not update the model weights,\n        that\'s performed in `apply_and_reset_gradients` method.\n\n        Once gradients are accumulated, they are accessed by `accumulated_gradients` property of this class.\xc3\xa5\n\n        :param inputs: typically the environment states (but can also contain other data for loss)\n            (e.g. `{\'observation\': numpy.ndarray}` with `observation` of shape (batch_size, observation_space_size) or\n             (batch_size, observation_space_size, stack_size) or\n            `{\'observation\': numpy.ndarray, \'output_0_0\': numpy.ndarray}` with `output_0_0` of shape (batch_size,))\n        :param targets: targets for calculating loss. For example discounted rewards for value network\n            for calculating the value-network loss would be a target. Length of list and order of arrays in\n            the list matches that of network losses which are defined by network parameters\n        :param additional_fetches: list of additional values to fetch and return. The type of each list\n            element is framework dependent.\n        :param importance_weights: ndarray of shape (batch_size,) to multiply with batch loss.\n        :param no_accumulation: if True, set gradient values to the new gradients, otherwise sum with previously\n            calculated gradients\n        :return: tuple of total_loss, losses, norm_unclipped_grads, fetched_tensors\n            total_loss (float): sum of all head losses\n            losses (list of float): list of all losses. The order is list of target losses followed by list of\n                regularization losses. The specifics of losses is dependant on the network parameters\n                (number of heads, etc.)\n            norm_unclippsed_grads (float): global norm of all gradients before any gradient clipping is applied\n            fetched_tensors: all values for additional_fetches\n        """"""\n        raise NotImplementedError\n\n    def apply_and_reset_gradients(self, gradients: List[np.ndarray], scaler: float=1.) -> None:\n        """"""\n        Applies the given gradients to the network weights and resets the gradient accumulations.\n        Has the same impact as calling `apply_gradients`, then `reset_accumulated_gradients`.\n\n        :param gradients: gradients for the parameter weights, taken from `accumulated_gradients` property\n            of an identical network (either self or another identical network)\n        :param scaler: A scaling factor that allows rescaling the gradients before applying them\n        """"""\n        raise NotImplementedError\n\n    def apply_gradients(self, gradients: List[np.ndarray], scaler: float=1.) -> None:\n        """"""\n        Applies the given gradients to the network weights.\n        Will be performed sync or async depending on `network_parameters.async_training`\n\n        :param gradients: gradients for the parameter weights, taken from `accumulated_gradients` property\n            of an identical network (either self or another identical network)\n        :param scaler: A scaling factor that allows rescaling the gradients before applying them\n        """"""\n        raise NotImplementedError\n\n    def get_variable_value(self, variable: Any) -> np.ndarray:\n        """"""\n        Gets value of a specified variable. Type of variable is dependant on the framework.\n        Example of a variable is head.kl_coefficient, which could be a symbol for evaluation\n        or could be a string representing the value.\n\n        :param variable: variable of interest\n        :return: value of the specified variable\n        """"""\n        raise NotImplementedError\n\n    def set_variable_value(self, assign_op: Any, value: np.ndarray, placeholder: Any):\n        """"""\n        Updates the value of a specified variable. Type of assign_op is dependant on the framework\n        and is a unique identifier for assigning value to a variable. For example an agent may use\n        head.assign_kl_coefficient. There is a one to one mapping between assign_op and placeholder\n        (in the example above, placeholder would be head.kl_coefficient_ph).\n\n        :param assign_op: a parameter representing the operation for assigning value to a specific variable\n        :param value: value of the specified variable used for update\n        :param placeholder: a placeholder for binding the value to assign_op.\n        """"""\n        raise NotImplementedError\n\n    def collect_savers(self, parent_path_suffix: str) -> SaverCollection:\n        """"""\n        Collection of all savers for the network (typically only one saver for network and one for ONNX export)\n        :param parent_path_suffix: path suffix of the parent of the network\n            (e.g. could be name of level manager plus name of agent)\n        :return: saver collection for the network\n        """"""\n        raise NotImplementedError\n'"
rl_coach/architectures/embedder_parameters.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List, Union\n\nfrom rl_coach.base_parameters import EmbedderScheme, NetworkComponentParameters\n\nMOD_NAMES = {\'image\': \'ImageEmbedder\', \'vector\': \'VectorEmbedder\', \'tensor\': \'TensorEmbedder\'}\n\nclass InputEmbedderParameters(NetworkComponentParameters):\n    def __init__(self, activation_function: str=\'relu\', scheme: Union[List, EmbedderScheme]=EmbedderScheme.Medium,\n                 batchnorm: bool=False, dropout_rate: float=0.0, name: str=\'embedder\', input_rescaling=None,\n                 input_offset=None, input_clipping=None, dense_layer=None, is_training=False):\n        super().__init__(dense_layer=dense_layer)\n        self.activation_function = activation_function\n        self.scheme = scheme\n        self.batchnorm = batchnorm\n        self.dropout_rate = dropout_rate\n\n        if input_rescaling is None:\n            input_rescaling = {\'image\': 255.0, \'vector\': 1.0, \'tensor\': 1.0}\n        if input_offset is None:\n            input_offset = {\'image\': 0.0, \'vector\': 0.0, \'tensor\': 0.0}\n\n        self.input_rescaling = input_rescaling\n        self.input_offset = input_offset\n        self.input_clipping = input_clipping\n        self.name = name\n        self.is_training = is_training\n\n    def path(self, emb_type):\n        return \'rl_coach.architectures.tensorflow_components.embedders:\' + MOD_NAMES[emb_type]\n'"
rl_coach/architectures/head_parameters.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Type\n\nfrom rl_coach.base_parameters import NetworkComponentParameters\n\n\nclass HeadParameters(NetworkComponentParameters):\n    def __init__(self, parameterized_class_name: str, activation_function: str = \'relu\', name: str= \'head\',\n                 num_output_head_copies: int=1, rescale_gradient_from_head_by_factor: float=1.0,\n                 loss_weight: float=1.0, dense_layer=None, is_training=False):\n        super().__init__(dense_layer=dense_layer)\n        self.activation_function = activation_function\n        self.name = name\n        self.num_output_head_copies = num_output_head_copies\n        self.rescale_gradient_from_head_by_factor = rescale_gradient_from_head_by_factor\n        self.loss_weight = loss_weight\n        self.parameterized_class_name = parameterized_class_name\n        self.is_training = is_training\n\n    @property\n    def path(self):\n        return \'rl_coach.architectures.tensorflow_components.heads:\' + self.parameterized_class_name\n\n\nclass PPOHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'tanh\', name: str=\'ppo_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""PPOHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass VHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'v_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None, initializer=\'normalized_columns\',\n                 output_bias_initializer=None):\n        super().__init__(parameterized_class_name=""VHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.initializer = initializer\n        self.output_bias_initializer = output_bias_initializer\n\n\nclass DDPGVHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'ddpg_v_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None, initializer=\'normalized_columns\',\n                 output_bias_initializer=None):\n        super().__init__(parameterized_class_name=""DDPGVHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.initializer = initializer\n        self.output_bias_initializer = output_bias_initializer\n\n\nclass CategoricalQHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'categorical_q_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None,\n                 output_bias_initializer=None):\n        super().__init__(parameterized_class_name=""CategoricalQHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.output_bias_initializer = output_bias_initializer\n\n\nclass RegressionHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'q_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None, scheme=None,\n                 output_bias_initializer=None):\n        super().__init__(parameterized_class_name=""RegressionHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.output_bias_initializer = output_bias_initializer\n\n\nclass DDPGActorHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'tanh\', name: str=\'policy_head_params\', batchnorm: bool=True,\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""DDPGActor"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.batchnorm = batchnorm\n\n\nclass WolpertingerActorHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'tanh\', name: str=\'policy_head_params\', batchnorm: bool=True,\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""WolpertingerActorHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.batchnorm = batchnorm\n\n\nclass DNDQHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'dnd_q_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""DNDQHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass DuelingQHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'dueling_q_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""DuelingQHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass MeasurementsPredictionHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'measurements_prediction_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""MeasurementsPredictionHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass NAFHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'tanh\', name: str=\'naf_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""NAFHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass PolicyHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'tanh\', name: str=\'policy_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""PolicyHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass PPOVHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'ppo_v_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None, output_bias_initializer=None):\n        super().__init__(parameterized_class_name=""PPOVHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.output_bias_initializer = output_bias_initializer\n\n\nclass QHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'q_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None, output_bias_initializer=None):\n        super().__init__(parameterized_class_name=""QHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.output_bias_initializer = output_bias_initializer\n\n\nclass ClassificationHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'classification_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""ClassificationHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass QuantileRegressionQHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'quantile_regression_q_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None, output_bias_initializer=None):\n        super().__init__(parameterized_class_name=""QuantileRegressionQHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.output_bias_initializer = output_bias_initializer\n\n\nclass RainbowQHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'rainbow_q_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""RainbowQHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass ACERPolicyHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'acer_policy_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None):\n        super().__init__(parameterized_class_name=""ACERPolicyHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n\n\nclass SACPolicyHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'sac_policy_head_params\', dense_layer=None):\n        super().__init__(parameterized_class_name=\'SACPolicyHead\', activation_function=activation_function, name=name,\n                         dense_layer=dense_layer)\n\n\nclass SACQHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'sac_q_head_params\', dense_layer=None,\n                 layers_sizes: tuple = (256, 256), output_bias_initializer=None):\n        super().__init__(parameterized_class_name=\'SACQHead\', activation_function=activation_function, name=name,\n                         dense_layer=dense_layer)\n        self.network_layers_sizes = layers_sizes\n        self.output_bias_initializer = output_bias_initializer\n\n\nclass TD3VHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str =\'relu\', name: str=\'td3_v_head_params\',\n                 num_output_head_copies: int = 1, rescale_gradient_from_head_by_factor: float = 1.0,\n                 loss_weight: float = 1.0, dense_layer=None, initializer=\'xavier\',\n                 output_bias_initializer=None):\n        super().__init__(parameterized_class_name=""TD3VHead"", activation_function=activation_function, name=name,\n                         dense_layer=dense_layer, num_output_head_copies=num_output_head_copies,\n                         rescale_gradient_from_head_by_factor=rescale_gradient_from_head_by_factor,\n                         loss_weight=loss_weight)\n        self.initializer = initializer\n        self.output_bias_initializer = output_bias_initializer\n'"
rl_coach/architectures/layers.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n""""""\nModule implementing base classes for common network layers used by preset schemes\n""""""\n\n\nclass Conv2d(object):\n    """"""\n    Base class for framework specfic Conv2d layer\n    """"""\n    def __init__(self, num_filters: int, kernel_size: int, strides: int):\n        self.num_filters = num_filters\n        self.kernel_size = kernel_size\n        self.strides = strides\n\n    def __str__(self):\n        return ""Convolution (num filters = {}, kernel size = {}, stride = {})""\\\n            .format(self.num_filters, self.kernel_size, self.strides)\n\n\nclass BatchnormActivationDropout(object):\n    """"""\n    Base class for framework specific batchnorm->activation->dropout layer group\n    """"""\n    def __init__(self, batchnorm: bool=False, activation_function: str=None, dropout_rate: float=0):\n        self.batchnorm = batchnorm\n        self.activation_function = activation_function\n        self.dropout_rate = dropout_rate\n\n    def __str__(self):\n        result = []\n        if self.batchnorm:\n            result += [""Batch Normalization""]\n        if self.activation_function:\n            result += [""Activation (type = {})"".format(self.activation_function)]\n        if self.dropout_rate > 0:\n            result += [""Dropout (rate = {})"".format(self.dropout_rate)]\n        return ""\\n"".join(result)\n\n\nclass Dense(object):\n    """"""\n    Base class for framework specific Dense layer\n    """"""\n    def __init__(self, units: int):\n        self.units = units\n\n    def __str__(self):\n        return ""Dense (num outputs = {})"".format(self.units)\n\n\nclass NoisyNetDense(object):\n    """"""\n    Base class for framework specific factorized Noisy Net layer\n\n    https://arxiv.org/abs/1706.10295.\n    """"""\n\n    def __init__(self, units: int):\n        self.units = units\n        self.sigma0 = 0.5\n\n    def __str__(self):\n        return ""Noisy Dense (num outputs = {})"".format(self.units)\n'"
rl_coach/architectures/middleware_parameters.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List, Type, Union\n\nfrom rl_coach.base_parameters import MiddlewareScheme, NetworkComponentParameters\n\n\nclass MiddlewareParameters(NetworkComponentParameters):\n    def __init__(self, parameterized_class_name: str,\n                 activation_function: str=\'relu\', scheme: Union[List, MiddlewareScheme]=MiddlewareScheme.Medium,\n                 batchnorm: bool=False, dropout_rate: float=0.0, name=\'middleware\', dense_layer=None, is_training=False):\n        super().__init__(dense_layer=dense_layer)\n        self.activation_function = activation_function\n        self.scheme = scheme\n        self.batchnorm = batchnorm\n        self.dropout_rate = dropout_rate\n        self.name = name\n        self.is_training = is_training\n        self.parameterized_class_name = parameterized_class_name\n\n    @property\n    def path(self):\n        return \'rl_coach.architectures.tensorflow_components.middlewares:\' + self.parameterized_class_name\n\n\nclass FCMiddlewareParameters(MiddlewareParameters):\n    def __init__(self, activation_function=\'relu\',\n                 scheme: Union[List, MiddlewareScheme] = MiddlewareScheme.Medium,\n                 batchnorm: bool = False, dropout_rate: float = 0.0,\n                 name=""middleware_fc_embedder"", dense_layer=None, is_training=False, num_streams=1):\n        super().__init__(parameterized_class_name=""FCMiddleware"", activation_function=activation_function,\n                         scheme=scheme, batchnorm=batchnorm, dropout_rate=dropout_rate, name=name, dense_layer=dense_layer,\n                         is_training=is_training)\n        self.num_streams = num_streams\n\n\nclass LSTMMiddlewareParameters(MiddlewareParameters):\n    def __init__(self, activation_function=\'relu\', number_of_lstm_cells=256,\n                 scheme: MiddlewareScheme = MiddlewareScheme.Medium,\n                 batchnorm: bool = False, dropout_rate: float = 0.0,\n                 name=""middleware_lstm_embedder"", dense_layer=None, is_training=False):\n        super().__init__(parameterized_class_name=""LSTMMiddleware"", activation_function=activation_function,\n                         scheme=scheme, batchnorm=batchnorm, dropout_rate=dropout_rate, name=name, dense_layer=dense_layer,\n                         is_training=is_training)\n        self.number_of_lstm_cells = number_of_lstm_cells'"
rl_coach/architectures/network_wrapper.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List, Tuple\n\nfrom rl_coach.base_parameters import Frameworks, AgentParameters\nfrom rl_coach.logger import failed_imports\nfrom rl_coach.saver import SaverCollection\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import force_list\n\n\nclass NetworkWrapper(object):\n    """"""\n    The network wrapper contains multiple copies of the same network, each one with a different set of weights which is\n    updating in a different time scale. The network wrapper will always contain an online network.\n    It will contain an additional slow updating target network if it was requested by the user,\n    and it will contain a global network shared between different workers, if Coach is run in a single-node\n    multi-process distributed mode. The network wrapper contains functionality for managing these networks and syncing\n    between them.\n    """"""\n    def __init__(self, agent_parameters: AgentParameters, has_target: bool, has_global: bool, name: str,\n                 spaces: SpacesDefinition, replicated_device=None, worker_device=None):\n        self.ap = agent_parameters\n        self.network_parameters = self.ap.network_wrappers[name]\n        self.has_target = has_target\n        self.has_global = has_global\n        self.name = name\n        self.sess = None\n\n        if self.network_parameters.framework == Frameworks.tensorflow:\n            try:\n                import tensorflow as tf\n            except ImportError:\n                raise Exception(\'Install tensorflow before using it as framework\')\n            from rl_coach.architectures.tensorflow_components.general_network import GeneralTensorFlowNetwork\n            general_network = GeneralTensorFlowNetwork.construct\n        elif self.network_parameters.framework == Frameworks.mxnet:\n            try:\n                import mxnet as mx\n            except ImportError:\n                raise Exception(\'Install mxnet before using it as framework\')\n            from rl_coach.architectures.mxnet_components.general_network import GeneralMxnetNetwork\n            general_network = GeneralMxnetNetwork.construct\n        else:\n            raise Exception(""{} Framework is not supported""\n                            .format(Frameworks().to_string(self.network_parameters.framework)))\n\n        variable_scope = ""{}/{}"".format(self.ap.full_name_id, name)\n\n        # Global network - the main network shared between threads\n        self.global_network = None\n        if self.has_global:\n            # we assign the parameters of this network on the parameters server\n            self.global_network = general_network(variable_scope=variable_scope,\n                                                  devices=force_list(replicated_device),\n                                                  agent_parameters=agent_parameters,\n                                                  name=\'{}/global\'.format(name),\n                                                  global_network=None,\n                                                  network_is_local=False,\n                                                  spaces=spaces,\n                                                  network_is_trainable=True)\n\n        # Online network - local copy of the main network used for playing\n        self.online_network = None\n        self.online_network = general_network(variable_scope=variable_scope,\n                                              devices=force_list(worker_device),\n                                              agent_parameters=agent_parameters,\n                                              name=\'{}/online\'.format(name),\n                                              global_network=self.global_network,\n                                              network_is_local=True,\n                                              spaces=spaces,\n                                              network_is_trainable=True)\n\n        # Target network - a local, slow updating network used for stabilizing the learning\n        self.target_network = None\n        if self.has_target:\n            self.target_network = general_network(variable_scope=variable_scope,\n                                                  devices=force_list(worker_device),\n                                                  agent_parameters=agent_parameters,\n                                                  name=\'{}/target\'.format(name),\n                                                  global_network=self.global_network,\n                                                  network_is_local=True,\n                                                  spaces=spaces,\n                                                  network_is_trainable=False)\n\n    def sync(self):\n        """"""\n        Initializes the weights of the networks to match each other\n\n        :return:\n        """"""\n        self.update_online_network()\n        self.update_target_network()\n\n    def update_target_network(self, rate=1.0):\n        """"""\n        Copy weights: online network >>> target network\n\n        :param rate: the rate of copying the weights - 1 for copying exactly\n        """"""\n        if self.target_network:\n            self.target_network.set_weights(self.online_network.get_weights(), rate)\n\n    def update_online_network(self, rate=1.0):\n        """"""\n        Copy weights: global network >>> online network\n\n        :param rate: the rate of copying the weights - 1 for copying exactly\n        """"""\n        if self.global_network:\n            self.online_network.set_weights(self.global_network.get_weights(), rate)\n\n    def apply_gradients_to_global_network(self, gradients=None, additional_inputs=None):\n        """"""\n        Apply gradients from the online network on the global network\n\n        :param gradients: optional gradients that will be used instead of teh accumulated gradients\n        :param additional_inputs: optional additional inputs required for when applying the gradients (e.g. batchnorm\'s\n                                  update ops also requires the inputs)\n        :return:\n        """"""\n        if gradients is None:\n            gradients = self.online_network.accumulated_gradients\n        if self.network_parameters.shared_optimizer:\n            self.global_network.apply_gradients(gradients, additional_inputs=additional_inputs)\n        else:\n            self.online_network.apply_gradients(gradients, additional_inputs=additional_inputs)\n\n    def apply_gradients_to_online_network(self, gradients=None, additional_inputs=None):\n        """"""\n        Apply gradients from the online network on itself\n        :param gradients: optional gradients that will be used instead of teh accumulated gradients\n        :param additional_inputs: optional additional inputs required for when applying the gradients (e.g. batchnorm\'s\n                                  update ops also requires the inputs)\n\n        :return:\n        """"""\n        if gradients is None:\n            gradients = self.online_network.accumulated_gradients\n        self.online_network.apply_gradients(gradients, additional_inputs=additional_inputs)\n\n    def train_and_sync_networks(self, inputs, targets, additional_fetches=[], importance_weights=None,\n                                use_inputs_for_apply_gradients=False):\n        """"""\n        A generic training function that enables multi-threading training using a global network if necessary.\n\n        :param inputs: The inputs for the network.\n        :param targets: The targets corresponding to the given inputs\n        :param additional_fetches: Any additional tensor the user wants to fetch\n        :param importance_weights: A coefficient for each sample in the batch, which will be used to rescale the loss\n                                   error of this sample. If it is not given, the samples losses won\'t be scaled\n        :param use_inputs_for_apply_gradients: Add the inputs also for when applying gradients\n                                              (e.g. for incorporating batchnorm update ops)\n        :return: The loss of the training iteration\n        """"""\n        result = self.online_network.accumulate_gradients(inputs, targets, additional_fetches=additional_fetches,\n                                                          importance_weights=importance_weights, no_accumulation=True)\n        if use_inputs_for_apply_gradients:\n            self.apply_gradients_and_sync_networks(reset_gradients=False, additional_inputs=inputs)\n        else:\n            self.apply_gradients_and_sync_networks(reset_gradients=False)\n\n        return result\n\n    def apply_gradients_and_sync_networks(self, reset_gradients=True, additional_inputs=None):\n        """"""\n        Applies the gradients accumulated in the online network to the global network or to itself and syncs the\n        networks if necessary\n\n        :param reset_gradients: If set to True, the accumulated gradients wont be reset to 0 after applying them to\n                                the network. this is useful when the accumulated gradients are overwritten instead\n                                if accumulated by the accumulate_gradients function. this allows reducing time\n                                complexity for this function by around 10%\n        :param additional_inputs: optional additional inputs required for when applying the gradients (e.g. batchnorm\'s\n                                  update ops also requires the inputs)\n\n        """"""\n        if self.global_network:\n            self.apply_gradients_to_global_network(additional_inputs=additional_inputs)\n            if reset_gradients:\n                self.online_network.reset_accumulated_gradients()\n            self.update_online_network()\n        else:\n            if reset_gradients:\n                self.online_network.apply_and_reset_gradients(self.online_network.accumulated_gradients,\n                                                              additional_inputs=additional_inputs)\n            else:\n                self.online_network.apply_gradients(self.online_network.accumulated_gradients,\n                                                    additional_inputs=additional_inputs)\n\n    def parallel_prediction(self, network_input_tuples: List[Tuple]):\n        """"""\n        Run several network prediction in parallel. Currently this only supports running each of the network once.\n\n        :param network_input_tuples: a list of tuples where the first element is the network (online_network,\n                                     target_network or global_network) and the second element is the inputs\n        :return: the outputs of all the networks in the same order as the inputs were given\n        """"""\n        return type(self.online_network).parallel_predict(self.sess, network_input_tuples)\n\n    def set_is_training(self, state: bool):\n        """"""\n        Set the phase of the network between training and testing\n\n        :param state: The current state (True = Training, False = Testing)\n        :return: None\n        """"""\n        self.online_network.set_is_training(state)\n        if self.has_target:\n            self.target_network.set_is_training(state)\n\n    def set_session(self, sess):\n        self.sess = sess\n        self.online_network.set_session(sess)\n        if self.global_network:\n            self.global_network.set_session(sess)\n        if self.target_network:\n            self.target_network.set_session(sess)\n\n    def __str__(self):\n        sub_networks = []\n        if self.global_network:\n            sub_networks.append(""global network"")\n        if self.online_network:\n            sub_networks.append(""online network"")\n        if self.target_network:\n            sub_networks.append(""target network"")\n\n        result = []\n        result.append(""Network: {}, Copies: {} ({})"".format(self.name, len(sub_networks), \' | \'.join(sub_networks)))\n        result.append(""-""*len(result[-1]))\n        result.append(str(self.online_network))\n        result.append("""")\n        return \'\\n\'.join(result)\n\n    def collect_savers(self, parent_path_suffix: str) -> SaverCollection:\n        """"""\n        Collect all of network\'s savers for global or online network\n        Note: global, online, and target network are all copies fo the same network which parameters that are\n            updated at different rates. So we only need to save one of the networks; the one that holds the most\n            recent parameters. target network is created for some agents and used for stabilizing training by\n            updating parameters from online network at a slower rate. As a result, target network never contains\n            the most recent set of parameters. In single-worker training, no global network is created and online\n            network contains the most recent parameters. In vertical distributed training with more than one worker,\n            global network is updated by all workers and contains the most recent parameters.\n            Therefore preference is given to global network if it exists, otherwise online network is used\n            for saving.\n        :param parent_path_suffix: path suffix of the parent of the network wrapper\n            (e.g. could be name of level manager plus name of agent)\n        :return: collection of all checkpoint objects\n        """"""\n        if self.global_network:\n            savers = self.global_network.collect_savers(parent_path_suffix)\n        else:\n            savers = self.online_network.collect_savers(parent_path_suffix)\n        return savers\n'"
rl_coach/dashboard_components/__init__.py,0,b''
rl_coach/dashboard_components/boards.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom bokeh.layouts import column\nfrom bokeh.models.widgets import Panel, Tabs\nfrom rl_coach.dashboard_components.experiment_board import experiment_board_layout\nfrom rl_coach.dashboard_components.episodic_board import episodic_board_layout\nfrom rl_coach.dashboard_components.globals import spinner, layouts\nfrom bokeh.models.widgets import Div\n\n# ---------------- Build Website Layout -------------------\n\n# title\ntitle = Div(text=""""""<h1>Coach Dashboard</h1>"""""")\ncenter = Div(text=""""""<style>html { padding-left: 50px; } </style>"""""")\ntab1 = Panel(child=experiment_board_layout, title=\'experiment board\')\n# tab2 = Panel(child=episodic_board_layout, title=\'episodic board\')\n# tabs = Tabs(tabs=[tab1, tab2])\ntabs = Tabs(tabs=[tab1])\n\nlayout = column(title, center, tabs)\nlayout = column(layout, spinner)\n\nlayouts[\'boards\'] = layout\n'"
rl_coach/dashboard_components/episodic_board.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom bokeh.layouts import row, column, widgetbox, Spacer\nfrom bokeh.models import ColumnDataSource, Range1d, LinearAxis, Legend\nfrom bokeh.models.widgets import RadioButtonGroup, MultiSelect, Button, Select, Slider, Div, CheckboxGroup, Toggle\nfrom bokeh.plotting import figure\nfrom rl_coach.dashboard_components.globals import layouts, crcolor, crx, cry, color_resolution, crRGBs\nfrom rl_coach.dashboard_components.experiment_board import file_selection_button, files_selector_spacer, \\\n    group_selection_button, unload_file_button, files_selector\n\n# ---------------- Build Website Layout -------------------\n\n# file refresh time placeholder\nrefresh_info = Div(text="""""""""""", width=210)\n\n# create figures\nplot = figure(plot_width=1200, plot_height=800,\n              tools=\'pan,box_zoom,wheel_zoom,crosshair,undo,redo,reset,save\',\n              toolbar_location=\'above\', x_axis_label=\'Episodes\',\n              x_range=Range1d(0, 10000), y_range=Range1d(0, 100000))\nplot.extra_y_ranges = {""secondary"": Range1d(start=-100, end=200)}\nplot.add_layout(LinearAxis(y_range_name=""secondary""), \'right\')\nplot.yaxis[-1].visible = False\n\n# legend\ndiv = Div(text="""""""""""")\nlegend = widgetbox([div])\n\nbokeh_legend = Legend(\n    # items=[(""12345678901234567890123456789012345678901234567890"", [])],  # 50 letters\n    items=[(""__________________________________________________"", [])],  # 50 letters\n    location=(0, 0), orientation=""vertical"",\n    border_line_color=""black"",\n    label_text_font_size={\'value\': \'9pt\'},\n    margin=30\n)\nplot.add_layout(bokeh_legend, ""right"")\n\n# select file\nfile_selection_button = Button(label=""Select Files"", button_type=""success"", width=120)\n# file_selection_button.on_click(load_files_group)\n\nfiles_selector_spacer = Spacer(width=10)\n\ngroup_selection_button = Button(label=""Select Directory"", button_type=""primary"", width=140)\n# group_selection_button.on_click(load_directory_group)\n\nunload_file_button = Button(label=""Unload"", button_type=""danger"", width=50)\n# unload_file_button.on_click(unload_file)\n\n# files selection box\nfiles_selector = Select(title=""Files:"", options=[])\n# files_selector.on_change(\'value\', change_data_selector)\n\n# data selection box\ndata_selector = MultiSelect(title=""Data:"", options=[], size=12)\n# data_selector.on_change(\'value\', select_data)\n\n# toggle second axis button\ntoggle_second_axis_button = Button(label=""Toggle Second Axis"", button_type=""success"")\n# toggle_second_axis_button.on_click(toggle_second_axis)\n\n# averaging slider\naveraging_slider = Slider(title=""Averaging window"", start=1, end=101, step=10)\n# averaging_slider.on_change(\'value\', update_averaging)\n\n# color selector\ncolor_selector_title = Div(text=""""""Select Color:"""""")\ncrsource = ColumnDataSource(data=dict(x=crx, y=cry, crcolor=crcolor, RGBs=crRGBs))\ncolor_selector = figure(x_range=(0, color_resolution), y_range=(0, 10),\n                        plot_width=300, plot_height=40,\n                        tools=\'tap\')\ncolor_selector.axis.visible = False\ncolor_range = color_selector.rect(x=\'x\', y=\'y\', width=1, height=10,\n                                  color=\'crcolor\', source=crsource)\n# crsource.on_change(\'selected\', select_color)\ncolor_range.nonselection_glyph = color_range.glyph\ncolor_selector.toolbar.logo = None\ncolor_selector.toolbar_location = None\n\nepisode_selector = MultiSelect(title=""Episode:"", options=[\'0\', \'1\', \'2\', \'3\', \'4\'], size=1)\n\nonline_toggle = Toggle(label=""Online"", button_type=""success"")\n\n# main layout of the document\nlayout = row(file_selection_button, files_selector_spacer, group_selection_button, width=300)\nlayout = column(layout, files_selector)\nlayout = column(layout, row(refresh_info, unload_file_button))\nlayout = column(layout, data_selector)\nlayout = column(layout, color_selector_title)\nlayout = column(layout, color_selector)\nlayout = column(layout, toggle_second_axis_button)\nlayout = column(layout, averaging_slider)\nlayout = column(layout, episode_selector)\nlayout = column(layout, online_toggle)\nlayout = row(layout, plot)\n\nepisodic_board_layout = layout\n\nlayouts[""episodic_board""] = episodic_board_layout'"
rl_coach/dashboard_components/experiment_board.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport copy\nimport datetime\nimport os\nimport sys\nimport time\nfrom itertools import cycle\nfrom os import listdir\nfrom os.path import isfile, join, isdir\n\nfrom bokeh.layouts import row, column, Spacer, ToolbarBox\nfrom bokeh.models import ColumnDataSource, Range1d, LinearAxis, Legend, \\\n    WheelZoomTool, CrosshairTool, ResetTool, SaveTool, Toolbar, PanTool, BoxZoomTool, \\\n    Toggle\nfrom bokeh.models.callbacks import CustomJS\nfrom bokeh.models.widgets import RadioButtonGroup, MultiSelect, Button, Select, Slider, Div, CheckboxGroup\nfrom bokeh.plotting import figure\nfrom rl_coach.dashboard_components.globals import signals_files, x_axis_labels, x_axis_options, show_spinner, hide_spinner, \\\n    dialog, FolderType, RunType, add_directory_csv_files, doc, display_boards, layouts, \\\n    crcolor, crx, cry, color_resolution, crRGBs, rgb_to_hex, x_axis\nfrom rl_coach.dashboard_components.signals_files_group import SignalsFilesGroup\n\nfrom rl_coach.dashboard_components.signals_file import SignalsFile\n\n\ndef update_axis_range(name, range_placeholder):\n    max_val = -float(\'inf\')\n    min_val = float(\'inf\')\n    selected_signal = None\n    if name in x_axis_options:\n        selected_signal = name\n    for signals_file in signals_files.values():\n        curr_min_val, curr_max_val = signals_file.get_range_of_selected_signals_on_axis(name, selected_signal)\n        max_val = max(max_val, curr_max_val)\n        min_val = min(min_val, curr_min_val)\n    if min_val != float(\'inf\'):\n        if min_val == max_val:\n            range = 5\n        else:\n            range = max_val - min_val\n        range_placeholder.start = min_val - 0.1 * range\n        range_placeholder.end = max_val + 0.1 * range\n\n\n# update axes ranges\ndef update_y_axis_ranges():\n    update_axis_range(\'default\', plot.y_range)\n    update_axis_range(\'secondary\', plot.extra_y_ranges[\'secondary\'])\n\n\ndef update_x_axis_ranges():\n    update_axis_range(x_axis[0], plot.x_range)\n\n\ndef get_all_selected_signals():\n    signals = []\n    for signals_file in signals_files.values():\n        signals += signals_file.get_selected_signals()\n    return signals\n\n\n# update legend using the legend text dictionary\ndef update_legend():\n    selected_signals = get_all_selected_signals()\n    max_line_length = 50\n    items = []\n    for signal in selected_signals:\n        side_sign = ""\xe2\x97\x80"" if signal.axis == \'default\' else ""\xe2\x96\xb6""\n        signal_name = side_sign + "" "" + signal.full_name\n        # bokeh legend does not respect a max_width parameter so we split the text manually to lines of constant width\n        signal_name = [signal_name[n:n + max_line_length] for n in range(0, len(signal_name), max_line_length)]\n        for idx, substr in enumerate(signal_name):\n            if idx == 0:\n                lines = [signal.line]\n                if signal.show_bollinger_bands:\n                    lines.append(signal.bands)\n                items.append((substr, lines))\n            else:\n                items.append((substr, []))\n\n    if bokeh_legend.items == [] or items == [] or \\\n            any([legend_item.renderers != item[1] for legend_item, item in zip(bokeh_legend.items, items)])\\\n            or any([legend_item.label != item[0] for legend_item, item in zip(bokeh_legend.items, items)]):\n        bokeh_legend.items = items  # this step takes a long time because it is redrawing the plot\n\n    # the visible=false => visible=true is a hack to make the legend render again\n    bokeh_legend.visible = False\n    bokeh_legend.visible = True\n\n\n# select lines to display\ndef select_data(args, old, new):\n    if selected_file is None:\n        return\n    show_spinner(""Updating the signal selection..."")\n    selected_signals = new\n    for signal_name in selected_file.signals.keys():\n        is_selected = signal_name in selected_signals\n        selected_file.set_signal_selection(signal_name, is_selected)\n\n    # update axes ranges\n    update_y_axis_ranges()\n    update_x_axis_ranges()\n\n    # update the legend\n    update_legend()\n\n    hide_spinner()\n\n\n# add new lines to the plot\ndef plot_signals(signals_file, signals):\n    for idx, signal in enumerate(signals):\n        signal.line = plot.line(\'index\', signal.name, source=signals_file.bokeh_source,\n                                line_color=signal.color, line_width=2)\n\n\ndef open_file_dialog():\n    return dialog.getFileDialog()\n\n\ndef open_directory_dialog():\n    return dialog.getDirDialog()\n\n\n# will create a group from the files\ndef create_files_group_signal(files):\n    global selected_file\n    signals_file = SignalsFilesGroup(files, plot)\n\n    signals_files[signals_file.filename] = signals_file\n\n    filenames = [signals_file.filename]\n    if files_selector.options[0] == """":\n        files_selector.options = filenames\n    else:\n        files_selector.options = files_selector.options + filenames\n    files_selector.value = filenames[0]\n    selected_file = signals_file\n\n\n# load files from disk as a group\ndef load_file():\n    file = open_file_dialog()\n    show_spinner(""Loading file..."")\n    # no file selected\n    if not file:\n        hide_spinner()\n        return\n\n    display_boards()\n\n    create_files_signal([file])\n\n    change_selected_signals_in_data_selector([""""])\n    hide_spinner()\n\n\n# classify the folder as containing a single file, multiple files or only folders\ndef classify_folder(dir_path):\n    files = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) and f.endswith(\'.csv\')]\n    folders = [d for d in listdir(dir_path) if isdir(join(dir_path, d)) and any(f.endswith("".csv"") for f in os.listdir(join(dir_path, d)))]\n    if len(files) == 1:\n        return FolderType.SINGLE_FILE\n    elif len(files) > 1:\n        return FolderType.MULTIPLE_FILES\n    elif len(folders) == 1:\n        return classify_folder(join(dir_path, folders[0]))\n    elif len(folders) > 1:\n        return FolderType.MULTIPLE_FOLDERS\n    else:\n        return FolderType.EMPTY\n\n\n# finds if this is single-threaded or multi-threaded\ndef get_run_type(dir_path):\n    folder_type = classify_folder(dir_path)\n    if folder_type == FolderType.SINGLE_FILE:\n        folder_type = RunType.SINGLE_FOLDER_SINGLE_FILE\n\n    elif folder_type == FolderType.MULTIPLE_FILES:\n        folder_type = RunType.SINGLE_FOLDER_MULTIPLE_FILES\n\n    elif folder_type == FolderType.MULTIPLE_FOLDERS:\n        # folder contains sub dirs -> we assume we can classify the folder using only the first sub dir\n        sub_dirs = [d for d in listdir(dir_path) if isdir(join(dir_path, d))]\n\n        # checking only the first folder in the root dir for its type, since we assume that all sub dirs will share the\n        # same structure (i.e. if one is a result of multi-threaded run, so will all the other).\n        folder_type = classify_folder(os.path.join(dir_path, sub_dirs[0]))\n        if folder_type == FolderType.SINGLE_FILE:\n            folder_type = RunType.MULTIPLE_FOLDERS_SINGLE_FILES\n        elif folder_type == FolderType.MULTIPLE_FILES:\n            folder_type = RunType.MULTIPLE_FOLDERS_MULTIPLE_FILES\n\n    return folder_type\n\n\n# create a signal file from the directory path according to the directory underlying structure\ndef handle_dir(dir_path, run_type):\n    paths = add_directory_csv_files(dir_path)\n    if run_type in [RunType.SINGLE_FOLDER_MULTIPLE_FILES,\n                    RunType.MULTIPLE_FOLDERS_SINGLE_FILES]:\n        create_files_group_signal(paths)\n    elif run_type == RunType.SINGLE_FOLDER_SINGLE_FILE:\n        create_files_signal(paths, use_dir_name=True)\n    elif run_type == RunType.MULTIPLE_FOLDERS_MULTIPLE_FILES:\n        sub_dirs = [d for d in listdir(dir_path) if isdir(join(dir_path, d))]\n        create_files_group_signal([os.path.join(dir_path, d) for d in sub_dirs])\n\n\n# load directory from disk as a group\ndef load_directory_group():\n    directory = open_directory_dialog()\n    show_spinner(""Loading directories group..."")\n    # no files selected\n    if not directory:\n        hide_spinner()\n        return\n\n    display_directory_group(directory)\n\n\ndef display_directory_group(directory):\n    pause_auto_update()\n\n    display_boards()\n    show_spinner(""Loading directories group..."")\n\n    while get_run_type(directory) == FolderType.EMPTY:\n        show_spinner(""Waiting for experiment directory to get populated..."")\n        sys.stdout.write(""Waiting for experiment directory to get populated...\\r"")\n        time.sleep(10)\n\n    handle_dir(directory, get_run_type(directory))\n\n    change_selected_signals_in_data_selector([""""])\n\n    resume_auto_update_according_to_toggle()\n    hide_spinner()\n\n\ndef create_files_signal(files, use_dir_name=False):\n    global selected_file\n    new_signal_files = []\n    for idx, file_path in enumerate(files):\n        signals_file = SignalsFile(str(file_path), plot=plot, use_dir_name=use_dir_name)\n        signals_files[signals_file.filename] = signals_file\n        new_signal_files.append(signals_file)\n\n    filenames = [f.filename for f in new_signal_files]\n\n    if files_selector.options[0] == """":\n        files_selector.options = filenames\n    else:\n        files_selector.options = files_selector.options + filenames\n    files_selector.value = filenames[0]\n    selected_file = new_signal_files[0]\n\n    # update x axis according to the file\'s default x-axis (which is the index, and thus the first column)\n    idx = x_axis_options.index(new_signal_files[0].csv.columns[0])\n    change_x_axis(idx)\n    x_axis_selector.active = idx\n\n\ndef display_files(files):\n    pause_auto_update()\n\n    display_boards()\n    show_spinner(""Loading files..."")\n\n    create_files_signal(files)\n\n    change_selected_signals_in_data_selector([""""])\n\n    resume_auto_update_according_to_toggle()\n    hide_spinner()\n\n\ndef unload_file():\n    global selected_file\n    if selected_file is None:\n        return\n    selected_file.hide_all_signals()\n    del signals_files[selected_file.filename]\n    data_selector.options = [""""]\n    filenames_list = copy.copy(files_selector.options)\n    filenames_list.remove(selected_file.filename)\n    if len(filenames_list) == 0:\n        filenames_list = [""""]\n    files_selector.options = filenames_list\n    filenames = cycle(filenames_list)\n    if files_selector.options[0] != """":\n        files_selector.value = next(filenames)\n    else:\n        files_selector.value = None\n\n    update_legend()\n    refresh_info.text = """"\n    if len(signals_files) == 0:\n        selected_file = None\n\n\n# reload the selected csv file\ndef reload_all_files(force=False):\n    pause_auto_update()\n\n    for file_to_load in signals_files.values():\n        if force or file_to_load.file_was_modified_on_disk():\n            show_spinner(""Updating files from the disk..."")\n            file_to_load.load()\n            hide_spinner()\n        refresh_info.text = ""Last Update: "" + str(datetime.datetime.now()).split(""."")[0]\n\n    resume_auto_update_according_to_toggle()\n\n\n# unselect the currently selected signals and then select the requested signals in the data selector\ndef change_selected_signals_in_data_selector(selected_signals):\n    # the default bokeh way is not working due to a bug since Bokeh 0.12.6 (https://github.com/bokeh/bokeh/issues/6501)\n    # remove the data selection callback before updating the selector\n    data_selector.remove_on_change(\'value\', select_data)\n    for value in list(data_selector.value):\n        if value in data_selector.options:\n            index = data_selector.options.index(value)\n            data_selector.options.remove(value)\n            data_selector.value.remove(value)\n            data_selector.options.insert(index, value)\n    data_selector.value = selected_signals\n    # add back the data selection callback\n    data_selector.on_change(\'value\', select_data)\n\n\n# change data options according to the selected file\ndef change_data_selector(args, old, new):\n    global selected_file\n    if new is None:\n        selected_file = None\n        return\n    show_spinner(""Updating selection..."")\n    selected_file = signals_files[new]\n    if isinstance(selected_file, SignalsFile):\n        group_cb.disabled = True\n    elif isinstance(selected_file, SignalsFilesGroup):\n        group_cb.disabled = False\n    data_selector.remove_on_change(\'value\', select_data)\n    data_selector.options = sorted(list(selected_file.signals.keys()))\n    data_selector.on_change(\'value\', select_data)\n    selected_signal_names = [s.name for s in selected_file.signals.values() if s.selected]\n    if not selected_signal_names:\n        selected_signal_names = [""""]\n    change_selected_signals_in_data_selector(selected_signal_names)\n    averaging_slider.value = selected_file.signals_averaging_window\n    if len(averaging_slider_dummy_source.data[\'value\']) > 0:\n        averaging_slider_dummy_source.data[\'value\'][0] = selected_file.signals_averaging_window\n    group_cb.active = [0 if selected_file.show_bollinger_bands else None]\n    group_cb.active += [1 if selected_file.separate_files else None]\n    hide_spinner()\n\n\n# smooth all the signals of the selected file\ndef update_averaging(args, old, new):\n    show_spinner(""Smoothing the signals..."")\n    # get the actual value from the dummy source\n    new = averaging_slider_dummy_source.data[\'value\'][0]\n    selected_file.change_averaging_window(new)\n    hide_spinner()\n\n\ndef change_x_axis(val):\n    global x_axis\n    show_spinner(""Updating the X axis..."")\n    x_axis[0] = x_axis_options[val]\n    plot.xaxis.axis_label = x_axis_labels[val]\n\n    for file_to_load in signals_files.values():\n        file_to_load.update_x_axis_index()\n        # this is needed in order to recalculate the mean of all the files\n        if isinstance(file_to_load, SignalsFilesGroup):\n            file_to_load.load()\n\n    update_axis_range(x_axis[0], plot.x_range)\n    hide_spinner()\n\n\n# move the signal between the main and secondary Y axes\ndef toggle_second_axis():\n    show_spinner(""Switching the Y axis..."")\n    plot.yaxis[-1].visible = True\n    selected_file.toggle_y_axis()\n\n    # this is just for redrawing the signals\n    selected_file.reload_data()\n\n    update_y_axis_ranges()\n    update_legend()\n\n    hide_spinner()\n\n\ndef toggle_group_property(new):\n    show_spinner(""Loading..."")\n\n    # toggle show / hide Bollinger bands\n    selected_file.change_bollinger_bands_state(0 in new)\n\n    # show a separate signal for each file in a group\n    selected_file.show_files_separately(1 in new)\n\n    update_legend()\n\n    hide_spinner()\n\n\n# Color selection - most of these functions are taken from bokeh examples (plotting/color_sliders.py)\ndef select_color(attr, old, new):\n    show_spinner(""Changing signal color..."")\n    signals = selected_file.get_selected_signals()\n    for signal in signals:\n        signal.set_color(rgb_to_hex(crRGBs[new[\'1d\'][\'indices\'][0]]))\n    hide_spinner()\n\n\ndef pause_auto_update():\n    toggle_auto_update(False)\n\n\ndef resume_auto_update_according_to_toggle():\n    toggle_auto_update(auto_update_toggle_button.active)\n\n\ndef toggle_auto_update(new):\n    global file_update_callback\n    if new is False and file_update_callback in doc._session_callbacks:\n        doc.remove_periodic_callback(file_update_callback)\n    elif file_update_callback not in doc._session_callbacks:\n        file_update_callback = doc.add_periodic_callback(reload_all_files, 30000)\n\n\nfile_update_callback = doc.add_periodic_callback(reload_all_files, 30000)\n\n# ---------------- Build Website Layout -------------------\n\n# file refresh time placeholder\nrefresh_info = Div(text="""""""""""", width=210)\n\n# create figures\nplot = figure(plot_width=1200, plot_height=800,\n              # tools=\'pan,box_zoom,wheel_zoom,crosshair,undo,redo,reset,save\',\n              toolbar_location=None, x_axis_label=\'Episodes\',\n              x_range=Range1d(0, 10000), y_range=Range1d(0, 100000), lod_factor=1000)\nplot.extra_y_ranges = {""secondary"": Range1d(start=-100, end=200)}\nplot.add_layout(LinearAxis(y_range_name=""secondary""), \'right\')\ntoolbar = Toolbar(tools=[PanTool(), BoxZoomTool(), WheelZoomTool(), CrosshairTool(), ResetTool(), SaveTool()])\n# plot.toolbar = toolbar\nplot.add_tools(*toolbar.tools)\nplot.yaxis[-1].visible = False\n\nbokeh_legend = Legend(\n    items=[("""", [])],\n    orientation=""vertical"",\n    border_line_color=""black"",\n    label_text_font_size={\'value\': \'9pt\'},\n    click_policy=\'hide\',\n    visible=False\n)\nbokeh_legend.label_width = 100\nplot.add_layout(bokeh_legend, ""right"")\nplot.y_range = Range1d(0, 100)\nplot.extra_y_ranges[\'secondary\'] = Range1d(0, 100)\n\n# select file\nfile_selection_button = Button(label=""Select File"", button_type=""success"", width=120)\nfile_selection_button.on_click(load_file)\n\nfiles_selector_spacer = Spacer(width=10)\n\ngroup_selection_button = Button(label=""Select Directory"", button_type=""primary"", width=140)\ngroup_selection_button.on_click(load_directory_group)\n\nupdate_files_button = Button(label=""Update Files"", button_type=""default"", width=50)\nupdate_files_button.on_click(reload_all_files)\n\nauto_update_toggle_button = Toggle(label=""Auto Update"", button_type=""default"", width=50, active=True)\nauto_update_toggle_button.on_click(toggle_auto_update)\n\nunload_file_button = Button(label=""Unload"", button_type=""danger"", width=50)\nunload_file_button.on_click(unload_file)\n\n# files selection box\nfiles_selector = Select(title=""Files:"", options=[""""])\nfiles_selector.on_change(\'value\', change_data_selector)\n\n# data selection box\ndata_selector = MultiSelect(title=""Data:"", options=[], size=12)\ndata_selector.on_change(\'value\', select_data)\n\n# x axis selection box\nx_axis_selector_title = Div(text=""""""X Axis:"""""", height=10)\nx_axis_selector = RadioButtonGroup(labels=x_axis_options, active=0)\nx_axis_selector.on_click(change_x_axis)\n\n# toggle second axis button\ntoggle_second_axis_button = Button(label=""Toggle Second Axis"", button_type=""success"")\ntoggle_second_axis_button.on_click(toggle_second_axis)\n\n# averaging slider\n# This data source is just used to communicate / trigger the real callback\naveraging_slider_dummy_source = ColumnDataSource(data=dict(value=[]))\naveraging_slider_dummy_source.on_change(\'data\', update_averaging)\naveraging_slider = Slider(title=""Averaging window"", start=1, end=101, step=10, callback_policy=\'mouseup\')\naveraging_slider.callback = CustomJS(args=dict(source=averaging_slider_dummy_source), code=""""""\n    source.data = { value: [cb_obj.value] }\n"""""")\n\n# group properties checkbox\ngroup_cb = CheckboxGroup(labels=[""Show statistics bands"", ""Ungroup signals""], active=[])\ngroup_cb.on_click(toggle_group_property)\n\n# color selector\ncolor_selector_title = Div(text=""""""Select Color:"""""")\ncrsource = ColumnDataSource(data=dict(x=crx, y=cry, crcolor=crcolor, RGBs=crRGBs))\ncolor_selector = figure(x_range=(0, color_resolution), y_range=(0, 10),\n                        plot_width=300, plot_height=40,\n                        tools=\'tap\')\ncolor_selector.axis.visible = False\ncolor_range = color_selector.rect(x=\'x\', y=\'y\', width=1, height=10,\n                                  color=\'crcolor\', source=crsource)\ncrsource.on_change(\'selected\', select_color)\ncolor_range.nonselection_glyph = color_range.glyph\ncolor_selector.toolbar.logo = None\ncolor_selector.toolbar_location = None\n\n# main layout of the document\nlayout = row(file_selection_button, files_selector_spacer, group_selection_button, width=300)\nlayout = column(layout, files_selector)\nlayout = column(layout, row(update_files_button, Spacer(width=50), auto_update_toggle_button,\n                            Spacer(width=50), unload_file_button))\nlayout = column(layout, row(refresh_info))\nlayout = column(layout, data_selector)\nlayout = column(layout, color_selector_title)\nlayout = column(layout, color_selector)\nlayout = column(layout, x_axis_selector_title)\nlayout = column(layout, x_axis_selector)\nlayout = column(layout, group_cb)\nlayout = column(layout, toggle_second_axis_button)\nlayout = column(layout, averaging_slider)\ntoolbox = ToolbarBox(toolbar=toolbar, toolbar_location=\'above\')\npanel = column(toolbox, plot)\nlayout = row(layout, panel)\n\nexperiment_board_layout = layout\n\nlayouts[""experiment_board""] = experiment_board_layout\n'"
rl_coach/dashboard_components/globals.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom collections import OrderedDict\n\nimport os\nfrom genericpath import isdir, isfile\nfrom os import listdir\nfrom os.path import join\nfrom enum import Enum\nfrom bokeh.models import Div\nfrom bokeh.plotting import curdoc\nimport tkinter as tk\nfrom tkinter import filedialog\nimport colorsys\n\nfrom rl_coach.core_types import TimeTypes\n\npatches = {}\nsignals_files = {}\nselected_file = None\nx_axis = [\'Episode #\']\nx_axis_options = [time_type.value.name for time_type in TimeTypes]\nx_axis_labels = [time_type.value.label for time_type in TimeTypes]\ncurrent_color = 0\n\n# spinner\nroot_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nwith open(os.path.join(root_dir, \'dashboard_components/spinner.css\'), \'r\') as f:\n    spinner_style = """"""<style>{}</style>"""""".format(f.read())\n    spinner_html = """"""<ul class=""spinner""><li></li><li></li><li></li><li></li>\n                      <li>\n                        <br>\n                        <span style=""font-size: 24px; font-weight: bold; margin-left: -175px; width: 400px; \n                        position: absolute; text-align: center;"">\n                            {}\n                        </span>\n                      </li></ul>""""""\nspinner = Div(text="""""""""""")\ndisplayed_doc = ""landing_page""\nlayouts = {}\n\n\ndef generate_color_range(N, I):\n    HSV_tuples = [(x*1.0/N, 0.5, I) for x in range(N)]\n    RGB_tuples = map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)\n    for_conversion = []\n    for RGB_tuple in RGB_tuples:\n        for_conversion.append((int(RGB_tuple[0]*255), int(RGB_tuple[1]*255), int(RGB_tuple[2]*255)))\n    hex_colors = [rgb_to_hex(RGB_tuple) for RGB_tuple in for_conversion]\n    return hex_colors, for_conversion\n\n\n# convert RGB tuple to hexadecimal code\ndef rgb_to_hex(rgb):\n    return \'#%02x%02x%02x\' % rgb\n\n\n# convert hexadecimal to RGB tuple\ndef hex_to_dec(hex):\n    red = \'\'.join(hex.strip(\'#\')[0:2])\n    green = \'\'.join(hex.strip(\'#\')[2:4])\n    blue = \'\'.join(hex.strip(\'#\')[4:6])\n    return int(red, 16), int(green, 16), int(blue,16)\n\n\ncolor_resolution = 1000\nbrightness = 0.75  # change to have brighter/darker colors\ncrx = list(range(1, color_resolution+1))  # the resolution is 1000 colors\ncry = [5 for i in range(len(crx))]\ncrcolor, crRGBs = generate_color_range(color_resolution, brightness)  # produce spectrum\n\n\ndef display_boards():\n    global displayed_doc\n    if displayed_doc == ""landing_page"":\n        doc.remove_root(doc.roots[0])\n        doc.add_root(layouts[""boards""])\n        displayed_doc = ""boards""\n\n\ndef show_spinner(text=""Loading...""):\n    spinner.text = spinner_style + spinner_html.format(text)\n\n\ndef hide_spinner():\n    spinner.text = """"\n\n\n# takes path to dir and recursively adds all its files to paths\ndef add_directory_csv_files(dir_path, paths=None):\n    if not paths:\n        paths = []\n\n    for p in listdir(dir_path):\n        path = join(dir_path, p)\n        if isdir(path):\n            # call recursively for each dir\n            paths = add_directory_csv_files(path, paths)\n        elif isfile(path) and path.endswith(\'.csv\'):\n            # add every file to the list\n            paths.append(path)\n\n    return paths\n\n\n\n\nclass DialogApp():\n\n    def getFileDialog(self):\n        application_window = tk.Tk()\n\n        # Build a list of tuples for each file type the file dialog should display\n        my_filetypes = [(\'csv files\', \'.csv\')]\n\n        # Ask the user to select a one or more file names.\n        answer = filedialog.askopenfilename(parent=application_window,\n                                             initialdir=os.getcwd(),\n                                             title=""Please select a file"",\n                                             filetypes=my_filetypes)\n        application_window.destroy()\n        return answer\n\n\n    def getDirDialog(self):\n        application_window = tk.Tk()\n\n        # Ask the user to select a folder.\n        answer = filedialog.askdirectory(parent=application_window,\n                                         initialdir=os.getcwd(),\n                                         title=""Please select a folder"")\n        application_window.destroy()\n        return answer\n\n\n\n\n\nclass RunType(Enum):\n    SINGLE_FOLDER_SINGLE_FILE = 1\n    SINGLE_FOLDER_MULTIPLE_FILES = 2\n    MULTIPLE_FOLDERS_SINGLE_FILES = 3\n    MULTIPLE_FOLDERS_MULTIPLE_FILES = 4\n    UNKNOWN = 0\n\n\nclass FolderType(Enum):\n    SINGLE_FILE = 1\n    MULTIPLE_FILES = 2\n    MULTIPLE_FOLDERS = 3\n    EMPTY = 4\n\n\ndialog = DialogApp()\n\n\ndoc = curdoc()\n'"
rl_coach/dashboard_components/landing_page.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom bokeh.layouts import row, column\nfrom bokeh.models.widgets import Div\n\nfrom rl_coach.dashboard_components.experiment_board import file_selection_button, group_selection_button\nfrom rl_coach.dashboard_components.globals import layouts\n\n# title\ntitle = Div(text=""""""<h1>Coach Dashboard</h1>"""""")\n\n# landing page\nlanding_page_description = Div(text=""""""<h3>Start by selecting an experiment file or directory to open:</h3>"""""")\ncenter = Div(text=""""""<style>html { text-align: center; } </style>"""""")\ncenter_buttons = Div(text=""""""<style>.bk-root .bk-widget { margin: 0 auto; }</style>"""""", width=0)\nlanding_page = column(center,\n                      title,\n                      landing_page_description,\n                      row(center_buttons),\n                      row(file_selection_button, sizing_mode=\'scale_width\'),\n                      row(group_selection_button, sizing_mode=\'scale_width\'),\n                      sizing_mode=\'scale_width\')\n\nlayouts[\'landing_page\'] = landing_page\n'"
rl_coach/dashboard_components/signals.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport random\n\nimport numpy as np\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.palettes import Dark2\nfrom rl_coach.dashboard_components.globals import show_spinner, hide_spinner, current_color\nfrom rl_coach.utils import squeeze_list\n\n\nclass Signal:\n    def __init__(self, name, parent, plot):\n        self.name = name\n        self.full_name = ""{}/{}"".format(parent.filename, self.name)\n        self.plot = plot\n        self.selected = False\n        self.color = random.choice(Dark2[8])\n        self.line = None\n        self.scatter = None\n        self.bands = None\n        self.bokeh_source = parent.bokeh_source\n        self.min_val = 0\n        self.max_val = 0\n        self.axis = \'default\'\n        self.sub_signals = []\n        for name in self.bokeh_source.data.keys():\n            if (len(name.split(\'/\')) == 1 and name == self.name) or \'/\'.join(name.split(\'/\')[:-1]) == self.name:\n                self.sub_signals.append(name)\n        if len(self.sub_signals) > 1:\n            self.mean_signal = squeeze_list([name for name in self.sub_signals if \'Mean\' in name.split(\'/\')[-1]])\n            self.stdev_signal = squeeze_list([name for name in self.sub_signals if \'Stdev\' in name.split(\'/\')[-1]])\n            self.min_signal = squeeze_list([name for name in self.sub_signals if \'Min\' in name.split(\'/\')[-1]])\n            self.max_signal = squeeze_list([name for name in self.sub_signals if \'Max\' in name.split(\'/\')[-1]])\n        else:\n            self.mean_signal = squeeze_list(self.name)\n            self.stdev_signal = None\n            self.min_signal = None\n            self.max_signal = None\n        self.has_bollinger_bands = False\n        if self.mean_signal and self.stdev_signal and self.min_signal and self.max_signal:\n            self.has_bollinger_bands = True\n        self.show_bollinger_bands = False\n        self.bollinger_bands_source = None\n        self.update_range()\n\n    def set_color(self, color):\n        self.color = color\n        if self.line:\n            self.line.glyph.line_color = color\n        if self.bands:\n            self.bands.glyph.fill_color = color\n\n    def plot_line(self):\n        global current_color\n        self.set_color(Dark2[8][current_color])\n        current_color = (current_color + 1) % len(Dark2[8])\n        if self.has_bollinger_bands:\n            self.set_bands_source()\n            self.create_bands()\n        self.line = self.plot.line(\'index\', self.mean_signal, source=self.bokeh_source,\n                                   line_color=self.color, line_width=2)\n        # self.scatter = self.plot.scatter(\'index\', self.mean_signal, source=self.bokeh_source)\n        self.line.visible = True\n\n    def set_selected(self, val):\n        if self.selected != val:\n            self.selected = val\n            if self.line:\n                # self.set_color(Dark2[8][current_color])\n                # current_color = (current_color + 1) % len(Dark2[8])\n                self.line.visible = self.selected\n                if self.bands:\n                    self.bands.visible = self.selected and self.show_bollinger_bands\n            elif self.selected:\n                # lazy plotting - plot only when selected for the first time\n                self.plot_line()\n\n    def set_dash(self, dash):\n        self.line.glyph.line_dash = dash\n\n    def create_bands(self):\n        self.bands = self.plot.patch(x=\'band_x\', y=\'band_y\', source=self.bollinger_bands_source,\n                                color=self.color, fill_alpha=0.4, alpha=0.1, line_width=0)\n        self.bands.visible = self.show_bollinger_bands\n        # self.min_line = plot.line(\'index\', self.min_signal, source=self.bokeh_source,\n        #                           line_color=self.color, line_width=3, line_dash=""4 4"")\n        # self.max_line = plot.line(\'index\', self.max_signal, source=self.bokeh_source,\n        #                           line_color=self.color, line_width=3, line_dash=""4 4"")\n        # self.min_line.visible = self.show_bollinger_bands\n        # self.max_line.visible = self.show_bollinger_bands\n\n    def set_bands_source(self):\n        x_ticks = self.bokeh_source.data[\'index\']\n        mean_values = self.bokeh_source.data[self.mean_signal]\n        stdev_values = self.bokeh_source.data[self.stdev_signal]\n        band_x = np.append(x_ticks, x_ticks[::-1])\n        band_y = np.append(mean_values - stdev_values, mean_values[::-1] + stdev_values[::-1])\n        source_data = {\'band_x\': band_x, \'band_y\': band_y}\n        if self.bollinger_bands_source:\n            self.bollinger_bands_source.data = source_data\n        else:\n            self.bollinger_bands_source = ColumnDataSource(source_data)\n\n    def change_bollinger_bands_state(self, new_state):\n        self.show_bollinger_bands = new_state\n        if self.bands and self.selected:\n            self.bands.visible = new_state\n            # self.min_line.visible = new_state\n            # self.max_line.visible = new_state\n\n    def update_range(self):\n        self.min_val = np.min(self.bokeh_source.data[self.mean_signal])\n        self.max_val = np.max(self.bokeh_source.data[self.mean_signal])\n\n    def set_axis(self, axis):\n        self.axis = axis\n        if not self.line:\n            self.plot_line()\n            self.line.visible = False\n        self.line.y_range_name = axis\n\n    def toggle_axis(self):\n        if self.axis == \'default\':\n            self.set_axis(\'secondary\')\n        else:\n            self.set_axis(\'default\')\n'"
rl_coach/dashboard_components/signals_file.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport os\nfrom os.path import basename\n\nimport pandas as pd\nfrom pandas.errors import EmptyDataError\n\nfrom rl_coach.dashboard_components.signals_file_base import SignalsFileBase\nfrom rl_coach.dashboard_components.globals import x_axis_options\nfrom rl_coach.utils import break_file_path\n\n\nclass SignalsFile(SignalsFileBase):\n    def __init__(self, csv_path, load=True, plot=None, use_dir_name=False):\n        super().__init__(plot)\n        self.use_dir_name = use_dir_name\n        self.full_csv_path = csv_path\n        self.dir, self.filename, _ = break_file_path(csv_path)\n\n        if use_dir_name:\n            parent_directory_path = os.path.abspath(os.path.join(os.path.dirname(csv_path), \'..\'))\n            if len(os.listdir(parent_directory_path)) == 1:\n                # get the parent directory name (since the current directory is the timestamp directory)\n                self.dir = parent_directory_path\n                self.filename = basename(self.dir)\n            else:\n                # get the common directory for all the experiments\n                self.dir = os.path.dirname(csv_path)\n                self.filename = ""{}/{}"".format(basename(parent_directory_path), basename(self.dir))\n\n        if load:\n            self.load()\n            # this helps set the correct x axis\n            self.change_averaging_window(1, force=True)\n\n    def load_csv(self, idx=None, result=None):\n        # load csv and fix sparse data.\n        # csv can be in the middle of being written so we use try - except\n        new_csv = None\n        while new_csv is None:\n            try:\n                new_csv = pd.read_csv(self.full_csv_path)\n                break\n            except EmptyDataError:\n                new_csv = None\n                continue\n\n        new_csv[\'Wall-Clock Time\'] /= 60.\n        new_csv = new_csv.interpolate()\n        # remove signals which don\'t contain any values\n        for k, v in new_csv.isna().all().items():\n            if v and k not in x_axis_options:\n                del new_csv[k]\n\n        # only fill the missing values that the previous interploation did not deal with (usually the ones before the\n        # first update to the signal was made). We do so again by interpolation (averaging the previous and the next\n        # values)\n        new_csv = ((new_csv.fillna(method=\'bfill\') + new_csv.fillna(method=\'ffill\')) / 2).fillna(method=\'bfill\').fillna(\n            method=\'ffill\')\n\n        self.csv = new_csv\n\n        self.last_modified = os.path.getmtime(self.full_csv_path)\n\n        if idx is not None:\n            result[idx] = (self.csv, self.last_modified)\n\n    def file_was_modified_on_disk(self):\n        return self.last_modified != os.path.getmtime(self.full_csv_path)\n'"
rl_coach/dashboard_components/signals_file_base.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport numpy as np\nfrom bokeh.models import ColumnDataSource\n\nfrom rl_coach.dashboard_components.signals import Signal\nfrom rl_coach.dashboard_components.globals import x_axis, x_axis_options, show_spinner\n\n\nclass SignalsFileBase:\n    def __init__(self, plot):\n        self.plot = plot\n        self.full_csv_path = """"\n        self.dir = """"\n        self.filename = """"\n        self.signals_averaging_window = 1\n        self.show_bollinger_bands = False\n        self.csv = None\n        self.bokeh_source = None\n        self.bokeh_source_orig = None\n        self.last_modified = None\n        self.signals = {}\n        self.separate_files = False\n        self.last_reload_data_fix = False\n\n    def load_csv(self):\n        pass\n\n    def update_x_axis_index(self):\n        global x_axis\n        self.bokeh_source_orig.data[\'index\'] = self.bokeh_source_orig.data[x_axis[0]]\n        self.bokeh_source.data[\'index\'] = self.bokeh_source.data[x_axis[0]]\n\n    def toggle_y_axis(self, signal_name=None):\n        if signal_name and signal_name in self.signals.keys():\n            self.signals[signal_name].toggle_axis()\n        else:\n            for signal in self.signals.values():\n                if signal.selected:\n                    signal.toggle_axis()\n\n    def update_source_and_signals(self):\n        # Remove old index\n        self.csv = self.csv.reset_index(drop=True)\n\n        # create bokeh data sources\n        self.bokeh_source_orig = ColumnDataSource(self.csv)\n\n        if self.bokeh_source is None:\n            self.bokeh_source = ColumnDataSource(self.csv)\n            self.update_x_axis_index()\n        else:\n            self.update_x_axis_index()\n            # smooth the data if necessary\n            self.change_averaging_window(self.signals_averaging_window, force=True)\n\n        # create all the signals\n        if len(self.signals.keys()) == 0:\n            self.signals = {}\n            unique_signal_names = []\n            for name in self.csv.columns:\n                if len(name.split(\'/\')) == 1:\n                    unique_signal_names.append(name)\n                else:\n                    unique_signal_names.append(\'/\'.join(name.split(\'/\')[:-1]))\n            unique_signal_names = list(set(unique_signal_names))\n            for signal_name in unique_signal_names:\n                self.signals[signal_name] = Signal(signal_name, self, self.plot)\n\n    def load(self):\n        self.load_csv()\n        self.update_source_and_signals()\n\n    def reload_data(self):\n        # this function is a workaround to reload the data of all the signals\n        # if the data doesn\'t change, bokeh does not refresh the line\n        temp_data = self.bokeh_source.data.copy()\n        for col in self.bokeh_source.data.keys():\n            if not self.last_reload_data_fix:\n                temp_data[col] = temp_data[col][:-1]\n        self.last_reload_data_fix = not self.last_reload_data_fix\n        self.bokeh_source.data = temp_data\n\n    def change_averaging_window(self, new_size, force=False, signals=None):\n        if force or self.signals_averaging_window != new_size:\n            self.signals_averaging_window = new_size\n            win = np.ones(new_size) / new_size\n            temp_data = self.bokeh_source_orig.data.copy()\n            for col in self.bokeh_source.data.keys():\n                if col == \'index\' or col in x_axis_options \\\n                        or (signals and not any(col in signal for signal in signals)):\n                    temp_data[col] = temp_data[col][:-new_size]\n                    continue\n                temp_data[col] = np.convolve(self.bokeh_source_orig.data[col], win, mode=\'same\')[:-new_size]\n            self.bokeh_source.data = temp_data\n\n            # smooth bollinger bands\n            for signal in self.signals.values():\n                if signal.has_bollinger_bands:\n                    signal.set_bands_source()\n\n    def hide_all_signals(self):\n        for signal_name in self.signals.keys():\n            self.set_signal_selection(signal_name, False)\n\n    def set_signal_selection(self, signal_name, val):\n        self.signals[signal_name].set_selected(val)\n\n    def change_bollinger_bands_state(self, new_state):\n        self.show_bollinger_bands = new_state\n        for signal in self.signals.values():\n            signal.change_bollinger_bands_state(new_state)\n\n    def file_was_modified_on_disk(self):\n        pass\n\n    def get_range_of_selected_signals_on_axis(self, axis, selected_signal=None):\n        max_val = -float(\'inf\')\n        min_val = float(\'inf\')\n        for signal in self.signals.values():\n            if (selected_signal and signal.name == selected_signal) or (signal.selected and signal.axis == axis):\n                max_val = max(max_val, signal.max_val)\n                min_val = min(min_val, signal.min_val)\n        return min_val, max_val\n\n    def get_selected_signals(self):\n        signals = []\n        for signal in self.signals.values():\n            if signal.selected:\n                signals.append(signal)\n        return signals\n\n    def show_files_separately(self, val):\n        pass'"
rl_coach/dashboard_components/signals_files_group.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport os\nfrom multiprocessing import Process, Manager\nfrom os.path import basename\n\nimport pandas as pd\nfrom rl_coach.dashboard_components.globals import x_axis_options, add_directory_csv_files, show_spinner, x_axis\nfrom rl_coach.dashboard_components.signals_file_base import SignalsFileBase\n\nfrom rl_coach.dashboard_components.signals_file import SignalsFile\n\n\nclass SignalsFilesGroup(SignalsFileBase):\n    def __init__(self, csv_paths, plot=None):\n        super().__init__(plot)\n        self.full_csv_paths = csv_paths\n        self.signals_files = []\n        if len(csv_paths) == 1 and os.path.isdir(csv_paths[0]):\n            self.signals_files = [SignalsFile(str(file), load=False, plot=plot) for file in add_directory_csv_files(csv_paths[0])]\n        else:\n            for csv_path in csv_paths:\n                if os.path.isdir(csv_path):\n                    self.signals_files.append(SignalsFilesGroup(add_directory_csv_files(csv_path), plot=plot))\n                else:\n                    self.signals_files.append(SignalsFile(str(csv_path), load=False, plot=plot))\n        parent_directory_path = os.path.abspath(os.path.join(os.path.dirname(csv_paths[0]), \'..\'))\n\n        if len(os.listdir(parent_directory_path)) == 1:\n            # get the parent directory name (since the current directory is the timestamp directory)\n            self.dir = parent_directory_path\n        else:\n            # get the common directory for all the experiments\n            self.dir = os.path.dirname(\'/\'.join(os.path.commonprefix(csv_paths).split(\'/\')[:-1]) + \'/\')\n\n        self.filename = \'{} - Group({})\'.format(basename(self.dir), len(self.signals_files))\n\n        self.signal_files_need_update = False\n\n        self.load()\n\n    def load_csv(self):\n        global x_axis\n        # load the csv\'s for all workers\n        processes = []\n        results = Manager().dict()\n        corrupted_files_idx = []\n        for idx, signal_file in enumerate(self.signals_files):\n            if not isinstance(signal_file, SignalsFilesGroup):\n                processes.append(Process(target=signal_file.load_csv, args=(idx, results)))\n                processes[-1].start()\n        [p.join() for p in processes]\n\n        # load csv\'s for SignalsFilesGroup serially for now. TODO: we should later parallelize this as well.\n        for idx, signal_file in enumerate(self.signals_files):\n            if isinstance(signal_file, SignalsFilesGroup):\n                signal_file.load_csv()\n\n        for idx, signal_file in enumerate(self.signals_files):\n            if len(list(results.keys())) > 0:\n                signal_file.csv, signal_file.last_modified = results[idx]\n            if not all(option in signal_file.csv.keys() for option in x_axis_options):\n                print(""Warning: {} file seems to be corrupted and does contain the necessary columns ""\n                          ""and will not be rendered"".format(signal_file.filename))\n                corrupted_files_idx.append(idx)\n\n        # remove corrupted worker files\n        for file_idx in corrupted_files_idx:\n            del self.signals_files[file_idx]\n\n        # get the stats of all the columns\n        if len(self.signals_files) > 1:\n            transformed_signals_files = []\n            subsampling = None\n            for idx in range(len(self.signals_files)):\n                transformed_signals_files.append(self.signals_files[idx].csv.copy(deep=True))\n\n                # change the index to be the currently selected x axis\n                transformed_signals_files[-1].index = transformed_signals_files[-1][x_axis[0]]\n\n                # remove all duplicate index rows\n                transformed_signals_files[-1] = transformed_signals_files[-1][~transformed_signals_files[-1].index.duplicated()]\n\n                # fill up missing row indices. we are going to take the mean over the group and we want to make sure\n                # the entire group has some value for every possible index.\n                num_rows = int(transformed_signals_files[-1].index.values[-1])\n                transformed_signals_files[-1] = transformed_signals_files[-1].reindex(range(num_rows))\n                transformed_signals_files[-1].interpolate(inplace=True)\n\n                # sub sample the csv to max of 5000 indices (do the same subsampling to all files)\n                if subsampling is None:\n                    subsampling = max(1, num_rows // 5000)\n                transformed_signals_files[-1] = transformed_signals_files[-1].iloc[::subsampling, :]\n\n            csv_group = pd.concat([signals_file for signals_file in transformed_signals_files])\n            columns_to_remove = [s for s in csv_group.columns if \'/Stdev\' in s] + \\\n                                [s for s in csv_group.columns if \'/Min\' in s] + \\\n                                [s for s in csv_group.columns if \'/Max\' in s]\n            for col in columns_to_remove:\n                del csv_group[col]\n            csv_group = csv_group.groupby(csv_group.index)\n            self.csv_mean = csv_group.mean()\n            self.csv_mean.columns = [s + \'/Mean\' for s in self.csv_mean.columns]\n            self.csv_stdev = csv_group.std()\n            self.csv_stdev.columns = [s + \'/Stdev\' for s in self.csv_stdev.columns]\n            self.csv_min = csv_group.min()\n            self.csv_min.columns = [s + \'/Min\' for s in self.csv_min.columns]\n            self.csv_max = csv_group.max()\n            self.csv_max.columns = [s + \'/Max\' for s in self.csv_max.columns]\n\n            # get the indices from the file with the least number of indices and which is not an evaluation worker\n            file_with_min_indices = transformed_signals_files[0]\n            for signals_file in transformed_signals_files:\n                if signals_file.shape[0] < file_with_min_indices.shape[0] and \\\n                                \'Training reward\' in signals_file.keys():\n                    file_with_min_indices = signals_file\n            self.index_columns = file_with_min_indices[x_axis_options]\n\n            # concat the stats and the indices columns\n            num_rows = file_with_min_indices.shape[0]\n            self.csv = pd.concat([self.index_columns, self.csv_mean.head(num_rows), self.csv_stdev.head(num_rows),\n                                  self.csv_min.head(num_rows), self.csv_max.head(num_rows)], axis=1)\n\n            # remove the stat columns for the indices columns\n            columns_to_remove = [s + \'/Mean\' for s in x_axis_options] + \\\n                                [s + \'/Stdev\' for s in x_axis_options] + \\\n                                [s + \'/Min\' for s in x_axis_options] + \\\n                                [s + \'/Max\' for s in x_axis_options]\n            for col in columns_to_remove:\n                if col in self.csv.keys():\n                    del self.csv[col]\n        else:  # This is a group of a single file\n            self.csv = self.signals_files[0].csv\n\n        # remove NaNs\n        self.csv.fillna(value=0, inplace=True)  # removing this line will make bollinger bands fail\n        for key in self.csv.keys():\n            if \'Stdev\' in key and \'Evaluation\' not in key:\n                self.csv[key] = self.csv[key].fillna(value=0)\n\n        self.signal_files_need_update = True\n\n    def reload_data(self):\n        SignalsFileBase.reload_data(self)\n\n    def update_x_axis_index(self):\n        SignalsFileBase.update_x_axis_index(self)\n\n        # update the x axis for the bollinger bands\n        for signal in self.signals.values():\n            if signal.has_bollinger_bands:\n                signal.set_bands_source()\n\n    def toggle_y_axis(self, signal_name=None):\n        for signal in self.signals.values():\n            if signal.selected:\n                signal.toggle_axis()\n\n    def change_averaging_window(self, new_size, force=False, signals=None):\n        SignalsFileBase.change_averaging_window(self, new_size, force, signals)\n\n    def set_signal_selection(self, signal_name, val):\n        self.show_files_separately(self.separate_files)\n        SignalsFileBase.set_signal_selection(self, signal_name, val)\n\n    def file_was_modified_on_disk(self):\n        for signal_file in self.signals_files:\n            if signal_file.file_was_modified_on_disk():\n                return True\n        return False\n\n    def show_files_separately(self, val):\n        self.separate_files = val\n\n        # lazy updating of the signals of each of the workers\n        if self.separate_files and self.signal_files_need_update:\n            for signal_file in self.signals_files:\n                signal_file.update_source_and_signals()\n            self.signal_files_need_update = False\n\n        for signal in self.signals.values():\n            if signal.selected:\n                if val:\n                    signal.set_dash(""4 4"")\n                else:\n                    signal.set_dash("""")\n            for signal_file in self.signals_files:\n                try:\n                    if val:\n                        signal_file.set_signal_selection(signal.name, signal.selected)\n                    else:\n                        signal_file.set_signal_selection(signal.name, False)\n                except:\n                    pass'"
rl_coach/data_stores/__init__.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
rl_coach/data_stores/checkpoint_data_store.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport time\nimport os\n\nfrom rl_coach.checkpoint import CheckpointStateReader\nfrom rl_coach.data_stores.data_store import SyncFiles\n\n\nclass CheckpointDataStore(object):\n    """"""\n    A DataStore which relies on the GraphManager check pointing methods to communicate policies.\n    """"""\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.checkpoint_num = 0\n\n    def end_of_policies(self) -> bool:\n        """"""\n        Returns true if no new policies will be added to this DataStore. This typically happens\n        because training has completed and is used to signal to the rollout workers to stop.\n        """"""\n        return os.path.exists(\n            os.path.join(self.checkpoint_dir, SyncFiles.FINISHED.value)\n        )\n\n    def save_policy(self, graph_manager):\n        # TODO: it would be nice if restore_checkpoint accepted a checkpoint path as a\n        # parameter. as it is, one cannot distinguish between checkpoints used for coordination\n        # and checkpoints requested to a persistent disk for later use\n        graph_manager.task_parameters.checkpoint_restore_path = self.checkpoint_dir\n        graph_manager.save_checkpoint()\n\n    def load_policy(self, graph_manager, require_new_policy=True, timeout=None):\n        """"""\n        Load a checkpoint into the specified graph_manager. The expectation here is that\n        save_to_store() and load_from_store() will synchronize a checkpoint directory with a\n        central repository such as NFS or S3.\n\n        :param graph_manager: the graph_manager to load the policy into\n        :param require_new_policy: if True, only load a policy if it hasn\'t been loaded in this\n        process yet before.\n        :param timeout: Will only try to load the policy once if timeout is None, otherwise will\n        retry for timeout seconds\n        """"""\n        if self._new_policy_exists(require_new_policy, timeout):\n            # TODO: it would be nice if restore_checkpoint accepted a checkpoint path as a\n            # parameter. as it is, one cannot distinguish between checkpoints used for coordination\n            # and checkpoints requested to a persistent disk for later use\n            graph_manager.task_parameters.checkpoint_restore_path = self.checkpoint_dir\n            graph_manager.restore_checkpoint()\n\n    def _new_policy_exists(self, require_new_policy=True, timeout=None) -> bool:\n        """"""\n        :param require_new_policy: if True, only load a policy if it hasn\'t been loaded in this\n        process yet before.\n        :param timeout: Will only try to load the policy once if timeout is None, otherwise will\n        retry for timeout seconds\n        """"""\n        checkpoint_state_reader = CheckpointStateReader(\n            self.checkpoint_dir, checkpoint_state_optional=False\n        )\n        checkpoint = ""first""\n        if timeout is None:\n            timeout = 0\n        timeout_ends = time.time() + timeout\n        while time.time() < timeout_ends or checkpoint == ""first"":\n            if self.end_of_policies():\n                return False\n\n            self.load_from_store()\n\n            checkpoint = checkpoint_state_reader.get_latest()\n            if checkpoint is not None:\n                if not require_new_policy or checkpoint.num > self.checkpoint_num:\n                    self.checkpoint_num = checkpoint.num\n                    return True\n\n        raise ValueError(\n            ""Waited for {timeout} seconds, but no first policy was received."".format(\n                timeout=timeout\n            )\n        )\n'"
rl_coach/data_stores/data_store.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom enum import Enum\n\n\nclass DataStoreParameters(object):\n    def __init__(self, store_type, orchestrator_type, orchestrator_params):\n        self.store_type = store_type\n        self.orchestrator_type = orchestrator_type\n        self.orchestrator_params = orchestrator_params\n\n\nclass DataStore(object):\n    """"""\n    DataStores are used primarily to synchronize policies between training workers and rollout\n    workers. In the case of the S3DataStore, it is also being used to explicitly log artifacts such\n    as videos and logs into s3 for users to look at later. Artifact logging should be moved into a\n    separate instance of the DataStore class, or a different class altogether. It is possible that\n    users might be interested in logging artifacts through s3, but coordinating communication of\n    policies using something else like redis.\n    """"""\n\n    def __init__(self, params: DataStoreParameters):\n        """"""\n        The parameters provided in the constructor to a DataStore are expected to contain the\n        parameters necessary to serialize and deserialize this DataStore.\n        """"""\n        raise NotImplementedError()\n\n    def deploy(self) -> bool:\n        raise NotImplementedError()\n\n    def get_info(self):\n        raise NotImplementedError()\n\n    def undeploy(self) -> bool:\n        raise NotImplementedError()\n\n    def save_to_store(self):\n        raise NotImplementedError()\n\n    def load_from_store(self):\n        raise NotImplementedError()\n\n    def save_policy(self, graph_manager):\n        raise NotImplementedError()\n\n    def load_policy(self, graph_manager, timeout=-1):\n        raise NotImplementedError()\n\n    def end_of_policies(self) -> bool:\n        raise NotImplementedError()\n\n    def setup_checkpoint_dir(self, crd=None):\n        pass\n\n\nclass SyncFiles(Enum):\n    FINISHED = "".finished""\n    LOCKFILE = "".lock""\n    TRAINER_READY = "".ready""\n'"
rl_coach/data_stores/data_store_impl.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom rl_coach.data_stores.nfs_data_store import NFSDataStore, NFSDataStoreParameters\nfrom rl_coach.data_stores.s3_data_store import S3DataStore, S3DataStoreParameters\nfrom rl_coach.data_stores.redis_data_store import (\n    RedisDataStore,\n    RedisDataStoreParameters,\n)\nfrom rl_coach.data_stores.data_store import DataStoreParameters\n\n\ndef get_data_store(params):\n    data_store = None\n    if type(params) == NFSDataStoreParameters:\n        data_store = NFSDataStore(params)\n    elif type(params) == S3DataStoreParameters:\n        data_store = S3DataStore(params)\n    elif type(params) == RedisDataStoreParameters:\n        data_store = RedisDataStore(params)\n    else:\n        raise ValueError(""invalid params type {}"".format(type(params)))\n\n    return data_store\n\n\ndef construct_data_store_params(json: dict):\n    ds_params_instance = None\n    ds_params = DataStoreParameters(\n        json[""store_type""], json[""orchestrator_type""], json[""orchestrator_params""]\n    )\n    if json[""store_type""] == ""nfs"":\n        ds_params_instance = NFSDataStoreParameters(\n            ds_params, checkpoint_dir=json[""checkpoint_dir""]\n        )\n    elif json[""store_type""] == ""s3"":\n        ds_params_instance = S3DataStoreParameters(\n            ds_params=ds_params,\n            end_point=json[""end_point""],\n            bucket_name=json[""bucket_name""],\n            checkpoint_dir=json[""checkpoint_dir""],\n            expt_dir=json[""expt_dir""],\n        )\n    elif json[""store_type""] == ""redis"":\n        ds_params_instance = RedisDataStoreParameters(\n            ds_params,\n            redis_address=json[""redis_address""],\n            redis_port=json[""redis_port""],\n            redis_channel=json[""redis_channel""],\n        )\n    else:\n        raise ValueError(""store_type {} was found, expected \'nfs\', \'redis\' or \'s3\'."")\n\n    return ds_params_instance\n'"
rl_coach/data_stores/nfs_data_store.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport uuid\n\nfrom rl_coach.data_stores.data_store import DataStoreParameters\nfrom rl_coach.data_stores.checkpoint_data_store import CheckpointDataStore\n\n\nclass NFSDataStoreParameters(DataStoreParameters):\n    def __init__(self, ds_params, deployed=False, server=None, path=None, checkpoint_dir: str=""""):\n        super().__init__(ds_params.store_type, ds_params.orchestrator_type, ds_params.orchestrator_params)\n        self.namespace = ""default""\n        if ""namespace"" in ds_params.orchestrator_params:\n            self.namespace = ds_params.orchestrator_params[""namespace""]\n        self.checkpoint_dir = checkpoint_dir\n        self.name = None\n        self.pvc_name = None\n        self.pv_name = None\n        self.svc_name = None\n        self.server = None\n        self.path = ""/""\n        self.deployed = deployed\n        if deployed:\n            self.server = server\n            self.path = path\n\n\nclass NFSDataStore(CheckpointDataStore):\n    """"""\n    An implementation of data store which uses NFS for storing policy checkpoints when using Coach in distributed mode.\n    The policy checkpoints are written by the trainer and read by the rollout worker.\n    """"""\n\n    def __init__(self, params: NFSDataStoreParameters):\n        """"""\n        :param params: The parameters required to use the NFS data store.\n        """"""\n        self.params = params\n\n    def deploy(self) -> bool:\n        """"""\n        Deploy the NFS server in an orchestrator if/when required.\n        """"""\n        if self.params.orchestrator_type == ""kubernetes"":\n            if not self.params.deployed:\n                if not self.deploy_k8s_nfs():\n                    return False\n            if not self.create_k8s_nfs_resources():\n                return False\n\n        return True\n\n    def get_info(self):\n        from kubernetes import client as k8sclient\n\n        return k8sclient.V1PersistentVolumeClaimVolumeSource(\n                claim_name=self.params.pvc_name\n        )\n\n    def undeploy(self) -> bool:\n        """"""\n        Undeploy the NFS server and resources from an orchestrator.\n        """"""\n        if self.params.orchestrator_type == ""kubernetes"":\n            if not self.params.deployed:\n                if not self.undeploy_k8s_nfs():\n                    return False\n            if not self.delete_k8s_nfs_resources():\n                return False\n\n        return True\n\n    def save_to_store(self):\n        pass\n\n    def load_from_store(self):\n        pass\n\n    def deploy_k8s_nfs(self) -> bool:\n        """"""\n        Deploy the NFS server in the Kubernetes orchestrator.\n        """"""\n        from kubernetes import client as k8sclient\n\n        name = ""nfs-server-{}"".format(uuid.uuid4())\n        container = k8sclient.V1Container(\n            name=name,\n            image=""k8s.gcr.io/volume-nfs:0.8"",\n            ports=[k8sclient.V1ContainerPort(\n                    name=""nfs"",\n                    container_port=2049,\n                    protocol=""TCP""\n                   ),\n                   k8sclient.V1ContainerPort(\n                    name=""rpcbind"",\n                    container_port=111\n                   ),\n                   k8sclient.V1ContainerPort(\n                    name=""mountd"",\n                    container_port=20048\n                   ),\n            ],\n            volume_mounts=[k8sclient.V1VolumeMount(\n                name=\'nfs-host-path\',\n                mount_path=\'/exports\'\n            )],\n            security_context=k8sclient.V1SecurityContext(privileged=True)\n        )\n        template = k8sclient.V1PodTemplateSpec(\n            metadata=k8sclient.V1ObjectMeta(labels={\'app\': name}),\n            spec=k8sclient.V1PodSpec(\n                containers=[container],\n                volumes=[k8sclient.V1Volume(\n                    name=""nfs-host-path"",\n                    host_path=k8sclient.V1HostPathVolumeSource(path=\'/tmp/nfsexports-{}\'.format(uuid.uuid4()))\n                )]\n            )\n        )\n        deployment_spec = k8sclient.V1DeploymentSpec(\n            replicas=1,\n            template=template,\n            selector=k8sclient.V1LabelSelector(\n                match_labels={\'app\': name}\n            )\n        )\n\n        deployment = k8sclient.V1Deployment(\n            api_version=\'apps/v1\',\n            kind=\'Deployment\',\n            metadata=k8sclient.V1ObjectMeta(name=name, labels={\'app\': name}),\n            spec=deployment_spec\n        )\n\n        k8s_apps_v1_api_client = k8sclient.AppsV1Api()\n        try:\n            k8s_apps_v1_api_client.create_namespaced_deployment(self.params.namespace, deployment)\n            self.params.name = name\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while creating nfs-server"", e)\n            return False\n\n        k8s_core_v1_api_client = k8sclient.CoreV1Api()\n\n        svc_name = ""nfs-service-{}"".format(uuid.uuid4())\n        service = k8sclient.V1Service(\n            api_version=\'v1\',\n            kind=\'Service\',\n            metadata=k8sclient.V1ObjectMeta(\n                name=svc_name\n            ),\n            spec=k8sclient.V1ServiceSpec(\n                selector={\'app\': self.params.name},\n                ports=[k8sclient.V1ServicePort(\n                    protocol=\'TCP\',\n                    port=2049,\n                    target_port=2049\n                )]\n            )\n        )\n\n        try:\n            svc_response = k8s_core_v1_api_client.create_namespaced_service(self.params.namespace, service)\n            self.params.svc_name = svc_name\n            self.params.server = svc_response.spec.cluster_ip\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while creating a service for nfs-server"", e)\n            return False\n\n        return True\n\n    def create_k8s_nfs_resources(self) -> bool:\n        """"""\n        Create NFS resources such as PV and PVC in Kubernetes.\n        """"""\n        from kubernetes import client as k8sclient\n\n        pv_name = ""nfs-ckpt-pv-{}"".format(uuid.uuid4())\n        persistent_volume = k8sclient.V1PersistentVolume(\n            api_version=""v1"",\n            kind=""PersistentVolume"",\n            metadata=k8sclient.V1ObjectMeta(\n                name=pv_name,\n                labels={\'app\': pv_name}\n            ),\n            spec=k8sclient.V1PersistentVolumeSpec(\n                access_modes=[""ReadWriteMany""],\n                nfs=k8sclient.V1NFSVolumeSource(\n                    path=self.params.path,\n                    server=self.params.server\n                ),\n                capacity={\'storage\': \'10Gi\'},\n                storage_class_name=""""\n            )\n        )\n        k8s_api_client = k8sclient.CoreV1Api()\n        try:\n            k8s_api_client.create_persistent_volume(persistent_volume)\n            self.params.pv_name = pv_name\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while creating the NFS PV"", e)\n            return False\n\n        pvc_name = ""nfs-ckpt-pvc-{}"".format(uuid.uuid4())\n        persistent_volume_claim = k8sclient.V1PersistentVolumeClaim(\n            api_version=""v1"",\n            kind=""PersistentVolumeClaim"",\n            metadata=k8sclient.V1ObjectMeta(\n                name=pvc_name\n            ),\n            spec=k8sclient.V1PersistentVolumeClaimSpec(\n                access_modes=[""ReadWriteMany""],\n                resources=k8sclient.V1ResourceRequirements(\n                    requests={\'storage\': \'10Gi\'}\n                ),\n                selector=k8sclient.V1LabelSelector(\n                    match_labels={\'app\': self.params.pv_name}\n                ),\n                storage_class_name=""""\n            )\n        )\n\n        try:\n            k8s_api_client.create_namespaced_persistent_volume_claim(self.params.namespace, persistent_volume_claim)\n            self.params.pvc_name = pvc_name\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while creating the NFS PVC"", e)\n            return False\n\n        return True\n\n    def undeploy_k8s_nfs(self) -> bool:\n        from kubernetes import client as k8sclient\n\n        del_options = k8sclient.V1DeleteOptions()\n\n        k8s_apps_v1_api_client = k8sclient.AppsV1Api()\n        try:\n            k8s_apps_v1_api_client.delete_namespaced_deployment(self.params.name, self.params.namespace, del_options)\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while deleting nfs-server"", e)\n            return False\n\n        k8s_core_v1_api_client = k8sclient.CoreV1Api()\n        try:\n            k8s_core_v1_api_client.delete_namespaced_service(self.params.svc_name, self.params.namespace, del_options)\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while deleting the service for nfs-server"", e)\n            return False\n\n        return True\n\n    def delete_k8s_nfs_resources(self) -> bool:\n        """"""\n        Delete NFS resources such as PV and PVC from the Kubernetes orchestrator.\n        """"""\n        from kubernetes import client as k8sclient\n\n        del_options = k8sclient.V1DeleteOptions()\n        k8s_api_client = k8sclient.CoreV1Api()\n\n        try:\n            k8s_api_client.delete_persistent_volume(self.params.pv_name, del_options)\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while deleting NFS PV"", e)\n            return False\n\n        try:\n            k8s_api_client.delete_namespaced_persistent_volume_claim(self.params.pvc_name, self.params.namespace, del_options)\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while deleting NFS PVC"", e)\n            return False\n\n        return True\n\n    def setup_checkpoint_dir(self, crd=None):\n        if crd:\n            # TODO: find a way to upload this to the deployed nfs store.\n            pass\n'"
rl_coach/data_stores/redis_data_store.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport time\nimport uuid\n\nimport redis\n\nfrom rl_coach.architectures.tensorflow_components.savers import GlobalVariableSaver\nfrom rl_coach.data_stores.data_store import DataStore, DataStoreParameters\n\n\nclass RedisDataStoreParameters(DataStoreParameters):\n    def __init__(\n        self,\n        ds_params,\n        redis_address: str = """",\n        redis_port: int = 6379,\n        redis_channel: str = ""data-store-channel-{}"".format(uuid.uuid4()),\n    ):\n        super().__init__(\n            ds_params.store_type,\n            ds_params.orchestrator_type,\n            ds_params.orchestrator_params,\n        )\n        self.redis_address = redis_address\n        self.redis_port = redis_port\n        self.redis_channel = redis_channel\n\n\nclass RedisDataStore(DataStore):\n    """"""\n    This DataStore sends policies over redis pubsub and get/set.\n\n    Deployment\n    ==========\n    It assumes that a redis server is already available. We make this assumption because during\n    multinode training at this time, redis is already used for communicating replay memories.\n\n    Communication\n    =============\n\n    A redis pubsub channel is used by the training worker to signal to the rollout workers that a\n    new policy is ready. When this occurs, a new policy is loaded from the redis key/value store\n    where key is the same as the pubsub channel. Originally, just the pubsub was used, but that\n    could result in a race condition where the master worker publishes the first policy and waits\n    for the rollout workers to submit all rollouts, while a delayed rollout worker waits for the\n    first policy since it subscribed to the channel after the initial policy was published.\n    """"""\n\n    def __init__(self, params: RedisDataStoreParameters):\n        self.params = params\n        self.saver = None\n        self._end_of_policies = False\n\n        # NOTE: a connection is not attempted at this stage because the address and port are likely\n        # not available yet. This is because of how the kubernetes orchestrator works. At the time\n        # of parameter construction, the address and port are not yet known since they are copied\n        # out of the redis memory backend after it is deployed. One improvement would be to use\n        # two separate redis deployments independently, and let this class deploy its own redis.\n\n    def _connect(self):\n        """"""\n        Connect to redis and subscribe to the pubsub channel\n        """"""\n        self.redis_connection = redis.Redis(\n            self.params.redis_address, self.params.redis_port\n        )\n        self.pubsub = self.redis_connection.pubsub(ignore_subscribe_messages=True)\n        self.pubsub.subscribe(self.params.redis_channel)\n\n        self._end_of_policies = False\n\n    def deploy(self):\n        """"""\n        For now, this data store does not handle its own deployment, it piggybacks off of the redis\n        memory backend\n        """"""\n        return True\n\n    def undeploy(self):\n        """"""\n        For now, this data store does not handle its own deployment, it piggybacks off of the redis\n        memory backend\n        """"""\n        pass\n\n    def save_to_store(self):\n        """"""\n        save_to_store and load_from_store are not used in the case where the data stored needs to\n        synchronize checkpoints saved to disk into a central file system, and not used here\n        """"""\n        pass\n\n    def load_from_store(self):\n        """"""\n        save_to_store and load_from_store are not used in the case where the data stored needs to\n        synchronize checkpoints saved to disk into a central file system, and not used here\n        """"""\n        pass\n\n    def save_policy(self, graph_manager):\n        """"""\n        Serialize the policy in graph_manager, set it as the latest policy and publish a new_policy\n        event\n        """"""\n        if self.saver is None:\n            self.saver = GlobalVariableSaver()\n\n            # TODO: only subscribe if this data store is being used to publish policies\n            self._connect()\n            self.pubsub.unsubscribe(self.params.redis_channel)\n\n        policy_string = self.saver.to_string(graph_manager.sess)\n        self.redis_connection.set(self.params.redis_channel, policy_string)\n        self.redis_connection.publish(self.params.redis_channel, ""new_policy"")\n\n    def _load_policy(self, graph_manager) -> bool:\n        """"""\n        Get the most recent policy from redis and loaded into the graph_manager\n        """"""\n        policy_string = self.redis_connection.get(self.params.redis_channel)\n        if policy_string is None:\n            return False\n\n        self.saver.from_string(graph_manager.sess, policy_string)\n        return True\n\n    def load_policy(self, graph_manager, require_new_policy=True, timeout=0):\n        """"""\n        :param graph_manager: the graph_manager to load the policy into\n        :param require_new_policy: if True, only load a policy if it hasn\'t been loaded in this\n        process yet before.\n        :param timeout: Will only try to load the policy once if timeout is None, otherwise will\n        retry for timeout seconds\n        """"""\n        if self.saver is None:\n            # the GlobalVariableSaver needs to be instantiated after the graph is created. For now,\n            # it can be instantiated here, but it might be nicer to have a more explicit\n            # on_graph_creation_end callback or similar to put it in\n            self.saver = GlobalVariableSaver()\n            self._connect()\n\n        if not require_new_policy:\n            # try just loading whatever policy is available most recently\n            if self._load_policy(graph_manager):\n                return\n\n        message = ""first""\n        timeout_ends = time.time() + timeout\n        while time.time() < timeout_ends or message == ""first"":\n            message = self.pubsub.get_message()\n\n            if message and message[""type""] == ""message"":\n                if message[""data""] == b""end_of_policies"":\n                    self._end_of_policies = True\n                    return\n                elif message[""data""] == b""new_policy"":\n                    if self._load_policy(graph_manager):\n                        return\n                    else:\n                        raise ValueError(""\'new_policy\' message was sent, but no policy was found."")\n\n            time.sleep(1.0)\n\n        if require_new_policy:\n            raise ValueError(\n                ""Waited for {timeout} seconds on channel {channel}, but no first policy was received."".format(\n                    timeout=timeout, channel=self.params.redis_channel\n                )\n            )\n\n    def end_of_policies(self) -> bool:\n        """"""\n        This is used by the rollout workers to detect a message from the training worker signaling\n        that training is complete.\n        """"""\n        return self._end_of_policies\n'"
rl_coach/data_stores/s3_data_store.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom rl_coach.data_stores.data_store import DataStoreParameters\nfrom rl_coach.data_stores.checkpoint_data_store import CheckpointDataStore\nfrom minio import Minio\nfrom minio.error import ResponseError\nfrom configparser import ConfigParser, Error\nfrom rl_coach.checkpoint import CheckpointStateFile\nfrom rl_coach.data_stores.data_store import SyncFiles\n\nimport os\nimport time\nimport io\n\n\nclass S3DataStoreParameters(DataStoreParameters):\n    def __init__(self, ds_params, creds_file: str = None, end_point: str = None, bucket_name: str = None,\n                 checkpoint_dir: str = None, expt_dir: str = None):\n\n        super().__init__(ds_params.store_type, ds_params.orchestrator_type, ds_params.orchestrator_params)\n        self.creds_file = creds_file\n        self.end_point = end_point\n        self.bucket_name = bucket_name\n        self.checkpoint_dir = checkpoint_dir\n        self.expt_dir = expt_dir\n\n\nclass S3DataStore(CheckpointDataStore):\n    """"""\n    An implementation of the data store using S3 for storing policy checkpoints when using Coach in distributed mode.\n    The policy checkpoints are written by the trainer and read by the rollout worker.\n    """"""\n\n    def __init__(self, params: S3DataStoreParameters):\n        """"""\n        :param params: The parameters required to use the S3 data store.\n        """"""\n\n        super(S3DataStore, self).__init__(params)\n        self.params = params\n        access_key = None\n        secret_key = None\n        if params.creds_file:\n            config = ConfigParser()\n            config.read(params.creds_file)\n            try:\n                access_key = config.get(\'default\', \'aws_access_key_id\')\n                secret_key = config.get(\'default\', \'aws_secret_access_key\')\n            except Error as e:\n                print(""Error when reading S3 credentials file: %s"", e)\n        else:\n            access_key = os.environ.get(\'ACCESS_KEY_ID\')\n            secret_key = os.environ.get(\'SECRET_ACCESS_KEY\')\n        self.mc = Minio(self.params.end_point, access_key=access_key, secret_key=secret_key)\n\n    def deploy(self) -> bool:\n        return True\n\n    def get_info(self):\n        return ""s3://{}/{}"".format(self.params.bucket_name)\n\n    def undeploy(self) -> bool:\n        return True\n\n    def save_to_store(self):\n        self._save_to_store(self.params.checkpoint_dir)\n\n    def _save_to_store(self, checkpoint_dir):\n        """"""\n        save_to_store() uploads the policy checkpoint, gifs and videos to the S3 data store. It reads the checkpoint state files and\n        uploads only the latest checkpoint files to S3. It is used by the trainer in Coach when used in the distributed mode.\n        """"""\n        try:\n            # remove lock file if it exists\n            self.mc.remove_object(self.params.bucket_name, SyncFiles.LOCKFILE.value)\n\n            # Acquire lock\n            self.mc.put_object(self.params.bucket_name, SyncFiles.LOCKFILE.value, io.BytesIO(b\'\'), 0)\n\n            state_file = CheckpointStateFile(os.path.abspath(checkpoint_dir))\n            if state_file.exists():\n                ckpt_state = state_file.read()\n                checkpoint_file = None\n                for root, dirs, files in os.walk(checkpoint_dir):\n                    for filename in files:\n                        if filename == CheckpointStateFile.checkpoint_state_filename:\n                            checkpoint_file = (root, filename)\n                            continue\n                        if filename.startswith(ckpt_state.name):\n                            abs_name = os.path.abspath(os.path.join(root, filename))\n                            rel_name = os.path.relpath(abs_name, checkpoint_dir)\n                            self.mc.fput_object(self.params.bucket_name, rel_name, abs_name)\n\n                abs_name = os.path.abspath(os.path.join(checkpoint_file[0], checkpoint_file[1]))\n                rel_name = os.path.relpath(abs_name, checkpoint_dir)\n                self.mc.fput_object(self.params.bucket_name, rel_name, abs_name)\n\n            # upload Finished if present\n            if os.path.exists(os.path.join(checkpoint_dir, SyncFiles.FINISHED.value)):\n                self.mc.put_object(self.params.bucket_name, SyncFiles.FINISHED.value, io.BytesIO(b\'\'), 0)\n\n            # upload Ready if present\n            if os.path.exists(os.path.join(checkpoint_dir, SyncFiles.TRAINER_READY.value)):\n                self.mc.put_object(self.params.bucket_name, SyncFiles.TRAINER_READY.value, io.BytesIO(b\'\'), 0)\n\n            # release lock\n            self.mc.remove_object(self.params.bucket_name, SyncFiles.LOCKFILE.value)\n\n            if self.params.expt_dir and os.path.exists(self.params.expt_dir):\n                for filename in os.listdir(self.params.expt_dir):\n                    if filename.endswith(("".csv"", "".json"")):\n                        self.mc.fput_object(self.params.bucket_name, filename, os.path.join(self.params.expt_dir, filename))\n\n            if self.params.expt_dir and os.path.exists(os.path.join(self.params.expt_dir, \'videos\')):\n                for filename in os.listdir(os.path.join(self.params.expt_dir, \'videos\')):\n                        self.mc.fput_object(self.params.bucket_name, filename, os.path.join(self.params.expt_dir, \'videos\', filename))\n\n            if self.params.expt_dir and os.path.exists(os.path.join(self.params.expt_dir, \'gifs\')):\n                for filename in os.listdir(os.path.join(self.params.expt_dir, \'gifs\')):\n                        self.mc.fput_object(self.params.bucket_name, filename, os.path.join(self.params.expt_dir, \'gifs\', filename))\n\n        except ResponseError as e:\n            print(""Got exception: %s\\n while saving to S3"", e)\n\n    def load_from_store(self):\n        """"""\n        load_from_store() downloads a new checkpoint from the S3 data store when it is not available locally. It is used\n        by the rollout workers when using Coach in distributed mode.\n        """"""\n        try:\n            state_file = CheckpointStateFile(os.path.abspath(self.params.checkpoint_dir))\n\n            # wait until lock is removed\n            while True:\n                objects = self.mc.list_objects_v2(self.params.bucket_name, SyncFiles.LOCKFILE.value)\n\n                if next(objects, None) is None:\n                    try:\n                        # fetch checkpoint state file from S3\n                        self.mc.fget_object(self.params.bucket_name, state_file.filename, state_file.path)\n                    except Exception as e:\n                        continue\n                    break\n                time.sleep(10)\n\n            # Check if there\'s a finished file\n            objects = self.mc.list_objects_v2(self.params.bucket_name, SyncFiles.FINISHED.value)\n\n            if next(objects, None) is not None:\n                try:\n                    self.mc.fget_object(\n                        self.params.bucket_name, SyncFiles.FINISHED.value,\n                        os.path.abspath(os.path.join(self.params.checkpoint_dir, SyncFiles.FINISHED.value))\n                    )\n                except Exception as e:\n                    pass\n\n            # Check if there\'s a ready file\n            objects = self.mc.list_objects_v2(self.params.bucket_name, SyncFiles.TRAINER_READY.value)\n\n            if next(objects, None) is not None:\n                try:\n                    self.mc.fget_object(\n                        self.params.bucket_name, SyncFiles.TRAINER_READY.value,\n                        os.path.abspath(os.path.join(self.params.checkpoint_dir, SyncFiles.TRAINER_READY.value))\n                    )\n                except Exception as e:\n                    pass\n\n            checkpoint_state = state_file.read()\n            if checkpoint_state is not None:\n                objects = self.mc.list_objects_v2(self.params.bucket_name, prefix=checkpoint_state.name, recursive=True)\n                for obj in objects:\n                    filename = os.path.abspath(os.path.join(self.params.checkpoint_dir, obj.object_name))\n                    if not os.path.exists(filename):\n                        self.mc.fget_object(obj.bucket_name, obj.object_name, filename)\n\n        except ResponseError as e:\n            print(""Got exception: %s\\n while loading from S3"", e)\n\n    def setup_checkpoint_dir(self, crd=None):\n        if crd:\n            self._save_to_store(crd)\n'"
rl_coach/environments/__init__.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n'"
rl_coach/environments/carla_environment.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport random\nimport sys\nfrom os import path, environ\nfrom rl_coach.logger import screen\nfrom rl_coach.filters.action.partial_discrete_action_space_map import PartialDiscreteActionSpaceMap\nfrom rl_coach.filters.observation.observation_rgb_to_y_filter import ObservationRGBToYFilter\nfrom rl_coach.filters.observation.observation_to_uint8_filter import ObservationToUInt8Filter\n\ntry:\n    if \'CARLA_ROOT\' in environ:\n        sys.path.append(path.join(environ.get(\'CARLA_ROOT\'), \'PythonClient\'))\n    else:\n        screen.error(""CARLA_ROOT was not defined. Please set it to point to the CARLA root directory and try again."", crash=False)\n\n    from carla.client import CarlaClient\n    from carla.settings import CarlaSettings\n    from carla.tcp import TCPConnectionError\n    from carla.sensor import Camera\n    from carla.client import VehicleControl\n    from carla.planner.planner import Planner\n    from carla.driving_benchmark.experiment_suites.experiment_suite import ExperimentSuite\nexcept ImportError:\n    from rl_coach.logger import failed_imports\n    failed_imports.append(""CARLA"")\n\nimport os\nimport signal\nimport logging\nimport subprocess\nimport numpy as np\nfrom rl_coach.environments.environment import Environment, EnvironmentParameters, LevelSelection\nfrom rl_coach.spaces import BoxActionSpace, ImageObservationSpace, StateSpace, VectorObservationSpace\nfrom rl_coach.utils import get_open_port, force_list\nfrom enum import Enum\nfrom typing import List, Union\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.filters.filter import InputFilter, NoOutputFilter\nfrom rl_coach.filters.observation.observation_rescale_to_size_filter import ObservationRescaleToSizeFilter\nfrom rl_coach.filters.observation.observation_stacking_filter import ObservationStackingFilter\n\n\n# enum of the available levels and their path\nclass CarlaLevel(Enum):\n    TOWN1 = {""map_name"": ""Town01"", ""map_path"": ""/Game/Maps/Town01""}\n    TOWN2 = {""map_name"": ""Town02"", ""map_path"": ""/Game/Maps/Town02""}\n\n\nkey_map = {\n    \'BRAKE\': (274,),  # down arrow\n    \'GAS\': (273,),  # up arrow\n    \'TURN_LEFT\': (276,),  # left arrow\n    \'TURN_RIGHT\': (275,),  # right arrow\n    \'GAS_AND_TURN_LEFT\': (273, 276),\n    \'GAS_AND_TURN_RIGHT\': (273, 275),\n    \'BRAKE_AND_TURN_LEFT\': (274, 276),\n    \'BRAKE_AND_TURN_RIGHT\': (274, 275),\n}\n\nCarlaInputFilter = InputFilter(is_a_reference_filter=True)\nCarlaInputFilter.add_observation_filter(\'forward_camera\', \'rescaling\',\n                                        ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([128, 180, 3]),\n                                                                                             high=255)))\nCarlaInputFilter.add_observation_filter(\'forward_camera\', \'to_grayscale\', ObservationRGBToYFilter())\nCarlaInputFilter.add_observation_filter(\'forward_camera\', \'to_uint8\', ObservationToUInt8Filter(0, 255))\nCarlaInputFilter.add_observation_filter(\'forward_camera\', \'stacking\', ObservationStackingFilter(4))\n\nCarlaOutputFilter = NoOutputFilter()\n\n\nclass CameraTypes(Enum):\n    FRONT = ""forward_camera""\n    LEFT = ""left_camera""\n    RIGHT = ""right_camera""\n    SEGMENTATION = ""segmentation""\n    DEPTH = ""depth""\n    LIDAR = ""lidar""\n\n\nclass CarlaEnvironmentParameters(EnvironmentParameters):\n    class Quality(Enum):\n        LOW = ""Low""\n        EPIC = ""Epic""\n\n    def __init__(self, level=""town1""):\n        super().__init__(level=level)\n        self.frame_skip = 3  # the frame skip affects the fps of the server directly. fps = 30 / frameskip\n        self.server_height = 512\n        self.server_width = 720\n        self.camera_height = 128\n        self.camera_width = 180\n        self.experiment_suite = None  # an optional CARLA experiment suite to use\n        self.config = None\n        self.level = level\n        self.quality = self.Quality.LOW\n        self.cameras = [CameraTypes.FRONT]\n        self.weather_id = [1]\n        self.verbose = True\n        self.episode_max_time = 100000  # miliseconds for each episode\n        self.allow_braking = False\n        self.separate_actions_for_throttle_and_brake = False\n        self.num_speedup_steps = 30\n        self.max_speed = 35.0  # km/h\n        self.default_input_filter = CarlaInputFilter\n        self.default_output_filter = CarlaOutputFilter\n\n    @property\n    def path(self):\n        return \'rl_coach.environments.carla_environment:CarlaEnvironment\'\n\n\nclass CarlaEnvironment(Environment):\n    def __init__(self, level: LevelSelection,\n                 seed: int, frame_skip: int, human_control: bool, custom_reward_threshold: Union[int, float],\n                 visualization_parameters: VisualizationParameters,\n                 server_height: int, server_width: int, camera_height: int, camera_width: int,\n                 verbose: bool, experiment_suite: ExperimentSuite, config: str, episode_max_time: int,\n                 allow_braking: bool, quality: CarlaEnvironmentParameters.Quality,\n                 cameras: List[CameraTypes], weather_id: List[int], experiment_path: str,\n                 separate_actions_for_throttle_and_brake: bool,\n                 num_speedup_steps: int, max_speed: float, target_success_rate: float = 1.0, **kwargs):\n        super().__init__(level, seed, frame_skip, human_control, custom_reward_threshold, visualization_parameters, target_success_rate)\n\n        # server configuration\n        self.server_height = server_height\n        self.server_width = server_width\n        self.port = get_open_port()\n        self.host = \'localhost\'\n        self.map_name = CarlaLevel[level.upper()].value[\'map_name\']\n        self.map_path = CarlaLevel[level.upper()].value[\'map_path\']\n        self.experiment_path = experiment_path\n\n        # client configuration\n        self.verbose = verbose\n        self.quality = quality\n        self.cameras = cameras\n        self.weather_id = weather_id\n        self.episode_max_time = episode_max_time\n        self.allow_braking = allow_braking\n        self.separate_actions_for_throttle_and_brake = separate_actions_for_throttle_and_brake\n        self.camera_width = camera_width\n        self.camera_height = camera_height\n\n        # setup server settings\n        self.experiment_suite = experiment_suite\n        self.config = config\n        if self.config:\n            # load settings from file\n            with open(self.config, \'r\') as fp:\n                self.settings = fp.read()\n        else:\n            # hard coded settings\n            self.settings = CarlaSettings()\n            self.settings.set(\n                SynchronousMode=True,\n                SendNonPlayerAgentsInfo=False,\n                NumberOfVehicles=15,\n                NumberOfPedestrians=30,\n                WeatherId=random.choice(force_list(self.weather_id)),\n                QualityLevel=self.quality.value,\n                SeedVehicles=seed,\n                SeedPedestrians=seed)\n            if seed is None:\n                self.settings.randomize_seeds()\n\n            self.settings = self._add_cameras(self.settings, self.cameras, self.camera_width, self.camera_height)\n\n        # open the server\n        self.server = self._open_server()\n\n        logging.disable(40)\n\n        # open the client\n        self.game = CarlaClient(self.host, self.port, timeout=99999999)\n        self.game.connect()\n        if self.experiment_suite:\n            self.current_experiment_idx = 0\n            self.current_experiment = self.experiment_suite.get_experiments()[self.current_experiment_idx]\n            self.scene = self.game.load_settings(self.current_experiment.conditions)\n        else:\n            self.scene = self.game.load_settings(self.settings)\n\n        # get available start positions\n        self.positions = self.scene.player_start_spots\n        self.num_positions = len(self.positions)\n        self.current_start_position_idx = 0\n        self.current_pose = 0\n\n        # state space\n        self.state_space = StateSpace({\n            ""measurements"": VectorObservationSpace(4, measurements_names=[""forward_speed"", ""x"", ""y"", ""z""])\n        })\n        for camera in self.scene.sensors:\n            self.state_space[camera.name] = ImageObservationSpace(\n                shape=np.array([self.camera_height, self.camera_width, 3]),\n                high=255)\n\n        # action space\n        if self.separate_actions_for_throttle_and_brake:\n            self.action_space = BoxActionSpace(shape=3, low=np.array([-1, 0, 0]), high=np.array([1, 1, 1]),\n                                               descriptions=[""steer"", ""gas"", ""brake""])\n        else:\n            self.action_space = BoxActionSpace(shape=2, low=np.array([-1, -1]), high=np.array([1, 1]),\n                                               descriptions=[""steer"", ""gas_and_brake""])\n\n        # human control\n        if self.human_control:\n            # convert continuous action space to discrete\n            self.steering_strength = 0.5\n            self.gas_strength = 1.0\n            self.brake_strength = 0.5\n            # TODO: reverse order of actions\n            self.action_space = PartialDiscreteActionSpaceMap(\n                target_actions=[[0., 0.],\n                                [0., -self.steering_strength],\n                                [0., self.steering_strength],\n                                [self.gas_strength, 0.],\n                                [-self.brake_strength, 0],\n                                [self.gas_strength, -self.steering_strength],\n                                [self.gas_strength, self.steering_strength],\n                                [self.brake_strength, -self.steering_strength],\n                                [self.brake_strength, self.steering_strength]],\n                descriptions=[\'NO-OP\', \'TURN_LEFT\', \'TURN_RIGHT\', \'GAS\', \'BRAKE\',\n                              \'GAS_AND_TURN_LEFT\', \'GAS_AND_TURN_RIGHT\',\n                              \'BRAKE_AND_TURN_LEFT\', \'BRAKE_AND_TURN_RIGHT\']\n            )\n\n            # map keyboard keys to actions\n            for idx, action in enumerate(self.action_space.descriptions):\n                for key in key_map.keys():\n                    if action == key:\n                        self.key_to_action[key_map[key]] = idx\n\n        self.num_speedup_steps = num_speedup_steps\n        self.max_speed = max_speed\n\n        # measurements\n        self.autopilot = None\n        self.planner = Planner(self.map_name)\n\n        # env initialization\n        self.reset_internal_state(True)\n\n        # render\n        if self.is_rendered:\n            image = self.get_rendered_image()\n            self.renderer.create_screen(image.shape[1], image.shape[0])\n\n        self.target_success_rate = target_success_rate\n\n    def _add_cameras(self, settings, cameras, camera_width, camera_height):\n        # add a front facing camera\n        if CameraTypes.FRONT in cameras:\n            camera = Camera(CameraTypes.FRONT.value)\n            camera.set(FOV=100)\n            camera.set_image_size(camera_width, camera_height)\n            camera.set_position(2.0, 0, 1.4)\n            camera.set_rotation(-15.0, 0, 0)\n            settings.add_sensor(camera)\n\n        # add a left facing camera\n        if CameraTypes.LEFT in cameras:\n            camera = Camera(CameraTypes.LEFT.value)\n            camera.set(FOV=100)\n            camera.set_image_size(camera_width, camera_height)\n            camera.set_position(2.0, 0, 1.4)\n            camera.set_rotation(-15.0, -30, 0)\n            settings.add_sensor(camera)\n\n        # add a right facing camera\n        if CameraTypes.RIGHT in cameras:\n            camera = Camera(CameraTypes.RIGHT.value)\n            camera.set(FOV=100)\n            camera.set_image_size(camera_width, camera_height)\n            camera.set_position(2.0, 0, 1.4)\n            camera.set_rotation(-15.0, 30, 0)\n            settings.add_sensor(camera)\n\n        # add a front facing depth camera\n        if CameraTypes.DEPTH in cameras:\n            camera = Camera(CameraTypes.DEPTH.value)\n            camera.set_image_size(camera_width, camera_height)\n            camera.set_position(0.2, 0, 1.3)\n            camera.set_rotation(8, 30, 0)\n            camera.PostProcessing = \'Depth\'\n            settings.add_sensor(camera)\n\n        # add a front facing semantic segmentation camera\n        if CameraTypes.SEGMENTATION in cameras:\n            camera = Camera(CameraTypes.SEGMENTATION.value)\n            camera.set_image_size(camera_width, camera_height)\n            camera.set_position(0.2, 0, 1.3)\n            camera.set_rotation(8, 30, 0)\n            camera.PostProcessing = \'SemanticSegmentation\'\n            settings.add_sensor(camera)\n\n        return settings\n\n    def _get_directions(self, current_point, end_point):\n        """"""\n        Class that should return the directions to reach a certain goal\n        """"""\n\n        directions = self.planner.get_next_command(\n            (current_point.location.x,\n             current_point.location.y, 0.22),\n            (current_point.orientation.x,\n             current_point.orientation.y,\n             current_point.orientation.z),\n            (end_point.location.x, end_point.location.y, 0.22),\n            (end_point.orientation.x, end_point.orientation.y, end_point.orientation.z))\n        return directions\n\n    def _open_server(self):\n        log_path = path.join(self.experiment_path if self.experiment_path is not None else \'.\', \'logs\',\n                             ""CARLA_LOG_{}.txt"".format(self.port))\n        if not os.path.exists(os.path.dirname(log_path)):\n            os.makedirs(os.path.dirname(log_path))\n        with open(log_path, ""wb"") as out:\n            cmd = [path.join(environ.get(\'CARLA_ROOT\'), \'CarlaUE4.sh\'), self.map_path,\n                   ""-benchmark"", ""-carla-server"", ""-fps={}"".format(30 / self.frame_skip),\n                   ""-world-port={}"".format(self.port),\n                   ""-windowed -ResX={} -ResY={}"".format(self.server_width, self.server_height),\n                   ""-carla-no-hud""]\n\n            if self.config:\n                cmd.append(""-carla-settings={}"".format(self.config))\n            p = subprocess.Popen(cmd, stdout=out, stderr=out)\n\n        return p\n\n    def _close_server(self):\n        os.killpg(os.getpgid(self.server.pid), signal.SIGKILL)\n\n    def _update_state(self):\n        # get measurements and observations\n        measurements = []\n        while type(measurements) == list:\n            measurements, sensor_data = self.game.read_data()\n        self.state = {}\n\n        for camera in self.scene.sensors:\n            self.state[camera.name] = sensor_data[camera.name].data\n\n        self.location = [measurements.player_measurements.transform.location.x,\n                         measurements.player_measurements.transform.location.y,\n                         measurements.player_measurements.transform.location.z]\n\n        self.distance_from_goal = np.linalg.norm(np.array(self.location[:2]) -\n                                                 [self.current_goal.location.x, self.current_goal.location.y])\n\n        is_collision = measurements.player_measurements.collision_vehicles != 0 \\\n                       or measurements.player_measurements.collision_pedestrians != 0 \\\n                       or measurements.player_measurements.collision_other != 0\n\n        speed_reward = measurements.player_measurements.forward_speed - 1\n        if speed_reward > 30.:\n            speed_reward = 30.\n        self.reward = speed_reward \\\n                      - (measurements.player_measurements.intersection_otherlane * 5) \\\n                      - (measurements.player_measurements.intersection_offroad * 5) \\\n                      - is_collision * 100 \\\n                      - np.abs(self.control.steer) * 10\n\n        # update measurements\n        self.measurements = [measurements.player_measurements.forward_speed] + self.location\n        self.autopilot = measurements.player_measurements.autopilot_control\n\n        # The directions to reach the goal (0 Follow lane, 1 Left, 2 Right, 3 Straight)\n        directions = int(self._get_directions(measurements.player_measurements.transform, self.current_goal) - 2)\n        self.state[\'high_level_command\'] = directions\n\n        if (measurements.game_timestamp >= self.episode_max_time) or is_collision:\n            self.done = True\n\n        self.state[\'measurements\'] = np.array(self.measurements)\n\n    def _take_action(self, action):\n        self.control = VehicleControl()\n\n        if self.separate_actions_for_throttle_and_brake:\n            self.control.steer = np.clip(action[0], -1, 1)\n            self.control.throttle = np.clip(action[1], 0, 1)\n            self.control.brake = np.clip(action[2], 0, 1)\n        else:\n            # transform the 2 value action (steer, throttle - brake) into a 3 value action (steer, throttle, brake)\n            self.control.steer = np.clip(action[0], -1, 1)\n            self.control.throttle = np.clip(action[1], 0, 1)\n            self.control.brake = np.abs(np.clip(action[1], -1, 0))\n\n        # prevent braking\n        if not self.allow_braking or self.control.brake < 0.1 or self.control.throttle > self.control.brake:\n            self.control.brake = 0\n\n        # prevent over speeding\n        if hasattr(self, \'measurements\') and self.measurements[0] * 3.6 > self.max_speed and self.control.brake == 0:\n            self.control.throttle = 0.0\n\n        self.control.hand_brake = False\n        self.control.reverse = False\n\n        self.game.send_control(self.control)\n\n    def _load_experiment(self, experiment_idx):\n        self.current_experiment = self.experiment_suite.get_experiments()[experiment_idx]\n        self.scene = self.game.load_settings(self.current_experiment.conditions)\n        self.positions = self.scene.player_start_spots\n        self.num_positions = len(self.positions)\n        self.current_start_position_idx = 0\n        self.current_pose = 0\n\n    def _restart_environment_episode(self, force_environment_reset=False):\n        # select start and end positions\n        if self.experiment_suite:\n            # if an expeirent suite is available, follow its given poses\n            if self.current_pose >= len(self.current_experiment.poses):\n                # load a new experiment\n                self.current_experiment_idx = (self.current_experiment_idx + 1) % len(self.experiment_suite.get_experiments())\n                self._load_experiment(self.current_experiment_idx)\n\n            self.current_start_position_idx = self.current_experiment.poses[self.current_pose][0]\n            self.current_goal = self.positions[self.current_experiment.poses[self.current_pose][1]]\n            self.current_pose += 1\n        else:\n            # go over all the possible positions in a cyclic manner\n            self.current_start_position_idx = (self.current_start_position_idx + 1) % self.num_positions\n\n            # choose a random goal destination\n            self.current_goal = random.choice(self.positions)\n\n        try:\n            self.game.start_episode(self.current_start_position_idx)\n        except:\n            self.game.connect()\n            self.game.start_episode(self.current_start_position_idx)\n\n        # start the game with some initial speed\n        for i in range(self.num_speedup_steps):\n            self.control = VehicleControl(throttle=1.0, brake=0, steer=0, hand_brake=False, reverse=False)\n            self.game.send_control(VehicleControl())\n\n    def get_rendered_image(self) -> np.ndarray:\n        """"""\n        Return a numpy array containing the image that will be rendered to the screen.\n        This can be different from the observation. For example, mujoco\'s observation is a measurements vector.\n        :return: numpy array containing the image that will be rendered to the screen\n        """"""\n        image = [self.state[camera.name] for camera in self.scene.sensors]\n        image = np.vstack(image)\n        return image\n\n    def get_target_success_rate(self) -> float:\n        return self.target_success_rate\n'"
rl_coach/environments/control_suite_environment.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\n\nimport random\nfrom enum import Enum\nfrom typing import Union\n\nimport numpy as np\n\ntry:\n    from dm_control import suite\n    from dm_control.suite.wrappers import pixels\nexcept ImportError:\n    from rl_coach.logger import failed_imports\n    failed_imports.append(""DeepMind Control Suite"")\n\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.environments.environment import Environment, EnvironmentParameters, LevelSelection\nfrom rl_coach.filters.filter import NoInputFilter, NoOutputFilter\nfrom rl_coach.spaces import BoxActionSpace, ImageObservationSpace, VectorObservationSpace, StateSpace\n\n\nclass ObservationType(Enum):\n    Measurements = 1\n    Image = 2\n    Image_and_Measurements = 3\n\n\n# Parameters\nclass ControlSuiteEnvironmentParameters(EnvironmentParameters):\n    def __init__(self, level=None):\n        super().__init__(level=level)\n        self.observation_type = ObservationType.Measurements\n        self.default_input_filter = ControlSuiteInputFilter\n        self.default_output_filter = ControlSuiteOutputFilter\n\n    @property\n    def path(self):\n        return \'rl_coach.environments.control_suite_environment:ControlSuiteEnvironment\'\n\n\n""""""\nControlSuite Environment Components\n""""""\nControlSuiteInputFilter = NoInputFilter()\nControlSuiteOutputFilter = NoOutputFilter()\n\ncontrol_suite_envs = {\':\'.join(env): \':\'.join(env) for env in suite.BENCHMARKING}\n\n\n# Environment\nclass ControlSuiteEnvironment(Environment):\n    def __init__(self, level: LevelSelection, frame_skip: int, visualization_parameters: VisualizationParameters,\n                 target_success_rate: float=1.0, seed: Union[None, int]=None, human_control: bool=False,\n                 observation_type: ObservationType=ObservationType.Measurements,\n                 custom_reward_threshold: Union[int, float]=None, **kwargs):\n        """"""\n        :param level: (str)\n            A string representing the control suite level to run. This can also be a LevelSelection object.\n            For example, cartpole:swingup.\n\n        :param frame_skip: (int)\n            The number of frames to skip between any two actions given by the agent. The action will be repeated\n            for all the skipped frames.\n\n        :param visualization_parameters: (VisualizationParameters)\n            The parameters used for visualizing the environment, such as the render flag, storing videos etc.\n\n        :param target_success_rate: (float)\n            Stop experiment if given target success rate was achieved.\n\n        :param seed: (int)\n            A seed to use for the random number generator when running the environment.\n\n        :param human_control: (bool)\n            A flag that allows controlling the environment using the keyboard keys.\n\n        :param observation_type: (ObservationType)\n            An enum which defines which observation to use. The current options are to use:\n            * Measurements only - a vector of joint torques and similar measurements\n            * Image only - an image of the environment as seen by a camera attached to the simulator\n            * Measurements & Image - both type of observations will be returned in the state using the keys\n            \'measurements\' and \'pixels\' respectively.\n\n        :param custom_reward_threshold: (float)\n            Allows defining a custom reward that will be used to decide when the agent succeeded in passing the environment.\n\n        """"""\n        super().__init__(level, seed, frame_skip, human_control, custom_reward_threshold, visualization_parameters, target_success_rate)\n\n        self.observation_type = observation_type\n\n        # load and initialize environment\n        domain_name, task_name = self.env_id.split("":"")\n        self.env = suite.load(domain_name=domain_name, task_name=task_name, task_kwargs={\'random\': seed})\n\n        if observation_type != ObservationType.Measurements:\n            self.env = pixels.Wrapper(self.env, pixels_only=observation_type == ObservationType.Image)\n\n        # seed\n        if self.seed is not None:\n            np.random.seed(self.seed)\n            random.seed(self.seed)\n\n        self.state_space = StateSpace({})\n\n        # image observations\n        if observation_type != ObservationType.Measurements:\n            self.state_space[\'pixels\'] = ImageObservationSpace(shape=self.env.observation_spec()[\'pixels\'].shape,\n                                                               high=255)\n\n        # measurements observations\n        if observation_type != ObservationType.Image:\n            measurements_space_size = 0\n            measurements_names = []\n            for observation_space_name, observation_space in self.env.observation_spec().items():\n                if len(observation_space.shape) == 0:\n                    measurements_space_size += 1\n                    measurements_names.append(observation_space_name)\n                elif len(observation_space.shape) == 1:\n                    measurements_space_size += observation_space.shape[0]\n                    measurements_names.extend([""{}_{}"".format(observation_space_name, i) for i in\n                                               range(observation_space.shape[0])])\n            self.state_space[\'measurements\'] = VectorObservationSpace(shape=measurements_space_size,\n                                                                      measurements_names=measurements_names)\n\n        # actions\n        self.action_space = BoxActionSpace(\n            shape=self.env.action_spec().shape[0],\n            low=self.env.action_spec().minimum,\n            high=self.env.action_spec().maximum\n        )\n\n        # initialize the state by getting a new state from the environment\n        self.reset_internal_state(True)\n\n        # render\n        if self.is_rendered:\n            image = self.get_rendered_image()\n            scale = 1\n            if self.human_control:\n                scale = 2\n            if not self.native_rendering:\n                self.renderer.create_screen(image.shape[1]*scale, image.shape[0]*scale)\n\n        self.target_success_rate = target_success_rate\n\n    def _update_state(self):\n        self.state = {}\n\n        if self.observation_type != ObservationType.Measurements:\n            self.pixels = self.last_result.observation[\'pixels\']\n            self.state[\'pixels\'] = self.pixels\n\n        if self.observation_type != ObservationType.Image:\n            self.measurements = np.array([])\n            for sub_observation in self.last_result.observation.values():\n                if isinstance(sub_observation, np.ndarray) and len(sub_observation.shape) == 1:\n                    self.measurements = np.concatenate((self.measurements, sub_observation))\n                else:\n                    self.measurements = np.concatenate((self.measurements, np.array([sub_observation])))\n            self.state[\'measurements\'] = self.measurements\n\n        self.reward = self.last_result.reward if self.last_result.reward is not None else 0\n\n        self.done = self.last_result.last()\n\n    def _take_action(self, action):\n        if type(self.action_space) == BoxActionSpace:\n            action = self.action_space.clip_action_to_space(action)\n\n        self.last_result = self.env.step(action)\n\n    def _restart_environment_episode(self, force_environment_reset=False):\n        self.last_result = self.env.reset()\n\n    def _render(self):\n        pass\n\n    def get_rendered_image(self):\n        return self.env.physics.render(camera_id=0)\n\n    def get_target_success_rate(self) -> float:\n        return self.target_success_rate'"
rl_coach/environments/doom_environment.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\ntry:\n    import vizdoom\nexcept ImportError:\n    from rl_coach.logger import failed_imports\n    failed_imports.append(""ViZDoom"")\n\nimport os\nfrom enum import Enum\nfrom os import path, environ\nfrom typing import Union, List\n\nimport numpy as np\n\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.environments.environment import Environment, EnvironmentParameters, LevelSelection\nfrom rl_coach.filters.action.full_discrete_action_space_map import FullDiscreteActionSpaceMap\nfrom rl_coach.filters.filter import InputFilter, OutputFilter\nfrom rl_coach.filters.observation.observation_rescale_to_size_filter import ObservationRescaleToSizeFilter\nfrom rl_coach.filters.observation.observation_rgb_to_y_filter import ObservationRGBToYFilter\nfrom rl_coach.filters.observation.observation_stacking_filter import ObservationStackingFilter\nfrom rl_coach.filters.observation.observation_to_uint8_filter import ObservationToUInt8Filter\nfrom rl_coach.spaces import MultiSelectActionSpace, ImageObservationSpace, \\\n    VectorObservationSpace, StateSpace\n\n\n# enum of the available levels and their path\nclass DoomLevel(Enum):\n    BASIC = ""basic.cfg""\n    DEFEND = ""defend_the_center.cfg""\n    DEATHMATCH = ""deathmatch.cfg""\n    MY_WAY_HOME = ""my_way_home.cfg""\n    TAKE_COVER = ""take_cover.cfg""\n    HEALTH_GATHERING = ""health_gathering.cfg""\n    HEALTH_GATHERING_SUPREME_COACH_LOCAL = ""D2_navigation.cfg""  # from https://github.com/IntelVCL/DirectFuturePrediction/tree/master/maps\n    DEFEND_THE_LINE = ""defend_the_line.cfg""\n    DEADLY_CORRIDOR = ""deadly_corridor.cfg""\n    BATTLE_COACH_LOCAL = ""D3_battle.cfg""  # from https://github.com/IntelVCL/DirectFuturePrediction/tree/master/maps\n\nkey_map = {\n    \'NO-OP\': 96,  # `\n    \'ATTACK\': 13,  # enter\n    \'CROUCH\': 306,  # ctrl\n    \'DROP_SELECTED_ITEM\': ord(""t""),\n    \'DROP_SELECTED_WEAPON\': ord(""t""),\n    \'JUMP\': 32,  # spacebar\n    \'LAND\': ord(""l""),\n    \'LOOK_DOWN\': 274,  # down arrow\n    \'LOOK_UP\': 273,  # up arrow\n    \'MOVE_BACKWARD\': ord(""s""),\n    \'MOVE_DOWN\': ord(""s""),\n    \'MOVE_FORWARD\': ord(""w""),\n    \'MOVE_LEFT\': 276,\n    \'MOVE_RIGHT\': 275,\n    \'MOVE_UP\': ord(""w""),\n    \'RELOAD\': ord(""r""),\n    \'SELECT_NEXT_WEAPON\': ord(""q""),\n    \'SELECT_PREV_WEAPON\': ord(""e""),\n    \'SELECT_WEAPON0\': ord(""0""),\n    \'SELECT_WEAPON1\': ord(""1""),\n    \'SELECT_WEAPON2\': ord(""2""),\n    \'SELECT_WEAPON3\': ord(""3""),\n    \'SELECT_WEAPON4\': ord(""4""),\n    \'SELECT_WEAPON5\': ord(""5""),\n    \'SELECT_WEAPON6\': ord(""6""),\n    \'SELECT_WEAPON7\': ord(""7""),\n    \'SELECT_WEAPON8\': ord(""8""),\n    \'SELECT_WEAPON9\': ord(""9""),\n    \'SPEED\': 304,  # shift\n    \'STRAFE\': 9,  # tab\n    \'TURN180\': ord(""u""),\n    \'TURN_LEFT\': ord(""a""),  # left arrow\n    \'TURN_RIGHT\': ord(""d""),  # right arrow\n    \'USE\': ord(""f""),\n}\n\n\nDoomInputFilter = InputFilter(is_a_reference_filter=True)\nDoomInputFilter.add_observation_filter(\'observation\', \'rescaling\',\n                                       ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([60, 76, 3]),\n                                                                                            high=255)))\nDoomInputFilter.add_observation_filter(\'observation\', \'to_grayscale\', ObservationRGBToYFilter())\nDoomInputFilter.add_observation_filter(\'observation\', \'to_uint8\', ObservationToUInt8Filter(0, 255))\nDoomInputFilter.add_observation_filter(\'observation\', \'stacking\', ObservationStackingFilter(3))\n\n\nDoomOutputFilter = OutputFilter(is_a_reference_filter=True)\nDoomOutputFilter.add_action_filter(\'to_discrete\', FullDiscreteActionSpaceMap())\n\n\nclass DoomEnvironmentParameters(EnvironmentParameters):\n    def __init__(self, level=None):\n        super().__init__(level=level)\n        self.default_input_filter = DoomInputFilter\n        self.default_output_filter = DoomOutputFilter\n        self.cameras = [DoomEnvironment.CameraTypes.OBSERVATION]\n\n    @property\n    def path(self):\n        return \'rl_coach.environments.doom_environment:DoomEnvironment\'\n\n\nclass DoomEnvironment(Environment):\n    class CameraTypes(Enum):\n        OBSERVATION = (""observation"", ""screen_buffer"")\n        DEPTH = (""depth"", ""depth_buffer"")\n        LABELS = (""labels"", ""labels_buffer"")\n        MAP = (""map"", ""automap_buffer"")\n\n    def __init__(self, level: LevelSelection, seed: int, frame_skip: int, human_control: bool,\n                 custom_reward_threshold: Union[int, float], visualization_parameters: VisualizationParameters,\n                 cameras: List[CameraTypes], target_success_rate: float=1.0, **kwargs):\n        """"""\n        :param level: (str)\n            A string representing the doom level to run. This can also be a LevelSelection object.\n            This should be one of the levels defined in the DoomLevel enum. For example, HEALTH_GATHERING.\n\n        :param seed: (int)\n            A seed to use for the random number generator when running the environment.\n\n        :param frame_skip: (int)\n            The number of frames to skip between any two actions given by the agent. The action will be repeated\n            for all the skipped frames.\n\n        :param human_control: (bool)\n            A flag that allows controlling the environment using the keyboard keys.\n\n        :param custom_reward_threshold: (float)\n            Allows defining a custom reward that will be used to decide when the agent succeeded in passing the environment.\n\n        :param visualization_parameters: (VisualizationParameters)\n            The parameters used for visualizing the environment, such as the render flag, storing videos etc.\n\n        :param cameras: (List[CameraTypes])\n            A list of camera types to use as observation in the state returned from the environment.\n            Each camera should be an enum from CameraTypes, and there are several options like an RGB observation,\n            a depth map, a segmentation map, and a top down map of the enviornment.\n\n\t\t:param target_success_rate: (float)\n\t\t\tStop experiment if given target success rate was achieved.\n\n        """"""\n        super().__init__(level, seed, frame_skip, human_control, custom_reward_threshold, visualization_parameters, target_success_rate)\n\n        self.cameras = cameras\n\n        # load the emulator with the required level\n        self.level = DoomLevel[level.upper()]\n        local_scenarios_path = path.join(os.path.dirname(os.path.realpath(__file__)), \'doom\')\n        if \'COACH_LOCAL\' in level:\n            self.scenarios_dir = local_scenarios_path\n        elif \'VIZDOOM_ROOT\' in environ:\n            self.scenarios_dir = path.join(environ.get(\'VIZDOOM_ROOT\'), \'scenarios\')\n        else:\n            self.scenarios_dir = path.join(os.path.dirname(os.path.realpath(vizdoom.__file__)), \'scenarios\')\n\n        self.game = vizdoom.DoomGame()\n        self.game.load_config(path.join(self.scenarios_dir, self.level.value))\n        self.game.set_window_visible(False)\n        self.game.add_game_args(""+vid_forcesurface 1"")\n\n        self.wait_for_explicit_human_action = True\n        if self.human_control:\n            self.game.set_screen_resolution(vizdoom.ScreenResolution.RES_640X480)\n        elif self.is_rendered:\n            self.game.set_screen_resolution(vizdoom.ScreenResolution.RES_320X240)\n        else:\n            # lower resolution since we actually take only 76x60 and we don\'t need to render\n            self.game.set_screen_resolution(vizdoom.ScreenResolution.RES_160X120)\n\n        self.game.set_render_hud(False)\n        self.game.set_render_crosshair(False)\n        self.game.set_render_decals(False)\n        self.game.set_render_particles(False)\n        for camera in self.cameras:\n            if hasattr(self.game, \'set_{}_enabled\'.format(camera.value[1])):\n                getattr(self.game, \'set_{}_enabled\'.format(camera.value[1]))(True)\n        self.game.init()\n\n        # actions\n        actions_description = [\'NO-OP\']\n        actions_description += [str(action).split(""."")[1] for action in self.game.get_available_buttons()]\n        actions_description = actions_description[::-1]\n        self.action_space = MultiSelectActionSpace(self.game.get_available_buttons_size(),\n                                                   max_simultaneous_selected_actions=1,\n                                                   descriptions=actions_description,\n                                                   allow_no_action_to_be_selected=True)\n\n        # human control\n        if self.human_control:\n            # TODO: add this to the action space\n            # map keyboard keys to actions\n            for idx, action in enumerate(self.action_space.descriptions):\n                if action in key_map.keys():\n                    self.key_to_action[(key_map[action],)] = idx\n\n        # states\n        self.state_space = StateSpace({\n            ""measurements"": VectorObservationSpace(self.game.get_state().game_variables.shape[0],\n                                                   measurements_names=[str(m) for m in\n                                                                       self.game.get_available_game_variables()])\n        })\n        for camera in self.cameras:\n            self.state_space[camera.value[0]] = ImageObservationSpace(\n                shape=np.array([self.game.get_screen_height(), self.game.get_screen_width(), 3]),\n                high=255)\n\n        # seed\n        if seed is not None:\n            self.game.set_seed(seed)\n        self.reset_internal_state()\n\n        # render\n        if self.is_rendered:\n            image = self.get_rendered_image()\n            self.renderer.create_screen(image.shape[1], image.shape[0])\n\n        self.target_success_rate = target_success_rate\n\n    def _update_state(self):\n        # extract all data from the current state\n        state = self.game.get_state()\n        if state is not None and state.screen_buffer is not None:\n            self.measurements = state.game_variables\n            self.state = {\'measurements\': self.measurements}\n            for camera in self.cameras:\n                observation = getattr(state, camera.value[1])\n                if len(observation.shape) == 3:\n                    self.state[camera.value[0]] = np.transpose(observation, (1, 2, 0))\n                elif len(observation.shape) == 2:\n                    self.state[camera.value[0]] = np.repeat(np.expand_dims(observation, -1), 3, axis=-1)\n\n        self.reward = self.game.get_last_reward()\n        self.done = self.game.is_episode_finished()\n\n    def _take_action(self, action):\n        self.game.make_action(list(action), self.frame_skip)\n\n    def _restart_environment_episode(self, force_environment_reset=False):\n        self.game.new_episode()\n\n    def get_rendered_image(self) -> np.ndarray:\n        """"""\n        Return a numpy array containing the image that will be rendered to the screen.\n        This can be different from the observation. For example, mujoco\'s observation is a measurements vector.\n        :return: numpy array containing the image that will be rendered to the screen\n        """"""\n        image = [self.state[camera.value[0]] for camera in self.cameras]\n        image = np.vstack(image)\n        return image\n\n    def get_target_success_rate(self) -> float:\n        return self.target_success_rate\n'"
rl_coach/environments/environment.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport operator\nimport time\nfrom collections import OrderedDict\nfrom typing import Union, List, Tuple, Dict\n\nimport numpy as np\n\nfrom rl_coach import logger\nfrom rl_coach.base_parameters import Parameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import GoalType, ActionType, EnvResponse, RunPhase\nfrom rl_coach.environments.environment_interface import EnvironmentInterface\nfrom rl_coach.logger import screen\nfrom rl_coach.renderer import Renderer\nfrom rl_coach.spaces import ActionSpace, ObservationSpace, DiscreteActionSpace, RewardSpace, StateSpace\nfrom rl_coach.utils import squeeze_list, force_list\n\n\nclass LevelSelection(object):\n    def __init__(self, level: str):\n        self.selected_level = level\n\n    def select(self, level: str):\n        self.selected_level = level\n\n    def __str__(self):\n        if self.selected_level is None:\n            logger.screen.error(""No level has been selected. Please select a level using the -lvl command line flag, ""\n                                ""or change the level in the preset."", crash=True)\n        return self.selected_level\n\n\nclass SingleLevelSelection(LevelSelection):\n    def __init__(self, levels: Union[str, List[str], Dict[str, str]]):\n        super().__init__(None)\n        self.levels = levels\n        if isinstance(levels, list):\n            self.levels = {level: level for level in levels}\n        if isinstance(levels, str):\n            self.levels = {levels: levels}\n\n    def __str__(self):\n        if self.selected_level is None:\n            logger.screen.error(""No level has been selected. Please select a level using the -lvl command line flag, ""\n                                ""or change the level in the preset. \\nThe available levels are: \\n{}""\n                                .format(\', \'.join(sorted(self.levels.keys()))), crash=True)\n        selected_level = self.selected_level.lower()\n        if selected_level not in self.levels.keys():\n            logger.screen.error(""The selected level ({}) is not part of the available levels ({})""\n                                .format(selected_level, \', \'.join(self.levels.keys())), crash=True)\n        return self.levels[selected_level]\n\n\n# class SingleLevelPerPhase(LevelSelection):\n#     def __init__(self, levels: Dict[RunPhase, str]):\n#         super().__init__(None)\n#         self.levels = levels\n#\n#     def __str__(self):\n#         super().__str__()\n#         if self.selected_level not in self.levels.keys():\n#             logger.screen.error(""The selected level ({}) is not part of the available levels ({})""\n#                                 .format(self.selected_level, self.levels.keys()), crash=True)\n#         return self.levels[self.selected_level]\n\n\nclass CustomWrapper(object):\n    def __init__(self, environment):\n        super().__init__()\n        self.environment = environment\n\n    def __getattr__(self, attr):\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        else:\n            return getattr(self.environment, attr, False)\n\n\nclass EnvironmentParameters(Parameters):\n    def __init__(self, level=None):\n        super().__init__()\n        self.level = level\n        self.frame_skip = 4\n        self.seed = None\n        self.human_control = False\n        self.custom_reward_threshold = None\n        self.default_input_filter = None\n        self.default_output_filter = None\n        self.experiment_path = None\n\n        # Set target reward and target_success if present\n        self.target_success_rate = 1.0\n\n    @property\n    def path(self):\n        return \'rl_coach.environments.environment:Environment\'\n\n\nclass Environment(EnvironmentInterface):\n    def __init__(self, level: LevelSelection, seed: int, frame_skip: int, human_control: bool,\n                 custom_reward_threshold: Union[int, float], visualization_parameters: VisualizationParameters,\n                 target_success_rate: float=1.0, **kwargs):\n        """"""\n        :param level: The environment level. Each environment can have multiple levels\n        :param seed: a seed for the random number generator of the environment\n        :param frame_skip: number of frames to skip (while repeating the same action) between each two agent directives\n        :param human_control: human should control the environment\n        :param visualization_parameters: a blob of parameters used for visualization of the environment\n        :param **kwargs: as the class is instantiated by EnvironmentParameters, this is used to support having\n                         additional arguments which will be ignored by this class, but might be used by others\n        """"""\n        super().__init__()\n\n        # env initialization\n\n        self.game = []\n\n        self.state = {}\n        self.observation = None\n        self.goal = None\n        self.reward = 0\n        self.done = False\n        self.info = {}\n        self._last_env_response = None\n        self.last_action = 0\n        self.episode_idx = 0\n        self.total_steps_counter = 0\n        self.current_episode_steps_counter = 0\n        self.last_episode_time = time.time()\n        self.key_to_action = {}\n        self.last_episode_images = []\n\n        # rewards\n        self.total_reward_in_current_episode = 0\n        self.max_reward_achieved = -np.inf\n        self.reward_success_threshold = custom_reward_threshold\n\n        # spaces\n        self.state_space = self._state_space = None\n        self.goal_space = self._goal_space = None\n        self.action_space = self._action_space = None\n        self.reward_space = RewardSpace(1, reward_success_threshold=self.reward_success_threshold)  # TODO: add a getter and setter\n\n        self.env_id = str(level)\n        self.seed = seed\n        self.frame_skip = frame_skip\n\n        # human interaction and visualization\n        self.human_control = human_control\n        self.wait_for_explicit_human_action = False\n        self.is_rendered = visualization_parameters.render or self.human_control\n        self.native_rendering = visualization_parameters.native_rendering and not self.human_control\n        self.visualization_parameters = visualization_parameters\n        if not self.native_rendering:\n            self.renderer = Renderer()\n\n        # Set target reward and target_success if present\n        self.target_success_rate = target_success_rate\n\n    @property\n    def action_space(self) -> Union[List[ActionSpace], ActionSpace]:\n        """"""\n        Get the action space of the environment\n\n        :return: the action space\n        """"""\n        return self._action_space\n\n    @action_space.setter\n    def action_space(self, val: Union[List[ActionSpace], ActionSpace]):\n        """"""\n        Set the action space of the environment\n\n        :return: None\n        """"""\n        self._action_space = val\n\n    @property\n    def state_space(self) -> Union[List[StateSpace], StateSpace]:\n        """"""\n        Get the state space of the environment\n\n        :return: the observation space\n        """"""\n        return self._state_space\n\n    @state_space.setter\n    def state_space(self, val: Union[List[StateSpace], StateSpace]):\n        """"""\n        Set the state space of the environment\n\n        :return: None\n        """"""\n        self._state_space = val\n\n    @property\n    def goal_space(self) -> Union[List[ObservationSpace], ObservationSpace]:\n        """"""\n        Get the state space of the environment\n\n        :return: the observation space\n        """"""\n        return self._goal_space\n\n    @goal_space.setter\n    def goal_space(self, val: Union[List[ObservationSpace], ObservationSpace]):\n        """"""\n        Set the goal space of the environment\n\n        :return: None\n        """"""\n        self._goal_space = val\n\n    def get_action_from_user(self) -> ActionType:\n        """"""\n        Get an action from the user keyboard\n\n        :return: action index\n        """"""\n        if self.wait_for_explicit_human_action:\n            while len(self.renderer.pressed_keys) == 0:\n                self.renderer.get_events()\n\n        if self.key_to_action == {}:\n            # the keys are the numbers on the keyboard corresponding to the action index\n            if len(self.renderer.pressed_keys) > 0:\n                action_idx = self.renderer.pressed_keys[0] - ord(""1"")\n                if 0 <= action_idx < self.action_space.shape[0]:\n                    return action_idx\n        else:\n            # the keys are mapped through the environment to more intuitive keyboard keys\n            # key = tuple(self.renderer.pressed_keys)\n            # for key in self.renderer.pressed_keys:\n            for env_keys in self.key_to_action.keys():\n                if set(env_keys) == set(self.renderer.pressed_keys):\n                    return self.action_space.actions[self.key_to_action[env_keys]]\n\n        # return the default action 0 so that the environment will continue running\n        return self.action_space.default_action\n\n    @property\n    def last_env_response(self) -> Union[List[EnvResponse], EnvResponse]:\n        """"""\n        Get the last environment response\n\n        :return: a dictionary that contains the state, reward, etc.\n        """"""\n        return squeeze_list(self._last_env_response)\n\n    @last_env_response.setter\n    def last_env_response(self, val: Union[List[EnvResponse], EnvResponse]):\n        """"""\n        Set the last environment response\n\n        :param val: the last environment response\n        """"""\n        self._last_env_response = force_list(val)\n\n    def step(self, action: ActionType) -> EnvResponse:\n        """"""\n        Make a single step in the environment using the given action\n\n        :param action: an action to use for stepping the environment. Should follow the definition of the action space.\n        :return: the environment response as returned in get_last_env_response\n        """"""\n        action = self.action_space.clip_action_to_space(action)\n        if self.action_space and not self.action_space.contains(action):\n            raise ValueError(""The given action does not match the action space definition. ""\n                             ""Action = {}, action space definition = {}"".format(action, self.action_space))\n\n        # store the last agent action done and allow passing None actions to repeat the previously done action\n        if action is None:\n            action = self.last_action\n        self.last_action = action\n        if self.visualization_parameters.add_rendered_image_to_env_response:\n            current_rendered_image = self.get_rendered_image()\n\n        self.current_episode_steps_counter += 1\n        if self.phase != RunPhase.UNDEFINED:\n            self.total_steps_counter += 1\n\n        # act\n        self._take_action(action)\n\n        # observe\n        self._update_state()\n\n        if self.is_rendered:\n            self.render()\n\n        self.total_reward_in_current_episode += self.reward\n\n        if self.visualization_parameters.add_rendered_image_to_env_response:\n            self.info[\'image\'] = current_rendered_image\n\n        self.last_env_response = \\\n            EnvResponse(\n                reward=self.reward,\n                next_state=self.state,\n                goal=self.goal,\n                game_over=self.done,\n                info=self.info\n            )\n\n        # store observations for video / gif dumping\n        if self.should_dump_video_of_the_current_episode(episode_terminated=False) and \\\n            (self.visualization_parameters.dump_mp4 or self.visualization_parameters.dump_gifs):\n            self.last_episode_images.append(self.get_rendered_image())\n\n        return self.last_env_response\n\n    def render(self) -> None:\n        """"""\n        Call the environment function for rendering to the screen\n\n        :return: None\n        """"""\n        if self.native_rendering:\n            self._render()\n        else:\n            self.renderer.render_image(self.get_rendered_image())\n\n    def handle_episode_ended(self) -> None:\n        """"""\n        End an episode\n\n        :return: None\n        """"""\n        self.dump_video_of_last_episode_if_needed()\n\n    def reset_internal_state(self, force_environment_reset=False) -> EnvResponse:\n        """"""\n        Reset the environment and all the variable of the wrapper\n\n        :param force_environment_reset: forces environment reset even when the game did not end\n        :return: A dictionary containing the observation, reward, done flag, action and measurements\n        """"""\n\n        self._restart_environment_episode(force_environment_reset)\n        self.last_episode_time = time.time()\n\n        if self.current_episode_steps_counter > 0 and self.phase != RunPhase.UNDEFINED:\n            self.episode_idx += 1\n\n        self.done = False\n        self.total_reward_in_current_episode = self.reward = 0.0\n        self.last_action = 0\n        self.current_episode_steps_counter = 0\n        self.last_episode_images = []\n        self._update_state()\n\n        # render before the preprocessing of the observation, so that the image will be in its original quality\n        if self.is_rendered:\n            self.render()\n\n        self.last_env_response = \\\n            EnvResponse(\n                reward=self.reward,\n                next_state=self.state,\n                goal=self.goal,\n                game_over=self.done,\n                info=self.info\n            )\n\n        return self.last_env_response\n\n    def get_random_action(self) -> ActionType:\n        """"""\n        Returns an action picked uniformly from the available actions\n\n        :return: a numpy array with a random action\n        """"""\n        return self.action_space.sample()\n\n    def get_available_keys(self) -> List[Tuple[str, ActionType]]:\n        """"""\n        Return a list of tuples mapping between action names and the keyboard key that triggers them\n\n        :return: a list of tuples mapping between action names and the keyboard key that triggers them\n        """"""\n        available_keys = []\n        if self.key_to_action != {}:\n            for key, idx in sorted(self.key_to_action.items(), key=operator.itemgetter(1)):\n                if key != ():\n                    key_names = [self.renderer.get_key_names([k])[0] for k in key]\n                    available_keys.append((self.action_space.descriptions[idx], \' + \'.join(key_names)))\n        elif type(self.action_space) == DiscreteActionSpace:\n            for action in range(self.action_space.shape):\n                available_keys.append((""Action {}"".format(action + 1), action + 1))\n        return available_keys\n\n    def get_goal(self) -> GoalType:\n        """"""\n        Get the current goal that the agents needs to achieve in the environment\n\n        :return: The goal\n        """"""\n        return self.goal\n\n    def set_goal(self, goal: GoalType) -> None:\n        """"""\n        Set the current goal that the agent needs to achieve in the environment\n\n        :param goal: the goal that needs to be achieved\n        :return: None\n        """"""\n        self.goal = goal\n\n    def should_dump_video_of_the_current_episode(self, episode_terminated=False):\n        if self.visualization_parameters.video_dump_filters:\n            for video_dump_filter in force_list(self.visualization_parameters.video_dump_filters):\n                if not video_dump_filter.should_dump(episode_terminated, **self.__dict__):\n                    return False\n            return True\n        return True\n\n    def dump_video_of_last_episode_if_needed(self):\n        if self.last_episode_images != [] and self.should_dump_video_of_the_current_episode(episode_terminated=True):\n            self.dump_video_of_last_episode()\n\n    def dump_video_of_last_episode(self):\n        frame_skipping = max(1, int(5 / self.frame_skip))\n        file_name = \'episode-{}_score-{}\'.format(self.episode_idx, self.total_reward_in_current_episode)\n        fps = 10\n        if self.visualization_parameters.dump_gifs:\n            logger.create_gif(self.last_episode_images[::frame_skipping], name=file_name, fps=fps)\n        if self.visualization_parameters.dump_mp4:\n            logger.create_mp4(self.last_episode_images[::frame_skipping], name=file_name, fps=fps)\n\n    # The following functions define the interaction with the environment.\n    # Any new environment that inherits the Environment class should use these signatures.\n    # Some of these functions are optional - please read their description for more details.\n\n    def _take_action(self, action_idx: ActionType) -> None:\n        """"""\n        An environment dependent function that sends an action to the simulator.\n\n        :param action_idx: the action to perform on the environment\n        :return: None\n        """"""\n        raise NotImplementedError("""")\n\n    def _update_state(self) -> None:\n        """"""\n        Updates the state from the environment.\n        Should update self.observation, self.reward, self.done, self.measurements and self.info\n\n        :return: None\n        """"""\n        raise NotImplementedError("""")\n\n    def _restart_environment_episode(self, force_environment_reset=False) -> None:\n        """"""\n        Restarts the simulator episode\n\n        :param force_environment_reset: Force the environment to reset even if the episode is not done yet.\n        :return: None\n        """"""\n        raise NotImplementedError("""")\n\n    def _render(self) -> None:\n        """"""\n        Renders the environment using the native simulator renderer\n\n        :return: None\n        """"""\n        pass\n\n    def get_rendered_image(self) -> np.ndarray:\n        """"""\n        Return a numpy array containing the image that will be rendered to the screen.\n        This can be different from the observation. For example, mujoco\'s observation is a measurements vector.\n\n        :return: numpy array containing the image that will be rendered to the screen\n        """"""\n        return np.transpose(self.state[\'observation\'], [1, 2, 0])\n\n    def get_target_success_rate(self) -> float:\n        return self.target_success_rate\n\n    def close(self) -> None:\n        """"""\n        Clean up steps.\n\n        :return: None\n        """"""\n        pass\n'"
rl_coach/environments/environment_interface.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union, Dict\n\nfrom rl_coach.core_types import ActionType, EnvResponse, RunPhase\nfrom rl_coach.spaces import ActionSpace\n\n\nclass EnvironmentInterface(object):\n    def __init__(self):\n        self._phase = RunPhase.UNDEFINED\n\n    @property\n    def phase(self) -> RunPhase:\n        """"""\n        Get the phase of the environment\n        :return: the current phase\n        """"""\n        return self._phase\n\n    @phase.setter\n    def phase(self, val: RunPhase):\n        """"""\n        Change the phase of the environment\n        :param val: the new phase\n        :return: None\n        """"""\n        self._phase = val\n\n    @property\n    def action_space(self) -> Union[Dict[str, ActionSpace], ActionSpace]:\n        """"""\n        Get the action space of the environment (or of each of the agents wrapped in this environment.\n        i.e. in the LevelManager case"")\n        :return: the action space\n        """"""\n        raise NotImplementedError("""")\n\n    def get_random_action(self) -> ActionType:\n        """"""\n        Get a random action from the environment action space\n        :return: An action that follows the definition of the action space.\n        """"""\n        raise NotImplementedError("""")\n\n    def step(self, action: ActionType) -> Union[None, EnvResponse]:\n        """"""\n        Make a single step in the environment using the given action\n        :param action: an action to use for stepping the environment. Should follow the definition of the action space.\n        :return: the environment response as returned in get_last_env_response or None for LevelManager\n        """"""\n        raise NotImplementedError("""")\n\n    def reset_internal_state(self, force_environment_reset: bool=False) -> Union[None, EnvResponse]:\n        """"""\n        Reset the environment episode\n        :param force_environment_reset: in some cases, resetting the environment can be suppressed by the environment\n                                        itself. This flag allows force the reset.\n        :return: the environment response as returned in get_last_env_response or None for LevelManager\n        """"""\n        raise NotImplementedError("""")\n'"
rl_coach/environments/gym_environment.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport gym\nimport numpy as np\nfrom enum import IntEnum\nimport scipy.ndimage\n\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.utils import lower_under_to_upper, short_dynamic_import\n\ntry:\n    import roboschool\n    from OpenGL import GL\nexcept ImportError:\n    from rl_coach.logger import failed_imports\n    failed_imports.append(""RoboSchool"")\n\ntry:\n    from gym_extensions.continuous import mujoco\nexcept:\n    from rl_coach.logger import failed_imports\n    failed_imports.append(""GymExtensions"")\n\ntry:\n    import pybullet_envs\nexcept ImportError:\n    from rl_coach.logger import failed_imports\n    failed_imports.append(""PyBullet"")\n\nfrom typing import Dict, Any, Union\nfrom rl_coach.core_types import RunPhase, EnvironmentSteps\nfrom rl_coach.environments.environment import Environment, EnvironmentParameters, LevelSelection\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace, ImageObservationSpace, VectorObservationSpace, \\\n    PlanarMapsObservationSpace, TensorObservationSpace, StateSpace, RewardSpace\nfrom rl_coach.filters.filter import NoInputFilter, NoOutputFilter\nfrom rl_coach.filters.reward.reward_clipping_filter import RewardClippingFilter\nfrom rl_coach.filters.observation.observation_rescale_to_size_filter import ObservationRescaleToSizeFilter\nfrom rl_coach.filters.observation.observation_stacking_filter import ObservationStackingFilter\nfrom rl_coach.filters.observation.observation_rgb_to_y_filter import ObservationRGBToYFilter\nfrom rl_coach.filters.observation.observation_to_uint8_filter import ObservationToUInt8Filter\nfrom rl_coach.filters.filter import InputFilter\nimport random\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.logger import screen\n\n\n# Parameters\nclass GymEnvironmentParameters(EnvironmentParameters):\n    def __init__(self, level=None):\n        super().__init__(level=level)\n        self.random_initialization_steps = 0\n        self.max_over_num_frames = 1\n        self.additional_simulator_parameters = {}\n        self.observation_space_type = None\n\n    @property\n    def path(self):\n        return \'rl_coach.environments.gym_environment:GymEnvironment\'\n\n\n# Generic parameters for vector environments such as mujoco, roboschool, bullet, etc.\nclass GymVectorEnvironment(GymEnvironmentParameters):\n    def __init__(self, level=None):\n        super().__init__(level=level)\n        self.frame_skip = 1\n        self.default_input_filter = NoInputFilter()\n        self.default_output_filter = NoOutputFilter()\n\n\n# Roboschool\ngym_roboschool_envs = [\'inverted_pendulum\', \'inverted_pendulum_swingup\', \'inverted_double_pendulum\', \'reacher\',\n                       \'hopper\', \'walker2d\', \'half_cheetah\', \'ant\', \'humanoid\', \'humanoid_flagrun\',\n                       \'humanoid_flagrun_harder\', \'pong\']\nroboschool_v1 = {e: ""Roboschool{}"".format(lower_under_to_upper(e) + \'-v1\') for e in gym_roboschool_envs}\n\n# Mujoco\ngym_mujoco_envs = [\'inverted_pendulum\', \'inverted_double_pendulum\', \'reacher\', \'hopper\', \'walker2d\', \'half_cheetah\',\n                   \'ant\', \'swimmer\', \'humanoid\', \'humanoid_standup\', \'pusher\', \'thrower\', \'striker\']\n\nmujoco_v2 = {e: ""{}"".format(lower_under_to_upper(e) + \'-v2\') for e in gym_mujoco_envs}\nmujoco_v2[\'walker2d\'] = \'Walker2d-v2\'\n\n# Fetch\ngym_fetch_envs = [\'reach\', \'slide\', \'push\', \'pick_and_place\']\nfetch_v1 = {e: ""{}"".format(\'Fetch\' + lower_under_to_upper(e) + \'-v1\') for e in gym_fetch_envs}\n\n\n""""""\nAtari Environment Components\n""""""\n\nAtariInputFilter = InputFilter(is_a_reference_filter=True)\nAtariInputFilter.add_reward_filter(\'clipping\', RewardClippingFilter(-1.0, 1.0))\nAtariInputFilter.add_observation_filter(\'observation\', \'rescaling\',\n                                        ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([84, 84, 3]),\n                                                                                             high=255)))\nAtariInputFilter.add_observation_filter(\'observation\', \'to_grayscale\', ObservationRGBToYFilter())\nAtariInputFilter.add_observation_filter(\'observation\', \'to_uint8\', ObservationToUInt8Filter(0, 255))\nAtariInputFilter.add_observation_filter(\'observation\', \'stacking\', ObservationStackingFilter(4))\nAtariOutputFilter = NoOutputFilter()\n\n\nclass Atari(GymEnvironmentParameters):\n    def __init__(self, level=None):\n        super().__init__(level=level)\n        self.frame_skip = 4\n        self.max_over_num_frames = 2\n        self.random_initialization_steps = 30\n        self.default_input_filter = AtariInputFilter\n        self.default_output_filter = AtariOutputFilter\n\n\ngym_atari_envs = [\'air_raid\', \'alien\', \'amidar\', \'assault\', \'asterix\', \'asteroids\', \'atlantis\',\n                  \'bank_heist\', \'battle_zone\', \'beam_rider\', \'berzerk\', \'bowling\', \'boxing\', \'breakout\', \'carnival\',\n                  \'centipede\', \'chopper_command\', \'crazy_climber\', \'demon_attack\', \'double_dunk\',\n                  \'elevator_action\', \'enduro\', \'fishing_derby\', \'freeway\', \'frostbite\', \'gopher\', \'gravitar\',\n                  \'hero\', \'ice_hockey\', \'jamesbond\', \'journey_escape\', \'kangaroo\', \'krull\', \'kung_fu_master\',\n                  \'montezuma_revenge\', \'ms_pacman\', \'name_this_game\', \'phoenix\', \'pitfall\', \'pong\', \'pooyan\',\n                  \'private_eye\', \'qbert\', \'riverraid\', \'road_runner\', \'robotank\', \'seaquest\', \'skiing\',\n                  \'solaris\', \'space_invaders\', \'star_gunner\', \'tennis\', \'time_pilot\', \'tutankham\', \'up_n_down\',\n                  \'venture\', \'video_pinball\', \'wizard_of_wor\', \'yars_revenge\', \'zaxxon\']\natari_deterministic_v4 = {e: ""{}"".format(lower_under_to_upper(e) + \'Deterministic-v4\') for e in gym_atari_envs}\natari_no_frameskip_v4 = {e: ""{}"".format(lower_under_to_upper(e) + \'NoFrameskip-v4\') for e in gym_atari_envs}\n\n\n# default atari schedule used in the DeepMind papers\natari_schedule = ScheduleParameters()\natari_schedule.improve_steps = EnvironmentSteps(50000000)\natari_schedule.steps_between_evaluation_periods = EnvironmentSteps(250000)\natari_schedule.evaluation_steps = EnvironmentSteps(135000)\natari_schedule.heatup_steps = EnvironmentSteps(50000)\n\n\nclass MaxOverFramesAndFrameskipEnvWrapper(gym.Wrapper):\n    def __init__(self, env, frameskip=4, max_over_num_frames=2):\n        super().__init__(env)\n        self.max_over_num_frames = max_over_num_frames\n        self.observations_stack = []\n        self.frameskip = frameskip\n        self.first_frame_to_max_over = self.frameskip - self.max_over_num_frames\n\n    def reset(self):\n        return self.env.reset()\n\n    def step(self, action):\n        total_reward = 0.0\n        done = None\n        info = None\n        self.observations_stack = []\n        for i in range(self.frameskip):\n            observation, reward, done, info = self.env.step(action)\n            if i >= self.first_frame_to_max_over:\n                self.observations_stack.append(observation)\n            total_reward += reward\n            if done:\n                # deal with last state in episode\n                if not self.observations_stack:\n                    self.observations_stack.append(observation)\n                break\n\n        max_over_frames_observation = np.max(self.observations_stack, axis=0)\n\n        return max_over_frames_observation, total_reward, done, info\n\n\n# Environment\nclass ObservationSpaceType(IntEnum):\n    Tensor = 0\n    Image = 1\n    Vector = 2\n\n\nclass GymEnvironment(Environment):\n    def __init__(self,\n                 level: LevelSelection,\n                 frame_skip: int,\n                 visualization_parameters: VisualizationParameters,\n                 target_success_rate: float=1.0,\n                 additional_simulator_parameters: Dict[str, Any] = {},\n                 seed: Union[None, int] = None,\n                 human_control: bool=False,\n                 custom_reward_threshold: Union[int, float]=None,\n                 random_initialization_steps: int=1,\n                 max_over_num_frames: int=1,\n                 observation_space_type: ObservationSpaceType=None,\n                 **kwargs):\n        """"""\n        :param level: (str)\n            A string representing the gym level to run. This can also be a LevelSelection object.\n            For example, BreakoutDeterministic-v0\n\n        :param frame_skip: (int)\n            The number of frames to skip between any two actions given by the agent. The action will be repeated\n            for all the skipped frames.\n\n        :param visualization_parameters: (VisualizationParameters)\n            The parameters used for visualizing the environment, such as the render flag, storing videos etc.\n\n        :param additional_simulator_parameters: (Dict[str, Any])\n            Any additional parameters that the user can pass to the Gym environment. These parameters should be\n            accepted by the __init__ function of the implemented Gym environment.\n\n        :param seed: (int)\n            A seed to use for the random number generator when running the environment.\n\n        :param human_control: (bool)\n            A flag that allows controlling the environment using the keyboard keys.\n\n        :param custom_reward_threshold: (float)\n            Allows defining a custom reward that will be used to decide when the agent succeeded in passing the environment.\n            If not set, this value will be taken from the Gym environment definition.\n\n        :param random_initialization_steps: (int)\n            The number of random steps that will be taken in the environment after each reset.\n            This is a feature presented in the DQN paper, which improves the variability of the episodes the agent sees.\n\n        :param max_over_num_frames: (int)\n            This value will be used for merging multiple frames into a single frame by taking the maximum value for each\n            of the pixels in the frame. This is particularly used in Atari games, where the frames flicker, and objects\n            can be seen in one frame but disappear in the next.\n\n        :param observation_space_type:\n            This value will be used for generating observation space. Allows a custom space. Should be one of\n            ObservationSpaceType. If not specified, observation space is inferred from the number of dimensions\n            of the observation: 1D: Vector space, 3D: Image space if 1 or 3 channels, PlanarMaps space otherwise.\n        """"""\n        super().__init__(level, seed, frame_skip, human_control, custom_reward_threshold,\n                         visualization_parameters, target_success_rate)\n\n        self.random_initialization_steps = random_initialization_steps\n        self.max_over_num_frames = max_over_num_frames\n        self.additional_simulator_parameters = additional_simulator_parameters\n\n        # hide warnings\n        gym.logger.set_level(40)\n\n        """"""\n        load and initialize environment\n        environment ids can be defined in 3 ways:\n        1. Native gym environments like BreakoutDeterministic-v0 for example\n        2. Custom gym environments written and installed as python packages.\n           This environments should have a python module with a class inheriting gym.Env, implementing the\n           relevant functions (_reset, _step, _render) and defining the observation and action space\n           For example: my_environment_package:MyEnvironmentClass will run an environment defined in the\n           MyEnvironmentClass class\n        3. Custom gym environments written as an independent module which is not installed.\n           This environments should have a python module with a class inheriting gym.Env, implementing the\n           relevant functions (_reset, _step, _render) and defining the observation and action space.\n           For example: path_to_my_environment.sub_directory.my_module:MyEnvironmentClass will run an\n           environment defined in the MyEnvironmentClass class which is located in the module in the relative path\n           path_to_my_environment.sub_directory.my_module\n        """"""\n        if \':\' in self.env_id:\n            # custom environments\n            if \'/\' in self.env_id or \'.\' in self.env_id:\n                # environment in a an absolute path module written as a unix path or in a relative path module\n                # written as a python import path\n                env_class = short_dynamic_import(self.env_id)\n            else:\n                # environment in a python package\n                env_class = gym.envs.registration.load(self.env_id)\n\n            # instantiate the environment\n            try:\n                self.env = env_class(**self.additional_simulator_parameters)\n            except:\n                screen.error(""Failed to instantiate Gym environment class %s with arguments %s"" %\n                             (env_class, self.additional_simulator_parameters), crash=False)\n                raise\n        else:\n            self.env = gym.make(self.env_id)\n\n        # for classic control we want to use the native renderer because otherwise we will get 2 renderer windows\n        environment_to_always_use_with_native_rendering = [\'classic_control\', \'mujoco\', \'robotics\']\n        self.native_rendering = self.native_rendering or \\\n                                any([env in str(self.env.unwrapped.__class__)\n                                     for env in environment_to_always_use_with_native_rendering])\n        if self.native_rendering:\n            if hasattr(self, \'renderer\'):\n                self.renderer.close()\n\n        # seed\n        if self.seed is not None:\n            self.env.seed(self.seed)\n            np.random.seed(self.seed)\n            random.seed(self.seed)\n\n        # frame skip and max between consecutive frames\n        self.is_mujoco_env = \'mujoco\' in str(self.env.unwrapped.__class__)\n        self.is_roboschool_env = \'roboschool\' in str(self.env.unwrapped.__class__)\n        self.is_atari_env = \'Atari\' in str(self.env.unwrapped.__class__)\n        if self.is_atari_env:\n            self.env.unwrapped.frameskip = 1  # this accesses the atari env that is wrapped with a timelimit wrapper env\n            if self.env_id == ""SpaceInvadersDeterministic-v4"" and self.frame_skip == 4:\n                screen.warning(""Warning: The frame-skip for Space Invaders was automatically updated from 4 to 3. ""\n                               ""This is following the DQN paper where it was noticed that a frame-skip of 3 makes the ""\n                               ""laser rays disappear. To force frame-skip of 4, please use SpaceInvadersNoFrameskip-v4."")\n                self.frame_skip = 3\n            self.env = MaxOverFramesAndFrameskipEnvWrapper(self.env,\n                                                           frameskip=self.frame_skip,\n                                                           max_over_num_frames=self.max_over_num_frames)\n        else:\n            self.env.unwrapped.frameskip = self.frame_skip\n\n        self.state_space = StateSpace({})\n\n        # observations\n        if not isinstance(self.env.observation_space, gym.spaces.dict.Dict):\n            state_space = {\'observation\': self.env.observation_space}\n        else:\n            state_space = self.env.observation_space.spaces\n\n        for observation_space_name, observation_space in state_space.items():\n            if observation_space_type == ObservationSpaceType.Tensor:\n                # we consider arbitrary input tensor which does not necessarily represent images\n                self.state_space[observation_space_name] = TensorObservationSpace(\n                    shape=np.array(observation_space.shape),\n                    low=observation_space.low,\n                    high=observation_space.high\n                )\n            elif observation_space_type == ObservationSpaceType.Image or len(observation_space.shape) == 3:\n                # we assume gym has image observations (with arbitrary number of channels) where their values are\n                # within 0-255, and where the channel dimension is the last dimension\n                if observation_space.shape[-1] in [1, 3]:\n                    self.state_space[observation_space_name] = ImageObservationSpace(\n                        shape=np.array(observation_space.shape),\n                        high=255,\n                        channels_axis=-1\n                    )\n                else:\n                    # For any number of channels other than 1 or 3, use the generic PlanarMaps space\n                    self.state_space[observation_space_name] = PlanarMapsObservationSpace(\n                        shape=np.array(observation_space.shape),\n                        low=0,\n                        high=255,\n                        channels_axis=-1\n                    )\n            elif observation_space_type == ObservationSpaceType.Vector or len(observation_space.shape) == 1:\n                self.state_space[observation_space_name] = VectorObservationSpace(\n                    shape=observation_space.shape[0],\n                    low=observation_space.low,\n                    high=observation_space.high\n                )\n            else:\n                raise screen.error(""Failed to instantiate Gym environment class %s with observation space type %s"" %\n                                 (env_class, observation_space_type), crash=True)\n\n        if \'desired_goal\' in state_space.keys():\n            self.goal_space = self.state_space[\'desired_goal\']\n\n        # actions\n        if type(self.env.action_space) == gym.spaces.box.Box:\n            self.action_space = BoxActionSpace(\n                shape=self.env.action_space.shape,\n                low=self.env.action_space.low,\n                high=self.env.action_space.high\n            )\n        elif type(self.env.action_space) == gym.spaces.discrete.Discrete:\n            actions_description = []\n            if hasattr(self.env.unwrapped, \'get_action_meanings\'):\n                actions_description = self.env.unwrapped.get_action_meanings()\n            self.action_space = DiscreteActionSpace(\n                num_actions=self.env.action_space.n,\n                descriptions=actions_description\n            )\n        else:\n            raise screen.error((\n                ""Failed to instantiate gym environment class {} due to unsupported ""\n                ""action space {}. Expected BoxActionSpace or DiscreteActionSpace.""\n            ).format(env_class, self.env.action_space), crash=True)\n\n        if self.human_control:\n            # TODO: add this to the action space\n            # map keyboard keys to actions\n            self.key_to_action = {}\n            if hasattr(self.env.unwrapped, \'get_keys_to_action\'):\n                self.key_to_action = self.env.unwrapped.get_keys_to_action()\n            else:\n                screen.error(""Error: Environment {} does not support human control."".format(self.env), crash=True)\n\n        # render\n        if self.is_rendered:\n            image = self.get_rendered_image()\n            scale = 1\n            if self.human_control:\n                scale = 2\n            if not self.native_rendering:\n                self.renderer.create_screen(image.shape[1]*scale, image.shape[0]*scale)\n\n        # the info is only updated after the first step\n        self.state_space[\'measurements\'] = VectorObservationSpace(shape=len(self.info.keys()))\n\n        if self.env.spec and custom_reward_threshold is None:\n                self.reward_success_threshold = self.env.spec.reward_threshold\n                self.reward_space = RewardSpace(1, reward_success_threshold=self.reward_success_threshold)\n\n        self.target_success_rate = target_success_rate\n\n    def _wrap_state(self, state):\n        if not isinstance(self.env.observation_space, gym.spaces.Dict):\n            return {\'observation\': state}\n        return state\n\n    def _update_state(self):\n        if self.is_atari_env and hasattr(self, \'current_ale_lives\') \\\n                and self.current_ale_lives != self.env.unwrapped.ale.lives():\n            if self.phase == RunPhase.TRAIN or self.phase == RunPhase.HEATUP:\n                # signal termination for life loss\n                self.done = True\n            elif self.phase == RunPhase.TEST and not self.done:\n                # the episode is not terminated in evaluation, but we need to press fire again\n                self._press_fire()\n            self._update_ale_lives()\n        # TODO: update the measurements\n        if self.state and ""desired_goal"" in self.state.keys():\n            self.goal = self.state[\'desired_goal\']\n\n    def _take_action(self, action):\n        if type(self.action_space) == BoxActionSpace:\n            action = self.action_space.clip_action_to_space(action)\n\n        self.state, self.reward, self.done, self.info = self.env.step(action)\n        self.state = self._wrap_state(self.state)\n\n    def _random_noop(self):\n        # simulate a random initial environment state by stepping for a random number of times between 0 and 30\n        step_count = 0\n        random_initialization_steps = random.randint(0, self.random_initialization_steps)\n        while self.action_space is not None and (self.state is None or step_count < random_initialization_steps):\n            step_count += 1\n            self.step(self.action_space.default_action)\n\n    def _press_fire(self):\n        fire_action = 1\n        if self.is_atari_env and self.env.unwrapped.get_action_meanings()[fire_action] == \'FIRE\':\n            self.current_ale_lives = self.env.unwrapped.ale.lives()\n            self.step(fire_action)\n            if self.done:\n                self.reset_internal_state()\n\n    def _update_ale_lives(self):\n        if self.is_atari_env:\n            self.current_ale_lives = self.env.unwrapped.ale.lives()\n\n    def _restart_environment_episode(self, force_environment_reset=False):\n        # prevent reset of environment if there are ale lives left\n        if (self.is_atari_env and self.env.unwrapped.ale.lives() > 0) \\\n                and not force_environment_reset and self.env.env._elapsed_steps < self.env.env._max_episode_steps:\n            self.step(self.action_space.default_action)\n        else:\n            self.state = self.env.reset()\n            self.state = self._wrap_state(self.state)\n            self._update_ale_lives()\n\n        if self.is_atari_env:\n            self._random_noop()\n            self._press_fire()\n\n        # initialize the number of lives\n        self._update_ale_lives()\n\n    def _render(self):\n        self.env.render(mode=\'human\')\n\n    def get_rendered_image(self):\n        image = self.env.render(mode=\'rgb_array\')\n        return image\n\n    def get_target_success_rate(self) -> float:\n        return self.target_success_rate\n\n    def close(self) -> None:\n        """"""\n        Clean up to close rendering windows.\n\n        :return: None\n        """"""\n        self.env.close()\n'"
rl_coach/environments/starcraft2_environment.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom enum import Enum\nfrom typing import Union, List\n\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_move_axis_filter import ObservationMoveAxisFilter\n\ntry:\n    from pysc2 import maps\n    from pysc2.env import sc2_env\n    from pysc2.env import available_actions_printer\n    from pysc2.lib import actions\n    from pysc2.lib import features\n    from pysc2.env import environment\n    from absl import app\n    from absl import flags\nexcept ImportError:\n    from rl_coach.logger import failed_imports\n    failed_imports.append(""PySc2"")\n\nfrom rl_coach.environments.environment import Environment, EnvironmentParameters, LevelSelection\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.spaces import BoxActionSpace, VectorObservationSpace, PlanarMapsObservationSpace, StateSpace, CompoundActionSpace, \\\n    DiscreteActionSpace\nfrom rl_coach.filters.filter import InputFilter, OutputFilter\nfrom rl_coach.filters.observation.observation_rescale_to_size_filter import ObservationRescaleToSizeFilter\nfrom rl_coach.filters.action.linear_box_to_box_map import LinearBoxToBoxMap\nfrom rl_coach.filters.observation.observation_to_uint8_filter import ObservationToUInt8Filter\n\nFLAGS = flags.FLAGS\nFLAGS([\'coach.py\'])\n\nSCREEN_SIZE = 84  # will also impact the action space size\n\n# Starcraft Constants\n_NOOP = actions.FUNCTIONS.no_op.id\n_MOVE_SCREEN = actions.FUNCTIONS.Move_screen.id\n_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n_NOT_QUEUED = [0]\n_SELECT_ALL = [0]\n\n\nclass StarcraftObservationType(Enum):\n    Features = 0\n    RGB = 1\n\n\nStarcraftInputFilter = InputFilter(is_a_reference_filter=True)\nStarcraftInputFilter.add_observation_filter(\'screen\', \'move_axis\', ObservationMoveAxisFilter(0, -1))\nStarcraftInputFilter.add_observation_filter(\'screen\', \'rescaling\',\n                                            ObservationRescaleToSizeFilter(\n                                                PlanarMapsObservationSpace(np.array([84, 84, 1]),\n                                                                           low=0, high=255, channels_axis=-1)))\nStarcraftInputFilter.add_observation_filter(\'screen\', \'to_uint8\', ObservationToUInt8Filter(0, 255))\n\nStarcraftInputFilter.add_observation_filter(\'minimap\', \'move_axis\', ObservationMoveAxisFilter(0, -1))\nStarcraftInputFilter.add_observation_filter(\'minimap\', \'rescaling\',\n                                            ObservationRescaleToSizeFilter(\n                                                PlanarMapsObservationSpace(np.array([64, 64, 1]),\n                                                                           low=0, high=255, channels_axis=-1)))\nStarcraftInputFilter.add_observation_filter(\'minimap\', \'to_uint8\', ObservationToUInt8Filter(0, 255))\n\n\nStarcraftNormalizingOutputFilter = OutputFilter(is_a_reference_filter=True)\nStarcraftNormalizingOutputFilter.add_action_filter(\n    \'normalization\', LinearBoxToBoxMap(input_space_low=-SCREEN_SIZE / 2, input_space_high=SCREEN_SIZE / 2 - 1))\n\n\nclass StarCraft2EnvironmentParameters(EnvironmentParameters):\n    def __init__(self, level=None):\n        super().__init__(level=level)\n        self.screen_size = 84\n        self.minimap_size = 64\n        self.feature_minimap_maps_to_use = range(7)\n        self.feature_screen_maps_to_use = range(17)\n        self.observation_type = StarcraftObservationType.Features\n        self.disable_fog = False\n        self.auto_select_all_army = True\n        self.default_input_filter = StarcraftInputFilter\n        self.default_output_filter = StarcraftNormalizingOutputFilter\n        self.use_full_action_space = False\n\n\n    @property\n    def path(self):\n        return \'rl_coach.environments.starcraft2_environment:StarCraft2Environment\'\n\n\n# Environment\nclass StarCraft2Environment(Environment):\n    def __init__(self, level: LevelSelection, frame_skip: int, visualization_parameters: VisualizationParameters,\n                 target_success_rate: float=1.0, seed: Union[None, int]=None, human_control: bool=False,\n                 custom_reward_threshold: Union[int, float]=None,\n                 screen_size: int=84, minimap_size: int=64,\n                 feature_minimap_maps_to_use: List=range(7), feature_screen_maps_to_use: List=range(17),\n                 observation_type: StarcraftObservationType=StarcraftObservationType.Features,\n                 disable_fog: bool=False, auto_select_all_army: bool=True,\n                 use_full_action_space: bool=False, **kwargs):\n        super().__init__(level, seed, frame_skip, human_control, custom_reward_threshold, visualization_parameters, target_success_rate)\n\n        self.screen_size = screen_size\n        self.minimap_size = minimap_size\n        self.feature_minimap_maps_to_use = feature_minimap_maps_to_use\n        self.feature_screen_maps_to_use = feature_screen_maps_to_use\n        self.observation_type = observation_type\n        self.features_screen_size = None\n        self.feature_minimap_size = None\n        self.rgb_screen_size = None\n        self.rgb_minimap_size = None\n        if self.observation_type == StarcraftObservationType.Features:\n            self.features_screen_size = screen_size\n            self.feature_minimap_size = minimap_size\n        elif self.observation_type == StarcraftObservationType.RGB:\n            self.rgb_screen_size = screen_size\n            self.rgb_minimap_size = minimap_size\n        self.disable_fog = disable_fog\n        self.auto_select_all_army = auto_select_all_army\n        self.use_full_action_space = use_full_action_space\n\n        # step_mul is the equivalent to frame skipping. Not sure if it repeats actions in between or not though.\n        self.env = sc2_env.SC2Env(map_name=self.env_id, step_mul=frame_skip,\n                                  visualize=self.is_rendered,\n                                  agent_interface_format=sc2_env.AgentInterfaceFormat(\n                                      feature_dimensions=sc2_env.Dimensions(\n                                          screen=self.features_screen_size,\n                                          minimap=self.feature_minimap_size\n                                      )\n                                      # rgb_dimensions=sc2_env.Dimensions(\n                                      #     screen=self.rgb_screen_size,\n                                      #     minimap=self.rgb_screen_size\n                                      # )\n                                  ),\n                                  # feature_screen_size=self.features_screen_size,\n                                  # feature_minimap_size=self.feature_minimap_size,\n                                  # rgb_screen_size=self.rgb_screen_size,\n                                  # rgb_minimap_size=self.rgb_screen_size,\n                                  disable_fog=disable_fog,\n                                  random_seed=self.seed\n                                  )\n\n        # print all the available actions\n        # self.env = available_actions_printer.AvailableActionsPrinter(self.env)\n\n        self.reset_internal_state(True)\n\n        """"""\n        feature_screen:  [height_map, visibility_map, creep, power, player_id, player_relative, unit_type, selected,\n                          unit_hit_points, unit_hit_points_ratio, unit_energy, unit_energy_ratio, unit_shields,\n                          unit_shields_ratio, unit_density, unit_density_aa, effects]\n        feature_minimap: [height_map, visibility_map, creep, camera, player_id, player_relative, selecte\n        d]\n        player:          [player_id, minerals, vespene, food_cap, food_army, food_workers, idle_worker_dount,\n                          army_count, warp_gate_count, larva_count]\n        """"""\n        self.screen_shape = np.array(self.env.observation_spec()[0][\'feature_screen\'])\n        self.screen_shape[0] = len(self.feature_screen_maps_to_use)\n        self.minimap_shape = np.array(self.env.observation_spec()[0][\'feature_minimap\'])\n        self.minimap_shape[0] = len(self.feature_minimap_maps_to_use)\n        self.state_space = StateSpace({\n            ""screen"": PlanarMapsObservationSpace(shape=self.screen_shape, low=0, high=255, channels_axis=0),\n            ""minimap"": PlanarMapsObservationSpace(shape=self.minimap_shape, low=0, high=255, channels_axis=0),\n            ""measurements"": VectorObservationSpace(self.env.observation_spec()[0][""player""][0])\n        })\n        if self.use_full_action_space:\n            action_identifiers = list(self.env.action_spec()[0].functions)\n            num_action_identifiers = len(action_identifiers)\n            action_arguments = [(arg.name, arg.sizes) for arg in self.env.action_spec()[0].types]\n            sub_action_spaces = [DiscreteActionSpace(num_action_identifiers)]\n            for argument in action_arguments:\n                for dimension in argument[1]:\n                    sub_action_spaces.append(DiscreteActionSpace(dimension))\n            self.action_space = CompoundActionSpace(sub_action_spaces)\n        else:\n            self.action_space = BoxActionSpace(2, 0, self.screen_size - 1, [""X-Axis, Y-Axis""],\n                                               default_action=np.array([self.screen_size/2, self.screen_size/2]))\n\n        self.target_success_rate = target_success_rate\n\n    def _update_state(self):\n        timestep = 0\n        self.screen = self.last_result[timestep].observation.feature_screen\n        # extract only the requested segmentation maps from the observation\n        self.screen = np.take(self.screen, self.feature_screen_maps_to_use, axis=0)\n        self.minimap = self.last_result[timestep].observation.feature_minimap\n        self.measurements = self.last_result[timestep].observation.player\n        self.reward = self.last_result[timestep].reward\n        self.done = self.last_result[timestep].step_type == environment.StepType.LAST\n        self.state = {\n            \'screen\': self.screen,\n            \'minimap\': self.minimap,\n            \'measurements\': self.measurements\n        }\n\n    def _take_action(self, action):\n        if self.use_full_action_space:\n            action_identifier = action[0]\n            action_arguments = action[1:]\n            action = actions.FunctionCall(action_identifier, action_arguments)\n        else:\n            coord = np.array(action[0:2])\n            noop = False\n            coord = coord.round()\n            coord = np.clip(coord, 0, SCREEN_SIZE - 1)\n            self.last_action_idx = coord\n\n            if noop:\n                action = actions.FunctionCall(_NOOP, [])\n            else:\n                action = actions.FunctionCall(_MOVE_SCREEN, [_NOT_QUEUED, coord])\n\n        self.last_result = self.env.step(actions=[action])\n\n    def _restart_environment_episode(self, force_environment_reset=False):\n        # reset the environment\n        self.last_result = self.env.reset()\n\n        # select all the units on the screen\n        if self.auto_select_all_army:\n            self.env.step(actions=[actions.FunctionCall(_SELECT_ARMY, [_SELECT_ALL])])\n\n    def get_rendered_image(self):\n        screen = np.squeeze(np.tile(np.expand_dims(self.screen, -1), (1, 1, 3)))\n        screen = screen / np.max(screen) * 255\n        return screen.astype(\'uint8\')\n\n    def dump_video_of_last_episode(self):\n        from rl_coach.logger import experiment_path\n        self.env._run_config.replay_dir = experiment_path\n        self.env.save_replay(\'replays\')\n        super().dump_video_of_last_episode()\n\n    def get_target_success_rate(self):\n        return self.target_success_rate\n'"
rl_coach/exploration_policies/__init__.py,0,b''
rl_coach/exploration_policies/additive_noise.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import RunPhase, ActionType\nfrom rl_coach.exploration_policies.exploration_policy import ContinuousActionExplorationPolicy, ExplorationParameters\nfrom rl_coach.schedules import Schedule, LinearSchedule\nfrom rl_coach.spaces import ActionSpace, BoxActionSpace\n\n\n# TODO: consider renaming to gaussian sampling\n\n\nclass AdditiveNoiseParameters(ExplorationParameters):\n    def __init__(self):\n        super().__init__()\n        self.noise_schedule = LinearSchedule(0.1, 0.1, 50000)\n        self.evaluation_noise = 0.05\n        self.noise_as_percentage_from_action_space = True\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.additive_noise:AdditiveNoise\'\n\n\nclass AdditiveNoise(ContinuousActionExplorationPolicy):\n    """"""\n    AdditiveNoise is an exploration policy intended for continuous action spaces. It takes the action from the agent\n    and adds a Gaussian distributed noise to it. The amount of noise added to the action follows the noise amount that\n    can be given in two different ways:\n    1. Specified by the user as a noise schedule which is taken in percentiles out of the action space size\n    2. Specified by the agents action. In case the agents action is a list with 2 values, the 1st one is assumed to\n    be the mean of the action, and 2nd is assumed to be its standard deviation.\n    """"""\n    def __init__(self, action_space: ActionSpace, noise_schedule: Schedule,\n                 evaluation_noise: float, noise_as_percentage_from_action_space: bool = True):\n        """"""\n        :param action_space: the action space used by the environment\n        :param noise_schedule: the schedule for the noise\n        :param evaluation_noise: the noise variance that will be used during evaluation phases\n        :param noise_as_percentage_from_action_space: a bool deciding whether the noise is absolute or as a percentage\n                                                      from the action space\n        """"""\n        super().__init__(action_space)\n        self.noise_schedule = noise_schedule\n        self.evaluation_noise = evaluation_noise\n        self.noise_as_percentage_from_action_space = noise_as_percentage_from_action_space\n\n        if not isinstance(action_space, BoxActionSpace) and \\\n                (hasattr(action_space, \'filtered_action_space\') and not\n                 isinstance(action_space.filtered_action_space, BoxActionSpace)):\n            raise ValueError(""Additive noise exploration works only for continuous controls.""\n                             ""The given action space is of type: {}"".format(action_space.__class__.__name__))\n\n        if not np.all(-np.inf < action_space.high) or not np.all(action_space.high < np.inf)\\\n                or not np.all(-np.inf < action_space.low) or not np.all(action_space.low < np.inf):\n            raise ValueError(""Additive noise exploration requires bounded actions"")\n\n    def get_action(self, action_values: List[ActionType]) -> ActionType:\n        # TODO-potential-bug consider separating internally defined stdev and externally defined stdev into 2 policies\n\n        # set the current noise\n        if self.phase == RunPhase.TEST:\n            current_noise = self.evaluation_noise\n        else:\n            current_noise = self.noise_schedule.current_value\n\n        # scale the noise to the action space range\n        if self.noise_as_percentage_from_action_space:\n            action_values_std = current_noise * (self.action_space.high - self.action_space.low)\n        else:\n            action_values_std = current_noise\n\n        # extract the mean values\n        if isinstance(action_values, list):\n            # the action values are expected to be a list with the action mean and optionally the action stdev\n            action_values_mean = action_values[0].squeeze()\n        else:\n            # the action values are expected to be a numpy array representing the action mean\n            action_values_mean = action_values.squeeze()\n\n        # step the noise schedule\n        if self.phase is not RunPhase.TEST:\n            self.noise_schedule.step()\n            # the second element of the list is assumed to be the standard deviation\n            if isinstance(action_values, list) and len(action_values) > 1:\n                action_values_std = action_values[1].squeeze()\n\n        # add noise to the action means\n        if self.phase is not RunPhase.TEST:\n            action = np.random.normal(action_values_mean, action_values_std)\n        else:\n            action = action_values_mean\n\n        return np.atleast_1d(action)\n\n    def get_control_param(self):\n        return np.ones(self.action_space.shape)*self.noise_schedule.current_value\n'"
rl_coach/exploration_policies/boltzmann.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import RunPhase, ActionType\nfrom rl_coach.exploration_policies.exploration_policy import DiscreteActionExplorationPolicy, ExplorationParameters\nfrom rl_coach.schedules import Schedule\nfrom rl_coach.spaces import ActionSpace\n\n\nclass BoltzmannParameters(ExplorationParameters):\n    def __init__(self):\n        super().__init__()\n        self.temperature_schedule = None\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.boltzmann:Boltzmann\'\n\n\nclass Boltzmann(DiscreteActionExplorationPolicy):\n    """"""\n    The Boltzmann exploration policy is intended for discrete action spaces. It assumes that each of the possible\n    actions has some value assigned to it (such as the Q value), and uses a softmax function to convert these values\n    into a distribution over the actions. It then samples the action for playing out of the calculated distribution.\n    An additional temperature schedule can be given by the user, and will control the steepness of the softmax function.\n    """"""\n    def __init__(self, action_space: ActionSpace, temperature_schedule: Schedule):\n        """"""\n        :param action_space: the action space used by the environment\n        :param temperature_schedule: the schedule for the temperature parameter of the softmax\n        """"""\n        super().__init__(action_space)\n        self.temperature_schedule = temperature_schedule\n\n    def get_action(self, action_values: List[ActionType]) -> (ActionType, List[float]):\n        if self.phase == RunPhase.TRAIN:\n            self.temperature_schedule.step()\n        # softmax calculation\n        exp_probabilities = np.exp(action_values / self.temperature_schedule.current_value)\n        probabilities = exp_probabilities / np.sum(exp_probabilities)\n        # make sure probs sum to 1\n        probabilities[-1] = 1 - np.sum(probabilities[:-1])\n        # choose actions according to the probabilities\n        action = np.random.choice(range(self.action_space.shape), p=probabilities)\n        return action, probabilities\n\n    def get_control_param(self):\n        return self.temperature_schedule.current_value\n'"
rl_coach/exploration_policies/bootstrapped.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import RunPhase, ActionType\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.exploration_policies.e_greedy import EGreedy, EGreedyParameters\nfrom rl_coach.exploration_policies.exploration_policy import ExplorationParameters\nfrom rl_coach.schedules import Schedule, LinearSchedule\nfrom rl_coach.spaces import ActionSpace\n\n\nclass BootstrappedParameters(EGreedyParameters):\n    def __init__(self):\n        super().__init__()\n        self.architecture_num_q_heads = 10\n        self.bootstrapped_data_sharing_probability = 1.0\n        self.epsilon_schedule = LinearSchedule(1, 0.01, 1000000)\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.bootstrapped:Bootstrapped\'\n\n\nclass Bootstrapped(EGreedy):\n    """"""\n    Bootstrapped exploration policy is currently only used for discrete action spaces along with the\n    Bootstrapped DQN agent. It assumes that there is an ensemble of network heads, where each one predicts the\n    values for all the possible actions. For each episode, a single head is selected to lead the agent, according\n    to its value predictions. In evaluation, the action is selected using a majority vote over all the heads\n    predictions.\n\n    .. note::\n       This exploration policy will only work for Discrete action spaces with Bootstrapped DQN style agents,\n       since it requires the agent to have a network with multiple heads.\n    """"""\n    def __init__(self, action_space: ActionSpace, epsilon_schedule: Schedule, evaluation_epsilon: float,\n                 architecture_num_q_heads: int,\n                 continuous_exploration_policy_parameters: ExplorationParameters = AdditiveNoiseParameters(),):\n        """"""\n        :param action_space: the action space used by the environment\n        :param epsilon_schedule: a schedule for the epsilon values\n        :param evaluation_epsilon: the epsilon value to use for evaluation phases\n        :param continuous_exploration_policy_parameters: the parameters of the continuous exploration policy to use\n                                                         if the e-greedy is used for a continuous policy\n        :param architecture_num_q_heads: the number of q heads to select from\n        """"""\n        super().__init__(action_space, epsilon_schedule, evaluation_epsilon, continuous_exploration_policy_parameters)\n        self.num_heads = architecture_num_q_heads\n        self.selected_head = 0\n        self.last_action_values = 0\n\n    def select_head(self):\n        self.selected_head = np.random.randint(self.num_heads)\n\n    def get_action(self, action_values: List[ActionType]) -> ActionType:\n        # action values are none in case the exploration policy is going to select a random action\n        if action_values is not None:\n            if self.phase == RunPhase.TRAIN:\n                action_values = action_values[self.selected_head]\n            else:\n                # ensemble voting for evaluation\n                top_action_votings = np.argmax(action_values, axis=-1)\n                counts = np.bincount(top_action_votings.squeeze())\n                top_action = np.argmax(counts)\n                # convert the top action to a one hot vector and pass it to e-greedy\n                action_values = np.eye(len(self.action_space.actions))[[top_action]]\n        self.last_action_values = action_values\n        return super().get_action(action_values)\n\n    def get_control_param(self):\n        return self.selected_head\n'"
rl_coach/exploration_policies/categorical.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import RunPhase, ActionType\nfrom rl_coach.exploration_policies.exploration_policy import DiscreteActionExplorationPolicy, ExplorationParameters\nfrom rl_coach.spaces import ActionSpace\n\n\nclass CategoricalParameters(ExplorationParameters):\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.categorical:Categorical\'\n\n\nclass Categorical(DiscreteActionExplorationPolicy):\n    """"""\n    Categorical exploration policy is intended for discrete action spaces. It expects the action values to\n    represent a probability distribution over the action, from which a single action will be sampled.\n    In evaluation, the action that has the highest probability will be selected. This is particularly useful for\n    actor-critic schemes, where the actors output is a probability distribution over the actions.\n    """"""\n    def __init__(self, action_space: ActionSpace):\n        """"""\n        :param action_space: the action space used by the environment\n        """"""\n        super().__init__(action_space)\n\n    def get_action(self, action_values: List[ActionType]) -> (ActionType, List[float]):\n        if self.phase == RunPhase.TRAIN:\n            # choose actions according to the probabilities\n            action = np.random.choice(self.action_space.actions, p=action_values)\n            return action, action_values\n        else:\n            # take the action with the highest probability\n            action = np.argmax(action_values)\n            one_hot_action_probabilities = np.zeros(len(self.action_space.actions))\n            one_hot_action_probabilities[action] = 1\n\n            return action, one_hot_action_probabilities\n\n    def get_control_param(self):\n        return 0\n'"
rl_coach/exploration_policies/continuous_entropy.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoise, AdditiveNoiseParameters\n\n\nclass ContinuousEntropyParameters(AdditiveNoiseParameters):\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.continuous_entropy:ContinuousEntropy\'\n\n\nclass ContinuousEntropy(AdditiveNoise):\n    """"""\n    Continuous entropy is an exploration policy that is actually implemented as part of the network.\n    The exploration policy class is only a placeholder for choosing this policy. The exploration policy is\n    implemented by adding a regularization factor to the network loss, which regularizes the entropy of the action.\n    This exploration policy is only intended for continuous action spaces, and assumes that the entire calculation\n    is implemented as part of the head.\n\n    .. warning::\n       This exploration policy expects the agent or the network to implement the exploration functionality.\n       There are only a few heads that actually are relevant and implement the entropy regularization factor.\n    """"""\n    pass\n'"
rl_coach/exploration_policies/e_greedy.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import RunPhase, ActionType\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.exploration_policies.exploration_policy import ExplorationParameters, ExplorationPolicy\nfrom rl_coach.schedules import Schedule, LinearSchedule\nfrom rl_coach.spaces import ActionSpace, DiscreteActionSpace, BoxActionSpace\nfrom rl_coach.utils import dynamic_import_and_instantiate_module_from_params\n\n\nclass EGreedyParameters(ExplorationParameters):\n    def __init__(self):\n        super().__init__()\n        self.epsilon_schedule = LinearSchedule(0.5, 0.01, 50000)\n        self.evaluation_epsilon = 0.05\n        self.continuous_exploration_policy_parameters = AdditiveNoiseParameters()\n        self.continuous_exploration_policy_parameters.noise_schedule = LinearSchedule(0.1, 0.1, 50000)\n        # for continuous control -\n        # (see http://www.cs.ubc.ca/~van/papers/2017-TOG-deepLoco/2017-TOG-deepLoco.pdf)\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.e_greedy:EGreedy\'\n\n\nclass EGreedy(ExplorationPolicy):\n    """"""\n    e-greedy is an exploration policy that is intended for both discrete and continuous action spaces.\n\n    For discrete action spaces, it assumes that each action is assigned a value, and it selects the action with the\n    highest value with probability 1 - epsilon. Otherwise, it selects a action sampled uniformly out of all the\n    possible actions. The epsilon value is given by the user and can be given as a schedule.\n    In evaluation, a different epsilon value can be specified.\n\n    For continuous action spaces, it assumes that the mean action is given by the agent. With probability epsilon,\n    it samples a random action out of the action space bounds. Otherwise, it selects the action according to a\n    given continuous exploration policy, which is set to AdditiveNoise by default. In evaluation, the action is\n    always selected according to the given continuous exploration policy (where its phase is set to evaluation as well).\n    """"""\n    def __init__(self, action_space: ActionSpace, epsilon_schedule: Schedule,\n                 evaluation_epsilon: float,\n                 continuous_exploration_policy_parameters: ExplorationParameters=AdditiveNoiseParameters()):\n        """"""\n        :param action_space: the action space used by the environment\n        :param epsilon_schedule: a schedule for the epsilon values\n        :param evaluation_epsilon: the epsilon value to use for evaluation phases\n        :param continuous_exploration_policy_parameters: the parameters of the continuous exploration policy to use\n                                                         if the e-greedy is used for a continuous policy\n        """"""\n        super().__init__(action_space)\n        self.epsilon_schedule = epsilon_schedule\n        self.evaluation_epsilon = evaluation_epsilon\n\n        if isinstance(self.action_space, BoxActionSpace):\n            # for continuous e-greedy (see http://www.cs.ubc.ca/~van/papers/2017-TOG-deepLoco/2017-TOG-deepLoco.pdf)\n            continuous_exploration_policy_parameters.action_space = action_space\n            self.continuous_exploration_policy = \\\n                dynamic_import_and_instantiate_module_from_params(continuous_exploration_policy_parameters)\n\n        self.current_random_value = np.random.rand()\n\n    def requires_action_values(self):\n        epsilon = self.evaluation_epsilon if self.phase == RunPhase.TEST else self.epsilon_schedule.current_value\n        return self.current_random_value >= epsilon\n\n    def get_action(self, action_values: List[ActionType]) -> (ActionType, List[float]):\n        epsilon = self.evaluation_epsilon if self.phase == RunPhase.TEST else self.epsilon_schedule.current_value\n\n        if isinstance(self.action_space, DiscreteActionSpace):\n            if self.current_random_value < epsilon:\n                chosen_action = self.action_space.sample()\n                probabilities = np.full(len(self.action_space.actions),\n                                      1. / (self.action_space.high[0] - self.action_space.low[0] + 1))\n            else:\n                chosen_action = np.argmax(np.random.random(action_values.shape) *\n                                          (np.isclose(action_values, action_values.max())))\n\n                # one-hot probabilities vector\n                probabilities = np.zeros(len(self.action_space.actions))\n                probabilities[chosen_action] = 1\n\n            self.step_epsilon()\n            return chosen_action, probabilities\n\n        else:\n            if self.current_random_value < epsilon and self.phase == RunPhase.TRAIN:\n                chosen_action = self.action_space.sample()\n            else:\n                chosen_action = self.continuous_exploration_policy.get_action(action_values)\n\n            self.step_epsilon()\n            return chosen_action\n\n    def get_control_param(self):\n        if isinstance(self.action_space, DiscreteActionSpace):\n            return self.evaluation_epsilon if self.phase == RunPhase.TEST else self.epsilon_schedule.current_value\n        elif isinstance(self.action_space, BoxActionSpace):\n            return self.continuous_exploration_policy.get_control_param()\n\n    def change_phase(self, phase):\n        super().change_phase(phase)\n        if isinstance(self.action_space, BoxActionSpace):\n            self.continuous_exploration_policy.change_phase(phase)\n\n    def step_epsilon(self):\n        # step the epsilon schedule and generate a new random value for next time\n        if self.phase == RunPhase.TRAIN:\n            self.epsilon_schedule.step()\n        self.current_random_value = np.random.rand()\n'"
rl_coach/exploration_policies/exploration_policy.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nfrom rl_coach.base_parameters import Parameters\nfrom rl_coach.core_types import RunPhase, ActionType\nfrom rl_coach.spaces import ActionSpace, DiscreteActionSpace, BoxActionSpace, GoalsSpace\n\n\nclass ExplorationParameters(Parameters):\n    def __init__(self):\n        self.action_space = None\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.exploration_policy:ExplorationPolicy\'\n\n\nclass ExplorationPolicy(object):\n    """"""\n    An exploration policy takes the predicted actions or action values from the agent, and selects the action to\n    actually apply to the environment using some predefined algorithm.\n    """"""\n    def __init__(self, action_space: ActionSpace):\n        """"""\n        :param action_space: the action space used by the environment\n        """"""\n        self.phase = RunPhase.HEATUP\n        self.action_space = action_space\n\n    def reset(self):\n        """"""\n        Used for resetting the exploration policy parameters when needed\n        :return: None\n        """"""\n        pass\n\n    def get_action(self, action_values: List[ActionType]) -> ActionType:\n        """"""\n        Given a list of values corresponding to each action, \n        choose one actions according to the exploration policy\n        :param action_values: A list of action values\n        :return: The chosen action,\n                 The probability of the action (if available, otherwise 1 for absolute certainty in the action)\n        """"""\n        raise NotImplementedError()\n\n    def change_phase(self, phase):\n        """"""\n        Change between running phases of the algorithm\n        :param phase: Either Heatup or Train\n        :return: none\n        """"""\n        self.phase = phase\n\n    def requires_action_values(self) -> bool:\n        """"""\n        Allows exploration policies to define if they require the action values for the current step.\n        This can save up a lot of computation. For example in e-greedy, if the random value generated is smaller\n        than epsilon, the action is completely random, and the action values don\'t need to be calculated\n        :return: True if the action values are required. False otherwise\n        """"""\n        return True\n\n    def get_control_param(self):\n        return 0\n\n\nclass DiscreteActionExplorationPolicy(ExplorationPolicy):\n    """"""\n    A discrete action exploration policy.\n    """"""\n    def __init__(self, action_space: ActionSpace):\n        """"""\n        :param action_space: the action space used by the environment\n        """"""\n        assert isinstance(action_space, DiscreteActionSpace)\n        super().__init__(action_space)\n\n    def get_action(self, action_values: List[ActionType]) -> (ActionType, List):\n        """"""\n        Given a list of values corresponding to each action,\n        choose one actions according to the exploration policy\n        :param action_values: A list of action values\n        :return: The chosen action,\n                 The probabilities of actions to select from (if not available a one-hot vector)\n        """"""\n        if self.__class__ == ExplorationPolicy:\n            raise ValueError(""The ExplorationPolicy class is an abstract class and should not be used directly. ""\n                             ""Please set the exploration parameters to point to an inheriting class like EGreedy or ""\n                             ""AdditiveNoise"")\n        else:\n            raise ValueError(""The get_action function should be overridden in the inheriting exploration class"")\n\n\nclass ContinuousActionExplorationPolicy(ExplorationPolicy):\n    """"""\n    A continuous action exploration policy.\n    """"""\n    def __init__(self, action_space: ActionSpace):\n        """"""\n        :param action_space: the action space used by the environment\n        """"""\n        assert isinstance(action_space, BoxActionSpace) or \\\n               (hasattr(action_space, \'filtered_action_space\') and\n                 isinstance(action_space.filtered_action_space, BoxActionSpace)) or \\\n               isinstance(action_space, GoalsSpace)\n        super().__init__(action_space)\n'"
rl_coach/exploration_policies/greedy.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import ActionType\nfrom rl_coach.exploration_policies.exploration_policy import ExplorationParameters, ExplorationPolicy\nfrom rl_coach.spaces import ActionSpace, DiscreteActionSpace, BoxActionSpace\n\n\nclass GreedyParameters(ExplorationParameters):\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.greedy:Greedy\'\n\n\nclass Greedy(ExplorationPolicy):\n    """"""\n    The Greedy exploration policy is intended for both discrete and continuous action spaces.\n    For discrete action spaces, it always selects the action with the maximum value, as given by the agent.\n    For continuous action spaces, it always return the exact action, as it was given by the agent.\n    """"""\n    def __init__(self, action_space: ActionSpace):\n        """"""\n        :param action_space: the action space used by the environment\n        """"""\n        super().__init__(action_space)\n\n    def get_action(self, action_values: List[ActionType]):\n        if type(self.action_space) == DiscreteActionSpace:\n            action = np.argmax(action_values)\n            one_hot_action_probabilities = np.zeros(len(self.action_space.actions))\n            one_hot_action_probabilities[action] = 1\n            return action, one_hot_action_probabilities\n        if type(self.action_space) == BoxActionSpace:\n            return action_values\n\n    def get_control_param(self):\n        return 0\n'"
rl_coach/exploration_policies/ou_process.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import RunPhase, ActionType\nfrom rl_coach.exploration_policies.exploration_policy import ContinuousActionExplorationPolicy, ExplorationParameters\nfrom rl_coach.spaces import ActionSpace, BoxActionSpace, GoalsSpace\n\n\n# Based on on the description in:\n# https://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n\nclass OUProcessParameters(ExplorationParameters):\n    def __init__(self):\n        super().__init__()\n        self.mu = 0\n        self.theta = 0.15\n        self.sigma = 0.2\n        self.dt = 0.01\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.ou_process:OUProcess\'\n\n\n# Ornstein-Uhlenbeck process\nclass OUProcess(ContinuousActionExplorationPolicy):\n    """"""\n    OUProcess exploration policy is intended for continuous action spaces, and selects the action according to\n    an Ornstein-Uhlenbeck process. The Ornstein-Uhlenbeck process implements the action as a Gaussian process, where\n    the samples are correlated between consequent time steps.\n    """"""\n    def __init__(self, action_space: ActionSpace, mu: float=0, theta: float=0.15, sigma: float=0.2, dt: float=0.01):\n        """"""\n        :param action_space: the action space used by the environment\n        """"""\n        super().__init__(action_space)\n        self.mu = float(mu) * np.ones(self.action_space.shape)\n        self.theta = float(theta)\n        self.sigma = float(sigma) * np.ones(self.action_space.shape)\n        self.state = np.zeros(self.action_space.shape)\n        self.dt = dt\n\n    def reset(self):\n        self.state = np.zeros(self.action_space.shape)\n\n    def noise(self):\n        x = self.state\n        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.random.randn(len(x)) * np.sqrt(self.dt)\n        self.state = x + dx\n        return self.state\n\n    def get_action(self, action_values: List[ActionType]) -> ActionType:\n        if self.phase == RunPhase.TRAIN:\n            noise = self.noise()\n        else:\n            noise = np.zeros(self.action_space.shape)\n\n        action = action_values.squeeze() + noise\n\n        return action\n\n    def get_control_param(self):\n        if self.phase == RunPhase.TRAIN:\n            return self.state\n        else:\n            return np.zeros(self.action_space.shape)\n'"
rl_coach/exploration_policies/parameter_noise.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List, Dict\n\nimport numpy as np\n\nfrom rl_coach.architectures.layers import NoisyNetDense\nfrom rl_coach.base_parameters import AgentParameters, NetworkParameters\nfrom rl_coach.spaces import ActionSpace, BoxActionSpace, DiscreteActionSpace\n\nfrom rl_coach.core_types import ActionType\nfrom rl_coach.exploration_policies.exploration_policy import ExplorationPolicy, ExplorationParameters\n\n\nclass ParameterNoiseParameters(ExplorationParameters):\n    def __init__(self, agent_params: AgentParameters):\n        super().__init__()\n\n        if not agent_params.algorithm.supports_parameter_noise:\n            raise ValueError(""Currently only DQN variants are supported for using an exploration type of ""\n                             ""ParameterNoise."")\n\n        self.network_params = agent_params.network_wrappers\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.parameter_noise:ParameterNoise\'\n\n\nclass ParameterNoise(ExplorationPolicy):\n    """"""\n    The ParameterNoise exploration policy is intended for both discrete and continuous action spaces.\n    It applies the exploration policy by replacing all the dense network layers with noisy layers.\n    The noisy layers have both weight means and weight standard deviations, and for each forward pass of the network\n    the weights are sampled from a normal distribution that follows the learned weights mean and standard deviation\n    values.\n\n    Warning: currently supported only by DQN variants\n    """"""\n    def __init__(self, network_params: Dict[str, NetworkParameters], action_space: ActionSpace):\n        """"""\n        :param action_space: the action space used by the environment\n        """"""\n        super().__init__(action_space)\n        self.network_params = network_params\n        self._replace_network_dense_layers()\n\n    def get_action(self, action_values: List[ActionType]):\n        if type(self.action_space) == DiscreteActionSpace:\n            action = np.argmax(action_values)\n            one_hot_action_probabilities = np.zeros(len(self.action_space.actions))\n            one_hot_action_probabilities[action] = 1\n\n            return action, one_hot_action_probabilities\n        elif type(self.action_space) == BoxActionSpace:\n            action_values_mean = action_values[0].squeeze()\n            action_values_std = action_values[1].squeeze()\n            return np.random.normal(action_values_mean, action_values_std)\n        else:\n            raise ValueError(""ActionSpace type {} is not supported for ParameterNoise."".format(type(self.action_space)))\n\n    def get_control_param(self):\n        return 0\n\n    def _replace_network_dense_layers(self):\n        # replace the dense type for all the networks components (embedders, mw, heads) with a NoisyNetDense\n\n        # NOTE: we are changing network params in a non-params class (an already instantiated class), this could have\n        #       been prone to a bug, but since the networks are created very late in the game\n        #       (after agent.init_environment_dependent()_modules is called) - then we are fine.\n\n        for network_wrapper_params in self.network_params.values():\n            for component_params in list(network_wrapper_params.input_embedders_parameters.values()) + \\\n                                    [network_wrapper_params.middleware_parameters] + \\\n                                    network_wrapper_params.heads_parameters:\n                component_params.dense_layer = NoisyNetDense\n\n'"
rl_coach/exploration_policies/truncated_normal.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\nfrom scipy.stats import truncnorm\n\nfrom rl_coach.core_types import RunPhase, ActionType\nfrom rl_coach.exploration_policies.exploration_policy import ExplorationParameters, ContinuousActionExplorationPolicy\nfrom rl_coach.schedules import Schedule, LinearSchedule\nfrom rl_coach.spaces import ActionSpace, BoxActionSpace\n\n\nclass TruncatedNormalParameters(ExplorationParameters):\n    def __init__(self):\n        super().__init__()\n        self.noise_schedule = LinearSchedule(0.1, 0.1, 50000)\n        self.evaluation_noise = 0.05\n        self.clip_low = 0\n        self.clip_high = 1\n        self.noise_as_percentage_from_action_space = True\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.truncated_normal:TruncatedNormal\'\n\n\nclass TruncatedNormal(ContinuousActionExplorationPolicy):\n    """"""\n    The TruncatedNormal exploration policy is intended for continuous action spaces. It samples the action from a\n    normal distribution, where the mean action is given by the agent, and the standard deviation can be given in t\n    wo different ways:\n    1. Specified by the user as a noise schedule which is taken in percentiles out of the action space size\n    2. Specified by the agents action. In case the agents action is a list with 2 values, the 1st one is assumed to\n    be the mean of the action, and 2nd is assumed to be its standard deviation.\n    When the sampled action is outside of the action bounds given by the user, it is sampled again and again, until it\n    is within the bounds.\n    """"""\n    def __init__(self, action_space: ActionSpace, noise_schedule: Schedule,\n                 evaluation_noise: float, clip_low: float, clip_high: float,\n                 noise_as_percentage_from_action_space: bool = True):\n        """"""\n        :param action_space: the action space used by the environment\n        :param noise_schedule: the schedule for the noise variance\n        :param evaluation_noise: the noise variance that will be used during evaluation phases\n        :param noise_as_percentage_from_action_space: whether to consider the noise as a percentage of the action space\n                                                        or absolute value\n        """"""\n        super().__init__(action_space)\n        self.noise_schedule = noise_schedule\n        self.evaluation_noise = evaluation_noise\n        self.noise_as_percentage_from_action_space = noise_as_percentage_from_action_space\n        self.clip_low = clip_low\n        self.clip_high = clip_high\n\n        if not isinstance(action_space, BoxActionSpace):\n            raise ValueError(""Truncated normal exploration works only for continuous controls.""\n                             ""The given action space is of type: {}"".format(action_space.__class__.__name__))\n\n        if not np.all(-np.inf < action_space.high) or not np.all(action_space.high < np.inf)\\\n                or not np.all(-np.inf < action_space.low) or not np.all(action_space.low < np.inf):\n            raise ValueError(""Additive noise exploration requires bounded actions"")\n\n    def get_action(self, action_values: List[ActionType]) -> ActionType:\n        # set the current noise\n        if self.phase == RunPhase.TEST:\n            current_noise = self.evaluation_noise\n        else:\n            current_noise = self.noise_schedule.current_value\n\n        # scale the noise to the action space range\n        if self.noise_as_percentage_from_action_space:\n            action_values_std = current_noise * (self.action_space.high - self.action_space.low)\n        else:\n            action_values_std = current_noise\n\n        # extract the mean values\n        if isinstance(action_values, list):\n            # the action values are expected to be a list with the action mean and optionally the action stdev\n            action_values_mean = action_values[0].squeeze()\n        else:\n            # the action values are expected to be a numpy array representing the action mean\n            action_values_mean = action_values.squeeze()\n\n        # step the noise schedule\n        if self.phase is not RunPhase.TEST:\n            self.noise_schedule.step()\n            # the second element of the list is assumed to be the standard deviation\n            if isinstance(action_values, list) and len(action_values) > 1:\n                action_values_std = action_values[1].squeeze()\n\n        # sample from truncated normal distribution\n        normalized_low = (self.clip_low - action_values_mean) / action_values_std\n        normalized_high = (self.clip_high - action_values_mean) / action_values_std\n        distribution = truncnorm(normalized_low, normalized_high, loc=action_values_mean, scale=action_values_std)\n        action = distribution.rvs(1)\n\n        return action\n\n    def get_control_param(self):\n        return np.ones(self.action_space.shape)*self.noise_schedule.current_value\n'"
rl_coach/exploration_policies/ucb.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import RunPhase, ActionType, EnvironmentSteps\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.exploration_policies.e_greedy import EGreedy, EGreedyParameters\nfrom rl_coach.exploration_policies.exploration_policy import ExplorationParameters\nfrom rl_coach.schedules import Schedule, LinearSchedule, PieceWiseSchedule\nfrom rl_coach.spaces import ActionSpace\n\n\nclass UCBParameters(EGreedyParameters):\n    def __init__(self):\n        super().__init__()\n        self.architecture_num_q_heads = 10\n        self.bootstrapped_data_sharing_probability = 1.0\n        self.epsilon_schedule = PieceWiseSchedule([\n            (LinearSchedule(1, 0.1, 1000000), EnvironmentSteps(1000000)),\n            (LinearSchedule(0.1, 0.01, 4000000), EnvironmentSteps(4000000))\n        ])\n        self.lamb = 0.1\n\n    @property\n    def path(self):\n        return \'rl_coach.exploration_policies.ucb:UCB\'\n\n\nclass UCB(EGreedy):\n    """"""\n    UCB exploration policy is following the upper confidence bound heuristic to sample actions in discrete action spaces.\n    It assumes that there are multiple network heads that are predicting action values, and that the standard deviation\n    between the heads predictions represents the uncertainty of the agent in each of the actions.\n    It then updates the action value estimates to by mean(actions)+lambda*stdev(actions), where lambda is\n    given by the user. This exploration policy aims to take advantage of the uncertainty of the agent in its predictions,\n    and select the action according to the tradeoff between how uncertain the agent is, and how large it predicts\n    the outcome from those actions to be.\n    """"""\n    def __init__(self, action_space: ActionSpace, epsilon_schedule: Schedule, evaluation_epsilon: float,\n                 architecture_num_q_heads: int, lamb: int,\n                 continuous_exploration_policy_parameters: ExplorationParameters = AdditiveNoiseParameters()):\n        """"""\n        :param action_space: the action space used by the environment\n        :param epsilon_schedule: a schedule for the epsilon values\n        :param evaluation_epsilon: the epsilon value to use for evaluation phases\n        :param architecture_num_q_heads: the number of q heads to select from\n        :param lamb: lambda coefficient for taking the standard deviation into account\n        :param continuous_exploration_policy_parameters: the parameters of the continuous exploration policy to use\n                                                         if the e-greedy is used for a continuous policy\n        """"""\n        super().__init__(action_space, epsilon_schedule, evaluation_epsilon, continuous_exploration_policy_parameters)\n        self.num_heads = architecture_num_q_heads\n        self.lamb = lamb\n        self.std = 0\n        self.last_action_values = 0\n\n    def select_head(self):\n        pass\n\n    def get_action(self, action_values: List[ActionType]) -> ActionType:\n        # action values are none in case the exploration policy is going to select a random action\n        if action_values is not None:\n            if self.requires_action_values():\n                mean = np.mean(action_values, axis=0)\n                if self.phase == RunPhase.TRAIN:\n                    self.std = np.std(action_values, axis=0)\n                    self.last_action_values = mean + self.lamb * self.std\n                else:\n                    self.last_action_values = mean\n        return super().get_action(self.last_action_values)\n\n    def get_control_param(self):\n        if self.phase == RunPhase.TRAIN:\n            return np.mean(self.std)\n        else:\n            return 0\n'"
rl_coach/filters/__init__.py,0,b''
rl_coach/filters/filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nimport os\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom typing import Dict, Union, List\n\nfrom rl_coach.core_types import EnvResponse, ActionInfo, Transition\nfrom rl_coach.spaces import ActionSpace, RewardSpace, ObservationSpace\nfrom rl_coach.utils import force_list\n\n\nclass Filter(object):\n    def __init__(self, name=None):\n        self.name = name\n\n    def reset(self) -> None:\n        """"""\n        Called from reset() and implements the reset logic for the filter.\n        :param name: the filter\'s name\n        :return: None\n        """"""\n        pass\n\n    def filter(self, env_response: Union[EnvResponse, Transition], update_internal_state: bool=True) \\\n            -> Union[EnvResponse, Transition]:\n        """"""\n        Filter some values in the env and return the filtered env_response\n        This is the function that each filter should update\n        :param update_internal_state: should the filter\'s internal state change due to this call\n        :param env_response: the input env_response\n        :return: the filtered env_response\n        """"""\n        raise NotImplementedError("""")\n\n    def set_device(self, device, memory_backend_params=None, mode=\'numpy\') -> None:\n        """"""\n        An optional function that allows the filter to get the device if it is required to use tensorflow ops\n        :param device: the device to use\n        :param memory_backend_params: parameters associated with the memory backend\n        :param mode: arithmetic backend to be used {numpy | tf}\n        :return: None\n        """"""\n        pass\n\n    def set_session(self, sess) -> None:\n        """"""\n        An optional function that allows the filter to get the session if it is required to use tensorflow ops\n        :param sess: the session\n        :return: None\n        """"""\n        pass\n\n    def save_state_to_checkpoint(self, checkpoint_dir, checkpoint_prefix)->None:\n        """"""\n        Save the filter\'s internal state to a checkpoint to file, so that it can be later restored.\n        :param checkpoint_dir: the directory in which to save the filter\'s state\n        :param checkpoint_prefix: the prefix of the checkpoint file to save\n        :return: None\n        """"""\n        pass\n\n    def restore_state_from_checkpoint(self, checkpoint_dir, checkpoint_prefix)->None:\n        """"""\n        Save the filter\'s internal state to a checkpoint to file, so that it can be later restored.\n        :param checkpoint_dir: the directory from which to restore\n        :param checkpoint_prefix: the checkpoint prefix to look for\n        :return: None\n        """"""\n        pass\n\n    def set_name(self, name: str) -> None:\n        """"""\n        Set the filter\'s name\n        :param name: the filter\'s name\n        :return: None\n        """"""\n        self.name = name\n\n\nclass OutputFilter(Filter):\n    """"""\n    An output filter is a module that filters the output from an agent to the environment.\n    """"""\n    def __init__(self, action_filters: OrderedDict([(str, \'ActionFilter\')])=None,\n                 is_a_reference_filter: bool=False, name=None):\n        super().__init__(name)\n\n        if action_filters is None:\n            action_filters = OrderedDict([])\n        self._action_filters = action_filters\n\n        # We do not want to allow reference filters such as Atari to be used directly. These have to be duplicated first\n        # and only then can change their values so to keep their original params intact for other agents in the graph.\n        self.i_am_a_reference_filter = is_a_reference_filter\n\n    def __call__(self):\n        duplicate = deepcopy(self)\n        duplicate.i_am_a_reference_filter = False\n        return duplicate\n\n    def set_device(self, device, memory_backend_params=None, mode=\'numpy\') -> None:\n        """"""\n        An optional function that allows the filter to get the device if it is required to use tensorflow ops\n        :param device: the device to use\n        :return: None\n        """"""\n        [f.set_device(device, memory_backend_params, mode=\'numpy\') for f in self.action_filters.values()]\n\n    def set_session(self, sess) -> None:\n        """"""\n        An optional function that allows the filter to get the session if it is required to use tensorflow ops\n        :param sess: the session\n        :return: None\n        """"""\n        [f.set_session(sess) for f in self.action_filters.values()]\n\n    def filter(self, action_info: ActionInfo) -> ActionInfo:\n        """"""\n        A wrapper around _filter which first copies the action_info so that we don\'t change the original one\n        This function should not be updated!\n        :param action_info: the input action_info\n        :return: the filtered action_info\n        """"""\n        if self.i_am_a_reference_filter:\n            raise Exception(""The filter being used is a reference filter. It is not to be used directly. ""\n                            ""Instead get a duplicate from it by calling __call__."")\n        if len(self.action_filters.values()) == 0:\n            return action_info\n        filtered_action_info = copy.deepcopy(action_info)\n        filtered_action = filtered_action_info.action\n        for filter in reversed(self.action_filters.values()):\n            filtered_action = filter.filter(filtered_action)\n\n        filtered_action_info.action = filtered_action\n\n        return filtered_action_info\n\n    def reverse_filter(self, action_info: ActionInfo) -> ActionInfo:\n        """"""\n        A wrapper around _reverse_filter which first copies the action_info so that we don\'t change the original one\n        This function should not be updated!\n        :param action_info: the input action_info\n        :return: the filtered action_info\n        """"""\n        if self.i_am_a_reference_filter:\n            raise Exception(""The filter being used is a reference filter. It is not to be used directly. ""\n                            ""Instead get a duplicate from it by calling __call__."")\n        filtered_action_info = copy.deepcopy(action_info)\n        filtered_action = filtered_action_info.action\n        for filter in self.action_filters.values():\n            filter.validate_output_action(filtered_action)\n            filtered_action = filter.reverse_filter(filtered_action)\n\n        filtered_action_info.action = filtered_action\n\n        return filtered_action_info\n\n    def get_unfiltered_action_space(self, output_action_space: ActionSpace) -> ActionSpace:\n        """"""\n        Given the output action space, returns the corresponding unfiltered action space\n        This function should not be updated!\n        :param output_action_space: the output action space\n        :return: the unfiltered action space\n        """"""\n        unfiltered_action_space = copy.deepcopy(output_action_space)\n        for filter in self._action_filters.values():\n            new_unfiltered_action_space = filter.get_unfiltered_action_space(unfiltered_action_space)\n            filter.validate_output_action_space(unfiltered_action_space)\n            unfiltered_action_space = new_unfiltered_action_space\n        return unfiltered_action_space\n\n    def reset(self) -> None:\n        """"""\n        Reset any internal memory stored in the filter.\n        This function should not be updated!\n        This is useful for stateful filters which stores information on previous filter calls.\n        :return: None\n        """"""\n        [action_filter.reset() for action_filter in self._action_filters.values()]\n\n    @property\n    def action_filters(self) -> OrderedDict([(str, \'ActionFilter\')]):\n        return self._action_filters\n\n    @action_filters.setter\n    def action_filters(self, val: OrderedDict([(str, \'ActionFilter\')])):\n        self._action_filters = val\n\n    def add_action_filter(self, filter_name: str, filter: \'ActionFilter\', add_as_the_first_filter: bool=False):\n        """"""\n        Add an action filter to the filters list\n        :param filter_name: the filter name\n        :param filter: the filter to add\n        :param add_as_the_first_filter: add the filter to the top of the filters stack\n        :return: None\n        """"""\n        self._action_filters[filter_name] = filter\n        if add_as_the_first_filter:\n            self._action_filters.move_to_end(filter_name, last=False)\n\n    def remove_action_filter(self, filter_name: str) -> None:\n        """"""\n        Remove an action filter from the filters list\n        :param filter_name: the filter name\n        :return: None\n        """"""\n        del self._action_filters[filter_name]\n\n    def save_state_to_checkpoint(self, checkpoint_dir, checkpoint_prefix):\n        """"""\n        Currently not in use for OutputFilter.\n        :param checkpoint_dir: the directory in which to save the filter\'s state\n        :param checkpoint_prefix: the prefix of the checkpoint file to save\n        :return:\n        """"""\n        pass\n\n    def restore_state_from_checkpoint(self, checkpoint_dir, checkpoint_prefix)->None:\n        """"""\n        Currently not in use for OutputFilter.\n        :param checkpoint_dir: the directory from which to restore\n        :param checkpoint_prefix: the checkpoint prefix to look for\n        :return: None\n        """"""\n        pass\n\n\n\nclass NoOutputFilter(OutputFilter):\n    """"""\n    Creates an empty output filter. Used only for readability when creating the presets\n    """"""\n    def __init__(self):\n        super().__init__(is_a_reference_filter=False)\n\n\nclass InputFilter(Filter):\n    """"""\n    An input filter is a module that filters the input from an environment to the agent.\n    """"""\n    def __init__(self, observation_filters: Dict[str, Dict[str, \'ObservationFilter\']]=None,\n                 reward_filters: Dict[str, \'RewardFilter\']=None,\n                 is_a_reference_filter: bool=False, name=None):\n        super().__init__(name)\n        if observation_filters is None:\n            observation_filters = {}\n        if reward_filters is None:\n            reward_filters = OrderedDict([])\n        self._observation_filters = observation_filters\n        self._reward_filters = reward_filters\n\n        # We do not want to allow reference filters such as Atari to be used directly. These have to be duplicated first\n        # and only then can change their values so to keep their original params intact for other agents in the graph.\n        self.i_am_a_reference_filter = is_a_reference_filter\n\n    def __call__(self):\n        duplicate = deepcopy(self)\n        duplicate.i_am_a_reference_filter = False\n        return duplicate\n\n    def set_device(self, device, memory_backend_params=None, mode=\'numpy\') -> None:\n        """"""\n        An optional function that allows the filter to get the device if it is required to use tensorflow ops\n        :param device: the device to use\n        :return: None\n        """"""\n        [f.set_device(device, memory_backend_params, mode) for f in self.reward_filters.values()]\n        [[f.set_device(device, memory_backend_params, mode) for f in filters.values()] for filters in self.observation_filters.values()]\n\n    def set_session(self, sess) -> None:\n        """"""\n        An optional function that allows the filter to get the session if it is required to use tensorflow ops\n        :param sess: the session\n        :return: None\n        """"""\n        [f.set_session(sess) for f in self.reward_filters.values()]\n        [[f.set_session(sess) for f in filters.values()] for filters in self.observation_filters.values()]\n\n    def filter(self, unfiltered_data: Union[EnvResponse, List[EnvResponse], Transition, List[Transition]],\n               update_internal_state: bool=True, deep_copy: bool=True) -> Union[List[EnvResponse], List[Transition]]:\n        """"""\n        A wrapper around _filter which first copies the env_response so that we don\'t change the original one\n        This function should not be updated!\n        :param unfiltered_data: the input data\n        :param update_internal_state: should the filter\'s internal state change due to this call\n        :return: the filtered env_response\n        """"""\n        if self.i_am_a_reference_filter:\n            raise Exception(""The filter being used is a reference filter. It is not to be used directly. ""\n                            ""Instead get a duplicate from it by calling __call__."")\n        if deep_copy:\n            filtered_data = copy.deepcopy(unfiltered_data)\n        else:\n            filtered_data = [copy.copy(t) for t in unfiltered_data]\n        filtered_data = force_list(filtered_data)\n\n        # TODO: implement observation space validation\n        # filter observations\n        if isinstance(filtered_data[0], Transition):\n            state_objects_to_filter = [[f.state for f in filtered_data],\n                                       [f.next_state for f in filtered_data]]\n        elif isinstance(filtered_data[0], EnvResponse):\n            state_objects_to_filter = [[f.next_state for f in filtered_data]]\n        else:\n            raise ValueError(""unfiltered_data should be either of type EnvResponse or Transition. "")\n\n        for state_object_list in state_objects_to_filter:\n            for observation_name, filters in self._observation_filters.items():\n                if observation_name in state_object_list[0].keys():\n                    for filter in filters.values():\n                        data_to_filter = [state_object[observation_name] for state_object in state_object_list]\n                        if filter.supports_batching:\n                            filtered_observations = filter.filter(\n                                data_to_filter, update_internal_state=update_internal_state)\n                        else:\n                            filtered_observations = []\n                            for data_point in data_to_filter:\n                                filtered_observations.append(filter.filter(\n                                    data_point, update_internal_state=update_internal_state))\n\n                        for i, state_object in enumerate(state_object_list):\n                            state_object[observation_name] = filtered_observations[i]\n\n        # filter reward\n        for filter in self._reward_filters.values():\n            if filter.supports_batching:\n                filtered_rewards = filter.filter([f.reward for f in filtered_data], update_internal_state)\n                for d, filtered_reward in zip(filtered_data, filtered_rewards):\n                    d.reward = filtered_reward\n            else:\n                for d in filtered_data:\n                    d.reward = filter.filter(d.reward, update_internal_state)\n\n        return filtered_data\n\n    def get_filtered_observation_space(self, observation_name: str,\n                                       input_observation_space: ObservationSpace) -> ObservationSpace:\n        """"""\n        Given the input observation space, returns the corresponding filtered observation space\n        This function should not be updated!\n        :param observation_name: the name of the observation to which we want to calculate the filtered space\n        :param input_observation_space: the input observation space\n        :return: the filtered observation space\n        """"""\n        filtered_observation_space = copy.deepcopy(input_observation_space)\n        if observation_name in self._observation_filters.keys():\n            for filter in self._observation_filters[observation_name].values():\n                filter.validate_input_observation_space(filtered_observation_space)\n                filtered_observation_space = filter.get_filtered_observation_space(filtered_observation_space)\n        return filtered_observation_space\n\n    def get_filtered_reward_space(self, input_reward_space: RewardSpace) -> RewardSpace:\n        """"""\n        Given the input reward space, returns the corresponding filtered reward space\n        This function should not be updated!\n        :param input_reward_space: the input reward space\n        :return: the filtered reward space\n        """"""\n        filtered_reward_space = copy.deepcopy(input_reward_space)\n        for filter in self._reward_filters.values():\n            filtered_reward_space = filter.get_filtered_reward_space(filtered_reward_space)\n        return filtered_reward_space\n\n    def reset(self) -> None:\n        """"""\n        Reset any internal memory stored in the filter.\n        This function should not be updated!\n        This is useful for stateful filters which stores information on previous filter calls.\n        :return: None\n        """"""\n        for curr_observation_filters in self._observation_filters.values():\n            [observation_filter.reset() for observation_filter in curr_observation_filters.values()]\n        [reward_filter.reset() for reward_filter in self._reward_filters.values()]\n\n    @property\n    def observation_filters(self) -> Dict[str, Dict[str, \'ObservationFilter\']]:\n        return self._observation_filters\n\n    @observation_filters.setter\n    def observation_filters(self, val: Dict[str, Dict[str, \'ObservationFilter\']]):\n        self._observation_filters = val\n\n    @property\n    def reward_filters(self) -> OrderedDict([(str, \'RewardFilter\')]):\n        return self._reward_filters\n\n    @reward_filters.setter\n    def reward_filters(self, val: OrderedDict([(str, \'RewardFilter\')])):\n        self._reward_filters = val\n\n    def copy_filters_from_one_observation_to_another(self, from_observation: str, to_observation: str):\n        """"""\n        Copy all the filters created for some observation to another observation\n        :param from_observation: the source observation to copy from\n        :param to_observation: the target observation to copy to\n        :return: None\n        """"""\n        self._observation_filters[to_observation] = copy.deepcopy(self._observation_filters[from_observation])\n\n    def add_observation_filter(self, observation_name: str, filter_name: str, filter: \'ObservationFilter\',\n                               add_as_the_first_filter: bool=False):\n        """"""\n        Add an observation filter to the filters list\n        :param observation_name: the name of the observation to apply to\n        :param filter_name: the filter name\n        :param filter: the filter to add\n        :param add_as_the_first_filter: add the filter to the top of the filters stack\n        :return: None\n        """"""\n        if observation_name not in self._observation_filters.keys():\n            self._observation_filters[observation_name] = OrderedDict()\n        self._observation_filters[observation_name][filter_name] = filter\n        if add_as_the_first_filter:\n            self._observation_filters[observation_name].move_to_end(filter_name, last=False)\n\n    def add_reward_filter(self, filter_name: str, filter: \'RewardFilter\', add_as_the_first_filter: bool=False):\n        """"""\n        Add a reward filter to the filters list\n        :param filter_name: the filter name\n        :param filter: the filter to add\n        :param add_as_the_first_filter: add the filter to the top of the filters stack\n        :return: None\n        """"""\n        self._reward_filters[filter_name] = filter\n        if add_as_the_first_filter:\n            self._reward_filters.move_to_end(filter_name, last=False)\n\n    def remove_observation_filter(self, observation_name: str, filter_name: str) -> None:\n        """"""\n        Remove an observation filter from the filters list\n        :param observation_name: the name of the observation to apply to\n        :param filter_name: the filter name\n        :return: None\n        """"""\n        del self._observation_filters[observation_name][filter_name]\n\n    def remove_reward_filter(self, filter_name: str) -> None:\n        """"""\n        Remove a reward filter from the filters list\n        :param filter_name: the filter name\n        :return: None\n        """"""\n        del self._reward_filters[filter_name]\n\n    def save_state_to_checkpoint(self, checkpoint_dir, checkpoint_prefix):\n        """"""\n        Save the filter\'s internal state to a checkpoint to file, so that it can be later restored.\n        :param checkpoint_dir: the directory in which to save the filter\'s state\n        :param checkpoint_prefix: the prefix of the checkpoint file to save\n        :return: None\n        """"""\n        checkpoint_prefix = \'.\'.join([checkpoint_prefix, \'filters\'])\n        if self.name is not None:\n            checkpoint_prefix = \'.\'.join([checkpoint_prefix, self.name])\n        for filter_name, filter in self._reward_filters.items():\n            curr_reward_filter_ckpt_prefix = \'.\'.join([checkpoint_prefix, \'reward_filters\', filter_name])\n            filter.save_state_to_checkpoint(checkpoint_dir, curr_reward_filter_ckpt_prefix)\n\n        for observation_name, filters_dict in self._observation_filters.items():\n            for filter_name, filter in filters_dict.items():\n                curr_obs_filter_ckpt_prefix = \'.\'.join([checkpoint_prefix, \'observation_filters\', observation_name,\n                                                                 filter_name])\n                filter.save_state_to_checkpoint(checkpoint_dir, curr_obs_filter_ckpt_prefix)\n\n    def restore_state_from_checkpoint(self, checkpoint_dir, checkpoint_prefix)->None:\n        """"""\n        Save the filter\'s internal state to a checkpoint to file, so that it can be later restored.\n        :param checkpoint_dir: the directory from which to restore\n        :param checkpoint_prefix: the checkpoint prefix to look for\n        :return: None\n        """"""\n        checkpoint_prefix = \'.\'.join([checkpoint_prefix, \'filters\'])\n        if self.name is not None:\n            checkpoint_prefix = \'.\'.join([checkpoint_prefix, self.name])\n        for filter_name, filter in self._reward_filters.items():\n            curr_reward_filter_ckpt_prefix = \'.\'.join([checkpoint_prefix, \'reward_filters\', filter_name])\n            filter.restore_state_from_checkpoint(checkpoint_dir, curr_reward_filter_ckpt_prefix)\n\n        for observation_name, filters_dict in self._observation_filters.items():\n            for filter_name, filter in filters_dict.items():\n                curr_obs_filter_ckpt_prefix = \'.\'.join([checkpoint_prefix, \'observation_filters\', observation_name,\n                                                                 filter_name])\n                filter.restore_state_from_checkpoint(checkpoint_dir, curr_obs_filter_ckpt_prefix)\n\n\nclass NoInputFilter(InputFilter):\n    """"""\n    Creates an empty input filter. Used only for readability when creating the presets\n    """"""\n    def __init__(self):\n        super().__init__(is_a_reference_filter=False, name=\'no_input_filter\')\n\n\n'"
rl_coach/graph_managers/__init__.py,0,b''
rl_coach/graph_managers/basic_rl_graph_manager.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom typing import Tuple, List\n\nfrom rl_coach.base_parameters import AgentParameters, VisualizationParameters, TaskParameters, \\\n    PresetValidationParameters\nfrom rl_coach.environments.environment import EnvironmentParameters, Environment\nfrom rl_coach.filters.filter import NoInputFilter, NoOutputFilter\nfrom rl_coach.graph_managers.graph_manager import GraphManager, ScheduleParameters\nfrom rl_coach.level_manager import LevelManager\nfrom rl_coach.utils import short_dynamic_import\n\n\nclass BasicRLGraphManager(GraphManager):\n    """"""\n    A basic RL graph manager creates the common scheme of RL where there is a single agent which interacts with a\n    single environment.\n    """"""\n    def __init__(self, agent_params: AgentParameters, env_params: EnvironmentParameters,\n                 schedule_params: ScheduleParameters,\n                 vis_params: VisualizationParameters=VisualizationParameters(),\n                 preset_validation_params: PresetValidationParameters = PresetValidationParameters(),\n                 name=\'simple_rl_graph\'):\n        super().__init__(name, schedule_params, vis_params)\n\n        self.agent_params = agent_params\n        self.env_params = env_params\n        self.preset_validation_params = preset_validation_params\n        self.agent_params.visualization = vis_params\n\n        if self.agent_params.input_filter is None:\n            if env_params is not None:\n                self.agent_params.input_filter = env_params.default_input_filter()\n            else:\n                # In cases where there is no environment (e.g. batch-rl and imitation learning), there is nowhere to get\n                # a default filter from. So using a default no-filter.\n                # When there is no environment, the user is expected to define input/output filters (if required) using\n                # the preset.\n                self.agent_params.input_filter = NoInputFilter()\n        if self.agent_params.output_filter is None:\n            if env_params is not None:\n                self.agent_params.output_filter = env_params.default_output_filter()\n            else:\n                self.agent_params.output_filter = NoOutputFilter()\n\n    def _create_graph(self, task_parameters: TaskParameters) -> Tuple[List[LevelManager], List[Environment]]:\n        # environment loading\n        self.env_params.seed = task_parameters.seed\n        self.env_params.experiment_path = task_parameters.experiment_path\n        env = short_dynamic_import(self.env_params.path)(**self.env_params.__dict__,\n                                                         visualization_parameters=self.visualization_parameters)\n\n        # agent loading\n        self.agent_params.task_parameters = task_parameters  # TODO: this should probably be passed in a different way\n        self.agent_params.name = ""agent""\n        agent = short_dynamic_import(self.agent_params.path)(self.agent_params)\n\n        # set level manager\n        level_manager = LevelManager(agents=agent, environment=env, name=""main_level"")\n\n        return [level_manager], [env]\n\n    def log_signal(self, signal_name, value):\n        self.level_managers[0].agents[\'agent\'].agent_logger.create_signal_value(signal_name, value)\n\n    def get_signal_value(self, signal_name):\n        return self.level_managers[0].agents[\'agent\'].agent_logger.get_signal_value(signal_name)\n\n    def get_agent(self):\n        return self.level_managers[0].agents[\'agent\']\n\n'"
rl_coach/graph_managers/batch_rl_graph_manager.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom copy import deepcopy\nfrom typing import Tuple, List, Union\n\nfrom rl_coach.agents.dqn_agent import DQNAgentParameters\nfrom rl_coach.agents.nec_agent import NECAgentParameters\nfrom rl_coach.architectures.network_wrapper import NetworkWrapper\nfrom rl_coach.base_parameters import AgentParameters, VisualizationParameters, TaskParameters, \\\n    PresetValidationParameters\nfrom rl_coach.core_types import RunPhase, TotalStepsCounter, TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import EnvironmentParameters, Environment\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n\nfrom rl_coach.level_manager import LevelManager\nfrom rl_coach.logger import screen\nfrom rl_coach.schedules import LinearSchedule\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import short_dynamic_import\n\nfrom rl_coach.memories.episodic import EpisodicExperienceReplayParameters\n\nfrom rl_coach.core_types import TimeTypes\n\n\nclass BatchRLGraphManager(BasicRLGraphManager):\n    """"""\n    A batch RL graph manager creates a scenario of learning from a dataset without a simulator.\n\n    If an environment is given (useful either for research purposes, or for experimenting with a toy problem before\n    actually working with a real dataset), we can use it in order to collect a dataset to later be used to train the\n    actual agent. The collected dataset, in this case, can be collected either by randomly acting in the environment\n    (only running in heatup), or alternatively by training a different agent in the environment and using its collected\n    data as a dataset. If an experience generating agent parameters are given, we will instantiate this agent and use it\n     in order to train on the environment and then use this dataset to actually train an agent. Otherwise, we will\n     collect a random dataset.\n    :param agent_params: the parameters of the agent to train using batch RL\n    :param env_params: [optional] environment parameters, for cases where we want to first collect a dataset\n    :param vis_params: visualization parameters\n    :param preset_validation_params: preset validation parameters, to be used for testing purposes\n    :param name: graph name\n    :param spaces_definition: when working with a dataset, we need to get a description of the actual state and action\n                              spaces of the problem\n    :param reward_model_num_epochs: the number of epochs to go over the dataset for training a reward model for the\n                                    \'direct method\' and \'doubly robust\' OPE methods.\n    :param train_to_eval_ratio: percentage of the data transitions to be used for training vs. evaluation. i.e. a value\n                                of 0.8 means ~80% of the transitions will be used for training and ~20% will be used for\n                                 evaluation using OPE.\n    :param experience_generating_agent_params: [optional] parameters of an agent to be trained vs. an environment, whose\n                                               his collected experience will be used to train the acutal (another) agent\n    :param experience_generating_schedule_params: [optional] graph scheduling parameters for training the experience\n                                                  generating agent\n    """"""\n    def __init__(self, agent_params: AgentParameters,\n                 env_params: Union[EnvironmentParameters, None],\n                 schedule_params: ScheduleParameters,\n                 vis_params: VisualizationParameters = VisualizationParameters(),\n                 preset_validation_params: PresetValidationParameters = PresetValidationParameters(),\n                 name=\'batch_rl_graph\', spaces_definition: SpacesDefinition = None, reward_model_num_epochs: int = 100,\n                 train_to_eval_ratio: float = 0.8, experience_generating_agent_params: AgentParameters = None,\n                 experience_generating_schedule_params: ScheduleParameters = None):\n\n        super().__init__(agent_params, env_params, schedule_params, vis_params, preset_validation_params, name)\n        self.is_batch_rl = True\n        self.time_metric = TimeTypes.Epoch\n        self.reward_model_num_epochs = reward_model_num_epochs\n        self.spaces_definition = spaces_definition\n        self.is_collecting_random_dataset = experience_generating_agent_params is None\n\n        # setting this here to make sure that, by default, train_to_eval_ratio gets a value < 1\n        # (its default value in the memory is 1, so not to affect other non-batch-rl scenarios)\n        if self.is_collecting_random_dataset:\n            self.agent_params.memory.train_to_eval_ratio = train_to_eval_ratio\n        else:\n            experience_generating_agent_params.memory.train_to_eval_ratio = train_to_eval_ratio\n            self.experience_generating_agent_params = experience_generating_agent_params\n            self.experience_generating_agent = None\n\n            self.set_schedule_params(experience_generating_schedule_params)\n            self.schedule_params = schedule_params\n\n    def _create_graph(self, task_parameters: TaskParameters) -> Tuple[List[LevelManager], List[Environment]]:\n        if self.env_params:\n            # environment loading\n            self.env_params.seed = task_parameters.seed\n            self.env_params.experiment_path = task_parameters.experiment_path\n            env = short_dynamic_import(self.env_params.path)(**self.env_params.__dict__,\n                                                             visualization_parameters=self.visualization_parameters)\n        else:\n            env = None\n\n        # Only DQN variants and NEC are supported at this point.\n        assert(isinstance(self.agent_params, DQNAgentParameters) or isinstance(self.agent_params, NECAgentParameters))\n        # Only Episodic memories are supported,\n        # for evaluating the sequential doubly robust estimator\n        assert(isinstance(self.agent_params.memory, EpisodicExperienceReplayParameters))\n\n        # agent loading\n        self.agent_params.task_parameters = task_parameters  # TODO: this should probably be passed in a different way\n        self.agent_params.name = ""agent""\n        self.agent_params.is_batch_rl_training = True\n        self.agent_params.network_wrappers[\'main\'].should_get_softmax_probabilities = True\n\n        if \'reward_model\' not in self.agent_params.network_wrappers:\n            # user hasn\'t defined params for the reward model. we will use the same params as used for the \'main\'\n            # network.\n            self.agent_params.network_wrappers[\'reward_model\'] = deepcopy(self.agent_params.network_wrappers[\'main\'])\n\n        self.agent = short_dynamic_import(self.agent_params.path)(self.agent_params)\n        agents = {\'agent\': self.agent}\n\n        if not self.is_collecting_random_dataset:\n            self.experience_generating_agent_params.visualization.dump_csv = False\n            self.experience_generating_agent_params.task_parameters = task_parameters\n            self.experience_generating_agent_params.name = ""experience_gen_agent""\n            self.experience_generating_agent_params.network_wrappers[\'main\'].should_get_softmax_probabilities = True\n\n            # we need to set these manually as these are usually being set for us only for the default agent\n            self.experience_generating_agent_params.input_filter = self.agent_params.input_filter\n            self.experience_generating_agent_params.output_filter = self.agent_params.output_filter\n\n            self.experience_generating_agent = short_dynamic_import(\n                self.experience_generating_agent_params.path)(self.experience_generating_agent_params)\n\n            agents[\'experience_generating_agent\'] = self.experience_generating_agent\n\n        if not env and not self.agent_params.memory.load_memory_from_file_path:\n            screen.warning(""A BatchRLGraph requires setting a dataset to load into the agent\'s memory or alternatively ""\n                           ""using an environment to create a (random) dataset from. This agent should only be used for ""\n                           ""inference. "")\n        # set level manager\n        # - although we will be using each agent separately, we have to have both agents initialized together with the\n        #   LevelManager, so to have them both properly initialized\n        level_manager = LevelManager(agents=agents,\n                                     environment=env, name=""main_level"",\n                                     spaces_definition=self.spaces_definition)\n        if env:\n            return [level_manager], [env]\n        else:\n            return [level_manager], []\n\n    def improve(self):\n        """"""\n        The main loop of the run.\n        Defined in the following steps:\n        1. Heatup\n        2. Repeat:\n            2.1. Repeat:\n                2.1.1. Train\n                2.1.2. Possibly save checkpoint\n            2.2. Evaluate\n        :return: None\n        """"""\n\n        self.verify_graph_was_created()\n\n        # initialize the network parameters from the global network\n        self.sync()\n\n        # If we have both an environment and a dataset to load from, we will use the environment only for\n        # evaluating the policy, and will not run heatup. If no dataset is available to load from, we will be collecting\n        # a dataset from an environment.\n        if not self.agent_params.memory.load_memory_from_file_path:\n            if self.is_collecting_random_dataset:\n                # heatup\n                if self.env_params is not None:\n                    screen.log_title(\n                        ""Collecting random-action experience to use for training the actual agent in a Batch RL ""\n                        ""fashion"")\n                    # Creating a random dataset during the heatup phase is useful mainly for tutorial and debug\n                    # purposes.\n                    self.heatup(self.heatup_steps)\n            else:\n                screen.log_title(\n                    ""Starting to improve an agent collecting experience to use for training the actual agent in a ""\n                    ""Batch RL fashion"")\n\n                # set the experience generating agent to train\n                self.level_managers[0].agents = {\'experience_generating_agent\': self.experience_generating_agent}\n\n                # collect a dataset using the experience generating agent\n                super().improve()\n\n                # set the acquired experience to the actual agent that we\'re going to train\n                self.agent.memory = self.experience_generating_agent.memory\n\n                # switch the graph scheduling parameters\n                self.set_schedule_params(self.schedule_params)\n\n                # set the actual agent to train\n                self.level_managers[0].agents = {\'agent\': self.agent}\n\n        # this agent never actually plays\n        self.level_managers[0].agents[\'agent\'].ap.algorithm.num_consecutive_playing_steps = EnvironmentSteps(0)\n\n        # from this point onwards, the dataset cannot be changed anymore. Allows for performance improvements.\n        self.level_managers[0].agents[\'agent\'].freeze_memory()\n\n        self.initialize_ope_models_and_stats()\n\n        # improve\n        if self.task_parameters.task_index is not None:\n            screen.log_title(""Starting to improve {} task index {}"".format(self.name, self.task_parameters.task_index))\n        else:\n            screen.log_title(""Starting to improve {}"".format(self.name))\n\n        # the outer most training loop\n        improve_steps_end = self.total_steps_counters[RunPhase.TRAIN] + self.improve_steps\n        while self.total_steps_counters[RunPhase.TRAIN] < improve_steps_end:\n            # perform several steps of training\n            if self.steps_between_evaluation_periods.num_steps > 0:\n                with self.phase_context(RunPhase.TRAIN):\n                    self.reset_internal_state(force_environment_reset=True)\n\n                    steps_between_evaluation_periods_end = self.current_step_counter + \\\n                                                           self.steps_between_evaluation_periods\n                    while self.current_step_counter < steps_between_evaluation_periods_end:\n                        self.train()\n\n            # the output of batch RL training is always a checkpoint of the trained agent. we always save a checkpoint,\n            # each epoch, regardless of the user\'s command line arguments.\n            self.save_checkpoint()\n\n            # run off-policy evaluation estimators to evaluate the agent\'s performance against the dataset\n            self.run_off_policy_evaluation()\n\n            if self.env_params is not None and self.evaluate(self.evaluation_steps):\n                # if we do have a simulator (although we are in a batch RL setting we might have a simulator, e.g. when\n                # demonstrating the batch RL use-case using one of the existing Coach environments),\n                # we might want to evaluate vs. the simulator every now and then.\n                break\n\n    def initialize_ope_models_and_stats(self):\n        """"""\n        Improve a reward model of the MDP, to be used for some of the off-policy evaluation (OPE) methods.\n        e.g. \'direct method\' and \'doubly robust\'.\n        """"""\n        agent = self.level_managers[0].agents[\'agent\']\n\n        # prepare dataset to be consumed in the expected formats for OPE\n        agent.memory.prepare_evaluation_dataset()\n\n        screen.log_title(""Training a regression model for estimating MDP rewards"")\n        agent.improve_reward_model(epochs=self.reward_model_num_epochs)\n\n        screen.log_title(""Collecting static statistics for OPE"")\n        agent.ope_manager.gather_static_shared_stats(evaluation_dataset_as_transitions=\n                                                     agent.memory.evaluation_dataset_as_transitions,\n                                                     batch_size=agent.ap.network_wrappers[\'main\'].batch_size,\n                                                     reward_model=agent.networks[\'reward_model\'].online_network,\n                                                     network_keys=list(agent.ap.network_wrappers[\'main\'].\n                                                                       input_embedders_parameters.keys()))\n\n    def run_off_policy_evaluation(self):\n        """"""\n        Run off-policy evaluation estimators to evaluate the trained policy performance against the dataset\n        :return:\n        """"""\n        self.level_managers[0].agents[\'agent\'].run_off_policy_evaluation()\n'"
rl_coach/graph_managers/graph_manager.py,5,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nimport os\nimport time\nfrom collections import OrderedDict\nfrom distutils.dir_util import copy_tree, remove_tree\nfrom typing import List, Tuple\nimport contextlib\n\nfrom rl_coach.base_parameters import iterable_to_items, TaskParameters, DistributedTaskParameters, Frameworks, \\\n    VisualizationParameters, Parameters, PresetValidationParameters, RunType\nfrom rl_coach.checkpoint import CheckpointStateUpdater, get_checkpoint_state, SingleCheckpoint, CheckpointState\nfrom rl_coach.core_types import TotalStepsCounter, RunPhase, PlayingStepsType, TrainingSteps, EnvironmentEpisodes, \\\n    EnvironmentSteps, StepMethod, Transition\nfrom rl_coach.environments.environment import Environment\nfrom rl_coach.level_manager import LevelManager\nfrom rl_coach.logger import screen, Logger\nfrom rl_coach.saver import SaverCollection\nfrom rl_coach.utils import set_cpu, start_shell_command_and_wait\nfrom rl_coach.data_stores.data_store_impl import get_data_store as data_store_creator\nfrom rl_coach.memories.backend.memory_impl import get_memory_backend\nfrom rl_coach.data_stores.data_store import SyncFiles\nfrom rl_coach.checkpoint import CheckpointStateReader\n\nfrom rl_coach.core_types import TimeTypes\n\n\nclass ScheduleParameters(Parameters):\n    def __init__(self):\n        super().__init__()\n        self.heatup_steps = None\n        self.evaluation_steps = None\n        self.steps_between_evaluation_periods = None\n        self.improve_steps = None\n\n\nclass HumanPlayScheduleParameters(ScheduleParameters):\n    def __init__(self):\n        super().__init__()\n        self.heatup_steps = EnvironmentSteps(0)\n        self.evaluation_steps = EnvironmentEpisodes(0)\n        self.steps_between_evaluation_periods = EnvironmentEpisodes(100000000)\n        self.improve_steps = TrainingSteps(10000000000)\n\n\nclass SimpleScheduleWithoutEvaluation(ScheduleParameters):\n    def __init__(self, improve_steps=TrainingSteps(10000000000)):\n        super().__init__()\n        self.heatup_steps = EnvironmentSteps(0)\n        self.evaluation_steps = EnvironmentEpisodes(0)\n        self.steps_between_evaluation_periods = improve_steps\n        self.improve_steps = improve_steps\n\n\nclass SimpleSchedule(ScheduleParameters):\n    def __init__(self,\n                 improve_steps=TrainingSteps(10000000000),\n                 steps_between_evaluation_periods=EnvironmentEpisodes(50),\n                 evaluation_steps=EnvironmentEpisodes(5)):\n        super().__init__()\n        self.heatup_steps = EnvironmentSteps(0)\n        self.evaluation_steps = evaluation_steps\n        self.steps_between_evaluation_periods = steps_between_evaluation_periods\n        self.improve_steps = improve_steps\n\n\nclass GraphManager(object):\n    """"""\n    A graph manager is responsible for creating and initializing a graph of agents, including all its internal\n    components. It is then used in order to schedule the execution of operations on the graph, such as acting and\n    training.\n    """"""\n    def __init__(self,\n                 name: str,\n                 schedule_params: ScheduleParameters,\n                 vis_params: VisualizationParameters = VisualizationParameters()):\n        self.sess = None\n        self.level_managers = []  # type: List[LevelManager]\n        self.top_level_manager = None\n        self.environments = []\n        self.set_schedule_params(schedule_params)\n        self.visualization_parameters = vis_params\n        self.name = name\n        self.task_parameters = None\n        self._phase = self.phase = RunPhase.UNDEFINED\n        self.preset_validation_params = PresetValidationParameters()\n        self.reset_required = False\n\n        # timers\n        self.graph_creation_time = None\n        self.last_checkpoint_saving_time = time.time()\n\n        # counters\n        self.total_steps_counters = {\n            RunPhase.HEATUP: TotalStepsCounter(),\n            RunPhase.TRAIN: TotalStepsCounter(),\n            RunPhase.TEST: TotalStepsCounter()\n        }\n        self.checkpoint_id = 0\n\n        self.checkpoint_saver = None\n        self.checkpoint_state_updater = None\n        self.graph_logger = Logger()\n        self.data_store = None\n        self.is_batch_rl = False\n        self.time_metric = TimeTypes.EpisodeNumber\n\n    def create_graph(self, task_parameters: TaskParameters=TaskParameters()):\n        # check if create graph has been already called\n        if self.graph_creation_time is not None:\n            return self\n\n        self.graph_creation_time = time.time()\n        self.task_parameters = task_parameters\n\n        if isinstance(task_parameters, DistributedTaskParameters):\n            screen.log_title(""Creating graph - name: {} task id: {} type: {}"".format(self.__class__.__name__,\n                                                                                     task_parameters.task_index,\n                                                                                     task_parameters.job_type))\n        else:\n            screen.log_title(""Creating graph - name: {}"".format(self.__class__.__name__))\n\n        # ""hide"" the gpu if necessary\n        if task_parameters.use_cpu:\n            set_cpu()\n\n        # create a target server for the worker and a device\n        if isinstance(task_parameters, DistributedTaskParameters):\n            task_parameters.worker_target, task_parameters.device = \\\n                self.create_worker_or_parameters_server(task_parameters=task_parameters)\n\n        # create the graph modules\n        self.level_managers, self.environments = self._create_graph(task_parameters)\n\n        # set self as the parent of all the level managers\n        self.top_level_manager = self.level_managers[0]\n        for level_manager in self.level_managers:\n            level_manager.parent_graph_manager = self\n\n        # create a session (it needs to be created after all the graph ops were created)\n        self.sess = None\n        self.create_session(task_parameters=task_parameters)\n\n        self._phase = self.phase = RunPhase.UNDEFINED\n\n        self.setup_logger()\n\n        return self\n\n    def _create_graph(self, task_parameters: TaskParameters) -> Tuple[List[LevelManager], List[Environment]]:\n        """"""\n        Create all the graph modules and the graph scheduler\n        :param task_parameters: the parameters of the task\n        :return: the initialized level managers and environments\n        """"""\n        raise NotImplementedError("""")\n\n    @staticmethod\n    def _create_worker_or_parameters_server_tf(task_parameters: DistributedTaskParameters):\n        import tensorflow as tf\n        config = tf.ConfigProto()\n        config.allow_soft_placement = True  # allow placing ops on cpu if they are not fit for gpu\n        config.gpu_options.allow_growth = True  # allow the gpu memory allocated for the worker to grow if needed\n        config.gpu_options.per_process_gpu_memory_fraction = 0.2\n        config.intra_op_parallelism_threads = 1\n        config.inter_op_parallelism_threads = 1\n\n        from rl_coach.architectures.tensorflow_components.distributed_tf_utils import \\\n            create_and_start_parameters_server, \\\n            create_cluster_spec, create_worker_server_and_device\n\n        # create cluster spec\n        cluster_spec = create_cluster_spec(parameters_server=task_parameters.parameters_server_hosts,\n                                           workers=task_parameters.worker_hosts)\n\n        # create and start parameters server (non-returning function) or create a worker and a device setter\n        if task_parameters.job_type == ""ps"":\n            create_and_start_parameters_server(cluster_spec=cluster_spec,\n                                               config=config)\n        elif task_parameters.job_type == ""worker"":\n            return create_worker_server_and_device(cluster_spec=cluster_spec,\n                                                   task_index=task_parameters.task_index,\n                                                   use_cpu=task_parameters.use_cpu,\n                                                   config=config)\n        else:\n            raise ValueError(""The job type should be either ps or worker and not {}""\n                             .format(task_parameters.job_type))\n\n    @staticmethod\n    def create_worker_or_parameters_server(task_parameters: DistributedTaskParameters):\n        if task_parameters.framework_type == Frameworks.tensorflow:\n            return GraphManager._create_worker_or_parameters_server_tf(task_parameters)\n        elif task_parameters.framework_type == Frameworks.mxnet:\n            raise NotImplementedError(\'Distributed training not implemented for MXNet\')\n        else:\n            raise ValueError(\'Invalid framework {}\'.format(task_parameters.framework_type))\n\n    def _create_session_tf(self, task_parameters: TaskParameters):\n        import tensorflow as tf\n        config = tf.ConfigProto()\n        config.allow_soft_placement = True  # allow placing ops on cpu if they are not fit for gpu\n        config.gpu_options.allow_growth = True  # allow the gpu memory allocated for the worker to grow if needed\n        # config.gpu_options.per_process_gpu_memory_fraction = 0.2\n        config.intra_op_parallelism_threads = 1\n        config.inter_op_parallelism_threads = 1\n\n        if isinstance(task_parameters, DistributedTaskParameters):\n            # the distributed tensorflow setting\n            from rl_coach.architectures.tensorflow_components.distributed_tf_utils import create_monitored_session\n            if hasattr(self.task_parameters, \'checkpoint_restore_path\') and self.task_parameters.checkpoint_restore_path:\n                checkpoint_dir = os.path.join(task_parameters.experiment_path, \'checkpoint\')\n                if os.path.exists(checkpoint_dir):\n                    remove_tree(checkpoint_dir)\n                # in the locally distributed case, checkpoints are always restored from a directory (and not from a\n                # file)\n                copy_tree(task_parameters.checkpoint_restore_path, checkpoint_dir)\n            else:\n                checkpoint_dir = task_parameters.checkpoint_save_dir\n\n            self.set_session(create_monitored_session(target=task_parameters.worker_target,\n                                                      task_index=task_parameters.task_index,\n                                                      checkpoint_dir=checkpoint_dir,\n                                                      checkpoint_save_secs=task_parameters.checkpoint_save_secs,\n                                                      config=config))\n        else:\n            # regular session\n            self.set_session(tf.Session(config=config))\n\n        # the TF graph is static, and therefore is saved once - in the beginning of the experiment\n        if hasattr(self.task_parameters, \'checkpoint_save_dir\') and self.task_parameters.checkpoint_save_dir:\n            self.save_graph()\n\n    def _create_session_mx(self):\n        """"""\n        Call set_session to initialize parameters and construct checkpoint_saver\n        """"""\n        self.set_session(sess=None)  # Initialize all modules\n\n    def create_session(self, task_parameters: TaskParameters):\n        if task_parameters.framework_type == Frameworks.tensorflow:\n            self._create_session_tf(task_parameters)\n        elif task_parameters.framework_type == Frameworks.mxnet:\n            self._create_session_mx()\n        else:\n            raise ValueError(\'Invalid framework {}\'.format(task_parameters.framework_type))\n\n        # Create parameter saver\n        self.checkpoint_saver = SaverCollection()\n        for level in self.level_managers:\n            self.checkpoint_saver.update(level.collect_savers())\n        # restore from checkpoint if given\n        self.restore_checkpoint()\n\n    def save_graph(self) -> None:\n        """"""\n        Save the TF graph to a protobuf description file in the experiment directory\n        :return: None\n        """"""\n        import tensorflow as tf\n\n        # write graph\n        tf.train.write_graph(tf.get_default_graph(),\n                             logdir=self.task_parameters.checkpoint_save_dir,\n                             name=\'graphdef.pb\',\n                             as_text=False)\n\n    def _save_onnx_graph_tf(self) -> None:\n        """"""\n        Save the tensorflow graph as an ONNX graph.\n        This requires the graph and the weights checkpoint to be stored in the experiment directory.\n        It then freezes the graph (merging the graph and weights checkpoint), and converts it to ONNX.\n        :return: None\n        """"""\n        # collect input and output nodes\n        input_nodes = []\n        output_nodes = []\n        for level in self.level_managers:\n            for agent in level.agents.values():\n                for network in agent.networks.values():\n                    for input_key, input in network.online_network.inputs.items():\n                        if not input_key.startswith(""output_""):\n                            input_nodes.append(input.name)\n                    for output in network.online_network.outputs:\n                        output_nodes.append(output.name)\n\n        from rl_coach.architectures.tensorflow_components.architecture import save_onnx_graph\n\n        save_onnx_graph(input_nodes, output_nodes, self.task_parameters.checkpoint_save_dir)\n\n    def save_onnx_graph(self) -> None:\n        """"""\n        Save the graph as an ONNX graph.\n        This requires the graph and the weights checkpoint to be stored in the experiment directory.\n        It then freezes the graph (merging the graph and weights checkpoint), and converts it to ONNX.\n        :return: None\n        """"""\n        if self.task_parameters.framework_type == Frameworks.tensorflow:\n            self._save_onnx_graph_tf()\n\n    def setup_logger(self) -> None:\n        # dump documentation\n        logger_prefix = ""{graph_name}"".format(graph_name=self.name)\n        self.graph_logger.set_logger_filenames(self.task_parameters.experiment_path, logger_prefix=logger_prefix,\n                                               add_timestamp=True, task_id=self.task_parameters.task_index)\n        if self.visualization_parameters.dump_parameters_documentation:\n            self.graph_logger.dump_documentation(str(self))\n        [manager.setup_logger() for manager in self.level_managers]\n\n    @property\n    def phase(self) -> RunPhase:\n        """"""\n        Get the phase of the graph\n        :return: the current phase\n        """"""\n        return self._phase\n\n    @phase.setter\n    def phase(self, val: RunPhase):\n        """"""\n        Change the phase of the graph and all the hierarchy levels below it\n        :param val: the new phase\n        :return: None\n        """"""\n        self._phase = val\n        for level_manager in self.level_managers:\n            level_manager.phase = val\n        for environment in self.environments:\n            environment.phase = val\n\n    @property\n    def current_step_counter(self) -> TotalStepsCounter:\n        return self.total_steps_counters[self.phase]\n\n    @contextlib.contextmanager\n    def phase_context(self, phase):\n        """"""\n        Create a context which temporarily sets the phase to the provided phase.\n        The previous phase is restored afterwards.\n        """"""\n        old_phase = self.phase\n        self.phase = phase\n        yield\n        self.phase = old_phase\n\n    def set_session(self, sess) -> None:\n        """"""\n        Set the deep learning framework session for all the modules in the graph\n        :return: None\n        """"""\n        self.sess = sess\n\n        [manager.set_session(sess) for manager in self.level_managers]\n\n    def heatup(self, steps: PlayingStepsType) -> None:\n        """"""\n        Perform heatup for several steps, which means taking random actions and storing the results in memory\n        :param steps: the number of steps as a tuple of steps time and steps count\n        :return: None\n        """"""\n        self.verify_graph_was_created()\n\n        if steps.num_steps > 0:\n            with self.phase_context(RunPhase.HEATUP):\n                screen.log_title(""{}: Starting heatup"".format(self.name))\n\n                # reset all the levels before starting to heatup\n                self.reset_internal_state(force_environment_reset=True)\n\n                # act for at least steps, though don\'t interrupt an episode\n                count_end = self.current_step_counter + steps\n                while self.current_step_counter < count_end:\n                    self.act(EnvironmentEpisodes(1))\n\n    def handle_episode_ended(self) -> None:\n        """"""\n        End an episode and reset all the episodic parameters\n        :return: None\n        """"""\n        self.current_step_counter[EnvironmentEpisodes] += 1\n\n        [environment.handle_episode_ended() for environment in self.environments]\n\n    def train(self) -> None:\n        """"""\n        Perform several training iterations for all the levels in the hierarchy\n        :param steps: number of training iterations to perform\n        :return: None\n        """"""\n        self.verify_graph_was_created()\n\n        with self.phase_context(RunPhase.TRAIN):\n            self.current_step_counter[TrainingSteps] += 1\n            [manager.train() for manager in self.level_managers]\n\n    def reset_internal_state(self, force_environment_reset=False) -> None:\n        """"""\n        Reset an episode for all the levels\n        :param force_environment_reset: force the environment to reset the episode even if it has some conditions that\n                                        tell it not to. for example, if ale life is lost, gym will tell the agent that\n                                        the episode is finished but won\'t actually reset the episode if there are more\n                                        lives available\n        :return: None\n        """"""\n        self.verify_graph_was_created()\n\n        self.reset_required = False\n        [environment.reset_internal_state(force_environment_reset) for environment in self.environments]\n        [manager.reset_internal_state() for manager in self.level_managers]\n\n    def act(self, steps: PlayingStepsType, wait_for_full_episodes=False) -> None:\n        """"""\n        Do several steps of acting on the environment\n        :param wait_for_full_episodes: if set, act for at least `steps`, but make sure that the last episode is complete\n        :param steps: the number of steps as a tuple of steps time and steps count\n        """"""\n        self.verify_graph_was_created()\n\n        if hasattr(self, \'data_store_params\') and hasattr(self.agent_params.memory, \'memory_backend_params\'):\n            if self.agent_params.memory.memory_backend_params.run_type == str(RunType.ROLLOUT_WORKER):\n                data_store = self.get_data_store(self.data_store_params)\n                data_store.load_from_store()\n\n        # perform several steps of playing\n        count_end = self.current_step_counter + steps\n        result = None\n        while self.current_step_counter < count_end or (wait_for_full_episodes and result is not None and not result.game_over):\n            # reset the environment if the previous episode was terminated\n            if self.reset_required:\n                self.reset_internal_state()\n\n            steps_begin = self.environments[0].total_steps_counter\n            result = self.top_level_manager.step(None)\n            steps_end = self.environments[0].total_steps_counter\n\n            if result.game_over:\n                self.handle_episode_ended()\n                self.reset_required = True\n\n            self.current_step_counter[EnvironmentSteps] += (steps_end - steps_begin)\n\n            # if no steps were made (can happen when no actions are taken while in the TRAIN phase, either in batch RL\n            # or in imitation learning), we force end the loop, so that it will not continue forever.\n            if (steps_end - steps_begin) == 0:\n                break\n\n    def train_and_act(self, steps: StepMethod) -> None:\n        """"""\n        Train the agent by doing several acting steps followed by several training steps continually\n        :param steps: the number of steps as a tuple of steps time and steps count\n        :return: None\n        """"""\n        self.verify_graph_was_created()\n\n        # perform several steps of training interleaved with acting\n        if steps.num_steps > 0:\n            with self.phase_context(RunPhase.TRAIN):\n                self.reset_internal_state(force_environment_reset=True)\n\n                count_end = self.current_step_counter + steps\n                while self.current_step_counter < count_end:\n                    # The actual number of steps being done on the environment\n                    # is decided by the agent, though this inner loop always\n                    # takes at least one step in the environment (at the GraphManager level).\n                    # The agent might also decide to skip acting altogether.\n                    # Depending on internal counters and parameters, it doesn\'t always train or save checkpoints.\n                    self.act(EnvironmentSteps(1))\n                    self.train()\n                    self.occasionally_save_checkpoint()\n\n    def sync(self) -> None:\n        """"""\n        Sync the global network parameters to the graph\n        :return:\n        """"""\n        [manager.sync() for manager in self.level_managers]\n\n    def evaluate(self, steps: PlayingStepsType) -> bool:\n        """"""\n        Perform evaluation for several steps\n        :param steps: the number of steps as a tuple of steps time and steps count\n        :return: bool, True if the target reward and target success has been reached\n        """"""\n        self.verify_graph_was_created()\n\n        if steps.num_steps > 0:\n            with self.phase_context(RunPhase.TEST):\n                # reset all the levels before starting to evaluate\n                self.reset_internal_state(force_environment_reset=True)\n                self.sync()\n\n                # act for at least `steps`, though don\'t interrupt an episode\n                count_end = self.current_step_counter + steps\n                while self.current_step_counter < count_end:\n                    self.act(EnvironmentEpisodes(1))\n                    self.sync()\n        if self.should_stop():\n            self.flush_finished()\n            screen.success(""Reached required success rate. Exiting."")\n            return True\n        return False\n\n    def improve(self):\n        """"""\n        The main loop of the run.\n        Defined in the following steps:\n        1. Heatup\n        2. Repeat:\n            2.1. Repeat:\n                2.1.1. Act\n                2.1.2. Train\n                2.1.3. Possibly save checkpoint\n            2.2. Evaluate\n        :return: None\n        """"""\n\n        self.verify_graph_was_created()\n\n        # initialize the network parameters from the global network\n        self.sync()\n\n        # heatup\n        self.heatup(self.heatup_steps)\n\n        # improve\n        if self.task_parameters.task_index is not None:\n            screen.log_title(""Starting to improve {} task index {}"".format(self.name, self.task_parameters.task_index))\n        else:\n            screen.log_title(""Starting to improve {}"".format(self.name))\n\n        count_end = self.total_steps_counters[RunPhase.TRAIN] + self.improve_steps\n        while self.total_steps_counters[RunPhase.TRAIN] < count_end:\n            self.train_and_act(self.steps_between_evaluation_periods)\n            if self.evaluate(self.evaluation_steps):\n                break\n\n    def restore_checkpoint(self):\n        self.verify_graph_was_created()\n\n        # TODO: find better way to load checkpoints that were saved with a global network into the online network\n        if self.task_parameters.checkpoint_restore_path:\n            if os.path.isdir(self.task_parameters.checkpoint_restore_path):\n                # a checkpoint dir\n                if self.task_parameters.framework_type == Frameworks.tensorflow and\\\n                        \'checkpoint\' in os.listdir(self.task_parameters.checkpoint_restore_path):\n                    # TODO-fixme checkpointing\n                    # MonitoredTrainingSession manages save/restore checkpoints autonomously. Doing so,\n                    # it creates it own names for the saved checkpoints, which do not match the ""{}_Step-{}.ckpt""\n                    # filename pattern. The names used are maintained in a CheckpointState protobuf file named\n                    # \'checkpoint\'. Using Coach\'s \'.coach_checkpoint\' protobuf file, results in an error when trying to\n                    # restore the model, as the checkpoint names defined do not match the actual checkpoint names.\n                    checkpoint = self._get_checkpoint_state_tf(self.task_parameters.checkpoint_restore_path)\n                else:\n                    checkpoint = get_checkpoint_state(self.task_parameters.checkpoint_restore_path)\n\n                if checkpoint is None:\n                    raise ValueError(""No checkpoint to restore in: {}"".format(\n                        self.task_parameters.checkpoint_restore_path))\n                model_checkpoint_path = checkpoint.model_checkpoint_path\n                checkpoint_restore_dir = self.task_parameters.checkpoint_restore_path\n\n                # Set the last checkpoint ID - only in the case of the path being a dir\n                chkpt_state_reader = CheckpointStateReader(self.task_parameters.checkpoint_restore_path,\n                                                           checkpoint_state_optional=False)\n                self.checkpoint_id = chkpt_state_reader.get_latest().num + 1\n            else:\n                # a checkpoint file\n                if self.task_parameters.framework_type == Frameworks.tensorflow:\n                    model_checkpoint_path = self.task_parameters.checkpoint_restore_path\n                    checkpoint_restore_dir = os.path.dirname(model_checkpoint_path)\n                else:\n                    raise ValueError(""Currently restoring a checkpoint using the --checkpoint_restore_file argument is""\n                                     "" only supported when with tensorflow."")\n\n            screen.log_title(""Loading checkpoint: {}"".format(model_checkpoint_path))\n\n            self.checkpoint_saver.restore(self.sess, model_checkpoint_path)\n\n            [manager.restore_checkpoint(checkpoint_restore_dir) for manager in self.level_managers]\n\n    def _get_checkpoint_state_tf(self, checkpoint_restore_dir):\n        import tensorflow as tf\n        return tf.train.get_checkpoint_state(checkpoint_restore_dir)\n\n    def occasionally_save_checkpoint(self):\n        # only the chief process saves checkpoints\n        if self.task_parameters.checkpoint_save_secs \\\n                and time.time() - self.last_checkpoint_saving_time >= self.task_parameters.checkpoint_save_secs \\\n                and (self.task_parameters.task_index == 0  # distributed\n                     or self.task_parameters.task_index is None  # single-worker\n                     ):\n            self.save_checkpoint()\n\n    def save_checkpoint(self):\n        # create current session\'s checkpoint directory\n        if self.task_parameters.checkpoint_save_dir is None:\n            self.task_parameters.checkpoint_save_dir = os.path.join(self.task_parameters.experiment_path, \'checkpoint\')\n\n        if not os.path.exists(self.task_parameters.checkpoint_save_dir):\n            os.mkdir(self.task_parameters.checkpoint_save_dir)  # Create directory structure\n\n        if self.checkpoint_state_updater is None:\n            self.checkpoint_state_updater = CheckpointStateUpdater(self.task_parameters.checkpoint_save_dir)\n\n        checkpoint_name = ""{}_Step-{}.ckpt"".format(\n            self.checkpoint_id, self.total_steps_counters[RunPhase.TRAIN][EnvironmentSteps])\n        checkpoint_path = os.path.join(self.task_parameters.checkpoint_save_dir, checkpoint_name)\n\n        if not isinstance(self.task_parameters, DistributedTaskParameters):\n            saved_checkpoint_path = self.checkpoint_saver.save(self.sess, checkpoint_path)\n        else:\n            saved_checkpoint_path = checkpoint_path\n\n        # this is required in order for agents to save additional information like a DND for example\n        [manager.save_checkpoint(checkpoint_name) for manager in self.level_managers]\n\n        # the ONNX graph will be stored only if checkpoints are stored and the -onnx flag is used\n        if self.task_parameters.export_onnx_graph:\n            self.save_onnx_graph()\n\n        # write the new checkpoint name to a file to signal this checkpoint has been fully saved\n        self.checkpoint_state_updater.update(SingleCheckpoint(self.checkpoint_id, checkpoint_name))\n\n        screen.log_dict(\n            OrderedDict([\n                (""Saving in path"", saved_checkpoint_path),\n            ]),\n            prefix=""Checkpoint""\n        )\n\n        self.checkpoint_id += 1\n        self.last_checkpoint_saving_time = time.time()\n\n        if hasattr(self, \'data_store_params\'):\n            data_store = self.get_data_store(self.data_store_params)\n            data_store.save_to_store()\n\n    def verify_graph_was_created(self):\n        """"""\n        Verifies that the graph was already created, and if not, it creates it with the default task parameters\n        :return: None\n        """"""\n        if self.graph_creation_time is None:\n            self.create_graph()\n\n    def __str__(self):\n        result = """"\n        for key, val in self.__dict__.items():\n            params = """"\n            if isinstance(val, list) or isinstance(val, dict) or isinstance(val, OrderedDict):\n                items = iterable_to_items(val)\n                for k, v in items:\n                    params += ""{}: {}\\n"".format(k, v)\n            else:\n                params = val\n            result += ""{}: \\n{}\\n"".format(key, params)\n\n        return result\n\n    def should_train(self) -> bool:\n        return any([manager.should_train() for manager in self.level_managers])\n\n    # TODO-remove - this is a temporary flow, used by the trainer worker, duplicated from observe() - need to create\n    #               an external trainer flow reusing the existing flow and methods [e.g. observe(), step(), act()]\n    def emulate_act_on_trainer(self, steps: PlayingStepsType, transition: Transition) -> None:\n        """"""\n        This emulates the act using the transition obtained from the rollout worker on the training worker\n        in case of distributed training.\n        Do several steps of acting on the environment\n        :param steps: the number of steps as a tuple of steps time and steps count\n        """"""\n        self.verify_graph_was_created()\n\n        # perform several steps of playing\n        count_end = self.current_step_counter + steps\n        while self.current_step_counter < count_end:\n            # reset the environment if the previous episode was terminated\n            if self.reset_required:\n                self.reset_internal_state()\n\n            steps_begin = self.environments[0].total_steps_counter\n            self.top_level_manager.emulate_step_on_trainer(transition)\n            steps_end = self.environments[0].total_steps_counter\n\n            # add the diff between the total steps before and after stepping, such that environment initialization steps\n            # (like in Atari) will not be counted.\n            # We add at least one step so that even if no steps were made (in case no actions are taken in the training\n            # phase), the loop will end eventually.\n            self.current_step_counter[EnvironmentSteps] += max(1, steps_end - steps_begin)\n\n            if transition.game_over:\n                self.handle_episode_ended()\n                self.reset_required = True\n\n    def fetch_from_worker(self, num_consecutive_playing_steps=None):\n        if hasattr(self, \'memory_backend\'):\n            with self.phase_context(RunPhase.TRAIN):\n                for transition in self.memory_backend.fetch(num_consecutive_playing_steps):\n                    self.emulate_act_on_trainer(EnvironmentSteps(1), transition)\n\n    def setup_memory_backend(self) -> None:\n        if hasattr(self.agent_params.memory, \'memory_backend_params\'):\n            self.memory_backend = get_memory_backend(self.agent_params.memory.memory_backend_params)\n\n    def should_stop(self) -> bool:\n        return self.task_parameters.apply_stop_condition and all([manager.should_stop() for manager in self.level_managers])\n\n    def get_data_store(self, param):\n        if self.data_store:\n            return self.data_store\n\n        return data_store_creator(param)\n\n    def signal_ready(self):\n        if self.task_parameters.checkpoint_save_dir and os.path.exists(self.task_parameters.checkpoint_save_dir):\n                open(os.path.join(self.task_parameters.checkpoint_save_dir, SyncFiles.TRAINER_READY.value), \'w\').close()\n        if hasattr(self, \'data_store_params\'):\n                data_store = self.get_data_store(self.data_store_params)\n                data_store.save_to_store()\n\n    def close(self) -> None:\n        """"""\n        Clean up to close environments.\n\n        :return: None\n        """"""\n        for env in self.environments:\n            env.close()\n\n    def get_current_episodes_count(self):\n        """"""\n        Returns the current EnvironmentEpisodes counter\n        """"""\n        return self.current_step_counter[EnvironmentEpisodes]\n\n    def flush_finished(self):\n        """"""\n        To indicate the training has finished, writes a `.finished` file to the checkpoint directory and calls\n        the data store to updload that file.\n        """"""\n        if self.task_parameters.checkpoint_save_dir and os.path.exists(self.task_parameters.checkpoint_save_dir):\n            open(os.path.join(self.task_parameters.checkpoint_save_dir, SyncFiles.FINISHED.value), \'w\').close()\n        if hasattr(self, \'data_store_params\'):\n            data_store = self.get_data_store(self.data_store_params)\n            data_store.save_to_store()\n\n    def set_schedule_params(self, schedule_params: ScheduleParameters):\n        """"""\n        Set schedule parameters for the graph.\n\n        :param schedule_params: the schedule params to set.\n        """"""\n        self.heatup_steps = schedule_params.heatup_steps\n        self.evaluation_steps = schedule_params.evaluation_steps\n        self.steps_between_evaluation_periods = schedule_params.steps_between_evaluation_periods\n        self.improve_steps = schedule_params.improve_steps\n'"
rl_coach/graph_managers/hac_graph_manager.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom typing import List, Union, Tuple\n\nfrom rl_coach.base_parameters import AgentParameters, VisualizationParameters, TaskParameters, \\\n    PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.environments.environment import EnvironmentParameters, Environment\nfrom rl_coach.graph_managers.graph_manager import GraphManager, ScheduleParameters\nfrom rl_coach.level_manager import LevelManager\nfrom rl_coach.utils import short_dynamic_import\n\n\nclass HACGraphManager(GraphManager):\n    """"""\n    A simple HAC graph manager creates a deep hierarchy with a single agent per hierarchy level, and a single\n    environment (on the bottom layer) which is interacted with.\n    """"""\n    def __init__(self, agents_params: List[AgentParameters], env_params: EnvironmentParameters,\n                 schedule_params: ScheduleParameters, vis_params: VisualizationParameters,\n                 consecutive_steps_to_run_non_top_levels: Union[EnvironmentSteps, List[EnvironmentSteps]],\n                 preset_validation_params: PresetValidationParameters = PresetValidationParameters()):\n        """"""\n        :param agents_params: the parameters of all the agents in the hierarchy starting from the top level of the\n                              hierarchy to the bottom level\n        :param env_params: the parameters of the environment\n        :param schedule_params: the parameters for scheduling the graph\n        :param vis_params: the visualization parameters\n        :param consecutive_steps_to_run_non_top_levels: the number of time steps that each level is ran.\n            for example, when the top level gives the bottom level a goal, the bottom level can act for\n            consecutive_steps_to_run_each_level steps and try to reach that goal. This is expected to be either\n            an EnvironmentSteps which will be used for all levels, or an EnvironmentSteps for each level as a list.\n        """"""\n        super().__init__(\'hac_graph\', schedule_params, vis_params)\n        self.agents_params = agents_params\n        self.env_params = env_params\n        self.preset_validation_params = preset_validation_params\n        self.should_test_current_sub_goal = None  # will be filled by the top level agent, and is used by all levels\n\n        if isinstance(consecutive_steps_to_run_non_top_levels, list):\n            if len(consecutive_steps_to_run_non_top_levels) != len(self.agents_params):\n                raise ValueError(""If the consecutive_steps_to_run_each_level is given as a list, it should match ""\n                                 ""the number of levels in the hierarchy. Alternatively, it is possible to use a single ""\n                                 ""value for all the levels, by passing an EnvironmentSteps"")\n        elif isinstance(consecutive_steps_to_run_non_top_levels, EnvironmentSteps):\n            self.consecutive_steps_to_run_non_top_levels = consecutive_steps_to_run_non_top_levels\n\n        for agent_params in agents_params:\n            agent_params.visualization = self.visualization_parameters\n            if agent_params.input_filter is None:\n                agent_params.input_filter = self.env_params.default_input_filter()\n            if agent_params.output_filter is None:\n                agent_params.output_filter = self.env_params.default_output_filter()\n\n        if len(self.agents_params) < 2:\n            raise ValueError(""The HAC graph manager must receive the agent parameters for at least two levels of the ""\n                             ""hierarchy. Otherwise, use the basic RL graph manager."")\n\n    def _create_graph(self, task_parameters: TaskParameters) -> Tuple[List[LevelManager], List[Environment]]:\n        env = short_dynamic_import(self.env_params.path)(**self.env_params.__dict__,\n                                                         visualization_parameters=self.visualization_parameters)\n\n        for agent_params in self.agents_params:\n            agent_params.task_parameters = task_parameters\n\n        # we need to build the hierarchy in reverse order (from the bottom up) in order for the spaces of each level\n        # to be known\n        level_managers = []\n        current_env = env\n        # out_action_space = env.action_space\n        for level_idx, agent_params in reversed(list(enumerate(self.agents_params))):\n            agent_params.name = ""agent_{}"".format(level_idx)\n            agent_params.is_a_highest_level_agent = level_idx == 0\n            agent_params.is_a_lowest_level_agent = level_idx == len(self.agents_params) - 1\n\n            agent = short_dynamic_import(agent_params.path)(agent_params)\n\n            level_manager = LevelManager(\n                agents=agent,\n                environment=current_env,\n                real_environment=env,\n                steps_limit=EnvironmentSteps(1) if level_idx == 0\n                            else self.consecutive_steps_to_run_non_top_levels,\n                should_reset_agent_state_after_time_limit_passes=level_idx > 0,\n                name=""level_{}"".format(level_idx)\n            )\n            current_env = level_manager\n            level_managers.insert(0, level_manager)\n\n        return level_managers, [env]\n\n\n\n'"
rl_coach/graph_managers/hrl_graph_manager.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List, Union, Tuple\n\nfrom rl_coach.base_parameters import AgentParameters, VisualizationParameters, TaskParameters, \\\n    PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.environments.environment import EnvironmentParameters, Environment\nfrom rl_coach.graph_managers.graph_manager import GraphManager, ScheduleParameters\nfrom rl_coach.level_manager import LevelManager\nfrom rl_coach.utils import short_dynamic_import\n\n\nclass HRLGraphManager(GraphManager):\n    """"""\n    A simple HRL graph manager creates a deep hierarchy with a single composite agent per hierarchy level, and a single\n    environment which is interacted with.\n    """"""\n    def __init__(self, agents_params: List[AgentParameters], env_params: EnvironmentParameters,\n                 schedule_params: ScheduleParameters, vis_params: VisualizationParameters,\n                 consecutive_steps_to_run_each_level: Union[EnvironmentSteps, List[EnvironmentSteps]],\n                 preset_validation_params: PresetValidationParameters = PresetValidationParameters()):\n        """"""\n        :param agents_params: the parameters of all the agents in the hierarchy starting from the top level of the\n                              hierarchy to the bottom level\n        :param env_params: the parameters of the environment\n        :param schedule_params: the parameters for scheduling the graph\n        :param vis_params: the visualization parameters\n        :param consecutive_steps_to_run_each_level: the number of time steps that each level is ran.\n            for example, when the top level gives the bottom level a goal, the bottom level can act for\n            consecutive_steps_to_run_each_level steps and try to reach that goal. This is expected to be either\n            an EnvironmentSteps which will be used for all levels, or an EnvironmentSteps for each level as a list.\n        """"""\n        super().__init__(\'hrl_graph\', schedule_params, vis_params)\n        self.agents_params = agents_params\n        self.env_params = env_params\n        self.preset_validation_params = preset_validation_params\n        if isinstance(consecutive_steps_to_run_each_level, list):\n            if len(consecutive_steps_to_run_each_level) != len(self.agents_params):\n                raise ValueError(""If the consecutive_steps_to_run_each_level is given as a list, it should match ""\n                                 ""the number of levels in the hierarchy. Alternatively, it is possible to use a single ""\n                                 ""value for all the levels, by passing an EnvironmentSteps"")\n        elif isinstance(consecutive_steps_to_run_each_level, EnvironmentSteps):\n            self.consecutive_steps_to_run_each_level = [consecutive_steps_to_run_each_level] * len(self.agents_params)\n\n        for agent_params in agents_params:\n            agent_params.visualization = self.visualization_parameters\n            if agent_params.input_filter is None:\n                agent_params.input_filter = self.env_params.default_input_filter()\n            if agent_params.output_filter is None:\n                agent_params.output_filter = self.env_params.default_output_filter()\n\n        if len(self.agents_params) < 2:\n            raise ValueError(""The HRL graph manager must receive the agent parameters for at least two levels of the ""\n                             ""hierarchy. Otherwise, use the basic RL graph manager."")\n\n    def _create_graph(self, task_parameters: TaskParameters) -> Tuple[List[LevelManager], List[Environment]]:\n        self.env_params.seed = task_parameters.seed\n        env = short_dynamic_import(self.env_params.path)(**self.env_params.__dict__,\n                                                         visualization_parameters=self.visualization_parameters)\n\n        for agent_params in self.agents_params:\n            agent_params.task_parameters = task_parameters\n\n        # we need to build the hierarchy in reverse order (from the bottom up) in order for the spaces of each level\n        # to be known\n        level_managers = []\n        current_env = env\n        # out_action_space = env.action_space\n        for level_idx, agent_params in reversed(list(enumerate(self.agents_params))):\n            # TODO: the code below is specific for HRL on observation scale\n            # in action space\n            # if level_idx == 0:\n            #     # top level agents do not get directives\n            #     in_action_space = None\n            # else:\n            #     pass\n\n                # attention_size = (env.state_space[\'observation\'].shape - 1)//4\n                # in_action_space = AttentionActionSpace(shape=2, low=0, high=env.state_space[\'observation\'].shape - 1,\n                #                             forced_attention_size=attention_size)\n                # agent_params.output_filter.action_filters[\'masking\'].set_masking(0, attention_size)\n\n            agent_params.name = ""agent_{}"".format(level_idx)\n            agent_params.is_a_highest_level_agent = level_idx == 0\n            agent = short_dynamic_import(agent_params.path)(agent_params)\n\n            level_manager = LevelManager(\n                agents=agent,\n                environment=current_env,\n                real_environment=env,\n                steps_limit=self.consecutive_steps_to_run_each_level[level_idx],\n                should_reset_agent_state_after_time_limit_passes=level_idx > 0,\n                name=""level_{}"".format(level_idx)\n            )\n            current_env = level_manager\n            level_managers.insert(0, level_manager)\n\n            # out_action_space = in_action_space\n\n        return level_managers, [env]\n\n\n'"
rl_coach/memories/__init__.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
rl_coach/memories/memory.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom enum import Enum\nfrom typing import Tuple\n\nfrom rl_coach.base_parameters import Parameters\nfrom rl_coach.memories.backend.memory import MemoryBackend\n\n\nclass MemoryGranularity(Enum):\n    Transitions = 0\n    Episodes = 1\n\n\nclass MemoryParameters(Parameters):\n    def __init__(self):\n        super().__init__()\n        self.max_size = None\n        self.shared_memory = False\n        self.load_memory_from_file_path = None\n\n    @property\n    def path(self):\n        return \'rl_coach.memories.memory:Memory\'\n\n\nclass Memory(object):\n    def __init__(self, max_size: Tuple[MemoryGranularity, int]):\n        """"""\n        :param max_size: the maximum number of objects to hold in the memory\n        """"""\n        self.max_size = max_size\n        self._length = 0\n        self.memory_backend = None\n\n    def store(self, obj):\n        if self.memory_backend:\n            self.memory_backend.store(obj)\n\n    def store_episode(self, episode):\n        if self.memory_backend:\n            self.memory_backend.store(episode)\n\n    def get(self, index):\n        raise NotImplementedError("""")\n\n    def length(self):\n        raise NotImplementedError("""")\n\n    def sample(self, size):\n        raise NotImplementedError("""")\n\n    def clean(self):\n        raise NotImplementedError("""")\n\n    def set_memory_backend(self, memory_backend: MemoryBackend):\n        self.memory_backend = memory_backend\n\n    def num_transitions(self) -> int:\n        """"""\n        Get the number of transitions in the ER\n        """"""\n        raise NotImplementedError("""")\n'"
rl_coach/off_policy_evaluators/__init__.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
rl_coach/off_policy_evaluators/ope_manager.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport math\nfrom collections import namedtuple\n\nimport numpy as np\nfrom typing import List\n\nfrom rl_coach.architectures.architecture import Architecture\nfrom rl_coach.core_types import Episode, Batch\nfrom rl_coach.off_policy_evaluators.bandits.doubly_robust import DoublyRobust\nfrom rl_coach.off_policy_evaluators.rl.sequential_doubly_robust import SequentialDoublyRobust\n\nfrom rl_coach.core_types import Transition\n\nfrom rl_coach.off_policy_evaluators.rl.weighted_importance_sampling import WeightedImportanceSampling\n\nOpeSharedStats = namedtuple(""OpeSharedStats"", [\'all_reward_model_rewards\', \'all_policy_probs\',\n                                               \'all_v_values_reward_model_based\', \'all_rewards\', \'all_actions\',\n                                               \'all_old_policy_probs\', \'new_policy_prob\', \'rho_all_dataset\'])\nOpeEstimation = namedtuple(""OpeEstimation"", [\'ips\', \'dm\', \'dr\', \'seq_dr\', \'wis\'])\n\n\nclass OpeManager(object):\n    def __init__(self):\n        self.evaluation_dataset_as_transitions = None\n        self.doubly_robust = DoublyRobust()\n        self.sequential_doubly_robust = SequentialDoublyRobust()\n        self.weighted_importance_sampling = WeightedImportanceSampling()\n        self.all_reward_model_rewards = None\n        self.all_old_policy_probs = None\n        self.all_rewards = None\n        self.all_actions = None\n        self.is_gathered_static_shared_data = False\n\n    def _prepare_ope_shared_stats(self, evaluation_dataset_as_transitions: List[Transition], batch_size: int,\n                                  q_network: Architecture, network_keys: List) -> OpeSharedStats:\n        """"""\n        Do the preparations needed for different estimators.\n        Some of the calcuations are shared, so we centralize all the work here.\n\n        :param evaluation_dataset_as_transitions: The evaluation dataset in the form of transitions.\n        :param batch_size: The batch size to use.\n        :param reward_model: A reward model to be used by DR\n        :param q_network: The Q network whose its policy we evaluate.\n        :param network_keys: The network keys used for feeding the neural networks.\n        :return:\n        """"""\n\n        assert self.is_gathered_static_shared_data, ""gather_static_shared_stats() should be called once before "" \\\n                                                    ""calling _prepare_ope_shared_stats()""\n        # IPS\n        all_policy_probs = []\n        all_v_values_reward_model_based, all_v_values_q_model_based = [], []\n\n        for i in range(math.ceil(len(evaluation_dataset_as_transitions) / batch_size)):\n            batch = evaluation_dataset_as_transitions[i * batch_size: (i + 1) * batch_size]\n            batch_for_inference = Batch(batch)\n\n            # we always use the first Q head to calculate OPEs. might want to change this in the future.\n            # for instance, this means that for bootstrapped dqn we always use the first QHead to calculate the OPEs.\n            q_values, sm_values = q_network.predict(batch_for_inference.states(network_keys),\n                                                    outputs=[q_network.output_heads[0].q_values,\n                                                             q_network.output_heads[0].softmax])\n\n            all_policy_probs.append(sm_values)\n            all_v_values_reward_model_based.append(np.sum(all_policy_probs[-1] * self.all_reward_model_rewards[i],\n                                                          axis=1))\n            all_v_values_q_model_based.append(np.sum(all_policy_probs[-1] * q_values, axis=1))\n\n            for j, t in enumerate(batch):\n                t.update_info({\n                    \'q_value\': q_values[j],\n                    \'softmax_policy_prob\': all_policy_probs[-1][j],\n                    \'v_value_q_model_based\': all_v_values_q_model_based[-1][j],\n\n                })\n\n        all_policy_probs = np.concatenate(all_policy_probs, axis=0)\n        all_v_values_reward_model_based = np.concatenate(all_v_values_reward_model_based, axis=0)\n\n        # generate model probabilities\n        new_policy_prob = all_policy_probs[np.arange(self.all_actions.shape[0]), self.all_actions]\n        rho_all_dataset = new_policy_prob / self.all_old_policy_probs\n\n        return OpeSharedStats(self.all_reward_model_rewards, all_policy_probs, all_v_values_reward_model_based,\n                              self.all_rewards, self.all_actions, self.all_old_policy_probs, new_policy_prob,\n                              rho_all_dataset)\n\n    def gather_static_shared_stats(self, evaluation_dataset_as_transitions: List[Transition], batch_size: int,\n                                   reward_model: Architecture, network_keys: List) -> None:\n        all_reward_model_rewards = []\n        all_old_policy_probs = []\n        all_rewards = []\n        all_actions = []\n\n        for i in range(math.ceil(len(evaluation_dataset_as_transitions) / batch_size)):\n            batch = evaluation_dataset_as_transitions[i * batch_size: (i + 1) * batch_size]\n            batch_for_inference = Batch(batch)\n\n            all_reward_model_rewards.append(reward_model.predict(batch_for_inference.states(network_keys)))\n            all_rewards.append(batch_for_inference.rewards())\n            all_actions.append(batch_for_inference.actions())\n            all_old_policy_probs.append(batch_for_inference.info(\'all_action_probabilities\')\n                                             [range(len(batch_for_inference.actions())),\n                                              batch_for_inference.actions()])\n\n        self.all_reward_model_rewards = np.concatenate(all_reward_model_rewards, axis=0)\n        self.all_old_policy_probs = np.concatenate(all_old_policy_probs, axis=0)\n        self.all_rewards = np.concatenate(all_rewards, axis=0)\n        self.all_actions = np.concatenate(all_actions, axis=0)\n\n        # mark that static shared data was collected and ready to be used\n        self.is_gathered_static_shared_data = True\n\n    def evaluate(self, evaluation_dataset_as_episodes: List[Episode], evaluation_dataset_as_transitions: List[Transition], batch_size: int,\n                 discount_factor: float, q_network: Architecture, network_keys: List) -> OpeEstimation:\n        """"""\n        Run all the OPEs and get estimations of the current policy performance based on the evaluation dataset.\n\n        :param evaluation_dataset_as_episodes: The evaluation dataset in a form of episodes.\n        :param evaluation_dataset_as_transitions: The evaluation dataset in a form of transitions.\n        :param batch_size: Batch size to use for the estimators.\n        :param discount_factor: The standard RL discount factor.\n        :param reward_model: A reward model to be used by DR\n        :param q_network: The Q network whose its policy we evaluate.\n        :param network_keys: The network keys used for feeding the neural networks.\n\n        :return: An OpeEstimation tuple which groups together all the OPE estimations\n        """"""\n        ope_shared_stats = self._prepare_ope_shared_stats(evaluation_dataset_as_transitions, batch_size, q_network,\n                                                          network_keys)\n\n        ips, dm, dr = self.doubly_robust.evaluate(ope_shared_stats)\n        seq_dr = self.sequential_doubly_robust.evaluate(evaluation_dataset_as_episodes, discount_factor)\n        wis = self.weighted_importance_sampling.evaluate(evaluation_dataset_as_episodes)\n        \n        return OpeEstimation(ips, dm, dr, seq_dr, wis)\n\n'"
rl_coach/orchestrators/__init__.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
rl_coach/orchestrators/deploy.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\n\nclass DeployParameters(object):\n\n    def __init__(self):\n        pass\n\n\nclass Deploy(object):\n\n    def __init__(self, deploy_parameters):\n        self.deploy_parameters = deploy_parameters\n\n    def setup(self) -> bool:\n        pass\n\n    def deploy(self) -> bool:\n        pass\n'"
rl_coach/orchestrators/kubernetes_orchestrator.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport os\nimport uuid\nimport json\nimport time\nimport sys\nfrom enum import Enum\nfrom typing import List\nfrom configparser import ConfigParser, Error\nfrom multiprocessing import Process\n\nfrom rl_coach.base_parameters import RunType\nfrom rl_coach.orchestrators.deploy import Deploy, DeployParameters\nfrom kubernetes import client as k8sclient, config as k8sconfig\nfrom rl_coach.memories.backend.memory import MemoryBackendParameters\nfrom rl_coach.memories.backend.memory_impl import get_memory_backend\nfrom rl_coach.data_stores.data_store import DataStoreParameters\nfrom rl_coach.data_stores.data_store_impl import get_data_store\n\n\nclass RunTypeParameters():\n\n    def __init__(self, image: str, command: list(), arguments: list() = None,\n                 run_type: str = str(RunType.TRAINER), checkpoint_dir: str = ""/checkpoint"",\n                 num_replicas: int = 1, orchestration_params: dict=None):\n        self.image = image\n        self.command = command\n        if not arguments:\n            arguments = list()\n        self.arguments = arguments\n        self.run_type = run_type\n        self.checkpoint_dir = checkpoint_dir\n        self.num_replicas = num_replicas\n        if not orchestration_params:\n            orchestration_params = dict()\n        self.orchestration_params = orchestration_params\n\n\nclass KubernetesParameters(DeployParameters):\n\n    def __init__(self, run_type_params: List[RunTypeParameters], kubeconfig: str = None, namespace: str = None,\n                 nfs_server: str = None, nfs_path: str = None, checkpoint_dir: str = \'/checkpoint\',\n                 memory_backend_parameters: MemoryBackendParameters = None, data_store_params: DataStoreParameters = None):\n\n        self.run_type_params = {}\n        for run_type_param in run_type_params:\n            self.run_type_params[run_type_param.run_type] = run_type_param\n        self.kubeconfig = kubeconfig\n        self.namespace = namespace\n        self.nfs_server = nfs_server\n        self.nfs_path = nfs_path\n        self.checkpoint_dir = checkpoint_dir\n        self.memory_backend_parameters = memory_backend_parameters\n        self.data_store_params = data_store_params\n\n\nclass Kubernetes(Deploy):\n    """"""\n    An orchestrator implmentation which uses Kubernetes to deploy the components such as training and rollout workers\n    and Redis Pub/Sub in Coach when used in the distributed mode.\n    """"""\n\n    def __init__(self, params: KubernetesParameters):\n        """"""\n        :param params: The Kubernetes parameters which are used for deploying the components in Coach. These parameters\n        include namespace and kubeconfig.\n        """"""\n\n        super().__init__(params)\n        self.params = params\n        if self.params.kubeconfig:\n            k8sconfig.load_kube_config()\n        else:\n            k8sconfig.load_incluster_config()\n\n        if not self.params.namespace:\n            _, current_context = k8sconfig.list_kube_config_contexts()\n            self.params.namespace = current_context[\'context\'][\'namespace\']\n\n        if os.environ.get(\'http_proxy\'):\n            k8sclient.Configuration._default.proxy = os.environ.get(\'http_proxy\')\n\n        self.params.memory_backend_parameters.orchestrator_params = {\'namespace\': self.params.namespace}\n        self.memory_backend = get_memory_backend(self.params.memory_backend_parameters)\n\n        self.params.data_store_params.orchestrator_params = {\'namespace\': self.params.namespace}\n        self.params.data_store_params.namespace = self.params.namespace\n        self.data_store = get_data_store(self.params.data_store_params)\n\n        if self.params.data_store_params.store_type == ""s3"":\n            self.s3_access_key = None\n            self.s3_secret_key = None\n            if self.params.data_store_params.creds_file:\n                s3config = ConfigParser()\n                s3config.read(self.params.data_store_params.creds_file)\n                try:\n                    self.s3_access_key = s3config.get(\'default\', \'aws_access_key_id\')\n                    self.s3_secret_key = s3config.get(\'default\', \'aws_secret_access_key\')\n                except Error as e:\n                    print(""Error when reading S3 credentials file: %s"", e)\n            else:\n                self.s3_access_key = os.environ.get(\'ACCESS_KEY_ID\')\n                self.s3_secret_key = os.environ.get(\'SECRET_ACCESS_KEY\')\n\n    def setup(self, crd=None) -> bool:\n        """"""\n        Deploys the memory backend and data stores if required.\n        """"""\n\n        self.memory_backend.deploy()\n\n        if self.params.data_store_params.store_type == ""redis"":\n            self.data_store.params.redis_address = self.memory_backend.params.redis_address\n            self.data_store.params.redis_port = self.memory_backend.params.redis_port\n\n        if not self.data_store.deploy():\n            return False\n        if self.params.data_store_params.store_type == ""nfs"":\n            self.nfs_pvc = self.data_store.get_info()\n\n        # Upload checkpoints in checkpoint_restore_dir (if provided) to the data store\n        self.data_store.setup_checkpoint_dir(crd)\n        return True\n\n    def deploy_trainer(self) -> bool:\n        """"""\n        Deploys the training worker in Kubernetes.\n        """"""\n\n        trainer_params = self.params.run_type_params.get(str(RunType.TRAINER), None)\n        if not trainer_params:\n            return False\n\n        trainer_params.command += [\'--memory_backend_params\', json.dumps(self.params.memory_backend_parameters.__dict__)]\n        trainer_params.command += [\'--data_store_params\', json.dumps(self.params.data_store_params.__dict__)]\n        name = ""{}-{}"".format(trainer_params.run_type, uuid.uuid4())\n\n        # TODO: instead of defining each container and template spec from scratch, loaded default\n        # configuration and modify them as necessary depending on the store type\n        if self.params.data_store_params.store_type == ""nfs"":\n            container = k8sclient.V1Container(\n                name=name,\n                image=trainer_params.image,\n                command=trainer_params.command,\n                args=trainer_params.arguments,\n                image_pull_policy=\'Always\',\n                volume_mounts=[k8sclient.V1VolumeMount(\n                    name=\'nfs-pvc\',\n                    mount_path=trainer_params.checkpoint_dir\n                )],\n                stdin=True,\n                tty=True\n            )\n            template = k8sclient.V1PodTemplateSpec(\n                metadata=k8sclient.V1ObjectMeta(labels={\'app\': name}),\n                spec=k8sclient.V1PodSpec(\n                    containers=[container],\n                    volumes=[k8sclient.V1Volume(\n                        name=""nfs-pvc"",\n                        persistent_volume_claim=self.nfs_pvc\n                    )],\n                    restart_policy=\'Never\'\n                ),\n            )\n        elif self.params.data_store_params.store_type == ""s3"":\n            container = k8sclient.V1Container(\n                name=name,\n                image=trainer_params.image,\n                command=trainer_params.command,\n                args=trainer_params.arguments,\n                image_pull_policy=\'Always\',\n                env=[k8sclient.V1EnvVar(""ACCESS_KEY_ID"", self.s3_access_key),\n                     k8sclient.V1EnvVar(""SECRET_ACCESS_KEY"", self.s3_secret_key)],\n                stdin=True,\n                tty=True\n            )\n            template = k8sclient.V1PodTemplateSpec(\n                metadata=k8sclient.V1ObjectMeta(labels={\'app\': name}),\n                spec=k8sclient.V1PodSpec(\n                    containers=[container],\n                    restart_policy=\'Never\'\n                ),\n            )\n        elif self.params.data_store_params.store_type == ""redis"":\n            container = k8sclient.V1Container(\n                name=name,\n                image=trainer_params.image,\n                command=trainer_params.command,\n                args=trainer_params.arguments,\n                image_pull_policy=\'Always\',\n                stdin=True,\n                tty=True,\n                resources=k8sclient.V1ResourceRequirements(\n                    limits={\n                        ""cpu"": ""24"",\n                        ""memory"": ""4Gi"",\n                        ""nvidia.com/gpu"": ""1"",\n                    }\n                ),\n            )\n            template = k8sclient.V1PodTemplateSpec(\n                metadata=k8sclient.V1ObjectMeta(labels={\'app\': name}),\n                spec=k8sclient.V1PodSpec(\n                    containers=[container],\n                    restart_policy=\'Never\'\n                ),\n            )\n        else:\n            raise ValueError(""unexpected store_type {}. expected \'s3\', \'nfs\', \'redis\'"".format(\n                self.params.data_store_params.store_type\n            ))\n\n        job_spec = k8sclient.V1JobSpec(\n            completions=1,\n            template=template\n        )\n\n        job = k8sclient.V1Job(\n            api_version=""batch/v1"",\n            kind=""Job"",\n            metadata=k8sclient.V1ObjectMeta(name=name),\n            spec=job_spec\n        )\n\n        api_client = k8sclient.BatchV1Api()\n        try:\n            api_client.create_namespaced_job(self.params.namespace, job)\n            trainer_params.orchestration_params[\'job_name\'] = name\n            return True\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while creating job"", e)\n            return False\n\n    def deploy_worker(self):\n        """"""\n        Deploys the rollout worker(s) in Kubernetes.\n        """"""\n\n        worker_params = self.params.run_type_params.get(str(RunType.ROLLOUT_WORKER), None)\n        if not worker_params:\n            return False\n\n        # At this point, the memory backend and data store have been deployed and in the process,\n        # these parameters have been updated to include things like the hostname and port the\n        # service can be found at.\n        worker_params.command += [\'--memory_backend_params\', json.dumps(self.params.memory_backend_parameters.__dict__)]\n        worker_params.command += [\'--data_store_params\', json.dumps(self.params.data_store_params.__dict__)]\n        worker_params.command += [\'--num_workers\', \'{}\'.format(worker_params.num_replicas)]\n\n        name = ""{}-{}"".format(worker_params.run_type, uuid.uuid4())\n\n        # TODO: instead of defining each container and template spec from scratch, loaded default\n        # configuration and modify them as necessary depending on the store type\n        if self.params.data_store_params.store_type == ""nfs"":\n            container = k8sclient.V1Container(\n                name=name,\n                image=worker_params.image,\n                command=worker_params.command,\n                args=worker_params.arguments,\n                image_pull_policy=\'Always\',\n                volume_mounts=[k8sclient.V1VolumeMount(\n                    name=\'nfs-pvc\',\n                    mount_path=worker_params.checkpoint_dir\n                )],\n                stdin=True,\n                tty=True\n            )\n            template = k8sclient.V1PodTemplateSpec(\n                metadata=k8sclient.V1ObjectMeta(labels={\'app\': name}),\n                spec=k8sclient.V1PodSpec(\n                    containers=[container],\n                    volumes=[k8sclient.V1Volume(\n                        name=""nfs-pvc"",\n                        persistent_volume_claim=self.nfs_pvc\n                    )],\n                    restart_policy=\'Never\'\n                ),\n            )\n        elif self.params.data_store_params.store_type == ""s3"":\n            container = k8sclient.V1Container(\n                name=name,\n                image=worker_params.image,\n                command=worker_params.command,\n                args=worker_params.arguments,\n                image_pull_policy=\'Always\',\n                env=[k8sclient.V1EnvVar(""ACCESS_KEY_ID"", self.s3_access_key),\n                     k8sclient.V1EnvVar(""SECRET_ACCESS_KEY"", self.s3_secret_key)],\n                stdin=True,\n                tty=True\n            )\n            template = k8sclient.V1PodTemplateSpec(\n                metadata=k8sclient.V1ObjectMeta(labels={\'app\': name}),\n                spec=k8sclient.V1PodSpec(\n                    containers=[container],\n                    restart_policy=\'Never\'\n                )\n            )\n        elif self.params.data_store_params.store_type == ""redis"":\n            container = k8sclient.V1Container(\n                name=name,\n                image=worker_params.image,\n                command=worker_params.command,\n                args=worker_params.arguments,\n                image_pull_policy=\'Always\',\n                stdin=True,\n                tty=True,\n                resources=k8sclient.V1ResourceRequirements(\n                    limits={\n                        ""cpu"": ""4"",\n                        ""memory"": ""4Gi"",\n                        # ""nvidia.com/gpu"": ""0"",\n                    }\n                ),\n            )\n            template = k8sclient.V1PodTemplateSpec(\n                metadata=k8sclient.V1ObjectMeta(labels={\'app\': name}),\n                spec=k8sclient.V1PodSpec(\n                    containers=[container],\n                    restart_policy=\'Never\'\n                )\n            )\n        else:\n            raise ValueError(\'unexpected store type {}\'.format(self.params.data_store_params.store_type))\n\n        job_spec = k8sclient.V1JobSpec(\n            completions=worker_params.num_replicas,\n            parallelism=worker_params.num_replicas,\n            template=template\n        )\n\n        job = k8sclient.V1Job(\n            api_version=""batch/v1"",\n            kind=""Job"",\n            metadata=k8sclient.V1ObjectMeta(name=name),\n            spec=job_spec\n        )\n\n        api_client = k8sclient.BatchV1Api()\n        try:\n            api_client.create_namespaced_job(self.params.namespace, job)\n            worker_params.orchestration_params[\'job_name\'] = name\n            return True\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while creating Job"", e)\n            return False\n\n    def worker_logs(self, path=\'./logs\'):\n        """"""\n        :param path: Path to store the worker logs.\n        """"""\n        worker_params = self.params.run_type_params.get(str(RunType.ROLLOUT_WORKER), None)\n        if not worker_params:\n            return\n\n        api_client = k8sclient.CoreV1Api()\n        pods = None\n        try:\n            pods = api_client.list_namespaced_pod(self.params.namespace, label_selector=\'app={}\'.format(\n                worker_params.orchestration_params[\'job_name\']\n            ))\n\n            # pod = pods.items[0]\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while reading pods"", e)\n            return\n\n        if not pods or len(pods.items) == 0:\n            return\n\n        for pod in pods.items:\n            Process(target=self._tail_log_file, args=(pod.metadata.name, api_client, self.params.namespace, path), daemon=True).start()\n\n    def _tail_log_file(self, pod_name, api_client, namespace, path):\n        if not os.path.exists(path):\n            os.mkdir(path)\n\n        sys.stdout = open(os.path.join(path, pod_name), \'w\')\n        self.tail_log(pod_name, api_client)\n\n    def trainer_logs(self):\n        """"""\n        Get the logs from trainer.\n        """"""\n        trainer_params = self.params.run_type_params.get(str(RunType.TRAINER), None)\n        if not trainer_params:\n            return\n\n        api_client = k8sclient.CoreV1Api()\n        pod = None\n        try:\n            pods = api_client.list_namespaced_pod(self.params.namespace, label_selector=\'app={}\'.format(\n                trainer_params.orchestration_params[\'job_name\']\n            ))\n\n            pod = pods.items[0]\n        except k8sclient.rest.ApiException as e:\n            print(""Got exception: %s\\n while reading pods"", e)\n            return\n\n        if not pod:\n            return\n\n        return self.tail_log(pod.metadata.name, api_client)\n\n    def tail_log(self, pod_name, corev1_api):\n        while True:\n            time.sleep(10)\n            # Try to tail the pod logs\n            try:\n                for line in corev1_api.read_namespaced_pod_log(\n                            pod_name, self.params.namespace, follow=True,\n                            _preload_content=False\n                        ):\n                    print(line.decode(\'utf-8\'), flush=True, end=\'\')\n            except k8sclient.rest.ApiException as e:\n                pass\n\n            # This part will get executed if the pod is one of the following phases: not ready, failed or terminated.\n            # Check if the pod has errored out, else just try again.\n            # Get the pod\n            try:\n                pod = corev1_api.read_namespaced_pod(pod_name, self.params.namespace)\n            except k8sclient.rest.ApiException as e:\n                continue\n\n            if not hasattr(pod, \'status\') or not pod.status:\n                continue\n            if not hasattr(pod.status, \'container_statuses\') or not pod.status.container_statuses:\n                continue\n\n            for container_status in pod.status.container_statuses:\n                if container_status.state.waiting is not None:\n                    if container_status.state.waiting.reason == \'Error\' or \\\n                       container_status.state.waiting.reason == \'CrashLoopBackOff\' or \\\n                       container_status.state.waiting.reason == \'ImagePullBackOff\' or \\\n                       container_status.state.waiting.reason == \'ErrImagePull\':\n                        return 1\n                if container_status.state.terminated is not None:\n                    return container_status.state.terminated.exit_code\n\n    def undeploy(self):\n        """"""\n        Undeploy all the components, such as trainer and rollout worker(s), Redis pub/sub and data store, when required.\n        """"""\n\n        trainer_params = self.params.run_type_params.get(str(RunType.TRAINER), None)\n        api_client = k8sclient.BatchV1Api()\n        delete_options = k8sclient.V1DeleteOptions(\n            propagation_policy=""Foreground""\n        )\n\n        if trainer_params:\n            try:\n                api_client.delete_namespaced_job(trainer_params.orchestration_params[\'job_name\'], self.params.namespace, delete_options)\n            except k8sclient.rest.ApiException as e:\n                print(""Got exception: %s\\n while deleting trainer"", e)\n        worker_params = self.params.run_type_params.get(str(RunType.ROLLOUT_WORKER), None)\n        if worker_params:\n            try:\n                api_client.delete_namespaced_job(worker_params.orchestration_params[\'job_name\'], self.params.namespace, delete_options)\n            except k8sclient.rest.ApiException as e:\n                print(""Got exception: %s\\n while deleting workers"", e)\n        self.memory_backend.undeploy()\n        self.data_store.undeploy()\n'"
rl_coach/presets/Acrobot_DDQN_BCQ_BatchRL.py,2,"b""import tensorflow as tf\n\nfrom rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, CsvDataset\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.batch_rl_graph_manager import BatchRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\nfrom rl_coach.memories.episodic import EpisodicExperienceReplayParameters\nfrom rl_coach.architectures.head_parameters import QHeadParameters\nfrom rl_coach.agents.ddqn_bcq_agent import DDQNBCQAgentParameters\n\nfrom rl_coach.agents.ddqn_bcq_agent import KNNParameters\n\nDATASET_SIZE = 50000\n\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = TrainingSteps(1)\nschedule_params.evaluation_steps = EnvironmentEpisodes(10)\nschedule_params.heatup_steps = EnvironmentSteps(DATASET_SIZE)\n\n#########\n# Agent #\n#########\n\nagent_params = DDQNBCQAgentParameters()\nagent_params.network_wrappers['main'].batch_size = 128\n# TODO cross-DL framework abstraction for a constant initializer?\nagent_params.network_wrappers['main'].heads_parameters = [QHeadParameters(output_bias_initializer=tf.constant_initializer(-100))]\n\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(\n    100)\nagent_params.algorithm.discount = 0.99\n\nagent_params.algorithm.action_drop_method_parameters = KNNParameters()\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\nagent_params.network_wrappers['main'].softmax_temperature = 0.2\n\n# ER size\nagent_params.memory = EpisodicExperienceReplayParameters()\n# DATATSET_PATH = 'acrobot.csv'\n# agent_params.memory.load_memory_from_file_path = CsvDataset(DATATSET_PATH, True)\n\n# E-Greedy schedule\nagent_params.exploration.epsilon_schedule = LinearSchedule(0, 0, 10000)\nagent_params.exploration.evaluation_epsilon = 0\n\n# Experience Generating Agent parameters\nexperience_generating_agent_params = DDQNAgentParameters()\n\n# schedule parameters\nexperience_generating_schedule_params = ScheduleParameters()\nexperience_generating_schedule_params.heatup_steps = EnvironmentSteps(1000)\nexperience_generating_schedule_params.improve_steps = TrainingSteps(\n    DATASET_SIZE - experience_generating_schedule_params.heatup_steps.num_steps)\nexperience_generating_schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nexperience_generating_schedule_params.evaluation_steps = EnvironmentEpisodes(1)\n\n# DQN params\nexperience_generating_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nexperience_generating_agent_params.algorithm.discount = 0.99\nexperience_generating_agent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# NN configuration\nexperience_generating_agent_params.network_wrappers['main'].learning_rate = 0.0001\nexperience_generating_agent_params.network_wrappers['main'].batch_size = 128\nexperience_generating_agent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\nexperience_generating_agent_params.network_wrappers['main'].heads_parameters = \\\n[QHeadParameters(output_bias_initializer=tf.constant_initializer(-100))]\n\n# ER size\nexperience_generating_agent_params.memory = EpisodicExperienceReplayParameters()\nexperience_generating_agent_params.memory.max_size = \\\n    (MemoryGranularity.Transitions,\n     experience_generating_schedule_params.heatup_steps.num_steps +\n     experience_generating_schedule_params.improve_steps.num_steps + 1)\n\n# E-Greedy schedule\nexperience_generating_agent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.01, DATASET_SIZE)\nexperience_generating_agent_params.exploration.evaluation_epsilon = 0\n\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='Acrobot-v1')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 50\npreset_validation_params.read_csv_tries = 500\n\ngraph_manager = BatchRLGraphManager(agent_params=agent_params,\n                                    experience_generating_agent_params=experience_generating_agent_params,\n                                    experience_generating_schedule_params=experience_generating_schedule_params,\n                                    env_params=env_params,\n                                    schedule_params=schedule_params,\n                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n                                    preset_validation_params=preset_validation_params,\n                                    reward_model_num_epochs=30,\n                                    train_to_eval_ratio=0.4)\n"""
rl_coach/presets/Atari_A3C.py,0,"b""from rl_coach.agents.actor_critic_agent import ActorCriticAgentParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(100)\nschedule_params.evaluation_steps = EnvironmentEpisodes(3)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ActorCriticAgentParameters()\n\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.algorithm.num_steps_between_gradient_updates = 20\nagent_params.algorithm.beta_entropy = 0.05\n\nagent_params.network_wrappers['main'].middleware_parameters = FCMiddlewareParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.0001\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_A3C_LSTM.py,0,"b""from rl_coach.agents.actor_critic_agent import ActorCriticAgentParameters\nfrom rl_coach.architectures.middleware_parameters import LSTMMiddlewareParameters\nfrom rl_coach.base_parameters import VisualizationParameters, MiddlewareScheme, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(100)\nschedule_params.evaluation_steps = EnvironmentEpisodes(3)\nschedule_params.heatup_steps = EnvironmentSteps(10000)\n\n#########\n# Agent #\n#########\nagent_params = ActorCriticAgentParameters()\n\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.algorithm.num_steps_between_gradient_updates = 20\nagent_params.algorithm.beta_entropy = 0.05\n\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].middleware_parameters = LSTMMiddlewareParameters(scheme=MiddlewareScheme.Medium,\n                                                                                       number_of_lstm_cells=256)\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_ACER.py,0,"b""from rl_coach.agents.acer_agent import ACERAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(100)\nschedule_params.evaluation_steps = EnvironmentEpisodes(3)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ACERAgentParameters()\n\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.algorithm.num_steps_between_gradient_updates = 20\nagent_params.algorithm.ratio_of_replay = 4\nagent_params.algorithm.num_transitions_to_start_replay = 10000\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 50000)\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.algorithm.beta_entropy = 0.05\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_Bootstrapped_DQN.py,0,"b""from rl_coach.agents.bootstrapped_dqn_agent import BootstrappedDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(50000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentSteps(250000)\nschedule_params.evaluation_steps = EnvironmentSteps(135000)\nschedule_params.heatup_steps = EnvironmentSteps(50000)\n\n#########\n# Agent #\n#########\nagent_params = BootstrappedDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_C51.py,0,"b""from rl_coach.agents.categorical_dqn_agent import CategoricalDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n\n#########\n# Agent #\n#########\nagent_params = CategoricalDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_DDQN.py,0,"b""from rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)"""
rl_coach/presets/Atari_DDQN_with_PER.py,0,"b""from rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.memories.non_episodic.prioritized_experience_replay import PrioritizedExperienceReplayParameters\nfrom rl_coach.schedules import LinearSchedule\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025/4\nagent_params.memory = PrioritizedExperienceReplayParameters()\nagent_params.memory.beta = LinearSchedule(0.4, 1, 12500000)  # 12.5M training iterations = 50M steps = 200M frames\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_DQN.py,0,"b""from rl_coach.agents.dqn_agent import DQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n\n#########\n# Agent #\n#########\nagent_params = DQNAgentParameters()\n# since we are using Adam instead of RMSProp, we adjust the learning rate as well\nagent_params.network_wrappers['main'].learning_rate = 0.0001\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_DQN_with_PER.py,0,"b""from rl_coach.agents.dqn_agent import DQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.memories.non_episodic.prioritized_experience_replay import PrioritizedExperienceReplayParameters\nfrom rl_coach.schedules import LinearSchedule\n\n\n#########\n# Agent #\n#########\nagent_params = DQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.memory = PrioritizedExperienceReplayParameters()\nagent_params.memory.beta = LinearSchedule(0.4, 1, 12500000)  # 12.5M training iterations = 50M steps = 200M frames\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_Dueling_DDQN.py,0,"b""import math\n\nfrom rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.architectures.head_parameters import DuelingQHeadParameters\nfrom rl_coach.base_parameters import VisualizationParameters, MiddlewareScheme, PresetValidationParameters\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\n\n# since we are using Adam instead of RMSProp, we adjust the learning rate as well\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].middleware_parameters.scheme = MiddlewareScheme.Empty\nagent_params.network_wrappers['main'].heads_parameters = \\\n    [DuelingQHeadParameters(rescale_gradient_from_head_by_factor=1/math.sqrt(2))]\nagent_params.network_wrappers['main'].clip_gradients = 10\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_Dueling_DDQN_with_PER_OpenAI.py,0,"b""from rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.architectures.head_parameters import DuelingQHeadParameters\nfrom rl_coach.base_parameters import VisualizationParameters, MiddlewareScheme, PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.memories.non_episodic.prioritized_experience_replay import PrioritizedExperienceReplayParameters\nfrom rl_coach.schedules import LinearSchedule, PieceWiseSchedule, ConstantSchedule\n\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].middleware_parameters.scheme = MiddlewareScheme.Empty\nagent_params.network_wrappers['main'].heads_parameters = [DuelingQHeadParameters()]\nagent_params.network_wrappers['main'].clip_gradients = 10\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(40000)\nagent_params.exploration.epsilon_schedule = PieceWiseSchedule(\n    [(LinearSchedule(1, 0.1, 1000000), EnvironmentSteps(1000000)),\n     (LinearSchedule(0.1, 0.01, 10000000), EnvironmentSteps(1000000)),\n     (ConstantSchedule(0.001), EnvironmentSteps(10000000))]\n)\nagent_params.memory = PrioritizedExperienceReplayParameters()\nagent_params.memory.beta = LinearSchedule(0.4, 1, 12500000)  # 12.5M training iterations = 50M steps = 200M frames\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_NEC.py,0,"b""from rl_coach.agents.nec_agent import NECAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, AtariInputFilter, atari_deterministic_v4\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(10000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(100)\nschedule_params.evaluation_steps = EnvironmentEpisodes(3)\nschedule_params.heatup_steps = EnvironmentSteps(2000)\n\n#########\n# Agent #\n#########\nagent_params = NECAgentParameters()\n\nagent_params.network_wrappers['main'].learning_rate = 0.00001\nagent_params.input_filter = AtariInputFilter()\nagent_params.input_filter.remove_reward_filter('clipping')\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\nenv_params.random_initialization_steps = 1\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test_using_a_trace_test = False\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_NStepQ.py,0,"b""from rl_coach.agents.n_step_q_agent import NStepQAgentParameters\nfrom rl_coach.architectures.layers import Conv2d, Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(100)\nschedule_params.evaluation_steps = EnvironmentEpisodes(3)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = NStepQAgentParameters()\n\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = [Conv2d(16, 8, 4),\n                                                                                          Conv2d(32, 4, 2)]\nagent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense(256)]\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_QR_DQN.py,0,"b""from rl_coach.agents.qr_dqn_agent import QuantileRegressionDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n\n#########\n# Agent #\n#########\nagent_params = QuantileRegressionDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00005  # called alpha in the paper\nagent_params.algorithm.huber_loss_interval = 1  # k = 0 for strict quantile loss, k = 1 for Huber quantile loss\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_Rainbow.py,0,"b""from rl_coach.agents.rainbow_dqn_agent import RainbowDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(50000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentSteps(1000000)\nschedule_params.evaluation_steps = EnvironmentSteps(125000)\nschedule_params.heatup_steps = EnvironmentSteps(20000)\n\n#########\n# Agent #\n#########\nagent_params = RainbowDQNAgentParameters()\n\nagent_params.network_wrappers['main'].learning_rate = 0.0000625\nagent_params.network_wrappers['main'].optimizer_epsilon = 1.5e-4\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(32000 // 4)  # 32k frames\nagent_params.memory.beta = LinearSchedule(0.4, 1, 12500000)  # 12.5M training iterations = 50M steps = 200M frames\nagent_params.memory.alpha = 0.5\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Atari_UCB_with_Q_Ensembles.py,0,"b""from rl_coach.agents.bootstrapped_dqn_agent import BootstrappedDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4, atari_schedule\nfrom rl_coach.exploration_policies.ucb import UCBParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n\n#########\n# Agent #\n#########\nagent_params = BootstrappedDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.exploration = UCBParameters()\n\n###############\n# Environment #\n###############\nenv_params = Atari(level=SingleLevelSelection(atari_deterministic_v4))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['breakout', 'pong', 'space_invaders']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/BitFlip_DQN.py,0,"b""from rl_coach.agents.dqn_agent import DQNAgentParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, EmbedderScheme, \\\n    PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import ConstantSchedule\n\nbit_length = 8\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(400000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(16 * 50)  # 50 cycles\nschedule_params.evaluation_steps = EnvironmentEpisodes(10)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = DQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.001\nagent_params.network_wrappers['main'].batch_size = 128\nagent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense(256)]\nagent_params.network_wrappers['main'].input_embedders_parameters = {\n    'state': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n    'desired_goal': InputEmbedderParameters(scheme=EmbedderScheme.Empty)\n}\nagent_params.algorithm.discount = 0.98\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(16)\nagent_params.algorithm.num_consecutive_training_steps = 40\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(40)\nagent_params.algorithm.rate_for_copying_weights_to_target = 0.05\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 10**6)\nagent_params.exploration.epsilon_schedule = ConstantSchedule(0.2)\nagent_params.exploration.evaluation_epsilon = 0\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='rl_coach.environments.toy_problems.bit_flip:BitFlip')\nenv_params.additional_simulator_parameters = {'bit_length': bit_length, 'mean_zero': True}\nenv_params.custom_reward_threshold = -bit_length + 1\n\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = -7.9\npreset_validation_params.max_episodes_to_achieve_reward = 10000\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n\n\n# self.algorithm.add_intrinsic_reward_for_reaching_the_goal = False\n\n"""
rl_coach/presets/BitFlip_DQN_HER.py,0,"b""from rl_coach.agents.dqn_agent import DQNAgentParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, EmbedderScheme, \\\n    PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.episodic.episodic_hindsight_experience_replay import \\\n    EpisodicHindsightExperienceReplayParameters, HindsightGoalSelectionMethod\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import ConstantSchedule\nfrom rl_coach.spaces import GoalsSpace, ReachingGoal\n\nbit_length = 20\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentEpisodes(16 * 50 * 200)  # 200 epochs\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(16 * 50)  # 50 cycles\nschedule_params.evaluation_steps = EnvironmentEpisodes(10)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = DQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.001\nagent_params.network_wrappers['main'].batch_size = 128\nagent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense(256)]\nagent_params.network_wrappers['main'].input_embedders_parameters = {\n    'state': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n    'desired_goal': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\nagent_params.algorithm.discount = 0.98\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(16)\nagent_params.algorithm.num_consecutive_training_steps = 40\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(40)\nagent_params.algorithm.rate_for_copying_weights_to_target = 0.05\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 10**6)\nagent_params.exploration.epsilon_schedule = ConstantSchedule(0.2)\nagent_params.exploration.evaluation_epsilon = 0\n\nagent_params.memory = EpisodicHindsightExperienceReplayParameters()\nagent_params.memory.hindsight_goal_selection_method = HindsightGoalSelectionMethod.Final\nagent_params.memory.hindsight_transitions_per_regular_transition = 1\nagent_params.memory.goals_space = GoalsSpace(goal_name='state',\n                                                    reward_type=ReachingGoal(distance_from_goal_threshold=0,\n                                                          goal_reaching_reward=0,\n                                                          default_reward=-1),\n                                                    distance_metric=GoalsSpace.DistanceMetric.Euclidean)\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='rl_coach.environments.toy_problems.bit_flip:BitFlip')\nenv_params.additional_simulator_parameters = {'bit_length': bit_length, 'mean_zero': True}\nenv_params.custom_reward_threshold = -bit_length + 1\n\n# currently no tests for this preset as the max reward can be accidently achieved. will be fixed with trace based tests.\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = -15\npreset_validation_params.max_episodes_to_achieve_reward = 10000\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n\n\n# self.algorithm.add_intrinsic_reward_for_reaching_the_goal = False\n\n"""
rl_coach/presets/CARLA_3_Cameras_DDPG.py,0,"b""import copy\n\nfrom rl_coach.agents.ddpg_agent import DDPGAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.carla_environment import CarlaEnvironmentParameters, CameraTypes, CarlaInputFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = DDPGAgentParameters()\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(4)\n\n# front camera\nagent_params.network_wrappers['actor'].input_embedders_parameters['forward_camera'] = \\\n    agent_params.network_wrappers['actor'].input_embedders_parameters.pop('observation')\nagent_params.network_wrappers['critic'].input_embedders_parameters['forward_camera'] = \\\n    agent_params.network_wrappers['critic'].input_embedders_parameters.pop('observation')\n\n# left camera\nagent_params.network_wrappers['actor'].input_embedders_parameters['left_camera'] = \\\n    copy.deepcopy(agent_params.network_wrappers['actor'].input_embedders_parameters['forward_camera'])\nagent_params.network_wrappers['critic'].input_embedders_parameters['left_camera'] = \\\n    copy.deepcopy(agent_params.network_wrappers['critic'].input_embedders_parameters['forward_camera'])\n\n# right camera\nagent_params.network_wrappers['actor'].input_embedders_parameters['right_camera'] = \\\n    copy.deepcopy(agent_params.network_wrappers['actor'].input_embedders_parameters['forward_camera'])\nagent_params.network_wrappers['critic'].input_embedders_parameters['right_camera'] = \\\n    copy.deepcopy(agent_params.network_wrappers['critic'].input_embedders_parameters['forward_camera'])\n\nagent_params.input_filter = CarlaInputFilter()\nagent_params.input_filter.copy_filters_from_one_observation_to_another('forward_camera', 'left_camera')\nagent_params.input_filter.copy_filters_from_one_observation_to_another('forward_camera', 'right_camera')\n\n###############\n# Environment #\n###############\nenv_params = CarlaEnvironmentParameters()\nenv_params.cameras = [CameraTypes.FRONT, CameraTypes.LEFT, CameraTypes.RIGHT]\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n"""
rl_coach/presets/CARLA_CIL.py,0,"b'import os\n\nimport numpy as np\n# make sure you have $CARLA_ROOT/PythonClient in your PYTHONPATH\nfrom carla.driving_benchmark.experiment_suites import CoRL2017\nfrom rl_coach.logger import screen\n\nfrom rl_coach.agents.cil_agent import CILAgentParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import RegressionHeadParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.architectures.layers import Conv2d, Dense, BatchnormActivationDropout\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.carla_environment import CarlaEnvironmentParameters\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.observation.observation_crop_filter import ObservationCropFilter\nfrom rl_coach.filters.observation.observation_reduction_by_sub_parts_name_filter import \\\n    ObservationReductionBySubPartsNameFilter\nfrom rl_coach.filters.observation.observation_rescale_to_size_filter import ObservationRescaleToSizeFilter\nfrom rl_coach.filters.observation.observation_to_uint8_filter import ObservationToUInt8Filter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import ConstantSchedule\nfrom rl_coach.spaces import ImageObservationSpace\nfrom rl_coach.utilities.carla_dataset_to_replay_buffer import create_dataset\nfrom rl_coach.core_types import PickledReplayBuffer\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = TrainingSteps(500)\nschedule_params.evaluation_steps = EnvironmentEpisodes(5)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n################\n# Agent Params #\n################\nagent_params = CILAgentParameters()\n\n# forward camera and measurements input\nagent_params.network_wrappers[\'main\'].input_embedders_parameters = {\n    \'CameraRGB\': InputEmbedderParameters(\n        scheme=[\n            Conv2d(32, 5, 2),\n            BatchnormActivationDropout(batchnorm=True, activation_function=\'tanh\'),\n            Conv2d(32, 3, 1),\n            BatchnormActivationDropout(batchnorm=True, activation_function=\'tanh\'),\n            Conv2d(64, 3, 2),\n            BatchnormActivationDropout(batchnorm=True, activation_function=\'tanh\'),\n            Conv2d(64, 3, 1),\n            BatchnormActivationDropout(batchnorm=True, activation_function=\'tanh\'),\n            Conv2d(128, 3, 2),\n            BatchnormActivationDropout(batchnorm=True, activation_function=\'tanh\'),\n            Conv2d(128, 3, 1),\n            BatchnormActivationDropout(batchnorm=True, activation_function=\'tanh\'),\n            Conv2d(256, 3, 1),\n            BatchnormActivationDropout(batchnorm=True, activation_function=\'tanh\'),\n            Conv2d(256, 3, 1),\n            BatchnormActivationDropout(batchnorm=True, activation_function=\'tanh\'),\n            Dense(512),\n            BatchnormActivationDropout(activation_function=\'tanh\', dropout_rate=0.3),\n            Dense(512),\n            BatchnormActivationDropout(activation_function=\'tanh\', dropout_rate=0.3)\n        ],\n        activation_function=\'none\'  # we define the activation function for each layer explicitly\n    ),\n    \'measurements\': InputEmbedderParameters(\n         scheme=[\n            Dense(128),\n            BatchnormActivationDropout(activation_function=\'tanh\', dropout_rate=0.5),\n            Dense(128),\n            BatchnormActivationDropout(activation_function=\'tanh\', dropout_rate=0.5)\n         ],\n         activation_function=\'none\'  # we define the activation function for each layer explicitly\n    )\n}\n\n# simple fc middleware\nagent_params.network_wrappers[\'main\'].middleware_parameters = \\\n    FCMiddlewareParameters(\n        scheme=[\n            Dense(512),\n            BatchnormActivationDropout(activation_function=\'tanh\', dropout_rate=0.5)\n        ],\n        activation_function=\'none\'\n    )\n\n# output branches\nagent_params.network_wrappers[\'main\'].heads_parameters = [\n    RegressionHeadParameters(\n        scheme=[\n            Dense(256),\n            BatchnormActivationDropout(activation_function=\'tanh\', dropout_rate=0.5),\n            Dense(256),\n            BatchnormActivationDropout(activation_function=\'tanh\')\n        ],\n        num_output_head_copies=4  # follow lane, left, right, straight\n    )\n]\n# TODO: there should be another head predicting the speed which is connected directly to the forward camera embedding\n\nagent_params.network_wrappers[\'main\'].batch_size = 120\nagent_params.network_wrappers[\'main\'].learning_rate = 0.0002\n\n\n# crop and rescale the image + use only the forward speed measurement\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_observation_filter(\'CameraRGB\', \'cropping\',\n                                                 ObservationCropFilter(crop_low=np.array([115, 0, 0]),\n                                                                       crop_high=np.array([510, -1, -1])))\nagent_params.input_filter.add_observation_filter(\'CameraRGB\', \'rescale\',\n                                                 ObservationRescaleToSizeFilter(\n                                                     ImageObservationSpace(np.array([88, 200, 3]), high=255)))\nagent_params.input_filter.add_observation_filter(\'CameraRGB\', \'to_uint8\', ObservationToUInt8Filter(0, 255))\nagent_params.input_filter.add_observation_filter(\n    \'measurements\', \'select_speed\',\n    ObservationReductionBySubPartsNameFilter(\n        [""forward_speed""], reduction_method=ObservationReductionBySubPartsNameFilter.ReductionMethod.Keep))\n\n# no exploration is used\nagent_params.exploration = AdditiveNoiseParameters()\nagent_params.exploration.noise_schedule = ConstantSchedule(0)\nagent_params.exploration.evaluation_noise = 0\n\n# no playing during the training phase\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(0)\n\n# use the following command line to download and extract the CARLA dataset:\n# python rl_coach/utilities/carla_dataset_to_replay_buffer.py\nagent_params.memory.load_memory_from_file_path = PickledReplayBuffer(""./datasets/carla_train_set_replay_buffer.p"")\nagent_params.memory.state_key_with_the_class_index = \'high_level_command\'\nagent_params.memory.num_classes = 4\n\n# download dataset if it doesn\'t exist\nif not os.path.exists(agent_params.memory.load_memory_from_file_path):\n    screen.log_title(""The CARLA dataset is not present in the following path: {}""\n                     .format(agent_params.memory.load_memory_from_file_path))\n    result = screen.ask_yes_no(""Do you want to download it now?"")\n    if result:\n        create_dataset(None, ""./datasets/carla_train_set_replay_buffer.p"")\n    else:\n        screen.error(""Please update the path to the CARLA dataset in the CARLA_CIL preset"", crash=True)\n\n\n###############\n# Environment #\n###############\nenv_params = CarlaEnvironmentParameters()\nenv_params.cameras = [\'CameraRGB\']\nenv_params.camera_height = 600\nenv_params.camera_width = 800\nenv_params.separate_actions_for_throttle_and_brake = True\nenv_params.allow_braking = True\nenv_params.quality = CarlaEnvironmentParameters.Quality.EPIC\nenv_params.experiment_suite = CoRL2017(\'Town01\')\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n'"
rl_coach/presets/CARLA_DDPG.py,0,"b""from rl_coach.agents.ddpg_agent import DDPGAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.carla_environment import CarlaEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = DDPGAgentParameters()\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(4)\nagent_params.network_wrappers['actor'].input_embedders_parameters['forward_camera'] = \\\n    agent_params.network_wrappers['actor'].input_embedders_parameters.pop('observation')\nagent_params.network_wrappers['critic'].input_embedders_parameters['forward_camera'] = \\\n    agent_params.network_wrappers['critic'].input_embedders_parameters.pop('observation')\n\n###############\n# Environment #\n###############\nenv_params = CarlaEnvironmentParameters()\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n"""
rl_coach/presets/CARLA_Dueling_DDQN.py,0,"b""import math\n\nfrom rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.architectures.head_parameters import DuelingQHeadParameters\nfrom rl_coach.base_parameters import VisualizationParameters, MiddlewareScheme\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.carla_environment import CarlaEnvironmentParameters\nfrom rl_coach.filters.action.box_discretization import BoxDiscretization\nfrom rl_coach.filters.filter import OutputFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.network_wrappers['main'].heads_parameters = \\\n    [DuelingQHeadParameters(rescale_gradient_from_head_by_factor=1/math.sqrt(2))]\nagent_params.network_wrappers['main'].middleware_parameters.scheme = MiddlewareScheme.Empty\nagent_params.network_wrappers['main'].clip_gradients = 10\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(4)\nagent_params.network_wrappers['main'].input_embedders_parameters['forward_camera'] = \\\n    agent_params.network_wrappers['main'].input_embedders_parameters.pop('observation')\nagent_params.output_filter = OutputFilter()\nagent_params.output_filter.add_action_filter('discretization', BoxDiscretization(5))\n\n###############\n# Environment #\n###############\nenv_params = CarlaEnvironmentParameters()\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n"""
rl_coach/presets/CartPole_A3C.py,0,"b""from rl_coach.agents.actor_critic_agent import ActorCriticAgentParameters\nfrom rl_coach.agents.policy_optimization_agent import PolicyGradientRescaler\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ActorCriticAgentParameters()\n\nagent_params.algorithm.policy_gradient_rescaler = PolicyGradientRescaler.GAE\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.algorithm.num_steps_between_gradient_updates = 5\nagent_params.algorithm.gae_lambda = 1\nagent_params.algorithm.beta_entropy = 0.01\n\nagent_params.network_wrappers['main'].optimizer_type = 'Adam'\nagent_params.network_wrappers['main'].learning_rate = 0.0001\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/200.))\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 300\npreset_validation_params.num_workers = 8\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_ACER.py,0,"b""from rl_coach.agents.acer_agent import ACERAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ACERAgentParameters()\n\nagent_params.algorithm.num_steps_between_gradient_updates = 5\nagent_params.algorithm.ratio_of_replay = 4\nagent_params.algorithm.num_transitions_to_start_replay = 1000\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 50000)\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/200.))\nagent_params.algorithm.beta_entropy = 0.0\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 300\npreset_validation_params.num_workers = 1\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_ClippedPPO.py,0,"b""from rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters, DistributedCoachSynchronizationType\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, RunPhase\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.observation.observation_normalization_filter import ObservationNormalizationFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentSteps(2048)\nschedule_params.evaluation_steps = EnvironmentEpisodes(5)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ClippedPPOAgentParameters()\n\n\nagent_params.network_wrappers['main'].learning_rate = 0.0003\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].activation_function = 'tanh'\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = [Dense(64)]\nagent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense(64)]\nagent_params.network_wrappers['main'].middleware_parameters.activation_function = 'tanh'\nagent_params.network_wrappers['main'].batch_size = 64\nagent_params.network_wrappers['main'].optimizer_epsilon = 1e-5\nagent_params.network_wrappers['main'].adam_optimizer_beta2 = 0.999\n\nagent_params.algorithm.clip_likelihood_ratio_using_epsilon = 0.2\nagent_params.algorithm.clipping_decay_schedule = LinearSchedule(1.0, 0, 1000000)\nagent_params.algorithm.beta_entropy = 0\nagent_params.algorithm.gae_lambda = 0.95\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.optimization_epochs = 10\nagent_params.algorithm.estimate_state_value_using_gae = True\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(2048)\n\n# Distributed Coach synchronization type.\nagent_params.algorithm.distributed_coach_synchronization_type = DistributedCoachSynchronizationType.SYNC\n\nagent_params.pre_network_filter = InputFilter()\nagent_params.pre_network_filter.add_observation_filter('observation', 'normalize_observation',\n                                                        ObservationNormalizationFilter(name='normalize_observation'))\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='CartPole-v0')\nenv_params.custom_reward_threshold = 200\n# Set the target success\nenv_params.target_success_rate = 1.0\n\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 400\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_DDQN_BCQ_BatchRL.py,0,"b""from copy import deepcopy\nfrom rl_coach.agents.dqn_agent import DQNAgentParameters\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward import RewardRescaleFilter\nfrom rl_coach.graph_managers.batch_rl_graph_manager import BatchRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\nfrom rl_coach.memories.episodic import EpisodicExperienceReplayParameters\nfrom rl_coach.architectures.head_parameters import ClassificationHeadParameters\nfrom rl_coach.agents.ddqn_bcq_agent import DDQNBCQAgentParameters\n\nfrom rl_coach.agents.ddqn_bcq_agent import KNNParameters\nfrom rl_coach.agents.ddqn_bcq_agent import NNImitationModelParameters\n\nDATASET_SIZE = 10000\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = TrainingSteps(1)\nschedule_params.evaluation_steps = EnvironmentEpisodes(10)\nschedule_params.heatup_steps = EnvironmentSteps(DATASET_SIZE)\n\n#########\n# Agent #\n#########\n\n# using a set of 'unstable' hyper-params to showcase the value of BCQ. Using the same hyper-params with standard DDQN\n# will cause Q values to unboundedly increase, and the policy convergence to be unstable.\nagent_params = DDQNBCQAgentParameters()\nagent_params.network_wrappers['main'].batch_size = 128\n# agent_params.network_wrappers['main'].batch_size = 1024\n\n# DQN params\n\n# For making this become Fitted Q-Iteration we can keep the targets constant for the entire dataset size -\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(\n    DATASET_SIZE / agent_params.network_wrappers['main'].batch_size)\n#\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(\n    100)\n# agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\n\nagent_params.algorithm.discount = 0.98\n\n# can use either a kNN or a NN based model for predicting which actions not to max over in the bellman equation\nagent_params.algorithm.action_drop_method_parameters = KNNParameters()\n# agent_params.algorithm.action_drop_method_parameters = NNImitationModelParameters()\n# agent_params.algorithm.action_drop_method_parameters.imitation_model_num_epochs = 500\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\nagent_params.network_wrappers['main'].l2_regularization = 0.0001\nagent_params.network_wrappers['main'].softmax_temperature = 0.2\n\n# reward model params\nagent_params.network_wrappers['reward_model'] = deepcopy(agent_params.network_wrappers['main'])\nagent_params.network_wrappers['reward_model'].learning_rate = 0.0001\nagent_params.network_wrappers['reward_model'].l2_regularization = 0\n\nagent_params.network_wrappers['imitation_model'] = deepcopy(agent_params.network_wrappers['main'])\nagent_params.network_wrappers['imitation_model'].learning_rate = 0.0001\nagent_params.network_wrappers['imitation_model'].l2_regularization = 0\n\nagent_params.network_wrappers['imitation_model'].heads_parameters = [ClassificationHeadParameters()]\nagent_params.network_wrappers['imitation_model'].input_embedders_parameters['observation'].scheme = \\\n    [Dense(1024), Dense(1024), Dense(512), Dense(512), Dense(256)]\nagent_params.network_wrappers['imitation_model'].middleware_parameters.scheme = [Dense(128), Dense(64)]\n\n\n# ER size\nagent_params.memory = EpisodicExperienceReplayParameters()\n\n# E-Greedy schedule\nagent_params.exploration.epsilon_schedule = LinearSchedule(0, 0, 10000)\nagent_params.exploration.evaluation_epsilon = 0\n\n# Input filtering\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/200.))\n\n\n\n\n# Experience Generating Agent parameters\nexperience_generating_agent_params = DQNAgentParameters()\n\n# schedule parameters\nexperience_generating_schedule_params = ScheduleParameters()\nexperience_generating_schedule_params.heatup_steps = EnvironmentSteps(1000)\nexperience_generating_schedule_params.improve_steps = TrainingSteps(\n    DATASET_SIZE - experience_generating_schedule_params.heatup_steps.num_steps)\nexperience_generating_schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nexperience_generating_schedule_params.evaluation_steps = EnvironmentEpisodes(1)\n\n# DQN params\nexperience_generating_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nexperience_generating_agent_params.algorithm.discount = 0.99\nexperience_generating_agent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# NN configuration\nexperience_generating_agent_params.network_wrappers['main'].learning_rate = 0.00025\nexperience_generating_agent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\n# ER size\nexperience_generating_agent_params.memory = EpisodicExperienceReplayParameters()\nexperience_generating_agent_params.memory.max_size = \\\n    (MemoryGranularity.Transitions,\n     experience_generating_schedule_params.heatup_steps.num_steps +\n     experience_generating_schedule_params.improve_steps.num_steps)\n\n# E-Greedy schedule\nexperience_generating_agent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.01, 10000)\n\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 50\npreset_validation_params.read_csv_tries = 500\n\ngraph_manager = BatchRLGraphManager(agent_params=agent_params,\n                                    experience_generating_agent_params=experience_generating_agent_params,\n                                    experience_generating_schedule_params=experience_generating_schedule_params,\n                                    env_params=env_params,\n                                    schedule_params=schedule_params,\n                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n                                    preset_validation_params=preset_validation_params,\n                                    reward_model_num_epochs=30,\n                                    train_to_eval_ratio=0.4)\n"""
rl_coach/presets/CartPole_DDQN_BatchRL.py,0,"b""from copy import deepcopy\n\nfrom rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward import RewardRescaleFilter\nfrom rl_coach.graph_managers.batch_rl_graph_manager import BatchRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\nfrom rl_coach.memories.episodic import EpisodicExperienceReplayParameters\n\nDATASET_SIZE = 40000\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = TrainingSteps(1)\nschedule_params.evaluation_steps = EnvironmentEpisodes(10)\nschedule_params.heatup_steps = EnvironmentSteps(DATASET_SIZE)\n\n#########\n# Agent #\n#########\n# TODO add a preset which uses a dataset to train a BatchRL graph. e.g. save a cartpole dataset in a csv format.\nagent_params = DDQNAgentParameters()\nagent_params.network_wrappers['main'].batch_size = 128\n\n# DQN params\n# agent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(100)\n\n# For making this become Fitted Q-Iteration we can keep the targets constant for the entire dataset size -\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(\n    DATASET_SIZE / agent_params.network_wrappers['main'].batch_size)\n\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(0)\nagent_params.algorithm.discount = 0.98\n# agent_params.algorithm.discount = 1.0\n\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\nagent_params.network_wrappers['main'].l2_regularization = 0.0001\nagent_params.network_wrappers['main'].softmax_temperature = 0.2\n# agent_params.network_wrappers['main'].learning_rate_decay_rate = 0.95\n# agent_params.network_wrappers['main'].learning_rate_decay_steps = int(DATASET_SIZE /\n#                                                                   agent_params.network_wrappers['main'].batch_size)\n\n# reward model params\nagent_params.network_wrappers['reward_model'] = deepcopy(agent_params.network_wrappers['main'])\nagent_params.network_wrappers['reward_model'].learning_rate = 0.0001\nagent_params.network_wrappers['reward_model'].l2_regularization = 0\n\n# ER size\nagent_params.memory = EpisodicExperienceReplayParameters()\nagent_params.memory.max_size = (MemoryGranularity.Transitions, DATASET_SIZE)\n\n\n# E-Greedy schedule\nagent_params.exploration.epsilon_schedule = LinearSchedule(0, 0, 10000)\nagent_params.exploration.evaluation_epsilon = 0\n\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/200.))\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 2000\n\ngraph_manager = BatchRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params,\n                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n                                    preset_validation_params=preset_validation_params,\n                                    reward_model_num_epochs=30,\n                                    train_to_eval_ratio=0.8)\n"""
rl_coach/presets/CartPole_DFP.py,0,"b""from rl_coach.agents.dfp_agent import DFPAgentParameters, HandlingTargetsAfterEpisodeEnd\nfrom rl_coach.base_parameters import VisualizationParameters, EmbedderScheme, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(100)\n\n\n#########\n# Agent #\n#########\nagent_params = DFPAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = EmbedderScheme.Medium\nagent_params.network_wrappers['main'].input_embedders_parameters['goal'].scheme = EmbedderScheme.Medium\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].scheme = EmbedderScheme.Medium\nagent_params.exploration.epsilon_schedule = LinearSchedule(0.5, 0.01, 3000)\nagent_params.exploration.evaluation_epsilon = 0.01\nagent_params.algorithm.discount = 1.0\nagent_params.algorithm.use_accumulated_reward_as_measurement = True\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\nagent_params.algorithm.goal_vector = [1]  # accumulated_reward\nagent_params.algorithm.handling_targets_after_episode_end = HandlingTargetsAfterEpisodeEnd.LastStep\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 120\npreset_validation_params.max_episodes_to_achieve_reward = 250\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_DQN.py,0,"b""from rl_coach.agents.dqn_agent import DQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = DQNAgentParameters()\n\n# DQN params\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\n# ER size\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 40000)\n\n# E-Greedy schedule\nagent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.01, 10000)\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 250\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_Dueling_DDQN.py,0,"b""import math\n\nfrom rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.architectures.head_parameters import DuelingQHeadParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\n\n# DDQN params\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\nagent_params.network_wrappers['main'].heads_parameters = \\\n    [DuelingQHeadParameters(rescale_gradient_from_head_by_factor=1/math.sqrt(2))]\n\n# ER size\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 40000)\n\n# E-Greedy schedule\nagent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.01, 10000)\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 300\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_NEC.py,0,"b""from rl_coach.agents.nec_agent import NECAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1300)\n\n#########\n# Agent #\n#########\n\nagent_params = NECAgentParameters()\n\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.exploration.epsilon_schedule = LinearSchedule(0.5, 0.1, 1000)\nagent_params.exploration.evaluation_epsilon = 0\nagent_params.algorithm.discount = 0.99\nagent_params.memory.max_size = (MemoryGranularity.Episodes, 200)\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/200.))\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 300\npreset_validation_params.test_using_a_trace_test = False\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_NStepQ.py,0,"b""from rl_coach.agents.n_step_q_agent import NStepQAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = NStepQAgentParameters()\n\nagent_params.algorithm.discount = 0.99\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/200.))\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 200\npreset_validation_params.num_workers = 8\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_PAL.py,0,"b""from rl_coach.agents.pal_agent import PALAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = PALAgentParameters()\n\n# DQN params\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\n# ER size\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 40000)\n\n# E-Greedy schedule\nagent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.01, 10000)\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 250\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_PG.py,0,"b""from rl_coach.agents.policy_gradients_agent import PolicyGradientsAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = PolicyGradientsAgentParameters()\n\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.apply_gradients_every_x_episodes = 5\nagent_params.algorithm.num_steps_between_gradient_updates = 20000\n\nagent_params.network_wrappers['main'].optimizer_type = 'Adam'\nagent_params.network_wrappers['main'].learning_rate = 0.0005\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/200.))\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 130\npreset_validation_params.max_episodes_to_achieve_reward = 550\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n\n"""
rl_coach/presets/CartPole_QR_DQN.py,0,"b""from rl_coach.agents.qr_dqn_agent import QuantileRegressionDQNAgentParameters\nfrom rl_coach.agents.rainbow_dqn_agent import RainbowDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = QuantileRegressionDQNAgentParameters()\n\n# DQN params\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\nagent_params.algorithm.atoms = 50\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.0005\n\n# ER size\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 40000)\n\n# E-Greedy schedule\nagent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.01, 10000)\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 250\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/CartPole_Rainbow.py,0,"b""from rl_coach.agents.rainbow_dqn_agent import RainbowDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = RainbowDQNAgentParameters()\n\n# DQN params\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\n# ER size\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 40000)\n\nagent_params.memory.beta = LinearSchedule(0.4, 1, 10000)\nagent_params.memory.alpha = 0.5\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 150\npreset_validation_params.max_episodes_to_achieve_reward = 250\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/ControlSuite_DDPG.py,0,"b'from rl_coach.agents.ddpg_agent import DDPGAgentParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, EmbedderScheme, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.control_suite_environment import ControlSuiteEnvironmentParameters, control_suite_envs\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = DDPGAgentParameters()\nagent_params.network_wrappers[\'actor\'].input_embedders_parameters[\'measurements\'] = \\\n    agent_params.network_wrappers[\'actor\'].input_embedders_parameters.pop(\'observation\')\nagent_params.network_wrappers[\'critic\'].input_embedders_parameters[\'measurements\'] = \\\n    agent_params.network_wrappers[\'critic\'].input_embedders_parameters.pop(\'observation\')\nagent_params.network_wrappers[\'actor\'].input_embedders_parameters[\'measurements\'].scheme = [Dense(300)]\nagent_params.network_wrappers[\'actor\'].middleware_parameters.scheme = [Dense(200)]\nagent_params.network_wrappers[\'critic\'].input_embedders_parameters[\'measurements\'].scheme = [Dense(400)]\nagent_params.network_wrappers[\'critic\'].middleware_parameters.scheme = [Dense(300)]\nagent_params.network_wrappers[\'critic\'].input_embedders_parameters[\'action\'].scheme = EmbedderScheme.Empty\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter(""rescale"", RewardRescaleFilter(1/10.))\n\n###############\n# Environment #\n###############\nenv_params = ControlSuiteEnvironmentParameters(level=SingleLevelSelection(control_suite_envs))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = [\'cartpole:swingup\', \'hopper:hop\']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n'"
rl_coach/presets/Doom_Basic_A3C.py,0,"b""from rl_coach.agents.actor_critic_agent import ActorCriticAgentParameters\nfrom rl_coach.agents.policy_optimization_agent import PolicyGradientRescaler\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n\n#########\n# Agent #\n#########\nagent_params = ActorCriticAgentParameters()\nagent_params.algorithm.policy_gradient_rescaler = PolicyGradientRescaler.GAE\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/100.))\nagent_params.algorithm.num_steps_between_gradient_updates = 30\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.algorithm.gae_lambda = 1.0\nagent_params.algorithm.beta_entropy = 0.01\nagent_params.network_wrappers['main'].clip_gradients = 40.\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='basic')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 20\npreset_validation_params.max_episodes_to_achieve_reward = 400\npreset_validation_params.num_workers = 8\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Doom_Basic_ACER.py,0,"b""from rl_coach.agents.acer_agent import ACERAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n\n#########\n# Agent #\n#########\nagent_params = ACERAgentParameters()\n\nagent_params.algorithm.num_steps_between_gradient_updates = 30\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.algorithm.ratio_of_replay = 4\nagent_params.algorithm.num_transitions_to_start_replay = 2000\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 100000)\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/100.))\nagent_params.algorithm.beta_entropy = 0.01\nagent_params.network_wrappers['main'].clip_gradients = 40.\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='basic')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 20\npreset_validation_params.max_episodes_to_achieve_reward = 400\npreset_validation_params.num_workers = 8\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Doom_Basic_BC.py,0,"b""from rl_coach.agents.bc_agent import BCAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\nfrom rl_coach.core_types import PickledReplayBuffer\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = TrainingSteps(500)\nschedule_params.evaluation_steps = EnvironmentEpisodes(5)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n\n#########\n# Agent #\n#########\nagent_params = BCAgentParameters()\n# agent_params.memory.max_size = (MemoryGranularity.Episodes, 1000)\nagent_params.network_wrappers['main'].learning_rate = 0.0005\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(1000)\nagent_params.exploration.epsilon_schedule = LinearSchedule(0, 0, 50000)\nagent_params.exploration.evaluation_epsilon = 0\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(0)\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\nagent_params.network_wrappers['main'].batch_size = 120\nagent_params.memory.load_memory_from_file_path = PickledReplayBuffer('datasets/doom_basic.p')\n\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='basic')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test_using_a_trace_test = False\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Doom_Basic_DFP.py,0,"b""from rl_coach.agents.dfp_agent import DFPAgentParameters, HandlingTargetsAfterEpisodeEnd\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(50)\nschedule_params.evaluation_steps = EnvironmentEpisodes(3)\n\n# There is no heatup for DFP. heatup length is determined according to batch size. See below.\n\n#########\n# Agent #\n#########\nagent_params = DFPAgentParameters()\nschedule_params.heatup_steps = EnvironmentSteps(agent_params.network_wrappers['main'].batch_size)\n\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.exploration.epsilon_schedule = LinearSchedule(0.5, 0, 10000)\nagent_params.exploration.evaluation_epsilon = 0\n\n# this works better than the default which is 64\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\nagent_params.algorithm.use_accumulated_reward_as_measurement = True\nagent_params.algorithm.goal_vector = [0, 1]  # ammo, accumulated_reward\nagent_params.algorithm.handling_targets_after_episode_end = HandlingTargetsAfterEpisodeEnd.LastStep\n\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='basic')\n\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_max_env_steps = 2000\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Doom_Basic_DQN.py,0,"b""from rl_coach.agents.dqn_agent import DQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n\n#########\n# Agent #\n#########\nagent_params = DQNAgentParameters()\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 5000)\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(1000)\nagent_params.exploration.epsilon_schedule = LinearSchedule(0, 0, 50000)\nagent_params.exploration.evaluation_epsilon = 0\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='basic')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 20\npreset_validation_params.max_episodes_to_achieve_reward = 400\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Doom_Basic_Dueling_DDQN.py,0,"b""from rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.architectures.head_parameters import DuelingQHeadParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(50)\nschedule_params.evaluation_steps = EnvironmentEpisodes(3)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 5000)\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(1000)\nagent_params.exploration.epsilon_schedule = LinearSchedule(0.5, 0.01, 50000)\nagent_params.exploration.evaluation_epsilon = 0\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\nagent_params.network_wrappers['main'].heads_parameters = [DuelingQHeadParameters()]\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='basic')\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n"""
rl_coach/presets/Doom_Battle_DFP.py,0,"b""from rl_coach.agents.dfp_agent import DFPAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters, DoomEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(6250000)\nschedule_params.steps_between_evaluation_periods = EnvironmentSteps(62500)\nschedule_params.evaluation_steps = EnvironmentSteps(6250)\nschedule_params.heatup_steps = EnvironmentSteps(1)\n\n#########\n# Agent #\n#########\nagent_params = DFPAgentParameters()\n\nagent_params.network_wrappers['main'].learning_rate = 0.0001\n# the original DFP code decays  epsilon in ~1.5M steps. Only that unlike other most other papers, these are 1.5M\n# training steps. i.e. it is equivalent to once every 8 playing steps (when a training batch is sampled).\n# so this is 1.5M*8 =~ 12M playing steps per worker.\n# TODO allow the epsilon schedule to be defined in terms of training steps.\nagent_params.exploration.epsilon_schedule = LinearSchedule(1, 0, 12000000)\nagent_params.exploration.evaluation_epsilon = 0\nagent_params.algorithm.use_accumulated_reward_as_measurement = False\nagent_params.algorithm.goal_vector = [0.5, 0.5, 1]  # ammo, health, frag count\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].input_rescaling['vector'] = 100.\nagent_params.algorithm.scale_measurements_targets['GameVariable.HEALTH'] = 30.0\nagent_params.algorithm.scale_measurements_targets['GameVariable.AMMO2'] = 7.5\nagent_params.algorithm.scale_measurements_targets['GameVariable.USER2'] = 1.0\nagent_params.network_wrappers['main'].learning_rate_decay_rate = 0.3\nagent_params.network_wrappers['main'].learning_rate_decay_steps = 250000\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].input_offset['vector'] = 0.5\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].input_offset['vector'] = 0.5\n\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='BATTLE_COACH_LOCAL')\nenv_params.cameras = [DoomEnvironment.CameraTypes.OBSERVATION]\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n"""
rl_coach/presets/Doom_Health_DFP.py,0,"b""from rl_coach.agents.dfp_agent import DFPAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, EmbedderScheme, MiddlewareScheme, \\\n    PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentSteps, EnvironmentEpisodes\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(6250000)\n# original paper evaluates according to these. But, this preset converges significantly faster - can be evaluated\n# much often.\n# schedule_params.steps_between_evaluation_periods = EnvironmentSteps(62500)\n# schedule_params.evaluation_steps = EnvironmentSteps(6250)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(5)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\n\n# There is no heatup for DFP. heatup length is determined according to batch size. See below.\n\n#########\n# Agent #\n#########\nagent_params = DFPAgentParameters()\nschedule_params.heatup_steps = EnvironmentSteps(agent_params.network_wrappers['main'].batch_size)\n\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.exploration.epsilon_schedule = LinearSchedule(0.5, 0, 10000)\nagent_params.exploration.evaluation_epsilon = 0\nagent_params.algorithm.goal_vector = [1]  # health\n\n# this works better than the default which is set to 8 (while running with 8 workers)\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# scale observation and measurements to be -0.5 <-> 0.5\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].input_rescaling['vector'] = 100.\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].input_offset['vector'] = 0.5\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].input_offset['vector'] = 0.5\n\n# changing the network scheme to match Coach's default network, as it performs better on this preset\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = EmbedderScheme.Medium\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].scheme = EmbedderScheme.Medium\nagent_params.network_wrappers['main'].input_embedders_parameters['goal'].scheme = EmbedderScheme.Medium\nagent_params.network_wrappers['main'].middleware_parameters.scheme = MiddlewareScheme.Medium\n\n# scale the target measurements according to the paper (dividing by standard deviation)\nagent_params.algorithm.scale_measurements_targets['GameVariable.HEALTH'] = 30.0\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='HEALTH_GATHERING')\n\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\n# reward threshold was set to 1000 since otherwise the test takes about an hour\npreset_validation_params.min_reward_threshold = 1000\npreset_validation_params.max_episodes_to_achieve_reward = 70\npreset_validation_params.test_using_a_trace_test = False\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Doom_Health_MMC.py,0,"b""from rl_coach.agents.mmc_agent import MixedMonteCarloAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(5)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = MixedMonteCarloAgentParameters()\n\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.exploration.epsilon_schedule = LinearSchedule(0.5, 0, 10000)\nagent_params.exploration.evaluation_epsilon = 0\nagent_params.memory.max_size = (MemoryGranularity.Episodes, 200)\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(1000)\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='HEALTH_GATHERING')\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test_using_a_trace_test = False\n# disabling this test for now, as it takes too long to converge\n# preset_validation_params.test = True\n# preset_validation_params.min_reward_threshold = 1000\n# preset_validation_params.max_episodes_to_achieve_reward = 300\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Doom_Health_Supreme_DFP.py,0,"b""from rl_coach.agents.dfp_agent import DFPAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, EmbedderScheme, MiddlewareScheme, \\\n    PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentSteps, EnvironmentEpisodes\nfrom rl_coach.environments.doom_environment import DoomEnvironmentParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(6250000)\n# original paper evaluates according to these. But, this preset converges significantly faster - can be evaluated\n# much often.\n# schedule_params.steps_between_evaluation_periods = EnvironmentSteps(62500)\n# schedule_params.evaluation_steps = EnvironmentSteps(6250)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(5)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\n\n# There is no heatup for DFP. heatup length is determined according to batch size. See below.\n\n#########\n# Agent #\n#########\nagent_params = DFPAgentParameters()\nschedule_params.heatup_steps = EnvironmentSteps(agent_params.network_wrappers['main'].batch_size)\n\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.exploration.epsilon_schedule = LinearSchedule(0.5, 0, 10000)\nagent_params.exploration.evaluation_epsilon = 0\nagent_params.algorithm.goal_vector = [1]  # health\n\n# this works better than the default which is set to 8 (while running with 8 workers)\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# scale observation and measurements to be -0.5 <-> 0.5\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].input_rescaling['vector'] = 100.\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].input_offset['vector'] = 0.5\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].input_offset['vector'] = 0.5\n\n# changing the network scheme to match Coach's default network, as it performs better on this preset\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = EmbedderScheme.Medium\nagent_params.network_wrappers['main'].input_embedders_parameters['measurements'].scheme = EmbedderScheme.Medium\nagent_params.network_wrappers['main'].input_embedders_parameters['goal'].scheme = EmbedderScheme.Medium\nagent_params.network_wrappers['main'].middleware_parameters.scheme = MiddlewareScheme.Medium\n\n# scale the target measurements according to the paper (dividing by standard deviation)\nagent_params.algorithm.scale_measurements_targets['GameVariable.HEALTH'] = 30.0\n\n###############\n# Environment #\n###############\nenv_params = DoomEnvironmentParameters(level='HEALTH_GATHERING_SUPREME_COACH_LOCAL')\n\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test_using_a_trace_test = False\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/ExplorationChain_Bootstrapped_DQN.py,0,"b""from rl_coach.agents.bootstrapped_dqn_agent import BootstrappedDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import NoInputFilter, NoOutputFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import ConstantSchedule\n\nN = 20\nnum_output_head_copies = 20\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentEpisodes(2000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(N)\n\n####################\n# DQN Agent Params #\n####################\nagent_params = BootstrappedDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 1000000)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(4)\nagent_params.network_wrappers['main'].heads_parameters[0].num_output_head_copies = num_output_head_copies\nagent_params.network_wrappers['main'].heads_parameters[0].rescale_gradient_from_head_by_factor = 1.0/num_output_head_copies\nagent_params.exploration.bootstrapped_data_sharing_probability = 1.0\nagent_params.exploration.architecture_num_q_heads = num_output_head_copies\nagent_params.exploration.epsilon_schedule = ConstantSchedule(0)\nagent_params.input_filter = NoInputFilter()\nagent_params.output_filter = NoOutputFilter()\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='rl_coach.environments.toy_problems.exploration_chain:ExplorationChain')\nenv_params.additional_simulator_parameters = {'chain_length': N, 'max_steps': N+7}\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n"""
rl_coach/presets/ExplorationChain_Dueling_DDQN.py,0,"b""from rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.architectures.head_parameters import DuelingQHeadParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymEnvironmentParameters\nfrom rl_coach.filters.filter import NoInputFilter, NoOutputFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\nN = 20\nnum_output_head_copies = 20\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentEpisodes(2000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(N)\n\n####################\n# DQN Agent Params #\n####################\nagent_params = DDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.network_wrappers['main'].heads_parameters = [DuelingQHeadParameters()]\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 1000000)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(4)\nagent_params.exploration.epsilon_schedule = LinearSchedule(1, 0.1, (N+7)*2000)\nagent_params.input_filter = NoInputFilter()\nagent_params.output_filter = NoOutputFilter()\n\n###############\n# Environment #\n###############\nenv_params = GymEnvironmentParameters(level='rl_coach.environments.toy_problems.exploration_chain:ExplorationChain')\nenv_params.additional_simulator_parameters = {'chain_length': N, 'max_steps': N+7}\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n"""
rl_coach/presets/ExplorationChain_UCB_Q_ensembles.py,0,"b""from rl_coach.agents.bootstrapped_dqn_agent import BootstrappedDQNAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.exploration_policies.ucb import UCBParameters\nfrom rl_coach.filters.filter import NoInputFilter, NoOutputFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import ConstantSchedule\n\nN = 20\nnum_output_head_copies = 20\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentEpisodes(2000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(N)\n\n####################\n# DQN Agent Params #\n####################\nagent_params = BootstrappedDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 1000000)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(4)\nagent_params.network_wrappers['main'].heads_parameters[0].num_output_head_copies = num_output_head_copies\nagent_params.network_wrappers['main'].heads_parameters[0].rescale_gradient_from_head_by_factor = 1.0/num_output_head_copies\nagent_params.exploration = UCBParameters()\nagent_params.exploration.bootstrapped_data_sharing_probability = 1.0\nagent_params.exploration.architecture_num_q_heads = num_output_head_copies\nagent_params.exploration.epsilon_schedule = ConstantSchedule(0)\nagent_params.exploration.lamb = 10\nagent_params.input_filter = NoInputFilter()\nagent_params.output_filter = NoOutputFilter()\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level='rl_coach.environments.toy_problems.exploration_chain:ExplorationChain')\nenv_params.additional_simulator_parameters = {'chain_length': N, 'max_steps': N+7}\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n"""
rl_coach/presets/Fetch_DDPG_HER_baselines.py,0,"b""from rl_coach.agents.ddpg_agent import DDPGAgentParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, EmbedderScheme, PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps, TrainingSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, fetch_v1\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.observation.observation_clipping_filter import ObservationClippingFilter\nfrom rl_coach.filters.observation.observation_normalization_filter import ObservationNormalizationFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.episodic.episodic_hindsight_experience_replay import EpisodicHindsightExperienceReplayParameters, \\\n    HindsightGoalSelectionMethod\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import ConstantSchedule\nfrom rl_coach.spaces import GoalsSpace, ReachingGoal\n\ncycles = 100  # 20 for reach. for others it's 100\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentEpisodes(cycles * 200)  # 200 epochs\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(cycles)  # 50 cycles\nschedule_params.evaluation_steps = EnvironmentEpisodes(10)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n################\n# Agent Params #\n################\nagent_params = DDPGAgentParameters()\n\n# actor\nactor_network = agent_params.network_wrappers['actor']\nactor_network.learning_rate = 0.001\nactor_network.batch_size = 256\nactor_network.optimizer_epsilon = 1e-08\nactor_network.adam_optimizer_beta1 = 0.9\nactor_network.adam_optimizer_beta2 = 0.999\nactor_network.input_embedders_parameters = {\n    'observation': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n    'desired_goal': InputEmbedderParameters(scheme=EmbedderScheme.Empty)\n}\nactor_network.middleware_parameters = FCMiddlewareParameters(scheme=[Dense(256), Dense(256), Dense(256)])\nactor_network.heads_parameters[0].batchnorm = False\n\n# critic\ncritic_network = agent_params.network_wrappers['critic']\ncritic_network.learning_rate = 0.001\ncritic_network.batch_size = 256\ncritic_network.optimizer_epsilon = 1e-08\ncritic_network.adam_optimizer_beta1 = 0.9\ncritic_network.adam_optimizer_beta2 = 0.999\ncritic_network.input_embedders_parameters = {\n    'action': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n    'desired_goal': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n    'observation': InputEmbedderParameters(scheme=EmbedderScheme.Empty)\n}\ncritic_network.middleware_parameters = FCMiddlewareParameters(scheme=[Dense(256), Dense(256), Dense(256)])\n\nagent_params.algorithm.discount = 0.98\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(1)\nagent_params.algorithm.num_consecutive_training_steps = 40\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(40)\nagent_params.algorithm.rate_for_copying_weights_to_target = 0.05\nagent_params.algorithm.action_penalty = 1\nagent_params.algorithm.use_non_zero_discount_for_terminal_states = True\nagent_params.algorithm.clip_critic_targets = [-50, 0]\n\n# HER parameters\nagent_params.memory = EpisodicHindsightExperienceReplayParameters()\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 10**6)\nagent_params.memory.hindsight_goal_selection_method = HindsightGoalSelectionMethod.Future\nagent_params.memory.hindsight_transitions_per_regular_transition = 4\nagent_params.memory.goals_space = GoalsSpace(goal_name='achieved_goal',\n                                             reward_type=ReachingGoal(distance_from_goal_threshold=0.05,\n                                                                      goal_reaching_reward=0,\n                                                                      default_reward=-1),\n                                             distance_metric=GoalsSpace.DistanceMetric.Euclidean)\nagent_params.memory.shared_memory = True\n\n# exploration parameters\nagent_params.exploration = EGreedyParameters()\nagent_params.exploration.epsilon_schedule = ConstantSchedule(0.3)\nagent_params.exploration.evaluation_epsilon = 0\n# they actually take the noise_schedule to be 0.2 * max_abs_range which is 0.1 * total_range\nagent_params.exploration.continuous_exploration_policy_parameters.noise_schedule = ConstantSchedule(0.1)\nagent_params.exploration.continuous_exploration_policy_parameters.evaluation_noise = 0\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_observation_filter('observation', 'clipping', ObservationClippingFilter(-200, 200))\n\nagent_params.pre_network_filter = InputFilter()\nagent_params.pre_network_filter.add_observation_filter('observation', 'normalize_observation',\n                                                       ObservationNormalizationFilter(name='normalize_observation'))\nagent_params.pre_network_filter.add_observation_filter('achieved_goal', 'normalize_achieved_goal',\n                                                       ObservationNormalizationFilter(name='normalize_achieved_goal'))\nagent_params.pre_network_filter.add_observation_filter('desired_goal', 'normalize_desired_goal',\n                                                       ObservationNormalizationFilter(name='normalize_desired_goal'))\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(fetch_v1))\nenv_params.custom_reward_threshold = -49\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.trace_test_levels = ['slide', 'pick_and_place', 'push', 'reach']\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n\n"""
rl_coach/presets/InvertedPendulum_PG.py,0,"b'from rl_coach.agents.policy_gradients_agent import PolicyGradientsAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.observation.observation_normalization_filter import ObservationNormalizationFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(50)\nschedule_params.evaluation_steps = EnvironmentEpisodes(3)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = PolicyGradientsAgentParameters()\nagent_params.algorithm.apply_gradients_every_x_episodes = 5\nagent_params.algorithm.num_steps_between_gradient_updates = 20000\nagent_params.network_wrappers[\'main\'].learning_rate = 0.0005\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter(\'rescale\', RewardRescaleFilter(1/20.))\nagent_params.input_filter.add_observation_filter(\'observation\', \'normalize\', ObservationNormalizationFilter())\n\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=""InvertedPendulum-v2"")\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n\n\n'"
rl_coach/presets/MontezumaRevenge_BC.py,0,"b""from rl_coach.agents.bc_agent import BCAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.gym_environment import Atari\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.core_types import PickledReplayBuffer\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = TrainingSteps(500)\nschedule_params.evaluation_steps = EnvironmentEpisodes(5)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = BCAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 1000000)\n# agent_params.memory.discount = 0.99\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(0)\nagent_params.memory.load_memory_from_file_path = PickledReplayBuffer('datasets/montezuma_revenge.p')\n\n###############\n# Environment #\n###############\nenv_params = Atari(level='MontezumaRevenge-v0')\nenv_params.random_initialization_steps = 30\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test_using_a_trace_test = False\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Mujoco_A3C.py,0,"b""from rl_coach.agents.actor_critic_agent import ActorCriticAgentParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.observation.observation_normalization_filter import ObservationNormalizationFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(20000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ActorCriticAgentParameters()\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.algorithm.num_steps_between_gradient_updates = 10000000\nagent_params.algorithm.beta_entropy = 0.0001\nagent_params.network_wrappers['main'].learning_rate = 0.00001\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/20.))\nagent_params.input_filter.add_observation_filter('observation', 'normalize', ObservationNormalizationFilter())\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 400\npreset_validation_params.max_episodes_to_achieve_reward = 1000\npreset_validation_params.num_workers = 8\npreset_validation_params.reward_test_level = 'inverted_pendulum'\npreset_validation_params.trace_test_levels = ['inverted_pendulum', 'hopper']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n\n\n"""
rl_coach/presets/Mujoco_A3C_LSTM.py,0,"b""from rl_coach.agents.actor_critic_agent import ActorCriticAgentParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.middleware_parameters import LSTMMiddlewareParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, MiddlewareScheme, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.observation.observation_normalization_filter import ObservationNormalizationFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ActorCriticAgentParameters()\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.algorithm.num_steps_between_gradient_updates = 20\nagent_params.algorithm.beta_entropy = 0.005\nagent_params.network_wrappers['main'].learning_rate = 0.00002\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'] = \\\n    InputEmbedderParameters(scheme=[Dense(200)])\nagent_params.network_wrappers['main'].middleware_parameters = LSTMMiddlewareParameters(scheme=MiddlewareScheme.Empty,\n                                                                                       number_of_lstm_cells=128)\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(1/20.))\nagent_params.input_filter.add_observation_filter('observation', 'normalize', ObservationNormalizationFilter())\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = False\npreset_validation_params.min_reward_threshold = 400\npreset_validation_params.max_episodes_to_achieve_reward = 1000\npreset_validation_params.num_workers = 8\npreset_validation_params.reward_test_level = 'inverted_pendulum'\npreset_validation_params.trace_test_levels = ['inverted_pendulum', 'hopper']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n\n\n"""
rl_coach/presets/Mujoco_ClippedPPO.py,0,"b""from rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters, DistributedCoachSynchronizationType\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.observation.observation_normalization_filter import ObservationNormalizationFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentSteps(2048)\nschedule_params.evaluation_steps = EnvironmentEpisodes(5)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ClippedPPOAgentParameters()\n\n\nagent_params.network_wrappers['main'].learning_rate = 0.0003\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].activation_function = 'tanh'\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = [Dense(64)]\nagent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense(64)]\nagent_params.network_wrappers['main'].middleware_parameters.activation_function = 'tanh'\nagent_params.network_wrappers['main'].batch_size = 64\nagent_params.network_wrappers['main'].optimizer_epsilon = 1e-5\nagent_params.network_wrappers['main'].adam_optimizer_beta2 = 0.999\n\nagent_params.algorithm.clip_likelihood_ratio_using_epsilon = 0.2\nagent_params.algorithm.clipping_decay_schedule = LinearSchedule(1.0, 0, 1000000)\nagent_params.algorithm.beta_entropy = 0\nagent_params.algorithm.gae_lambda = 0.95\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.optimization_epochs = 10\nagent_params.algorithm.estimate_state_value_using_gae = True\n# Distributed Coach synchronization type.\nagent_params.algorithm.distributed_coach_synchronization_type = DistributedCoachSynchronizationType.SYNC\n\nagent_params.input_filter = InputFilter()\nagent_params.exploration = AdditiveNoiseParameters()\nagent_params.pre_network_filter = InputFilter()\nagent_params.pre_network_filter.add_observation_filter('observation', 'normalize_observation',\n                                                       ObservationNormalizationFilter(name='normalize_observation'))\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n# Set the target success\nenv_params.target_success_rate = 1.0\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 400\npreset_validation_params.max_episodes_to_achieve_reward = 1000\npreset_validation_params.reward_test_level = 'inverted_pendulum'\npreset_validation_params.trace_test_levels = ['inverted_pendulum', 'hopper']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n\n\n"""
rl_coach/presets/Mujoco_DDPG.py,0,"b""from rl_coach.agents.ddpg_agent import DDPGAgentParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters, EmbedderScheme\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(2000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(10000)\n\n#########\n# Agent #\n#########\nagent_params = DDPGAgentParameters()\nagent_params.network_wrappers['actor'].input_embedders_parameters['observation'].scheme = [Dense(400)]\nagent_params.network_wrappers['actor'].middleware_parameters.scheme = [Dense(300)]\nagent_params.network_wrappers['critic'].input_embedders_parameters['observation'].scheme = [Dense(400)]\nagent_params.network_wrappers['critic'].middleware_parameters.scheme = [Dense(300)]\nagent_params.network_wrappers['critic'].input_embedders_parameters['action'].scheme = EmbedderScheme.Empty\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 400\npreset_validation_params.max_episodes_to_achieve_reward = 3000\npreset_validation_params.reward_test_level = 'inverted_pendulum'\npreset_validation_params.trace_test_levels = ['inverted_pendulum', 'hopper']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Mujoco_NAF.py,0,"b""from rl_coach.agents.naf_agent import NAFAgentParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, GradientClippingMethod\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n\n#########\n# Agent #\n#########\nagent_params = NAFAgentParameters()\nagent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = [Dense(200)]\nagent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense(200)]\nagent_params.network_wrappers['main'].clip_gradients = 1000\nagent_params.network_wrappers['main'].gradients_clipping_method = GradientClippingMethod.ClipByValue\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n\n\n# this preset is currently broken - no test\n\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\n# preset_validation_params.test = True\n# preset_validation_params.min_reward_threshold = 200\n# preset_validation_params.max_episodes_to_achieve_reward = 600\n# preset_validation_params.reward_test_level = 'inverted_pendulum'\npreset_validation_params.trace_test_levels = ['inverted_pendulum', 'hopper']\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Mujoco_PPO.py,0,"b""from rl_coach.agents.ppo_agent import PPOAgentParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters, DistributedCoachSynchronizationType\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.observation.observation_normalization_filter import ObservationNormalizationFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(1e10)\nschedule_params.steps_between_evaluation_periods = EnvironmentSteps(4000)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = PPOAgentParameters()\nagent_params.network_wrappers['actor'].learning_rate = 5e-5\nagent_params.network_wrappers['critic'].learning_rate = 5e-5\n\nagent_params.network_wrappers['actor'].input_embedders_parameters['observation'].scheme = [Dense(64)]\nagent_params.network_wrappers['actor'].middleware_parameters.scheme = [Dense(64)]\nagent_params.network_wrappers['critic'].input_embedders_parameters['observation'].scheme = [Dense(64)]\nagent_params.network_wrappers['critic'].middleware_parameters.scheme = [Dense(64)]\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_observation_filter('observation', 'normalize', ObservationNormalizationFilter())\n\nagent_params.algorithm.initial_kl_coefficient = 0.2\nagent_params.algorithm.gae_lambda = 1.0\n\n# Distributed Coach synchronization type.\nagent_params.algorithm.distributed_coach_synchronization_type = DistributedCoachSynchronizationType.SYNC\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 400\npreset_validation_params.max_episodes_to_achieve_reward = 3000\npreset_validation_params.reward_test_level = 'inverted_pendulum'\npreset_validation_params.trace_test_levels = ['inverted_pendulum', 'hopper']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Mujoco_SAC.py,0,"b""from rl_coach.agents.soft_actor_critic_agent import SoftActorCriticAgentParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n\n####################\n# Graph Scheduling #\n####################\n\n# see graph_manager.py for possible schedule parameters\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(3000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentSteps(1000)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(10000)\n\n\n#########\n# Agent #\n#########\nagent_params = SoftActorCriticAgentParameters()\n# override default parameters:\n# value (v) networks parameters\nagent_params.network_wrappers['v'].batch_size = 256\nagent_params.network_wrappers['v'].learning_rate = 0.0003\nagent_params.network_wrappers['v'].middleware_parameters.scheme = [Dense(256)]\n\n# critic (q) network parameters\nagent_params.network_wrappers['q'].heads_parameters[0].network_layers_sizes = (256, 256)\nagent_params.network_wrappers['q'].batch_size = 256\nagent_params.network_wrappers['q'].learning_rate = 0.0003\n\n# actor (policy) network parameters\nagent_params.network_wrappers['policy'].batch_size = 256\nagent_params.network_wrappers['policy'].learning_rate = 0.0003\nagent_params.network_wrappers['policy'].middleware_parameters.scheme = [Dense(256)]\n\n# Input Filter\n# SAC requires reward scaling for Mujoco environments.\n# according to the paper:\n# Hopper, Walker-2d, HalfCheetah, Ant - requires scaling of 5\n# Humanoid - requires scaling of 20\n\nagent_params.input_filter = InputFilter()\nagent_params.input_filter.add_reward_filter('rescale', RewardRescaleFilter(5))\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 400\npreset_validation_params.max_episodes_to_achieve_reward = 2200\npreset_validation_params.reward_test_level = 'inverted_pendulum'\npreset_validation_params.trace_test_levels = ['inverted_pendulum', 'hopper']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Mujoco_TD3.py,0,"b""from rl_coach.agents.td3_agent import TD3AgentParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters, EmbedderScheme\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(1000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentSteps(5000)\nschedule_params.evaluation_steps = EnvironmentEpisodes(10)\nschedule_params.heatup_steps = EnvironmentSteps(10000)\n\n#########\n# Agent #\n#########\nagent_params = TD3AgentParameters()\nagent_params.network_wrappers['actor'].input_embedders_parameters['observation'].scheme = [Dense(400)]\nagent_params.network_wrappers['actor'].middleware_parameters.scheme = [Dense(300)]\n\nagent_params.network_wrappers['critic'].input_embedders_parameters['observation'].scheme = EmbedderScheme.Empty\nagent_params.network_wrappers['critic'].input_embedders_parameters['action'].scheme = EmbedderScheme.Empty\nagent_params.network_wrappers['critic'].middleware_parameters.scheme = [Dense(400), Dense(300)]\n\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 500\npreset_validation_params.max_episodes_to_achieve_reward = 1100\npreset_validation_params.reward_test_level = 'hopper'\npreset_validation_params.trace_test_levels = ['inverted_pendulum', 'hopper']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Mujoco_Wolpertinger.py,0,"b""from collections import OrderedDict\n\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters, EmbedderScheme\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.environment import SingleLevelSelection\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment, mujoco_v2\nfrom rl_coach.filters.action import BoxDiscretization\nfrom rl_coach.filters.filter import OutputFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.agents.wolpertinger_agent import WolpertingerAgentParameters\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentSteps(2000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(20)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(3000)\n\n#########\n# Agent #\n#########\nagent_params = WolpertingerAgentParameters()\nagent_params.network_wrappers['actor'].input_embedders_parameters['observation'].scheme = [Dense(400)]\nagent_params.network_wrappers['actor'].middleware_parameters.scheme = [Dense(300)]\nagent_params.network_wrappers['critic'].input_embedders_parameters['observation'].scheme = [Dense(400)]\nagent_params.network_wrappers['critic'].middleware_parameters.scheme = [Dense(300)]\nagent_params.network_wrappers['critic'].input_embedders_parameters['action'].scheme = EmbedderScheme.Empty\nagent_params.output_filter = \\\n    OutputFilter(\n        action_filters=OrderedDict([\n            ('discretization', BoxDiscretization(num_bins_per_dimension=int(1e6)))\n        ]),\n        is_a_reference_filter=False\n    )\n\n###############\n# Environment #\n###############\nenv_params = GymVectorEnvironment(level=SingleLevelSelection(mujoco_v2))\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 500\npreset_validation_params.max_episodes_to_achieve_reward = 1000\npreset_validation_params.reward_test_level = 'inverted_pendulum'\npreset_validation_params.trace_test_levels = ['inverted_pendulum']\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n"""
rl_coach/presets/Pendulum_HAC.py,0,"b'import numpy as np\n\nfrom rl_coach.agents.hac_ddpg_agent import HACDDPGAgentParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, EmbeddingMergerType, EmbedderScheme\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps, TrainingSteps\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.exploration_policies.ou_process import OUProcessParameters\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.graph_managers.hac_graph_manager import HACGraphManager\nfrom rl_coach.memories.episodic.episodic_hindsight_experience_replay import HindsightGoalSelectionMethod, \\\n    EpisodicHindsightExperienceReplayParameters\nfrom rl_coach.memories.episodic.episodic_hrl_hindsight_experience_replay import \\\n    EpisodicHRLHindsightExperienceReplayParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import ConstantSchedule\nfrom rl_coach.spaces import GoalsSpace, ReachingGoal\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentEpisodes(40 * 4 * 64)  # 40 epochs\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(4 * 64)  # 4 small batches of 64 episodes\nschedule_params.evaluation_steps = EnvironmentEpisodes(64)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n\npolar_coordinates = False\n\n#########\n# Agent #\n#########\n\nif polar_coordinates:\n    distance_from_goal_threshold = np.array([0.075, 0.75])\nelse:\n    distance_from_goal_threshold = np.array([0.075, 0.075, 0.75])\ngoals_space = GoalsSpace(\'achieved_goal\',\n                         ReachingGoal(default_reward=-1, goal_reaching_reward=0,\n                                      distance_from_goal_threshold=distance_from_goal_threshold),\n                         lambda goal, state: np.abs(goal - state))  # raw L1 distance\n\n# top agent\ntop_agent_params = HACDDPGAgentParameters()\n\ntop_agent_params.memory = EpisodicHRLHindsightExperienceReplayParameters()\ntop_agent_params.memory.max_size = (MemoryGranularity.Transitions, 10000000)\ntop_agent_params.memory.hindsight_transitions_per_regular_transition = 3\ntop_agent_params.memory.hindsight_goal_selection_method = HindsightGoalSelectionMethod.Future\ntop_agent_params.memory.goals_space = goals_space\ntop_agent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(32)\ntop_agent_params.algorithm.num_consecutive_training_steps = 40\ntop_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(40)\n\n# exploration - OU process\ntop_agent_params.exploration = OUProcessParameters()\ntop_agent_params.exploration.theta = 0.1\n\n# actor\ntop_actor = top_agent_params.network_wrappers[\'actor\']\ntop_actor.input_embedders_parameters = {\'observation\': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                        \'desired_goal\': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\ntop_actor.middleware_parameters.scheme = [Dense(64)] * 3\ntop_actor.learning_rate = 0.001\ntop_actor.batch_size = 4096\n\n# critic\ntop_critic = top_agent_params.network_wrappers[\'critic\']\ntop_critic.input_embedders_parameters = {\'observation\': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                         \'action\': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                         \'desired_goal\': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\ntop_critic.embedding_merger_type = EmbeddingMergerType.Concat\ntop_critic.middleware_parameters.scheme = [Dense(64)] * 3\ntop_critic.learning_rate = 0.001\ntop_critic.batch_size = 4096\n\n# ----------\n\n# bottom agent\nbottom_agent_params = HACDDPGAgentParameters()\n\n# TODO: we should do this is a cleaner way. probably HACGraphManager, should set this for all non top-level agents\nbottom_agent_params.algorithm.in_action_space = goals_space\n\nbottom_agent_params.memory = EpisodicHindsightExperienceReplayParameters()\nbottom_agent_params.memory.max_size = (MemoryGranularity.Transitions, 12000000)\nbottom_agent_params.memory.hindsight_transitions_per_regular_transition = 4\nbottom_agent_params.memory.hindsight_goal_selection_method = HindsightGoalSelectionMethod.Future\nbottom_agent_params.memory.goals_space = goals_space\nbottom_agent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(16 * 25)  # 25 episodes is one true env episode\nbottom_agent_params.algorithm.num_consecutive_training_steps = 40\nbottom_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(40)\n\nbottom_agent_params.exploration = EGreedyParameters()\nbottom_agent_params.exploration.epsilon_schedule = ConstantSchedule(0.2)\nbottom_agent_params.exploration.evaluation_epsilon = 0\nbottom_agent_params.exploration.continuous_exploration_policy_parameters = OUProcessParameters()\nbottom_agent_params.exploration.continuous_exploration_policy_parameters.theta = 0.1\n\n# actor\nbottom_actor = bottom_agent_params.network_wrappers[\'actor\']\nbottom_actor.input_embedders_parameters = {\'observation\': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                           \'desired_goal\': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\nbottom_actor.middleware_parameters.scheme = [Dense(64)] * 3\nbottom_actor.learning_rate = 0.001\nbottom_actor.batch_size = 4096\n\n# critic\nbottom_critic = bottom_agent_params.network_wrappers[\'critic\']\nbottom_critic.input_embedders_parameters = {\'observation\': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                            \'action\': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                            \'desired_goal\': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\nbottom_critic.embedding_merger_type = EmbeddingMergerType.Concat\nbottom_critic.middleware_parameters.scheme = [Dense(64)] * 3\nbottom_critic.learning_rate = 0.001\nbottom_critic.batch_size = 4096\n\nagents_params = [top_agent_params, bottom_agent_params]\n\n###############\n# Environment #\n###############\ntime_limit = 1000\n\nenv_params = GymVectorEnvironment(level=""rl_coach.environments.mujoco.pendulum_with_goals:PendulumWithGoals"")\nenv_params.additional_simulator_parameters = {""time_limit"": time_limit,\n                                              ""random_goals_instead_of_standing_goal"": False,\n                                              ""polar_coordinates"": polar_coordinates,\n                                              ""goal_reaching_thresholds"": distance_from_goal_threshold}\nenv_params.frame_skip = 10\nenv_params.custom_reward_threshold = -time_limit + 1\n\nvis_params = VisualizationParameters()\nvis_params.native_rendering = False\n\ngraph_manager = HACGraphManager(agents_params=agents_params, env_params=env_params,\n                                schedule_params=schedule_params, vis_params=vis_params,\n                                consecutive_steps_to_run_non_top_levels=EnvironmentSteps(40))\n'"
rl_coach/presets/Starcraft_CollectMinerals_A3C.py,0,"b'from rl_coach.agents.actor_critic_agent import ActorCriticAgentParameters\nfrom rl_coach.agents.policy_optimization_agent import PolicyGradientRescaler\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.starcraft2_environment import StarCraft2EnvironmentParameters\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoiseParameters\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.schedules import ConstantSchedule\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(50)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n\n#########\n# Agent #\n#########\nagent_params = ActorCriticAgentParameters()\n\nagent_params.algorithm.policy_gradient_rescaler = PolicyGradientRescaler.GAE\nagent_params.algorithm.apply_gradients_every_x_episodes = 1\nagent_params.algorithm.num_steps_between_gradient_updates = 20\nagent_params.algorithm.gae_lambda = 0.96\nagent_params.algorithm.beta_entropy = 0\n\nagent_params.network_wrappers[\'main\'].clip_gradients = 10.0\nagent_params.network_wrappers[\'main\'].learning_rate = 0.00001\n# agent_params.network_wrappers[\'main\'].batch_size = 20\nagent_params.network_wrappers[\'main\'].input_embedders_parameters = {\n    ""screen"": InputEmbedderParameters(input_rescaling={\'image\': 3.0})\n}\n\nagent_params.exploration = AdditiveNoiseParameters()\nagent_params.exploration.noise_schedule = ConstantSchedule(0.05)\n# agent_params.exploration.noise_schedule = LinearSchedule(0.4, 0.05, 100000)\nagent_params.exploration.evaluation_noise = 0.05\n\nagent_params.network_wrappers[\'main\'].batch_size = 64\nagent_params.network_wrappers[\'main\'].optimizer_epsilon = 1e-5\nagent_params.network_wrappers[\'main\'].adam_optimizer_beta2 = 0.999\n\n###############\n# Environment #\n###############\n\nenv_params = StarCraft2EnvironmentParameters(level=\'CollectMineralShards\')\nenv_params.feature_screen_maps_to_use = [5]\nenv_params.feature_minimap_maps_to_use = [5]\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 50\npreset_validation_params.max_episodes_to_achieve_reward = 200\npreset_validation_params.num_workers = 1\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params,\n                                    vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n'"
rl_coach/presets/Starcraft_CollectMinerals_Dueling_DDQN.py,0,"b'from collections import OrderedDict\n\nfrom rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import DuelingQHeadParameters\nfrom rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.starcraft2_environment import StarCraft2EnvironmentParameters\nfrom rl_coach.filters.action.box_discretization import BoxDiscretization\nfrom rl_coach.filters.filter import OutputFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\n\n####################\n# Graph Scheduling #\n####################\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(10000000000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(50)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(50000)\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\n\nagent_params.network_wrappers[\'main\'].learning_rate = 0.0001\nagent_params.network_wrappers[\'main\'].input_embedders_parameters = {\n    ""screen"": InputEmbedderParameters(input_rescaling={\'image\': 3.0})\n}\nagent_params.network_wrappers[\'main\'].heads_parameters = [DuelingQHeadParameters()]\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 1000000)\n# slave_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(10000)\nagent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.1, 1000000)\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(4)\nagent_params.output_filter = \\\n    OutputFilter(\n        action_filters=OrderedDict([\n            (\'discretization\', BoxDiscretization(num_bins_per_dimension=4, force_int_bins=True))\n        ]),\n        is_a_reference_filter=False\n    )\n\n\n###############\n# Environment #\n###############\n\nenv_params = StarCraft2EnvironmentParameters(level=\'CollectMineralShards\')\nenv_params.feature_screen_maps_to_use = [5]\nenv_params.feature_minimap_maps_to_use = [5]\n\n########\n# Test #\n########\npreset_validation_params = PresetValidationParameters()\npreset_validation_params.test = True\npreset_validation_params.min_reward_threshold = 50\npreset_validation_params.max_episodes_to_achieve_reward = 200\npreset_validation_params.num_workers = 1\n\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\n                                    preset_validation_params=preset_validation_params)\n'"
rl_coach/presets/__init__.py,0,b''
rl_coach/tests/__init__.py,0,b''
rl_coach/tests/conftest.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n""""""PyTest configuration.""""""\n\nimport os\nimport shutil\nimport sys\nimport pytest\nimport rl_coach.tests.utils.args_utils as a_utils\nimport rl_coach.tests.utils.presets_utils as p_utils\nfrom rl_coach.tests.utils.definitions import Definitions as Def\nfrom os import path\n\n\n@pytest.fixture(scope=""module"", params=list(p_utils.collect_presets()))\ndef preset_name(request):\n    """"""\n    Return all preset names\n    """"""\n    yield request.param\n\n\n@pytest.fixture(scope=""function"", params=list(a_utils.collect_args()))\ndef flag(request):\n    """"""\n    Return flags names in function scope\n    """"""\n    yield request.param\n\n\n@pytest.fixture(scope=""module"", params=list(a_utils.collect_preset_for_args()))\ndef preset_args(request):\n    """"""\n    Return preset names that can be used for args testing only; working in\n    module scope\n    """"""\n    yield request.param\n\n\n@pytest.fixture(scope=""module"", params=list(a_utils.collect_preset_for_seed()))\ndef preset_args_for_seed(request):\n    """"""\n    Return preset names that can be used for args testing only and for special\n    action when using seed argument; working in module scope\n    """"""\n    yield request.param\n\n\n@pytest.fixture(scope=""module"",\n                params=list(a_utils.collect_preset_for_mxnet()))\ndef preset_for_mxnet_args(request):\n    """"""\n    Return preset names that can be used for args testing only; this special\n    fixture will be used for mxnet framework only. working in module scope\n    """"""\n    yield request.param\n\n\n@pytest.fixture(scope=""function"")\ndef clres(request):\n    """"""\n    Create both file csv and log for testing\n    :yield: class of both files paths\n    """"""\n\n    class CreateCsvLog:\n        """"""\n        Create a test and log paths\n        """"""\n        def __init__(self, csv, log, pattern):\n            self.exp_path = csv\n            self.stdout = open(log, \'w\')\n            self.fn_pattern = pattern\n\n        @property\n        def experiment_path(self):\n            return self.exp_path\n\n        @property\n        def stdout_path(self):\n            return self.stdout\n\n        @experiment_path.setter\n        def experiment_path(self, val):\n            self.exp_path = val\n\n        @stdout_path.setter\n        def stdout_path(self, val):\n            self.stdout = open(val, \'w\')\n\n    # get preset name from test request params\n    idx = 0 if \'preset\' in list(request.node.funcargs.items())[0][0] else 1\n    p_name = list(request.node.funcargs.items())[idx][1]\n\n    p_valid_params = p_utils.validation_params(p_name)\n\n    sys.path.append(\'.\')\n    test_name = \'ExpName_{}\'.format(p_name)\n    test_path = os.path.join(Def.Path.experiments, test_name)\n    if path.exists(test_path):\n        shutil.rmtree(test_path)\n\n    # get the stdout for logs results\n    log_file_name = \'test_log_{}.txt\'.format(p_name)\n    fn_pattern = \'*.csv\' if p_valid_params.num_workers > 1 else \'worker_0*.csv\'\n\n    res = CreateCsvLog(test_path, log_file_name, fn_pattern)\n\n    yield res\n\n    # clean files\n    if path.exists(res.exp_path):\n        shutil.rmtree(res.exp_path)\n\n    if path.exists(res.stdout.name):\n        os.remove(res.stdout.name)\n'"
rl_coach/tests/test_checkpoint.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport shutil\nimport subprocess\nimport time\nimport pytest\nimport signal\nimport tempfile\nimport pandas as pd\nimport rl_coach.tests.utils.args_utils as a_utils\nimport rl_coach.tests.utils.test_utils as test_utils\nimport rl_coach.tests.utils.presets_utils as p_utils\nfrom rl_coach import checkpoint\nfrom rl_coach.logger import screen\nfrom rl_coach.tests.utils.definitions import Definitions as Def\n\n\n@pytest.mark.unit_test\ndef test_get_checkpoint_state():\n    files = [\'4.test.ckpt.ext\', \'2.test.ckpt.ext\', \'3.test.ckpt.ext\',\n             \'1.test.ckpt.ext\', \'prefix.10.test.ckpt.ext\']\n    with tempfile.TemporaryDirectory() as temp_dir:\n        [open(os.path.join(temp_dir, fn), \'a\').close() for fn in files]\n        checkpoint_state = \\\n            checkpoint.get_checkpoint_state(temp_dir,\n                                            all_checkpoints=True)\n        assert checkpoint_state.model_checkpoint_path == os.path.join(\n            temp_dir, \'4.test.ckpt\')\n        assert checkpoint_state.all_model_checkpoint_paths == \\\n               [os.path.join(temp_dir, f[:-4]) for f in sorted(files[:-1])]\n\n        reader = \\\n            checkpoint.CheckpointStateReader(temp_dir,\n                                             checkpoint_state_optional=False)\n        assert reader.get_latest() is None\n        assert len(reader.get_all()) == 0\n\n        reader = checkpoint.CheckpointStateReader(temp_dir)\n        assert reader.get_latest().num == 4\n        assert [ckp.num for ckp in reader.get_all()] == [1, 2, 3, 4]\n\n\n@pytest.mark.functional_test\n@pytest.mark.parametrize(""framework"", [""tensorflow""])\ndef test_restore_checkpoint(preset_args, clres, framework,\n                            timeout=Def.TimeOuts.test_time_limit):\n    """"""\n    Create checkpoints and restore them in second run.\n    :param preset_args: all preset that can be tested for argument tests\n    :param clres: logs and csv files\n    :param framework: name of the test framework\n    :param timeout: max time for test\n    """"""\n\n    def _create_cmd_and_run(flag):\n        """"""\n        Create default command with given flag and run it\n        :param flag: name of the tested flag, this flag will be extended to the\n                     running command line\n        :return: active process\n        """"""\n        run_cmd = [\n            \'python3\', \'rl_coach/coach.py\',\n            \'-p\', \'{}\'.format(preset_args),\n            \'-e\', \'{}\'.format(""ExpName_"" + preset_args),\n            \'--seed\', \'{}\'.format(4),\n            \'-f\', \'{}\'.format(framework),\n        ]\n\n        test_flag = a_utils.add_one_flag_value(flag=flag)\n        run_cmd.extend(test_flag)\n        print(str(run_cmd))\n        p = subprocess.Popen(run_cmd, stdout=clres.stdout, stderr=clres.stdout)\n\n        return p\n\n    start_time=time.time()\n\n    if framework == ""mxnet"":\n        # update preset name - for mxnet framework we are using *_DQN\n        preset_args = Def.Presets.mxnet_args_test[0]\n        # update logs paths\n        test_name = \'ExpName_{}\'.format(preset_args)\n        test_path = os.path.join(Def.Path.experiments, test_name)\n        clres.experiment_path = test_path\n        clres.stdout_path = \'test_log_{}.txt\'.format(preset_args)\n\n    p_valid_params = p_utils.validation_params(preset_args)\n    create_cp_proc = _create_cmd_and_run(flag=[\'--checkpoint_save_secs\', \'5\'])\n\n    # wait for checkpoint files\n    csv_list = a_utils.get_csv_path(clres=clres)\n    assert len(csv_list) > 0\n    exp_dir = os.path.dirname(csv_list[0])\n\n    checkpoint_dir = os.path.join(exp_dir, Def.Path.checkpoint)\n\n    checkpoint_test_dir = os.path.join(Def.Path.experiments, Def.Path.test_dir)\n    if os.path.exists(checkpoint_test_dir):\n        shutil.rmtree(checkpoint_test_dir)\n\n    res = a_utils.is_reward_reached(csv_path=csv_list[0],\n                                    p_valid_params=p_valid_params,\n                                    start_time=start_time, time_limit=timeout)\n    if not res:\n        screen.error(open(clres.stdout.name).read(), crash=False)\n        assert False\n\n    entities = a_utils.get_files_from_dir(checkpoint_dir)\n\n    assert len(entities) > 0\n    assert any("".ckpt."" in file for file in entities)\n\n    # send CTRL+C to close experiment\n    create_cp_proc.send_signal(signal.SIGINT)\n\n    if os.path.isdir(checkpoint_dir):\n        shutil.copytree(exp_dir, checkpoint_test_dir)\n        shutil.rmtree(exp_dir)\n\n    create_cp_proc.kill()\n    checkpoint_test_dir = ""{}/{}"".format(checkpoint_test_dir,\n                                         Def.Path.checkpoint)\n    # run second time with checkpoint folder  (restore)\n    restore_cp_proc = _create_cmd_and_run(flag=[\'-crd\', checkpoint_test_dir,\n                                                \'--evaluate\'])\n\n    new_csv_list = test_utils.get_csv_path(clres=clres)\n    time.sleep(10)\n\n    csv = pd.read_csv(new_csv_list[0])\n    res = csv[\'Episode Length\'].values[-1]\n    expected_reward = 100\n    assert res >= expected_reward, Def.Consts.ASSERT_MSG.format(\n        str(expected_reward), str(res))\n    restore_cp_proc.kill()\n\n    test_folder = os.path.join(Def.Path.experiments, Def.Path.test_dir)\n    if os.path.exists(test_folder):\n        shutil.rmtree(test_folder)\n'"
rl_coach/tests/test_coach_args.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport subprocess\nimport time\nimport pytest\nimport rl_coach.tests.utils.args_utils as a_utils\nimport rl_coach.tests.utils.presets_utils as p_utils\nfrom rl_coach.logger import screen\nfrom rl_coach.tests.utils.definitions import Definitions as Def\n\n\n@pytest.mark.functional_test\ndef test_preset_args(preset_args, flag, clres, start_time=time.time(),\n                     time_limit=Def.TimeOuts.test_time_limit):\n    """""" Test command arguments - the test will check all flags one-by-one.""""""\n\n    p_valid_params = p_utils.validation_params(preset_args)\n\n    run_cmd = [\n        \'python3\', \'rl_coach/coach.py\',\n        \'-p\', \'{}\'.format(preset_args),\n        \'-e\', \'{}\'.format(""ExpName_"" + preset_args),\n    ]\n\n    if p_valid_params.reward_test_level:\n        lvl = [\'-lvl\', \'{}\'.format(p_valid_params.reward_test_level)]\n        run_cmd.extend(lvl)\n\n    # add flags to run command\n    test_flag = a_utils.add_one_flag_value(flag=flag)\n\n    if flag[0] == ""-cp"":\n        seed = [\'--seed\', \'42\']\n        seed_flag = a_utils.add_one_flag_value(flag=seed)\n        run_cmd.extend(seed_flag)\n\n    run_cmd.extend(test_flag)\n    print(str(run_cmd))\n\n    try:\n        proc = subprocess.Popen(run_cmd, stdout=clres.stdout, stderr=clres.stdout)\n\n        try:\n            a_utils.validate_arg_result(flag=test_flag,\n                                        p_valid_params=p_valid_params, clres=clres,\n                                        process=proc, start_time=start_time,\n                                        timeout=time_limit)\n        except AssertionError:\n            # close process once get assert false\n            proc.kill()\n            # if test failed - print logs\n            screen.error(open(clres.stdout.name).read(), crash=False)\n            assert False\n\n    except OSError as e:\n        # if test launch failed due to OSError - skip test\n        pytest.skip(e)\n\n    proc.kill()\n\n\n@pytest.mark.functional_test\ndef test_preset_seed(preset_args_for_seed, clres, start_time=time.time(),\n                     time_limit=Def.TimeOuts.test_time_limit):\n    """"""\n    Test command arguments - the test will check seed argument with all\n    presets\n    """"""\n\n    def close_processes():\n        """"""\n        close all processes that still active in the process list\n        """"""\n        for i in range(seed_num):\n            proc[i].kill()\n\n    proc = []\n    seed_num = 2\n    flag = [""--seed"", str(seed_num)]\n    p_valid_params = p_utils.validation_params(preset_args_for_seed)\n\n    run_cmd = [\n        \'python3\', \'rl_coach/coach.py\',\n        \'-p\', \'{}\'.format(preset_args_for_seed),\n        \'-e\', \'{}\'.format(""ExpName_"" + preset_args_for_seed),\n    ]\n\n    if p_valid_params.trace_test_levels:\n        lvl = [\'-lvl\', \'{}\'.format(p_valid_params.trace_test_levels[0])]\n        run_cmd.extend(lvl)\n\n    # add flags to run command\n    test_flag = a_utils.add_one_flag_value(flag=flag)\n    run_cmd.extend(test_flag)\n    print(str(run_cmd))\n\n    try:\n        for _ in range(seed_num):\n            proc.append(subprocess.Popen(run_cmd, stdout=clres.stdout,\n                                         stderr=clres.stdout))\n        try:\n            a_utils.validate_arg_result(flag=test_flag,\n                                        p_valid_params=p_valid_params, clres=clres,\n                                        process=proc, start_time=start_time,\n                                        timeout=time_limit)\n        except AssertionError:\n            close_processes()\n            # if test failed - print logs\n            screen.error(open(clres.stdout.name).read(), crash=False)\n            assert False\n\n    except OSError as e:\n        # if test launch failed due to OSError - skip test\n        pytest.skip(e)\n\n    close_processes()\n\n\n@pytest.mark.functional_test\ndef test_preset_n_and_ew(preset_args, clres, start_time=time.time(),\n                         time_limit=Def.TimeOuts.test_time_limit):\n    """"""\n    Test command arguments - check evaluation worker with number of workers\n    """"""\n\n    ew_flag = [\'-ew\']\n    n_flag = [\'-n\', Def.Flags.enw]\n    p_valid_params = p_utils.validation_params(preset_args)\n\n    run_cmd = [\n        \'python3\', \'rl_coach/coach.py\',\n        \'-p\', \'{}\'.format(preset_args),\n        \'-e\', \'{}\'.format(""ExpName_"" + preset_args),\n    ]\n\n    # add flags to run command\n    test_ew_flag = a_utils.add_one_flag_value(flag=ew_flag)\n    test_n_flag = a_utils.add_one_flag_value(flag=n_flag)\n    run_cmd.extend(test_ew_flag)\n    run_cmd.extend(test_n_flag)\n\n    print(str(run_cmd))\n\n    try:\n        proc = subprocess.Popen(run_cmd, stdout=clres.stdout, stderr=clres.stdout)\n\n        try:\n            a_utils.validate_arg_result(flag=test_ew_flag,\n                                        p_valid_params=p_valid_params, clres=clres,\n                                        process=proc, start_time=start_time,\n                                        timeout=time_limit)\n\n            a_utils.validate_arg_result(flag=test_n_flag,\n                                        p_valid_params=p_valid_params, clres=clres,\n                                        process=proc, start_time=start_time,\n                                        timeout=time_limit)\n        except AssertionError:\n            # close process once get assert false\n            proc.kill()\n            # if test failed - print logs\n            screen.error(open(clres.stdout.name).read(), crash=False)\n            assert False\n\n    except OSError as e:\n        # if test launch failed due to OSError - skip test\n        pytest.skip(e)\n\n    proc.kill()\n\n\n@pytest.mark.functional_test\n@pytest.mark.skip(reason=""https://github.com/NervanaSystems/coach/issues/257"")\ndef test_preset_n_and_ew_and_onnx(preset_args, clres, start_time=time.time(),\n                                  time_limit=Def.TimeOuts.test_time_limit):\n    """"""\n    Test command arguments - check evaluation worker, number of workers and\n                             onnx.\n    """"""\n\n    ew_flag = [\'-ew\']\n    n_flag = [\'-n\', Def.Flags.enw]\n    onnx_flag = [\'-onnx\']\n    s_flag = [\'-s\', Def.Flags.css]\n    p_valid_params = p_utils.validation_params(preset_args)\n\n    run_cmd = [\n        \'python3\', \'rl_coach/coach.py\',\n        \'-p\', \'{}\'.format(preset_args),\n        \'-e\', \'{}\'.format(""ExpName_"" + preset_args),\n    ]\n\n    # add flags to run command\n    test_ew_flag = a_utils.add_one_flag_value(flag=ew_flag)\n    test_n_flag = a_utils.add_one_flag_value(flag=n_flag)\n    test_onnx_flag = a_utils.add_one_flag_value(flag=onnx_flag)\n    test_s_flag = a_utils.add_one_flag_value(flag=s_flag)\n\n    run_cmd.extend(test_ew_flag)\n    run_cmd.extend(test_n_flag)\n    run_cmd.extend(test_onnx_flag)\n    run_cmd.extend(test_s_flag)\n\n    print(str(run_cmd))\n\n    try:\n        proc = subprocess.Popen(run_cmd, stdout=clres.stdout, stderr=clres.stdout)\n\n        try:\n            # Check csv files has been created\n            a_utils.validate_arg_result(flag=test_ew_flag,\n                                        p_valid_params=p_valid_params, clres=clres,\n                                        process=proc, start_time=start_time,\n                                        timeout=time_limit)\n\n            # Check csv files created same as the number of the workers\n            a_utils.validate_arg_result(flag=test_n_flag,\n                                        p_valid_params=p_valid_params, clres=clres,\n                                        process=proc, start_time=start_time,\n                                        timeout=time_limit)\n\n            # Check checkpoint files\n            a_utils.validate_arg_result(flag=test_s_flag,\n                                        p_valid_params=p_valid_params, clres=clres,\n                                        process=proc, start_time=start_time,\n                                        timeout=time_limit)\n\n            # TODO: add onnx check; issue found #257\n\n        except AssertionError:\n            # close process once get assert false\n            proc.kill()\n            # if test failed - print logs\n            screen.error(open(clres.stdout.name).read(), crash=False)\n            assert False\n\n    except OSError as e:\n        # if test launch failed due to OSError - skip test\n        pytest.skip(e)\n\n    proc.kill()\n'"
rl_coach/tests/test_core_types.py,0,"b'from rl_coach.core_types import (\n    TotalStepsCounter,\n    EnvironmentSteps,\n    EnvironmentEpisodes,\n    StepMethod,\n    EnvironmentSteps,\n    EnvironmentEpisodes,\n)\n\nimport pytest\n\n\n@pytest.mark.unit_test\ndef test_add_total_steps_counter():\n    counter = TotalStepsCounter()\n    steps = counter + EnvironmentSteps(10)\n    assert steps.num_steps == 10\n\n\n@pytest.mark.unit_test\ndef test_add_total_steps_counter_non_zero():\n    counter = TotalStepsCounter()\n    counter[EnvironmentSteps] += 10\n    steps = counter + EnvironmentSteps(10)\n    assert steps.num_steps == 20\n\n\n@pytest.mark.unit_test\ndef test_total_steps_counter_less_than():\n    counter = TotalStepsCounter()\n    steps = counter + EnvironmentSteps(0)\n    assert not (counter < steps)\n    steps = counter + EnvironmentSteps(1)\n    assert counter < steps\n\n\n@pytest.mark.unit_test\ndef test_step_method_div():\n    assert StepMethod(10) / 2 == StepMethod(5)\n    assert StepMethod(10) / StepMethod(2) == 5\n\n\n@pytest.mark.unit_test\ndef test_step_method_div_ceil():\n    assert StepMethod(10) / 3 == StepMethod(4)\n    assert StepMethod(10) / StepMethod(3) == 4\n\n\n@pytest.mark.unit_test\ndef test_step_method_rdiv_ceil():\n    assert 10 / StepMethod(3) == StepMethod(4)\n    assert StepMethod(10) / StepMethod(3) == 4\n\n\n@pytest.mark.unit_test\ndef test_step_method_rdiv():\n    assert 10 / StepMethod(2) == StepMethod(5)\n    assert StepMethod(10) / StepMethod(2) == 5\n\n\n@pytest.mark.unit_test\ndef test_step_method_div_type():\n    with pytest.raises(TypeError):\n        EnvironmentEpisodes(10) / EnvironmentSteps(2)\n'"
rl_coach/tests/test_dist_coach.py,0,"b'\nfrom configparser import ConfigParser\nimport pytest\nimport argparse\nimport os\nfrom rl_coach.coach import CoachLauncher\nimport sys\nfrom minio import Minio\n\n\ndef generate_config(image, memory_backend, s3_end_point, s3_bucket_name, s3_creds_file, config_file):\n    """"""\n    Generate the s3 config file to be used and also the dist-coach-config.template to be used for the test\n    It reads the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` env vars and fails if they are not provided.\n    """"""\n    # Write s3 creds\n    aws_config = ConfigParser({\n        \'aws_access_key_id\': os.environ.get(\'AWS_ACCESS_KEY_ID\'),\n        \'aws_secret_access_key\': os.environ.get(\'AWS_SECRET_ACCESS_KEY\')\n    }, default_section=\'default\')\n    with open(s3_creds_file, \'w\') as f:\n        aws_config.write(f)\n\n    coach_config = ConfigParser({\n        \'image\': image,\n        \'memory_backend\': memory_backend,\n        \'data_store\': \'s3\',\n        \'s3_end_point\': s3_end_point,\n        \'s3_bucket_name\': s3_bucket_name,\n        \'s3_creds_file\': s3_creds_file\n    }, default_section=""coach"")\n    with open(config_file, \'w\') as f:\n        coach_config.write(f)\n\n\ndef test_command(command):\n    """"""\n    Launches the actual training.\n    """"""\n    sys.argv = command\n    launcher = CoachLauncher()\n    with pytest.raises(SystemExit) as e:\n        launcher.launch()\n    assert e.value.code == 0\n\n\ndef clear_bucket(s3_end_point, s3_bucket_name):\n    """"""\n    Clear the bucket before starting the test.\n    """"""\n    access_key = os.environ.get(\'AWS_ACCESS_KEY_ID\')\n    secret_access_key = os.environ.get(\'AWS_SECRET_ACCESS_KEY\')\n    minio_client = Minio(s3_end_point, access_key=access_key, secret_key=secret_access_key)\n    try:\n        for obj in minio_client.list_objects_v2(s3_bucket_name, recursive=True):\n            minio_client.remove_object(s3_bucket_name, obj.object_name)\n    except Exception:\n        pass\n\n\ndef test_dc(command, image, memory_backend, s3_end_point, s3_bucket_name, s3_creds_file, config_file):\n    """"""\n    Entry point into the test\n    """"""\n    clear_bucket(s3_end_point, s3_bucket_name)\n    command = command.format(template=config_file).split(\' \')\n    test_command(command)\n\n\ndef get_tests():\n    """"""\n    All the presets to test. New presets should be added here.\n    """"""\n    tests = [\n        \'rl_coach/coach.py -p CartPole_ClippedPPO -dc -e sample -dcp {template} --dump_worker_logs -asc --is_multi_node_test --seed 1\',\n        \'rl_coach/coach.py -p Mujoco_ClippedPPO -lvl inverted_pendulum -dc -e sample -dcp {template} --dump_worker_logs -asc --is_multi_node_test --seed 1\'\n    ]\n    return tests\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \'-i\', \'--image\', help=""(string) Name of the testing image"", type=str, default=None\n    )\n    parser.add_argument(\n        \'-mb\', \'--memory_backend\', help=""(string) Name of the memory backend"", type=str, default=""redispubsub""\n    )\n    parser.add_argument(\n        \'-e\', \'--endpoint\', help=""(string) Name of the s3 endpoint"", type=str, default=\'s3.amazonaws.com\'\n    )\n    parser.add_argument(\n        \'-cr\', \'--creds_file\', help=""(string) Path of the s3 creds file"", type=str, default=\'.aws_creds\'\n    )\n    parser.add_argument(\n        \'-b\', \'--bucket\', help=""(string) Name of the bucket for s3"", type=str, default=None\n    )\n\n    args = parser.parse_args()\n\n    if not args.bucket:\n        print(""bucket_name required for s3"")\n        exit(1)\n    if not os.environ.get(\'AWS_ACCESS_KEY_ID\') or not os.environ.get(\'AWS_SECRET_ACCESS_KEY\'):\n        print(""AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY env vars need to be set"")\n        exit(1)\n\n    config_file = \'./tmp.cred\'\n    generate_config(args.image, args.memory_backend, args.endpoint, args.bucket, args.creds_file, config_file)\n    for command in get_tests():\n        test_dc(command, args.image, args.memory_backend, args.endpoint, args.bucket, args.creds_file, config_file)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
rl_coach/tests/test_eks.py,0,"b'\nimport argparse\nimport pytest\nimport time\nfrom kubernetes import client, config\n\n\nclass EKSHandler():\n\n    def __init__(self, cluster, build_num, test_name, test_command, image, cpu, memory, working_dir):\n        self.cluster = cluster\n        self.build_num = build_num\n        self.test_name = test_name\n        self.test_command = test_command\n        self.image = image\n        self.cpu = cpu\n        self.memory = memory\n        self.refresh_config()\n        self.namespace = \'{}-{}\'.format(test_name, build_num)\n        self.create_namespace()\n        self.working_dir = working_dir\n\n    def refresh_config(self):\n        # on AWS tokens only last 10 minutes so this must periodically be\n        # called to prevent auth related errors\n        config.load_kube_config()\n        self.corev1_api = client.CoreV1Api()\n\n    def create_namespace(self):\n        namespace = client.V1Namespace(\n            api_version=\'v1\',\n            kind=""Namespace"",\n            metadata=client.V1ObjectMeta(name=self.namespace)\n        )\n\n        try:\n            self.corev1_api.create_namespace(namespace)\n        except client.rest.ApiException as e:\n            raise RuntimeError(""Failed to create namesapce. Got exception: {}"".format(e))\n\n    def deploy(self):\n        container = client.V1Container(\n            name=self.test_name,\n            image=self.image,\n            command=[\'bash\', \'-c\'],\n            args=[self.test_command],\n            image_pull_policy=\'Always\',\n            working_dir=self.working_dir,\n            stdin=True,\n            tty=True\n        )\n        pod_spec = client.V1PodSpec(\n            containers=[container],\n            restart_policy=\'Never\'\n        )\n        pod = client.V1Pod(\n            api_version=""v1"",\n            kind=""Pod"",\n            metadata=client.V1ObjectMeta(name=self.test_name),\n            spec=pod_spec\n        )\n\n        try:\n            self.corev1_api.create_namespaced_pod(self.namespace, pod)\n        except client.rest.ApiException as e:\n            print(""Got exception: {} while creating a pod"".format(e))\n            return 1\n\n        return 0\n\n    def print_logs(self):\n        while True:\n            time.sleep(10)\n            # Try to tail the pod logs\n            try:\n                for line in self.corev1_api.read_namespaced_pod_log(\n                    self.test_name, self.namespace, follow=True,\n                    _preload_content=False\n                ):\n                    print(line.decode(\'utf-8\'), flush=True, end=\'\')\n                # above call blocks for pod lifetime, so we may need to refresh tokens\n                self.refresh_config()\n\n            except client.rest.ApiException as e:\n                print(""Got exception: {} while reading pod logs"".format(e))\n                pass\n\n            try:\n                pod = self.corev1_api.read_namespaced_pod(self.test_name, self.namespace)\n            except client.rest.ApiException as e:\n                print(""Got exception: {} while reading pod"".format(e))\n                continue\n\n            if not hasattr(pod, \'status\') or not pod.status:\n                continue\n            if not hasattr(pod.status, \'container_statuses\') or not pod.status.container_statuses:\n                continue\n\n            for container_status in pod.status.container_statuses:\n                if container_status.state.waiting is not None:\n                    if container_status.state.waiting.reason == \'Error\' or \\\n                       container_status.state.waiting.reason == \'CrashLoopBackOff\' or \\\n                       container_status.state.waiting.reason == \'ImagePullBackOff\' or \\\n                       container_status.state.waiting.reason == \'ErrImagePull\':\n                        return\n                if container_status.state.terminated is not None:\n                    return\n\n    def get_return_status(self):\n        # This part will get executed if the pod is one of the following phases: not ready, failed or terminated.\n        # Check if the pod has errored out, else just try again.\n        # Get the pod\n        try:\n            pod = self.corev1_api.read_namespaced_pod(self.test_name, self.namespace)\n        except client.rest.ApiException as e:\n            print(""Got exception: {} while reading pod"".format(e))\n            return 1\n\n        if not hasattr(pod, \'status\') or not pod.status:\n            return 0\n        if not hasattr(pod.status, \'container_statuses\') or not pod.status.container_statuses:\n            return 0\n\n        for container_status in pod.status.container_statuses:\n            if container_status.state.waiting is not None:\n                if container_status.state.waiting.reason == \'Error\' or \\\n                   container_status.state.waiting.reason == \'CrashLoopBackOff\' or \\\n                   container_status.state.waiting.reason == \'ImagePullBackOff\' or \\\n                   container_status.state.waiting.reason == \'ErrImagePull\':\n                    return 1\n            if container_status.state.terminated is not None:\n                return container_status.state.terminated.exit_code\n\n    def cleanup(self):\n\n        # Delete pod\n        try:\n            self.corev1_api.delete_namespaced_pod(self.test_name, self.namespace, client.V1DeleteOptions())\n        except client.rest.ApiException as e:\n            print(""Got exception while deleting pod: {}"".format(e))\n\n        # Delete namespace\n        try:\n            self.corev1_api.delete_namespace(self.namespace, client.V1DeleteOptions())\n        except client.rest.ApiException as e:\n            print(""Got exception while deleting namespace: {}"".format(e))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \'-c\', \'--cluster\', help=""(string) Name of the cluster"", type=str, required=True\n    )\n    parser.add_argument(\n        \'-bn\', \'--build-num\', help=""(int) CI Build number"", type=int, required=True\n    )\n    parser.add_argument(\n        \'-tn\', \'--test-name\', help=""(string) Name of the test"", type=str, required=True\n    )\n    parser.add_argument(\n        \'-tc\', \'--test-command\', help=""(string) command to execute"", type=str, required=True\n    )\n    parser.add_argument(\n        \'-i\', \'--image\', help=""(string) Container image"", type=str, required=True\n    )\n    parser.add_argument(\n        \'-cpu\', help=""(string) Units of cpu to use"", type=str, required=True\n    )\n    parser.add_argument(\n        \'-mem\', help=""(string) The amount in megabytes"", type=str, required=True\n    )\n    parser.add_argument(\n        \'--working-dir\', help=""(string) The working dir in the container"", type=str, required=False,\n        default=\'/root/src/docker\'\n    )\n    args = parser.parse_args()\n\n    obj = EKSHandler(\n        args.cluster, args.build_num, args.test_name, args.test_command,\n        args.image, args.cpu, args.mem, args.working_dir\n    )\n\n    if obj.deploy() != 0:\n        obj.cleanup()\n        pytest.fail(""Failed to deploy"")\n\n    obj.print_logs()\n\n    if obj.get_return_status() != 0:\n        obj.cleanup()\n        pytest.fail(""Failed to run tests"")\n\n    obj.cleanup()\n'"
rl_coach/tests/test_global_variable_saver.py,12,"b'import random\nimport pickle\n\nimport pytest\nimport tensorflow as tf\nimport numpy as np\n\nfrom rl_coach.architectures.tensorflow_components.savers import GlobalVariableSaver\n\n\ndef random_name():\n    return ""%032x"" % random.randrange(16 ** 32)\n\n\n@pytest.fixture\ndef name():\n    return random_name()\n\n\n@pytest.fixture\ndef variable(shape, name):\n    tf.reset_default_graph()\n    return tf.Variable(tf.zeros(shape), name=name)\n\n\n@pytest.fixture\ndef shape():\n    return (3, 5)\n\n\ndef assert_arrays_ones_shape(arrays, shape, name):\n    assert list(arrays.keys()) == [name]\n    assert len(arrays) == 1\n    assert np.all(list(arrays[name][0]) == np.ones(shape))\n\n\n@pytest.mark.unit_test\ndef test_global_variable_saver_to_arrays(variable, name, shape):\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        session.run(variable.assign(tf.ones(shape)))\n\n        saver = GlobalVariableSaver(""name"")\n        arrays = saver.to_arrays(session)\n        assert_arrays_ones_shape(arrays, shape, name)\n\n\n@pytest.mark.unit_test\ndef test_global_variable_saver_from_arrays(variable, name, shape):\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n\n        saver = GlobalVariableSaver(""name"")\n        saver.from_arrays(session, {name: np.ones(shape)})\n        arrays = saver.to_arrays(session)\n        assert_arrays_ones_shape(arrays, shape, name)\n\n\n@pytest.mark.unit_test\ndef test_global_variable_saver_to_string(variable, name, shape):\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        session.run(variable.assign(tf.ones(shape)))\n\n        saver = GlobalVariableSaver(""name"")\n        string = saver.to_string(session)\n        arrays = pickle.loads(string)\n        assert_arrays_ones_shape(arrays, shape, name)\n\n\n@pytest.mark.unit_test\ndef test_global_variable_saver_from_string(variable, name, shape):\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n\n        saver = GlobalVariableSaver(""name"")\n        saver.from_string(session, pickle.dumps({name: np.ones(shape)}, protocol=-1))\n        arrays = saver.to_arrays(session)\n        assert_arrays_ones_shape(arrays, shape, name)\n'"
rl_coach/tests/test_golden.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport argparse\nimport glob\nimport os\nimport shutil\nimport signal\nimport subprocess\nimport sys\nfrom importlib import import_module\nfrom os import path\nsys.path.append(\'.\')\nimport numpy as np\nimport pandas as pd\nimport time\nimport pytest\n\n# -*- coding: utf-8 -*-\nfrom rl_coach.logger import screen\n\n\ndef read_csv_paths(test_path, filename_pattern, read_csv_tries=200):\n    csv_paths = []\n    tries_counter = 0\n    while not csv_paths:\n        csv_paths = glob.glob(path.join(test_path, \'*\', filename_pattern))\n        if tries_counter > read_csv_tries:\n            break\n        tries_counter += 1\n        time.sleep(1)\n    return csv_paths\n\n\ndef print_progress(averaged_rewards, last_num_episodes, preset_validation_params, start_time, time_limit):\n    percentage = int((100 * last_num_episodes) / preset_validation_params.max_episodes_to_achieve_reward)\n    sys.stdout.write(""\\rReward: ({}/{})"".format(round(averaged_rewards[-1], 1),\n                                                preset_validation_params.min_reward_threshold))\n    sys.stdout.write(\' Time (sec): ({}/{})\'.format(round(time.time() - start_time, 2), time_limit))\n    sys.stdout.write(\' Episode: ({}/{})\'.format(last_num_episodes,\n                                                preset_validation_params.max_episodes_to_achieve_reward))\n    sys.stdout.write(\n        \' {}%|{}{}|  \'.format(percentage, \'#\' * int(percentage / 10), \' \' * (10 - int(percentage / 10))))\n    sys.stdout.flush()\n\n\ndef import_preset(preset_name):\n    return import_module(\'rl_coach.presets.{}\'.format(preset_name))\n\n\ndef validation_params(preset_name):\n    return import_preset(preset_name).graph_manager.preset_validation_params\n\n\ndef all_presets():\n    return [\n        f[:-3] for f in os.listdir(os.path.join(\'rl_coach\', \'presets\'))\n        if f[-3:] == \'.py\' and not f == \'__init__.py\'\n    ]\n\n\ndef importable(preset_name):\n    try:\n        import_preset(preset_name)\n        return True\n    except BaseException:\n        return False\n\n\ndef has_test_parameters(preset_name):\n    return bool(validation_params(preset_name).test)\n\n\ndef collect_presets():\n    for preset_name in all_presets():\n        # if it isn\'t importable, still include it so we can fail the test\n        if not importable(preset_name):\n            yield preset_name\n        # otherwise, make sure it has test parameters before including it\n        elif has_test_parameters(preset_name):\n            yield preset_name\n\n\n@pytest.fixture(params=list(collect_presets()))\ndef preset_name(request):\n    return request.param\n\n\n@pytest.mark.golden_test\ndef test_preset_reward(preset_name, no_progress_bar=True, time_limit=60 * 60, verbose=True):\n    preset_validation_params = validation_params(preset_name)\n\n    win_size = 10\n\n    test_name = \'__test_reward_{}\'.format(preset_name)\n    test_path = os.path.join(\'./experiments\', test_name)\n    if path.exists(test_path):\n        shutil.rmtree(test_path)\n\n    # run the experiment in a separate thread\n    screen.log_title(""Running test {}"".format(preset_name))\n    log_file_name = \'test_log_{preset_name}.txt\'.format(preset_name=preset_name)\n    cmd = [\n        \'python3\',\n        \'rl_coach/coach.py\',\n        \'-p\', \'{preset_name}\'.format(preset_name=preset_name),\n        \'-e\', \'{test_name}\'.format(test_name=test_name),\n        \'-n\', \'{num_workers}\'.format(num_workers=preset_validation_params.num_workers),\n        \'--seed\', \'0\',\n        \'-c\'\n    ]\n    if preset_validation_params.reward_test_level:\n        cmd += [\'-lvl\', \'{level}\'.format(level=preset_validation_params.reward_test_level)]\n\n    stdout = open(log_file_name, \'w\')\n\n    p = subprocess.Popen(cmd, stdout=stdout, stderr=stdout)\n\n    start_time = time.time()\n\n    reward_str = \'Evaluation Reward\'\n    if preset_validation_params.num_workers > 1:\n        filename_pattern = \'worker_0*.csv\'\n    else:\n        filename_pattern = \'*.csv\'\n\n    test_passed = False\n\n    # get the csv with the results\n    csv_paths = read_csv_paths(test_path, filename_pattern, read_csv_tries=preset_validation_params.read_csv_tries)\n\n    if csv_paths:\n        csv_path = csv_paths[0]\n\n        # verify results\n        csv = None\n        time.sleep(1)\n        averaged_rewards = [0]\n\n        last_num_episodes = 0\n\n        if not no_progress_bar:\n            print_progress(averaged_rewards, last_num_episodes, preset_validation_params, start_time, time_limit)\n\n        while csv is None or (csv[csv.columns[0]].values[\n                                  -1] < preset_validation_params.max_episodes_to_achieve_reward and time.time() - start_time < time_limit):\n            try:\n                csv = pd.read_csv(csv_path)\n            except:\n                # sometimes the csv is being written at the same time we are\n                # trying to read it. no problem -> try again\n                continue\n\n            if reward_str not in csv.keys():\n                continue\n\n            rewards = csv[reward_str].values\n            rewards = rewards[~np.isnan(rewards)]\n\n            if len(rewards) >= 1:\n                averaged_rewards = np.convolve(rewards, np.ones(min(len(rewards), win_size)) / win_size, mode=\'valid\')\n            else:\n                time.sleep(1)\n                continue\n\n            if not no_progress_bar:\n                print_progress(averaged_rewards, last_num_episodes, preset_validation_params, start_time, time_limit)\n\n            if csv[csv.columns[0]].shape[0] - last_num_episodes <= 0:\n                continue\n\n            last_num_episodes = csv[csv.columns[0]].values[-1]\n\n            # check if reward is enough\n            if np.any(averaged_rewards >= preset_validation_params.min_reward_threshold):\n                test_passed = True\n                break\n            time.sleep(1)\n\n    # kill test and print result\n    # os.killpg(os.getpgid(p.pid), signal.SIGKILL)\n    p.kill()\n    screen.log(\'\')\n    if test_passed:\n        screen.success(""Passed successfully"")\n    else:\n        if time.time() - start_time > time_limit:\n            screen.error(""Failed due to exceeding time limit"", crash=False)\n            if verbose:\n                screen.error(""command exitcode: {}"".format(p.returncode), crash=False)\n                screen.error(open(log_file_name).read(), crash=False)\n        elif csv_paths:\n            screen.error(""Failed due to insufficient reward"", crash=False)\n            if verbose:\n                screen.error(""command exitcode: {}"".format(p.returncode), crash=False)\n                screen.error(open(log_file_name).read(), crash=False)\n            screen.error(""preset_validation_params.max_episodes_to_achieve_reward: {}"".format(\n                preset_validation_params.max_episodes_to_achieve_reward), crash=False)\n            screen.error(""preset_validation_params.min_reward_threshold: {}"".format(\n                preset_validation_params.min_reward_threshold), crash=False)\n            screen.error(""averaged_rewards: {}"".format(averaged_rewards), crash=False)\n            screen.error(""episode number: {}"".format(csv[\'Episode #\'].values[-1]), crash=False)\n            screen.error(""training iteration: {}"".format(csv[\'Training Iter\'].values[-1]), crash=False)\n        else:\n            screen.error(""csv file never found"", crash=False)\n            if verbose:\n                screen.error(""command exitcode: {}"".format(p.returncode), crash=False)\n                screen.error(open(log_file_name).read(), crash=False)\n\n    shutil.rmtree(test_path)\n    os.remove(log_file_name)\n    if not test_passed:\n        raise ValueError(\'golden test failed\')\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-p\', \'--preset\', \'--presets\',\n                        help=""(string) Name of preset(s) to run (comma separated, and as configured in presets.py)"",\n                        default=None,\n                        type=str)\n    parser.add_argument(\'-ip\', \'--ignore_presets\',\n                        help=""(string) Name of preset(s) to ignore (comma separated, and as configured in presets.py)"",\n                        default=None,\n                        type=str)\n    parser.add_argument(\'-v\', \'--verbose\',\n                        help=""(flag) display verbose logs in the event of an error"",\n                        action=\'store_true\')\n    parser.add_argument(\'--stop_after_first_failure\',\n                        help=""(flag) stop executing tests after the first error"",\n                        action=\'store_true\')\n    parser.add_argument(\'-tl\', \'--time_limit\',\n                        help=""time limit for each test in minutes"",\n                        default=60,  # setting time limit to be so high due to DDPG being very slow - its tests are long\n                        type=int)\n    parser.add_argument(\'-np\', \'--no_progress_bar\',\n                        help=""(flag) Don\'t print the progress bar (makes jenkins logs more readable)"",\n                        action=\'store_true\')\n\n    args = parser.parse_args()\n    if args.preset is not None:\n        presets_lists = args.preset.split(\',\')\n    else:\n        presets_lists = all_presets()\n\n    fail_count = 0\n    test_count = 0\n\n    args.time_limit = 60 * args.time_limit\n\n    if args.ignore_presets is not None:\n        presets_to_ignore = args.ignore_presets.split(\',\')\n    else:\n        presets_to_ignore = []\n    for idx, preset_name in enumerate(sorted(presets_lists)):\n        if args.stop_after_first_failure and fail_count > 0:\n            break\n        if preset_name not in presets_to_ignore:\n            print(""Attempting to run Preset: %s"" % preset_name)\n            if not importable(preset_name):\n                screen.error(""Failed to load preset <{}>"".format(preset_name), crash=False)\n                fail_count += 1\n                test_count += 1\n                continue\n\n            if not has_test_parameters(preset_name):\n                continue\n\n            test_count += 1\n            try:\n                test_preset_reward(preset_name, args.no_progress_bar, args.time_limit, args.verbose)\n            except Exception as e:\n                fail_count += 1\n\n    screen.separator()\n    if fail_count == 0:\n        screen.success("" Summary: "" + str(test_count) + ""/"" + str(test_count) + "" tests passed successfully"")\n    else:\n        screen.error("" Summary: "" + str(test_count - fail_count) + ""/"" + str(test_count) + "" tests passed successfully"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
rl_coach/tests/test_saver.py,0,"b'import pytest\n\nfrom rl_coach.saver import Saver, SaverCollection\n\n\n@pytest.mark.unit_test\ndef test_checkpoint_collection():\n    class SaverTest(Saver):\n        def __init__(self, path):\n            self._path = path\n            self._count = 1\n\n        @property\n        def path(self):\n            return self._path\n\n        def merge(self, other: \'Saver\'):\n            assert isinstance(other, SaverTest)\n            assert self.path == other.path\n            self._count += other._count\n\n    # test add\n    savers = SaverCollection(SaverTest(\'123\'))\n    savers.add(SaverTest(\'123\'))\n    savers.add(SaverTest(\'456\'))\n\n    def check_collection(mul):\n        paths = [\'123\', \'456\']\n        for c in savers:\n            paths.remove(c.path)\n            if c.path == \'123\':\n                assert c._count == 2 * mul\n            elif c.path == \'456\':\n                assert c._count == 1 * mul\n            else:\n                assert False, ""invalid path""\n\n    check_collection(1)\n\n    # test update\n    savers.update(savers)\n    check_collection(2)\n'"
rl_coach/tests/test_schedules.py,0,"b'import os\nimport sys\n\nfrom rl_coach.core_types import EnvironmentSteps\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\n\nfrom rl_coach.schedules import LinearSchedule, ConstantSchedule, ExponentialSchedule, PieceWiseSchedule\nimport numpy as np\n\n\n@pytest.mark.unit_test\ndef test_constant_schedule():\n    schedule = ConstantSchedule(0.3)\n\n    # make sure the values in the constant schedule don\'t change over time\n    for i in range(1000):\n        assert schedule.initial_value == 0.3\n        assert schedule.current_value == 0.3\n        schedule.step()\n\n\n@pytest.mark.unit_test\ndef test_linear_schedule():\n    # increasing schedule\n    schedule = LinearSchedule(1, 3, 10)\n\n    # the schedule is defined in number of steps to get from 1 to 3 so there are 10 steps\n    # the linspace is defined in number of bins between 1 and 3 so theres are 11 bins\n    target_values = np.linspace(1, 3, 11)\n    for i in range(10):\n        # we round to 4 because there is a very small floating point division difference (1e-10)\n        assert round(schedule.current_value, 4) == round(target_values[i], 4)\n        schedule.step()\n\n    # make sure the value does not change after 10 steps\n    for i in range(10):\n        assert schedule.current_value == 3\n\n    # decreasing schedule\n    schedule = LinearSchedule(3, 1, 10)\n\n    target_values = np.linspace(3, 1, 11)\n    for i in range(10):\n        # we round to 4 because there is a very small floating point division difference (1e-10)\n        assert round(schedule.current_value, 4) == round(target_values[i], 4)\n        schedule.step()\n\n    # make sure the value does not change after 10 steps\n    for i in range(10):\n        assert schedule.current_value == 1\n\n    # constant schedule\n    schedule = LinearSchedule(3, 3, 10)\n\n    for i in range(10):\n        # we round to 4 because there is a very small floating point division difference (1e-10)\n        assert round(schedule.current_value, 4) == 3\n        schedule.step()\n\n\n@pytest.mark.unit_test\ndef test_exponential_schedule():\n    # decreasing schedule\n    schedule = ExponentialSchedule(10, 3, 0.99)\n\n    current_power = 1\n    for i in range(100):\n        assert round(schedule.current_value,6) == round(10*current_power,6)\n        current_power *= 0.99\n        schedule.step()\n\n    for i in range(100):\n        schedule.step()\n    assert schedule.current_value == 3\n\n\n@pytest.mark.unit_test\ndef test_piece_wise_schedule():\n    # decreasing schedule\n    schedule = PieceWiseSchedule(\n        [(LinearSchedule(1, 3, 10), EnvironmentSteps(5)),\n         (ConstantSchedule(4), EnvironmentSteps(10)),\n         (ExponentialSchedule(3, 1, 0.99), EnvironmentSteps(10))\n         ]\n    )\n\n    target_values = np.append(np.linspace(1, 2, 6), np.ones(11)*4)\n    for i in range(16):\n        assert round(schedule.current_value, 4) == round(target_values[i], 4)\n        schedule.step()\n\n    current_power = 1\n    for i in range(10):\n        assert round(schedule.current_value, 4) == round(3*current_power, 4)\n        current_power *= 0.99\n        schedule.step()\n\n\nif __name__ == ""__main__"":\n    test_constant_schedule()\n    test_linear_schedule()\n    test_exponential_schedule()\n    test_piece_wise_schedule()\n'"
rl_coach/tests/test_spaces.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace, MultiSelectActionSpace, ObservationSpace, AgentSelection, VectorObservationSpace, AttentionActionSpace\nimport numpy as np\n\n\n@pytest.mark.unit_test\ndef test_discrete():\n    action_space = DiscreteActionSpace(3, [""zero"", ""one"", ""two""])\n    assert action_space.shape == 1\n    for i in range(100):\n        assert 3 > action_space.sample() >= 0\n    action_info = action_space.sample_with_info()\n    assert action_info.all_action_probabilities[0] == 1. / 3\n    assert action_space.high == 2\n    assert action_space.low == 0\n\n    # list descriptions\n    assert action_space.get_description(1) == ""one""\n\n    # dict descriptions\n    action_space = DiscreteActionSpace(3, {1: ""one"", 2: ""two"", 0: ""zero""})\n    assert action_space.get_description(0) == ""zero""\n\n    # no descriptions\n    action_space = DiscreteActionSpace(3)\n    assert action_space.get_description(0) == ""0""\n\n    # descriptions for invalid action\n    with pytest.raises(ValueError):\n        assert action_space.get_description(3) == ""0""\n\n\n@pytest.mark.unit_test\ndef test_box():\n    # simple action space\n    action_space = BoxActionSpace(4, -5, 5, [""a"", ""b"", ""c"", ""d""])\n    for i in range(100):\n        sample = action_space.sample()\n        assert np.all(-5 <= sample) and np.all(sample <= 5)\n        assert sample.shape == (4,)\n        assert sample.dtype == float\n\n    # test clipping\n    clipped_action = action_space.clip_action_to_space(np.array([-10, 10, 2, 5]))\n    assert np.all(clipped_action == np.array([-5, 5, 2, 5]))\n\n    # more complex high and low definition\n    action_space = BoxActionSpace(4, np.array([-5, -1, -0.5, 0]), np.array([1, 2, 4, 5]), [""a"", ""b"", ""c"", ""d""])\n    for i in range(100):\n        sample = action_space.sample()\n        assert np.all(np.array([-5, -1, -0.5, 0]) <= sample) and np.all(sample <= np.array([1, 2, 4, 5]))\n        assert sample.shape == (4,)\n        assert sample.dtype == float\n\n    # test clipping\n    clipped_action = action_space.clip_action_to_space(np.array([-10, 10, 2, 5]))\n    assert np.all(clipped_action == np.array([-5, 2, 2, 5]))\n\n    # mixed high and low definition\n    action_space = BoxActionSpace(4, np.array([-5, -1, -0.5, 0]), 5, [""a"", ""b"", ""c"", ""d""])\n    for i in range(100):\n        sample = action_space.sample()\n        assert np.all(np.array([-5, -1, -0.5, 0]) <= sample) and np.all(sample <= 5)\n        assert sample.shape == (4,)\n        assert sample.dtype == float\n\n    # test clipping\n    clipped_action = action_space.clip_action_to_space(np.array([-10, 10, 2, 5]))\n    assert np.all(clipped_action == np.array([-5, 5, 2, 5]))\n\n    # invalid bounds\n    with pytest.raises(ValueError):\n        action_space = BoxActionSpace(4, np.array([-5, -1, -0.5, 0]), -1, [""a"", ""b"", ""c"", ""d""])\n\n    # TODO: test descriptions\n\n\n@pytest.mark.unit_test\ndef test_multiselect():\n    action_space = MultiSelectActionSpace(4, 2, [""a"", ""b"", ""c"", ""d""])\n    for i in range(100):\n        action = action_space.sample()\n        assert action.shape == (4,)\n        assert np.sum(action) <= 2\n\n    # check that descriptions of multiple actions are working\n    description = action_space.get_description(np.array([1, 0, 1, 0]))\n    assert description == ""a + c""\n\n    description = action_space.get_description(np.array([0, 0, 0, 0]))\n    assert description == ""no-op""\n\n\n@pytest.mark.unit_test\ndef test_attention():\n    low = np.array([-1, -2, -3, -4])\n    high = np.array([1, 2, 3, 4])\n    action_space = AttentionActionSpace(4, low=low, high=high)\n    for i in range(100):\n        action = action_space.sample()\n        assert len(action) == 2\n        assert action[0].shape == (4,)\n        assert action[1].shape == (4,)\n        assert np.all(action[0] <= action[1])\n        assert np.all(action[0] >= low)\n        assert np.all(action[1] < high)\n\n\n@pytest.mark.unit_test\ndef test_goal():\n    # TODO: test goal action space\n    pass\n\n\n@pytest.mark.unit_test\ndef test_agent_selection():\n    action_space = AgentSelection(10)\n\n    assert action_space.shape == 1\n    assert action_space.high == 9\n    assert action_space.low == 0\n    with pytest.raises(ValueError):\n        assert action_space.get_description(10)\n    assert action_space.get_description(0) == ""0""\n\n\n@pytest.mark.unit_test\ndef test_observation_space():\n    observation_space = ObservationSpace(np.array([1, 10]), -10, 10)\n\n    # testing that contains works\n    assert observation_space.contains(np.ones([1, 10]))\n    assert not observation_space.contains(np.ones([2, 10]))\n    assert not observation_space.contains(np.ones([1, 10]) * 100)\n    assert not observation_space.contains(np.ones([1, 1, 10]))\n\n    # is_valid_index\n    assert observation_space.is_valid_index(np.array([0, 9]))\n    assert observation_space.is_valid_index(np.array([0, 0]))\n    assert not observation_space.is_valid_index(np.array([1, 8]))\n    assert not observation_space.is_valid_index(np.array([0, 10]))\n    assert not observation_space.is_valid_index(np.array([-1, 6]))\n\n\n@pytest.mark.unit_test\ndef test_image_observation_space():\n    # TODO: test image observation space\n    pass\n\n\n@pytest.mark.unit_test\ndef test_measurements_observation_space():\n    # empty measurements space\n    measurements_space = VectorObservationSpace(0)\n\n    # vector space\n    measurements_space = VectorObservationSpace(3, measurements_names=[\'a\', \'b\', \'c\'])\n\n\n@pytest.mark.unit_test\ndef test_reward_space():\n    # TODO: test reward space\n    pass\n\n\n# def test_discrete_to_linspace_action_space_map():\n#     box = BoxActionSpace(2, np.array([0, 0]), np.array([10, 10]))\n#     linspace = BoxDiscretization(box, [5, 3])\n#     assert np.all(linspace.actions == np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]))\n#     assert np.all(linspace.target_actions ==\n#                   np.array([[0.0, 0.0], [0.0, 5.0], [0.0, 10.0],\n#                             [2.5, 0.0], [2.5, 5.0], [2.5, 10.0],\n#                             [5.0, 0.0], [5.0, 5.0], [5.0, 10.0],\n#                             [7.5, 0.0], [7.5, 5.0], [7.5, 10.0],\n#                             [10.0, 0.0], [10.0, 5.0], [10.0, 10.0]]))\n#\n#\n# def test_discrete_to_attention_action_space_map():\n#     attention = AttentionActionSpace(2, np.array([0, 0]), np.array([10, 10]))\n#     linspace = AttentionDiscretization(attention, 2)\n#     assert np.all(linspace.actions == np.array([0, 1, 2, 3]))\n#     assert np.all(linspace.target_actions ==\n#                   np.array(\n#                       [[[0., 0.], [5., 5.]],\n#                       [[0., 5.], [5., 10.]],\n#                       [[5., 0.], [10., 5.]],\n#                       [[5., 5.], [10., 10.]]])\n#                   )\n\n\nif __name__ == ""__main__"":\n    test_observation_space()\n    test_discrete_to_linspace_action_space_map()\n    test_discrete_to_attention_action_space_map()\n'"
rl_coach/tests/trace_tests.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport argparse\nimport glob\nimport os\nimport shutil\nimport subprocess\nimport multiprocessing\nimport sys\nimport signal\nimport pandas as pd\nimport time\nfrom configparser import ConfigParser\nfrom importlib import import_module\nfrom os import path\nsys.path.append(\'.\')\nfrom rl_coach.logger import screen\n\n\nprocesses = []\n\n\ndef sigint_handler(signum, frame):\n    for proc in processes:\n        os.killpg(os.getpgid(proc[2].pid), signal.SIGTERM)\n    for f in os.listdir(\'experiments/\'):\n        if \'__test_trace\' in f:\n            shutil.rmtree(os.path.join(\'experiments\', f))\n    for f in os.listdir(\'.\'):\n        if \'trace_test_log\' in f:\n            os.remove(f)\n    exit()\n\n\nsignal.signal(signal.SIGINT, sigint_handler)\n\n\ndef read_csv_paths(test_path, filename_pattern, read_csv_tries=100):\n    csv_paths = []\n    tries_counter = 0\n    while not csv_paths:\n        csv_paths = glob.glob(path.join(test_path, \'*\', filename_pattern))\n        if tries_counter > read_csv_tries:\n            break\n        tries_counter += 1\n        time.sleep(1)\n    return csv_paths\n\n\ndef clean_df(df):\n    if \'Wall-Clock Time\' in df.keys():\n        df.drop([\'Wall-Clock Time\'], 1, inplace=True)\n    return df\n\n\ndef run_trace_based_test(preset_name, num_env_steps, level=None):\n    test_name = \'__test_trace_{}{}\'.format(preset_name, \'_\' + level if level else \'\').replace(\':\', \'_\')\n    test_path = os.path.join(\'./experiments\', test_name)\n    if path.exists(test_path):\n        shutil.rmtree(test_path)\n\n    # run the experiment in a separate thread\n    screen.log_title(""Running test {}{}"".format(preset_name, \' - \' + level if level else \'\'))\n    log_file_name = \'trace_test_log_{preset_name}.txt\'.format(preset_name=test_name[13:])\n\n    config_file = \'./tmp.cred\'\n\n    cmd = (\n        \'python3 rl_coach/coach.py \'\n        \'-p {preset_name} \' \n        \'-e {test_name} \'\n        \'--seed 42 \'\n        \'-c \'\n        \'-dcp {template} \'\n        \'--no_summary \'\n        \'-cp {custom_param} \'\n        \'{level} \'\n        \'&> {log_file_name} \'\n    ).format(\n        preset_name=preset_name,\n        test_name=test_name,\n        template=config_file,\n        log_file_name=log_file_name,\n        level=\'-lvl \' + level if level else \'\',\n        custom_param=\'\\""improve_steps=EnvironmentSteps({n});\'\n                     \'steps_between_evaluation_periods=EnvironmentSteps({n});\'\n                     \'evaluation_steps=EnvironmentSteps(1);\'\n                     \'heatup_steps=EnvironmentSteps(1024)\\""\'.format(n=num_env_steps)\n    )\n\n    p = subprocess.Popen(cmd, shell=True, executable=""/bin/bash"", preexec_fn=os.setsid)\n\n    return test_path, log_file_name, p\n\n\ndef wait_and_check(args, processes, force=False):\n    if not force and len(processes) < args.max_threads:\n        return None\n\n    test_path = processes[0][0]\n    test_name = test_path.split(\'/\')[-1]\n    log_file_name = processes[0][1]\n    p = processes[0][2]\n    p.wait()\n\n    filename_pattern = \'*.csv\'\n\n    # get the csv with the results\n    csv_paths = read_csv_paths(test_path, filename_pattern)\n\n    test_passed = False\n    screen.log(\'Results for {}: \'.format(test_name[13:]))\n    if not csv_paths:\n        screen.error(""csv file never found"", crash=False)\n        if args.verbose:\n            screen.error(""command exitcode: {}"".format(p.returncode), crash=False)\n            screen.error(open(log_file_name).read(), crash=False)\n    else:\n        trace_path = os.path.join(\'./rl_coach\', \'traces\', test_name[13:])\n        if not os.path.exists(trace_path):\n            screen.log(\'No trace found, creating new trace in: {}\'.format(trace_path))\n            os.makedirs(trace_path)\n            df = pd.read_csv(csv_paths[0])\n            df = clean_df(df)\n            try:\n                df.to_csv(os.path.join(trace_path, \'trace.csv\'), index=False)\n            except:\n                pass\n            screen.success(""Successfully created new trace."")\n            test_passed = True\n        else:\n            test_df = pd.read_csv(csv_paths[0])\n            test_df = clean_df(test_df)\n            new_trace_csv_path = os.path.join(trace_path, \'trace_new.csv\')\n            test_df.to_csv(new_trace_csv_path, index=False)\n            test_df = pd.read_csv(new_trace_csv_path)\n            trace_csv_path = glob.glob(path.join(trace_path, \'trace.csv\'))\n            trace_csv_path = trace_csv_path[0]\n            trace_df = pd.read_csv(trace_csv_path)\n            test_passed = test_df.equals(trace_df)\n            if test_passed:\n                screen.success(""Passed successfully."")\n                os.remove(new_trace_csv_path)\n                test_passed = True\n            else:\n                screen.error(""Trace test failed."", crash=False)\n                if args.overwrite:\n                    os.remove(trace_csv_path)\n                    os.rename(new_trace_csv_path, trace_csv_path)\n                    screen.error(""Overwriting old trace."", crash=False)\n                else:\n                    screen.error(""bcompare {} {}"".format(trace_csv_path, new_trace_csv_path), crash=False)\n\n    shutil.rmtree(test_path)\n    os.remove(log_file_name)\n    processes.pop(0)\n    return test_passed\n\n\ndef generate_config(image, memory_backend, s3_end_point, s3_bucket_name, s3_creds_file, config_file):\n    """"""\n    Generate the s3 config file to be used and also the dist-coach-config.template to be used for the test\n    It reads the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` env vars and fails if they are not provided.\n    """"""\n    # Write s3 creds\n    aws_config = ConfigParser({\n        \'aws_access_key_id\': os.environ.get(\'AWS_ACCESS_KEY_ID\'),\n        \'aws_secret_access_key\': os.environ.get(\'AWS_SECRET_ACCESS_KEY\')\n    }, default_section=\'default\')\n    with open(s3_creds_file, \'w\') as f:\n        aws_config.write(f)\n\n    coach_config = ConfigParser({\n        \'image\': image,\n        \'memory_backend\': memory_backend,\n        \'data_store\': \'s3\',\n        \'s3_end_point\': s3_end_point,\n        \'s3_bucket_name\': s3_bucket_name,\n        \'s3_creds_file\': s3_creds_file\n    }, default_section=""coach"")\n    with open(config_file, \'w\') as f:\n        coach_config.write(f)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-p\', \'--preset\', \'--presets\',\n                        help=""(string) Name of preset(s) to run (comma separated, as configured in presets.py)"",\n                        default=None,\n                        type=str)\n    parser.add_argument(\'-ip\', \'--ignore_presets\',\n                        help=""(string) Name of preset(s) to ignore (comma separated, and as configured in presets.py)"",\n                        default=None,\n                        type=str)\n    parser.add_argument(\'-v\', \'--verbose\',\n                        help=""(flag) display verbose logs in the event of an error"",\n                        action=\'store_true\')\n    parser.add_argument(\'--stop_after_first_failure\',\n                        help=""(flag) stop executing tests after the first error"",\n                        action=\'store_true\')\n    parser.add_argument(\'-ow\', \'--overwrite\',\n                        help=""(flag) overwrite old trace with new ones in trace testing mode"",\n                        action=\'store_true\')\n    parser.add_argument(\'-prl\', \'--parallel\',\n                        help=""(flag) run tests in parallel"",\n                        action=\'store_true\')\n    parser.add_argument(\'-ut\', \'--update_traces\',\n                        help=""(flag) update traces on repository"",\n                        action=\'store_true\')\n    parser.add_argument(\'-mt\', \'--max_threads\',\n                        help=""(int) maximum number of threads to run in parallel"",\n                        default=multiprocessing.cpu_count()-2,\n                        type=int)\n    parser.add_argument(\n        \'-i\', \'--image\', help=""(string) Name of the testing image"", type=str, default=None\n    )\n    parser.add_argument(\n        \'-mb\', \'--memory_backend\', help=""(string) Name of the memory backend"", type=str, default=""redispubsub""\n    )\n    parser.add_argument(\n        \'-e\', \'--endpoint\', help=""(string) Name of the s3 endpoint"", type=str, default=\'s3.amazonaws.com\'\n    )\n    parser.add_argument(\n        \'-cr\', \'--creds_file\', help=""(string) Path of the s3 creds file"", type=str, default=\'.aws_creds\'\n    )\n    parser.add_argument(\n        \'-b\', \'--bucket\', help=""(string) Name of the bucket for s3"", type=str, default=None\n    )\n\n    args = parser.parse_args()\n\n    if args.update_traces:\n        if not args.bucket:\n            print(""bucket_name required for s3"")\n            exit(1)\n        if not os.environ.get(\'AWS_ACCESS_KEY_ID\') or not os.environ.get(\'AWS_SECRET_ACCESS_KEY\'):\n            print(""AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY env vars need to be set"")\n            exit(1)\n\n        config_file = \'./tmp.cred\'\n        generate_config(args.image, args.memory_backend, args.endpoint, args.bucket, args.creds_file, config_file)\n\n    if not args.parallel:\n        args.max_threads = 1\n\n    if args.preset is not None:\n        presets_lists = args.preset.split(\',\')\n    else:\n        presets_lists = [f[:-3] for f in os.listdir(os.path.join(\'rl_coach\', \'presets\')) if\n                         f[-3:] == \'.py\' and not f == \'__init__.py\']\n\n    fail_count = 0\n    test_count = 0\n\n    if args.ignore_presets is not None:\n        presets_to_ignore = args.ignore_presets.split(\',\')\n    else:\n        presets_to_ignore = []\n\n    for idx, preset_name in enumerate(sorted(presets_lists)):\n        if args.stop_after_first_failure and fail_count > 0:\n            break\n        if preset_name not in presets_to_ignore:\n            try:\n                preset = import_module(\'rl_coach.presets.{}\'.format(preset_name))\n            except:\n                screen.error(""Failed to load preset <{}>"".format(preset_name), crash=False)\n                fail_count += 1\n                test_count += 1\n                continue\n\n            preset_validation_params = preset.graph_manager.preset_validation_params\n            num_env_steps = preset_validation_params.trace_max_env_steps\n            if preset_validation_params.test_using_a_trace_test:\n                if preset_validation_params.trace_test_levels:\n                    for level in preset_validation_params.trace_test_levels:\n                        test_count += 1\n                        test_path, log_file, p = run_trace_based_test(preset_name, num_env_steps, level)\n                        processes.append((test_path, log_file, p))\n                        test_passed = wait_and_check(args, processes)\n                        if test_passed is not None and not test_passed:\n                            fail_count += 1\n                else:\n                    test_count += 1\n                    test_path, log_file, p = run_trace_based_test(preset_name, num_env_steps)\n                    processes.append((test_path, log_file, p))\n                    test_passed = wait_and_check(args, processes)\n                    if test_passed is not None and not test_passed:\n                        fail_count += 1\n\n    while len(processes) > 0:\n        test_passed = wait_and_check(args, processes, force=True)\n        if test_passed is not None and not test_passed:\n            fail_count += 1\n\n    screen.separator()\n    if fail_count == 0:\n        screen.success("" Summary: "" + str(test_count) + ""/"" + str(test_count) + "" tests passed successfully"")\n    else:\n        screen.error("" Summary: "" + str(test_count - fail_count) + ""/"" + str(test_count) + "" tests passed successfully"", crash=False)\n\n    # check fail counts just if update traces is not activated!\n    if not args.update_traces:\n        assert fail_count == 0\n\n\nif __name__ == \'__main__\':\n    os.environ[\'DISABLE_MUJOCO_RENDERING\'] = \'1\'\n    main()\n    del os.environ[\'DISABLE_MUJOCO_RENDERING\']\n'"
rl_coach/utilities/__init__.py,0,b''
rl_coach/utilities/carla_dataset_to_replay_buffer.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport argparse\nimport os\nimport sys\n\nimport h5py\nimport numpy as np\n\nfrom rl_coach.core_types import Transition\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.memories.non_episodic.experience_replay import ExperienceReplay\nfrom rl_coach.utils import ProgressBar, start_shell_command_and_wait\nfrom rl_coach.logger import screen\n\n\ndef maybe_download(dataset_root):\n    if not dataset_root or not os.path.exists(os.path.join(dataset_root, ""AgentHuman"")):\n        screen.log_title(""Downloading the CARLA dataset. This might take a while."")\n\n        google_drive_download_id = ""1hloAeyamYn-H6MfV1dRtY1gJPhkR55sY""\n        filename_to_save = ""datasets/CORL2017ImitationLearningData.tar.gz""\n        download_command = \'wget --load-cookies /tmp/cookies.txt ""https://docs.google.com/uc?export=download&confirm=\' \\\n                           \'$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies \' \\\n                           \'--no-check-certificate \\""https://docs.google.com/uc?export=download&id={}\\"" -O- | \' \\\n                           \'sed -rn \\\'s/.*confirm=([0-9A-Za-z_]+).*/\\\\1\\\\n/p\\\')&id={}"" -O {} && rm -rf /tmp/cookies.txt\'\\\n                           .format(google_drive_download_id, google_drive_download_id, filename_to_save)\n\n        # start downloading and wait for it to finish\n        start_shell_command_and_wait(download_command)\n\n        screen.log_title(""Unzipping the dataset"")\n        unzip_command = \'tar -xzf {} --checkpoint=.10000\'.format(filename_to_save)\n        if dataset_root is not None:\n            unzip_command += "" -C {}"".format(dataset_root)\n\n        if not os.path.exists(dataset_root):\n            os.makedirs(dataset_root)\n        start_shell_command_and_wait(unzip_command)\n\n\ndef create_dataset(dataset_root, output_path):\n    maybe_download(dataset_root)\n\n    dataset_root = os.path.join(dataset_root, \'AgentHuman\')\n    train_set_root = os.path.join(dataset_root, \'SeqTrain\')\n    validation_set_root = os.path.join(dataset_root, \'SeqVal\')\n\n    # training set extraction\n    memory = ExperienceReplay(max_size=(MemoryGranularity.Transitions, sys.maxsize))\n    train_set_files = sorted(os.listdir(train_set_root))\n    print(""found {} files"".format(len(train_set_files)))\n    progress_bar = ProgressBar(len(train_set_files))\n    for file_idx, file in enumerate(train_set_files[:3000]):\n        progress_bar.update(file_idx, ""extracting file {}"".format(file))\n        train_set = h5py.File(os.path.join(train_set_root, file), \'r\')\n        observations = train_set[\'rgb\'][:]  # forward camera\n        measurements = np.expand_dims(train_set[\'targets\'][:, 10], -1)  # forward speed\n        actions = train_set[\'targets\'][:, :3]  # steer, gas, brake\n\n        high_level_commands = train_set[\'targets\'][:, 24].astype(\'int\') - 2  # follow lane, left, right, straight\n\n        file_length = train_set[\'rgb\'].len()\n        assert train_set[\'rgb\'].len() == train_set[\'targets\'].len()\n\n        for transition_idx in range(file_length):\n            transition = Transition(\n                state={\n                    \'CameraRGB\': observations[transition_idx],\n                    \'measurements\': measurements[transition_idx],\n                    \'high_level_command\': high_level_commands[transition_idx]\n                },\n                action=actions[transition_idx],\n                reward=0\n            )\n            memory.store(transition)\n    progress_bar.close()\n    print(""Saving pickle file to {}"".format(output_path))\n    memory.save(output_path)\n\n\nif __name__ == ""__main__"":\n    argparser = argparse.ArgumentParser(description=__doc__)\n    argparser.add_argument(\'-d\', \'--dataset_root\', help=\'The path to the CARLA dataset root folder\')\n    argparser.add_argument(\'-o\', \'--output_path\', help=\'The path to save the resulting replay buffer\',\n                           default=\'carla_train_set_replay_buffer.p\')\n    args = argparser.parse_args()\n\n    create_dataset(args.dataset_root, args.output_path)\n'"
rl_coach/utilities/shared_running_stats.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nfrom abc import ABC, abstractmethod\nimport threading\nimport pickle\nimport redis\nimport numpy as np\n\nfrom rl_coach.utils import get_latest_checkpoint\n\n\nclass SharedRunningStatsSubscribe(threading.Thread):\n    def __init__(self, shared_running_stats):\n        super().__init__()\n        self.shared_running_stats = shared_running_stats\n        self.redis_address = self.shared_running_stats.pubsub.params.redis_address\n        self.redis_port = self.shared_running_stats.pubsub.params.redis_port\n        self.redis_connection = redis.Redis(self.redis_address, self.redis_port)\n        self.pubsub = self.redis_connection.pubsub()\n        self.channel = self.shared_running_stats.channel\n        self.pubsub.subscribe(self.channel)\n\n    def run(self):\n        for message in self.pubsub.listen():\n            try:\n                obj = pickle.loads(message[\'data\'])\n                self.shared_running_stats.push_val(obj)\n            except Exception:\n                continue\n\n\nclass SharedRunningStats(ABC):\n    def __init__(self, name="""", pubsub_params=None):\n        self.name = name\n        self.pubsub = None\n        if pubsub_params:\n            self.channel = ""channel-srs-{}"".format(self.name)\n            from rl_coach.memories.backend.memory_impl import get_memory_backend\n            self.pubsub = get_memory_backend(pubsub_params)\n            subscribe_thread = SharedRunningStatsSubscribe(self)\n            subscribe_thread.daemon = True\n            subscribe_thread.start()\n\n    @abstractmethod\n    def set_params(self, shape=[1], clip_values=None):\n        pass\n\n    def push(self, x):\n        if self.pubsub:\n            self.pubsub.redis_connection.publish(self.channel, pickle.dumps(x))\n            return\n\n        self.push_val(x)\n\n    @abstractmethod\n    def push_val(self, x):\n        pass\n\n    @property\n    @abstractmethod\n    def n(self):\n        pass\n\n    @property\n    @abstractmethod\n    def mean(self):\n        pass\n\n    @property\n    @abstractmethod\n    def var(self):\n        pass\n\n    @property\n    @abstractmethod\n    def std(self):\n        pass\n\n    @property\n    @abstractmethod\n    def shape(self):\n        pass\n\n    @abstractmethod\n    def normalize(self, batch):\n        pass\n\n    @abstractmethod\n    def set_session(self, sess):\n        pass\n\n    @abstractmethod\n    def save_state_to_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: int):\n        pass\n\n    @abstractmethod\n    def restore_state_from_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        pass\n\n\nclass NumpySharedRunningStats(SharedRunningStats):\n    def __init__(self, name, epsilon=1e-2, pubsub_params=None):\n        super().__init__(name=name, pubsub_params=pubsub_params)\n        self._count = epsilon\n        self.epsilon = epsilon\n        self.checkpoint_file_extension = \'srs\'\n\n    def set_params(self, shape=[1], clip_values=None):\n        self._shape = shape\n        self._mean = np.zeros(shape)\n        self._std = np.sqrt(self.epsilon) * np.ones(shape)\n        self._sum = np.zeros(shape)\n        self._sum_squares = self.epsilon * np.ones(shape)\n        self.clip_values = clip_values\n\n    def push_val(self, samples: np.ndarray):\n        assert len(samples.shape) >= 2  # we should always have a batch dimension\n        assert samples.shape[1:] == self._mean.shape, \'RunningStats input shape mismatch\'\n        samples = samples.astype(np.float64)\n        self._sum += samples.sum(axis=0)\n        self._sum_squares += np.square(samples).sum(axis=0)\n        self._count += np.shape(samples)[0]\n        self._mean = self._sum / self._count\n        self._std = np.sqrt(np.maximum(\n            (self._sum_squares - self._count * np.square(self._mean)) / np.maximum(self._count - 1, 1),\n            self.epsilon))\n\n    @property\n    def n(self):\n        return self._count\n\n    @property\n    def mean(self):\n        return self._mean\n\n    @property\n    def var(self):\n        return self._std ** 2\n\n    @property\n    def std(self):\n        return self._std\n\n    @property\n    def shape(self):\n        return self._mean.shape\n\n    def normalize(self, batch):\n        batch = (batch - self.mean) / (self.std + 1e-15)\n        return np.clip(batch, *self.clip_values)\n\n    def set_session(self, sess):\n        # no session for the numpy implementation\n        pass\n\n    def save_state_to_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: int):\n        dict_to_save = {\'_mean\': self._mean,\n                        \'_std\': self._std,\n                        \'_count\': self._count,\n                        \'_sum\': self._sum,\n                        \'_sum_squares\': self._sum_squares}\n\n        with open(os.path.join(checkpoint_dir, str(checkpoint_prefix) + \'.\' + self.checkpoint_file_extension), \'wb\') as f:\n            pickle.dump(dict_to_save, f, pickle.HIGHEST_PROTOCOL)\n\n    def restore_state_from_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        latest_checkpoint_filename = get_latest_checkpoint(checkpoint_dir, checkpoint_prefix,\n                                                           self.checkpoint_file_extension)\n\n        if latest_checkpoint_filename == \'\':\n            raise ValueError(""Could not find NumpySharedRunningStats checkpoint file. "")\n\n        with open(os.path.join(checkpoint_dir, str(latest_checkpoint_filename)), \'rb\') as f:\n            saved_dict = pickle.load(f)\n            self.__dict__.update(saved_dict)\n'"
tutorials/Resources/exploration.py,0,"b'\nimport numpy as np\nfrom typing import List\nfrom rl_coach.core_types import ActionType\nfrom rl_coach.spaces import ActionSpace\nfrom rl_coach.exploration_policies.exploration_policy import ExplorationPolicy, ExplorationParameters\n\n\nclass MyExplorationPolicy(ExplorationPolicy):\n    """"""\n    An exploration policy takes the predicted actions or action values from the agent, and selects the action to\n    actually apply to the environment using some predefined algorithm.\n    """"""\n    def __init__(self, action_space: ActionSpace):\n        #self.phase = RunPhase.HEATUP\n        self.action_space = action_space\n        super().__init__(action_space)\n\n    def get_action(self, action_values: List[ActionType]) -> ActionType:\n        if (np.random.rand() < 0.5):\n            chosen_action = self.action_space.sample()\n        else:\n            chosen_action = np.argmax(action_values)\n        probabilities = np.zeros(len(self.action_space.actions))\n        probabilities[chosen_action] = 1\n        return chosen_action, probabilities\n\n    def get_control_param(self):\n        return 0\n\n\n\nclass MyExplorationParameters(ExplorationParameters):\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def path(self):\n        return \'exploration:MyExplorationPolicy\'\n'"
rl_coach/architectures/mxnet_components/__init__.py,0,b''
rl_coach/architectures/mxnet_components/architecture.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom typing import Any, Dict, Generator, List, Tuple, Union\n\nimport numpy as np\nimport mxnet as mx\nfrom mxnet import autograd, gluon, nd\nfrom mxnet.ndarray import NDArray\n\nfrom rl_coach.architectures.architecture import Architecture\nfrom rl_coach.architectures.mxnet_components.heads.head import LOSS_OUT_TYPE_LOSS, LOSS_OUT_TYPE_REGULARIZATION\nfrom rl_coach.architectures.mxnet_components import utils\nfrom rl_coach.architectures.mxnet_components.savers import ParameterDictSaver, OnnxSaver\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.logger import screen\nfrom rl_coach.saver import SaverCollection\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import force_list, squeeze_list\n\n\nclass MxnetArchitecture(Architecture):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, devices: List[mx.Context],\n                 name: str = """", global_network=None, network_is_local: bool=True, network_is_trainable: bool=False):\n        """"""\n        :param agent_parameters: the agent parameters\n        :param spaces: the spaces definition of the agent\n        :param name: the name of the network\n        :param global_network: the global network replica that is shared between all the workers\n        :param network_is_local: is the network global (shared between workers) or local (dedicated to the worker)\n        :param network_is_trainable: is the network trainable (we can apply gradients on it)\n        """"""\n        super().__init__(agent_parameters, spaces, name)\n        self.middleware = None\n        self.network_is_local = network_is_local\n        self.global_network = global_network\n        if not self.network_parameters.tensorflow_support:\n            raise ValueError(\'TensorFlow is not supported for this agent\')\n        self.losses = []  # type: List[HeadLoss]\n        self.shared_accumulated_gradients = []\n        self.curr_rnn_c_in = None\n        self.curr_rnn_h_in = None\n        self.gradients_wrt_inputs = []\n        self.train_writer = None\n        self.accumulated_gradients = None\n        self.network_is_trainable = network_is_trainable\n        self.is_training = False\n        self.model = None  # type: GeneralModel\n        self._devices = self._sanitize_device_list(devices)\n\n        self.is_chief = self.ap.task_parameters.task_index == 0\n        self.network_is_global = not self.network_is_local and global_network is None\n        self.distributed_training = self.network_is_global or self.network_is_local and global_network is not None\n\n        self.optimizer_type = self.network_parameters.optimizer_type\n        if self.ap.task_parameters.seed is not None:\n            mx.random.seed(self.ap.task_parameters.seed)\n\n        # Call to child class to create the model\n        self.construct_model()\n\n        self.trainer = None  # type: gluon.Trainer\n\n    def __str__(self):\n        return self.model.summary(*self._dummy_model_inputs())\n\n    @staticmethod\n    def _sanitize_device_list(devices: List[mx.Context]) -> List[mx.Context]:\n        """"""\n        Returns intersection of devices with available devices. If no intersection, returns mx.cpu()\n        :param devices: list of requested devices\n        :return: list of devices that are actually available\n        """"""\n        actual_device = [mx.cpu()] + [mx.gpu(i) for i in mx.test_utils.list_gpus()]\n        intersection = [dev for dev in devices if dev in actual_device]\n        if len(intersection) == 0:\n            intersection = [mx.cpu()]\n            screen.log(\'Requested devices {} not available. Default to CPU context.\'.format(devices))\n        elif len(intersection) < len(devices):\n            screen.log(\'{} not available, using {}.\'.format(\n                [dev for dev in devices if dev not in intersection], intersection))\n        return intersection\n\n    def _model_grads(self, index: int=0) ->\\\n            Union[Generator[NDArray, NDArray, Any], Generator[List[NDArray], List[NDArray], Any]]:\n        """"""\n        Creates a copy of model gradients and returns them in a list, in the same order as collect_params()\n        :param index: device index. Set to -1 to get a tuple of list of NDArrays for all devices\n        :return: a generator for model gradient values\n        """"""\n        if index < 0:\n            return (p.list_grad() for p in self.model.collect_params().values() if p.grad_req != \'null\')\n        else:\n            return (p.list_grad()[index] for p in self.model.collect_params().values() if p.grad_req != \'null\')\n\n    def _model_input_shapes(self) -> List[List[int]]:\n        """"""\n        Create a list of input array shapes\n        :return: type of input shapes\n        """"""\n        allowed_inputs = copy.copy(self.spaces.state.sub_spaces)\n        allowed_inputs[""action""] = copy.copy(self.spaces.action)\n        allowed_inputs[""goal""] = copy.copy(self.spaces.goal)\n        embedders = self.model.nets[0].input_embedders\n        return list([1] + allowed_inputs[emb.embedder_name].shape.tolist() for emb in embedders)\n\n    def _dummy_model_inputs(self) -> Tuple[NDArray, ...]:\n        """"""\n        Creates a tuple of input arrays with correct shapes that can be used for shape inference\n        of the model weights and for printing the summary\n        :return: tuple of inputs for model forward pass\n        """"""\n        input_shapes = self._model_input_shapes()\n        inputs = tuple(nd.zeros(tuple(shape), ctx=self._devices[0]) for shape in input_shapes)\n        return inputs\n\n    def construct_model(self) -> None:\n        """"""\n        Construct network model. Implemented by child class.\n        """"""\n        raise NotImplementedError\n\n    def set_session(self, sess) -> None:\n        """"""\n        Initializes the model parameters and creates the model trainer.\n        NOTEL Session for mxnet backend must be None.\n        :param sess: must be None\n        """"""\n        assert sess is None\n        # FIXME Add initializer\n        self.model.collect_params().initialize(ctx=self._devices)\n        # Hybridize model and losses\n        self.model.hybridize()\n        for l in self.losses:\n            l.hybridize()\n\n        # Pass dummy data with correct shape to trigger shape inference and full parameter initialization\n        self.model(*self._dummy_model_inputs())\n\n        if self.network_is_trainable:\n            self.trainer = gluon.Trainer(\n                self.model.collect_params(), optimizer=self.optimizer, update_on_kvstore=False)\n\n    def reset_accumulated_gradients(self) -> None:\n        """"""\n        Reset model gradients as well as accumulated gradients to zero. If accumulated gradients\n        have not been created yet, it constructs them on CPU.\n        """"""\n        # Set model gradients to zero\n        for p in self.model.collect_params().values():\n            p.zero_grad()\n        # Set accumulated gradients to zero if already initialized, otherwise create a copy\n        if self.accumulated_gradients:\n            for a in self.accumulated_gradients:\n                a *= 0\n        else:\n            self.accumulated_gradients = [g.copy() for g in self._model_grads()]\n\n    def accumulate_gradients(self,\n                             inputs: Dict[str, np.ndarray],\n                             targets: List[np.ndarray],\n                             additional_fetches: List[Tuple[int, str]] = None,\n                             importance_weights: np.ndarray = None,\n                             no_accumulation: bool = False) -> Tuple[float, List[float], float, list]:\n        """"""\n        Runs a forward & backward pass, clips gradients if needed and accumulates them into the accumulation\n        :param inputs: environment states (observation, etc.) as well extra inputs required by loss. Shape of ndarray\n            is (batch_size, observation_space_size) or (batch_size, observation_space_size, stack_size)\n        :param targets: targets required by  loss (e.g. sum of discounted rewards)\n        :param additional_fetches: additional fetches to calculate and return. Each fetch is specified as (int, str)\n            tuple of head-type-index and fetch-name. The tuple is obtained from each head.\n        :param importance_weights: ndarray of shape (batch_size,) to multiply with batch loss.\n        :param no_accumulation: if True, set gradient values to the new gradients, otherwise sum with previously\n            calculated gradients\n        :return: tuple of total_loss, losses, norm_unclipped_grads, fetched_tensors\n            total_loss (float): sum of all head losses\n            losses (list of float): list of all losses. The order is list of target losses followed by list of\n                regularization losses. The specifics of losses is dependant on the network parameters\n                (number of heads, etc.)\n            norm_unclippsed_grads (float): global norm of all gradients before any gradient clipping is applied\n            fetched_tensors: all values for additional_fetches\n        """"""\n        if self.accumulated_gradients is None:\n            self.reset_accumulated_gradients()\n\n        embedders = [emb.embedder_name for emb in self.model.nets[0].input_embedders]\n        nd_inputs = tuple(nd.array(inputs[emb], ctx=self._devices[0]) for emb in embedders)\n\n        assert self.middleware.__class__.__name__ != \'LSTMMiddleware\', ""LSTM middleware not supported""\n\n        targets = force_list(targets)\n        with autograd.record():\n            out_per_head = utils.split_outputs_per_head(self.model(*nd_inputs), self.model.output_heads)\n            tgt_per_loss = utils.split_targets_per_loss(targets, self.losses)\n\n            losses = list()\n            regularizations = list()\n            additional_fetches = [(k, None) for k in additional_fetches]\n            for h, h_loss, h_out, l_tgt in zip(self.model.output_heads, self.losses, out_per_head, tgt_per_loss):\n                l_in = utils.get_loss_agent_inputs(inputs, head_type_idx=h.head_type_idx, loss=h_loss)\n                # Align arguments with loss.loss_forward and convert to NDArray\n                l_args = utils.to_mx_ndarray(utils.align_loss_args(h_out, l_in, l_tgt, h_loss), h_out[0].context)\n                # Calculate loss and all auxiliary outputs\n                loss_outputs = utils.loss_output_dict(utils.to_list(h_loss(*l_args)), h_loss.output_schema)\n                if LOSS_OUT_TYPE_LOSS in loss_outputs:\n                    losses.extend(loss_outputs[LOSS_OUT_TYPE_LOSS])\n                if LOSS_OUT_TYPE_REGULARIZATION in loss_outputs:\n                    regularizations.extend(loss_outputs[LOSS_OUT_TYPE_REGULARIZATION])\n                # Set additional fetches\n                for i, fetch in enumerate(additional_fetches):\n                    head_type_idx, fetch_name = fetch[0]  # fetch key is a tuple of (head_type_index, fetch_name)\n                    if head_type_idx == h.head_type_idx:\n                        assert fetch[1] is None  # sanity check that fetch is None\n                        additional_fetches[i] = (fetch[0], loss_outputs[fetch_name])\n\n            # Total loss is losses and regularization (NOTE: order is important)\n            total_loss_list = losses + regularizations\n            total_loss = nd.add_n(*total_loss_list)\n\n        # Calculate gradients\n        total_loss.backward()\n\n        assert self.optimizer_type != \'LBFGS\', \'LBFGS not supported\'\n\n        # allreduce gradients from all contexts\n        self.trainer.allreduce_grads()\n\n        model_grads_cpy = [g.copy() for g in self._model_grads()]\n        # Calculate global norm of gradients\n        # FIXME global norm is returned even when not used for clipping! Is this necessary?\n        # FIXME global norm might be calculated twice if clipping method is global norm\n        norm_unclipped_grads = utils.global_norm(model_grads_cpy)\n\n        # Clip gradients\n        if self.network_parameters.clip_gradients:\n            utils.clip_grad(\n                model_grads_cpy,\n                clip_method=self.network_parameters.gradients_clipping_method,\n                clip_val=self.network_parameters.clip_gradients,\n                inplace=True)\n\n        # Update self.accumulated_gradients depending on no_accumulation flag\n        if no_accumulation:\n            for acc_grad, model_grad in zip(self.accumulated_gradients, model_grads_cpy):\n                acc_grad[:] = model_grad\n        else:\n            for acc_grad, model_grad in zip(self.accumulated_gradients, model_grads_cpy):\n                acc_grad += model_grad\n\n        # result of of additional fetches\n        fetched_tensors = [fetch[1] for fetch in additional_fetches]\n\n        # convert everything to numpy or scalar before returning\n        result = utils.asnumpy_or_asscalar((total_loss, total_loss_list, norm_unclipped_grads, fetched_tensors))\n        return result\n\n    def apply_and_reset_gradients(self, gradients: List[np.ndarray], scaler: float=1.) -> None:\n        """"""\n        Applies the given gradients to the network weights and resets accumulated gradients to zero\n        :param gradients: The gradients to use for the update\n        :param scaler: A scaling factor that allows rescaling the gradients before applying them\n        """"""\n        self.apply_gradients(gradients, scaler)\n        self.reset_accumulated_gradients()\n\n    def apply_gradients(self, gradients: List[np.ndarray], scaler: float=1.) -> None:\n        """"""\n        Applies the given gradients to the network weights\n        :param gradients: The gradients to use for the update\n        :param scaler: A scaling factor that allows rescaling the gradients before applying them.\n                       The gradients will be MULTIPLIED by this factor\n        """"""\n        assert self.optimizer_type != \'LBFGS\'\n\n        batch_size = 1\n        if self.distributed_training and not self.network_parameters.async_training:\n            # rescale the gradients so that they average out with the gradients from the other workers\n            if self.network_parameters.scale_down_gradients_by_number_of_workers_for_sync_training:\n                batch_size = self.ap.task_parameters.num_training_tasks\n\n        # set parameter gradients to gradients passed in\n        for param_grad, gradient in zip(self._model_grads(-1), gradients):\n            for pg in param_grad:\n                pg[:] = gradient\n        # update gradients\n        self.trainer.update(batch_size=batch_size)\n\n    def _predict(self, inputs: Dict[str, np.ndarray]) -> Tuple[NDArray, ...]:\n        """"""\n        Run a forward pass of the network using the given input\n        :param inputs: The input dictionary for the network. Key is name of the embedder.\n        :return: The network output\n\n        WARNING: must only call once per state since each call is assumed by LSTM to be a new time step.\n        """"""\n        embedders = [emb.embedder_name for emb in self.model.nets[0].input_embedders]\n        nd_inputs = tuple(nd.array(inputs[emb], ctx=self._devices[0]) for emb in embedders)\n\n        assert self.middleware.__class__.__name__ != \'LSTMMiddleware\'\n\n        output = self.model(*nd_inputs)\n        return output\n\n    def predict(self,\n                inputs: Dict[str, np.ndarray],\n                outputs: List[str]=None,\n                squeeze_output: bool=True,\n                initial_feed_dict: Dict[str, np.ndarray]=None) -> Tuple[np.ndarray, ...]:\n        """"""\n        Run a forward pass of the network using the given input\n        :param inputs: The input dictionary for the network. Key is name of the embedder.\n        :param outputs: list of outputs to return. Return all outputs if unspecified (currently not supported)\n        :param squeeze_output: call squeeze_list on output if True\n        :param initial_feed_dict: a dictionary of extra inputs for forward pass (currently not supported)\n        :return: The network output\n\n        WARNING: must only call once per state since each call is assumed by LSTM to be a new time step.\n        """"""\n        assert initial_feed_dict is None, ""initial_feed_dict must be None""\n        assert outputs is None, ""outputs must be None""\n\n        output = self._predict(inputs)\n        output = list(o.asnumpy() for o in output)\n        if squeeze_output:\n            output = squeeze_list(output)\n        return output\n\n    @staticmethod\n    def parallel_predict(sess: Any,\n                         network_input_tuples: List[Tuple[\'MxnetArchitecture\', Dict[str, np.ndarray]]]) -> \\\n            Tuple[np.ndarray, ...]:\n        """"""\n        :param sess: active session to use for prediction (must be None for MXNet)\n        :param network_input_tuples: tuple of network and corresponding input\n        :return: tuple of outputs from all networks\n        """"""\n        assert sess is None\n        output = list()\n        for net, inputs in network_input_tuples:\n            output += net._predict(inputs)\n        return tuple(o.asnumpy() for o in output)\n\n    def train_on_batch(self,\n                       inputs: Dict[str, np.ndarray],\n                       targets: List[np.ndarray],\n                       scaler: float = 1.,\n                       additional_fetches: list = None,\n                       importance_weights: np.ndarray = None) -> Tuple[float, List[float], float, list]:\n        """"""\n        Given a batch of inputs (e.g. states) and targets (e.g. discounted rewards), takes a training step: i.e. runs a\n        forward pass and backward pass of the network, accumulates the gradients and applies an optimization step to\n        update the weights.\n        :param inputs: environment states (observation, etc.) as well extra inputs required by loss. Shape of ndarray\n            is (batch_size, observation_space_size) or (batch_size, observation_space_size, stack_size)\n        :param targets: targets required by  loss (e.g. sum of discounted rewards)\n        :param scaler: value to scale gradients by before optimizing network weights\n        :param additional_fetches: additional fetches to calculate and return. Each fetch is specified as (int, str)\n            tuple of head-type-index and fetch-name. The tuple is obtained from each head.\n        :param importance_weights: ndarray of shape (batch_size,) to multiply with batch loss.\n        :return: tuple of total_loss, losses, norm_unclipped_grads, fetched_tensors\n            total_loss (float): sum of all head losses\n            losses (list of float): list of all losses. The order is list of target losses followed by list\n                of regularization losses. The specifics of losses is dependant on the network parameters\n                (number of heads, etc.)\n            norm_unclippsed_grads (float): global norm of all gradients before any gradient clipping is applied\n            fetched_tensors: all values for additional_fetches\n        """"""\n        loss = self.accumulate_gradients(inputs, targets, additional_fetches=additional_fetches,\n                                         importance_weights=importance_weights)\n        self.apply_and_reset_gradients(self.accumulated_gradients, scaler)\n        return loss\n\n    def get_weights(self) -> gluon.ParameterDict:\n        """"""\n        :return: a ParameterDict containing all network weights\n        """"""\n        return self.model.collect_params()\n\n    def set_weights(self, weights: gluon.ParameterDict, new_rate: float=1.0) -> None:\n        """"""\n        Sets the network weights from the given ParameterDict\n        :param new_rate: ratio for adding new and old weight values: val=rate*weights + (1-rate)*old_weights\n        """"""\n        old_weights = self.model.collect_params()\n        for name, p in weights.items():\n            name = name[len(weights.prefix):]  # Strip prefix\n            old_p = old_weights[old_weights.prefix + name]  # Add prefix\n            old_p.set_data(new_rate * p._reduce() + (1 - new_rate) * old_p._reduce())\n\n    def get_variable_value(self, variable: Union[gluon.Parameter, NDArray]) -> np.ndarray:\n        """"""\n        Get the value of a variable\n        :param variable: the variable\n        :return: the value of the variable\n        """"""\n        if isinstance(variable, gluon.Parameter):\n            variable = variable._reduce().asnumpy()\n        if isinstance(variable, NDArray):\n            return variable.asnumpy()\n        return variable\n\n    def set_variable_value(self, assign_op: callable, value: Any, placeholder=None) -> None:\n        """"""\n        Updates value of a variable.\n        :param assign_op: a callable assign function for setting the variable\n        :param value: a value to set the variable to\n        :param placeholder: unused (placeholder in symbolic framework backends)\n        """"""\n        assert callable(assign_op)\n        assign_op(value)\n\n    def set_is_training(self, state: bool) -> None:\n        """"""\n        Set the phase of the network between training and testing\n        :param state: The current state (True = Training, False = Testing)\n        :return: None\n        """"""\n        self.is_training = state\n\n    def reset_internal_memory(self) -> None:\n        """"""\n        Reset any internal memory used by the network. For example, an LSTM internal state\n        :return: None\n        """"""\n        assert self.middleware.__class__.__name__ != \'LSTMMiddleware\', \'LSTM middleware not supported\'\n\n    def collect_savers(self, parent_path_suffix: str) -> SaverCollection:\n        """"""\n        Collection of all checkpoints for the network (typically only one checkpoint)\n        :param parent_path_suffix: path suffix of the parent of the network\n            (e.g. could be name of level manager plus name of agent)\n        :return: checkpoint collection for the network\n        """"""\n        name = self.name.replace(\'/\', \'.\')\n        savers = SaverCollection(ParameterDictSaver(\n            name=""{}.{}"".format(parent_path_suffix, name),\n            param_dict=self.model.collect_params()))\n        if self.ap.task_parameters.export_onnx_graph:\n            savers.add(OnnxSaver(\n                name=""{}.{}.onnx"".format(parent_path_suffix, name),\n                model=self.model,\n                input_shapes=self._model_input_shapes()))\n        return savers\n'"
rl_coach/architectures/mxnet_components/general_network.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom itertools import chain\nfrom typing import List, Tuple, Union\nfrom types import ModuleType\n\nimport numpy as np\nimport mxnet as mx\nfrom mxnet import nd, sym\nfrom mxnet.gluon import HybridBlock\nfrom mxnet.ndarray import NDArray\nfrom mxnet.symbol import Symbol\n\nfrom rl_coach.base_parameters import NetworkParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import HeadParameters, PPOHeadParameters\nfrom rl_coach.architectures.head_parameters import PPOVHeadParameters, VHeadParameters, QHeadParameters\nfrom rl_coach.architectures.middleware_parameters import MiddlewareParameters\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters, LSTMMiddlewareParameters\nfrom rl_coach.architectures.mxnet_components.architecture import MxnetArchitecture\nfrom rl_coach.architectures.mxnet_components.embedders import ImageEmbedder, TensorEmbedder, VectorEmbedder\nfrom rl_coach.architectures.mxnet_components.heads import Head, HeadLoss, PPOHead, PPOVHead, VHead, QHead\nfrom rl_coach.architectures.mxnet_components.middlewares import FCMiddleware, LSTMMiddleware\nfrom rl_coach.architectures.mxnet_components import utils\nfrom rl_coach.base_parameters import AgentParameters, Device, DeviceType, EmbeddingMergerType\nfrom rl_coach.spaces import SpacesDefinition, PlanarMapsObservationSpace, TensorObservationSpace\n\n\nclass GeneralMxnetNetwork(MxnetArchitecture):\n    """"""\n    A generalized version of all possible networks implemented using mxnet.\n    """"""\n    @staticmethod\n    def construct(variable_scope: str, devices: List[str], *args, **kwargs) -> \'GeneralTensorFlowNetwork\':\n        """"""\n        Construct a network class using the provided variable scope and on requested devices\n        :param variable_scope: string specifying variable scope under which to create network variables\n        :param devices: list of devices (can be list of Device objects, or string for TF distributed)\n        :param args: all other arguments for class initializer\n        :param kwargs: all other keyword arguments for class initializer\n        :return: a GeneralTensorFlowNetwork object\n        """"""\n        return GeneralMxnetNetwork(*args, devices=[GeneralMxnetNetwork._mx_device(d) for d in devices], **kwargs)\n\n    @staticmethod\n    def _mx_device(device: Union[str, Device]) -> mx.Context:\n        """"""\n        Convert device to tensorflow-specific device representation\n        :param device: either a specific string (used in distributed mode) which is returned without\n            any change or a Device type\n        :return: tensorflow-specific string for device\n        """"""\n        if isinstance(device, Device):\n            if device.device_type == DeviceType.CPU:\n                return mx.cpu()\n            elif device.device_type == DeviceType.GPU:\n                return mx.gpu(device.index)\n            else:\n                raise ValueError(""Invalid device_type: {}"".format(device.device_type))\n        else:\n            raise ValueError(""Invalid device instance type: {}"".format(type(device)))\n\n    def __init__(self,\n                 agent_parameters: AgentParameters,\n                 spaces: SpacesDefinition,\n                 devices: List[mx.Context],\n                 name: str,\n                 global_network=None,\n                 network_is_local: bool=True,\n                 network_is_trainable: bool=False):\n        """"""\n        :param agent_parameters: the agent parameters\n        :param spaces: the spaces definition of the agent\n        :param devices: list of devices to run the network on\n        :param name: the name of the network\n        :param global_network: the global network replica that is shared between all the workers\n        :param network_is_local: is the network global (shared between workers) or local (dedicated to the worker)\n        :param network_is_trainable: is the network trainable (we can apply gradients on it)\n        """"""\n        self.network_wrapper_name = name.split(\'/\')[0]\n        self.network_parameters = agent_parameters.network_wrappers[self.network_wrapper_name]\n        if self.network_parameters.use_separate_networks_per_head:\n            self.num_heads_per_network = 1\n            self.num_networks = len(self.network_parameters.heads_parameters)\n        else:\n            self.num_heads_per_network = len(self.network_parameters.heads_parameters)\n            self.num_networks = 1\n\n        super().__init__(agent_parameters, spaces, devices, name, global_network,\n                         network_is_local, network_is_trainable)\n\n    def construct_model(self):\n        # validate the configuration\n        if len(self.network_parameters.input_embedders_parameters) == 0:\n            raise ValueError(""At least one input type should be defined"")\n\n        if len(self.network_parameters.heads_parameters) == 0:\n            raise ValueError(""At least one output type should be defined"")\n\n        if self.network_parameters.middleware_parameters is None:\n            raise ValueError(""Exactly one middleware type should be defined"")\n\n        self.model = GeneralModel(\n            num_networks=self.num_networks,\n            num_heads_per_network=self.num_heads_per_network,\n            network_is_local=self.network_is_local,\n            network_name=self.network_wrapper_name,\n            agent_parameters=self.ap,\n            network_parameters=self.network_parameters,\n            spaces=self.spaces)\n\n        self.losses = self.model.losses()\n\n        # Learning rate\n        lr_scheduler = None\n        if self.network_parameters.learning_rate_decay_rate != 0:\n            lr_scheduler = mx.lr_scheduler.FactorScheduler(\n                step=self.network_parameters.learning_rate_decay_steps,\n                factor=self.network_parameters.learning_rate_decay_rate)\n\n        # Optimizer\n        # FIXME Does this code for distributed training make sense?\n        if self.distributed_training and self.network_is_local and self.network_parameters.shared_optimizer:\n            # distributed training + is a local network + optimizer shared -> take the global optimizer\n            self.optimizer = self.global_network.optimizer\n        elif (self.distributed_training and self.network_is_local and not self.network_parameters.shared_optimizer)\\\n                or self.network_parameters.shared_optimizer or not self.distributed_training:\n\n            if self.network_parameters.optimizer_type == \'Adam\':\n                self.optimizer = mx.optimizer.Adam(\n                    learning_rate=self.network_parameters.learning_rate,\n                    beta1=self.network_parameters.adam_optimizer_beta1,\n                    beta2=self.network_parameters.adam_optimizer_beta2,\n                    epsilon=self.network_parameters.optimizer_epsilon,\n                    lr_scheduler=lr_scheduler)\n            elif self.network_parameters.optimizer_type == \'RMSProp\':\n                self.optimizer = mx.optimizer.RMSProp(\n                    learning_rate=self.network_parameters.learning_rate,\n                    gamma1=self.network_parameters.rms_prop_optimizer_decay,\n                    epsilon=self.network_parameters.optimizer_epsilon,\n                    lr_scheduler=lr_scheduler)\n            elif self.network_parameters.optimizer_type == \'LBFGS\':\n                raise NotImplementedError(\'LBFGS optimizer not implemented\')\n            else:\n                raise Exception(""{} is not a valid optimizer type"".format(self.network_parameters.optimizer_type))\n\n    @property\n    def output_heads(self):\n        return self.model.output_heads\n\n\ndef _get_activation(activation_function_string: str):\n    """"""\n    Map the activation function from a string to the mxnet framework equivalent\n    :param activation_function_string: the type of the activation function\n    :return: mxnet activation function string\n    """"""\n    return utils.get_mxnet_activation_name(activation_function_string)\n\n\ndef _sanitize_activation(params: Union[InputEmbedderParameters, MiddlewareParameters, HeadParameters]) ->\\\n        Union[InputEmbedderParameters, MiddlewareParameters, HeadParameters]:\n    """"""\n    Change activation function to the mxnet specific value\n    :param params: any parameter that has activation_function property\n    :return: copy of params with activation function correctly set\n    """"""\n    params_copy = copy.copy(params)\n    params_copy.activation_function = _get_activation(params.activation_function)\n    return params_copy\n\n\ndef _get_input_embedder(spaces: SpacesDefinition,\n                        input_name: str,\n                        embedder_params: InputEmbedderParameters) -> ModuleType:\n    """"""\n    Given an input embedder parameters class, creates the input embedder and returns it\n    :param input_name: the name of the input to the embedder (used for retrieving the shape). The input should\n                       be a value within the state or the action.\n    :param embedder_params: the parameters of the class of the embedder\n    :return: the embedder instance\n    """"""\n    allowed_inputs = copy.copy(spaces.state.sub_spaces)\n    allowed_inputs[""action""] = copy.copy(spaces.action)\n    allowed_inputs[""goal""] = copy.copy(spaces.goal)\n\n    if input_name not in allowed_inputs.keys():\n        raise ValueError(""The key for the input embedder ({}) must match one of the following keys: {}""\n                         .format(input_name, allowed_inputs.keys()))\n\n    type = ""vector""\n    if isinstance(allowed_inputs[input_name], TensorObservationSpace):\n        type = ""tensor""\n    elif isinstance(allowed_inputs[input_name], PlanarMapsObservationSpace):\n        type = ""image""\n\n    def sanitize_params(params: InputEmbedderParameters):\n        params_copy = _sanitize_activation(params)\n        # params_copy.input_rescaling = params_copy.input_rescaling[type]\n        # params_copy.input_offset = params_copy.input_offset[type]\n        params_copy.name = input_name\n        return params_copy\n\n    embedder_params = sanitize_params(embedder_params)\n    if type == \'vector\':\n        module = VectorEmbedder(embedder_params)\n    elif type == \'image\':\n        module = ImageEmbedder(embedder_params)\n    elif type == \'tensor\':\n        module = TensorEmbedder(embedder_params)\n    else:\n        raise KeyError(\'Unsupported embedder type: {}\'.format(type))\n    return module\n\n\ndef _get_middleware(middleware_params: MiddlewareParameters) -> ModuleType:\n    """"""\n    Given a middleware type, creates the middleware and returns it\n    :param middleware_params: the paramaeters of the middleware class\n    :return: the middleware instance\n    """"""\n    middleware_params = _sanitize_activation(middleware_params)\n    if isinstance(middleware_params, FCMiddlewareParameters):\n        module = FCMiddleware(middleware_params)\n    elif isinstance(middleware_params, LSTMMiddlewareParameters):\n        module = LSTMMiddleware(middleware_params)\n    else:\n        raise KeyError(\'Unsupported middleware type: {}\'.format(type(middleware_params)))\n\n    return module\n\n\ndef _get_output_head(\n        head_params: HeadParameters,\n        head_idx: int,\n        head_type_index: int,\n        agent_params: AgentParameters,\n        spaces: SpacesDefinition,\n        network_name: str,\n        is_local: bool) -> Head:\n    """"""\n    Given a head type, creates the head and returns it\n    :param head_params: the parameters of the head to create\n    :param head_idx: the head index\n    :param head_type_index: the head type index (same index if head_param.num_output_head_copies>0)\n    :param agent_params: agent parameters\n    :param spaces: state and action space definitions\n    :param network_name: name of the network\n    :param is_local:\n    :return: head block\n    """"""\n    head_params = _sanitize_activation(head_params)\n    if isinstance(head_params, PPOHeadParameters):\n        module = PPOHead(\n            agent_parameters=agent_params,\n            spaces=spaces,\n            network_name=network_name,\n            head_type_idx=head_type_index,\n            loss_weight=head_params.loss_weight,\n            is_local=is_local,\n            activation_function=head_params.activation_function,\n            dense_layer=head_params.dense_layer)\n    elif isinstance(head_params, VHeadParameters):\n        module = VHead(\n            agent_parameters=agent_params,\n            spaces=spaces,\n            network_name=network_name,\n            head_type_idx=head_type_index,\n            loss_weight=head_params.loss_weight,\n            is_local=is_local,\n            activation_function=head_params.activation_function,\n            dense_layer=head_params.dense_layer)\n    elif isinstance(head_params, PPOVHeadParameters):\n        module = PPOVHead(\n            agent_parameters=agent_params,\n            spaces=spaces,\n            network_name=network_name,\n            head_type_idx=head_type_index,\n            loss_weight=head_params.loss_weight,\n            is_local=is_local,\n            activation_function=head_params.activation_function,\n            dense_layer=head_params.dense_layer)\n    elif isinstance(head_params, QHeadParameters):\n        module = QHead(\n            agent_parameters=agent_params,\n            spaces=spaces,\n            network_name=network_name,\n            head_type_idx=head_type_index,\n            loss_weight=head_params.loss_weight,\n            is_local=is_local,\n            activation_function=head_params.activation_function,\n            dense_layer=head_params.dense_layer)\n    else:\n        raise KeyError(\'Unsupported head type: {}\'.format(type(head_params)))\n\n    return module\n\n\nclass ScaledGradHead(HybridBlock, utils.OnnxHandlerBlock):\n    """"""\n    Wrapper block for applying gradient scaling to input before feeding the head network\n    """"""\n    def __init__(self,\n                 head_index: int,\n                 head_type_index: int,\n                 network_name: str,\n                 spaces: SpacesDefinition,\n                 network_is_local: bool,\n                 agent_params: AgentParameters,\n                 head_params: HeadParameters) -> None:\n        """"""\n        :param head_index: the head index\n        :param head_type_index: the head type index (same index if head_param.num_output_head_copies>0)\n        :param network_name: name of the network\n        :param spaces: state and action space definitions\n        :param network_is_local: whether network is local\n        :param agent_params: agent parameters\n        :param head_params: head parameters\n        """"""\n        super(ScaledGradHead, self).__init__()\n        utils.OnnxHandlerBlock.__init__(self)\n\n        head_params = _sanitize_activation(head_params)\n        with self.name_scope():\n            self.head = _get_output_head(\n                head_params=head_params,\n                head_idx=head_index,\n                head_type_index=head_type_index,\n                agent_params=agent_params,\n                spaces=spaces,\n                network_name=network_name,\n                is_local=network_is_local)\n            self.gradient_rescaler = self.params.get_constant(\n                name=\'gradient_rescaler\',\n                value=np.array([float(head_params.rescale_gradient_from_head_by_factor)]))\n            # self.gradient_rescaler = self.params.get(\n            #     name=\'gradient_rescaler\',\n            #     shape=(1,),\n            #     init=mx.init.Constant(float(head_params.rescale_gradient_from_head_by_factor)))\n\n    def hybrid_forward(self,\n                       F: ModuleType,\n                       x: Union[NDArray, Symbol],\n                       gradient_rescaler: Union[NDArray, Symbol]) -> Tuple[Union[NDArray, Symbol], ...]:\n        """""" Overrides gluon.HybridBlock.hybrid_forward\n        :param nd or sym F: ndarray or symbol module\n        :param x: head input\n        :param gradient_rescaler: gradient rescaler for partial blocking of gradient\n        :return: head output\n        """"""\n        if self._onnx:\n            # ONNX doesn\'t support BlockGrad() operator, but it\'s not typically needed for\n            # ONNX because mostly forward calls are performed using ONNX exported network.\n            grad_scaled_x = x\n        else:\n            grad_scaled_x = (F.broadcast_mul((1 - gradient_rescaler), F.BlockGrad(x)) +\n                             F.broadcast_mul(gradient_rescaler, x))\n        out = self.head(grad_scaled_x)\n        return out\n\n\nclass SingleModel(HybridBlock):\n    """"""\n    Block that connects a single embedder, with middleware and one to multiple heads\n    """"""\n    def __init__(self,\n                 network_is_local: bool,\n                 network_name: str,\n                 agent_parameters: AgentParameters,\n                 in_emb_param_dict: {str: InputEmbedderParameters},\n                 embedding_merger_type: EmbeddingMergerType,\n                 middleware_param: MiddlewareParameters,\n                 head_param_list: [HeadParameters],\n                 head_type_idx_start: int,\n                 spaces: SpacesDefinition,\n                 *args, **kwargs):\n        """"""\n        :param network_is_local: True if network is local\n        :param network_name: name of the network\n        :param agent_parameters: agent parameters\n        :param in_emb_param_dict: dictionary of embedder name to embedding parameters\n        :param embedding_merger_type: type of merging output of embedders: concatenate or sum\n        :param middleware_param: middleware parameters\n        :param head_param_list: list of head parameters, one per head type\n        :param head_type_idx_start: start index for head type index counting\n        :param spaces: state and action space definition\n        """"""\n        super(SingleModel, self).__init__(*args, **kwargs)\n\n        self._embedding_merger_type = embedding_merger_type\n        self._input_embedders = list()  # type: List[HybridBlock]\n        self._output_heads = list()  # type: List[ScaledGradHead]\n\n        with self.name_scope():\n            for input_name in sorted(in_emb_param_dict):\n                input_type = in_emb_param_dict[input_name]\n                input_embedder = _get_input_embedder(spaces, input_name, input_type)\n                self.register_child(input_embedder)\n                self._input_embedders.append(input_embedder)\n\n            self.middleware = _get_middleware(middleware_param)\n\n            for i, head_param in enumerate(head_param_list):\n                for head_copy_idx in range(head_param.num_output_head_copies):\n                    # create output head and add it to the output heads list\n                    output_head = ScaledGradHead(\n                        head_index=(head_type_idx_start + i) * head_param.num_output_head_copies + head_copy_idx,\n                        head_type_index=head_type_idx_start + i,\n                        network_name=network_name,\n                        spaces=spaces,\n                        network_is_local=network_is_local,\n                        agent_params=agent_parameters,\n                        head_params=head_param)\n                    self.register_child(output_head)\n                    self._output_heads.append(output_head)\n\n    def hybrid_forward(self, F, *inputs: Union[NDArray, Symbol]) -> Tuple[Union[NDArray, Symbol], ...]:\n        """""" Overrides gluon.HybridBlock.hybrid_forward\n        :param nd or sym F: ndarray or symbol block\n        :param inputs: model inputs, one for each embedder\n        :return: head outputs in a tuple\n        """"""\n        # Input Embeddings\n        state_embedding = list()\n        for input, embedder in zip(inputs, self._input_embedders):\n            state_embedding.append(embedder(input))\n\n        # Merger\n        if len(state_embedding) == 1:\n            state_embedding = state_embedding[0]\n        else:\n            if self._embedding_merger_type == EmbeddingMergerType.Concat:\n                state_embedding = F.concat(*state_embedding, dim=1, name=\'merger\')  # NC or NCHW layout\n            elif self._embedding_merger_type == EmbeddingMergerType.Sum:\n                state_embedding = F.add_n(*state_embedding, name=\'merger\')\n\n        # Middleware\n        state_embedding = self.middleware(state_embedding)\n\n        # Head\n        outputs = tuple()\n        for head in self._output_heads:\n            out = head(state_embedding)\n            if not isinstance(out, tuple):\n                out = (out,)\n            outputs += out\n\n        return outputs\n\n    @property\n    def input_embedders(self) -> List[HybridBlock]:\n        """"""\n        :return: list of input embedders\n        """"""\n        return self._input_embedders\n\n    @property\n    def output_heads(self) -> List[Head]:\n        """"""\n        :return: list of output heads\n        """"""\n        return [h.head for h in self._output_heads]\n\n\nclass GeneralModel(HybridBlock):\n    """"""\n    Block that creates multiple single models\n    """"""\n    def __init__(self,\n                 num_networks: int,\n                 num_heads_per_network: int,\n                 network_is_local: bool,\n                 network_name: str,\n                 agent_parameters: AgentParameters,\n                 network_parameters: NetworkParameters,\n                 spaces: SpacesDefinition,\n                 *args, **kwargs):\n        """"""\n        :param num_networks: number of networks to create\n        :param num_heads_per_network: number of heads per network to create\n        :param network_is_local: True if network is local\n        :param network_name: name of the network\n        :param agent_parameters: agent parameters\n        :param network_parameters: network parameters\n        :param spaces: state and action space definitions\n        """"""\n        super(GeneralModel, self).__init__(*args, **kwargs)\n\n        with self.name_scope():\n            self.nets = list()\n            for network_idx in range(num_networks):\n                head_type_idx_start = network_idx * num_heads_per_network\n                head_type_idx_end = head_type_idx_start + num_heads_per_network\n                net = SingleModel(\n                    head_type_idx_start=head_type_idx_start,\n                    network_name=network_name,\n                    network_is_local=network_is_local,\n                    agent_parameters=agent_parameters,\n                    in_emb_param_dict=network_parameters.input_embedders_parameters,\n                    embedding_merger_type=network_parameters.embedding_merger_type,\n                    middleware_param=network_parameters.middleware_parameters,\n                    head_param_list=network_parameters.heads_parameters[head_type_idx_start:head_type_idx_end],\n                    spaces=spaces)\n                self.register_child(net)\n                self.nets.append(net)\n\n    def hybrid_forward(self, F, *inputs):\n        """""" Overrides gluon.HybridBlock.hybrid_forward\n        :param nd or sym F: ndarray or symbol block\n        :param inputs: model inputs, one for each embedder. Passed to all networks.\n        :return: head outputs in a tuple\n        """"""\n        outputs = tuple()\n        for net in self.nets:\n            out = net(*inputs)\n            outputs += out\n        return outputs\n\n    @property\n    def output_heads(self) -> List[Head]:\n        """""" Return all heads in a single list\n        Note: There is a one-to-one mapping between output_heads and losses\n        :return: list of heads\n        """"""\n        return list(chain.from_iterable(net.output_heads for net in self.nets))\n\n    def losses(self) -> List[HeadLoss]:\n        """""" Construct loss blocks for network training\n        Note: There is a one-to-one mapping between output_heads and losses\n        :return: list of loss blocks\n        """"""\n        return [h.loss() for net in self.nets for h in net.output_heads]\n'"
rl_coach/architectures/mxnet_components/layers.py,1,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\n""""""\nModule implementing basic layers in mxnet\n""""""\n\nfrom types import FunctionType\n\nfrom mxnet.gluon import nn\n\nfrom rl_coach.architectures import layers\nfrom rl_coach.architectures.mxnet_components import utils\n\n\n# define global dictionary for storing layer type to layer implementation mapping\nmx_layer_dict = dict()\n\n\ndef reg_to_mx(layer_type) -> FunctionType:\n    """""" function decorator that registers layer implementation\n    :return: decorated function\n    """"""\n    def reg_impl_decorator(func):\n        assert layer_type not in mx_layer_dict\n        mx_layer_dict[layer_type] = func\n        return func\n    return reg_impl_decorator\n\n\ndef convert_layer(layer):\n    """"""\n    If layer is callable, return layer, otherwise convert to MX type\n    :param layer: layer to be converted\n    :return: converted layer if not callable, otherwise layer itself\n    """"""\n    if callable(layer):\n        return layer\n    return mx_layer_dict[type(layer)](layer)\n\n\nclass Conv2d(layers.Conv2d):\n    def __init__(self, num_filters: int, kernel_size: int, strides: int):\n        super(Conv2d, self).__init__(num_filters=num_filters, kernel_size=kernel_size, strides=strides)\n\n    def __call__(self) -> nn.Conv2D:\n        """"""\n        returns a conv2d block\n        :return: conv2d block\n        """"""\n        return nn.Conv2D(channels=self.num_filters, kernel_size=self.kernel_size, strides=self.strides)\n\n    @staticmethod\n    @reg_to_mx(layers.Conv2d)\n    def to_mx(base: layers.Conv2d):\n        return Conv2d(num_filters=base.num_filters, kernel_size=base.kernel_size, strides=base.strides)\n\n\nclass BatchnormActivationDropout(layers.BatchnormActivationDropout):\n    def __init__(self, batchnorm: bool=False, activation_function=None, dropout_rate: float=0):\n        super(BatchnormActivationDropout, self).__init__(\n            batchnorm=batchnorm, activation_function=activation_function, dropout_rate=dropout_rate)\n\n    def __call__(self):\n        """"""\n        returns a list of mxnet batchnorm, activation and dropout layers\n        :return: batchnorm, activation and dropout layers\n        """"""\n        block = nn.HybridSequential()\n        if self.batchnorm:\n            block.add(nn.BatchNorm())\n        if self.activation_function:\n            block.add(nn.Activation(activation=utils.get_mxnet_activation_name(self.activation_function)))\n        if self.dropout_rate:\n            block.add(nn.Dropout(self.dropout_rate))\n        return block\n\n    @staticmethod\n    @reg_to_mx(layers.BatchnormActivationDropout)\n    def to_mx(base: layers.BatchnormActivationDropout):\n        return BatchnormActivationDropout(\n            batchnorm=base.batchnorm,\n            activation_function=base.activation_function,\n            dropout_rate=base.dropout_rate)\n\n\nclass Dense(layers.Dense):\n    def __init__(self, units: int):\n        super(Dense, self).__init__(units=units)\n\n    def __call__(self):\n        """"""\n        returns a mxnet dense layer\n        :return: dense layer\n        """"""\n        # Set flatten to False for consistent behavior with tf.layers.dense\n        return nn.Dense(self.units, flatten=False)\n\n    @staticmethod\n    @reg_to_mx(layers.Dense)\n    def to_mx(base: layers.Dense):\n        return Dense(units=base.units)\n'"
rl_coach/architectures/mxnet_components/savers.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom typing import Any, List, Tuple\n\nfrom mxnet import gluon, sym\nfrom mxnet.contrib import onnx as onnx_mxnet\nimport numpy as np\n\nfrom rl_coach.architectures.mxnet_components.utils import ScopedOnnxEnable\nfrom rl_coach.saver import Saver\n\n\nclass ParameterDictSaver(Saver):\n    """"""\n    Child class that implements saver for mxnet gluon parameter dictionary\n    """"""\n    def __init__(self, name: str, param_dict: gluon.ParameterDict):\n        self._name = name\n        self._param_dict = param_dict\n\n    @property\n    def path(self):\n        """"""\n        Relative path for save/load. If two checkpoint objects return the same path, they must be merge-able.\n        """"""\n        return self._name\n\n    def save(self, sess: None, save_path: str) -> List[str]:\n        """"""\n        Save to save_path\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param save_path: full path to save checkpoint (typically directory plus self.path plus checkpoint count).\n        :return: list of all saved paths\n        """"""\n        assert sess is None\n        self._param_dict.save(save_path)\n        return [save_path]\n\n    def restore(self, sess: Any, restore_path: str):\n        """"""\n        Restore from restore_path\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param restore_path: full path to load checkpoint from.\n        """"""\n        assert sess is None\n        self._param_dict.load(restore_path)\n\n    def merge(self, other: \'Saver\'):\n        """"""\n        Merge other saver into this saver\n        :param other: saver to be merged into self\n        """"""\n        if not isinstance(other, ParameterDictSaver):\n            raise TypeError(\'merging only supported with ParameterDictSaver (type:{})\'.format(type(other)))\n        self._param_dict.update(other._param_dict)\n\n\nclass OnnxSaver(Saver):\n    """"""\n    Child class that implements saver for exporting gluon HybridBlock to ONNX\n    """"""\n    def __init__(self, name: str, model: gluon.HybridBlock, input_shapes: List[List[int]]):\n        self._name = name\n        self._sym = self._get_onnx_sym(model, len(input_shapes))\n        self._param_dict = model.collect_params()\n        self._input_shapes = input_shapes\n\n    @staticmethod\n    def _get_onnx_sym(model: gluon.HybridBlock, num_inputs: int) -> sym.Symbol:\n        """"""\n        Returns a symbolic graph for the model\n        :param model: gluon HybridBlock that constructs the symbolic graph\n        :param num_inputs: number of inputs to the graph\n        :return: symbol for the network\n        """"""\n        var_args = [sym.Variable(\'Data{}\'.format(i)) for i in range(num_inputs)]\n        with ScopedOnnxEnable(model):\n            return sym.Group(gluon.block._flatten(model(*var_args), ""output"")[0])\n\n    @property\n    def path(self):\n        """"""\n        Relative path for save/load. If two checkpoint objects return the same path, they must be merge-able.\n        """"""\n        return self._name\n\n    def save(self, sess: None, save_path: str) -> List[str]:\n        """"""\n        Save to save_path\n        :param sess: active session for session-based frameworks (e.g. TF). Must be None.\n        :param save_path: full path to save checkpoint (typically directory plus self.path plus checkpoint count).\n        :return: list of all saved paths\n        """"""\n        assert sess is None\n        params = {name:param._reduce() for name, param in self._param_dict.items()}\n        export_path = onnx_mxnet.export_model(self._sym, params, self._input_shapes, np.float32, save_path)\n\n        return [export_path]\n\n    def restore(self, sess: Any, restore_path: str):\n        """"""\n        Restore from restore_path\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param restore_path: full path to load checkpoint from.\n        """"""\n        assert sess is None\n        # Nothing to restore for ONNX\n\n    def merge(self, other: \'Saver\'):\n        """"""\n        Merge other saver into this saver\n        :param other: saver to be merged into self\n        """"""\n        # No merging is supported for ONNX. self.path must be unique\n        raise RuntimeError(\'merging not supported for ONNX exporter\')'"
rl_coach/architectures/mxnet_components/utils.py,2,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\n""""""\nModule defining utility functions\n""""""\nimport inspect\nfrom typing import Any, Dict, Generator, Iterable, List, Tuple, Union\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom mxnet import gluon, nd\nfrom mxnet.ndarray import NDArray\nimport numpy as np\n\nfrom rl_coach.core_types import GradientClippingMethod\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\ndef to_mx_ndarray(data: Union[list, tuple, np.ndarray, NDArray, int, float], ctx: mx.Context=None) ->\\\n        Union[List[NDArray], Tuple[NDArray], NDArray]:\n    """"""\n    Convert data to mx.nd.NDArray. Data can be a list or tuple of np.ndarray, int, or float or\n    it can be np.ndarray, int, or float\n    :param data: input data to be converted\n    :param ctx: context of the data (CPU, GPU0, GPU1, etc.)\n    :return: converted output data\n    """"""\n    if isinstance(data, list):\n        data = [to_mx_ndarray(d, ctx=ctx) for d in data]\n    elif isinstance(data, tuple):\n        data = tuple(to_mx_ndarray(d, ctx=ctx) for d in data)\n    elif isinstance(data, np.ndarray):\n        data = nd.array(data, ctx=ctx)\n    elif isinstance(data, NDArray):\n        assert data.context == ctx\n        pass\n    elif isinstance(data, int) or isinstance(data, float):\n        data = nd.array([data], ctx=ctx)\n    else:\n        raise TypeError(\'Unsupported data type: {}\'.format(type(data)))\n    return data\n\n\ndef asnumpy_or_asscalar(data: Union[NDArray, list, tuple]) -> Union[np.ndarray, np.number, list, tuple]:\n    """"""\n    Convert NDArray (or list or tuple of NDArray) to numpy. If shape is (1,), then convert to scalar instead.\n    NOTE: This behavior is consistent with tensorflow\n    :param data: NDArray or list or tuple of NDArray\n    :return: data converted to numpy ndarray or to numpy scalar\n    """"""\n    if isinstance(data, list):\n        data = [asnumpy_or_asscalar(d) for d in data]\n    elif isinstance(data, tuple):\n        data = tuple(asnumpy_or_asscalar(d) for d in data)\n    elif isinstance(data, NDArray):\n        data = data.asscalar() if data.shape == (1,) else data.asnumpy()\n    else:\n        raise TypeError(\'Unsupported data type: {}\'.format(type(data)))\n    return data\n\n\ndef global_norm(arrays: Union[Generator[NDArray, NDArray, NDArray], List[NDArray], Tuple[NDArray]]) -> NDArray:\n    """"""\n    Calculate global norm on list or tuple of NDArrays using this formula:\n        `global_norm = sqrt(sum([l2norm(p)**2 for p in parameters]))`\n\n    :param arrays: list or tuple of parameters to calculate global norm on\n    :return: single-value NDArray\n    """"""\n    def _norm(array):\n        if array.stype == \'default\':\n            x = array.reshape((-1,))\n            return nd.dot(x, x)\n        return array.norm().square()\n\n    total_norm = nd.add_n(*[_norm(arr) for arr in arrays])\n    total_norm = nd.sqrt(total_norm)\n    return total_norm\n\n\ndef split_outputs_per_head(outputs: Tuple[NDArray], heads: list) -> List[List[NDArray]]:\n    """"""\n    Split outputs into outputs per head\n    :param outputs: list of all outputs\n    :param heads: list of all heads\n    :return: list of outputs for each head\n    """"""\n    head_outputs = []\n    for h in heads:\n        head_outputs.append(list(outputs[:h.num_outputs]))\n        outputs = outputs[h.num_outputs:]\n    assert len(outputs) == 0\n    return head_outputs\n\n\ndef split_targets_per_loss(targets: list, losses: list) -> List[list]:\n    """"""\n    Splits targets into targets per loss\n    :param targets: list of all targets (typically numpy ndarray)\n    :param losses: list of all losses\n    :return: list of targets for each loss\n    """"""\n    loss_targets = list()\n    for l in losses:\n        loss_data_len = len(l.input_schema.targets)\n        assert len(targets) >= loss_data_len, ""Data length doesn\'t match schema""\n        loss_targets.append(targets[:loss_data_len])\n        targets = targets[loss_data_len:]\n    assert len(targets) == 0\n    return loss_targets\n\n\ndef get_loss_agent_inputs(inputs: Dict[str, np.ndarray], head_type_idx: int, loss: Any) -> List[np.ndarray]:\n    """"""\n    Collects all inputs with prefix \'output_<head_idx>_\' and matches them against agent_inputs in loss input schema.\n    :param inputs: list of all agent inputs\n    :param head_type_idx: head-type index of the corresponding head\n    :param loss: corresponding loss\n    :return: list of agent inputs for this loss. This list matches the length in loss input schema.\n    """"""\n    loss_inputs = list()\n    for k in sorted(inputs.keys()):\n        if k.startswith(\'output_{}_\'.format(head_type_idx)):\n            loss_inputs.append(inputs[k])\n    # Enforce that number of inputs for head_type are the same as agent_inputs specified by loss input_schema\n    assert len(loss_inputs) == len(loss.input_schema.agent_inputs), ""agent_input length doesn\'t match schema""\n    return loss_inputs\n\n\ndef align_loss_args(\n        head_outputs: List[NDArray],\n        agent_inputs: List[np.ndarray],\n        targets: List[np.ndarray],\n        loss: Any) -> List[np.ndarray]:\n    """"""\n    Creates a list of arguments from head_outputs, agent_inputs, and targets aligned with parameters of\n    loss.loss_forward() based on their name in loss input_schema\n    :param head_outputs: list of all head_outputs for this loss\n    :param agent_inputs: list of all agent_inputs for this loss\n    :param targets: list of all targets for this loss\n    :param loss: corresponding loss\n    :return: list of arguments in correct order to be passed to loss\n    """"""\n    arg_list = list()\n    schema = loss.input_schema\n    assert len(schema.head_outputs) == len(head_outputs)\n    assert len(schema.agent_inputs) == len(agent_inputs)\n    assert len(schema.targets) == len(targets)\n\n    prev_found = True\n    for arg_name in inspect.getfullargspec(loss.loss_forward).args[2:]:  # First two args are self and F\n        found = False\n        for schema_list, data in [(schema.head_outputs, head_outputs),\n                                  (schema.agent_inputs, agent_inputs),\n                                  (schema.targets, targets)]:\n            try:\n                arg_list.append(data[schema_list.index(arg_name)])\n                found = True\n                break\n            except ValueError:\n                continue\n        assert not found or prev_found, ""missing arguments detected!""\n        prev_found = found\n    return arg_list\n\n\ndef to_tuple(data: Union[tuple, list, Any]):\n    """"""\n    If input is list, it is converted to tuple. If it\'s tuple, it is returned untouched. Otherwise\n    returns a single-element tuple of the data.\n    :return: tuple-ified data\n    """"""\n    if isinstance(data, tuple):\n        pass\n    elif isinstance(data, list):\n        data = tuple(data)\n    else:\n        data = (data,)\n    return data\n\n\ndef to_list(data: Union[tuple, list, Any]):\n    """"""\n    If input is tuple, it is converted to list. If it\'s list, it is returned untouched. Otherwise\n    returns a single-element list of the data.\n    :return: list-ified data\n    """"""\n    if isinstance(data, list):\n        pass\n    elif isinstance(data, tuple):\n        data = list(data)\n    else:\n        data = [data]\n    return data\n\n\ndef loss_output_dict(output: List[NDArray], schema: List[str]) -> Dict[str, List[NDArray]]:\n    """"""\n    Creates a dictionary for loss output based on the output schema. If two output values have the same\n    type string in the schema they are concatenated in the same dicrionary item.\n    :param output: list of output values\n    :param schema: list of type-strings for output values\n    :return: dictionary of keyword to list of NDArrays\n    """"""\n    assert len(output) == len(schema)\n    output_dict = dict()\n    for name, val in zip(schema, output):\n        if name in output_dict:\n            output_dict[name].append(val)\n        else:\n            output_dict[name] = [val]\n    return output_dict\n\n\ndef clip_grad(\n        grads: Union[Generator[NDArray, NDArray, NDArray], List[NDArray], Tuple[NDArray]],\n        clip_method: GradientClippingMethod,\n        clip_val: float,\n        inplace=True) -> List[NDArray]:\n    """"""\n    Clip gradient values inplace\n    :param grads: gradients to be clipped\n    :param clip_method: clipping method\n    :param clip_val: clipping value. Interpreted differently depending on clipping method.\n    :param inplace: modify grads if True, otherwise create NDArrays\n    :return: clipped gradients\n    """"""\n    output = list(grads) if inplace else list(nd.empty(g.shape) for g in grads)\n    if clip_method == GradientClippingMethod.ClipByGlobalNorm:\n        norm_unclipped_grads = global_norm(grads)\n        scale = clip_val / (norm_unclipped_grads.asscalar() + 1e-8)  # todo: use branching operators?\n        if scale < 1.0:\n            for g, o in zip(grads, output):\n                nd.broadcast_mul(g, nd.array([scale]), out=o)\n    elif clip_method == GradientClippingMethod.ClipByValue:\n        for g, o in zip(grads, output):\n            g.clip(-clip_val, clip_val, out=o)\n    elif clip_method == GradientClippingMethod.ClipByNorm:\n        for g, o in zip(grads, output):\n            nd.broadcast_mul(g, nd.minimum(1.0, clip_val / (g.norm() + 1e-8)), out=o)\n    else:\n        raise KeyError(\'Unsupported gradient clipping method\')\n    return output\n\n\ndef hybrid_clip(F: ModuleType, x: nd_sym_type, clip_lower: nd_sym_type, clip_upper: nd_sym_type) -> nd_sym_type:\n    """"""\n    Apply clipping to input x between clip_lower and clip_upper.\n    Added because F.clip doesn\'t support clipping bounds that are mx.nd.NDArray or mx.sym.Symbol.\n\n    :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n    :param x: input data\n    :param clip_lower: lower bound used for clipping, should be of shape (1,)\n    :param clip_upper: upper bound used for clipping, should be of shape (1,)\n    :return: clipped data\n    """"""\n    x_clip_lower = broadcast_like(F, clip_lower, x)\n    x_clip_upper = broadcast_like(F, clip_upper, x)\n    x_clipped = F.minimum(F.maximum(x, x_clip_lower), x_clip_upper)\n    return x_clipped\n\n\ndef broadcast_like(F: ModuleType, x: nd_sym_type, y: nd_sym_type) -> nd_sym_type:\n    """"""\n    Implementation of broadcast_like using broadcast_add and broadcast_mul because ONNX doesn\'t support broadcast_like.\n    :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n    :param x: input to be broadcast\n    :param y: tensor to broadcast x like\n    :return: broadcast x\n    """"""\n    return F.broadcast_mul(x, (y * 0) + 1)\n\n\ndef get_mxnet_activation_name(activation_name: str):\n    """"""\n    Convert coach activation name to mxnet specific activation name\n    :param activation_name: name of the activation inc coach\n    :return: name of the activation in mxnet\n    """"""\n    activation_functions = {\n        \'relu\': \'relu\',\n        \'tanh\': \'tanh\',\n        \'sigmoid\': \'sigmoid\',\n        # FIXME Add other activations\n        # \'elu\': tf.nn.elu,\n        \'selu\': \'softrelu\',\n        # \'leaky_relu\': tf.nn.leaky_relu,\n        \'none\': None\n    }\n    assert activation_name in activation_functions, \\\n        ""Activation function must be one of the following {}. instead it was: {}"".format(\n            activation_functions.keys(), activation_name)\n    return activation_functions[activation_name]\n\n\nclass OnnxHandlerBlock(object):\n    """"""\n    Helper base class for gluon blocks that must behave differently for ONNX export forward pass\n    """"""\n    def __init__(self):\n        self._onnx = False\n\n    def enable_onnx(self):\n        self._onnx = True\n\n    def disable_onnx(self):\n        self._onnx = False\n\n\nclass ScopedOnnxEnable(object):\n    """"""\n    Helper scoped ONNX enable class\n    """"""\n    def __init__(self, net: gluon.HybridBlock):\n        self._onnx_handlers = self._get_onnx_handlers(net)\n\n    def __enter__(self):\n        for b in self._onnx_handlers:\n            b.enable_onnx()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        for b in self._onnx_handlers:\n            b.disable_onnx()\n\n    @staticmethod\n    def _get_onnx_handlers(block: gluon.HybridBlock) -> List[OnnxHandlerBlock]:\n        """"""\n        Iterates through all child blocks and return all of them that are instance of OnnxHandlerBlock\n        :return: list of OnnxHandlerBlock child blocks\n        """"""\n        handlers = list()\n        if isinstance(block, OnnxHandlerBlock):\n            handlers.append(block)\n        for child_block in block._children.values():\n            handlers += ScopedOnnxEnable._get_onnx_handlers(child_block)\n        return handlers\n'"
rl_coach/architectures/tensorflow_components/__init__.py,0,b''
rl_coach/architectures/tensorflow_components/architecture.py,43,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport time\nfrom typing import Any, List, Tuple, Dict\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom rl_coach.architectures.architecture import Architecture\nfrom rl_coach.architectures.tensorflow_components.savers import GlobalVariableSaver\nfrom rl_coach.base_parameters import AgentParameters, DistributedTaskParameters\nfrom rl_coach.core_types import GradientClippingMethod\nfrom rl_coach.saver import SaverCollection\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import force_list, squeeze_list, start_shell_command_and_wait\n\n\ndef variable_summaries(var):\n    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n    with tf.name_scope(\'summaries\'):\n        layer_weight_name = \'_\'.join(var.name.split(\'/\')[-3:])[:-2]\n\n        with tf.name_scope(layer_weight_name):\n            mean = tf.reduce_mean(var)\n            tf.summary.scalar(\'mean\', mean)\n            with tf.name_scope(\'stddev\'):\n                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n            tf.summary.scalar(\'stddev\', stddev)\n            tf.summary.scalar(\'max\', tf.reduce_max(var))\n            tf.summary.scalar(\'min\', tf.reduce_min(var))\n            tf.summary.histogram(\'histogram\', var)\n\n\ndef local_getter(getter, name, *args, **kwargs):\n    """"""\n    This is a wrapper around the tf.get_variable function which puts the variables in the local variables collection\n    instead of the global variables collection. The local variables collection will hold variables which are not shared\n    between workers. these variables are also assumed to be non-trainable (the optimizer does not apply gradients to\n    these variables), but we can calculate the gradients wrt these variables, and we can update their content.\n    """"""\n    kwargs[\'collections\'] = [tf.GraphKeys.LOCAL_VARIABLES]\n    return getter(name, *args, **kwargs)\n\n\nclass TensorFlowArchitecture(Architecture):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, name: str= """",\n                 global_network=None, network_is_local: bool=True, network_is_trainable: bool=False):\n        """"""\n        :param agent_parameters: the agent parameters\n        :param spaces: the spaces definition of the agent\n        :param name: the name of the network\n        :param global_network: the global network replica that is shared between all the workers\n        :param network_is_local: is the network global (shared between workers) or local (dedicated to the worker)\n        :param network_is_trainable: is the network trainable (we can apply gradients on it)\n        """"""\n        super().__init__(agent_parameters, spaces, name)\n        self.middleware = None\n        self.network_is_local = network_is_local\n        self.global_network = global_network\n        if not self.network_parameters.tensorflow_support:\n            raise ValueError(\'TensorFlow is not supported for this agent\')\n        self.sess = None\n        self.inputs = {}\n        self.outputs = []\n        self.targets = []\n        self.importance_weights = []\n        self.losses = []\n        self.total_loss = None\n        self.trainable_weights = []\n        self.weights_placeholders = []\n        self.shared_accumulated_gradients = []\n        self.curr_rnn_c_in = None\n        self.curr_rnn_h_in = None\n        self.gradients_wrt_inputs = []\n        self.train_writer = None\n        self.accumulated_gradients = None\n        self.network_is_trainable = network_is_trainable\n\n        self.is_chief = self.ap.task_parameters.task_index == 0\n        self.network_is_global = not self.network_is_local and global_network is None\n        self.distributed_training = self.network_is_global or self.network_is_local and global_network is not None\n\n        self.optimizer_type = self.network_parameters.optimizer_type\n        if self.ap.task_parameters.seed is not None:\n            tf.set_random_seed(self.ap.task_parameters.seed)\n        with tf.variable_scope(""/"".join(self.name.split(""/"")[1:]), initializer=tf.contrib.layers.xavier_initializer(),\n                               custom_getter=local_getter if network_is_local and global_network else None):\n            self.global_step = tf.train.get_or_create_global_step()\n\n            # build the network\n            self.weights = self.get_model()\n\n            # create the placeholder for the assigning gradients and some tensorboard summaries for the weights\n            for idx, var in enumerate(self.weights):\n                placeholder = tf.placeholder(tf.float32, shape=var.get_shape(), name=str(idx) + \'_holder\')\n                self.weights_placeholders.append(placeholder)\n                if self.ap.visualization.tensorboard:\n                    variable_summaries(var)\n\n            # create op for assigning a list of weights to the network weights\n            self.update_weights_from_list = [weights.assign(holder) for holder, weights in\n                                             zip(self.weights_placeholders, self.weights)]\n\n            # locks for synchronous training\n            if self.network_is_global:\n                self._create_locks_for_synchronous_training()\n\n            # gradients ops\n            self._create_gradient_ops()\n\n            self.inc_step = self.global_step.assign_add(1)\n\n            # reset LSTM hidden cells\n            self.reset_internal_memory()\n\n            if self.ap.visualization.tensorboard:\n                current_scope_summaries = tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                                            scope=tf.contrib.framework.get_name_scope())\n                self.merged = tf.summary.merge(current_scope_summaries)\n\n            # initialize or restore model\n            self.init_op = tf.group(\n                tf.global_variables_initializer(),\n                tf.local_variables_initializer()\n            )\n\n            # set the fetches for training\n            self._set_initial_fetch_list()\n\n    def get_model(self) -> List:\n        """"""\n        Constructs the model using `network_parameters` and sets `input_embedders`, `middleware`,\n        `output_heads`, `outputs`, `losses`, `total_loss`, `adaptive_learning_rate_scheme`,\n        `current_learning_rate`, and `optimizer`.\n\n        :return: A list of the model\'s weights\n        """"""\n        raise NotImplementedError\n\n    def _set_initial_fetch_list(self):\n        """"""\n        Create an initial list of tensors to fetch in each training iteration\n        :return: None\n        """"""\n        self.train_fetches = [self.gradients_norm]\n        if self.network_parameters.clip_gradients:\n            self.train_fetches.append(self.clipped_grads)\n        else:\n            self.train_fetches.append(self.tensor_gradients)\n        self.train_fetches += [self.total_loss, self.losses]\n        if self.middleware.__class__.__name__ == \'LSTMMiddleware\':\n            self.train_fetches.append(self.middleware.state_out)\n        self.additional_fetches_start_idx = len(self.train_fetches)\n\n    def _create_locks_for_synchronous_training(self):\n        """"""\n        Create locks for synchronizing the different workers during training\n        :return: None\n        """"""\n        self.lock_counter = tf.get_variable(""lock_counter"", [], tf.int32,\n                                            initializer=tf.constant_initializer(0, dtype=tf.int32),\n                                            trainable=False)\n        self.lock = self.lock_counter.assign_add(1, use_locking=True)\n        self.lock_init = self.lock_counter.assign(0)\n\n        self.release_counter = tf.get_variable(""release_counter"", [], tf.int32,\n                                               initializer=tf.constant_initializer(0, dtype=tf.int32),\n                                               trainable=False)\n        self.release = self.release_counter.assign_add(1, use_locking=True)\n        self.release_decrement = self.release_counter.assign_add(-1, use_locking=True)\n        self.release_init = self.release_counter.assign(0)\n\n    def _create_gradient_ops(self):\n        """"""\n        Create all the tensorflow operations for calculating gradients, processing the gradients and applying them\n        :return: None\n        """"""\n\n        self.tensor_gradients = tf.gradients(self.total_loss, self.weights)\n        self.gradients_norm = tf.global_norm(self.tensor_gradients)\n\n        # gradient clipping\n        if self.network_parameters.clip_gradients is not None and self.network_parameters.clip_gradients != 0:\n            self._create_gradient_clipping_ops()\n\n        # when using a shared optimizer, we create accumulators to store gradients from all the workers before\n        # applying them\n        if self.distributed_training:\n            self._create_gradient_accumulators()\n\n        # gradients of the outputs w.r.t. the inputs\n        self.gradients_wrt_inputs = [{name: tf.gradients(output, input_ph) for name, input_ph in\n                                      self.inputs.items()} for output in self.outputs]\n        self.gradients_weights_ph = [tf.placeholder(\'float32\', self.outputs[i].shape, \'output_gradient_weights\')\n                                     for i in range(len(self.outputs))]\n        self.weighted_gradients = []\n        for i in range(len(self.outputs)):\n            unnormalized_gradients = tf.gradients(self.outputs[i], self.weights, self.gradients_weights_ph[i])\n            # unnormalized gradients seems to be better at the time. TODO: validate this accross more environments\n            # self.weighted_gradients.append(list(map(lambda x: tf.div(x, self.network_parameters.batch_size),\n            #                                         unnormalized_gradients)))\n            self.weighted_gradients.append(unnormalized_gradients)\n\n        # defining the optimization process (for LBFGS we have less control over the optimizer)\n        if self.optimizer_type != \'LBFGS\' and self.network_is_trainable:\n            self._create_gradient_applying_ops()\n\n    def _create_gradient_accumulators(self):\n        if self.network_is_global:\n            self.shared_accumulated_gradients = [tf.Variable(initial_value=tf.zeros_like(var)) for var in self.weights]\n            self.accumulate_shared_gradients = [var.assign_add(holder, use_locking=True) for holder, var in\n                                                zip(self.weights_placeholders, self.shared_accumulated_gradients)]\n            self.init_shared_accumulated_gradients = [var.assign(tf.zeros_like(var)) for var in\n                                                      self.shared_accumulated_gradients]\n        elif self.network_is_local:\n            self.accumulate_shared_gradients = self.global_network.accumulate_shared_gradients\n            self.init_shared_accumulated_gradients = self.global_network.init_shared_accumulated_gradients\n\n    def _create_gradient_clipping_ops(self):\n        """"""\n        Create tensorflow ops for clipping the gradients according to the given GradientClippingMethod\n        :return: None\n        """"""\n        if self.network_parameters.gradients_clipping_method == GradientClippingMethod.ClipByGlobalNorm:\n            self.clipped_grads, self.grad_norms = tf.clip_by_global_norm(self.tensor_gradients,\n                                                                         self.network_parameters.clip_gradients)\n        elif self.network_parameters.gradients_clipping_method == GradientClippingMethod.ClipByValue:\n            self.clipped_grads = [tf.clip_by_value(grad,\n                                                   -self.network_parameters.clip_gradients,\n                                                   self.network_parameters.clip_gradients)\n                                  for grad in self.tensor_gradients]\n        elif self.network_parameters.gradients_clipping_method == GradientClippingMethod.ClipByNorm:\n            self.clipped_grads = [tf.clip_by_norm(grad, self.network_parameters.clip_gradients)\n                                  for grad in self.tensor_gradients]\n\n    def _create_gradient_applying_ops(self):\n        """"""\n        Create tensorflow ops for applying the gradients to the network weights according to the training scheme\n        (distributed training - local or global network, shared optimizer, etc.)\n        :return: None\n        """"""\n        if self.network_is_global and self.network_parameters.shared_optimizer and \\\n                not self.network_parameters.async_training:\n            # synchronous training with shared optimizer? -> create an operation for applying the gradients\n            # accumulated in the shared gradients accumulator\n            self.update_weights_from_shared_gradients = self.optimizer.apply_gradients(\n                zip(self.shared_accumulated_gradients, self.weights),\n                global_step=self.global_step)\n\n        elif self.distributed_training and self.network_is_local:\n            # distributed training but independent optimizer? -> create an operation for applying the gradients\n            # to the global weights\n            self.update_weights_from_batch_gradients = self.optimizer.apply_gradients(\n                zip(self.weights_placeholders, self.global_network.weights), global_step=self.global_step)\n\n        elif self.network_is_trainable:\n            # not any of the above but is trainable? -> create an operation for applying the gradients to\n            # this network weights\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.full_name)\n\n            with tf.control_dependencies(update_ops):\n                self.update_weights_from_batch_gradients = self.optimizer.apply_gradients(\n                    zip(self.weights_placeholders, self.weights), global_step=self.global_step)\n\n    def set_session(self, sess):\n        self.sess = sess\n\n        task_is_distributed = isinstance(self.ap.task_parameters, DistributedTaskParameters)\n        # initialize the session parameters in single threaded runs. Otherwise, this is done through the\n        # MonitoredSession object in the graph manager\n        if not task_is_distributed:\n            self.sess.run(self.init_op)\n\n        if self.ap.visualization.tensorboard:\n            # Write the merged summaries to the current experiment directory\n            if not task_is_distributed:\n                self.train_writer = tf.summary.FileWriter(self.ap.task_parameters.experiment_path + \'/tensorboard\')\n                self.train_writer.add_graph(self.sess.graph)\n            elif self.network_is_local:\n                self.train_writer = tf.summary.FileWriter(self.ap.task_parameters.experiment_path +\n                                                          \'/tensorboard/worker{}\'.format(self.ap.task_parameters.task_index))\n                self.train_writer.add_graph(self.sess.graph)\n\n        # wait for all the workers to set their session\n        if not self.network_is_local:\n            self.wait_for_all_workers_barrier()\n\n    def reset_accumulated_gradients(self):\n        """"""\n        Reset the gradients accumulation placeholder\n        """"""\n        if self.accumulated_gradients is None:\n            self.accumulated_gradients = self.sess.run(self.weights)\n\n        for ix, grad in enumerate(self.accumulated_gradients):\n            self.accumulated_gradients[ix] = grad * 0\n\n    def accumulate_gradients(self, inputs, targets, additional_fetches=None, importance_weights=None,\n                             no_accumulation=False):\n        """"""\n        Runs a forward pass & backward pass, clips gradients if needed and accumulates them into the accumulation\n        placeholders\n        :param additional_fetches: Optional tensors to fetch during gradients calculation\n        :param inputs: The input batch for the network\n        :param targets: The targets corresponding to the input batch\n        :param importance_weights: A coefficient for each sample in the batch, which will be used to rescale the loss\n                                   error of this sample. If it is not given, the samples losses won\'t be scaled\n        :param no_accumulation: If is set to True, the gradients in the accumulated gradients placeholder will be\n                                replaced by the newely calculated gradients instead of accumulating the new gradients.\n                                This can speed up the function runtime by around 10%.\n        :return: A list containing the total loss and the individual network heads losses\n        """"""\n\n        if self.accumulated_gradients is None:\n            self.reset_accumulated_gradients()\n\n        # feed inputs\n        if additional_fetches is None:\n            additional_fetches = []\n        feed_dict = self.create_feed_dict(inputs)\n\n        # feed targets\n        targets = force_list(targets)\n        for placeholder_idx, target in enumerate(targets):\n            feed_dict[self.targets[placeholder_idx]] = target\n\n        # feed importance weights\n        importance_weights = force_list(importance_weights)\n        for placeholder_idx, target_ph in enumerate(targets):\n            if len(importance_weights) <= placeholder_idx or importance_weights[placeholder_idx] is None:\n                importance_weight = np.ones(target_ph.shape[0])\n            else:\n                importance_weight = importance_weights[placeholder_idx]\n            importance_weight = np.reshape(importance_weight, (-1,) + (1,) * (len(target_ph.shape) - 1))\n\n            feed_dict[self.importance_weights[placeholder_idx]] = importance_weight\n\n        if self.optimizer_type != \'LBFGS\':\n\n            # feed the lstm state if necessary\n            if self.middleware.__class__.__name__ == \'LSTMMiddleware\':\n                # we can\'t always assume that we are starting from scratch here can we?\n                feed_dict[self.middleware.c_in] = self.middleware.c_init\n                feed_dict[self.middleware.h_in] = self.middleware.h_init\n\n            fetches = self.train_fetches + additional_fetches\n            if self.ap.visualization.tensorboard:\n                fetches += [self.merged]\n\n            # get grads\n            result = self.sess.run(fetches, feed_dict=feed_dict)\n            if hasattr(self, \'train_writer\') and self.train_writer is not None:\n                self.train_writer.add_summary(result[-1], self.sess.run(self.global_step))\n\n            # extract the fetches\n            norm_unclipped_grads, grads, total_loss, losses = result[:4]\n            if self.middleware.__class__.__name__ == \'LSTMMiddleware\':\n                (self.curr_rnn_c_in, self.curr_rnn_h_in) = result[4]\n            fetched_tensors = []\n            if len(additional_fetches) > 0:\n                fetched_tensors = result[self.additional_fetches_start_idx:self.additional_fetches_start_idx +\n                                                                      len(additional_fetches)]\n\n            # accumulate the gradients\n            for idx, grad in enumerate(grads):\n                if no_accumulation:\n                    self.accumulated_gradients[idx] = grad\n                else:\n                    self.accumulated_gradients[idx] += grad\n\n            return total_loss, losses, norm_unclipped_grads, fetched_tensors\n\n        else:\n            self.optimizer.minimize(session=self.sess, feed_dict=feed_dict)\n\n            return [0]\n\n    def create_feed_dict(self, inputs):\n        feed_dict = {}\n        for input_name, input_value in inputs.items():\n            if isinstance(input_name, str):\n                if input_name not in self.inputs:\n                    raise ValueError((\n                        \'input name {input_name} was provided to create a feed \'\n                        \'dictionary, but there is no placeholder with that name. \'\n                        \'placeholder names available include: {placeholder_names}\'\n                    ).format(\n                        input_name=input_name,\n                        placeholder_names=\', \'.join(self.inputs.keys())\n                    ))\n\n                feed_dict[self.inputs[input_name]] = input_value\n            elif isinstance(input_name, tf.Tensor) and input_name.op.type == \'Placeholder\':\n                feed_dict[input_name] = input_value\n            else:\n                raise ValueError((\n                    \'input dictionary expects strings or placeholders as keys, \'\n                    \'but found key {key} of type {type}\'\n                ).format(\n                    key=input_name,\n                    type=type(input_name),\n                ))\n\n        return feed_dict\n\n    def apply_and_reset_gradients(self, gradients, scaler=1., additional_inputs=None):\n        """"""\n        Applies the given gradients to the network weights and resets the accumulation placeholder\n        :param gradients: The gradients to use for the update\n        :param scaler: A scaling factor that allows rescaling the gradients before applying them\n        :param additional_inputs: optional additional inputs required for when applying the gradients (e.g. batchnorm\'s\n                                  update ops also requires the inputs)\n\n        """"""\n        self.apply_gradients(gradients, scaler, additional_inputs=additional_inputs)\n        self.reset_accumulated_gradients()\n\n    def wait_for_all_workers_to_lock(self, lock: str, include_only_training_workers: bool=False):\n        """"""\n        Waits for all the workers to lock a certain lock and then continues\n        :param lock: the name of the lock to use\n        :param include_only_training_workers: wait only for training workers or for all the workers?\n        :return: None\n        """"""\n        if include_only_training_workers:\n            num_workers_to_wait_for = self.ap.task_parameters.num_training_tasks\n        else:\n            num_workers_to_wait_for = self.ap.task_parameters.num_tasks\n\n        # lock\n        if hasattr(self, \'{}_counter\'.format(lock)):\n            self.sess.run(getattr(self, lock))\n            while self.sess.run(getattr(self, \'{}_counter\'.format(lock))) % num_workers_to_wait_for != 0:\n                time.sleep(0.00001)\n            # self.sess.run(getattr(self, \'{}_init\'.format(lock)))\n        else:\n            raise ValueError(""no counter was defined for the lock {}"".format(lock))\n\n    def wait_for_all_workers_barrier(self, include_only_training_workers: bool=False):\n        """"""\n        A barrier that allows waiting for all the workers to finish a certain block of commands\n        :param include_only_training_workers: wait only for training workers or for all the workers?\n        :return: None\n        """"""\n        self.wait_for_all_workers_to_lock(\'lock\', include_only_training_workers=include_only_training_workers)\n        self.sess.run(self.lock_init)\n\n        # we need to lock again (on a different lock) in order to prevent a situation where one of the workers continue\n        # and then was able to first increase the lock again by one, only to have a late worker to reset it again.\n        # so we want to make sure that all workers are done resetting the lock before continuting to reuse that lock.\n\n        self.wait_for_all_workers_to_lock(\'release\', include_only_training_workers=include_only_training_workers)\n        self.sess.run(self.release_init)\n\n    def apply_gradients(self, gradients, scaler=1., additional_inputs=None):\n        """"""\n        Applies the given gradients to the network weights\n        :param gradients: The gradients to use for the update\n        :param scaler: A scaling factor that allows rescaling the gradients before applying them.\n                       The gradients will be MULTIPLIED by this factor\n        :param additional_inputs: optional additional inputs required for when applying the gradients (e.g. batchnorm\'s\n                                  update ops also requires the inputs)\n        """"""\n\n        if self.network_parameters.async_training or not isinstance(self.ap.task_parameters, DistributedTaskParameters):\n            if hasattr(self, \'global_step\') and not self.network_is_local:\n                self.sess.run(self.inc_step)\n\n        if self.optimizer_type != \'LBFGS\':\n\n            if self.distributed_training and not self.network_parameters.async_training:\n                # rescale the gradients so that they average out with the gradients from the other workers\n                if self.network_parameters.scale_down_gradients_by_number_of_workers_for_sync_training:\n                    scaler /= float(self.ap.task_parameters.num_training_tasks)\n\n            # rescale the gradients\n            if scaler != 1.:\n                for gradient in gradients:\n                    gradient *= scaler\n\n            # apply the gradients\n            feed_dict = dict(zip(self.weights_placeholders, gradients))\n            if self.distributed_training and self.network_parameters.shared_optimizer \\\n                    and not self.network_parameters.async_training:\n                # synchronous distributed training with shared optimizer:\n                # - each worker adds its gradients to the shared gradients accumulators\n                # - we wait for all the workers to add their gradients\n                # - the chief worker (worker with task index = 0) applies the gradients once and resets the accumulators\n\n                self.sess.run(self.accumulate_shared_gradients, feed_dict=feed_dict)\n\n                self.wait_for_all_workers_barrier(include_only_training_workers=True)\n\n                if self.is_chief:\n                    self.sess.run(self.update_weights_from_shared_gradients)\n                    self.sess.run(self.init_shared_accumulated_gradients)\n            else:\n                # async distributed training / distributed training with independent optimizer\n                #  / non-distributed training - just apply the gradients\n                feed_dict = dict(zip(self.weights_placeholders, gradients))\n                if additional_inputs is not None:\n                    feed_dict = {**feed_dict, **self.create_feed_dict(additional_inputs)}\n                self.sess.run(self.update_weights_from_batch_gradients, feed_dict=feed_dict)\n\n            # release barrier\n            if self.distributed_training and not self.network_parameters.async_training:\n                self.wait_for_all_workers_barrier(include_only_training_workers=True)\n\n    def predict(self, inputs, outputs=None, squeeze_output=True, initial_feed_dict=None):\n        """"""\n        Run a forward pass of the network using the given input\n        :param inputs: The input for the network\n        :param outputs: The output for the network, defaults to self.outputs\n        :param squeeze_output: call squeeze_list on output\n        :param initial_feed_dict: a dictionary to use as the initial feed_dict. other inputs will be added to this dict\n        :return: The network output\n\n        WARNING: must only call once per state since each call is assumed by LSTM to be a new time step.\n        """"""\n        feed_dict = self.create_feed_dict(inputs)\n        if initial_feed_dict:\n            feed_dict.update(initial_feed_dict)\n        if outputs is None:\n            outputs = self.outputs\n\n        if self.middleware.__class__.__name__ == \'LSTMMiddleware\':\n            feed_dict[self.middleware.c_in] = self.curr_rnn_c_in\n            feed_dict[self.middleware.h_in] = self.curr_rnn_h_in\n\n            output, (self.curr_rnn_c_in, self.curr_rnn_h_in) = self.sess.run([outputs, self.middleware.state_out],\n                                                                             feed_dict=feed_dict)\n        else:\n            output = self.sess.run(outputs, feed_dict)\n\n        if squeeze_output:\n            output = squeeze_list(output)\n        return output\n\n    @staticmethod\n    def parallel_predict(sess: Any,\n                         network_input_tuples: List[Tuple[\'TensorFlowArchitecture\', Dict[str, np.ndarray]]]) ->\\\n            List[np.ndarray]:\n        """"""\n        :param sess: active session to use for prediction\n        :param network_input_tuples: tuple of network and corresponding input\n        :return: list of outputs from all networks\n        """"""\n        feed_dict = {}\n        fetches = []\n\n        for network, input in network_input_tuples:\n            feed_dict.update(network.create_feed_dict(input))\n            fetches += network.outputs\n\n        outputs = sess.run(fetches, feed_dict)\n\n        return outputs\n\n    def train_on_batch(self, inputs, targets, scaler=1., additional_fetches=None, importance_weights=None):\n        """"""\n        Given a batch of examples and targets, runs a forward pass & backward pass and then applies the gradients\n        :param additional_fetches: Optional tensors to fetch during the training process\n        :param inputs: The input for the network\n        :param targets: The targets corresponding to the input batch\n        :param scaler: A scaling factor that allows rescaling the gradients before applying them\n        :param importance_weights: A coefficient for each sample in the batch, which will be used to rescale the loss\n                                   error of this sample. If it is not given, the samples losses won\'t be scaled\n        :return: The loss of the network\n        """"""\n        if additional_fetches is None:\n            additional_fetches = []\n        force_list(additional_fetches)\n        loss = self.accumulate_gradients(inputs, targets, additional_fetches=additional_fetches,\n                                         importance_weights=importance_weights)\n        self.apply_and_reset_gradients(self.accumulated_gradients, scaler)\n        return loss\n\n    def get_weights(self):\n        """"""\n        :return: a list of tensors containing the network weights for each layer\n        """"""\n        return self.weights\n\n    def set_weights(self, weights, new_rate=1.0):\n        """"""\n        Sets the network weights from the given list of weights tensors\n        """"""\n        feed_dict = {}\n        old_weights, new_weights = self.sess.run([self.get_weights(), weights])\n        for placeholder_idx, new_weight in enumerate(new_weights):\n            feed_dict[self.weights_placeholders[placeholder_idx]]\\\n                = new_rate * new_weight + (1 - new_rate) * old_weights[placeholder_idx]\n        self.sess.run(self.update_weights_from_list, feed_dict)\n\n    def get_variable_value(self, variable):\n        """"""\n        Get the value of a variable from the graph\n        :param variable: the variable\n        :return: the value of the variable\n        """"""\n        return self.sess.run(variable)\n\n    def set_variable_value(self, assign_op, value, placeholder=None):\n        """"""\n        Updates the value of a variable.\n        This requires having an assign operation for the variable, and a placeholder which will provide the value\n        :param assign_op: an assign operation for the variable\n        :param value: a value to set the variable to\n        :param placeholder: a placeholder to hold the given value for injecting it into the variable\n        """"""\n        self.sess.run(assign_op, feed_dict={placeholder: value})\n\n    def set_is_training(self, state: bool):\n        """"""\n        Set the phase of the network between training and testing\n        :param state: The current state (True = Training, False = Testing)\n        :return: None\n        """"""\n        self.set_variable_value(self.assign_is_training, state, self.is_training_placeholder)\n\n    def reset_internal_memory(self):\n        """"""\n        Reset any internal memory used by the network. For example, an LSTM internal state\n        :return: None\n        """"""\n        # initialize LSTM hidden states\n        if self.middleware.__class__.__name__ == \'LSTMMiddleware\':\n            self.curr_rnn_c_in = self.middleware.c_init\n            self.curr_rnn_h_in = self.middleware.h_init\n\n    def collect_savers(self, parent_path_suffix: str) -> SaverCollection:\n        """"""\n        Collection of all checkpoints for the network (typically only one checkpoint)\n        :param parent_path_suffix: path suffix of the parent of the network\n            (e.g. could be name of level manager plus name of agent)\n        :return: checkpoint collection for the network\n        """"""\n        savers = SaverCollection()\n        if not self.distributed_training:\n            savers.add(GlobalVariableSaver(self.name))\n        return savers\n\n\ndef save_onnx_graph(input_nodes, output_nodes, checkpoint_save_dir: str) -> None:\n    """"""\n    Given the input nodes and output nodes of the TF graph, save it as an onnx graph\n    This requires the TF graph and the weights checkpoint to be stored in the experiment directory.\n    It then freezes the graph (merging the graph and weights checkpoint), and converts it to ONNX.\n\n    :param input_nodes: A list of input nodes for the TF graph\n    :param output_nodes: A list of output nodes for the TF graph\n    :param checkpoint_save_dir: The directory to save the ONNX graph to\n    :return: None\n    """"""\n    import tf2onnx  # just to verify that tf2onnx is installed\n\n    # freeze graph\n    frozen_graph_path = os.path.join(checkpoint_save_dir, ""frozen_graph.pb"")\n    freeze_graph_command = [\n        ""python -m tensorflow.python.tools.freeze_graph"",\n        ""--input_graph={}"".format(os.path.join(checkpoint_save_dir, ""graphdef.pb"")),\n        ""--input_binary=true"",\n        ""--output_node_names=\'{}\'"".format(\',\'.join([o.split("":"")[0] for o in output_nodes])),\n        ""--input_checkpoint={}"".format(tf.train.latest_checkpoint(checkpoint_save_dir)),\n        ""--output_graph={}"".format(frozen_graph_path)\n    ]\n    start_shell_command_and_wait("" "".join(freeze_graph_command))\n\n    # convert graph to onnx\n    onnx_graph_path = os.path.join(checkpoint_save_dir, ""model.onnx"")\n    convert_to_onnx_command = [\n        ""python -m tf2onnx.convert"",\n        ""--input {}"".format(frozen_graph_path),\n        ""--inputs \'{}\'"".format(\',\'.join(input_nodes)),\n        ""--outputs \'{}\'"".format(\',\'.join(output_nodes)),\n        ""--output {}"".format(onnx_graph_path),\n        ""--verbose""\n    ]\n    start_shell_command_and_wait("" "".join(convert_to_onnx_command))\n'"
rl_coach/architectures/tensorflow_components/distributed_tf_utils.py,13,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Tuple\n\nimport tensorflow as tf\n\n\ndef create_cluster_spec(parameters_server: str, workers: str) -> tf.train.ClusterSpec:\n    """"""\n    Creates a ClusterSpec object representing the cluster.\n    :param parameters_server: comma-separated list of hostname:port pairs to which the parameter servers are assigned\n    :param workers: comma-separated list of hostname:port pairs to which the workers are assigned\n    :return: a ClusterSpec object representing the cluster\n    """"""\n    # extract the parameter servers and workers from the given strings\n    ps_hosts = parameters_server.split("","")\n    worker_hosts = workers.split("","")\n\n    # Create a cluster spec from the parameter server and worker hosts\n    cluster_spec = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})\n\n    return cluster_spec\n\n\ndef create_and_start_parameters_server(cluster_spec: tf.train.ClusterSpec, config: tf.ConfigProto=None) -> None:\n    """"""\n    Create and start a parameter server\n    :param cluster_spec: the ClusterSpec object representing the cluster\n    :param config: the tensorflow config to use\n    :return: None\n    """"""\n    # create a server object for the parameter server\n    server = tf.train.Server(cluster_spec, job_name=""ps"", task_index=0, config=config)\n\n    # wait for the server to finish\n    server.join()\n\n\ndef create_worker_server_and_device(cluster_spec: tf.train.ClusterSpec, task_index: int,\n                                    use_cpu: bool=True, config: tf.ConfigProto=None) -> Tuple[str, tf.device]:\n    """"""\n    Creates a worker server and a device setter used to assign the workers operations to\n    :param cluster_spec: a ClusterSpec object representing the cluster\n    :param task_index: the index of the worker task\n    :param use_cpu: if use_cpu=True, all the agent operations will be assigned to a CPU instead of a GPU\n    :param config: the tensorflow config to use\n    :return: the target string for the tf.Session and the worker device setter object\n    """"""\n    # Create and start a worker\n    server = tf.train.Server(cluster_spec, job_name=""worker"", task_index=task_index, config=config)\n\n    # Assign ops to the local worker\n    worker_device = ""/job:worker/task:{}"".format(task_index)\n    if use_cpu:\n        worker_device += ""/cpu:0""\n    else:\n        worker_device += ""/device:GPU:0""\n    device = tf.train.replica_device_setter(worker_device=worker_device, cluster=cluster_spec)\n\n    return server.target, device\n\n\ndef create_monitored_session(target: tf.train.Server, task_index: int,\n                             checkpoint_dir: str, checkpoint_save_secs: int, config: tf.ConfigProto=None) -> tf.Session:\n    """"""\n    Create a monitored session for the worker\n    :param target: the target string for the tf.Session\n    :param task_index: the task index of the worker\n    :param checkpoint_dir: a directory path where the checkpoints will be stored\n    :param checkpoint_save_secs: number of seconds between checkpoints storing\n    :param config: the tensorflow configuration (optional)\n    :return: the session to use for the run\n    """"""\n    # we chose the first task to be the chief\n    is_chief = task_index == 0\n\n    # Create the monitored session\n    sess = tf.train.MonitoredTrainingSession(\n        master=target,\n        is_chief=is_chief,\n        hooks=[],\n        checkpoint_dir=checkpoint_dir,\n        save_checkpoint_secs=checkpoint_save_secs,\n        config=config,\n        log_step_count_steps=0  # disable logging of steps to avoid TF warning during inference\n    )\n\n    return sess\n\n'"
rl_coach/architectures/tensorflow_components/general_network.py,24,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom types import MethodType\nfrom typing import Dict, List, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.head_parameters import HeadParameters\nfrom rl_coach.architectures.middleware_parameters import MiddlewareParameters\nfrom rl_coach.architectures.tensorflow_components.architecture import TensorFlowArchitecture\nfrom rl_coach.architectures.tensorflow_components import utils\nfrom rl_coach.base_parameters import AgentParameters, Device, DeviceType, EmbeddingMergerType\nfrom rl_coach.core_types import PredictionType\nfrom rl_coach.logger import screen\nfrom rl_coach.spaces import SpacesDefinition, PlanarMapsObservationSpace, TensorObservationSpace\nfrom rl_coach.utils import get_all_subclasses, dynamic_import_and_instantiate_module_from_params, indent_string\n\n\nclass GeneralTensorFlowNetwork(TensorFlowArchitecture):\n    """"""\n    A generalized version of all possible networks implemented using tensorflow.\n    """"""\n    # dictionary of variable-scope name to variable-scope object to prevent tensorflow from\n    # creating a new auxiliary variable scope even when name is properly specified\n    variable_scopes_dict = dict()\n\n    @staticmethod\n    def construct(variable_scope: str, devices: List[str], *args, **kwargs) -> \'GeneralTensorFlowNetwork\':\n        """"""\n        Construct a network class using the provided variable scope and on requested devices\n        :param variable_scope: string specifying variable scope under which to create network variables\n        :param devices: list of devices (can be list of Device objects, or string for TF distributed)\n        :param args: all other arguments for class initializer\n        :param kwargs: all other keyword arguments for class initializer\n        :return: a GeneralTensorFlowNetwork object\n        """"""\n        if len(devices) > 1:\n            screen.warning(""Tensorflow implementation only support a single device. Using {}"".format(devices[0]))\n\n        def construct_on_device():\n            with tf.device(GeneralTensorFlowNetwork._tf_device(devices[0])):\n                return GeneralTensorFlowNetwork(*args, **kwargs)\n\n        # If variable_scope is in our dictionary, then this is not the first time that this variable_scope\n        # is being used with construct(). So to avoid TF adding an incrementing number to the end of the\n        # variable_scope to uniquify it, we have to both pass the previous variable_scope object to the new\n        # variable_scope() call and also recover the name space using name_scope\n        if variable_scope in GeneralTensorFlowNetwork.variable_scopes_dict:\n            variable_scope = GeneralTensorFlowNetwork.variable_scopes_dict[variable_scope]\n            with tf.variable_scope(variable_scope, auxiliary_name_scope=False) as vs:\n                with tf.name_scope(vs.original_name_scope):\n                    return construct_on_device()\n        else:\n            with tf.variable_scope(variable_scope, auxiliary_name_scope=True) as vs:\n                # Add variable_scope object to dictionary for next call to construct\n                GeneralTensorFlowNetwork.variable_scopes_dict[variable_scope] = vs\n                return construct_on_device()\n\n    @staticmethod\n    def _tf_device(device: Union[str, MethodType, Device]) -> str:\n        """"""\n        Convert device to tensorflow-specific device representation\n        :param device: either a specific string or method (used in distributed mode) which is returned without\n            any change or a Device type, which will be converted to a string\n        :return: tensorflow-specific string for device\n        """"""\n        if isinstance(device, str) or isinstance(device, MethodType):\n            return device\n        elif isinstance(device, Device):\n            if device.device_type == DeviceType.CPU:\n                return ""/cpu:0""\n            elif device.device_type == DeviceType.GPU:\n                return ""/device:GPU:{}"".format(device.index)\n            else:\n                raise ValueError(""Invalid device_type: {}"".format(device.device_type))\n        else:\n            raise ValueError(""Invalid device instance type: {}"".format(type(device)))\n\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, name: str,\n                 global_network=None, network_is_local: bool=True, network_is_trainable: bool=False):\n        """"""\n        :param agent_parameters: the agent parameters\n        :param spaces: the spaces definition of the agent\n        :param name: the name of the network\n        :param global_network: the global network replica that is shared between all the workers\n        :param network_is_local: is the network global (shared between workers) or local (dedicated to the worker)\n        :param network_is_trainable: is the network trainable (we can apply gradients on it)\n        """"""\n        self.global_network = global_network\n        self.network_is_local = network_is_local\n        self.network_wrapper_name = name.split(\'/\')[0]\n        self.network_parameters = agent_parameters.network_wrappers[self.network_wrapper_name]\n        self.num_heads_per_network = 1 if self.network_parameters.use_separate_networks_per_head else \\\n            len(self.network_parameters.heads_parameters)\n        self.num_networks = 1 if not self.network_parameters.use_separate_networks_per_head else \\\n            len(self.network_parameters.heads_parameters)\n\n        self.gradients_from_head_rescalers = []\n        self.gradients_from_head_rescalers_placeholders = []\n        self.update_head_rescaler_value_ops = []\n\n        self.adaptive_learning_rate_scheme = None\n        self.current_learning_rate = None\n\n        # init network modules containers\n        self.input_embedders = []\n        self.output_heads = []\n        super().__init__(agent_parameters, spaces, name, global_network,\n                         network_is_local, network_is_trainable)\n\n        self.available_return_types = self._available_return_types()\n        self.is_training = None\n\n    def _available_return_types(self):\n        ret_dict = {cls: [] for cls in get_all_subclasses(PredictionType)}\n\n        components = self.input_embedders + [self.middleware] + self.output_heads\n        for component in components:\n            if not hasattr(component, \'return_type\'):\n                raise ValueError((\n                    ""{} has no return_type attribute. Without this, it is ""\n                    ""unclear how this component should be used.""\n                ).format(component))\n\n            if component.return_type is not None:\n                ret_dict[component.return_type].append(component)\n\n        return ret_dict\n\n    def predict_with_prediction_type(self, states: Dict[str, np.ndarray],\n                                     prediction_type: PredictionType) -> Dict[str, np.ndarray]:\n        """"""\n        Search for a component[s] which has a return_type set to the to the requested PredictionType, and get\n        predictions for it.\n\n        :param states: The input states to the network.\n        :param prediction_type: The requested PredictionType to look for in the network components\n        :return: A dictionary with predictions for all components matching the requested prediction type\n        """"""\n\n        ret_dict = {}\n        for component in self.available_return_types[prediction_type]:\n            ret_dict[component] = self.predict(inputs=states, outputs=component.output)\n\n        return ret_dict\n\n    def get_input_embedder(self, input_name: str, embedder_params: InputEmbedderParameters):\n        """"""\n        Given an input embedder parameters class, creates the input embedder and returns it\n        :param input_name: the name of the input to the embedder (used for retrieving the shape). The input should\n                           be a value within the state or the action.\n        :param embedder_params: the parameters of the class of the embedder\n        :return: the embedder instance\n        """"""\n        allowed_inputs = copy.copy(self.spaces.state.sub_spaces)\n        allowed_inputs[""action""] = copy.copy(self.spaces.action)\n        allowed_inputs[""goal""] = copy.copy(self.spaces.goal)\n\n        if input_name not in allowed_inputs.keys():\n            raise ValueError(""The key for the input embedder ({}) must match one of the following keys: {}""\n                             .format(input_name, allowed_inputs.keys()))\n\n        emb_type = ""vector""\n        if isinstance(allowed_inputs[input_name], TensorObservationSpace):\n            emb_type = ""tensor""\n        elif isinstance(allowed_inputs[input_name], PlanarMapsObservationSpace):\n            emb_type = ""image""\n\n        embedder_path = embedder_params.path(emb_type)\n        embedder_params_copy = copy.copy(embedder_params)\n        embedder_params_copy.is_training = self.is_training\n        embedder_params_copy.activation_function = utils.get_activation_function(embedder_params.activation_function)\n        embedder_params_copy.input_rescaling = embedder_params_copy.input_rescaling[emb_type]\n        embedder_params_copy.input_offset = embedder_params_copy.input_offset[emb_type]\n        embedder_params_copy.name = input_name\n        module = dynamic_import_and_instantiate_module_from_params(embedder_params_copy,\n                                                                   path=embedder_path,\n                                                                   positional_args=[allowed_inputs[input_name].shape])\n        return module\n\n    def get_middleware(self, middleware_params: MiddlewareParameters):\n        """"""\n        Given a middleware type, creates the middleware and returns it\n        :param middleware_params: the paramaeters of the middleware class\n        :return: the middleware instance\n        """"""\n        mod_name = middleware_params.parameterized_class_name\n        middleware_path = middleware_params.path\n        middleware_params_copy = copy.copy(middleware_params)\n        middleware_params_copy.activation_function = utils.get_activation_function(middleware_params.activation_function)\n        middleware_params_copy.is_training = self.is_training\n        module = dynamic_import_and_instantiate_module_from_params(middleware_params_copy, path=middleware_path)\n        return module\n\n    def get_output_head(self, head_params: HeadParameters, head_idx: int):\n        """"""\n        Given a head type, creates the head and returns it\n        :param head_params: the parameters of the head to create\n        :param head_idx: the head index\n        :return: the head\n        """"""\n        mod_name = head_params.parameterized_class_name\n        head_path = head_params.path\n        head_params_copy = copy.copy(head_params)\n        head_params_copy.activation_function = utils.get_activation_function(head_params_copy.activation_function)\n        head_params_copy.is_training = self.is_training\n        return dynamic_import_and_instantiate_module_from_params(head_params_copy, path=head_path, extra_kwargs={\n            \'agent_parameters\': self.ap, \'spaces\': self.spaces, \'network_name\': self.network_wrapper_name,\n            \'head_idx\': head_idx, \'is_local\': self.network_is_local})\n\n    def get_model(self) -> List:\n        # validate the configuration\n        if len(self.network_parameters.input_embedders_parameters) == 0:\n            raise ValueError(""At least one input type should be defined"")\n\n        if len(self.network_parameters.heads_parameters) == 0:\n            raise ValueError(""At least one output type should be defined"")\n\n        if self.network_parameters.middleware_parameters is None:\n            raise ValueError(""Exactly one middleware type should be defined"")\n\n        # ops for defining the training / testing phase\n        self.is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n        self.is_training_placeholder = tf.placeholder(""bool"")\n        self.assign_is_training = tf.assign(self.is_training, self.is_training_placeholder)\n\n        for network_idx in range(self.num_networks):\n            with tf.variable_scope(\'network_{}\'.format(network_idx)):\n\n                ####################\n                # Input Embeddings #\n                ####################\n\n                state_embedding = []\n                for input_name in sorted(self.network_parameters.input_embedders_parameters):\n                    input_type = self.network_parameters.input_embedders_parameters[input_name]\n                    # get the class of the input embedder\n                    input_embedder = self.get_input_embedder(input_name, input_type)\n                    self.input_embedders.append(input_embedder)\n\n                    # input placeholders are reused between networks. on the first network, store the placeholders\n                    # generated by the input_embedders in self.inputs. on the rest of the networks, pass\n                    # the existing input_placeholders into the input_embedders.\n                    if network_idx == 0:\n                        input_placeholder, embedding = input_embedder()\n                        self.inputs[input_name] = input_placeholder\n                    else:\n                        input_placeholder, embedding = input_embedder(self.inputs[input_name])\n\n                    state_embedding.append(embedding)\n\n                ##########\n                # Merger #\n                ##########\n\n                if len(state_embedding) == 1:\n                    state_embedding = state_embedding[0]\n                else:\n                    if self.network_parameters.embedding_merger_type == EmbeddingMergerType.Concat:\n                        state_embedding = tf.concat(state_embedding, axis=-1, name=""merger"")\n                    elif self.network_parameters.embedding_merger_type == EmbeddingMergerType.Sum:\n                        state_embedding = tf.add_n(state_embedding, name=""merger"")\n\n                ##############\n                # Middleware #\n                ##############\n\n                self.middleware = self.get_middleware(self.network_parameters.middleware_parameters)\n                _, self.state_embedding = self.middleware(state_embedding)\n\n                ################\n                # Output Heads #\n                ################\n\n                head_count = 0\n                for head_idx in range(self.num_heads_per_network):\n\n                    if self.network_parameters.use_separate_networks_per_head:\n                        # if we use separate networks per head, then the head type corresponds to the network idx\n                        head_type_idx = network_idx\n                        head_count = network_idx\n                    else:\n                        # if we use a single network with multiple embedders, then the head type is the current head idx\n                        head_type_idx = head_idx\n                    head_params = self.network_parameters.heads_parameters[head_type_idx]\n\n                    for head_copy_idx in range(head_params.num_output_head_copies):\n                        # create output head and add it to the output heads list\n                        self.output_heads.append(\n                            self.get_output_head(head_params,\n                                                 head_idx*head_params.num_output_head_copies + head_copy_idx)\n                        )\n\n                        # rescale the gradients from the head\n                        self.gradients_from_head_rescalers.append(\n                            tf.get_variable(\'gradients_from_head_{}-{}_rescalers\'.format(head_idx, head_copy_idx),\n                                            initializer=float(head_params.rescale_gradient_from_head_by_factor),\n                                            dtype=tf.float32))\n\n                        self.gradients_from_head_rescalers_placeholders.append(\n                            tf.placeholder(\'float\',\n                                           name=\'gradients_from_head_{}-{}_rescalers\'.format(head_type_idx, head_copy_idx)))\n\n                        self.update_head_rescaler_value_ops.append(self.gradients_from_head_rescalers[head_count].assign(\n                            self.gradients_from_head_rescalers_placeholders[head_count]))\n\n                        head_input = (1-self.gradients_from_head_rescalers[head_count]) * tf.stop_gradient(self.state_embedding) + \\\n                                     self.gradients_from_head_rescalers[head_count] * self.state_embedding\n\n                        # build the head\n                        if self.network_is_local:\n                            output, target_placeholder, input_placeholders, importance_weight_ph = \\\n                                self.output_heads[-1](head_input)\n\n                            self.targets.extend(target_placeholder)\n                            self.importance_weights.extend(importance_weight_ph)\n                        else:\n                            output, input_placeholders = self.output_heads[-1](head_input)\n\n                        self.outputs.extend(output)\n                        # TODO: use head names as well\n                        for placeholder_index, input_placeholder in enumerate(input_placeholders):\n                            self.inputs[\'output_{}_{}\'.format(head_type_idx, placeholder_index)] = input_placeholder\n\n                        head_count += 1\n\n        # model weights\n        if not self.distributed_training or self.network_is_global:\n            self.weights = [var for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.full_name) if\n                            \'global_step\' not in var.name]\n        else:\n            self.weights = [var for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.full_name)]\n\n        # Losses\n        self.losses = tf.losses.get_losses(self.full_name)\n\n        # L2 regularization\n        if self.network_parameters.l2_regularization != 0:\n            self.l2_regularization = tf.add_n([tf.nn.l2_loss(v) for v in self.weights]) \\\n                                     * self.network_parameters.l2_regularization\n            self.losses += self.l2_regularization\n\n        self.total_loss = tf.reduce_sum(self.losses)\n        # tf.summary.scalar(\'total_loss\', self.total_loss)\n\n        # Learning rate\n        if self.network_parameters.learning_rate_decay_rate != 0:\n            self.adaptive_learning_rate_scheme = \\\n                tf.train.exponential_decay(\n                    self.network_parameters.learning_rate,\n                    self.global_step,\n                    decay_steps=self.network_parameters.learning_rate_decay_steps,\n                    decay_rate=self.network_parameters.learning_rate_decay_rate,\n                    staircase=True)\n\n            self.current_learning_rate = self.adaptive_learning_rate_scheme\n        else:\n            self.current_learning_rate = self.network_parameters.learning_rate\n\n        # Optimizer\n        if self.distributed_training and self.network_is_local and self.network_parameters.shared_optimizer:\n            # distributed training + is a local network + optimizer shared -> take the global optimizer\n            self.optimizer = self.global_network.optimizer\n        elif (self.distributed_training and self.network_is_local and not self.network_parameters.shared_optimizer) \\\n                or self.network_parameters.shared_optimizer or not self.distributed_training:\n            # distributed training + is a global network + optimizer shared\n            # OR\n            # distributed training + is a local network + optimizer not shared\n            # OR\n            # non-distributed training\n            # -> create an optimizer\n\n            if self.network_parameters.optimizer_type == \'Adam\':\n                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.current_learning_rate,\n                                                        beta1=self.network_parameters.adam_optimizer_beta1,\n                                                        beta2=self.network_parameters.adam_optimizer_beta2,\n                                                        epsilon=self.network_parameters.optimizer_epsilon)\n            elif self.network_parameters.optimizer_type == \'RMSProp\':\n                self.optimizer = tf.train.RMSPropOptimizer(self.current_learning_rate,\n                                                           decay=self.network_parameters.rms_prop_optimizer_decay,\n                                                           epsilon=self.network_parameters.optimizer_epsilon)\n            elif self.network_parameters.optimizer_type == \'LBFGS\':\n                self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.total_loss, method=\'L-BFGS-B\',\n                                                                        options={\'maxiter\': 25})\n            else:\n                raise Exception(""{} is not a valid optimizer type"".format(self.network_parameters.optimizer_type))\n\n        return self.weights\n\n    def __str__(self):\n        result = []\n\n        for network in range(self.num_networks):\n            network_structure = []\n\n            # embedder\n            for embedder in self.input_embedders:\n                network_structure.append(""Input Embedder: {}"".format(embedder.name))\n                network_structure.append(indent_string(str(embedder)))\n\n            if len(self.input_embedders) > 1:\n                network_structure.append(""{} ({})"".format(self.network_parameters.embedding_merger_type.name,\n                                               "", "".join([""{} embedding"".format(e.name) for e in self.input_embedders])))\n\n            # middleware\n            network_structure.append(""Middleware:"")\n            network_structure.append(indent_string(str(self.middleware)))\n\n            # head\n            if self.network_parameters.use_separate_networks_per_head:\n                heads = range(network, network+1)\n            else:\n                heads = range(0, len(self.output_heads))\n\n            for head_idx in heads:\n                head = self.output_heads[head_idx]\n                head_params = self.network_parameters.heads_parameters[head_idx]\n                if head_params.num_output_head_copies > 1:\n                    network_structure.append(""Output Head: {} (num copies = {})"".format(head.name, head_params.num_output_head_copies))\n                else:\n                    network_structure.append(""Output Head: {}"".format(head.name))\n                    network_structure.append(indent_string(str(head)))\n\n            # finalize network\n            if self.num_networks > 1:\n                result.append(""Sub-network for head: {}"".format(self.output_heads[network].name))\n                result.append(indent_string(\'\\n\'.join(network_structure)))\n            else:\n                result.append(\'\\n\'.join(network_structure))\n\n        result = \'\\n\'.join(result)\n        return result\n'"
rl_coach/architectures/tensorflow_components/layers.py,19,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport math\nfrom types import FunctionType\nimport tensorflow as tf\n\nfrom rl_coach.architectures import layers\nfrom rl_coach.architectures.tensorflow_components import utils\n\n\ndef batchnorm_activation_dropout(input_layer, batchnorm, activation_function, dropout_rate, is_training, name):\n    layers = [input_layer]\n\n    # Rationale: passing a bool here will mean that batchnorm and or activation will never activate\n    assert not isinstance(is_training, bool)\n\n    # batchnorm\n    if batchnorm:\n        layers.append(\n            tf.layers.batch_normalization(layers[-1], name=""{}_batchnorm"".format(name), training=is_training)\n        )\n\n    # activation\n    if activation_function:\n        if isinstance(activation_function, str):\n            activation_function = utils.get_activation_function(activation_function)\n        layers.append(\n            activation_function(layers[-1], name=""{}_activation"".format(name))\n        )\n\n    # dropout\n    if dropout_rate > 0:\n        layers.append(\n            tf.layers.dropout(layers[-1], dropout_rate, name=""{}_dropout"".format(name), training=is_training)\n        )\n\n    # remove the input layer from the layers list\n    del layers[0]\n\n    return layers\n\n\n# define global dictionary for storing layer type to layer implementation mapping\ntf_layer_dict = dict()\ntf_layer_class_dict = dict()\n\n\ndef reg_to_tf_instance(layer_type) -> FunctionType:\n    """""" function decorator that registers layer implementation\n    :return: decorated function\n    """"""\n    def reg_impl_decorator(func):\n        assert layer_type not in tf_layer_dict\n        tf_layer_dict[layer_type] = func\n        return func\n    return reg_impl_decorator\n\n\ndef reg_to_tf_class(layer_type) -> FunctionType:\n    """""" function decorator that registers layer type\n    :return: decorated function\n    """"""\n    def reg_impl_decorator(func):\n        assert layer_type not in tf_layer_class_dict\n        tf_layer_class_dict[layer_type] = func\n        return func\n    return reg_impl_decorator\n\n\ndef convert_layer(layer):\n    """"""\n    If layer instance is callable (meaning this is already a concrete TF class), return layer, otherwise convert to TF type\n    :param layer: layer to be converted\n    :return: converted layer if not callable, otherwise layer itself\n    """"""\n    if callable(layer):\n        return layer\n    return tf_layer_dict[type(layer)](layer)\n\n\ndef convert_layer_class(layer_class):\n    """"""\n    If layer instance is callable, return layer, otherwise convert to TF type\n    :param layer: layer to be converted\n    :return: converted layer if not callable, otherwise layer itself\n    """"""\n    if hasattr(layer_class, \'to_tf_instance\'):\n        return layer_class\n    else:\n        return tf_layer_class_dict[layer_class]()\n\n\nclass Conv2d(layers.Conv2d):\n    def __init__(self, num_filters: int, kernel_size: int, strides: int):\n        super(Conv2d, self).__init__(num_filters=num_filters, kernel_size=kernel_size, strides=strides)\n\n    def __call__(self, input_layer, name: str=None, is_training=None):\n        """"""\n        returns a tensorflow conv2d layer\n        :param input_layer: previous layer\n        :param name: layer name\n        :return: conv2d layer\n        """"""\n        return tf.layers.conv2d(input_layer, filters=self.num_filters, kernel_size=self.kernel_size,\n                                strides=self.strides, data_format=\'channels_last\', name=name)\n\n    @staticmethod\n    @reg_to_tf_instance(layers.Conv2d)\n    def to_tf_instance(base: layers.Conv2d):\n            return Conv2d(\n                num_filters=base.num_filters,\n                kernel_size=base.kernel_size,\n                strides=base.strides)\n\n    @staticmethod\n    @reg_to_tf_class(layers.Conv2d)\n    def to_tf_class():\n        return Conv2d\n\n\nclass BatchnormActivationDropout(layers.BatchnormActivationDropout):\n    def __init__(self, batchnorm: bool=False, activation_function=None, dropout_rate: float=0):\n        super(BatchnormActivationDropout, self).__init__(\n            batchnorm=batchnorm, activation_function=activation_function, dropout_rate=dropout_rate)\n\n    def __call__(self, input_layer, name: str=None, is_training=None):\n        """"""\n        returns a list of tensorflow batchnorm, activation and dropout layers\n        :param input_layer: previous layer\n        :param name: layer name\n        :return: batchnorm, activation and dropout layers\n        """"""\n        return batchnorm_activation_dropout(input_layer, batchnorm=self.batchnorm,\n                                            activation_function=self.activation_function,\n                                            dropout_rate=self.dropout_rate,\n                                            is_training=is_training, name=name)\n\n    @staticmethod\n    @reg_to_tf_instance(layers.BatchnormActivationDropout)\n    def to_tf_instance(base: layers.BatchnormActivationDropout):\n        return BatchnormActivationDropout, BatchnormActivationDropout(\n                batchnorm=base.batchnorm,\n                activation_function=base.activation_function,\n                dropout_rate=base.dropout_rate)\n\n    @staticmethod\n    @reg_to_tf_class(layers.BatchnormActivationDropout)\n    def to_tf_class():\n        return BatchnormActivationDropout\n\n\nclass Dense(layers.Dense):\n    def __init__(self, units: int):\n        super(Dense, self).__init__(units=units)\n\n    def __call__(self, input_layer, name: str=None, kernel_initializer=None, bias_initializer=None,\n                 activation=None, is_training=None):\n        """"""\n        returns a tensorflow dense layer\n        :param input_layer: previous layer\n        :param name: layer name\n        :return: dense layer\n        """"""\n        if bias_initializer is None:\n            bias_initializer = tf.zeros_initializer()\n        return tf.layers.dense(input_layer, self.units, name=name, kernel_initializer=kernel_initializer,\n                               activation=activation, bias_initializer=bias_initializer)\n\n    @staticmethod\n    @reg_to_tf_instance(layers.Dense)\n    def to_tf_instance(base: layers.Dense):\n        return Dense(units=base.units)\n\n    @staticmethod\n    @reg_to_tf_class(layers.Dense)\n    def to_tf_class():\n        return Dense\n\n\nclass NoisyNetDense(layers.NoisyNetDense):\n    """"""\n    A factorized Noisy Net layer\n\n    https://arxiv.org/abs/1706.10295.\n    """"""\n\n    def __init__(self, units: int):\n        super(NoisyNetDense, self).__init__(units=units)\n\n    def __call__(self, input_layer, name: str, kernel_initializer=None, activation=None, is_training=None,\n                 bias_initializer=None):\n        """"""\n        returns a NoisyNet dense layer\n        :param input_layer: previous layer\n        :param name: layer name\n        :param kernel_initializer: initializer for kernels. Default is to use Gaussian noise that preserves stddev.\n        :param activation: the activation function\n        :return: dense layer\n        """"""\n        #TODO: noise sampling should be externally controlled. DQN is fine with sampling noise for every\n        #      forward (either act or train, both for online and target networks).\n        #      A3C, on the other hand, should sample noise only when policy changes (i.e. after every t_max steps)\n\n        def _f(values):\n            return tf.sqrt(tf.abs(values)) * tf.sign(values)\n\n        def _factorized_noise(inputs, outputs):\n            # TODO: use factorized noise only for compute intensive algos (e.g. DQN).\n            #      lighter algos (e.g. DQN) should not use it\n            noise1 = _f(tf.random_normal((inputs, 1)))\n            noise2 = _f(tf.random_normal((1, outputs)))\n            return tf.matmul(noise1, noise2)\n\n        num_inputs = input_layer.get_shape()[-1].value\n        num_outputs = self.units\n\n        stddev = 1 / math.sqrt(num_inputs)\n        activation = activation if activation is not None else (lambda x: x)\n\n        if kernel_initializer is None:\n            kernel_mean_initializer = tf.random_uniform_initializer(-stddev, stddev)\n            kernel_stddev_initializer = tf.random_uniform_initializer(-stddev * self.sigma0, stddev * self.sigma0)\n        else:\n            kernel_mean_initializer = kernel_stddev_initializer = kernel_initializer\n        if bias_initializer is None:\n            bias_initializer = tf.zeros_initializer()\n        with tf.variable_scope(None, default_name=name):\n            weight_mean = tf.get_variable(\'weight_mean\', shape=(num_inputs, num_outputs),\n                                          initializer=kernel_mean_initializer)\n            bias_mean = tf.get_variable(\'bias_mean\', shape=(num_outputs,), initializer=bias_initializer)\n\n            weight_stddev = tf.get_variable(\'weight_stddev\', shape=(num_inputs, num_outputs),\n                                            initializer=kernel_stddev_initializer)\n            bias_stddev = tf.get_variable(\'bias_stddev\', shape=(num_outputs,),\n                                          initializer=kernel_stddev_initializer)\n            bias_noise = _f(tf.random_normal((num_outputs,)))\n            weight_noise = _factorized_noise(num_inputs, num_outputs)\n\n        bias = bias_mean + bias_stddev * bias_noise\n        weight = weight_mean + weight_stddev * weight_noise\n        return activation(tf.matmul(input_layer, weight) + bias)\n\n    @staticmethod\n    @reg_to_tf_instance(layers.NoisyNetDense)\n    def to_tf_instance(base: layers.NoisyNetDense):\n        return NoisyNetDense(units=base.units)\n\n    @staticmethod\n    @reg_to_tf_class(layers.NoisyNetDense)\n    def to_tf_class():\n        return NoisyNetDense\n'"
rl_coach/architectures/tensorflow_components/savers.py,4,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pickle\nfrom typing import Any, List, Dict\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom rl_coach.saver import Saver\n\n\nclass GlobalVariableSaver(Saver):\n    def __init__(self, name=""""):\n        self._names = [name]\n        # if graph is finalized, savers must have already already been added. This happens\n        # in the case of a MonitoredSession\n        self._variables = tf.global_variables()\n\n        # target network is never saved or restored directly from checkpoint, so we are removing all its variables from the list\n        # the target network would be synched back from the online network in graph_manager.improve(...), at the beginning of the run flow.\n        self._variables = [v for v in self._variables if ""/target"" not in v.name]\n\n        # Using a placeholder to update the variable during restore to avoid memory leak.\n        # Ref: https://github.com/tensorflow/tensorflow/issues/4151\n        self._variable_placeholders = []\n        self._variable_update_ops = []\n        for v in self._variables:\n            variable_placeholder = tf.placeholder(v.dtype, shape=v.get_shape())\n            self._variable_placeholders.append(variable_placeholder)\n            self._variable_update_ops.append(v.assign(variable_placeholder))\n\n        self._saver = tf.train.Saver(self._variables, max_to_keep=None)\n\n    @property\n    def path(self):\n        """"""\n        Relative path for save/load. If two checkpoint objects return the same path, they must be merge-able.\n        """"""\n        return """"  # use empty string for global file\n\n    def save(self, sess: None, save_path: str) -> List[str]:\n        """"""\n        Save to save_path\n        :param sess: active session\n        :param save_path: full path to save checkpoint (typically directory plus checkpoint prefix plus self.path)\n        :return: list of all saved paths\n        """"""\n        save_path = self._saver.save(sess, save_path)\n        return [save_path]\n\n    def to_arrays(self, session: Any) -> Dict[str, np.ndarray]:\n        """"""\n        Save to dictionary of arrays\n        :param sess: active session\n        :return: dictionary of arrays\n        """"""\n        return {\n            k.name.split("":"")[0]: v for k, v in zip(self._variables, session.run(self._variables))\n        }\n\n    def from_arrays(self, session: Any, tensors: Any):\n        """"""\n        Restore from restore_path\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param tensors: {name: array}\n        """"""\n        # if variable was saved using global network, re-map it to online\n        # network\n        # TODO: Can this be more generic so that `global/` and `online/` are not\n        # hardcoded here?\n        if isinstance(tensors, dict):\n            tensors = tensors.items()\n\n        variables = {k.replace(""global/"", ""online/""): v for k, v in tensors}\n\n        # Assign all variables using placeholder\n        placeholder_dict = {\n            ph: variables[v.name.split("":"")[0]]\n            for ph, v in zip(self._variable_placeholders, self._variables)\n        }\n        session.run(self._variable_update_ops, placeholder_dict)\n\n    def to_string(self, session: Any) -> str:\n        """"""\n        Save to byte string\n        :param session: active session\n        :return: serialized byte string\n        """"""\n        return pickle.dumps(self.to_arrays(session), protocol=-1)\n\n    def from_string(self, session: Any, string: str):\n        """"""\n        Restore from byte string\n        :param session: active session\n        :param string: byte string to restore from\n        """"""\n        self.from_arrays(session, pickle.loads(string))\n\n    def _read_tensors(self, restore_path: str):\n        """"""\n        Load tensors from a checkpoint\n        :param restore_path: full path to load checkpoint from.\n        """"""\n        # We don\'t use saver.restore() because checkpoint is loaded to online\n        # network, but if the checkpoint is from the global network, a namespace\n        # mismatch exists and variable name must be modified before loading.\n        reader = tf.contrib.framework.load_checkpoint(restore_path)\n        for var_name, _ in reader.get_variable_to_shape_map().items():\n            yield var_name, reader.get_tensor(var_name)\n\n    def restore(self, sess: Any, restore_path: str):\n        """"""\n        Restore from restore_path\n        :param sess: active session for session-based frameworks (e.g. TF)\n        :param restore_path: full path to load checkpoint from.\n        """"""\n        self.from_arrays(sess, self._read_tensors(restore_path))\n\n    def merge(self, other: ""Saver""):\n        """"""\n        Merge other saver into this saver\n        :param other: saver to be merged into self\n        """"""\n        assert isinstance(other, GlobalVariableSaver)\n        self._names.extend(other._names)\n        # There is nothing else to do because variables must already be part of\n        # the global collection.\n'"
rl_coach/architectures/tensorflow_components/shared_variables.py,27,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport pickle\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom rl_coach.utilities.shared_running_stats import SharedRunningStats\n\n\nclass TFSharedRunningStats(SharedRunningStats):\n    def __init__(self, replicated_device=None, epsilon=1e-2, name="""", create_ops=True, pubsub_params=None):\n        super().__init__(name=name, pubsub_params=pubsub_params)\n        self.sess = None\n        self.replicated_device = replicated_device\n        self.epsilon = epsilon\n        self.ops_were_created = False\n        if create_ops:\n            with tf.device(replicated_device):\n                self.set_params()\n\n    def set_params(self, shape=[1], clip_values=None):\n        """"""\n        set params and create ops\n\n        :param shape: shape of the stats to track\n        :param clip_values: if not None, sets clip min/max thresholds\n        """"""\n\n        self.clip_values = clip_values\n        with tf.variable_scope(self.name):\n            self._sum = tf.get_variable(\n                dtype=tf.float64,\n                initializer=tf.constant_initializer(0.0),\n                name=""running_sum"", trainable=False, shape=shape, validate_shape=False,\n                collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n            self._sum_squares = tf.get_variable(\n                dtype=tf.float64,\n                initializer=tf.constant_initializer(self.epsilon),\n                name=""running_sum_squares"", trainable=False, shape=shape, validate_shape=False,\n                collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n            self._count = tf.get_variable(\n                dtype=tf.float64,\n                shape=(),\n                initializer=tf.constant_initializer(self.epsilon),\n                name=""count"", trainable=False, collections=[tf.GraphKeys.GLOBAL_VARIABLES])\n\n            self._shape = None\n            self._mean = tf.div(self._sum, self._count, name=""mean"")\n            self._std = tf.sqrt(tf.maximum((self._sum_squares - self._count * tf.square(self._mean))\n                                           / tf.maximum(self._count-1, 1), self.epsilon), name=""stdev"")\n            self.tf_mean = tf.cast(self._mean, \'float32\')\n            self.tf_std = tf.cast(self._std, \'float32\')\n\n            self.new_sum = tf.placeholder(dtype=tf.float64, name=\'sum\')\n            self.new_sum_squares = tf.placeholder(dtype=tf.float64, name=\'var\')\n            self.newcount = tf.placeholder(shape=[], dtype=tf.float64, name=\'count\')\n\n            self._inc_sum = tf.assign_add(self._sum, self.new_sum, use_locking=True)\n            self._inc_sum_squares = tf.assign_add(self._sum_squares, self.new_sum_squares, use_locking=True)\n            self._inc_count = tf.assign_add(self._count, self.newcount, use_locking=True)\n\n            self.raw_obs = tf.placeholder(dtype=tf.float64, name=\'raw_obs\')\n            self.normalized_obs = (self.raw_obs - self._mean) / self._std\n            if self.clip_values is not None:\n                self.clipped_obs = tf.clip_by_value(self.normalized_obs, self.clip_values[0], self.clip_values[1])\n\n            self.ops_were_created = True\n\n    def set_session(self, sess):\n        self.sess = sess\n\n    def push_val(self, x):\n        x = x.astype(\'float64\')\n        self.sess.run([self._inc_sum, self._inc_sum_squares, self._inc_count],\n                      feed_dict={\n                          self.new_sum: x.sum(axis=0).ravel(),\n                          self.new_sum_squares: np.square(x).sum(axis=0).ravel(),\n                          self.newcount: np.array(len(x), dtype=\'float64\')\n                     })\n        if self._shape is None:\n            self._shape = x.shape\n\n    @property\n    def n(self):\n        return self.sess.run(self._count)\n\n    @property\n    def mean(self):\n        return self.sess.run(self._mean)\n\n    @property\n    def var(self):\n        return self.std ** 2\n\n    @property\n    def std(self):\n        return self.sess.run(self._std)\n\n    @property\n    def shape(self):\n        return self._shape\n\n    @shape.setter\n    def shape(self, val):\n        self._shape = val\n        self.new_sum.set_shape(val)\n        self.new_sum_squares.set_shape(val)\n        self.tf_mean.set_shape(val)\n        self.tf_std.set_shape(val)\n        self._sum.set_shape(val)\n        self._sum_squares.set_shape(val)\n\n    def normalize(self, batch):\n        if self.clip_values is not None:\n            return self.sess.run(self.clipped_obs, feed_dict={self.raw_obs: batch})\n        else:\n            return self.sess.run(self.normalized_obs, feed_dict={self.raw_obs: batch})\n\n    def save_state_to_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        # Since the internal state is maintained as part of the TF graph, no need to do anything special for\n        # save/restore, when going from single-node-multi-thread run back to a single-node-multi-worker run.\n        # Nevertheless, if we\'ll want to restore a checkpoint back to either a * single-worker, or a\n        # multi-node-multi-worker * run, we have to save the internal state, so that it can be restored to the\n        # NumpySharedRunningStats implementation.\n\n        dict_to_save = {\'_mean\': self.mean,\n                        \'_std\': self.std,\n                        \'_count\': self.n,\n                        \'_sum\': self.sess.run(self._sum),\n                        \'_sum_squares\': self.sess.run(self._sum_squares)}\n\n        with open(os.path.join(checkpoint_dir, str(checkpoint_prefix) + \'.srs\'), \'wb\') as f:\n            pickle.dump(dict_to_save, f, pickle.HIGHEST_PROTOCOL)\n\n    def restore_state_from_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        # Since the internal state is maintained as part of the TF graph, no need to do anything special for\n        # save/restore, when going from single-node-multi-thread run back to a single-node-multi-worker run.\n        # Restoring from either a * single-worker, or a multi-node-multi-worker * run, to a single-node-multi-thread run\n        # is not supported.\n        pass\n'"
rl_coach/architectures/tensorflow_components/utils.py,6,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n""""""\nModule containing utility functions\n""""""\nimport tensorflow as tf\n\n\ndef get_activation_function(activation_function_string: str):\n    """"""\n    Map the activation function from a string to the tensorflow framework equivalent\n    :param activation_function_string: the type of the activation function\n    :return: the tensorflow activation function\n    """"""\n    activation_functions = {\n        \'relu\': tf.nn.relu,\n        \'tanh\': tf.nn.tanh,\n        \'sigmoid\': tf.nn.sigmoid,\n        \'elu\': tf.nn.elu,\n        \'selu\': tf.nn.selu,\n        \'leaky_relu\': tf.nn.leaky_relu,\n        \'none\': None\n    }\n    assert activation_function_string in activation_functions.keys(), \\\n        ""Activation function must be one of the following {}. instead it was: {}"" \\\n            .format(activation_functions.keys(), activation_function_string)\n    return activation_functions[activation_function_string]\n\n\ndef squeeze_tensor(tensor):\n    if tensor.shape[0] == 1:\n        return tensor[0]\n    else:\n        return tensor'"
rl_coach/environments/mujoco/__init__.py,0,b''
rl_coach/environments/mujoco/pendulum_with_goals.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\n\nimport gym\nimport numpy as np\nfrom gym import spaces\nfrom gym.envs.registration import EnvSpec\nfrom mujoco_py import load_model_from_path, MjSim, MjViewer, MjRenderContextOffscreen\n\n\nclass PendulumWithGoals(gym.Env):\n    metadata = {\n        \'render.modes\': [\'human\', \'rgb_array\'], \'video.frames_per_second\': 30\n    }\n\n    def __init__(self, goal_reaching_thresholds=np.array([0.075, 0.075, 0.75]),\n                 goal_not_reached_penalty=-1, goal_reached_reward=0, terminate_on_goal_reaching=True,\n                 time_limit=1000, frameskip=1, random_goals_instead_of_standing_goal=False,\n                 polar_coordinates: bool=False):\n        super().__init__()\n        dir = os.path.dirname(__file__)\n        model = load_model_from_path(dir + ""/pendulum_with_goals.xml"")\n\n        self.sim = MjSim(model)\n        self.viewer = None\n        self.rgb_viewer = None\n\n        self.frameskip = frameskip\n        self.goal = None\n        self.goal_reaching_thresholds = goal_reaching_thresholds\n        self.goal_not_reached_penalty = goal_not_reached_penalty\n        self.goal_reached_reward = goal_reached_reward\n        self.terminate_on_goal_reaching = terminate_on_goal_reaching\n        self.time_limit = time_limit\n        self.current_episode_steps_counter = 0\n        self.random_goals_instead_of_standing_goal = random_goals_instead_of_standing_goal\n        self.polar_coordinates = polar_coordinates\n\n        # spaces definition\n        self.action_space = spaces.Box(low=-self.sim.model.actuator_ctrlrange[:, 1],\n                                       high=self.sim.model.actuator_ctrlrange[:, 1],\n                                       dtype=np.float32)\n        if self.polar_coordinates:\n            self.observation_space = spaces.Dict({\n                ""observation"": spaces.Box(low=np.array([-np.pi, -15]),\n                                          high=np.array([np.pi, 15]),\n                                          dtype=np.float32),\n                ""desired_goal"": spaces.Box(low=np.array([-np.pi, -15]),\n                                           high=np.array([np.pi, 15]),\n                                           dtype=np.float32),\n                ""achieved_goal"": spaces.Box(low=np.array([-np.pi, -15]),\n                                            high=np.array([np.pi, 15]),\n                                            dtype=np.float32)\n            })\n        else:\n            self.observation_space = spaces.Dict({\n                ""observation"": spaces.Box(low=np.array([-1, -1, -15]),\n                                          high=np.array([1, 1, 15]),\n                                          dtype=np.float32),\n                ""desired_goal"": spaces.Box(low=np.array([-1, -1, -15]),\n                                           high=np.array([1, 1, 15]),\n                                           dtype=np.float32),\n                ""achieved_goal"": spaces.Box(low=np.array([-1, -1, -15]),\n                                            high=np.array([1, 1, 15]),\n                                            dtype=np.float32)\n            })\n\n        self.spec = EnvSpec(\'PendulumWithGoals-v0\')\n        self.spec.reward_threshold = self.goal_not_reached_penalty * self.time_limit\n\n        self.reset()\n\n    def _goal_reached(self):\n        observation = self._get_obs()\n        if np.any(np.abs(observation[\'achieved_goal\'] - observation[\'desired_goal\']) > self.goal_reaching_thresholds):\n            return False\n        else:\n            return True\n\n    def _terminate(self):\n        if (self._goal_reached() and self.terminate_on_goal_reaching) or \\\n                        self.current_episode_steps_counter >= self.time_limit:\n            return True\n        else:\n            return False\n\n    def _reward(self):\n        if self._goal_reached():\n            return self.goal_reached_reward\n        else:\n            return self.goal_not_reached_penalty\n\n    def step(self, action):\n        self.sim.data.ctrl[:] = action\n        for _ in range(self.frameskip):\n            self.sim.step()\n\n        self.current_episode_steps_counter += 1\n\n        state = self._get_obs()\n\n        # visualize the angular velocities\n        state_velocity = np.copy(state[\'observation\'][-1] / 20)\n        goal_velocity = self.goal[-1] / 20\n        self.sim.model.site_size[2] = np.array([0.01, 0.01, state_velocity])\n        self.sim.data.mocap_pos[2] = np.array([0.85, 0, 0.75 + state_velocity])\n        self.sim.model.site_size[3] = np.array([0.01, 0.01, goal_velocity])\n        self.sim.data.mocap_pos[3] = np.array([1.15, 0, 0.75 + goal_velocity])\n\n        return state, self._reward(), self._terminate(), {}\n\n    def _get_obs(self):\n\n        """"""\n        y\n\n        ^\n        |____\n        |   /\n        |  /\n        |~/\n        |/\n        --------> x\n\n        """"""\n\n        # observation\n        angle = self.sim.data.qpos\n        angular_velocity = self.sim.data.qvel\n        if self.polar_coordinates:\n            observation = np.concatenate([angle - np.pi, angular_velocity])\n        else:\n            x = np.sin(angle)\n            y = np.cos(angle)  # qpos is the angle relative to a standing pole\n            observation = np.concatenate([x, y, angular_velocity])\n\n        return {\n            ""observation"": observation,\n            ""desired_goal"": self.goal,\n            ""achieved_goal"": observation\n        }\n\n    def reset(self):\n        self.current_episode_steps_counter = 0\n\n        # set initial state\n        angle = np.random.uniform(np.pi / 4, 7 * np.pi / 4)\n        angular_velocity = np.random.uniform(-0.05, 0.05)\n        self.sim.data.qpos[0] = angle\n        self.sim.data.qvel[0] = angular_velocity\n        self.sim.step()\n\n        # goal\n        if self.random_goals_instead_of_standing_goal:\n            angle_target = np.random.uniform(-np.pi / 8, np.pi / 8)\n            angular_velocity_target = np.random.uniform(-0.2, 0.2)\n        else:\n            angle_target = 0\n            angular_velocity_target = 0\n\n        # convert target values to goal\n        x_target = np.sin(angle_target)\n        y_target = np.cos(angle_target)\n        if self.polar_coordinates:\n            self.goal = np.array([angle_target - np.pi, angular_velocity_target])\n        else:\n            self.goal = np.array([x_target, y_target, angular_velocity_target])\n\n        # visualize the goal\n        self.sim.data.mocap_pos[0] = [x_target, 0, y_target]\n\n        return self._get_obs()\n\n    def render(self, mode=\'human\', close=False):\n        if mode == \'human\':\n            if self.viewer is None:\n                self.viewer = MjViewer(self.sim)\n            self.viewer.render()\n        elif mode == \'rgb_array\':\n            if self.rgb_viewer is None:\n                self.rgb_viewer = MjRenderContextOffscreen(self.sim, 0)\n            self.rgb_viewer.render(500, 500)\n            # window size used for old mujoco-py:\n            data = self.rgb_viewer.read_pixels(500, 500, depth=False)\n            # original image is upside-down, so flip it\n            return data[::-1, :, :]\n'"
rl_coach/environments/toy_problems/__init__.py,0,b''
rl_coach/environments/toy_problems/bit_flip.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport random\n\nimport gym\nimport numpy as np\nfrom gym import spaces\n\n\nclass BitFlip(gym.Env):\n    metadata = {\n        \'render.modes\': [\'human\', \'rgb_array\'], \'video.frames_per_second\': 30\n    }\n\n    def __init__(self, bit_length=16, max_steps=None, mean_zero=False):\n        super(BitFlip, self).__init__()\n        if bit_length < 1:\n            raise ValueError(\'bit_length must be >= 1, found {}\'.format(bit_length))\n        self.bit_length = bit_length\n        self.mean_zero = mean_zero\n\n        if max_steps is None:\n            # default to bit_length\n            self.max_steps = bit_length\n        elif max_steps == 0:\n            self.max_steps = None\n        else:\n            self.max_steps = max_steps\n\n        # spaces documentation: https://gym.openai.com/docs/\n        self.action_space = spaces.Discrete(bit_length)\n        self.observation_space = spaces.Dict({\n            \'state\': spaces.Box(low=0, high=1, shape=(bit_length, )),\n            \'desired_goal\': spaces.Box(low=0, high=1, shape=(bit_length, )),\n            \'achieved_goal\': spaces.Box(low=0, high=1, shape=(bit_length, ))\n        })\n\n        self.reset()\n\n    def _terminate(self):\n        return (self.state == self.goal).all() or self.steps >= self.max_steps\n\n    def _reward(self):\n        return -1 if (self.state != self.goal).any() else 0\n\n    def step(self, action):\n        # action is an int in the range [0, self.bit_length)\n        self.state[action] = int(not self.state[action])\n        self.steps += 1\n\n        return (self._get_obs(), self._reward(), self._terminate(), {})\n\n    def reset(self):\n        self.steps = 0\n\n        self.state = np.array([random.choice([1, 0]) for _ in range(self.bit_length)])\n\n        # make sure goal is not the initial state\n        self.goal = self.state\n        while (self.goal == self.state).all():\n            self.goal = np.array([random.choice([1, 0]) for _ in range(self.bit_length)])\n\n        return self._get_obs()\n\n    def _mean_zero(self, x):\n        if self.mean_zero:\n            return (x - 0.5) / 0.5\n        else:\n            return x\n\n    def _get_obs(self):\n        return {\n            \'state\': self._mean_zero(self.state),\n            \'desired_goal\': self._mean_zero(self.goal),\n            \'achieved_goal\': self._mean_zero(self.state)\n        }\n\n    def render(self, mode=\'human\', close=False):\n        observation = np.zeros((20, 20 * self.bit_length, 3))\n        for bit_idx, (state_bit, goal_bit) in enumerate(zip(self.state, self.goal)):\n            # green if the bit matches\n            observation[:, bit_idx * 20:(bit_idx + 1) * 20, 1] = (state_bit == goal_bit) * 255\n            # red if the bit doesn\'t match\n            observation[:, bit_idx * 20:(bit_idx + 1) * 20, 0] = (state_bit != goal_bit) * 255\n        return observation\n'"
rl_coach/environments/toy_problems/exploration_chain.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom enum import Enum\n\nimport gym\nimport numpy as np\nfrom gym import spaces\n\n\nclass ExplorationChain(gym.Env):\n    metadata = {\n        \'render.modes\': [\'human\', \'rgb_array\'], \'video.frames_per_second\': 30\n    }\n\n    class ObservationType(Enum):\n        OneHot = 0\n        Therm = 1\n\n    def __init__(self, chain_length=16, start_state=1, max_steps=None, observation_type=ObservationType.Therm,\n                 left_state_reward=1/1000, right_state_reward=1, simple_render=True):\n        super().__init__()\n        if chain_length <= 3:\n            raise ValueError(\'Chain length must be > 3, found {}\'.format(chain_length))\n        if not 0 <= start_state < chain_length:\n            raise ValueError(\'The start state should be within the chain bounds, found {}\'.format(start_state))\n        self.chain_length = chain_length\n        self.start_state = start_state\n        self.max_steps = max_steps\n        self.observation_type = observation_type\n        self.left_state_reward = left_state_reward\n        self.right_state_reward = right_state_reward\n        self.simple_render = simple_render\n\n        # spaces documentation: https://gym.openai.com/docs/\n        self.action_space = spaces.Discrete(2)  # 0 -> Go left, 1 -> Go right\n        self.observation_space = spaces.Box(0, 1, shape=(chain_length,))#spaces.MultiBinary(chain_length)\n\n        self.reset()\n\n    def _terminate(self):\n        return self.steps >= self.max_steps\n\n    def _reward(self):\n        if self.state == 0:\n            return self.left_state_reward\n        elif self.state == self.chain_length - 1:\n            return self.right_state_reward\n        else:\n            return 0\n\n    def step(self, action):\n        # action is 0 or 1\n        if action == 0:\n            if 0 < self.state:\n                self.state -= 1\n        elif action == 1:\n            if self.state < self.chain_length - 1:\n                self.state += 1\n        else:\n            raise ValueError(""An invalid action was given. The available actions are - 0 or 1, found {}"".format(action))\n\n        self.steps += 1\n\n        return self._get_obs(), self._reward(), self._terminate(), {}\n\n    def reset(self):\n        self.steps = 0\n\n        self.state = self.start_state\n\n        return self._get_obs()\n\n    def _get_obs(self):\n        self.observation = np.zeros((self.chain_length,))\n        if self.observation_type == self.ObservationType.OneHot:\n            self.observation[self.state] = 1\n        elif self.observation_type == self.ObservationType.Therm:\n            self.observation[:(self.state+1)] = 1\n\n        return self.observation\n\n    def render(self, mode=\'human\', close=False):\n        if self.simple_render:\n            observation = np.zeros((20, 20*self.chain_length))\n            observation[:, self.state*20:(self.state+1)*20] = 255\n            return observation\n        else:\n            # lazy loading of networkx and matplotlib to allow using the environment without installing them if\n            # necessary\n            import networkx as nx\n            from networkx.drawing.nx_agraph import graphviz_layout\n            import matplotlib.pyplot as plt\n\n            if not hasattr(self, \'G\'):\n                self.states = list(range(self.chain_length))\n                self.G = nx.DiGraph(directed=True)\n                for i, origin_state in enumerate(self.states):\n                    if i < self.chain_length - 1:\n                        self.G.add_edge(origin_state,\n                                        origin_state + 1,\n                                        weight=0.5)\n                    if i > 0:\n                        self.G.add_edge(origin_state,\n                                        origin_state - 1,\n                                        weight=0.5, )\n                    if i == 0 or i < self.chain_length - 1:\n                        self.G.add_edge(origin_state,\n                                        origin_state,\n                                        weight=0.5, )\n\n            fig = plt.gcf()\n            if np.all(fig.get_size_inches() != [10, 2]):\n                fig.set_size_inches(5, 1)\n            color = [\'y\']*(len(self.G))\n            color[self.state] = \'r\'\n            options = {\n                \'node_color\': color,\n                \'node_size\': 50,\n                \'width\': 1,\n                \'arrowstyle\': \'-|>\',\n                \'arrowsize\': 5,\n                \'font_size\': 6\n            }\n            pos = graphviz_layout(self.G, prog=\'dot\', args=\'-Grankdir=LR\')\n            nx.draw_networkx(self.G, pos, arrows=True, **options)\n            fig.canvas.draw()\n            data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            return data\n'"
rl_coach/filters/action/__init__.py,0,"b""from .attention_discretization import AttentionDiscretization\nfrom .box_discretization import BoxDiscretization\nfrom .box_masking import BoxMasking\nfrom .full_discrete_action_space_map import FullDiscreteActionSpaceMap\nfrom .linear_box_to_box_map import LinearBoxToBoxMap\nfrom .partial_discrete_action_space_map import PartialDiscreteActionSpaceMap\n__all__ = [\n    'AttentionDiscretization',\n    'BoxDiscretization',\n    'BoxMasking',\n    'FullDiscreteActionSpaceMap',\n    'LinearBoxToBoxMap',\n    'PartialDiscreteActionSpaceMap'\n]"""
rl_coach/filters/action/action_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom rl_coach.core_types import ActionType\nfrom rl_coach.filters.filter import Filter\nfrom rl_coach.spaces import ActionSpace\n\n\nclass ActionFilter(Filter):\n    def __init__(self, input_action_space: ActionSpace=None):\n        self.input_action_space = input_action_space\n        self.output_action_space = None\n        super().__init__()\n\n    def get_unfiltered_action_space(self, output_action_space: ActionSpace) -> ActionSpace:\n        """"""\n        This function should contain the logic for getting the unfiltered action space\n        :param output_action_space: the output action space\n        :return: the unfiltered action space\n        """"""\n        return output_action_space\n\n    def validate_output_action_space(self, output_action_space: ActionSpace):\n        """"""\n        A function that implements validation of the output action space\n        :param output_action_space: the input action space\n        :return: None\n        """"""\n        pass\n\n    def validate_output_action(self, action: ActionType):\n        """"""\n        A function that verifies that the given action is in the expected output action space\n        :param action: an action to validate\n        :return: None\n        """"""\n        if not self.output_action_space.contains(action):\n            raise ValueError(""The given action ({}) does not match the action space ({})""\n                             .format(action, self.output_action_space))\n\n    def filter(self, action: ActionType) -> ActionType:\n        """"""\n        A function that transforms from the agent\'s action space to the environment\'s action space\n        :param action: an action to transform\n        :return: transformed action\n        """"""\n        raise NotImplementedError("""")\n\n    def reverse_filter(self, action: ActionType) -> ActionType:\n        """"""\n        A function that transforms from the environment\'s action space to the agent\'s action space\n        :param action: an action to transform\n        :return: transformed action\n        """"""\n        raise NotImplementedError("""")'"
rl_coach/filters/action/attention_discretization.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union, List\n\nimport numpy as np\n\nfrom rl_coach.filters.action.box_discretization import BoxDiscretization\nfrom rl_coach.filters.action.partial_discrete_action_space_map import PartialDiscreteActionSpaceMap\nfrom rl_coach.spaces import AttentionActionSpace, BoxActionSpace, DiscreteActionSpace\n\n\nclass AttentionDiscretization(PartialDiscreteActionSpaceMap):\n    """"""\n    Discretizes an **AttentionActionSpace**. The attention action space defines the actions\n    as choosing sub-boxes in a given box. For example, consider an image of size 100x100, where the action is choosing\n    a crop window of size 20x20 to attend to in the image. AttentionDiscretization allows discretizing the possible crop\n    windows to choose into a finite number of options, and map a discrete action space into those crop windows.\n\n    Warning! this will currently only work for attention spaces with 2 dimensions.\n    """"""\n    def __init__(self, num_bins_per_dimension: Union[int, List[int]], force_int_bins=False):\n        """"""\n        :param num_bins_per_dimension: Number of discrete bins to use for each dimension of the action space\n        :param force_int_bins: If set to True, all the bins will represent integer coordinates in space.\n        """"""\n        # we allow specifying either a single number for all dimensions, or a single number per dimension in the target\n        # action space\n        self.num_bins_per_dimension = num_bins_per_dimension\n\n        self.force_int_bins = force_int_bins\n\n        # TODO: this will currently only work for attention spaces with 2 dimensions. generalize it.\n\n        super().__init__()\n\n    def validate_output_action_space(self, output_action_space: AttentionActionSpace):\n        if not isinstance(output_action_space, AttentionActionSpace):\n            raise ValueError(""AttentionActionSpace discretization only works with an output space of type AttentionActionSpace. ""\n                             ""The given output space is {}"".format(output_action_space))\n\n    def get_unfiltered_action_space(self, output_action_space: AttentionActionSpace) -> DiscreteActionSpace:\n        if isinstance(self.num_bins_per_dimension, int):\n            self.num_bins_per_dimension = [self.num_bins_per_dimension] * output_action_space.shape[0]\n\n        # create a discrete to linspace map to ease the extraction of attention actions\n        discrete_to_box = BoxDiscretization([n+1 for n in self.num_bins_per_dimension],\n                                            self.force_int_bins)\n        discrete_to_box.get_unfiltered_action_space(BoxActionSpace(output_action_space.shape,\n                                                                   output_action_space.low,\n                                                                   output_action_space.high), )\n\n        rows, cols = self.num_bins_per_dimension\n        start_ind = [i * (cols + 1) + j for i in range(rows + 1) if i < rows for j in range(cols + 1) if j < cols]\n        end_ind = [i + cols + 2 for i in start_ind]\n        self.target_actions = [np.array([discrete_to_box.target_actions[start],\n                                         discrete_to_box.target_actions[end]])\n                               for start, end in zip(start_ind, end_ind)]\n\n        return super().get_unfiltered_action_space(output_action_space)\n'"
rl_coach/filters/action/box_discretization.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom itertools import product\nfrom typing import Union, List\n\nimport numpy as np\n\nfrom rl_coach.filters.action.partial_discrete_action_space_map import PartialDiscreteActionSpaceMap\nfrom rl_coach.spaces import BoxActionSpace, DiscreteActionSpace\n\n\nclass BoxDiscretization(PartialDiscreteActionSpaceMap):\n    """"""\n    Discretizes a continuous action space into a discrete action space, allowing the usage of\n    agents such as DQN for continuous environments such as MuJoCo. Given the number of bins to discretize into, the\n    original continuous action space is uniformly separated into the given number of bins, each mapped to a discrete\n    action index. Each discrete action is mapped to a single N dimensional action in the BoxActionSpace action space.\n    For example, if the original actions space is between -1 and 1 and 5 bins were selected, the new action\n    space will consist of 5 actions mapped to -1, -0.5, 0, 0.5 and 1.\n    """"""\n    def __init__(self, num_bins_per_dimension: Union[int, List[int]], force_int_bins=False):\n        """"""\n        :param num_bins_per_dimension: The number of bins to use for each dimension of the target action space.\n                                       The bins will be spread out uniformly over this space\n        :param force_int_bins: force the bins to represent only integer actions. for example, if the action space is in\n                               the range 0-10 and there are 5 bins, then the bins will be placed at 0, 2, 5, 7, 10,\n                               instead of 0, 2.5, 5, 7.5, 10.\n        """"""\n        # we allow specifying either a single number for all dimensions, or a single number per dimension in the target\n        # action space\n        self.num_bins_per_dimension = num_bins_per_dimension\n        self.force_int_bins = force_int_bins\n        super().__init__()\n\n    def validate_output_action_space(self, output_action_space: BoxActionSpace):\n        if not isinstance(output_action_space, BoxActionSpace):\n            raise ValueError(""BoxActionSpace discretization only works with an output space of type BoxActionSpace. ""\n                             ""The given output space is {}"".format(output_action_space))\n\n        if len(self.num_bins_per_dimension) != output_action_space.shape:\n            # TODO: this check is not sufficient. it does not deal with actions spaces with more than one axis\n            raise ValueError(""The length of the list of bins per dimension ({}) does not match the number of ""\n                             ""dimensions in the action space ({})""\n                             .format(len(self.num_bins_per_dimension), output_action_space))\n\n    def get_unfiltered_action_space(self, output_action_space: BoxActionSpace) -> DiscreteActionSpace:\n        if isinstance(self.num_bins_per_dimension, int):\n            self.num_bins_per_dimension = np.ones(output_action_space.shape).astype(int) * self.num_bins_per_dimension\n\n        bins = []\n        for i in range(len(output_action_space.low)):\n            dim_bins = np.linspace(output_action_space.low[i], output_action_space.high[i],\n                                   self.num_bins_per_dimension[i])\n            if self.force_int_bins:\n                dim_bins = dim_bins.astype(int)\n            bins.append(dim_bins)\n        self.target_actions = [list(action) for action in list(product(*bins))]\n\n        return super().get_unfiltered_action_space(output_action_space)\n'"
rl_coach/filters/action/box_masking.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.core_types import ActionType\nfrom rl_coach.filters.action.action_filter import ActionFilter\nfrom rl_coach.spaces import BoxActionSpace\n\n\nclass BoxMasking(ActionFilter):\n    """"""\n    Masks part of the action space to enforce the agent to work in a defined space. For example,\n    if the original action space is between -1 and 1, then this filter can be used in order to constrain the agent actions\n    to the range 0 and 1 instead. This essentially masks the range -1 and 0 from the agent.\n    The resulting action space will be shifted and will always start from 0 and have the size of the unmasked area.\n    """"""\n    def __init__(self,\n                 masked_target_space_low: Union[None, int, float, np.ndarray],\n                 masked_target_space_high: Union[None, int, float, np.ndarray]):\n        """"""\n        :param masked_target_space_low: the lowest values that can be chosen in the target action space\n        :param masked_target_space_high: the highest values that can be chosen in the target action space\n        """"""\n        self.masked_target_space_low = masked_target_space_low\n        self.masked_target_space_high = masked_target_space_high\n        self.offset = masked_target_space_low\n        super().__init__()\n\n    def set_masking(self, masked_target_space_low: Union[None, int, float, np.ndarray],\n                    masked_target_space_high: Union[None, int, float, np.ndarray]):\n        self.masked_target_space_low = masked_target_space_low\n        self.masked_target_space_high = masked_target_space_high\n        self.offset = masked_target_space_low\n        if self.output_action_space:\n            self.validate_output_action_space(self.output_action_space)\n            self.input_action_space = BoxActionSpace(self.output_action_space.shape,\n                                                     low=0,\n                                                     high=self.masked_target_space_high - self.masked_target_space_low)\n\n    def validate_output_action_space(self, output_action_space: BoxActionSpace):\n        if not isinstance(output_action_space, BoxActionSpace):\n            raise ValueError(""BoxActionSpace discretization only works with an output space of type BoxActionSpace. ""\n                             ""The given output space is {}"".format(output_action_space))\n        if self.masked_target_space_low is None or self.masked_target_space_high is None:\n            raise ValueError(""The masking target space size was not set. Please call set_masking."")\n        if not (np.all(output_action_space.low <= self.masked_target_space_low)\n                and np.all(self.masked_target_space_low <= output_action_space.high)):\n            raise ValueError(""The low values for masking the action space ({}) are not within the range of the ""\n                             ""target space (low = {}, high = {})""\n                             .format(self.masked_target_space_low, output_action_space.low, output_action_space.high))\n        if not (np.all(output_action_space.low <= self.masked_target_space_high)\n                and np.all(self.masked_target_space_high <= output_action_space.high)):\n            raise ValueError(""The high values for masking the action space ({}) are not within the range of the ""\n                             ""target space (low = {}, high = {})""\n                             .format(self.masked_target_space_high, output_action_space.low, output_action_space.high))\n\n    def get_unfiltered_action_space(self, output_action_space: BoxActionSpace) -> BoxActionSpace:\n        self.output_action_space = output_action_space\n        self.input_action_space = BoxActionSpace(output_action_space.shape,\n                                                 low=0,\n                                                 high=self.masked_target_space_high - self.masked_target_space_low)\n        return self.input_action_space\n\n    def filter(self, action: ActionType) -> ActionType:\n        return action + self.offset\n'"
rl_coach/filters/action/full_discrete_action_space_map.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom rl_coach.filters.action.partial_discrete_action_space_map import PartialDiscreteActionSpaceMap\nfrom rl_coach.spaces import ActionSpace, DiscreteActionSpace\n\n\nclass FullDiscreteActionSpaceMap(PartialDiscreteActionSpaceMap):\n    """"""\n    Full map of two countable action spaces. This works in a similar way to the\n    PartialDiscreteActionSpaceMap, but maps the entire source action space into the entire target action space, without\n    masking any actions.\n    For example, if there are 10 multiselect actions in the output space, the actions 0-9 will be mapped to those\n    multiselect actions.\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def get_unfiltered_action_space(self, output_action_space: ActionSpace) -> DiscreteActionSpace:\n        self.target_actions = output_action_space.actions\n        return super().get_unfiltered_action_space(output_action_space)\n'"
rl_coach/filters/action/linear_box_to_box_map.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\n\nimport numpy as np\n\nfrom rl_coach.core_types import ActionType\nfrom rl_coach.filters.action.action_filter import ActionFilter\nfrom rl_coach.spaces import BoxActionSpace\n\n\nclass LinearBoxToBoxMap(ActionFilter):\n    """"""\n    A linear mapping of two box action spaces. For example, if the action space of the\n    environment consists of continuous actions between 0 and 1, and we want the agent to choose actions between -1 and 1,\n    the LinearBoxToBoxMap can be used to map the range -1 and 1 to the range 0 and 1 in a linear way. This means that the\n    action -1 will be mapped to 0, the action 1 will be mapped to 1, and the rest of the actions will be linearly mapped\n    between those values.\n    """"""\n    def __init__(self,\n                 input_space_low: Union[None, int, float, np.ndarray],\n                 input_space_high: Union[None, int, float, np.ndarray]):\n        """"""\n        :param input_space_low: the low values of the desired action space\n        :param input_space_high: the high values of the desired action space\n        """"""\n        self.input_space_low = input_space_low\n        self.input_space_high = input_space_high\n        self.rescale = None\n        self.offset = None\n        super().__init__()\n\n    def validate_output_action_space(self, output_action_space: BoxActionSpace):\n        if not isinstance(output_action_space, BoxActionSpace):\n            raise ValueError(""BoxActionSpace discretization only works with an output space of type BoxActionSpace. ""\n                             ""The given output space is {}"".format(output_action_space))\n\n    def get_unfiltered_action_space(self, output_action_space: BoxActionSpace) -> BoxActionSpace:\n        self.input_action_space = BoxActionSpace(output_action_space.shape, self.input_space_low, self.input_space_high)\n        self.rescale = \\\n            (output_action_space.high - output_action_space.low) / (self.input_space_high - self.input_space_low)\n        self.offset = output_action_space.low - self.input_space_low\n        self.output_action_space = output_action_space\n        return self.input_action_space\n\n    def filter(self, action: ActionType) -> ActionType:\n        return self.output_action_space.low + (action - self.input_space_low) * self.rescale\n\n'"
rl_coach/filters/action/partial_discrete_action_space_map.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nfrom rl_coach.core_types import ActionType\nfrom rl_coach.filters.action.action_filter import ActionFilter\nfrom rl_coach.spaces import DiscreteActionSpace, ActionSpace\n\n\nclass PartialDiscreteActionSpaceMap(ActionFilter):\n    """"""\n    Partial map of two countable action spaces. For example, consider an environment\n    with a MultiSelect action space (select multiple actions at the same time, such as jump and go right), with 8 actual\n    MultiSelect actions. If we want the agent to be able to select only 5 of those actions by their index (0-4), we can\n    map a discrete action space with 5 actions into the 5 selected MultiSelect actions. This will both allow the agent to\n    use regular discrete actions, and mask 3 of the actions from the agent.\n    """"""\n    def __init__(self, target_actions: List[ActionType]=None, descriptions: List[str]=None):\n        """"""\n        :param target_actions: A partial list of actions from the target space to map to.\n        :param descriptions: a list of descriptions of each of the actions\n        """"""\n        self.target_actions = target_actions\n        self.descriptions = descriptions\n        super().__init__()\n\n    def validate_output_action_space(self, output_action_space: ActionSpace):\n        if not self.target_actions:\n            raise ValueError(""The target actions were not set"")\n        for v in self.target_actions:\n            if not output_action_space.contains(v):\n                raise ValueError(""The values in the output actions ({}) do not match the output action ""\n                                 ""space definition ({})"".format(v, output_action_space))\n\n    def get_unfiltered_action_space(self, output_action_space: ActionSpace) -> DiscreteActionSpace:\n        self.output_action_space = output_action_space\n        self.input_action_space = DiscreteActionSpace(len(self.target_actions), self.descriptions,\n                                                      filtered_action_space=output_action_space)\n        return self.input_action_space\n\n    def filter(self, action: ActionType) -> ActionType:\n        return self.target_actions[action]\n\n    def reverse_filter(self, action: ActionType) -> ActionType:\n        return [(action == x).all() for x in self.target_actions].index(True)\n\n'"
rl_coach/filters/observation/__init__.py,0,"b""from .observation_clipping_filter import ObservationClippingFilter\nfrom .observation_crop_filter import ObservationCropFilter\nfrom .observation_move_axis_filter import ObservationMoveAxisFilter\nfrom .observation_normalization_filter import ObservationNormalizationFilter\nfrom .observation_reduction_by_sub_parts_name_filter import ObservationReductionBySubPartsNameFilter\nfrom .observation_rescale_size_by_factor_filter import ObservationRescaleSizeByFactorFilter\nfrom .observation_rescale_to_size_filter import ObservationRescaleToSizeFilter\nfrom .observation_rgb_to_y_filter import ObservationRGBToYFilter\nfrom .observation_squeeze_filter import ObservationSqueezeFilter\nfrom .observation_stacking_filter import ObservationStackingFilter\nfrom .observation_to_uint8_filter import ObservationToUInt8Filter\n\n__all__ = [\n    'ObservationClippingFilter',\n    'ObservationCropFilter',\n    'ObservationMoveAxisFilter',\n    'ObservationNormalizationFilter',\n    'ObservationReductionBySubPartsNameFilter',\n    'ObservationRescaleSizeByFactorFilter',\n    'ObservationRescaleToSizeFilter',\n    'ObservationRGBToYFilter',\n    'ObservationSqueezeFilter',\n    'ObservationStackingFilter',\n    'ObservationToUInt8Filter'\n]"""
rl_coach/filters/observation/observation_clipping_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace\n\n\nclass ObservationClippingFilter(ObservationFilter):\n    """"""\n    Clips the observation values to a given range of values.\n    For example, if the observation consists of measurements in an arbitrary range,\n    and we want to control the minimum and maximum values of these observations,\n    we can define a range and clip the values of the measurements.\n    """"""\n    def __init__(self, clipping_low: float=-np.inf, clipping_high: float=np.inf):\n        """"""\n        :param clipping_low: The minimum value to allow after normalizing the observation\n        :param clipping_high: The maximum value to allow after normalizing the observation\n        """"""\n        super().__init__()\n        self.clip_min = clipping_low\n        self.clip_max = clipping_high\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        observation = np.clip(observation, self.clip_min, self.clip_max)\n\n        return observation\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        return input_observation_space\n'"
rl_coach/filters/observation/observation_crop_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom typing import Union, Tuple\n\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace\n\n\nclass ObservationCropFilter(ObservationFilter):\n    """"""\n    Crops the size of the observation to a given crop window. For example, in Atari, the\n    observations are images with a shape of 210x160. Usually, we will want to crop the size of the observation to a\n    square of 160x160 before rescaling them.\n    """"""\n    def __init__(self, crop_low: np.ndarray=None, crop_high: np.ndarray=None):\n        """"""\n        :param crop_low: a vector where each dimension describes the start index for cropping the observation in the\n                         corresponding dimension. a negative value of -1 will be mapped to the max size\n        :param crop_high: a vector where each dimension describes the end index for cropping the observation in the\n                          corresponding dimension. a negative value of -1 will be mapped to the max size\n        """"""\n        super().__init__()\n        if crop_low is None and crop_high is None:\n            raise ValueError(""At least one of crop_low and crop_high should be set to a real value. "")\n        if crop_low is None:\n            crop_low = np.array([0] * len(crop_high))\n        if crop_high is None:\n            crop_high = np.array([-1] * len(crop_low))\n\n        self.crop_low = crop_low\n        self.crop_high = crop_high\n\n        for h, l in zip(crop_high, crop_low):\n            if h < l and h != -1:\n                raise ValueError(""Some of the cropping low values are higher than cropping high values"")\n        if np.any(crop_high < -1) or np.any(crop_low < -1):\n            raise ValueError(""Cropping values cannot be negative"")\n        if crop_low.shape != crop_high.shape:\n            raise ValueError(""The low values and high values for cropping must have the same number of dimensions"")\n        if crop_low.dtype != int or crop_high.dtype != int:\n            raise ValueError(""The crop values should be int values, instead they are defined as: {} and {}""\n                             .format(crop_low.dtype, crop_high.dtype))\n\n    def _replace_negative_one_in_crop_size(self, crop_size: np.ndarray, observation_shape: Union[Tuple, np.ndarray]):\n        # replace -1 with the max size\n        crop_size = crop_size.copy()\n        for i in range(len(observation_shape)):\n            if crop_size[i] == -1:\n                crop_size[i] = observation_shape[i]\n        return crop_size\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        crop_high = self._replace_negative_one_in_crop_size(self.crop_high, input_observation_space.shape)\n        crop_low = self._replace_negative_one_in_crop_size(self.crop_low, input_observation_space.shape)\n        if np.any(crop_high > input_observation_space.shape) or \\\n                np.any(crop_low > input_observation_space.shape):\n            raise ValueError(""The cropping values are outside of the observation space"")\n        if not input_observation_space.is_valid_index(crop_low) or \\\n                not input_observation_space.is_valid_index(crop_high - 1):\n            raise ValueError(""The cropping indices are outside of the observation space"")\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        # replace -1 with the max size\n        crop_high = self._replace_negative_one_in_crop_size(self.crop_high, observation.shape)\n        crop_low = self._replace_negative_one_in_crop_size(self.crop_low, observation.shape)\n\n        # crop\n        indices = [slice(i, j) for i, j in zip(crop_low, crop_high)]\n        observation = observation[indices]\n        return observation\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        # replace -1 with the max size\n        crop_high = self._replace_negative_one_in_crop_size(self.crop_high, input_observation_space.shape)\n        crop_low = self._replace_negative_one_in_crop_size(self.crop_low, input_observation_space.shape)\n\n        input_observation_space.shape = crop_high - crop_low\n        return input_observation_space\n'"
rl_coach/filters/observation/observation_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom rl_coach.filters.filter import Filter\nfrom rl_coach.spaces import ObservationSpace\n\n\nclass ObservationFilter(Filter):\n    def __init__(self):\n        super().__init__()\n        self.supports_batching = False\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        """"""\n        This function should contain the logic for getting the filtered observation space\n        :param input_observation_space: the input observation space\n        :return: the filtered observation space\n        """"""\n        return input_observation_space\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        """"""\n        A function that implements validation of the input observation space\n        :param input_observation_space: the input observation space\n        :return: None\n        """"""\n        pass'"
rl_coach/filters/observation/observation_move_axis_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace, PlanarMapsObservationSpace\n\n\nclass ObservationMoveAxisFilter(ObservationFilter):\n    """"""\n    Reorders the axes of the observation. This can be useful when the observation is an\n    image, and we want to move the channel axis to be the last axis instead of the first axis.\n    """"""\n    def __init__(self, axis_origin: int = None, axis_target: int=None):\n        """"""\n        :param axis_origin: The axis to move\n        :param axis_target: Where to move the selected axis to\n        """"""\n        super().__init__()\n        self.axis_origin = axis_origin\n        self.axis_target = axis_target\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        shape = input_observation_space.shape\n        if not -len(shape) <= self.axis_origin < len(shape) or not -len(shape) <= self.axis_target < len(shape):\n            raise ValueError(""The given axis does not exist in the context of the input observation shape. "")\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        return np.moveaxis(observation, self.axis_origin, self.axis_target)\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        axis_size = input_observation_space.shape[self.axis_origin]\n        input_observation_space.shape = np.delete(input_observation_space.shape, self.axis_origin)\n        if self.axis_target == -1:\n            input_observation_space.shape = np.append(input_observation_space.shape, axis_size)\n        elif self.axis_target < -1:\n            input_observation_space.shape = np.insert(input_observation_space.shape, self.axis_target+1, axis_size)\n        else:\n            input_observation_space.shape = np.insert(input_observation_space.shape, self.axis_target, axis_size)\n\n        # move the channels axis according to the axis change\n        if isinstance(input_observation_space, PlanarMapsObservationSpace):\n            if input_observation_space.channels_axis == self.axis_origin:\n                input_observation_space.channels_axis = self.axis_target\n            elif input_observation_space.channels_axis == self.axis_target:\n                input_observation_space.channels_axis = self.axis_origin\n            elif self.axis_origin < input_observation_space.channels_axis < self.axis_target:\n                input_observation_space.channels_axis -= 1\n            elif self.axis_target < input_observation_space.channels_axis < self.axis_origin:\n                input_observation_space.channels_axis += 1\n\n        return input_observation_space\n'"
rl_coach/filters/observation/observation_normalization_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport pickle\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace\nfrom rl_coach.utilities.shared_running_stats import NumpySharedRunningStats, NumpySharedRunningStats\n\n\nclass ObservationNormalizationFilter(ObservationFilter):\n    """"""\n    Normalizes the observation values with a running mean and standard deviation of\n    all the observations seen so far. The normalization is performed element-wise. Additionally, when working with\n    multiple workers, the statistics used for the normalization operation are accumulated over all the workers.\n    """"""\n    def __init__(self, clip_min: float=-5.0, clip_max: float=5.0, name=\'observation_stats\'):\n        """"""\n        :param clip_min: The minimum value to allow after normalizing the observation\n        :param clip_max: The maximum value to allow after normalizing the observation\n        """"""\n        super().__init__()\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n        self.running_observation_stats = None\n        self.name = name\n        self.supports_batching = True\n        self.observation_space = None\n\n    def set_device(self, device, memory_backend_params=None, mode=\'numpy\') -> None:\n        """"""\n        An optional function that allows the filter to get the device if it is required to use tensorflow ops\n        :param device: the device to use\n        :memory_backend_params: if not None, holds params for a memory backend for sharing data (e.g. Redis)\n        :param mode: the arithmetic module to use {\'tf\' | \'numpy\'}\n        :return: None\n        """"""\n        if mode == \'tf\':\n            from rl_coach.architectures.tensorflow_components.shared_variables import TFSharedRunningStats\n            self.running_observation_stats = TFSharedRunningStats(device, name=self.name, create_ops=False,\n                                                            pubsub_params=memory_backend_params)\n        elif mode == \'numpy\':\n            self.running_observation_stats = NumpySharedRunningStats(name=self.name,\n                                                                     pubsub_params=memory_backend_params)\n\n    def set_session(self, sess) -> None:\n        """"""\n        An optional function that allows the filter to get the session if it is required to use tensorflow ops\n        :param sess: the session\n        :return: None\n        """"""\n        self.running_observation_stats.set_session(sess)\n\n    def filter(self, observations: List[ObservationType], update_internal_state: bool=True) -> ObservationType:\n        observations = np.array(observations)\n        if update_internal_state:\n            self.running_observation_stats.push(observations)\n            self.last_mean = self.running_observation_stats.mean\n            self.last_stdev = self.running_observation_stats.std\n\n        return self.running_observation_stats.normalize(observations)\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        self.running_observation_stats.set_params(shape=input_observation_space.shape,\n                                                  clip_values=(self.clip_min, self.clip_max))\n        return input_observation_space\n\n    def save_state_to_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        self.running_observation_stats.save_state_to_checkpoint(checkpoint_dir, checkpoint_prefix)\n\n    def restore_state_from_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        self.running_observation_stats.restore_state_from_checkpoint(checkpoint_dir, checkpoint_prefix)\n '"
rl_coach/filters/observation/observation_reduction_by_sub_parts_name_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport copy\nfrom enum import Enum\nfrom typing import List\n\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace, VectorObservationSpace\n\n\nclass ObservationReductionBySubPartsNameFilter(ObservationFilter):\n    """"""\n    Allows keeping only parts of the observation, by specifying their\n    name. This is useful when the environment has a measurements vector as observation which includes several different\n    measurements, but you want the agent to only see some of the measurements and not all.\n    For example, the CARLA environment extracts multiple measurements that can be used by the agent, such as\n    speed and location. If we want to only use the speed, it can be done using this filter.\n    This will currently work only for VectorObservationSpace observations\n    """"""\n    class ReductionMethod(Enum):\n        Keep = 0\n        Discard = 1\n\n    def __init__(self, part_names: List[str], reduction_method: ReductionMethod):\n        """"""\n        :param part_names: A list of part names to reduce\n        :param reduction_method: A reduction method to use - keep or discard the given parts\n        """"""\n        super().__init__()\n        self.part_names = part_names\n        self.reduction_method = reduction_method\n        self.measurement_names = None\n        self.indices_to_keep = None\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        if not isinstance(observation, np.ndarray):\n            raise ValueError(""All the state values are expected to be numpy arrays"")\n        if self.indices_to_keep is None:\n            raise ValueError(""To use ObservationReductionBySubPartsNameFilter, the get_filtered_observation_space ""\n                             ""function should be called before filtering an observation"")\n        observation = observation[..., self.indices_to_keep]\n        return observation\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        if not isinstance(input_observation_space, VectorObservationSpace):\n            raise ValueError(""The ObservationReductionBySubPartsNameFilter support only VectorObservationSpace ""\n                             ""observations. The given observation space was: {}""\n                             .format(input_observation_space.__class__))\n\n    def get_filtered_observation_space(self, input_observation_space: VectorObservationSpace) -> ObservationSpace:\n        self.measurement_names = copy.copy(input_observation_space.measurements_names)\n\n        if self.reduction_method == self.ReductionMethod.Keep:\n            input_observation_space.shape[-1] = len(self.part_names)\n            self.indices_to_keep = [idx for idx, val in enumerate(self.measurement_names) if val in self.part_names]\n            input_observation_space.measurements_names = copy.copy(self.part_names)\n        elif self.reduction_method == self.ReductionMethod.Discard:\n            input_observation_space.shape[-1] -= len(self.part_names)\n            self.indices_to_keep = [idx for idx, val in enumerate(self.measurement_names) if val not in self.part_names]\n            input_observation_space.measurements_names = [val for val in input_observation_space.measurements_names if\n                                                          val not in self.part_names]\n        else:\n            raise ValueError(""The given reduction method is not supported"")\n\n        return input_observation_space\n'"
rl_coach/filters/observation/observation_rescale_size_by_factor_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom skimage.transform import resize\n\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace\n\n\nclass ObservationRescaleSizeByFactorFilter(ObservationFilter):\n    """"""\n    Rescales an image observation by some factor. For example, the image size\n    can be reduced by a factor of 2.\n    """"""\n    def __init__(self, rescale_factor: float):\n        """"""\n        :param rescale_factor: the factor by which the observation will be rescaled\n        """"""\n        super().__init__()\n        self.rescale_factor = float(rescale_factor)\n        # TODO: allow selecting the channels dim\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        if not 2 <= input_observation_space.num_dimensions <= 3:\n            raise ValueError(""The rescale filter only applies to image observations where the number of dimensions is""\n                             ""either 2 (grayscale) or 3 (RGB). The number of dimensions defined for the ""\n                             ""output observation was {}"".format(input_observation_space.num_dimensions))\n        if input_observation_space.num_dimensions == 3 and input_observation_space.shape[-1] != 3:\n            raise ValueError(""Observations with 3 dimensions must have 3 channels in the last axis (RGB)"")\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        observation = observation.astype(\'uint8\')\n        rescaled_output_size = tuple([int(self.rescale_factor * dim) for dim in observation.shape[:2]])\n\n        if len(observation.shape) == 3:\n            rescaled_output_size += (3,)\n\n        # rescale\n        observation = resize(observation, rescaled_output_size, anti_aliasing=False, preserve_range=True).astype(\'uint8\')\n\n        return observation\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        input_observation_space.shape[:2] = (input_observation_space.shape[:2] * self.rescale_factor).astype(\'int\')\n        return input_observation_space\n'"
rl_coach/filters/observation/observation_rescale_to_size_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom skimage.transform import resize\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace, PlanarMapsObservationSpace, ImageObservationSpace\n\n\nclass ObservationRescaleToSizeFilter(ObservationFilter):\n    """"""\n    Rescales an image observation to a given size. The target size does not\n    necessarily keep the aspect ratio of the original observation.\n    Warning: this requires the input observation to be of type uint8 due to scipy requirements!\n    """"""\n    def __init__(self, output_observation_space: PlanarMapsObservationSpace):\n        """"""\n        :param output_observation_space: the output observation space\n        """"""\n        super().__init__()\n        self.output_observation_space = output_observation_space\n\n        if not isinstance(output_observation_space, PlanarMapsObservationSpace):\n            raise ValueError(""The rescale filter only applies to observation spaces that inherit from ""\n                             ""PlanarMapsObservationSpace. This includes observations which consist of a set of 2D ""\n                             ""images or an RGB image. Instead the output observation space was defined as: {}""\n                             .format(output_observation_space.__class__))\n\n        self.planar_map_output_shape = copy.copy(self.output_observation_space.shape)\n        self.planar_map_output_shape = np.delete(self.planar_map_output_shape,\n                                                 self.output_observation_space.channels_axis)\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        if not isinstance(input_observation_space, PlanarMapsObservationSpace):\n            raise ValueError(""The rescale filter only applies to observation spaces that inherit from ""\n                             ""PlanarMapsObservationSpace. This includes observations which consist of a set of 2D ""\n                             ""images or an RGB image. Instead the input observation space was defined as: {}""\n                             .format(input_observation_space.__class__))\n        if input_observation_space.shape[input_observation_space.channels_axis] \\\n                != self.output_observation_space.shape[self.output_observation_space.channels_axis]:\n            raise ValueError(""The number of channels between the input and output observation spaces must match. ""\n                             ""Instead the number of channels were: {}, {}""\n                             .format(input_observation_space.shape[input_observation_space.channels_axis],\n                             self.output_observation_space.shape[self.output_observation_space.channels_axis]))\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        observation = observation.astype(\'uint8\')\n\n        # rescale\n        if isinstance(self.output_observation_space, ImageObservationSpace):\n            observation = resize(observation, tuple(self.output_observation_space.shape), anti_aliasing=False,\n                                 preserve_range=True).astype(\'uint8\')\n\n        else:\n            new_observation = []\n            for i in range(self.output_observation_space.shape[self.output_observation_space.channels_axis]):\n                new_observation.append(resize(observation.take(i, self.output_observation_space.channels_axis),\n                                              tuple(self.planar_map_output_shape),\n                                              preserve_range=True).astype(\'uint8\'))\n            new_observation = np.array(new_observation)\n            observation = new_observation.swapaxes(0, self.output_observation_space.channels_axis)\n\n        return observation\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        input_observation_space.shape = self.output_observation_space.shape\n        return input_observation_space\n'"
rl_coach/filters/observation/observation_rgb_to_y_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace\n\n\nclass ObservationRGBToYFilter(ObservationFilter):\n    """"""\n    Converts a color image observation specified using the RGB encoding into a grayscale\n    image observation, by keeping only the luminance (Y) channel of the YUV encoding. This can be useful if the colors\n    in the original image are not relevant for solving the task at hand.\n    The channels axis is assumed to be the last axis\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        if input_observation_space.num_dimensions != 3:\n            raise ValueError(""The rescale filter only applies to image observations where the number of dimensions is""\n                             ""3 (RGB). The number of dimensions defined for the input observation was {}""\n                             .format(input_observation_space.num_dimensions))\n        if input_observation_space.shape[-1] != 3:\n            raise ValueError(""The observation space is expected to have 3 channels in the 1st dimension. The number of ""\n                             ""dimensions received is {}"".format(input_observation_space.shape[-1]))\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n\n        # rgb to y\n        r, g, b = observation[:, :, 0], observation[:, :, 1], observation[:, :, 2]\n        observation = 0.2989 * r + 0.5870 * g + 0.1140 * b\n\n        return observation\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        input_observation_space.shape = input_observation_space.shape[:-1]\n        return input_observation_space\n'"
rl_coach/filters/observation/observation_squeeze_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace\n\n\nclass ObservationSqueezeFilter(ObservationFilter):\n    """"""\n    Removes redundant axes from the observation, which are axes with a dimension of 1.\n    """"""\n    def __init__(self, axis: int = None):\n        """"""\n        :param axis: Specifies which axis to remove. If set to None, all the axes of size 1 will be removed.\n        """"""\n        super().__init__()\n        self.axis = axis\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        if self.axis is None:\n            return\n\n        shape = input_observation_space.shape\n        if self.axis >= len(shape) or self.axis < -len(shape):\n            raise ValueError(""The given axis does not exist in the context of the input observation shape. "")\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        return observation.squeeze(axis=self.axis)\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        dummy_tensor = np.random.rand(*tuple(input_observation_space.shape))\n        input_observation_space.shape = dummy_tensor.squeeze(axis=self.axis).shape\n        return input_observation_space\n'"
rl_coach/filters/observation/observation_stacking_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom collections import deque\n\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace, VectorObservationSpace\n\n\nclass LazyStack(object):\n    """"""\n    A lazy version of np.stack which avoids copying the memory until it is\n    needed.\n    """"""\n\n    def __init__(self, history, axis=None):\n        self.history = copy.copy(history)\n        self.axis = axis\n\n    def __array__(self, dtype=None):\n        array = np.stack(self.history, axis=self.axis)\n        if dtype is not None:\n            array = array.astype(dtype)\n        return array\n\n\nclass ObservationStackingFilter(ObservationFilter):\n    """"""\n    Stacks several observations on top of each other. For image observation this will\n    create a 3D blob. The stacking is done in a lazy manner in order to reduce memory consumption. To achieve this,\n    a LazyStack object is used in order to wrap the observations in the stack. For this reason, the\n    ObservationStackingFilter **must** be the last filter in the inputs filters stack.\n    This filter is stateful since it stores the previous step result and depends on it.\n    The filter adds an additional dimension to the output observation.\n\n    Warning!!! The filter replaces the observation with a LazyStack object, so no filters should be\n    applied after this filter. applying more filters will cause the LazyStack object to be converted to a numpy array\n    and increase the memory footprint.\n    """"""\n    def __init__(self, stack_size: int, stacking_axis: int=-1):\n        """"""\n        :param stack_size: the number of previous observations in the stack\n        :param stacking_axis: the axis on which to stack the observation on\n        """"""\n        super().__init__()\n        self.stack_size = stack_size\n        self.stacking_axis = stacking_axis\n        self.stack = []\n        self.input_observation_space = None\n\n        if stack_size <= 0:\n            raise ValueError(""The stack shape must be a positive number"")\n        if type(stack_size) != int:\n            raise ValueError(""The stack shape must be of int type"")\n\n    @property\n    def next_filter(self) -> \'InputFilter\':\n        return self._next_filter\n\n    @next_filter.setter\n    def next_filter(self, val: \'InputFilter\'):\n        raise ValueError(""ObservationStackingFilter can have no other filters after it since they break its ""\n                         ""functionality"")\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        if len(self.stack) > 0 and not input_observation_space.contains(self.stack[-1]):\n            raise ValueError(""The given input observation space is different than the observations already stored in""\n                             ""the filters memory"")\n        if input_observation_space.num_dimensions <= self.stacking_axis:\n            raise ValueError(""The stacking axis is larger than the number of dimensions in the observation space"")\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        if len(self.stack) == 0:\n            self.stack = deque([observation] * self.stack_size, maxlen=self.stack_size)\n        else:\n            if update_internal_state:\n                self.stack.append(observation)\n        observation = LazyStack(self.stack, self.stacking_axis)\n\n        if isinstance(self.input_observation_space, VectorObservationSpace):\n            # when stacking vectors, we cannot avoid copying the memory as we\'re flattening it all\n            observation = np.array(observation).flatten()\n\n        return observation\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        if isinstance(input_observation_space, VectorObservationSpace):\n            self.input_observation_space = input_observation_space = VectorObservationSpace(input_observation_space.shape * self.stack_size)\n        else:\n            if self.stacking_axis == -1:\n                input_observation_space.shape = np.append(input_observation_space.shape, values=[self.stack_size], axis=0)\n            else:\n                input_observation_space.shape = np.insert(input_observation_space.shape, obj=self.stacking_axis,\n                                                         values=[self.stack_size], axis=0)\n        return input_observation_space\n\n    def reset(self) -> None:\n        self.stack = []\n'"
rl_coach/filters/observation/observation_to_uint8_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\n\nfrom rl_coach.core_types import ObservationType\nfrom rl_coach.filters.observation.observation_filter import ObservationFilter\nfrom rl_coach.spaces import ObservationSpace\n\n\nclass ObservationToUInt8Filter(ObservationFilter):\n    """"""\n    Converts a floating point observation into an unsigned int 8 bit observation. This is\n    mostly useful for reducing memory consumption and is usually used for image observations. The filter will first\n    spread the observation values over the range 0-255 and then discretize them into integer values.\n    """"""\n    def __init__(self, input_low: float, input_high: float):\n        """"""\n        :param input_low: The lowest value currently present in the observation\n        :param input_high: The highest value currently present in the observation\n        """"""\n        super().__init__()\n        self.input_low = input_low\n        self.input_high = input_high\n\n        if input_high <= input_low:\n            raise ValueError(""The input observation space high values can be less or equal to the input observation ""\n                             ""space low values"")\n\n    def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n        if np.all(input_observation_space.low != self.input_low) or \\\n                np.all(input_observation_space.high != self.input_high):\n            raise ValueError(""The observation space values range don\'t match the configuration of the filter.""\n                             ""The configuration is: low = {}, high = {}. The actual values are: low = {}, high = {}""\n                             .format(self.input_low, self.input_high,\n                                     input_observation_space.low, input_observation_space.high))\n\n    def filter(self, observation: ObservationType, update_internal_state: bool=True) -> ObservationType:\n        # scale to 0-1\n        observation = (observation - self.input_low) / (self.input_high - self.input_low)\n\n        # scale to 0-255\n        observation *= 255\n\n        observation = observation.astype(\'uint8\')\n\n        return observation\n\n    def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n        input_observation_space.low = 0\n        input_observation_space.high = 255\n        return input_observation_space\n'"
rl_coach/filters/reward/__init__.py,0,"b""from .reward_rescale_filter import RewardRescaleFilter\nfrom .reward_clipping_filter import RewardClippingFilter\nfrom .reward_normalization_filter import RewardNormalizationFilter\nfrom .reward_ewma_normalization_filter import RewardEwmaNormalizationFilter\n\n__all__ = [\n    'RewardRescaleFilter',\n    'RewardClippingFilter',\n    'RewardNormalizationFilter',\n    'RewardEwmaNormalizationFilter'\n]"""
rl_coach/filters/reward/reward_clipping_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\n\nfrom rl_coach.core_types import RewardType\nfrom rl_coach.filters.reward.reward_filter import RewardFilter\nfrom rl_coach.spaces import RewardSpace\n\n\nclass RewardClippingFilter(RewardFilter):\n    """"""\n    Clips the reward values into a given range. For example, in DQN, the Atari rewards are\n    clipped into the range -1 and 1 in order to control the scale of the returns.\n    """"""\n    def __init__(self, clipping_low: float=-np.inf, clipping_high: float=np.inf):\n        """"""\n        :param clipping_low: The low threshold for reward clipping\n        :param clipping_high: The high threshold for reward clipping\n        """"""\n        super().__init__()\n        self.clipping_low = clipping_low\n        self.clipping_high = clipping_high\n\n        if clipping_low > clipping_high:\n            raise ValueError(""The reward clipping low must be lower than the reward clipping max"")\n\n    def filter(self, reward: RewardType, update_internal_state: bool=True) -> RewardType:\n        reward = float(reward)\n\n        if self.clipping_high:\n            reward = min(reward, self.clipping_high)\n        if self.clipping_low:\n            reward = max(reward, self.clipping_low)\n\n        return reward\n\n    def get_filtered_reward_space(self, input_reward_space: RewardSpace) -> RewardSpace:\n        input_reward_space.high = min(self.clipping_high, input_reward_space.high)\n        input_reward_space.low = max(self.clipping_low, input_reward_space.low)\n        return input_reward_space\n'"
rl_coach/filters/reward/reward_ewma_normalization_filter.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\n\nimport numpy as np\nimport pickle\n\nfrom rl_coach.core_types import RewardType\nfrom rl_coach.filters.reward.reward_filter import RewardFilter\nfrom rl_coach.spaces import RewardSpace\nfrom rl_coach.utils import get_latest_checkpoint\n\n\nclass RewardEwmaNormalizationFilter(RewardFilter):\n    """"""\n    Normalizes the reward values based on Exponential Weighted Moving Average.   \n    """"""\n    def __init__(self, alpha: float = 0.01):\n        """"""\n        :param alpha: the degree of weighting decrease, a constant smoothing factor between 0 and 1.\n                      A higher alpha discounts older observations faster\n        """"""\n        super().__init__()\n        self.alpha = alpha\n        self.moving_average = 0\n        self.initialized = False\n        self.checkpoint_file_extension = \'ewma\'\n        self.supports_batching = True\n\n    def filter(self, reward: RewardType, update_internal_state: bool=True) -> RewardType:\n        if not isinstance(reward, np.ndarray):\n            reward = np.array(reward)\n\n        if update_internal_state:\n            mean_rewards = np.mean(reward)\n\n            if not self.initialized:\n                self.moving_average = mean_rewards\n                self.initialized = True\n            else:\n                self.moving_average += self.alpha * (mean_rewards - self.moving_average)\n\n        return reward - self.moving_average\n\n    def get_filtered_reward_space(self, input_reward_space: RewardSpace) -> RewardSpace:\n        return input_reward_space\n\n    def save_state_to_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: int):\n        dict_to_save = {\'moving_average\': self.moving_average}\n\n        with open(os.path.join(checkpoint_dir, str(checkpoint_prefix) + \'.\' + self.checkpoint_file_extension), \'wb\') as f:\n            pickle.dump(dict_to_save, f, pickle.HIGHEST_PROTOCOL)\n\n    def restore_state_from_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        latest_checkpoint_filename = get_latest_checkpoint(checkpoint_dir, checkpoint_prefix,\n                                                           self.checkpoint_file_extension)\n\n        if latest_checkpoint_filename == \'\':\n            raise ValueError(""Could not find RewardEwmaNormalizationFilter checkpoint file. "")\n\n        with open(os.path.join(checkpoint_dir, str(latest_checkpoint_filename)), \'rb\') as f:\n            saved_dict = pickle.load(f)\n            self.__dict__.update(saved_dict)\n'"
rl_coach/filters/reward/reward_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom rl_coach.filters.filter import Filter\nfrom rl_coach.spaces import RewardSpace\n\n\nclass RewardFilter(Filter):\n    def __init__(self):\n        super().__init__()\n        self.supports_batching = False\n\n    def get_filtered_reward_space(self, input_reward_space: RewardSpace) -> RewardSpace:\n        """"""\n        This function should contain the logic for getting the filtered reward space\n        :param input_reward_space: the input reward space\n        :return: the filtered reward space\n        """"""\n        return input_reward_space'"
rl_coach/filters/reward/reward_normalization_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\n\nimport numpy as np\n\nfrom rl_coach.core_types import RewardType\nfrom rl_coach.filters.reward.reward_filter import RewardFilter\nfrom rl_coach.spaces import RewardSpace\nfrom rl_coach.utilities.shared_running_stats import NumpySharedRunningStats\n\n\nclass RewardNormalizationFilter(RewardFilter):\n    """"""\n    Normalizes the reward values with a running mean and standard deviation of\n    all the rewards seen so far. When working with multiple workers, the statistics used for the normalization operation\n    are accumulated over all the workers.\n    """"""\n    def __init__(self, clip_min: float=-5.0, clip_max: float=5.0):\n        """"""\n        :param clip_min: The minimum value to allow after normalizing the reward\n        :param clip_max: The maximum value to allow after normalizing the reward\n        """"""\n        super().__init__()\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n        self.running_rewards_stats = None\n\n    def set_device(self, device, memory_backend_params=None, mode=\'numpy\') -> None:\n        """"""\n        An optional function that allows the filter to get the device if it is required to use tensorflow ops\n        :param device: the device to use\n        :return: None\n        """"""\n\n        if mode == \'tf\':\n            from rl_coach.architectures.tensorflow_components.shared_variables import TFSharedRunningStats\n            self.running_rewards_stats = TFSharedRunningStats(device, name=\'rewards_stats\', create_ops=False,\n                                                            pubsub_params=memory_backend_params)\n        elif mode == \'numpy\':\n            self.running_rewards_stats = NumpySharedRunningStats(name=\'rewards_stats\',\n                                                          pubsub_params=memory_backend_params)\n\n    def set_session(self, sess) -> None:\n        """"""\n        An optional function that allows the filter to get the session if it is required to use tensorflow ops\n        :param sess: the session\n        :return: None\n        """"""\n        self.running_rewards_stats.set_session(sess)\n\n    def filter(self, reward: RewardType, update_internal_state: bool=True) -> RewardType:\n        if update_internal_state:\n            if not isinstance(reward, np.ndarray) or len(reward.shape) < 2:\n                reward = np.array([[reward]])\n            self.running_rewards_stats.push(reward)\n\n        return self.running_rewards_stats.normalize(reward).squeeze()\n\n    def get_filtered_reward_space(self, input_reward_space: RewardSpace) -> RewardSpace:\n        self.running_rewards_stats.set_params(shape=(1,), clip_values=(self.clip_min, self.clip_max))\n        return input_reward_space\n\n    def save_state_to_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        self.running_rewards_stats.save_state_to_checkpoint(checkpoint_dir, checkpoint_prefix)\n\n    def restore_state_from_checkpoint(self, checkpoint_dir: str, checkpoint_prefix: str):\n        self.running_rewards_stats.restore_state_from_checkpoint(checkpoint_dir, checkpoint_prefix)\n'"
rl_coach/filters/reward/reward_rescale_filter.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom rl_coach.core_types import RewardType\nfrom rl_coach.filters.reward.reward_filter import RewardFilter\nfrom rl_coach.spaces import RewardSpace\n\n\nclass RewardRescaleFilter(RewardFilter):\n    """"""\n    Rescales the reward by a given factor. Rescaling the rewards of the environment has been\n    observed to have a large effect (negative or positive) on the behavior of the learning process.\n    """"""\n    def __init__(self, rescale_factor: float):\n        """"""\n        :param rescale_factor: The reward rescaling factor by which the reward will be multiplied\n        """"""\n        super().__init__()\n        self.rescale_factor = rescale_factor\n\n        if rescale_factor == 0:\n            raise ValueError(""The reward rescale value can not be set to 0"")\n\n    def filter(self, reward: RewardType, update_internal_state: bool=True) -> RewardType:\n        reward = float(reward) * self.rescale_factor\n        return reward\n\n    def get_filtered_reward_space(self, input_reward_space: RewardSpace) -> RewardSpace:\n        input_reward_space.high = input_reward_space.high * self.rescale_factor\n        input_reward_space.low = input_reward_space.low * self.rescale_factor\n        return input_reward_space\n'"
rl_coach/memories/backend/__init__.py,0,b''
rl_coach/memories/backend/memory.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nclass MemoryBackendParameters(object):\n\n    def __init__(self, store_type, orchestrator_type, run_type, deployed: str = False):\n        self.store_type = store_type\n        self.orchestrator_type = orchestrator_type\n        self.run_type = run_type\n        self.deployed = deployed\n\n\nclass MemoryBackend(object):\n\n    def __init__(self, params: MemoryBackendParameters):\n        pass\n\n    def deploy(self):\n        raise NotImplemented(""Not yet implemented"")\n\n    def get_endpoint(self):\n        raise NotImplemented(""Not yet implemented"")\n\n    def undeploy(self):\n        raise NotImplemented(""Not yet implemented"")\n\n    def sample(self, size: int):\n        raise NotImplemented(""Not yet implemented"")\n\n    def store(self, obj):\n        raise NotImplemented(""Not yet implemented"")\n\n    def store_episode(self, obj):\n        raise NotImplemented(""Not yet implemented"")\n\n    def fetch(self, num_steps=0):\n        raise NotImplemented(""Not yet implemented"")\n'"
rl_coach/memories/backend/memory_impl.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom rl_coach.memories.backend.memory import MemoryBackendParameters\nfrom rl_coach.memories.backend.redis import RedisPubSubBackend, RedisPubSubMemoryBackendParameters\n\n\ndef get_memory_backend(params: MemoryBackendParameters):\n\n    backend = None\n    if type(params) == RedisPubSubMemoryBackendParameters:\n        backend = RedisPubSubBackend(params)\n\n    return backend\n\n\ndef construct_memory_params(json: dict):\n\n    if json[\'store_type\'] == \'redispubsub\':\n        memory_params = RedisPubSubMemoryBackendParameters(\n            json[\'redis_address\'], json[\'redis_port\'], channel=json.get(\'channel\', \'\'), run_type=json[\'run_type\']\n        )\n        return memory_params\n'"
rl_coach/memories/backend/redis.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport redis\nimport pickle\nimport uuid\nimport time\n\nfrom rl_coach.memories.backend.memory import MemoryBackend, MemoryBackendParameters\nfrom rl_coach.core_types import Transition, Episode, EnvironmentSteps, EnvironmentEpisodes\n\n\nclass RedisPubSubMemoryBackendParameters(MemoryBackendParameters):\n\n    def __init__(self, redis_address: str="""", redis_port: int=6379, channel: str=""channel-{}"".format(uuid.uuid4()),\n                 orchestrator_params: dict=None, run_type=\'trainer\', orchestrator_type: str = ""kubernetes"", deployed: str = False):\n        self.redis_address = redis_address\n        self.redis_port = redis_port\n        self.channel = channel\n        if not orchestrator_params:\n            orchestrator_params = {}\n        self.orchestrator_params = orchestrator_params\n        self.run_type = run_type\n        self.store_type = ""redispubsub""\n        self.orchestrator_type = orchestrator_type\n        self.deployed = deployed\n\n\nclass RedisPubSubBackend(MemoryBackend):\n    """"""\n    A memory backend which transfers the experiences from the rollout to the training worker using Redis Pub/Sub in\n    Coach when distributed mode is used.\n    """"""\n\n    def __init__(self, params: RedisPubSubMemoryBackendParameters):\n        """"""\n        :param params: The Redis parameters to be used with this Redis Pub/Sub instance.\n        """"""\n        self.params = params\n        self.redis_connection = redis.Redis(self.params.redis_address, self.params.redis_port)\n        self.redis_server_name = \'redis-server-{}\'.format(uuid.uuid4())\n        self.redis_service_name = \'redis-service-{}\'.format(uuid.uuid4())\n\n    def store(self, obj):\n        """"""\n        :param obj: The object to store in memory. The object is either a Tranisition or Episode type.\n        """"""\n        self.redis_connection.publish(self.params.channel, pickle.dumps(obj))\n\n    def deploy(self):\n        """"""\n        Deploy the Redis Pub/Sub service in an orchestrator.\n        """"""\n        if not self.params.deployed:\n            if self.params.orchestrator_type == \'kubernetes\':\n                self.deploy_kubernetes()\n\n        # Wait till subscribe to the channel is possible or else it will cause delays in the trainer.\n        time.sleep(10)\n\n    def deploy_kubernetes(self):\n        """"""\n        Deploy the Redis Pub/Sub service in Kubernetes orchestrator.\n        """"""\n        if \'namespace\' not in self.params.orchestrator_params:\n            self.params.orchestrator_params[\'namespace\'] = ""default""\n        from kubernetes import client, config\n\n        container = client.V1Container(\n            name=self.redis_server_name,\n            image=\'redis:4-alpine\',\n            resources=client.V1ResourceRequirements(\n                limits={\n                    ""cpu"": ""8"",\n                    ""memory"": ""4Gi""\n                    # ""nvidia.com/gpu"": ""0"",\n                }\n            ),\n        )\n        template = client.V1PodTemplateSpec(\n            metadata=client.V1ObjectMeta(labels={\'app\': self.redis_server_name}),\n            spec=client.V1PodSpec(\n                containers=[container]\n            )\n        )\n        deployment_spec = client.V1DeploymentSpec(\n            replicas=1,\n            template=template,\n            selector=client.V1LabelSelector(\n                match_labels={\'app\': self.redis_server_name}\n            )\n        )\n\n        deployment = client.V1Deployment(\n            api_version=\'apps/v1\',\n            kind=\'Deployment\',\n            metadata=client.V1ObjectMeta(name=self.redis_server_name, labels={\'app\': self.redis_server_name}),\n            spec=deployment_spec\n        )\n\n        config.load_kube_config()\n        api_client = client.AppsV1Api()\n        try:\n            print(self.params.orchestrator_params)\n            api_client.create_namespaced_deployment(self.params.orchestrator_params[\'namespace\'], deployment)\n        except client.rest.ApiException as e:\n            print(""Got exception: %s\\n while creating redis-server"", e)\n            return False\n\n        core_v1_api = client.CoreV1Api()\n\n        service = client.V1Service(\n            api_version=\'v1\',\n            kind=\'Service\',\n            metadata=client.V1ObjectMeta(\n                name=self.redis_service_name\n            ),\n            spec=client.V1ServiceSpec(\n                selector={\'app\': self.redis_server_name},\n                ports=[client.V1ServicePort(\n                    protocol=\'TCP\',\n                    port=6379,\n                    target_port=6379\n                )]\n            )\n        )\n\n        try:\n            core_v1_api.create_namespaced_service(self.params.orchestrator_params[\'namespace\'], service)\n            self.params.redis_address = \'{}.{}.svc\'.format(\n                self.redis_service_name, self.params.orchestrator_params[\'namespace\']\n            )\n            self.params.redis_port = 6379\n            return True\n        except client.rest.ApiException as e:\n            print(""Got exception: %s\\n while creating a service for redis-server"", e)\n            return False\n\n    def undeploy(self):\n        """"""\n        Undeploy the Redis Pub/Sub service in an orchestrator.\n        """"""\n        from kubernetes import client\n        if self.params.deployed:\n            return\n\n        from kubernetes import client\n        api_client = client.AppsV1Api()\n        delete_options = client.V1DeleteOptions()\n        try:\n            api_client.delete_namespaced_deployment(self.redis_server_name, self.params.orchestrator_params[\'namespace\'], delete_options)\n        except client.rest.ApiException as e:\n            print(""Got exception: %s\\n while deleting redis-server"", e)\n\n        api_client = client.CoreV1Api()\n        try:\n            api_client.delete_namespaced_service(self.redis_service_name, self.params.orchestrator_params[\'namespace\'], delete_options)\n        except client.rest.ApiException as e:\n            print(""Got exception: %s\\n while deleting redis-server"", e)\n\n    def sample(self, size):\n        pass\n\n    def fetch(self, num_consecutive_playing_steps=None):\n        """"""\n        :param num_consecutive_playing_steps: The number steps to fetch.\n        """"""\n        return RedisSub(redis_address=self.params.redis_address, redis_port=self.params.redis_port, channel=self.params.channel).run(num_consecutive_playing_steps)\n\n    def subscribe(self, agent):\n        """"""\n        :param agent: The agent in use.\n        """"""\n        redis_sub = RedisSub(redis_address=self.params.redis_address, redis_port=self.params.redis_port, channel=self.params.channel)\n        return redis_sub\n\n    def get_endpoint(self):\n        return {\'redis_address\': self.params.redis_address,\n                \'redis_port\': self.params.redis_port}\n\n\nclass RedisSub(object):\n    def __init__(self, redis_address: str = ""localhost"", redis_port: int=6379, channel: str = ""PubsubChannel""):\n        super().__init__()\n        self.redis_connection = redis.Redis(redis_address, redis_port)\n        self.pubsub = self.redis_connection.pubsub()\n        self.subscriber = None\n        self.channel = channel\n        self.subscriber = self.pubsub.subscribe(self.channel)\n\n    def run(self, num_consecutive_playing_steps):\n        """"""\n        :param num_consecutive_playing_steps: The number steps to fetch.\n        """"""\n        transitions = 0\n        episodes = 0\n        steps = 0\n        for message in self.pubsub.listen():\n            if message and \'data\' in message:\n                try:\n                    obj = pickle.loads(message[\'data\'])\n                    if type(obj) == Transition:\n                        transitions += 1\n                        if obj.game_over:\n                            episodes += 1\n                        yield obj\n                    elif type(obj) == Episode:\n                        episodes += 1\n                        transitions += len(obj.transitions)\n                        yield from obj.transitions\n                except Exception:\n                    continue\n\n            if type(num_consecutive_playing_steps) == EnvironmentSteps:\n                steps = transitions\n            if type(num_consecutive_playing_steps) == EnvironmentEpisodes:\n                steps = episodes\n\n            if steps >= num_consecutive_playing_steps.num_steps:\n                break\n'"
rl_coach/memories/episodic/__init__.py,0,"b""from .episodic_experience_replay import EpisodicExperienceReplayParameters, EpisodicExperienceReplay\nfrom .episodic_hindsight_experience_replay import EpisodicHindsightExperienceReplayParameters, EpisodicHindsightExperienceReplay\nfrom .episodic_hrl_hindsight_experience_replay import EpisodicHRLHindsightExperienceReplayParameters, EpisodicHRLHindsightExperienceReplay\nfrom .single_episode_buffer import SingleEpisodeBufferParameters, SingleEpisodeBuffer\n__all__ = [\n    'EpisodicExperienceReplayParameters',\n    'EpisodicHindsightExperienceReplayParameters',\n    'EpisodicHRLHindsightExperienceReplayParameters',\n    'SingleEpisodeBufferParameters',\n    'EpisodicExperienceReplay',\n    'EpisodicHindsightExperienceReplay',\n    'EpisodicHRLHindsightExperienceReplay',\n    'SingleEpisodeBuffer'\n]\n"""
rl_coach/memories/episodic/episodic_experience_replay.py,0,"b'#\n#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport ast\n\nimport pickle\nfrom copy import deepcopy\n\nimport math\n\nimport pandas as pd\nfrom typing import List, Tuple, Union\nimport numpy as np\nimport random\n\nfrom rl_coach.core_types import Transition, Episode\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.memory import Memory, MemoryGranularity, MemoryParameters\nfrom rl_coach.utils import ReaderWriterLock, ProgressBar\nfrom rl_coach.core_types import CsvDataset\n\n\nclass EpisodicExperienceReplayParameters(MemoryParameters):\n    def __init__(self):\n        super().__init__()\n        self.max_size = (MemoryGranularity.Transitions, 1000000)\n        self.n_step = -1\n        self.train_to_eval_ratio = 1  # for OPE we\'ll want a value < 1\n\n    @property\n    def path(self):\n        return \'rl_coach.memories.episodic.episodic_experience_replay:EpisodicExperienceReplay\'\n\n\nclass EpisodicExperienceReplay(Memory):\n    """"""\n    A replay buffer that stores episodes of transitions. The additional structure allows performing various\n    calculations of total return and other values that depend on the sequential behavior of the transitions\n    in the episode.\n    """"""\n\n    def __init__(self, max_size: Tuple[MemoryGranularity, int] = (MemoryGranularity.Transitions, 1000000), n_step=-1,\n                 train_to_eval_ratio: int = 1):\n        """"""\n        :param max_size: the maximum number of transitions or episodes to hold in the memory\n        """"""\n        super().__init__(max_size)\n        self.n_step = n_step\n        self._buffer = [Episode(n_step=self.n_step)]  # list of episodes\n        self.transitions = []\n        self._length = 1  # the episodic replay buffer starts with a single empty episode\n        self._num_transitions = 0\n        self._num_transitions_in_complete_episodes = 0\n        self.reader_writer_lock = ReaderWriterLock()\n        self.last_training_set_episode_id = None  # used in batch-rl\n        self.last_training_set_transition_id = None  # used in batch-rl\n        self.train_to_eval_ratio = train_to_eval_ratio  # used in batch-rl\n        self.evaluation_dataset_as_episodes = None\n        self.evaluation_dataset_as_transitions = None\n\n        self.frozen = False\n\n    def length(self, lock: bool = False) -> int:\n        """"""\n        Get the number of episodes in the ER (even if they are not complete)\n        """"""\n        length = self._length\n        if self._length is not 0 and self._buffer[-1].is_empty():\n            length = self._length - 1\n\n        return length\n\n    def num_complete_episodes(self):\n        """""" Get the number of complete episodes in ER """"""\n        length = self._length - 1\n\n        return length\n\n    def num_transitions(self):\n        return self._num_transitions\n\n    def num_transitions_in_complete_episodes(self):\n        return self._num_transitions_in_complete_episodes\n\n    def get_last_training_set_episode_id(self):\n        return self.last_training_set_episode_id\n\n    def sample(self, size: int, is_consecutive_transitions=False) -> List[Transition]:\n        """"""\n        Sample a batch of transitions from the replay buffer. If the requested size is larger than the number\n        of samples available in the replay buffer then the batch will return empty.\n        :param size: the size of the batch to sample\n        :param is_consecutive_transitions: if set True, samples a batch of consecutive transitions.\n        :return: a batch (list) of selected transitions from the replay buffer\n        """"""\n        self.reader_writer_lock.lock_writing()\n\n        if self.num_complete_episodes() >= 1:\n            if is_consecutive_transitions:\n                episode_idx = np.random.randint(0, self.num_complete_episodes())\n                if self._buffer[episode_idx].length() <= size:\n                    batch = self._buffer[episode_idx].transitions\n                else:\n                    transition_idx = np.random.randint(size, self._buffer[episode_idx].length())\n                    batch = self._buffer[episode_idx].transitions[transition_idx - size:transition_idx]\n            else:\n                transitions_idx = np.random.randint(self.num_transitions_in_complete_episodes(), size=size)\n                batch = [self.transitions[i] for i in transitions_idx]\n\n        else:\n            raise ValueError(""The episodic replay buffer cannot be sampled since there are no complete episodes yet. ""\n                             ""There is currently 1 episodes with {} transitions"".format(self._buffer[0].length()))\n\n        self.reader_writer_lock.release_writing()\n\n        return batch\n\n    def get_episode_for_transition(self, transition: Transition) -> (int, Episode):\n        """"""\n        Get the episode from which that transition came from.\n        :param transition: The transition to lookup the episode for\n        :return: (Episode number, the episode) or (-1, None) if could not find a matching episode.\n        """"""\n\n        for i, episode in enumerate(self._buffer):\n            if transition in episode.transitions:\n                return i, episode\n        return -1, None\n\n    def shuffle_episodes(self):\n        """"""\n        Shuffle all the complete episodes in the replay buffer, while deleting the last non-complete episode\n        :return:\n        """"""\n        self.reader_writer_lock.lock_writing()\n\n        self.assert_not_frozen()\n\n        # unlike the standard usage of the EpisodicExperienceReplay, where we always leave an empty episode after\n        # the last full one, so that new transitions will have where to be added, in this case we delibrately remove\n        # that empty last episode, as we are about to shuffle the memory, and we don\'t want it to be shuffled in\n        self.remove_last_episode(lock=False)\n\n        random.shuffle(self._buffer)\n        self.transitions = [t for e in self._buffer for t in e.transitions]\n\n        # create a new Episode for the next transitions to be placed into\n        self._buffer.append(Episode(n_step=self.n_step))\n        self._length += 1\n\n        self.reader_writer_lock.release_writing()\n\n    def get_shuffled_training_data_generator(self, size: int) -> List[Transition]:\n        """"""\n        Get an generator for iterating through the shuffled replay buffer, for processing the data in epochs.\n        If the requested size is larger than the number of samples available in the replay buffer then the batch will\n        return empty. The last returned batch may be smaller than the size requested, to accommodate for all the\n        transitions in the replay buffer.\n\n        :param size: the size of the batch to return\n        :return: a batch (list) of selected transitions from the replay buffer\n        """"""\n        self.reader_writer_lock.lock_writing()\n\n        shuffled_transition_indices = list(range(self.last_training_set_transition_id))\n        random.shuffle(shuffled_transition_indices)\n\n        # The last batch drawn will usually be < batch_size (=the size variable)\n        for i in range(math.ceil(len(shuffled_transition_indices) / size)):\n            sample_data = [self.transitions[j] for j in shuffled_transition_indices[i * size: (i + 1) * size]]\n            self.reader_writer_lock.release_writing()\n\n            yield sample_data\n\n    def get_all_complete_episodes_transitions(self) -> List[Transition]:\n        """"""\n        Get all the transitions from all the complete episodes in the buffer\n        :return: a list of transitions\n        """"""\n        return self.transitions[:self.num_transitions_in_complete_episodes()]\n\n    def get_all_complete_episodes(self) -> List[Episode]:\n        """"""\n        Get all the transitions from all the complete episodes in the buffer\n        :return: a list of transitions\n        """"""\n        return self.get_all_complete_episodes_from_to(0, self.num_complete_episodes())\n\n    def get_all_complete_episodes_from_to(self, start_episode_id, end_episode_id) -> List[Episode]:\n        """"""\n        Get all the transitions from all the complete episodes in the buffer matching the given episode range\n        :return: a list of transitions\n        """"""\n        return self._buffer[start_episode_id:end_episode_id]\n\n    def _enforce_max_length(self) -> None:\n        """"""\n        Make sure that the size of the replay buffer does not pass the maximum size allowed.\n        If it passes the max size, the oldest episode in the replay buffer will be removed.\n        :return: None\n        """"""\n        granularity, size = self.max_size\n        if granularity == MemoryGranularity.Transitions:\n            while size != 0 and self.num_transitions() > size:\n                self.remove_first_episode(lock=False)\n        elif granularity == MemoryGranularity.Episodes:\n            while self.length() > size:\n                self.remove_first_episode(lock=False)\n\n    def _update_episode(self, episode: Episode) -> None:\n        episode.update_transitions_rewards_and_bootstrap_data()\n\n    def verify_last_episode_is_closed(self) -> None:\n        """"""\n        Verify that there is no open episodes in the replay buffer\n        :return: None\n        """"""\n        self.reader_writer_lock.lock_writing_and_reading()\n\n        last_episode = self.get(-1, False)\n        if last_episode and last_episode.length() > 0:\n            self.close_last_episode(lock=False)\n\n        self.reader_writer_lock.release_writing_and_reading()\n\n    def close_last_episode(self, lock=True) -> None:\n        """"""\n        Close the last episode in the replay buffer and open a new one\n        :return: None\n        """"""\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        last_episode = self._buffer[-1]\n\n        self._num_transitions_in_complete_episodes += last_episode.length()\n        self._length += 1\n\n        # create a new Episode for the next transitions to be placed into\n        self._buffer.append(Episode(n_step=self.n_step))\n\n        # if update episode adds to the buffer, a new Episode needs to be ready first\n        # it would be better if this were less state full\n        self._update_episode(last_episode)\n\n        self._enforce_max_length()\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    def store(self, transition: Transition) -> None:\n        """"""\n        Store a new transition in the memory. If the transition game_over flag is on, this closes the episode and\n        creates a new empty episode.\n        Warning! using the episodic memory by storing individual transitions instead of episodes will use the default\n        Episode class parameters in order to create new episodes.\n        :param transition: a transition to store\n        :return: None\n        """"""\n        self.assert_not_frozen()\n\n        # Calling super.store() so that in case a memory backend is used, the memory backend can store this transition.\n        super().store(transition)\n\n        self.reader_writer_lock.lock_writing_and_reading()\n\n        if len(self._buffer) == 0:\n            self._buffer.append(Episode(n_step=self.n_step))\n        last_episode = self._buffer[-1]\n        last_episode.insert(transition)\n        self.transitions.append(transition)\n        self._num_transitions += 1\n        if transition.game_over:\n            self.close_last_episode(False)\n\n        self._enforce_max_length()\n\n        self.reader_writer_lock.release_writing_and_reading()\n\n    def store_episode(self, episode: Episode, lock: bool = True) -> None:\n        """"""\n        Store a new episode in the memory.\n        :param episode: the new episode to store\n        :return: None\n        """"""\n        self.assert_not_frozen()\n\n        # Calling super.store() so that in case a memory backend is used, the memory backend can store this episode.\n        super().store_episode(episode)\n\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        if self._buffer[-1].length() == 0:\n            self._buffer[-1] = episode\n        else:\n            self._buffer.append(episode)\n        self.transitions.extend(episode.transitions)\n        self._num_transitions += episode.length()\n        self.close_last_episode(False)\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    def get_episode(self, episode_index: int, lock: bool = True) -> Union[None, Episode]:\n        """"""\n        Returns the episode in the given index. If the episode does not exist, returns None instead.\n        :param episode_index: the index of the episode to return\n        :return: the corresponding episode\n        """"""\n        if lock:\n            self.reader_writer_lock.lock_writing()\n\n        if self.length() == 0 or episode_index >= self.length():\n            episode = None\n        else:\n            episode = self._buffer[episode_index]\n\n        if lock:\n            self.reader_writer_lock.release_writing()\n        return episode\n\n    def _remove_episode(self, episode_index: int) -> None:\n        """"""\n        Remove either the first or the last index\n        :param episode_index: the index of the episode to remove (either 0 or -1)\n        :return: None\n        """"""\n        self.assert_not_frozen()\n        assert episode_index == 0 or episode_index == -1, ""_remove_episode only supports removing the first or the last "" \\\n                                                          ""episode""\n\n        if len(self._buffer) > 0:\n            episode_length = self._buffer[episode_index].length()\n            self._length -= 1\n            self._num_transitions -= episode_length\n            self._num_transitions_in_complete_episodes -= episode_length\n            if episode_index == 0:\n                del self.transitions[:episode_length]\n            else:  # episode_index = -1\n                del self.transitions[-episode_length:]\n            del self._buffer[episode_index]\n\n    def remove_first_episode(self, lock: bool = True) -> None:\n        """"""\n        Remove the first episode (even if it is not complete yet)\n        :param lock: if true, will lock the readers writers lock. this can cause a deadlock if an inheriting class\n                     locks and then calls store with lock = True\n        :return: None\n        """"""\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        self._remove_episode(0)\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    def remove_last_episode(self, lock: bool = True) -> None:\n        """"""\n        Remove the last episode (even if it is not complete yet)\n        :param lock: if true, will lock the readers writers lock. this can cause a deadlock if an inheriting class\n                     locks and then calls store with lock = True\n        :return: None\n        """"""\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        self._remove_episode(-1)\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    # for API compatibility\n    def get(self, episode_index: int, lock: bool = True) -> Union[None, Episode]:\n        """"""\n        Returns the episode in the given index. If the episode does not exist, returns None instead.\n        :param episode_index: the index of the episode to return\n        :return: the corresponding episode\n        """"""\n        return self.get_episode(episode_index, lock)\n\n    def get_last_complete_episode(self) -> Union[None, Episode]:\n        """"""\n        Returns the last complete episode in the memory or None if there are no complete episodes\n        :return: None or the last complete episode\n        """"""\n        self.reader_writer_lock.lock_writing()\n\n        last_complete_episode_index = self.num_complete_episodes() - 1\n        episode = None\n        if last_complete_episode_index >= 0:\n            episode = self.get(last_complete_episode_index)\n\n        self.reader_writer_lock.release_writing()\n\n        return episode\n\n    def clean(self) -> None:\n        """"""\n        Clean the memory by removing all the episodes\n        :return: None\n        """"""\n        self.assert_not_frozen()\n        self.reader_writer_lock.lock_writing_and_reading()\n\n        self.transitions = []\n        self._buffer = [Episode(n_step=self.n_step)]\n        self._length = 1\n        self._num_transitions = 0\n        self._num_transitions_in_complete_episodes = 0\n\n        self.reader_writer_lock.release_writing_and_reading()\n\n    def mean_reward(self) -> np.ndarray:\n        """"""\n        Get the mean reward in the replay buffer\n        :return: the mean reward\n        """"""\n        self.reader_writer_lock.lock_writing()\n\n        mean = np.mean([transition.reward for transition in self.transitions])\n\n        self.reader_writer_lock.release_writing()\n        return mean\n\n    def load_csv(self, csv_dataset: CsvDataset, input_filter: InputFilter) -> None:\n        """"""\n        Restore the replay buffer contents from a csv file.\n        The csv file is assumed to include a list of transitions.\n        :param csv_dataset: A construct which holds the dataset parameters\n        :param input_filter: A filter used to filter the CSV data before feeding it to the memory.\n        """"""\n        self.assert_not_frozen()\n\n        df = pd.read_csv(csv_dataset.filepath)\n        if len(df) > self.max_size[1]:\n            screen.warning(""Warning! The number of transitions to load into the replay buffer ({}) is ""\n                           ""bigger than the max size of the replay buffer ({}). The excessive transitions will ""\n                           ""not be stored."".format(len(df), self.max_size[1]))\n\n        episode_ids = df[\'episode_id\'].unique()\n        progress_bar = ProgressBar(len(episode_ids))\n        state_columns = [col for col in df.columns if col.startswith(\'state_feature\')]\n\n        for e_id in episode_ids:\n            progress_bar.update(e_id)\n            df_episode_transitions = df[df[\'episode_id\'] == e_id]\n            input_filter.reset()\n\n            if len(df_episode_transitions) < 2:\n                # we have to have at least 2 rows in each episode for creating a transition\n                continue\n\n            episode = Episode()\n            transitions = []\n            for (_, current_transition), (_, next_transition) in zip(df_episode_transitions[:-1].iterrows(),\n                                                                     df_episode_transitions[1:].iterrows()):\n                state = np.array([current_transition[col] for col in state_columns])\n                next_state = np.array([next_transition[col] for col in state_columns])\n\n                transitions.append(\n                    Transition(state={\'observation\': state},\n                               action=int(current_transition[\'action\']), reward=current_transition[\'reward\'],\n                               next_state={\'observation\': next_state}, game_over=False,\n                               info={\'all_action_probabilities\':\n                                         ast.literal_eval(current_transition[\'all_action_probabilities\'])}),\n                    )\n\n            transitions = input_filter.filter(transitions, deep_copy=False)\n            for t in transitions:\n                episode.insert(t)\n\n            # Set the last transition to end the episode\n            if csv_dataset.is_episodic:\n                episode.get_last_transition().game_over = True\n\n            self.store_episode(episode)\n\n        # close the progress bar\n        progress_bar.update(len(episode_ids))\n        progress_bar.close()\n\n    def freeze(self):\n        """"""\n        Freezing the replay buffer does not allow any new transitions to be added to the memory.\n        Useful when working with a dataset (e.g. batch-rl or imitation learning).\n        :return: None\n        """"""\n        self.frozen = True\n\n    def assert_not_frozen(self):\n        """"""\n        Check that the memory is not frozen, and can be changed.\n        :return:\n        """"""\n        assert self.frozen is False, ""Memory is frozen, and cannot be changed.""\n\n    def prepare_evaluation_dataset(self):\n        """"""\n        Gather the memory content that will be used for off-policy evaluation in episodes and transitions format\n        :return:\n        """"""\n        self.reader_writer_lock.lock_writing_and_reading()\n\n        self._split_training_and_evaluation_datasets()\n        self.evaluation_dataset_as_episodes = deepcopy(\n                self.get_all_complete_episodes_from_to(self.get_last_training_set_episode_id() + 1,\n                                                       self.num_complete_episodes()))\n\n        if len(self.evaluation_dataset_as_episodes) == 0:\n            raise ValueError(\'train_to_eval_ratio is too high causing the evaluation set to be empty. \'\n                             \'Consider decreasing its value.\')\n\n        self.evaluation_dataset_as_transitions = [t for e in self.evaluation_dataset_as_episodes\n                                                  for t in e.transitions]\n        self.reader_writer_lock.release_writing_and_reading()\n\n    def _split_training_and_evaluation_datasets(self):\n        """"""\n        If the data in the buffer was not split to training and evaluation yet, split it accordingly.\n        :return: None\n        """"""\n\n        if self.last_training_set_transition_id is None:\n            if self.train_to_eval_ratio < 0 or self.train_to_eval_ratio >= 1:\n                raise ValueError(\'train_to_eval_ratio should be in the (0, 1] range.\')\n\n            transition = self.transitions[round(self.train_to_eval_ratio * self.num_transitions_in_complete_episodes())]\n            episode_num, episode = self.get_episode_for_transition(transition)\n            self.last_training_set_episode_id = episode_num\n            self.last_training_set_transition_id = \\\n                len([t for e in self.get_all_complete_episodes_from_to(0, self.last_training_set_episode_id + 1) for t in e])\n\n    def save(self, file_path: str) -> None:\n        """"""\n        Save the replay buffer contents to a pickle file\n        :param file_path: the path to the file that will be used to store the pickled transitions\n        """"""\n        with open(file_path, \'wb\') as file:\n            pickle.dump(self.get_all_complete_episodes(), file)\n\n    def load_pickled(self, file_path: str) -> None:\n        """"""\n        Restore the replay buffer contents from a pickle file.\n        The pickle file is assumed to include a list of transitions.\n        :param file_path: The path to a pickle file to restore\n        """"""\n        self.assert_not_frozen()\n\n        with open(file_path, \'rb\') as file:\n            episodes = pickle.load(file)\n            num_transitions = sum([len(e.transitions) for e in episodes])\n            if num_transitions > self.max_size[1]:\n                screen.warning(""Warning! The number of transition to load into the replay buffer ({}) is ""\n                               ""bigger than the max size of the replay buffer ({}). The excessive transitions will ""\n                               ""not be stored."".format(num_transitions, self.max_size[1]))\n\n            progress_bar = ProgressBar(len(episodes))\n            for episode_idx, episode in enumerate(episodes):\n                self.store_episode(episode)\n\n                # print progress\n                progress_bar.update(episode_idx)\n\n            progress_bar.close()\n'"
rl_coach/memories/episodic/episodic_hindsight_experience_replay.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport copy\nfrom enum import Enum\nfrom typing import Tuple, List\n\nimport numpy as np\n\nfrom rl_coach.core_types import Episode, Transition\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplayParameters, \\\n    EpisodicExperienceReplay\nfrom rl_coach.memories.non_episodic.experience_replay import MemoryGranularity\nfrom rl_coach.spaces import GoalsSpace\n\n\nclass HindsightGoalSelectionMethod(Enum):\n    Future = 0\n    Final = 1\n    Episode = 2\n    Random = 3\n\n\nclass EpisodicHindsightExperienceReplayParameters(EpisodicExperienceReplayParameters):\n    def __init__(self):\n        super().__init__()\n        self.hindsight_transitions_per_regular_transition = None\n        self.hindsight_goal_selection_method = None\n        self.goals_space = None\n\n    @property\n    def path(self):\n        return \'rl_coach.memories.episodic.episodic_hindsight_experience_replay:EpisodicHindsightExperienceReplay\'\n\n\nclass EpisodicHindsightExperienceReplay(EpisodicExperienceReplay):\n    """"""\n    Implements Hindsight Experience Replay as described in the following paper: https://arxiv.org/pdf/1707.01495.pdf\n\n    """"""\n    def __init__(self, max_size: Tuple[MemoryGranularity, int],\n                 hindsight_transitions_per_regular_transition: int,\n                 hindsight_goal_selection_method: HindsightGoalSelectionMethod,\n                 goals_space: GoalsSpace):\n        """"""\n        :param max_size: The maximum size of the memory. should be defined in a granularity of Transitions\n        :param hindsight_transitions_per_regular_transition: The number of hindsight artificial transitions to generate\n                                                             for each actual transition\n        :param hindsight_goal_selection_method: The method that will be used for generating the goals for the\n                                                hindsight transitions. Should be one of HindsightGoalSelectionMethod\n        :param goals_space: A GoalsSpace which defines the base properties of the goals space\n        """"""\n        super().__init__(max_size)\n\n        self.hindsight_transitions_per_regular_transition = hindsight_transitions_per_regular_transition\n        self.hindsight_goal_selection_method = hindsight_goal_selection_method\n        self.goals_space = goals_space\n        self.last_episode_start_idx = 0\n\n    def _sample_goal(self, episode_transitions: List, transition_index: int):\n        """"""\n        Sample a single goal state according to the sampling method\n        :param episode_transitions: a list of all the transitions in the current episode\n        :param transition_index: the transition to start sampling from\n        :return: a goal corresponding to the sampled state\n        """"""\n        if self.hindsight_goal_selection_method == HindsightGoalSelectionMethod.Future:\n            # states that were observed in the same episode after the transition that is being replayed\n            selected_transition = np.random.choice(episode_transitions[transition_index+1:])\n        elif self.hindsight_goal_selection_method == HindsightGoalSelectionMethod.Final:\n            # the final state in the episode\n            selected_transition = episode_transitions[-1]\n        elif self.hindsight_goal_selection_method == HindsightGoalSelectionMethod.Episode:\n            # a random state from the episode\n            selected_transition = np.random.choice(episode_transitions)\n        elif self.hindsight_goal_selection_method == HindsightGoalSelectionMethod.Random:\n            # a random state from the entire replay buffer\n            selected_transition = np.random.choice(self.transitions)\n        else:\n            raise ValueError(""Invalid goal selection method was used for the hindsight goal selection"")\n        return self.goals_space.goal_from_state(selected_transition.state)\n\n    def _sample_goals(self, episode_transitions: List, transition_index: int):\n        """"""\n        Sample a batch of goal states according to the sampling method\n        :param episode_transitions: a list of all the transitions in the current episode\n        :param transition_index: the transition to start sampling from\n        :return: a goal corresponding to the sampled state\n        """"""\n        return [\n            self._sample_goal(episode_transitions, transition_index)\n            for _ in range(self.hindsight_transitions_per_regular_transition)\n        ]\n\n    def store_episode(self, episode: Episode, lock: bool=True) -> None:\n        # generate hindsight transitions only when an episode is finished\n        last_episode_transitions = copy.copy(episode.transitions)\n\n        # cannot create a future hindsight goal in the last transition of an episode\n        if self.hindsight_goal_selection_method == HindsightGoalSelectionMethod.Future:\n            relevant_base_transitions = last_episode_transitions[:-1]\n        else:\n            relevant_base_transitions = last_episode_transitions\n\n        # for each transition in the last episode, create a set of hindsight transitions\n        for transition_index, transition in enumerate(relevant_base_transitions):\n            sampled_goals = self._sample_goals(last_episode_transitions, transition_index)\n            for goal in sampled_goals:\n                hindsight_transition = copy.copy(transition)\n\n                if hindsight_transition.state[\'desired_goal\'].shape != goal.shape:\n                    raise ValueError((\n                        \'goal shape {goal_shape} already in transition is \'\n                        \'different than the one sampled as a hindsight goal \'\n                        \'{hindsight_goal_shape}.\'\n                    ).format(\n                        goal_shape=hindsight_transition.state[\'desired_goal\'].shape,\n                        hindsight_goal_shape=goal.shape,\n                    ))\n\n                # update the goal in the transition\n                hindsight_transition.state[\'desired_goal\'] = goal\n                hindsight_transition.next_state[\'desired_goal\'] = goal\n\n                # update the reward and terminal signal according to the goal\n                hindsight_transition.reward, hindsight_transition.game_over = \\\n                    self.goals_space.get_reward_for_goal_and_state(goal, hindsight_transition.next_state)\n\n                hindsight_transition.n_step_discounted_rewards = None\n                episode.insert(hindsight_transition)\n\n        super().store_episode(episode)\n\n    def store(self, transition: Transition):\n        raise ValueError(""An episodic HER cannot store a single transition. Only full episodes are to be stored."")\n'"
rl_coach/memories/episodic/episodic_hrl_hindsight_experience_replay.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Tuple\n\nfrom rl_coach.core_types import Episode, Transition\nfrom rl_coach.memories.episodic.episodic_hindsight_experience_replay import HindsightGoalSelectionMethod, \\\n    EpisodicHindsightExperienceReplay, EpisodicHindsightExperienceReplayParameters\nfrom rl_coach.memories.non_episodic.experience_replay import MemoryGranularity\nfrom rl_coach.spaces import GoalsSpace\n\n\nclass EpisodicHRLHindsightExperienceReplayParameters(EpisodicHindsightExperienceReplayParameters):\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def path(self):\n        return \'rl_coach.memories.episodic.episodic_hrl_hindsight_experience_replay:EpisodicHRLHindsightExperienceReplay\'\n\n\nclass EpisodicHRLHindsightExperienceReplay(EpisodicHindsightExperienceReplay):\n    """"""\n    Implements HRL Hindsight Experience Replay as described in the following paper:  https://arxiv.org/abs/1805.08180\n\n    This is the memory you should use if you want a shared hindsight experience replay buffer between multiple workers\n    """"""\n    def __init__(self, max_size: Tuple[MemoryGranularity, int],\n                 hindsight_transitions_per_regular_transition: int,\n                 hindsight_goal_selection_method: HindsightGoalSelectionMethod,\n                 goals_space: GoalsSpace,\n                 ):\n        """"""\n        :param max_size: The maximum size of the memory. should be defined in a granularity of Transitions\n        :param hindsight_transitions_per_regular_transition: The number of hindsight artificial transitions to generate\n                                                             for each actual transition\n        :param hindsight_goal_selection_method: The method that will be used for generating the goals for the\n                                                hindsight transitions. Should be one of HindsightGoalSelectionMethod\n        :param goals_space: A GoalsSpace  which defines the properties of the goals\n        :param do_action_hindsight: Replace the action (sub-goal) given to a lower layer, with the actual achieved goal\n        """"""\n        super().__init__(max_size, hindsight_transitions_per_regular_transition, hindsight_goal_selection_method,\n                         goals_space)\n\n    def store_episode(self, episode: Episode, lock: bool=True) -> None:\n        # for a layer producing sub-goals, we will replace in hindsight the action (sub-goal) given to the lower\n        # level with the actual achieved goal. the achieved goal (and observation) seen is assumed to be the same\n        # for all levels - we can use this level\'s achieved goal instead of the lower level\'s one\n\n        # Calling super.store() so that in case a memory backend is used, the memory backend can store this episode.\n        super().store_episode(episode)\n\n        for transition in episode.transitions:\n            new_achieved_goal = transition.next_state[self.goals_space.goal_name]\n            transition.action = new_achieved_goal\n\n        super().store_episode(episode)\n\n    def store(self, transition: Transition):\n        raise ValueError(""An episodic HER cannot store a single transition. Only full episodes are to be stored."")\n'"
rl_coach/memories/episodic/single_episode_buffer.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom rl_coach.memories.episodic.episodic_experience_replay import EpisodicExperienceReplay\nfrom rl_coach.memories.memory import MemoryGranularity, MemoryParameters\n\n\nclass SingleEpisodeBufferParameters(MemoryParameters):\n    def __init__(self):\n        super().__init__()\n        del self.max_size\n\n    @property\n    def path(self):\n        return \'rl_coach.memories.episodic.single_episode_buffer:SingleEpisodeBuffer\'\n\n\nclass SingleEpisodeBuffer(EpisodicExperienceReplay):\n    def __init__(self):\n        super().__init__((MemoryGranularity.Episodes, 1))\n'"
rl_coach/memories/non_episodic/__init__.py,0,"b""from .balanced_experience_replay import BalancedExperienceReplayParameters, BalancedExperienceReplay\nfrom .differentiable_neural_dictionary import QDND\nfrom .experience_replay import ExperienceReplayParameters, ExperienceReplay\nfrom .prioritized_experience_replay import PrioritizedExperienceReplayParameters, PrioritizedExperienceReplay\nfrom .transition_collection import TransitionCollection\n__all__ = [\n    'BalancedExperienceReplayParameters',\n    'BalancedExperienceReplay',\n    'QDND',\n    'ExperienceReplay',\n    'PrioritizedExperienceReplay',\n    'TransitionCollection'\n]\n"""
rl_coach/memories/non_episodic/balanced_experience_replay.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport operator\nimport random\nfrom enum import Enum\nfrom typing import List, Tuple, Any, Union\n\nimport numpy as np\n\nfrom rl_coach.core_types import Transition\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters, ExperienceReplay\nfrom rl_coach.schedules import Schedule, ConstantSchedule\n\n\nclass BalancedExperienceReplayParameters(ExperienceReplayParameters):\n    def __init__(self):\n        super().__init__()\n        self.max_size = (MemoryGranularity.Transitions, 1000000)\n        self.allow_duplicates_in_batch_sampling = False\n        self.num_classes = 0\n        self.state_key_with_the_class_index = \'class\'\n\n    @property\n    def path(self):\n        return \'rl_coach.memories.non_episodic.balanced_experience_replay:BalancedExperienceReplay\'\n\n\n""""""\nA replay buffer which allows sampling batches which are balanced in terms of the classes that are sampled\n""""""\nclass BalancedExperienceReplay(ExperienceReplay):\n    def __init__(self, max_size: Tuple[MemoryGranularity, int], allow_duplicates_in_batch_sampling: bool=True,\n                 num_classes: int=0, state_key_with_the_class_index: Any=\'class\'):\n        """"""\n        :param max_size: the maximum number of transitions or episodes to hold in the memory\n        :param allow_duplicates_in_batch_sampling: allow having the same transition multiple times in a batch\n        :param num_classes: the number of classes in the replayed data\n        :param state_key_with_the_class_index: the class index is assumed to be a value in the state dictionary.\n                                           this parameter determines the key to retrieve the class index value\n        """"""\n        super().__init__(max_size, allow_duplicates_in_batch_sampling)\n        self.current_class_to_sample_from = 0\n        self.num_classes = num_classes\n        self.state_key_with_the_class_index = state_key_with_the_class_index\n        self.transitions = [[] for _ in range(self.num_classes)]\n        self.transitions_order = []\n\n        if self.num_classes < 2:\n            raise ValueError(""The number of classes for a balanced replay buffer should be at least 2. ""\n                             ""The number of classes that were defined are: {}"".format(self.num_classes))\n\n    def store(self, transition: Transition, lock: bool=True) -> None:\n        """"""\n        Store a new transition in the memory.\n        :param transition: a transition to store\n        :param lock: if true, will lock the readers writers lock. this can cause a deadlock if an inheriting class\n                     locks and then calls store with lock = True\n        :return: None\n        """"""\n        # Calling super.store() so that in case a memory backend is used, the memory backend can store this transition.\n        super().store(transition)\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        self._num_transitions += 1\n\n        if self.state_key_with_the_class_index not in transition.state.keys():\n            raise ValueError(""The class index was not present in the state of the transition under the given key ({})""\n                             .format(self.state_key_with_the_class_index))\n\n        class_idx = transition.state[self.state_key_with_the_class_index]\n\n        if class_idx >= self.num_classes:\n            raise ValueError(""The given class index is outside the defined number of classes for the replay buffer. ""\n                             ""The given class was: {} and the number of classes defined is: {}""\n                             .format(class_idx, self.num_classes))\n\n        self.transitions[class_idx].append(transition)\n        self.transitions_order.append(class_idx)\n        self._enforce_max_length()\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    def sample(self, size: int) -> List[Transition]:\n        """"""\n        Sample a batch of transitions form the replay buffer. If the requested size is larger than the number\n        of samples available in the replay buffer then the batch will return empty.\n        :param size: the size of the batch to sample\n        :return: a batch (list) of selected transitions from the replay buffer\n        """"""\n        self.reader_writer_lock.lock_writing()\n\n        if size % self.num_classes != 0:\n            raise ValueError(""Sampling batches from a balanced replay buffer should be done only using batch sizes ""\n                             ""which are a multiple of the number of classes. The number of classes defined is: {} ""\n                             ""and the batch size requested is: {}"".format(self.num_classes, size))\n\n        batch_size_from_each_class = size // self.num_classes\n\n        if self.allow_duplicates_in_batch_sampling:\n            transitions_idx = [np.random.randint(len(class_transitions), size=batch_size_from_each_class)\n                               for class_transitions in self.transitions]\n\n        else:\n            for class_idx, class_transitions in enumerate(self.transitions):\n                if self.num_transitions() < batch_size_from_each_class:\n                    raise ValueError(""The replay buffer cannot be sampled since there are not enough transitions yet. ""\n                                     ""There are currently {} transitions for class {}""\n                                     .format(len(class_transitions), class_idx))\n\n            transitions_idx = [np.random.choice(len(class_transitions), size=batch_size_from_each_class, replace=False)\n                               for class_transitions in self.transitions]\n\n        batch = []\n        for class_idx, class_transitions_idx in enumerate(transitions_idx):\n            batch += [self.transitions[class_idx][i] for i in class_transitions_idx]\n\n        self.reader_writer_lock.release_writing()\n\n        return batch\n\n    def remove_transition(self, transition_index: int, lock: bool=True) -> None:\n        raise ValueError(""It is not possible to remove specific transitions with a balanced replay buffer"")\n\n    def get_transition(self, transition_index: int, lock: bool=True) -> Union[None, Transition]:\n        raise ValueError(""It is not possible to access specific transitions with a balanced replay buffer"")\n\n    def _enforce_max_length(self) -> None:\n        """"""\n        Make sure that the size of the replay buffer does not pass the maximum size allowed.\n        If it passes the max size, the oldest transition in the replay buffer will be removed.\n        This function does not use locks since it is only called internally\n        :return: None\n        """"""\n        granularity, size = self.max_size\n        if granularity == MemoryGranularity.Transitions:\n            while size != 0 and self.num_transitions() > size:\n                self._num_transitions -= 1\n                del self.transitions[self.transitions_order[0]][0]\n                del self.transitions_order[0]\n        else:\n            raise ValueError(""The granularity of the replay buffer can only be set in terms of transitions"")\n\n    def clean(self, lock: bool=True) -> None:\n        """"""\n        Clean the memory by removing all the episodes\n        :return: None\n        """"""\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        self.transitions = [[] for _ in range(self.num_classes)]\n        self.transitions_order = []\n        self._num_transitions = 0\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n'"
rl_coach/memories/non_episodic/differentiable_neural_dictionary.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation \n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport pickle\n\nimport numpy as np\ntry:\n    import annoy\n    from annoy import AnnoyIndex\nexcept ImportError:\n    from rl_coach.logger import failed_imports\n    failed_imports.append(""annoy"")\n\n\nclass AnnoyDictionary(object):\n    def __init__(self, dict_size, key_width, new_value_shift_coefficient=0.1, batch_size=100, key_error_threshold=0.01,\n                 num_neighbors=50, override_existing_keys=True, rebuild_on_every_update=False):\n        self.rebuild_on_every_update = rebuild_on_every_update\n        self.max_size = dict_size\n        self.curr_size = 0\n        self.new_value_shift_coefficient = new_value_shift_coefficient\n        self.num_neighbors = num_neighbors\n        self.override_existing_keys = override_existing_keys\n\n        self.index = AnnoyIndex(key_width, metric=\'euclidean\')\n        self.index.set_seed(1)\n\n        self.embeddings = np.zeros((dict_size, key_width))\n        self.values = np.zeros(dict_size)\n        self.additional_data = [None] * dict_size\n\n        self.lru_timestamps = np.zeros(dict_size)\n        self.current_timestamp = 0.0\n\n        # keys that are in this distance will be considered as the same key\n        self.key_error_threshold = key_error_threshold\n\n        self.initial_update_size = batch_size\n        self.min_update_size = self.initial_update_size\n        self.key_dimension = key_width\n        self.value_dimension = 1\n        self._reset_buffer()\n\n        self.built_capacity = 0\n\n    def add(self, keys, values, additional_data=None, force_rebuild_tree=False):\n        if not additional_data:\n            additional_data = [None] * len(keys)\n\n        # Adds new embeddings and values to the dictionary\n        indices = []\n        indices_to_remove = []\n        for i in range(keys.shape[0]):\n            index = self._lookup_key_index(keys[i])\n            if index and self.override_existing_keys:\n                # update existing value\n                self.values[index] += self.new_value_shift_coefficient * (values[i] - self.values[index])\n                self.additional_data[index[0][0]] = additional_data[i]\n                self.lru_timestamps[index] = self.current_timestamp\n                indices_to_remove.append(i)\n            else:\n                # add new\n                if self.curr_size >= self.max_size:\n                    # find the LRU entry\n                    index = np.argmin(self.lru_timestamps)\n                else:\n                    index = self.curr_size\n                    self.curr_size += 1\n                self.lru_timestamps[index] = self.current_timestamp\n                indices.append(index)\n\n        for i in reversed(indices_to_remove):\n            keys = np.delete(keys, i, 0)\n            values = np.delete(values, i, 0)\n            del additional_data[i]\n\n        self.buffered_keys = np.vstack((self.buffered_keys, keys))\n        self.buffered_values = np.vstack((self.buffered_values, values))\n        self.buffered_indices = self.buffered_indices + indices\n        self.buffered_additional_data = self.buffered_additional_data + additional_data\n\n        if len(self.buffered_indices) >= self.min_update_size:\n            self.min_update_size = max(self.initial_update_size, int(self.curr_size * 0.02))\n            self._rebuild_index()\n        elif force_rebuild_tree or self.rebuild_on_every_update:\n            self._rebuild_index()\n\n        self.current_timestamp += 1\n\n    # Returns the stored embeddings and values of the closest embeddings\n    def query(self, keys, k):\n        if not self.has_enough_entries(k):\n            # this will only happen when the DND is not yet populated with enough entries, which is only during heatup\n            # these values won\'t be used and therefore they are meaningless\n            return [0.0], [0.0], [0], [None]\n\n        _, indices = self._get_k_nearest_neighbors_indices(keys, k)\n\n        embeddings = []\n        values = []\n        additional_data = []\n        for ind in indices:\n            self.lru_timestamps[ind] = self.current_timestamp\n            embeddings.append(self.embeddings[ind])\n            values.append(self.values[ind])\n            curr_additional_data = []\n            for sub_ind in ind:\n                curr_additional_data.append(self.additional_data[sub_ind])\n            additional_data.append(curr_additional_data)\n\n        self.current_timestamp += 1\n\n        return embeddings, values, indices, additional_data\n\n    def has_enough_entries(self, k):\n        return self.curr_size > k and (self.built_capacity > k)\n\n    def sample_embeddings(self, num_embeddings):\n        return self.embeddings[np.random.choice(self.curr_size, num_embeddings)]\n\n    def _get_k_nearest_neighbors_indices(self, keys, k):\n        distances = []\n        indices = []\n        for key in keys:\n            index, distance = self.index.get_nns_by_vector(key, k, include_distances=True)\n            distances.append(distance)\n            indices.append(index)\n        return distances, indices\n\n    def _rebuild_index(self):\n        self.index.unbuild()\n        self.embeddings[self.buffered_indices] = self.buffered_keys\n        self.values[self.buffered_indices] = np.squeeze(self.buffered_values)\n        for i, data in zip(self.buffered_indices, self.buffered_additional_data):\n            self.additional_data[i] = data\n        for idx, key in zip(self.buffered_indices, self.buffered_keys):\n            self.index.add_item(idx, key)\n\n        self._reset_buffer()\n\n        self.index.build(self.num_neighbors)\n        self.built_capacity = self.curr_size\n\n    def _reset_buffer(self):\n        self.buffered_keys = np.zeros((0, self.key_dimension))\n        self.buffered_values = np.zeros((0, self.value_dimension))\n        self.buffered_indices = []\n        self.buffered_additional_data = []\n\n    def _lookup_key_index(self, key):\n        distance, index = self._get_k_nearest_neighbors_indices([key], 1)\n        if distance != [[]] and distance[0][0] <= self.key_error_threshold:\n            return index\n        return None\n\n\nclass QDND(object):\n    def __init__(self, dict_size, key_width, num_actions, new_value_shift_coefficient=0.1, key_error_threshold=0.01,\n                 learning_rate=0.01, num_neighbors=50, return_additional_data=False, override_existing_keys=False,\n                 rebuild_on_every_update=False):\n        self.dict_size = dict_size\n        self.key_width = key_width\n        self.num_actions = num_actions\n        self.new_value_shift_coefficient = new_value_shift_coefficient\n        self.key_error_threshold = key_error_threshold\n        self.learning_rate = learning_rate\n        self.num_neighbors = num_neighbors\n        self.return_additional_data = return_additional_data\n        self.override_existing_keys = override_existing_keys\n        self.dicts = []\n\n        # create a dict for each action\n        for a in range(num_actions):\n            new_dict = AnnoyDictionary(dict_size, key_width, new_value_shift_coefficient,\n                                       key_error_threshold=key_error_threshold, num_neighbors=num_neighbors,\n                                       override_existing_keys=override_existing_keys,\n                                       rebuild_on_every_update=rebuild_on_every_update)\n            self.dicts.append(new_dict)\n\n    def add(self, embeddings, actions, values, additional_data=None):\n        # add a new set of embeddings and values to each of the underlining dictionaries\n        embeddings = np.array(embeddings)\n        actions = np.array(actions)\n        values = np.array(values)\n        for a in range(self.num_actions):\n            idx = np.where(actions == a)\n            curr_action_embeddings = embeddings[idx]\n            curr_action_values = np.expand_dims(values[idx], -1)\n            if additional_data:\n                curr_additional_data = []\n                for i in idx[0]:\n                    curr_additional_data.append(additional_data[i])\n            else:\n                curr_additional_data = None\n\n            self.dicts[a].add(curr_action_embeddings, curr_action_values, curr_additional_data)\n        return True\n\n    def query(self, embeddings, action, k):\n        # query for nearest neighbors to the given embeddings\n        dnd_embeddings = []\n        dnd_values = []\n        dnd_indices = []\n        dnd_additional_data = []\n        for i in range(len(embeddings)):\n            embedding, value, indices, additional_data = self.dicts[action].query([embeddings[i]], k)\n            dnd_embeddings.append(embedding[0])\n            dnd_values.append(value[0])\n            dnd_indices.append(indices[0])\n            dnd_additional_data.append(additional_data[0])\n\n        if self.return_additional_data:\n            return dnd_embeddings, dnd_values, dnd_indices, dnd_additional_data\n        else:\n            return dnd_embeddings, dnd_values, dnd_indices\n\n    def has_enough_entries(self, k):\n        # check if each of the action dictionaries has at least k entries\n        for a in range(self.num_actions):\n            if not self.dicts[a].has_enough_entries(k):\n                return False\n        return True\n\n    def update_keys_and_values(self, actions, key_gradients, value_gradients, indices):\n        # Update DND keys and values\n        for batch_action, batch_keys, batch_values, batch_indices in zip(actions, key_gradients, value_gradients, indices):\n            # Update keys (embeddings) and values in DND\n            for i, index in enumerate(batch_indices):\n                self.dicts[batch_action].embeddings[index, :] -= self.learning_rate * batch_keys[i, :]\n                self.dicts[batch_action].values[index] -= self.learning_rate * batch_values[i]\n\n    def sample_embeddings(self, num_embeddings):\n        num_actions = len(self.dicts)\n        embeddings = []\n        num_embeddings_per_action = int(num_embeddings/num_actions)\n        for action in range(num_actions):\n            embeddings.append(self.dicts[action].sample_embeddings(num_embeddings_per_action))\n        embeddings = np.vstack(embeddings)\n\n        # the numbers did not divide nicely, let\'s just randomly sample some more embeddings\n        if num_embeddings_per_action * num_actions < num_embeddings:\n            action = np.random.randint(0, num_actions)\n            extra_embeddings = self.dicts[action].sample_embeddings(num_embeddings -\n                                                                   num_embeddings_per_action * num_actions)\n            embeddings = np.vstack([embeddings, extra_embeddings])\n        return embeddings\n\n    def clean(self):\n        # create a new dict for each action\n        self.dicts = []\n        for a in range(self.num_actions):\n            new_dict = AnnoyDictionary(self.dict_size, self.key_width, self.new_value_shift_coefficient,\n                                       key_error_threshold=self.key_error_threshold, num_neighbors=self.num_neighbors)\n            self.dicts.append(new_dict)\n\n\ndef load_dnd(model_dir):\n    latest_checkpoint_id = -1\n    latest_checkpoint = \'\'\n    # get all checkpoint files\n    for fname in os.listdir(model_dir):\n        path = os.path.join(model_dir, fname)\n        if os.path.isdir(path) or fname.split(\'.\')[-1] != \'srs\':\n            continue\n        checkpoint_id = int(fname.split(\'_\')[0])\n        if checkpoint_id > latest_checkpoint_id:\n            latest_checkpoint = fname\n            latest_checkpoint_id = checkpoint_id\n\n    with open(os.path.join(model_dir, str(latest_checkpoint)), \'rb\') as f:\n        DND = pickle.load(f)\n\n        for a in range(DND.num_actions):\n            DND.dicts[a].index = AnnoyIndex(512, metric=\'euclidean\')\n            DND.dicts[a].index.set_seed(1)\n\n            for idx, key in zip(range(DND.dicts[a].curr_size), DND.dicts[a].embeddings[:DND.dicts[a].curr_size]):\n                DND.dicts[a].index.add_item(idx, key)\n\n            DND.dicts[a].index.build(50)\n\n    return DND\n'"
rl_coach/memories/non_episodic/experience_replay.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List, Tuple, Union\nimport pickle\nimport random\nimport math\n\nimport numpy as np\n\nfrom rl_coach.core_types import Transition\nfrom rl_coach.logger import screen\nfrom rl_coach.memories.memory import Memory, MemoryGranularity, MemoryParameters\nfrom rl_coach.utils import ReaderWriterLock, ProgressBar\n\n\nclass ExperienceReplayParameters(MemoryParameters):\n    def __init__(self):\n        super().__init__()\n        self.max_size = (MemoryGranularity.Transitions, 1000000)\n        self.allow_duplicates_in_batch_sampling = True\n\n    @property\n    def path(self):\n        return \'rl_coach.memories.non_episodic.experience_replay:ExperienceReplay\'\n\n\nclass ExperienceReplay(Memory):\n    """"""\n    A regular replay buffer which stores transition without any additional structure\n    """"""\n    def __init__(self, max_size: Tuple[MemoryGranularity, int], allow_duplicates_in_batch_sampling: bool=True):\n        """"""\n        :param max_size: the maximum number of transitions or episodes to hold in the memory\n        :param allow_duplicates_in_batch_sampling: allow having the same transition multiple times in a batch\n        """"""\n        super().__init__(max_size)\n        if max_size[0] != MemoryGranularity.Transitions:\n            raise ValueError(""Experience replay size can only be configured in terms of transitions"")\n        self.transitions = []\n        self.allow_duplicates_in_batch_sampling = allow_duplicates_in_batch_sampling\n\n        self.reader_writer_lock = ReaderWriterLock()\n        self.frozen = False\n\n    def length(self) -> int:\n        """"""\n        Get the number of transitions in the ER\n        """"""\n        return self.num_transitions()\n\n    def num_transitions(self) -> int:\n        """"""\n        Get the number of transitions in the ER\n        """"""\n        return len(self.transitions)\n\n    def sample(self, size: int) -> List[Transition]:\n        """"""\n        Sample a batch of transitions form the replay buffer. If the requested size is larger than the number\n        of samples available in the replay buffer then the batch will return empty.\n        :param size: the size of the batch to sample\n        :return: a batch (list) of selected transitions from the replay buffer\n        """"""\n        self.reader_writer_lock.lock_writing()\n\n        if self.allow_duplicates_in_batch_sampling:\n            transitions_idx = np.random.randint(self.num_transitions(), size=size)\n\n        else:\n            if self.num_transitions() >= size:\n                transitions_idx = np.random.choice(self.num_transitions(), size=size, replace=False)\n            else:\n                raise ValueError(""The replay buffer cannot be sampled since there are not enough transitions yet. ""\n                                 ""There are currently {} transitions"".format(self.num_transitions()))\n\n        batch = [self.transitions[i] for i in transitions_idx]\n\n        self.reader_writer_lock.release_writing()\n        return batch\n\n    def get_shuffled_training_data_generator(self, size: int) -> List[Transition]:\n        """"""\n        Get an generator for iterating through the shuffled replay buffer, for processing the data in epochs.\n        If the requested size is larger than the number of samples available in the replay buffer then the batch will\n        return empty. The last returned batch may be smaller than the size requested, to accommodate for all the\n        transitions in the replay buffer.\n\n        :param size: the size of the batch to return\n        :return: a batch (list) of selected transitions from the replay buffer\n        """"""\n        self.reader_writer_lock.lock_writing()\n        shuffled_transition_indices = list(range(len(self.transitions)))\n        random.shuffle(shuffled_transition_indices)\n\n        # we deliberately drop some of the ending data which is left after dividing to batches of size `size`\n        # for i in range(math.ceil(len(shuffled_transition_indices) / size)):\n        for i in range(int(len(shuffled_transition_indices) / size)):\n            sample_data = [self.transitions[j] for j in shuffled_transition_indices[i * size: (i + 1) * size]]\n            self.reader_writer_lock.release_writing()\n\n            yield sample_data\n\n    def _enforce_max_length(self) -> None:\n        """"""\n        Make sure that the size of the replay buffer does not pass the maximum size allowed.\n        If it passes the max size, the oldest transition in the replay buffer will be removed.\n        This function does not use locks since it is only called internally\n        :return: None\n        """"""\n        granularity, size = self.max_size\n        if granularity == MemoryGranularity.Transitions:\n            while size != 0 and self.num_transitions() > size:\n                self.remove_transition(0, False)\n        else:\n            raise ValueError(""The granularity of the replay buffer can only be set in terms of transitions"")\n\n    def store(self, transition: Transition, lock: bool=True) -> None:\n        """"""\n        Store a new transition in the memory.\n        :param transition: a transition to store\n        :param lock: if true, will lock the readers writers lock. this can cause a deadlock if an inheriting class\n                     locks and then calls store with lock = True\n        :return: None\n        """"""\n        self.assert_not_frozen()\n\n        # Calling super.store() so that in case a memory backend is used, the memory backend can store this transition.\n        super().store(transition)\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        self.transitions.append(transition)\n        self._enforce_max_length()\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    def get_transition(self, transition_index: int, lock: bool=True) -> Union[None, Transition]:\n        """"""\n        Returns the transition in the given index. If the transition does not exist, returns None instead.\n        :param transition_index: the index of the transition to return\n        :param lock: use write locking if this is a shared memory\n        :return: the corresponding transition\n        """"""\n        if lock:\n            self.reader_writer_lock.lock_writing()\n\n        if self.length() == 0 or transition_index >= self.length():\n            transition = None\n        else:\n            transition = self.transitions[transition_index]\n\n        if lock:\n            self.reader_writer_lock.release_writing()\n\n        return transition\n\n    def remove_transition(self, transition_index: int, lock: bool=True) -> None:\n        """"""\n        Remove the transition in the given index.\n\n        This does not remove the transition from the segment trees! it is just used to remove the transition\n        from the transitions list\n        :param transition_index: the index of the transition to remove\n        :return: None\n        """"""\n        self.assert_not_frozen()\n\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        if self.num_transitions() > transition_index:\n            del self.transitions[transition_index]\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    # for API compatibility\n    def get(self, transition_index: int, lock: bool=True) -> Union[None, Transition]:\n        """"""\n        Returns the transition in the given index. If the transition does not exist, returns None instead.\n        :param transition_index: the index of the transition to return\n        :return: the corresponding transition\n        """"""\n        return self.get_transition(transition_index, lock)\n\n    def clean(self, lock: bool=True) -> None:\n        """"""\n        Clean the memory by removing all the episodes\n        :return: None\n        """"""\n        self.assert_not_frozen()\n\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        self.transitions = []\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    def mean_reward(self) -> np.ndarray:\n        """"""\n        Get the mean reward in the replay buffer\n        :return: the mean reward\n        """"""\n        self.reader_writer_lock.lock_writing()\n\n        mean = np.mean([transition.reward for transition in self.transitions])\n\n        self.reader_writer_lock.release_writing()\n\n        return mean\n\n    def save(self, file_path: str) -> None:\n        """"""\n        Save the replay buffer contents to a pickle file\n        :param file_path: the path to the file that will be used to store the pickled transitions\n        """"""\n        with open(file_path, \'wb\') as file:\n            pickle.dump(self.transitions, file)\n\n    def load_pickled(self, file_path: str) -> None:\n        """"""\n        Restore the replay buffer contents from a pickle file.\n        The pickle file is assumed to include a list of transitions.\n        :param file_path: The path to a pickle file to restore\n        """"""\n        self.assert_not_frozen()\n\n        with open(file_path, \'rb\') as file:\n            transitions = pickle.load(file)\n            num_transitions = len(transitions)\n            if num_transitions > self.max_size[1]:\n                screen.warning(""Warning! The number of transition to load into the replay buffer ({}) is ""\n                               ""bigger than the max size of the replay buffer ({}). The excessive transitions will ""\n                               ""not be stored."".format(num_transitions, self.max_size[1]))\n\n            progress_bar = ProgressBar(num_transitions)\n            for transition_idx, transition in enumerate(transitions):\n                self.store(transition)\n\n                # print progress\n                if transition_idx % 100 == 0:\n                    progress_bar.update(transition_idx)\n\n            progress_bar.close()\n\n    def freeze(self):\n        """"""\n        Freezing the replay buffer does not allow any new transitions to be added to the memory.\n        Useful when working with a dataset (e.g. batch-rl or imitation learning).\n        :return: None\n        """"""\n        self.frozen = True\n\n    def assert_not_frozen(self):\n        """"""\n        Check that the memory is not frozen, and can be changed.\n        :return:\n        """"""\n        assert self.frozen is False, ""Memory is frozen, and cannot be changed.""\n'"
rl_coach/memories/non_episodic/prioritized_experience_replay.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport operator\nimport random\nfrom enum import Enum\nfrom typing import List, Tuple, Any\n\nimport numpy as np\n\nfrom rl_coach.core_types import Transition\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters, ExperienceReplay\nfrom rl_coach.schedules import Schedule, ConstantSchedule\n\n\nclass PrioritizedExperienceReplayParameters(ExperienceReplayParameters):\n    def __init__(self):\n        super().__init__()\n        self.max_size = (MemoryGranularity.Transitions, 1000000)\n        self.alpha = 0.6\n        self.beta = ConstantSchedule(0.4)\n        self.epsilon = 1e-6\n\n    @property\n    def path(self):\n        return \'rl_coach.memories.non_episodic.prioritized_experience_replay:PrioritizedExperienceReplay\'\n\n\nclass SegmentTree(object):\n    """"""\n    A tree which can be used as a min/max heap or a sum tree\n    Add or update item value - O(log N)\n    Sampling an item - O(log N)\n    """"""\n    class Operation(Enum):\n        MAX = {""operator"": max, ""initial_value"": -float(""inf"")}\n        MIN = {""operator"": min, ""initial_value"": float(""inf"")}\n        SUM = {""operator"": operator.add, ""initial_value"": 0}\n\n    def __init__(self, size: int, operation: Operation):\n        self.next_leaf_idx_to_write = 0\n        self.size = size\n        if not (size > 0 and size & (size - 1) == 0):\n            raise ValueError(""A segment tree size must be a positive power of 2. The given size is {}"".format(self.size))\n        self.operation = operation\n        self.tree = np.ones(2 * size - 1) * self.operation.value[\'initial_value\']\n        self.data = [None] * size\n\n    def _propagate(self, node_idx: int) -> None:\n        """"""\n        Propagate an update of a node\'s value to its parent node\n        :param node_idx: the index of the node that was updated\n        :return: None\n        """"""\n        parent = (node_idx - 1) // 2\n\n        self.tree[parent] = self.operation.value[\'operator\'](self.tree[parent * 2 + 1], self.tree[parent * 2 + 2])\n\n        if parent != 0:\n            self._propagate(parent)\n\n    def _retrieve(self, root_node_idx: int, val: float)-> int:\n        """"""\n        Retrieve the first node that has a value larger than val and is a child of the node at index idx\n        :param root_node_idx: the index of the root node to search from\n        :param val: the value to query for\n        :return: the index of the resulting node\n        """"""\n        left = 2 * root_node_idx + 1\n        right = left + 1\n\n        if left >= len(self.tree):\n            return root_node_idx\n\n        if val <= self.tree[left]:\n            return self._retrieve(left, val)\n        else:\n            return self._retrieve(right, val-self.tree[left])\n\n    def total_value(self) -> float:\n        """"""\n        Return the total value of the tree according to the tree operation. For SUM for example, this will return\n        the total sum of the tree. for MIN, this will return the minimal value\n        :return: the total value of the tree\n        """"""\n        return self.tree[0]\n\n    def add(self, val: float, data: Any) -> None:\n        """"""\n        Add a new value to the tree with data assigned to it\n        :param val: the new value to add to the tree\n        :param data: the data that should be assigned to this value\n        :return: None\n        """"""\n        self.data[self.next_leaf_idx_to_write] = data\n        self.update(self.next_leaf_idx_to_write, val)\n\n        self.next_leaf_idx_to_write += 1\n        if self.next_leaf_idx_to_write >= self.size:\n            self.next_leaf_idx_to_write = 0\n\n    def update(self, leaf_idx: int, new_val: float) -> None:\n        """"""\n        Update the value of the node at index idx\n        :param leaf_idx: the index of the node to update\n        :param new_val: the new value of the node\n        :return: None\n        """"""\n        node_idx = leaf_idx + self.size - 1\n        if not 0 <= node_idx < len(self.tree):\n            raise ValueError(""The given left index ({}) can not be found in the tree. The available leaves are: 0-{}""\n                             .format(leaf_idx, self.size - 1))\n\n        self.tree[node_idx] = new_val\n        self._propagate(node_idx)\n\n    def get_element_by_partial_sum(self, val: float) -> Tuple[int, float, Any]:\n        """"""\n        Given a value between 0 and the tree sum, return the object which this value is in it\'s range.\n        For example, if we have 3 leaves: 10, 20, 30, and val=35, this will return the 3rd leaf, by accumulating\n        leaves by their order until getting to 35. This allows sampling leaves according to their proportional\n        probability.\n        :param val: a value within the range 0 and the tree sum\n        :return: the index of the resulting leaf in the tree, its probability and\n                 the object itself\n        """"""\n        node_idx = self._retrieve(0, val)\n        leaf_idx = node_idx - self.size + 1\n        data_value = self.tree[node_idx]\n        data = self.data[leaf_idx]\n\n        return leaf_idx, data_value, data\n\n    def __str__(self):\n        result = """"\n        start = 0\n        size = 1\n        while size <= self.size:\n            result += ""{}\\n"".format(self.tree[start:(start + size)])\n            start += size\n            size *= 2\n        return result\n\n\nclass PrioritizedExperienceReplay(ExperienceReplay):\n    """"""\n    This is the proportional sampling variant of the prioritized experience replay as described\n    in https://arxiv.org/pdf/1511.05952.pdf.\n    """"""\n    def __init__(self, max_size: Tuple[MemoryGranularity, int], alpha: float=0.6, beta: Schedule=ConstantSchedule(0.4),\n                 epsilon: float=1e-6, allow_duplicates_in_batch_sampling: bool=True):\n        """"""\n        :param max_size: the maximum number of transitions or episodes to hold in the memory\n        :param alpha: the alpha prioritization coefficient\n        :param beta: the beta parameter used for importance sampling\n        :param epsilon: a small value added to the priority of each transition\n        :param allow_duplicates_in_batch_sampling: allow having the same transition multiple times in a batch\n        """"""\n        if max_size[0] != MemoryGranularity.Transitions:\n            raise ValueError(""Prioritized Experience Replay currently only support setting the memory size in ""\n                             ""transitions granularity."")\n        self.power_of_2_size = 1\n        while self.power_of_2_size < max_size[1]:\n            self.power_of_2_size *= 2\n        super().__init__((MemoryGranularity.Transitions, self.power_of_2_size), allow_duplicates_in_batch_sampling)\n        self.sum_tree = SegmentTree(self.power_of_2_size, SegmentTree.Operation.SUM)\n        self.min_tree = SegmentTree(self.power_of_2_size, SegmentTree.Operation.MIN)\n        self.max_tree = SegmentTree(self.power_of_2_size, SegmentTree.Operation.MAX)\n        self.alpha = alpha\n        self.beta = beta\n        self.epsilon = epsilon\n        self.maximal_priority = 1.0\n\n    def _update_priority(self, leaf_idx: int, error: float) -> None:\n        """"""\n        Update the priority of a given transition, using its index in the tree and its error\n        :param leaf_idx: the index of the transition leaf in the tree\n        :param error: the new error value\n        :return: None\n        """"""\n        if error < 0:\n            raise ValueError(""The priorities must be non-negative values"")\n        priority = (error + self.epsilon)\n        self.sum_tree.update(leaf_idx, priority ** self.alpha)\n        self.min_tree.update(leaf_idx, priority ** self.alpha)\n        self.max_tree.update(leaf_idx, priority)\n        self.maximal_priority = self.max_tree.total_value()\n\n    def update_priorities(self, indices: List[int], error_values: List[float]) -> None:\n        """"""\n        Update the priorities of a batch of transitions using their indices and their new TD error terms\n        :param indices: the indices of the transitions to update\n        :param error_values: the new error values\n        :return: None\n        """"""\n        self.reader_writer_lock.lock_writing_and_reading()\n\n        if len(indices) != len(error_values):\n            raise ValueError(""The number of indexes requested for update don\'t match the number of error values given"")\n        for transition_idx, error in zip(indices, error_values):\n            self._update_priority(transition_idx, error)\n\n        self.reader_writer_lock.release_writing_and_reading()\n\n    def sample(self, size: int) -> List[Transition]:\n        """"""\n        Sample a batch of transitions form the replay buffer. If the requested size is larger than the number\n        of samples available in the replay buffer then the batch will return empty.\n        :param size: the size of the batch to sample\n        :return: a batch (list) of selected transitions from the replay buffer\n        """"""\n\n        self.reader_writer_lock.lock_writing()\n\n        if self.num_transitions() >= size:\n            # split the tree leaves to equal segments and sample one transition from each segment\n            batch = []\n            segment_size = self.sum_tree.total_value() / size\n\n            # get the maximum weight in the memory\n            min_probability = self.min_tree.total_value() / self.sum_tree.total_value()  # min P(j) = min p^a / sum(p^a)\n            max_weight = (min_probability * self.num_transitions()) ** -self.beta.current_value  # max wi\n\n            # sample a batch\n            for i in range(size):\n                segment_start = segment_size * i\n                segment_end = segment_size * (i + 1)\n\n                # sample leaf and calculate its weight\n                val = random.uniform(segment_start, segment_end)\n                leaf_idx, priority, transition = self.sum_tree.get_element_by_partial_sum(val)\n                priority /= self.sum_tree.total_value()   # P(j) = p^a / sum(p^a)\n                weight = (self.num_transitions() * priority) ** -self.beta.current_value  # (N * P(j)) ^ -beta\n                normalized_weight = weight / max_weight  # wj = ((N * P(j)) ^ -beta) / max wi\n\n                transition.info[\'idx\'] = leaf_idx\n                transition.info[\'weight\'] = normalized_weight\n\n                batch.append(transition)\n\n            self.beta.step()\n\n        else:\n            raise ValueError(""The replay buffer cannot be sampled since there are not enough transitions yet. ""\n                             ""There are currently {} transitions"".format(self.num_transitions()))\n\n        self.reader_writer_lock.release_writing()\n        return batch\n\n    def store(self, transition: Transition, lock=True) -> None:\n        """"""\n        Store a new transition in the memory.\n        :param transition: a transition to store\n        :return: None\n        """"""\n        # Calling super.store() so that in case a memory backend is used, the memory backend can store this transition.\n        super().store(transition)\n\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        transition_priority = self.maximal_priority\n        self.sum_tree.add(transition_priority ** self.alpha, transition)\n        self.min_tree.add(transition_priority ** self.alpha, transition)\n        self.max_tree.add(transition_priority, transition)\n        super().store(transition, False)\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n\n    def clean(self, lock=True) -> None:\n        """"""\n        Clean the memory by removing all the episodes\n        :return: None\n        """"""\n        if lock:\n            self.reader_writer_lock.lock_writing_and_reading()\n\n        super().clean(lock=False)\n        self.sum_tree = SegmentTree(self.power_of_2_size, SegmentTree.Operation.SUM)\n        self.min_tree = SegmentTree(self.power_of_2_size, SegmentTree.Operation.MIN)\n        self.max_tree = SegmentTree(self.power_of_2_size, SegmentTree.Operation.MAX)\n\n        if lock:\n            self.reader_writer_lock.release_writing_and_reading()\n'"
rl_coach/memories/non_episodic/transition_collection.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom rl_coach.core_types import Transition\n\n\nclass TransitionCollection(object):\n    """"""\n    Simple python implementation of transitions collection non-episodic memories\n    are constructed on top of.\n    """"""\n    def __init__(self):\n        super(TransitionCollection, self).__init__()\n\n    def append(self, transition):\n        pass\n\n    def extend(self, transitions):\n        for transition in transitions:\n            self.append(transition)\n\n    def __len__(self):\n        pass\n\n    def __del__(self, range: slice):\n        # NOTE: the only slice used is the form: slice(None, n)\n        # NOTE: if it is easier, what we really want here is the ability to\n        # constrain the size of the collection. as new transitions are added,\n        # old transitions can be removed to maintain a maximum collection size.\n        pass\n\n    def __getitem__(self, key: int):\n        # NOTE: we can switch to a method which fetches multiple items at a time\n        # if that would significantly improve performance\n        pass\n\n    def __iter__(self):\n        # this is not high priority\n        pass\n'"
rl_coach/off_policy_evaluators/bandits/__init__.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
rl_coach/off_policy_evaluators/bandits/doubly_robust.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport numpy as np\n\n\nclass DoublyRobust(object):\n\n    @staticmethod\n    def evaluate(ope_shared_stats: \'OpeSharedStats\') -> tuple:\n        """"""\n        Run the off-policy evaluator to get a score for the goodness of the new policy, based on the dataset,\n        which was collected using other policy(ies).\n\n        Papers:\n        https://arxiv.org/abs/1103.4601\n        https://arxiv.org/pdf/1612.01205 (some more clearer explanations)\n\n        :return: the evaluation score\n        """"""\n\n        ips = np.mean(ope_shared_stats.rho_all_dataset * ope_shared_stats.all_rewards)\n        dm = np.mean(ope_shared_stats.all_v_values_reward_model_based)\n        dr = np.mean(ope_shared_stats.rho_all_dataset *\n                     (ope_shared_stats.all_rewards - ope_shared_stats.all_reward_model_rewards[\n                         range(len(ope_shared_stats.all_actions)), ope_shared_stats.all_actions])) + dm\n\n        return ips, dm, dr\n'"
rl_coach/off_policy_evaluators/rl/__init__.py,0,b''
rl_coach/off_policy_evaluators/rl/sequential_doubly_robust.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom typing import List\nimport numpy as np\n\nfrom rl_coach.core_types import Episode\n\n\nclass SequentialDoublyRobust(object):\n\n    @staticmethod\n    def evaluate(evaluation_dataset_as_episodes: List[Episode], discount_factor: float) -> float:\n        """"""\n        Run the off-policy evaluator to get a score for the goodness of the new policy, based on the dataset,\n        which was collected using other policy(ies).\n        When the epsiodes are of changing lengths, this estimator might prove problematic due to its nature of recursion\n        of adding rewards up to the end of the episode (horizon). It will probably work best with episodes of fixed\n        length.\n        Paper: https://arxiv.org/pdf/1511.03722.pdf\n\n        :return: the evaluation score\n        """"""\n\n        # Sequential Doubly Robust\n        per_episode_seq_dr = []\n\n        for episode in evaluation_dataset_as_episodes:\n            episode_seq_dr = 0\n            for transition in reversed(episode.transitions):\n                rho = transition.info[\'softmax_policy_prob\'][transition.action] / \\\n                      transition.info[\'all_action_probabilities\'][transition.action]\n                episode_seq_dr = transition.info[\'v_value_q_model_based\'] + rho * (transition.reward + discount_factor\n                                                                                   * episode_seq_dr -\n                                                                                   transition.info[\'q_value\'][\n                                                                                       transition.action])\n            per_episode_seq_dr.append(episode_seq_dr)\n\n        seq_dr = np.array(per_episode_seq_dr).mean()\n\n        return seq_dr\n'"
rl_coach/off_policy_evaluators/rl/weighted_importance_sampling.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom typing import List\nimport numpy as np\n\nfrom rl_coach.core_types import Episode\n\n\nclass WeightedImportanceSampling(object):\n# TODO add PDIS\n    @staticmethod\n    def evaluate(evaluation_dataset_as_episodes: List[Episode]) -> float:\n        """"""\n        Run the off-policy evaluator to get a score for the goodness of the new policy, based on the dataset,\n        which was collected using other policy(ies).\n\n        References:\n        - Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction. Chapter 5.5.\n        - https://people.cs.umass.edu/~pthomas/papers/Thomas2015c.pdf\n        - http://videolectures.net/deeplearning2017_thomas_safe_rl/\n\n        :return: the evaluation score\n        """"""\n\n        # Weighted Importance Sampling\n        per_episode_w_i = []\n\n        for episode in evaluation_dataset_as_episodes:\n            w_i = 1\n            for transition in episode.transitions:\n                w_i *= transition.info[\'softmax_policy_prob\'][transition.action] / \\\n                      transition.info[\'all_action_probabilities\'][transition.action]\n            per_episode_w_i.append(w_i)\n\n        total_w_i_sum_across_episodes = sum(per_episode_w_i)\n\n        wis = 0\n        if total_w_i_sum_across_episodes != 0:\n            for i, episode in enumerate(evaluation_dataset_as_episodes):\n                if len(episode.transitions) != 0:\n                    wis += per_episode_w_i[i] * episode.transitions[0].n_step_discounted_rewards\n            wis /= total_w_i_sum_across_episodes\n\n        return wis\n'"
rl_coach/tests/agents/__init__.py,0,b''
rl_coach/tests/agents/test_agent_external_communication.py,1,"b'import os\nimport sys\n\nfrom rl_coach.base_parameters import TaskParameters, Frameworks\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\nimport tensorflow as tf\nfrom tensorflow import logging\nimport pytest\nlogging.set_verbosity(logging.INFO)\n\n\n@pytest.mark.unit_test\ndef test_get_QActionStateValue_predictions():\n    tf.reset_default_graph()\n    from rl_coach.presets.CartPole_DQN import graph_manager as cartpole_dqn_graph_manager\n    assert cartpole_dqn_graph_manager\n    cartpole_dqn_graph_manager.create_graph(task_parameters=\n                                            TaskParameters(framework_type=Frameworks.tensorflow,\n                                                           experiment_path=""./experiments/test""))\n    cartpole_dqn_graph_manager.improve_steps.num_steps = 1\n    cartpole_dqn_graph_manager.steps_between_evaluation_periods.num_steps = 5\n\n    # graph_manager.improve()\n    #\n    # agent = graph_manager.level_managers[0].composite_agents[\'simple_rl_agent\'].agents[\'simple_rl_agent/agent\']\n    # some_state = agent.memory.sample(1)[0].state\n    # cartpole_dqn_predictions = agent.get_predictions(states=some_state, prediction_type=QActionStateValue)\n    # assert cartpole_dqn_predictions.shape == (1, 2)\n\n\nif __name__ == \'__main__\':\n    test_get_QActionStateValue_predictions()\n'"
rl_coach/tests/architectures/__init__.py,0,b''
rl_coach/tests/environments/__init__.py,0,b''
rl_coach/tests/environments/test_gym_environment.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nfrom rl_coach.environments.gym_environment import GymEnvironment\nfrom rl_coach.base_parameters import VisualizationParameters\nimport numpy as np\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace, ImageObservationSpace, VectorObservationSpace\n\n\n@pytest.fixture()\ndef atari_env():\n    # create a breakout gym environment\n    env = GymEnvironment(level='Breakout-v0',\n                         seed=1,\n                         frame_skip=4,\n                         visualization_parameters=VisualizationParameters())\n    env.reset_internal_state(True)\n    return env\n\n\n@pytest.fixture()\ndef continuous_env():\n    # create a breakout gym environment\n    env = GymEnvironment(level='Pendulum-v0',\n                         seed=1,\n                         frame_skip=1,\n                         visualization_parameters=VisualizationParameters())\n    env.reset_internal_state(True)\n    return env\n\n\n@pytest.mark.unit_test\ndef test_gym_discrete_environment(atari_env):\n    # observation space\n    assert type(atari_env.state_space['observation']) == ImageObservationSpace\n    assert np.all(atari_env.state_space['observation'].shape == [210, 160, 3])\n    assert np.all(atari_env.last_env_response.next_state['observation'].shape == (210, 160, 3))\n\n    # action space\n    assert type(atari_env.action_space) == DiscreteActionSpace\n    assert np.all(atari_env.action_space.high == 3)\n\n    # make sure that the seed is working properly\n    assert np.sum(atari_env.last_env_response.next_state['observation']) == 4115856\n\n\n@pytest.mark.unit_test\ndef test_gym_continuous_environment(continuous_env):\n    # observation space\n    assert type(continuous_env.state_space['observation']) == VectorObservationSpace\n    assert np.all(continuous_env.state_space['observation'].shape == [3])\n    assert np.all(continuous_env.last_env_response.next_state['observation'].shape == (3,))\n\n    # action space\n    assert type(continuous_env.action_space) == BoxActionSpace\n    assert np.all(continuous_env.action_space.shape == np.array([1]))\n\n    # make sure that the seed is working properly\n    assert np.sum(continuous_env.last_env_response.next_state['observation']) == 0.6118565010687202\n\n\n@pytest.mark.unit_test\ndef test_step(atari_env):\n    result = atari_env.step(0)\n\nif __name__ == '__main__':\n    test_gym_continuous_environment(continuous_env())\n"""
rl_coach/tests/exploration_policies/__init__.py,0,b''
rl_coach/tests/exploration_policies/test_additive_noise.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\n\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\nfrom rl_coach.exploration_policies.additive_noise import AdditiveNoise\nfrom rl_coach.schedules import LinearSchedule\nimport numpy as np\n\n\n@pytest.mark.unit_test\ndef test_init():\n    # discrete control\n    action_space = DiscreteActionSpace(3)\n    noise_schedule = LinearSchedule(1.0, 1.0, 1000)\n\n    # additive noise requires a bounded range for the actions\n    action_space = BoxActionSpace(np.array([10]))\n    with pytest.raises(ValueError):\n        policy = AdditiveNoise(action_space, noise_schedule, 0)\n\n\n@pytest.mark.unit_test\ndef test_get_action():\n    # make sure noise is in range\n    action_space = BoxActionSpace(np.array([10]), -1, 1)\n    noise_schedule = LinearSchedule(1.0, 1.0, 1000)\n    policy = AdditiveNoise(action_space, noise_schedule, 0)\n\n    # the action range is 2, so there is a ~0.1% chance that the noise will be larger than 3*std=3*2=6\n    for i in range(1000):\n        action = policy.get_action(np.zeros([10]))\n        assert np.all(action < 10)\n        # make sure there is no clipping of the action since it should be the environment that clips actions\n        assert np.all(action != 1.0)\n        assert np.all(action != -1.0)\n        # make sure that each action element has a different value\n        assert np.all(action[0] != action[1:])\n'"
rl_coach/tests/exploration_policies/test_e_greedy.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\n\nfrom rl_coach.spaces import DiscreteActionSpace\nfrom rl_coach.exploration_policies.e_greedy import EGreedy\nfrom rl_coach.schedules import LinearSchedule\nimport numpy as np\nfrom rl_coach.core_types import RunPhase\n\n\n@pytest.mark.unit_test\ndef test_get_action():\n    # discrete control\n    action_space = DiscreteActionSpace(3)\n    epsilon_schedule = LinearSchedule(1.0, 1.0, 1000)\n    policy = EGreedy(action_space, epsilon_schedule, evaluation_epsilon=0)\n\n    # verify that test phase gives greedy actions (evaluation_epsilon = 0)\n    policy.change_phase(RunPhase.TEST)\n    for i in range(100):\n        best_action, _ = policy.get_action(np.array([10, 20, 30]))\n        assert best_action == 2\n\n    # verify that train phase gives uniform actions (exploration = 1)\n    policy.change_phase(RunPhase.TRAIN)\n    counters = np.array([0, 0, 0])\n    for i in range(30000):\n        best_action, _ = policy.get_action(np.array([10, 20, 30]))\n        counters[best_action] += 1\n    assert np.all(counters > 9500)  # this is noisy so we allow 5% error\n\n    # TODO: test continuous actions\n\n\n@pytest.mark.unit_test\ndef test_change_phase():\n    # discrete control\n    action_space = DiscreteActionSpace(3)\n    epsilon_schedule = LinearSchedule(1.0, 0.1, 1000)\n    policy = EGreedy(action_space, epsilon_schedule, evaluation_epsilon=0.01)\n\n    # verify schedule not applying if not in training phase\n    assert policy.get_control_param() == 1.0\n    policy.change_phase(RunPhase.TEST)\n    best_action = policy.get_action(np.array([10, 20, 30]))\n    assert policy.epsilon_schedule.current_value == 1.0\n    policy.change_phase(RunPhase.HEATUP)\n    best_action = policy.get_action(np.array([10, 20, 30]))\n    assert policy.epsilon_schedule.current_value == 1.0\n    policy.change_phase(RunPhase.UNDEFINED)\n    best_action = policy.get_action(np.array([10, 20, 30]))\n    assert policy.epsilon_schedule.current_value == 1.0\n\n\n@pytest.mark.unit_test\ndef test_get_control_param():\n    # discrete control\n    action_space = DiscreteActionSpace(3)\n    epsilon_schedule = LinearSchedule(1.0, 0.1, 1000)\n    policy = EGreedy(action_space, epsilon_schedule, evaluation_epsilon=0.01)\n\n    # verify schedule applies to TRAIN phase\n    policy.change_phase(RunPhase.TRAIN)\n    for i in range(999):\n        best_action = policy.get_action(np.array([10, 20, 30]))\n        assert 1.0 > policy.get_control_param() > 0.1\n    best_action = policy.get_action(np.array([10, 20, 30]))\n    assert policy.get_control_param() == 0.1\n\n    # test phases\n    policy.change_phase(RunPhase.TEST)\n    assert policy.get_control_param() == 0.01\n\n    policy.change_phase(RunPhase.TRAIN)\n    assert policy.get_control_param() == 0.1\n\n    policy.change_phase(RunPhase.HEATUP)\n    assert policy.get_control_param() == 0.1\n'"
rl_coach/tests/exploration_policies/test_greedy.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\n\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\nfrom rl_coach.exploration_policies.greedy import Greedy\nimport numpy as np\n\n\n@pytest.mark.unit_test\ndef test_get_action():\n    # discrete control\n    action_space = DiscreteActionSpace(3)\n    policy = Greedy(action_space)\n\n    best_action, _ = policy.get_action(np.array([10, 20, 30]))\n    assert best_action == 2\n\n    # continuous control\n    action_space = BoxActionSpace(np.array([10]))\n    policy = Greedy(action_space)\n\n    best_action = policy.get_action(np.array([1, 1, 1]))\n    assert np.all(best_action == np.array([1, 1, 1]))\n\n\n@pytest.mark.unit_test\ndef test_get_control_param():\n    action_space = DiscreteActionSpace(3)\n    policy = Greedy(action_space)\n    assert policy.get_control_param() == 0\n\n'"
rl_coach/tests/exploration_policies/test_ou_process.py,0,"b'import os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\n\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace\nfrom rl_coach.exploration_policies.ou_process import OUProcess\nfrom rl_coach.core_types import RunPhase\nimport numpy as np\n\n\n@pytest.mark.unit_test\ndef test_init():\n    # discrete control\n    action_space = DiscreteActionSpace(3)\n\n\n@pytest.mark.unit_test\ndef test_get_action():\n    action_space = BoxActionSpace(np.array([10]), -1, 1)\n    policy = OUProcess(action_space, mu=0, theta=0.1, sigma=0.2, dt=0.01)\n\n    # make sure no noise is added in the testing phase\n    policy.change_phase(RunPhase.TEST)\n    assert np.all(policy.get_action(np.zeros((10,))) == np.zeros((10,)))\n    rand_action = np.random.rand(10)\n    assert np.all(policy.get_action(rand_action) == rand_action)\n\n    # make sure the noise added in the training phase matches the golden\n    policy.change_phase(RunPhase.TRAIN)\n    np.random.seed(0)\n    targets = [\n        [0.03528105, 0.00800314, 0.01957476, 0.04481786, 0.03735116, - 0.01954556, 0.01900177, - 0.00302714, - 0.00206438, 0.00821197],\n        [0.03812664, 0.03708061, 0.03477594, 0.04720655, 0.04619107, - 0.01285253, 0.04886435, - 0.00712728, 0.00419904, - 0.00887816],\n        [-0.01297129, 0.0501159, 0.05202989, 0.03231604, 0.09153997, - 0.04192699, 0.04973065, - 0.01086383, 0.03485043, 0.0205179],\n        [-0.00985937, 0.05762904, 0.03422214, - 0.00733221, 0.08449019, - 0.03875808, 0.07428674, 0.01319463, 0.02706904, 0.01445132],\n        [-3.08205658e-02, 2.91710492e-02, 6.25166679e-05, 3.16906342e-02, 7.42126579e-02, - 4.74808080e-02, 4.91565431e-02, 2.87312413e-02, - 5.23598615e-03, 1.01820670e-02],\n        [-0.04869908, 0.03687993, - 0.01015365, 0.0080463, 0.0735748, -0.03886669, 0.05043773, 0.03475195, - 0.01791719, 0.00291706],\n        [-0.06209959, 0.02965198, - 0.02640642, - 0.0264874, 0.07704975, - 0.04686344, 0.01778333, 0.04397284, - 0.03604524, 0.00395305],\n        [-0.04745568, 0.03220199, - 0.003592, -0.05115743, 0.08501953, - 0.06051278, 0.0003496, 0.03235188, - 0.04224025, 0.00507241],\n        [-0.07071122, 0.05018632, 0.00572484, - 0.08183114, 0.11469956, - 0.02253448, 0.02392484, 0.02872103, - 0.06361306, 0.02615637],\n        [-0.07870404, 0.07458503, 0.00988462, - 0.06221653, 0.12171218, - 0.00838049, 0.02411092, 0.06440972, - 0.0610112, 0.03417],\n        [-0.04096233, 0.04755527, - 0.01553497, - 0.04276638, 0.098128, 0.03050032, 0.01581443, 0.04939621, - 0.02249135, 0.06374613],\n        [-0.00357018, 0.06562861, - 0.03274395, - 0.00452232, 0.09266981, 0.04651895, 0.03474365, 0.04624661, - 0.01018727, 0.08212651],\n    ]\n    for i in range(10):\n        current_noise = policy.get_action(np.zeros((10,)))\n        assert np.all(np.abs(current_noise - targets[i]) < 1e-7)\n\n    # get some statistics. check very roughly that the mean acts according to the definition of the policy\n\n    # mean of 0\n    vals = []\n    for i in range(50000):\n        current_noise = policy.get_action(np.zeros((10,)))\n        vals.append(current_noise)\n    assert np.all(np.abs(np.mean(vals, axis=0)) < 1)\n\n    # mean of 10\n    policy = OUProcess(action_space, mu=10, theta=0.1, sigma=0.2, dt=0.01)\n    policy.change_phase(RunPhase.TRAIN)\n    vals = []\n    for i in range(50000):\n        current_noise = policy.get_action(np.zeros((10,)))\n        vals.append(current_noise)\n    assert np.all(np.abs(np.mean(vals, axis=0) - 10) < 1)\n\n    # plot the noise values - only used for understanding how the noise actually looks\n    # import matplotlib.pyplot as plt\n    # vals = np.array(vals)\n    # for i in range(10):\n    #     plt.plot(list(range(10000)), vals[:, i])\n    #     plt.plot(list(range(10000)), vals[:, i])\n    #     plt.plot(list(range(10000)), vals[:, i])\n    # plt.show()\n\n\nif __name__ == ""__main__"":\n    test_get_action()\n'"
rl_coach/tests/filters/__init__.py,0,b''
rl_coach/tests/filters/test_filters_stacking.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\n\nfrom rl_coach.filters.observation.observation_rescale_to_size_filter import ObservationRescaleToSizeFilter\nfrom rl_coach.filters.observation.observation_crop_filter import ObservationCropFilter\nfrom rl_coach.filters.reward.reward_clipping_filter import RewardClippingFilter\nfrom rl_coach.filters.observation.observation_stacking_filter import ObservationStackingFilter\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.spaces import ImageObservationSpace\nimport numpy as np\nfrom rl_coach.core_types import EnvResponse\nfrom collections import OrderedDict\n\n\n@pytest.mark.filterwarnings(\'ignore:Conversion of\')\n@pytest.mark.unit_test\ndef test_filter_stacking():\n    # test that filter stacking works fine by taking as input a transition with:\n    # - an observation of shape 210x160,\n    # - a reward of 100\n    # filtering it by:\n    # - rescaling the observation to 110x84\n    # - cropping the observation to 84x84\n    # - clipping the reward to 1\n    # - stacking 4 observations to get 84x84x4\n\n    env_response = EnvResponse({\'observation\': np.ones([210, 160])}, reward=100, game_over=False)\n\n    filter1 = ObservationRescaleToSizeFilter(\n        output_observation_space=ImageObservationSpace(np.array([110, 84]), high=255),\n    )\n\n    filter2 = ObservationCropFilter(\n        crop_low=np.array([16, 0]),\n        crop_high=np.array([100, 84])\n    )\n\n    filter3 = RewardClippingFilter(\n        clipping_low=-1,\n        clipping_high=1\n    )\n\n    output_filter = ObservationStackingFilter(\n        stack_size=4,\n        stacking_axis=-1\n    )\n\n    input_filter = InputFilter(\n        observation_filters={\n            ""observation"": OrderedDict([\n                (""filter1"", filter1),\n                (""filter2"", filter2),\n                (""output_filter"", output_filter)\n            ])},\n        reward_filters=OrderedDict([\n            (""filter3"", filter3)\n        ])\n    )\n\n    result = input_filter.filter(env_response)[0]\n    observation = np.array(result.next_state[\'observation\'])\n    assert observation.shape == (84, 84, 4)\n    assert np.all(observation == np.ones([84, 84, 4]))\n    assert result.reward == 1\n\n\n'"
rl_coach/tests/graph_managers/__init__.py,0,b''
rl_coach/tests/graph_managers/test_basic_rl_graph_manager.py,4,"b'import gc\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\nimport tensorflow as tf\nfrom rl_coach.base_parameters import TaskParameters, DistributedTaskParameters, Frameworks\nfrom rl_coach.core_types import EnvironmentSteps\nfrom rl_coach.utils import get_open_port\nfrom multiprocessing import Process\nfrom tensorflow import logging\nimport pytest\nlogging.set_verbosity(logging.INFO)\n\n\n@pytest.mark.unit_test\ndef test_basic_rl_graph_manager_with_pong_a3c():\n    tf.reset_default_graph()\n    from rl_coach.presets.Atari_A3C import graph_manager\n    assert graph_manager\n    graph_manager.env_params.level = ""PongDeterministic-v4""\n    graph_manager.create_graph(task_parameters=TaskParameters(framework_type=Frameworks.tensorflow,\n                                                              experiment_path=""./experiments/test""))\n    # graph_manager.improve()\n\n\n@pytest.mark.unit_test\ndef test_basic_rl_graph_manager_with_pong_nec():\n    tf.reset_default_graph()\n    from rl_coach.presets.Atari_NEC import graph_manager\n    assert graph_manager\n    graph_manager.env_params.level = ""PongDeterministic-v4""\n    graph_manager.create_graph(task_parameters=TaskParameters(framework_type=Frameworks.tensorflow,\n                                                              experiment_path=""./experiments/test""))\n    # graph_manager.improve()\n\n\n@pytest.mark.unit_test\ndef test_basic_rl_graph_manager_with_cartpole_dqn():\n    tf.reset_default_graph()\n    from rl_coach.presets.CartPole_DQN import graph_manager\n    assert graph_manager\n    graph_manager.create_graph(task_parameters=TaskParameters(framework_type=Frameworks.tensorflow,\n                                                              experiment_path=""./experiments/test""))\n    # graph_manager.improve()\n\n# Test for identifying memory leak in restore_checkpoint\n@pytest.mark.unit_test\ndef test_basic_rl_graph_manager_with_cartpole_dqn_and_repeated_checkpoint_restore():\n    tf.reset_default_graph()\n    from rl_coach.presets.CartPole_DQN import graph_manager\n    assert graph_manager\n    graph_manager.create_graph(task_parameters=TaskParameters(framework_type=Frameworks.tensorflow,\n                                                              experiment_path=""./experiments/test"",\n                                                              apply_stop_condition=True))\n    # graph_manager.improve()\n    # graph_manager.evaluate(EnvironmentSteps(1000))\n    # graph_manager.save_checkpoint()\n    #\n    # graph_manager.task_parameters.checkpoint_restore_path = ""./experiments/test/checkpoint""\n    # while True:\n    #     graph_manager.restore_checkpoint()\n    #     graph_manager.evaluate(EnvironmentSteps(1000))\n    #     gc.collect()\n\nif __name__ == \'__main__\':\n    pass\n    # test_basic_rl_graph_manager_with_pong_a3c()\n    # test_basic_rl_graph_manager_with_ant_a3c()\n    # test_basic_rl_graph_manager_with_pong_nec()\n\t# test_basic_rl_graph_manager_with_cartpole_dqn()\n    # test_basic_rl_graph_manager_with_cartpole_dqn_and_repeated_checkpoint_restore()\n    #test_basic_rl_graph_manager_multithreaded_with_pong_a3c()\n\t#test_basic_rl_graph_manager_with_doom_basic_dqn()'"
rl_coach/tests/graph_managers/test_graph_manager.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nfrom rl_coach.graph_managers.graph_manager import GraphManager, ScheduleParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import RunPhase\n\n\n@pytest.mark.unit_test\ndef test_phase_context():\n    graph_manager = GraphManager(name='', schedule_params=ScheduleParameters(), vis_params=VisualizationParameters())\n\n    assert graph_manager.phase == RunPhase.UNDEFINED\n    with graph_manager.phase_context(RunPhase.TRAIN):\n        assert graph_manager.phase == RunPhase.TRAIN\n    assert graph_manager.phase == RunPhase.UNDEFINED\n\n\n@pytest.mark.unit_test\ndef test_phase_context_nested():\n    graph_manager = GraphManager(name='', schedule_params=ScheduleParameters(), vis_params=VisualizationParameters())\n\n    assert graph_manager.phase == RunPhase.UNDEFINED\n    with graph_manager.phase_context(RunPhase.TRAIN):\n        assert graph_manager.phase == RunPhase.TRAIN\n        with graph_manager.phase_context(RunPhase.TEST):\n            assert graph_manager.phase == RunPhase.TEST\n        assert graph_manager.phase == RunPhase.TRAIN\n    assert graph_manager.phase == RunPhase.UNDEFINED\n\n\n@pytest.mark.unit_test\ndef test_phase_context_double_nested():\n    graph_manager = GraphManager(name='', schedule_params=ScheduleParameters(), vis_params=VisualizationParameters())\n\n    assert graph_manager.phase == RunPhase.UNDEFINED\n    with graph_manager.phase_context(RunPhase.TRAIN):\n        assert graph_manager.phase == RunPhase.TRAIN\n        with graph_manager.phase_context(RunPhase.TEST):\n            assert graph_manager.phase == RunPhase.TEST\n            with graph_manager.phase_context(RunPhase.UNDEFINED):\n                assert graph_manager.phase == RunPhase.UNDEFINED\n            assert graph_manager.phase == RunPhase.TEST\n        assert graph_manager.phase == RunPhase.TRAIN\n    assert graph_manager.phase == RunPhase.UNDEFINED\n"""
rl_coach/tests/memories/__init__.py,0,b''
rl_coach/tests/memories/test_differential_neural_dictionary.py,12,"b'# nasty hack to deal with issue #46\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nimport time\nfrom rl_coach.memories.non_episodic.differentiable_neural_dictionary import QDND\nimport tensorflow as tf\n\nNUM_ACTIONS = 3\nNUM_DND_ENTRIES_TO_ADD = 10000\nEMBEDDING_SIZE = 512\nNUM_SAMPLED_EMBEDDINGS = 500\nNUM_NEIGHBORS = 10\nDND_SIZE = 500000\n\n@pytest.fixture()\ndef dnd():\n    return QDND(\n                DND_SIZE,\n                EMBEDDING_SIZE,\n                NUM_ACTIONS,\n                0.1,\n                key_error_threshold=0,\n                learning_rate=0.0001,\n                num_neighbors=NUM_NEIGHBORS\n                )\n\n\n@pytest.mark.unit_test\ndef test_random_sample_from_dnd(dnd: QDND):\n    # store single non terminal transition\n    embeddings = [np.random.rand(EMBEDDING_SIZE) for j in range(NUM_DND_ENTRIES_TO_ADD)]\n    actions = [np.random.randint(NUM_ACTIONS) for j in range(NUM_DND_ENTRIES_TO_ADD)]\n    values = [np.random.rand() for j in range(NUM_DND_ENTRIES_TO_ADD)]\n    dnd.add(embeddings, actions, values)\n    dnd_embeddings, dnd_values, dnd_indices = dnd.query(embeddings[0:10], 0, NUM_NEIGHBORS)\n\n    # calculate_normalization_factor\n    sampled_embeddings = dnd.sample_embeddings(NUM_SAMPLED_EMBEDDINGS)\n    coefficient = 1/(NUM_SAMPLED_EMBEDDINGS * (NUM_SAMPLED_EMBEDDINGS - 1.0))\n    tf_current_embedding = tf.placeholder(tf.float32, shape=(EMBEDDING_SIZE), name=\'current_embedding\')\n    tf_other_embeddings = tf.placeholder(tf.float32, shape=(NUM_SAMPLED_EMBEDDINGS - 1, EMBEDDING_SIZE), name=\'other_embeddings\')\n\n    sub = tf_current_embedding - tf_other_embeddings\n    square = tf.square(sub)\n    result = tf.reduce_sum(square)\n\n\n\n    ###########################\n    # more efficient method\n    ###########################\n    sampled_embeddings_expanded = tf.placeholder(\n        tf.float32, shape=(1, NUM_SAMPLED_EMBEDDINGS, EMBEDDING_SIZE), name=\'sampled_embeddings_expanded\')\n    sampled_embeddings_tiled = tf.tile(sampled_embeddings_expanded, (sampled_embeddings_expanded.shape[1], 1, 1))\n    sampled_embeddings_transposed = tf.transpose(sampled_embeddings_tiled, (1, 0, 2))\n    sub2 = sampled_embeddings_tiled - sampled_embeddings_transposed\n    square2 = tf.square(sub2)\n    result2 = tf.reduce_sum(square2)\n\n    config = tf.ConfigProto()\n    config.allow_soft_placement = True  # allow placing ops on cpu if they are not fit for gpu\n    config.gpu_options.allow_growth = True  # allow the gpu memory allocated for the worker to grow if needed\n\n    sess = tf.Session(config=config)\n\n    sum1 = 0\n    start = time.time()\n    for i in range(NUM_SAMPLED_EMBEDDINGS):\n        curr_sampled_embedding = sampled_embeddings[i]\n        other_embeddings = np.delete(sampled_embeddings, i, 0)\n        sum1 += sess.run(result, feed_dict={tf_current_embedding: curr_sampled_embedding, tf_other_embeddings: other_embeddings})\n    print(""1st method: {} sec"".format(time.time()-start))\n\n    start = time.time()\n    sum2 = sess.run(result2, feed_dict={sampled_embeddings_expanded: np.expand_dims(sampled_embeddings,0)})\n    print(""2nd method: {} sec"".format(time.time()-start))\n\n    # validate that results are equal\n    print(""sum1 = {}, sum2 = {}"".format(sum1, sum2))\n\n    norm_factor = -0.5/(coefficient * sum2)\n\nif __name__ == \'__main__\':\n    test_random_sample_from_dnd(dnd())\n\n'"
rl_coach/tests/memories/test_hindsight_experience_replay.py,0,"b""# nasty hack to deal with issue #46\nimport os\nimport sys\n\nfrom rl_coach.memories.episodic.episodic_hindsight_experience_replay import EpisodicHindsightExperienceReplayParameters\nfrom rl_coach.spaces import GoalsSpace, ReachingGoal\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n# print(sys.path)\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.core_types import Transition, Episode\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.memories.episodic.episodic_hindsight_experience_replay import EpisodicHindsightExperienceReplay, \\\n     HindsightGoalSelectionMethod\n\n\n#TODO: change from defining a new class to creating an instance from the parameters\nclass Parameters(EpisodicHindsightExperienceReplayParameters):\n    def __init__(self):\n        super().__init__()\n        self.max_size = (MemoryGranularity.Transitions, 100)\n        self.hindsight_transitions_per_regular_transition = 4\n        self.hindsight_goal_selection_method = HindsightGoalSelectionMethod.Future\n        self.goals_space = GoalsSpace(goal_name='observation',\n                                      reward_type=ReachingGoal(distance_from_goal_threshold=0.1),\n                                      distance_metric=GoalsSpace.DistanceMetric.Euclidean)\n\n\n@pytest.fixture\ndef episode():\n    episode = []\n    for i in range(10):\n        episode.append(Transition(\n            state={'observation': np.array([i]), 'desired_goal': np.array([i]), 'achieved_goal': np.array([i])},\n            action=i,\n        ))\n    return episode\n\n\n@pytest.fixture\ndef her():\n    params = Parameters().__dict__\n\n    import inspect\n    args = set(inspect.getfullargspec(EpisodicHindsightExperienceReplay.__init__).args).intersection(params)\n    params = {k: params[k] for k in args}\n\n    return EpisodicHindsightExperienceReplay(**params)\n\n\n@pytest.mark.unit_test\ndef test_sample_goal(her, episode):\n    assert her._sample_goal(episode, 8) == 9\n\n\n@pytest.mark.unit_test\ndef test_sample_goal_range(her, episode):\n    unseen_goals = set(range(1, 9))\n    for _ in range(500):\n        unseen_goals -= set([int(her._sample_goal(episode, 0))])\n        if not unseen_goals:\n            return\n\n    assert unseen_goals == set()\n\n\n@pytest.mark.unit_test\ndef test_update_episode(her):\n    episode = Episode()\n    for i in range(10):\n        episode.insert(Transition(\n            state={'observation': np.array([i]), 'desired_goal': np.array([i+1]), 'achieved_goal': np.array([i+1])},\n            action=i,\n            game_over=i == 9,\n            reward=0 if i == 9 else -1,\n        ))\n\n    her.store_episode(episode)\n    # print('her._num_transitions', her._num_transitions)\n\n    # 10 original transitions, and 9 transitions * 4 hindsight episodes\n    assert her.num_transitions() == 10 + (4 * 9)\n\n    # make sure that the goal state was never sampled from the past\n    for transition in her.transitions:\n        assert transition.state['desired_goal'] > transition.state['observation']\n        assert transition.next_state['desired_goal'] >= transition.next_state['observation']\n\n        if transition.reward == 0:\n            assert transition.game_over\n        else:\n            assert not transition.game_over\n\n# test_update_episode(her())\n"""
rl_coach/tests/memories/test_prioritized_experience_replay.py,0,"b'# nasty hack to deal with issue #46\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\n\nfrom rl_coach.memories.non_episodic.prioritized_experience_replay import SegmentTree\n\n\n@pytest.mark.unit_test\ndef test_sum_tree():\n    # test power of 2 sum tree\n    sum_tree = SegmentTree(size=4, operation=SegmentTree.Operation.SUM)\n    sum_tree.add(10, ""10"")\n    assert sum_tree.total_value() == 10\n    sum_tree.add(20, ""20"")\n    assert sum_tree.total_value() == 30\n    sum_tree.add(5, ""5"")\n    assert sum_tree.total_value() == 35\n    sum_tree.add(7.5, ""7.5"")\n    assert sum_tree.total_value() == 42.5\n    sum_tree.add(2.5, ""2.5"")\n    assert sum_tree.total_value() == 35\n    sum_tree.add(5, ""5"")\n    assert sum_tree.total_value() == 20\n\n    assert sum_tree.get_element_by_partial_sum(2) == (0, 2.5, \'2.5\')\n    assert sum_tree.get_element_by_partial_sum(3) == (1, 5.0, \'5\')\n    assert sum_tree.get_element_by_partial_sum(10) == (2, 5.0, \'5\')\n    assert sum_tree.get_element_by_partial_sum(13) == (3, 7.5, \'7.5\')\n\n    sum_tree.update(2, 10)\n    assert sum_tree.__str__() == ""[25.]\\n[ 7.5 17.5]\\n[ 2.5  5.  10.   7.5]\\n""\n\n    # test non power of 2 sum tree\n    with pytest.raises(ValueError):\n        sum_tree = SegmentTree(size=5, operation=SegmentTree.Operation.SUM)\n\n\n@pytest.mark.unit_test\ndef test_min_tree():\n    min_tree = SegmentTree(size=4, operation=SegmentTree.Operation.MIN)\n    min_tree.add(10, ""10"")\n    assert min_tree.total_value() == 10\n    min_tree.add(20, ""20"")\n    assert min_tree.total_value() == 10\n    min_tree.add(5, ""5"")\n    assert min_tree.total_value() == 5\n    min_tree.add(7.5, ""7.5"")\n    assert min_tree.total_value() == 5\n    min_tree.add(2, ""2"")\n    assert min_tree.total_value() == 2\n    min_tree.add(3, ""3"")\n    min_tree.add(3, ""3"")\n    min_tree.add(3, ""3"")\n    min_tree.add(5, ""5"")\n    assert min_tree.total_value() == 3\n\n\n@pytest.mark.unit_test\ndef test_max_tree():\n    max_tree = SegmentTree(size=4, operation=SegmentTree.Operation.MAX)\n    max_tree.add(10, ""10"")\n    assert max_tree.total_value() == 10\n    max_tree.add(20, ""20"")\n    assert max_tree.total_value() == 20\n    max_tree.add(5, ""5"")\n    assert max_tree.total_value() == 20\n    max_tree.add(7.5, ""7.5"")\n    assert max_tree.total_value() == 20\n    max_tree.add(2, ""2"")\n    assert max_tree.total_value() == 20\n    max_tree.add(3, ""3"")\n    max_tree.add(3, ""3"")\n    max_tree.add(3, ""3"")\n    max_tree.add(5, ""5"")\n    assert max_tree.total_value() == 5\n\n    # update\n    max_tree.update(1, 10)\n    assert max_tree.total_value() == 10\n    assert max_tree.__str__() == ""[10.]\\n[10.  3.]\\n[ 5. 10.  3.  3.]\\n""\n    max_tree.update(1, 2)\n    assert max_tree.total_value() == 5\n    assert max_tree.__str__() == ""[5.]\\n[5. 3.]\\n[5. 2. 3. 3.]\\n""\n\n\nif __name__ == ""__main__"":\n    test_sum_tree()\n    test_min_tree()\n    test_max_tree()\n'"
rl_coach/tests/memories/test_single_episode_buffer.py,0,"b'# nasty hack to deal with issue #46\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.core_types import Transition\nfrom rl_coach.memories.episodic.single_episode_buffer import SingleEpisodeBuffer\n\n\n@pytest.fixture()\ndef buffer():\n    return SingleEpisodeBuffer()\n\n\n@pytest.mark.unit_test\ndef test_store_and_get(buffer: SingleEpisodeBuffer):\n    # store single non terminal transition\n    transition = Transition(state={""observation"": np.array([1, 2, 3])}, action=1, reward=1, game_over=False)\n    buffer.store(transition)\n    assert buffer.length() == 1\n    assert buffer.num_complete_episodes() == 0\n    assert buffer.num_transitions_in_complete_episodes() == 0\n    assert buffer.num_transitions() == 1\n\n    # get the single stored transition\n    episode = buffer.get(0)\n    assert episode.length() == 1\n    assert episode.get_first_transition() is transition    # check addresses are the same\n    assert episode.get_last_transition() is transition   # check addresses are the same\n\n    # store single terminal transition\n    transition = Transition(state={""observation"": np.array([1, 2, 3])}, action=1, reward=1, game_over=True)\n    buffer.store(transition)\n    assert buffer.length() == 1\n    assert buffer.num_complete_episodes() == 1\n    assert buffer.num_transitions_in_complete_episodes() == 2\n\n    # check that the episode is valid\n    episode = buffer.get(0)\n    assert episode.length() == 2\n    assert episode.get_transition(0).n_step_discounted_rewards == 1 + 0.99\n    assert episode.get_transition(1).n_step_discounted_rewards == 1\n    assert buffer.mean_reward() == 1\n\n    # only one episode in the replay buffer\n    episode = buffer.get(1)\n    assert episode is None\n\n    # adding transitions after the first episode was closed\n    transition = Transition(state={""observation"": np.array([1, 2, 3])}, action=1, reward=0, game_over=False)\n    buffer.store(transition)\n    assert buffer.length() == 1\n    assert buffer.num_complete_episodes() == 0\n    assert buffer.num_transitions_in_complete_episodes() == 0\n\n    # still only one episode\n    assert buffer.get(1) is None\n    assert buffer.mean_reward() == 0\n\n\n@pytest.mark.unit_test\ndef test_clean(buffer: SingleEpisodeBuffer):\n    # add several transitions and then clean the buffer\n    transition = Transition(state={""observation"": np.array([1, 2, 3])}, action=1, reward=1, game_over=False)\n    for i in range(10):\n        buffer.store(transition)\n    assert buffer.num_transitions() == 10\n    buffer.clean()\n    assert buffer.num_transitions() == 0\n\n    # add more transitions after the clean and make sure they were really cleaned\n    transition = Transition(state={""observation"": np.array([1, 2, 3])}, action=1, reward=1, game_over=True)\n    buffer.store(transition)\n    assert buffer.num_transitions() == 1\n    assert buffer.num_transitions_in_complete_episodes() == 1\n    assert buffer.num_complete_episodes() == 1\n    for i in range(10):\n        assert buffer.sample(1)[0] is transition\n'"
rl_coach/tests/presets/__init__.py,0,b''
rl_coach/tests/presets/test_presets.py,0,"b'# nasty hack to deal with issue #46\nimport os\nimport sys\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport os\nimport time\nimport shutil\nfrom subprocess import Popen, DEVNULL\nfrom rl_coach.logger import screen\n\nFAILING_PRESETS = [\n    \'Fetch_DDPG_HER_baselines\',\n    \'MontezumaRevenge_BC\',\n    \'ControlSuite_DDPG\',\n    \'Doom_Basic_BC\',\n    \'CARLA_CIL\',\n    \'CARLA_DDPG\',\n    \'CARLA_Dueling_DDQN\',\n    \'CARLA_3_Cameras_DDPG\',\n    \'Starcraft_CollectMinerals_A3C\',\n    \'Starcraft_CollectMinerals_Dueling_DDQN\',\n]\n\ndef all_presets():\n    result = []\n    for f in sorted(os.listdir(\'rl_coach/presets\')):\n        if f.endswith(\'.py\') and f != \'__init__.py\':\n            preset = f.split(\'.\')[0]\n            if preset not in FAILING_PRESETS:\n                result.append(preset)\n    return result\n\n\n@pytest.fixture(params=all_presets())\ndef preset(request):\n    return request.param\n\n\n@pytest.mark.integration_test\ndef test_preset_runs(preset):\n    test_failed = False\n\n    print(""Testing preset {}"".format(preset))\n\n    # TODO: this is a temporary workaround for presets which define more than a single available level.\n    # we should probably do this in a more robust way\n    level = """"\n    if ""Atari"" in preset:\n        level = ""breakout""\n    elif ""Mujoco"" in preset:\n        level = ""inverted_pendulum""\n    elif ""ControlSuite"" in preset:\n        level = ""pendulum:swingup""\n\n    experiment_name = "".test-"" + preset\n\n    # overriding heatup steps to some small number of steps (1000), so to finish the heatup stage, and get to train\n    params = [""python3"", ""rl_coach/coach.py"", ""-p"", preset, ""-ns"", ""-e"", experiment_name, \'-cp\',\n              \'heatup_steps=EnvironmentSteps(1000)\']\n    if level != """":\n        params += [""-lvl"", level]\n\n    p = Popen(params)\n\n    # wait 30 seconds overhead of initialization, and finishing heatup.\n    time.sleep(30)\n    return_value = p.poll()\n\n    if return_value is None:\n        screen.success(""{} passed successfully"".format(preset))\n    else:\n        test_failed = True\n        screen.error(""{} failed"".format(preset), crash=False)\n\n    p.kill()\n    if os.path.exists(""experiments/"" + experiment_name):\n        shutil.rmtree(""experiments/"" + experiment_name)\n\n    assert not test_failed\n'"
rl_coach/tests/utils/__init__.py,0,b''
rl_coach/tests/utils/args_utils.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n""""""Manage all command arguments.""""""\n\nimport os\nimport signal\nimport time\nimport pandas as pd\nimport numpy as np\nimport pytest\nfrom rl_coach.tests.utils.test_utils import get_csv_path, get_files_from_dir, \\\n    find_string_in_logs\nfrom rl_coach.tests.utils.definitions import Definitions as Def\n\n\ndef collect_preset_for_mxnet():\n    """"""\n    Collect presets that relevant for args testing only.\n    This used for testing arguments for specific presets that defined in the\n    definitions (args_test under Presets).\n    :return: preset(s) list\n    """"""\n    for pn in Def.Presets.mxnet_args_test:\n        assert pn, Def.Consts.ASSERT_MSG.format(""Preset name"", pn)\n        yield pn\n\n\ndef collect_preset_for_args():\n    """"""\n    Collect presets that relevant for args testing only.\n    This used for testing arguments for specific presets that defined in the\n    definitions (args_test under Presets).\n    :return: preset(s) list\n    """"""\n    for pn in Def.Presets.args_test:\n        assert pn, Def.Consts.ASSERT_MSG.format(""Preset name"", pn)\n        yield pn\n\n\ndef collect_preset_for_seed():\n    """"""\n    Collect presets that relevant for seed argument testing only.\n    This used for testing arguments for specific presets that defined in the\n    definitions (args_test under Presets).\n    :return: preset(s) list\n    """"""\n    for pn in Def.Presets.args_for_seed_test:\n        assert pn, Def.Consts.ASSERT_MSG.format(""Preset name"", pn)\n        yield pn\n\n\ndef collect_args():\n    """"""\n    Collect args from the cmd args list - on each test iteration, it will\n    yield one line (one arg).\n    :yield: one arg foe each test iteration\n    """"""\n    for i in Def.Flags.cmd_args:\n        assert i, Def.Consts.ASSERT_MSG.format(""flag list"", str(i))\n        yield i\n\n\ndef add_one_flag_value(flag):\n    """"""\n    Add value to flag format in order to run the python command with arguments.\n    :param flag: dict flag\n    :return: flag with format\n    """"""\n    if not flag or len(flag) == 0:\n        return []\n\n    if len(flag) == 1:\n        return flag\n\n    if Def.Flags.enw in flag[1]:\n        flag[1] = \'2\'\n\n    elif Def.Flags.css in flag[1]:\n        flag[1] = \'5\'\n\n    elif Def.Flags.fw_ten in flag[1]:\n        flag[1] = ""tensorflow""\n\n    elif Def.Flags.fw_mx in flag[1]:\n        flag[1] = ""mxnet""\n\n    elif Def.Flags.cp in flag[1]:\n        flag[1] = ""heatup_steps=EnvironmentSteps({})"".format(Def.Consts.num_hs)\n\n    return flag\n\n\ndef is_reward_reached(csv_path, p_valid_params, start_time, time_limit):\n    """"""\n    Check the result of the experiment, by collecting all the Evaluation Reward\n    and average should be bigger than the min reward threshold.\n    :param csv_path: csv file  (results)\n    :param p_valid_params: experiment test params\n    :param start_time: start time of the test\n    :param time_limit: timeout of the test\n    :return: |Bool| true if reached the reward minimum\n    """"""\n    win_size = 10\n    last_num_episodes = 0\n    csv = None\n    reward_reached = False\n    reward_str = \'Evaluation Reward\'\n\n    while csv is None or (csv[csv.columns[0]].values[\n                              -1] < p_valid_params.max_episodes_to_achieve_reward and time.time() - start_time < time_limit):\n        try:\n            csv = pd.read_csv(csv_path)\n        except:\n            # sometimes the csv is being written at the same time we are\n            # trying to read it. no problem -> try again\n            continue\n\n        if reward_str not in csv.keys():\n            continue\n\n        rewards = csv[reward_str].values\n        rewards = rewards[~np.isnan(rewards)]\n\n        if len(rewards) >= 1:\n            averaged_rewards = np.convolve(rewards, np.ones(min(len(rewards), win_size)) / win_size, mode=\'valid\')\n        else:\n            time.sleep(1)\n            continue\n\n        if csv[csv.columns[0]].shape[0] - last_num_episodes <= 0:\n            continue\n\n        last_num_episodes = csv[csv.columns[0]].values[-1]\n\n        # check if reward is enough\n        if np.any(averaged_rewards >= p_valid_params.min_reward_threshold):\n            reward_reached = True\n            break\n        time.sleep(1)\n\n    return reward_reached\n\n\ndef validate_arg_result(flag, p_valid_params, clres=None, process=None,\n                        start_time=None, timeout=Def.TimeOuts.test_time_limit):\n    """"""\n    Validate results of one argument.\n    :param flag: flag to check\n    :param p_valid_params: params test per preset\n    :param clres: object of files paths (results of test experiment)\n    :param process: process object\n    :param start_time: start time of the test\n    :param timeout: timeout of the test- fail test once over the timeout\n    """"""\n\n    if flag[0] == ""-ns"" or flag[0] == ""--no-summary"":\n        """"""\n        --no-summary: Once selected, summary lines shouldn\'t appear in logs\n        """"""\n        # send CTRL+C to close experiment\n        process.send_signal(signal.SIGINT)\n\n        assert not find_string_in_logs(log_path=clres.stdout.name,\n                                       str=Def.Consts.RESULTS_SORTED), \\\n            Def.Consts.ASSERT_MSG.format(""No Result summary"",\n                                         Def.Consts.RESULTS_SORTED)\n\n        assert not find_string_in_logs(log_path=clres.stdout.name,\n                                       str=Def.Consts.TOTAL_RUNTIME), \\\n            Def.Consts.ASSERT_MSG.format(""No Total runtime summary"",\n                                         Def.Consts.TOTAL_RUNTIME)\n\n        assert not find_string_in_logs(log_path=clres.stdout.name,\n                                       str=Def.Consts.DISCARD_EXP), \\\n            Def.Consts.ASSERT_MSG.format(""No discard message"",\n                                         Def.Consts.DISCARD_EXP)\n\n    elif flag[0] == ""-asc"" or flag[0] == ""--apply_stop_condition"":\n        """"""\n        -asc, --apply_stop_condition: Once selected, coach stopped when \n                                      required success rate reached\n        """"""\n        assert find_string_in_logs(log_path=clres.stdout.name,\n                                   str=Def.Consts.REACHED_REQ_ASC,\n                                   wait_and_find=True), \\\n            Def.Consts.ASSERT_MSG.format(Def.Consts.REACHED_REQ_ASC,\n                                         ""Message Not Found"")\n\n    elif flag[0] == ""--print_networks_summary"":\n        """"""\n        --print_networks_summary: Once selected, agent summary should appear in\n                                  stdout.\n        """"""\n        if find_string_in_logs(log_path=clres.stdout.name,\n                               str=Def.Consts.INPUT_EMBEDDER):\n            assert True, Def.Consts.ASSERT_MSG.format(\n                Def.Consts.INPUT_EMBEDDER, ""Not found"")\n\n        if find_string_in_logs(log_path=clres.stdout.name,\n                               str=Def.Consts.MIDDLEWARE):\n            assert True, Def.Consts.ASSERT_MSG.format(\n                Def.Consts.MIDDLEWARE, ""Not found"")\n\n        if find_string_in_logs(log_path=clres.stdout.name,\n                               str=Def.Consts.OUTPUT_HEAD):\n            assert True, Def.Consts.ASSERT_MSG.format(\n                Def.Consts.OUTPUT_HEAD, ""Not found"")\n\n    elif flag[0] == ""-tb"" or flag[0] == ""--tensorboard"":\n        """"""\n        -tb, --tensorboard: Once selected, a new folder should be created in \n                            experiment folder.\n        """"""\n        csv_path = get_csv_path(clres)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n\n        exp_path = os.path.dirname(csv_path[0])\n        tensorboard_path = os.path.join(exp_path, Def.Path.tensorboard)\n\n        assert os.path.isdir(tensorboard_path), \\\n            Def.Consts.ASSERT_MSG.format(""tensorboard path"", tensorboard_path)\n\n        # check if folder contain files and check extensions\n        files = get_files_from_dir(dir_path=tensorboard_path)\n        assert any("".tfevents."" in file for file in files)\n\n    elif flag[0] == ""-onnx"" or flag[0] == ""--export_onnx_graph"":\n        """"""\n        -onnx, --export_onnx_graph: Once selected, warning message should \n                                    appear, it should be with another flag.\n        """"""\n        assert find_string_in_logs(log_path=clres.stdout.name,\n                                   str=Def.Consts.ONNX_WARNING,\n                                   wait_and_find=True), \\\n            Def.Consts.ASSERT_MSG.format(Def.Consts.ONNX_WARNING, ""Not found"")\n\n    elif flag[0] == ""-dg"" or flag[0] == ""--dump_gifs"":\n        """"""\n        -dg, --dump_gifs: Once selected, a new folder should be created in \n                          experiment folder for gifs files.\n        """"""\n        pytest.xfail(reason=""GUI issue on CI"")\n\n        csv_path = get_csv_path(clres)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n\n        exp_path = os.path.dirname(csv_path[0])\n        gifs_path = os.path.join(exp_path, Def.Path.gifs)\n\n        # wait until gif folder were created\n        while time.time() - start_time < timeout:\n            if os.path.isdir(gifs_path):\n                assert os.path.isdir(gifs_path), \\\n                    Def.Consts.ASSERT_MSG.format(""gifs path"", gifs_path)\n                break\n\n        # check if folder contain files\n        get_files_from_dir(dir_path=gifs_path)\n\n    elif flag[0] == ""-dm"" or flag[0] == ""--dump_mp4"":\n        """"""\n        -dm, --dump_mp4: Once selected, a new folder should be created in \n                         experiment folder for videos files.\n        """"""\n        pytest.xfail(reason=""GUI issue on CI"")\n\n        csv_path = get_csv_path(clres)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n\n        exp_path = os.path.dirname(csv_path[0])\n        videos_path = os.path.join(exp_path, Def.Path.videos)\n\n        # wait until video folder were created\n        while time.time() - start_time < timeout:\n            if os.path.isdir(videos_path):\n                assert os.path.isdir(videos_path), \\\n                    Def.Consts.ASSERT_MSG.format(""videos path"", videos_path)\n                break\n\n        # check if folder contain files\n        get_files_from_dir(dir_path=videos_path)\n\n    elif flag[0] == ""--nocolor"":\n        """"""\n        --nocolor: Once selected, check if color prefix is replacing the actual\n                   color; example: \'## agent: ...\'\n        """"""\n        assert find_string_in_logs(log_path=clres.stdout.name,\n                                   str=Def.Consts.COLOR_PREFIX,\n                                   wait_and_find=True), \\\n            Def.Consts.ASSERT_MSG.format(Def.Consts.COLOR_PREFIX,\n                                         ""Color Prefix Not Found"")\n\n    elif flag[0] == ""--evaluate"":\n        """"""\n        --evaluate: Once selected, Coach start testing, there is not training.\n        """"""\n        # wait until files created\n        get_csv_path(clres=clres)\n        time.sleep(15)\n        assert not find_string_in_logs(log_path=clres.stdout.name,\n                                       str=Def.Consts.TRAINING), \\\n            Def.Consts.ASSERT_MSG.format(""Training Not Found"",\n                                         Def.Consts.TRAINING)\n\n    elif flag[0] == ""--play"":\n        """"""\n        --play: Once selected alone, an warning message should appear, it \n                should be with another flag.\n        """"""\n        assert find_string_in_logs(log_path=clres.stdout.name,\n                                   str=Def.Consts.PLAY_WARNING,\n                                   wait_and_find=True), \\\n            Def.Consts.ASSERT_MSG.format(Def.Consts.PLAY_WARNING, ""Not found"")\n\n    elif flag[0] == ""-et"" or flag[0] == ""--environment_type"":\n        """"""\n        -et, --environment_type: Once selected check csv results is created.\n        """"""\n        csv_path = get_csv_path(clres)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n\n    elif flag[0] == ""-s"" or flag[0] == ""--checkpoint_save_secs"":\n        """"""\n        -s, --checkpoint_save_secs: Once selected, check if files added to the\n                                    experiment path.\n        """"""\n        csv_path = get_csv_path(clres)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n\n        exp_path = os.path.dirname(csv_path[0])\n        checkpoint_path = os.path.join(exp_path, Def.Path.checkpoint)\n\n        # wait until video folder were created\n        while time.time() - start_time < timeout:\n            if os.path.isdir(checkpoint_path):\n                assert os.path.isdir(checkpoint_path), \\\n                    Def.Consts.ASSERT_MSG.format(""checkpoint path"",\n                                                 checkpoint_path)\n                break\n\n        # check if folder contain files\n        get_files_from_dir(dir_path=checkpoint_path)\n\n    elif flag[0] == ""-ew"" or flag[0] == ""--evaluation_worker"":\n        """"""\n        -ew, --evaluation_worker: Once selected, check that an evaluation \n                                  worker is created. e.g. by checking that it\'s\n                                  csv file is created.        \n        """"""\n        # wait until files created\n        csv_path = get_csv_path(clres=clres, extra_tries=10)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n\n    elif flag[0] == ""-cp"" or flag[0] == ""--custom_parameter"":\n        """"""\n        -cp, --custom_parameter: Once selected, check that the total steps are\n                                 around the given param with +/- gap.\n                                 also, check the heat-up param      \n        """"""\n        # wait until files created\n        csv_path = get_csv_path(clres=clres)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n\n        # read csv file\n        csv = pd.read_csv(csv_path[0])\n\n        # check heat-up value\n        while csv[""In Heatup""].values[-1] == 1:\n            csv = pd.read_csv(csv_path[0])\n            time.sleep(1)\n\n        csv.columns = [column.replace("" "", ""_"") for column in csv.columns]\n        results = csv.query(""In_Heatup == 1"")\n        total_values = len(results.Total_steps.values)\n        assert len(results.Total_steps.values) > 0, \\\n            Def.Consts.ASSERT_MSG(""no data in csv"", str(total_values))\n\n        last_val_in_heatup = results.Total_steps.values[-1]\n        assert int(last_val_in_heatup) >= Def.Consts.num_hs, \\\n            Def.Consts.ASSERT_MSG.format(""bigger than "" +\n                                         str(Def.Consts.num_hs), last_val_in_heatup)\n\n    elif flag[0] == ""-f"" or flag[0] == ""--framework"":\n        """"""\n        -f, --framework: Once selected, f = tensorflow or mxnet\n        """"""\n        # wait until files created\n        csv_path = get_csv_path(clres=clres)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n        time.sleep(5)\n\n        get_reward = is_reward_reached(csv_path=csv_path[0],\n                                       p_valid_params=p_valid_params,\n                                       start_time=start_time,\n                                       time_limit=timeout)\n\n        # check if experiment is working and reached the reward\n        assert get_reward, Def.Consts.ASSERT_MSG.format(\n            ""Doesn\'t reached the reward"", get_reward)\n\n        # check if there is no exception\n        assert not find_string_in_logs(log_path=clres.stdout.name,\n                                       str=Def.Consts.LOG_ERROR)\n\n        ret_val = process.poll()\n        assert ret_val is None, Def.Consts.ASSERT_MSG.format(""None"", ret_val)\n\n    elif flag[0] == ""-crd"" or flag[0] == ""--checkpoint_restore_dir"":\n\n        """"""\n        -crd, --checkpoint_restore_dir: Once selected alone, check that can\'t\n                                        restore checkpoint dir (negative test).\n        """"""\n        # wait until files created\n        csv_path = get_csv_path(clres=clres)\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""path not found"", csv_path)\n        assert find_string_in_logs(log_path=clres.stdout.name,\n                                   str=Def.Consts.NO_CHECKPOINT), \\\n            Def.Consts.ASSERT_MSG.format(Def.Consts.NO_CHECKPOINT, ""Not found"")\n\n    elif flag[0] == ""--seed"":\n        """"""\n        --seed: Once selected, check logs of process list if all are the same\n                results.\n        """"""\n        lst_csv = []\n        # wait until files created\n        csv_path = get_csv_path(clres=clres, extra_tries=20,\n                                num_expected_files=int(flag[1]))\n\n        assert len(csv_path) > 0, \\\n            Def.Consts.ASSERT_MSG.format(""paths are not found"", str(csv_path))\n\n        assert int(flag[1]) == len(csv_path), Def.Consts.ASSERT_MSG. \\\n            format(int(flag[1]), len(csv_path))\n\n        # wait for getting results in csv\'s\n        for i in range(len(csv_path)):\n\n            lines_in_file = pd.read_csv(csv_path[i])\n            while len(lines_in_file[\'Episode #\'].values) < 100 and \\\n                    time.time() - start_time < Def.TimeOuts.test_time_limit:\n                lines_in_file = pd.read_csv(csv_path[i])\n                time.sleep(1)\n\n            lst_csv.append(pd.read_csv(csv_path[i],\n                                       nrows=Def.Consts.N_csv_lines))\n\n        assert len(lst_csv) > 1, Def.Consts.ASSERT_MSG.format(""> 1"",\n                                                              len(lst_csv))\n\n        df1 = lst_csv[0]\n        for df in lst_csv[1:]:\n            assert list(df1[\'Training Iter\'].values) == list(\n                df[\'Training Iter\'].values)\n\n            assert list(df1[\'ER #Transitions\'].values) == list(\n                df[\'ER #Transitions\'].values)\n\n            assert list(df1[\'Total steps\'].values) == list(\n                df[\'Total steps\'].values)\n\n    elif flag[0] == ""-c"" or flag[0] == ""--use_cpu"":\n        pass\n\n    elif flag[0] == ""-n"" or flag[0] == ""--num_workers"":\n\n        """"""\n        -n, --num_workers: Once selected alone, check that csv created for each\n                           worker, and check results.\n        """"""\n        # wait until files created\n        num_expected_files = int(flag[1])\n        csv_path = get_csv_path(clres=clres, extra_tries=20,\n                                num_expected_files=num_expected_files)\n\n        assert len(csv_path) >= num_expected_files, \\\n            Def.Consts.ASSERT_MSG.format(str(num_expected_files),\n                                         str(len(csv_path)))\n\n'"
rl_coach/tests/utils/definitions.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n""""""\nDefinitions file:\n\nIt\'s main functionality are:\n1) housing project constants and enums.\n2) housing configuration parameters.\n3) housing resource paths.\n""""""\n\n\nclass Definitions:\n    GROUP_NAME = ""rl_coach""\n    PROCESS_NAME = ""coach""\n    DASHBOARD_PROC = ""firefox""\n\n    class Flags:\n        css = ""checkpoint_save_secs""\n        crd = ""checkpoint_restore_dir""\n        et = ""environment_type""\n        cp = ""custom_parameter""\n        seed = ""seed""\n        dccp = ""distributed_coach_config_path""\n        enw = ""num_workers""\n        fw_ten = ""framework_tensorflow""\n        fw_mx = ""framework_mxnet""\n        # et = ""rl_coach.environments.gym_environment:Atari"" TODO\n\n        """"""\n        Arguments that can be tested for python coach command\n         ** 1 parameter    = Flag - no need for string or int\n         ** 2 parameters   = add value for the selected flag\n        """"""\n        cmd_args = [\n            [\'-ew\'],\n            [\'--play\'],\n            [\'--evaluate\'],\n            [\'-f\', fw_ten],\n            [\'--nocolor\'],\n            [\'-s\', css],\n            [\'-cp\', cp],\n            [\'--print_networks_summary\'],\n            [\'-tb\'],\n            [\'-ns\'],\n            [\'-onnx\'],\n            [\'-asc\'],\n            [\'--dump_worker_logs\'],\n            [\'-dg\'],\n            [\'-dm\'],\n            # [\'-crd\', crd], # Tested in checkpoint test\n            # [\'-et\', et],\n            # \'-lvl\': \'{level}\',  # TODO: Add test validation on args_utils\n            # \'-e\': \'{}\',  # TODO: Add test validation on args_utils\n            # \'-l\': None,  # TODO: Add test validation on args_utils\n            # \'-c\': None,  # TODO: Add test validation using nvidia-smi\n            # \'-v\': None,  # TODO: Add test validation on args_utils\n            # \'--seed\': \'{\' + seed + \'}\', # DONE - new test added\n            # \'-dc\': None,  # TODO: Add test validation on args_utils\n            # \'-dcp\': \'{}\'  # TODO: Add test validation on args_utils\n            # [\'-n\', enw],  # Duplicated arg test\n            # [\'-d\'],  # Arg can\'t be automated - no GUI in the CI\n            # \'-r\': None,  # No automation test\n            # \'-tfv\': None,  # No automation test\n            # \'-ept\': \'{\' + ept + \'}\',  # No automation test - not supported\n        ]\n\n    class Presets:\n        # Preset list for testing the flags/ arguments of python coach command\n        args_test = [\n            ""CartPole_A3C"",\n        ]\n\n        # Preset list for mxnet framework testing\n        mxnet_args_test = [\n            ""CartPole_DQN""\n        ]\n\n        # Preset for testing seed argument\n        args_for_seed_test = [\n            ""Atari_DQN"",\n            ""Doom_Basic_DQN"",\n            ""BitFlip_DQN"",\n            ""CartPole_DQN"",\n            ""CARLA_Dueling_DDQN"",\n            ""ControlSuite_DDPG"",\n            ""ExplorationChain_Dueling_DDQN"",\n            ""Fetch_DDPG_HER_baselines"",\n            ""Mujoco_ClippedPPO"",\n            ""Starcraft_CollectMinerals_Dueling_DDQN"",\n        ]\n\n    class Path:\n        experiments = ""./experiments""\n        tensorboard = ""tensorboard""\n        test_dir = ""test_dir""\n        gifs = ""gifs""\n        videos = ""videos""\n        checkpoint = ""checkpoint""\n\n    class Consts:\n        ASSERT_MSG = ""Expected: {}, Actual: {}.""\n        RESULTS_SORTED = ""Results stored at:""\n        TOTAL_RUNTIME = ""Total runtime:""\n        DISCARD_EXP = ""Do you want to discard the experiment results""\n        REACHED_REQ_ASC = ""Reached required success rate. Exiting.""\n        INPUT_EMBEDDER = ""Input Embedder:""\n        MIDDLEWARE = ""Middleware:""\n        OUTPUT_HEAD = ""Output Head:""\n        ONNX_WARNING = ""Exporting ONNX graphs requires setting the "" \\\n                       ""--checkpoint_save_secs flag. The --export_onnx_graph"" \\\n                       "" will have no effect.""\n        COLOR_PREFIX = ""## agent: Starting evaluation phase""\n        TRAINING = ""Training - ""\n        PLAY_WARNING = ""Both the --preset and the --play flags were set. "" \\\n                       ""These flags can not be used together. For human "" \\\n                       ""control, please use the --play flag together with "" \\\n                       ""the environment type flag (-et)""\n        NO_CHECKPOINT = ""No checkpoint to restore in:""\n        LOG_ERROR = ""KeyError:""\n\n        num_hs = 200  # heat-up steps (used for agent custom parameters)\n\n        f_comb = 2  # number of flags in cmd for creating flags combinations\n\n        N_csv_lines = 100  # number of lines to validate on csv file\n\n    class TimeOuts:\n        test_time_limit = 30 * 60\n        wait_for_files = 20\n        wait_for_csv = 240\n        test_run = 60\n'"
rl_coach/tests/utils/presets_utils.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n""""""Manage all preset""""""\n\nimport os\nimport pytest\nfrom importlib import import_module\nfrom rl_coach.tests.utils.definitions import Definitions as Def\n\n\ndef import_preset(preset_name):\n    """"""\n    Import preset name module from presets directory\n    :param preset_name: preset name\n    :return: imported module\n    """"""\n    try:\n        module = import_module(\'{}.presets.{}\'\n                               .format(Def.GROUP_NAME, preset_name))\n    except:\n        pytest.skip(""Can\'t import module: {}"".format(preset_name))\n\n    return module\n\n\ndef validation_params(preset_name):\n    """"""\n    Validate parameters based on preset name\n    :param preset_name: preset name\n    :return: |bool| true if preset has params\n    """"""\n    return import_preset(preset_name).graph_manager.preset_validation_params\n\n\ndef all_presets():\n    """"""\n    Get all preset from preset directory\n    :return: |Array| preset list\n    """"""\n    return [\n        f[:-3] for f in os.listdir(os.path.join(Def.GROUP_NAME, \'presets\'))\n        if f[-3:] == \'.py\' and not f == \'__init__.py\'\n    ]\n\n\ndef importable(preset_name):\n    """"""\n    Try to import preset name\n    :param preset_name: |name| preset name\n    :return: |bool| true if possible to import preset\n    """"""\n    try:\n        import_preset(preset_name)\n        return True\n    except BaseException:\n        return False\n\n\ndef has_test_parameters(preset_name):\n    """"""\n    Check if preset has parameters\n    :param preset_name: |string| preset name\n    :return: |bool| true: if preset have parameters\n    """"""\n    return bool(validation_params(preset_name).test)\n\n\ndef collect_presets():\n    """"""\n    Collect all presets in presets directory\n    :yield: preset name\n    """"""\n    for preset_name in all_presets():\n        # if it isn\'t importable, still include it so we can fail the test\n        if not importable(preset_name):\n            yield preset_name\n        # otherwise, make sure it has test parameters before including it\n        elif has_test_parameters(preset_name):\n            yield preset_name\n'"
rl_coach/tests/utils/test_utils.py,0,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n""""""Common functionality shared across tests.""""""\n\nimport glob\nimport sys\nimport time\nimport os\nfrom os import path\nfrom rl_coach.tests.utils.definitions import Definitions as Def\n\n\ndef print_progress(averaged_rewards, last_num_episodes, start_time, time_limit,\n                   p_valid_params):\n    """"""\n    Print progress bar for preset run test\n    :param averaged_rewards: average rewards of test\n    :param last_num_episodes: last episode number\n    :param start_time: start time of test\n    :param time_limit: time out of test\n    :param p_valid_params: preset validation parameters\n    """"""\n    max_episodes_to_archive = p_valid_params.max_episodes_to_achieve_reward\n    min_reward = p_valid_params.min_reward_threshold\n    avg_reward = round(averaged_rewards[-1], 1)\n    percentage = int((100 * last_num_episodes) / max_episodes_to_archive)\n    cur_time = round(time.time() - start_time, 2)\n\n    sys.stdout.write(""\\rReward: ({}/{})"".format(avg_reward, min_reward))\n    sys.stdout.write(\' Time (sec): ({}/{})\'.format(cur_time, time_limit))\n    sys.stdout.write(\' Episode: ({}/{})\'.format(last_num_episodes,\n                                                max_episodes_to_archive))\n    sys.stdout.write(\' {}%|{}{}|  \'\n                     .format(percentage, \'#\' * int(percentage / 10),\n                             \' \' * (10 - int(percentage / 10))))\n\n    sys.stdout.flush()\n\n\ndef read_csv_paths(test_path, filename_pattern, read_csv_tries=120,\n                   extra_tries=0, num_expected_files=None):\n    """"""\n    Return file path once it found\n    :param test_path: |string| test folder path\n    :param filename_pattern: |string| csv file pattern\n    :param read_csv_tries: |int| number of iterations until file found\n    :param extra_tries: |int| add number of extra tries to check after getting\n                        all the paths.\n    :param num_expected_files: find all expected file in experiment folder.\n    :return: |string| return csv file path\n    """"""\n    csv_paths = []\n    tries_counter = 0\n\n    if isinstance(extra_tries, int) and extra_tries >= 0:\n        read_csv_tries += extra_tries\n\n    while tries_counter < read_csv_tries:\n        csv_paths = glob.glob(path.join(test_path, \'*\', filename_pattern))\n\n        if num_expected_files:\n            if num_expected_files == len(csv_paths):\n                break\n            else:\n                time.sleep(1)\n                tries_counter += 1\n                continue\n        elif csv_paths:\n            break\n\n        time.sleep(1)\n        tries_counter += 1\n\n    return csv_paths\n\n\ndef get_files_from_dir(dir_path):\n    """"""\n    Check if folder has files\n    :param dir_path: |string| folder path\n    :return: |list| return files in folder\n    """"""\n    start_time = time.time()\n    entities = []\n    while time.time() - start_time < Def.TimeOuts.wait_for_files:\n        # wait until logs created\n        if os.path.exists(dir_path):\n            entities = os.listdir(dir_path)\n            if len(entities) > 0:\n                break\n        time.sleep(1)\n\n    assert len(entities) > 0, \\\n        Def.Consts.ASSERT_MSG.format(""num files > 0"", len(entities))\n    return entities\n\n\ndef find_string_in_logs(log_path, str, timeout=Def.TimeOuts.wait_for_files,\n                        wait_and_find=False):\n    """"""\n    Find string into the log file\n    :param log_path: |string| log path\n    :param str: |string| search text\n    :param timeout: |int| timeout for searching on file\n    :param wait_and_find: |bool| true if need to wait until reaching timeout\n    :return: |bool| true if string found in the log file\n    """"""\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        # wait until logs created\n        if os.path.exists(log_path):\n            break\n        time.sleep(1)\n\n    if not os.path.exists(log_path):\n        return False\n\n    while time.time() - start_time < Def.TimeOuts.test_time_limit:\n        with open(log_path, \'r\') as fr:\n            if str in fr.read():\n                return True\n            fr.close()\n\n        if not wait_and_find:\n            break\n\n    return False\n\n\ndef get_csv_path(clres, tries_for_csv=Def.TimeOuts.wait_for_csv,\n                 extra_tries=0, num_expected_files=None):\n    """"""\n    Get the csv path with the results - reading csv paths will take some time\n    :param clres: object of files that test is creating\n    :param tries_for_csv: timeout of tires until getting all csv files\n    :param extra_tries: add number of extra tries to check after getting all\n                        the paths.\n    :param num_expected_files: find all expected file in experiment folder.\n    :return: |list| csv path\n    """"""\n    return read_csv_paths(test_path=clres.exp_path,\n                          filename_pattern=clres.fn_pattern,\n                          read_csv_tries=tries_for_csv,\n                          extra_tries=extra_tries,\n                          num_expected_files=num_expected_files)\n\n'"
rl_coach/architectures/mxnet_components/embedders/__init__.py,0,"b""from .image_embedder import ImageEmbedder\nfrom .tensor_embedder import TensorEmbedder\nfrom .vector_embedder import VectorEmbedder\n\n__all__ = ['ImageEmbedder',\n           'TensorEmbedder',\n           'VectorEmbedder']\n"""
rl_coach/architectures/mxnet_components/embedders/embedder.py,0,"b'from typing import Union\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom mxnet.gluon import nn\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.mxnet_components.layers import convert_layer\nfrom rl_coach.base_parameters import EmbedderScheme\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass InputEmbedder(nn.HybridBlock):\n    def __init__(self, params: InputEmbedderParameters):\n        """"""\n        An input embedder is the first part of the network, which takes the input from the state and produces a vector\n        embedding by passing it through a neural network. The embedder will mostly be input type dependent, and there\n        can be multiple embedders in a single network.\n\n        :param params: parameters object containing input_clipping, input_rescaling, batchnorm, activation_function\n            and dropout properties.\n        """"""\n        super(InputEmbedder, self).__init__()\n        self.embedder_name = params.name\n        self.input_clipping = params.input_clipping\n        self.scheme = params.scheme\n\n        with self.name_scope():\n            self.net = nn.HybridSequential()\n            if isinstance(self.scheme, EmbedderScheme):\n                blocks = self.schemes[self.scheme]\n            else:\n                # if scheme is specified directly, convert to MX layer if it\'s not a callable object\n                # NOTE: if layer object is callable, it must return a gluon block when invoked\n                blocks = [convert_layer(l) for l in self.scheme]\n            for block in blocks:\n                self.net.add(block())\n                if params.batchnorm:\n                    self.net.add(nn.BatchNorm())\n                if params.activation_function:\n                    self.net.add(nn.Activation(params.activation_function))\n                if params.dropout_rate:\n                    self.net.add(nn.Dropout(rate=params.dropout_rate))\n\n    @property\n    def schemes(self) -> dict:\n        """"""\n        Schemes are the pre-defined network architectures of various depths and complexities that can be used for the\n        InputEmbedder. Should be implemented in child classes, and are used to create Block when InputEmbedder is\n        initialised.\n\n        :return: dictionary of schemes, with key of type EmbedderScheme enum and value being list of mxnet.gluon.Block.\n        """"""\n        raise NotImplementedError(""Inheriting embedder must define schemes matching its allowed default ""\n                                  ""configurations."")\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type, *args, **kwargs) -> nd_sym_type:\n        """"""\n        Used for forward pass through embedder network.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param x: environment state, where first dimension is batch_size, then dimensions are data type dependent.\n        :return: embedding of environment state, where shape is (batch_size, channels).\n        """"""\n        # `input_rescaling` and `input_offset` set on inheriting embedder\n        x = x / self.input_rescaling\n        x = x - self.input_offset\n        if self.input_clipping is not None:\n            x.clip(a_min=self.input_clipping[0], a_max=self.input_clipping[1])\n        x = self.net(x)\n        return x.flatten()\n'"
rl_coach/architectures/mxnet_components/embedders/image_embedder.py,0,"b'from typing import Union\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.mxnet_components.embedders.embedder import InputEmbedder\nfrom rl_coach.architectures.mxnet_components.layers import Conv2d\nfrom rl_coach.base_parameters import EmbedderScheme\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass ImageEmbedder(InputEmbedder):\n    def __init__(self, params: InputEmbedderParameters):\n        """"""\n        An image embedder is an input embedder that takes an image input from the state and produces a vector\n        embedding by passing it through a neural network.\n\n        :param params: parameters object containing input_clipping, input_rescaling, batchnorm, activation_function\n            and dropout properties.\n        """"""\n        super(ImageEmbedder, self).__init__(params)\n        self.input_rescaling = params.input_rescaling[\'image\']\n        self.input_offset = params.input_offset[\'image\']\n\n    @property\n    def schemes(self) -> dict:\n        """"""\n        Schemes are the pre-defined network architectures of various depths and complexities that can be used. Are used\n        to create Block when ImageEmbedder is initialised.\n\n        :return: dictionary of schemes, with key of type EmbedderScheme enum and value being list of mxnet.gluon.Block.\n        """"""\n        return {\n            EmbedderScheme.Empty:\n                [],\n\n            EmbedderScheme.Shallow:\n                [\n                    Conv2d(num_filters=32, kernel_size=8, strides=4)\n                ],\n\n            # Use for Atari DQN\n            EmbedderScheme.Medium:\n                [\n                    Conv2d(num_filters=32, kernel_size=8, strides=4),\n                    Conv2d(num_filters=64, kernel_size=4, strides=2),\n                    Conv2d(num_filters=64, kernel_size=3, strides=1)\n                ],\n\n            # Use for Carla\n            EmbedderScheme.Deep:\n                [\n                    Conv2d(num_filters=32, kernel_size=5, strides=2),\n                    Conv2d(num_filters=32, kernel_size=3, strides=1),\n                    Conv2d(num_filters=64, kernel_size=3, strides=2),\n                    Conv2d(num_filters=64, kernel_size=3, strides=1),\n                    Conv2d(num_filters=128, kernel_size=3, strides=2),\n                    Conv2d(num_filters=128, kernel_size=3, strides=1),\n                    Conv2d(num_filters=256, kernel_size=3, strides=2),\n                    Conv2d(num_filters=256, kernel_size=3, strides=1)\n                ]\n        }\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type, *args, **kwargs) -> nd_sym_type:\n        """"""\n        Used for forward pass through embedder network.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param x: image representing environment state, of shape (batch_size, in_channels, height, width).\n        :return: embedding of environment state, of shape (batch_size, channels).\n        """"""\n        # convert from NHWC to NCHW (default for MXNet Convolutions)\n        x = x.transpose((0,3,1,2))\n        return super(ImageEmbedder, self).hybrid_forward(F, x, *args, **kwargs)\n'"
rl_coach/architectures/mxnet_components/embedders/tensor_embedder.py,0,"b'from typing import Union\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.mxnet_components.embedders.embedder import InputEmbedder\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass TensorEmbedder(InputEmbedder):\n    def __init__(self, params: InputEmbedderParameters):\n        """"""\n        A tensor embedder is an input embedder that takes a tensor with arbitrary dimension and produces a vector\n        embedding by passing it through a neural network. An example is video data or 3D image data (i.e. 4D tensors)\n    or other type of data that is more than 1 dimension (i.e. not vector) but is not an image.\n\n        NOTE: There are no pre-defined schemes for tensor embedder. User must define a custom scheme by passing\n        a callable object as InputEmbedderParameters.scheme when defining the respective preset. This callable\n        object must return a Gluon HybridBlock. The hybrid_forward() of this block must accept a single input,\n        normalized observation, and return an embedding vector for each sample in the batch.\n        Keep in mind that the scheme is a list of blocks, which are stacked by optional batchnorm,\n        activation, and dropout in between as specified in InputEmbedderParameters.\n\n        :param params: parameters object containing input_clipping, input_rescaling, batchnorm, activation_function\n            and dropout properties.\n        """"""\n        super(TensorEmbedder, self).__init__(params)\n        self.input_rescaling = params.input_rescaling[\'tensor\']\n        self.input_offset = params.input_offset[\'tensor\']\n\n    @property\n    def schemes(self) -> dict:\n        """"""\n        Schemes are the pre-defined network architectures of various depths and complexities that can be used. Are used\n        to create Block when InputEmbedder is initialised.\n\n        Note: Tensor embedder doesn\'t define any pre-defined scheme. User must provide custom scheme in preset.\n\n        :return: dictionary of schemes, with key of type EmbedderScheme enum and value being list of mxnet.gluon.Block.\n            For tensor embedder, this is an empty dictionary.\n        """"""\n        return {}\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type, *args, **kwargs) -> nd_sym_type:\n        """"""\n        Used for forward pass through embedder network.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param x: image representing environment state, of shape (batch_size, in_channels, height, width).\n        :return: embedding of environment state, of shape (batch_size, channels).\n        """"""\n        return super(TensorEmbedder, self).hybrid_forward(F, x, *args, **kwargs)\n'"
rl_coach/architectures/mxnet_components/embedders/vector_embedder.py,0,"b'from typing import Union\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom mxnet import nd, sym\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.mxnet_components.embedders.embedder import InputEmbedder\nfrom rl_coach.architectures.mxnet_components.layers import Dense\nfrom rl_coach.base_parameters import EmbedderScheme\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass VectorEmbedder(InputEmbedder):\n    def __init__(self, params: InputEmbedderParameters):\n        """"""\n        An vector embedder is an input embedder that takes an vector input from the state and produces a vector\n        embedding by passing it through a neural network.\n\n        :param params: parameters object containing input_clipping, input_rescaling, batchnorm, activation_function\n            and dropout properties.\n        """"""\n        super(VectorEmbedder, self).__init__(params)\n        self.input_rescaling = params.input_rescaling[\'vector\']\n        self.input_offset = params.input_offset[\'vector\']\n\n    @property\n    def schemes(self):\n        """"""\n        Schemes are the pre-defined network architectures of various depths and complexities that can be used. Are used\n        to create Block when VectorEmbedder is initialised.\n\n        :return: dictionary of schemes, with key of type EmbedderScheme enum and value being list of mxnet.gluon.Block.\n        """"""\n        return {\n            EmbedderScheme.Empty:\n                [],\n\n            EmbedderScheme.Shallow:\n                [\n                    Dense(units=128)\n                ],\n\n            # Use for DQN\n            EmbedderScheme.Medium:\n                [\n                    Dense(units=256)\n                ],\n\n            # Use for Carla\n            EmbedderScheme.Deep:\n                [\n                    Dense(units=128),\n                    Dense(units=128),\n                    Dense(units=128)\n                ]\n        }\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type, *args, **kwargs) -> nd_sym_type:\n        """"""\n        Used for forward pass through embedder network.\n\n        :param F: backend api, either `nd` or `sym` (if block has been hybridized).\n        :type F: nd or sym\n        :param x: vector representing environment state, of shape (batch_size, in_channels).\n        :return: embedding of environment state, of shape (batch_size, channels).\n        """"""\n        if isinstance(x, nd.NDArray) and len(x.shape) != 2 and self.scheme != EmbedderScheme.Empty:\n            raise ValueError(""Vector embedders expect the input size to have 2 dimensions. The given size is: {}""\n                             .format(x.shape))\n        return super(VectorEmbedder, self).hybrid_forward(F, x, *args, **kwargs)\n'"
rl_coach/architectures/mxnet_components/heads/__init__.py,0,"b""from .head import Head, HeadLoss\nfrom .q_head import QHead\nfrom .ppo_head import PPOHead\nfrom .ppo_v_head import PPOVHead\nfrom .v_head import VHead\n\n__all__ = [\n    'Head',\n    'HeadLoss',\n    'QHead',\n    'PPOHead',\n    'PPOVHead',\n    'VHead'\n]\n"""
rl_coach/architectures/mxnet_components/heads/head.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom typing import Dict, List, Union, Tuple\n\nimport mxnet as mx\nfrom mxnet.initializer import Initializer, register\nfrom mxnet.gluon import nn, loss\nfrom mxnet.ndarray import NDArray\nfrom mxnet.symbol import Symbol\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.spaces import SpacesDefinition\n\n\nLOSS_OUT_TYPE_LOSS = \'loss\'\nLOSS_OUT_TYPE_REGULARIZATION = \'regularization\'\n\n\n@register\nclass NormalizedRSSInitializer(Initializer):\n    """"""\n    Standardizes Root Sum of Squares along the input channel dimension.\n    Used for Dense layer weight matrices only (ie. do not use on Convolution kernels).\n    MXNet Dense layer weight matrix is of shape (out_ch, in_ch), so standardize across axis 1.\n    Root Sum of Squares set to `rss`, which is 1.0 by default.\n    Called `normalized_columns_initializer` in TensorFlow backend (but we work with rows instead of columns for MXNet).\n    """"""\n    def __init__(self, rss=1.0):\n        super(NormalizedRSSInitializer, self).__init__(rss=rss)\n        self.rss = float(rss)\n\n    def _init_weight(self, name, arr):\n        mx.nd.random.normal(0, 1, out=arr)\n        sample_rss = arr.square().sum(axis=1).sqrt()\n        scalers = self.rss / sample_rss\n        arr *= scalers.expand_dims(1)\n\n\nclass LossInputSchema(object):\n    """"""\n    Helper class to contain schema for loss hybrid_forward input\n    """"""\n    def __init__(self, head_outputs: List[str], agent_inputs: List[str], targets: List[str]):\n        """"""\n        :param head_outputs: list of argument names in hybrid_forward that are outputs of the head.\n            The order and number MUST MATCH the output from the head.\n        :param agent_inputs: list of argument names in hybrid_forward that are inputs from the agent.\n            The order and number MUST MATCH `output_<head_type_idx>_<order>` for this head.\n        :param targets: list of argument names in hybrid_forward that are targets for the loss.\n            The order and number MUST MATCH targets passed from the agent.\n        """"""\n        self._head_outputs = head_outputs\n        self._agent_inputs = agent_inputs\n        self._targets = targets\n\n    @property\n    def head_outputs(self):\n        return self._head_outputs\n\n    @property\n    def agent_inputs(self):\n        return self._agent_inputs\n\n    @property\n    def targets(self):\n        return self._targets\n\n\nclass HeadLoss(loss.Loss):\n    """"""\n    ABC for loss functions of each head. Child class must implement input_schema() and loss_forward()\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(HeadLoss, self).__init__(*args, **kwargs)\n        self._output_schema = None  # type: List[str]\n\n    @property\n    def input_schema(self) -> LossInputSchema:\n        """"""\n        :return: schema for input of hybrid_forward. Read docstring for LossInputSchema for details.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def output_schema(self) -> List[str]:\n        """"""\n        :return: schema for output of hybrid_forward. Must contain \'loss\' and \'regularization\' keys at least once.\n            The order and total number must match that of returned values from the loss. \'loss\' and \'regularization\'\n            are special keys. Any other string is treated as auxiliary outputs and must include match auxiliary\n            fetch names returned by the head.\n        """"""\n        return self._output_schema\n\n    def forward(self, *args):\n        """"""\n        Override forward() so that number of outputs can be checked against the schema\n        """"""\n        outputs = super(HeadLoss, self).forward(*args)\n        if isinstance(outputs, tuple) or isinstance(outputs, list):\n            num_outputs = len(outputs)\n        else:\n            assert isinstance(outputs, NDArray) or isinstance(outputs, Symbol)\n            num_outputs = 1\n        assert num_outputs == len(self.output_schema), ""Number of outputs don\'t match schema ({} != {})"".format(\n            num_outputs, len(self.output_schema))\n        return outputs\n\n    def _loss_output(self, outputs: List[Tuple[Union[NDArray, Symbol], str]]):\n        """"""\n        Must be called on the output from hybrid_forward().\n        Saves the returned output as the schema and returns output values in a list\n        :return: list of output values\n        """"""\n        output_schema = [o[1] for o in outputs]\n        assert self._output_schema is None or self._output_schema == output_schema\n        self._output_schema = output_schema\n        return tuple(o[0] for o in outputs)\n\n    def hybrid_forward(self, F, x, *args, **kwargs):\n        """"""\n        Passes the cal to loss_forward() and constructs output schema from its output by calling loss_output()\n        """"""\n        return self._loss_output(self.loss_forward(F, x, *args, **kwargs))\n\n    def loss_forward(self, F, x, *args, **kwargs) -> List[Tuple[Union[NDArray, Symbol], str]]:\n        """"""\n        Similar to hybrid_forward, but returns list of (NDArray, type_str)\n        """"""\n        raise NotImplementedError\n\n\nclass Head(nn.HybridBlock):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition,\n                 network_name: str, head_type_idx: int=0, loss_weight: float=1., is_local: bool=True,\n                 activation_function: str=\'relu\', dense_layer: None=None):\n        """"""\n        A head is the final part of the network. It takes the embedding from the middleware embedder and passes it\n        through a neural network to produce the output of the network. There can be multiple heads in a network, and\n        each one has an assigned loss function. The heads are algorithm dependent.\n\n        :param agent_parameters: containing algorithm parameters such as clip_likelihood_ratio_using_epsilon\n            and beta_entropy.\n        :param spaces: containing action spaces used for defining size of network output.\n        :param network_name: name of head network. currently unused.\n        :param head_type_idx: index of head network. currently unused.\n        :param loss_weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param is_local: flag to denote if network is local. currently unused.\n        :param activation_function: activation function to use between layers. currently unused.\n        :param dense_layer: type of dense layer to use in network. currently unused.\n        """"""\n        super(Head, self).__init__()\n        self.head_type_idx = head_type_idx\n        self.network_name = network_name\n        self.loss_weight = loss_weight\n        self.is_local = is_local\n        self.ap = agent_parameters\n        self.spaces = spaces\n        self.return_type = None\n        self.activation_function = activation_function\n        self.dense_layer = dense_layer\n        self._num_outputs = None\n\n    def loss(self) -> HeadLoss:\n        """"""\n        Returns loss block to be used for specific head implementation.\n\n        :return: loss block (can be called as function) for outputs returned by the head network.\n        """"""\n        raise NotImplementedError()\n\n    @property\n    def num_outputs(self):\n        """""" Returns number of outputs that forward() call will return\n\n        :return:\n        """"""\n        assert self._num_outputs is not None, \'must call forward() once to configure number of outputs\'\n        return self._num_outputs\n\n    def forward(self, *args):\n        """"""\n        Override forward() so that number of outputs can be automatically set\n        """"""\n        outputs = super(Head, self).forward(*args)\n        if isinstance(outputs, tuple):\n            num_outputs = len(outputs)\n        else:\n            assert isinstance(outputs, NDArray) or isinstance(outputs, Symbol)\n            num_outputs = 1\n        if self._num_outputs is None:\n            self._num_outputs = num_outputs\n        else:\n            assert self._num_outputs == num_outputs, \'Number of outputs cannot change ({} != {})\'.format(\n                self._num_outputs, num_outputs)\n        assert self._num_outputs == len(self.loss().input_schema.head_outputs)\n        return outputs\n\n    def hybrid_forward(self, F, x, *args, **kwargs):\n        """"""\n        Used for forward pass through head network.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param x: middleware state representation, of shape (batch_size, in_channels).\n        :return: final output of network, that will be used in loss calculations.\n        """"""\n        raise NotImplementedError()\n'"
rl_coach/architectures/mxnet_components/heads/ppo_head.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom typing import List, Tuple, Union\nfrom types import ModuleType\n\nimport math\nimport mxnet as mx\nfrom mxnet.gluon import nn\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import ActionProbabilities\nfrom rl_coach.spaces import SpacesDefinition, BoxActionSpace, DiscreteActionSpace\nfrom rl_coach.utils import eps\nfrom rl_coach.architectures.mxnet_components.heads.head import Head, HeadLoss, LossInputSchema,\\\n    NormalizedRSSInitializer\nfrom rl_coach.architectures.mxnet_components.heads.head import LOSS_OUT_TYPE_LOSS, LOSS_OUT_TYPE_REGULARIZATION\nfrom rl_coach.architectures.mxnet_components.utils import hybrid_clip, broadcast_like\n\n\nLOSS_OUT_TYPE_KL = \'kl_divergence\'\nLOSS_OUT_TYPE_ENTROPY = \'entropy\'\nLOSS_OUT_TYPE_LIKELIHOOD_RATIO = \'likelihood_ratio\'\nLOSS_OUT_TYPE_CLIPPED_LIKELIHOOD_RATIO = \'clipped_likelihood_ratio\'\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass MultivariateNormalDist:\n    def __init__(self,\n                 num_var: int,\n                 mean: nd_sym_type,\n                 sigma: nd_sym_type,\n                 F: ModuleType=mx.nd) -> None:\n        """"""\n        Distribution object for Multivariate Normal. Works with batches. \n        Optionally works with batches and time steps, but be consistent in usage: i.e. if using time_step,\n        mean, sigma and data for log_prob must all include a time_step dimension.\n\n        :param num_var: number of variables in distribution\n        :param mean: mean for each variable,\n            of shape (num_var) or\n            of shape (batch_size, num_var) or\n            of shape (batch_size, time_step, num_var).\n        :param sigma: covariance matrix,\n            of shape (num_var, num_var) or\n            of shape (batch_size, num_var, num_var) or\n            of shape (batch_size, time_step, num_var, num_var).\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        """"""\n        self.num_var = num_var\n        self.mean = mean\n        self.sigma = sigma\n        self.F = F\n\n    def inverse_using_cholesky(self, matrix: nd_sym_type) -> nd_sym_type:\n        """"""\n        Calculate inverses for a batch of matrices using Cholesky decomposition method.\n\n        :param matrix: matrix (or matrices) to invert,\n            of shape (num_var, num_var) or\n            of shape (batch_size, num_var, num_var) or\n            of shape (batch_size, time_step, num_var, num_var).\n        :return: inverted matrix (or matrices),\n            of shape (num_var, num_var) or\n            of shape (batch_size, num_var, num_var) or\n            of shape (batch_size, time_step, num_var, num_var).\n        """"""\n        cholesky_factor = self.F.linalg.potrf(matrix)\n        return self.F.linalg.potri(cholesky_factor)\n\n    def log_det(self, matrix: nd_sym_type) -> nd_sym_type:\n        """"""\n        Calculate log of the determinant for a batch of matrices using Cholesky decomposition method.\n\n        :param matrix: matrix (or matrices) to invert,\n            of shape (num_var, num_var) or\n            of shape (batch_size, num_var, num_var) or\n            of shape (batch_size, time_step, num_var, num_var).\n        :return: inverted matrix (or matrices),\n            of shape (num_var, num_var) or\n            of shape (batch_size, num_var, num_var) or\n            of shape (batch_size, time_step, num_var, num_var).\n        """"""\n        cholesky_factor = self.F.linalg.potrf(matrix)\n        return 2 * self.F.linalg.sumlogdiag(cholesky_factor)\n\n    def log_prob(self, x: nd_sym_type) -> nd_sym_type:\n        """"""\n        Calculate the log probability of data given the current distribution.\n\n        See http://www.notenoughthoughts.net/posts/normal-log-likelihood-gradient.html\n        and https://discuss.mxnet.io/t/multivariate-gaussian-log-density-operator/1169/7\n\n        :param x: input data,\n            of shape (num_var) or\n            of shape (batch_size, num_var) or\n            of shape (batch_size, time_step, num_var).\n        :return: log_probability,\n            of shape (1) or\n            of shape (batch_size) or\n            of shape (batch_size, time_step).\n        """"""\n        a = (self.num_var / 2) * math.log(2 * math.pi)\n        log_det_sigma = self.log_det(self.sigma)\n        b = (1 / 2) * log_det_sigma\n        sigma_inv = self.inverse_using_cholesky(self.sigma)\n        # deviation from mean, and dev_t is equivalent to transpose on last two dims.\n        dev = (x - self.mean).expand_dims(-1)\n        dev_t = (x - self.mean).expand_dims(-2)\n\n        # since batch_dot only works with ndarrays with ndim of 3,\n        # and we could have ndarrays with ndim of 4,\n        # we flatten batch_size and time_step into single dim.\n        dev_flat = dev.reshape(shape=(-1, 0, 0), reverse=1)\n        sigma_inv_flat = sigma_inv.reshape(shape=(-1, 0, 0), reverse=1)\n        dev_t_flat = dev_t.reshape(shape=(-1, 0, 0), reverse=1)\n        c = (1 / 2) * self.F.batch_dot(self.F.batch_dot(dev_t_flat, sigma_inv_flat), dev_flat)\n        # and now reshape back to (batch_size, time_step) if required.\n        c = c.reshape_like(b)\n\n        log_likelihood = -a - b - c\n        return log_likelihood\n\n    def entropy(self) -> nd_sym_type:\n        """"""\n        Calculate entropy of current distribution.\n\n        See http://www.nowozin.net/sebastian/blog/the-entropy-of-a-normal-distribution.html\n        :return: entropy,\n            of shape (1) or\n            of shape (batch_size) or\n            of shape (batch_size, time_step).\n        """"""\n        # todo: check if differential entropy is correct\n        log_det_sigma = self.log_det(self.sigma)\n        return (self.num_var / 2) + ((self.num_var / 2) * math.log(2 * math.pi)) + ((1 / 2) * log_det_sigma)\n\n    def kl_div(self, alt_dist) -> nd_sym_type:\n        """"""\n        Calculated KL-Divergence with another MultivariateNormalDist distribution\n        See https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n        Specifically https://wikimedia.org/api/rest_v1/media/math/render/svg/a3bf3b4917bd1fcb8be48d6d6139e2e387bdc7d3\n\n        :param alt_dist: alternative distribution used for kl divergence calculation\n        :type alt_dist: MultivariateNormalDist\n        :return: KL-Divergence, of shape (1,)\n        """"""\n        sigma_a_inv = self.F.linalg.potri(self.F.linalg.potrf(self.sigma))\n        sigma_b_inv = self.F.linalg.potri(self.F.linalg.potrf(alt_dist.sigma))\n        term1a = mx.nd.batch_dot(sigma_b_inv, self.sigma)\n        # sum of diagonal for batch of matrices\n        term1 = (broadcast_like(self.F, self.F.eye(self.num_var), term1a) * term1a).sum(axis=-1).sum(axis=-1)\n        mean_diff = (alt_dist.mean - self.mean).expand_dims(-1)\n        mean_diff_t = (alt_dist.mean - self.mean).expand_dims(-2)\n        term2 = self.F.batch_dot(self.F.batch_dot(mean_diff_t, sigma_b_inv), mean_diff).reshape_like(term1)\n        term3 = (2 * self.F.linalg.sumlogdiag(self.F.linalg.potrf(alt_dist.sigma))) -\\\n                (2 * self.F.linalg.sumlogdiag(self.F.linalg.potrf(self.sigma)))\n        return 0.5 * (term1 + term2 - self.num_var + term3)\n\n\nclass CategoricalDist:\n    def __init__(self, n_classes: int, probs: nd_sym_type, F: ModuleType=mx.nd) -> None:\n        """"""\n        Distribution object for Categorical data.\n        Optionally works with batches and time steps, but be consistent in usage: i.e. if using time_step,\n        mean, sigma and data for log_prob must all include a time_step dimension.\n\n        :param n_classes: number of classes in distribution\n        :param probs: probabilities for each class,\n            of shape (n_classes),\n            of shape (batch_size, n_classes) or\n            of shape (batch_size, time_step, n_classes)\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        """"""\n        self.n_classes = n_classes\n        self.probs = probs\n        self.F = F\n\n\n    def log_prob(self, actions: nd_sym_type) -> nd_sym_type:\n        """"""\n        Calculate the log probability of data given the current distribution.\n\n        :param actions: actions, with int8 data type,\n            of shape (1) if probs was (n_classes),\n            of shape (batch_size) if probs was (batch_size, n_classes) and\n            of shape (batch_size, time_step) if probs was (batch_size, time_step, n_classes)\n        :return: log_probability,\n            of shape (1) if probs was (n_classes),\n            of shape (batch_size) if probs was (batch_size, n_classes) and\n            of shape (batch_size, time_step) if probs was (batch_size, time_step, n_classes)\n        """"""\n        action_mask = actions.one_hot(depth=self.n_classes)\n        action_probs = (self.probs * action_mask).sum(axis=-1)\n        return action_probs.log()\n\n    def entropy(self) -> nd_sym_type:\n        """"""\n        Calculate entropy of current distribution.\n\n        :return: entropy,\n            of shape (1) if probs was (n_classes),\n            of shape (batch_size) if probs was (batch_size, n_classes) and\n            of shape (batch_size, time_step) if probs was (batch_size, time_step, n_classes)\n        """"""\n        # todo: look into numerical stability\n        return -(self.probs.log()*self.probs).sum(axis=-1)\n\n    def kl_div(self, alt_dist) -> nd_sym_type:\n        """"""\n        Calculated KL-Divergence with another Categorical distribution\n\n        :param alt_dist: alternative distribution used for kl divergence calculation\n        :type alt_dist: CategoricalDist\n        :return: KL-Divergence\n        """"""\n        logits_a = self.probs.clip(a_min=eps, a_max=1 - eps).log()\n        logits_b = alt_dist.probs.clip(a_min=eps, a_max=1 - eps).log()\n        t = self.probs * (logits_a - logits_b)\n        t = self.F.where(condition=(alt_dist.probs == 0), x=self.F.ones_like(alt_dist.probs) * math.inf, y=t)\n        t = self.F.where(condition=(self.probs == 0), x=self.F.zeros_like(self.probs), y=t)\n        return t.sum(axis=-1)\n\n\nclass DiscretePPOHead(nn.HybridBlock):\n    def __init__(self, num_actions: int) -> None:\n        """"""\n        Head block for Discrete Proximal Policy Optimization, to calculate probabilities for each action given\n        middleware representation of the environment state.\n\n        :param num_actions: number of actions in action space.\n        """"""\n        super(DiscretePPOHead, self).__init__()\n        with self.name_scope():\n            self.dense = nn.Dense(units=num_actions, flatten=False,\n                                  weight_initializer=NormalizedRSSInitializer(0.01))\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type) -> nd_sym_type:\n        """"""\n        Used for forward pass through head network.\n\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        :param x: middleware state representation,\n            of shape (batch_size, in_channels) or\n            of shape (batch_size, time_step, in_channels).\n        :return: batch of probabilities for each action,\n            of shape (batch_size, num_actions) or\n            of shape (batch_size, time_step, num_actions).\n        """"""\n        policy_values = self.dense(x)\n        policy_probs = F.softmax(policy_values)\n        return policy_probs\n\n\nclass ContinuousPPOHead(nn.HybridBlock):\n    def __init__(self, num_actions: int) -> None:\n        """"""\n        Head block for Continuous Proximal Policy Optimization, to calculate probabilities for each action given\n        middleware representation of the environment state.\n\n        :param num_actions: number of actions in action space.\n        """"""\n        super(ContinuousPPOHead, self).__init__()\n        with self.name_scope():\n            self.dense = nn.Dense(units=num_actions, flatten=False,\n                                  weight_initializer=NormalizedRSSInitializer(0.01))\n            # all samples (across batch, and time step) share the same covariance, which is learnt,\n            # but since we assume the action probability variables are independent,\n            # only the diagonal entries of the covariance matrix are specified.\n            self.log_std = self.params.get(\'log_std\',\n                                           shape=(num_actions,),\n                                           init=mx.init.Zero(),\n                                           allow_deferred_init=True)\n        # todo: is_local?\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type, log_std: nd_sym_type) -> Tuple[nd_sym_type, nd_sym_type]:\n        """"""\n        Used for forward pass through head network.\n\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        :param x: middleware state representation,\n            of shape (batch_size, in_channels) or\n            of shape (batch_size, time_step, in_channels).\n        :return: batch of probabilities for each action,\n            of shape (batch_size, action_mean) or\n            of shape (batch_size, time_step, action_mean).\n        """"""\n        policy_means = self.dense(x)\n        policy_std = broadcast_like(F, log_std.exp().expand_dims(0), policy_means)\n        return policy_means, policy_std\n\n\nclass ClippedPPOLossDiscrete(HeadLoss):\n    def __init__(self,\n                 num_actions: int,\n                 clip_likelihood_ratio_using_epsilon: float,\n                 beta: float=0,\n                 use_kl_regularization: bool=False,\n                 initial_kl_coefficient: float=1,\n                 kl_cutoff: float=0,\n                 high_kl_penalty_coefficient: float=1,\n                 weight: float=1,\n                 batch_axis: int=0) -> None:\n        """"""\n        Loss for discrete version of Clipped PPO.\n\n        :param num_actions: number of actions in action space.\n        :param clip_likelihood_ratio_using_epsilon: epsilon to use for likelihood ratio clipping.\n        :param beta: loss coefficient applied to entropy\n        :param use_kl_regularization: option to add kl divergence loss\n        :param initial_kl_coefficient: loss coefficient applied kl divergence loss (also see high_kl_penalty_coefficient).\n        :param kl_cutoff: threshold for using high_kl_penalty_coefficient\n        :param high_kl_penalty_coefficient: loss coefficient applied to kv divergence above kl_cutoff\n        :param weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param batch_axis: axis used for mini-batch (default is 0) and excluded from loss aggregation.\n        """"""\n        super(ClippedPPOLossDiscrete, self).__init__(weight=weight, batch_axis=batch_axis)\n        self.weight = weight\n        self.num_actions = num_actions\n        self.clip_likelihood_ratio_using_epsilon = clip_likelihood_ratio_using_epsilon\n        self.beta = beta\n        self.use_kl_regularization = use_kl_regularization\n        self.initial_kl_coefficient = initial_kl_coefficient if self.use_kl_regularization else 0.0\n        self.kl_coefficient = self.params.get(\'kl_coefficient\',\n                                              shape=(1,),\n                                              init=mx.init.Constant([initial_kl_coefficient,]),\n                                              differentiable=False)\n        self.kl_cutoff = kl_cutoff\n        self.high_kl_penalty_coefficient = high_kl_penalty_coefficient\n\n    @property\n    def input_schema(self) -> LossInputSchema:\n        return LossInputSchema(\n            head_outputs=[\'new_policy_probs\'],\n            agent_inputs=[\'actions\', \'old_policy_probs\', \'clip_param_rescaler\'],\n            targets=[\'advantages\']\n        )\n\n    def loss_forward(self,\n                     F: ModuleType,\n                     new_policy_probs: nd_sym_type,\n                     actions: nd_sym_type,\n                     old_policy_probs: nd_sym_type,\n                     clip_param_rescaler: nd_sym_type,\n                     advantages: nd_sym_type,\n                     kl_coefficient: nd_sym_type) -> List[Tuple[nd_sym_type, str]]:\n        """"""\n        Used for forward pass through loss computations.\n        Works with batches of data, and optionally time_steps, but be consistent in usage: i.e. if using time_step,\n        new_policy_probs, old_policy_probs, actions and advantages all must include a time_step dimension.\n\n        NOTE: order of input arguments MUST NOT CHANGE because it matches the order\n        parameters are passed in ppo_agent:train_network()\n\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        :param new_policy_probs: action probabilities predicted by DiscretePPOHead network,\n            of shape (batch_size, num_actions) or\n            of shape (batch_size, time_step, num_actions).\n        :param old_policy_probs: action probabilities for previous policy,\n            of shape (batch_size, num_actions) or\n            of shape (batch_size, time_step, num_actions).\n        :param actions: true actions taken during rollout,\n            of shape (batch_size) or\n            of shape (batch_size, time_step).\n        :param clip_param_rescaler: scales epsilon to use for likelihood ratio clipping.\n        :param advantages: change in state value after taking action (a.k.a advantage)\n            of shape (batch_size) or\n            of shape (batch_size, time_step).\n        :param kl_coefficient: loss coefficient applied kl divergence loss (also see high_kl_penalty_coefficient).\n        :return: loss, of shape (batch_size).\n        """"""\n\n        old_policy_dist = CategoricalDist(self.num_actions, old_policy_probs, F=F)\n        action_probs_wrt_old_policy = old_policy_dist.log_prob(actions)\n\n        new_policy_dist = CategoricalDist(self.num_actions, new_policy_probs, F=F)\n        action_probs_wrt_new_policy = new_policy_dist.log_prob(actions)\n\n        entropy_loss = - self.beta * new_policy_dist.entropy().mean()\n\n        if self.use_kl_regularization:\n            kl_div = old_policy_dist.kl_div(new_policy_dist).mean()\n            weighted_kl_div = kl_coefficient * kl_div\n            high_kl_div = F.stack(F.zeros_like(kl_div), kl_div - self.kl_cutoff).max().square()\n            weighted_high_kl_div = self.high_kl_penalty_coefficient * high_kl_div\n            kl_div_loss = weighted_kl_div + weighted_high_kl_div\n        else:\n            kl_div_loss = F.zeros(shape=(1,))\n\n        # working with log probs, so minus first, then exponential (same as division)\n        likelihood_ratio = (action_probs_wrt_new_policy - action_probs_wrt_old_policy).exp()\n\n        if self.clip_likelihood_ratio_using_epsilon is not None:\n            # clipping of likelihood ratio\n            min_value = 1 - self.clip_likelihood_ratio_using_epsilon * clip_param_rescaler\n            max_value = 1 + self.clip_likelihood_ratio_using_epsilon * clip_param_rescaler\n\n            # can\'t use F.clip (with variable clipping bounds), hence custom implementation\n            clipped_likelihood_ratio = hybrid_clip(F, likelihood_ratio, clip_lower=min_value, clip_upper=max_value)\n\n            # lower bound of original, and clipped versions or each scaled advantage\n            # element-wise min between the two ndarrays\n            unclipped_scaled_advantages = likelihood_ratio * advantages\n            clipped_scaled_advantages = clipped_likelihood_ratio * advantages\n            scaled_advantages = F.stack(unclipped_scaled_advantages, clipped_scaled_advantages).min(axis=0)\n        else:\n            scaled_advantages = likelihood_ratio * advantages\n            clipped_likelihood_ratio = F.zeros_like(likelihood_ratio)\n\n        # for each batch, calculate expectation of scaled_advantages across time steps,\n        # but want code to work with data without time step too, so reshape to add timestep if doesn\'t exist.\n        scaled_advantages_w_time = scaled_advantages.reshape(shape=(0, -1))\n        expected_scaled_advantages = scaled_advantages_w_time.mean(axis=1)\n        # want to maximize expected_scaled_advantages, add minus so can minimize.\n        surrogate_loss = (-expected_scaled_advantages * self.weight).mean()\n\n        return [\n            (surrogate_loss, LOSS_OUT_TYPE_LOSS),\n            (entropy_loss + kl_div_loss, LOSS_OUT_TYPE_REGULARIZATION),\n            (kl_div_loss, LOSS_OUT_TYPE_KL),\n            (entropy_loss, LOSS_OUT_TYPE_ENTROPY),\n            (likelihood_ratio, LOSS_OUT_TYPE_LIKELIHOOD_RATIO),\n            (clipped_likelihood_ratio, LOSS_OUT_TYPE_CLIPPED_LIKELIHOOD_RATIO)\n        ]\n\n\nclass ClippedPPOLossContinuous(HeadLoss):\n    def __init__(self,\n                 num_actions: int,\n                 clip_likelihood_ratio_using_epsilon: float,\n                 beta: float=0,\n                 use_kl_regularization: bool=False,\n                 initial_kl_coefficient: float=1,\n                 kl_cutoff: float=0,\n                 high_kl_penalty_coefficient: float=1,\n                 weight: float=1,\n                 batch_axis: int=0):\n        """"""\n        Loss for continuous version of Clipped PPO.\n\n        :param num_actions: number of actions in action space.\n        :param clip_likelihood_ratio_using_epsilon: epsilon to use for likelihood ratio clipping.\n        :param beta: loss coefficient applied to entropy\n        :param batch_axis: axis used for mini-batch (default is 0) and excluded from loss aggregation.\n        :param use_kl_regularization: option to add kl divergence loss\n        :param initial_kl_coefficient: initial loss coefficient applied kl divergence loss (also see high_kl_penalty_coefficient).\n        :param kl_cutoff: threshold for using high_kl_penalty_coefficient\n        :param high_kl_penalty_coefficient: loss coefficient applied to kv divergence above kl_cutoff\n        :param weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param batch_axis: axis used for mini-batch (default is 0) and excluded from loss aggregation.\n        """"""\n        super(ClippedPPOLossContinuous, self).__init__(weight=weight, batch_axis=batch_axis)\n        self.weight = weight\n        self.num_actions = num_actions\n        self.clip_likelihood_ratio_using_epsilon = clip_likelihood_ratio_using_epsilon\n        self.beta = beta\n        self.use_kl_regularization = use_kl_regularization\n        self.initial_kl_coefficient = initial_kl_coefficient if self.use_kl_regularization else 0.0\n        self.kl_coefficient = self.params.get(\'kl_coefficient\',\n                                              shape=(1,),\n                                              init=mx.init.Constant([initial_kl_coefficient,]),\n                                              differentiable=False)\n        self.kl_cutoff = kl_cutoff\n        self.high_kl_penalty_coefficient = high_kl_penalty_coefficient\n\n    @property\n    def input_schema(self) -> LossInputSchema:\n        return LossInputSchema(\n            head_outputs=[\'new_policy_means\',\'new_policy_stds\'],\n            agent_inputs=[\'actions\', \'old_policy_means\', \'old_policy_stds\', \'clip_param_rescaler\'],\n            targets=[\'advantages\']\n        )\n\n    def loss_forward(self,\n                     F: ModuleType,\n                     new_policy_means: nd_sym_type,\n                     new_policy_stds: nd_sym_type,\n                     actions: nd_sym_type,\n                     old_policy_means: nd_sym_type,\n                     old_policy_stds: nd_sym_type,\n                     clip_param_rescaler: nd_sym_type,\n                     advantages: nd_sym_type,\n                     kl_coefficient: nd_sym_type) -> List[Tuple[nd_sym_type, str]]:\n        """"""\n        Used for forward pass through loss computations.\n        Works with batches of data, and optionally time_steps, but be consistent in usage: i.e. if using time_step,\n        new_policy_means, old_policy_means, actions and advantages all must include a time_step dimension.\n\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        :param new_policy_means: action means predicted by MultivariateNormalDist network,\n            of shape (batch_size, num_actions) or\n            of shape (batch_size, time_step, num_actions).\n        :param new_policy_stds: action standard deviation returned by head,\n            of shape (batch_size, num_actions) or\n            of shape (batch_size, time_step, num_actions).\n        :param actions: true actions taken during rollout,\n            of shape (batch_size, num_actions) or\n            of shape (batch_size, time_step, num_actions).\n        :param old_policy_means: action means for previous policy,\n            of shape (batch_size, num_actions) or\n            of shape (batch_size, time_step, num_actions).\n        :param old_policy_stds: action standard deviation returned by head previously,\n            of shape (batch_size, num_actions) or\n            of shape (batch_size, time_step, num_actions).\n        :param clip_param_rescaler: scales epsilon to use for likelihood ratio clipping.\n        :param advantages: change in state value after taking action (a.k.a advantage)\n            of shape (batch_size,) or\n            of shape (batch_size, time_step).\n        :param kl_coefficient: loss coefficient applied kl divergence loss (also see high_kl_penalty_coefficient).\n        :return: loss, of shape (batch_size).\n        """"""\n\n        def diagonal_covariance(stds, size):\n            vars = stds ** 2\n            # sets diagonal in (batch size and time step) covariance matrices\n            vars_tiled = vars.expand_dims(2).tile((1, 1, size))\n            covars = F.broadcast_mul(vars_tiled, F.eye(size))\n            return covars\n\n        old_covar = diagonal_covariance(stds=old_policy_stds, size=self.num_actions)\n        old_policy_dist = MultivariateNormalDist(self.num_actions, old_policy_means, old_covar, F=F)\n        action_probs_wrt_old_policy = old_policy_dist.log_prob(actions)\n\n        new_covar = diagonal_covariance(stds=new_policy_stds, size=self.num_actions)\n        new_policy_dist = MultivariateNormalDist(self.num_actions, new_policy_means, new_covar, F=F)\n        action_probs_wrt_new_policy = new_policy_dist.log_prob(actions)\n\n        entropy_loss = - self.beta * new_policy_dist.entropy().mean()\n\n        if self.use_kl_regularization:\n            kl_div = old_policy_dist.kl_div(new_policy_dist).mean()\n            weighted_kl_div = kl_coefficient * kl_div\n            high_kl_div = F.stack(F.zeros_like(kl_div), kl_div - self.kl_cutoff).max().square()\n            weighted_high_kl_div = self.high_kl_penalty_coefficient * high_kl_div\n            kl_div_loss = weighted_kl_div + weighted_high_kl_div\n        else:\n            kl_div_loss = F.zeros(shape=(1,))\n\n        # working with log probs, so minus first, then exponential (same as division)\n        likelihood_ratio = (action_probs_wrt_new_policy - action_probs_wrt_old_policy).exp()\n\n        if self.clip_likelihood_ratio_using_epsilon is not None:\n            # clipping of likelihood ratio\n            min_value = 1 - self.clip_likelihood_ratio_using_epsilon * clip_param_rescaler\n            max_value = 1 + self.clip_likelihood_ratio_using_epsilon * clip_param_rescaler\n\n            # can\'t use F.clip (with variable clipping bounds), hence custom implementation\n            clipped_likelihood_ratio = hybrid_clip(F, likelihood_ratio, clip_lower=min_value, clip_upper=max_value)\n\n            # lower bound of original, and clipped versions or each scaled advantage\n            # element-wise min between the two ndarrays\n            unclipped_scaled_advantages = likelihood_ratio * advantages\n            clipped_scaled_advantages = clipped_likelihood_ratio * advantages\n            scaled_advantages = F.stack(unclipped_scaled_advantages, clipped_scaled_advantages).min(axis=0)\n        else:\n            scaled_advantages = likelihood_ratio * advantages\n            clipped_likelihood_ratio = F.zeros_like(likelihood_ratio)\n\n        # for each batch, calculate expectation of scaled_advantages across time steps,\n        # but want code to work with data without time step too, so reshape to add timestep if doesn\'t exist.\n        scaled_advantages_w_time = scaled_advantages.reshape(shape=(0, -1))\n        expected_scaled_advantages = scaled_advantages_w_time.mean(axis=1)\n        # want to maximize expected_scaled_advantages, add minus so can minimize.\n        surrogate_loss = (-expected_scaled_advantages * self.weight).mean()\n\n        return [\n            (surrogate_loss, LOSS_OUT_TYPE_LOSS),\n            (entropy_loss + kl_div_loss, LOSS_OUT_TYPE_REGULARIZATION),\n            (kl_div_loss, LOSS_OUT_TYPE_KL),\n            (entropy_loss, LOSS_OUT_TYPE_ENTROPY),\n            (likelihood_ratio, LOSS_OUT_TYPE_LIKELIHOOD_RATIO),\n            (clipped_likelihood_ratio, LOSS_OUT_TYPE_CLIPPED_LIKELIHOOD_RATIO)\n        ]\n\n\nclass PPOHead(Head):\n    def __init__(self,\n                 agent_parameters: AgentParameters,\n                 spaces: SpacesDefinition,\n                 network_name: str,\n                 head_type_idx: int=0,\n                 loss_weight: float=1.,\n                 is_local: bool=True,\n                 activation_function: str=\'tanh\',\n                 dense_layer: None=None) -> None:\n        """"""\n        Head block for Proximal Policy Optimization, to calculate probabilities for each action given middleware\n        representation of the environment state.\n\n        :param agent_parameters: containing algorithm parameters such as clip_likelihood_ratio_using_epsilon\n            and beta_entropy.\n        :param spaces: containing action spaces used for defining size of network output.\n        :param network_name: name of head network. currently unused.\n        :param head_type_idx: index of head network. currently unused.\n        :param loss_weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param is_local: flag to denote if network is local. currently unused.\n        :param activation_function: activation function to use between layers. currently unused.\n        :param dense_layer: type of dense layer to use in network. currently unused.\n        """"""\n        super().__init__(agent_parameters, spaces, network_name, head_type_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.return_type = ActionProbabilities\n\n        self.clip_likelihood_ratio_using_epsilon = agent_parameters.algorithm.clip_likelihood_ratio_using_epsilon\n        self.beta = agent_parameters.algorithm.beta_entropy\n        self.use_kl_regularization = agent_parameters.algorithm.use_kl_regularization\n        if self.use_kl_regularization:\n            self.initial_kl_coefficient = agent_parameters.algorithm.initial_kl_coefficient\n            self.kl_cutoff = 2 * agent_parameters.algorithm.target_kl_divergence\n            self.high_kl_penalty_coefficient = agent_parameters.algorithm.high_kl_penalty_coefficient\n        else:\n            self.initial_kl_coefficient, self.kl_cutoff, self.high_kl_penalty_coefficient = (None, None, None)\n        self._loss = []\n\n        if isinstance(self.spaces.action, DiscreteActionSpace):\n            self.net = DiscretePPOHead(num_actions=len(self.spaces.action.actions))\n        elif isinstance(self.spaces.action, BoxActionSpace):\n            self.net = ContinuousPPOHead(num_actions=self.spaces.action.shape[0])\n        else:\n            raise ValueError(""Only discrete or continuous action spaces are supported for PPO."")\n\n    def hybrid_forward(self,\n                       F: ModuleType,\n                       x: nd_sym_type) -> nd_sym_type:\n        """"""\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        :param x: middleware embedding\n        :return: policy parameters/probabilities\n        """"""\n        return self.net(x)\n\n    def loss(self) -> mx.gluon.loss.Loss:\n        """"""\n        Specifies loss block to be used for this policy head.\n\n        :return: loss block (can be called as function) for action probabilities returned by this policy network.\n        """"""\n        if isinstance(self.spaces.action, DiscreteActionSpace):\n            loss = ClippedPPOLossDiscrete(len(self.spaces.action.actions),\n                                          self.clip_likelihood_ratio_using_epsilon,\n                                          self.beta,\n                                          self.use_kl_regularization, self.initial_kl_coefficient,\n                                          self.kl_cutoff, self.high_kl_penalty_coefficient,\n                                          self.loss_weight)\n        elif isinstance(self.spaces.action, BoxActionSpace):\n            loss = ClippedPPOLossContinuous(self.spaces.action.shape[0],\n                                            self.clip_likelihood_ratio_using_epsilon,\n                                            self.beta,\n                                            self.use_kl_regularization, self.initial_kl_coefficient,\n                                            self.kl_cutoff, self.high_kl_penalty_coefficient,\n                                            self.loss_weight)\n        else:\n            raise ValueError(""Only discrete or continuous action spaces are supported for PPO."")\n        loss.initialize()\n        # set a property so can assign_kl_coefficient in future,\n        # make a list, otherwise it would be added as a child of Head Block (due to type check)\n        self._loss = [loss]\n        return loss\n\n    @property\n    def kl_divergence(self):\n        return self.head_type_idx, LOSS_OUT_TYPE_KL\n\n    @property\n    def entropy(self):\n        return self.head_type_idx, LOSS_OUT_TYPE_ENTROPY\n\n    @property\n    def likelihood_ratio(self):\n        return self.head_type_idx, LOSS_OUT_TYPE_LIKELIHOOD_RATIO\n\n    @property\n    def clipped_likelihood_ratio(self):\n        return self.head_type_idx, LOSS_OUT_TYPE_CLIPPED_LIKELIHOOD_RATIO\n\n    def assign_kl_coefficient(self, kl_coefficient: float) -> None:\n        self._loss[0].kl_coefficient.set_data(mx.nd.array((kl_coefficient,)))'"
rl_coach/architectures/mxnet_components/heads/ppo_v_head.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom typing import List, Tuple, Union\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom mxnet.gluon import nn\nfrom rl_coach.architectures.mxnet_components.heads.head import Head, HeadLoss, LossInputSchema,\\\n    NormalizedRSSInitializer\nfrom rl_coach.architectures.mxnet_components.heads.head import LOSS_OUT_TYPE_LOSS\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import ActionProbabilities\nfrom rl_coach.spaces import SpacesDefinition\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass PPOVHeadLoss(HeadLoss):\n    def __init__(self, clip_likelihood_ratio_using_epsilon: float, weight: float=1, batch_axis: int=0) -> None:\n        """"""\n        Loss for PPO Value network.\n        Schulman implemented this extension in OpenAI baselines for PPO2\n        See https://github.com/openai/baselines/blob/master/baselines/ppo2/ppo2.py#L72\n\n        :param clip_likelihood_ratio_using_epsilon: epsilon to use for likelihood ratio clipping.\n        :param weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param batch_axis: axis used for mini-batch (default is 0) and excluded from loss aggregation.\n        """"""\n        super(PPOVHeadLoss, self).__init__(weight=weight, batch_axis=batch_axis)\n        self.weight = weight\n        self.clip_likelihood_ratio_using_epsilon = clip_likelihood_ratio_using_epsilon\n\n    @property\n    def input_schema(self) -> LossInputSchema:\n        return LossInputSchema(\n            head_outputs=[\'new_policy_values\'],\n            agent_inputs=[\'old_policy_values\'],\n            targets=[\'target_values\']\n        )\n\n    def loss_forward(self,\n                     F: ModuleType,\n                     new_policy_values: nd_sym_type,\n                     old_policy_values: nd_sym_type,\n                     target_values: nd_sym_type) -> List[Tuple[nd_sym_type, str]]:\n        """"""\n        Used for forward pass through loss computations.\n        Calculates two losses (L2 and a clipped difference L2 loss) and takes the maximum of the two.\n        Works with batches of data, and optionally time_steps, but be consistent in usage: i.e. if using time_step,\n        new_policy_values, old_policy_values and target_values all must include a time_step dimension.\n\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        :param new_policy_values: values predicted by PPOVHead network,\n            of shape (batch_size) or\n            of shape (batch_size, time_step).\n        :param old_policy_values: values predicted by old value network,\n            of shape (batch_size) or\n            of shape (batch_size, time_step).\n        :param target_values: actual state values,\n            of shape (batch_size) or\n            of shape (batch_size, time_step).\n        :return: loss, of shape (batch_size).\n        """"""\n        # L2 loss\n        value_loss_1 = (new_policy_values - target_values).square()\n        # Clipped difference L2 loss\n        diff = new_policy_values - old_policy_values\n        clipped_diff = diff.clip(a_min=-self.clip_likelihood_ratio_using_epsilon,\n                                 a_max=self.clip_likelihood_ratio_using_epsilon)\n        value_loss_2 = (old_policy_values + clipped_diff - target_values).square()\n        # Maximum of the two losses, element-wise maximum.\n        value_loss_max = mx.nd.stack(value_loss_1, value_loss_2).max(axis=0)\n        # Aggregate over temporal axis, adding if doesn\'t exist (hense reshape)\n        value_loss_max_w_time = value_loss_max.reshape(shape=(0, -1))\n        value_loss = value_loss_max_w_time.mean(axis=1)\n        # Weight the loss (and average over samples of batch)\n        value_loss_weighted = value_loss.mean() * self.weight\n        return [(value_loss_weighted, LOSS_OUT_TYPE_LOSS)]\n\n\nclass PPOVHead(Head):\n    def __init__(self,\n                 agent_parameters: AgentParameters,\n                 spaces: SpacesDefinition,\n                 network_name: str,\n                 head_type_idx: int=0,\n                 loss_weight: float=1.,\n                 is_local: bool = True,\n                 activation_function: str=\'relu\',\n                 dense_layer: None=None) -> None:\n        """"""\n        PPO Value Head for predicting state values.\n\n        :param agent_parameters: containing algorithm parameters, but currently unused.\n        :param spaces: containing action spaces, but currently unused.\n        :param network_name: name of head network. currently unused.\n        :param head_type_idx: index of head network. currently unused.\n        :param loss_weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param is_local: flag to denote if network is local. currently unused.\n        :param activation_function: activation function to use between layers. currently unused.\n        :param dense_layer: type of dense layer to use in network. currently unused.\n        """"""\n        super(PPOVHead, self).__init__(agent_parameters, spaces, network_name, head_type_idx, loss_weight, is_local,\n                                       activation_function, dense_layer=dense_layer)\n        self.clip_likelihood_ratio_using_epsilon = agent_parameters.algorithm.clip_likelihood_ratio_using_epsilon\n        self.return_type = ActionProbabilities\n        with self.name_scope():\n            self.dense = nn.Dense(units=1, weight_initializer=NormalizedRSSInitializer(1.0))\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type) -> nd_sym_type:\n        """"""\n        Used for forward pass through value head network.\n\n        :param (mx.nd or mx.sym) F: backend api (mx.sym if block has been hybridized).\n        :param x: middleware state representation, of shape (batch_size, in_channels).\n        :return: final value output of network, of shape (batch_size).\n        """"""\n        return self.dense(x).squeeze(axis=1)\n\n    def loss(self) -> mx.gluon.loss.Loss:\n        """"""\n        Specifies loss block to be used for specific value head implementation.\n\n        :return: loss block (can be called as function) for outputs returned by the value head network.\n        """"""\n        return PPOVHeadLoss(self.clip_likelihood_ratio_using_epsilon, weight=self.loss_weight)\n'"
rl_coach/architectures/mxnet_components/heads/q_head.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom typing import Union, List, Tuple\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom mxnet.gluon.loss import Loss, HuberLoss, L2Loss\nfrom mxnet.gluon import nn\nfrom rl_coach.architectures.mxnet_components.heads.head import Head, HeadLoss, LossInputSchema\nfrom rl_coach.architectures.mxnet_components.heads.head import LOSS_OUT_TYPE_LOSS\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import QActionStateValue\nfrom rl_coach.spaces import SpacesDefinition, BoxActionSpace, DiscreteActionSpace\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass QHeadLoss(HeadLoss):\n    def __init__(self, loss_type: Loss=L2Loss, weight: float=1, batch_axis: int=0) -> None:\n        """"""\n        Loss for Q-Value Head.\n\n        :param loss_type: loss function with default of mean squared error (i.e. L2Loss).\n        :param weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param batch_axis: axis used for mini-batch (default is 0) and excluded from loss aggregation.\n        """"""\n        super(QHeadLoss, self).__init__(weight=weight, batch_axis=batch_axis)\n        with self.name_scope():\n            self.loss_fn = loss_type(weight=weight, batch_axis=batch_axis)\n\n    @property\n    def input_schema(self) -> LossInputSchema:\n        return LossInputSchema(\n            head_outputs=[\'pred\'],\n            agent_inputs=[],\n            targets=[\'target\']\n        )\n\n    def loss_forward(self,\n                     F: ModuleType,\n                     pred: nd_sym_type,\n                     target: nd_sym_type) -> List[Tuple[nd_sym_type, str]]:\n        """"""\n        Used for forward pass through loss computations.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param pred: state-action q-values predicted by QHead network, of shape (batch_size, num_actions).\n        :param target: actual state-action q-values, of shape (batch_size, num_actions).\n        :return: loss, of shape (batch_size).\n        """"""\n        loss = self.loss_fn(pred, target).mean()\n        return [(loss, LOSS_OUT_TYPE_LOSS)]\n\n\nclass QHead(Head):\n    def __init__(self,\n                 agent_parameters: AgentParameters,\n                 spaces: SpacesDefinition,\n                 network_name: str,\n                 head_type_idx: int=0,\n                 loss_weight: float=1.,\n                 is_local: bool=True,\n                 activation_function: str=\'relu\',\n                 dense_layer: None=None,\n                 loss_type: Union[HuberLoss, L2Loss]=L2Loss) -> None:\n        """"""\n        Q-Value Head for predicting state-action Q-Values.\n\n        :param agent_parameters: containing algorithm parameters, but currently unused.\n        :param spaces: containing action spaces used for defining size of network output.\n        :param network_name: name of head network. currently unused.\n        :param head_type_idx: index of head network. currently unused.\n        :param loss_weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param is_local: flag to denote if network is local. currently unused.\n        :param activation_function: activation function to use between layers. currently unused.\n        :param dense_layer: type of dense layer to use in network. currently unused.\n        :param loss_type: loss function to use.\n        """"""\n        super(QHead, self).__init__(agent_parameters, spaces, network_name, head_type_idx, loss_weight,\n                                    is_local, activation_function, dense_layer)\n        if isinstance(self.spaces.action, BoxActionSpace):\n            self.num_actions = 1\n        elif isinstance(self.spaces.action, DiscreteActionSpace):\n            self.num_actions = len(self.spaces.action.actions)\n        self.return_type = QActionStateValue\n        assert (loss_type == L2Loss) or (loss_type == HuberLoss), ""Only expecting L2Loss or HuberLoss.""\n        self.loss_type = loss_type\n\n        with self.name_scope():\n            self.dense = nn.Dense(units=self.num_actions)\n\n    def loss(self) -> Loss:\n        """"""\n        Specifies loss block to be used for specific value head implementation.\n\n        :return: loss block (can be called as function) for outputs returned by the head network.\n        """"""\n        return QHeadLoss(loss_type=self.loss_type, weight=self.loss_weight)\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type) -> nd_sym_type:\n        """"""\n        Used for forward pass through Q-Value head network.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param x: middleware state representation, of shape (batch_size, in_channels).\n        :return: predicted state-action q-values, of shape (batch_size, num_actions).\n        """"""\n        return self.dense(x)\n'"
rl_coach/architectures/mxnet_components/heads/v_head.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union, List, Tuple\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom mxnet.gluon.loss import Loss, HuberLoss, L2Loss\nfrom mxnet.gluon import nn\nfrom rl_coach.architectures.mxnet_components.heads.head import Head, HeadLoss, LossInputSchema,\\\n    NormalizedRSSInitializer\nfrom rl_coach.architectures.mxnet_components.heads.head import LOSS_OUT_TYPE_LOSS\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import VStateValue\nfrom rl_coach.spaces import SpacesDefinition\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass VHeadLoss(HeadLoss):\n    def __init__(self, loss_type: Loss=L2Loss, weight: float=1, batch_axis: int=0) -> None:\n        """"""\n        Loss for Value Head.\n\n        :param loss_type: loss function with default of mean squared error (i.e. L2Loss).\n        :param weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param batch_axis: axis used for mini-batch (default is 0) and excluded from loss aggregation.\n        """"""\n        super(VHeadLoss, self).__init__(weight=weight, batch_axis=batch_axis)\n        with self.name_scope():\n            self.loss_fn = loss_type(weight=weight, batch_axis=batch_axis)\n\n    @property\n    def input_schema(self) -> LossInputSchema:\n        return LossInputSchema(\n            head_outputs=[\'pred\'],\n            agent_inputs=[],\n            targets=[\'target\']\n        )\n\n    def loss_forward(self,\n                     F: ModuleType,\n                     pred: nd_sym_type,\n                     target: nd_sym_type) -> List[Tuple[nd_sym_type, str]]:\n        """"""\n        Used for forward pass through loss computations.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param pred: state values predicted by VHead network, of shape (batch_size).\n        :param target: actual state values, of shape (batch_size).\n        :return: loss, of shape (batch_size).\n        """"""\n        loss = self.loss_fn(pred, target).mean()\n        return [(loss, LOSS_OUT_TYPE_LOSS)]\n\n\nclass VHead(Head):\n    def __init__(self,\n                 agent_parameters: AgentParameters,\n                 spaces: SpacesDefinition,\n                 network_name: str,\n                 head_type_idx: int=0,\n                 loss_weight: float=1.,\n                 is_local: bool=True,\n                 activation_function: str=\'relu\',\n                 dense_layer: None=None,\n                 loss_type: Union[HuberLoss, L2Loss]=L2Loss):\n        """"""\n        Value Head for predicting state values.\n        :param agent_parameters: containing algorithm parameters, but currently unused.\n        :param spaces: containing action spaces, but currently unused.\n        :param network_name: name of head network. currently unused.\n        :param head_type_idx: index of head network. currently unused.\n        :param loss_weight: scalar used to adjust relative weight of loss (if using this loss with others).\n        :param is_local: flag to denote if network is local. currently unused.\n        :param activation_function: activation function to use between layers. currently unused.\n        :param dense_layer: type of dense layer to use in network. currently unused.\n        :param loss_type: loss function with default of mean squared error (i.e. L2Loss), or alternatively HuberLoss.\n        """"""\n        super(VHead, self).__init__(agent_parameters, spaces, network_name, head_type_idx, loss_weight,\n                                    is_local, activation_function, dense_layer)\n        assert (loss_type == L2Loss) or (loss_type == HuberLoss), ""Only expecting L2Loss or HuberLoss.""\n        self.loss_type = loss_type\n        self.return_type = VStateValue\n        with self.name_scope():\n            self.dense = nn.Dense(units=1, weight_initializer=NormalizedRSSInitializer(1.0))\n\n    def loss(self) -> Loss:\n        """"""\n        Specifies loss block to be used for specific value head implementation.\n\n        :return: loss block (can be called as function) for outputs returned by the head network.\n        """"""\n        return VHeadLoss(loss_type=self.loss_type, weight=self.loss_weight)\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type) -> nd_sym_type:\n        """"""\n        Used for forward pass through value head network.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param x: middleware state representation, of shape (batch_size, in_channels).\n        :return: final output of value network, of shape (batch_size).\n        """"""\n        return self.dense(x).squeeze(axis=1)\n'"
rl_coach/architectures/mxnet_components/middlewares/__init__.py,0,"b'from .fc_middleware import FCMiddleware\nfrom .lstm_middleware import LSTMMiddleware\n\n__all__ = [""FCMiddleware"", ""LSTMMiddleware""]'"
rl_coach/architectures/mxnet_components/middlewares/fc_middleware.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n""""""\nModule that defines the fully-connected middleware class\n""""""\n\nfrom rl_coach.architectures.mxnet_components.layers import Dense\nfrom rl_coach.architectures.mxnet_components.middlewares.middleware import Middleware\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.base_parameters import MiddlewareScheme\n\n\nclass FCMiddleware(Middleware):\n    def __init__(self, params: FCMiddlewareParameters):\n        """"""\n        FCMiddleware or Fully-Connected Middleware can be used in the middle part of the network. It takes the\n        embeddings from the input embedders, after they were aggregated in some method (for example, concatenation)\n        and passes it through a neural network  which can be customizable but shared between the heads of the network.\n\n        :param params: parameters object containing batchnorm, activation_function and dropout properties.\n        """"""\n        super(FCMiddleware, self).__init__(params)\n\n    @property\n    def schemes(self) -> dict:\n        """"""\n        Schemes are the pre-defined network architectures of various depths and complexities that can be used for the\n        Middleware. Are used to create Block when FCMiddleware is initialised.\n\n        :return: dictionary of schemes, with key of type MiddlewareScheme enum and value being list of mxnet.gluon.Block.\n        """"""\n        return {\n            MiddlewareScheme.Empty:\n                [],\n\n            # Use for PPO\n            MiddlewareScheme.Shallow:\n                [\n                    Dense(units=64)\n                ],\n\n            # Use for DQN\n            MiddlewareScheme.Medium:\n                [\n                    Dense(units=512)\n                ],\n\n            MiddlewareScheme.Deep:\n                [\n                    Dense(units=128),\n                    Dense(units=128),\n                    Dense(units=128)\n                ]\n        }\n'"
rl_coach/architectures/mxnet_components/middlewares/lstm_middleware.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n""""""\nModule that defines the LSTM middleware class\n""""""\n\nfrom typing import Union\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom mxnet.gluon import rnn\nfrom rl_coach.architectures.mxnet_components.layers import Dense\nfrom rl_coach.architectures.mxnet_components.middlewares.middleware import Middleware\nfrom rl_coach.architectures.middleware_parameters import LSTMMiddlewareParameters\nfrom rl_coach.base_parameters import MiddlewareScheme\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass LSTMMiddleware(Middleware):\n    def __init__(self, params: LSTMMiddlewareParameters):\n        """"""\n        LSTMMiddleware or Long Short Term Memory Middleware can be used in the middle part of the network. It takes the\n        embeddings from the input embedders, after they were aggregated in some method (for example, concatenation)\n        and passes it through a neural network  which can be customizable but shared between the heads of the network.\n\n        :param params: parameters object containing batchnorm, activation_function, dropout and\n            number_of_lstm_cells properties.\n        """"""\n        super(LSTMMiddleware, self).__init__(params)\n        self.number_of_lstm_cells = params.number_of_lstm_cells\n        with self.name_scope():\n            self.lstm = rnn.LSTM(hidden_size=self.number_of_lstm_cells)\n\n    @property\n    def schemes(self) -> dict:\n        """"""\n        Schemes are the pre-defined network architectures of various depths and complexities that can be used for the\n        Middleware. Are used to create Block when LSTMMiddleware is initialised, and are applied before the LSTM.\n\n        :return: dictionary of schemes, with key of type MiddlewareScheme enum and value being list of mxnet.gluon.Block.\n        """"""\n        return {\n            MiddlewareScheme.Empty:\n                [],\n\n            # Use for PPO\n            MiddlewareScheme.Shallow:\n                [\n                    Dense(units=64)\n                ],\n\n            # Use for DQN\n            MiddlewareScheme.Medium:\n                [\n                    Dense(units=512)\n                ],\n\n            MiddlewareScheme.Deep:\n                [\n                    Dense(units=128),\n                    Dense(units=128),\n                    Dense(units=128)\n                ]\n        }\n\n    def hybrid_forward(self,\n                       F: ModuleType,\n                       x: nd_sym_type,\n                       *args, **kwargs) -> nd_sym_type:\n        """"""\n        Used for forward pass through LSTM middleware network.\n        Applies dense layers from selected scheme before passing result to LSTM layer.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param x: state embedding, of shape (batch_size, in_channels).\n        :return: state middleware embedding, where shape is (batch_size, channels).\n        """"""\n        x_ntc = x.reshape(shape=(0, 0, -1))\n        emb_ntc = super(LSTMMiddleware, self).hybrid_forward(F, x_ntc, *args, **kwargs)\n        emb_tnc = emb_ntc.transpose(axes=(1, 0, 2))\n        return self.lstm(emb_tnc)\n'"
rl_coach/architectures/mxnet_components/middlewares/middleware.py,0,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Union\nfrom types import ModuleType\n\nimport mxnet as mx\nfrom mxnet.gluon import nn\nfrom rl_coach.architectures.middleware_parameters import MiddlewareParameters\nfrom rl_coach.architectures.mxnet_components.layers import convert_layer\nfrom rl_coach.base_parameters import MiddlewareScheme\n\nnd_sym_type = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\nclass Middleware(nn.HybridBlock):\n    def __init__(self, params: MiddlewareParameters):\n        """"""\n        Middleware is the middle part of the network. It takes the embeddings from the input embedders,\n        after they were aggregated in some method (for example, concatenation) and passes it through a neural network\n        which can be customizable but shared between the heads of the network.\n\n        :param params: parameters object containing batchnorm, activation_function and dropout properties.\n        """"""\n        super(Middleware, self).__init__()\n        self.scheme = params.scheme\n\n        with self.name_scope():\n            self.net = nn.HybridSequential()\n            if isinstance(self.scheme, MiddlewareScheme):\n                blocks = self.schemes[self.scheme]\n            else:\n                # if scheme is specified directly, convert to MX layer if it\'s not a callable object\n                # NOTE: if layer object is callable, it must return a gluon block when invoked\n                blocks = [convert_layer(l) for l in self.scheme]\n            for block in blocks:\n                self.net.add(block())\n                if params.batchnorm:\n                    self.net.add(nn.BatchNorm())\n                if params.activation_function:\n                    self.net.add(nn.Activation(params.activation_function))\n                if params.dropout_rate:\n                    self.net.add(nn.Dropout(rate=params.dropout_rate))\n\n    @property\n    def schemes(self) -> dict:\n        """"""\n        Schemes are the pre-defined network architectures of various depths and complexities that can be used for the\n        Middleware. Should be implemented in child classes, and are used to create Block when Middleware is initialised.\n\n        :return: dictionary of schemes, with key of type MiddlewareScheme enum and value being list of mxnet.gluon.Block.\n        """"""\n        raise NotImplementedError(""Inheriting embedder must define schemes matching its allowed default ""\n                                  ""configurations."")\n\n    def hybrid_forward(self, F: ModuleType, x: nd_sym_type, *args, **kwargs) -> nd_sym_type:\n        """"""\n        Used for forward pass through middleware network.\n\n        :param F: backend api, either `mxnet.nd` or `mxnet.sym` (if block has been hybridized).\n        :param x: state embedding, of shape (batch_size, in_channels).\n        :return: state middleware embedding, where shape is (batch_size, channels).\n        """"""\n        return self.net(x)\n'"
rl_coach/architectures/tensorflow_components/embedders/__init__.py,0,"b""from .image_embedder import ImageEmbedder\nfrom .vector_embedder import VectorEmbedder\nfrom .tensor_embedder import TensorEmbedder\n\n__all__ = ['ImageEmbedder', 'VectorEmbedder', 'TensorEmbedder']\n"""
rl_coach/architectures/tensorflow_components/embedders/embedder.py,6,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List, Union, Tuple\nimport copy\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import BatchnormActivationDropout, convert_layer, Dense\nfrom rl_coach.base_parameters import EmbedderScheme, NetworkComponentParameters\n\nfrom rl_coach.core_types import InputEmbedding\nfrom rl_coach.utils import force_list\n\n\nclass InputEmbedder(object):\n    """"""\n    An input embedder is the first part of the network, which takes the input from the state and produces a vector\n    embedding by passing it through a neural network. The embedder will mostly be input type dependent, and there\n    can be multiple embedders in a single network\n    """"""\n    def __init__(self, input_size: List[int], activation_function=tf.nn.relu,\n                 scheme: EmbedderScheme=None, batchnorm: bool=False, dropout_rate: float=0.0,\n                 name: str= ""embedder"", input_rescaling=1.0, input_offset=0.0, input_clipping=None, dense_layer=Dense,\n                 is_training=False):\n        self.name = name\n        self.input_size = input_size\n        self.activation_function = activation_function\n        self.batchnorm = batchnorm\n        self.dropout_rate = dropout_rate\n        self.input = None\n        self.output = None\n        self.scheme = scheme\n        self.return_type = InputEmbedding\n        self.layers_params = []\n        self.layers = []\n        self.input_rescaling = input_rescaling\n        self.input_offset = input_offset\n        self.input_clipping = input_clipping\n        self.dense_layer = dense_layer\n        if self.dense_layer is None:\n            self.dense_layer = Dense\n        self.is_training = is_training\n\n        # layers order is conv -> batchnorm -> activation -> dropout\n        if isinstance(self.scheme, EmbedderScheme):\n            self.layers_params = copy.copy(self.schemes[self.scheme])\n            self.layers_params = [convert_layer(l) for l in self.layers_params]\n        else:\n            # if scheme is specified directly, convert to TF layer if it\'s not a callable object\n            # NOTE: if layer object is callable, it must return a TF tensor when invoked\n            self.layers_params = [convert_layer(l) for l in copy.copy(self.scheme)]\n\n        # we allow adding batchnorm, dropout or activation functions after each layer.\n        # The motivation is to simplify the transition between a network with batchnorm and a network without\n        # batchnorm to a single flag (the same applies to activation function and dropout)\n        if self.batchnorm or self.activation_function or self.dropout_rate > 0:\n            for layer_idx in reversed(range(len(self.layers_params))):\n                self.layers_params.insert(layer_idx+1,\n                                          BatchnormActivationDropout(batchnorm=self.batchnorm,\n                                                                     activation_function=self.activation_function,\n                                                                     dropout_rate=self.dropout_rate))\n\n    def __call__(self, prev_input_placeholder: tf.placeholder=None) -> Tuple[tf.Tensor, tf.Tensor]:\n        """"""\n        Wrapper for building the module graph including scoping and loss creation\n        :param prev_input_placeholder: the input to the graph\n        :return: the input placeholder and the output of the last layer\n        """"""\n        with tf.variable_scope(self.get_name()):\n            if prev_input_placeholder is None:\n                self.input = tf.placeholder(""float"", shape=[None] + self.input_size, name=self.get_name())\n            else:\n                self.input = prev_input_placeholder\n            self._build_module()\n\n        return self.input, self.output\n\n    def _build_module(self) -> None:\n        """"""\n        Builds the graph of the module\n        This method is called early on from __call__. It is expected to store the graph\n        in self.output.\n        :return: None\n        """"""\n        # NOTE: for image inputs, we expect the data format to be of type uint8, so to be memory efficient. we chose not\n        #  to implement the rescaling as an input filters.observation.observation_filter, as this would have caused the\n        #  input to the network to be float, which is 4x more expensive in memory.\n        #  thus causing each saved transition in the memory to also be 4x more pricier.\n\n        input_layer = self.input / self.input_rescaling\n        input_layer -= self.input_offset\n        # clip input using te given range\n        if self.input_clipping is not None:\n            input_layer = tf.clip_by_value(input_layer, self.input_clipping[0], self.input_clipping[1])\n\n        self.layers.append(input_layer)\n\n        for idx, layer_params in enumerate(self.layers_params):\n            self.layers.extend(force_list(\n                layer_params(input_layer=self.layers[-1], name=\'{}_{}\'.format(layer_params.__class__.__name__, idx),\n                             is_training=self.is_training)\n            ))\n\n        self.output = tf.contrib.layers.flatten(self.layers[-1])\n\n    @property\n    def input_size(self) -> List[int]:\n        return self._input_size\n\n    @input_size.setter\n    def input_size(self, value: Union[int, List[int]]):\n        if isinstance(value, np.ndarray) or isinstance(value, tuple):\n            value = list(value)\n        elif isinstance(value, int):\n            value = [value]\n        if not isinstance(value, list):\n            raise ValueError((\n                \'input_size expected to be a list, found {value} which has type {type}\'\n            ).format(value=value, type=type(value)))\n        self._input_size = value\n\n    @property\n    def schemes(self):\n        raise NotImplementedError(""Inheriting embedder must define schemes matching its allowed default ""\n                                  ""configurations."")\n\n    def get_name(self) -> str:\n        """"""\n        Get a formatted name for the module\n        :return: the formatted name\n        """"""\n        return self.name\n\n    def __str__(self):\n        result = [\'Input size = {}\'.format(self._input_size)]\n        if self.input_rescaling != 1.0 or self.input_offset != 0.0:\n            result.append(\'Input Normalization (scale = {}, offset = {})\'.format(self.input_rescaling, self.input_offset))\n        result.extend([str(l) for l in self.layers_params])\n        if not self.layers_params:\n            result.append(\'No layers\')\n\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/embedders/image_embedder.py,1,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Conv2d, Dense\nfrom rl_coach.architectures.tensorflow_components.embedders.embedder import InputEmbedder\nfrom rl_coach.base_parameters import EmbedderScheme\nfrom rl_coach.core_types import InputImageEmbedding\n\n\nclass ImageEmbedder(InputEmbedder):\n    """"""\n    An input embedder that performs convolutions on the input and then flattens the result.\n    The embedder is intended for image like inputs, where the channels are expected to be the last axis.\n    The embedder also allows custom rescaling of the input prior to the neural network.\n    """"""\n\n    def __init__(self, input_size: List[int], activation_function=tf.nn.relu,\n                 scheme: EmbedderScheme=EmbedderScheme.Medium, batchnorm: bool=False, dropout_rate: float=0.0,\n                 name: str= ""embedder"", input_rescaling: float=255.0, input_offset: float=0.0, input_clipping=None,\n                 dense_layer=Dense, is_training=False):\n        super().__init__(input_size, activation_function, scheme, batchnorm, dropout_rate, name, input_rescaling,\n                         input_offset, input_clipping, dense_layer=dense_layer, is_training=is_training)\n        self.return_type = InputImageEmbedding\n        if len(input_size) != 3 and scheme != EmbedderScheme.Empty:\n            raise ValueError(""Image embedders expect the input size to have 3 dimensions. The given size is: {}""\n                             .format(input_size))\n\n    @property\n    def schemes(self):\n        return {\n            EmbedderScheme.Empty:\n                [],\n\n            EmbedderScheme.Shallow:\n                [\n                    Conv2d(32, 3, 1)\n                ],\n\n            # atari dqn\n            EmbedderScheme.Medium:\n                [\n                    Conv2d(32, 8, 4),\n                    Conv2d(64, 4, 2),\n                    Conv2d(64, 3, 1)\n                ],\n\n            # carla\n            EmbedderScheme.Deep: \\\n                [\n                    Conv2d(32, 5, 2),\n                    Conv2d(32, 3, 1),\n                    Conv2d(64, 3, 2),\n                    Conv2d(64, 3, 1),\n                    Conv2d(128, 3, 2),\n                    Conv2d(128, 3, 1),\n                    Conv2d(256, 3, 2),\n                    Conv2d(256, 3, 1)\n                ]\n        }\n\n\n'"
rl_coach/architectures/tensorflow_components/embedders/tensor_embedder.py,1,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Conv2d, Dense\nfrom rl_coach.architectures.tensorflow_components.embedders.embedder import InputEmbedder\nfrom rl_coach.base_parameters import EmbedderScheme\nfrom rl_coach.core_types import InputTensorEmbedding\n\n\nclass TensorEmbedder(InputEmbedder):\n    """"""\n    A tensor embedder is an input embedder that takes a tensor with arbitrary dimension and produces a vector\n    embedding by passing it through a neural network. An example is video data or 3D image data (i.e. 4D tensors)\n    or other type of data that is more than 1 dimension (i.e. not vector) but is not an image.\n\n    NOTE: There are no pre-defined schemes for tensor embedder. User must define a custom scheme by passing\n    a callable object as InputEmbedderParameters.scheme when defining the respective preset. This callable\n    object must accept a single input, the normalized observation, and return a Tensorflow symbol which\n    will calculate an embedding vector for each sample in the batch.\n    Keep in mind that the scheme is a list of Tensorflow symbols, which are stacked by optional batchnorm,\n    activation, and dropout in between as specified in InputEmbedderParameters.\n    """"""\n\n    def __init__(self, input_size: List[int], activation_function=tf.nn.relu,\n                 scheme: EmbedderScheme=None, batchnorm: bool=False, dropout_rate: float=0.0,\n                 name: str= ""embedder"", input_rescaling: float=1.0, input_offset: float=0.0, input_clipping=None,\n                 dense_layer=Dense, is_training=False):\n        super().__init__(input_size, activation_function, scheme, batchnorm, dropout_rate, name, input_rescaling,\n                         input_offset, input_clipping, dense_layer=dense_layer, is_training=is_training)\n        self.return_type = InputTensorEmbedding\n        assert scheme is not None, ""Custom scheme (a list of callables) must be specified for TensorEmbedder""\n\n    @property\n    def schemes(self):\n        return {}\n'"
rl_coach/architectures/tensorflow_components/embedders/vector_embedder.py,1,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import List\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.embedders.embedder import InputEmbedder\nfrom rl_coach.base_parameters import EmbedderScheme\nfrom rl_coach.core_types import InputVectorEmbedding\n\n\nclass VectorEmbedder(InputEmbedder):\n    """"""\n    An input embedder that is intended for inputs that can be represented as vectors.\n    The embedder flattens the input, applies several dense layers to it and returns the output.\n    """"""\n\n    def __init__(self, input_size: List[int], activation_function=tf.nn.relu,\n                 scheme: EmbedderScheme=EmbedderScheme.Medium, batchnorm: bool=False, dropout_rate: float=0.0,\n                 name: str= ""embedder"", input_rescaling: float=1.0, input_offset: float=0.0, input_clipping=None,\n                 dense_layer=Dense, is_training=False):\n        super().__init__(input_size, activation_function, scheme, batchnorm, dropout_rate, name,\n                         input_rescaling, input_offset, input_clipping, dense_layer=dense_layer,\n                         is_training=is_training)\n\n        self.return_type = InputVectorEmbedding\n        if len(self.input_size) != 1 and scheme != EmbedderScheme.Empty:\n            raise ValueError(""The input size of a vector embedder must contain only a single dimension"")\n\n    @property\n    def schemes(self):\n        return {\n            EmbedderScheme.Empty:\n                [],\n\n            EmbedderScheme.Shallow:\n                [\n                    self.dense_layer(128)\n                ],\n\n            # dqn\n            EmbedderScheme.Medium:\n                [\n                    self.dense_layer(256)\n                ],\n\n            # carla\n            EmbedderScheme.Deep: \\\n                [\n                    self.dense_layer(128),\n                    self.dense_layer(128),\n                    self.dense_layer(128)\n                ]\n        }\n'"
rl_coach/architectures/tensorflow_components/heads/__init__.py,0,"b""from .q_head import QHead\nfrom .categorical_q_head import CategoricalQHead\nfrom .ddpg_actor_head import DDPGActor\nfrom .dnd_q_head import DNDQHead\nfrom .dueling_q_head import DuelingQHead\nfrom .measurements_prediction_head import MeasurementsPredictionHead\nfrom .naf_head import NAFHead\nfrom .policy_head import PolicyHead\nfrom .ppo_head import PPOHead\nfrom .ppo_v_head import PPOVHead\nfrom .quantile_regression_q_head import QuantileRegressionQHead\nfrom .rainbow_q_head import RainbowQHead\nfrom .v_head import VHead\nfrom .acer_policy_head import ACERPolicyHead\nfrom .sac_head import SACPolicyHead\nfrom .sac_q_head import SACQHead\nfrom .classification_head import ClassificationHead\nfrom .cil_head import RegressionHead\nfrom .td3_v_head import TD3VHead\nfrom .ddpg_v_head import DDPGVHead\nfrom .wolpertinger_actor_head import WolpertingerActorHead\n\n__all__ = [\n    'CategoricalQHead',\n    'DDPGActor',\n    'DNDQHead',\n    'DuelingQHead',\n    'MeasurementsPredictionHead',\n    'NAFHead',\n    'PolicyHead',\n    'PPOHead',\n    'PPOVHead',\n    'QHead',\n    'QuantileRegressionQHead',\n    'RainbowQHead',\n    'VHead',\n    'ACERPolicyHead',\n    'SACPolicyHead',\n    'SACQHead',\n    'ClassificationHead',\n    'RegressionHead',\n    'TD3VHead',\n    'DDPGVHead',\n    'WolpertingerActorHead'\n]\n"""
rl_coach/architectures/tensorflow_components/heads/acer_policy_head.py,33,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import ActionProbabilities\nfrom rl_coach.spaces import DiscreteActionSpace\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import eps\n\n\nclass ACERPolicyHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'acer_policy_head\'\n        self.return_type = ActionProbabilities\n        self.beta = None\n        self.action_penalty = None\n\n        # a scalar weight that penalizes low entropy values to encourage exploration\n        if hasattr(agent_parameters.algorithm, \'beta_entropy\'):\n            # we set the beta value as a tf variable so it can be updated later if needed\n            self.beta = tf.Variable(float(agent_parameters.algorithm.beta_entropy),\n                                    trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n            self.beta_placeholder = tf.placeholder(\'float\')\n            self.set_beta = tf.assign(self.beta, self.beta_placeholder)\n\n    def _build_module(self, input_layer):\n        if isinstance(self.spaces.action, DiscreteActionSpace):\n            # create a discrete action network (softmax probabilities output)\n            self._build_discrete_net(input_layer, self.spaces.action)\n        else:\n            raise ValueError(""only discrete action spaces are supported for ACER"")\n\n        if self.is_local:\n            # add entropy regularization\n            if self.beta:\n                self.entropy = tf.reduce_mean(self.policy_distribution.entropy())\n                self.regularizations += [-tf.multiply(self.beta, self.entropy, name=\'entropy_regularization\')]\n\n            # Truncated importance sampling with bias corrections\n            importance_sampling_weight = tf.placeholder(tf.float32, [None, self.num_actions],\n                                                        name=\'{}_importance_sampling_weight\'.format(self.get_name()))\n            self.input.append(importance_sampling_weight)\n            importance_sampling_weight_i = tf.placeholder(tf.float32, [None],\n                                                          name=\'{}_importance_sampling_weight_i\'.format(self.get_name()))\n            self.input.append(importance_sampling_weight_i)\n\n            V_values = tf.placeholder(tf.float32, [None], name=\'{}_V_values\'.format(self.get_name()))\n            self.target.append(V_values)\n            Q_values = tf.placeholder(tf.float32, [None, self.num_actions], name=\'{}_Q_values\'.format(self.get_name()))\n            self.input.append(Q_values)\n            Q_retrace = tf.placeholder(tf.float32, [None], name=\'{}_Q_retrace\'.format(self.get_name()))\n            self.input.append(Q_retrace)\n\n            action_log_probs_wrt_policy = self.policy_distribution.log_prob(self.actions)\n            self.probability_loss = -tf.reduce_mean(action_log_probs_wrt_policy\n                                                    * (Q_retrace - V_values)\n                                                    * tf.minimum(self.ap.algorithm.importance_weight_truncation,\n                                                                 importance_sampling_weight_i))\n\n            log_probs_wrt_policy = tf.log(self.policy_probs + eps)\n            bias_correction_gain = tf.reduce_sum(log_probs_wrt_policy\n                                                 * (Q_values - tf.expand_dims(V_values, 1))\n                                                 * tf.nn.relu(1.0 - (self.ap.algorithm.importance_weight_truncation\n                                                                     / (importance_sampling_weight + eps)))\n                                                 * tf.stop_gradient(self.policy_probs),\n                                                 axis=1)\n            self.bias_correction_loss = -tf.reduce_mean(bias_correction_gain)\n\n            self.loss = self.probability_loss + self.bias_correction_loss\n            tf.losses.add_loss(self.loss)\n\n            # Trust region\n            batch_size = tf.to_float(tf.shape(input_layer)[0])\n            average_policy = tf.placeholder(tf.float32, [None, self.num_actions],\n                                            name=\'{}_average_policy\'.format(self.get_name()))\n            self.input.append(average_policy)\n            average_policy_distribution = tf.contrib.distributions.Categorical(probs=(average_policy + eps))\n            self.kl_divergence = tf.reduce_mean(tf.distributions.kl_divergence(average_policy_distribution,\n                                                                               self.policy_distribution))\n            if self.ap.algorithm.use_trust_region_optimization:\n                @tf.custom_gradient\n                def trust_region_layer(x):\n                    def grad(g):\n                        g = - g * batch_size\n                        k = - average_policy / (self.policy_probs + eps)\n                        adj = tf.nn.relu(\n                            (tf.reduce_sum(k * g, axis=1) - self.ap.algorithm.max_KL_divergence)\n                            / (tf.reduce_sum(tf.square(k), axis=1) + eps))\n                        g = g - tf.expand_dims(adj, 1) * k\n                        return - g / batch_size\n                    return tf.identity(x), grad\n                self.output = trust_region_layer(self.output)\n\n    def _build_discrete_net(self, input_layer, action_space):\n        self.num_actions = len(action_space.actions)\n        self.actions = tf.placeholder(tf.int32, [None], name=\'{}_actions\'.format(self.get_name()))\n        self.input.append(self.actions)\n\n        policy_values = self.dense_layer(self.num_actions)(input_layer, name=\'fc\')\n        self.policy_probs = tf.nn.softmax(policy_values, name=\'{}_policy\'.format(self.get_name()))\n\n        # (the + eps is to prevent probability 0 which will cause the log later on to be -inf)\n        self.policy_distribution = tf.contrib.distributions.Categorical(probs=(self.policy_probs + eps))\n        self.output = self.policy_probs\n'"
rl_coach/architectures/tensorflow_components/heads/categorical_q_head.py,8,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\nimport numpy as np\nfrom rl_coach.architectures.tensorflow_components.heads import QHead\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\n\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass CategoricalQHead(QHead):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str =\'relu\',\n                 dense_layer=Dense, output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer, output_bias_initializer=output_bias_initializer)\n        self.name = \'categorical_dqn_head\'\n        self.num_actions = len(self.spaces.action.actions)\n        self.num_atoms = agent_parameters.algorithm.atoms\n        self.z_values = tf.cast(tf.constant(np.linspace(self.ap.algorithm.v_min, self.ap.algorithm.v_max,\n                                                        self.ap.algorithm.atoms), dtype=tf.float32), dtype=tf.float64)\n        self.loss_type = []\n\n    def _build_module(self, input_layer):\n        values_distribution = self.dense_layer(self.num_actions * self.num_atoms)\\\n            (input_layer, name=\'output\', bias_initializer=self.output_bias_initializer)\n        values_distribution = tf.reshape(values_distribution, (tf.shape(values_distribution)[0], self.num_actions,\n                                                               self.num_atoms))\n        # softmax on atoms dimension\n        self.output = tf.nn.softmax(values_distribution)\n\n        # calculate cross entropy loss\n        self.distributions = tf.placeholder(tf.float32, shape=(None, self.num_actions, self.num_atoms),\n                                            name=""distributions"")\n        self.target = self.distributions\n        self.loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.target, logits=values_distribution)\n        tf.losses.add_loss(self.loss)\n\n        self.q_values = tf.tensordot(tf.cast(self.output, tf.float64), self.z_values, 1)\n\n        # used in batch-rl to estimate a probablity distribution over actions\n        self.softmax = self.add_softmax_with_temperature()\n\n    def __str__(self):\n        result = [\n            ""Dense (num outputs = {})"".format(self.num_actions * self.num_atoms),\n            ""Reshape (output size = {} x {})"".format(self.num_actions, self.num_atoms),\n            ""Softmax""\n        ]\n        return \'\\n\'.join(result)\n\n'"
rl_coach/architectures/tensorflow_components/heads/cil_head.py,2,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import QActionStateValue\nfrom rl_coach.spaces import SpacesDefinition, BoxActionSpace, DiscreteActionSpace\nfrom rl_coach.utils import force_list\n\n\nclass RegressionHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense, scheme=[Dense(256), Dense(256)], output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'regression_head\'\n        self.scheme = scheme\n        self.layers = []\n        if isinstance(self.spaces.action, BoxActionSpace):\n            self.num_actions = self.spaces.action.shape[0]\n        elif isinstance(self.spaces.action, DiscreteActionSpace):\n            self.num_actions = len(self.spaces.action.actions)\n        self.return_type = QActionStateValue\n        if agent_parameters.network_wrappers[self.network_name].replace_mse_with_huber_loss:\n            self.loss_type = tf.losses.huber_loss\n        else:\n            self.loss_type = tf.losses.mean_squared_error\n        self.output_bias_initializer = output_bias_initializer\n\n    def _build_module(self, input_layer):\n        self.layers.append(input_layer)\n        for idx, layer_params in enumerate(self.scheme):\n            self.layers.extend(force_list(\n                layer_params(input_layer=self.layers[-1], name=\'{}_{}\'.format(layer_params.__class__.__name__, idx))\n            ))\n\n        self.layers.append(self.dense_layer(self.num_actions)(self.layers[-1], name=\'output\',\n                                                              bias_initializer=self.output_bias_initializer))\n        self.output = self.layers[-1]\n\n    def __str__(self):\n        result = []\n        for layer in self.layers:\n            result.append(str(layer))\n        return \'\\n\'.join(result)\n\n'"
rl_coach/architectures/tensorflow_components/heads/classification_head.py,4,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.spaces import SpacesDefinition, BoxActionSpace, DiscreteActionSpace\n\n\nclass ClassificationHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'classification_head\'\n        if isinstance(self.spaces.action, BoxActionSpace):\n            self.num_actions = 1\n        elif isinstance(self.spaces.action, DiscreteActionSpace):\n            self.num_actions = len(self.spaces.action.actions)\n        else:\n            raise ValueError(\n                \'ClassificationHead does not support action spaces of type: {class_name}\'.format(\n                    class_name=self.spaces.action.__class__.__name__,\n                )\n            )\n\n    def _build_module(self, input_layer):\n        # Standard classification Network\n        self.class_values = self.output = self.dense_layer(self.num_actions)(input_layer, name=\'output\')\n\n        self.output = tf.nn.softmax(self.class_values)\n\n        # calculate cross entropy loss\n        self.target = tf.placeholder(tf.float32, shape=(None, self.num_actions), name=""target"")\n        self.loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.target, logits=self.class_values)\n        tf.losses.add_loss(self.loss)\n\n    def __str__(self):\n        result = [\n            ""Dense (num outputs = {})"".format(self.num_actions)\n        ]\n        return \'\\n\'.join(result)\n\n\n'"
rl_coach/architectures/tensorflow_components/heads/ddpg_actor_head.py,2,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import batchnorm_activation_dropout, Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import ActionProbabilities\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass DDPGActor(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'tanh\',\n                 batchnorm: bool=True, dense_layer=Dense, is_training=False):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer, is_training=is_training)\n        self.name = \'ddpg_actor_head\'\n        self.return_type = ActionProbabilities\n\n        self.num_actions = self.spaces.action.shape\n\n        self.batchnorm = batchnorm\n\n        # bounded actions\n        self.output_scale = self.spaces.action.max_abs_range\n\n        # a scalar weight that penalizes high activation values (before the activation function) for the final layer\n        if hasattr(agent_parameters.algorithm, \'action_penalty\'):\n            self.action_penalty = agent_parameters.algorithm.action_penalty\n\n    def _build_module(self, input_layer):\n        # mean\n        pre_activation_policy_values_mean = self.dense_layer(self.num_actions)(input_layer, name=\'fc_mean\')\n        policy_values_mean = batchnorm_activation_dropout(input_layer=pre_activation_policy_values_mean,\n                                                          batchnorm=self.batchnorm,\n                                                          activation_function=self.activation_function,\n                                                          dropout_rate=0,\n                                                          is_training=self.is_training,\n                                                          name=""BatchnormActivationDropout_0"")[-1]\n        self.policy_mean = tf.multiply(policy_values_mean, self.output_scale, name=\'output_mean\')\n\n        if self.is_local:\n            # add a squared penalty on the squared pre-activation features of the action\n            if self.action_penalty and self.action_penalty != 0:\n                self.regularizations += \\\n                    [self.action_penalty * tf.reduce_mean(tf.square(pre_activation_policy_values_mean))]\n\n        self.output = [self.policy_mean]\n\n    def __str__(self):\n        result = [\n            \'Dense (num outputs = {})\'.format(self.num_actions[0])\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/ddpg_v_head.py,1,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.heads import VHead\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass DDPGVHead(VHead):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense, initializer=\'normalized_columns\', output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer, initializer=initializer,\n                         output_bias_initializer=output_bias_initializer)\n\n    def _build_module(self, input_layer):\n        super()._build_module(input_layer)\n        self.output = [self.output, tf.reduce_mean(self.output)]\n\n    def __str__(self):\n        result = [\n            ""Dense (num outputs = 1)""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/dnd_q_head.py,11,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.q_head import QHead\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.memories.non_episodic import differentiable_neural_dictionary\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass DNDQHead(QHead):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'dnd_q_values_head\'\n        self.DND_size = agent_parameters.algorithm.dnd_size\n        self.DND_key_error_threshold = agent_parameters.algorithm.DND_key_error_threshold\n        self.l2_norm_added_delta = agent_parameters.algorithm.l2_norm_added_delta\n        self.new_value_shift_coefficient = agent_parameters.algorithm.new_value_shift_coefficient\n        self.number_of_nn = agent_parameters.algorithm.number_of_knn\n        self.ap = agent_parameters\n        self.dnd_embeddings = [None] * self.num_actions\n        self.dnd_values = [None] * self.num_actions\n        self.dnd_indices = [None] * self.num_actions\n        self.dnd_distances = [None] * self.num_actions\n        if self.ap.memory.shared_memory:\n            self.shared_memory_scratchpad = self.ap.task_parameters.shared_memory_scratchpad\n\n    def _build_module(self, input_layer):\n        if hasattr(self.ap.task_parameters, \'checkpoint_restore_path\') and\\\n                self.ap.task_parameters.checkpoint_restore_path:\n            self.DND = differentiable_neural_dictionary.load_dnd(self.ap.task_parameters.checkpoint_restore_path)\n        else:\n            self.DND = differentiable_neural_dictionary.QDND(\n                self.DND_size, input_layer.get_shape()[-1], self.num_actions, self.new_value_shift_coefficient,\n                key_error_threshold=self.DND_key_error_threshold,\n                learning_rate=self.network_parameters.learning_rate,\n                num_neighbors=self.number_of_nn,\n                override_existing_keys=True)\n\n        # Retrieve info from DND dictionary\n        # We assume that all actions have enough entries in the DND\n        self.q_values = self.output = tf.transpose([\n            self._q_value(input_layer, action)\n            for action in range(self.num_actions)\n        ])\n\n        # used in batch-rl to estimate a probablity distribution over actions\n        self.softmax = self.add_softmax_with_temperature()\n\n    def _q_value(self, input_layer, action):\n        result = tf.py_func(self.DND.query,\n                            [input_layer, action, self.number_of_nn],\n                            [tf.float64, tf.float64, tf.int64])\n        self.dnd_embeddings[action] = tf.to_float(result[0])\n        self.dnd_values[action] = tf.to_float(result[1])\n        self.dnd_indices[action] = result[2]\n\n        # DND calculation\n        square_diff = tf.square(self.dnd_embeddings[action] - tf.expand_dims(input_layer, 1))\n        distances = tf.reduce_sum(square_diff, axis=2) + [self.l2_norm_added_delta]\n        self.dnd_distances[action] = distances\n        weights = 1.0 / distances\n        normalised_weights = weights / tf.reduce_sum(weights, axis=1, keep_dims=True)\n        q_value = tf.reduce_sum(self.dnd_values[action] * normalised_weights, axis=1)\n        q_value.set_shape((None,))\n        return q_value\n\n    def _post_build(self):\n        # DND gradients\n        self.dnd_embeddings_grad = tf.gradients(self.loss[0], self.dnd_embeddings)\n        self.dnd_values_grad = tf.gradients(self.loss[0], self.dnd_values)\n\n    def __str__(self):\n        result = [\n            ""DND fetch (num outputs = {})"".format(self.num_actions)\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/dueling_q_head.py,4,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.q_head import QHead\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass DuelingQHead(QHead):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'dueling_q_values_head\'\n\n    def _build_module(self, input_layer):\n        # state value tower - V\n        with tf.variable_scope(""state_value""):\n            self.state_value = self.dense_layer(512)(input_layer, activation=self.activation_function, name=\'fc1\')\n            self.state_value = self.dense_layer(1)(self.state_value, name=\'fc2\')\n\n        # action advantage tower - A\n        with tf.variable_scope(""action_advantage""):\n            self.action_advantage = self.dense_layer(512)(input_layer, activation=self.activation_function, name=\'fc1\')\n            self.action_advantage = self.dense_layer(self.num_actions)(self.action_advantage, name=\'fc2\')\n            self.action_mean = tf.reduce_mean(self.action_advantage, axis=1, keepdims=True)\n            self.action_advantage = self.action_advantage - self.action_mean\n\n        # merge to state-action value function Q\n        self.q_values = self.output = tf.add(self.state_value, self.action_advantage, name=\'output\')\n\n        # used in batch-rl to estimate a probablity distribution over actions\n        self.softmax = self.add_softmax_with_temperature()\n\n    def __str__(self):\n        result = [\n            ""State Value Stream - V"",\n            ""\\tDense (num outputs = 512)"",\n            ""\\tDense (num outputs = 1)"",\n            ""Action Advantage Stream - A"",\n            ""\\tDense (num outputs = 512)"",\n            ""\\tDense (num outputs = {})"".format(self.num_actions),\n            ""\\tSubtract(A, Mean(A))"".format(self.num_actions),\n            ""Add (V, A)""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/head.py,10,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.losses.losses_impl import Reduction\nfrom rl_coach.architectures.tensorflow_components.layers import Dense, convert_layer_class\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import force_list\nfrom rl_coach.architectures.tensorflow_components.utils import squeeze_tensor\n\n# Used to initialize weights for policy and value output layers\ndef normalized_columns_initializer(std=1.0):\n    def _initializer(shape, dtype=None, partition_info=None):\n        out = np.random.randn(*shape).astype(np.float32)\n        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n        return tf.constant(out)\n    return _initializer\n\n\nclass Head(object):\n    """"""\n    A head is the final part of the network. It takes the embedding from the middleware embedder and passes it through\n    a neural network to produce the output of the network. There can be multiple heads in a network, and each one has\n    an assigned loss function. The heads are algorithm dependent.\n    """"""\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int=0, loss_weight: float=1., is_local: bool=True, activation_function: str=\'relu\',\n                 dense_layer=Dense, is_training=False):\n        self.head_idx = head_idx\n        self.network_name = network_name\n        self.network_parameters = agent_parameters.network_wrappers[self.network_name]\n        self.name = ""head""\n        self.output = []\n        self.loss = []\n        self.loss_type = []\n        self.regularizations = []\n        self.loss_weight = tf.Variable([float(w) for w in force_list(loss_weight)],\n                                       trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n        self.target = []\n        self.importance_weight = []\n        self.input = []\n        self.is_local = is_local\n        self.ap = agent_parameters\n        self.spaces = spaces\n        self.return_type = None\n        self.activation_function = activation_function\n        self.dense_layer = dense_layer\n        if self.dense_layer is None:\n            self.dense_layer = Dense\n        else:\n            self.dense_layer = convert_layer_class(self.dense_layer)\n        self.is_training = is_training\n\n    def __call__(self, input_layer):\n        """"""\n        Wrapper for building the module graph including scoping and loss creation\n        :param input_layer: the input to the graph\n        :return: the output of the last layer and the target placeholder\n        """"""\n\n        with tf.variable_scope(self.get_name(), initializer=tf.contrib.layers.xavier_initializer()):\n            self._build_module(squeeze_tensor(input_layer))\n\n            self.output = force_list(self.output)\n            self.target = force_list(self.target)\n            self.input = force_list(self.input)\n            self.loss_type = force_list(self.loss_type)\n            self.loss = force_list(self.loss)\n            self.regularizations = force_list(self.regularizations)\n            if self.is_local:\n                self.set_loss()\n            self._post_build()\n\n        if self.is_local:\n            return self.output, self.target, self.input, self.importance_weight\n        else:\n            return self.output, self.input\n\n    def _build_module(self, input_layer):\n        """"""\n        Builds the graph of the module\n        This method is called early on from __call__. It is expected to store the graph\n        in self.output.\n        :param input_layer: the input to the graph\n        :return: None\n        """"""\n        pass\n\n    def _post_build(self):\n        """"""\n        Optional function that allows adding any extra definitions after the head has been fully defined\n        For example, this allows doing additional calculations that are based on the loss\n        :return: None\n        """"""\n        pass\n\n    def get_name(self):\n        """"""\n        Get a formatted name for the module\n        :return: the formatted name\n        """"""\n        return \'{}_{}\'.format(self.name, self.head_idx)\n\n    def set_loss(self):\n        """"""\n        Creates a target placeholder and loss function for each loss_type and regularization\n        :param loss_type: a tensorflow loss function\n        :param scope: the name scope to include the tensors in\n        :return: None\n        """"""\n\n        # there are heads that define the loss internally, but we need to create additional placeholders for them\n        for idx in range(len(self.loss)):\n            importance_weight = tf.placeholder(\'float\',\n                                               [None] + [1] * (len(self.target[idx].shape) - 1),\n                                               \'{}_importance_weight\'.format(self.get_name()))\n            self.importance_weight.append(importance_weight)\n\n        # add losses and target placeholder\n        for idx in range(len(self.loss_type)):\n            # create target placeholder\n            target = tf.placeholder(\'float\', self.output[idx].shape, \'{}_target\'.format(self.get_name()))\n            self.target.append(target)\n\n            # create importance sampling weights placeholder\n            num_target_dims = len(self.target[idx].shape)\n            importance_weight = tf.placeholder(\'float\', [None] + [1] * (num_target_dims - 1),\n                                               \'{}_importance_weight\'.format(self.get_name()))\n            self.importance_weight.append(importance_weight)\n\n            # compute the weighted loss. importance_weight weights over the samples in the batch, while self.loss_weight\n            # weights the specific loss of this head against other losses in this head or in other heads\n            loss_weight = self.loss_weight[idx]*importance_weight\n            loss = self.loss_type[idx](self.target[-1], self.output[idx],\n                                       scope=self.get_name(), reduction=Reduction.NONE, loss_collection=None)\n\n            # the loss is first summed over each sample in the batch and then the mean over the batch is taken\n            loss = tf.reduce_mean(loss_weight*tf.reduce_sum(loss, axis=list(range(1, num_target_dims))))\n\n            # we add the loss to the losses collection and later we will extract it in general_network\n            tf.losses.add_loss(loss)\n            self.loss.append(loss)\n\n        # add regularizations\n        for regularization in self.regularizations:\n            self.loss.append(regularization)\n            tf.losses.add_loss(regularization)\n\n    @classmethod\n    def path(cls):\n        return cls.__class__.__name__\n'"
rl_coach/architectures/tensorflow_components/heads/measurements_prediction_head.py,11,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import Measurements\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass MeasurementsPredictionHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'future_measurements_head\'\n        self.num_actions = len(self.spaces.action.actions)\n        self.num_measurements = self.spaces.state[\'measurements\'].shape[0]\n        self.num_prediction_steps = agent_parameters.algorithm.num_predicted_steps_ahead\n        self.multi_step_measurements_size = self.num_measurements * self.num_prediction_steps\n        self.return_type = Measurements\n\n    def _build_module(self, input_layer):\n        # This is almost exactly the same as Dueling Network but we predict the future measurements for each action\n        # actions expectation tower (expectation stream) - E\n        with tf.variable_scope(""expectation_stream""):\n            expectation_stream = self.dense_layer(256)(input_layer, activation=self.activation_function, name=\'fc1\')\n            expectation_stream = self.dense_layer(self.multi_step_measurements_size)(expectation_stream, name=\'output\')\n            expectation_stream = tf.expand_dims(expectation_stream, axis=1)\n\n        # action fine differences tower (action stream) - A\n        with tf.variable_scope(""action_stream""):\n            action_stream = self.dense_layer(256)(input_layer, activation=self.activation_function, name=\'fc1\')\n            action_stream = self.dense_layer(self.num_actions * self.multi_step_measurements_size)(action_stream,\n                                                                                                   name=\'output\')\n            action_stream = tf.reshape(action_stream,\n                                       (tf.shape(action_stream)[0], self.num_actions, self.multi_step_measurements_size))\n            action_stream = action_stream - tf.reduce_mean(action_stream, reduction_indices=1, keepdims=True)\n\n        # merge to future measurements predictions\n        self.output = tf.add(expectation_stream, action_stream, name=\'output\')\n        self.target = tf.placeholder(tf.float32, [None, self.num_actions, self.multi_step_measurements_size],\n                                     name=""targets"")\n        targets_nonan = tf.where(tf.is_nan(self.target), self.output, self.target)\n        self.loss = tf.reduce_sum(tf.reduce_mean(tf.square(targets_nonan - self.output), reduction_indices=0))\n        tf.losses.add_loss(self.loss_weight[0] * self.loss)\n\n    def __str__(self):\n        result = [\n            ""State Value Stream - V"",\n            ""\\tDense (num outputs = 256)"",\n            ""\\tDense (num outputs = {})"".format(self.multi_step_measurements_size),\n            ""Action Advantage Stream - A"",\n            ""\\tDense (num outputs = 256)"",\n            ""\\tDense (num outputs = {})"".format(self.num_actions * self.multi_step_measurements_size),\n            ""\\tReshape (new size = {} x {})"".format(self.num_actions, self.multi_step_measurements_size),\n            ""\\tSubtract(A, Mean(A))"".format(self.num_actions),\n            ""Add (V, A)""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/naf_head.py,13,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import QActionStateValue\nfrom rl_coach.spaces import BoxActionSpace\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass NAFHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True,activation_function: str=\'relu\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        if not isinstance(self.spaces.action, BoxActionSpace):\n            raise ValueError(""NAF works only for continuous action spaces (BoxActionSpace)"")\n\n        self.name = \'naf_q_values_head\'\n        self.num_actions = self.spaces.action.shape[0]\n        self.output_scale = self.spaces.action.max_abs_range\n        self.return_type = QActionStateValue\n        if agent_parameters.network_wrappers[self.network_name].replace_mse_with_huber_loss:\n            self.loss_type = tf.losses.huber_loss\n        else:\n            self.loss_type = tf.losses.mean_squared_error\n\n    def _build_module(self, input_layer):\n        # NAF\n        self.action = tf.placeholder(tf.float32, [None, self.num_actions], name=""action"")\n        self.input = self.action\n\n        # V Head\n        self.V = self.dense_layer(1)(input_layer, name=\'V\')\n\n        # mu Head\n        mu_unscaled = self.dense_layer(self.num_actions)(input_layer, activation=self.activation_function, name=\'mu_unscaled\')\n        self.mu = tf.multiply(mu_unscaled, self.output_scale, name=\'mu\')\n\n        # A Head\n        # l_vector is a vector that includes a lower-triangular matrix values\n        self.l_vector = self.dense_layer((self.num_actions * (self.num_actions + 1)) / 2)(input_layer, name=\'l_vector\')\n\n        # Convert l to a lower triangular matrix and exponentiate its diagonal\n\n        i = 0\n        columns = []\n        for col in range(self.num_actions):\n            start_row = col\n            num_non_zero_elements = self.num_actions - start_row\n            zeros_column_part = tf.zeros_like(self.l_vector[:, 0:start_row])\n            diag_element = tf.expand_dims(tf.exp(self.l_vector[:, i]), 1)\n            non_zeros_non_diag_column_part = self.l_vector[:, (i + 1):(i + num_non_zero_elements)]\n            columns.append(tf.concat([zeros_column_part, diag_element, non_zeros_non_diag_column_part], axis=1))\n            i += num_non_zero_elements\n        self.L = tf.transpose(tf.stack(columns, axis=1), (0, 2, 1))\n\n        # P = L*L^T\n        self.P = tf.matmul(self.L, tf.transpose(self.L, (0, 2, 1)))\n\n        # A = -1/2 * (u - mu)^T * P * (u - mu)\n        action_diff = tf.expand_dims(self.action - self.mu, -1)\n        a_matrix_form = -0.5 * tf.matmul(tf.transpose(action_diff, (0, 2, 1)), tf.matmul(self.P, action_diff))\n        self.A = tf.reshape(a_matrix_form, [-1, 1])\n\n        # Q Head\n        self.Q = tf.add(self.V, self.A, name=\'Q\')\n\n        self.output = self.Q\n\n    def __str__(self):\n        result = [\n            ""State Value Stream - V"",\n            ""\\tDense (num outputs = 1)"",\n            ""Action Advantage Stream - A"",\n            ""\\tDense (num outputs = {})"".format((self.num_actions * (self.num_actions + 1)) / 2),\n            ""\\tReshape to lower triangular matrix L (new size = {} x {})"".format(self.num_actions, self.num_actions),\n            ""\\tP = L*L^T"",\n            ""\\tA = -1/2 * (u - mu)^T * P * (u - mu)"",\n            ""Action Stream - mu"",\n            ""\\tDense (num outputs = {})"".format(self.num_actions),\n            ""\\tActivation (type = {})"".format(self.activation_function.__name__),\n            ""\\tMultiply (factor = {})"".format(self.output_scale),\n            ""State-Action Value Stream - Q"",\n            ""\\tAdd (V, A)""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/policy_head.py,23,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head, normalized_columns_initializer\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import ActionProbabilities\nfrom rl_coach.exploration_policies.continuous_entropy import ContinuousEntropyParameters\nfrom rl_coach.spaces import DiscreteActionSpace, BoxActionSpace, CompoundActionSpace\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import eps, indent_string\n\n\nclass PolicyHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'tanh\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'policy_values_head\'\n        self.return_type = ActionProbabilities\n        self.beta = None\n        self.action_penalty = None\n\n        self.exploration_policy = agent_parameters.exploration\n\n        # a scalar weight that penalizes low entropy values to encourage exploration\n        if hasattr(agent_parameters.algorithm, \'beta_entropy\'):\n            # we set the beta value as a tf variable so it can be updated later if needed\n            self.beta = tf.Variable(float(agent_parameters.algorithm.beta_entropy),\n                                    trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n            self.beta_placeholder = tf.placeholder(\'float\')\n            self.set_beta = tf.assign(self.beta, self.beta_placeholder)\n\n        # a scalar weight that penalizes high activation values (before the activation function) for the final layer\n        if hasattr(agent_parameters.algorithm, \'action_penalty\'):\n            self.action_penalty = agent_parameters.algorithm.action_penalty\n\n    def _build_module(self, input_layer):\n        self.actions = []\n        self.input = self.actions\n        self.policy_distributions = []\n        self.output = []\n\n        action_spaces = [self.spaces.action]\n        if isinstance(self.spaces.action, CompoundActionSpace):\n            action_spaces = self.spaces.action.sub_action_spaces\n\n        # create a compound action network\n        for action_space_idx, action_space in enumerate(action_spaces):\n            with tf.variable_scope(""sub_action_{}"".format(action_space_idx)):\n                if isinstance(action_space, DiscreteActionSpace):\n                    # create a discrete action network (softmax probabilities output)\n                    self._build_discrete_net(input_layer, action_space)\n                elif isinstance(action_space, BoxActionSpace):\n                    # create a continuous action network (bounded mean and stdev outputs)\n                    self._build_continuous_net(input_layer, action_space)\n\n        if self.is_local:\n            # add entropy regularization\n            if self.beta:\n                self.entropy = tf.add_n([tf.reduce_mean(dist.entropy()) for dist in self.policy_distributions])\n                self.regularizations += [-tf.multiply(self.beta, self.entropy, name=\'entropy_regularization\')]\n\n            # calculate loss\n            self.action_log_probs_wrt_policy = \\\n                tf.add_n([dist.log_prob(action) for dist, action in zip(self.policy_distributions, self.actions)])\n            self.advantages = tf.placeholder(tf.float32, [None], name=""advantages"")\n            self.target = self.advantages\n            self.loss = -tf.reduce_mean(self.action_log_probs_wrt_policy * self.advantages)\n            tf.losses.add_loss(self.loss_weight[0] * self.loss)\n\n    def _build_discrete_net(self, input_layer, action_space):\n        num_actions = len(action_space.actions)\n        self.actions.append(tf.placeholder(tf.int32, [None], name=""actions""))\n\n        policy_values = self.dense_layer(num_actions)(input_layer, name=\'fc\')\n        self.policy_probs = tf.nn.softmax(policy_values, name=""policy"")\n\n        # define the distributions for the policy and the old policy\n        # (the + eps is to prevent probability 0 which will cause the log later on to be -inf)\n        policy_distribution = tf.contrib.distributions.Categorical(probs=(self.policy_probs + eps))\n        self.policy_distributions.append(policy_distribution)\n        self.output.append(self.policy_probs)\n\n    def _build_continuous_net(self, input_layer, action_space):\n        num_actions = action_space.shape\n        self.actions.append(tf.placeholder(tf.float32, [None, num_actions], name=""actions""))\n\n        # output activation function\n        if np.all(action_space.max_abs_range < np.inf):\n            # bounded actions\n            self.output_scale = action_space.max_abs_range\n            self.continuous_output_activation = self.activation_function\n        else:\n            # unbounded actions\n            self.output_scale = 1\n            self.continuous_output_activation = None\n\n        # mean\n        pre_activation_policy_values_mean = self.dense_layer(num_actions)(input_layer, name=\'fc_mean\')\n        policy_values_mean = self.continuous_output_activation(pre_activation_policy_values_mean)\n        self.policy_mean = tf.multiply(policy_values_mean, self.output_scale, name=\'output_mean\')\n\n        self.output.append(self.policy_mean)\n\n        # standard deviation\n        if isinstance(self.exploration_policy, ContinuousEntropyParameters):\n            # the stdev is an output of the network and uses a softplus activation as defined in A3C\n            policy_values_std = self.dense_layer(num_actions)(input_layer,\n                                                              kernel_initializer=normalized_columns_initializer(0.01),\n                                                              name=\'fc_std\')\n            self.policy_std = tf.nn.softplus(policy_values_std, name=\'output_variance\') + eps\n\n            self.output.append(self.policy_std)\n        else:\n            # the stdev is an externally given value\n            # Warning: we need to explicitly put this variable in the local variables collections, since defining\n            # it as not trainable puts it for some reason in the global variables collections. If this is not done,\n            # the variable won\'t be initialized and when working with multiple workers they will get stuck.\n            self.policy_std = tf.Variable(np.ones(num_actions), dtype=\'float32\', trainable=False,\n                                          name=\'policy_stdev\', collections=[tf.GraphKeys.LOCAL_VARIABLES])\n\n            # assign op for the policy std\n            self.policy_std_placeholder = tf.placeholder(\'float32\', (num_actions,))\n            self.assign_policy_std = tf.assign(self.policy_std, self.policy_std_placeholder)\n\n        # define the distributions for the policy and the old policy\n        policy_distribution = tf.contrib.distributions.MultivariateNormalDiag(self.policy_mean, self.policy_std)\n        self.policy_distributions.append(policy_distribution)\n\n        if self.is_local:\n            # add a squared penalty on the squared pre-activation features of the action\n            if self.action_penalty and self.action_penalty != 0:\n                self.regularizations += [\n                    self.action_penalty * tf.reduce_mean(tf.square(pre_activation_policy_values_mean))]\n\n    def __str__(self):\n        action_spaces = [self.spaces.action]\n        if isinstance(self.spaces.action, CompoundActionSpace):\n            action_spaces = self.spaces.action.sub_action_spaces\n\n        result = []\n        for action_space_idx, action_space in enumerate(action_spaces):\n            action_head_mean_result = []\n            if isinstance(action_space, DiscreteActionSpace):\n                # create a discrete action network (softmax probabilities output)\n                action_head_mean_result.append(""Dense (num outputs = {})"".format(len(action_space.actions)))\n                action_head_mean_result.append(""Softmax"")\n            elif isinstance(action_space, BoxActionSpace):\n                # create a continuous action network (bounded mean and stdev outputs)\n                action_head_mean_result.append(""Dense (num outputs = {})"".format(action_space.shape))\n                if np.all(action_space.max_abs_range < np.inf):\n                    # bounded actions\n                    action_head_mean_result.append(""Activation (type = {})"".format(self.activation_function.__name__))\n                    action_head_mean_result.append(""Multiply (factor = {})"".format(action_space.max_abs_range))\n\n            action_head_stdev_result = []\n            if isinstance(self.exploration_policy, ContinuousEntropyParameters):\n                action_head_stdev_result.append(""Dense (num outputs = {})"".format(action_space.shape))\n                action_head_stdev_result.append(""Softplus"")\n\n            action_head_result = []\n            if action_head_stdev_result:\n                action_head_result.append(""Mean Stream"")\n                action_head_result.append(indent_string(\'\\n\'.join(action_head_mean_result)))\n                action_head_result.append(""Stdev Stream"")\n                action_head_result.append(indent_string(\'\\n\'.join(action_head_stdev_result)))\n            else:\n                action_head_result.append(\'\\n\'.join(action_head_mean_result))\n\n            if len(action_spaces) > 1:\n                result.append(""Action head {}"".format(action_space_idx))\n                result.append(indent_string(\'\\n\'.join(action_head_result)))\n            else:\n                result.append(\'\\n\'.join(action_head_result))\n\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/ppo_head.py,32,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head, normalized_columns_initializer\nfrom rl_coach.base_parameters import AgentParameters, DistributedTaskParameters\nfrom rl_coach.core_types import ActionProbabilities\nfrom rl_coach.spaces import BoxActionSpace, DiscreteActionSpace\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import eps\n\n\nclass PPOHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'tanh\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'ppo_head\'\n        self.return_type = ActionProbabilities\n\n        # used in regular PPO\n        self.use_kl_regularization = agent_parameters.algorithm.use_kl_regularization\n        if self.use_kl_regularization:\n            # kl coefficient and its corresponding assignment operation and placeholder\n            self.kl_coefficient = tf.Variable(agent_parameters.algorithm.initial_kl_coefficient,\n                                              trainable=False, name=\'kl_coefficient\')\n            self.kl_coefficient_ph = tf.placeholder(\'float\', name=\'kl_coefficient_ph\')\n            self.assign_kl_coefficient = tf.assign(self.kl_coefficient, self.kl_coefficient_ph)\n            self.kl_cutoff = 2 * agent_parameters.algorithm.target_kl_divergence\n            self.high_kl_penalty_coefficient = agent_parameters.algorithm.high_kl_penalty_coefficient\n\n        self.clip_likelihood_ratio_using_epsilon = agent_parameters.algorithm.clip_likelihood_ratio_using_epsilon\n        self.beta = agent_parameters.algorithm.beta_entropy\n\n    def _build_module(self, input_layer):\n        if isinstance(self.spaces.action, DiscreteActionSpace):\n            self._build_discrete_net(input_layer, self.spaces.action)\n        elif isinstance(self.spaces.action, BoxActionSpace):\n            self._build_continuous_net(input_layer, self.spaces.action)\n        else:\n            raise ValueError(""only discrete or continuous action spaces are supported for PPO"")\n\n        self.action_probs_wrt_policy = self.policy_distribution.log_prob(self.actions)\n        self.action_probs_wrt_old_policy = self.old_policy_distribution.log_prob(self.actions)\n        self.entropy = tf.reduce_mean(self.policy_distribution.entropy())\n\n        # Used by regular PPO only\n        # add kl divergence regularization\n        self.kl_divergence = tf.reduce_mean(tf.distributions.kl_divergence(self.old_policy_distribution, self.policy_distribution))\n\n        if self.use_kl_regularization:\n            # no clipping => use kl regularization\n            self.weighted_kl_divergence = tf.multiply(self.kl_coefficient, self.kl_divergence)\n            self.regularizations += [self.weighted_kl_divergence + self.high_kl_penalty_coefficient * \\\n                                                tf.square(tf.maximum(0.0, self.kl_divergence - self.kl_cutoff))]\n\n        # calculate surrogate loss\n        self.advantages = tf.placeholder(tf.float32, [None], name=""advantages"")\n        self.target = self.advantages\n        # action_probs_wrt_old_policy != 0 because it is e^...\n        self.likelihood_ratio = tf.exp(self.action_probs_wrt_policy - self.action_probs_wrt_old_policy)\n        if self.clip_likelihood_ratio_using_epsilon is not None:\n            self.clip_param_rescaler = tf.placeholder(tf.float32, ())\n            self.input.append(self.clip_param_rescaler)\n            max_value = 1 + self.clip_likelihood_ratio_using_epsilon * self.clip_param_rescaler\n            min_value = 1 - self.clip_likelihood_ratio_using_epsilon * self.clip_param_rescaler\n            self.clipped_likelihood_ratio = tf.clip_by_value(self.likelihood_ratio, min_value, max_value)\n            self.scaled_advantages = tf.minimum(self.likelihood_ratio * self.advantages,\n                                                self.clipped_likelihood_ratio * self.advantages)\n        else:\n            self.scaled_advantages = self.likelihood_ratio * self.advantages\n        # minus sign is in order to set an objective to minimize (we actually strive for maximizing the surrogate loss)\n        self.surrogate_loss = -tf.reduce_mean(self.scaled_advantages)\n        if self.is_local:\n            # add entropy regularization\n            if self.beta:\n                self.entropy = tf.reduce_mean(self.policy_distribution.entropy())\n                self.regularizations += [-tf.multiply(self.beta, self.entropy, name=\'entropy_regularization\')]\n\n        self.loss = self.surrogate_loss\n        tf.losses.add_loss(self.loss)\n\n    def _build_discrete_net(self, input_layer, action_space):\n        num_actions = len(action_space.actions)\n        self.actions = tf.placeholder(tf.int32, [None], name=""actions"")\n\n        self.old_policy_mean = tf.placeholder(tf.float32, [None, num_actions], ""old_policy_mean"")\n        self.old_policy_std = tf.placeholder(tf.float32, [None, num_actions], ""old_policy_std"")\n\n        # Policy Head\n        self.input = [self.actions, self.old_policy_mean]\n        policy_values = self.dense_layer(num_actions)(input_layer, name=\'policy_fc\')\n        self.policy_mean = tf.nn.softmax(policy_values, name=""policy"")\n\n        # define the distributions for the policy and the old policy\n        self.policy_distribution = tf.contrib.distributions.Categorical(probs=self.policy_mean)\n        self.old_policy_distribution = tf.contrib.distributions.Categorical(probs=self.old_policy_mean)\n\n        self.output = self.policy_mean\n\n    def _build_continuous_net(self, input_layer, action_space):\n        num_actions = action_space.shape[0]\n        self.actions = tf.placeholder(tf.float32, [None, num_actions], name=""actions"")\n\n        self.old_policy_mean = tf.placeholder(tf.float32, [None, num_actions], ""old_policy_mean"")\n        self.old_policy_std = tf.placeholder(tf.float32, [None, num_actions], ""old_policy_std"")\n\n        self.input = [self.actions, self.old_policy_mean, self.old_policy_std]\n        self.policy_mean = self.dense_layer(num_actions)(input_layer, name=\'policy_mean\',\n                                           kernel_initializer=normalized_columns_initializer(0.01))\n\n        # for local networks in distributed settings, we need to move variables we create manually to the\n        # tf.GraphKeys.LOCAL_VARIABLES collection, since the variable scope custom getter which is set in\n        # Architecture does not apply to them\n        if self.is_local and isinstance(self.ap.task_parameters, DistributedTaskParameters):\n            self.policy_logstd = tf.Variable(np.zeros((1, num_actions)), dtype=\'float32\',\n                                             collections=[tf.GraphKeys.LOCAL_VARIABLES], name=""policy_log_std"")\n        else:\n            self.policy_logstd = tf.Variable(np.zeros((1, num_actions)), dtype=\'float32\', name=""policy_log_std"")\n\n        self.policy_std = tf.tile(tf.exp(self.policy_logstd), [tf.shape(input_layer)[0], 1], name=\'policy_std\')\n\n        # define the distributions for the policy and the old policy\n        self.policy_distribution = tf.contrib.distributions.MultivariateNormalDiag(self.policy_mean, self.policy_std + eps)\n        self.old_policy_distribution = tf.contrib.distributions.MultivariateNormalDiag(self.old_policy_mean, self.old_policy_std + eps)\n\n        self.output = [self.policy_mean, self.policy_std]\n\n    def __str__(self):\n        action_head_mean_result = []\n        if isinstance(self.spaces.action, DiscreteActionSpace):\n            # create a discrete action network (softmax probabilities output)\n            action_head_mean_result.append(""Dense (num outputs = {})"".format(len(self.spaces.action.actions)))\n            action_head_mean_result.append(""Softmax"")\n        elif isinstance(self.spaces.action, BoxActionSpace):\n            # create a continuous action network (bounded mean and stdev outputs)\n            action_head_mean_result.append(""Dense (num outputs = {})"".format(self.spaces.action.shape))\n\n        return \'\\n\'.join(action_head_mean_result)\n'"
rl_coach/architectures/tensorflow_components/heads/ppo_v_head.py,7,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head, normalized_columns_initializer\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import ActionProbabilities\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass PPOVHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense, output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'ppo_v_head\'\n        self.clip_likelihood_ratio_using_epsilon = agent_parameters.algorithm.clip_likelihood_ratio_using_epsilon\n        self.return_type = ActionProbabilities\n        self.output_bias_initializer = output_bias_initializer\n\n    def _build_module(self, input_layer):\n        self.old_policy_value = tf.placeholder(tf.float32, [None], ""old_policy_values"")\n        self.input = [self.old_policy_value]\n        self.output = self.dense_layer(1)(input_layer, name=\'output\',\n                                          kernel_initializer=normalized_columns_initializer(1.0),\n                                          bias_initializer=self.output_bias_initializer)\n        self.target = self.total_return = tf.placeholder(tf.float32, [None], name=""total_return"")\n\n        value_loss_1 = tf.square(self.output - self.target)\n        value_loss_2 = tf.square(self.old_policy_value +\n                                 tf.clip_by_value(self.output - self.old_policy_value,\n                                                  -self.clip_likelihood_ratio_using_epsilon,\n                                                  self.clip_likelihood_ratio_using_epsilon) - self.target)\n        self.vf_loss = tf.reduce_mean(tf.maximum(value_loss_1, value_loss_2))\n        self.loss = self.vf_loss\n        tf.losses.add_loss(self.loss)\n\n    def __str__(self):\n        result = [\n            ""Dense (num outputs = 1)""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/q_head.py,3,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import QActionStateValue\nfrom rl_coach.spaces import SpacesDefinition, BoxActionSpace, DiscreteActionSpace\n\n\nclass QHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense, output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'q_values_head\'\n        if isinstance(self.spaces.action, BoxActionSpace):\n            self.num_actions = 1\n        elif isinstance(self.spaces.action, DiscreteActionSpace):\n            self.num_actions = len(self.spaces.action.actions)\n        else:\n            raise ValueError(\n                \'QHead does not support action spaces of type: {class_name}\'.format(\n                    class_name=self.spaces.action.__class__.__name__,\n                )\n            )\n        self.return_type = QActionStateValue\n        if agent_parameters.network_wrappers[self.network_name].replace_mse_with_huber_loss:\n            self.loss_type = tf.losses.huber_loss\n        else:\n            self.loss_type = tf.losses.mean_squared_error\n\n        self.output_bias_initializer = output_bias_initializer\n\n    def _build_module(self, input_layer):\n        # Standard Q Network\n        self.q_values = self.output = self.dense_layer(self.num_actions)\\\n            (input_layer, name=\'output\', bias_initializer=self.output_bias_initializer)\n\n        # used in batch-rl to estimate a probablity distribution over actions\n        self.softmax = self.add_softmax_with_temperature()\n\n    def __str__(self):\n        result = [\n            ""Dense (num outputs = {})"".format(self.num_actions)\n        ]\n        return \'\\n\'.join(result)\n\n    def add_softmax_with_temperature(self):\n        temperature = self.ap.network_wrappers[self.network_name].softmax_temperature\n        temperature_scaled_outputs = self.q_values / temperature\n        return tf.nn.softmax(temperature_scaled_outputs, name=""softmax"")\n\n'"
rl_coach/architectures/tensorflow_components/heads/quantile_regression_q_head.py,17,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\nimport numpy as np\nfrom rl_coach.architectures.tensorflow_components.heads import QHead\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass QuantileRegressionQHead(QHead):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense, output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer, output_bias_initializer=output_bias_initializer)\n        self.name = \'quantile_regression_dqn_head\'\n        self.num_actions = len(self.spaces.action.actions)\n        self.num_atoms = agent_parameters.algorithm.atoms  # we use atom / quantile interchangeably\n        self.huber_loss_interval = agent_parameters.algorithm.huber_loss_interval  # k\n        self.quantile_probabilities = tf.cast(\n            tf.constant(np.ones(self.ap.algorithm.atoms) / float(self.ap.algorithm.atoms), dtype=tf.float32),\n            dtype=tf.float64)\n        self.loss_type = []\n\n    def _build_module(self, input_layer):\n        self.actions = tf.placeholder(tf.int32, [None, 2], name=""actions"")\n        self.quantile_midpoints = tf.placeholder(tf.float32, [None, self.num_atoms], name=""quantile_midpoints"")\n        self.input = [self.actions, self.quantile_midpoints]\n\n        # the output of the head is the N unordered quantile locations {theta_1, ..., theta_N}\n        quantiles_locations = self.dense_layer(self.num_actions * self.num_atoms)\\\n            (input_layer, name=\'output\', bias_initializer=self.output_bias_initializer)\n        quantiles_locations = tf.reshape(quantiles_locations, (tf.shape(quantiles_locations)[0], self.num_actions, self.num_atoms))\n        self.output = quantiles_locations\n\n        self.quantiles = tf.placeholder(tf.float32, shape=(None, self.num_atoms), name=""quantiles"")\n        self.target = self.quantiles\n\n        # only the quantiles of the taken action are taken into account\n        quantiles_for_used_actions = tf.gather_nd(quantiles_locations, self.actions)\n\n        # reorder the output quantiles and the target quantiles as a preparation step for calculating the loss\n        # the output quantiles vector and the quantile midpoints are tiled as rows of a NxN matrix (N = num quantiles)\n        # the target quantiles vector is tiled as column of a NxN matrix\n        theta_i = tf.tile(tf.expand_dims(quantiles_for_used_actions, -1), [1, 1, self.num_atoms])\n        T_theta_j = tf.tile(tf.expand_dims(self.target, -2), [1, self.num_atoms, 1])\n        tau_i = tf.tile(tf.expand_dims(self.quantile_midpoints, -1), [1, 1, self.num_atoms])\n\n        # Huber loss of T(theta_j) - theta_i\n        error = T_theta_j - theta_i\n        abs_error = tf.abs(error)\n        quadratic = tf.minimum(abs_error, self.huber_loss_interval)\n        huber_loss = self.huber_loss_interval * (abs_error - quadratic) + 0.5 * quadratic ** 2\n\n        # Quantile Huber loss\n        quantile_huber_loss = tf.abs(tau_i - tf.cast(error < 0, dtype=tf.float32)) * huber_loss\n\n        # Quantile regression loss (the probability for each quantile is 1/num_quantiles)\n        quantile_regression_loss = tf.reduce_sum(quantile_huber_loss) / float(self.num_atoms)\n        self.loss = quantile_regression_loss\n        tf.losses.add_loss(self.loss)\n\n        self.q_values = tf.tensordot(tf.cast(self.output, tf.float64), self.quantile_probabilities, 1)\n\n        # used in batch-rl to estimate a probablity distribution over actions\n        self.softmax = self.add_softmax_with_temperature()\n\n    def __str__(self):\n        result = [\n            ""Dense (num outputs = {})"".format(self.num_actions * self.num_atoms),\n            ""Reshape (new size = {} x {})"".format(self.num_actions, self.num_atoms)\n        ]\n        return \'\\n\'.join(result)\n\n'"
rl_coach/architectures/tensorflow_components/heads/rainbow_q_head.py,13,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\nimport numpy as np\nfrom rl_coach.architectures.tensorflow_components.heads import QHead\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass RainbowQHead(QHead):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.num_actions = len(self.spaces.action.actions)\n        self.num_atoms = agent_parameters.algorithm.atoms\n        self.name = \'rainbow_q_values_head\'\n        self.z_values = tf.cast(tf.constant(np.linspace(self.ap.algorithm.v_min, self.ap.algorithm.v_max,\n                                                        self.ap.algorithm.atoms), dtype=tf.float32), dtype=tf.float64)\n        self.loss_type = []\n\n    def _build_module(self, input_layer):\n        # state value tower - V\n        with tf.variable_scope(""state_value""):\n            state_value = self.dense_layer(512)(input_layer, activation=self.activation_function, name=\'fc1\')\n            state_value = self.dense_layer(self.num_atoms)(state_value, name=\'fc2\')\n            state_value = tf.expand_dims(state_value, axis=1)\n\n        # action advantage tower - A\n        with tf.variable_scope(""action_advantage""):\n            action_advantage = self.dense_layer(512)(input_layer, activation=self.activation_function, name=\'fc1\')\n            action_advantage = self.dense_layer(self.num_actions * self.num_atoms)(action_advantage, name=\'fc2\')\n            action_advantage = tf.reshape(action_advantage, (tf.shape(input_layer)[0], self.num_actions,\n                                                             self.num_atoms))\n            action_mean = tf.reduce_mean(action_advantage, axis=1, keepdims=True)\n            action_advantage = action_advantage - action_mean\n\n        # merge to state-action value function Q\n        values_distribution = tf.add(state_value, action_advantage, name=\'output\')\n\n        # softmax on atoms dimension\n        self.output = tf.nn.softmax(values_distribution)\n\n        # calculate cross entropy loss\n        self.distributions = tf.placeholder(tf.float32, shape=(None, self.num_actions, self.num_atoms),\n                                            name=""distributions"")\n        self.target = self.distributions\n        self.loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.target, logits=values_distribution)\n        tf.losses.add_loss(self.loss)\n\n        self.q_values = tf.tensordot(tf.cast(self.output, tf.float64), self.z_values, 1)\n\n        # used in batch-rl to estimate a probablity distribution over actions\n        self.softmax = self.add_softmax_with_temperature()\n\n    def __str__(self):\n        result = [\n            ""State Value Stream - V"",\n            ""\\tDense (num outputs = 512)"",\n            ""\\tDense (num outputs = {})"".format(self.num_atoms),\n            ""Action Advantage Stream - A"",\n            ""\\tDense (num outputs = 512)"",\n            ""\\tDense (num outputs = {})"".format(self.num_actions * self.num_atoms),\n            ""\\tReshape (new size = {} x {})"".format(self.num_actions, self.num_atoms),\n            ""\\tSubtract(A, Mean(A))"".format(self.num_actions),\n            ""Add (V, A)"",\n            ""Softmax""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/sac_head.py,8,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import ActionProbabilities\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.utils import eps\n\nLOG_SIG_CAP_MAX = 2\nLOG_SIG_CAP_MIN = -20\n\n\nclass SACPolicyHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 squash: bool = True, dense_layer=Dense):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'sac_policy_head\'\n        self.return_type = ActionProbabilities\n        self.num_actions = self.spaces.action.shape     # continuous actions\n        self.squash = squash        # squashing using tanh\n\n    def _build_module(self, input_layer):\n        self.given_raw_actions = tf.placeholder(tf.float32, [None, self.num_actions], name=""actions"")\n        self.input = [self.given_raw_actions]\n        self.output = []\n\n        # build the network\n        self._build_continuous_net(input_layer, self.spaces.action)\n\n    def _squash_correction(self,actions):\n        \'\'\'\n        correct squash operation (in case of bounded actions) according to appendix C in the paper.\n        NOTE : this correction assume the squash is done with tanh.\n        :param actions: unbounded actions\n        :return: the correction to be applied to the log_prob of the actions, assuming tanh squash\n        \'\'\'\n        if not self.squash:\n            return 0\n        return tf.reduce_sum(tf.log(1 - tf.tanh(actions) ** 2 + eps), axis=1)\n\n    def _build_continuous_net(self, input_layer, action_space):\n        num_actions = action_space.shape[0]\n\n        self.policy_mu_and_logsig = self.dense_layer(2*num_actions)(input_layer, name=\'policy_mu_logsig\')\n        self.policy_mean = tf.identity(self.policy_mu_and_logsig[..., :num_actions], name=\'policy_mean\')\n        self.policy_log_std = tf.clip_by_value(self.policy_mu_and_logsig[..., num_actions:],\n                                               LOG_SIG_CAP_MIN, LOG_SIG_CAP_MAX,name=\'policy_log_std\')\n\n        self.output.append(self.policy_mean)        # output[0]\n        self.output.append(self.policy_log_std)     # output[1]\n\n        # define the distributions for the policy\n        # Tensorflow\'s multivariate normal distribution supports reparameterization\n        tfd = tf.contrib.distributions\n        self.policy_distribution = tfd.MultivariateNormalDiag(loc=self.policy_mean,\n                                                              scale_diag=tf.exp(self.policy_log_std))\n\n        # define network outputs\n        # note that tensorflow supports reparametrization.\n        # i.e. policy_action_sample is a tensor through which gradients can flow\n        self.raw_actions = self.policy_distribution.sample()\n\n        if self.squash:\n            self.actions = tf.tanh(self.raw_actions)\n            # correct log_prob in case of squash (see appendix C in the paper)\n            squash_correction = self._squash_correction(self.raw_actions)\n        else:\n            self.actions = self.raw_actions\n            squash_correction = 0\n\n        # policy_action_logprob is a tensor through which gradients can flow\n        self.sampled_actions_logprob = self.policy_distribution.log_prob(self.raw_actions) - squash_correction\n        self.sampled_actions_logprob_mean = tf.reduce_mean(self.sampled_actions_logprob)\n\n        self.output.append(self.raw_actions)    # output[2] : sampled raw action (before squash)\n        self.output.append(self.actions)        # output[3] : squashed (if needed) version of sampled raw_actions\n        self.output.append(self.sampled_actions_logprob)   # output[4]: log prob of sampled action (squash corrected)\n        self.output.append(self.sampled_actions_logprob_mean)    # output[5]: mean of log prob of sampled actions (squash corrected)\n\n    def __str__(self):\n        result = [\n            ""policy head:""\n            ""\\t\\tDense (num outputs = 256)"",\n            ""\\t\\tDense (num outputs = 256)"",\n            ""\\t\\tDense (num outputs = {0})"".format(2*self.num_actions),\n            ""policy_mu = output[:num_actions], policy_std = output[num_actions:]""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/sac_q_head.py,10,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import QActionStateValue\nfrom rl_coach.spaces import SpacesDefinition, BoxActionSpace\n\n\nclass SACQHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense, output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'q_values_head\'\n        if isinstance(self.spaces.action, BoxActionSpace):\n            self.num_actions = self.spaces.action.shape  # continuous actions\n        else:\n            raise ValueError(\n                \'SACQHead does not support action spaces of type: {class_name}\'.format(\n                    class_name=self.spaces.action.__class__.__name__,\n                )\n            )\n        self.return_type = QActionStateValue\n        # extract the topology from the SACQHeadParameters\n        self.network_layers_sizes = agent_parameters.network_wrappers[\'q\'].heads_parameters[0].network_layers_sizes\n        self.output_bias_initializer = output_bias_initializer\n\n    def _build_module(self, input_layer):\n        # SAC Q network is basically 2 networks running in parallel on the same input (state , action)\n        # state is the observation fed through the input_layer, action is fed through placeholder to the header\n        # each is calculating q value  : q1(s,a) and q2(s,a)\n        # the output of the head is min(q1,q2)\n        self.actions = tf.placeholder(tf.float32, [None, self.num_actions], name=""actions"")\n        self.target = tf.placeholder(tf.float32, [None, 1], name=""q_targets"")\n        self.input = [self.actions]\n        self.output = []\n        # Note (1) : in the author\'s implementation of sac (in rllab) they summarize the embedding of observation and\n        # action (broadcasting the bias) in the first layer of the network.\n\n        # build q1 network head\n        with tf.variable_scope(""q1_head""):\n            layer_size = self.network_layers_sizes[0]\n            qi_obs_emb = self.dense_layer(layer_size)(input_layer, activation=self.activation_function)\n            qi_act_emb = self.dense_layer(layer_size)(self.actions, activation=self.activation_function)\n            qi_output = qi_obs_emb + qi_act_emb     # merging the inputs by summarizing them (see Note (1))\n            for layer_size in self.network_layers_sizes[1:]:\n                qi_output = self.dense_layer(layer_size)(qi_output, activation=self.activation_function)\n            # the output layer\n            self.q1_output = self.dense_layer(1)(qi_output, name=\'q1_output\',\n                                                 bias_initializer=self.output_bias_initializer)\n\n        # build q2 network head\n        with tf.variable_scope(""q2_head""):\n            layer_size = self.network_layers_sizes[0]\n            qi_obs_emb = self.dense_layer(layer_size)(input_layer, activation=self.activation_function)\n            qi_act_emb = self.dense_layer(layer_size)(self.actions, activation=self.activation_function)\n            qi_output = qi_obs_emb + qi_act_emb     # merging the inputs by summarizing them (see Note (1))\n            for layer_size in self.network_layers_sizes[1:]:\n                qi_output = self.dense_layer(layer_size)(qi_output, activation=self.activation_function)\n            # the output layer\n            self.q2_output = self.dense_layer(1)(qi_output, name=\'q2_output\',\n                                                 bias_initializer=self.output_bias_initializer)\n\n        # take the minimum as the network\'s output. this is the log_target (in the original implementation)\n        self.q_output = tf.minimum(self.q1_output, self.q2_output, name=\'q_output\')\n        # the policy gradients\n        # self.q_output_mean = tf.reduce_mean(self.q1_output)         # option 1: use q1\n        self.q_output_mean = tf.reduce_mean(self.q_output)        # option 2: use min(q1,q2)\n\n        self.output.append(self.q_output)\n        self.output.append(self.q_output_mean)\n\n        # defining the loss\n        self.q1_loss = 0.5*tf.reduce_mean(tf.square(self.q1_output - self.target))\n        self.q2_loss = 0.5*tf.reduce_mean(tf.square(self.q2_output - self.target))\n        # eventually both losses are depends on different parameters so we can sum them up\n        self.loss = self.q1_loss+self.q2_loss\n        tf.losses.add_loss(self.loss)\n\n    def __str__(self):\n        result = [\n            ""q1 output""\n            ""\\t\\tDense (num outputs = 256)"",\n            ""\\t\\tDense (num outputs = 256)"",\n            ""\\t\\tDense (num outputs = 1)"",\n            ""q2 output""\n            ""\\t\\tDense (num outputs = 256)"",\n            ""\\t\\tDense (num outputs = 256)"",\n            ""\\t\\tDense (num outputs = 1)"",\n            ""min(Q1,Q2)""\n        ]\n        return \'\\n\'.join(result)\n\n\n\n\n\n\n\n\n\n'"
rl_coach/architectures/tensorflow_components/heads/td3_v_head.py,5,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head, normalized_columns_initializer\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import VStateValue\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass TD3VHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense, initializer=\'xavier\', output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'td3_v_values_head\'\n        self.return_type = VStateValue\n        self.loss_type = []\n        self.initializer = initializer\n        self.loss = []\n        self.output = []\n        self.output_bias_initializer = output_bias_initializer\n\n    def _build_module(self, input_layer):\n        # Standard V Network\n        q_outputs = []\n        self.target = tf.placeholder(tf.float32, shape=(None, 1), name=""q_networks_min_placeholder"")\n\n        for i in range(input_layer.shape[0]): # assuming that the actual size is 2, as there are two critic networks\n            if self.initializer == \'normalized_columns\':\n                q_outputs.append(self.dense_layer(1)(input_layer[i], name=\'q_output_{}\'.format(i + 1),\n                                                     kernel_initializer=normalized_columns_initializer(1.0),\n                                                     bias_initializer=self.output_bias_initializer),)\n            elif self.initializer == \'xavier\' or self.initializer is None:\n                q_outputs.append(self.dense_layer(1)(input_layer[i], name=\'q_output_{}\'.format(i + 1),\n                                                     bias_initializer=self.output_bias_initializer))\n\n            self.output.append(q_outputs[i])\n            self.loss.append(tf.reduce_mean((self.target-q_outputs[i])**2))\n\n        self.output.append(tf.reduce_min(q_outputs, axis=0))\n        self.output.append(tf.reduce_mean(self.output[0]))\n        self.loss = sum(self.loss)\n        tf.losses.add_loss(self.loss)\n\n    def __str__(self):\n        result = [\n            ""Q1 Action-Value Stream"",\n            ""\\tDense (num outputs = 1)"",\n            ""Q2 Action-Value Stream"",\n            ""\\tDense (num outputs = 1)"",\n            ""Min (Q1, Q2)""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/v_head.py,2,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head, normalized_columns_initializer\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import VStateValue\nfrom rl_coach.spaces import SpacesDefinition\n\n\nclass VHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'relu\',\n                 dense_layer=Dense, initializer=\'normalized_columns\', output_bias_initializer=None):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer)\n        self.name = \'v_values_head\'\n        self.return_type = VStateValue\n\n        if agent_parameters.network_wrappers[self.network_name.split(\'/\')[0]].replace_mse_with_huber_loss:\n            self.loss_type = tf.losses.huber_loss\n        else:\n            self.loss_type = tf.losses.mean_squared_error\n\n        self.initializer = initializer\n        self.output_bias_initializer = output_bias_initializer\n\n    def _build_module(self, input_layer):\n        # Standard V Network\n        if self.initializer == \'normalized_columns\':\n            self.output = self.dense_layer(1)(input_layer, name=\'output\',\n                                              kernel_initializer=normalized_columns_initializer(1.0),\n                                              bias_initializer=self.output_bias_initializer)\n        elif self.initializer == \'xavier\' or self.initializer is None:\n            self.output = self.dense_layer(1)(input_layer, name=\'output\',\n                                              bias_initializer=self.output_bias_initializer)\n\n    def __str__(self):\n        result = [\n            ""Dense (num outputs = 1)""\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/heads/wolpertinger_actor_head.py,1,"b'#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import batchnorm_activation_dropout, Dense\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import Embedding\nfrom rl_coach.spaces import SpacesDefinition, BoxActionSpace\n\n\nclass WolpertingerActorHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str=\'tanh\',\n                 batchnorm: bool=True, dense_layer=Dense, is_training=False):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function,\n                         dense_layer=dense_layer, is_training=is_training)\n        self.name = \'wolpertinger_actor_head\'\n        self.return_type = Embedding\n        self.action_embedding_width = agent_parameters.algorithm.action_embedding_width\n        self.batchnorm = batchnorm\n        self.output_scale = self.spaces.action.filtered_action_space.max_abs_range if \\\n            (hasattr(self.spaces.action, \'filtered_action_space\') and\n             isinstance(self.spaces.action.filtered_action_space, BoxActionSpace)) \\\n            else None\n\n    def _build_module(self, input_layer):\n        # mean\n        pre_activation_policy_value = self.dense_layer(self.action_embedding_width)(input_layer,\n                                                                                    name=\'actor_action_embedding\')\n        self.proto_action = batchnorm_activation_dropout(input_layer=pre_activation_policy_value,\n                                                         batchnorm=self.batchnorm,\n                                                         activation_function=self.activation_function,\n                                                         dropout_rate=0,\n                                                         is_training=self.is_training,\n                                                         name=""BatchnormActivationDropout_0"")[-1]\n        if self.output_scale is not None:\n            self.proto_action = tf.multiply(self.proto_action, self.output_scale, name=\'proto_action\')\n\n        self.output = [self.proto_action]\n\n    def __str__(self):\n        result = [\n            \'Dense (num outputs = {})\'.format(self.action_embedding_width)\n        ]\n        return \'\\n\'.join(result)\n'"
rl_coach/architectures/tensorflow_components/middlewares/__init__.py,0,"b'from .fc_middleware import FCMiddleware\nfrom .lstm_middleware import LSTMMiddleware\n\n__all__ = [""FCMiddleware"", ""LSTMMiddleware""]\n'"
rl_coach/architectures/tensorflow_components/middlewares/fc_middleware.py,1,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom typing import Union, List\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.middlewares.middleware import Middleware\nfrom rl_coach.base_parameters import MiddlewareScheme\nfrom rl_coach.core_types import Middleware_FC_Embedding\nfrom rl_coach.utils import force_list\n\n\nclass FCMiddleware(Middleware):\n    def __init__(self, activation_function=tf.nn.relu,\n                 scheme: MiddlewareScheme = MiddlewareScheme.Medium,\n                 batchnorm: bool = False, dropout_rate: float = 0.0,\n                 name=""middleware_fc_embedder"", dense_layer=Dense, is_training=False, num_streams: int = 1):\n        super().__init__(activation_function=activation_function, batchnorm=batchnorm,\n                         dropout_rate=dropout_rate, scheme=scheme, name=name, dense_layer=dense_layer,\n                         is_training=is_training)\n        self.return_type = Middleware_FC_Embedding\n\n        assert(isinstance(num_streams, int) and num_streams >= 1)\n        self.num_streams = num_streams\n\n    def _build_module(self):\n        self.output = []\n\n        for stream_idx in range(self.num_streams):\n            layers = [self.input]\n\n            for idx, layer_params in enumerate(self.layers_params):\n                layers.extend(force_list(\n                    layer_params(layers[-1], name=\'{}_{}\'.format(layer_params.__class__.__name__,\n                                                                 idx + stream_idx * len(self.layers_params)),\n                                 is_training=self.is_training)\n                ))\n            self.output.append((layers[-1]))\n\n    @property\n    def schemes(self):\n        return {\n            MiddlewareScheme.Empty:\n                [],\n\n            # ppo\n            MiddlewareScheme.Shallow:\n                [\n                    self.dense_layer(64)\n                ],\n\n            # dqn\n            MiddlewareScheme.Medium:\n                [\n                    self.dense_layer(512)\n                ],\n\n            MiddlewareScheme.Deep: \\\n                [\n                    self.dense_layer(128),\n                    self.dense_layer(128),\n                    self.dense_layer(128)\n                ]\n        }\n\n    def __str__(self):\n        stream = [str(l) for l in self.layers_params]\n        if self.layers_params:\n            if self.num_streams > 1:\n                stream = [\'\'] + [\'\\t\' + l for l in stream]\n                result = stream * self.num_streams\n                result[0::len(stream)] = [\'Stream {}\'.format(i) for i in range(self.num_streams)]\n            else:\n                result = stream\n            return \'\\n\'.join(result)\n        else:\n            return \'No layers\'\n'"
rl_coach/architectures/tensorflow_components/middlewares/lstm_middleware.py,9,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.architectures.tensorflow_components.middlewares.middleware import Middleware\nfrom rl_coach.base_parameters import MiddlewareScheme\nfrom rl_coach.core_types import Middleware_LSTM_Embedding\nfrom rl_coach.utils import force_list\n\n\nclass LSTMMiddleware(Middleware):\n    def __init__(self, activation_function=tf.nn.relu, number_of_lstm_cells: int=256,\n                 scheme: MiddlewareScheme = MiddlewareScheme.Medium,\n                 batchnorm: bool = False, dropout_rate: float = 0.0,\n                 name=""middleware_lstm_embedder"", dense_layer=Dense, is_training=False):\n        super().__init__(activation_function=activation_function, batchnorm=batchnorm,\n                         dropout_rate=dropout_rate, scheme=scheme, name=name, dense_layer=dense_layer,\n                         is_training=is_training)\n        self.return_type = Middleware_LSTM_Embedding\n        self.number_of_lstm_cells = number_of_lstm_cells\n        self.layers = []\n\n    def _build_module(self):\n        """"""\n        self.state_in: tuple of placeholders containing the initial state\n        self.state_out: tuple of output state\n\n        todo: it appears that the shape of the output is batch, feature\n        the code here seems to be slicing off the first element in the batch\n        which would definitely be wrong. need to double check the shape\n        """"""\n\n        self.layers.append(self.input)\n\n        # optionally insert some layers before the LSTM\n        for idx, layer_params in enumerate(self.layers_params):\n            self.layers.extend(force_list(\n                layer_params(self.layers[-1], name=\'fc{}\'.format(idx),\n                             is_training=self.is_training)\n            ))\n\n        # add the LSTM layer\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self.number_of_lstm_cells, state_is_tuple=True)\n        self.c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n        self.h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n        self.state_init = [self.c_init, self.h_init]\n        self.c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n        self.h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n        self.state_in = (self.c_in, self.h_in)\n        rnn_in = tf.expand_dims(self.layers[-1], [0])\n        step_size = tf.shape(self.layers[-1])[:1]\n        state_in = tf.nn.rnn_cell.LSTMStateTuple(self.c_in, self.h_in)\n        lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n            lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size, time_major=False)\n        lstm_c, lstm_h = lstm_state\n        self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n        self.output = tf.reshape(lstm_outputs, [-1, self.number_of_lstm_cells])\n\n    @property\n    def schemes(self):\n        return {\n            MiddlewareScheme.Empty:\n                [],\n\n            # ppo\n            MiddlewareScheme.Shallow:\n                [\n                    self.dense_layer(64)\n                ],\n\n            # dqn\n            MiddlewareScheme.Medium:\n                [\n                    self.dense_layer(512)\n                ],\n\n            MiddlewareScheme.Deep: \\\n                [\n                    self.dense_layer(128),\n                    self.dense_layer(128),\n                    self.dense_layer(128)\n                ]\n        }\n\n'"
rl_coach/architectures/tensorflow_components/middlewares/middleware.py,3,"b'#\n# Copyright (c) 2017 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport copy\nfrom typing import Union, Tuple\n\nimport tensorflow as tf\n\nfrom rl_coach.architectures.tensorflow_components.layers import BatchnormActivationDropout, convert_layer, Dense\nfrom rl_coach.base_parameters import MiddlewareScheme, NetworkComponentParameters\nfrom rl_coach.core_types import MiddlewareEmbedding\n\n\nclass Middleware(object):\n    """"""\n    A middleware embedder is the middle part of the network. It takes the embeddings from the input embedders,\n    after they were aggregated in some method (for example, concatenation) and passes it through a neural network\n    which can be customizable but shared between the heads of the network\n    """"""\n    def __init__(self, activation_function=tf.nn.relu,\n                 scheme: MiddlewareScheme = MiddlewareScheme.Medium,\n                 batchnorm: bool = False, dropout_rate: float = 0.0, name=""middleware_embedder"", dense_layer=Dense,\n                 is_training=False):\n        self.name = name\n        self.input = None\n        self.output = None\n        self.activation_function = activation_function\n        self.batchnorm = batchnorm\n        self.dropout_rate = dropout_rate\n        self.scheme = scheme\n        self.return_type = MiddlewareEmbedding\n        self.dense_layer = dense_layer\n        if self.dense_layer is None:\n            self.dense_layer = Dense\n        self.is_training = is_training\n\n        # layers order is conv -> batchnorm -> activation -> dropout\n        if isinstance(self.scheme, MiddlewareScheme):\n            self.layers_params = copy.copy(self.schemes[self.scheme])\n            self.layers_params = [convert_layer(l) for l in self.layers_params]\n        else:\n            # if scheme is specified directly, convert to TF layer if it\'s not a callable object\n            # NOTE: if layer object is callable, it must return a TF tensor when invoked\n            self.layers_params = [convert_layer(l) for l in copy.copy(self.scheme)]\n\n        # we allow adding batchnorm, dropout or activation functions after each layer.\n        # The motivation is to simplify the transition between a network with batchnorm and a network without\n        # batchnorm to a single flag (the same applies to activation function and dropout)\n        if self.batchnorm or self.activation_function or self.dropout_rate > 0:\n            for layer_idx in reversed(range(len(self.layers_params))):\n                self.layers_params.insert(layer_idx+1,\n                                          BatchnormActivationDropout(batchnorm=self.batchnorm,\n                                                                     activation_function=self.activation_function,\n                                                                     dropout_rate=self.dropout_rate))\n\n    def __call__(self, input_layer: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n        """"""\n        Wrapper for building the module graph including scoping and loss creation\n        :param input_layer: the input to the graph\n        :return: the input placeholder and the output of the last layer\n        """"""\n        with tf.variable_scope(self.get_name()):\n            self.input = input_layer\n            self._build_module()\n\n        return self.input, self.output\n\n    def _build_module(self) -> None:\n        """"""\n        Builds the graph of the module\n        This method is called early on from __call__. It is expected to store the graph\n        in self.output.\n        :param input_layer: the input to the graph\n        :return: None\n        """"""\n        pass\n\n    def get_name(self) -> str:\n        """"""\n        Get a formatted name for the module\n        :return: the formatted name\n        """"""\n        return self.name\n\n    @property\n    def schemes(self):\n        raise NotImplementedError(""Inheriting middleware must define schemes matching its allowed default ""\n                                  ""configurations."")\n\n    def __str__(self):\n        result = [str(l) for l in self.layers_params]\n        if self.layers_params:\n            return \'\\n\'.join(result)\n        else:\n            return \'No layers\'\n'"
rl_coach/environments/mujoco/common/__init__.py,0,"b'# Copyright 2017 The dm_control Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Functions to manage the common assets for domains.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom dm_control.utils import resources\n\n_SUITE_DIR = os.path.dirname(os.path.dirname(__file__))\n_FILENAMES = [\n    ""common/materials.xml"",\n    ""common/skybox.xml"",\n    ""common/visual.xml"",\n]\n\nASSETS = {filename: resources.GetResource(os.path.join(_SUITE_DIR, filename))\n          for filename in _FILENAMES}\n\n\ndef read_model(model_filename):\n  """"""Reads a model XML file and returns its contents as a string.""""""\n  return resources.GetResource(os.path.join(_SUITE_DIR, model_filename))\n'"
rl_coach/tests/architectures/mxnet_components/__init__.py,0,b''
rl_coach/tests/architectures/mxnet_components/test_utils.py,0,"b""import pytest\n\nimport mxnet as mx\nfrom mxnet import nd\nimport numpy as np\n\nfrom rl_coach.architectures.mxnet_components.utils import *\n\n\n@pytest.mark.unit_test\ndef test_to_mx_ndarray():\n    # scalar\n    assert to_mx_ndarray(1.2) == nd.array([1.2])\n    # list of one scalar\n    assert to_mx_ndarray([1.2]) == [nd.array([1.2])]\n    # list of multiple scalars\n    assert to_mx_ndarray([1.2, 3.4]) == [nd.array([1.2]), nd.array([3.4])]\n    # list of lists of scalars\n    assert to_mx_ndarray([[1.2], [3.4]]) == [[nd.array([1.2])], [nd.array([3.4])]]\n    # numpy\n    assert np.array_equal(to_mx_ndarray(np.array([[1.2], [3.4]])).asnumpy(), nd.array([[1.2], [3.4]]).asnumpy())\n    # tuple\n    assert to_mx_ndarray(((1.2,), (3.4,))) == ((nd.array([1.2]),), (nd.array([3.4]),))\n\n\n@pytest.mark.unit_test\ndef test_asnumpy_or_asscalar():\n    # scalar float32\n    assert asnumpy_or_asscalar(nd.array([1.2])) == np.float32(1.2)\n    # scalar int32\n    assert asnumpy_or_asscalar(nd.array([2], dtype=np.int32)) == np.int32(2)\n    # list of one scalar\n    assert asnumpy_or_asscalar([nd.array([1.2])]) == [np.float32(1.2)]\n    # list of multiple scalars\n    assert asnumpy_or_asscalar([nd.array([1.2]), nd.array([3.4])]) == [np.float32([1.2]), np.float32([3.4])]\n    # list of lists of scalars\n    assert asnumpy_or_asscalar([[nd.array([1.2])], [nd.array([3.4])]]) == [[np.float32([1.2])], [np.float32([3.4])]]\n    # tensor\n    assert np.array_equal(asnumpy_or_asscalar(nd.array([[1.2], [3.4]])), np.array([[1.2], [3.4]], dtype=np.float32))\n    # tuple\n    assert (asnumpy_or_asscalar(((nd.array([1.2]),), (nd.array([3.4]),))) ==\n            ((np.array([1.2], dtype=np.float32),), (np.array([3.4], dtype=np.float32),)))\n\n\n@pytest.mark.unit_test\ndef test_global_norm():\n    data = list()\n    for i in range(1, 6):\n        data.append(np.ones((i * 10, i * 10)) * i)\n    gnorm = np.asscalar(np.sqrt(sum([np.sum(np.square(d)) for d in data])))\n    assert np.isclose(gnorm, global_norm([nd.array(d) for d in data]).asscalar())\n\n\n@pytest.mark.unit_test\ndef test_split_outputs_per_head():\n    class TestHead:\n        def __init__(self, num_outputs):\n            self.num_outputs = num_outputs\n\n    assert split_outputs_per_head((1, 2, 3, 4), [TestHead(2), TestHead(1), TestHead(1)]) == [[1, 2], [3], [4]]\n\n\nclass DummySchema:\n    def __init__(self, num_head_outputs, num_agent_inputs, num_targets):\n        self.head_outputs = ['head_output_{}'.format(i) for i in range(num_head_outputs)]\n        self.agent_inputs = ['agent_input_{}'.format(i) for i in range(num_agent_inputs)]\n        self.targets = ['target_{}'.format(i) for i in range(num_targets)]\n\n\nclass DummyLoss:\n    def __init__(self, num_head_outputs, num_agent_inputs, num_targets):\n        self.input_schema = DummySchema(num_head_outputs, num_agent_inputs, num_targets)\n\n\n@pytest.mark.unit_test\ndef test_split_targets_per_loss():\n    assert split_targets_per_loss([1, 2, 3, 4],\n                                  [DummyLoss(10, 100, 2), DummyLoss(20, 200, 1), DummyLoss(30, 300, 1)]) == \\\n           [[1, 2], [3], [4]]\n\n\n@pytest.mark.unit_test\ndef test_get_loss_agent_inputs():\n    input_dict = {'output_0_0': [1, 2], 'output_0_1': [3, 4], 'output_1_0': [5]}\n    assert get_loss_agent_inputs(input_dict, 0, DummyLoss(10, 2, 100)) == [[1, 2], [3, 4]]\n    assert get_loss_agent_inputs(input_dict, 1, DummyLoss(20, 1, 200)) == [[5]]\n\n\n@pytest.mark.unit_test\ndef test_align_loss_args():\n    class TestLossFwd(DummyLoss):\n        def __init__(self, num_targets, num_agent_inputs, num_head_outputs):\n            super(TestLossFwd, self).__init__(num_targets, num_agent_inputs, num_head_outputs)\n\n        def loss_forward(self, F, head_output_2, head_output_1, agent_input_2, target_0, agent_input_1, param1, param2):\n            pass\n\n    assert align_loss_args([1, 2, 3], [4, 5, 6, 7], [8, 9], TestLossFwd(3, 4, 2)) == [3, 2, 6, 8, 5]\n\n\n@pytest.mark.unit_test\ndef test_to_tuple():\n    assert to_tuple(123) == (123,)\n    assert to_tuple((1, 2, 3)) == (1, 2, 3)\n    assert to_tuple([1, 2, 3]) == (1, 2, 3)\n\n\n@pytest.mark.unit_test\ndef test_to_list():\n    assert to_list(123) == [123]\n    assert to_list((1, 2, 3)) == [1, 2, 3]\n    assert to_list([1, 2, 3]) == [1, 2, 3]\n\n\n@pytest.mark.unit_test\ndef test_loss_output_dict():\n    assert loss_output_dict([1, 2, 3], ['loss', 'loss', 'reg']) == {'loss': [1, 2], 'reg': [3]}\n\n\n@pytest.mark.unit_test\ndef test_clip_grad():\n    a = np.array([1, 2, -3])\n    b = np.array([4, 5, -6])\n    clip = 2\n    gscale = np.minimum(1.0, clip / np.sqrt(np.sum(np.square(a)) + np.sum(np.square(b))))\n    for lhs, rhs in zip(clip_grad([nd.array(a), nd.array(b)], GradientClippingMethod.ClipByGlobalNorm, clip_val=clip),\n                        [a, b]):\n        assert np.allclose(lhs.asnumpy(), rhs * gscale)\n    for lhs, rhs in zip(clip_grad([nd.array(a), nd.array(b)], GradientClippingMethod.ClipByValue, clip_val=clip),\n                        [a, b]):\n        assert np.allclose(lhs.asnumpy(), np.clip(rhs, -clip, clip))\n    for lhs, rhs in zip(clip_grad([nd.array(a), nd.array(b)], GradientClippingMethod.ClipByNorm, clip_val=clip),\n                        [a, b]):\n        scale = np.minimum(1.0, clip / np.sqrt(np.sum(np.square(rhs))))\n        assert np.allclose(lhs.asnumpy(), rhs * scale)\n\n\n@pytest.mark.unit_test\ndef test_hybrid_clip():\n    x = mx.nd.array((0.5, 1.5, 2.5))\n    a = mx.nd.array((1,))\n    b = mx.nd.array((2,))\n    clipped = hybrid_clip(F=mx.nd, x=x, clip_lower=a, clip_upper=b)\n    assert (np.isclose(a=clipped.asnumpy(), b=(1, 1.5, 2))).all()\n\n\n@pytest.mark.unit_test\ndef test_broadcast_like():\n    x = nd.ones((1, 2)) * 10\n    y = nd.ones((100, 100, 2)) * 20\n    assert mx.test_utils.almost_equal(x.broadcast_like(y).asnumpy(), broadcast_like(nd, x, y).asnumpy())\n\n\n@pytest.mark.unit_test\ndef test_scoped_onxx_enable():\n    class Counter(object):\n        def __init__(self):\n            self._count = 0\n\n        def increment(self):\n            self._count += 1\n\n        @property\n        def count(self):\n            return self._count\n\n    class TempBlock(gluon.HybridBlock, OnnxHandlerBlock):\n        def __init__(self, counter: Counter):\n            super(TempBlock, self).__init__()\n            OnnxHandlerBlock.__init__(self)\n            self._counter = counter\n\n        def hybrid_forward(self, F, x, *args, **kwargs):\n            if self._onnx:\n                self._counter.increment()\n            return x\n\n    counter = Counter()\n    net = gluon.nn.HybridSequential()\n    for _ in range(10):\n        net.add(TempBlock(counter))\n\n    # ONNX disabled\n    net(nd.zeros((1,)))\n    assert counter.count == 0\n\n    # ONNX enabled\n    with ScopedOnnxEnable(net):\n        net(nd.zeros((1,)))\n    assert counter.count == 10\n"""
rl_coach/tests/architectures/tensorflow_components/__init__.py,0,b''
rl_coach/tests/filters/action/__init__.py,0,b''
rl_coach/tests/filters/action/test_attention_discretization.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nfrom rl_coach.filters.action.attention_discretization import AttentionDiscretization\nfrom rl_coach.spaces import BoxActionSpace, DiscreteActionSpace, AttentionActionSpace\nimport numpy as np\n\n\n@pytest.mark.unit_test\ndef test_filter():\n    filter = AttentionDiscretization(2)\n\n    # passing an output space that is wrong\n    with pytest.raises(ValueError):\n        filter.validate_output_action_space(DiscreteActionSpace(10))\n    with pytest.raises(ValueError):\n        filter.validate_output_action_space(BoxActionSpace(10))\n\n    # 1 dimensional box\n    output_space = AttentionActionSpace(2, 0, 83)\n    input_space = filter.get_unfiltered_action_space(output_space)\n\n    assert np.all(filter.target_actions == np.array([[[0., 0.], [41.5, 41.5]],\n                                     [[0., 41.5], [41.5, 83.]],\n                                     [[41.5, 0], [83., 41.5]],\n                                     [[41.5, 41.5], [83., 83.]]]))\n    assert input_space.actions == list(range(4))\n\n    action = 2\n\n    result = filter.filter(action)\n    assert np.all(result == np.array([[41.5, 0], [83., 41.5]]))\n    assert output_space.contains(result)\n\n    # force int bins\n    filter = AttentionDiscretization(2, force_int_bins=True)\n    input_space = filter.get_unfiltered_action_space(output_space)\n\n    assert np.all(filter.target_actions == np.array([[[0., 0.], [41, 41]],\n                                                     [[0., 41], [41, 83.]],\n                                                     [[41, 0], [83., 41]],\n                                                     [[41, 41], [83., 83.]]]))\n'"
rl_coach/tests/filters/action/test_box_discretization.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nfrom rl_coach.filters.action.box_discretization import BoxDiscretization\nfrom rl_coach.spaces import BoxActionSpace, DiscreteActionSpace\n\n\n@pytest.mark.unit_test\ndef test_filter():\n    filter = BoxDiscretization(9)\n\n    # passing an output space that is wrong\n    with pytest.raises(ValueError):\n        filter.validate_output_action_space(DiscreteActionSpace(10))\n\n    # 1 dimensional box\n    output_space = BoxActionSpace(1, 5, 15)\n    input_space = filter.get_unfiltered_action_space(output_space)\n\n    assert filter.target_actions == [[5.], [6.25], [7.5], [8.75], [10.], [11.25], [12.5], [13.75], [15.]]\n    assert input_space.actions == list(range(9))\n\n    action = 2\n\n    result = filter.filter(action)\n    assert result == [7.5]\n    assert output_space.contains(result)\n\n    # 2 dimensional box\n    filter = BoxDiscretization(3)\n    output_space = BoxActionSpace(2, 5, 15)\n    input_space = filter.get_unfiltered_action_space(output_space)\n\n    assert filter.target_actions == [[5., 5.], [5., 10.], [5., 15.],\n                                     [10., 5.], [10., 10.], [10., 15.],\n                                     [15., 5.], [15., 10.], [15., 15.]]\n    assert input_space.actions == list(range(9))\n\n    action = 2\n\n    result = filter.filter(action)\n    assert result == [5., 15.]\n    assert output_space.contains(result)\n'"
rl_coach/tests/filters/action/test_box_masking.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nfrom rl_coach.filters.action.box_masking import BoxMasking\nfrom rl_coach.spaces import BoxActionSpace, DiscreteActionSpace\nimport numpy as np\n\n\n@pytest.mark.unit_test\ndef test_filter():\n    filter = BoxMasking(10, 20)\n\n    # passing an output space that is wrong\n    with pytest.raises(ValueError):\n        filter.validate_output_action_space(DiscreteActionSpace(10))\n\n    # 1 dimensional box\n    output_space = BoxActionSpace(1, 5, 30)\n    input_space = filter.get_unfiltered_action_space(output_space)\n\n    action = np.array([2])\n    result = filter.filter(action)\n    assert result == np.array([12])\n    assert output_space.contains(result)\n\n'"
rl_coach/tests/filters/action/test_linear_box_to_box_map.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nfrom rl_coach.filters.action.linear_box_to_box_map import LinearBoxToBoxMap\nfrom rl_coach.spaces import BoxActionSpace, DiscreteActionSpace\nimport numpy as np\n\n\n@pytest.mark.unit_test\ndef test_filter():\n    filter = LinearBoxToBoxMap(10, 20)\n\n    # passing an output space that is wrong\n    with pytest.raises(ValueError):\n        filter.validate_output_action_space(DiscreteActionSpace(10))\n\n    # 1 dimensional box\n    output_space = BoxActionSpace(1, 5, 35)\n    input_space = filter.get_unfiltered_action_space(output_space)\n\n    action = np.array([2])\n\n    action = np.array([12])\n    result = filter.filter(action)\n    assert result == np.array([11])\n    assert output_space.contains(result)\n\n'"
rl_coach/tests/filters/observation/__init__.py,0,b''
rl_coach/tests/filters/observation/test_observation_crop_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_crop_filter import ObservationCropFilter\nfrom rl_coach.filters.filter import InputFilter\nfrom rl_coach.spaces import ObservationSpace\nfrom rl_coach.core_types import EnvResponse\n\n\n@pytest.fixture\ndef env_response():\n    observation = np.random.rand(10, 20, 30)\n    return EnvResponse(next_state={'observation': observation}, reward=0, game_over=False)\n\n\n@pytest.mark.unit_test\ndef test_filter(env_response):\n    crop_low = np.array([0, 5, 10])\n    crop_high = np.array([5, 10, 20])\n    crop_filter = InputFilter()\n    crop_filter.add_observation_filter('observation', 'crop', ObservationCropFilter(crop_low, crop_high))\n\n    result = crop_filter.filter(env_response)[0]\n    unfiltered_observation = env_response.next_state['observation']\n    filtered_observation = result.next_state['observation']\n\n    # validate the shape of the filtered observation\n    assert filtered_observation.shape == (5, 5, 10)\n\n    # validate the content of the filtered observation\n    assert np.all(filtered_observation == unfiltered_observation[0:5, 5:10, 10:20])\n\n    # crop with -1 on some axes\n    crop_low = np.array([0, 0, 0])\n    crop_high = np.array([5, -1, -1])\n    crop_filter = InputFilter()\n    crop_filter.add_observation_filter('observation', 'crop', ObservationCropFilter(crop_low, crop_high))\n\n    result = crop_filter.filter(env_response)[0]\n    unfiltered_observation = env_response.next_state['observation']\n    filtered_observation = result.next_state['observation']\n\n    # validate the shape of the filtered observation\n    assert filtered_observation.shape == (5, 20, 30)\n\n    # validate the content of the filtered observation\n    assert np.all(filtered_observation == unfiltered_observation[0:5, :, :])\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_observation_space():\n    crop_low = np.array([0, 5, 10])\n    crop_high = np.array([5, 10, 20])\n    crop_filter = InputFilter()\n    crop_filter.add_observation_filter('observation', 'crop', ObservationCropFilter(crop_low, crop_high))\n\n    observation_space = ObservationSpace(np.array([5, 10, 20]))\n    filtered_observation_space = crop_filter.get_filtered_observation_space('observation', observation_space)\n\n    # make sure the new observation space shape is calculated correctly\n    assert np.all(filtered_observation_space.shape == np.array([5, 5, 10]))\n\n    # make sure the original observation space is unchanged\n    assert np.all(observation_space.shape == np.array([5, 10, 20]))\n\n    # crop_high is bigger than the observation space\n    high_error_observation_space = ObservationSpace(np.array([3, 8, 14]))\n    with pytest.raises(ValueError):\n        crop_filter.get_filtered_observation_space('observation', high_error_observation_space)\n\n    # crop_low is bigger than the observation space\n    low_error_observation_space = ObservationSpace(np.array([3, 3, 10]))\n    with pytest.raises(ValueError):\n        crop_filter.get_filtered_observation_space('observation', low_error_observation_space)\n\n    # crop with -1 on some axes\n    crop_low = np.array([0, 0, 0])\n    crop_high = np.array([5, -1, -1])\n    crop_filter = InputFilter()\n    crop_filter.add_observation_filter('observation', 'crop', ObservationCropFilter(crop_low, crop_high))\n\n    observation_space = ObservationSpace(np.array([5, 10, 20]))\n    filtered_observation_space = crop_filter.get_filtered_observation_space('observation', observation_space)\n\n    # make sure the new observation space shape is calculated correctly\n    assert np.all(filtered_observation_space.shape == np.array([5, 10, 20]))\n"""
rl_coach/tests/filters/observation/test_observation_reduction_by_sub_parts_name_filter.py,0,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_reduction_by_sub_parts_name_filter import ObservationReductionBySubPartsNameFilter\nfrom rl_coach.spaces import VectorObservationSpace\nfrom rl_coach.core_types import EnvResponse\nfrom rl_coach.filters.filter import InputFilter\n\n\n@pytest.mark.unit_test\ndef test_filter():\n    # Keep\n    observation_space = VectorObservationSpace(3, measurements_names=[\'a\', \'b\', \'c\'])\n    env_response = EnvResponse(next_state={\'observation\': np.ones([3])}, reward=0, game_over=False)\n    reduction_filter = InputFilter()\n    reduction_filter.add_observation_filter(\'observation\', \'reduce\',\n                                          ObservationReductionBySubPartsNameFilter(\n                                              [""a""],\n                                              ObservationReductionBySubPartsNameFilter.ReductionMethod.Keep\n                                          ))\n\n    reduction_filter.get_filtered_observation_space(\'observation\', observation_space)\n    result = reduction_filter.filter(env_response)[0]\n    unfiltered_observation = env_response.next_state[\'observation\']\n    filtered_observation = result.next_state[\'observation\']\n\n    # make sure the original observation is unchanged\n    assert unfiltered_observation.shape == (3,)\n\n    # validate the shape of the filtered observation\n    assert filtered_observation.shape == (1,)\n\n    # Discard\n    reduction_filter = InputFilter()\n    reduction_filter.add_observation_filter(\'observation\', \'reduce\',\n                                          ObservationReductionBySubPartsNameFilter(\n                                              [""a""],\n                                              ObservationReductionBySubPartsNameFilter.ReductionMethod.Discard\n                                          ))\n    reduction_filter.get_filtered_observation_space(\'observation\', observation_space)\n    result = reduction_filter.filter(env_response)[0]\n    unfiltered_observation = env_response.next_state[\'observation\']\n    filtered_observation = result.next_state[\'observation\']\n\n    # make sure the original observation is unchanged\n    assert unfiltered_observation.shape == (3,)\n\n    # validate the shape of the filtered observation\n    assert filtered_observation.shape == (2,)\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_observation_space():\n    # Keep\n    observation_space = VectorObservationSpace(3, measurements_names=[\'a\', \'b\', \'c\'])\n    env_response = EnvResponse(next_state={\'observation\': np.ones([3])}, reward=0, game_over=False)\n    reduction_filter = InputFilter()\n    reduction_filter.add_observation_filter(\'observation\', \'reduce\',\n                                            ObservationReductionBySubPartsNameFilter(\n                                                [""a""],\n                                                ObservationReductionBySubPartsNameFilter.ReductionMethod.Keep\n                                            ))\n\n    filtered_observation_space = reduction_filter.get_filtered_observation_space(\'observation\', observation_space)\n    assert np.all(filtered_observation_space.shape == np.array([1]))\n    assert filtered_observation_space.measurements_names == [\'a\']\n\n    # Discard\n    observation_space = VectorObservationSpace(3, measurements_names=[\'a\', \'b\', \'c\'])\n    env_response = EnvResponse(next_state={\'observation\': np.ones([3])}, reward=0, game_over=False)\n    reduction_filter = InputFilter()\n    reduction_filter.add_observation_filter(\'observation\', \'reduce\',\n                                            ObservationReductionBySubPartsNameFilter(\n                                                [""a""],\n                                                ObservationReductionBySubPartsNameFilter.ReductionMethod.Discard\n                                            ))\n\n    filtered_observation_space = reduction_filter.get_filtered_observation_space(\'observation\', observation_space)\n    assert np.all(filtered_observation_space.shape == np.array([2]))\n    assert filtered_observation_space.measurements_names == [\'b\', \'c\']\n'"
rl_coach/tests/filters/observation/test_observation_rescale_size_by_factor_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_rescale_size_by_factor_filter import ObservationRescaleSizeByFactorFilter\nfrom rl_coach.spaces import ObservationSpace\nfrom rl_coach.core_types import EnvResponse\nfrom rl_coach.filters.filter import InputFilter\n\n@pytest.mark.filterwarnings('ignore:Conversion of')\n@pytest.mark.unit_test\ndef test_filter():\n    # make an RGB observation smaller\n    env_response = EnvResponse(next_state={'observation': np.ones([20, 30, 3])}, reward=0, game_over=False)\n    rescale_filter = InputFilter()\n    rescale_filter.add_observation_filter('observation', 'rescale',\n                                          ObservationRescaleSizeByFactorFilter(0.5))\n\n    result = rescale_filter.filter(env_response)[0]\n    unfiltered_observation = env_response.next_state['observation']\n    filtered_observation = result.next_state['observation']\n\n    # make sure the original observation is unchanged\n    assert unfiltered_observation.shape == (20, 30, 3)\n\n    # validate the shape of the filtered observation\n    assert filtered_observation.shape == (10, 15, 3)\n\n    # make a grayscale observation bigger\n    env_response = EnvResponse(next_state={'observation': np.ones([20, 30])}, reward=0, game_over=False)\n    rescale_filter = InputFilter()\n    rescale_filter.add_observation_filter('observation', 'rescale',\n                                          ObservationRescaleSizeByFactorFilter(2))\n    result = rescale_filter.filter(env_response)[0]\n    filtered_observation = result.next_state['observation']\n\n    # validate the shape of the filtered observation\n    assert filtered_observation.shape == (40, 60)\n    assert np.all(filtered_observation == np.ones([40, 60]))\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_observation_space():\n    # error on wrong number of channels\n    rescale_filter = InputFilter()\n    rescale_filter.add_observation_filter('observation', 'rescale',\n                                          ObservationRescaleSizeByFactorFilter(0.5))\n    observation_space = ObservationSpace(np.array([10, 20, 5]))\n    with pytest.raises(ValueError):\n        filtered_observation_space = rescale_filter.get_filtered_observation_space('observation', observation_space)\n\n    # error on wrong number of dimensions\n    observation_space = ObservationSpace(np.array([10, 20, 10, 3]))\n    with pytest.raises(ValueError):\n        filtered_observation_space = rescale_filter.get_filtered_observation_space('observation', observation_space)\n\n    # make sure the new observation space shape is calculated correctly\n    observation_space = ObservationSpace(np.array([10, 20, 3]))\n    filtered_observation_space = rescale_filter.get_filtered_observation_space('observation', observation_space)\n    assert np.all(filtered_observation_space.shape == np.array([5, 10, 3]))\n\n    # make sure the original observation space is unchanged\n    assert np.all(observation_space.shape == np.array([10, 20, 3]))\n\nif __name__ == '__main__':\n    test_filter()"""
rl_coach/tests/filters/observation/test_observation_rescale_to_size_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_rescale_to_size_filter import ObservationRescaleToSizeFilter\nfrom rl_coach.spaces import ObservationSpace, ImageObservationSpace, PlanarMapsObservationSpace\nfrom rl_coach.core_types import EnvResponse\nfrom rl_coach.filters.filter import InputFilter\n\n\n@pytest.mark.filterwarnings('ignore:Conversion of')\n@pytest.mark.unit_test\ndef test_filter():\n    # make an RGB observation smaller\n    transition = EnvResponse(next_state={'observation': np.ones([20, 30, 3])}, reward=0, game_over=False)\n    rescale_filter = InputFilter()\n    rescale_filter.add_observation_filter('observation', 'rescale',\n                                          ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([10, 20, 3]),\n                                                                                               high=255)))\n\n    result = rescale_filter.filter(transition)[0]\n    unfiltered_observation = transition.next_state['observation']\n    filtered_observation = result.next_state['observation']\n\n    # make sure the original observation is unchanged\n    assert unfiltered_observation.shape == (20, 30, 3)\n\n    # validate the shape of the filtered observation\n    assert filtered_observation.shape == (10, 20, 3)\n    assert np.all(filtered_observation == np.ones([10, 20, 3]))\n\n    # make a grayscale observation bigger\n    transition = EnvResponse(next_state={'observation': np.ones([20, 30])}, reward=0, game_over=False)\n    rescale_filter = InputFilter()\n    rescale_filter.add_observation_filter('observation', 'rescale',\n                                         ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([40, 60]),\n                                                                                              high=255)))\n    result = rescale_filter.filter(transition)[0]\n    filtered_observation = result.next_state['observation']\n\n    # validate the shape of the filtered observation\n    assert filtered_observation.shape == (40, 60)\n    assert np.all(filtered_observation == np.ones([40, 60]))\n\n    # rescale channels -> error\n    # with pytest.raises(ValueError):\n    #     InputFilter(\n    #         observation_filters=OrderedDict([('rescale',\n    #                                          ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([10, 20, 1]),\n    #                                                                                               high=255)\n    #                                                                        ))]))\n\n    # TODO: validate input to filter\n    # different number of axes -> error\n    # env_response = EnvResponse(state={'observation': np.ones([20, 30, 3])}, reward=0, game_over=False)\n    # rescale_filter = ObservationRescaleToSizeFilter(ObservationSpace(np.array([10, 20]))\n    #                                                 )\n    # with pytest.raises(ValueError):\n    #     result = rescale_filter.filter(transition)\n\n    # channels first -> error\n    with pytest.raises(ValueError):\n        ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([3, 10, 20]), high=255))\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_observation_space():\n    # error on wrong number of channels\n    with pytest.raises(ValueError):\n        observation_filters = InputFilter()\n        observation_filters.add_observation_filter('observation', 'rescale',\n                                             ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([5, 10, 5]),\n                                                                                                  high=255)))\n\n    # mismatch and wrong number of channels\n    rescale_filter = InputFilter()\n    rescale_filter.add_observation_filter('observation', 'rescale',\n                                         ObservationRescaleToSizeFilter(ImageObservationSpace(np.array([5, 10, 3]),\n                                                                                              high=255)))\n\n    observation_space = PlanarMapsObservationSpace(np.array([10, 20, 5]), low=0, high=255)\n    with pytest.raises(ValueError):\n        rescale_filter.get_filtered_observation_space('observation', observation_space)\n\n    # error on wrong number of dimensions\n    observation_space = ObservationSpace(np.array([10, 20, 10, 3]), high=255)\n    with pytest.raises(ValueError):\n        rescale_filter.get_filtered_observation_space('observation', observation_space)\n\n    # make sure the new observation space shape is calculated correctly\n    observation_space = ImageObservationSpace(np.array([10, 20, 3]), high=255)\n    filtered_observation_space = rescale_filter.get_filtered_observation_space('observation', observation_space)\n    assert np.all(filtered_observation_space.shape == np.array([5, 10, 3]))\n\n    # make sure the original observation space is unchanged\n    assert np.all(observation_space.shape == np.array([10, 20, 3]))\n\n    # TODO: test that the type of the observation space stays the same\n"""
rl_coach/tests/filters/observation/test_observation_rgb_to_y_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_rgb_to_y_filter import ObservationRGBToYFilter\nfrom rl_coach.spaces import ObservationSpace\nfrom rl_coach.core_types import EnvResponse\n\nfrom rl_coach.filters.filter import InputFilter\n\n@pytest.fixture\ndef rgb_to_y_filter():\n    rgb_to_y_filter = InputFilter()\n    rgb_to_y_filter.add_observation_filter('observation', 'rgb_to_y', ObservationRGBToYFilter())\n    return rgb_to_y_filter\n\n\n@pytest.mark.unit_test\ndef test_filter(rgb_to_y_filter):\n    # convert RGB observation to graysacle\n    observation = np.random.rand(20, 30, 3)*255.0\n    transition = EnvResponse(next_state={'observation': observation}, reward=0, game_over=False)\n\n    result = rgb_to_y_filter.filter(transition)[0]\n    unfiltered_observation = transition.next_state['observation']\n    filtered_observation = result.next_state['observation']\n\n    # make sure the original observation is unchanged\n    assert unfiltered_observation.shape == (20, 30, 3)\n\n    # make sure the filtering is done correctly\n    assert filtered_observation.shape == (20, 30)\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_observation_space(rgb_to_y_filter):\n    # error on observation space which are not RGB\n    observation_space = ObservationSpace(np.array([1, 2, 4]), 0, 100)\n    with pytest.raises(ValueError):\n        rgb_to_y_filter.get_filtered_observation_space('observation', observation_space)\n\n    observation_space = ObservationSpace(np.array([1, 2, 3]), 0, 100)\n    result = rgb_to_y_filter.get_filtered_observation_space('observation', observation_space)\n    assert np.all(result.shape == np.array([1, 2]))\n"""
rl_coach/tests/filters/observation/test_observation_squeeze_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_squeeze_filter import ObservationSqueezeFilter\nfrom rl_coach.spaces import ObservationSpace\nfrom rl_coach.core_types import EnvResponse\nfrom rl_coach.filters.filter import InputFilter\n\n\n@pytest.mark.unit_test\ndef test_filter():\n    # make an RGB observation smaller\n    squeeze_filter = InputFilter()\n    squeeze_filter.add_observation_filter('observation', 'squeeze', ObservationSqueezeFilter())\n    squeeze_filter_with_axis = InputFilter()\n    squeeze_filter_with_axis.add_observation_filter('observation', 'squeeze', ObservationSqueezeFilter(2))\n\n    observation = np.random.rand(20, 30, 1, 3)\n    env_response = EnvResponse(next_state={'observation': observation}, reward=0, game_over=False)\n\n    result = squeeze_filter.filter(env_response)[0]\n    result_with_axis = squeeze_filter_with_axis.filter(env_response)[0]\n    unfiltered_observation_shape = env_response.next_state['observation'].shape\n    filtered_observation_shape = result.next_state['observation'].shape\n    filtered_observation_with_axis_shape = result_with_axis.next_state['observation'].shape\n\n    # make sure the original observation is unchanged\n    assert unfiltered_observation_shape == observation.shape\n\n    # make sure the filtering is done correctly\n    assert filtered_observation_shape == (20, 30, 3)\n    assert filtered_observation_with_axis_shape == (20, 30, 3)\n\n    observation = np.random.rand(1, 30, 1, 3)\n    env_response = EnvResponse(next_state={'observation': observation}, reward=0, game_over=False)\n\n    result = squeeze_filter.filter(env_response)[0]\n    assert result.next_state['observation'].shape == (30, 3)\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_observation_space():\n    # error on observation space with shape not matching the filter squeeze axis configuration\n    squeeze_filter = InputFilter()\n    squeeze_filter.add_observation_filter('observation', 'squeeze', ObservationSqueezeFilter(axis=3))\n\n    observation_space = ObservationSpace(np.array([20, 1, 30, 3]), 0, 100)\n    small_observation_space = ObservationSpace(np.array([20, 1, 30]), 0, 100)\n    with pytest.raises(ValueError):\n        squeeze_filter.get_filtered_observation_space('observation', observation_space)\n        squeeze_filter.get_filtered_observation_space('observation', small_observation_space)\n\n    # verify output observation space is correct\n    observation_space = ObservationSpace(np.array([1, 2, 3, 1]), 0, 200)\n    result = squeeze_filter.get_filtered_observation_space('observation', observation_space)\n    assert np.all(result.shape == np.array([1, 2, 3]))\n\n    squeeze_filter = InputFilter()\n    squeeze_filter.add_observation_filter('observation', 'squeeze', ObservationSqueezeFilter())\n\n    result = squeeze_filter.get_filtered_observation_space('observation', observation_space)\n    assert np.all(result.shape == np.array([2, 3]))\n\n\nif __name__ == '__main__':\n    test_filter()\n    test_get_filtered_observation_space()\n\n"""
rl_coach/tests/filters/observation/test_observation_stacking_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_stacking_filter import ObservationStackingFilter\nfrom rl_coach.spaces import ObservationSpace\nfrom rl_coach.core_types import EnvResponse\nfrom rl_coach.filters.filter import InputFilter\n\n\n@pytest.fixture\ndef env_response():\n    observation = np.random.rand(20, 30, 1)\n    return EnvResponse(next_state={'observation': observation}, reward=0, game_over=False)\n\n\n@pytest.fixture\ndef stack_filter():\n    stack_filter = InputFilter()\n    stack_filter.add_observation_filter('observation', 'stack', ObservationStackingFilter(4, stacking_axis=-1))\n    return stack_filter\n\n\n@pytest.mark.unit_test\ndef test_filter(stack_filter, env_response):\n    # stack observation on empty stack\n    result = stack_filter.filter(env_response)[0]\n    unfiltered_observation = env_response.next_state['observation']\n    filtered_observation = result.next_state['observation']\n\n    # validate that the shape of the unfiltered observation is unchanged\n    assert unfiltered_observation.shape == (20, 30, 1)\n    assert np.array(filtered_observation).shape == (20, 30, 1, 4)\n    assert np.all(np.array(filtered_observation)[:, :, :, -1] == unfiltered_observation)\n\n    # stack observation on non-empty stack\n    result = stack_filter.filter(env_response)[0]\n    filtered_observation = result.next_state['observation']\n    assert np.array(filtered_observation).shape == (20, 30, 1, 4)\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_observation_space(stack_filter, env_response):\n    observation_space = ObservationSpace(np.array([5, 10, 20]))\n    filtered_observation_space = stack_filter.get_filtered_observation_space('observation', observation_space)\n\n    # make sure the new observation space shape is calculated correctly\n    assert np.all(filtered_observation_space.shape == np.array([5, 10, 20, 4]))\n\n    # make sure the original observation space is unchanged\n    assert np.all(observation_space.shape == np.array([5, 10, 20]))\n\n    # call after stack is already created with non-matching shape -> error\n    result = stack_filter.filter(env_response)[0]\n    with pytest.raises(ValueError):\n        filtered_observation_space = stack_filter.get_filtered_observation_space('observation', observation_space)\n\n\n@pytest.mark.unit_test\ndef test_reset(stack_filter, env_response):\n    # stack observation on empty stack\n    result = stack_filter.filter(env_response)[0]\n    unfiltered_observation = env_response.next_state['observation']\n    filtered_observation = result.next_state['observation']\n\n    assert np.all(np.array(filtered_observation)[:, :, :, -1] == unfiltered_observation)\n\n    # reset and make sure the outputs are correct\n    stack_filter.reset()\n    unfiltered_observation = np.random.rand(20, 30, 1)\n    new_env_response = EnvResponse(next_state={'observation': unfiltered_observation}, reward=0, game_over=False)\n    result = stack_filter.filter(new_env_response)[0]\n    filtered_observation = result.next_state['observation']\n    assert np.all(np.array(filtered_observation)[:, :, :, 0] == unfiltered_observation)\n    assert np.all(np.array(filtered_observation)[:, :, :, -1] == unfiltered_observation)\n"""
rl_coach/tests/filters/observation/test_observation_to_uint8_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.observation.observation_to_uint8_filter import ObservationToUInt8Filter\nfrom rl_coach.spaces import ObservationSpace\nfrom rl_coach.core_types import EnvResponse\nfrom rl_coach.filters.filter import InputFilter\n\n\n@pytest.mark.unit_test\ndef test_filter():\n    # make an RGB observation smaller\n    uint8_filter = InputFilter()\n    uint8_filter.add_observation_filter('observation', 'to_uint8', ObservationToUInt8Filter(input_low=0, input_high=255))\n\n    observation = np.random.rand(20, 30, 3)*255.0\n    env_response = EnvResponse(next_state={'observation': observation}, reward=0, game_over=False)\n\n    result = uint8_filter.filter(env_response)[0]\n    unfiltered_observation = env_response.next_state['observation']\n    filtered_observation = result.next_state['observation']\n\n    # make sure the original observation is unchanged\n    assert unfiltered_observation.dtype == 'float64'\n\n    # make sure the filtering is done correctly\n    assert filtered_observation.dtype == 'uint8'\n    assert np.all(filtered_observation == observation.astype('uint8'))\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_observation_space():\n    # error on observation space with values not matching the filter configuration\n    uint8_filter = InputFilter()\n    uint8_filter.add_observation_filter('observation', 'to_uint8', ObservationToUInt8Filter(input_low=0, input_high=200))\n\n    observation_space = ObservationSpace(np.array([1, 2, 3]), 0, 100)\n    with pytest.raises(ValueError):\n        uint8_filter.get_filtered_observation_space('observation', observation_space)\n\n    # verify output observation space is correct\n    observation_space = ObservationSpace(np.array([1, 2, 3]), 0, 200)\n    result = uint8_filter.get_filtered_observation_space('observation', observation_space)\n    assert np.all(result.high == 255)\n    assert np.all(result.low == 0)\n    assert np.all(result.shape == observation_space.shape)\n"""
rl_coach/tests/filters/reward/__init__.py,0,b''
rl_coach/tests/filters/reward/test_reward_clipping_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.reward.reward_clipping_filter import RewardClippingFilter\nfrom rl_coach.spaces import RewardSpace\nfrom rl_coach.core_types import EnvResponse\n\nfrom collections import OrderedDict\nfrom rl_coach.filters.filter import InputFilter\n\n\n@pytest.fixture\ndef clip_filter():\n    return InputFilter(reward_filters=OrderedDict([('clip', RewardClippingFilter(2, 10))]))\n\n\n@pytest.mark.unit_test\ndef test_filter(clip_filter):\n    transition = EnvResponse(next_state={'observation': np.zeros(10)}, reward=100, game_over=False)\n    result = clip_filter.filter(transition)[0]\n    unfiltered_reward = transition.reward\n    filtered_reward = result.reward\n\n    # validate that the reward was clipped correctly\n    assert filtered_reward == 10\n\n    # make sure the original reward is unchanged\n    assert unfiltered_reward == 100\n\n    # reward in bounds\n    transition = EnvResponse(next_state={'observation': np.zeros(10)}, reward=5, game_over=False)\n    result = clip_filter.filter(transition)[0]\n    assert result.reward == 5\n\n    # reward below bounds\n    transition = EnvResponse(next_state={'observation': np.zeros(10)}, reward=-5, game_over=False)\n    result = clip_filter.filter(transition)[0]\n    assert result.reward == 2\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_reward_space(clip_filter):\n    # reward is clipped\n    reward_space = RewardSpace(1, -100, 100)\n    filtered_reward_space = clip_filter.get_filtered_reward_space(reward_space)\n\n    # make sure the new reward space shape is calculated correctly\n    assert filtered_reward_space.shape == 1\n    assert filtered_reward_space.low == 2\n    assert filtered_reward_space.high == 10\n\n    # reward is unclipped\n    reward_space = RewardSpace(1, 5, 7)\n    filtered_reward_space = clip_filter.get_filtered_reward_space(reward_space)\n\n    # make sure the new reward space shape is calculated correctly\n    assert filtered_reward_space.shape == 1\n    assert filtered_reward_space.low == 5\n    assert filtered_reward_space.high == 7\n\n    # infinite reward is clipped\n    reward_space = RewardSpace(1, -np.inf, np.inf)\n    filtered_reward_space = clip_filter.get_filtered_reward_space(reward_space)\n\n    # make sure the new reward space shape is calculated correctly\n    assert filtered_reward_space.shape == 1\n    assert filtered_reward_space.low == 2\n    assert filtered_reward_space.high == 10\n\n\n"""
rl_coach/tests/filters/reward/test_reward_rescale_filter.py,0,"b""import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\n\nfrom rl_coach.filters.reward.reward_rescale_filter import RewardRescaleFilter\nfrom rl_coach.spaces import RewardSpace\nfrom rl_coach.core_types import EnvResponse\nfrom rl_coach.filters.filter import InputFilter\nfrom collections import OrderedDict\n\n\n@pytest.mark.unit_test\ndef test_filter():\n    rescale_filter = InputFilter(reward_filters=OrderedDict([('rescale', RewardRescaleFilter(1/10.))]))\n    env_response = EnvResponse(next_state={'observation': np.zeros(10)}, reward=100, game_over=False)\n    print(rescale_filter.observation_filters)\n    result = rescale_filter.filter(env_response)[0]\n    unfiltered_reward = env_response.reward\n    filtered_reward = result.reward\n\n    # validate that the reward was clipped correctly\n    assert filtered_reward == 10\n\n    # make sure the original reward is unchanged\n    assert unfiltered_reward == 100\n\n    # negative reward\n    env_response = EnvResponse(next_state={'observation': np.zeros(10)}, reward=-50, game_over=False)\n    result = rescale_filter.filter(env_response)[0]\n    assert result.reward == -5\n\n\n@pytest.mark.unit_test\ndef test_get_filtered_reward_space():\n    rescale_filter = InputFilter(reward_filters=OrderedDict([('rescale', RewardRescaleFilter(1/10.))]))\n\n    # reward is clipped\n    reward_space = RewardSpace(1, -100, 100)\n    filtered_reward_space = rescale_filter.get_filtered_reward_space(reward_space)\n\n    # make sure the new reward space shape is calculated correctly\n    assert filtered_reward_space.shape == 1\n    assert filtered_reward_space.low == -10\n    assert filtered_reward_space.high == 10\n\n    # unbounded rewards\n    reward_space = RewardSpace(1, -np.inf, np.inf)\n    filtered_reward_space = rescale_filter.get_filtered_reward_space(reward_space)\n\n    # make sure the new reward space shape is calculated correctly\n    assert filtered_reward_space.shape == 1\n    assert filtered_reward_space.low == -np.inf\n    assert filtered_reward_space.high == np.inf\n"""
rl_coach/tests/architectures/mxnet_components/embedders/__init__.py,0,b''
rl_coach/tests/architectures/mxnet_components/embedders/test_image_embedder.py,0,"b'import mxnet as mx\nimport os\nimport pytest\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.base_parameters import EmbedderScheme\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.mxnet_components.embedders.image_embedder import ImageEmbedder\n\n\n@pytest.mark.unit_test\ndef test_image_embedder():\n    params = InputEmbedderParameters(scheme=EmbedderScheme.Medium)\n    emb = ImageEmbedder(params=params)\n    emb.initialize()\n    # input is NHWC, and not MXNet default NCHW\n    input_data = mx.nd.random.uniform(low=0, high=1, shape=(10, 244, 244, 3))\n    output = emb(input_data)\n    assert len(output.shape) == 2  # since last block was flatten\n    assert output.shape[0] == 10  # since batch_size is 10\n'"
rl_coach/tests/architectures/mxnet_components/embedders/test_vector_embedder.py,0,"b'import mxnet as mx\nimport os\nimport pytest\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.mxnet_components.embedders.vector_embedder import VectorEmbedder\nfrom rl_coach.base_parameters import EmbedderScheme\n\n\n@pytest.mark.unit_test\ndef test_vector_embedder():\n    params = InputEmbedderParameters(scheme=EmbedderScheme.Medium)\n    emb = VectorEmbedder(params=params)\n    emb.initialize()\n    input_data = mx.nd.random.uniform(low=0, high=255, shape=(10, 100))\n    output = emb(input_data)\n    assert len(output.shape) == 2  # since last block was flatten\n    assert output.shape[0] == 10  # since batch_size is 10\n    assert output.shape[1] == 256  # since last dense layer has 256 units\n'"
rl_coach/tests/architectures/mxnet_components/heads/__init__.py,0,b''
rl_coach/tests/architectures/mxnet_components/heads/test_head.py,0,"b'import mxnet as mx\nimport numpy as np\nimport os\nimport pytest\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.architectures.mxnet_components.heads.head import NormalizedRSSInitializer\n\n\n@pytest.mark.unit_test\ndef test_normalized_rss_initializer():\n    target_rss = 0.5\n    units = 10\n    dense = mx.gluon.nn.Dense(units=units, weight_initializer=NormalizedRSSInitializer(target_rss))\n    dense.initialize()\n\n    input_data = mx.random.uniform(shape=(25, 5))\n    output_data = dense(input_data)\n\n    weights = dense.weight.data()\n    assert weights.shape == (10, 5)\n    rss = weights.square().sum(axis=1).sqrt()\n    np.testing.assert_almost_equal(rss.asnumpy(), np.tile(target_rss, units))\n'"
rl_coach/tests/architectures/mxnet_components/heads/test_ppo_head.py,0,"b'import mxnet as mx\nimport numpy as np\nimport os\nimport pytest\nfrom scipy import stats as sp_stats\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.architectures.head_parameters import PPOHeadParameters\nfrom rl_coach.architectures.mxnet_components.heads.ppo_head import CategoricalDist, MultivariateNormalDist,\\\n    DiscretePPOHead, ClippedPPOLossDiscrete, ClippedPPOLossContinuous, PPOHead\nfrom rl_coach.agents.clipped_ppo_agent import ClippedPPOAlgorithmParameters, ClippedPPOAgentParameters\nfrom rl_coach.spaces import SpacesDefinition, DiscreteActionSpace\n\n\n@pytest.mark.unit_test\ndef test_multivariate_normal_dist_shape():\n    num_var = 2\n    means = mx.nd.array((0, 1))\n    covar = mx.nd.array(((1, 0),(0, 0.5)))\n    data = mx.nd.array((0.5, 0.8))\n    policy_dist = MultivariateNormalDist(num_var, means, covar)\n    log_probs = policy_dist.log_prob(data)\n    assert log_probs.ndim == 1\n    assert log_probs.shape[0] == 1\n\n\n@pytest.mark.unit_test\ndef test_multivariate_normal_dist_batch_shape():\n    num_var = 2\n    batch_size = 3\n    means = mx.nd.random.uniform(shape=(batch_size, num_var))\n    # create batch of covariance matrices only defined on diagonal\n    std = mx.nd.array((1, 0.5)).broadcast_like(means).expand_dims(-2)\n    covar = mx.nd.eye(N=num_var) * std\n    data = mx.nd.random.uniform(shape=(batch_size, num_var))\n    policy_dist = MultivariateNormalDist(num_var, means, covar)\n    log_probs = policy_dist.log_prob(data)\n    assert log_probs.ndim == 1\n    assert log_probs.shape[0] == batch_size\n\n\n@pytest.mark.unit_test\ndef test_multivariate_normal_dist_batch_time_shape():\n    num_var = 2\n    batch_size = 3\n    time_steps = 4\n    means = mx.nd.random.uniform(shape=(batch_size, time_steps, num_var))\n    # create batch (per time step) of covariance matrices only defined on diagonal\n    std = mx.nd.array((1, 0.5)).broadcast_like(means).expand_dims(-2)\n    covar = mx.nd.eye(N=num_var) * std\n    data = mx.nd.random.uniform(shape=(batch_size, time_steps, num_var))\n    policy_dist = MultivariateNormalDist(num_var, means, covar)\n    log_probs = policy_dist.log_prob(data)\n    assert log_probs.ndim == 2\n    assert log_probs.shape[0] == batch_size\n    assert log_probs.shape[1] == time_steps\n\n\n@pytest.mark.unit_test\ndef test_multivariate_normal_dist_kl_div():\n    n_classes = 2\n    dist_a = MultivariateNormalDist(num_var=n_classes,\n                                    mean = mx.nd.array([0.2, 0.8]).expand_dims(0),\n                                    sigma = mx.nd.array([[1, 0.5], [0.5, 0.5]]).expand_dims(0))\n    dist_b = MultivariateNormalDist(num_var=n_classes,\n                                    mean = mx.nd.array([0.3, 0.7]).expand_dims(0),\n                                    sigma = mx.nd.array([[1, 0.2], [0.2, 0.5]]).expand_dims(0))\n\n    actual = dist_a.kl_div(dist_b).asnumpy()\n    np.testing.assert_almost_equal(actual=actual, desired=0.195100128)\n\n\n@pytest.mark.unit_test\ndef test_multivariate_normal_dist_kl_div_batch():\n    n_classes = 2\n    dist_a = MultivariateNormalDist(num_var=n_classes,\n                                    mean = mx.nd.array([[0.2, 0.8],\n                                                        [0.2, 0.8]]),\n                                    sigma = mx.nd.array([[[1, 0.5], [0.5, 0.5]],\n                                                         [[1, 0.5], [0.5, 0.5]]]))\n    dist_b = MultivariateNormalDist(num_var=n_classes,\n                                    mean = mx.nd.array([[0.3, 0.7],\n                                                        [0.3, 0.7]]),\n                                    sigma = mx.nd.array([[[1, 0.2], [0.2, 0.5]],\n                                                         [[1, 0.2], [0.2, 0.5]]]))\n\n    actual = dist_a.kl_div(dist_b).asnumpy()\n    np.testing.assert_almost_equal(actual=actual, desired=[0.195100128, 0.195100128])\n\n\n@pytest.mark.unit_test\ndef test_categorical_dist_shape():\n    num_actions = 2\n    # actions taken, of shape (batch_size, time_steps)\n    actions = mx.nd.array((1,))\n    # action probabilities, of shape (batch_size, time_steps, num_actions)\n    policy_probs = mx.nd.array((0.8, 0.2))\n    policy_dist = CategoricalDist(num_actions, policy_probs)\n    action_probs = policy_dist.log_prob(actions)\n    assert action_probs.ndim == 1\n    assert action_probs.shape[0] == 1\n\n\n@pytest.mark.unit_test\ndef test_categorical_dist_batch_shape():\n    batch_size = 3\n    num_actions = 2\n    # actions taken, of shape (batch_size, time_steps)\n    actions = mx.nd.array((0, 1, 0))\n    # action probabilities, of shape (batch_size, time_steps, num_actions)\n    policy_probs = mx.nd.array(((0.8, 0.2), (0.5, 0.5), (0.5, 0.5)))\n    policy_dist = CategoricalDist(num_actions, policy_probs)\n    action_probs = policy_dist.log_prob(actions)\n    assert action_probs.ndim == 1\n    assert action_probs.shape[0] == batch_size\n\n\n@pytest.mark.unit_test\ndef test_categorical_dist_batch_time_shape():\n    batch_size = 3\n    time_steps = 4\n    num_actions = 2\n    # actions taken, of shape (batch_size, time_steps)\n    actions = mx.nd.array(((0, 1, 0, 0),\n                           (1, 1, 0, 0),\n                           (0, 0, 0, 0)))\n    # action probabilities, of shape (batch_size, time_steps, num_actions)\n    policy_probs = mx.nd.array((((0.8, 0.2), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)),\n                                ((0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)),\n                                ((0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5))))\n    policy_dist = CategoricalDist(num_actions, policy_probs)\n    action_probs = policy_dist.log_prob(actions)\n    assert action_probs.ndim == 2\n    assert action_probs.shape[0] == batch_size\n    assert action_probs.shape[1] == time_steps\n\n\n@pytest.mark.unit_test\ndef test_categorical_dist_batch():\n    n_classes = 2\n    probs = mx.nd.array(((0.8, 0.2),\n                         (0.7, 0.3),\n                         (0.5, 0.5)))\n\n    dist = CategoricalDist(n_classes, probs)\n    # check log_prob\n    actions = mx.nd.array((0, 1, 0))\n    manual_log_prob = np.array((-0.22314353, -1.20397282, -0.69314718))\n    np.testing.assert_almost_equal(actual=dist.log_prob(actions).asnumpy(), desired=manual_log_prob)\n    # check entropy\n    sp_entropy = np.array([sp_stats.entropy(pk=(0.8, 0.2)),\n                           sp_stats.entropy(pk=(0.7, 0.3)),\n                           sp_stats.entropy(pk=(0.5, 0.5))])\n    np.testing.assert_almost_equal(actual=dist.entropy().asnumpy(), desired=sp_entropy)\n\n\n@pytest.mark.unit_test\ndef test_categorical_dist_kl_div():\n    n_classes = 3\n    dist_a = CategoricalDist(n_classes=n_classes, probs=mx.nd.array([0.4, 0.2, 0.4]))\n    dist_b = CategoricalDist(n_classes=n_classes, probs=mx.nd.array([0.3, 0.4, 0.3]))\n    dist_c = CategoricalDist(n_classes=n_classes, probs=mx.nd.array([0.2, 0.6, 0.2]))\n    dist_d = CategoricalDist(n_classes=n_classes, probs=mx.nd.array([0.0, 1.0, 0.0]))\n    np.testing.assert_almost_equal(actual=dist_a.kl_div(dist_b).asnumpy(), desired=0.09151624)\n    np.testing.assert_almost_equal(actual=dist_a.kl_div(dist_c).asnumpy(), desired=0.33479536)\n    np.testing.assert_almost_equal(actual=dist_c.kl_div(dist_a).asnumpy(), desired=0.38190854)\n    np.testing.assert_almost_equal(actual=dist_a.kl_div(dist_d).asnumpy(), desired=np.nan)\n    np.testing.assert_almost_equal(actual=dist_d.kl_div(dist_a).asnumpy(), desired=1.60943782)\n\n\n@pytest.mark.unit_test\ndef test_categorical_dist_kl_div_batch():\n    n_classes = 3\n    dist_a = CategoricalDist(n_classes=n_classes, probs=mx.nd.array([[0.4, 0.2, 0.4],\n                                                                     [0.4, 0.2, 0.4],\n                                                                     [0.4, 0.2, 0.4]]))\n    dist_b = CategoricalDist(n_classes=n_classes, probs=mx.nd.array([[0.3, 0.4, 0.3],\n                                                                     [0.3, 0.4, 0.3],\n                                                                     [0.3, 0.4, 0.3]]))\n    actual = dist_a.kl_div(dist_b).asnumpy()\n    np.testing.assert_almost_equal(actual=actual, desired=[0.09151624, 0.09151624, 0.09151624])\n\n\n@pytest.mark.unit_test\ndef test_clipped_ppo_loss_continuous_batch():\n    # check lower loss for policy with better probabilities:\n    # i.e. higher probability on high advantage actions, low probability on low advantage actions.\n    loss_fn = ClippedPPOLossContinuous(num_actions=2,\n                                       clip_likelihood_ratio_using_epsilon=0.2)\n    loss_fn.initialize()\n    # actual actions taken, of shape (batch_size)\n    actions = mx.nd.array(((0.5, -0.5), (0.2, 0.3), (0.4, 2.0)))\n    # advantages from taking action, of shape (batch_size)\n    advantages = mx.nd.array((2, -2, 1))\n    # action probabilities, of shape (batch_size, num_actions)\n    old_policy_means = mx.nd.array(((1, 0), (0, 0), (-1, 0)))\n    new_policy_means_worse = mx.nd.array(((2, 0), (0, 0), (-1, 0)))\n    new_policy_means_better = mx.nd.array(((0.5, 0), (0, 0), (-1, 0)))\n\n    policy_stds = mx.nd.array(((1, 1), (1, 1), (1, 1)))\n    clip_param_rescaler = mx.nd.array((1,))\n\n    loss_worse = loss_fn(new_policy_means_worse, policy_stds,\n                         actions, old_policy_means, policy_stds,\n                         clip_param_rescaler, advantages)\n    loss_better = loss_fn(new_policy_means_better, policy_stds,\n                          actions, old_policy_means, policy_stds,\n                          clip_param_rescaler, advantages)\n\n    assert len(loss_worse) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\n    loss_worse_val = loss_worse[0]\n    assert loss_worse_val.ndim == 1\n    assert loss_worse_val.shape[0] == 1\n    assert len(loss_better) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\n    loss_better_val = loss_better[0]\n    assert loss_better_val.ndim == 1\n    assert loss_better_val.shape[0] == 1\n    assert loss_worse_val > loss_better_val\n\n\n@pytest.mark.unit_test\ndef test_clipped_ppo_loss_discrete_batch():\n    # check lower loss for policy with better probabilities:\n    # i.e. higher probability on high advantage actions, low probability on low advantage actions.\n    loss_fn = ClippedPPOLossDiscrete(num_actions=2,\n                                     clip_likelihood_ratio_using_epsilon=None,\n                                     use_kl_regularization=True,\n                                     initial_kl_coefficient=1)\n    loss_fn.initialize()\n\n    # actual actions taken, of shape (batch_size)\n    actions = mx.nd.array((0, 1, 0))\n    # advantages from taking action, of shape (batch_size)\n    advantages = mx.nd.array((-2, 2, 1))\n    # action probabilities, of shape (batch_size, num_actions)\n    old_policy_probs = mx.nd.array(((0.7, 0.3), (0.2, 0.8), (0.4, 0.6)))\n    new_policy_probs_worse = mx.nd.array(((0.9, 0.1), (0.2, 0.8), (0.4, 0.6)))\n    new_policy_probs_better = mx.nd.array(((0.5, 0.5), (0.2, 0.8), (0.4, 0.6)))\n\n    clip_param_rescaler = mx.nd.array((1,))\n\n    loss_worse = loss_fn(new_policy_probs_worse, actions, old_policy_probs, clip_param_rescaler, advantages)\n    loss_better = loss_fn(new_policy_probs_better, actions, old_policy_probs, clip_param_rescaler, advantages)\n\n    assert len(loss_worse) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\n    lw_loss, lw_reg, lw_kl, lw_ent, lw_lr, lw_clip_lr = loss_worse\n    assert lw_loss.ndim == 1\n    assert lw_loss.shape[0] == 1\n    assert len(loss_better) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\n    lb_loss, lb_reg, lb_kl, lb_ent, lb_lr, lb_clip_lr = loss_better\n    assert lb_loss.ndim == 1\n    assert lb_loss.shape[0] == 1\n    assert lw_loss > lb_loss\n    assert lw_kl > lb_kl\n\n\n@pytest.mark.unit_test\ndef test_clipped_ppo_loss_discrete_batch_kl_div():\n    # check lower loss for policy with better probabilities:\n    # i.e. higher probability on high advantage actions, low probability on low advantage actions.\n    loss_fn = ClippedPPOLossDiscrete(num_actions=2,\n                                     clip_likelihood_ratio_using_epsilon=None,\n                                     use_kl_regularization=True,\n                                     initial_kl_coefficient=0.5)\n    loss_fn.initialize()\n\n    # actual actions taken, of shape (batch_size)\n    actions = mx.nd.array((0, 1, 0))\n    # advantages from taking action, of shape (batch_size)\n    advantages = mx.nd.array((-2, 2, 1))\n    # action probabilities, of shape (batch_size, num_actions)\n    old_policy_probs = mx.nd.array(((0.7, 0.3), (0.2, 0.8), (0.4, 0.6)))\n    new_policy_probs_worse = mx.nd.array(((0.9, 0.1), (0.2, 0.8), (0.4, 0.6)))\n    new_policy_probs_better = mx.nd.array(((0.5, 0.5), (0.2, 0.8), (0.4, 0.6)))\n\n    clip_param_rescaler = mx.nd.array((1,))\n\n    loss_worse = loss_fn(new_policy_probs_worse, actions, old_policy_probs, clip_param_rescaler, advantages)\n    loss_better = loss_fn(new_policy_probs_better, actions, old_policy_probs, clip_param_rescaler, advantages)\n\n    assert len(loss_worse) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\n    lw_loss, lw_reg, lw_kl, lw_ent, lw_lr, lw_clip_lr = loss_worse\n    assert lw_kl.ndim == 1\n    assert lw_kl.shape[0] == 1\n    assert len(loss_better) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\n    lb_loss, lb_reg, lb_kl, lb_ent, lb_lr, lb_clip_lr = loss_better\n    assert lb_kl.ndim == 1\n    assert lb_kl.shape[0] == 1\n    assert lw_kl > lb_kl\n    assert lw_reg > lb_reg\n\n\n@pytest.mark.unit_test\ndef test_clipped_ppo_loss_discrete_batch_time():\n    batch_size = 3\n    time_steps = 4\n    num_actions = 2\n\n    # actions taken, of shape (batch_size, time_steps)\n    actions = mx.nd.array(((0, 1, 0, 0),\n                           (1, 1, 0, 0),\n                           (0, 0, 0, 0)))\n    # advantages from taking action, of shape (batch_size, time_steps)\n    advantages = mx.nd.array(((-2, 2, 1, 0),\n                              (-1, 1, 0, 1),\n                              (-1, 0, 1, 0)))\n    # action probabilities, of shape (batch_size, num_actions)\n    old_policy_probs = mx.nd.array((((0.8, 0.2), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)),\n                                     ((0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)),\n                                     ((0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5))))\n    new_policy_probs_worse = mx.nd.array((((0.9, 0.1), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)),\n                                          ((0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)),\n                                          ((0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5))))\n    new_policy_probs_better = mx.nd.array((((0.2, 0.8), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)),\n                                           ((0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)),\n                                           ((0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5))))\n\n    # check lower loss for policy with better probabilities:\n    # i.e. higher probability on high advantage actions, low probability on low advantage actions.\n    loss_fn = ClippedPPOLossDiscrete(num_actions=num_actions,\n                                     clip_likelihood_ratio_using_epsilon=0.2)\n    loss_fn.initialize()\n\n    clip_param_rescaler = mx.nd.array((1,))\n\n    loss_worse = loss_fn(new_policy_probs_worse, actions, old_policy_probs, clip_param_rescaler, advantages)\n    loss_better = loss_fn(new_policy_probs_better, actions, old_policy_probs, clip_param_rescaler, advantages)\n\n    assert len(loss_worse) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\n    loss_worse_val = loss_worse[0]\n    assert loss_worse_val.ndim == 1\n    assert loss_worse_val.shape[0] == 1\n    assert len(loss_better) == 6  # (LOSS, REGULARIZATION, KL, ENTROPY, LIKELIHOOD_RATIO, CLIPPED_LIKELIHOOD_RATIO)\n    loss_better_val = loss_better[0]\n    assert loss_better_val.ndim == 1\n    assert loss_better_val.shape[0] == 1\n    assert loss_worse_val > loss_better_val\n\n\n@pytest.mark.unit_test\ndef test_clipped_ppo_loss_discrete_weight():\n    actions = mx.nd.array((0, 1, 0))\n    advantages = mx.nd.array((-2, 2, 1))\n    old_policy_probs = mx.nd.array(((0.7, 0.3), (0.2, 0.8), (0.4, 0.6)))\n    new_policy_probs = mx.nd.array(((0.9, 0.1), (0.2, 0.8), (0.4, 0.6)))\n\n    clip_param_rescaler = mx.nd.array((1,))\n    loss_fn = ClippedPPOLossDiscrete(num_actions=2,\n                                     clip_likelihood_ratio_using_epsilon=0.2)\n    loss_fn.initialize()\n    loss = loss_fn(new_policy_probs, actions, old_policy_probs, clip_param_rescaler, advantages)\n    loss_fn_weighted = ClippedPPOLossDiscrete(num_actions=2,\n                                     clip_likelihood_ratio_using_epsilon=0.2,\n                                     weight=0.5)\n    loss_fn_weighted.initialize()\n    loss_weighted = loss_fn_weighted(new_policy_probs, actions, old_policy_probs, clip_param_rescaler, advantages)\n    assert loss[0] == loss_weighted[0] * 2\n\n\n@pytest.mark.unit_test\ndef test_clipped_ppo_loss_discrete_hybridize():\n    loss_fn = ClippedPPOLossDiscrete(num_actions=2,\n                                     clip_likelihood_ratio_using_epsilon=0.2)\n    loss_fn.initialize()\n    loss_fn.hybridize()\n    actions = mx.nd.array((0, 1, 0))\n    advantages = mx.nd.array((-2, 2, 1))\n    old_policy_probs = mx.nd.array(((0.7, 0.3), (0.2, 0.8), (0.4, 0.6)))\n    new_policy_probs = mx.nd.array(((0.9, 0.1), (0.2, 0.8), (0.4, 0.6)))\n    clip_param_rescaler = mx.nd.array((1,))\n\n    loss = loss_fn(new_policy_probs, actions, old_policy_probs, clip_param_rescaler, advantages)\n    assert loss[0] == mx.nd.array((-0.142857153,))\n\n\n@pytest.mark.unit_test\ndef test_discrete_ppo_head():\n    head = DiscretePPOHead(num_actions=2)\n    head.initialize()\n    middleware_data = mx.nd.random.uniform(shape=(10, 100))\n    probs = head(middleware_data)\n    assert probs.ndim == 2  # (batch_size, num_actions)\n    assert probs.shape[0] == 10  # since batch_size is 10\n    assert probs.shape[1] == 2  # since num_actions is 2\n\n\n@pytest.mark.unit_test\ndef test_ppo_head():\n    agent_parameters = ClippedPPOAgentParameters()\n    num_actions = 5\n    action_space = DiscreteActionSpace(num_actions=num_actions)\n    spaces = SpacesDefinition(state=None, goal=None, action=action_space, reward=None)\n    head = PPOHead(agent_parameters=agent_parameters,\n                   spaces=spaces,\n                   network_name=""test_ppo_head"")\n\n    head.initialize()\n\n    batch_size = 15\n    middleware_data = mx.nd.random.uniform(shape=(batch_size, 100))\n    probs = head(middleware_data)\n    assert probs.ndim == 2  # (batch_size, num_actions)\n    assert probs.shape[0] == batch_size\n    assert probs.shape[1] == num_actions\n'"
rl_coach/tests/architectures/mxnet_components/heads/test_ppo_v_head.py,0,"b'import mxnet as mx\nimport os\nimport pytest\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.architectures.mxnet_components.heads.ppo_v_head import PPOVHead, PPOVHeadLoss\nfrom rl_coach.agents.clipped_ppo_agent import ClippedPPOAlgorithmParameters, ClippedPPOAgentParameters\nfrom rl_coach.spaces import SpacesDefinition, DiscreteActionSpace\n\n\n@pytest.mark.unit_test\ndef test_ppo_v_head_loss_batch():\n    loss_fn = PPOVHeadLoss(clip_likelihood_ratio_using_epsilon=0.1)\n    total_return = mx.nd.array((5, -3, 0))\n    old_policy_values = mx.nd.array((3, -1, -1))\n    new_policy_values_worse = mx.nd.array((2, 0, -1))\n    new_policy_values_better = mx.nd.array((4, -2, -1))\n\n    loss_worse = loss_fn(new_policy_values_worse, old_policy_values, total_return)\n    loss_better = loss_fn(new_policy_values_better, old_policy_values, total_return)\n\n    assert len(loss_worse) == 1  # (LOSS)\n    loss_worse_val = loss_worse[0]\n    assert loss_worse_val.ndim == 1\n    assert loss_worse_val.shape[0] == 1\n    assert len(loss_better) == 1  # (LOSS)\n    loss_better_val = loss_better[0]\n    assert loss_better_val.ndim == 1\n    assert loss_better_val.shape[0] == 1\n    assert loss_worse_val > loss_better_val\n\n\n@pytest.mark.unit_test\ndef test_ppo_v_head_loss_batch_time():\n    loss_fn = PPOVHeadLoss(clip_likelihood_ratio_using_epsilon=0.1)\n    total_return = mx.nd.array(((3, 1, 1, 0),\n                                (1, 0, 0, 1),\n                                (3, 0, 1, 0)))\n    old_policy_values = mx.nd.array(((2, 1, 1, 0),\n                                     (1, 0, 0, 1),\n                                     (0, 0, 1, 0)))\n    new_policy_values_worse = mx.nd.array(((2, 1, 1, 0),\n                                           (1, 0, 0, 1),\n                                           (2, 0, 1, 0)))\n    new_policy_values_better = mx.nd.array(((3, 1, 1, 0),\n                                            (1, 0, 0, 1),\n                                            (2, 0, 1, 0)))\n\n    loss_worse = loss_fn(new_policy_values_worse, old_policy_values, total_return)\n    loss_better = loss_fn(new_policy_values_better, old_policy_values, total_return)\n\n    assert len(loss_worse) == 1  # (LOSS)\n    loss_worse_val = loss_worse[0]\n    assert loss_worse_val.ndim == 1\n    assert loss_worse_val.shape[0] == 1\n    assert len(loss_better) == 1  # (LOSS)\n    loss_better_val = loss_better[0]\n    assert loss_better_val.ndim == 1\n    assert loss_better_val.shape[0] == 1\n    assert loss_worse_val > loss_better_val\n\n\n@pytest.mark.unit_test\ndef test_ppo_v_head_loss_weight():\n    total_return = mx.nd.array((5, -3, 0))\n    old_policy_values = mx.nd.array((3, -1, -1))\n    new_policy_values = mx.nd.array((4, -2, -1))\n    loss_fn = PPOVHeadLoss(clip_likelihood_ratio_using_epsilon=0.2, weight=1)\n    loss = loss_fn(new_policy_values, old_policy_values, total_return)\n    loss_fn_weighted = PPOVHeadLoss(clip_likelihood_ratio_using_epsilon=0.2, weight=0.5)\n    loss_weighted = loss_fn_weighted(new_policy_values, old_policy_values, total_return)\n    assert loss[0].sum() == loss_weighted[0].sum() * 2\n\n\n@pytest.mark.unit_test\ndef test_ppo_v_head():\n    agent_parameters = ClippedPPOAgentParameters()\n    action_space = DiscreteActionSpace(num_actions=5)\n    spaces = SpacesDefinition(state=None, goal=None, action=action_space, reward=None)\n    value_net = PPOVHead(agent_parameters=agent_parameters,\n                         spaces=spaces,\n                         network_name=""test_ppo_v_head"")\n    value_net.initialize()\n    batch_size = 15\n    middleware_data = mx.nd.random.uniform(shape=(batch_size, 100))\n    values = value_net(middleware_data)\n    assert values.ndim == 1  # (batch_size)\n    assert values.shape[0] == batch_size\n'"
rl_coach/tests/architectures/mxnet_components/heads/test_q_head.py,0,"b'import mxnet as mx\nimport os\nimport pytest\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.architectures.mxnet_components.heads.q_head import QHead, QHeadLoss\nfrom rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters\nfrom rl_coach.spaces import SpacesDefinition, DiscreteActionSpace\n\n\n\n@pytest.mark.unit_test\ndef test_q_head_loss():\n    loss_fn = QHeadLoss()\n    # example with batch_size of 3, and num_actions of 2\n    target_q_values = mx.nd.array(((3, 5), (-1, -2), (0, 2)))\n    pred_q_values_worse = mx.nd.array(((6, 5), (-1, -2), (0, 2)))\n    pred_q_values_better = mx.nd.array(((4, 5), (-2, -2), (1, 2)))\n    loss_worse = loss_fn(pred_q_values_worse, target_q_values)\n    loss_better = loss_fn(pred_q_values_better, target_q_values)\n    assert len(loss_worse) == 1  # (LOSS)\n    loss_worse_val = loss_worse[0]\n    assert loss_worse_val.ndim == 1\n    assert loss_worse_val.shape[0] == 1\n    assert len(loss_better) == 1  # (LOSS)\n    loss_better_val = loss_better[0]\n    assert loss_better_val.ndim == 1\n    assert loss_better_val.shape[0] == 1\n    assert loss_worse_val > loss_better_val\n\n\n@pytest.mark.unit_test\ndef test_v_head_loss_weight():\n    target_q_values = mx.nd.array(((3, 5), (-1, -2), (0, 2)))\n    pred_q_values = mx.nd.array(((4, 5), (-2, -2), (1, 2)))\n    loss_fn = QHeadLoss()\n    loss = loss_fn(pred_q_values, target_q_values)\n    loss_fn_weighted = QHeadLoss(weight=0.5)\n    loss_weighted = loss_fn_weighted(pred_q_values, target_q_values)\n    assert loss[0] == loss_weighted[0]*2\n\n\n@pytest.mark.unit_test\ndef test_ppo_v_head():\n    agent_parameters = ClippedPPOAgentParameters()\n    num_actions = 5\n    action_space = DiscreteActionSpace(num_actions=num_actions)\n    spaces = SpacesDefinition(state=None, goal=None, action=action_space, reward=None)\n    value_net = QHead(agent_parameters=agent_parameters,\n                      spaces=spaces,\n                      network_name=""test_q_head"")\n    value_net.initialize()\n    batch_size = 15\n    middleware_data = mx.nd.random.uniform(shape=(batch_size, 100))\n    values = value_net(middleware_data)\n    assert values.ndim == 2  # (batch_size, num_actions)\n    assert values.shape[0] == batch_size\n    assert values.shape[1] == num_actions'"
rl_coach/tests/architectures/mxnet_components/heads/test_v_head.py,0,"b'import mxnet as mx\nimport os\nimport pytest\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.architectures.mxnet_components.heads.v_head import VHead, VHeadLoss\nfrom rl_coach.agents.clipped_ppo_agent import ClippedPPOAlgorithmParameters, ClippedPPOAgentParameters\nfrom rl_coach.spaces import SpacesDefinition, DiscreteActionSpace\n\n\n\n@pytest.mark.unit_test\ndef test_v_head_loss():\n    loss_fn = VHeadLoss()\n    target_values = mx.nd.array((3, -1, 0))\n    pred_values_worse = mx.nd.array((0, 0, 1))\n    pred_values_better = mx.nd.array((2, -1, 0))\n    loss_worse = loss_fn(pred_values_worse, target_values)\n    loss_better = loss_fn(pred_values_better, target_values)\n    assert len(loss_worse) == 1  # (LOSS)\n    loss_worse_val = loss_worse[0]\n    assert loss_worse_val.ndim == 1\n    assert loss_worse_val.shape[0] == 1\n    assert len(loss_better) == 1  # (LOSS)\n    loss_better_val = loss_better[0]\n    assert loss_better_val.ndim == 1\n    assert loss_better_val.shape[0] == 1\n    assert loss_worse_val > loss_better_val\n\n\n@pytest.mark.unit_test\ndef test_v_head_loss_weight():\n    target_values = mx.nd.array((3, -1, 0))\n    pred_values = mx.nd.array((0, 0, 1))\n    loss_fn = VHeadLoss()\n    loss = loss_fn(pred_values, target_values)\n    loss_fn_weighted = VHeadLoss(weight=0.5)\n    loss_weighted = loss_fn_weighted(pred_values, target_values)\n    assert loss[0] == loss_weighted[0]*2\n\n\n@pytest.mark.unit_test\ndef test_ppo_v_head():\n    agent_parameters = ClippedPPOAgentParameters()\n    action_space = DiscreteActionSpace(num_actions=5)\n    spaces = SpacesDefinition(state=None, goal=None, action=action_space, reward=None)\n    value_net = VHead(agent_parameters=agent_parameters,\n                      spaces=spaces,\n                      network_name=""test_v_head"")\n    value_net.initialize()\n    batch_size = 15\n    middleware_data = mx.nd.random.uniform(shape=(batch_size, 100))\n    values = value_net(middleware_data)\n    assert values.ndim == 1  # (batch_size)\n    assert values.shape[0] == batch_size'"
rl_coach/tests/architectures/mxnet_components/middlewares/__init__.py,0,b''
rl_coach/tests/architectures/mxnet_components/middlewares/test_fc_middleware.py,0,"b'import mxnet as mx\nimport os\nimport pytest\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.base_parameters import MiddlewareScheme\nfrom rl_coach.architectures.middleware_parameters import FCMiddlewareParameters\nfrom rl_coach.architectures.mxnet_components.middlewares.fc_middleware import FCMiddleware\n\n\n@pytest.mark.unit_test\ndef test_fc_middleware():\n    params = FCMiddlewareParameters(scheme=MiddlewareScheme.Medium)\n    mid = FCMiddleware(params=params)\n    mid.initialize()\n    embedded_data = mx.nd.random.uniform(low=0, high=1, shape=(10, 100))\n    output = mid(embedded_data)\n    assert output.ndim == 2  # since last block was flatten\n    assert output.shape[0] == 10  # since batch_size is 10\n    assert output.shape[1] == 512  # since last layer of middleware (middle scheme) had 512 units\n'"
rl_coach/tests/architectures/mxnet_components/middlewares/test_lstm_middleware.py,0,"b'import mxnet as mx\nimport os\nimport pytest\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n\nfrom rl_coach.base_parameters import MiddlewareScheme\nfrom rl_coach.architectures.middleware_parameters import LSTMMiddlewareParameters\nfrom rl_coach.architectures.mxnet_components.middlewares.lstm_middleware import LSTMMiddleware\n\n\n@pytest.mark.unit_test\ndef test_lstm_middleware():\n    params = LSTMMiddlewareParameters(number_of_lstm_cells=25, scheme=MiddlewareScheme.Medium)\n    mid = LSTMMiddleware(params=params)\n    mid.initialize()\n    # NTC\n    embedded_data = mx.nd.random.uniform(low=0, high=1, shape=(10, 15, 20))\n    # NTC -> TNC\n    output = mid(embedded_data)\n    assert output.ndim == 3  # since last block was flatten\n    assert output.shape[0] == 15  # since t is 15\n    assert output.shape[1] == 10  # since batch_size is 10\n    assert output.shape[2] == 25  # since number_of_lstm_cells is 25\n'"
rl_coach/tests/architectures/tensorflow_components/embedders/__init__.py,0,b''
rl_coach/tests/architectures/tensorflow_components/embedders/test_identity_embedder.py,4,"b'import os\nimport sys\n\nfrom rl_coach.base_parameters import EmbedderScheme\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\nfrom rl_coach.architectures.tensorflow_components.embedders.vector_embedder import VectorEmbedder\nimport tensorflow as tf\nfrom tensorflow import logging\n\nlogging.set_verbosity(logging.INFO)\n\n@pytest.fixture\ndef reset():\n    tf.reset_default_graph()\n\n\n@pytest.mark.unit_test\ndef test_embedder(reset):\n    embedder = VectorEmbedder(np.array([10, 10]), name=""test"", scheme=EmbedderScheme.Empty)\n\n    # make sure the ops where not created yet\n    assert len(tf.get_default_graph().get_operations()) == 0\n\n    # call the embedder\n    input_ph, output_ph = embedder()\n\n    # make sure that now the ops were created\n    assert len(tf.get_default_graph().get_operations()) > 0\n\n    # try feeding a batch of one example  # TODO: consider auto converting to batch\n    input = np.random.rand(1, 10, 10)\n    sess = tf.Session()\n    output = sess.run(embedder.output, {embedder.input: input})\n    assert output.shape == (1, 100)  # should have flattened the input\n\n    # now make sure the returned placeholders behave the same\n    output = sess.run(output_ph, {input_ph: input})\n    assert output.shape == (1, 100)  # should have flattened the input\n\n    # make sure the naming is correct\n    assert embedder.get_name() == ""test""\n'"
rl_coach/tests/architectures/tensorflow_components/embedders/test_image_embedder.py,17,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\nfrom rl_coach.architectures.tensorflow_components.embedders.image_embedder import ImageEmbedder, EmbedderScheme\nimport tensorflow as tf\nfrom tensorflow import logging\n\nlogging.set_verbosity(logging.INFO)\n\n@pytest.fixture\ndef reset():\n    tf.reset_default_graph()\n\n\n@pytest.mark.unit_test\ndef test_embedder(reset):\n    # creating an embedder with a non-image input\n    with pytest.raises(ValueError):\n        embedder = ImageEmbedder(np.array([100]), name=""test"")\n    with pytest.raises(ValueError):\n        embedder = ImageEmbedder(np.array([100, 100]), name=""test"")\n    with pytest.raises(ValueError):\n        embedder = ImageEmbedder(np.array([10, 100, 100, 100]), name=""test"")\n\n\n    is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n    pre_ops = len(tf.get_default_graph().get_operations())\n    # creating a simple image embedder\n    embedder = ImageEmbedder(np.array([100, 100, 10]), name=""test"", is_training=is_training)\n\n    # make sure the only the is_training op is creates\n    assert len(tf.get_default_graph().get_operations()) == pre_ops\n\n    # call the embedder\n    input_ph, output_ph = embedder()\n\n    # make sure that now the ops were created\n    assert len(tf.get_default_graph().get_operations()) > pre_ops\n\n    # try feeding a batch of one example\n    input = np.random.rand(1, 100, 100, 10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    output = sess.run(embedder.output, {embedder.input: input})\n    assert output.shape == (1, 5184)\n\n    # now make sure the returned placeholders behave the same\n    output = sess.run(output_ph, {input_ph: input})\n    assert output.shape == (1, 5184)\n\n    # make sure the naming is correct\n    assert embedder.get_name() == ""test""\n\n\n@pytest.mark.unit_test\ndef test_complex_embedder(reset):\n    # creating a deep vector embedder\n    is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n    embedder = ImageEmbedder(np.array([100, 100, 10]), name=""test"", scheme=EmbedderScheme.Deep, \n        is_training=is_training)\n\n    # call the embedder\n    embedder()\n\n    # try feeding a batch of one example\n    input = np.random.rand(1, 100, 100, 10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    output = sess.run(embedder.output, {embedder.input: input})\n    assert output.shape == (1, 256)  # should have flattened the input\n\n\n@pytest.mark.unit_test\ndef test_activation_function(reset):\n    # creating a deep image embedder with relu\n    is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n    embedder = ImageEmbedder(np.array([100, 100, 10]), name=""relu"", scheme=EmbedderScheme.Deep,\n                             activation_function=tf.nn.relu, is_training=is_training)\n\n    # call the embedder\n    embedder()\n\n    # try feeding a batch of one example\n    input = np.random.rand(1, 100, 100, 10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    output = sess.run(embedder.output, {embedder.input: input})\n    assert np.all(output >= 0)  # should have flattened the input\n\n    # creating a deep image embedder with tanh\n    embedder_tanh = ImageEmbedder(np.array([100, 100, 10]), name=""tanh"", scheme=EmbedderScheme.Deep,\n                                  activation_function=tf.nn.tanh, is_training=is_training)\n\n    # call the embedder\n    embedder_tanh()\n\n    # try feeding a batch of one example\n    input = np.random.rand(1, 100, 100, 10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    output = sess.run(embedder_tanh.output, {embedder_tanh.input: input})\n    assert np.all(output >= -1) and np.all(output <= 1)\n'"
rl_coach/tests/architectures/tensorflow_components/embedders/test_vector_embedder.py,17,"b'import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nimport pytest\nimport numpy as np\nfrom rl_coach.architectures.tensorflow_components.embedders.vector_embedder import VectorEmbedder, EmbedderScheme\nimport tensorflow as tf\nfrom tensorflow import logging\n\nlogging.set_verbosity(logging.INFO)\n\n@pytest.fixture\ndef reset():\n    tf.reset_default_graph()\n\n\n@pytest.mark.unit_test\ndef test_embedder(reset):\n    # creating a vector embedder with a matrix\n    with pytest.raises(ValueError):\n        embedder = VectorEmbedder(np.array([10, 10]), name=""test"")\n\n    # creating a simple vector embedder\n    is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n    pre_ops = len(tf.get_default_graph().get_operations())\n\n    embedder = VectorEmbedder(np.array([10]), name=""test"", is_training=is_training)\n\n    # make sure the ops where not created yet\n    assert len(tf.get_default_graph().get_operations()) == pre_ops\n\n    # call the embedder\n    input_ph, output_ph = embedder()\n\n    # make sure that now the ops were created\n    assert len(tf.get_default_graph().get_operations()) > pre_ops\n\n    # try feeding a batch of one example\n    input = np.random.rand(1, 10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    output = sess.run(embedder.output, {embedder.input: input})\n    assert output.shape == (1, 256)\n\n    # now make sure the returned placeholders behave the same\n    output = sess.run(output_ph, {input_ph: input})\n    assert output.shape == (1, 256)\n\n    # make sure the naming is correct\n    assert embedder.get_name() == ""test""\n\n\n@pytest.mark.unit_test\ndef test_complex_embedder(reset):\n    # creating a deep vector embedder\n    is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n    embedder = VectorEmbedder(np.array([10]), name=""test"", scheme=EmbedderScheme.Deep, is_training=is_training)\n\n    # call the embedder\n    embedder()\n\n    # try feeding a batch of one example\n    input = np.random.rand(1, 10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    output = sess.run(embedder.output, {embedder.input: input})\n    assert output.shape == (1, 128)  # should have flattened the input\n\n\n@pytest.mark.unit_test\ndef test_activation_function(reset):\n    # creating a deep vector embedder with relu\n    is_training = tf.Variable(False, trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])\n    embedder = VectorEmbedder(np.array([10]), name=""relu"", scheme=EmbedderScheme.Deep,\n                              activation_function=tf.nn.relu, is_training=is_training)\n\n    # call the embedder\n    embedder()\n\n    # try feeding a batch of one example\n    input = np.random.rand(1, 10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    output = sess.run(embedder.output, {embedder.input: input})\n    assert np.all(output >= 0)  # should have flattened the input\n\n    # creating a deep vector embedder with tanh\n    embedder_tanh = VectorEmbedder(np.array([10]), name=""tanh"", scheme=EmbedderScheme.Deep,\n                                   activation_function=tf.nn.tanh, is_training=is_training)\n\n    # call the embedder\n    embedder_tanh()\n\n    # try feeding a batch of one example\n    input = np.random.rand(1, 10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    output = sess.run(embedder_tanh.output, {embedder_tanh.input: input})\n    assert np.all(output >= -1) and np.all(output <= 1)\n'"
