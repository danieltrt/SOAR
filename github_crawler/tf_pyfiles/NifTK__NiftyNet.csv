file_path,api_count,code
net_autoencoder.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\n\nfrom niftynet import main\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
net_classify.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\n\nfrom niftynet import main\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
net_download.py,0,"b'#!/usr/bin/env python\n#  -*- coding: utf-8 -*-\nimport sys\n\nfrom niftynet.utilities.download import main\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
net_gan.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\n\nfrom niftynet import main\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
net_regress.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\n\nfrom niftynet import main\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
net_run.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\n\nfrom niftynet import main\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
net_segment.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\n\nfrom niftynet import main\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
setup.py,0,"b""# -*- coding: utf-8 -*-\nfrom setuptools import setup, find_packages\nfrom io import open\n\nimport versioneer\nfrom niftynet.utilities.versioning import get_niftynet_version\n\nniftynet_version = get_niftynet_version()\n\n# Get the summary\ndescription = 'An open-source convolutional neural networks platform' + \\\n              ' for research in medical image analysis and' + \\\n              ' image-guided therapy'\n\n# Get the long description\nwith open('README.md', encoding='utf-8') as f:\n    long_description = f.read()\n\nsetup(\n    name='NiftyNet',\n\n    version=niftynet_version,\n    cmdclass=versioneer.get_cmdclass(),\n\n    description=description,\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n\n    url='http://niftynet.io/',\n\n    author='NiftyNet Consortium',\n    author_email='niftynet-team@googlegroups.com',\n\n    license='Apache 2.0',\n\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n\n        'Intended Audience :: Developers',\n        'Intended Audience :: Healthcare Industry',\n        'Intended Audience :: Science/Research',\n\n        'License :: OSI Approved :: Apache Software License',\n\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 3',\n\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Scientific/Engineering :: Image Recognition',\n        'Topic :: Scientific/Engineering :: Medical Science Apps.',\n    ],\n\n    packages=find_packages(\n        exclude=[\n            'pip',\n            'config',\n            'data',\n            'demos',\n            'tests',\n        ]\n    ),\n\n    install_requires=[\n        'six>=1.10',\n        'nibabel>=2.1.0',\n        'numpy>=1.13.3',\n        'scipy>=0.18',\n        'configparser',\n        'pandas',\n        'pillow',\n        'blinker',\n        'packaging'\n    ],\n\n    entry_points={\n        'console_scripts': [\n            'net_segment=niftynet:main',\n            'net_download=niftynet.utilities.download:main',\n            'net_run=niftynet:main',\n            'net_regress=niftynet:main',\n            'net_gan=niftynet:main',\n            'net_autoencoder=niftynet:main',\n            'net_classify=niftynet:main',\n        ],\n    },\n)\n"""
versioneer.py,0,"b'\n# Version: 0.18\n\n""""""The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'date\']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  ""master"" and ""slave"" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other langauges) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n""Entry-point scripts"" (`setup(entry_points={""console_scripts"": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It\'s not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons ""Public Domain\nDedication"" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n""""""\n\nfrom __future__ import print_function\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_root():\n    """"""Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    """"""\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (""Versioneer was unable to run the project root directory. ""\n               ""Versioneer requires setup.py to be executed from ""\n               ""its immediate directory (like \'python setup.py COMMAND\'), ""\n               ""or in a way that lets it use sys.argv[0] to find the root ""\n               ""(like \'python path/to/setup.py COMMAND\')."")\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(""Warning: build in %s is using versioneer.py from %s""\n                  % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    """"""Read the project setup.cfg file to determine Versioneer config.""""""\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    if cfg.tag_prefix in (""\'\'"", \'""""\'):\n        cfg.tag_prefix = """"\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\'git\'] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    git_date = ""%(DOLLAR)sFormat:%%ci%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n            print(""stdout was %%s"" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %%s but none started with prefix %%s"" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %%s not under git control"" %% root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%%s*"" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%s*"" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    """"""Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    """"""Try to determine the version from _version.py if present.""""""\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"",\n                   contents, re.M | re.S)\n    if not mo:\n        mo = re.search(r""version_json = \'\'\'\\r\\n(.*)\'\'\'  # END VERSION_JSON"",\n                       contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    """"""Write the given version number to the given _version.py file.""""""\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True,\n                          indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\nclass VersioneerBadRootError(Exception):\n    """"""The project root directory is unknown or missing key files.""""""\n\n\ndef get_versions(verbose=False):\n    """"""Get the project version from whatever source is available.\n\n    Returns dict with two keys: \'version\' and \'full\'.\n    """"""\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \\\n        ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None, ""error"": ""unable to compute version"",\n            ""date"": None}\n\n\ndef get_version():\n    """"""Get the short version string for this project.""""""\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    """"""Get the custom setuptools/distutils subclasses used by Versioneer.""""""\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            print("" date: %s"" % vers.get(""date""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn\'t copied too, \'git describe\' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different ""build_py"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib,\n                                                  cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n        # nczeczulin reports that py2exe won\'t like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   ""version"": versioneer.get_version().split(""+"", 1)[0], # FILEVERSION\n        #   ""product_version"": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    if \'py2exe\' in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""py2exe""] = cmd_py2exe\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(target_versionfile,\n                                  self._versioneer_generated_versions)\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    """"""Main VCS-independent setup function for installing Versioneer.""""""\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError,\n            configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"",\n                  file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(LONG % {""DOLLAR"": ""$"",\n                        ""STYLE"": cfg.style,\n                        ""TAG_PREFIX"": cfg.tag_prefix,\n                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        })\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                       ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print("" appending versionfile_source (\'%s\') to MANIFEST.in"" %\n              cfg.versionfile_source)\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    """"""Validate the contents of setup.py against Versioneer\'s expectations.""""""\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
niftynet/__init__.py,9,"b'# -*- coding: utf-8 -*-\n""""""\n\n.. module:: niftynet\n   :synopsis: Entry points for the NiftyNet CLI.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n# Before doing anything else, check TF is installed\n# and fail gracefully if not.\ntry:\n    import tensorflow as tf\nexcept ImportError:\n    raise ImportError(\'NiftyNet is based on TensorFlow, which\'\n                      \' does not seem to be installed on your\'\n                      \' system.\\n\\nPlease install TensorFlow\'\n                      \' (https://www.tensorflow.org/) to be\'\n                      \' able to use NiftyNet.\')\n\ntry:\n    from distutils.version import LooseVersion\n\n    minimal_required_version = LooseVersion(""1.5"")\n    tf_version = LooseVersion(tf.__version__)\n    if tf_version < minimal_required_version:\n        tf.logging.fatal(\'TensorFlow %s or later is required.\'\n                         \'\\n\\nPlease upgrade TensorFlow\'\n                         \' (https://www.tensorflow.org/) to be\'\n                         \' able to use NiftyNet.\\nCurrently using \'\n                         \'TensorFlow %s:\\ninstalled at %s\\n\\n\',\n                         minimal_required_version, tf_version, tf.__file__)\n        raise ImportError\n    else:\n        tf.logging.info(\'TensorFlow version %s\', tf_version)\nexcept AttributeError:\n    pass\n\nfrom niftynet.utilities.versioning import get_niftynet_version_string\n\n__version__ = get_niftynet_version_string()\n\nimport os\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nfrom tensorflow.python.util import deprecation\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\ntry:\n    from tensorflow.python.util import module_wrapper as deprecation\nexcept ImportError:\n    from tensorflow.python.util import deprecation_wrapper as deprecation\ndeprecation._PER_MODULE_WARNING_LIMIT = 0\n\nfrom niftynet.io.misc_io import set_logger, close_logger\n\nset_logger()\n\nfrom niftynet.utilities.util_import import require_module\n\nrequire_module(\'blinker\', descriptor=\'New dependency\', mandatory=True)\n\nfrom tensorflow.python.util import deprecation\n\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\n\nfrom niftynet.engine.signal import TRAIN, INFER, EVAL\nimport niftynet.utilities.util_common as util\nimport niftynet.utilities.user_parameters_parser as user_parameters_parser\nfrom niftynet.engine.application_driver import ApplicationDriver\nfrom niftynet.evaluation.evaluation_application_driver import \\\n    EvaluationApplicationDriver\nfrom niftynet.io.misc_io import touch_folder\nfrom niftynet.io.misc_io import resolve_module_dir\nfrom niftynet.io.misc_io import to_absolute_path\n\n\ndef main():\n    system_param, input_data_param = user_parameters_parser.run()\n    if util.has_bad_inputs(system_param):\n        return -1\n\n    # print all parameters to txt file for future reference\n    all_param = {}\n    all_param.update(system_param)\n    all_param.update(input_data_param)\n\n    # Set up path for niftynet model_root\n    # (rewriting user input with an absolute path)\n    system_param[\'SYSTEM\'].model_dir = resolve_module_dir(\n        system_param[\'SYSTEM\'].model_dir,\n        create_new=system_param[\'SYSTEM\'].action == TRAIN)\n\n    # writing all params for future reference\n    txt_file = \'settings_{}.txt\'.format(system_param[\'SYSTEM\'].action)\n    txt_file = os.path.join(system_param[\'SYSTEM\'].model_dir, txt_file)\n    try:\n        util.print_save_input_parameters(all_param, txt_file)\n    except IOError:\n        tf.logging.fatal(\n            \'Unable to write %s,\\nplease check \'\n            \'model_dir parameter, current value: %s\',\n            txt_file, system_param[\'SYSTEM\'].model_dir)\n        raise\n\n    # keep all commandline outputs to model_root\n    log_file_name = os.path.join(\n        system_param[\'SYSTEM\'].model_dir,\n        \'{}_{}\'.format(all_param[\'SYSTEM\'].action, \'niftynet_log\'))\n    set_logger(file_name=log_file_name)\n\n    # set up all model folder related parameters here\n    # see https://cmiclab.cs.ucl.ac.uk/CMIC/NiftyNet/issues/168\n    # 1. resolve mapping file:\n    try:\n        if system_param[\'NETWORK\'].histogram_ref_file:\n            system_param[\'NETWORK\'].histogram_ref_file = to_absolute_path(\n                input_path=system_param[\'NETWORK\'].histogram_ref_file,\n                model_root=system_param[\'SYSTEM\'].model_dir)\n    except (AttributeError, KeyError):\n        pass\n    # 2. resolve output file:\n    try:\n        if system_param[\'INFERENCE\'].save_seg_dir:\n            system_param[\'INFERENCE\'].save_seg_dir = to_absolute_path(\n                input_path=system_param[\'INFERENCE\'].save_seg_dir,\n                model_root=system_param[\'SYSTEM\'].model_dir)\n    except (AttributeError, KeyError):\n        pass\n    # 3. resolve dataset splitting file:\n    try:\n        if system_param[\'SYSTEM\'].dataset_split_file:\n            system_param[\'SYSTEM\'].dataset_split_file = to_absolute_path(\n                input_path=system_param[\'SYSTEM\'].dataset_split_file,\n                model_root=system_param[\'SYSTEM\'].model_dir)\n    except (AttributeError, KeyError):\n        pass\n\n    # 4. resolve evaluation dir:\n    try:\n        if system_param[\'EVALUATION\'].save_csv_dir:\n            system_param[\'EVALUATION\'].save_csv_dir = to_absolute_path(\n                input_path=system_param[\'EVALUATION\'].save_csv_dir,\n                model_root=system_param[\'SYSTEM\'].model_dir)\n    except (AttributeError, KeyError):\n        pass\n\n    # start application\n    driver_table = {\n        TRAIN: ApplicationDriver,\n        INFER: ApplicationDriver,\n        EVAL: EvaluationApplicationDriver}\n    app_driver = driver_table[system_param[\'SYSTEM\'].action]()\n    app_driver.initialise_application(system_param, input_data_param)\n    app_driver.run(app_driver.app)\n\n    if tf.get_default_session() is not None:\n        tf.get_default_session().close()\n    tf.reset_default_graph()\n    close_logger()\n\n    return 0\n'"
tests/__init__.py,0,b''
tests/activation_test.py,8,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.layer.activation import ActiLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass ActivationTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def run_test(self, is_3d, type_str, expected_shape):\n        if is_3d:\n            x = self.get_3d_input()\n        else:\n            x = self.get_2d_input()\n        activation_layer = ActiLayer(func=type_str)\n        out_acti = activation_layer(x)\n        print(activation_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_acti)\n            self.assertAllClose(out.shape, expected_shape)\n\n    # 3d test\n    def test_3d_relu_shape(self):\n        self.run_test(True, \'relu\', (2, 16, 16, 16, 8))\n\n    def test_3d_relu6_shape(self):\n        self.run_test(True, \'relu6\', (2, 16, 16, 16, 8))\n\n    def test_3d_elu_shape(self):\n        self.run_test(True, \'elu\', (2, 16, 16, 16, 8))\n\n    def test_3d_selu_shape(self):\n        self.run_test(True, \'selu\', (2, 16, 16, 16, 8))\n\n    def test_3d_softplus_shape(self):\n        self.run_test(True, \'softplus\', (2, 16, 16, 16, 8))\n\n    def test_3d_softsign_shape(self):\n        self.run_test(True, \'softsign\', (2, 16, 16, 16, 8))\n\n    def test_3d_sigmoid_shape(self):\n        self.run_test(True, \'sigmoid\', (2, 16, 16, 16, 8))\n\n    def test_3d_tanh_shape(self):\n        self.run_test(True, \'tanh\', (2, 16, 16, 16, 8))\n\n    def test_3d_prelu_shape(self):\n        self.run_test(True, \'prelu\', (2, 16, 16, 16, 8))\n\n    def test_3d_prelu_reg_shape(self):\n        x = self.get_3d_input()\n        prelu_layer = ActiLayer(func=\'prelu\',\n                                regularizer=regularizers.l2_regularizer(0.5),\n                                name=\'regularized\')\n        out_prelu = prelu_layer(x)\n        print(prelu_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_prelu)\n            self.assertAllClose((2, 16, 16, 16, 8), out.shape)\n\n    def test_3d_dropout_shape(self):\n        x = self.get_3d_input()\n        dropout_layer = ActiLayer(func=\'dropout\')\n        out_dropout = dropout_layer(x, keep_prob=0.8)\n        print(dropout_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_dropout)\n            self.assertAllClose((2, 16, 16, 16, 8), out.shape)\n\n            # self.assertAllClose(input_shape, out.shape)\n            # self.assertAllClose(np.zeros(input_shape), out)\n\n    # 2d test\n    def test_2d_relu_shape(self):\n        self.run_test(False, \'relu\', (2, 16, 16, 8))\n\n    def test_2d_relu6_shape(self):\n        self.run_test(False, \'relu6\', (2, 16, 16, 8))\n\n    def test_2d_elu_shape(self):\n        self.run_test(False, \'elu\', (2, 16, 16, 8))\n\n    def test_2d_softplus_shape(self):\n        self.run_test(False, \'softplus\', (2, 16, 16, 8))\n\n    def test_2d_softsign_shape(self):\n        self.run_test(False, \'softsign\', (2, 16, 16, 8))\n\n    def test_2d_sigmoid_shape(self):\n        self.run_test(False, \'sigmoid\', (2, 16, 16, 8))\n\n    def test_2d_tanh_shape(self):\n        self.run_test(False, \'tanh\', (2, 16, 16, 8))\n\n    def test_2d_prelu_shape(self):\n        self.run_test(False, \'prelu\', (2, 16, 16, 8))\n\n    def test_2d_selu_shape(self):\n        self.run_test(False, \'selu\', (2, 16, 16, 8))\n\n    def test_2d_prelu_reg_shape(self):\n        x = self.get_2d_input()\n        prelu_layer = ActiLayer(func=\'prelu\',\n                                regularizer=regularizers.l2_regularizer(0.5),\n                                name=\'regularized\')\n        out_prelu = prelu_layer(x)\n        print(prelu_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_prelu)\n            self.assertAllClose((2, 16, 16, 8), out.shape)\n\n    def test_2d_dropout_shape(self):\n        x = self.get_2d_input()\n        dropout_layer = ActiLayer(func=\'dropout\')\n        out_dropout = dropout_layer(x, keep_prob=0.8)\n        print(dropout_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_dropout)\n            self.assertAllClose((2, 16, 16, 8), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/additive_upsample_test.py,6,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.additive_upsample import AdditiveUpsampleLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\ndef get_3d_input():\n    input_shape = (2, 16, 16, 16, 4)\n    x = tf.ones(input_shape)\n    return x\n\ndef get_2d_input():\n    input_shape = (2, 16, 16, 4)\n    x = tf.ones(input_shape)\n    return x\n\nclass AdditiveUpsampleTest(NiftyNetTestCase):\n    def run_test(self, new_size, n_splits, expected_shape, is_3d=True):\n        if is_3d:\n            x = get_3d_input()\n        else:\n            x = get_2d_input()\n\n        resize_layer = AdditiveUpsampleLayer(\n            new_size=new_size, n_splits=n_splits)\n        resized = resize_layer(x)\n        print(resize_layer)\n        with self.cached_session() as sess:\n            out = sess.run(resized)\n            self.assertAllClose(out.shape, expected_shape)\n\n    def test_3d_shape(self):\n        new_shape = (8, 8, 7)\n        n_splits = 2\n        expected_shape = (2,) + new_shape + (2,)\n        self.run_test(new_shape, n_splits, expected_shape, True)\n\n        new_shape = (20, 18, 17)\n        n_splits = 2\n        expected_shape = (2,) + new_shape + (2,)\n        self.run_test(new_shape, n_splits, expected_shape, True)\n\n        new_shape = (16, 16, 16)\n        n_splits = 2\n        expected_shape = (2,) + new_shape + (2,)\n        self.run_test(new_shape, n_splits, expected_shape)\n\n        new_shape = (16, 16, 16)\n        n_splits = 4\n        expected_shape = (2,) + new_shape + (1,)\n        self.run_test(new_shape, n_splits, expected_shape)\n\n    def test_2d_shape(self):\n        new_shape = (8, 7)\n        n_splits = 2\n        expected_shape = (2,) + new_shape + (2,)\n        self.run_test(new_shape, n_splits, expected_shape, False)\n\n        new_shape = (20, 18)\n        n_splits = 2\n        expected_shape = (2,) + new_shape + (2,)\n        self.run_test(new_shape, n_splits, expected_shape, False)\n\n        new_shape = (16, 16)\n        n_splits = 2\n        expected_shape = (2,) + new_shape + (2,)\n        self.run_test(new_shape, n_splits, expected_shape, False)\n\n        new_shape = (16, 16)\n        n_splits = 4\n        expected_shape = (2,) + new_shape + (1,)\n        self.run_test(new_shape, n_splits, expected_shape, False)\n\n    def test_int_shape(self):\n        new_shape = 20\n        n_splits = 2\n        expected_shape = (2,) + (new_shape,) * 2 + (2,)\n        self.run_test(new_shape, n_splits, expected_shape, False)\n        expected_shape = (2,) + (new_shape,) * 3 + (2,)\n        self.run_test(new_shape, n_splits, expected_shape, True)\n\n        n_splits = 4\n        expected_shape = (2,) + (new_shape,) * 2 + (1,)\n        self.run_test(new_shape, n_splits, expected_shape, False)\n        expected_shape = (2,) + (new_shape,) * 3 + (1,)\n        self.run_test(new_shape, n_splits, expected_shape, True)\n\n    def test_bad_int_shape(self):\n        new_shape = 0\n        n_splits = 2\n        with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, """"):\n            self.run_test(new_shape, n_splits, (new_shape,) * 2, False)\n\n        with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, """"):\n            self.run_test(new_shape, n_splits, (new_shape,) * 3, True)\n\n        new_shape = 2\n        n_splits = 0\n        with self.assertRaisesRegexp(AssertionError, """"):\n            self.run_test(new_shape, n_splits, (new_shape,) * 2, False)\n\n        with self.assertRaisesRegexp(AssertionError, """"):\n            self.run_test(new_shape, n_splits, (new_shape,) * 3, True)\n\n    def test_bad_shape(self):\n        new_shape = (20, 5)\n        n_splits = 2\n        with self.assertRaisesRegexp(AssertionError, """"):\n            self.run_test(new_shape, n_splits, new_shape)\n\n        new_shape = (0, 0, 0)\n        with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, """"):\n            self.run_test(new_shape, n_splits, new_shape)\n\n    def test_ill_input(self):\n        new_shape = (20, 15, 1)\n        n_splits = 2\n        expected_shape = (2,) + new_shape[:2] + (2,)\n        self.run_test(new_shape, n_splits, expected_shape, False)\n\n        new_shape = (20, 20)\n        n_splits = 2\n        expected_shape = (2,) + new_shape[:2] + (2,)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            self.run_test(new_shape, n_splits, expected_shape, True)\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/affine_augmentation_test.py,6,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.affine_augmentation import AffineAugmentationLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass RandRotationTest(NiftyNetTestCase):\n    def get_3d_image(self):\n        image_shape = (1, 200, 200, 1)\n        elements = tf.range(np.prod(image_shape), dtype=tf.float32)\n        image = tf.reshape(elements, image_shape)\n        return image\n\n    def get_2d_image(self):\n        image_shape = (2, 50, 50, 3)\n        elements = tf.range(np.prod(image_shape), dtype=tf.float32)\n        image = tf.reshape(elements, image_shape)\n        return image\n\n    def test_2d_shape(self):\n        augment_layer = AffineAugmentationLayer(0.2)\n        input_tensor = self.get_2d_image()\n        deformed = augment_layer(input_tensor)\n\n        inverse_augment_layer = augment_layer.inverse()\n        inverse = inverse_augment_layer(deformed)\n\n        with self.cached_session() as sess:\n            test_out = sess.run([input_tensor, deformed, inverse])\n            original, deformed_image, resumed_image = test_out\n            to_compare = resumed_image > 0\n            original = original[to_compare]\n            resumed_image = resumed_image[to_compare]\n\n            correct = np.sum(np.abs(original - resumed_image) < 1.0)\n            ratio = float(correct) / float(original.size)\n            print(ratio)\n            self.assertGreaterEqual(ratio, 0.9)\n\n    def test_3d_shape(self):\n        augment_layer = AffineAugmentationLayer(0.2)\n        input_tensor = self.get_3d_image()\n        deformed = augment_layer(input_tensor)\n\n        inverse_augment_layer = augment_layer.inverse()\n        inverse = inverse_augment_layer(deformed)\n\n        # with tf.Session() as sess:\n        with self.cached_session() as sess:\n            test_out = sess.run([input_tensor, deformed, inverse])\n            original, deformed_image, resumed_image = test_out\n            to_compare = resumed_image > 0\n            original = original[to_compare]\n            resumed_image = resumed_image[to_compare]\n\n            correct = np.sum(np.abs(original - resumed_image) < 1.0)\n            ratio = float(correct) / float(original.size)\n            print(ratio)\n            self.assertGreaterEqual(ratio, 0.95)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/application_driver_test.py,12,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\n\nimport uuid\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.application_driver import ApplicationDriver\nfrom niftynet.engine.application_variables import global_vars_init_or_restore\nfrom niftynet.engine.handler_model import ModelRestorer\nfrom niftynet.io.misc_io import set_logger\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom niftynet.engine.signal import SESS_FINISHED, SESS_STARTED\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n# def _run_test_application():\n#    test_driver = get_initialised_driver()\n#    test_driver.run_application()\n#    return\n\n\ndef get_initialised_driver(starting_iter=0,\n                           model_dir_rand=True,\n                           vars_to_restore=\'\',\n                           application=\'tests.toy_application.ToyApplication\'):\n    if model_dir_rand:\n        model_dir = os.path.join(\'.\', \'testing_data\', \'tmp\', str(uuid.uuid4()))\n        os.makedirs(model_dir)\n    else:\n        model_dir = os.path.join(\'.\', \'testing_data\')\n    system_param = {\n        \'SYSTEM\': ParserNamespace(\n            action=\'train\',\n            num_threads=2,\n            num_gpus=4,\n            cuda_devices=\'6\',\n            model_dir=model_dir,\n            dataset_split_file=os.path.join(\n                \'.\', \'testing_data\', \'testtoyapp.csv\'),\n            event_handler=[\n                \'niftynet.engine.handler_model.ModelRestorer\',\n                \'niftynet.engine.handler_sampler.SamplerThreading\',\n                \'niftynet.engine.handler_gradient.ApplyGradients\'],\n            iteration_generator=None),\n        \'NETWORK\': ParserNamespace(\n            batch_size=20,\n            name=\'tests.toy_application.TinyNet\'),\n        \'TRAINING\': ParserNamespace(\n            starting_iter=starting_iter,\n            max_iter=500,\n            save_every_n=20,\n            tensorboard_every_n=1,\n            max_checkpoints=20,\n            optimiser=\'niftynet.engine.application_optimiser.Adagrad\',\n            validation_every_n=-1,\n            exclude_fraction_for_validation=0.1,\n            exclude_fraction_for_inference=0.1,\n            vars_to_restore=vars_to_restore,\n            patience=100,\n            lr=0.01),\n        \'CUSTOM\': ParserNamespace(\n            vector_size=100,\n            mean=10.0,\n            stddev=2.0,\n            name=application)\n    }\n    app_driver = ApplicationDriver()\n    app_driver.initialise_application(system_param, {})\n    # set parameters without __init__\n    app_driver.app.action_param = system_param[\'TRAINING\']\n    app_driver.app.net_param = system_param[\'NETWORK\']\n    app_driver.app.action = \'train\'\n    return app_driver\n\n\nclass ApplicationDriverTest(NiftyNetTestCase):\n    def test_wrong_init(self):\n        app_driver = ApplicationDriver()\n        with self.assertRaisesRegexp(AttributeError, \'\'):\n            app_driver.initialise_application([], [])\n\n    # def test_create_app(self):\n    #     test_driver = get_initialised_driver(499, True)\n    #     with self.assertRaisesRegexp(ValueError, \'Could not import\'):\n    #         test_driver._create_app(\'test.test\')\n    #     with self.assertRaisesRegexp(ValueError, \'Could not import\'):\n    #         test_driver._create_app(\'testtest\')\n    #     with self.assertRaisesRegexp(ValueError, \'Could not import\'):\n    #         test_driver._create_app(1)\n    #     test_driver._create_app(\'tests.toy_application.ToyApplication\')\n\n    # def test_stop_app(self):\n    #     test_driver = get_initialised_driver()\n    #     graph = test_driver.create_graph(\n    #         test_driver.app, test_driver.num_gpus, True)\n    #     with self.cached_session(graph=graph) as sess:\n    #         sess.run(global_vars_init_or_restore())\n    #         GRAPH_CREATED.send(test_driver.app, iter_msg=None)\n    #         SESS_STARTED.send(test_driver.app, iter_msg=None)\n    #         train_op = test_driver.app.gradient_op\n    #         SESS_FINISHED.send(test_driver.app, itermsg=None)\n    #         test_driver.app.stop()\n    #         try:\n    #             while True:\n    #                 sess.run(train_op)\n    #         except tf.errors.OutOfRangeError:\n    #             for thread in test_driver.app.sampler[0][0]._threads:\n    #                 self.assertFalse(thread.isAlive(), ""threads not closed"")\n\n    def test_training_update(self):\n        test_driver = get_initialised_driver()\n        graph = test_driver.create_graph(test_driver.app, 1, True)\n        with self.cached_session(graph=graph) as sess:\n            SESS_STARTED.send(test_driver.app, iter_msg=None)\n\n            train_op = test_driver.app.gradient_op\n            test_tensor = graph.get_tensor_by_name(\n                \'G/conv_bn_selu/conv_/w:0\')\n            var_0 = sess.run(test_tensor)\n            sess.run(train_op)\n            var_1 = sess.run(test_tensor)\n            square_diff = np.sum(np.abs(var_0 - var_1))\n            self.assertGreater(\n                square_diff, 0.0, \'train_op does not change model\')\n            SESS_FINISHED.send(test_driver.app, itermsg=None)\n            test_driver.app.stop()\n\n    def test_multi_device_inputs(self):\n        test_driver = get_initialised_driver()\n        graph = test_driver.create_graph(\n            test_driver.app, test_driver.num_gpus, True)\n        with self.cached_session(graph=graph) as sess:\n            SESS_STARTED.send(test_driver.app, iter_msg=None)\n            for i in range(2):\n                sess.run(test_driver.app.gradient_op)\n                s_0, s_1, s_2, s_3 = sess.run([\n                    graph.get_tensor_by_name(\n                        \'worker_0/feature_input:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_1/feature_input:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_2/feature_input:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_3/feature_input:0\')\n                ])\n                msg = \'same input data for different devices\'\n                self.assertGreater(np.sum(np.abs(s_0 - s_1)), 0.0, msg)\n                self.assertGreater(np.sum(np.abs(s_0 - s_2)), 0.0, msg)\n                self.assertGreater(np.sum(np.abs(s_0 - s_3)), 0.0, msg)\n                self.assertGreater(np.sum(np.abs(s_1 - s_2)), 0.0, msg)\n                self.assertGreater(np.sum(np.abs(s_1 - s_3)), 0.0, msg)\n                self.assertGreater(np.sum(np.abs(s_2 - s_3)), 0.0, msg)\n            SESS_FINISHED.send(test_driver.app, itermsg=None)\n            test_driver.app.stop()\n\n    def test_multi_device_gradients(self):\n        test_driver = get_initialised_driver()\n        graph = test_driver.create_graph(\n            test_driver.app, test_driver.num_gpus, True)\n        with self.cached_session(graph=graph) as sess:\n            SESS_STARTED.send(test_driver.app, iter_msg=None)\n            for i in range(2):\n                sess.run(test_driver.app.gradient_op)\n                g_0, g_1, g_2, g_3, g_ave = sess.run([\n                    graph.get_tensor_by_name(\n                        \'worker_0/ComputeGradients/gradients/AddN_5:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_1/ComputeGradients/gradients/AddN_5:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_2/ComputeGradients/gradients/AddN_5:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_3/ComputeGradients/gradients/AddN_5:0\'),\n                    graph.get_tensor_by_name(\n                        \'ApplyGradients/AveOverDevices:0\')\n                ])\n                self.check_gradients(g_0, g_1, g_2, g_3, g_ave)\n            SESS_FINISHED.send(test_driver.app, itermsg=None)\n            test_driver.app.stop()\n\n    def test_multi_device_multi_optimiser_gradients(self):\n        test_driver = get_initialised_driver(\n            application=\'tests.toy_application.ToyApplicationMultOpti\')\n        graph = test_driver.create_graph(\n            test_driver.app, test_driver.num_gpus, True)\n        with self.cached_session(graph=graph) as sess:\n            SESS_STARTED.send(test_driver.app, iter_msg=None)\n            for i in range(2):\n                sess.run(test_driver.app.gradient_op)\n                # query generator gradient sample to check\n                dis_0, dis_1, dis_2, dis_3, dis_ave = sess.run([\n                    graph.get_tensor_by_name(\n                        \'worker_0/ComputeGradientsD/gradients/AddN_5:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_1/ComputeGradientsD/gradients/AddN_5:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_2/ComputeGradientsD/gradients/AddN_5:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_3/ComputeGradientsD/gradients/AddN_5:0\'),\n                    graph.get_tensor_by_name(\n                        \'ApplyGradients/AveOverDevices:0\')\n                ])\n\n                # query discriminator gradient sample to check\n                gen_0, gen_1, gen_2, gen_3, gen_ave = sess.run([\n                    graph.get_tensor_by_name(\n                        \'worker_0/ComputeGradientsG/gradients/worker_0/tinynet/G/conv/conv_/conv/ExpandDims_1_grad/Reshape:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_1/ComputeGradientsG/gradients/worker_1/tinynet/G/conv/conv_/conv/ExpandDims_1_grad/Reshape:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_2/ComputeGradientsG/gradients/worker_2/tinynet/G/conv/conv_/conv/ExpandDims_1_grad/Reshape:0\'),\n                    graph.get_tensor_by_name(\n                        \'worker_3/ComputeGradientsG/gradients/worker_3/tinynet/G/conv/conv_/conv/ExpandDims_1_grad/Reshape:0\'),\n                    graph.get_tensor_by_name(\n                        \'ApplyGradients/AveOverDevices_14:0\')\n                ])\n                self.check_gradients(gen_0, gen_1, gen_2, gen_3, gen_ave)\n                self.check_gradients(dis_0, dis_1, dis_2, dis_3, dis_ave)\n            SESS_FINISHED.send(test_driver.app, itermsg=None)\n            test_driver.app.stop()\n\n    def check_gradients(self, g_0, g_1, g_2, g_3, g_ave):\n        msg = \'same gradients for different devices\'\n        self.assertGreater(np.sum(np.abs(g_0 - g_1)), 0.0, msg)\n        self.assertGreater(np.sum(np.abs(g_0 - g_2)), 0.0, msg)\n        self.assertGreater(np.sum(np.abs(g_0 - g_3)), 0.0, msg)\n        self.assertGreater(np.sum(np.abs(g_1 - g_2)), 0.0, msg)\n        self.assertGreater(np.sum(np.abs(g_1 - g_3)), 0.0, msg)\n        self.assertGreater(np.sum(np.abs(g_2 - g_3)), 0.0, msg)\n        g_array = np.concatenate([g_0.reshape((1, -1)),\n                                  g_1.reshape((1, -1)),\n                                  g_2.reshape((1, -1)),\n                                  g_3.reshape((1, -1))], axis=0)\n        g_ave = g_ave.reshape(-1)\n        g_np_ave = np.mean(g_array, axis=0)\n        self.assertAllClose(g_np_ave, g_ave)\n\n    def test_rand_initialisation(self):\n        test_driver = get_initialised_driver(0, True)\n        graph = test_driver.create_graph(test_driver.app, 1, True)\n        with self.cached_session(graph=graph) as sess:\n            test_tensor = graph.get_tensor_by_name(\n                ""G/conv_bn_selu/conv_/w:0"")\n            with self.assertRaisesRegexp(\n                    tf.errors.FailedPreconditionError,\n                    \'uninitialized value\'):\n                sess.run(test_tensor)\n            ModelRestorer(**vars(test_driver)).rand_init_model(None)\n            sess.run(test_tensor)\n            _ = sess.run(tf.global_variables())\n\n    def test_from_latest_file_initialisation(self):\n        test_driver = get_initialised_driver(-1, False)\n        expected_init = np.array(\n            [[-0.03544217, 0.0228963, -0.04585603, 0.16923568, -0.51635778,\n              0.60694504, 0.01968583, -0.6252712, 0.28622296, -0.29527491,\n              0.61191976, 0.27878678, -0.07661559, -0.41357407, 0.70488983,\n              -0.10836645, 0.06488426, 0.0746650, -0.188567, -0.64652514]],\n            dtype=np.float32)\n        graph = test_driver.create_graph(test_driver.app, 1, True)\n        with self.cached_session(graph=graph) as sess:\n            test_tensor = graph.get_tensor_by_name(\n                ""G/conv_bn_selu/conv_/w:0"")\n            with self.assertRaisesRegexp(\n                    tf.errors.FailedPreconditionError,\n                    \'uninitialized value\'):\n                _ = sess.run(test_tensor)\n            ModelRestorer(**vars(test_driver)).restore_model(None)\n            after_init = sess.run(test_tensor)\n            self.assertAllClose(after_init[0], expected_init)\n            _ = sess.run(tf.global_variables())\n\n    # def test_not_found_file_initialisation(self):\n    #     test_driver = get_initialised_driver(42, False)\n    #     graph = test_driver.create_graph(test_driver.app, 1, True)\n    #     with self.cached_session(graph=graph) as sess:\n    #         with self.assertRaisesRegexp(\n    #                 ValueError, \'\'):\n    #             ModelRestorer(**vars(test_driver)).restore_model(None)\n    #         with self.assertRaisesRegexp(\n    #                 tf.errors.NotFoundError, \'Failed to find\'):\n    #             ModelRestorer(**vars(test_driver)).restore_model(None)\n\n    def test_from_file_initialisation(self):\n        test_driver = get_initialised_driver(40, False)\n        expected_init = np.array(\n            [[-0.23192197, 0.60880029, -0.24921742, -0.00186354, -0.3345384,\n              0.16067748, -0.2210995, -0.19460233, -0.3035436, -0.42839912,\n              -0.0489039, -0.90753943, -0.12664583, -0.23129687, 0.01584663,\n              -0.43854219, 0.40412974, 0.0396539, -0.1590578, -0.53759819]],\n            dtype=np.float32)\n        graph = test_driver.create_graph(test_driver.app, 1, True)\n        with self.cached_session(graph=graph) as sess:\n            test_tensor = graph.get_tensor_by_name(\n                ""G/conv_bn_selu/conv_/w:0"")\n            with self.assertRaisesRegexp(\n                    tf.errors.FailedPreconditionError,\n                    \'uninitialized value\'):\n                _ = sess.run(test_tensor)\n            ModelRestorer(**vars(test_driver)).restore_model(None)\n            after_init = sess.run(test_tensor)\n            self.assertAllClose(after_init[0], expected_init)\n            _ = sess.run(tf.global_variables())\n\n    def test_from_file_finetuning(self):\n        test_driver = get_initialised_driver(\n            40, False, \'.*conv_bn_selu/.*conv_.*\')\n        expected_init = np.array(\n            [[-0.23192197, 0.60880029, -0.24921742, -0.00186354, -0.3345384,\n              0.16067748, -0.2210995, -0.19460233, -0.3035436, -0.42839912,\n              -0.0489039, -0.90753943, -0.12664583, -0.23129687, 0.01584663,\n              -0.43854219, 0.40412974, 0.0396539, -0.1590578, -0.53759819]],\n            dtype=np.float32)\n        graph = test_driver.create_graph(test_driver.app, 1, True)\n        with self.cached_session(graph=graph) as sess:\n            test_tensor = graph.get_tensor_by_name(\n                ""G/conv_bn_selu/conv_/w:0"")\n            test_negative_tensor = graph.get_tensor_by_name(\n                ""D/conv_relu/conv_/b:0"")\n            with self.assertRaisesRegexp(\n                    tf.errors.FailedPreconditionError,\n                    \'uninitialized value\'):\n                _ = sess.run(test_tensor)\n            ModelRestorer(**vars(test_driver)).restore_model(None)\n            after_init = sess.run(test_tensor)\n            after_init_negative = sess.run(test_negative_tensor)\n            self.assertAllClose(after_init[0], expected_init)\n            # the not matched should be initialised using default initializer\n            self.assertEqual(np.any(after_init_negative), False)\n            _ = sess.run(tf.global_variables())\n            bad_init = sess.run(tf.report_uninitialized_variables())\n            self.assertEqual(bad_init.size, 0)\n\n\nif __name__ == ""__main__"":\n    set_logger()\n    # _run_test_application()\n    tf.test.main()\n'"
tests/application_factory_test.py,1,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nimport niftynet.engine.application_factory as Factory\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass FactoryTest(NiftyNetTestCase):\n    def test_import(self):\n        var_names = [\n            item for item in list(dir(Factory)) if item.startswith(""SUPPORTED"")]\n        for var_name in var_names:\n            mod_table = Factory.__dict__[var_name]\n            for mod_name in list(mod_table):\n                Factory.select_module(mod_name, \'test\', mod_table)\n\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/approximated_smoothing_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.approximated_smoothing import SmoothingLayer as Smoothing\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass SmoothingTest(NiftyNetTestCase):\n    def get_1d_input(self):\n        input_shape = (2, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def run_test(self, ndim, sigma, type_str):\n        if ndim == 3:\n            x = self.get_3d_input()\n        elif ndim == 2:\n            x = self.get_2d_input()\n        else:\n            x = self.get_1d_input()\n\n        smoothing_layer = Smoothing(sigma=sigma, type_str=type_str)\n        smoothed = smoothing_layer(x)\n        print(smoothing_layer)\n        with self.cached_session() as sess:\n            out = sess.run(smoothed)\n            self.assertAllClose(out.shape, x.shape.as_list())\n        return out\n\n    def test_shape(self):\n        self.run_test(1, 2, \'cauchy\')\n        self.run_test(1, 2, \'gaussian\')\n        self.run_test(2, 2, \'cauchy\')\n        self.run_test(2, 2, \'gaussian\')\n        self.run_test(3, 2, \'cauchy\')\n        self.run_test(3, 2, \'gaussian\')\n\n    def test_ill_inputs(self):\n        self.run_test(3, -1, \'gaussian\')\n        self.run_test(3, -1, \'cauchy\')\n        self.run_test(3, 100, \'cauchy\')\n\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            self.run_test(3, -1, \'gassian\')\n\n    def test_original(self):\n        out = self.run_test(3, 0, \'gaussian\')\n        self.assertTrue(np.all(out==1))\n        out = self.run_test(2, 0, \'gaussian\')\n        self.assertTrue(np.all(out==1))\n        out = self.run_test(1, 0, \'gaussian\')\n        self.assertTrue(np.all(out==1))\n        out = self.run_test(3, 0, \'cauchy\')\n        self.assertTrue(np.all(out==1))\n        out = self.run_test(2, 0, \'cauchy\')\n        self.assertTrue(np.all(out==1))\n        out = self.run_test(1, 0, \'cauchy\')\n        self.assertTrue(np.all(out==1))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/binary_masking_test.py,1,"b'from __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass BinaryMaskingTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (16, 16, 16)\n        x = np.random.randint(-10, 10, size=input_shape)\n        return x\n\n    def get_5d_input(self):\n        input_shape = (16, 16, 16, 3, 2)\n        x = np.random.randint(-10, 10, size=input_shape)\n        return x\n\n    def test_3d_plus_shape(self):\n        x = self.get_3d_input()\n        mask_layer = BinaryMaskingLayer(\n            type_str=\'otsu_plus\',\n            multimod_fusion=\'or\',\n            threshold=0.0)\n        mask_out = mask_layer(x)\n        print(mask_layer)\n        self.assertAllClose(x.shape, mask_out.shape)\n\n    def test_3d_minus_shape(self):\n        x = self.get_3d_input()\n        mask_layer = BinaryMaskingLayer(\n            type_str=\'otsu_minus\',\n            multimod_fusion=\'or\',\n            threshold=0.0)\n        mask_out = mask_layer(x)\n        print(mask_layer)\n        self.assertAllClose(x.shape, mask_out.shape)\n\n    def test_5d_shape(self):\n        x = self.get_5d_input()\n        mask_layer = BinaryMaskingLayer(\n            type_str=\'threshold_minus\',\n            multimod_fusion=\'and\',\n            threshold=0.0)\n        mask_out = mask_layer(x)\n        print(mask_layer)\n        self.assertAllClose(x.shape, mask_out.shape)\n\n    def test_5d_mean_shape(self):\n        x = self.get_5d_input()\n        mask_layer = BinaryMaskingLayer(\n            type_str=\'mean_plus\',\n            multimod_fusion=\'and\',\n            threshold=0.0)\n        mask_out = mask_layer(x)\n        print(mask_layer)\n        self.assertAllClose(x.shape, mask_out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/bn_test.py,10,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.layer.bn import BNLayer\nfrom niftynet.layer.bn import InstanceNormLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass BNTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def test_3d_bn_shape(self):\n        x = self.get_3d_input()\n        bn_layer = BNLayer()\n        print(bn_layer)\n        out_bn = bn_layer(x, is_training=True)\n        print(bn_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_bn)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n    def test_3d_instnorm_shape(self):\n        x = self.get_3d_input()\n        instnorm_layer = InstanceNormLayer()\n        print(instnorm_layer)\n        out_inst = instnorm_layer(x)\n        print(instnorm_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_inst)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n    def test_3d_bn_reg_shape(self):\n        x = self.get_3d_input()\n        bn_layer = BNLayer(regularizer=regularizers.l2_regularizer(0.5))\n        out_bn = bn_layer(x, is_training=True)\n        test_bn = bn_layer(x, is_training=False)\n        print(bn_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_bn)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n            out = sess.run(test_bn)\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n    def test_2d_bn_shape(self):\n        x = self.get_2d_input()\n        bn_layer = BNLayer()\n        out_bn = bn_layer(x, is_training=True)\n        print(bn_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_bn)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n    def test_2d_instnorm_shape(self):\n        x = self.get_2d_input()\n        instnorm_layer = InstanceNormLayer()\n        out_inst = instnorm_layer(x)\n        print(instnorm_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_inst)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n    def test_2d_bn_reg_shape(self):\n        x = self.get_2d_input()\n        bn_layer = BNLayer(regularizer=regularizers.l2_regularizer(0.5))\n        out_bn = bn_layer(x, is_training=True)\n        test_bn = bn_layer(x, is_training=False)\n        print(bn_layer)\n        reg_loss = tf.add_n(bn_layer.regularizer_loss())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_bn)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n            out = sess.run(test_bn)\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n            out = sess.run(reg_loss)\n            self.assertAlmostEqual(out, 2.0)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/channel_sparse_convolution_test.py,7,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom niftynet.layer.channel_sparse_convolution import ChannelSparseConvolutionalLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\n\ndef get_config():\n    rewrite_options = rewriter_config_pb2.RewriterConfig(\n        layout_optimizer=rewriter_config_pb2.RewriterConfig.OFF)\n    graph_options = config_pb2.GraphOptions(\n        rewrite_options=rewrite_options, build_cost_model=1)\n    config = config_pb2.ConfigProto(graph_options=graph_options)\n    return config\n\n\nclass ChannelSparseConvolutionalLayerTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        x = tf.random_normal(shape=[2,4,5,6,4])\n        conv1 = ChannelSparseConvolutionalLayer(4)\n        conv2 = ChannelSparseConvolutionalLayer(8, kernel_size=[1,1,3])\n        conv3 = ChannelSparseConvolutionalLayer(4, acti_func=\'relu\')\n        conv4 = ChannelSparseConvolutionalLayer(8, feature_normalization=None)\n        conv5 = ChannelSparseConvolutionalLayer(4, with_bias=True)\n        x1, mask1=conv1(x, None, True, 1.)\n        x2, mask2=conv2(x1, mask1, True, 1.)\n        x3, mask3=conv3(x2, mask2, True, .5)\n        x4, mask4=conv4(x3, mask3, True, .75)\n        x5, mask5=conv5(x4, mask4, True, 1.)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out1, out2, out3, out4, out5 = sess.run([x1,x2,x3,x4,x5])\n        self.assertAllClose([2,4,5,6,4], out1.shape)\n        self.assertAllClose([2,4,5,6,8], out2.shape)\n        self.assertAllClose([2,4,5,6,2], out3.shape)\n        self.assertAllClose([2,4,5,6,6], out4.shape)\n        self.assertAllClose([2,4,5,6,4], out5.shape)\n\n    def test_2d_shape(self):\n        x = tf.random_normal(shape=[2,4,5,4])\n        conv1 = ChannelSparseConvolutionalLayer(4)\n        conv2 = ChannelSparseConvolutionalLayer(8, kernel_size=[1,1,3])\n        conv3 = ChannelSparseConvolutionalLayer(4, acti_func=\'relu\')\n        conv4 = ChannelSparseConvolutionalLayer(8, feature_normalization=None)\n        conv5 = ChannelSparseConvolutionalLayer(4, with_bias=True)\n        x1, mask1=conv1(x, None, True, 1.)\n        x2, mask2=conv2(x1, mask1, True, 1.)\n        x3, mask3=conv3(x2, mask2, True, .5)\n        x4, mask4=conv4(x3, mask3, True, .75)\n        x5, mask5=conv5(x4, mask4, True, 1.)\n\n        with self.cached_session(config=get_config()) as sess:\n            sess.run(tf.global_variables_initializer())\n            out1, out2, out3, out4, out5 = sess.run([x1,x2,x3,x4,x5])\n        self.assertAllClose([2,4,5,4], out1.shape)\n        self.assertAllClose([2,4,5,8], out2.shape)\n        self.assertAllClose([2,4,5,2], out3.shape)\n        self.assertAllClose([2,4,5,6], out4.shape)\n        self.assertAllClose([2,4,5,4], out5.shape)\n\n    def test_masks(self):\n        x = tf.random_normal(shape=[2,4,5,4])\n        conv1 = ChannelSparseConvolutionalLayer(10)\n        conv2 = ChannelSparseConvolutionalLayer(10)\n        conv3 = ChannelSparseConvolutionalLayer(10)\n        conv4 = ChannelSparseConvolutionalLayer(10)\n        x1, mask1=conv1(x, None, True, 1.)\n        x2, mask2=conv2(x1, mask1, True, .5)\n        x3, mask3=conv3(x2, mask2, True, .2)\n        x4, mask4=conv4(x3, mask3, True, 1.)\n\n        with self.cached_session(config=get_config()) as sess:\n            sess.run(tf.global_variables_initializer())\n            out1, out2, out3, out4 = sess.run([mask1, mask2, mask3, mask4])\n        self.assertAllClose([10, 5, 2, 10], [np.sum(out1),\n                                             np.sum(out2),\n                                             np.sum(out3),\n                                             np.sum(out4)])\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/classification_evaluator_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport six\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.evaluation.classification_evaluator import ClassificationEvaluator\nfrom niftynet.io.misc_io import set_logger\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass ClassificationEvaluatorTest(NiftyNetTestCase):\n    def test_basic(self):\n        class NS(object):\n            def __init__(self, dict):\n                self.__dict__.update(dict)\n        classification_param=NS({\'num_classes\':2,\n                                 \'output_prob\':False})\n        eval_param=NS({\'evaluations\':\'niftynet.evaluation.classification_evaluations.accuracy\'})\n        positive = np.reshape(1,[1,1,1,1,1])\n        negative = np.reshape(0,[1,1,1,1,1])\n        mask = np.reshape(np.abs(np.linspace(0.,2.,64)-1)>.8,[4,4,4,1,1])\n        tp = {\'label\':positive,\'inferred\':positive}\n        fp = {\'label\':negative,\'inferred\':positive}\n        tn = {\'label\':negative,\'inferred\':negative}\n        fn = {\'label\':positive,\'inferred\':negative}\n        interp_orders = {\'label\':0,\'inferred\':-1}\n        image_folder = \'.\'\n        e = ClassificationEvaluator(None, classification_param, eval_param)\n\n        def generator():\n            yield (\'test1\', tp,interp_orders)\n            yield (\'test2\', tp,interp_orders)\n            yield (\'test3\', fn,interp_orders)\n            yield (\'test4\', fp,interp_orders)\n\n        result_dict = e.evaluate_from_generator(generator())\n        self.assertIn((None,), result_dict)\n        self.assertEqual(result_dict[(None,)].to_dict(\'index\'),\n                      {0: {\'accuracy\': 0.5}})\n\n\n\nif __name__ == ""__main__"":\n    set_logger()\n    # _run_test_application()\n    tf.test.main()\n'"
tests/convolution_test.py,12,"b'from __future__ import division, absolute_import, print_function\n\nimport functools as ft\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.layer.convolution import ConvLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass ConvTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x_3d = tf.ones(input_shape)\n        return x_3d\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x_2d = tf.ones(input_shape)\n        return x_2d\n\n    def _test_conv_output_shape(self,\n                                rank,\n                                param_dict,\n                                output_shape):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        conv_layer = ConvLayer(**param_dict)\n        output_data = conv_layer(input_data)\n        print(conv_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_value = sess.run(output_data)\n            self.assertAllClose(output_shape, output_value.shape)\n\n    def _test_conv_layer_output_shape(self,\n                                      rank,\n                                      param_dict,\n                                      output_shape,\n                                      is_training=None,\n                                      dropout_prob=None):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        conv_layer = ConvolutionalLayer(**param_dict)\n        output_data = conv_layer(input_data,\n                                 is_training=is_training,\n                                 keep_prob=dropout_prob)\n        print(conv_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_value = sess.run(output_data)\n            self.assertAllClose(output_shape, output_value.shape)\n\n    def _test_extended_conv(self, orig_input, init_dict):\n        """"""\n        Tests the extended padding options of ConvLayer\n        """"""\n\n        def _w_init(shape, dtype=tf.float32, **kwargs):\n            data = np.arange(ft.reduce(lambda prod, x: prod*x, shape, 1))\\\n                     .astype(np.float32)\n            data *= 2.374/data.mean()\n            data -= data.mean()\n\n            return tf.constant(data.reshape(shape), dtype=dtype)\n\n        def _b_init(shape, dtype=tf.float32, **kwargs):\n            data = np.arange(shape[0]).astype(np.float32)\n            data *= 0.273/data.mean()\n            data -= data.mean()\n\n            return tf.constant(data.reshape(shape), dtype=dtype)\n\n        init_dict[\'w_initializer\'] = _w_init\n        init_dict[\'b_initializer\'] = _b_init\n\n        conv_layer = ConvLayer(**init_dict)\n        small_output = conv_layer(tf.constant(orig_input))\n\n        input_shape = orig_input.shape\n        multiplier = init_dict[\'kernel_size\'] + init_dict[\'dilation\'] \\\n            + init_dict[\'stride\']\n        pad = [d*multiplier for d in input_shape[1:-1]]\n        paddings = [(0, 0)] + [(p, p) for p in pad] + [(0, 0)]\n\n        if init_dict[\'padding\'] == \'CONSTANT\':\n            opts = {\'constant_values\': init_dict.get(\'padding_constant\', 0)}\n        else:\n            opts = {}\n\n        enlarged_input = np.pad(orig_input,\n                                paddings,\n                                init_dict[\'padding\'].lower(),\n                                **opts)\n\n        conv_layer.padding = \'SAME\'\n        large_output = conv_layer(tf.constant(enlarged_input))\n\n        def _extract_valid_region(output_tensor, target_tensor):\n            output_shape = output_tensor.shape\n            target_shape = target_tensor.shape\n            extr_slices = []\n            for d in range(len(target_shape)):\n                opad = (output_shape[d] - target_shape[d])//2\n                extr_slices.append(slice(\n                    opad, opad + target_shape[d]))\n\n            return output_tensor[tuple(extr_slices)]\n\n        assert np.square(\n            _extract_valid_region(enlarged_input, orig_input) - orig_input).sum() \\\n            <= 1e-6*np.square(orig_input).sum()\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            small_value = sess.run(small_output)\n            large_value = sess.run(large_output)\n\n            extr_value = _extract_valid_region(large_value, small_value)\n\n            print(np.square(small_value - extr_value).sum()/np.square(extr_value).sum())\n\n            self.assertAllClose(small_value, extr_value, rtol=1e-3)\n\n    def _get_pad_test_input_3d(self):\n        data = np.arange(1024, dtype=np.float32)\n\n        return data.reshape([1, 16, 4, 4, 4])\n\n    def _get_pad_test_input_2d(self):\n        data = np.arange(256, dtype=np.float32)\n\n        return data.reshape([4, 8, 4, 2])\n\n    # padding tests\n    def _test_extended_padding(self, pad, do_2d):\n        batch = self._get_pad_test_input_2d() if do_2d \\\n            else self._get_pad_test_input_3d()\n\n        const = 127.23\n        min_dim = min(batch.shape[1:-1]) - 1\n        for ks in (2, min_dim):\n            for ds in (1, min_dim):\n                name = \'pad_test_conv\' + (\'2\' if do_2d else \'3\')\n                name += ""%i_%i"" % (ks, ds)\n                init_dict = {\'n_output_chns\': 4,\n                             \'kernel_size\': ks,\n                             \'stride\': 1,\n                             \'dilation\': ds,\n                             \'padding\': pad,\n                             \'name\': name}\n\n                if ds%2 == 0:\n                    init_dict[\'padding_constant\'] = const\n\n                self._test_extended_conv(batch, init_dict)\n\n    def test_2d_const_padding(self):\n        self._test_extended_padding(\'CONSTANT\', True)\n\n    def test_2d_reflect_padding(self):\n        self._test_extended_padding(\'REFLECT\', True)\n\n    def test_2d_symmetric_padding(self):\n        self._test_extended_padding(\'SYMMETRIC\', True)\n\n    def test_3d_const_padding(self):\n        self._test_extended_padding(\'CONSTANT\', False)\n\n    def test_3d_reflect_padding(self):\n        self._test_extended_padding(\'REFLECT\', False)\n\n    def test_3d_symmetric_padding(self):\n        self._test_extended_padding(\'SYMMETRIC\', False)\n\n    # 3d tests\n    def test_3d_conv_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1}\n        self._test_conv_output_shape(rank=3,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 16, 10))\n\n    def test_3d_conv_full_kernel_size(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 3, 1],\n                       \'stride\': 1}\n        self._test_conv_output_shape(rank=3,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 16, 10))\n\n    def test_3d_conv_full_strides(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 3, 1],\n                       \'stride\': [1, 1, 2]}\n        self._test_conv_output_shape(rank=3,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 8, 10))\n\n    def test_3d_anisotropic_conv(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 2, 1],\n                       \'stride\': [1, 1, 2]}\n        self._test_conv_output_shape(rank=3,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 8, 10))\n\n    def test_3d_conv_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1, 3],\n                       \'stride\': [1, 1, 2],\n                       \'with_bias\': True}\n        self._test_conv_output_shape(rank=3,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 8, 10))\n\n    def test_conv_3d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1, 3],\n                       \'stride\': [2, 2, 2],\n                       \'with_bias\': True,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_output_shape(rank=3,\n                                     param_dict=input_param,\n                                     output_shape=(2, 8, 8, 8, 10))\n\n    def test_3d_convlayer_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1}\n        self._test_conv_output_shape(rank=3,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 16, 10))\n\n    def test_3d_convlayer_dilation_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1,\n                       \'dilation\': [1, 2, 1]}\n        self._test_conv_output_shape(rank=3,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 16, 10))\n\n    def test_3d_convlayer_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1,\n                       \'with_bias\': True,\n                       \'feature_normalization\': None}\n        self._test_conv_layer_output_shape(rank=3,\n                                           param_dict=input_param,\n                                           output_shape=(2, 16, 16, 16, 10))\n\n    def test_convlayer_3d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1,\n                       \'with_bias\': True,\n                       \'feature_normalization\': None,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=3,\n                                           param_dict=input_param,\n                                           output_shape=(2, 16, 16, 16, 10))\n\n    def test_convlayer_3d_bn_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [5, 1, 2],\n                       \'stride\': 1,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=3,\n                                           param_dict=input_param,\n                                           output_shape=(2, 16, 16, 16, 10),\n                                           is_training=True)\n\n    def test_convlayer_3d_bn_reg_prelu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [5, 1, 2],\n                       \'stride\': [1, 1, 2],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=3,\n                                           param_dict=input_param,\n                                           output_shape=(2, 16, 16, 8, 10),\n                                           is_training=True)\n\n    def test_convlayer_3d_relu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [5, 1, 2],\n                       \'stride\': [1, 2, 2],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'relu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=3,\n                                           param_dict=input_param,\n                                           output_shape=(2, 16, 8, 8, 10),\n                                           is_training=True)\n\n    def test_convlayer_3d_bn_reg_dropout_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [5, 1, 2],\n                       \'stride\': [1, 2, 2],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\'}\n        self._test_conv_layer_output_shape(rank=3,\n                                           param_dict=input_param,\n                                           output_shape=(2, 16, 8, 8, 10),\n                                           is_training=True,\n                                           dropout_prob=0.4)\n\n    def test_convlayer_3d_bn_reg_dropout_valid_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [5, 3, 2],\n                       \'stride\': [2, 2, 3],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'acti_func\': \'prelu\',\n                       \'padding\': \'VALID\'}\n        self._test_conv_layer_output_shape(rank=3,\n                                           param_dict=input_param,\n                                           output_shape=(2, 6, 7, 5, 10),\n                                           is_training=True,\n                                           dropout_prob=0.4)\n\n    def test_convlayer_3d_group_reg_dropout_valid_shape(self):\n        input_param = {\'n_output_chns\': 8,\n                       \'kernel_size\': [5, 3, 2],\n                       \'stride\': [2, 2, 3],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'group\',\n                       \'group_size\': 4,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=3,\n                                           param_dict=input_param,\n                                           output_shape=(2, 8, 8, 6, 8),\n                                           is_training=True,\n                                           dropout_prob=0.4)\n\n    # 2d tests\n    def test_2d_conv_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [5, 3],\n                       \'stride\': [2, 2]}\n        self._test_conv_output_shape(rank=2,\n                                     param_dict=input_param,\n                                     output_shape=(2, 8, 8, 10))\n\n    def test_2d_conv_dilation_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [5, 3],\n                       \'stride\': [1, 1],\n                       \'dilation\': [2, 1]}\n        self._test_conv_output_shape(rank=2,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 10))\n\n    def test_2d_conv_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [5, 3],\n                       \'stride\': [1, 2],\n                       \'with_bias\': True}\n        self._test_conv_output_shape(rank=2,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 8, 10))\n\n    def test_conv_2d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 5,\n                       \'stride\': 1,\n                       \'with_bias\': True,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_output_shape(rank=2,\n                                     param_dict=input_param,\n                                     output_shape=(2, 16, 16, 10))\n\n    def test_2d_convlayer_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 2,\n                       \'stride\': 1,\n                       \'with_bias\': True}\n        self._test_conv_layer_output_shape(rank=2,\n                                           param_dict=input_param,\n                                           output_shape=(2, 16, 16, 10),\n                                           is_training=True)\n\n    def test_2d_convlayer_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 2,\n                       \'stride\': [2, 1],\n                       \'with_bias\': True,\n                       \'feature_normalization\': None}\n        self._test_conv_layer_output_shape(rank=2,\n                                           param_dict=input_param,\n                                           output_shape=(2, 8, 16, 10))\n\n    def test_convlayer_2d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 5],\n                       \'stride\': [2, 1],\n                       \'with_bias\': True,\n                       \'feature_normalization\': None,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=2,\n                                           param_dict=input_param,\n                                           output_shape=(2, 8, 16, 10))\n\n    def test_convlayer_2d_bn_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 5],\n                       \'stride\': [2, 1],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=2,\n                                           param_dict=input_param,\n                                           output_shape=(2, 8, 16, 10),\n                                           is_training=True)\n\n    def test_convlayer_2d_bn_reg_prelu_2_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': [2, 1],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\'}\n        self._test_conv_layer_output_shape(rank=2,\n                                           param_dict=input_param,\n                                           output_shape=(2, 8, 16, 10),\n                                           is_training=True)\n\n    def test_convlayer_2d_relu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': [3, 1],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'relu\'}\n        self._test_conv_layer_output_shape(rank=2,\n                                           param_dict=input_param,\n                                           output_shape=(2, 6, 16, 10),\n                                           is_training=True)\n\n    def test_convlayer_2d_bn_reg_prelu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=2,\n                                           param_dict=input_param,\n                                           output_shape=(2, 16, 16, 10),\n                                           is_training=True)\n\n    def test_convlayer_2d_bn_reg_valid_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 2],\n                       \'stride\': [2, 3],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'padding\': \'VALID\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_conv_layer_output_shape(rank=2,\n                                           param_dict=input_param,\n                                           output_shape=(2, 7, 5, 10),\n                                           is_training=True)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/crf_test.py,38,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.crf import CRFAsRNNLayer\nfrom niftynet.layer.crf import permutohedral_prepare, permutohedral_compute\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass CRFTest(NiftyNetTestCase):\n    def test_2d3d_shape(self):\n        tf.reset_default_graph()\n        I = tf.random_normal(shape=[2, 4, 5, 6, 3])\n        U = tf.random_normal(shape=[2, 4, 5, 6, 2])\n        crf_layer = CRFAsRNNLayer(T=3)\n        crf_layer2 = CRFAsRNNLayer(T=2)\n        out1 = crf_layer(I, U)\n        out2 = crf_layer2(I[:, :, :, 0, :], out1[:, :, :, 0, :])\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out1, out2 = sess.run([out1, out2])\n            U_shape = tuple(U.shape.as_list())\n            self.assertAllClose(U_shape, out1.shape)\n            U_shape = tuple(U[:, :, :, 0, :].shape.as_list())\n            self.assertAllClose(U_shape, out2.shape)\n\n    def test_training_3d(self):\n        n_features = 2\n        n_classes = 3\n        # 4-features\n        features = tf.random_normal(shape=[2, 8, 8, 8, n_features])\n        # 3-class classification\n        logits = tf.random_normal(shape=[2, 8, 8, 8, n_classes])\n        # ground truth\n        gt = tf.random_uniform(\n            shape=[2, 8, 8, 8, n_classes], minval=0, maxval=1)\n\n        crf_layer = CRFAsRNNLayer()\n        smoothed_logits = crf_layer(features, logits)\n        loss = tf.reduce_mean(tf.abs(smoothed_logits - gt))\n        opt = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            params = sess.run(tf.trainable_variables())\n            for param in params:\n                if param.shape == (n_classes, n_classes):\n                    self.assertAllClose(param, -1.0 * np.eye(n_classes))\n            sess.run(opt)\n            params_1 = sess.run(tf.trainable_variables())\n            self.assertGreater(np.sum(np.abs(params_1[0] - params[0])), 0.0)\n\n    def test_training_2d(self):\n        batch_size = 1\n        n_features = 2\n        n_classes = 3\n        # 2-features\n        features = tf.random_normal(shape=[batch_size, 8, 8, n_features])\n        # 3-class classification\n        logits = tf.random_normal(shape=[batch_size, 8, 8, n_classes])\n        # ground truth\n        gt = tf.random_uniform(\n            shape=[batch_size, 8, 8, n_classes], minval=0, maxval=1)\n\n        crf_layer = CRFAsRNNLayer(\n            w_init=[[1] * n_classes, [1] * n_classes],\n            mu_init=np.eye(n_classes),\n            T=2)\n        smoothed_logits = crf_layer(features, logits)\n        pred = tf.nn.softmax(smoothed_logits)\n        loss = tf.reduce_mean(tf.abs(smoothed_logits - gt))\n        opt = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            params = sess.run(tf.trainable_variables())\n            for param in params:\n                if param.shape == (n_classes, n_classes):\n                    self.assertAllClose(param, np.eye(n_classes))\n            sess.run(opt)\n            params_1 = sess.run(tf.trainable_variables())\n            print(params_1)\n            self.assertGreater(np.sum(np.abs(params_1[0] - params[0])), 0.0)\n\n    def test_training_4d(self):\n        sp = 8\n        batch_size = 2\n        n_features = 2\n        n_classes = 3\n        # 2-features\n        features = tf.random_normal(\n            shape=[batch_size, sp, sp, sp, sp, n_features])\n        # 3-class classification\n        logits = tf.random_normal(\n            shape=[batch_size, sp, sp, sp, sp, n_classes])\n        # ground truth\n        gt = tf.random_uniform(\n            shape=[batch_size, sp, sp, sp, sp, n_classes], minval=0, maxval=1)\n\n        with tf.device(\'/cpu:0\'):\n            crf_layer = CRFAsRNNLayer(\n                w_init=[[1] * n_classes, [1] * n_classes],\n                mu_init=np.eye(n_classes),\n                T=2)\n            smoothed_logits = crf_layer(features, logits)\n        loss = tf.reduce_mean(tf.abs(smoothed_logits - gt))\n        opt = tf.train.GradientDescentOptimizer(0.5).minimize(\n            loss, colocate_gradients_with_ops=True)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            params = sess.run(tf.trainable_variables())\n            for param in params:\n                if param.shape == (n_classes, n_classes):\n                    self.assertAllClose(param, np.eye(n_classes))\n            sess.run(opt)\n            params_1 = sess.run(tf.trainable_variables())\n            self.assertGreater(np.sum(np.abs(params_1[0] - params[0])), 0.0)\n\n    def test_batch_mix(self):\n        feat = tf.random.uniform(shape=[2, 64, 5])\n        desc = tf.ones(shape=[1, 64, 1])\n        desc_ = tf.zeros(shape=[1, 64, 1])\n        desc = tf.concat([desc, desc_], axis=0)\n        barycentric, blur_neighbours1, blur_neighbours2, indices = permutohedral_prepare(feat)\n        sliced = permutohedral_compute(desc,\n                          barycentric,\n                          blur_neighbours1,\n                          blur_neighbours2,\n                          indices,\n                          ""test"",\n                          True)\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sliced_np = sess.run(sliced)\n            self.assertAllClose(sliced_np[1:], np.zeros(shape=[1, 64, 1]))\n\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/crop_test.py,5,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom niftynet.layer.crop import CropLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass CropTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 16, 16, 16, 8)\n        test_border = 3\n        x = tf.ones(input_shape)\n\n        crop_layer = CropLayer(border=test_border)\n        out_crop = crop_layer(x)\n        print(crop_layer)\n\n        input_shape = (2, 7, 7, 7, 8)\n        test_border = 3\n        x = tf.ones(input_shape)\n\n        crop_layer = CropLayer(border=test_border)\n        out_crop_1 = crop_layer(x)\n        print(crop_layer)\n\n        with self.cached_session() as sess:\n            out = sess.run(out_crop)\n            out_1 = sess.run(out_crop_1)\n            self.assertAllClose((2, 10, 10, 10, 8), out.shape)\n            self.assertAllClose((2, 1, 1, 1, 8), out_1.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 16, 16, 8)\n        test_border = 3\n        x = tf.ones(input_shape)\n\n        crop_layer = CropLayer(border=test_border)\n        out_crop = crop_layer(x)\n        print(crop_layer)\n\n        input_shape = (2, 7, 7, 8)\n        test_border = 3\n        x = tf.ones(input_shape)\n\n        crop_layer = CropLayer(border=test_border)\n        out_crop_1 = crop_layer(x)\n        print(crop_layer)\n\n        with self.cached_session() as sess:\n            out = sess.run(out_crop)\n            out_1 = sess.run(out_crop_1)\n            self.assertAllClose((2, 10, 10, 8), out.shape)\n            self.assertAllClose((2, 1, 1, 8), out_1.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/deconvolution_test.py,5,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.layer.deconvolution import DeconvLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass DeconvTest(NiftyNetTestCase):\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x_2d = tf.ones(input_shape)\n        return x_2d\n\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x_3d = tf.ones(input_shape)\n        return x_3d\n\n    def _test_deconv_output_shape(self,\n                                  rank,\n                                  param_dict,\n                                  output_shape):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        deconv_layer = DeconvLayer(**param_dict)\n        output_data = deconv_layer(input_data)\n        print(deconv_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_value = sess.run(output_data)\n            self.assertAllClose(output_shape, output_value.shape)\n\n    def _test_deconv_layer_output_shape(self,\n                                        rank,\n                                        param_dict,\n                                        output_shape,\n                                        is_training=None,\n                                        dropout_prob=None):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        deconv_layer = DeconvolutionalLayer(**param_dict)\n        output_data = deconv_layer(input_data,\n                                   is_training=is_training,\n                                   keep_prob=dropout_prob)\n        print(deconv_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_value = sess.run(output_data)\n            self.assertAllClose(output_shape, output_value.shape)\n\n    def test_3d_deconv_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 3, 1],\n                       \'stride\': 2}\n        self._test_deconv_output_shape(rank=3,\n                                       param_dict=input_param,\n                                       output_shape=(2, 32, 32, 32, 10))\n\n    def test_3d_deconv_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 2,\n                       \'with_bias\': True}\n        self._test_deconv_output_shape(rank=3,\n                                       param_dict=input_param,\n                                       output_shape=(2, 32, 32, 32, 10))\n\n    def test_deconv_3d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': [2, 2, 1],\n                       \'with_bias\': True,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_output_shape(rank=3,\n                                       param_dict=input_param,\n                                       output_shape=(2, 32, 32, 16, 10))\n\n    def test_3d_deconvlayer_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1}\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 16, 10),\n                                             is_training=True)\n\n    def test_3d_deconvlayer_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 2,\n                       \'with_bias\': True,\n                       \'feature_normalization\': None}\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 32, 32, 32, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 32, 32, 32, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_3d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1,\n                       \'with_bias\': True,\n                       \'feature_normalization\': None,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 16, 10))\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 16, 10))\n\n    def test_deconvlayer_3d_bn_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': 3,\n                       \'stride\': 1,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 16, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 16, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_3d_bn_reg_prelu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 5, 2],\n                       \'stride\': [1, 1, 2],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'acti_func\': \'prelu\'}\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 32, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 32, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_3d_relu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 5, 2],\n                       \'stride\': [1, 1, 2],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'relu\'}\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 32, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 16, 32, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_3d_bn_reg_dropout_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 5, 2],\n                       \'stride\': [1, 2, 2],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 32, 32, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 32, 32, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_3d_bn_reg_dropout_valid_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 5, 2],\n                       \'stride\': [1, 2, 1],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 32, 16, 10),\n                                             is_training=True,\n                                             dropout_prob=0.4)\n        self._test_deconv_layer_output_shape(rank=3,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 32, 16, 10),\n                                             is_training=False,\n                                             dropout_prob=1.0)\n\n    def test_2d_deconv_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1],\n                       \'stride\': [2, 1]}\n        self._test_deconv_output_shape(rank=2,\n                                       param_dict=input_param,\n                                       output_shape=(2, 32, 16, 10))\n\n    def test_2d_deconv_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1],\n                       \'stride\': [2, 1],\n                       \'with_bias\': True}\n        self._test_deconv_output_shape(rank=2,\n                                       param_dict=input_param,\n                                       output_shape=(2, 32, 16, 10))\n\n    def test_deconv_2d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1],\n                       \'stride\': [2, 1],\n                       \'with_bias\': True,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_output_shape(rank=2,\n                                       param_dict=input_param,\n                                       output_shape=(2, 32, 16, 10))\n\n    def test_2d_deconvlayer_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1],\n                       \'stride\': [2, 1]}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 32, 16, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 32, 16, 10),\n                                             is_training=False)\n\n    def test_2d_deconvlayer_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1],\n                       \'stride\': [2, 1],\n                       \'with_bias\': True,\n                       \'feature_normalization\': None}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 32, 16, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 32, 16, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_2d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1],\n                       \'stride\': [2, 3],\n                       \'with_bias\': True,\n                       \'feature_normalization\': None,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 32, 48, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 32, 48, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_2d_bn_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [3, 1],\n                       \'stride\': [1, 3],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 48, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 48, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_2d_bn_reg_prelu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [4, 1],\n                       \'stride\': [1, 3],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 48, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 48, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_2d_relu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [4, 1],\n                       \'stride\': [1, 3],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'relu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 48, 10),\n                                             is_training=True)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 48, 10),\n                                             is_training=False)\n\n    def test_deconvlayer_2d_bn_reg_dropout_prelu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [4, 1],\n                       \'stride\': [1, 3],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 48, 10),\n                                             is_training=True,\n                                             dropout_prob=0.4)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 16, 48, 10),\n                                             is_training=False,\n                                             dropout_prob=1.0)\n\n    def test_deconvlayer_2d_bn_reg_valid_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [4, 3],\n                       \'stride\': [1, 2],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'padding\': \'VALID\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 19, 33, 10),\n                                             is_training=True,\n                                             dropout_prob=0.4)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 19, 33, 10),\n                                             is_training=False,\n                                             dropout_prob=1.0)\n\n    def test_deconvlayer_2d_group_reg_valid_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'kernel_size\': [4, 3],\n                       \'stride\': [1, 2],\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'group\',\n                       \'group_size\': 5,\n                       \'acti_func\': \'prelu\',\n                       \'padding\': \'VALID\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 19, 33, 10),\n                                             is_training=True,\n                                             dropout_prob=0.4)\n        self._test_deconv_layer_output_shape(rank=2,\n                                             param_dict=input_param,\n                                             output_shape=(2, 19, 33, 10),\n                                             is_training=False,\n                                             dropout_prob=1.0)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/deepmedic_test.py,11,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.deepmedic import DeepMedic\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass DeepMedicTest(NiftyNetTestCase):\n    def test_3d_reg_shape(self):\n        input_shape = (2, 57, 57, 57, 1)\n        x = tf.ones(input_shape)\n\n        deepmedic_instance = DeepMedic(\n            num_classes=160,\n            w_regularizer=regularizers.l2_regularizer(0.5),\n            b_regularizer=regularizers.l2_regularizer(0.5))\n        out = deepmedic_instance(x, is_training=True)\n        # print(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 9, 9, 9, 160), out.shape)\n\n    def test_2d_reg_shape(self):\n        input_shape = (2, 57, 57, 1)\n        x = tf.ones(input_shape)\n\n        deepmedic_instance = DeepMedic(\n            num_classes=160,\n            w_regularizer=regularizers.l2_regularizer(0.5),\n            b_regularizer=regularizers.l2_regularizer(0.5))\n        out = deepmedic_instance(x, is_training=True)\n        # print(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 9, 9, 160), out.shape)\n\n    def test_3d_shape(self):\n        input_shape = (2, 57, 57, 57, 1)\n        x = tf.ones(input_shape)\n\n        deepmedic_instance = DeepMedic(num_classes=160)\n        out = deepmedic_instance(x, is_training=True)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 9, 9, 9, 160), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 57, 57, 1)\n        x = tf.ones(input_shape)\n\n        deepmedic_instance = DeepMedic(num_classes=160)\n        out = deepmedic_instance(x, is_training=True)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 9, 9, 160), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/dense_vnet_test.py,7,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.dense_vnet import DenseVNet\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass DenseVNetTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 72, 72, 72, 3)\n        x = tf.ones(input_shape)\n\n        dense_vnet_instance = DenseVNet(\n            num_classes=2)\n        out = dense_vnet_instance(x, is_training=True)\n        # print(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 72, 72, 72, 2), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 72, 72, 3)\n        x = tf.ones(input_shape)\n\n        dense_vnet_instance = DenseVNet(\n            num_classes=2)\n        out = dense_vnet_instance(x, is_training=True)\n        # print(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 72, 72, 2), out.shape)\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/dilatedcontext_test.py,3,"b'from __future__ import absolute_import, print_function\nimport tensorflow as tf\n\nfrom niftynet.layer.dilatedcontext import DilatedTensor\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass BNTest(NiftyNetTestCase):\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def test_2d_dilating_shape(self):\n        x = self.get_2d_input()\n        with DilatedTensor(x, 4) as dilated:\n            intermediate = dilated.tensor\n        x = dilated.tensor\n\n        with self.cached_session() as sess:\n            out = sess.run(x)\n            out_dilated = sess.run(intermediate)\n            self.assertAllClose((2, 16, 16, 8), out.shape)\n            self.assertAllClose((32, 4, 4, 8), out_dilated.shape)\n\n    def test_3d_dilating_shape(self):\n        x = self.get_3d_input()\n        with DilatedTensor(x, 4) as dilated:\n            intermediate = dilated.tensor\n        x = dilated.tensor\n\n        with self.cached_session() as sess:\n            out = sess.run(x)\n            out_dilated = sess.run(intermediate)\n            self.assertAllClose((2, 16, 16, 16, 8), out.shape)\n            self.assertAllClose((128, 4, 4, 4, 8), out_dilated.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/download_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.utilities.download import download\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMODEL_HOME = NiftyNetGlobalConfig().get_niftynet_home_folder()\n\nTEST_CASE_1 = \'dense_vnet_abdominal_ct_model_zoo\'\nTEST_CASE_1_TARGET = os.path.join(\n    MODEL_HOME, \'models\', \'dense_vnet_abdominal_ct\')\nTEST_CASE_2 = \'default\'\nTEST_CASE_2_TARGET = os.path.join(MODEL_HOME, \'examples\', TEST_CASE_2)\nTEST_CASE_3 = \'default_multimodal\'\nTEST_CASE_3_TARGET = os.path.join(MODEL_HOME, \'examples\', TEST_CASE_3)\n\nTEST_WRONG_ID = \'42\'\n\n\nclass NetDownloadTest(NiftyNetTestCase):\n    def test_download(self):\n        self.assertTrue(download(TEST_CASE_1, False))\n        self.assertTrue(os.path.isdir(TEST_CASE_1_TARGET))\n\n        if os.path.isdir(TEST_CASE_1_TARGET):\n            print(\'skipping tests: %s folder exists\' % TEST_CASE_1_TARGET)\n        else:\n            self.assertTrue(download(TEST_CASE_1, True))\n            self.assertTrue(os.path.isdir(TEST_CASE_1_TARGET))\n\n    def test_wrong_ids(self):\n        self.assertFalse(download([], False))\n        self.assertFalse(download((), False))\n        self.assertFalse(download(None, False))\n        self.assertFalse(download([], True))\n        self.assertFalse(download((), True))\n        self.assertFalse(download(None, True))\n        self.assertFalse(download(TEST_WRONG_ID, True))\n        self.assertFalse(download(TEST_WRONG_ID, False))\n\n    def test_multiple_ids(self):\n        self.assertTrue(\n            download([TEST_CASE_1, TEST_CASE_2, TEST_CASE_3], False))\n        self.assertTrue(os.path.isdir(TEST_CASE_1_TARGET))\n        self.assertTrue(os.path.isdir(TEST_CASE_2_TARGET))\n        self.assertTrue(os.path.isdir(TEST_CASE_3_TARGET))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/downsample_res_block_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.downsample_res_block import DownBlock\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass DownsampleResBlockTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def _test_nd_output_shape(self,\n                              rank,\n                              param_dict,\n                              output_shape):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        downsample_layer = DownBlock(**param_dict)\n        output_data = downsample_layer(input_data)\n        print(downsample_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(output_data)\n            self.assertAllClose(output_shape, out[0].shape)\n\n    def test_3d_shape(self):\n        expected_shape = (2, 8, 8, 8, 4)\n        self._test_nd_output_shape(3, {}, expected_shape)\n\n        params = {\'n_output_chns\': 16, \'kernel_size\': 5,\n                  \'downsample_kernel_size\': 3, \'downsample_stride\': 2,\n                  \'acti_func\': \'relu\'}\n        expected_shape = (2, 8, 8, 8, 16)\n        self._test_nd_output_shape(3, params, expected_shape)\n\n        params = {\'n_output_chns\': 16, \'kernel_size\': 3,\n                  \'downsample_kernel_size\': 4, \'downsample_stride\': 3,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 6, 6, 6, 16)\n        self._test_nd_output_shape(3, params, expected_shape)\n\n    def test_2d_shape(self):\n        expected_shape = (2, 8, 8, 4)\n        self._test_nd_output_shape(2, {}, expected_shape)\n\n        params = {\'n_output_chns\': 16, \'kernel_size\': 5,\n                  \'downsample_kernel_size\': 3, \'downsample_stride\': 2,\n                  \'acti_func\': \'relu\'}\n        expected_shape = (2, 8, 8, 16)\n        self._test_nd_output_shape(2, params, expected_shape)\n\n        params = {\'n_output_chns\': 16, \'kernel_size\': 3,\n                  \'downsample_kernel_size\': 4, \'downsample_stride\': 3,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 6, 6, 16)\n        self._test_nd_output_shape(2, params, expected_shape)\n\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/downsample_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.downsample import DownSampleLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass DownSampleTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def _test_nd_downsample_output_shape(self,\n                                         rank,\n                                         param_dict,\n                                         output_shape):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n        downsample_layer = DownSampleLayer(**param_dict)\n        output_data = downsample_layer(input_data)\n        print(downsample_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(output_data)\n            self.assertAllClose(output_shape, out.shape)\n\n    def test_3d_max_shape(self):\n        input_param = {\'func\': \'MAX\',\n                       \'kernel_size\': 3,\n                       \'stride\': 3}\n\n        self._test_nd_downsample_output_shape(rank=3,\n                                              param_dict=input_param,\n                                              output_shape=(2, 6, 6, 6, 8))\n\n    def test_3d_avg_shape(self):\n        input_param = {\'func\': \'AVG\',\n                       \'kernel_size\': [3, 3, 2],\n                       \'stride\': [3, 2, 1]}\n        self._test_nd_downsample_output_shape(rank=3,\n                                              param_dict=input_param,\n                                              output_shape=(2, 6, 8, 16, 8))\n\n    def test_3d_const_shape(self):\n        input_param = {\'func\': \'CONSTANT\',\n                       \'kernel_size\': [1, 3, 2],\n                       \'stride\': [3, 2, 2]}\n        self._test_nd_downsample_output_shape(rank=3,\n                                              param_dict=input_param,\n                                              output_shape=(2, 6, 8, 8, 8))\n\n    def test_2d_max_shape(self):\n        input_param = {\'func\': \'CONSTANT\',\n                       \'kernel_size\': [1, 3],\n                       \'stride\': 3}\n        self._test_nd_downsample_output_shape(rank=2,\n                                              param_dict=input_param,\n                                              output_shape=(2, 6, 6, 8))\n\n    def test_2d_avg_shape(self):\n        input_param = {\'func\': \'AVG\',\n                       \'kernel_size\': [2, 3],\n                       \'stride\': 2}\n        self._test_nd_downsample_output_shape(rank=2,\n                                              param_dict=input_param,\n                                              output_shape=(2, 8, 8, 8))\n\n    def test_2d_const_shape(self):\n        input_param = {\'func\': \'CONSTANT\',\n                       \'kernel_size\': [2, 3],\n                       \'stride\': [2, 3]}\n        self._test_nd_downsample_output_shape(rank=2,\n                                              param_dict=input_param,\n                                              output_shape=(2, 8, 6, 8))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/driver_partitioner_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_driver import ApplicationDriver\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nTARGET_FILE = os.path.join(\'testing_data\', \'test_splitting.csv\')\n\n\ndef _generate_base_params():\n    # initialise compulsory params that are irrelevant\n    # to this unit test\n    user_param = dict()\n    user_param[\'SYSTEM\'] = ParserNamespace(\n        model_dir=\'./testing_data\',\n        num_threads=2,\n        num_gpus=1,\n        cuda_devices=\'\',\n        event_handler=None,\n        iteration_generator=None)\n\n    user_param[\'NETWORK\'] = ParserNamespace(\n        batch_size=20,\n        name=\'tests.toy_application.TinyNet\')\n\n    user_param[\'TRAINING\'] = ParserNamespace(\n        starting_iter=0,\n        max_iter=2,\n        save_every_n=2,\n        tensorboard_every_n=0,\n        max_checkpoints=100)\n\n    user_param[\'INFERENCE\'] = ParserNamespace(\n        inference_iter=-1)\n\n    user_param[\'CUSTOM\'] = ParserNamespace(\n        name=\'tests.toy_application.ToyApplication\',\n        vector_size=100,\n        mean=10.0,\n        stddev=2.0)\n    return user_param\n\n\ndef _generate_data_param():\n    user_param = dict()\n    user_param[\'modality\'] = ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'mod1test.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=\'nii\')\n\n    user_param[\'modality2\'] = ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'mod2test.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=\'nii\')\n    return user_param\n\n\ndef generate_input_params(**arg_dicts):\n    user_param = _generate_base_params()\n    for key in list(arg_dicts):\n        if not arg_dicts[key]:\n            continue\n        user_param[key].update(**arg_dicts[key])\n    return user_param\n\n\ndef clear_target():\n    if not os.path.isfile(TARGET_FILE):\n        return\n    os.remove(TARGET_FILE)\n\n\ndef write_target():\n    clear_target()\n    user_param = generate_input_params(\n        SYSTEM={\'action\': \'train\',\n                \'dataset_split_file\': TARGET_FILE\n                },\n        TRAINING={\'validation_every_n\': 2,\n                  \'validation_max_iter\': 1,\n                  \'exclude_fraction_for_validation\': 0.1,\n                  \'exclude_fraction_for_inference\': 0.1,\n                  }\n    )\n    data_param = _generate_data_param()\n    app_driver = ApplicationDriver()\n    app_driver.initialise_application(user_param, data_param)\n    assert os.path.isfile(TARGET_FILE)\n    return\n\n\nclass DriverPartitionerTestExistingFile(NiftyNetTestCase):\n    def test_training(self):\n        write_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'train\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': 2,\n                      \'validation_max_iter\': 1,\n                      \'exclude_fraction_for_validation\': 0.1,\n                      \'exclude_fraction_for_inference\': 0.1,\n                      }\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n        partitioner = app_driver.data_partitioner\n        self.assertTrue(partitioner.has_training)\n        self.assertTrue(partitioner.has_inference)\n        self.assertTrue(partitioner.has_validation)\n\n    def test_training_no_validation(self):\n        write_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'train\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': -1,\n                      \'validation_max_iter\': 1,\n                      \'exclude_fraction_for_validation\': 0.0,\n                      \'exclude_fraction_for_inference\': 0.0,\n                      }\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n        partitioner = app_driver.data_partitioner\n        self.assertTrue(partitioner.has_training)\n        self.assertTrue(partitioner.has_inference)\n        self.assertTrue(partitioner.has_validation)\n\n    def test_inference_no_validation(self):\n        write_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'inference\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': -1,\n                      \'validation_max_iter\': 1,\n                      \'exclude_fraction_for_validation\': 0.0,\n                      \'exclude_fraction_for_inference\': 0.0,\n                      }\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n        partitioner = app_driver.data_partitioner\n        self.assertTrue(partitioner.has_training)\n        self.assertTrue(partitioner.has_inference)\n        self.assertTrue(partitioner.has_validation)\n\n    def test_inference_validation(self):\n        write_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'inference\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': 10,\n                      \'validation_max_iter\': 1,\n                      \'exclude_fraction_for_validation\': 0.0,\n                      \'exclude_fraction_for_inference\': 0.0,\n                      }\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n        partitioner = app_driver.data_partitioner\n        self.assertTrue(partitioner.has_training)\n        self.assertTrue(partitioner.has_inference)\n        self.assertTrue(partitioner.has_validation)\n\n\nclass DriverPartitionerTestNoFile(NiftyNetTestCase):\n    def test_training(self):\n        clear_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'train\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': 2,\n                      \'validation_max_iter\': 1,\n                      \'exclude_fraction_for_validation\': 0.1,\n                      \'exclude_fraction_for_inference\': 0.1,\n                      }\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n        partitioner = app_driver.data_partitioner\n        self.assertTrue(partitioner.has_training)\n        self.assertTrue(partitioner.has_inference)\n        self.assertTrue(partitioner.has_validation)\n\n    def test_training_no_validation(self):\n        clear_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'train\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': -1,\n                      \'validation_max_iter\': 1,\n                      \'exclude_fraction_for_validation\': 0.0,\n                      \'exclude_fraction_for_inference\': 0.0,\n                      }\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n        partitioner = app_driver.data_partitioner\n        self.assertFalse(partitioner.has_training)\n        self.assertFalse(partitioner.has_inference)\n        self.assertFalse(partitioner.has_validation)\n        self.assertTrue(partitioner.all_files is not None)\n\n        clear_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'train\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': -1,\n                      \'validation_max_iter\': 1,\n                      \'exclude_fraction_for_validation\': 0.0,\n                      \'exclude_fraction_for_inference\': 0.0,\n                      }\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n        partitioner = app_driver.data_partitioner\n        self.assertFalse(partitioner.has_training)\n        self.assertFalse(partitioner.has_inference)\n        self.assertFalse(partitioner.has_validation)\n        self.assertTrue(partitioner.all_files is not None)\n\n    def test_inference(self):\n        clear_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'inference\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': 1,\n                      \'validation_max_iter\': 1,\n                      \'exclude_fraction_for_validation\': 0.1,\n                      \'exclude_fraction_for_inference\': 0.0,\n                      }\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n\n        self.assertTrue(app_driver.data_partitioner is not None)\n        self.assertFalse(os.path.isfile(TARGET_FILE))\n        partitioner = app_driver.data_partitioner\n        self.assertTrue(partitioner._partition_ids is None)\n\n    def test_inference_no_validation(self):\n        clear_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'inference\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n        )\n        data_param = _generate_data_param()\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, data_param)\n\n        self.assertTrue(app_driver.data_partitioner is not None)\n        self.assertFalse(os.path.isfile(TARGET_FILE))\n        partitioner = app_driver.data_partitioner\n        self.assertTrue(partitioner._partition_ids is None)\n\n\nclass DriverPartitionerTestNoData(NiftyNetTestCase):\n    def test_no_data_param_infer(self):\n        clear_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'inference\',\n                    \'dataset_split_file\': TARGET_FILE\n                    }\n        )\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, {})\n        self.assertTrue(app_driver.data_partitioner is not None)\n        self.assertFalse(os.path.isfile(TARGET_FILE))\n\n        partitioner = app_driver.data_partitioner\n        self.assertFalse(partitioner.all_files)\n\n    def test_no_data_param_train(self):\n        clear_target()\n        user_param = generate_input_params(\n            SYSTEM={\'action\': \'train\',\n                    \'dataset_split_file\': TARGET_FILE\n                    },\n            TRAINING={\'validation_every_n\': -1,\n                      \'exclude_fraction_for_validation\': 0.1,\n                      \'exclude_fraction_for_inference\': 0.1,\n                      }\n        )\n        app_driver = ApplicationDriver()\n        app_driver.initialise_application(user_param, {})\n        self.assertTrue(app_driver.data_partitioner is not None)\n        self.assertFalse(os.path.isfile(TARGET_FILE))\n\n        partitioner = app_driver.data_partitioner\n        self.assertFalse(partitioner.all_files)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/elementwise_test.py,20,"b'from __future__ import absolute_import, print_function\nimport tensorflow as tf\n\nfrom niftynet.layer.elementwise import ElementwiseLayer\n\n\nclass ElementwiseTest(tf.test.TestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 16, 16, 16, 6)\n        x_1 = tf.ones(input_shape)\n        input_shape = (2, 16, 16, 16, 6)\n        x_2 = tf.zeros(input_shape)\n        sum_layer = ElementwiseLayer(\'SUM\')\n        out_sum_1 = sum_layer(x_1, x_2)\n\n        input_shape = (2, 16, 16, 16, 8)\n        x_1 = tf.ones(input_shape)\n        input_shape = (2, 16, 16, 16, 6)\n        x_2 = tf.zeros(input_shape)\n        sum_layer = ElementwiseLayer(\'SUM\')\n        out_sum_2 = sum_layer(x_1, x_2)\n\n        input_shape = (2, 16, 16, 16, 6)\n        x_1 = tf.ones(input_shape)\n        input_shape = (2, 16, 16, 16, 8)\n        x_2 = tf.zeros(input_shape)\n        sum_layer = ElementwiseLayer(\'SUM\')\n        out_sum_3 = sum_layer(x_1, x_2)\n\n        input_shape = (2, 16, 16, 16, 6)\n        x_1 = tf.ones(input_shape)\n        input_shape = (2, 16, 16, 16, 8)\n        x_2 = tf.zeros(input_shape)\n        sum_layer = ElementwiseLayer(\'CONCAT\')\n        out_sum_4 = sum_layer(x_1, x_2)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_sum_1)\n            self.assertAllClose((2, 16, 16, 16, 6), out.shape)\n            out = sess.run(out_sum_2)\n            self.assertAllClose((2, 16, 16, 16, 8), out.shape)\n            out = sess.run(out_sum_3)\n            self.assertAllClose((2, 16, 16, 16, 6), out.shape)\n            out = sess.run(out_sum_4)\n            self.assertAllClose((2, 16, 16, 16, 14), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 16, 16, 6)\n        x_1 = tf.ones(input_shape)\n        input_shape = (2, 16, 16, 6)\n        x_2 = tf.zeros(input_shape)\n        sum_layer = ElementwiseLayer(\'SUM\')\n        out_sum_1 = sum_layer(x_1, x_2)\n\n        input_shape = (2, 16, 16, 8)\n        x_1 = tf.ones(input_shape)\n        input_shape = (2, 16, 16, 6)\n        x_2 = tf.zeros(input_shape)\n        sum_layer = ElementwiseLayer(\'SUM\')\n        out_sum_2 = sum_layer(x_1, x_2)\n\n        input_shape = (2, 16, 16, 6)\n        x_1 = tf.ones(input_shape)\n        input_shape = (2, 16, 16, 8)\n        x_2 = tf.zeros(input_shape)\n        sum_layer = ElementwiseLayer(\'SUM\')\n        out_sum_3 = sum_layer(x_1, x_2)\n\n        input_shape = (2, 16, 16, 6)\n        x_1 = tf.ones(input_shape)\n        input_shape = (2, 16, 16, 8)\n        x_2 = tf.zeros(input_shape)\n        sum_layer = ElementwiseLayer(\'CONCAT\')\n        out_sum_4 = sum_layer(x_1, x_2)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_sum_1)\n            self.assertAllClose((2, 16, 16, 6), out.shape)\n            out = sess.run(out_sum_2)\n            self.assertAllClose((2, 16, 16, 8), out.shape)\n            out = sess.run(out_sum_3)\n            self.assertAllClose((2, 16, 16, 6), out.shape)\n            out = sess.run(out_sum_4)\n            self.assertAllClose((2, 16, 16, 14), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/entry_point_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\n\nimport net_autoencoder\nimport net_gan\nimport net_regress\nimport net_run\nimport net_segment\n\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass EntryPointTest(NiftyNetTestCase):\n    def test_wrong_app(self):\n        sys.argv = [\'\', \'train\',\n                    \'-a\', \'foo\',\n                    \'-c\', os.path.join(\'config\', \'default_segmentation.ini\')]\n        with self.assertRaisesRegexp(ValueError, \'application\'):\n            net_run.main()\n\n        sys.argv = [\'\', \'train\',\n                    \'-c\', os.path.join(\'config\', \'default_segmentation.ini\')]\n        with self.assertRaisesRegexp(ValueError, \'application\'):\n            net_run.main()\n\n    def test_wrong_config(self):\n        sys.argv = [\'\', \'train\',\n                    \'-a\', \'net_segment\',\n                    \'-c\', os.path.join(\'foo\', \'default_segmentation.ini\')]\n        with self.assertRaisesRegexp(IOError, \'\'):\n            net_run.main()\n\n        sys.argv = [\'\', \'train\',\n                    \'-a\', \'net_segment\']\n        with self.assertRaisesRegexp(IOError, \'\'):\n            net_run.main()\n\n    def test_no_action(self):\n        sys.argv = [\'\',\n                    \'-a\', \'net_segment\',\n                    \'-c\', os.path.join(\'config\', \'default_segmentation.ini\')]\n        with self.assertRaisesRegexp(SystemExit, \'\'):\n            net_run.main()\n\n    def test_wrong_param(self):\n        sys.argv = [\'\',\n                    \'-a\', \'net_segment\',\n                    \'-c\', os.path.join(\'config\', \'default_segmentation.ini\'),\n                    \'--foo=bar\']\n        with self.assertRaisesRegexp(SystemExit, \'\'):\n            net_run.main()\n\n    def test_empty(self):\n        with self.assertRaisesRegexp(SystemExit, \'\'):\n            net_run.main()\n        with self.assertRaisesRegexp(SystemExit, \'\'):\n            net_gan.main()\n        with self.assertRaisesRegexp(SystemExit, \'\'):\n            net_segment.main()\n        with self.assertRaisesRegexp(SystemExit, \'\'):\n            net_regress.main()\n        with self.assertRaisesRegexp(SystemExit, \'\'):\n            net_autoencoder.main()\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/evaluation_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom argparse import Namespace\nimport tensorflow as tf\nimport numpy as np\n\nfrom niftynet.evaluation.pairwise_measures import PairwiseMeasures\nfrom niftynet.utilities.util_common import MorphologyOps\nfrom niftynet.evaluation.segmentation_evaluator import SegmentationEvaluator\nimport niftynet.evaluation.segmentation_evaluations as segmentation_evaluations\nimport niftynet.evaluation.regression_evaluations as regression_evaluations\nfrom niftynet.evaluation.classification_evaluator import ClassificationEvaluator\n\nfrom niftynet.evaluation.regression_evaluator import RegressionEvaluator\n\nTEST_CASES = {0: {\'seg_img\': np.array([1, 0, 0, 0]), \'ref_img\': np.array([1, 0, 0, 0])},\n              1: {\'seg_img\': np.array([1, 0, 1, 0]), \'ref_img\': np.array([1, 0, 0, 0])},\n              2: {\'seg_img\': np.array([3, 2, 0, 0]), \'ref_img\': np.array([1, 2, 0, 0])},\n              3: {\'seg_img\': np.array([1, 0, 0.5, 0]), \'ref_img\': np.array([1, 0, 0, 0])},\n              4: {\'seg_img\': np.reshape([1, 1, 1, 0, 0, 0, 0, 0],[2,2,2,1,1]),\n                  \'ref_img\': np.reshape([0, 0, 0, 0, 0, 0, 1, 1],[2,2,2,1,1])},\n\n              }\n\n\nclass BinaryCheckTest(np.testing.TestCase):\n    def test_binary_check_for_labels(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[2][\'seg_img\'],\n                                             ref_img=TEST_CASES[2][\'ref_img\'])\n        self.assertRaises(ValueError, pairwise_measures.check_binary)\n\n    def test_binary_check_for_probabilities(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[3][\'seg_img\'],\n                                             ref_img=TEST_CASES[3][\'ref_img\'])\n        self.assertRaises(ValueError, pairwise_measures.check_binary)\n\n\nclass PairwiseTests(np.testing.TestCase):\n    def test_dice_score(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[0][\'seg_img\'],\n                                             ref_img=TEST_CASES[0][\'ref_img\'])\n        self.assertEqual(pairwise_measures.dice_score(), 1.0)\n\n    def test_true_positive(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.tp(), 1.0)\n\n    def test_faulty_inputs(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[3][\'seg_img\'],\n                                             ref_img=TEST_CASES[3][\'ref_img\'])\n        self.assertRaises(ValueError, pairwise_measures.tp)\n\n    def test_true_negative(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.tn(), 2.)\n\n    def test_n_negative(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.n_neg_ref(), 3.)\n        self.assertEqual(pairwise_measures.n_neg_seg(), 2.)\n\n    def test_union(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.n_union(), 2.)\n\n    def test_intersection(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.n_intersection(), 1.)\n\n    def test_sensitivity(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.sensitivity(), 1.)\n\n    def test_specificity(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.specificity(), 2. / 3)\n\n    def test_accuracy(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.accuracy(), 3. / 4)\n\n    def test_false_positive_rate(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.false_positive_rate(), 1. / 3)\n\n    def test_positive_predictive_value(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.positive_predictive_values(), 1. / 2)\n\n    def test_negative_predictive_value(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.negative_predictive_values(), 1.)\n\n    def test_intersection_over_union(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.intersection_over_union(), 1. / 2)\n\n    def test_jaccard(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.jaccard(), 1. / 2)\n\n    def test_informedness(self):\n        # true positive rate - false positive rate\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertAlmostEqual(pairwise_measures.informedness(), 2. / 3)\n\n    def test_markedness(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'])\n        self.assertEqual(pairwise_measures.markedness(), 1. / 2)\n\n    def test_centre_of_mass(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'],\n                                             pixdim=[2])\n        self.assertListEqual(list(pairwise_measures.com_ref()), [0.0])\n        self.assertListEqual(list(pairwise_measures.com_seg()), [1.0])\n        self.assertEqual(pairwise_measures.com_dist(), 2.)\n\n    def test_vol_diff(self):\n        pairwise_measures = PairwiseMeasures(seg_img=TEST_CASES[1][\'seg_img\'],\n                                             ref_img=TEST_CASES[1][\'ref_img\'],\n                                             pixdim=[2])\n        self.assertEqual(pairwise_measures.vol_diff(), 1.)\n\nclass MorphologyTests(np.testing.TestCase):\n    def test_2d_offset(self):\n        test_img = np.concatenate([np.zeros([3, 3]), np.ones([3, 3])])\n        # expected border -- everywhere the test_img is 1, except the centre of it\n        expected_border = np.zeros([6, 3])\n        expected_border[3:][:] = 1\n        expected_border[4, 1] = 0\n        self.assertRaises(AssertionError, MorphologyOps, test_img, 8)\n        #calculated_border = MorphologyOps(test_img, 8).border_map()\n        #self.assertTrue(np.array_equal(calculated_border, expected_border))\n\n    def test_3d_offset(self):\n        test_img = np.zeros([10, 10, 10])\n        test_img[5, 5, 5] = 1\n        # border is the same as the test image -- just the one positive voxel\n        calculated_border = MorphologyOps(test_img, 8).border_map()\n        self.assertTrue(np.array_equal(test_img, calculated_border))\n\n    def test_1d_error(self):\n        test_img = np.zeros([1])\n        self.assertRaises(AssertionError, MorphologyOps, test_img, 8)\n        #self.assertRaises(ValueError, MorphologyOps(test_img, 8).border_map)\n\n\nclass RegressionEvaluationTests(np.testing.TestCase):\n    def build_data(self):\n        ref = np.reshape([1., .2, 2., 1., .9, .2, 3., 2.], [2, 2, 2, 1, 1])\n        out = np.reshape([1., .3, 2., 1., .9, .2, 3., 2.], [2, 2, 2, 1, 1])\n        return ref, out\n\n    def test_mse(self):\n        rd = regression_evaluations.mse(None, None, None).metric(\n            *self.build_data())\n        self.assertAlmostEqual(rd, 0.00125, 3)\n\n    def test_rmse(self):\n        rd = regression_evaluations.rmse(None, None, None).metric(\n            *self.build_data())\n        self.assertAlmostEqual(rd, 0.03535, 3)\n\n\n    def test_mae(self):\n        rd = regression_evaluations.mae(None, None, None).metric(\n            *self.build_data())\n        self.assertAlmostEqual(rd, 0.0125,  3)\n\n\nclass SegmentationEvaluationTests(np.testing.TestCase):\n    def metric(self, cls, case):\n        return cls(None, None, None).metric_from_binarized(\n                                  seg=TEST_CASES[case][\'seg_img\'],\n                                  ref=TEST_CASES[case][\'ref_img\'])\n\n    def test_average_distance(self):\n        self.assertAlmostEqual(self.metric(\n            segmentation_evaluations.average_distance, 4), 1.2485,3)\n\n    def test_hausdorff_distance(self):\n        self.assertAlmostEqual(self.metric(\n            segmentation_evaluations.hausdorff_distance, 4), 1.414,3)\n\n    def test_hausdorff95_distance(self):\n        self.assertAlmostEqual(self.metric(\n            segmentation_evaluations.hausdorff95_distance, 4), 1.414,3)\n\n    def test_dice_score(self):\n        self.assertEqual(self.metric(segmentation_evaluations.dice, 0), 1.0)\n\n    def test_true_positive(self):\n        self.assertEqual(self.metric(segmentation_evaluations.tp, 1), 1.0)\n\n    def test_true_negative(self):\n        self.assertEqual(self.metric(segmentation_evaluations.tn, 1), 2.)\n\n    def test_n_negative(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.n_neg_ref, 1), 3.0)\n        self.assertEqual(self.metric(\n            segmentation_evaluations.n_neg_seg, 1), 2.0)\n\n    def test_union(self):\n        self.assertEqual(self.metric(segmentation_evaluations.n_union, 1), 2.0)\n\n    def test_intersection(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.n_intersection,1), 1.0)\n\n    def test_sensitivity(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.sensitivity, 1), 1.0)\n\n    def test_specificity(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.specificity, 1), 2. / 3)\n\n    def test_accuracy(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.accuracy, 1), 3. / 4)\n\n    def test_false_positive_rate(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.false_positive_rate, 1), 1. / 3)\n\n    def test_positive_predictive_value(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.positive_predictive_values, 1), 1. / 2)\n\n    def test_negative_predictive_value(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.negative_predictive_values, 1), 1.0)\n\n    def test_intersection_over_union(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.intersection_over_union, 1), 1. / 2)\n\n    def test_jaccard(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.jaccard, 1), 1. / 2)\n\n    def test_informedness(self):\n        self.assertAlmostEqual(self.metric(\n            segmentation_evaluations.informedness, 1), 2. / 3)\n\n    def test_markedness(self):\n        self.assertEqual(self.metric(\n            segmentation_evaluations.markedness,1), 1. /2)\n\nclass ClassificationEvaluationTests(np.testing.TestCase):\n    def data1(self):\n        raw_data = [[0,.12],[0,.24],[1,.36], [0,.45],\n                    [0,.61],[1,.28],[1,.99], [1,.89]]\n        formatted_data = [{\'label\':np.reshape(datum[0],[1,1,1,1,1]),\n                           \'inferred\':np.reshape([1-datum[1],datum[1]],[1,1,1,1,2])} for datum in raw_data]\n        return formatted_data\n    def data2(self):\n        raw_data = [[0,0],[0,0],[1,0],[0,0],\n                    [0,1],[1,0],[1,1],[1,1]]\n        formatted_data = [{\'label\':np.reshape(datum[0],[1,1,1,1,1]),\n                           \'inferred\':np.reshape([datum[1]],[1,1,1,1,1])} for datum in raw_data]\n        return formatted_data\n\n    def generator(self, data):\n        interp_orders = {\'label\':0,\'inferred\':-1}\n        for idx, datum in enumerate(data):\n            yield (\'test\'+str(idx), datum,interp_orders)\n\n    def evaluator(self, eval_str, output_prob=True):\n        class NS(object):\n            def __init__(self, dict):\n                self.__dict__.update(dict)\n        classification_param=NS({\'num_classes\':2,\n                                 \'output_prob\':output_prob})\n        eval_param=NS({\'evaluations\':eval_str})\n        return ClassificationEvaluator(None, classification_param, eval_param)\n\n    def test_accuracy_output_prob(self):\n        data = self.data1()\n        evl = self.evaluator(\'niftynet.evaluation.classification_evaluations.accuracy\')\n        result_dict = evl.evaluate_from_generator(self.generator(data))\n        self.assertIn((None,), result_dict)\n        by_threshold = result_dict[(None,)].to_dict(\'index\')\n        \n        self.assertEqual(by_threshold,\n                      {0: {\'accuracy\': 0.625}})\n\n    def test_accuracy_output_label(self):\n        data = self.data2()\n        evl = self.evaluator(\'niftynet.evaluation.classification_evaluations.accuracy\', False)\n        result_dict = evl.evaluate_from_generator(self.generator(data))\n        self.assertIn((None,), result_dict)\n        by_threshold = result_dict[(None,)].to_dict(\'index\')\n        \n        self.assertEqual(by_threshold,\n                      {0: {\'accuracy\': 0.625}})\n\n    def test_contrib_roc(self):\n        data = self.data1()\n        evl = self.evaluator(\'niftynet.contrib.evaluation.classification_evaluations.roc\')\n        result_dict = evl.evaluate_from_generator(self.generator(data))\n        self.assertIn((\'thresholds\',), result_dict)\n        by_threshold = result_dict[(\'thresholds\',)].to_dict(\'index\')\n        get_key = lambda x: [k for k in by_threshold.keys() if np.abs(k-x)<.01][0]\n        sample = by_threshold[get_key(0.444)]\n        self.assertEqual(sample[\'fp\'],2)\n        self.assertEqual(sample[\'spec\'],0.5)\n        self.assertEqual(sample[\'sens\'],0.5)\n        \n#  FPF:   0.0000  0.0000  0.3333  0.3333  1.0000\n#   TPF:   0.0000  0.6667  0.6667  1.0000  1.0000\n#\n#AREA UNDER ROC CURVE:\n#  Area under fitted curve (Az) = 0.9043\n#          Estimated std. error = 0.1260\n#  Trapezoidal (Wilcoxon) area = 0.8889\n\n    def test_contrib_roc_auc(self):\n        data = self.data1()\n        evl = self.evaluator(\'niftynet.contrib.evaluation.classification_evaluations.roc_auc\')\n        result_dict = evl.evaluate_from_generator(self.generator(data))\n        self.assertIn((None,), result_dict)\n        print(result_dict[(None,)].to_dict(\'index\'))\n        self.assertEqual(result_dict[(None,)].to_dict(\'index\'),\n                      {0: {\'roc_auc\': 0.71875}})\n\n\nclass SegmentationEvaluatorTests(np.testing.TestCase):\n    """"""\n    Tests that evaluator - evaluations integration works\n    """"""\n\n    class ReaderStub():\n        def __init__(self):\n            self.count = 0\n            sz=[2,2,2,1,1]\n            self.data=((0,\n                        {\'label\': np.reshape([1, 0, 0, 0, 1, 0, 0, 0], sz),\n                         \'inferred\': np.reshape([1, 0, 0, 0, 1, 0, 0, 0], sz)},\n                        None),\n                       (1,\n                        {\'label\': np.reshape([1, 1, 0, 0, 1, 0, 0, 0], sz),\n                         \'inferred\': np.reshape([1, 0, 0, 0, 1, 0, 0, 0], sz)},\n                        None),\n                       (-1, None, None))\n\n        def __call__(self, shuffle):\n            return_value = self.data[self.count]\n            self.count += 1\n            return return_value\n\n        def get_subject_id(self, image_id):\n            return [\'foo\',\'bar\'][image_id]\n\n    def test_segmentation_evaluator(self):\n        app_param = Namespace(evaluation_units=\'label,cc\',output_prob=False,\n                              num_classes=2)\n        eval_param = Namespace(evaluations=\'Dice,Jaccard,average_distance\')\n        evalu = SegmentationEvaluator(SegmentationEvaluatorTests.ReaderStub(),\n                                      app_param, eval_param)\n        result_dict = evalu.evaluate()\n        self.assertIn((\'subject_id\', \'cc_id\'), result_dict)\n        self.assertIn((\'subject_id\', \'label\'), result_dict)\n\n        group_cc = result_dict[(\'subject_id\', \'cc_id\')]\n        group_l = result_dict[(\'subject_id\', \'label\')]\n\n        self.assertIn(\'jaccard\', list(group_l.columns))\n        self.assertIn(\'dice\', list(group_l.columns))\n        self.assertIn(\'jaccard\', list(group_cc.columns))\n        self.assertIn(\'dice\', list(group_cc.columns))\n        self.assertIn(\'average_distance\', list(group_cc.columns))\n\n        self.assertIn((\'foo\',\'r1_s1\'), list(group_cc.index))\n        self.assertIn((\'bar\',\'r1_s1\'), list(group_cc.index))\n        self.assertIn((\'foo\',1), list(group_l.index))\n        self.assertIn((\'bar\',1), list(group_l.index))\n\nclass RegressionEvaluatorTests(np.testing.TestCase):\n    """"""\n    Tests that evaluator - evaluations integration works\n    """"""\n\n    class ReaderStub():\n        def __init__(self):\n            self.count = 0\n            sz = [2, 2, 2, 1, 1]\n            self.data = ((0,\n                          {\'output\': np.reshape([1, 0, 0, 0, 1, 0, 0, 0],\n                                               sz),\n                           \'inferred\': np.reshape([1, 0, 0, 0, 1, 0, 0, 0],\n                                                  sz)},\n                          None),\n                         (1,\n                          {\'output\': np.reshape([1, 1, 0, 0, 1, 0, 0, 0],\n                                               sz),\n                           \'inferred\': np.reshape([1, 0, 0, 0, 1, 0, 0, 0],\n                                                  sz)},\n                          None),\n                         (-1, None, None))\n\n        def __call__(self, shuffle):\n            return_value = self.data[self.count]\n            self.count += 1\n            return return_value\n\n        def get_subject_id(self, image_id):\n            return [\'foo\', \'bar\'][image_id]\n\n    def test_regression_evaluator(self):\n        app_param = Namespace()\n        eval_param = Namespace(evaluations=\'rmse,mse\')\n        evalu = RegressionEvaluator(RegressionEvaluatorTests.ReaderStub(),\n                                    app_param, eval_param)\n        result_dict = evalu.evaluate()\n        self.assertIn((\'subject_id\',), result_dict)\n\n        group = result_dict[(\'subject_id\',)]\n        self.assertEqual((\'subject_id\',), group.index.names)\n        self.assertIn(\'mse\', list(group.columns))\n        self.assertIn(\'rmse\', list(group.columns))\n        self.assertIn(\'foo\', list(group.index))\n        self.assertIn(\'bar\', list(group.index))\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
tests/filename_matching_test.py,1,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.utilities.filename_matching import KeywordsMatching\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass FileNameMatchingTest(NiftyNetTestCase):\n    def test_default(self):\n        matcher = KeywordsMatching()\n        with self.assertRaisesRegexp(ValueError, """"):\n            matcher.matching_subjects_and_filenames()\n        with self.assertRaisesRegexp(AttributeError, """"):\n            KeywordsMatching.from_dict(\'wrong_argument\')\n\n    def test_from_dict(self):\n        with self.assertRaisesRegexp(ValueError, """"):\n            KeywordsMatching.from_dict({\'path_to_search\': \'wrong_folder\'})\n        matcher = KeywordsMatching.from_dict(\n            {\'path_to_search\': \'testing_data/images2d\'})\n        f_list, s_list = matcher.matching_subjects_and_filenames()\n        self.assertEqual(len(f_list), 30)\n        self.assertEqual(len(s_list), 30)\n        self.assertEqual(s_list[0][0], \'img0_g\')\n\n    def test_keywords_grep(self):\n        matcher = KeywordsMatching.from_dict(\n            {\'path_to_search\': \'testing_data/images2d\',\n             \'filename_contains\': \'img\'})\n        f_list, s_list = matcher.matching_subjects_and_filenames()\n        self.assertEqual(len(f_list), 30)\n        self.assertEqual(len(s_list), 30)\n        # filename matched \'img\' will return and\n        # the matched string is removed from subject_id\n        self.assertEqual(s_list[0][0], \'0_g\')\n\n    def test_keywords_not_contain(self):\n        matcher = KeywordsMatching.from_dict(\n            {\'path_to_search\': \'testing_data/images2d\',\n             \'filename_not_contains\': \'img\'})\n        with self.assertRaisesRegexp(ValueError, """"):\n            # not filename (not containing \'img\') matched\n            matcher.matching_subjects_and_filenames()\n\n        matcher = KeywordsMatching.from_dict(\n            {\'path_to_search\': \'testing_data/images2d\',\n             \'filename_not_contains\': (\'_m\', \'_u\')})\n        f_list, s_list = matcher.matching_subjects_and_filenames()\n        self.assertEqual(len(f_list), 10)\n        self.assertEqual(len(s_list), 10)\n\n        matcher_comp = KeywordsMatching.from_dict(\n            {\'path_to_search\': \'testing_data/images2d\',\n             \'filename_contains\': \'_g\'})\n        f_comp, s_comp = matcher_comp.matching_subjects_and_filenames()\n        self.assertEqual(len(f_comp), 10)\n        self.assertEqual(len(f_comp), 10)\n        self.assertEqual(f_comp, f_list)\n\n    def test_keywords_params_combine(self):\n        matcher = KeywordsMatching.from_dict(\n            {\'path_to_search\': \'testing_data/images2d\',\n             \'filename_contains\': \'_g\',\n             \'filename_removefromid\': \'img|_g\'})\n        f_list, s_list = matcher.matching_subjects_and_filenames()\n        self.assertEqual(len(f_list), 10)\n        self.assertEqual(len(s_list), 10)\n\n        matcher_comp = KeywordsMatching.from_dict(\n            {\'path_to_search\': \'testing_data/images2d\',\n             \'filename_not_contains\': (\'_m\', \'_u\'),\n             \'filename_removefromid\': ""img|_g""})\n        f_comp, s_comp = matcher_comp.matching_subjects_and_filenames()\n        self.assertEqual(f_comp, f_list)\n        self.assertEqual(s_comp, s_list)\n\n        matcher = KeywordsMatching.from_dict(\n            {\'path_to_search\': \'testing_data/images2d\',\n             \'filename_removefromid\': \'img|_g|_m|_u\'})\n        with self.assertRaisesRegexp(ValueError, """"):\n            matcher.matching_subjects_and_filenames()\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/fully_connected_test.py,5,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.layer.fully_connected import FCLayer\nfrom niftynet.layer.fully_connected import FullyConnectedLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass FCTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 10, 10, 5, 8)\n        x_3d = tf.ones(input_shape)\n        return x_3d\n\n    def get_2d_input(self):\n        input_shape = (2, 8, 4, 8)\n        x_2d = tf.ones(input_shape)\n        return x_2d\n\n    def _test_fc_output_shape(self,\n                              rank,\n                              param_dict,\n                              output_shape):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        fc_layer = FCLayer(**param_dict)\n        output_data = fc_layer(input_data)\n        print(fc_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_value = sess.run(output_data)\n            self.assertAllClose(output_shape, output_value.shape)\n\n    def _test_fc_layer_output_shape(self,\n                                    rank,\n                                    param_dict,\n                                    output_shape,\n                                    is_training=None,\n                                    dropout_prob=None):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        fc_layer = FullyConnectedLayer(**param_dict)\n        output_data = fc_layer(input_data,\n                               is_training=is_training,\n                               keep_prob=dropout_prob)\n        print(fc_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_value = sess.run(output_data)\n            self.assertAllClose(output_shape, output_value.shape)\n\n    # 3d tests\n    def test_3d_fc_default_shape(self):\n        input_param = {\'n_output_chns\': 10}\n        self._test_fc_output_shape(rank=3,\n                                   param_dict=input_param,\n                                   output_shape=(2, 10))\n\n    def test_3d_fc_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': True}\n        self._test_fc_output_shape(rank=3,\n                                   param_dict=input_param,\n                                   output_shape=(2, 10))\n\n    def test_fc_3d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': True,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_output_shape(rank=3,\n                                   param_dict=input_param,\n                                   output_shape=(2, 10))\n\n    def test_3d_fclayer_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': True,\n                       \'feature_normalization\': None}\n        self._test_fc_layer_output_shape(rank=3,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10))\n\n    def test_fclayer_3d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'feature_normalization\': None,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_layer_output_shape(rank=3,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10))\n\n    def test_fclayer_3d_bn_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_layer_output_shape(rank=3,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True)\n\n    def test_fclayer_3d_bn_reg_prelu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_layer_output_shape(rank=3,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True)\n\n    def test_fclayer_3d_relu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'relu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_layer_output_shape(rank=3,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True)\n\n    def test_fclayer_3d_bn_reg_dropout_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\'}\n        self._test_fc_layer_output_shape(rank=3,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True,\n                                         dropout_prob=0.4)\n\n    def test_fclayer_3d_bn_reg_dropout_valid_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'acti_func\': \'prelu\', }\n        self._test_fc_layer_output_shape(rank=3,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True,\n                                         dropout_prob=0.4)\n\n    # 2d tests\n    def test_2d_fc_default_shape(self):\n        input_param = {\'n_output_chns\': 10}\n        self._test_fc_output_shape(rank=2,\n                                   param_dict=input_param,\n                                   output_shape=(2, 10))\n\n    def test_2d_fc_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': True}\n        self._test_fc_output_shape(rank=2,\n                                   param_dict=input_param,\n                                   output_shape=(2, 10))\n\n    def test_fc_2d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': True,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_output_shape(rank=2,\n                                   param_dict=input_param,\n                                   output_shape=(2, 10))\n\n    def test_2d_fclayer_default_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': True}\n        self._test_fc_layer_output_shape(rank=2,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True)\n\n    def test_2d_fclayer_bias_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': True,\n                       \'feature_normalization\': None}\n        self._test_fc_layer_output_shape(rank=2,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10))\n\n    def test_fclayer_2d_bias_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': True,\n                       \'feature_normalization\': None,\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_layer_output_shape(rank=2,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10))\n\n    def test_fclayer_2d_bn_reg_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5),\n                       \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_layer_output_shape(rank=2,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True)\n\n    def test_fclayer_2d_bn_reg_prelu_2_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\'}\n        self._test_fc_layer_output_shape(rank=2,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True)\n\n    def test_fclayer_2d_relu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'relu\'}\n        self._test_fc_layer_output_shape(rank=2,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True)\n\n    def test_fclayer_2d_bn_reg_prelu_shape(self):\n        input_param = {\'n_output_chns\': 10,\n                       \'with_bias\': False,\n                       \'feature_normalization\': \'batch\',\n                       \'acti_func\': \'prelu\',\n                       \'w_regularizer\': regularizers.l2_regularizer(0.5)}\n        self._test_fc_layer_output_shape(rank=2,\n                                         param_dict=input_param,\n                                         output_shape=(2, 10),\n                                         is_training=True)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/get_gpu_index.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom subprocess import Popen, PIPE\n\nbash_string = ""nvidia-smi --query-gpu=memory.free --format=csv""\np1 = Popen(bash_string.split(), stdout=PIPE)\nbash_string = ""tail -n 2""\np2 = Popen(bash_string.split(), stdin=p1.stdout, stdout=PIPE)\np3 = Popen([\'sed\', \'s: MiB::\'], stdin=p2.stdout, stdout=PIPE)\noutput, error = p3.communicate()\n\nfree_memory = [float(x) for x in output.decode(\'utf-8\').split(\'\\n\')[:-1]]\nif free_memory[1] > free_memory[0]:\n    print(\'1\')\nelse:\n    print(\'0\')\n'"
tests/gn_test.py,8,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.layer.gn import GNLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass GNTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def test_3d_gn_shape(self):\n        x = self.get_3d_input()\n        gn_layer = GNLayer(4)\n        print(gn_layer)\n        out_gn = gn_layer(x)\n        print(gn_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_gn)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n    def test_3d_gn_reg_shape(self):\n        x = self.get_3d_input()\n        gn_layer = GNLayer(4, regularizer=regularizers.l2_regularizer(0.5))\n        out_gn = gn_layer(x)\n        test_gn = gn_layer(x)\n        print(gn_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_gn)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n            out = sess.run(test_gn)\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n    def test_2d_gn_shape(self):\n        x = self.get_2d_input()\n        gn_layer = GNLayer(4)\n        out_gn = gn_layer(x)\n        print(gn_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_gn)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n    def test_2d_gn_reg_shape(self):\n        x = self.get_2d_input()\n        gn_layer = GNLayer(4, regularizer=regularizers.l2_regularizer(0.5))\n        out_gn = gn_layer(x)\n        test_gn = gn_layer(x)\n        print(gn_layer)\n        reg_loss = tf.add_n(gn_layer.regularizer_loss())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            out = sess.run(out_gn)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n            out = sess.run(test_gn)\n            self.assertAllClose(x_shape, out.shape)\n            # self.assertAllClose(np.zeros(x_shape), out)\n\n            out = sess.run(reg_loss)\n            self.assertAlmostEqual(out, 2.0)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/gradient_collector_test.py,29,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_variables import GradientsCollector\nfrom niftynet.engine.handler_gradient import ApplyGradients\nfrom niftynet.network.toynet import ToyNet\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\ndef get_test_network():\n    net = ToyNet(num_classes=4)\n    return net\n\n\nclass GradientCollectorTest(NiftyNetTestCase):\n    def test_nested_gradients(self):\n        n_device = 3\n        grad_collector = GradientsCollector(n_devices=n_device)\n\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx) as scope:\n                image = tf.ones([2, 32, 32, 32, 4], dtype=tf.float32)\n                test_net = get_test_network()(image, is_training=True)\n                loss = tf.reduce_mean(tf.square(test_net - image))\n                optimiser = tf.train.GradientDescentOptimizer(0.1)\n                grads = optimiser.compute_gradients(loss)\n                grad_collector.add_to_collection([grads])\n        self.assertAllClose(len(grad_collector._gradients), n_device)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            grad_collector.add_to_collection(grads)\n        ave_grads = grad_collector.gradients\n        self.assertAllClose(len(grad_collector._gradients[0]), len(ave_grads))\n        self.assertAllClose(\n            grad_collector._gradients[0][0][0][0].shape.as_list(),\n            ave_grads[0][0][0].shape.as_list())\n\n    def test_gradients(self):\n        n_device = 3\n        grad_collector = GradientsCollector(n_devices=n_device)\n\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx) as scope:\n                image = tf.ones([2, 32, 32, 32, 4], dtype=tf.float32)\n                test_net = get_test_network()(image, is_training=True)\n                loss = tf.reduce_mean(tf.square(test_net - image))\n                optimiser = tf.train.GradientDescentOptimizer(0.1)\n                grads = optimiser.compute_gradients(loss)\n                grad_collector.add_to_collection(grads)\n        self.assertAllClose(len(grad_collector._gradients), n_device)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            grad_collector.add_to_collection(grads)\n        ave_grads = grad_collector.gradients\n        self.assertAllClose(len(grad_collector._gradients[0]), len(ave_grads))\n        self.assertAllClose(\n            grad_collector._gradients[0][0][0][0].shape.as_list(),\n            ave_grads[0][0][0].shape.as_list())\n\n    def test_multiple_loss_gradients(self):\n        n_device = 3\n        grad_collector = GradientsCollector(n_devices=n_device)\n\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx) as scope:\n                image = tf.ones([2, 32, 32, 32, 4], dtype=tf.float32)\n                test_net = get_test_network()(image, is_training=True)\n                loss = tf.reduce_mean(tf.square(test_net - image))\n                loss_1 = tf.reduce_mean(tf.abs(test_net - image))\n                optimiser = tf.train.GradientDescentOptimizer(0.1)\n                grads = optimiser.compute_gradients(loss)\n                grads_1 = optimiser.compute_gradients(loss_1)\n                grad_collector.add_to_collection([grads, grads_1])\n        self.assertAllClose(len(grad_collector._gradients), n_device)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            grad_collector.add_to_collection(grads)\n        ave_grads = grad_collector.gradients\n        self.assertAllClose(len(grad_collector._gradients[0]), len(ave_grads))\n        self.assertAllClose(\n            grad_collector._gradients[0][0][0][0].shape.as_list(),\n            ave_grads[0][0][0].shape.as_list())\n\n    def test_multiple_device_multiple_loss_gradients_with_multiple_optimiser(self):\n        n_device = 3\n        grad_collector = GradientsCollector(n_devices=n_device)\n\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx) as scope:\n                image = tf.ones([2, 32, 32, 32, 4], dtype=tf.float32)\n                test_net = get_test_network()(image, is_training=True)\n                loss = tf.reduce_mean(tf.square(test_net - image))\n                loss_1 = tf.reduce_mean(tf.abs(test_net - image))\n                grads = dict()\n                optimiser = dict()\n                optimiser[\'opt\'] = tf.train.GradientDescentOptimizer(0.1)\n                optimiser[\'opt_1\'] = tf.train.GradientDescentOptimizer(0.1)\n                grads[\'opt\'] = optimiser[\'opt\'].compute_gradients(loss)\n                grads[\'opt_1\'] = optimiser[\'opt_1\'].compute_gradients(loss_1)\n                grad_collector.add_to_collection(grads)\n        self.assertAllClose(len(grad_collector._gradients), n_device)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            grad_collector.add_to_collection(grads)\n        ave_grads = grad_collector.gradients\n        self.assertAllClose(len(grad_collector._gradients[0]), len(ave_grads))\n        self.assertAllClose(\n            grad_collector._gradients[0][\'opt\'][0][0][0].shape.as_list(),\n            ave_grads[\'opt\'][0][0][0].shape.as_list())\n        self.assertAllClose(\n            grad_collector._gradients[0][\'opt_1\'][0][0][0].shape.as_list(),\n            ave_grads[\'opt_1\'][0][0][0].shape.as_list())\n\n    def test_single_device_gradients(self):\n        n_device = 1\n        grad_collector = GradientsCollector(n_devices=n_device)\n\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx) as scope:\n                image = tf.ones([2, 32, 32, 32, 4], dtype=tf.float32)\n                test_net = get_test_network()(image, is_training=True)\n                loss = tf.reduce_mean(tf.square(test_net - image))\n                loss_1 = tf.reduce_mean(tf.abs(test_net - image))\n                optimiser = tf.train.GradientDescentOptimizer(0.1)\n                grads = optimiser.compute_gradients(loss)\n                grads_1 = optimiser.compute_gradients(loss_1)\n                grad_collector.add_to_collection([grads, grads_1])\n        self.assertAllClose(len(grad_collector._gradients), n_device)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            grad_collector.add_to_collection(grads)\n        ave_grads = grad_collector.gradients\n        self.assertAllClose(len(grad_collector._gradients[0]), len(ave_grads))\n        self.assertAllClose(\n            grad_collector._gradients[0][0][0][0].shape.as_list(),\n            ave_grads[0][0][0].shape.as_list())\n\n    def test_simple_gradients(self):\n        n_device = 1\n        grad_collector = GradientsCollector(n_devices=n_device)\n\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx) as scope:\n                image = tf.ones([2, 32, 32, 32, 4], dtype=tf.float32)\n                test_net = get_test_network()(image, is_training=True)\n                loss = tf.reduce_mean(tf.square(test_net - image))\n                optimiser = tf.train.GradientDescentOptimizer(0.1)\n                grads = optimiser.compute_gradients(loss)\n                grad_collector.add_to_collection(grads)\n        self.assertAllClose(len(grad_collector._gradients), n_device)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            grad_collector.add_to_collection(grads)\n        ave_grads = grad_collector.gradients\n        self.assertAllClose(len(grad_collector._gradients[0]), len(ave_grads))\n        self.assertAllClose(\n            grad_collector._gradients[0][0][0].shape.as_list(),\n            ave_grads[0][0].shape.as_list())\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/grid_warper_test.py,14,"b'from __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.grid_warper import AffineWarpConstraints\nfrom niftynet.layer.grid_warper import _create_affine_features\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass GridWarperTest(NiftyNetTestCase):\n    def test_regular_grids(self):\n        out = _create_affine_features([3, 3], [2])\n        expected = [\n            np.array([0., 0., 0., 1., 1., 1., 2., 2., 2.], dtype=np.float32),\n            np.array([0., 1., 2., 0., 1., 2., 0., 1., 2.], dtype=np.float32),\n            np.array([1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=np.float32)]\n        self.assertAllClose(expected, out)\n\n\nclass GridWarperRelativeTest(NiftyNetTestCase):\n    def test_regular_grids(self):\n        out = _create_affine_features([3, 3], [2], True)\n        expected = [\n            np.array([-1., -1., -1., 0., 0., 0., 1., 1., 1.], dtype=np.float32),\n            np.array([-1., 0., 1., -1., 0., 1., -1., 0., 1.], dtype=np.float32),\n            np.array([1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=np.float32)]\n        self.assertAllClose(expected, out)\n\n\nclass AffineWarpConstraintsTest(NiftyNetTestCase):\n    def test_default(self):\n        aff_c = AffineWarpConstraints()\n        self.assertAllEqual(aff_c.constraints,\n                            ((None, None, None), (None, None, None)))\n        self.assertAllClose(aff_c.num_free_params, 6)\n        self.assertAllClose(aff_c.num_dim, 2)\n\n        customise_constraints = (\n            (None, None, 1, 1),\n            (None, None, None, 2),\n            (None, 1, None, 3))\n        aff_c = AffineWarpConstraints(customise_constraints)\n        self.assertEqual(aff_c.constraints, customise_constraints)\n\n    def test_no_constraints(self):\n        aff_c = AffineWarpConstraints.no_constraints(num_dim=2)\n        self.assertEqual(aff_c.constraints,\n                         ((None, None, None), (None, None, None)))\n\n    def test_all_constraints_2d(self):\n        test_x, test_y = 1, 2\n\n        aff_c = AffineWarpConstraints.translation_2d(x=test_x, y=test_y)\n        self.assertEqual(aff_c.constraints, ((None, None, 1), (None, None, 2)))\n\n        aff_c = AffineWarpConstraints.scale_2d(x=test_x, y=test_y)\n        self.assertEqual(aff_c.constraints, ((1, None, None), (None, 2, None)))\n\n        aff_c = AffineWarpConstraints.shear_2d(x=test_x, y=test_y)\n        self.assertEqual(aff_c.constraints, ((None, 1, None), (2, None, None)))\n\n        aff_c = AffineWarpConstraints.no_shear_2d()\n        self.assertEqual(aff_c.constraints, ((None, 0, None), (0, None, None)))\n\n    def test_all_constraints_3d(self):\n        test_x, test_y, test_z = 1, 2, 3\n\n        aff_c = AffineWarpConstraints.translation_3d(\n            x=test_x, y=test_y, z=test_z)\n        self.assertEqual(\n            aff_c.constraints,\n            ((None, None, None, 1),\n            (None, None, None, 2),\n            (None, None, None, 3)))\n\n        aff_c = AffineWarpConstraints.scale_3d(\n            x=test_x, y=test_y, z=test_z)\n        self.assertEqual(\n            aff_c.constraints,\n            ((1, None, None, None),\n            (None, 2, None, None),\n            (None, None, 3, None)))\n\n        aff_c = AffineWarpConstraints.no_shear_3d()\n        self.assertEqual(\n            aff_c.constraints,\n            ((None, 0, 0, None),\n            (0, None, 0, None),\n            (0, 0, None, None)))\n\n    def test_combine_constraints(self):\n        test_x, test_y, test_z = 1, 2, 3\n        aff_c_1 = AffineWarpConstraints.translation_2d(x=test_x, y=test_y)\n        aff_c_2 = AffineWarpConstraints.shear_2d(x=test_x, y=test_y)\n        aff_comb = aff_c_1.combine_with(aff_c_2)\n        self.assertEqual(aff_comb.constraints, ((None, 1, 1), (2, None, 2)))\n\n        aff_c_1 = AffineWarpConstraints.translation_3d(\n            x=test_x, y=test_y, z=test_z)\n        aff_c_2 = AffineWarpConstraints.no_shear_3d()\n        aff_comb = aff_c_1.combine_with(aff_c_2)\n        self.assertEqual(aff_comb.constraints,\n                         ((None, 0, 0, 1), (0, None, 0, 2), (0, 0, None, 3)))\n\n        aff_c_1 = AffineWarpConstraints.translation_2d(x=test_x, y=test_y)\n        aff_c_2 = AffineWarpConstraints.no_shear_3d()\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            aff_c_1.combine_with(aff_c_2)\n\n\nclass AffineGridWarperLayerTest(NiftyNetTestCase):\n    def _test_correctness(self, args, aff, expected_value):\n        grid_warper = AffineGridWarperLayer(**args)\n        computed_grid = grid_warper(aff)\n        with self.cached_session() as sess:\n            output_val = sess.run(computed_grid)\n            self.assertAllClose(expected_value, output_val)\n\n    def test_no_constraints(self):\n        grid_warper = AffineGridWarperLayer(source_shape=(3, 3),\n                                            output_shape=(2,))\n        self.assertEqual(grid_warper.constraints.constraints,\n                         ((None, None, None), (None, None, None)))\n\n    def test_constraints(self):\n        grid_warper = AffineGridWarperLayer(\n            source_shape=(3, 3),\n            output_shape=(2, 4),\n            constraints=AffineWarpConstraints.no_shear_2d())\n        self.assertEqual(grid_warper.constraints.constraints,\n                         ((None, 0, None), (0, None, None)))\n\n    def test_2d(self):\n        aff = tf.constant([[1, 0, 5, 0, 1, 6]], dtype=tf.float32)\n        params = {\'source_shape\': (3, 3), \'output_shape\': (2, 2)}\n        expected = [[[[5, 6], [5, 8]], [[7, 6], [7, 8]]]]\n        self._test_correctness(params, aff, expected)\n\n    def test_3d(self):\n        aff = tf.constant([[\n            2, 0, 0, 5,\n            0, 2, 0, 6,\n            0, 0, 1, 4]], dtype=tf.float32)\n        params = {\'source_shape\': (3, 3, 3), \'output_shape\': (2, 2, 2)}\n        expected = [[[[[4, 5, 4], [4, 5, 6]], [[4, 9, 4], [4, 9, 6]]],\n            [[[8, 5, 4], [8, 5, 6]], [[8, 9, 4], [8, 9, 6]]]]]\n        self._test_correctness(params, aff, expected)\n\n    def test_3d_2d(self):\n        aff = tf.constant([[\n            1.5, 0, 0, 3,\n            0, 1.5, 0, 4,\n            0, 0, 1.0, 5]], dtype=tf.float32)\n        params = {\'source_shape\': (3, 3, 3), \'output_shape\': (2, 2)}\n        expected = [\n            [[[2.5, 3.5, 5], [2.5, 6.5, 5]], [[5.5, 3.5, 5], [5.5, 6.5, 5]]]]\n        self._test_correctness(params, aff, expected)\n\n    def test_2d_1d(self):\n        aff = tf.constant([[1.5, 2, 3, 2, 1.5, 4]], dtype=tf.float32)\n        params = {\'source_shape\': (3, 3), \'output_shape\': (2,)}\n        expected = [[[0.5, 1.5], [3.5, 5.5]]]\n        self._test_correctness(params, aff, expected)\n\n    def test_3d_1d(self):\n        aff = tf.constant([[\n            1.5, 0, 0, 3,\n            0, 2, 0, 4,\n            0, 0, 1.0, 5]], dtype=tf.float32)\n        params = {\'source_shape\': (3, 3, 3), \'output_shape\': (2,)}\n        expected = [[[2.5, 3, 5], [5.5, 3, 5]]]\n        self._test_correctness(params, aff, expected)\n\n    def test_3d_2d_scale(self):\n        aff = tf.constant([[\n            0, 0, 3,\n            0, 0, 4,\n            0, 0, 5]], dtype=tf.float32)\n        params = {\'source_shape\': (3, 3, 3), \'output_shape\': (2, 2),\n            \'constraints\': AffineWarpConstraints.scale_3d(1, 1, 1)}\n        expected = [[[[3, 4, 5], [3, 6, 5]], [[5, 4, 5], [5, 6, 5]]]]\n        self._test_correctness(params, aff, expected)\n\n    def test_3d_3d_translation(self):\n        aff = tf.constant([[2, 0, 0,\n            0, 1.5, 0,\n            0, 0, 3]], dtype=tf.float32)\n        params = {\'source_shape\': (3, 3, 3), \'output_shape\': (2, 2),\n            \'constraints\': AffineWarpConstraints.translation_3d(3, 4, 5)}\n        expected = [[[[2, 3.5, 3], [2, 6.5, 3]], [[6, 3.5, 3], [6, 6.5, 3]]]]\n        self._test_correctness(params, aff, expected)\n\n\nclass AffineGridWarperInvLayerTest(NiftyNetTestCase):\n    def test_simple_inverse(self):\n        expected_grid = np.array([[\n            [[0.38, 0.08],\n                [-0.02, 0.68],\n                [-0.42, 1.28]],\n            [[0.98, -0.32],\n                [0.58, 0.28],\n                [0.18, 0.88]],\n            [[1.58, -0.72],\n                [1.18, -0.12],\n                [0.78, 0.48]]]], dtype=np.float32)\n\n        inverse_grid = AffineGridWarperLayer(source_shape=(3, 3),\n                                             output_shape=(2, 2)).inverse_op()\n        aff = tf.constant([[1.5, 1.0, 0.2, 1.0, 1.5, 0.5]])\n        output = inverse_grid(aff)\n        with self.cached_session() as sess:\n            out_val = sess.run(output)\n        self.assertAllClose(out_val, expected_grid)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/handler_console_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tests.application_driver_test import get_initialised_driver\nfrom niftynet.engine.application_iteration import IterationMessage\nfrom niftynet.engine.signal import SESS_STARTED, ITER_FINISHED\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass EventConsoleTest(NiftyNetTestCase):\n    def test_init(self):\n        ITER_FINISHED.connect(self.iteration_listener)\n\n        app_driver = get_initialised_driver()\n        app_driver.load_event_handlers(\n            [\'niftynet.engine.handler_model.ModelRestorer\',\n             \'niftynet.engine.handler_console.ConsoleLogger\',\n             \'niftynet.engine.handler_sampler.SamplerThreading\'])\n        graph = app_driver.create_graph(app_driver.app, 1, True)\n        with self.cached_session(graph=graph) as sess:\n            SESS_STARTED.send(app_driver.app, iter_msg=None)\n            msg = IterationMessage()\n            msg.current_iter = 1\n            app_driver.loop(app_driver.app, [msg])\n        app_driver.app.stop()\n\n        ITER_FINISHED.disconnect(self.iteration_listener)\n\n    def iteration_listener(self, sender, **msg):\n        msg = msg[\'iter_msg\']\n        self.assertRegexpMatches(msg.to_console_string(), \'mean\')\n        self.assertRegexpMatches(msg.to_console_string(), \'var\')\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/handler_early_stopping_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom niftynet.engine.handler_early_stopping import check_should_stop\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass EarlyStopperTest(NiftyNetTestCase):\n\n    def test_mean(self):\n        should_stop = check_should_stop(mode=\'mean\',\n                                        performance_history=[1, 2, 1, 2, 1,\n                                                             2, 1, 2, 3])\n        self.assertTrue(should_stop)\n        should_stop = check_should_stop(mode=\'mean\',\n                                        performance_history=[1, 2, 1, 2, 1, 2,\n                                                             1, 2, 3, 0])\n        self.assertFalse(should_stop)\n\n    def test_robust_mean(self):\n        should_stop = check_should_stop(mode=\'robust_mean\',\n                                        performance_history=[1, 2, 1, 2, 1, 2,\n                                                             1, 200, -10, 1.4])\n        self.assertFalse(should_stop)\n        should_stop = check_should_stop(mode=\'robust_mean\',\n                                        performance_history=[1, 2, 1, 2, 1, 2,\n                                                             1, 200, -10, 1.5])\n        self.assertTrue(should_stop)\n\n    def test_median(self):\n        should_stop = check_should_stop(mode=\'median\',\n                                        performance_history=[1, 2, 1, 2, 1, 2,\n                                                             1, 2, 3])\n        self.assertTrue(should_stop)\n        should_stop = check_should_stop(mode=\'median\',\n                                        performance_history=[1, 2, 1, 2, 1, 2,\n                                                             1, 2, 3, 0])\n        self.assertFalse(should_stop)\n\n    def test_generalisation_loss(self):\n        should_stop = check_should_stop(mode=\'generalisation_loss\',\n                                        performance_history=[1, 2, 1, 2, 1,\n                                                             2, 1, 2, 3])\n        self.assertTrue(should_stop)\n        should_stop = check_should_stop(mode=\'generalisation_loss\',\n                                        performance_history=[1, 2, 1, 2, 3,\n                                                             2, 1, 2, 1])\n        self.assertFalse(should_stop)\n\n    def test_validation_up(self):\n        data = []\n        for i in range(10):\n            data.extend(np.arange(1, 9))\n            data.extend(np.arange(2, 10)[::-1])\n        should_stop = check_should_stop(mode=\'validation_up\',\n                                        performance_history=\n                                        np.arange(0, 20) / 10.0)\n        print(""1 val"")\n        self.assertTrue(should_stop)\n        should_stop = check_should_stop(mode=\'validation_up\',\n                                        performance_history=np.arange(\n                                            0, 20)[::-1] / 10)\n        print(""2 val"")\n        self.assertFalse(should_stop)\n\n        should_stop = check_should_stop(mode=\'validation_up\',\n                                        performance_history=data,\n                                        min_delta=0.2)\n        print(""3 val"")\n        self.assertFalse(should_stop)\n\n    def test_median_smoothing(self):\n        data = []\n        for i in range(10):\n            data.extend(np.arange(0, 8))\n            data.extend(np.arange(1, 9)[::-1])\n        should_stop = \\\n            check_should_stop(mode=\'median_smoothing\',\n                              performance_history=np.arange(0, 20) / 10.0)\n        self.assertTrue(should_stop)\n        should_stop = check_should_stop(mode=\'median_smoothing\',\n                                        performance_history=np.arange(\n                                            0, 20)[::-1] / 10)\n        self.assertFalse(should_stop)\n\n        should_stop = check_should_stop(mode=\'median_smoothing\',\n                                        performance_history=data)\n        self.assertFalse(should_stop)\n\n    def test_weird_mode(self):\n        with self.assertRaises(Exception):\n            check_should_stop(mode=\'adslhfjdkas\',\n                              performance_history=[1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/handler_network_output_test.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_iteration import IterationMessageGenerator\nfrom niftynet.engine.application_variables import NETWORK_OUTPUT\nfrom tests.application_driver_test import get_initialised_driver\nfrom niftynet.engine.signal import SESS_STARTED\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\ndef set_iteration_update(msg):\n    msg.ops_to_run[NETWORK_OUTPUT] = \\\n        tf.get_default_graph().get_tensor_by_name(""G/conv_bn_selu/conv_/w:0"")\n\n\nclass EventConsoleTest(NiftyNetTestCase):\n    def create_interpreter(self):\n        def mini_interpreter(np_array):\n            self.assertEqual(np_array.shape, (10, 1, 20))\n            return False\n\n        return mini_interpreter\n\n    def test_init(self):\n        app_driver = get_initialised_driver()\n        test_graph = app_driver.create_graph(app_driver.app, 1, True)\n\n        app_driver.app.set_iteration_update = set_iteration_update\n        app_driver.app.interpret_output = self.create_interpreter()\n\n        app_driver.load_event_handlers(\n            [\'niftynet.engine.handler_model.ModelRestorer\',\n             \'niftynet.engine.handler_network_output.OutputInterpreter\',\n             \'niftynet.engine.handler_sampler.SamplerThreading\'])\n        with self.cached_session(graph=test_graph) as sess:\n            SESS_STARTED.send(app_driver.app, iter_msg=None)\n\n            iterator = IterationMessageGenerator(is_training_action=False)\n            app_driver.loop(app_driver.app, iterator())\n        app_driver.app.stop()\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/handler_performance_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom tests.application_driver_test import get_initialised_driver\nfrom niftynet.engine.application_iteration import IterationMessage\nfrom niftynet.engine.signal import SESS_STARTED, ITER_FINISHED, VALID\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass PerformanceLoggerTest(NiftyNetTestCase):\n    def test_init(self):\n        ITER_FINISHED.connect(self.iteration_listener)\n        app_driver = get_initialised_driver()\n        app_driver.load_event_handlers(\n            [\'niftynet.engine.handler_model.ModelRestorer\',\n             \'niftynet.engine.handler_console.ConsoleLogger\',\n             \'niftynet.engine.handler_sampler.SamplerThreading\',\n             \'niftynet.engine.handler_performance.PerformanceLogger\'])\n        graph = app_driver.create_graph(app_driver.app, 1, True)\n        with self.cached_session(graph=graph) as sess:\n            for i in range(110):\n                SESS_STARTED.send(app_driver.app, iter_msg=None)\n                msg = IterationMessage()\n                msg._phase = VALID\n                msg.current_iter = i\n                app_driver.loop(app_driver.app, [msg])\n        app_driver.app.stop()\n        ITER_FINISHED.disconnect(self.iteration_listener)\n\n    def iteration_listener(self, sender, **msg):\n        msg = msg[\'iter_msg\']\n        self.assertRegexpMatches(msg.to_console_string(), \'.*total_loss.*\')\n        if msg.current_iter > 1:\n            self.assertTrue(isinstance(sender.performance_history, list))\n            self.assertTrue(len(sender.performance_history) <= sender.patience)\n            self.assertTrue(all([isinstance(p, np.float32) for p in sender.performance_history]))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/highres3dnet_test.py,5,"b'from __future__ import absolute_import, print_function\n\nimport os\nimport unittest\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.highres3dnet import HighRes3DNet\nfrom niftynet.network.highres3dnet_large import HighRes3DNetLarge\nfrom niftynet.network.highres3dnet_small import HighRes3DNetSmall\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"",\n                 \'Skipping slow tests\')\nclass HighRes3DNetTest(NiftyNetTestCase):\n    def shape_test(self, input_shape, expected_shape):\n        x = tf.ones(input_shape)\n\n        highres_layer = HighRes3DNet(num_classes=5)\n        highres_layer_small = HighRes3DNetSmall(num_classes=5)\n        highres_layer_large = HighRes3DNetLarge(num_classes=5)\n\n        out = highres_layer(x, is_training=True)\n        out_small = highres_layer_small(x, is_training=True)\n        out_large = highres_layer_large(x, is_training=True)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out, out_large, out_small = sess.run([out, out_large, out_small])\n            self.assertAllClose(expected_shape, out.shape)\n            self.assertAllClose(expected_shape, out_large.shape)\n            self.assertAllClose(expected_shape, out_small.shape)\n\n    def shape_test_reg(self, input_shape, expected_shape):\n        x = tf.ones(input_shape)\n        layer_param = {\n            \'num_classes\': 5,\n            \'w_regularizer\': regularizers.l2_regularizer(0.5),\n            \'b_regularizer\': regularizers.l2_regularizer(0.5)}\n\n        highres_layer = HighRes3DNet(**layer_param)\n        highres_layer_small = HighRes3DNetSmall(**layer_param)\n        highres_layer_large = HighRes3DNetLarge(**layer_param)\n\n        out = highres_layer(x, is_training=True)\n        out_small = highres_layer_small(x, is_training=True)\n        out_large = highres_layer_large(x, is_training=True)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out, out_large, out_small = sess.run([out, out_large, out_small])\n            self.assertAllClose(expected_shape, out.shape)\n            self.assertAllClose(expected_shape, out_large.shape)\n            self.assertAllClose(expected_shape, out_small.shape)\n\n    def test_2d(self):\n        self.shape_test(input_shape=(2, 32, 32, 1),\n                        expected_shape=(2, 32, 32, 5))\n        self.shape_test_reg(input_shape=(2, 32, 32, 1),\n                            expected_shape=(2, 32, 32, 5))\n\n    def test_3d(self):\n        self.shape_test(input_shape=(2, 32, 32, 32, 1),\n                        expected_shape=(2, 32, 32, 32, 5))\n        self.shape_test_reg(input_shape=(2, 32, 32, 32, 1),\n                            expected_shape=(2, 32, 32, 32, 5))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/highresblock_test.py,19,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.highres3dnet import HighResBlock\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass HighResBlockTest(NiftyNetTestCase):\n    def test_3d_increase_shape(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(n_output_chns=16,\n                                     kernels=(3, 3),\n                                     with_res=True)\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 16, 16), out.shape)\n\n    def test_3d_same_shape(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(n_output_chns=8,\n                                     kernels=(3, 3),\n                                     with_res=True)\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 16, 8), out.shape)\n\n    def test_3d_reduce_shape(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(n_output_chns=4,\n                                     kernels=(3, 3),\n                                     with_res=True)\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 16, 4), out.shape)\n\n    def test_3d_reg_increase_shape(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(\n            n_output_chns=16,\n            kernels=(3, 3),\n            with_res=True,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 16, 16), out.shape)\n\n    def test_3d_reg_same_shape(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(\n            n_output_chns=8,\n            kernels=(3, 3),\n            with_res=True,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 16, 8), out.shape)\n\n    def test_3d_reg_reduce_shape(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(\n            n_output_chns=4,\n            kernels=(3, 3),\n            with_res=True,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 16, 4), out.shape)\n\n    def test_2d_increase_shape(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(n_output_chns=16,\n                                     kernels=(3, 3),\n                                     with_res=True)\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 16), out.shape)\n\n    def test_2d_same_shape(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(n_output_chns=8,\n                                     kernels=(3, 3),\n                                     with_res=True)\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 8), out.shape)\n\n    def test_2d_reduce_shape(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n\n        highres_layer = HighResBlock(n_output_chns=4,\n                                     kernels=(3, 3),\n                                     with_res=True)\n        out = highres_layer(x, is_training=True)\n        print(highres_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 4), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/histogram_normalisation_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nDATA_PARAM = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIR.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    )\n}\nTASK_PARAM = ParserNamespace(image=(\'T1\', \'FLAIR\'))\nMODEL_FILE = os.path.join(\'testing_data\', \'std_models.txt\')\ndata_partitioner = ImageSetsPartitioner()\nfile_list = data_partitioner.initialise(DATA_PARAM).get_file_list()\n\n\n# @unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass HistTest(NiftyNetTestCase):\n    def test_volume_loader(self):\n        expected_T1 = np.array(\n            [0.0, 8.24277910972, 21.4917343731,\n             27.0551695202, 32.6186046672, 43.5081573038,\n             53.3535675285, 61.9058849776, 70.0929786194,\n             73.9944243858, 77.7437509974, 88.5331971492,\n             100.0])\n        expected_FLAIR = np.array(\n            [0.0, 5.36540863446, 15.5386130103,\n             20.7431912042, 26.1536608309, 36.669150376,\n             44.7821246138, 50.7930589961, 56.1703089214,\n             59.2393548654, 63.1565641037, 78.7271261392,\n             100.0])\n\n        reader = ImageReader([\'image\'])\n        reader.initialise(DATA_PARAM, TASK_PARAM, file_list)\n        self.assertAllClose(len(reader._file_list), 4)\n\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=\'otsu_plus\',\n            multimod_fusion=\'or\')\n        hist_norm = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(TASK_PARAM).get(\'image\'),\n            model_filename=MODEL_FILE,\n            binary_masking_func=foreground_masking_layer,\n            cutoff=(0.05, 0.95),\n            name=\'hist_norm_layer\')\n        if os.path.exists(MODEL_FILE):\n            os.remove(MODEL_FILE)\n        hist_norm.train(reader.output_list)\n        out_map = hist_norm.mapping\n\n        self.assertAllClose(out_map[\'T1\'], expected_T1)\n        self.assertAllClose(out_map[\'FLAIR\'], expected_FLAIR)\n\n        # normalise a uniformly sampled random image\n        test_shape = (20, 20, 20, 3, 2)\n        rand_image = np.random.uniform(low=-10.0, high=10.0, size=test_shape)\n        norm_image = np.copy(rand_image)\n        norm_image_dict, mask_dict = hist_norm({\'image\': norm_image})\n        norm_image, mask = hist_norm(norm_image, mask_dict)\n        self.assertAllClose(norm_image_dict[\'image\'], norm_image)\n        self.assertAllClose(mask_dict[\'image\'], mask)\n\n        # apply mean std normalisation\n        mv_norm = MeanVarNormalisationLayer(\n            image_name=\'image\',\n            binary_masking_func=foreground_masking_layer)\n        norm_image, _ = mv_norm(norm_image, mask)\n        self.assertAllClose(norm_image.shape, mask.shape)\n\n        mv_norm = MeanVarNormalisationLayer(\n            image_name=\'image\',\n            binary_masking_func=None)\n        norm_image, _ = mv_norm(norm_image)\n\n        # mapping should keep at least the order of the images\n        rand_image = rand_image[:, :, :, 1, 1].flatten()\n        norm_image = norm_image[:, :, :, 1, 1].flatten()\n\n        order_before = rand_image[1:] > rand_image[:-1]\n        order_after = norm_image[1:] > norm_image[:-1]\n        self.assertAllClose(np.mean(norm_image), 0.0)\n        self.assertAllClose(np.std(norm_image), 1.0)\n        self.assertAllClose(order_before, order_after)\n        if os.path.exists(MODEL_FILE):\n            os.remove(MODEL_FILE)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/holistic_net_test.py,7,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.holistic_net import HolisticNet\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass HolisticNetTest(NiftyNetTestCase):\n    def test_3d_reg_shape(self):\n        input_shape = (2, 20, 20, 20, 1)\n        x = tf.ones(input_shape)\n\n        holistic_net_instance = HolisticNet(\n            num_classes=3,\n            w_regularizer=regularizers.l2_regularizer(0.5),\n            b_regularizer=regularizers.l2_regularizer(0.5))\n        out = holistic_net_instance(x, is_training=False)\n        # print(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 20, 20, 20, 3), out.shape)\n\n    def test_2d_reg_shape(self):\n        input_shape = (2, 20, 20, 1)\n        x = tf.ones(input_shape)\n\n        holistic_net_instance = HolisticNet(\n            num_classes=3,\n            w_regularizer=regularizers.l2_regularizer(0.5),\n            b_regularizer=regularizers.l2_regularizer(0.5))\n        out = holistic_net_instance(x, is_training=False)\n        # print(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 20, 20, 3), out.shape)\n\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/image_loader_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport niftynet.io.image_loader as image_loader\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nCASE_NIBABEL_3D = \'testing_data/FLAIR_1023.nii.gz\'\nCASE_LOGO_2D = \'niftynet-logo.png\'\n\nclass ImageLoaderTest(NiftyNetTestCase):\n    def test_nibabel_3d(self):\n        data = image_loader.load_image_obj(CASE_NIBABEL_3D).get_data()\n        self.assertAllClose(data.shape, (256, 168, 256))\n\n    def load_2d_image(self, loader=None):\n        data = image_loader.load_image_obj(CASE_LOGO_2D, loader=loader).get_data()\n        self.assertAllClose(data.shape, (400, 677, 1, 1, 4))\n\n    def test_convert_bool(self):\n        boolarr=np.ones((256,256,256),np.bool)\n        img=image_loader.image2nibabel(boolarr)\n\n    def test_2d_loaders(self):\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            self.load_2d_image(\'test\')\n        self.load_2d_image()\n        for _loader in image_loader.AVAILABLE_LOADERS.keys():\n            print(\'testing {}\'.format(_loader))\n            if _loader == \'nibabel\':\n                continue # skip nibabel for 2d image\n            if _loader == \'dummy\':\n                continue # skip the toy example\n            self.load_2d_image(_loader)\n\n    def test_all_data(self):\n        folder = \'testing_data\'\n        all_files = [\n            os.path.join(folder, f)\n            for f in os.listdir(folder)\n            if os.path.isfile(os.path.join(folder, f))]\n\n        for f in all_files:\n            if f.endswith(\'nii.gz\'):\n                loaded_shape = image_loader.load_image_obj(f).get_data().shape\n                print(loaded_shape)\n                self.assertGreaterEqual(5, len(loaded_shape))\n            else:\n                with self.assertRaisesRegexp(ValueError, \'\'):\n                    image_loader.load_image_obj(f)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/image_reader_test.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.reader_modular_test import generate_2d_images, SEG_THRESHOLD\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\ngenerate_2d_images()\n# test multiple modalities\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1reader.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRreader.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\n# test single modalities\nSINGLE_MOD_DATA = {\n    \'lesion\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'lesion.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'Lesion\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    )\n}\nSINGLE_MOD_TASK = ParserNamespace(image=(\'lesion\',))\n\nEXISTING_DATA = {\n    \'lesion\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'lesion.csv\'),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    )\n}\n\n# test labels\nLABEL_DATA = {\n    \'parcellation\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'labels.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'Parcellation\',),\n        filename_not_contains=(\'Lesion\',),\n        interp_order=0,\n        pixdim=(3, 3.9, 3),\n        axcodes=None,\n        loader=None\n    )\n}\nLABEL_TASK = ParserNamespace(label=(\'parcellation\',))\n\nBAD_DATA = {\n    \'lesion\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'lesion.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'Lesion\',),\n        filename_not_contains=(\'Parcellation\',),\n        pixdim=None,\n        axcodes=None,\n        loader=None\n        # missing interp_order\n    )\n}\nBAD_TASK = ParserNamespace(image=(\'test\',))\n\nIMAGE_2D_DATA = {\n    \'color_images\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'images_2d_u.csv\'),\n        path_to_search=os.path.join(\'testing_data\', \'images2d\'),\n        filename_contains=(\'_u.png\',),\n        interp_order=1,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    ),\n    \'gray_images\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'images_2d_g.csv\'),\n        path_to_search=os.path.join(\'testing_data\', \'images2d\'),\n        filename_contains=(\'_g.png\',),\n        interp_order=1,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    ),\n    \'seg_masks\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'images_2d_m.csv\'),\n        path_to_search=os.path.join(\'testing_data\', \'images2d\'),\n        filename_contains=(\'_m.png\',),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        loader=None\n    )\n}\n\nIMAGE_2D_TASK_COLOR = ParserNamespace(image=(\'color_images\',))\nIMAGE_2D_TASK_GRAY = ParserNamespace(image=(\'gray_images\',))\nIMAGE_2D_TASK_MASK = ParserNamespace(image=(\'seg_masks\',))\n\n# default data_partitioner\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\nsingle_mod_list = data_partitioner.initialise(SINGLE_MOD_DATA).get_file_list()\nexisting_list = data_partitioner.initialise(EXISTING_DATA).get_file_list()\nlabel_list = data_partitioner.initialise(LABEL_DATA).get_file_list()\nbad_data_list = data_partitioner.initialise(BAD_DATA).get_file_list()\nimage2d_data_list = data_partitioner.initialise(IMAGE_2D_DATA).get_file_list()\n\n\nclass ImageReaderTest(NiftyNetTestCase):\n    def test_initialisation(self):\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            reader = ImageReader([\'test\'])\n            reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n        reader = ImageReader(None)\n        # reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n\n        reader = ImageReader([\'image\'])\n        reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n        self.assertEqual(len(reader.output_list), 4)\n\n        reader = ImageReader([\'image\'])\n        reader.initialise(SINGLE_MOD_DATA, SINGLE_MOD_TASK, single_mod_list)\n        self.assertEqual(len(reader.output_list), 4)\n\n        reader = ImageReader([\'image\'])\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            reader.initialise(SINGLE_MOD_DATA, SINGLE_MOD_TASK, [])\n\n    def test_properties(self):\n        reader = ImageReader([\'image\'])\n        reader.initialise(SINGLE_MOD_DATA, SINGLE_MOD_TASK, single_mod_list)\n        self.assertEqual(len(reader.output_list), 4)\n        self.assertDictEqual(reader.spatial_ranks, {\'image\': 3})\n        self.assertDictEqual(reader.shapes,\n                             {\'image\': (256, 168, 256, 1, 1)})\n        self.assertDictEqual(reader.tf_dtypes, {\'image\': tf.float32})\n        self.assertEqual(reader.names, (\'image\',))\n        self.assertDictEqual(reader.input_sources,\n                             {\'image\': (\'lesion\',)})\n        self.assertEqual(reader.get_subject_id(1)[:4], \'Fin_\')\n        self.assertTrue(isinstance(reader.get_subject(1), dict))\n\n    def test_existing_csv(self):\n        reader_for_csv = ImageReader([\'image\'])\n        reader_for_csv.initialise(\n            SINGLE_MOD_DATA, SINGLE_MOD_TASK, single_mod_list)\n        reader = ImageReader([\'image\'])\n        reader.initialise(EXISTING_DATA, SINGLE_MOD_TASK, existing_list)\n        self.assertEqual(len(reader.output_list), 4)\n        self.assertDictEqual(reader.spatial_ranks, {\'image\': 3})\n        self.assertDictEqual(reader.shapes,\n                             {\'image\': (256, 168, 256, 1, 1)})\n        self.assertDictEqual(reader.tf_dtypes, {\'image\': tf.float32})\n        self.assertEqual(reader.names, (\'image\',))\n        self.assertDictEqual(reader.input_sources,\n                             {\'image\': (\'lesion\',)})\n        self.assertEqual(reader.get_subject_id(1)[:4], \'Fin_\')\n        self.assertTrue(isinstance(reader.get_subject(1), dict))\n\n    def test_operations(self):\n        reader = ImageReader([\'image\'])\n        reader.initialise(SINGLE_MOD_DATA, SINGLE_MOD_TASK, single_mod_list)\n        idx, data, interp_order = reader()\n        self.assertEqual(\n            SINGLE_MOD_DATA[\'lesion\'].interp_order, interp_order[\'image\'][0])\n        self.assertAllClose(data[\'image\'].shape, (256, 168, 256, 1, 1))\n\n    def test_preprocessing(self):\n        reader = ImageReader([\'image\'])\n        reader.initialise(SINGLE_MOD_DATA, SINGLE_MOD_TASK, single_mod_list)\n        idx, data, interp_order = reader()\n        self.assertEqual(SINGLE_MOD_DATA[\'lesion\'].interp_order,\n                         interp_order[\'image\'][0])\n        self.assertAllClose(data[\'image\'].shape, (256, 168, 256, 1, 1))\n        reader.add_preprocessing_layers(\n            [PadLayer(image_name=[\'image\'], border=(10, 5, 5))])\n        idx, data, interp_order = reader(idx=2)\n        self.assertEqual(idx, 2)\n        self.assertAllClose(data[\'image\'].shape, (276, 178, 266, 1, 1))\n\n    def test_preprocessing_zero_padding(self):\n        reader = ImageReader([\'image\'])\n        reader.initialise(SINGLE_MOD_DATA, SINGLE_MOD_TASK, single_mod_list)\n        idx, data, interp_order = reader()\n        self.assertEqual(SINGLE_MOD_DATA[\'lesion\'].interp_order,\n                         interp_order[\'image\'][0])\n        self.assertAllClose(data[\'image\'].shape, (256, 168, 256, 1, 1))\n        reader.add_preprocessing_layers(\n            [PadLayer(image_name=[\'image\'], border=(0, 0, 0))])\n        idx, data, interp_order = reader(idx=2)\n        self.assertEqual(idx, 2)\n        self.assertAllClose(data[\'image\'].shape, (256, 168, 256, 1, 1))\n\n    def test_trainable_preprocessing(self):\n        label_file = os.path.join(\'testing_data\', \'label_reader.txt\')\n        if os.path.exists(label_file):\n            os.remove(label_file)\n        label_normaliser = DiscreteLabelNormalisationLayer(\n            image_name=\'label\',\n            modalities=vars(LABEL_TASK).get(\'label\'),\n            model_filename=os.path.join(\'testing_data\', \'label_reader.txt\'))\n        reader = ImageReader([\'label\'])\n        with self.assertRaisesRegexp(AssertionError, \'\'):\n            reader.add_preprocessing_layers(label_normaliser)\n        reader.initialise(LABEL_DATA, LABEL_TASK, label_list)\n        reader.add_preprocessing_layers(label_normaliser)\n        reader.add_preprocessing_layers(\n            [PadLayer(image_name=[\'label\'], border=(10, 5, 5))])\n        idx, data, interp_order = reader(idx=0)\n        unique_data = np.unique(data[\'label\'])\n        expected_v1 = np.array(\n            [0., 1., 2., 3., 4., 5., 6., 7., 8.,\n                9., 10., 11., 12., 13., 14., 15., 16., 17.,\n                18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n                28., 29., 30., 31., 32., 33., 34., 35., 36.,\n                37., 38., 39., 40., 41., 42., 43., 44., 45.,\n                46., 47., 48., 49., 50., 51., 52., 53., 54.,\n                55., 56., 57., 58., 59., 60., 61., 62., 63.,\n                64., 65., 66., 67., 68., 69., 70., 71., 72.,\n                73., 74., 75., 76., 77., 78., 79., 80., 81.,\n                82., 83., 84., 85., 86., 87., 88., 89., 90.,\n                91., 92., 93., 94., 95., 96., 97., 98., 99.,\n                100., 101., 102., 103., 104., 105., 106., 107., 108.,\n                109., 110., 111., 112., 113., 114., 115., 116., 117.,\n                118., 119., 120., 121., 122., 123., 124., 125., 126.,\n                127., 128., 129., 130., 131., 132., 133., 134., 135.,\n                136., 137., 138., 139., 140., 141., 142., 143., 144.,\n                145., 146., 147., 148., 149., 150., 151., 152., 153.,\n                154., 155., 156., 157.], dtype=np.float32)\n        expected_v2 = np.array(\n            [0., 1., 2., 3., 4., 5., 6., 7., 8.,\n                9., 10., 11., 12., 13., 14., 15., 16., 17.,\n                18., 20., 21., 22., 23., 24., 25., 26., 27.,\n                28., 29., 30., 31., 32., 33., 34., 35., 36.,\n                37., 38., 39., 40., 41., 42., 43., 44., 45.,\n                46., 47., 48., 49., 50., 51., 52., 53., 54.,\n                55., 56., 57., 58., 59., 60., 61., 62., 63.,\n                64., 65., 66., 67., 68., 69., 70., 71., 72.,\n                73., 74., 75., 76., 77., 78., 79., 80., 81.,\n                82., 83., 84., 85., 86., 87., 88., 89., 90.,\n                91., 92., 93., 94., 95., 96., 97., 98., 99.,\n                100., 101., 102., 103., 104., 105., 106., 107., 108.,\n                109., 110., 111., 112., 113., 114., 115., 116., 117.,\n                118., 119., 120., 121., 122., 123., 124., 125., 126.,\n                127., 128., 129., 130., 131., 132., 133., 134., 135.,\n                136., 137., 138., 139., 140., 141., 142., 143., 144.,\n                145., 146., 147., 148., 149., 150., 151., 152., 153.,\n                154., 155., 156., 157.], dtype=np.float32)\n        compatible_assert = \\\n            np.all(unique_data == expected_v1) or \\\n            np.all(unique_data == expected_v2)\n        self.assertTrue(compatible_assert)\n        self.assertAllClose(data[\'label\'].shape, (103, 74, 93, 1, 1))\n\n    def test_errors(self):\n        reader = ImageReader([\'image\'])\n        reader.initialise(BAD_DATA, SINGLE_MOD_TASK, bad_data_list)\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            reader = ImageReader([\'image\'])\n            reader.initialise(SINGLE_MOD_DATA, BAD_TASK, single_mod_list)\n\n        reader = ImageReader([\'image\'])\n        reader.initialise(SINGLE_MOD_DATA, SINGLE_MOD_TASK, single_mod_list)\n        idx, data, interp_order = reader(idx=100)\n        self.assertEqual(idx, -1)\n        self.assertEqual(data, None)\n        idx, data, interp_order = reader(shuffle=True)\n        self.assertEqual(data[\'image\'].shape, (256, 168, 256, 1, 1))\n\n    def test_images2d(self):\n        reader = ImageReader([\'image\'])\n\n        # COLOR IMAGES\n        reader.initialise(IMAGE_2D_DATA, IMAGE_2D_TASK_COLOR,\n                          image2d_data_list)\n\n        idx, data, interp_order = reader()\n        image = data[\'image\']\n        # Check index\n        self.assertGreaterEqual(idx, 0)\n        self.assertLess(idx, 10)\n        # Check data type\n        self.assertGreaterEqual(image.min(), 0)\n        self.assertLessEqual(image.max(), 255)\n        self.assertEqual(image.dtype, np.float32)\n        # Check shape\n        self.assertEqual(image.ndim, 5)\n        self.assertAllEqual(image.shape, (100, 100, 1, 1, 3))\n        self.assertEqual(interp_order[\'image\'], (1,))\n\n        # GRAY IMAGES\n        reader.initialise(IMAGE_2D_DATA, IMAGE_2D_TASK_GRAY,\n                          image2d_data_list)\n\n        idx, data, interp_order = reader()\n        image = data[\'image\']\n\n        # Check index\n        self.assertGreaterEqual(idx, 0)\n        self.assertLess(idx, 10)\n        # Check data type\n        self.assertGreaterEqual(image.min(), 0)\n        self.assertLessEqual(image.max(), 255)\n        self.assertEqual(image.dtype, np.float32)\n        # Check shape\n        self.assertEqual(image.ndim, 5)\n        self.assertAllEqual(image.shape, (100, 100, 1, 1, 1))\n        self.assertEqual(interp_order[\'image\'], (1,))\n\n        gray_idx, gray_data, gray_order = reader(idx=5)\n\n        # SEGMENTATION MASKS\n        reader.initialise(IMAGE_2D_DATA, IMAGE_2D_TASK_MASK,\n                          image2d_data_list)\n\n        idx, data, interp_order = reader()\n        image = data[\'image\']\n\n        # Check index\n        self.assertGreaterEqual(idx, 0)\n        self.assertLess(idx, 10)\n        # Check data type\n        self.assertGreaterEqual(image.min(), 0)\n        self.assertLessEqual(image.max(), 255)\n        self.assertEqual(image.dtype, np.float32)\n        self.assertEqual(np.unique(image).size, 2)\n        # Check shape\n        self.assertEqual(image.ndim, 5)\n        self.assertAllEqual(image.shape, (100, 100, 1, 1, 1))\n        self.assertEqual(interp_order[\'image\'], (0,))\n\n        # Compare segmentation masks to thresholding original image\n        mask_idx, mask_data, mask_order = reader(idx=5)\n\n        gray_data = gray_data[\'image\']\n        mask_data = mask_data[\'image\']\n\n        self.assertEqual(gray_idx, mask_idx)\n        self.assertEqual(gray_order[\'image\'], (1,))\n        self.assertEqual(mask_order[\'image\'], (0,))\n        self.assertAllEqual((gray_data > SEG_THRESHOLD) * 255, mask_data)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/image_sets_partitioner_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.io.image_sets_partitioner import COLUMN_UNIQ_ID\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.engine.signal import TRAIN, VALID, INFER\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\ntest_sections = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'test_reader.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contain=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        loader=None),\n\n    \'Flair\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'test_Flairreader.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        loader=None)}\n\npartition_output = os.path.join(\'testing_data\', \'partition.csv\')\n\n\nclass ImageSetsPartitionerTest(NiftyNetTestCase):\n    def test_no_partition_file(self):\n        if os.path.isfile(partition_output):\n            os.remove(partition_output)\n\n        data_param = test_sections\n        test_partitioner = ImageSetsPartitioner()\n        test_partitioner.initialise(\n            data_param,\n            new_partition=False,\n            data_split_file=partition_output)\n        self.assertEqual(\n            test_partitioner.get_file_list()[COLUMN_UNIQ_ID].count(), 4)\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            test_partitioner.get_file_list(TRAIN)\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            test_partitioner.get_file_list(VALID)\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            test_partitioner.get_file_list(INFER)\n\n\nclass ImageSetsPartitionerNewPartition(NiftyNetTestCase):\n    def test_new_partition(self):\n        data_param = test_sections\n        test_partitioner = ImageSetsPartitioner()\n        with self.assertRaisesRegexp(TypeError, \'\'):\n            test_partitioner.initialise(\n                data_param,\n                new_partition=True,\n                data_split_file=partition_output)\n        test_partitioner.initialise(\n            data_param,\n            new_partition=True,\n            ratios=(2.0, 2.0),\n            data_split_file=partition_output)\n        self.assertEqual(\n            test_partitioner.get_file_list()[COLUMN_UNIQ_ID].count(), 4)\n        self.assertEqual(\n            test_partitioner.get_file_list(TRAIN), None)\n        self.assertEqual(\n            test_partitioner.get_file_list(VALID)[COLUMN_UNIQ_ID].count(), 4)\n        self.assertEqual(\n            test_partitioner.get_file_list(INFER), None)\n        self.assertEqual(\n            test_partitioner.get_file_list(\n                VALID, \'T1\', \'Flair\')[COLUMN_UNIQ_ID].count(), 4)\n        self.assertEqual(\n            test_partitioner.get_file_list(\n                VALID, \'Flair\')[COLUMN_UNIQ_ID].count(), 4)\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            test_partitioner.get_file_list(VALID, \'foo\')\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            test_partitioner.get_file_list(\'T1\')\n\n        self.assertFalse(test_partitioner.has_training)\n        self.assertFalse(test_partitioner.has_inference)\n        self.assertTrue(test_partitioner.has_validation)\n\n\nclass ImageSetsPartitionerIllPartition(NiftyNetTestCase):\n    def test_incompatible_partition_file(self):\n        self._reset_partition_file()\n        # adding invalid line\n        with open(partition_output, \'a\') as partition_file:\n            partition_file.write(\'foo, bar\')\n        test_partitioner = ImageSetsPartitioner()\n        with self.assertRaisesRegexp(ValueError, """"):\n            test_partitioner.initialise(\n                test_sections,\n                new_partition=False,\n                data_split_file=partition_output)\n\n    def test_replicated_ids(self):\n        self._reset_partition_file()\n        with open(partition_output, \'a\') as partition_file:\n            partition_file.write(\'1065,Training\\n\')\n            partition_file.write(\'1065,Validation\')\n        test_partitioner = ImageSetsPartitioner()\n        test_partitioner.initialise(\n            test_sections,\n            new_partition=False,\n            data_split_file=partition_output)\n        self.assertEqual(\n            test_partitioner.get_file_list()[COLUMN_UNIQ_ID].count(), 4)\n        self.assertEqual(\n            test_partitioner.get_file_list(TRAIN)[COLUMN_UNIQ_ID].count(), 3)\n        self.assertEqual(\n            test_partitioner.get_file_list(VALID)[COLUMN_UNIQ_ID].count(), 2)\n        self.assertEqual(\n            test_partitioner.get_file_list(INFER)[COLUMN_UNIQ_ID].count(), 1)\n\n    def test_empty(self):\n        self._reset_partition_file()\n        with open(partition_output, \'w\') as partition_file:\n            partition_file.write(\'\')\n        test_partitioner = ImageSetsPartitioner()\n        with self.assertRaisesRegexp(ValueError, """"):\n            test_partitioner.initialise(\n                test_sections,\n                new_partition=False,\n                data_split_file=partition_output)\n\n    def _reset_partition_file(self):\n        test_partitioner = ImageSetsPartitioner()\n        test_partitioner.initialise(\n            test_sections,\n            new_partition=True,\n            ratios=(0.2, 0.2),\n            data_split_file=partition_output)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/image_type_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.io.image_type import ImageFactory\nfrom niftynet.io.image_type import SpatialImage2D\nfrom niftynet.io.image_type import SpatialImage3D\nfrom niftynet.io.image_type import SpatialImage4D\nfrom niftynet.io.image_type import SpatialImage5D\nfrom niftynet.io.misc_io import set_logger\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nCASE_2D = \'testing_data/2d_3_000044.nii.gz\'\nCASE_3D_a = \'testing_data/1040_o_T1_time_01.nii.gz\'\nCASE_3D_b = \'testing_data/1023_o_T1_time_01.nii.gz\'\nCASE_5D = \'testing_data/pat2__niftynet_out.nii.gz\'\n\n\nclass ImageTypeTest(NiftyNetTestCase):\n    def test_2d(self):\n        image = ImageFactory.create_instance(\n            file_path=CASE_2D,\n            name=\'2d_image\',\n            interp_order=3,\n            output_pixdim=None,\n            output_axcodes=None,\n            loader=None)\n        self.assertIsInstance(image, SpatialImage2D)\n        output = image.get_data()\n        self.assertEqual(np.dtype(np.float32), image.dtype[0])\n        self.assertEqual(2, image.spatial_rank)\n        self.assertAllClose(np.array([128, 128, 1, 1, 1]), output.shape)\n        self.assertAllClose(np.array([128, 128, 1, 1, 1]), image.shape)\n\n    def test_3d(self):\n        image = ImageFactory.create_instance(\n            file_path=CASE_3D_a,\n            name=\'3d_image\',\n            interp_order=3,\n            output_pixdim=None,\n            output_axcodes=None,\n            loader=None)\n        self.assertIsInstance(image, SpatialImage3D)\n        self.assertEqual(3, image.spatial_rank)\n        output = image.get_data()\n        self.assertAllClose(np.array([256, 168, 256, 1, 1]), output.shape)\n        self.assertAllClose(np.array([256, 168, 256, 1, 1]), image.shape)\n\n    def test_3d_reorientation(self):\n        image = ImageFactory.create_instance(\n            file_path=CASE_3D_a,\n            name=\'3d_image\',\n            interp_order=3,\n            output_pixdim=None,\n            output_axcodes=\'ALS\',\n            loader=None)\n        self.assertIsInstance(image, SpatialImage3D)\n        self.assertEqual(3, image.spatial_rank)\n        output = image.get_data()\n        self.assertAllClose(np.array([168, 256, 256, 1, 1]), output.shape)\n        self.assertAllClose(np.array([168, 256, 256, 1, 1]), image.shape)\n\n    def test_3d_resample(self):\n        image = ImageFactory.create_instance(\n            file_path=CASE_3D_a,\n            name=\'3d_image\',\n            interp_order=3,\n            output_pixdim=(3.5, 0.9, 8),\n            output_axcodes=(None,),\n            loader=None)\n        self.assertIsInstance(image, SpatialImage3D)\n        self.assertEqual(3, image.spatial_rank)\n        output = image.get_data()\n        self.assertAllClose(np.array([71, 278, 31, 1, 1]), output.shape)\n        self.assertAllClose(np.array([71, 278, 31, 1, 1]), image.shape)\n\n    def test_3d_resample_reorientation(self):\n        image = ImageFactory.create_instance(\n            file_path=CASE_3D_a,\n            name=\'3d_image\',\n            interp_order=3,\n            output_pixdim=((7, 1.2, 8),),\n            output_axcodes=(\'ASL\',),\n            loader=None)\n        self.assertIsInstance(image, SpatialImage3D)\n        output = image.get_data()\n        self.assertAllClose(np.array([36, 208, 31, 1, 1]), output.shape)\n        self.assertAllClose(np.array([36, 208, 31, 1, 1]), image.shape)\n\n    def test_multiple_3d_as_4d(self):\n        image = ImageFactory.create_instance(\n            file_path=(CASE_3D_a, CASE_3D_b),\n            name=(\'3d_image_a\', \'3d_image_b\'),\n            interp_order=(3, 3),\n            output_pixdim=None,\n            output_axcodes=(None, None),\n            loader=(None, None))\n        self.assertIsInstance(image, SpatialImage4D)\n        self.assertEqual(3, image.spatial_rank)\n        output = image.get_data()\n        self.assertAllClose(np.array([256, 168, 256, 1, 2]), output.shape)\n        self.assertAllClose(np.array([256, 168, 256, 1, 2]), image.shape)\n\n    def test_multiple_3d_as_4d_resample(self):\n        with self.assertRaisesRegexp(ValueError, \'concatenation\'):\n            image = ImageFactory.create_instance(\n                file_path=(CASE_3D_a, CASE_3D_b),\n                name=(\'3d_image_a\', \'3d_image_b\'),\n                interp_order=(3, 3),\n                output_pixdim=((5, 4, 10.0), (2, 4, 1.0)),\n                output_axcodes=(None, None),\n                loader=(None, None))\n            _ = image.get_data()\n        image = ImageFactory.create_instance(\n            file_path=(CASE_3D_a, CASE_3D_b),\n            name=(\'3d_image_a\', \'3d_image_b\'),\n            interp_order=(3, 3),\n            output_pixdim=(2.6, 0.9, 5),\n            output_axcodes=(None, None),\n            loader=(None, None))\n        self.assertIsInstance(image, SpatialImage4D)\n        self.assertEqual(3, image.spatial_rank)\n        output = image.get_data()\n        self.assertAllClose(np.array([96, 278, 50, 1, 2]), output.shape)\n        self.assertAllClose(np.array([96, 278, 50, 1, 2]), image.shape)\n\n    def test_multiple_3d_as_4d_reorientation(self):\n        with self.assertRaisesRegexp(ValueError, \'concatenation\'):\n            image = ImageFactory.create_instance(\n                file_path=(CASE_3D_a, CASE_3D_b),\n                name=(\'3d_image_a\', \'3d_image_b\'),\n                interp_order=(0, 3),\n                output_pixdim=(None, None),\n                output_axcodes=((\'A\', \'S\', \'L\'), (\'L\', \'S\', \'A\')),\n                loader=(None, None))\n            _ = image.get_data()\n        image = ImageFactory.create_instance(\n            file_path=(CASE_3D_a, CASE_3D_b),\n            name=(\'3d_image_a\', \'3d_image_b\'),\n            interp_order=(3, 0),\n            output_pixdim=(None, None),\n            output_axcodes=((\'A\', \'S\', \'L\'), (\'A\', \'S\', \'L\')),\n            loader=(None, None))\n        self.assertIsInstance(image, SpatialImage4D)\n        output = image.get_data()\n        self.assertAllClose(np.array([168, 256, 256, 1, 2]), output.shape)\n        self.assertAllClose(np.array([168, 256, 256, 1, 2]), image.shape)\n\n    def test_multiple_3d_as_4d_resample_reorientation(self):\n        with self.assertRaisesRegexp(ValueError, \'concatenation\'):\n            image = ImageFactory.create_instance(\n                file_path=(CASE_3D_a, CASE_3D_b),\n                name=(\'3d_image_a\', \'3d_image_b\'),\n                interp_order=(3, 0),\n                output_pixdim=((8.0, 6.0, 2.0), (7.0, 6.0, 10.0)),\n                output_axcodes=((\'A\', \'S\', \'L\'), (\'L\', \'S\', \'A\')),\n                loader=(None, None))\n            _ = image.get_data()\n        image = ImageFactory.create_instance(\n            file_path=(CASE_3D_a, CASE_3D_b),\n            name=(\'3d_image_a\', \'3d_image_b\'),\n            interp_order=(3, 3),\n            output_pixdim=((5, 2.0, 6), (5, 2.0, 6)),\n            output_axcodes=\'ASL\',\n            loader=(None, None))\n        self.assertIsInstance(image, SpatialImage4D)\n        output = image.get_data()\n        self.assertAllClose(np.array([50, 125, 42, 1, 2]), output.shape)\n        self.assertAllClose(np.array([50, 125, 42, 1, 2]), image.shape)\n\n    def test_5d(self):\n        image = ImageFactory.create_instance(\n            file_path=CASE_5D,\n            name=\'5d_image\',\n            interp_order=3,\n            output_pixdim=None,\n            output_axcodes=None,\n            loader=None)\n        self.assertIsInstance(image, SpatialImage5D)\n        self.assertEqual(3, image.spatial_rank)\n        output = image.get_data()\n        self.assertAllClose(np.array([208, 256, 256, 1, 1]), output.shape)\n        self.assertAllClose(np.array([208, 256, 256, 1, 1]), image.shape)\n\n    def test_5d_resample(self):\n        image = ImageFactory.create_instance(\n            file_path=CASE_5D,\n            name=\'5d_image\',\n            interp_order=1,\n            output_pixdim=(4, 8, 6.0),\n            output_axcodes=None,\n            loader=None)\n        self.assertIsInstance(image, SpatialImage5D)\n        output = image.get_data()\n        self.assertAllClose(np.array([58, 35, 49, 1, 1]), output.shape)\n        self.assertAllClose(np.array([58, 35, 49, 1, 1]), image.shape)\n\n    def test_5d_reorientation(self):\n        image = ImageFactory.create_instance(\n            file_path=(CASE_5D,),\n            name=\'5d_image\',\n            interp_order=3,\n            output_pixdim=(None,),\n            output_axcodes=((\'S\', \'A\', \'L\'),),\n            loader=(None,))\n        self.assertIsInstance(image, SpatialImage5D)\n        output = image.get_data()\n        self.assertAllClose(np.array([256, 256, 208, 1, 1]), output.shape)\n        self.assertAllClose(np.array([256, 256, 208, 1, 1]), image.shape)\n\n    def test_5d_reorientation_resample(self):\n        image = ImageFactory.create_instance(\n            file_path=(CASE_5D,),\n            name=\'5d_image\',\n            interp_order=3,\n            output_pixdim=((8, 9, 10),),\n            output_axcodes=(\'RSA\',),\n            loader=(None,))\n        self.assertIsInstance(image, SpatialImage5D)\n        output = image.get_data()\n        self.assertAllClose(np.array([29, 33, 28, 1, 1]), output.shape)\n        self.assertAllClose(np.array([29, 33, 28, 1, 1]), image.shape)\n\n\nif __name__ == ""__main__"":\n    set_logger()\n    tf.test.main()\n'"
tests/image_window_dataset_generator_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.io.image_reader import ImageReader\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nIMAGE_PATH_2D_1 = os.path.join(\'.\', \'example_volumes\', \'gan_test_data\')\nIMAGE_PATH_3D = os.path.join(\'.\', \'testing_data\')\n\n\ndef get_2d_reader():\n    data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D_1}}\n    reader = ImageReader().initialise(data_param)\n    return reader\n\n\ndef get_3d_reader():\n    data_param = {\n        \'mr\': {\n            \'path_to_search\': IMAGE_PATH_3D,\n            \'filename_contains\': \'FLAIR\',\n            \'interp_order\': 1}}\n    reader = ImageReader().initialise(data_param)\n    return reader\n\n\nclass ImageWindowGenerator(ImageWindowDataset):\n    """"""\n    simple test class, replace ImageWindowDataset\'s layer_op\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        ImageWindowDataset.__init__(self, *args, **kwargs)\n\n    def layer_op(self):\n        for idx in range(self.n_subjects):\n            yield super(ImageWindowGenerator, self).layer_op(idx)\n        # for idx in range(self.n_subjects):\n        #     image_id, image_data, _ = self.reader(idx=idx)\n        #     for mod in list(image_data):\n        #         spatial_shape = image_data[mod].shape[:N_SPATIAL]\n        #         coords = self.dummy_coordinates(image_id, spatial_shape, 1)\n        #         image_data[LOCATION_FORMAT.format(mod)] = coords\n        #         image_data[mod] = image_data[mod][np.newaxis, ...]\n        #     yield image_data\n\n\n@unittest.skip(""temp skipping window generator test"")\nclass ImageWindowDataset_Generator_2D_Test(NiftyNetTestCase):\n    def assert_window(self, window):\n        if not isinstance(window, dict):\n            window = next(window)\n        self.assertEqual(window[\'mr\'].shape[1:3], (120, 160))\n        self.assertEqual(window[\'mr_location\'][0, 1:].tolist(),\n                         [0, 0, 0, 120, 160, 1])\n        self.assertEqual(window[\'mr\'].dtype, np.float32)\n        self.assertEqual(window[\'mr_location\'].dtype, np.int32)\n\n    def assert_tf_window(self, sampler):\n        with self.cached_session() as sess:\n            window = sess.run(sampler.pop_batch_op())\n        self.assert_window(window)\n\n    def test_simple(self):\n        sampler = ImageWindowGenerator(reader=get_2d_reader())\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_batch_size(self):\n        # batch size doesn\'t change the numpy interface\n        sampler = ImageWindowGenerator(reader=get_2d_reader(), batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_window_size(self):\n        sampler = ImageWindowGenerator(\n            reader=get_2d_reader(),\n            window_sizes=(0, 0, 0), batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_window_size_dict(self):\n        sampler = ImageWindowGenerator(\n            reader=get_2d_reader(),\n            window_sizes={\'mr\': (0, 0, 0)},\n            batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    # # sampler layer_op()\'s output shape is not checked\n    # def test_wrong_window_size_dict(self):\n    #     sampler = ImageWindowGenerator(\n    #         reader=get_2d_reader(),\n    #         batch_size=2,\n    #         window_sizes=(3, 3, 0))\n    #     self.assert_tf_window(sampler)\n\n    def test_windows_per_image(self):\n        sampler = ImageWindowGenerator(\n            reader=get_2d_reader(), batch_size=2, windows_per_image=2)\n        self.assert_window(sampler())\n\n    def test_epoch(self):\n        reader = get_2d_reader()\n        batch_size = 3\n        sampler = ImageWindowGenerator(\n            reader=reader, batch_size=batch_size, epoch=1)\n        with self.cached_session() as sess:\n            next_element = sampler.pop_batch_op()\n            iters = 0\n            try:\n                for _ in range(400):\n                    window = sess.run(next_element)\n                    iters = iters + 1\n            except tf.errors.OutOfRangeError:\n                pass\n            # batch size 3, 40 images in total\n            self.assertEqual(\n                np.ceil(reader.num_subjects / np.float(batch_size)), iters)\n\n\n@unittest.skip(""temp skipping window generator test"")\nclass ImageWindowDataset_Generator_3D_Test(NiftyNetTestCase):\n    def assert_window(self, window):\n        if not isinstance(window, dict):\n            window = next(window)\n        self.assertEqual(window[\'mr\'].shape[1:4], (256, 168, 256))\n        self.assertEqual(window[\'mr_location\'][0, 1:].tolist(),\n                         [0, 0, 0, 256, 168, 256])\n        self.assertEqual(window[\'mr\'].dtype, np.float32)\n        self.assertEqual(window[\'mr_location\'].dtype, np.int32)\n\n    def assert_tf_window(self, sampler):\n        with self.cached_session() as sess:\n            window = sess.run(sampler.pop_batch_op())\n        self.assert_window(window)\n\n    def test_simple(self):\n        sampler = ImageWindowGenerator(reader=get_3d_reader())\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_batch_size(self):\n        # batch size doesn\'t change the numpy interface\n        sampler = ImageWindowGenerator(reader=get_3d_reader(), batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_window_size(self):\n        sampler = ImageWindowGenerator(\n            reader=get_3d_reader(),\n            window_sizes=(0, 0, 0), batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_window_size_dict(self):\n        sampler = ImageWindowGenerator(\n            reader=get_3d_reader(),\n            window_sizes={\'mr\': (0, 0, 0)},\n            batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_windows_per_image(self):\n        sampler = ImageWindowGenerator(\n            reader=get_3d_reader(), batch_size=2,\n            windows_per_image=2)\n        self.assert_window(sampler())\n\n    def test_epoch(self):\n        reader = get_3d_reader()\n        batch_size = 3\n        sampler = ImageWindowGenerator(\n            reader=reader, batch_size=batch_size, epoch=1)\n        with self.cached_session() as sess:\n            next_element = sampler.pop_batch_op()\n            iters = 0\n            try:\n                for _ in range(400):\n                    window = sess.run(next_element)\n                    iters = iters + 1\n            except tf.errors.OutOfRangeError:\n                pass\n            # batch size 3, 4 images in total\n            self.assertEqual(\n                np.ceil(reader.num_subjects / np.float(batch_size)), iters)\n\n\n@unittest.skip(""temp skipping window generator test"")\nclass ImageDatasetParamTest(NiftyNetTestCase):\n    def run_dataset(self, n_iters, n_threads, **kwargs):\n        sampler = ImageWindowGenerator(**kwargs)\n        sampler.set_num_threads(n_threads)\n        with self.cached_session() as sess:\n            true_iters = 0\n            next_element = sampler.pop_batch_op()\n            windows = []\n            try:\n                for _ in range(min(n_iters, 100)):\n                    windows.append(\n                        sess.run(next_element)[\'mr_location\'])\n                    true_iters = true_iters + 1\n            except (tf.errors.OutOfRangeError, EOFError):\n                pass\n            assert true_iters <= 100, \'keep the test smaller than 100 iters\'\n        return true_iters, np.concatenate(windows, 0)\n\n    def test_function(self):\n        reader = get_2d_reader()\n        #### with default batch padding\n        n_iters, windows = self.run_dataset(\n            n_iters=2,\n            n_threads=4,\n            reader=reader,\n            batch_size=100,\n            smaller_final_batch_mode=\'pad\',\n            windows_per_image=1,\n            epoch=4)\n        # elements: 4 * 40, batch size 100, resulting 2 batches\n        self.assertEqual(n_iters, 2)\n        self.assertEqual(windows.shape[0], 200)\n        # all subjects evaluated\n        uniq, counts = np.unique(windows[:, 0], return_counts=True)\n        self.assertEqual(len(uniq), 41)\n        self.assertTrue(np.all(counts[1:] == 4))\n\n        #### with drop batch\n        n_iters, windows = self.run_dataset(\n            n_iters=2,\n            n_threads=3,\n            reader=reader,\n            batch_size=100,\n            smaller_final_batch_mode=\'drop\',\n            epoch=3)\n        # elements: 4 * 40, batch size 100, resulting 1 batches\n        self.assertEqual(n_iters, 1)\n        self.assertEqual(windows.shape[0], 100)\n        # all subjects evaluated, might not get all unique items\n        # self.assertEqual(len(np.unique(windows[:, 0])), 40)\n\n        #### with drop batch\n        n_iters, windows = self.run_dataset(\n            n_iters=2,\n            n_threads=4,\n            reader=reader,\n            batch_size=100,\n            queue_length=100,\n            smaller_final_batch_mode=\'dynamic\',\n            epoch=4)\n        # elements: 4 * 40, batch size 100, resulting 2 batches\n        self.assertEqual(n_iters, 2)\n        self.assertEqual(windows.shape[0], 160)\n        # all subjects evaluated\n        uniq, counts = np.unique(windows[:, 0], return_counts=True)\n        self.assertEqual(len(uniq), 40)\n        self.assertTrue(np.all(counts == 4))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/image_window_dataset_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.io.image_reader import ImageReader\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nIMAGE_PATH_2D_1 = os.path.join(\'.\', \'example_volumes\', \'gan_test_data\')\nIMAGE_PATH_3D = os.path.join(\'.\', \'testing_data\')\n\n\ndef get_2d_reader():\n    data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D_1}}\n    reader = ImageReader().initialise(data_param)\n    return reader\n\n\ndef get_3d_reader():\n    data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_3D,\n        \'filename_contains\': \'FLAIR\',\n        \'interp_order\': 1}}\n    reader = ImageReader().initialise(data_param)\n    return reader\n\n\nclass ImageWindowDataset_2D_Test(NiftyNetTestCase):\n    def assert_window(self, window):\n        if not isinstance(window, dict):\n            window = next(window)\n        self.assertEqual(window[\'mr\'].shape[1:3], (120, 160))\n        self.assertEqual(window[\'mr_location\'][0, 1:].tolist(),\n                         [0, 0, 0, 120, 160, 1])\n        self.assertEqual(window[\'mr\'].dtype, np.float32)\n        self.assertEqual(window[\'mr_location\'].dtype, np.int32)\n\n    def assert_tf_window(self, sampler):\n        with self.cached_session() as sess:\n            window = sess.run(sampler.pop_batch_op())\n        self.assert_window(window)\n\n    def test_simple(self):\n        sampler = ImageWindowDataset(reader=get_2d_reader())\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_batch_size(self):\n        # batch size doesn\'t change the numpy interface\n        sampler = ImageWindowDataset(reader=get_2d_reader(), batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_window_size(self):\n        sampler = ImageWindowDataset(reader=get_2d_reader(),\n                                     window_sizes=(0, 0, 0), batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_window_size_dict(self):\n        sampler = ImageWindowDataset(reader=get_2d_reader(),\n                                     window_sizes={\'mr\': (0, 0, 0)},\n                                     batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    # # sampler layer_op()\'s output shape is not checked\n    # def test_wrong_window_size_dict(self):\n    #    sampler = ImageWindowDataset(reader=get_2d_reader(),\n    #                                 batch_size=2,\n    #                                 window_sizes=(3,3,0))\n    #    self.assert_tf_window(sampler)\n\n    def test_windows_per_image(self):\n        sampler = ImageWindowDataset(reader=get_2d_reader(), batch_size=2,\n                                     windows_per_image=2)\n        self.assert_window(sampler())\n\n    def test_epoch(self):\n        reader = get_2d_reader()\n        batch_size = 3\n        sampler = ImageWindowDataset(\n            reader=reader, batch_size=batch_size, epoch=1)\n        with self.cached_session() as sess:\n            next_element = sampler.pop_batch_op()\n            iters = 0\n            try:\n                for _ in range(400):\n                    window = sess.run(next_element)\n                    iters = iters + 1\n            except tf.errors.OutOfRangeError:\n                pass\n            # batch size 3, 40 images in total\n            self.assertEqual(\n                np.ceil(reader.num_subjects / np.float(batch_size)), iters)\n\n\nclass ImageWindowDataset_3D_Test(NiftyNetTestCase):\n    def assert_window(self, window):\n        if not isinstance(window, dict):\n            window = next(window)\n        self.assertEqual(window[\'mr\'].shape[1:4], (256, 168, 256))\n        self.assertEqual(window[\'mr_location\'][0, 1:].tolist(),\n                         [0, 0, 0, 256, 168, 256])\n        self.assertEqual(window[\'mr\'].dtype, np.float32)\n        self.assertEqual(window[\'mr_location\'].dtype, np.int32)\n\n    def assert_tf_window(self, sampler):\n        with self.cached_session() as sess:\n            window = sess.run(sampler.pop_batch_op())\n        self.assert_window(window)\n\n    def test_simple(self):\n        sampler = ImageWindowDataset(reader=get_3d_reader())\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_batch_size(self):\n        # batch size doesn\'t change the numpy interface\n        sampler = ImageWindowDataset(reader=get_3d_reader(), batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_window_size(self):\n        sampler = ImageWindowDataset(reader=get_3d_reader(),\n                                     window_sizes=(0, 0, 0), batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_window_size_dict(self):\n        sampler = ImageWindowDataset(reader=get_3d_reader(),\n                                     window_sizes={\'mr\': (0, 0, 0)},\n                                     batch_size=2)\n        self.assert_tf_window(sampler)\n        self.assert_window(sampler())\n\n    def test_windows_per_image(self):\n        sampler = ImageWindowDataset(reader=get_3d_reader(), batch_size=2,\n                                     windows_per_image=2)\n        self.assert_window(sampler())\n\n    def test_epoch(self):\n        reader = get_3d_reader()\n        batch_size = 3\n        sampler = ImageWindowDataset(\n            reader=reader, batch_size=batch_size, epoch=1)\n        with self.cached_session() as sess:\n            next_element = sampler.pop_batch_op()\n            iters = 0\n            try:\n                for _ in range(400):\n                    window = sess.run(next_element)\n                    iters = iters + 1\n            except tf.errors.OutOfRangeError:\n                pass\n            # batch size 3, 4 images in total\n            self.assertEqual(\n                np.ceil(reader.num_subjects / np.float(batch_size)), iters)\n\n\nclass ImageDatasetParamTest(NiftyNetTestCase):\n    def run_dataset(self, n_iters, n_threads, **kwargs):\n        sampler = ImageWindowDataset(**kwargs)\n        sampler.set_num_threads(n_threads)\n        with self.cached_session() as sess:\n            true_iters = 0\n            next_element = sampler.pop_batch_op()\n            windows = []\n            try:\n                for _ in range(min(n_iters, 100)):\n                    windows.append(\n                        sess.run(next_element)[\'mr_location\'])\n                    true_iters = true_iters + 1\n            except (tf.errors.OutOfRangeError, EOFError):\n                pass\n            assert true_iters <= 100, \'keep the test smaller than 100 iters\'\n        return true_iters, np.concatenate(windows, 0)\n\n    def test_function(self):\n        reader = get_2d_reader()\n        #### with default batch padding\n        n_iters, windows = self.run_dataset(\n            n_iters=2,\n            n_threads=4,\n            reader=reader,\n            batch_size=100,\n            smaller_final_batch_mode=\'pad\',\n            epoch=4)\n        # elements: 4 * 40, batch size 100, resulting 2 batches\n        self.assertEqual(n_iters, 2)\n        self.assertEqual(windows.shape[0], 200)\n        # all subjects evaluated\n        uniq, counts = np.unique(windows[:, 0], return_counts=True)\n        self.assertEqual(len(uniq), 41)\n        self.assertTrue(np.all(counts[1:] == 4))\n\n        #### with drop batch\n        n_iters, windows = self.run_dataset(\n            n_iters=2,\n            n_threads=3,\n            reader=reader,\n            batch_size=100,\n            smaller_final_batch_mode=\'drop\',\n            epoch=3)\n        # elements: 4 * 40, batch size 100, resulting 1 batches\n        self.assertEqual(n_iters, 1)\n        self.assertEqual(windows.shape[0], 100)\n        # all subjects evaluated, might not get all unique items\n        # self.assertEqual(len(np.unique(windows[:, 0])), 40)\n\n        #### with drop batch\n        n_iters, windows = self.run_dataset(\n            n_iters=2,\n            n_threads=4,\n            reader=reader,\n            batch_size=100,\n            queue_length=100,\n            smaller_final_batch_mode=\'dynamic\',\n            epoch=4)\n        # elements: 4 * 40, batch size 100, resulting 2 batches\n        self.assertEqual(n_iters, 2)\n        self.assertEqual(windows.shape[0], 160)\n        # all subjects evaluated\n        uniq, counts = np.unique(windows[:, 0], return_counts=True)\n        self.assertEqual(len(uniq), 40)\n        self.assertTrue(np.all(counts == 4))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/image_window_test.py,21,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window import ImageWindow\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\ndef get_static_window_param():\n    return dict(\n        source_names={\n            \'image\': (u\'modality1\', u\'modality2\'),\n            \'label\': (u\'modality3\',)},\n        image_shapes={\n            \'image\': (192, 160, 192, 1, 2),\n            \'label\': (192, 160, 192, 1, 1)},\n        image_dtypes={\n            \'image\': tf.float32,\n            \'label\': tf.float32},\n        window_sizes={\n            \'modality1\': ParserNamespace(spatial_window_size=(10, 10, 2)),\n            \'modality2\': (10, 10, 2),\n            \'modality3\': (5, 5, 1)}\n    )\n\n\ndef get_dynamic_image_window():\n    return dict(\n        source_names={\n            \'image\': (u\'modality1\', u\'modality2\'),\n            \'label\': (u\'modality3\',)},\n        image_shapes={\n            \'image\': (192, 160, 192, 1, 2),\n            \'label\': (192, 160, 192, 1, 1)},\n        image_dtypes={\n            \'image\': tf.float32,\n            \'label\': tf.float32},\n        window_sizes={\n            \'modality1\': ParserNamespace(spatial_window_size=(10, 10)),\n            \'modality2\': ParserNamespace(spatial_window_size=(10, 10)),\n            \'modality3\': ParserNamespace(spatial_window_size=(2,))},\n        allow_dynamic=True\n    )\n\n\ndef get_all_dynamic_image_window():\n    return dict(\n        source_names={\n            \'image\': (u\'modality1\', u\'modality2\'),\n            \'label\': (u\'modality3\',)},\n        image_shapes={\n            \'image\': (192, 160, 192, 1, 2),\n            \'label\': (192, 160, 192, 1, 1)},\n        image_dtypes={\n            \'image\': tf.float32,\n            \'label\': tf.float32},\n        window_sizes={\n            \'modality1\': ParserNamespace(spatial_window_size=(-1, -1)),\n            \'modality2\': ParserNamespace(spatial_window_size=(-1, -1)),\n            \'modality3\': ParserNamespace(spatial_window_size=(-1,))},\n        allow_dynamic=True\n    )\n\n\ndef get_ill_image_window():\n    return dict(\n        source_names={\n            \'image\': (u\'modality1\',),\n            \'label\': (u\'modality3\',)},\n        image_shapes={\n            \'image\': (192, 160, 192, 1, 2),\n            \'label\': (192, 160, 192, 1, 1)},\n        image_dtypes={\n            \'image\': tf.float32,\n            \'label\': tf.float32},\n        window_sizes={\n            \'modality1\': ParserNamespace(spatial_window_size=(10, 10)),\n            \'modality3\': ParserNamespace(spatial_window_size=())}\n    )\n\n\ndef get_ill_image_window_1():\n    return dict(\n        source_names={\n            \'image\': (u\'modality1\', u\'modality2\'),\n            \'label\': (u\'modality3\',)},\n        image_shapes={\n            \'image\': (192, 160, 192, 1, 2),\n            \'label\': (192, 160, 192, 1, 1)},\n        image_dtypes={\n            \'image\': tf.float32,\n            \'label\': tf.float32},\n        window_sizes={\n            \'modality1\': ParserNamespace(spatial_window_size=(10, 10)),\n            \'modality2\': ParserNamespace(spatial_window_size=(10, 10)),\n            \'modality3\': ParserNamespace(spatial_window_size=())}\n    )\n\n\ndef get_ill_image_window_2():\n    return dict(\n        source_names={\n            \'image\': (u\'modality1\', u\'modality2\'),\n            \'label\': (u\'modality3\',)},\n        image_shapes={\n            \'image\': (192, 160, 192, 1, 2),\n            \'label\': (192, 160, 192, 1, 1)},\n        image_dtypes={\n            \'image\': tf.float32},\n        window_sizes={\n            \'modality1\': ParserNamespace(spatial_window_size=(10, 10)),\n            \'modality2\': ParserNamespace(spatial_window_size=(10, 10)),\n            \'modality3\': ParserNamespace(spatial_window_size=())}\n    )\n\n\n# def get_ill_image_window_3():\n#    return dict(\n#        source_names={\n#            \'image\': (u\'modality1\', u\'modality2\'),\n#            \'label\': (u\'modality3\',)},\n#        image_shapes={\n#            \'image\': (192, 160, 192, 1, 2),\n#            \'label\': (192, 160, 192, 1, 1)},\n#        image_dtypes={\n#            \'image\': tf.float32},\n#        data_param={\n#            \'modality1\': ParserNamespace(spatial_window_size=(10, 10)),\n#            \'modality2\': ParserNamespace(spatial_window_size=(10, 10)),\n#            \'modality3\': ParserNamespace(spatial_window_size=())}\n#\n#    )\n\n\nclass ImageWindowTest(NiftyNetTestCase):\n    def test_init(self):\n        window = ImageWindow.from_data_reader_properties(\n            **get_static_window_param())\n        self.assertAllEqual(\n            window.placeholders_dict(1)[\'image\'].shape.as_list(),\n            [1, 10, 10, 2, 1, 2])\n        self.assertAllEqual(\n            window.placeholders_dict(1)[\'label\'].shape.as_list(),\n            [1, 5, 5, 1, 1, 1])\n\n        window = ImageWindow.from_data_reader_properties(\n            **get_dynamic_image_window())\n        self.assertAllEqual(\n            window.placeholders_dict(1)[\'image\'].shape.as_list(),\n            [1, 10, 10, None, 1, 2])\n        self.assertAllEqual(\n            window.placeholders_dict(1)[\'label\'].shape.as_list(),\n            [1, 2, None, None, 1, 1])\n\n        window = ImageWindow.from_data_reader_properties(\n            **get_all_dynamic_image_window())\n        self.assertAllEqual(\n            window.placeholders_dict(1)[\'image\'].shape.as_list(),\n            [1, None, None, None, 1, 2])\n        self.assertAllEqual(\n            window.placeholders_dict(1)[\'label\'].shape.as_list(),\n            [1, None, None, None, 1, 1])\n\n    def test_ill_cases(self):\n        with self.assertRaisesRegexp(ValueError, """"):\n            ImageWindow.from_data_reader_properties(\n                **get_ill_image_window())\n\n        with self.assertRaisesRegexp(ValueError, """"):\n            ImageWindow.from_data_reader_properties(\n                **get_ill_image_window_1())\n\n        with self.assertRaisesRegexp(ValueError, """"):\n            ImageWindow.from_data_reader_properties(\n                **get_ill_image_window_2())\n\n    def test_matching_image_shapes(self):\n        to_match = {\n            \'image\': (42, 43, 44, 1, 2),\n            \'label\': (42, 43, 44, 1, 2)}\n\n        new_shape = ImageWindow.from_data_reader_properties(\n            **get_static_window_param()).match_image_shapes(to_match)\n        self.assertAllEqual(new_shape[\'image\'], [10, 10, 2, 1, 2])\n        self.assertAllEqual(new_shape[\'label\'], [5, 5, 1, 1, 1])\n\n        new_shape = ImageWindow.from_data_reader_properties(\n            **get_dynamic_image_window()).match_image_shapes(to_match)\n        self.assertAllEqual(new_shape[\'image\'], [10, 10, 44, 1, 2])\n        self.assertAllEqual(new_shape[\'label\'], [2, 43, 44, 1, 1])\n\n        new_shape = ImageWindow.from_data_reader_properties(\n            **get_all_dynamic_image_window()).match_image_shapes(to_match)\n        self.assertAllEqual(new_shape[\'image\'], [42, 43, 44, 1, 2])\n        self.assertAllEqual(new_shape[\'label\'], [42, 43, 44, 1, 1])\n\n    def test_placeholders(self):\n        window = ImageWindow.from_data_reader_properties(\n            **get_static_window_param())\n        window.placeholders_dict(10)\n\n        self.assertAllEqual(\n            window.image_data_placeholder(\'image\').shape.as_list(),\n            [10, 10, 10, 2, 1, 2])\n        self.assertAllEqual(\n            window.image_data_placeholder(\'image\').dtype,\n            window.tf_dtypes[\'image\'])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'image\').shape.as_list(),\n            [10, 7])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'image\').dtype, tf.int32)\n\n        self.assertAllEqual(\n            window.image_data_placeholder(\'label\').shape.as_list(),\n            [10, 5, 5, 1, 1, 1])\n        self.assertAllEqual(\n            window.image_data_placeholder(\'label\').dtype,\n            window.tf_dtypes[\'label\'])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'label\').shape.as_list(),\n            [10, 7])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'label\').dtype, tf.int32)\n\n        window = ImageWindow.from_data_reader_properties(\n            **get_dynamic_image_window())\n        window.placeholders_dict(10)\n\n        self.assertAllEqual(\n            window.image_data_placeholder(\'image\').shape.as_list(),\n            [1, 10, 10, None, 1, 2])\n        self.assertAllEqual(\n            window.image_data_placeholder(\'image\').dtype,\n            window.tf_dtypes[\'image\'])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'image\').shape.as_list(),\n            [1, 7])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'image\').dtype, tf.int32)\n\n        self.assertAllEqual(\n            window.image_data_placeholder(\'label\').shape.as_list(),\n            [1, 2, None, None, 1, 1])\n        self.assertAllEqual(\n            window.image_data_placeholder(\'label\').dtype,\n            window.tf_dtypes[\'label\'])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'label\').shape.as_list(),\n            [1, 7])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'label\').dtype, tf.int32)\n\n        window = ImageWindow.from_data_reader_properties(\n            **get_all_dynamic_image_window())\n        window.placeholders_dict(10)\n\n        self.assertAllEqual(\n            window.image_data_placeholder(\'image\').shape.as_list(),\n            [1, None, None, None, 1, 2])\n        self.assertAllEqual(\n            window.image_data_placeholder(\'image\').dtype,\n            window.tf_dtypes[\'image\'])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'image\').shape.as_list(),\n            [1, 7])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'image\').dtype, tf.int32)\n\n        self.assertAllEqual(\n            window.image_data_placeholder(\'label\').shape.as_list(),\n            [1, None, None, None, 1, 1])\n        self.assertAllEqual(\n            window.image_data_placeholder(\'label\').dtype,\n            window.tf_dtypes[\'label\'])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'label\').shape.as_list(),\n            [1, 7])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'label\').dtype, tf.int32)\n\n    def test_set_spatial_size(self):\n        new_shape = (42, 43, 44)\n        window = ImageWindow.from_data_reader_properties(\n            **get_static_window_param())\n        window.placeholders_dict(10)\n        window.set_spatial_shape(new_shape)\n        self.assertAllClose(window.shapes[\'image\'], (10, 42, 43, 44, 1, 2))\n        self.assertAllClose(window.shapes[\'label\'], (10, 42, 43, 44, 1, 1))\n        window.placeholders_dict(10)\n\n        self.assertAllEqual(\n            window.image_data_placeholder(\'image\').shape.as_list(),\n            [10, 42, 43, 44, 1, 2])\n        self.assertAllEqual(\n            window.image_data_placeholder(\'image\').dtype,\n            window.tf_dtypes[\'image\'])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'image\').shape.as_list(),\n            [10, 7])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'image\').dtype, tf.int32)\n\n        self.assertAllEqual(\n            window.image_data_placeholder(\'label\').shape.as_list(),\n            [10, 42, 43, 44, 1, 1])\n        self.assertAllEqual(\n            window.image_data_placeholder(\'label\').dtype,\n            window.tf_dtypes[\'label\'])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'label\').shape.as_list(),\n            [10, 7])\n        self.assertAllEqual(\n            window.coordinates_placeholder(\'label\').dtype, tf.int32)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/interventional_affine_net_test.py,5,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.network.toynet import ToyNet\nfrom niftynet.network.interventional_affine_net import INetAffine\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass INetAffineTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 32, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        affinenet_instance = INetAffine()\n        out = affinenet_instance(x, x, is_training=True)\n        print(affinenet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 3), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        affinenet_instance = INetAffine()\n        out = affinenet_instance(x, x, is_training=True)\n        print(affinenet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 2), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/interventional_dense_net_test.py,9,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.network.interventional_dense_net import INetDense\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass INetDenseTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 32, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        densenet_instance = INetDense()\n        out = densenet_instance(x, x, is_training=True)\n        print(densenet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 3), out.shape)\n\n    def test_multi_scale_3d_shape(self):\n        input_shape = (2, 16, 16, 16, 1)\n        x = tf.ones(input_shape)\n\n        densenet_instance = INetDense(multi_scale_fusion=True)\n        out = densenet_instance(x, x, is_training=True)\n        print(densenet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 16, 3), out.shape)\n\n    def test_multi_scale_3d_1_shape(self):\n        input_shape = (2, 48, 48, 48, 1)\n        x = tf.ones(input_shape)\n\n        densenet_instance = INetDense(multi_scale_fusion=True)\n        out = densenet_instance(x, x, is_training=True)\n        print(densenet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 48, 48, 48, 3), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 16, 16, 1)\n        x = tf.ones(input_shape)\n\n        densenet_instance = INetDense()\n        out = densenet_instance(x, x, is_training=True)\n        print(densenet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 16, 16, 2), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/interventional_hybrid_net_test.py,9,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.network.toynet import ToyNet\nfrom niftynet.network.interventional_hybrid_net import INetHybridPreWarp, INetHybridTwoStream\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass INetHybridPreWarpTest(NiftyNetTestCase):\n\n    def test_3d_shape(self):\n        input_shape = (2, 32, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        hybridnet_instance = INetHybridPreWarp(1e-6)\n        out = hybridnet_instance(x, x, is_training=True)\n        print(hybridnet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 3), out[0].shape)\n            self.assertAllClose((2, 32, 32, 32, 3), out[1].shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        hybridnet_instance = INetHybridPreWarp(1e-6)\n        out = hybridnet_instance(x, x, is_training=True)\n        print(hybridnet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 2), out[0].shape)\n            self.assertAllClose((2, 32, 32, 2), out[1].shape)\n\nclass INetHybridTwoStreamTest(NiftyNetTestCase):\n\n\n    def test_3d_shape(self):\n        input_shape = (2, 32, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        hybridnet_instance = INetHybridTwoStream(1e-6)\n        out = hybridnet_instance(x, x, is_training=True)\n        print(hybridnet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 3), out[0].shape)\n            self.assertAllClose((2, 32, 32, 32, 3), out[1].shape)\n            self.assertAllClose((2, 32, 32, 32, 3), out[2].shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        hybridnet_instance = INetHybridTwoStream(1e-6)\n        out = hybridnet_instance(x, x, is_training=True)\n        print(hybridnet_instance)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 2), out[0].shape)\n            self.assertAllClose((2, 32, 32, 2), out[1].shape)\n            self.assertAllClose((2, 32, 32, 2), out[2].shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/linear_resize_test.py,6,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.linear_resize import LinearResizeLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass LinearResizeTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 4)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 4)\n        x = tf.ones(input_shape)\n        return x\n\n    def run_test(self, new_size, expected_spatial_shape, is_3d=True):\n        if is_3d:\n            x = self.get_3d_input()\n        else:\n            x = self.get_2d_input()\n\n        x_shape = x.shape.as_list()\n        expected_shape = \\\n            [x_shape[0]] + list(expected_spatial_shape) + [x_shape[-1]]\n\n        resize_layer = LinearResizeLayer(new_size=new_size)\n        resized = resize_layer(x)\n        print(resize_layer)\n        with self.cached_session() as sess:\n            out = sess.run(resized)\n            self.assertAllClose(out.shape, expected_shape)\n\n    def test_3d_shape(self):\n        new_shape = (8, 8, 7)\n        self.run_test(new_shape, new_shape)\n\n        new_shape = (20, 18, 17)\n        self.run_test(new_shape, new_shape)\n\n        new_shape = (16, 16, 16)\n        self.run_test(new_shape, new_shape)\n\n    def test_2d_shape(self):\n        new_shape = (8, 7)\n        self.run_test(new_shape, new_shape, False)\n\n        new_shape = (20, 18)\n        self.run_test(new_shape, new_shape, False)\n\n        new_shape = (16, 16)\n        self.run_test(new_shape, new_shape, False)\n\n    def test_int_shape(self):\n        new_shape = 20\n\n        self.run_test(new_shape, (new_shape,) * 2, False)\n        self.run_test(new_shape, (new_shape,) * 3, True)\n\n    def test_bad_int_shape(self):\n        new_shape = 0\n\n        with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, """"):\n            self.run_test(new_shape, (new_shape,) * 2, False)\n\n        with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, """"):\n            self.run_test(new_shape, (new_shape,) * 3, True)\n\n    def test_bad_shape(self):\n        new_shape = (20, 5)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            self.run_test(new_shape, new_shape)\n\n        new_shape = (0, 0, 0)\n        with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, """"):\n            self.run_test(new_shape, new_shape)\n\n    def test_ill_input(self):\n        new_shape = (20, 15, 1)\n        self.run_test(new_shape, new_shape[:2], False)\n\n        new_shape = (20, 20)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            self.run_test(new_shape, new_shape[:2], True)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/loss_classification_multi_test.py,7,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.loss_classification_multi import LossFunction, variability\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass VariabilityTests(NiftyNetTestCase):\n    def test_variability_value(self):\n        # test value is -0.5 * [1 * log(e / (1+e)) + 1 * log(e^2 / (e^2 + 1))]\n        with self.cached_session():\n            # [0,1,0] 2/3 , 1/3 4/9\n            # [0,0,0] 1, 0 0\n            predicted = [[0, 1,0],[1, 1,1]]\n\n            computed_variability = variability(predicted, nrater=3)\n            self.assertAlmostEqual(\n                computed_variability[0].eval(),4.0/9.0)\n\n\n\nclass LossConfusionTest(NiftyNetTestCase):\n    def test_confusion_matrix_loss(self):\n        with self.cached_session():\n            predicted = tf.constant([[[1,-1],[-1,1],[1,-1]],[[1,-1],[1,-1],[1,\n                                                                       -1]]],\n                                    dtype=tf.float32)\n            predicted *= 1000\n            ground_truth = [[0,0,1],[0,0,1]]\n            test_loss_func = LossFunction(2, 3, loss_type=\'ConfusionMatrix\',\n                                          loss_func_params={\'nrater\':3})\n            computed_loss = test_loss_func(ground_truth=ground_truth,\n                                           pred_multi=predicted)\n            self.assertAlmostEqual(computed_loss.eval(), 4.0/3.0)\n\nclass LossVariabilityTest(NiftyNetTestCase):\n    def test_variability_loss(self):\n        with self.cached_session():\n            predicted = tf.constant([[[1,-1],[-1,1],[1,-1]],[[1,-1],[1,-1],[1,\n                                                                       -1]]],\n                                    dtype=tf.float32)\n            predicted *= 1000\n            ground_truth = [[0,0,1],[0,0,1]]\n            test_loss_func = LossFunction(2, 3, loss_type=\'Variability\')\n            computed_loss = test_loss_func(ground_truth=ground_truth,\n                                           pred_multi=predicted)\n            self.assertAlmostEqual(computed_loss.eval(), np.sqrt(16.0/162.0))\n\n\nclass LossConsistencyTest(NiftyNetTestCase):\n    def test_consistency_loss(self):\n        with self.cached_session():\n            predicted = tf.constant([[[1,-1],[-1,1],[1,-1]],[[1,-1],[1,-1],[1,\n                                                                       -1]]],\n                                    dtype=tf.float32)\n            predicted *= 1000\n            pred_ave = [[[0.66,0.33],[1,0]]]\n            test_loss_func = LossFunction(2, 3, loss_type=\'Consistency\')\n            computed_loss = test_loss_func(pred_ave=pred_ave,\n                                           pred_multi=predicted)\n            self.assertAllClose(computed_loss.eval(), 0, atol=1E-2)\n\n\n\n# class LossFunctionErrorTest(NiftyNetTestCase):\n#     """"""\n#     These tests check that a ValueError is called\n#     for non-existent loss functions.\n#     They also check that suggestions are returned\n#     if the name is close to a real one.\n#     """"""\n#\n#     def test_value_error_for_bad_loss_function(self):\n#         with self.cached_session():\n#             with self.assertRaises(ValueError):\n#                 LossFunction(0, loss_type=\'wrong answer\')\n#\n#     # Note: sensitive to precise wording of ValueError message.\n#     def test_suggestion_for_dice_typo(self):\n#         with self.cached_session():\n#             with self.assertRaisesRegexp(ValueError, \'CrossEntropy\'):\n#                 LossFunction(0, loss_type=\'cross_entropy\')\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
tests/loss_classification_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.loss_classification import LossFunction\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass CrossEntropyTests(NiftyNetTestCase):\n    def test_cross_entropy_value(self):\n        # test value is -0.5 * [1 * log(e / (1+e)) + 1 * log(e^2 / (e^2 + 1))]\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 1], [2, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0], dtype=tf.int64, name=\'labels\')\n            test_loss_func = LossFunction(2, loss_type=\'CrossEntropy\')\n            computed_cross_entropy = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_cross_entropy.eval(),\n                -.5 * (np.log(np.e / (1 + np.e)) + np.log(\n                    np.e ** 2 / (1 + np.e ** 2))))\n\n\nclass LossFunctionErrorsTest(NiftyNetTestCase):\n    """"""\n    These tests check that a ValueError is called\n    for non-existent loss functions.\n    They also check that suggestions are returned\n    if the name is close to a real one.\n    """"""\n\n    def test_value_error_for_bad_loss_function(self):\n        with self.cached_session():\n            with self.assertRaises(ValueError):\n                LossFunction(0, loss_type=\'wrong answer\')\n\n    # Note: sensitive to precise wording of ValueError message.\n    def test_suggestion_for_dice_typo(self):\n        with self.cached_session():\n            with self.assertRaisesRegexp(ValueError, \'CrossEntropy\'):\n                LossFunction(0, loss_type=\'cross_entropy\')\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
tests/loss_regression_test.py,111,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.loss_regression import LossFunction\nfrom niftynet.layer.loss_regression import l1_loss, l2_loss, huber_loss, \\\n    smooth_l1_loss, cosine_loss\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass L1LossTests(NiftyNetTestCase):\n    def test_l1_loss_value(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [1, 1],\n                dtype=tf.float32, name='predicted')\n            labels = tf.constant([1, 0], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='L1Loss')\n            computed_l1_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_l1_loss.eval(), 0.5)\n\n    def test_l1_loss_value_weight(self):\n        with self.cached_session():\n            weights = tf.constant(\n                [[1, 2]], dtype=tf.float32, name='weights')\n            predicted = tf.constant(\n                [[1, 1]], dtype=tf.float32, name='predicted')\n            labels = tf.constant([[1, 0]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='L1Loss')\n            computed_l1_loss = test_loss_func(predicted, labels, weights)\n            self.assertAlmostEqual(computed_l1_loss.eval(), 2.0 / 3.0)\n\n    def test_l1_loss(self):\n        # expected loss: mean(.2 + 2) = 1.1\n        with self.cached_session():\n            predicted = tf.constant(\n                [1.2, 1],\n                dtype=tf.float32, name='predicted')\n            gold_standard = tf.constant(\n                [1, 3], dtype=tf.float32, name='gold_standard')\n            self.assertAlmostEqual(\n                l1_loss(predicted, gold_standard).eval(), 1.1)\n\n\nclass L2LossTests(NiftyNetTestCase):\n    def test_l2_loss_value(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[1, 2]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[1, 0]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='L2Loss')\n            computed_l2_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(computed_l2_loss.eval(), 2.0)\n\n    def test_l2_loss_value_weight(self):\n        with self.cached_session():\n            weights = tf.constant(\n                [[1, 2]], dtype=tf.float32, name='weights')\n            predicted = tf.constant(\n                [[1, 2]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[1, 0]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='L2Loss')\n            computed_l2_loss = test_loss_func(predicted, labels, weights)\n            self.assertAlmostEqual(\n                computed_l2_loss.eval(), 8.0 / 9.0, places=3)\n\n    def test_l2_loss(self):\n        # expected loss: (0.04 + 4 + 1) /2 = 2.52\n        # (note - not the mean, just the sum)\n        with self.cached_session():\n            predicted = tf.constant(\n                [1.2, 1, 2],\n                dtype=tf.float32, name='predicted')\n            gold_standard = tf.constant(\n                [1, 3, 3],\n                dtype=tf.float32, name='gold_standard')\n            self.assertAlmostEqual(\n                l2_loss(predicted, gold_standard).eval(), 2.52)\n\n\nclass HuberLossTests(NiftyNetTestCase):\n    def test_huber_loss(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name='predicted')\n            gold_standard = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name='gold_standard')\n            self.assertEqual(huber_loss(predicted, gold_standard).eval(), 0.0)\n\n    def test_huber_continuous(self):\n        with self.cached_session():\n            epsilon = tf.constant(\n                1e-10, dtype=tf.float32)\n            predicted = tf.constant(\n                [1], dtype=tf.float32, name='predicted')\n            gold_standard = tf.constant(\n                [0], dtype=tf.float32, name='gold_standard')\n            huber_loss_inside_delta = huber_loss(\n                predicted + epsilon, gold_standard, delta=1.0)\n            huber_loss_outside_delta = huber_loss(\n                predicted - epsilon, gold_standard, delta=1.0)\n            self.assertAlmostEqual(huber_loss_inside_delta.eval(),\n                                   huber_loss_outside_delta.eval())\n\n    def test_huber_loss_hand_example(self):\n        with self.cached_session():\n            # loss should be: mean( 0.2 ** 2/ 2 + (2-0.5) ) == 1.52/2 == 0.76\n            predicted = tf.constant(\n                [1.2, 1], dtype=tf.float32, name='predicted')\n            gold_standard = tf.constant(\n                [1, 3], dtype=tf.float32, name='gold_standard')\n            loss = huber_loss(predicted, gold_standard, delta=1.0)\n            self.assertAlmostEqual(loss.eval(), .76)\n\n    def test_huber_loss_value(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[1, 2, 0.5]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[1, 0, 1]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='Huber')\n            computed_huber_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_huber_loss.eval(), 0.5417, places=4)\n\n    def test_huber_loss_value_weight(self):\n        with self.cached_session():\n            weights = tf.constant(\n                [[1, 2, 1]], dtype=tf.float32, name='weights')\n            predicted = tf.constant(\n                [[1, 2, 0.5]], dtype=tf.float32, name='predicted')\n            labels = tf.constant([[1, 0, 1]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='Huber')\n            computed_huber_loss = test_loss_func(\n                predicted, labels, weight_map=weights)\n            self.assertAlmostEqual(\n                computed_huber_loss.eval(), 3.125 / 4)\n\n\nclass RMSELossTests(NiftyNetTestCase):\n    def test_rmse_loss_value(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[1.2, 1]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[1, 0]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='RMSE')\n            computed_rmse_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_rmse_loss.eval(), 0.7211, places=4)\n\n    def test_rmse_loss_value_weight(self):\n        with self.cached_session():\n            weights = tf.constant(\n                [[1, 2.1]], dtype=tf.float32, name='weights')\n            predicted = tf.constant(\n                [[1, 1]], dtype=tf.float32, name='predicted')\n            labels = tf.constant([[1, 0]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='RMSE')\n            computed_rmse_loss = test_loss_func(predicted, labels, weights)\n            self.assertAlmostEqual(\n                computed_rmse_loss.eval(), 0.8231, places=4)\n\n\nclass MAELossTests(NiftyNetTestCase):\n    def test_MAE_loss_value(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[1, 2]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[1.2, 0]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='MAE')\n            computed_MAE_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_MAE_loss.eval(), 1.1)\n\n    def test_MAE_loss_value_weight(self):\n        with self.cached_session():\n            weights = tf.constant(\n                [[1, 2]], dtype=tf.float32, name='weights')\n            predicted = tf.constant(\n                [[1, 1]], dtype=tf.float32, name='predicted')\n            labels = tf.constant([[1, 0]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='MAE')\n            computed_MAE_loss = test_loss_func(predicted, labels, weights)\n            self.assertAlmostEqual(\n                computed_MAE_loss.eval(), 2.0 / 3.0)\n\n\nclass SmoothL1LossTests(NiftyNetTestCase):\n    def test_smooth_l1_loss(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name='predicted')\n            gold_standard = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name='gold_standard')\n            self.assertEqual(huber_loss(predicted, gold_standard).eval(), 0.0)\n\n    def test_smooth_l1_continuous(self):\n        with self.cached_session():\n            epsilon = tf.constant(\n                1e-10, dtype=tf.float32)\n            predicted = tf.constant(\n                [0.5], dtype=tf.float32, name='predicted')\n            gold_standard = tf.constant(\n                [0], dtype=tf.float32, name='gold_standard')\n            huber_loss_inside_delta = smooth_l1_loss(\n                predicted + epsilon, gold_standard, value_thresh=0.5)\n            huber_loss_outside_delta = smooth_l1_loss(\n                predicted - epsilon, gold_standard)\n            self.assertAlmostEqual(huber_loss_inside_delta.eval(),\n                                   huber_loss_outside_delta.eval())\n\n    def test_smooth_l1_continuous_max(self):\n        with self.cached_session():\n            epsilon = tf.constant(\n                1e-10, dtype=tf.float32)\n            predicted = tf.constant(\n                [2.375], dtype=tf.float32, name='predicted')\n            gold_standard = tf.constant(\n                [0], dtype=tf.float32, name='gold_standard')\n            huber_loss_inside_delta = smooth_l1_loss(\n                predicted + epsilon, gold_standard, value_thresh=0.5)\n            huber_loss_outside_delta = smooth_l1_loss(\n                predicted - epsilon, gold_standard)\n            self.assertAlmostEqual(huber_loss_inside_delta.eval(),\n                                   huber_loss_outside_delta.eval())\n\n    def test_smooth_loss_value(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[1, 2.375, 0.5]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[1, 0, 1]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='SmoothL1')\n            computed_smooth_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_smooth_loss.eval(), 2.125/3, places=4)\n\n    def test_smooth_loss_value_weight(self):\n        with self.cached_session():\n            weights = tf.constant(\n                [[1, 2, 1]], dtype=tf.float32, name='weights')\n            predicted = tf.constant(\n                [[1, 2.375, 0.5]], dtype=tf.float32, name='predicted')\n            labels = tf.constant([[1, 0, 1]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='SmoothL1')\n            computed_smooth_l1_loss = test_loss_func(\n                predicted, labels, weight_map=weights)\n            self.assertAlmostEqual(\n                computed_smooth_l1_loss.eval(), 4.125 / 4)\n\n\nclass CosineLossTests(NiftyNetTestCase):\n    def test_cosine_loss_value(self):\n\n        with self.cached_session():\n            predicted = tf.constant(\n                [[[1, 0],[0.5,0.5]]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[[0, 1],[0.5,0.5]]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='Cosine')\n            computed_cosine_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_cosine_loss.eval(), 0.5)\n\n    def test_cosine_loss_value_equal(self):\n\n        with self.cached_session():\n            predicted = tf.constant(\n                [[[0.5,0.5]]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[[0.5,0.5]]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='Cosine')\n            computed_cosine_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_cosine_loss.eval(), 0)\n\n    def test_cosine_loss_value_equal2(self):\n\n        with self.cached_session():\n            predicted = tf.constant(\n                [[[1, 0],[0.5,0.5]]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[[1, 0],[0.5,0.5]]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='Cosine')\n            computed_cosine_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_cosine_loss.eval(), 0)\n\n    def test_cosine_loss_value_weight(self):\n        with self.cached_session():\n            weights = tf.constant(\n                [[[2], [1]]], dtype=tf.float32, name='weights')\n            predicted = tf.constant(\n                [[[1, 0],[0.5,0.5]]], dtype=tf.float32, name='predicted')\n            labels = tf.constant(\n                [[[0, 1],[0.5,0.5]]], dtype=tf.float32, name='labels')\n            test_loss_func = LossFunction(loss_type='Cosine')\n            computed_cosine_loss = test_loss_func(predicted, labels, weights)\n            self.assertAlmostEqual(\n                computed_cosine_loss.eval(), 2.0 / 3.0)\n\n\nif __name__ == '__main__':\n    tf.test.main()\n"""
tests/loss_segmentation_test.py,138,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.loss_segmentation import LossFunction, labels_to_one_hot\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass DiceWithMissingClass(NiftyNetTestCase):\n    # all dice methods should return 0.0 for this case:\n    def test_missing_class(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 0, 1], [1, 0, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([2, 0], dtype=tf.int32)\n\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            for loss_func in (\'Dice\', \'Dice_NS\', \'Dice_Dense\', \'Dice_Dense_NS\'):\n\n                test_loss_func = LossFunction(3, loss_type=loss_func, softmax=False)\n                if \'Dense\' in loss_func:\n                    loss_value = test_loss_func(predicted, tf.one_hot(labels, 3))\n                else:\n                    loss_value = test_loss_func(predicted, labels)\n\n                # softmax of zero, Dice loss of -1, so sum \\approx -1\n                self.assertAllClose(loss_value.eval(), 0.0, atol=1e-4)\n\n    def test_missing_class_dice_plus_xent(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 0, 999], [999, 0, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([2, 0], dtype=tf.int32)\n\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(3, loss_type=\'DicePlusXEnt\', softmax=False)\n            loss_value = test_loss_func(predicted, labels)\n\n            # softmax of zero, Dice loss of -1, so sum \\approx -1\n            self.assertAllClose(loss_value.eval(), -1.0, atol=1e-4)\n\n\nclass DicePlusXEntTest(NiftyNetTestCase):\n    def test_dice_plus(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 9999], [9999, 0], [9999, 0], [9999, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int16, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n            test_loss_func = LossFunction(2, loss_type=\'DicePlusXEnt\', softmax=False)\n            loss_value = test_loss_func(predicted, labels)\n\n            # softmax of zero, Dice loss of -1, so sum \\approx -1\n            self.assertAllClose(loss_value.eval(), -1.0, atol=1e-3)\n\n    def test_dice_plus_multilabel(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 0, 9999], [9999, 0, 0], [0, 9999, 0], [9999, 0, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([2, 0, 1, 0], dtype=tf.int16, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n            test_loss_func = LossFunction(3, loss_type=\'DicePlusXEnt\', softmax=False)\n            loss_value = test_loss_func(predicted, labels)\n\n            # cross-ent of zero, Dice loss of -1, so sum \\approx -1\n            self.assertAllClose(loss_value.eval(), -1.0, atol=1e-3)\n\n    def test_dice_plus_non_zeros(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 9999, 9999], [9999, 0, 0], [0, 9999, 9999], [9999, 0, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([2, 0, 1, 0], dtype=tf.int16, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n            test_loss_func = LossFunction(3, loss_type=\'DicePlusXEnt\', softmax=False)\n            loss_value = test_loss_func(predicted, labels)\n            # cross-ent of mean(ln(2), 0, 0, ln(2)) = .5*ln(2)\n            # Dice loss of -mean(1, .5, .5)=-2/3\n            self.assertAllClose(loss_value.eval(), .5 * np.log(2) - 2. / 3., atol=1e-3)\n\n    def test_dice_plus_wrong_softmax(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 9999, 9999], [9999, 0, 0], [0, 9999, 9999], [9999, 0, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([2, 0, 1, 0], dtype=tf.int16, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n            test_loss_func = LossFunction(3, loss_type=\'DicePlusXEnt\', softmax=True)\n            loss_value = test_loss_func(predicted, labels)\n            # cross-ent of mean(ln(2), 0, 0, ln(2)) = .5*ln(2)\n            # Dice loss of -mean(1, .5, .5)=-2/3\n            self.assertAllClose(loss_value.eval(), .5 * np.log(2) - 2. / 3., atol=1e-3)\n\n    def test_dice_plus_weighted(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 9999, 9999], [9999, 0, 0], [0, 9999, 9999], [9999, 0, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([2, 0, 1, 0], dtype=tf.int16, name=\'labels\')\n            weights = tf.expand_dims(tf.constant([0, 1, 0, 0], dtype=tf.float32), axis=0)\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n            test_loss_func = LossFunction(3, loss_type=\'DicePlusXEnt\', softmax=True)\n            loss_value = test_loss_func(predicted, labels, weight_map=weights)\n            self.assertAllClose(loss_value.eval(), -1.)\n\n\nclass OneHotTester(NiftyNetTestCase):\n    def test_vs_tf_onehot(self):\n        with self.cached_session():\n            labels = tf.constant([1, 2, 3, 0], dtype=tf.int64, name=\'labels\')\n            tf_one_hot = tf.one_hot(labels, depth=4)\n            niftynet_one_hot = tf.sparse_tensor_to_dense(labels_to_one_hot(labels, 4))\n            self.assertAllEqual(tf_one_hot.eval(), niftynet_one_hot.eval())\n\n    def test_one_hot(self):\n        ref = np.asarray(\n            [[[0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.]],\n             [[0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]]],\n            dtype=np.float32)\n\n        with self.cached_session():\n            labels = tf.constant([[1, 2], [3, 4]])\n            # import pdb; pdb.set_trace()\n            one_hot = tf.sparse_tensor_to_dense(\n                labels_to_one_hot(labels, 5)).eval()\n            self.assertAllEqual(one_hot, ref)\n\n\nclass SensitivitySpecificityTests(NiftyNetTestCase):\n    # test done by regression for refactoring purposes\n    def test_sens_spec_loss_by_regression(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'SensSpec\')\n            test_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(test_loss.eval(), 2.06106e-9)\n\n    def test_multi_label_sens_spec(self):\n        with self.cached_session():\n            # answer calculated by hand -\n            predicted = tf.constant(\n                [[0, 1, 0], [0, 0, 1]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 2], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(3, loss_type=\'SensSpec\',\n                                          loss_func_params={\'r\': 0.05})\n            test_loss = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(test_loss.eval(), 0.14598623)\n\n\nclass GeneralisedDiceTest(NiftyNetTestCase):\n    # test done by regression for refactoring purposes\n    def test_generalised_dice_score_regression(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'GDSC\')\n            one_minus_generalised_dice_score = test_loss_func(\n                predicted, labels)\n            self.assertAllClose(\n                one_minus_generalised_dice_score.eval(), 0.0, atol=1e-4)\n\n    def test_gdsc_incorrect_type_weight_error(self):\n        with self.cached_session():\n            with self.assertRaises(ValueError) as cm:\n                predicted = tf.constant(\n                    [[0, 10], [10, 0], [10, 0], [10, 0]],\n                    dtype=tf.float32, name=\'predicted\')\n                labels = tf.constant(\n                    [1, 0, 0, 0],\n                    dtype=tf.int64, name=\'labels\')\n                predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n                test_loss_func = LossFunction(\n                    2, loss_type=\'GDSC\',\n                    loss_func_params={\'type_weight\': \'unknown\'})\n                one_minus_generalised_dice_score = test_loss_func(predicted,\n                                                                  labels)\n\n    def test_generalised_dice_score_uniform_regression(self):\n        with self.cached_session():\n            predicted = tf.constant([[0, 10], [10, 0], [10, 0], [10, 0]],\n                                    dtype=tf.float32, name=\'predicted\')\n\n            labels = tf.constant([[1, 0, 0, 0]], dtype=tf.int64, name=\'labels\')\n            weights = tf.cast(labels, tf.float32)\n            predicted, labels, weights = [tf.expand_dims(x, axis=0) for x in\n                                          (predicted, labels, weights)]\n\n            test_loss_func = LossFunction(\n                2, loss_type=\'GDSC\',\n                loss_func_params={\'type_weight\': \'Uniform\'})\n            one_minus_generalised_dice_score = test_loss_func(\n                predicted, labels, weights)\n            self.assertAllClose(one_minus_generalised_dice_score.eval(),\n                                0.3333, atol=1e-4)\n\n\nclass DiceTest(NiftyNetTestCase):\n    def test_dice_score(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'Dice\')\n            one_minus_dice_score = test_loss_func(predicted, labels)\n            self.assertAllClose(one_minus_dice_score.eval(), 0.0, atol=1e-5)\n\n    def test_dice_score_weights(self):\n        with self.cached_session():\n            weights = tf.constant([[1, 1, 0, 0]], dtype=tf.float32,\n                                  name=\'weights\')\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([[1, 0, 0, 0]], dtype=tf.int64, name=\'labels\')\n            predicted, labels, weights = [tf.expand_dims(x, axis=0) for x in (predicted, labels, weights)]\n\n            test_loss_func = LossFunction(2, loss_type=\'Dice\')\n            one_minus_dice_score = test_loss_func(predicted, labels,\n                                                  weight_map=weights)\n            self.assertAllClose(one_minus_dice_score.eval(), 0.0, atol=1e-4)\n\n    def test_wrong_prediction(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 100]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'Dice\')\n            one_minus_dice_score = test_loss_func(predicted, labels)\n            self.assertAllClose(one_minus_dice_score.eval(), 1.0, atol=1e-4)\n\n    def test_dice_batch_size_greater_than_one(self):\n        # test for Github issue #22: need to take mean per-image before\n        # averaging Dice of ~1 and ~0.16, should get dice ~ 1 - 0.5816\n        with self.cached_session():\n            # predictions ~ [1, 0, 0]; [0, 0, 1]; [0, .5, .5]; [.333, .333, .333]\n            predictions_numpy = np.array([[[10., 0, 0], [0, 0, 10]],\n                                          [[-10, 0, 0], [0, 0, 0]]]).reshape([2, 2, 1, 1, 3])\n            labels_numpy = np.array([[[0, 2]], [[0, 1]]]).reshape([2, 2, 1, 1, 1])\n\n            predicted = tf.constant(predictions_numpy, dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant(labels_numpy, dtype=tf.int64, name=\'labels\')\n\n            test_loss_func = LossFunction(3, loss_type=\'Dice\')\n            one_minus_dice_score = test_loss_func(predicted, labels)\n\n            self.assertAllClose(one_minus_dice_score.eval(), 1 - 0.5816, atol=1e-4)\n\n\nclass CrossEntropyTests(NiftyNetTestCase):\n    def test_cross_entropy_value(self):\n        # test value is -0.5 * [1 * log(e / (1+e)) + 1 * log(e^2 / (e^2 + 1))]\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 1], [2, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'CrossEntropy\')\n            computed_cross_entropy = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(\n                computed_cross_entropy.eval(),\n                -.5 * (np.log(np.e / (1 + np.e)) + np.log(\n                    np.e ** 2 / (1 + np.e ** 2))))\n\n            test_dense_loss = LossFunction(2, loss_type=\'CrossEntropy_Dense\')\n            labels = tf.sparse_tensor_to_dense(labels_to_one_hot(labels, 2))\n            computed_cross_entropy = test_loss_func(predicted, tf.to_int32(labels))\n            self.assertAlmostEqual(\n                computed_cross_entropy.eval(),\n                -.5 * (np.log(np.e / (1 + np.e)) + np.log(\n                    np.e ** 2 / (1 + np.e ** 2))))\n\n    def test_cross_entropy_value_weight(self):\n        with self.cached_session():\n            weights = tf.constant([[1], [2]], dtype=tf.float32, name=\'weights\')\n            predicted = tf.constant(\n                [[0, 1], [2, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([[1], [0]], dtype=tf.int64, name=\'labels\')\n            predicted, labels, weights = \\\n                [tf.expand_dims(x, axis=0) for x in (predicted, labels, weights)]\n\n            test_loss_func = LossFunction(2, loss_type=\'CrossEntropy\')\n            computed_cross_entropy = test_loss_func(predicted, labels, weights)\n            self.assertAlmostEqual(\n                computed_cross_entropy.eval(),\n                -.5 * (\n                        2.0 / 3.0 * np.log(np.e / (1 + np.e)) + 4.0 / 3.0 * np.log(\n                    np.e ** 2 / (1 + np.e ** 2))))\n\n\nclass DiceTestNoSquare(NiftyNetTestCase):\n    def test_dice_score_nosquare(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'Dice_NS\')\n            one_minus_dice_score = test_loss_func(predicted, labels)\n            self.assertAllClose(one_minus_dice_score.eval(), 0.0, atol=1e-4)\n\n    def test_dice_score_nosquare_weights(self):\n        with self.cached_session():\n            weights = tf.constant([[1, 1, 0, 0]], dtype=tf.float32,\n                                  name=\'weights\')\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([[1, 0, 0, 0]], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2,\n                                          loss_type=\'Dice_NS\')\n            one_minus_dice_score = test_loss_func(predicted, labels,\n                                                  weight_map=weights)\n            self.assertAllClose(one_minus_dice_score.eval(), 0.0, atol=1e-4)\n\n    def test_wrong_prediction(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 100]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'Dice_NS\')\n            one_minus_dice_score = test_loss_func(predicted, labels)\n            self.assertAllClose(one_minus_dice_score.eval(), 1.0, atol=1e-4)\n\n\nclass TverskyTest(NiftyNetTestCase):\n    def test_tversky_index(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0)\n                                 for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'Tversky\')\n            one_minus_tversky_index = test_loss_func(predicted, labels)\n            self.assertAllClose(one_minus_tversky_index.eval(), 0.0, atol=1e-4)\n\n    def test_tversky_index_weights(self):\n        with self.cached_session():\n            weights = tf.constant([[1, 1, 0, 0]], dtype=tf.float32,\n                                  name=\'weights\')\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([[1, 0, 1, 0]], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0)\n                                 for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2,\n                                          loss_type=\'Tversky\')\n            one_minus_tversky_index = test_loss_func(predicted, labels,\n                                                     weight_map=weights)\n            self.assertAllClose(one_minus_tversky_index.eval(), 0.0, atol=1e-4)\n\n    def test_wrong_prediction(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 100]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0)\n                                 for x in (predicted, labels)]\n\n            test_loss_func = LossFunction(2, loss_type=\'Tversky\')\n            one_minus_tversky_index = test_loss_func(predicted, labels)\n            self.assertAlmostEqual(one_minus_tversky_index.eval(), 1.0)\n\n\nclass DiceDenseTest(NiftyNetTestCase):\n    def test_dice_dense_score(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 10], [10, 0], [10, 0], [10, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            one_hot = tf.constant([[1, 0], [0, 1], [0, 1], [0, 1]],\n                                  dtype=tf.int64, name=\'one_hot\')\n            predicted, one_hot = [tf.expand_dims(x, axis=0) for x in (predicted, one_hot)]\n\n            test_loss_func = LossFunction(2, loss_type=\'Dice_Dense\')\n            one_minus_dice_score = test_loss_func(predicted, one_hot)\n            self.assertAllClose(one_minus_dice_score.eval(), 1.0, atol=1e-4)\n\n    def test_wrong_prediction(self):\n        with self.cached_session():\n            predicted = tf.constant(\n                [[0, 100]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([0], dtype=tf.int64, name=\'labels\')\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n            one_hot = tf.one_hot(labels, axis=-1, depth=2)\n\n            test_loss_func = LossFunction(2, loss_type=\'Dice_Dense\')\n            one_minus_dice_score = test_loss_func(predicted, one_hot)\n            self.assertAllClose(one_minus_dice_score.eval(), 1.0, atol=1e-4)\n\n    def test_dense_dice_vs_sparse(self):\n        # regression test vs dense version\n        with self.cached_session():\n            predicted = tf.constant(\n                [[2, 3], [9, 8], [0, 0], [1, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            sparse_loss_func = LossFunction(2, loss_type=\'Dice\')\n            sparse_dice = sparse_loss_func(predicted, labels)\n\n            one_hot = tf.one_hot(labels, axis=-1, depth=2)\n            dense_loss_func = LossFunction(2, loss_type=\'Dice_Dense\')\n            dense_dice = dense_loss_func(predicted, one_hot)\n\n            self.assertAllEqual(sparse_dice.eval(), dense_dice.eval())\n\n\nclass DiceDenseNoSquareTest(NiftyNetTestCase):\n\n    def test_dense_dice_nosquare_vs_sparse(self):\n        # regression test vs dense version\n        with self.cached_session():\n            predicted = tf.constant(\n                [[2, 3], [9, 8], [0, 0], [1, 0]],\n                dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in (predicted, labels)]\n\n            sparse_loss_func = LossFunction(2, loss_type=\'Dice_NS\')\n            sparse_dice = sparse_loss_func(predicted, labels)\n\n            one_hot = tf.one_hot(labels, axis=-1, depth=2)\n            dense_loss_func = LossFunction(2, loss_type=\'Dice_Dense_NS\')\n            dense_dice = dense_loss_func(predicted, one_hot)\n\n            self.assertAllEqual(sparse_dice.eval(), dense_dice.eval())\n\n\nclass VolumeEnforcementTest(NiftyNetTestCase):\n    def test_volume_enforcement_equal(self):\n        with self.cached_session():\n            predicted = tf.constant([[-1000, 1000], [1000, -1000], [1000,\n                                                                     -1000], [1000, -1000]],\n                                    dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in\n                                 (predicted, labels)]\n            venf_loss_func = LossFunction(2, loss_type=\'VolEnforcement\')\n            venf_loss = venf_loss_func(predicted, labels)\n            self.assertAllClose(venf_loss.eval(), 0.0, atol=1e-4)\n\n    def test_volume_enforcement_nonexist(self):\n        with self.cached_session():\n            predicted = tf.constant([[1000, -1000], [1000, -1000], [1000, -1000], [1000, -1000]],\n                                    dtype=tf.float32, name=\'predicted\')\n            labels = tf.constant([1, 0, 0, 0], dtype=tf.int64, name=\'labels\')\n\n            predicted, labels = [tf.expand_dims(x, axis=0) for x in\n                                 (predicted, labels)]\n            venf_loss_func = LossFunction(2, loss_type=\'VolEnforcement\')\n            venf_loss = venf_loss_func(predicted, labels)\n            self.assertAllClose(venf_loss.eval(), 500.75, atol=0.1)\n\n\n\n\n\n\n\nclass LossFunctionErrorsTest(NiftyNetTestCase):\n    """"""\n    These tests check that a ValueError is called\n    for non-existent loss functions.\n    They also check that suggestions are returned\n    if the name is close to a real one.\n    """"""\n\n    def test_value_error_for_bad_loss_function(self):\n        with self.cached_session():\n            with self.assertRaises(ValueError):\n                LossFunction(1, loss_type=\'wrong answer\')\n\n    # Note: sensitive to precise wording of ValueError message.\n    def test_suggestion_for_dice_typo(self):\n        with self.cached_session():\n            with self.assertRaisesRegexp(ValueError, \'Dice\'):\n                LossFunction(1, loss_type=\'dice\')\n\n    def test_suggestion_for_gdsc_typo(self):\n        with self.cached_session():\n            with self.assertRaisesRegexp(ValueError, \'GDSC\'):\n                LossFunction(1, loss_type=\'GSDC\')\n\n\nif __name__ == \'__main__\':\n    tf.test.main()\n'"
tests/mean_variance_normalisation_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.mean_variance_normalisation import MeanVarNormalisationLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass BinaryMaskingTEst(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (16, 16, 16)\n        x = np.random.randint(-10, 10, size=input_shape)\n        return x\n\n    def get_5d_input(self):\n        input_shape = (16, 16, 16, 3, 2)\n        x = np.random.randint(-10, 10, size=input_shape)\n        return x\n\n    def test_3d_plus_shape(self):\n        x = self.get_3d_input()\n        mask_layer = BinaryMaskingLayer(\n            type_str=\'otsu_plus\',\n            multimod_fusion=\'or\',\n            threshold=0.0)\n        mean_var_layer = MeanVarNormalisationLayer(\n            image_name=\'image\',\n            binary_masking_func=mask_layer)\n        mask_out, _ = mean_var_layer(x, mask_layer(x))\n        self.assertAllClose(x.shape, mask_out.shape)\n\n    def test_3d_minus_shape(self):\n        x = self.get_3d_input()\n        mask_layer = BinaryMaskingLayer(\n            type_str=\'otsu_minus\',\n            multimod_fusion=\'or\',\n            threshold=0.0)\n        mean_var_layer = MeanVarNormalisationLayer(\n            image_name=\'image\',\n            binary_masking_func=mask_layer)\n        mask_out, m = mean_var_layer(x)\n        self.assertAllClose(x.shape, mask_out.shape)\n\n    def test_5d_shape(self):\n        x = self.get_5d_input()\n        mask_layer = BinaryMaskingLayer(\n            type_str=\'threshold_minus\',\n            multimod_fusion=\'and\',\n            threshold=0.0)\n        mean_var_layer = MeanVarNormalisationLayer(\n            image_name=\'image\',\n            binary_masking_func=mask_layer)\n        mask_out, m = mean_var_layer(x)\n        self.assertAllClose(x.shape, mask_out.shape)\n\n    def test_5d_mean_shape(self):\n        x = self.get_5d_input()\n        mask_layer = BinaryMaskingLayer(\n            type_str=\'mean_plus\',\n            multimod_fusion=\'and\',\n            threshold=0.0)\n        mean_var_layer = MeanVarNormalisationLayer(\n            image_name=\'image\',\n            binary_masking_func=mask_layer)\n        mask_out, _ = mean_var_layer(x, mask_layer(x))\n        self.assertAllClose(x.shape, mask_out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/niftynet_global_config_test.py,0,"b'from glob import glob\nfrom os import (remove, makedirs)\nfrom os.path import (expanduser, join, isdir, isfile)\nfrom os.path import getmtime\nfrom shutil import rmtree\nfrom unittest import TestCase\n\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\n\n\nclass NiftyNetGlobalConfigTestBase(TestCase):\n    """"""Tests included here all pertain to the NiftyNet global configuration\n    file and `NiftyNetGlobalConfig` is a singleton. These require each test\n    to be run separately. This is why all tests are decorated with\n    `skipUnless`.\n    """"""\n\n    @classmethod\n    def typify(cls, file_path):\n        """"""Append file type extension to passed file path.""""""\n        return \'.\'.join([file_path, cls.file_type])\n\n    @classmethod\n    def remove_path(cls, path):\n        """"""Remove passed item, whether it\'s a file or directory.""""""\n        print(""removing {}"".format(path))\n        if isdir(path):\n            rmtree(path)\n        elif isfile(path):\n            remove(path)\n\n    @classmethod\n    def setUpClass(cls):\n        # use this method in sub-classes\n        if cls is not NiftyNetGlobalConfigTestBase:\n            NiftyNetGlobalConfigTestBase.setUpClass()\n            return\n\n        cls.config_home = join(expanduser(\'~\'), \'.niftynet\')\n        cls.file_type = \'ini\'\n        cls.config_file = join(cls.config_home, cls.typify(\'config\'))\n\n        cls.header = \'[global]\'\n        cls.default_config_opts = {\n            \'home\': \'~/niftynet\',\n            \'ext\': \'extensions\',\n            \'ext_mods\': [\'network\']\n        }\n\n    def setUp(self):\n        NiftyNetGlobalConfigTestBase.remove_path(\n            NiftyNetGlobalConfigTestBase.config_home)\n        NiftyNetGlobalConfigTestBase.remove_path(\n            expanduser(NiftyNetGlobalConfigTestBase.default_config_opts[\'home\'])\n        )\n\n    def tearDown(self):\n        self.setUp()\n\n\nclass TestGlobalConfigSingleton(NiftyNetGlobalConfigTestBase):\n\n    @classmethod\n    def setUpClass(cls):\n        NiftyNetGlobalConfigTestBase.setUpClass()\n\n    def test_global_config_singleton(self):\n        global_config_1 = NiftyNetGlobalConfig()\n        global_config_2 = NiftyNetGlobalConfig()\n        self.assertEqual(global_config_1, global_config_2)\n        self.assertTrue(global_config_1 is global_config_2)\n\n\nclass TestNonExistingConfigFileCreated(NiftyNetGlobalConfigTestBase):\n\n    def test_non_existing_config_file_created(self):\n        self.assertFalse(isfile(NiftyNetGlobalConfigTestBase.config_file))\n        global_config = NiftyNetGlobalConfig().setup()\n        self.assertTrue(isfile(NiftyNetGlobalConfigTestBase.config_file))\n        self.assertEqual(global_config.get_niftynet_config_folder(),\n                         NiftyNetGlobalConfigTestBase.config_home)\n\n\nclass TestExistingConfigFileLoaded(NiftyNetGlobalConfigTestBase):\n\n    def test_existing_config_file_loaded(self):\n        # create a config file with a custom NiftyNet home\n        makedirs(NiftyNetGlobalConfigTestBase.config_home)\n        custom_niftynet_home = \'~/customniftynethome\'\n        custom_niftynet_home_abs = expanduser(custom_niftynet_home)\n        config = \'\'.join([\'home = \', custom_niftynet_home])\n        with open(NiftyNetGlobalConfigTestBase.config_file, \'w\') as config_file:\n            config_file.write(\'\\n\'.join(\n                [NiftyNetGlobalConfigTestBase.header, config]))\n\n        global_config = NiftyNetGlobalConfig().setup()\n        self.assertEqual(global_config.get_niftynet_home_folder(),\n                         custom_niftynet_home_abs)\n        NiftyNetGlobalConfigTestBase.remove_path(custom_niftynet_home_abs)\n\n\nclass TestIncorrectConfigFileBackedUp(NiftyNetGlobalConfigTestBase):\n\n    def test_incorrect_config_file_backed_up(self):\n        # create an incorrect config file at the correct location\n        makedirs(NiftyNetGlobalConfigTestBase.config_home)\n        incorrect_config = \'\\n\'.join([NiftyNetGlobalConfigTestBase.header,\n                                      \'invalid_home_tag = ~/niftynet\'])\n        with open(NiftyNetGlobalConfigTestBase.config_file, \'w\') as config_file:\n            config_file.write(incorrect_config)\n\n        # the following should back it up and replace it with default config\n        global_config = NiftyNetGlobalConfig().setup()\n\n        self.assertTrue(isfile(NiftyNetGlobalConfigTestBase.config_file))\n        self.assertEqual(global_config.get_niftynet_config_folder(),\n                         NiftyNetGlobalConfigTestBase.config_home)\n\n        # check if incorrect file was backed up\n        found_files = glob(\n            join(NiftyNetGlobalConfigTestBase.config_home,\n                 NiftyNetGlobalConfigTestBase.typify(\'config-backup-*\')))\n        self.assertTrue(len(found_files) == 1)\n        with open(found_files[0], \'r\') as backup_file:\n            self.assertEqual(backup_file.read(), incorrect_config)\n\n        # cleanup: remove backup file\n        NiftyNetGlobalConfigTestBase.remove_path(found_files[0])\n\n\nclass TestNonExistingNiftynetHomeCreated(NiftyNetGlobalConfigTestBase):\n\n    def test_non_existing_niftynet_home_created(self):\n        niftynet_home = expanduser(\n            NiftyNetGlobalConfigTestBase.default_config_opts[\'home\'])\n        NiftyNetGlobalConfigTestBase.remove_path(niftynet_home)\n        self.assertFalse(isdir(niftynet_home))\n        niftynet_ext = join(\n            niftynet_home, NiftyNetGlobalConfigTestBase.default_config_opts[\'ext\']\n        )\n        self.assertFalse(isfile(join(niftynet_ext, \'__init__.py\')))\n        for mod in NiftyNetGlobalConfigTestBase.default_config_opts[\'ext_mods\']:\n            self.assertFalse(isfile(join(niftynet_ext, mod, \'__init__.py\')))\n\n        global_config = NiftyNetGlobalConfig().setup()\n\n        self.assertTrue(isdir(niftynet_home))\n        self.assertTrue(isfile(join(niftynet_ext, \'__init__.py\')))\n        for mod in NiftyNetGlobalConfigTestBase.default_config_opts[\'ext_mods\']:\n            self.assertTrue(isfile(join(niftynet_ext, mod, \'__init__.py\')))\n\n\nclass TestExistingNiftynetHomeNotTouched(NiftyNetGlobalConfigTestBase):\n\n    def test_existing_niftynet_home_not_touched(self):\n        niftynet_home = expanduser(\n            NiftyNetGlobalConfigTestBase.default_config_opts[\'home\'])\n        makedirs(niftynet_home)\n        niftynet_ext = join(\n            niftynet_home, NiftyNetGlobalConfigTestBase.default_config_opts[\'ext\']\n        )\n        makedirs(niftynet_ext)\n        niftynet_ext_init = join(niftynet_ext, \'__init__.py\')\n        open(niftynet_ext_init, \'w\').close()\n        mtime_before = getmtime(niftynet_ext_init)\n\n        global_config = NiftyNetGlobalConfig()\n\n        mtime_after = getmtime(niftynet_ext_init)\n        self.assertEqual(mtime_before, mtime_after)\n'"
tests/niftynet_testcase.py,2,"b""import tensorflow as tf\n\n# This UGLY solution is done to bypass the issue\n# outlined in the NiftyNet issue #381 and Tensorflow\n# issue #29439\n#\n# https://github.com/NifTK/NiftyNet/issues/381\n# https://github.com/tensorflow/tensorflow/issues/29439\n\ntry:\n    delattr(tf.test.TestCase,'test_session')\nexcept AttributeError:\n    pass\n\n\nclass NiftyNetTestCase(tf.test.TestCase):\n    pass"""
tests/output_collector_test.py,28,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_variables import \\\n    NETWORK_OUTPUT, CONSOLE, TF_SUMMARIES\nfrom niftynet.engine.application_variables import OutputsCollector\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n# def get_test_network():\n#    net = ToyNet(num_classes=4)\n#    return net\n\n\nclass OutputCollectorTest(NiftyNetTestCase):\n    def test_add_to_single_device(self):\n        n_device = 1\n        collector = OutputsCollector(n_devices=n_device)\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx):\n                image = tf.ones([2, 32, 32, 32, 1])\n                foo = tf.zeros([2, 2])\n                bar = tf.zeros([42])\n                collector.add_to_collection(name=\'image\',\n                                            var=image,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'foo\',\n                                            var=foo,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'bar\',\n                                            var=bar,\n                                            collection=NETWORK_OUTPUT,\n                                            average_over_devices=False)\n        self.assertDictEqual(collector.variables(collection=CONSOLE),\n                             {\'image\': image, \'foo\': foo})\n        self.assertDictEqual(collector.variables(collection=NETWORK_OUTPUT),\n                             {\'bar\': bar})\n\n    def test_add_to_multiple_device(self):\n        n_device = 4\n        collector = OutputsCollector(n_devices=n_device)\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx):\n                image = tf.ones([2, 32, 32, 32, 1])\n                foo = tf.zeros([2, 2])\n                bar = tf.zeros([42])\n                collector.add_to_collection(name=\'image\',\n                                            var=image,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'foo\',\n                                            var=foo,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'bar\',\n                                            var=bar,\n                                            average_over_devices=True)\n        self.assertEqual(\n            set(collector.variables()),\n            {\'image_1\', \'image_3\', \'image_2\',\n             \'image\', \'foo_1\', \'foo_2\', \'foo_3\', \'foo\', \'bar\'})\n        self.assertEqual(len(collector.variables()[\'bar\']), n_device)\n        collector.finalise_output_op()\n        self.assertIsInstance(collector.variables()[\'bar\'], tf.Tensor)\n\n    def test_netout_single_device(self):\n        n_device = 1\n        collector = OutputsCollector(n_devices=n_device)\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx):\n                image = tf.ones([2, 32, 32, 32, 1])\n                foo = tf.zeros([2, 2])\n                collector.add_to_collection(name=\'image\',\n                                            var=image,\n                                            collection=NETWORK_OUTPUT,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'foo\',\n                                            var=foo,\n                                            collection=NETWORK_OUTPUT,\n                                            average_over_devices=False)\n            self.assertDictEqual(collector.variables(collection=NETWORK_OUTPUT),\n                                 {\'image\': image, \'foo\': foo})\n\n    def test_netout_multiple_device(self):\n        n_device = 4\n        collector = OutputsCollector(n_devices=n_device)\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx):\n                image = tf.ones([2, 32, 32, 32, 1])\n                foo = tf.zeros([2, 2])\n                bar = tf.zeros([42])\n                collector.add_to_collection(name=\'image\',\n                                            var=image,\n                                            collection=NETWORK_OUTPUT,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'foo\',\n                                            var=foo,\n                                            collection=NETWORK_OUTPUT,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'bar\',\n                                            var=bar,\n                                            collection=NETWORK_OUTPUT,\n                                            average_over_devices=True)\n        self.assertEqual(\n            set(collector.variables(NETWORK_OUTPUT)),\n            {\'image_1\', \'image_3\', \'image_2\',\n             \'image\', \'foo_1\', \'foo_2\', \'foo_3\', \'foo\', \'bar\'})\n        self.assertEqual(len(collector.variables(NETWORK_OUTPUT)[\'bar\']),\n                         n_device)\n        collector.finalise_output_op()\n        self.assertIsInstance(collector.variables(NETWORK_OUTPUT)[\'bar\'],\n                              tf.Tensor)\n\n    def test_tf_summary_single_device(self):\n        n_device = 1\n        collector = OutputsCollector(n_devices=n_device)\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx):\n                image = tf.ones([2, 32, 32, 32, 1])\n                foo = tf.zeros([2, 2])\n                collector.add_to_collection(name=\'image\',\n                                            var=image,\n                                            collection=TF_SUMMARIES,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'foo\',\n                                            var=foo,\n                                            collection=TF_SUMMARIES,\n                                            average_over_devices=False)\n        self.assertDictEqual(collector.summary_vars,\n                             {\'image\': image, \'foo\': foo})\n\n    def test_tf_summary_multiple_device(self):\n        n_device = 4\n        collector = OutputsCollector(n_devices=n_device)\n        for idx in range(n_device):\n            with tf.name_scope(\'worker_%d\' % idx):\n                image = tf.ones([2, 32, 32, 32, 1])\n                foo = tf.zeros([2, 2])\n                bar = tf.zeros([42])\n                collector.add_to_collection(name=\'image\',\n                                            var=image,\n                                            collection=TF_SUMMARIES,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'foo\',\n                                            var=foo,\n                                            collection=TF_SUMMARIES,\n                                            average_over_devices=False)\n                collector.add_to_collection(name=\'bar\',\n                                            var=bar,\n                                            collection=TF_SUMMARIES,\n                                            average_over_devices=True)\n        self.assertEqual(\n            set(collector.summary_vars),\n            {\'image_1\', \'image_3\', \'image_2\',\n             \'image\', \'foo_1\', \'foo_2\', \'foo_3\', \'foo\', \'bar\'})\n        self.assertEqual(len(collector.summary_vars[\'bar\']), n_device)\n        collector.finalise_output_op()\n        self.assertIsInstance(collector.summary_vars[\'bar\'], tf.Tensor)\n\n    def test_ill_add(self):\n        collector = OutputsCollector(n_devices=2)\n        foo = tf.zeros([2, 2])\n        bar = tf.zeros([42])\n        with self.assertRaisesRegexp(AssertionError, """"):\n            collector.add_to_collection(name=None, var=None)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            collector.add_to_collection(name=None, var=bar)\n        with self.assertRaisesRegexp(ValueError, """"):\n            collector.add_to_collection(name=foo, var=bar,\n                                        average_over_devices=True)\n            collector.add_to_collection(name=foo, var=bar,\n                                        average_over_devices=True)\n            collector.add_to_collection(name=foo, var=bar,\n                                        average_over_devices=True)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/pad_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.pad import PadLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass PaddingTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (16, 16, 16, 8)\n        x = np.ones(input_shape)\n        return x\n\n    def get_3d_input_dict(self):\n        input_shape = (13, 14, 10, 8)\n        x = {\'image\': np.ones(input_shape)}\n        return x\n\n    def run_test(self, is_array, layer_param, expected_shape):\n        if is_array:\n            x = self.get_3d_input()\n        else:\n            x = self.get_3d_input_dict()\n        padding_layer = PadLayer(**layer_param)\n        out_acti, _ = padding_layer(x)\n        print(padding_layer)\n        if is_array:\n            self.assertAllClose(out_acti.shape, expected_shape)\n        else:\n            self.assertAllClose(out_acti[\'image\'].shape, expected_shape)\n\n    def run_inverse_test(self, is_array, layer_param, expected_shape):\n        if is_array:\n            x = self.get_3d_input()\n        else:\n            x = self.get_3d_input_dict()\n        padding_layer = PadLayer(**layer_param)\n        out_acti, _ = padding_layer.inverse_op(x)\n        print(padding_layer)\n        if is_array:\n            self.assertAllClose(out_acti.shape, expected_shape)\n        else:\n            self.assertAllClose(out_acti[\'image\'].shape, expected_shape)\n\n    # 3d test\n    def test_3d_pad_shape(self):\n        input_param = {\'image_name\': (\'image\',),\n                       \'border\': (3,)}\n        self.run_test(True, input_param, (22, 16, 16, 8))\n        self.run_inverse_test(True, input_param, (10, 16, 16, 8))\n\n    def test_3d_twopad_shape(self):\n        input_param = {\'image_name\': (\'image\',),\n                       \'border\': (3, 3)}\n        self.run_test(True, input_param, (22, 22, 16, 8))\n        self.run_inverse_test(True, input_param, (10, 10, 16, 8))\n\n    def test_3d_int_pad_shape(self):\n        input_param = {\'image_name\': (\'image\',),\n                       \'border\': 3}\n        with self.assertRaisesRegexp(TypeError, \'iter\'):\n            self.run_test(True, input_param, (22, 22, 16, 8))\n        with self.assertRaisesRegexp(TypeError, \'iter\'):\n            self.run_inverse_test(True, input_param, (10, 10, 16, 8))\n\n    def test_3d_large_pad_shape(self):\n        input_param = {\'image_name\': (\'image\',),\n                       \'border\': (18,)}\n        self.run_test(True, input_param, (52, 16, 16, 8))\n        self.run_inverse_test(True, input_param, (16, 16, 16, 8))\n\n    # 3d dict test\n    def test_3d_dict_pad_shape(self):\n        input_param = {\'image_name\': (\'image\',),\n                       \'border\': (3,)}\n        self.run_test(False, input_param, (19, 14, 10, 8))\n        self.run_inverse_test(False, input_param, (7, 14, 10, 8))\n\n    def test_3d_dict_twopad_shape(self):\n        input_param = {\'image_name\': (\'image\',),\n                       \'border\': (3, 3)}\n        self.run_test(False, input_param, (19, 20, 10, 8))\n        self.run_inverse_test(False, input_param, (7, 8, 10, 8))\n\n    def test_3d_dict_int_pad_shape(self):\n        input_param = {\'image_name\': (\'image\',),\n                       \'border\': 3}\n        with self.assertRaisesRegexp(TypeError, \'iter\'):\n            self.run_test(False, input_param, (22, 22, 16, 8))\n        with self.assertRaisesRegexp(TypeError, \'iter\'):\n            self.run_inverse_test(False, input_param, (10, 10, 16, 8))\n\n    def test_3d_dict_large_pad_shape(self):\n        input_param = {\'image_name\': (\'image\',),\n                       \'border\': (18, 2, 1)}\n        self.run_test(False, input_param, (49, 18, 12, 8))\n        self.run_inverse_test(False, input_param, (13, 10, 8, 8))\n\n    def test_pad_to_simple(self):\n        rand_image = np.random.random([10, 10, 2, 1])\n        data_dict = {\'image\': rand_image}\n        tst = PadLayer((\'image\',), (0,), pad_to=(52, 52, 2))\n\n        padded = tst.layer_op(data_dict)\n        self.assertTrue(padded[0][\'image\'].shape == (52, 52, 2, 1))\n        depadded = tst.inverse_op(padded[0])\n        self.assertTrue(np.all(depadded[0][\'image\'] == rand_image))\n\n    def test_pad_to_smaller_window_than_input(self):\n        rand_image = np.random.random([10, 10, 2, 1])\n        data_dict = {\'image\': rand_image}\n        tst = PadLayer((\'image\',), (0,), pad_to=(5, 5, 10))\n        # test straightforward pad_to\n        padded = tst.layer_op(data_dict)\n        self.assertTrue(padded[0][\'image\'].shape == (10, 10, 10, 1))\n        depadded = tst.inverse_op(padded[0])\n        self.assertTrue(np.all(depadded[0][\'image\'] == rand_image))\n\n    def test_pad_to_odd_numbers(self):\n        rand_image = np.random.random([10, 10, 2, 1])\n        data_dict = {\'image\': rand_image}\n        tst = PadLayer((\'image\',), (0,), pad_to=(15, 17, 10))\n        # test straightforward pad_to\n        padded = tst.layer_op(data_dict)\n        self.assertTrue(padded[0][\'image\'].shape == (15, 17, 10, 1))\n        depadded = tst.inverse_op(padded[0])\n        self.assertTrue(np.all(depadded[0][\'image\'] == rand_image))\n\n    def test_pad_to_without_data_dict(self):\n        rand_image = np.random.random([10, 10, 2, 1])\n        data_dict = {\'image\': rand_image}\n        # test without dictionary\n        tst = PadLayer((\'image\',), (0,), pad_to=(5, 5, 10))\n        padded = tst.layer_op(data_dict[\'image\'])[0]\n        self.assertTrue(padded.shape == (10, 10, 10, 1))\n        depadded = tst.inverse_op(padded)[0]\n        self.assertTrue(np.all(depadded == rand_image))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/post_processing_test.py,3,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass PostProcessingTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def test_3d_shape(self):\n        x = self.get_3d_input()\n        post_process_layer = PostProcessingLayer(""SOFTMAX"")\n        print(post_process_layer)\n        out_post = post_process_layer(x)\n        print(post_process_layer)\n\n        with self.cached_session() as sess:\n            out = sess.run(out_post)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n\n    def test_2d_shape(self):\n        x = self.get_2d_input()\n        post_process_layer = PostProcessingLayer(""IDENTITY"")\n        out_post = post_process_layer(x)\n        print(post_process_layer)\n\n        with self.cached_session() as sess:\n            out = sess.run(out_post)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n\n    def test_3d_argmax_shape(self):\n        x = self.get_3d_input()\n        post_process_layer = PostProcessingLayer(""ARGMAX"")\n        out_post = post_process_layer(x)\n        print(post_process_layer)\n\n        with self.cached_session() as sess:\n            out = sess.run(out_post)\n            x_shape = tuple(x.shape.as_list()[:-1])\n            self.assertAllClose(x_shape + (1,), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/rand_bias_field_test.py,1,"b'from __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.rand_bias_field import RandomBiasFieldLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nSHAPE_4D = (10, 16, 16, 2)\nSHAPE_5D = (10, 32, 32, 8, 1)\n\n\nclass RandDeformationTests(NiftyNetTestCase):\n    # def get_3d_input(self):\n    #     input_3d = {\'image\': np.random.randn(10, 16, 2)}\n    #     interp_order = {\'image\': (3,) * 2}\n    #     return input_3d, interp_order\n\n    def get_4d_input(self):\n        input_4d = {\'image\': np.random.randn(*SHAPE_4D)}\n        interp_order = {\'image\': (3,) * 2}\n        return input_4d, interp_order\n\n    def get_5d_input(self):\n        input_5d = {\'image\': np.random.randn(*SHAPE_5D)}\n        interp_order = {\'image\': (3,)}\n        return input_5d, interp_order\n\n    def test_4d_shape(self):\n        x, interp_orders = self.get_4d_input()\n        rand_bias_field_layer = RandomBiasFieldLayer()\n        rand_bias_field_layer.randomise()\n        out = rand_bias_field_layer(x, interp_orders)\n        self.assertEqual(out[\'image\'].shape, SHAPE_4D)\n\n    def test_5d_shape(self):\n        x, interp_orders = self.get_5d_input()\n        rand_bias_field_layer = RandomBiasFieldLayer()\n        rand_bias_field_layer.randomise()\n        out = rand_bias_field_layer(x, interp_orders)\n        self.assertEqual(out[\'image\'].shape, SHAPE_5D)\n\n    def test_augmentation(self):\n        rand_bias_field_layer = RandomBiasFieldLayer()\n        x, interp_orders = self.get_5d_input()\n        x_old = np.copy(x[\'image\'])\n        for _ in range(10):\n            x[\'image\'] = np.copy(x_old)\n\n            rand_bias_field_layer.randomise()\n            out = rand_bias_field_layer(x, interp_orders)\n            out = np.copy(out[\'image\'])\n\n            rand_bias_field_layer.init_order(2)\n            rand_bias_field_layer.init_uniform_coeff((-5.0, 5.0))\n\n            rand_bias_field_layer.randomise()\n            x[\'image\'] = np.copy(x_old)\n            out2 = rand_bias_field_layer(x, interp_orders)\n\n            self.assertFalse(np.array_equal(out, x_old))\n            self.assertFalse(np.array_equal(out2[\'image\'], x_old))\n            self.assertFalse(np.array_equal(out, out2[\'image\']))\n\n    # def test_gugmentation_on_2d_imgs(self):\n    #     rand_bias_field_layer = RandomBiasFieldLayer()\n    #     for _ in range(100):\n    #         x, interp_orders = self.get_3d_input()\n    #         x_old = np.copy(x[\'image\'])\n\n    #         rand_bias_field_layer.randomise(x)\n    #         out = rand_bias_field_layer(x, interp_orders)\n\n    #         with self.cached_session():\n    #             self.assertFalse(np.array_equal(out[\'image\'], x_old))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/rand_elastic_deformation_test.py,1,"b'from __future__ import absolute_import, print_function\n\nimport unittest\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.rand_elastic_deform import RandomElasticDeformationLayer\nfrom niftynet.layer.rand_elastic_deform import sitk\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nSHAPE_3D = (10, 16, 2)\nSHAPE_4D = (10, 16, 16, 2)\nSHAPE_5D = (10, 32, 32, 8, 1)\n\n\n@unittest.skipIf(not sitk, \'SimpleITK not found\')\nclass RandDeformationTests(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_3d = {\'testdata\': np.random.randn(*SHAPE_3D)}\n        interp_order = {\'testdata\': (3,) * 2}\n        return input_3d, interp_order\n\n    def get_4d_input(self):\n        input_4d = {\'testdata\': np.random.randn(*SHAPE_4D)}\n        interp_order = {\'testdata\': (3,) * 2}\n        return input_4d, interp_order\n\n    def get_5d_input(self):\n        input_5d = {\'testdata\': np.random.randn(*SHAPE_5D)}\n        interp_order = {\'testdata\': (3,)}\n        return input_5d, interp_order\n\n    def test_4d_shape(self):\n        x, interp_orders = self.get_4d_input()\n        rand_deformation_layer = RandomElasticDeformationLayer(num_controlpoints=4,\n                                                               std_deformation_sigma=15,\n                                                               proportion_to_augment=0.5)\n        rand_deformation_layer.randomise(x)\n        out = rand_deformation_layer(x, interp_orders)\n        self.assertEqual(out[\'testdata\'].shape, SHAPE_4D)\n\n    def test_5d_shape(self):\n        x, interp_orders = self.get_5d_input()\n        rand_deformation_layer = RandomElasticDeformationLayer(num_controlpoints=4,\n                                                               std_deformation_sigma=15,\n                                                               proportion_to_augment=0.5)\n        rand_deformation_layer.randomise(x)\n        out = rand_deformation_layer(x, interp_orders)\n        self.assertEqual(out[\'testdata\'].shape, SHAPE_5D)\n\n    def test_no_deformation(self):\n        # testing the \'proportion_to_augment\' parameter\n        rand_deformation_layer = RandomElasticDeformationLayer(num_controlpoints=4,\n                                                               std_deformation_sigma=15,\n                                                               proportion_to_augment=0.)\n        for _ in range(100):\n            x, interp_orders = self.get_5d_input()\n            x_old = np.copy(x[\'testdata\'])\n            rand_deformation_layer.randomise(x)\n            out = rand_deformation_layer(x, interp_orders)\n\n            with self.cached_session():\n                self.assertTrue(np.array_equal(out[\'testdata\'], x_old))\n\n    def test_deformation(self):\n        # testing the \'proportion_to_augment\' parameter\n\n        rand_deformation_layer = RandomElasticDeformationLayer(num_controlpoints=4,\n                                                               std_deformation_sigma=1,\n                                                               proportion_to_augment=1.)\n        for _ in range(100):\n            x, interp_orders = self.get_5d_input()\n            x_old = np.copy(x[\'testdata\'])\n\n            rand_deformation_layer.randomise(x)\n            out = rand_deformation_layer(x, interp_orders)\n\n            with self.cached_session():\n                self.assertFalse(np.array_equal(out[\'testdata\'], x_old))\n\n    def test_deformation_on_2d_imgs(self):\n        # testing the \'proportion_to_augment\' parameter\n\n        rand_deformation_layer = RandomElasticDeformationLayer(num_controlpoints=4,\n                                                               std_deformation_sigma=1,\n                                                               proportion_to_augment=1.,\n                                                               spatial_rank = 2)\n        for _ in range(100):\n            x, interp_orders = self.get_3d_input()\n            x_old = np.copy(x[\'testdata\'])\n\n            rand_deformation_layer.randomise(x)\n            out = rand_deformation_layer(x, interp_orders)\n\n            with self.cached_session():\n                self.assertFalse(np.array_equal(out[\'testdata\'], x_old))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/rand_flip_test.py,1,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass RandFlipTest(NiftyNetTestCase):\n    def test_1d_flip(self):\n        a = np.array([[0, 1], [2, 3]])\n        flip_layer = RandomFlipLayer(flip_axes=[0], flip_probability=1)\n        flip_layer.randomise(spatial_rank=2)\n        transformed_a = flip_layer._apply_transformation(a)\n        with self.cached_session() as sess:\n            self.assertTrue(\n                np.array_equal(transformed_a, np.array([[2, 3], [0, 1]])))\n\n    def test_no_flip(self):\n        a = np.array([[0, 1], [2, 3]])\n        flip_layer = RandomFlipLayer(flip_axes=[0], flip_probability=0)\n        flip_layer.randomise(spatial_rank=2)\n        transformed_a = flip_layer._apply_transformation(a)\n        with self.cached_session() as sess:\n            self.assertTrue(np.array_equal(transformed_a, a))\n\n    def test_3d_flip(self):\n        a = np.zeros(24).reshape(2, 3, 4)\n        a[0, 0, 0] = 1\n        flip_layer = RandomFlipLayer(flip_axes=[0, 1, 2], flip_probability=1)\n        flip_layer.randomise(spatial_rank=3)\n        transformed_a = flip_layer._apply_transformation(a)\n        with self.cached_session() as sess:\n            # cube of zeros with opposite corner as 1\n            expected_a = np.zeros(24).reshape(2, 3, 4)\n            expected_a[-1, -1, -1] = 1\n            self.assertTrue(np.array_equal(transformed_a, expected_a))\n\n    def test_no_flip_layer(self):\n        a = np.array([[0, 1], [2, 3]])\n        flip_layer = RandomFlipLayer(flip_axes=[0], flip_probability=0)\n        flip_layer.randomise(spatial_rank=2)\n        transformed_a = flip_layer(a)\n        with self.cached_session() as sess:\n            self.assertTrue(np.array_equal(transformed_a, a))\n\n    def test_2d_flip_layer(self):\n        a = np.array([[0, 1], [2, 3]])\n        flip_layer = RandomFlipLayer(flip_axes=[0], flip_probability=1)\n        flip_layer.randomise(spatial_rank=2)\n        transformed_a = flip_layer(a)\n        with self.cached_session() as sess:\n            self.assertTrue(\n                np.array_equal(transformed_a, np.array([[2, 3], [0, 1]])))\n\n    def test_2d_flip_layer_1(self):\n        a = np.array([[0, 1], [2, 3]])\n        a = {'image': a}\n        i = {'image': [0]}\n        flip_layer = RandomFlipLayer(flip_axes=[0], flip_probability=1)\n        flip_layer.randomise(spatial_rank=2)\n        transformed_a = flip_layer(a, i)\n        with self.cached_session() as sess:\n            self.assertTrue(\n                np.array_equal(transformed_a['image'],\n                               np.array([[2, 3], [0, 1]])))\n\n\nif __name__ == '__main__':\n    tf.test.main()\n"""
tests/rand_rotation_test.py,1,"b'from __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass RandRotationTest(NiftyNetTestCase):\n    def get_4d_input(self):\n        input_4d = {\'testdata\': np.ones((16, 16, 16, 8))}\n        interp_order = {\'testdata\': (3,) * 8}\n        return input_4d, interp_order\n\n    def get_5d_input(self):\n        input_5d = {\'testdata\': np.ones((32, 32, 32, 8, 1))}\n        interp_order = {\'testdata\': (3,)}\n        return input_5d, interp_order\n\n    def test_4d_shape(self):\n        x, interp_orders = self.get_4d_input()\n        rand_rotation_layer = RandomRotationLayer()\n        rand_rotation_layer.init_uniform_angle((-10.0, 10.0))\n        rand_rotation_layer.randomise()\n        out = rand_rotation_layer(x, interp_orders)\n\n    def test_5d_shape(self):\n        x, interp_orders = self.get_5d_input()\n        rand_rotation_layer = RandomRotationLayer()\n        rand_rotation_layer.init_uniform_angle((-10.0, 10.0))\n        rand_rotation_layer.randomise()\n        out = rand_rotation_layer(x, interp_orders)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/rand_spatial_scaling_test.py,1,"b'from __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass RandRotationTest(NiftyNetTestCase):\n    def get_4d_input(self):\n        input_4d = {\'testdata\': np.ones((16, 16, 16, 8))}\n        interp_order = {\'testdata\': (3,) * 8}\n        return input_4d, interp_order\n\n    def get_5d_input(self):\n        input_5d = {\'testdata\': np.ones((32, 32, 32, 8, 1))}\n        interp_order = {\'testdata\': (1,)}\n        return input_5d, interp_order\n\n    def test_4d_shape(self):\n        x, interp = self.get_4d_input()\n        rand_layer = RandomSpatialScalingLayer(min_percentage=-10.0,\n                                               max_percentage=10.0)\n        rand_layer.randomise()\n        out = rand_layer(x, interp)\n\n    def test_5d_shape(self):\n        x, interp = self.get_5d_input()\n        rand_layer = RandomSpatialScalingLayer(min_percentage=-10.0,\n                                               max_percentage=10.0)\n        rand_layer.randomise()\n        out = rand_layer(x, interp)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/reader_modular_test.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport os\n\nfrom PIL import Image\nimport nibabel as nib\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.io.image_reader import ImageReader\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n# from niftynet.io.image_sets_partitioner import ImageSetsPartitioner\n\n\nSEG_THRESHOLD = 100\n\n\ndef generate_2d_images():\n    # Test 2D Image Loader\n\n    # Generate 10 fake 2d grayscale and color images of size 100x100\n    img_path = os.path.join(os.path.dirname(__file__), \'..\', \'testing_data\')\n    img_path = os.path.realpath(os.path.join(img_path, \'images2d\'))\n    if not os.path.isdir(img_path):\n        os.makedirs(img_path)\n\n    # Generate fake testing data\n    for i in range(10):\n        img1 = np.random.randint(0, 255, size=(100, 100, 3)).astype(np.uint8)\n        gray = np.random.randint(0, 255, size=(100, 100)).astype(np.uint8)\n        mask = ((gray > SEG_THRESHOLD) * 255).astype(np.uint8)\n        Image.fromarray(img1).save(os.path.join(img_path, \'img%d_u.png\' % i))\n        Image.fromarray(gray).save(os.path.join(img_path, \'img%d_g.png\' % i))\n        Image.fromarray(mask).save(os.path.join(img_path, \'img%d_m.png\' % i))\n    return\n\n\ndef generate_2d_1d_images():\n    img_path = os.path.join(os.path.dirname(__file__), \'..\', \'testing_data\')\n    img_path = os.path.realpath(os.path.join(img_path, \'images_x_1_y\'))\n    if not os.path.isdir(img_path):\n        os.makedirs(img_path)\n    for idx in range(3):\n        image_data = np.random.randint(0, 255, size=(100, 1, 100))\n        image_data = image_data.astype(np.uint8)\n        nib_image = nib.Nifti1Image(image_data, np.eye(4))\n        nib.save(nib_image,\n                 os.path.join(img_path, \'x_1_y_{}.nii.gz\'.format(idx)))\n    return\n\n\ndef generate_3d_1_1_d_images():\n    img_path = os.path.join(os.path.dirname(__file__), \'..\', \'testing_data\')\n    img_path = os.path.realpath(os.path.join(img_path, \'images_x_y_z_1_1\'))\n    if not os.path.isdir(img_path):\n        os.makedirs(img_path)\n    for idx in range(3):\n        image_data = np.random.randint(0, 255, size=(50, 24, 42, 1, 1))\n        image_data = image_data.astype(np.uint8)\n        nib_image = nib.Nifti1Image(image_data, np.eye(4))\n        nib.save(nib_image,\n                 os.path.join(img_path, \'x_y_z_1_1_{}.nii.gz\'.format(idx)))\n    return\n\n\ngenerate_2d_images()\ngenerate_2d_1d_images()\ngenerate_3d_1_1_d_images()\n\nIMAGE_PATH_2D = os.path.join(\'.\', \'testing_data\', \'images2d\')\nIMAGE_PATH_2D_1 = os.path.join(\'.\', \'example_volumes\', \'gan_test_data\')\nIMAGE_PATH_2D_2 = os.path.join(\'.\', \'testing_data\', \'images_x_1_y\')\nIMAGE_PATH_3D = os.path.join(\'.\', \'testing_data\')\nIMAGE_PATH_3D_1 = os.path.join(\'.\', \'testing_data\', \'images_x_y_z_1_1\')\n\n\nclass Read2DTest(NiftyNetTestCase):\n    def default_property_asserts(self, reader):\n        self.assertDictEqual(reader.spatial_ranks, {\'mr\': 2})\n        self.assertDictEqual(reader.input_sources, {\'mr\': (\'mr\',)})\n        self.assertDictEqual(reader.shapes, {\'mr\': (100, 100, 1, 1, 1)})\n        self.assertDictEqual(reader.tf_dtypes, {\'mr\': tf.float32})\n        self.assertEqual(reader.names, (\'mr\',))\n        self.assertEqual(len(reader.output_list), 30)\n\n    def renamed_property_asserts(self, reader):\n        # test properties\n        self.assertDictEqual(reader.spatial_ranks, {\'ct\': 2})\n        self.assertDictEqual(reader.input_sources, {\'ct\': (\'mr\',)})\n        self.assertDictEqual(reader.shapes, {\'ct\': (100, 100, 1, 1, 1)})\n        self.assertDictEqual(reader.tf_dtypes, {\'ct\': tf.float32})\n        self.assertEqual(reader.names, (\'ct\',))\n        self.assertEqual(len(reader.output_list), 30)\n\n    def test_simple(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D}}\n        reader = ImageReader().initialise(data_param)\n        # test properties\n        self.default_property_asserts(reader)\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,)})\n        self.assertEqual(data[\'mr\'].shape, (100, 100, 1, 1, 1))\n\n    def test_renaming(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D}}\n        group_param = {\'ct\': (\'mr\',)}\n        reader = ImageReader().initialise(data_param, group_param)\n        self.renamed_property_asserts(reader)\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'ct\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'ct\': (1,)})\n        self.assertEqual(data[\'ct\'].shape, (100, 100, 1, 1, 1))\n\n    def test_reader_field(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D}}\n        group_param = {\'ct\': (\'mr\',)}\n        reader = ImageReader([\'ct\']).initialise(data_param, group_param)\n        self.renamed_property_asserts(reader)\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'ct\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'ct\': (1,)})\n        self.assertEqual(data[\'ct\'].shape, (100, 100, 1, 1, 1))\n\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            # grouping name \'ct\' but\n            reader = ImageReader([\'mr\']).initialise(data_param, group_param)\n\n    def test_input_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D,\n                             \'csv_file\': \'2d_test.csv\'}}\n        reader = ImageReader().initialise(data_param)\n        self.default_property_asserts(reader)\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,)})\n        self.assertEqual(data[\'mr\'].shape, (100, 100, 1, 1, 1))\n\n    def test_no_2d_resampling_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D,\n                             \'csv_file\': \'2d_test.csv\',\n                             \'pixdim\': (2, 2, 2),\n                             \'axcodes\': \'RAS\'}}\n        reader = ImageReader().initialise(data_param)\n        self.assertEqual(reader.output_list[0][\'mr\'].output_pixdim, (None,))\n        self.assertEqual(reader.output_list[0][\'mr\'].output_axcodes, (None,))\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,)})\n        self.assertEqual(data[\'mr\'].shape, (100, 100, 1, 1, 1))\n\n    def test_2d_as_5D_multimodal_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D,\n                             \'filename_contains\': \'_u\',\n                             \'pixdim\': (2, 2, 2),\n                             \'axcodes\': \'RAS\'}}\n        grouping_param = {\'ct\': (\'mr\', \'mr\', \'mr\')}\n        reader = ImageReader().initialise(data_param, grouping_param)\n        self.assertEqual(reader.spatial_ranks, {\'ct\': 2})\n\n    def test_2D_multimodal_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D,\n                             \'filename_contains\': \'_g\',\n                             \'pixdim\': (2, 1.5, 2),\n                             \'axcodes\': \'RAS\'}}\n        grouping_param = {\'ct\': (\'mr\', \'mr\', \'mr\')}\n        reader = ImageReader().initialise(data_param, grouping_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'ct\': 2})\n        self.assertEqual(reader.output_list[0][\'ct\'].output_pixdim,\n                         ((2.0, 1.5, 2.0),) * 3)\n        self.assertEqual(reader.output_list[0][\'ct\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),) * 3)\n\n        # test output\n        idx, data, interp = reader()\n        self.assertTrue(\'ct\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'ct\': (1, 1, 1)})\n        self.assertEqual(data[\'ct\'].shape, (100, 100, 1, 1, 3))\n\n\nclass Read2D_1DTest(NiftyNetTestCase):\n    # loading 2d images of rank 3: [x, y, 1]\n    def test_no_2d_resampling_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D_1,\n                             \'csv_file\': \'2d_test.csv\',\n                             \'filename_contains\': \'_img\',\n                             \'pixdim\': (2, 2, 2),\n                             \'axcodes\': \'RAS\'}}\n        reader = ImageReader().initialise(data_param)\n        self.assertEqual(reader.output_list[0][\'mr\'].output_pixdim,\n                         ((2.0, 2.0, 2.0),))\n        self.assertEqual(reader.output_list[0][\'mr\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),))\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,)})\n        self.assertEqual(data[\'mr\'].shape, (120, 160, 1, 1, 1))\n\n    def test_2D_multimodal_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D_1,\n                             \'filename_contains\': \'_img\',\n                             \'pixdim\': (2, 1.5, 2),\n                             \'axcodes\': \'RAS\'}}\n        grouping_param = {\'ct\': (\'mr\', \'mr\', \'mr\')}\n        reader = ImageReader().initialise(data_param, grouping_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'ct\': 2})\n        self.assertEqual(reader.output_list[0][\'ct\'].output_pixdim,\n                         ((2.0, 1.5, 2.0),) * 3)\n        self.assertEqual(reader.output_list[0][\'ct\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),) * 3)\n\n        # test output\n        idx, data, interp = reader()\n        self.assertTrue(\'ct\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'ct\': (1, 1, 1)})\n        self.assertEqual(data[\'ct\'].shape, (120, 160, 1, 1, 3))\n\n\nclass Read2D_1D_x1y_Test(NiftyNetTestCase):\n    # loading 2d images of rank 3: [x, 1, y]\n    def test_no_2d_resampling_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D_2,\n                             \'filename_contains\': \'x_1_y\',\n                             \'pixdim\': (2, 2, 2),\n                             \'axcodes\': \'RAS\'}}\n        reader = ImageReader().initialise(data_param)\n        self.assertEqual(reader.output_list[0][\'mr\'].output_pixdim,\n                         ((2.0, 2.0, 2.0),))\n        self.assertEqual(reader.output_list[0][\'mr\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),))\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,)})\n        self.assertEqual(data[\'mr\'].shape, (100, 100, 1, 1, 1))\n\n    def test_2D_multimodal_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D_2,\n                             \'filename_contains\': \'x_1_y\',\n                             \'pixdim\': (2, 1.5, 2),\n                             \'axcodes\': \'RAS\'}}\n        grouping_param = {\'ct\': (\'mr\', \'mr\', \'mr\')}\n        reader = ImageReader().initialise(data_param, grouping_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'ct\': 2})\n        self.assertEqual(reader.output_list[0][\'ct\'].output_pixdim,\n                         ((2.0, 1.5, 2.0),) * 3)\n        self.assertEqual(reader.output_list[0][\'ct\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),) * 3)\n\n        # test output\n        idx, data, interp = reader()\n        self.assertTrue(\'ct\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'ct\': (1, 1, 1)})\n        self.assertEqual(data[\'ct\'].shape, (100, 100, 1, 1, 3))\n\n\nclass Read2D_colorTest(NiftyNetTestCase):\n    # loading 2d images of rank 3: [x, y, 3] or [x, y, 4]\n    def test_no_2d_resampling_properties(self):\n        data_param = {\'mr\': {\'path_to_search\': IMAGE_PATH_2D,\n                             \'csv_file\': \'2d_test.csv\',\n                             \'filename_contains\': \'_u\',\n                             \'pixdim\': (2, 2, 2),\n                             \'axcodes\': \'RAS\'}}\n        reader = ImageReader().initialise(data_param)\n        self.assertEqual(reader.output_list[0][\'mr\'].output_pixdim,\n                         ((2.0, 2.0, 2.0),))\n        self.assertEqual(reader.output_list[0][\'mr\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),))\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,)})\n        self.assertEqual(data[\'mr\'].shape, (100, 100, 1, 1, 3))\n\n    def test_2D_multimodal_properties(self):\n        data_param = {\n            \'mr\': {\'path_to_search\': IMAGE_PATH_2D,\n                   \'filename_contains\': \'_u\',\n                   \'pixdim\': (2, 1.5, 2),\n                   \'axcodes\': \'RAS\'}}\n        grouping_param = {\'ct\': (\'mr\', \'mr\', \'mr\')}\n        reader = ImageReader().initialise(data_param, grouping_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'ct\': 2})\n        self.assertEqual(reader.output_list[0][\'ct\'].output_pixdim,\n                         ((2.0, 1.5, 2.0),) * 3)\n        self.assertEqual(reader.output_list[0][\'ct\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),) * 3)\n\n        # test output\n        idx, data, interp = reader()\n        self.assertTrue(\'ct\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'ct\': (1, 1, 1)})\n        self.assertEqual(data[\'ct\'].shape, (100, 100, 1, 1, 9))\n\n\nclass Read3DTest(NiftyNetTestCase):\n    # loading 3d images of rank 3: [x, y, z]\n    def test_3d_resampling_properties(self):\n        data_param = {\n            \'mr\': {\'path_to_search\': IMAGE_PATH_3D,\n                   \'filename_contains\': \'Lesion\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'}}\n        reader = ImageReader().initialise(data_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'mr\': 3})\n        self.assertEqual(reader.output_list[0][\'mr\'].output_pixdim,\n                         ((4.0, 3.0, 4.0),))\n        self.assertEqual(reader.output_list[0][\'mr\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),))\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,)})\n        # allows rounding error spatially\n        self.assertAllClose(data[\'mr\'].shape[:3], (62, 83, 62), atol=1)\n        self.assertAllClose(data[\'mr\'].shape[3:], (1, 1))\n\n    def test_3d_multiple_properties(self):\n        """"""\n        loading two modalities, grouping subject names only\n        """"""\n        data_param = {\n            \'mr\': {\'path_to_search\': IMAGE_PATH_3D,\n                   \'filename_contains\': \'Lesion\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'},\n            \'ct\': {\'path_to_search\': IMAGE_PATH_3D,\n                   \'filename_contains\': \'Lesion\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'}}\n        reader = ImageReader().initialise(data_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'mr\': 3, \'ct\': 3})\n        self.assertEqual(reader.output_list[0][\'mr\'].output_pixdim,\n                         ((4.0, 3.0, 4.0),))\n        self.assertEqual(reader.output_list[0][\'mr\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),))\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(\'ct\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,), \'ct\': (1,)})\n        # allows rounding error spatially\n        self.assertAllClose(data[\'mr\'].shape[:3], (62, 83, 62), atol=1)\n        self.assertAllClose(data[\'mr\'].shape[3:], (1, 1))\n        self.assertAllClose(data[\'ct\'].shape[:3], (62, 83, 62), atol=1)\n        self.assertAllClose(data[\'ct\'].shape[3:], (1, 1))\n\n    def test_3d_concat_properties(self):\n        """"""\n        loading two modalities, grouping subject names only\n        """"""\n        data_param = {\n            \'mr\': {\'path_to_search\': IMAGE_PATH_3D,\n                   \'filename_contains\': \'LesionFin\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'},\n            \'ct\': {\'path_to_search\': IMAGE_PATH_3D,\n                   \'filename_contains\': \'FLAIR\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'}}\n        grouping_param = {\'image\': (\'mr\', \'ct\')}\n        reader = ImageReader().initialise(data_param, grouping_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'image\': 3})\n        self.assertEqual(reader.output_list[0][\'image\'].output_pixdim,\n                         ((4.0, 3.0, 4.0),) * 2)\n        self.assertEqual(reader.output_list[0][\'image\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),) * 2)\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'image\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'image\': (1, 1)})\n        # allows rounding error spatially\n        self.assertAllClose(data[\'image\'].shape[:3], (62, 83, 62), atol=1)\n        self.assertAllClose(data[\'image\'].shape[3:], (1, 2))\n\n\nclass Read3D_1_1_Test(NiftyNetTestCase):\n    # loading 5d images of rank 3: [x, y, z, 1, 1]\n    def test_3d_resampling_properties(self):\n        data_param = {\n            \'mr\': {\'path_to_search\': IMAGE_PATH_3D_1,\n                   \'filename_contains\': \'x_y_z_1_1\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'}}\n        reader = ImageReader().initialise(data_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'mr\': 3})\n        self.assertEqual(reader.output_list[0][\'mr\'].output_pixdim,\n                         ((4.0, 3.0, 4.0),))\n        self.assertEqual(reader.output_list[0][\'mr\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),))\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,)})\n        # allows rounding error spatially\n        self.assertAllClose(data[\'mr\'].shape[:3], (12, 8, 10), atol=1)\n        self.assertAllClose(data[\'mr\'].shape[3:], (1, 1))\n\n    def test_3d_multiple_properties(self):\n        """"""\n        loading two modalities, grouping subject names only\n        """"""\n        data_param = {\n            \'mr\': {\'path_to_search\': IMAGE_PATH_3D_1,\n                   \'filename_contains\': \'x_y_z_1_1\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'},\n            \'ct\': {\'path_to_search\': IMAGE_PATH_3D_1,\n                   \'filename_contains\': \'x_y_z_1_1\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'}}\n        reader = ImageReader().initialise(data_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'mr\': 3, \'ct\': 3})\n        self.assertEqual(reader.output_list[0][\'mr\'].output_pixdim,\n                         ((4.0, 3.0, 4.0),))\n        self.assertEqual(reader.output_list[0][\'mr\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),))\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'mr\' in data)\n        self.assertTrue(\'ct\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'mr\': (1,), \'ct\': (1,)})\n        # allows rounding error spatially\n        self.assertAllClose(data[\'mr\'].shape[:3], (12, 8, 10), atol=1)\n        self.assertAllClose(data[\'mr\'].shape[3:], (1, 1))\n        self.assertAllClose(data[\'ct\'].shape[:3], (12, 8, 10), atol=1)\n        self.assertAllClose(data[\'ct\'].shape[3:], (1, 1))\n\n    def test_3d_concat_properties(self):\n        """"""\n        loading two modalities, grouping subject names only\n        """"""\n        data_param = {\n            \'mr\': {\'path_to_search\': IMAGE_PATH_3D_1,\n                   \'filename_contains\': \'x_y_z_1_1\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'},\n            \'ct\': {\'path_to_search\': IMAGE_PATH_3D_1,\n                   \'filename_contains\': \'x_y_z_1_1\',\n                   \'pixdim\': (4, 3, 4),\n                   \'axcodes\': \'RAS\'}}\n        grouping_param = {\'image\': (\'mr\', \'ct\')}\n        reader = ImageReader().initialise(data_param, grouping_param)\n        self.assertDictEqual(reader.spatial_ranks, {\'image\': 3})\n        self.assertEqual(reader.output_list[0][\'image\'].output_pixdim,\n                         ((4.0, 3.0, 4.0),) * 2)\n        self.assertEqual(reader.output_list[0][\'image\'].output_axcodes,\n                         ((\'R\', \'A\', \'S\'),) * 2)\n        idx, data, interp = reader()\n\n        # test output\n        self.assertTrue(\'image\' in data)\n        self.assertTrue(idx in range(len(reader.output_list)))\n        self.assertDictEqual(interp, {\'image\': (1, 1)})\n        # allows rounding error spatially\n        self.assertAllClose(data[\'image\'].shape[:3], (12, 8, 10), atol=1)\n        self.assertAllClose(data[\'image\'].shape[3:], (1, 2))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/resampler_batch_test.py,52,"b'from __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass ResamplerTest(NiftyNetTestCase):\n\n    def test_shape_interface(self):\n        test_input = tf.zeros((2, 10, 10, 10, 3))\n        test_coords = tf.zeros((3, 5, 5, 5, 3))\n        # bad batch sizes\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            out = ResamplerLayer()(test_input, test_coords)\n\n        test_input = tf.zeros((2, 10, 10, 10, 3))\n        test_coords = tf.zeros((5, 5, 5, 3))\n        # bad batch sizes\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            out = ResamplerLayer()(test_input, test_coords)\n\n        test_input = tf.zeros((1, 10, 10, 3))\n        test_coords = tf.zeros((1, 5, 5, 3))\n        # bad n coordinates\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            out = ResamplerLayer()(test_input, test_coords)\n\n    def test_linear_shape(self):\n        # 3D\n        test_input = np.zeros((2, 8, 8, 8, 2))\n        test_input[0, 0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 5, 5, 3)) * 0.1\n        out = ResamplerLayer(""LINEAR"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 0.9**3, atol=1e-5)))\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertEqual(out_value.shape, (2, 5, 5, 5, 2))\n\n        # 2D\n        test_input = np.zeros((2, 8, 8, 2))\n        test_input[0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 5, 2)) * 0.1\n        out = ResamplerLayer(""LINEAR"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 0.9**2, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 2))\n\n        # 1D\n        test_input = np.zeros((2, 8, 2))\n        test_input[0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 1)) * 0.1\n        out = ResamplerLayer(""LINEAR"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 0.9, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 2))\n\n    def test_linear_no_broadcasting(self):\n        # 3D\n        test_input = np.zeros((2, 8, 8, 8, 2))\n        test_input[:, 0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 5, 5, 3)) * 0.1,\n                                 tf.ones((1, 5, 5, 5, 3)) * 0.2], axis=0)\n        out = ResamplerLayer(""LINEAR"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 0.9**3, atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.8**3, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 5, 2))\n        # 2D\n        test_input = np.zeros((2, 8, 8, 2))\n        test_input[:, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 5, 2)) * 0.1,\n                                 tf.ones((1, 5, 5, 2)) * 0.2], axis=0)\n        out = ResamplerLayer(""LINEAR"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 0.9**2, atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.8**2, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 2))\n        # 1D\n        test_input = np.zeros((2, 8, 2))\n        test_input[:, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 1)) * 0.1,\n                                 tf.ones((1, 5, 1)) * 0.2], axis=0)\n        out = ResamplerLayer(""LINEAR"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 0.9, atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.8, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 2))\n\n\n    def test_nearest_shape(self):\n        # 3D\n        test_input = np.zeros((2, 8, 8, 8, 2))\n        test_input[0, 0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 5, 5, 3)) * 0.1\n        out = ResamplerLayer(""NEAREST"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 1.0, atol=1e-5)))\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertEqual(out_value.shape, (2, 5, 5, 5, 2))\n\n        # 2D\n        test_input = np.zeros((2, 8, 8, 2))\n        test_input[0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 5, 2)) * 0.1\n        out = ResamplerLayer(""NEAREST"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 1.0, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 2))\n\n        # 1D\n        test_input = np.zeros((2, 8, 2))\n        test_input[0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 1)) * 0.1\n        out = ResamplerLayer(""NEAREST"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 1.0, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 2))\n\n    def test_nearest_no_broadcasting(self):\n        # 3D\n        test_input = np.zeros((2, 3, 3, 3, 2))\n        test_input[:, 0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 5, 5, 3)) * 0.1,\n                                 tf.ones((1, 5, 5, 5, 3)) * 1.2], axis=0)\n        out = ResamplerLayer(""NEAREST"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 1.0, atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.0, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 5, 2))\n        # 2D\n        test_input = np.zeros((2, 3, 3, 2))\n        test_input[:, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 5, 2)) * 0.1,\n                                 tf.ones((1, 5, 5, 2)) * 1.2], axis=0)\n        out = ResamplerLayer(""NEAREST"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 1.0, atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.0, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 2))\n        # 1D\n        test_input = np.zeros((2, 3, 2))\n        test_input[:, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 1)) * 0.1,\n                                 tf.ones((1, 5, 1)) * 1.2], axis=0)\n        out = ResamplerLayer(""NEAREST"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0], 1.0, atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.0, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 2))\n\n    def test_idw_shape(self):\n        # 3D\n        test_input = np.zeros((2, 8, 8, 8, 2))\n        test_input[0, 0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 5, 5, 3)) * 0.1\n        out = ResamplerLayer(""IDW"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0],\n                    1.0/(1. + 9./83 + 9./163 + 3./243), atol=1e-5)))\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertEqual(out_value.shape, (2, 5, 5, 5, 2))\n\n        # 2D\n        test_input = np.zeros((2, 8, 8, 2))\n        test_input[0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 5, 2)) * 0.1\n        out = ResamplerLayer(""IDW"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0],\n                    1./(2./41. + 1./81.0 + 1.0), atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 2))\n\n        # 1D\n        test_input = np.zeros((2, 8, 2))\n        test_input[0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.ones((1, 5, 1)) * 0.1\n        out = ResamplerLayer(""IDW"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(np.all(out_value[1, ...]==0))\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0],\n                       100.0/(100.0+1/0.81), atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 2))\n\n    def test_idw_no_broadcasting(self):\n        # 3D\n        test_input = np.zeros((2, 3, 3, 3, 2))\n        test_input[:, 0, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 5, 5, 3)) * 0.2,\n                                 tf.ones((1, 5, 5, 5, 3)) * 1.2], axis=0)\n        out = ResamplerLayer(""IDW"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0],\n                    1.0/(1. + 1./2. + 36./132. + 12./192.), atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.0, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 5, 2))\n        # 2D\n        test_input = np.zeros((2, 3, 3, 2))\n        test_input[:, 0, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 5, 2)) * 0.2,\n                                 tf.ones((1, 5, 5, 2)) * 1.2], axis=0)\n        out = ResamplerLayer(""IDW"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0],\n                    1.0/(1.0 + 1.0/16.0 + 16./68.), atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.0, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 5, 2))\n        # 1D\n        test_input = np.zeros((2, 3, 2))\n        test_input[:, 0, 0] = 1.0\n        test_input = tf.constant(test_input)\n        test_coords = tf.concat([tf.ones((1, 5, 1)) * 0.2,\n                                 tf.ones((1, 5, 1)) * 1.2], axis=0)\n        out = ResamplerLayer(""IDW"")(test_input, test_coords)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertTrue(\n                np.all(np.isclose(out_value[0, ..., 0],\n                    1.0/(1.0 + 1/16.0), atol=1e-5)))\n            self.assertTrue(\n                np.all(np.isclose(out_value[1, ..., 0], 0.0, atol=1e-5)))\n            self.assertEqual(out_value.shape, (2, 5, 2))\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/resampler_grid_warper_test.py,18,"b'from __future__ import absolute_import, print_function, division\n\nimport base64\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\ntest_case_2d_1 = {\n    \'data\': ""+/b9/+3/377dpX+Mxp+Y/9nT/d/X6vfMuf+hX/hSY/1pvf/P9/z//+///+7z""\n            ""//ve19noiHuXVjlVSCUpwpyH/9i/9+LDwuufS84yGOYKGOgYQspG2v7Q/uXg""\n            ""07aonZBtS1NqWVRycl9zZEY86sSf/+u/7uezlNlvIdYPA/8AAP8AK+MfgMRd""\n            ""f3JGVzYTdV0xW2d9Y2N7c2NuZEgz58CV/+S66OS1jdt2KOclAP8AAP8AFtkB""\n            ""V6Ema1wjkmZDkmdFXGd5XltwdWFqdldF8c2r/+/V//7szP/JOs9AC+gNGvkS""\n            ""P9YlrNp4fl41kVdDj1ZDYWN8ZFdzblFjfVpU/+/a//Hp/e718P/2v/+8bOdb""\n            ""auVOtv6Q9fW/om9eiEg/oGFYXFR9e2GOdEttbkZO7tPI//v2//P/+/f47PjQ""\n            ""3Pmn3fmi3eGm/+rRyZCHhEg9l19Oal2TbU6HeUp2lm17x7Wn5eXZ7e7w9evp""\n            ""+OXH/+yz+uWs3b+b/9/N3a6ebj8lg1Y1ZFyNcFWIelB0fFde2Mu48fjm+f/7""\n            ""+PPt9uLH/+m6/+W24cSk/+TNz62SUS0LeVYuYGGAa1x9dFRpdldS9OXO/P3r""\n            ""8vb1//78//bg8OG28d6z/OjH/+nLwqWHbksrh2JFWmB6ZWB2aVVedl9R893F""\n            ""//Hl//r/++/z//Xh/PDG9Oa38Nqx/uC+ontcek04kWFVYWWKX1x5bWBqZE0/""\n            ""8dO7/+re89HS//Xx/uvK7+Cp/++1/+u74rWMhE8vilJBk1lYWVmNX1iCbF1y""\n            ""VToz58Gs/9rH/tLF/+DG/+y2/uej/+Ki/92pq3hLjVcxlFtHkVZSbGmYTkNt""\n            ""gmqCWzg22K2a/+TL/93C++C1++eq+OOi/+q489GsfVk3dlArkGRJkGFR3dnw""\n            ""lIadXT5NSiEdvpOA/93C8+DA8+rB+PLA/PDI//fn//v47eHVpph9cVo7ZkYt""\n            ""/f37//f678zQxpeRrYJx993G8OvO7vTQ8PbU/fvs/Pj/9/n/9///+P/t8OnM""\n            ""4s2u"".encode(\'ascii\'),\n    \'shape\': (16, 16, 3)\n}\n\ntest_case_2d_target = {\n    # [[0.96592583, -0.25881905,  2.34314575],\n    # [0.25881905,  0.96592583, -1.79795897]]\n    \'data\': ""////19jdbXKIZFl3TC5GVzM1yaKR/9vN/ODU7vnR2v/M0v7N9f/2///9////""\n            ""////////pau3Vlx2aF90aFFXkW5a8c+s/uTD6+a8sOiPauRTR/M9a/102P7n""\n            ""/v7///v////9dYGPXmB1cWVzX0c7v5dz/t+z++q8wN+RWdQ9E98ECO8DINkj""\n            ""keSW//76/+z49vf5YmR7X1duc11pdFRF6cGe/+fD9OvEoNyENuIuAv0AAP8B""\n            ""Gu4Qd9thx8mi07Gly8nWZFmBc1l8bUtck3Jp//Te//Ll/f7wxP7DKdIvAPUA""\n            ""BP8CE9wAVKspbWguWjgToZq8bVaOeE5+b0NcqoSD/vTo//T4/fP74f7of+19""\n            ""KugkLPMeTNUvjrhWclgnlmhHc2yYb1SLdkt5jGF1u6OZ5uDU9/L4+/T88fni""\n            ""zPirletwmfF21P6ox7mKhlI8klVDYmGAblp/eVJve1db1ci36+/e8PTz9Ozq""\n            ""+OjO+fC18Pas3eKg+vDM06WVj1BHllZNXWF6aVxwbFFYkXps/fPY+v7v+P35""\n            ""+fLq9+LF/+m3/eOw3L6a/+DO2qmbg0k7lVxLX2B/aF1tZVBLuaOM//Db/fr1""\n            ""+Pn7//309+zM9+K19dyz5suu/N3IwpmDYjcXkGdJYFmFa19zWEE52ryk/+zd""\n            ""/OPm/O/2/fPp/PLP8uS39uK9/+7Q6tC1lHNUXTkVr5aAUUVtemR5XT8368Ww""\n            ""/9zM987I//Ho/+zM8OKx+ey3896z/+fDwJ9+f1o/gFtA2tDGbVlyVTQ/dlBH""\n            ""6sau/uTL/9fB/uK9/+yx+eai/+qr/ee247OLilk7gk88kWFX+Pf13MPJj2Zk""\n            ""kmZZ68as/eLE+eG9+uWw+uWk/OSk/uWs4rqJn2w/iE8xkVVKpXNy/////vj4""\n            ""7NDPuJKF79G58ebK8e3I9fPD+++9/vHO/+vQr45vcEgkiVg4lV1QxaOi////""\n            ""//////////////78+fnv9Pni8PfY/frn/Pj5/f3/9+7lp5Z8eFo4gVdB5drW""\n            ""////"".encode(""ASCII""),\n    \'shape\': (16, 16, 3)\n}\n\n\ndef get_2d_images(test_case):\n    try:\n        out = base64.decodebytes(test_case[\'data\'])\n    except AttributeError:\n        out = base64.decodestring(test_case[\'data\'])\n    out = np.frombuffer(out, dtype=np.uint8)\n    out = out.reshape(test_case[\'shape\'])\n    return out, out.shape\n\n\ndef get_multiple_2d_images():\n    image_1, shape = get_2d_images(test_case_2d_1)\n    image_2 = image_1[::-1, ::-1]\n    image_3 = image_1[::-1, ]\n    image_4 = image_1[:, ::-1, ]\n    return np.stack([image_1, image_2, image_3, image_4]), [4] + list(shape)\n\n\ndef get_multiple_2d_rotated_targets():\n    image_1, shape = get_2d_images(test_case_2d_target)\n    image_2 = image_1[::-1, ::-1]\n    image_3 = image_1[::-1, ]\n    image_4 = image_1[:, ::-1, ]\n    return np.stack([image_1, image_2, image_3, image_4]), [4] + list(shape)\n\n\ndef get_multiple_2d_targets():\n    test_image, input_shape = get_multiple_2d_images()\n    test_target = np.array(test_image)\n\n    test_target[0] = test_target[0, ::-1]\n    test_target[1] = test_target[1, :, ::-1]\n    test_target[2] = test_target[2, ::-1, ::-1]\n\n    factor = 1.5\n    shape = input_shape[:]\n    shape[1] = np.floor(input_shape[1] * factor).astype(np.int)\n    shape[2] = np.floor(input_shape[2] * factor).astype(np.int)\n\n    from scipy.ndimage import zoom\n    zoomed_target = []\n    for img in test_target:\n        zoomed_target.append(zoom(img, [factor, factor, 1]))\n    test_target = np.stack(zoomed_target, axis=0).astype(np.uint8)\n    return test_target, shape\n\n\ndef get_multiple_3d_images():\n    image_1, shape = get_2d_images(test_case_2d_1)\n    image_2 = image_1[::-1, ::-1]\n    image_3 = image_1[::-1, ]\n    image_4 = image_1[:, ::-1, ]\n\n    image_2d = np.stack([image_1, image_2, image_3, image_4])\n    image_3d = np.expand_dims(image_2d, axis=1)\n    image_3d = np.concatenate([image_3d, image_3d], axis=1)\n    return image_3d, image_3d.shape\n\n\ndef get_multiple_3d_targets():\n    test_image, input_shape = get_multiple_2d_images()\n    test_target = np.array(test_image)\n\n    test_target[0] = test_target[0, ::-1]\n    test_target[1] = test_target[1, :, ::-1]\n    test_target[2] = test_target[2, ::-1, ::-1]\n\n    factor = 1.5\n    shape = input_shape[:]\n    shape[1] = np.floor(input_shape[1] * factor).astype(np.int)\n    shape[2] = np.floor(input_shape[2] * factor).astype(np.int)\n\n    from scipy.ndimage import zoom\n    zoomed_target = []\n    for img in test_target:\n        zoomed_target.append(zoom(img, [factor, factor, 1]))\n    test_target = np.stack(zoomed_target, axis=0).astype(np.uint8)\n    test_target = np.expand_dims(test_target, axis=1)\n    test_target = np.concatenate([test_target, test_target], axis=1)\n    return test_target, test_target.shape\n\n\ndef get_3d_input1():\n    test_case = tf.constant(\n        [[[[1, 2, -1], [3, 4, -2]], [[5, 6, -3], [7, 8, -4]]],\n         [[[9, 10, -5], [11, 12, -6]], [[13, 14, -7], [15, 16, -8]]]],\n        dtype=tf.float32)\n    return tf.expand_dims(test_case, 4)\n\n\nclass ResamplerGridWarperTest(NiftyNetTestCase):\n    def _test_correctness(\n            self, inputs, grid, interpolation, boundary, expected_value):\n        resampler = ResamplerLayer(\n            interpolation=interpolation, boundary=boundary)\n        out = resampler(inputs, grid)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            self.assertAllClose(expected_value, out_value)\n\n    def test_combined(self):\n        expected = [[[[[1], [-1]], [[3], [-2]]],\n                     [[[5], [-3]], [[7], [-4]]]],\n                    [[[[9.5], [-5]], [[11.5], [-6]]],\n                     [[[13.5], [-7]], [[15.5], [-8]]]]]\n        affine_grid = AffineGridWarperLayer(source_shape=(2, 2, 3),\n                                            output_shape=(2, 2, 2))\n        test_grid = affine_grid(\n            tf.constant([[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n                         [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, .5]],\n                        dtype=tf.float32))\n        self._test_correctness(inputs=get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'idw\',\n                               boundary=\'replicate\',\n                               expected_value=expected)\n\n\nclass image_test(NiftyNetTestCase):\n    def _test_grads_images(self,\n                           interpolation=\'linear\',\n                           boundary=\'replicate\',\n                           ndim=2):\n        if ndim == 2:\n            test_image, input_shape = get_multiple_2d_images()\n            test_target, target_shape = get_multiple_2d_targets()\n            identity_affine = [[1., 0., 0., 0., 1., 0.]] * 4\n        else:\n            test_image, input_shape = get_multiple_3d_images()\n            test_target, target_shape = get_multiple_3d_targets()\n            identity_affine = [[1., 0., 0., 0., 1., 0.,\n                                1., 0., 0., 0., 1., 0.]] * 4\n        affine_var = tf.get_variable(\'affine\', initializer=identity_affine)\n        grid = AffineGridWarperLayer(source_shape=input_shape[1:-1],\n                                     output_shape=target_shape[1:-1],\n                                     constraints=None)\n        warp_coords = grid(affine_var)\n        resampler = ResamplerLayer(interpolation, boundary=boundary)\n        new_image = resampler(tf.constant(test_image, dtype=tf.float32),\n                              warp_coords)\n\n        diff = tf.reduce_mean(tf.squared_difference(\n            new_image, tf.constant(test_target, dtype=tf.float32)))\n        optimiser = tf.train.AdagradOptimizer(0.01)\n        grads = optimiser.compute_gradients(diff)\n        opt = optimiser.apply_gradients(grads)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            init_val, affine_val = sess.run([diff, affine_var])\n            for _ in range(5):\n                _, diff_val, affine_val = sess.run([opt, diff, affine_var])\n                print(\'{}, {}\'.format(diff_val, affine_val[0]))\n            self.assertGreater(init_val, diff_val)\n\n    def test_2d_linear_replicate(self):\n        self._test_grads_images(\'linear\', \'replicate\')\n\n    def test_2d_idw_replicate(self):\n        self._test_grads_images(\'idw\', \'replicate\')\n\n    def test_2d_linear_circular(self):\n        self._test_grads_images(\'linear\', \'circular\')\n\n    def test_2d_idw_circular(self):\n        self._test_grads_images(\'idw\', \'circular\')\n\n    def test_2d_linear_symmetric(self):\n        self._test_grads_images(\'linear\', \'symmetric\')\n\n    def test_2d_idw_symmetric(self):\n        self._test_grads_images(\'idw\', \'symmetric\')\n\n    def test_3d_linear_replicate(self):\n        self._test_grads_images(\'linear\', \'replicate\', ndim=3)\n\n    def test_3d_idw_replicate(self):\n        self._test_grads_images(\'idw\', \'replicate\', ndim=3)\n\n    def test_3d_linear_circular(self):\n        self._test_grads_images(\'linear\', \'circular\', ndim=3)\n\n    def test_3d_idw_circular(self):\n        self._test_grads_images(\'idw\', \'circular\', ndim=3)\n\n    def test_3d_linear_symmetric(self):\n        self._test_grads_images(\'linear\', \'symmetric\', ndim=3)\n\n    def test_3d_idw_symmetric(self):\n        self._test_grads_images(\'idw\', \'symmetric\', ndim=3)\n\n\nclass image_2D_test_converge(NiftyNetTestCase):\n    def _test_simple_2d_images(self,\n                               interpolation=\'linear\',\n                               boundary=\'replicate\'):\n        # rotating around the center (8, 8) by 15 degree\n        expected = [[0.96592583, -0.25881905, 2.34314575],\n                    [0.25881905, 0.96592583, -1.79795897]]\n        expected = np.asarray(expected).flatten()\n        test_image, input_shape = get_multiple_2d_images()\n        test_target, target_shape = get_multiple_2d_rotated_targets()\n\n        identity_affine = [[1., 0., 0., 0., 1., 0.],\n                           [1., 0., 0., 0., 1., 0.],\n                           [1., 0., 0., 0., 1., 0.],\n                           [1., 0., 0., 0., 1., 0.]]\n        affine_var = tf.get_variable(\'affine\', initializer=identity_affine)\n        grid = AffineGridWarperLayer(source_shape=input_shape[1:-1],\n                                     output_shape=target_shape[1:-1],\n                                     constraints=None)\n        warp_coords = grid(affine_var)\n        resampler = ResamplerLayer(interpolation, boundary=boundary)\n        new_image = resampler(tf.constant(test_image, dtype=tf.float32),\n                              warp_coords)\n\n        diff = tf.reduce_mean(tf.squared_difference(\n            new_image, tf.constant(test_target, dtype=tf.float32)))\n        learning_rate = 0.05\n        if(interpolation == \'linear\') and (boundary == \'zero\'):\n            learning_rate = 0.0003\n        optimiser = tf.train.AdagradOptimizer(learning_rate)\n        grads = optimiser.compute_gradients(diff)\n        opt = optimiser.apply_gradients(grads)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            init_val, affine_val = sess.run([diff, affine_var])\n            # compute the MAE between the initial estimated parameters and the expected parameters\n            init_var_diff = np.sum(np.abs(affine_val[0] - expected))\n            for it in range(500):\n                _, diff_val, affine_val = sess.run([opt, diff, affine_var])\n                # print(\'{} diff: {}, {}\'.format(it, diff_val, affine_val[0]))\n            # import matplotlib.pyplot as plt\n            # plt.figure()\n            # plt.imshow(test_target[0])\n            # plt.draw()\n\n            # plt.figure()\n            # plt.imshow(sess.run(new_image).astype(np.uint8)[0])\n            # plt.draw()\n\n            # plt.show()\n            self.assertGreater(init_val, diff_val)\n            # compute the MAE between the final estimated parameters and the expected parameters\n            var_diff = np.sum(np.abs(affine_val[0] - expected))\n            self.assertGreater(init_var_diff, var_diff)\n            print(\'{} {} -- diff {}\'.format(\n                interpolation, boundary, var_diff))\n            print(\'{}\'.format(affine_val[0]))\n\n    def test_2d_linear_zero_converge(self):\n        self._test_simple_2d_images(\'linear\', \'zero\')\n\n    def test_2d_linear_replicate_converge(self):\n        self._test_simple_2d_images(\'linear\', \'replicate\')\n\n    def test_2d_idw_replicate_converge(self):\n        self._test_simple_2d_images(\'idw\', \'replicate\')\n\n    def test_2d_linear_circular_converge(self):\n        self._test_simple_2d_images(\'linear\', \'circular\')\n\n    def test_2d_idw_circular_converge(self):\n        self._test_simple_2d_images(\'idw\', \'circular\')\n\n    def test_2d_linear_symmetric_converge(self):\n        self._test_simple_2d_images(\'linear\', \'symmetric\')\n\n    def test_2d_idw_symmetric_converge(self):\n        self._test_simple_2d_images(\'idw\', \'symmetric\')\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/resampler_optional_niftyreg_test.py,40,"b'from __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.test as tft\n\nfrom niftynet.contrib.layer.resampler_optional_niftyreg import ResamplerOptionalNiftyRegLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\nimport niftynet.contrib.layer.resampler_optional_niftyreg as resampler_module\n\nclass ResamplerTest(NiftyNetTestCase):\n    def get_2d_input(self, as_tensor=True):\n        test_array = np.array(\n            [[[[1, 2, -1], [3, 4, -2]], [[5, 6, -3], [7, 8, -4]]],\n             [[[9, 10, -5], [11, 12, -6]], [[13, 14, -7], [15, 16, -8]]]])\n        if as_tensor:\n            test_array = tf.constant(test_array, dtype=tf.float32)\n            return test_array\n        return test_array.astype(np.float32)\n\n    def get_3d_input1(self, as_tensor=True):\n        test_array = np.array(\n            [[[[1, 2, -1], [3, 4, -2]], [[5, 6, -3], [7, 8, -4]]],\n             [[[9, 10, -5], [11, 12, -6]], [[13, 14, -7], [15, 16, -8]]]])\n        if as_tensor:\n            test_array = tf.constant(test_array, dtype=tf.float32)\n            return tf.expand_dims(test_array, 4)\n        return np.expand_dims(test_array, 4).astype(np.float32)\n\n    def get_3d_input2(self, as_tensor=True):\n        one_channel = self.get_3d_input1(as_tensor=as_tensor)\n        if as_tensor:\n            return tf.concat([one_channel, 100 + one_channel], 4)\n        return np.concatenate([one_channel, 100 + one_channel], 4)\n\n    def _get_devs(self):\n        devs = [False]\n        if tft.is_gpu_available(cuda_only=True) and tft.is_built_with_cuda():\n            devs += [True]\n\n        return devs\n\n    def _test_correctness(\n            self, input, grid, interpolation, boundary, expected_value):\n        resampler = ResamplerOptionalNiftyRegLayer(interpolation=interpolation,\n                                                 boundary=boundary)\n        out = resampler(input, grid)\n\n        for use_gpu in self._get_devs():\n            with self.cached_session(use_gpu=use_gpu) as sess:\n                out_value = sess.run(out)\n                self.assertAllClose(expected_value, out_value)\n\n    def test_resampler_2d_replicate_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25], [.25, .78]],\n             [[.62, .25], [.25, .28]]],\n            dtype=tf.float32)\n        expected = [[[2.5, 3.5, -1.75],\n                     [3.56, 4.56, -2.28]],\n                    [[11.98, 12.98, -6.49],\n                     [10.56, 11.56, -5.78]]]\n        self._test_correctness(input=self.get_2d_input(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'ZERO\',\n                               expected_value=expected)\n\n    def test_gradient_correctness(self):\n        if not resampler_module.HAS_NIFTYREG_RESAMPLING:\n            self.skipTest(\'Using native NiftyNet resampler; skipping test\')\n            return\n\n        for inter in (\'LINEAR\', \'BSPLINE\'):\n            for b in (\'ZERO\', \'REPLICATE\', \'SYMMETRIC\'):\n                for use_gpu in self._get_devs():\n                    inputs = ((self.get_3d_input1(as_tensor=False),\n                               [[[-5.2, .25, .25], [.25, .95, .25]],\n                                [[.75, .25, .25], [.25, .25, .75]]]),\n                              (self.get_2d_input(as_tensor=False),\n                               [[[.25, .25], [.25, .78]],\n                                [[.62, .25], [.25, .28]]]),)\n\n                    for np_img, np_u in inputs:\n                        with self.session(use_gpu=use_gpu):\n                            np_u = np.array(np_u)\n\n                            while len(np_u.shape) < len(np_img.shape):\n                                np_u = np.expand_dims(np_u, axis=2)\n\n                            img = tf.constant(np_img, dtype=tf.float32)\n                            disp = tf.constant(np_u, dtype=tf.float32)\n\n                            # multimodal needs addressing\n                            if img.shape.as_list()[-1] > 1:\n                                img = tf.reshape(img[...,0],\n                                                 img.shape.as_list()[:-1] + [1])\n\n                            warped = ResamplerOptionalNiftyRegLayer(interpolation=inter,\n                                                                  boundary=b)\n                            warped = warped(img, disp)\n                            #warped = tf.reduce_sum(warped)\n\n                            tgrad, refgrad = tft.compute_gradient(\n                                disp,\n                                disp.shape,\n                                warped,\n                                warped.shape)\n\n                            error = np.power(tgrad - refgrad, 2).sum()\n                            refmag = np.power(refgrad, 2).sum()\n\n                            self.assertLessEqual(error, 1e-2*refmag)\n\n    def test_image_derivative_correctness(self):\n        if not resampler_module.HAS_NIFTYREG_RESAMPLING:\n            self.skipTest(\'Using native NiftyNet resampler; skipping test\')\n            return\n\n        for inter in (\'LINEAR\', \'BSPLINE\'):\n            for b in (\'ZERO\', \'REPLICATE\', \'SYMMETRIC\'):\n                for use_gpu in self._get_devs():\n                    if inter != \'LINEAR\' and use_gpu:\n                        continue\n\n                    inputs = ((self.get_3d_input1(as_tensor=False),\n                               [[[-5.2, .25, .25], [.25, .95, .25]],\n                                [[.75, .25, .25], [.25, .25, .75]]]),\n                              (self.get_2d_input(as_tensor=False),\n                               [[[.25, .25], [.25, .78]],\n                                [[.62, .25], [.25, .28]]]),)\n\n                    for np_img, np_u in inputs:\n                        with self.session(use_gpu=use_gpu):\n                            np_u = np.array(np_u)\n\n                            while len(np_u.shape) < len(np_img.shape):\n                                np_u = np.expand_dims(np_u, axis=2)\n\n                            img = tf.constant(np_img, dtype=tf.float32)\n                            disp = tf.constant(np_u, dtype=tf.float32)\n\n                            warped = ResamplerOptionalNiftyRegLayer(interpolation=inter,\n                                                                  boundary=b)\n                            warped = warped(img, disp)\n                            #warped = tf.reduce_sum(warped)\n\n                            tgrad, refgrad = tft.compute_gradient(\n                                img,\n                                img.shape,\n                                warped,\n                                warped.shape)\n\n                            error = np.power(tgrad - refgrad, 2).sum()\n                            refmag = np.power(refgrad, 2).sum()\n\n                            self.assertLessEqual(error, 1e-2*refmag)\n\n    def test_resampler_3d_zero_nearest_correctness(self):\n        test_grid = tf.constant(\n            [[[-5.2, .25, .25], [.25, .95, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[0, 0], [3, 103]],\n                    [[13, 113], [10, 110]]]\n        self._test_correctness(input=self.get_3d_input2(),\n                               grid=test_grid,\n                               interpolation=\'NEAREST\',\n                               boundary=\'ZERO\',\n                               expected_value=expected)\n\n    def test_resampler_3d_symmetric_nearest_correctness(self):\n        test_grid = tf.constant(\n            [[[-.25, -.25, -.25],\n              [.25 + 2, .75 + 2, .25 + 4]],\n             [[.75, .25, -.25 + 4],\n              [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[1], [3]], [[13], [10]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'NEAREST\',\n                               boundary=\'SYMMETRIC\',\n                               expected_value=expected)\n\n    def test_resampler_3d_symmetric_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[-.25, -.25, -.25],\n              [.25 + 2, .75 + 2, .25 + 4]],\n             [[.75, .25, -.25 + 4],\n              [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[2.75], [3.75]],\n                    [[12.75], [11.25]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'SYMMETRIC\',\n                               expected_value=expected)\n\n    def test_resampler_3d_symmetric_cubic_correctness(self):\n        test_grid = tf.constant(\n            [[[-.25, -.25, -.25],\n              [.25 + 2, .75 + 2, .25 + 4]],\n             [[.75, .25, -.25 + 4],\n              [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[3.683675], [4.140218]],\n                    [[12.56551075], [10.69881153]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'BSPLINE\',\n                               boundary=\'SYMMETRIC\',\n                               expected_value=expected)\n\n    def _test_partial_shape_correctness(self,\n                                        input,\n                                        rank,\n                                        batch_size,\n                                        grid,\n                                        interpolation,\n                                        boundary,\n                                        expected_value=None):\n\n        resampler = ResamplerOptionalNiftyRegLayer(interpolation=interpolation,\n                                                   boundary=boundary)\n        input_default = tf.random_uniform(input.shape)\n        if batch_size > 0 and rank > 0:\n            input_placeholder = tf.placeholder_with_default(\n                input_default, shape=[batch_size] + [None] * (rank + 1))\n        elif batch_size <= 0 and rank > 0:\n            input_placeholder = tf.placeholder_with_default(\n                input_default, shape=[None] * (rank + 2))\n        elif batch_size <= 0 and rank <= 0:\n            input_placeholder = tf.placeholder_with_default(\n                input_default, shape=None)\n\n        out = resampler(input_placeholder, grid)\n        with self.cached_session() as sess:\n            out_value = sess.run(\n                out, feed_dict={input_placeholder: input})\n            if expected_value is not None:\n                self.assertAllClose(expected_value, out_value)\n\n    def test_2d_linear_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[.25, .25], [.25, .78]],\n             [[.62, .25], [.25, .28]]], dtype=tf.float32)\n        expected = [[[2.5, 3.5, -1.75],\n                     [3.56, 4.56, -2.28]],\n                    [[11.98, 12.98, -6.49],\n                     [10.56, 11.56, -5.78]]]\n        interp = \'linear\'\n\n        for b in (\'ZERO\',):\n            self._test_partial_shape_correctness(\n                input=self.get_2d_input(False),\n                rank=2,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=2,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n    def test_3d_linear_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[2.75, 102.75], [3.75, 103.75]],\n                    [[12.75, 112.75], [11.25, 111.25]]]\n        interp = \'linear\'\n\n        for b in (\'ZERO\',):\n            self._test_partial_shape_correctness(\n                input=self.get_3d_input2(False),\n                rank=3,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=3,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n    def test_2d_nearest_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[.25, .25], [.25, .78]],\n             [[.62, .25], [.25, .28]]], dtype=tf.float32)\n        expected = [[[1, 2, -1],\n                     [3, 4, -2]],\n                    [[13, 14, -7],\n                     [9, 10, -5]]]\n        interp = \'nearest\'\n\n        for b in (\'ZERO\', \'REPLICATE\'):\n            self._test_partial_shape_correctness(\n                input=self.get_2d_input(False),\n                rank=2,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=2,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n    def test_resampler_3d_multivariate_replicate_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[2.75, 102.75], [3.75, 103.75]],\n                    [[12.75, 112.75], [11.25, 111.25]]]\n        self._test_correctness(input=self.get_3d_input2(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_resampler_3d_replicate_nearest_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[1, 101], [3, 103]],\n                    [[13, 113], [10, 110]]]\n        self._test_correctness(input=self.get_3d_input2(),\n                               grid=test_grid,\n                               interpolation=\'NEAREST\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_resampler_3d_replicate_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[2.75], [3.75]],\n                    [[12.75], [11.25]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_3d_nearest_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[0, 1, 2], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[-2, 98], [3, 103]],\n                    [[13, 113], [10, 110]]]\n        interp = \'nearest\'\n\n        for b in (\'ZERO\', \'REPLICATE\'):\n            self._test_partial_shape_correctness(\n                input=self.get_3d_input2(False),\n                rank=3,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=3,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/resampler_test.py,49,"b'from __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass ResamplerTest(NiftyNetTestCase):\n    def get_2d_input(self, as_tensor=True):\n        test_array = np.array(\n            [[[[1, 2, -1], [3, 4, -2]], [[5, 6, -3], [7, 8, -4]]],\n             [[[9, 10, -5], [11, 12, -6]], [[13, 14, -7], [15, 16, -8]]]])\n        if as_tensor:\n            test_array = tf.constant(test_array, dtype=tf.float32)\n            return test_array\n        return test_array.astype(np.float32)\n\n    def get_3d_input1(self, as_tensor=True):\n        test_array = np.array(\n            [[[[1, 2, -1], [3, 4, -2]], [[5, 6, -3], [7, 8, -4]]],\n             [[[9, 10, -5], [11, 12, -6]], [[13, 14, -7], [15, 16, -8]]]])\n        if as_tensor:\n            test_array = tf.constant(test_array, dtype=tf.float32)\n            return tf.expand_dims(test_array, 4)\n        return np.expand_dims(test_array, 4).astype(np.float32)\n\n    def get_3d_input2(self, as_tensor=True):\n        one_channel = self.get_3d_input1(as_tensor=as_tensor)\n        if as_tensor:\n            return tf.concat([one_channel, 100 + one_channel], 4)\n        return np.concatenate([one_channel, 100 + one_channel], 4)\n\n    def _test_correctness(\n            self, input, grid, interpolation, boundary, expected_value):\n        resampler = ResamplerLayer(interpolation=interpolation,\n                                   boundary=boundary)\n        out = resampler(input, grid)\n        with self.cached_session() as sess:\n            out_value = sess.run(out)\n            # print(expected_value)\n            # print(out_value)\n            self.assertAllClose(expected_value, out_value)\n\n    def test_resampler_3d_multivariate_zero_weight_idw_correctness(self):\n        test_grid = tf.constant(\n            [[[0, 1, 2], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[-2, 98], [3.87344956, 103.873459]],\n                    [[12.70884895, 112.70884705], [11.45574856, 111.45578003]]]\n        self._test_correctness(input=self.get_3d_input2(),\n                               grid=test_grid,\n                               interpolation=\'IDW\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_resampler_3d_multivariate_replicate_idw_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[3.03804874, 103.03804016], [3.87344956, 103.87354279]],\n                    [[12.70884895, 112.70884705], [11.45574856, 111.45578003]]]\n        self._test_correctness(input=self.get_3d_input2(),\n                               grid=test_grid,\n                               interpolation=\'IDW\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_resampler_2d_replicate_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25], [.25, .78]],\n             [[.62, .25], [.25, .28]]],\n            dtype=tf.float32)\n        expected = [[[2.5, 3.5, -1.75],\n                     [3.56, 4.56, -2.28]],\n                    [[11.98, 12.98, -6.49],\n                     [10.56, 11.56, -5.78]]]\n        self._test_correctness(input=self.get_2d_input(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'ZERO\',\n                               expected_value=expected)\n\n    def test_resampler_3d_multivariate_replicate_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[2.75, 102.75], [3.75, 103.75]],\n                    [[12.75, 112.75], [11.25, 111.25]]]\n        self._test_correctness(input=self.get_3d_input2(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_resampler_3d_replicate_nearest_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[1, 101], [3, 103]],\n                    [[13, 113], [10, 110]]]\n        self._test_correctness(input=self.get_3d_input2(),\n                               grid=test_grid,\n                               interpolation=\'NEAREST\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_resampler_3d_zero_nearest_correctness(self):\n        test_grid = tf.constant(\n            [[[-5.2, .25, .25], [.25, .95, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[0, 0], [3, 103]],\n                    [[13, 113], [10, 110]]]\n        self._test_correctness(input=self.get_3d_input2(),\n                               grid=test_grid,\n                               interpolation=\'NEAREST\',\n                               boundary=\'ZERO\',\n                               expected_value=expected)\n\n    def test_resampler_3d_replicate_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[2.75], [3.75]],\n                    [[12.75], [11.25]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_resampler_3d_replicate_cubic_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[3.20869954], [3.93501790]],\n                    [[12.63008626], [10.33280436]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'BSPLINE\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected)\n\n    def test_resampler_3d_circular_nearest_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[1], [3]], [[13], [10]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'NEAREST\',\n                               boundary=\'CIRCULAR\',\n                               expected_value=expected)\n\n    def test_resampler_3d_circular_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[.25, .25 + 2, .25 + 3],\n              [.25 - 2, .75 - 2, .25 - 3]],\n             [[.75 + 2, .25 - 2, .25 - 3],\n              [.25 + 2, .25 - 2, .75 + 3]]],\n            dtype=tf.float32)\n        expected = [[[2.75], [3.75]],\n                    [[12.75], [11.25]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'CIRCULAR\',\n                               expected_value=expected)\n\n    def test_resampler_3d_symmetric_nearest_correctness(self):\n        test_grid = tf.constant(\n            [[[-.25, -.25, -.25],\n              [.25 + 2, .75 + 2, .25 + 4]],\n             [[.75, .25, -.25 + 4],\n              [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[1], [3]], [[13], [10]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'NEAREST\',\n                               boundary=\'SYMMETRIC\',\n                               expected_value=expected)\n\n    def test_resampler_3d_symmetric_linear_correctness(self):\n        test_grid = tf.constant(\n            [[[-.25, -.25, -.25],\n              [.25 + 2, .75 + 2, .25 + 4]],\n             [[.75, .25, -.25 + 4],\n              [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[2.75], [3.75]],\n                    [[12.75], [11.25]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'SYMMETRIC\',\n                               expected_value=expected)\n\n    def test_resampler_3d_symmetric_cubic_correctness(self):\n        test_grid = tf.constant(\n            [[[-.25, -.25, -.25],\n              [.25 + 2, .75 + 2, .25 + 4]],\n             [[.75, .25, -.25 + 4],\n              [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[3.683675], [4.140218]],\n                    [[12.56551075], [10.69881153]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'BSPLINE\',\n                               boundary=\'SYMMETRIC\',\n                               expected_value=expected)\n\n    def test_resampler_3d_circular_cubic_correctness(self):\n        test_grid = tf.constant(\n            [[[-.25, -.25, -.25],\n              [.25 + 2, .75 + 2, .25 + 4]],\n             [[.75, .25, -.25 + 4],\n              [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[1.66219068], [2.44295263]],\n                    [[11.46712303], [10.65071392]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=test_grid,\n                               interpolation=\'BSPLINE\',\n                               boundary=\'CIRCULAR\',\n                               expected_value=expected)\n\n    def _test_partial_shape_correctness(self,\n                                        input,\n                                        rank,\n                                        batch_size,\n                                        grid,\n                                        interpolation,\n                                        boundary,\n                                        expected_value=None):\n\n        resampler = ResamplerLayer(interpolation=interpolation,\n                                   boundary=boundary)\n        input_default = tf.random_uniform(input.shape)\n        if batch_size > 0 and rank > 0:\n            input_placeholder = tf.placeholder_with_default(\n                input_default, shape=[batch_size] + [None] * (rank + 1))\n        elif batch_size <= 0 and rank > 0:\n            input_placeholder = tf.placeholder_with_default(\n                input_default, shape=[None] * (rank + 2))\n        elif batch_size <= 0 and rank <= 0:\n            input_placeholder = tf.placeholder_with_default(\n                input_default, shape=None)\n\n        out = resampler(input_placeholder, grid)\n        with self.cached_session() as sess:\n            out_value = sess.run(\n                out, feed_dict={input_placeholder: input})\n            # print(expected_value)\n            # print(out_value)\n            if expected_value is not None:\n                self.assertAllClose(expected_value, out_value)\n\n    def test_2d_linear_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[.25, .25], [.25, .78]],\n             [[.62, .25], [.25, .28]]], dtype=tf.float32)\n        expected = [[[2.5, 3.5, -1.75],\n                     [3.56, 4.56, -2.28]],\n                    [[11.98, 12.98, -6.49],\n                     [10.56, 11.56, -5.78]]]\n        interp = \'linear\'\n\n        from niftynet.layer.resampler import SUPPORTED_BOUNDARY\n        for b in list(SUPPORTED_BOUNDARY):\n            self._test_partial_shape_correctness(\n                input=self.get_2d_input(False),\n                rank=2,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=2,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n    def test_3d_linear_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[.25, .25, .25], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[2.75, 102.75], [3.75, 103.75]],\n                    [[12.75, 112.75], [11.25, 111.25]]]\n        interp = \'linear\'\n\n        from niftynet.layer.resampler import SUPPORTED_BOUNDARY\n        for b in list(SUPPORTED_BOUNDARY):\n            self._test_partial_shape_correctness(\n                input=self.get_3d_input2(False),\n                rank=3,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=3,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n    def test_2d_idw_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[.25, .25], [.25, .78]],\n             [[.62, .25], [.25, .28]]], dtype=tf.float32)\n        expected = [[[2.23529005, 3.23529005, -1.61764503],\n                     [3.40578985, 4.40578985, -2.20289493]],\n                    [[12.13710022, 13.13710022, -6.56855011],\n                     [10.34773731, 11.34773731, -5.67386866]]]\n        interp = \'IDW\'\n\n        from niftynet.layer.resampler import SUPPORTED_BOUNDARY\n        for b in list(SUPPORTED_BOUNDARY):\n            self._test_partial_shape_correctness(\n                input=self.get_2d_input(False),\n                rank=2,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=2,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n    def test_3d_idw_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[0, 1, 2], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[-2, 98], [3.87344956, 103.873459]],\n                    [[12.70884895, 112.70884705], [11.45574856, 111.45578003]]]\n        interp = \'IDW\'\n\n        from niftynet.layer.resampler import SUPPORTED_BOUNDARY\n        for b in list(SUPPORTED_BOUNDARY):\n            self._test_partial_shape_correctness(\n                input=self.get_3d_input2(False),\n                rank=3,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=3,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n    def test_2d_nearest_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[.25, .25], [.25, .78]],\n             [[.62, .25], [.25, .28]]], dtype=tf.float32)\n        expected = [[[1, 2, -1],\n                     [3, 4, -2]],\n                    [[13, 14, -7],\n                     [9, 10, -5]]]\n        interp = \'nearest\'\n\n        from niftynet.layer.resampler import SUPPORTED_BOUNDARY\n        for b in list(SUPPORTED_BOUNDARY):\n            self._test_partial_shape_correctness(\n                input=self.get_2d_input(False),\n                rank=2,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=2,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_2d_input(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n    def test_3d_nearest_partial_shapes(self):\n        test_grid = tf.constant(\n            [[[0, 1, 2], [.25, .75, .25]],\n             [[.75, .25, .25], [.25, .25, .75]]],\n            dtype=tf.float32)\n        expected = [[[-2, 98], [3, 103]],\n                    [[13, 113], [10, 110]]]\n        interp = \'nearest\'\n\n        from niftynet.layer.resampler import SUPPORTED_BOUNDARY\n        for b in list(SUPPORTED_BOUNDARY):\n            self._test_partial_shape_correctness(\n                input=self.get_3d_input2(False),\n                rank=3,\n                batch_size=2,\n                grid=test_grid,\n                interpolation=interp,\n                boundary=b,\n                expected_value=expected)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=3,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n            with self.assertRaisesRegexp(TypeError, \'shape\'):\n                self._test_partial_shape_correctness(\n                    input=self.get_3d_input2(False),\n                    rank=-1,\n                    batch_size=-1,\n                    grid=test_grid,\n                    interpolation=interp,\n                    boundary=b,\n                    expected_value=None)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/residual_unit_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.residual_unit import ResidualUnit as Res\nfrom niftynet.layer.residual_unit import SUPPORTED_OP as connection_types\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass ResidualUnitTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def _test_nd_output_shape(self, rank, param_dict, expected_shape):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        res_layer = Res(**param_dict)\n        output_data = res_layer(input_data)\n        print(res_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(output_data)\n            self.assertAllClose(expected_shape, out.shape)\n\n    def test_3d_shape(self):\n        expected_shape = (2, 16, 16, 16, 1)\n        self._test_nd_output_shape(3, {}, expected_shape)\n\n        params = {\'n_output_chns\': 6, \'kernel_size\': 5}\n        expected_shape = (2, 16, 16, 16, 6)\n        self._test_nd_output_shape(3, params, expected_shape)\n\n        params = {\'n_output_chns\': 3, \'kernel_size\': 3, \'acti_func\': \'prelu\'}\n        expected_shape = (2, 16, 16, 16, 3)\n        self._test_nd_output_shape(3, params, expected_shape)\n\n        for type_str in connection_types:\n            params = {\'n_output_chns\': 3, \'type_string\': type_str}\n            expected_shape = (2, 16, 16, 16, 3)\n            self._test_nd_output_shape(3, params, expected_shape)\n\n    def test_2d_shape(self):\n        expected_shape = (2, 16, 16, 1)\n        self._test_nd_output_shape(2, {}, expected_shape)\n\n        params = {\'n_output_chns\': 16, \'kernel_size\': 5, \'acti_func\': \'relu\'}\n        expected_shape = (2, 16, 16, 16)\n        self._test_nd_output_shape(2, params, expected_shape)\n\n        params = {\'n_output_chns\': 3, \'kernel_size\': 3, \'acti_func\': \'prelu\'}\n        expected_shape = (2, 16, 16, 3)\n        self._test_nd_output_shape(2, params, expected_shape)\n\n        for type_str in connection_types:\n            params = {\'n_output_chns\': 3, \'type_string\': type_str}\n            expected_shape = (2, 16, 16, 3)\n            self._test_nd_output_shape(2, params, expected_shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/residual_upsample_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.additive_upsample import ResidualUpsampleLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\ndef get_3d_input():\n    input_shape = (2, 16, 16, 16, 4)\n    x = tf.ones(input_shape)\n    return x\n\ndef get_2d_input():\n    input_shape = (2, 16, 16, 4)\n    x = tf.ones(input_shape)\n    return x\n\nclass ResidualUpsampleTest(NiftyNetTestCase):\n    def run_test(self, param_dict, expected_shape, is_3d=True):\n        if is_3d:\n            x = get_3d_input()\n        else:\n            x = get_2d_input()\n\n        upsample_layer = ResidualUpsampleLayer(**param_dict)\n        resized = upsample_layer(x)\n        print(upsample_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(resized)\n            self.assertAllClose(out.shape, expected_shape)\n\n    def test_3d_shape(self):\n        params = {\'kernel_size\': 3, \'stride\': 2, \'n_splits\': 2}\n        expected_shape = (2, 32, 32, 32, 2)\n        self.run_test(params, expected_shape, True)\n\n        params = {\'kernel_size\': 2, \'stride\': 3, \'n_splits\': 4}\n        expected_shape = (2, 48, 48, 48, 1)\n        self.run_test(params, expected_shape, True)\n\n        params = {\'kernel_size\': 2, \'stride\': 3, \'n_splits\': 1,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 48, 48, 4)\n        self.run_test(params, expected_shape, True)\n\n        params = {\'kernel_size\': 2, \'stride\': (3, 2, 3), \'n_splits\': 1,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 32, 48, 4)\n        self.run_test(params, expected_shape, True)\n\n    def test_2d_shape(self):\n        params = {\'kernel_size\': 3, \'stride\': 2, \'n_splits\': 2}\n        expected_shape = (2, 32, 32, 2)\n        self.run_test(params, expected_shape, False)\n\n        params = {\'kernel_size\': 2, \'stride\': 3, \'n_splits\': 4}\n        expected_shape = (2, 48, 48, 1)\n        self.run_test(params, expected_shape, False)\n\n        params = {\'kernel_size\': 2, \'stride\': 3, \'n_splits\': 1,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 48, 4)\n        self.run_test(params, expected_shape, False)\n\n        params = {\'kernel_size\': 2, \'stride\': (3, 2), \'n_splits\': 1,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 32, 4)\n        self.run_test(params, expected_shape, False)\n\n    def test_float_params(self):\n        params = {\'kernel_size\': 2.1, \'stride\': 3, \'n_splits\': 1.1,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 48, 4)\n        self.run_test(params, expected_shape, False)\n\n\n    def test_bad_int_shape(self):\n        params = {\'kernel_size\': 2, \'stride\': 3, \'n_splits\': 3,\n                  \'acti_func\': \'prelu\'}\n        with self.assertRaisesRegexp(AssertionError, """"):\n            self.run_test(params, (None,) * 2, False)\n\n        with self.assertRaisesRegexp(AssertionError, """"):\n            self.run_test(params, (None,) * 3, True)\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/resnet_test.py,9,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.resnet import ResNet\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass ResNet3DTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 8, 16, 32, 1)\n        x = tf.ones(input_shape)\n\n        resnet_instance = ResNet(num_classes=160)\n        out = resnet_instance(x, is_training=True)\n        print(resnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 160), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 8, 16, 1)\n        x = tf.ones(input_shape)\n\n        resnet_instance = ResNet(num_classes=160)\n        out = resnet_instance(x, is_training=True)\n        print(resnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 160), out.shape)\n\n    def test_3d_reg_shape(self):\n        input_shape = (2, 8, 16, 24, 1)\n        x = tf.ones(input_shape)\n\n        resnet_instance = ResNet(num_classes=160,\n                               w_regularizer=regularizers.l2_regularizer(0.4))\n        out = resnet_instance(x, is_training=True)\n        print(resnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 160), out.shape)\n\n    def test_2d_reg_shape(self):\n        input_shape = (2, 8, 16, 1)\n        x = tf.ones(input_shape)\n\n        resnet_instance = ResNet(num_classes=160,\n                               w_regularizer=regularizers.l2_regularizer(0.4))\n        out = resnet_instance(x, is_training=True)\n        print(resnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 160), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/restorer_test.py,19,"b'from __future__ import absolute_import, print_function\r\n\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom niftynet.engine.application_variables \\\r\n    import RESTORABLE, global_vars_init_or_restore\r\nfrom niftynet.layer.convolution import ConvolutionalLayer\r\nfrom tests.niftynet_testcase import NiftyNetTestCase\r\n\r\n\r\nclass RestorerTest(NiftyNetTestCase):\r\n    def make_checkpoint(self, checkpoint_name, definition):\r\n        scopes = {}\r\n        tf.reset_default_graph()\r\n        [tf.Variable(definition[k], name=k, dtype=np.float32)\r\n            for k in definition]\r\n        with tf.Session() as sess:\r\n            saver = tf.train.Saver()\r\n            sess.run(tf.global_variables_initializer())\r\n            fn = os.path.join(\'testing_data\', checkpoint_name)\r\n            saver.save(sess, fn)\r\n        return fn\r\n\r\n    def test_restore_block(self):\r\n        definition = {\'foo\': [1], \'bar/conv_/w\': np.random.randn(3, 3, 1, 3),\r\n            \'bar2/conv_/w\': np.random.randn(3, 3, 1, 3),\r\n            \'foo3/conv_/w\': np.random.randn(3, 3, 1, 3),\r\n            \'bar/bing/boffin\': [2]}\r\n        checkpoint_name = self.make_checkpoint(\'chk1\', definition)\r\n        tf.reset_default_graph()\r\n        block1 = ConvolutionalLayer(3, 3, feature_normalization=None, name=\'foo\')\r\n        b1 = block1(tf.ones([1., 5., 5., 1.]))\r\n        tf.add_to_collection(RESTORABLE,\r\n                             (\'foo\', checkpoint_name, \'bar\'))\r\n        block2 = ConvolutionalLayer(4, 3, name=\'bar\', feature_normalization=None,\r\n                                    w_initializer=tf.constant_initializer(1.))\r\n        b2 = block2(tf.ones([1., 5., 5., 1.]))\r\n        block3 = ConvolutionalLayer(3, 3, feature_normalization=None, name=\'foo2\')\r\n        block3.restore_from_checkpoint(checkpoint_name, \'bar2\')\r\n        b3 = block3(tf.ones([1., 5., 5., 1.]))\r\n        block4 = ConvolutionalLayer(3, 3, feature_normalization=None, name=\'foo3\')\r\n        block4.restore_from_checkpoint(checkpoint_name)\r\n        b4 = block4(tf.ones([1., 5., 5., 1.]))\r\n        tf.add_to_collection(RESTORABLE,\r\n                             (\'foo\', checkpoint_name, \'bar\'))\r\n        init_op = global_vars_init_or_restore()\r\n        all_vars = tf.global_variables()\r\n        with self.cached_session() as sess:\r\n            sess.run(init_op)\r\n\r\n            def getvar(x):\r\n                return [v for v in all_vars if v.name == x][0]\r\n\r\n            foo_w_var = getvar(block1.layer_scope().name + \'/conv_/w:0\')\r\n            bar_w_var = getvar(block2.layer_scope().name + \'/conv_/w:0\')\r\n            foo2_w_var = getvar(block3.layer_scope().name + \'/conv_/w:0\')\r\n            foo3_w_var = getvar(block4.layer_scope().name + \'/conv_/w:0\')\r\n            vars = [foo_w_var, bar_w_var, foo2_w_var, foo3_w_var]\r\n            [foo_w, bar_w, foo2_w, foo3_w] = sess.run(vars)\r\n            self.assertAllClose(foo_w, definition[\'bar/conv_/w\'])\r\n            self.assertAllClose(bar_w, np.ones([3, 3, 1, 4]))\r\n            self.assertAllClose(foo2_w, definition[\'bar2/conv_/w\'])\r\n            self.assertAllClose(foo3_w, definition[\'foo3/conv_/w\'])\r\n\r\n    def test_no_restores(self):\r\n        tf.reset_default_graph()\r\n        block1 = ConvolutionalLayer(4, 3, name=\'bar\', feature_normalization=None,\r\n                                    w_initializer=tf.constant_initializer(1.))\r\n        b2 = block1(tf.ones([1., 5., 5., 1.]))\r\n        init_op = global_vars_init_or_restore()\r\n        all_vars = tf.global_variables()\r\n        with self.cached_session() as sess:\r\n            sess.run(init_op)\r\n\r\n            def getvar(x):\r\n                return [v for v in all_vars if v.name == x][0]\r\n\r\n            bar_w_var = getvar(block1.layer_scope().name + \'/conv_/w:0\')\r\n            [bar_w] = sess.run([bar_w_var])\r\n            self.assertAllClose(bar_w, np.ones([3, 3, 1, 4]))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    tf.test.main()\r\n'"
tests/rgb_histogram_equilisation_test.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.rgb_histogram_equilisation import \\\n    RGBHistogramEquilisationLayer\nfrom niftynet.utilities.util_import import require_module\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nIMAGE_DATA = \\\n    np.array([[[0.49803922, 0.19215687, 0.3529412 ],\n               [0.49411765, 0.16862746, 0.31764707],\n               [0.5254902 , 0.21176471, 0.3647059 ],\n               [0.45882353, 0.21176471, 0.38039216],\n               [0.44705883, 0.19215687, 0.39607844],\n               [0.44705883, 0.18431373, 0.39607844],\n               [0.43137255, 0.18039216, 0.3882353 ],\n               [0.42352942, 0.16470589, 0.34901962],\n               [0.41960785, 0.14509805, 0.3647059 ],\n               [0.46666667, 0.18039216, 0.38431373]],\n              [[0.4509804 , 0.15686275, 0.34901962],\n               [0.43137255, 0.16862746, 0.3529412 ],\n               [0.4745098 , 0.21176471, 0.4       ],\n               [0.47058824, 0.2784314 , 0.4509804 ],\n               [0.46666667, 0.25490198, 0.4627451 ],\n               [0.4509804 , 0.24705882, 0.41960785],\n               [0.42745098, 0.17254902, 0.37254903],\n               [0.4392157 , 0.16078432, 0.36078432],\n               [0.48235294, 0.1882353 , 0.41960785],\n               [0.53333336, 0.2901961 , 0.48235294]],\n              [[0.45882353, 0.20784314, 0.38431373],\n               [0.48235294, 0.21960784, 0.40784314],\n               [0.4627451 , 0.24705882, 0.40392157],\n               [0.45490196, 0.23529412, 0.4117647 ],\n               [0.41568628, 0.14901961, 0.34901962],\n               [0.4509804 , 0.16862746, 0.34901962],\n               [0.45882353, 0.2       , 0.41568628],\n               [0.5019608 , 0.24313726, 0.41960785],\n               [0.5058824 , 0.23529412, 0.4392157 ],\n               [0.53333336, 0.29411766, 0.46666667]],\n              [[0.52156866, 0.21960784, 0.41568628],\n               [0.47058824, 0.16862746, 0.36862746],\n               [0.4392157 , 0.1882353 , 0.38039216],\n               [0.4117647 , 0.18431373, 0.38039216],\n               [0.40784314, 0.15686275, 0.36078432],\n               [0.43137255, 0.14901961, 0.3529412 ],\n               [0.5176471 , 0.2901961 , 0.47058824],\n               [0.5137255 , 0.2509804 , 0.4627451 ],\n               [0.45882353, 0.21176471, 0.39215687],\n               [0.44313726, 0.18039216, 0.38039216]],\n              [[0.47843137, 0.19215687, 0.36078432],\n               [0.44313726, 0.14901961, 0.37254903],\n               [0.40392157, 0.13333334, 0.32941177],\n               [0.41568628, 0.12941177, 0.34901962],\n               [0.43529412, 0.14509805, 0.38431373],\n               [0.49411765, 0.23529412, 0.44313726],\n               [0.5294118 , 0.3019608 , 0.45882353],\n               [0.50980395, 0.25882354, 0.4392157 ],\n               [0.43529412, 0.19607843, 0.3529412 ],\n               [0.39215687, 0.13333334, 0.3254902 ]],\n              [[0.44705883, 0.14117648, 0.34117648],\n               [0.39607844, 0.12156863, 0.3137255 ],\n               [0.4117647 , 0.14117648, 0.34509805],\n               [0.44705883, 0.15686275, 0.3764706 ],\n               [0.5058824 , 0.20784314, 0.40392157],\n               [0.5294118 , 0.25490198, 0.42745098],\n               [0.5137255 , 0.25882354, 0.42352942],\n               [0.48235294, 0.20392157, 0.41568628],\n               [0.39215687, 0.13725491, 0.3137255 ],\n               [0.36078432, 0.11372549, 0.29411766]],\n              [[0.41960785, 0.13333334, 0.3647059 ],\n               [0.43529412, 0.1882353 , 0.38431373],\n               [0.4509804 , 0.16862746, 0.3647059 ],\n               [0.50980395, 0.23529412, 0.44705883],\n               [0.56078434, 0.28235295, 0.45882353],\n               [0.5372549 , 0.27450982, 0.42745098],\n               [0.5176471 , 0.27450982, 0.47843137],\n               [0.48235294, 0.24705882, 0.4       ],\n               [0.39215687, 0.14901961, 0.3254902 ],\n               [0.38431373, 0.13725491, 0.3137255 ]],\n              [[0.45882353, 0.1764706 , 0.4       ],\n               [0.50980395, 0.21568628, 0.42352942],\n               [0.50980395, 0.20784314, 0.42352942],\n               [0.56078434, 0.29803923, 0.49411765],\n               [0.5294118 , 0.26666668, 0.43137255],\n               [0.54509807, 0.3254902 , 0.50980395],\n               [0.5254902 , 0.3137255 , 0.50980395],\n               [0.42745098, 0.16078432, 0.32156864],\n               [0.39607844, 0.12156863, 0.32156864],\n               [0.3647059 , 0.09803922, 0.30588236]],\n              [[0.50980395, 0.21568628, 0.4117647 ],\n               [0.54509807, 0.27450982, 0.43137255],\n               [0.5529412 , 0.23137255, 0.4392157 ],\n               [0.5568628 , 0.26666668, 0.4627451 ],\n               [0.5372549 , 0.2784314 , 0.47058824],\n               [0.5529412 , 0.30980393, 0.5176471 ],\n               [0.49411765, 0.21568628, 0.3882353 ],\n               [0.42352942, 0.1764706 , 0.3529412 ],\n               [0.38039216, 0.10980392, 0.3019608 ],\n               [0.38039216, 0.09411765, 0.28627452]],\n              [[0.49803922, 0.1882353 , 0.3764706 ],\n               [0.54901963, 0.23529412, 0.41568628],\n               [0.5568628 , 0.2901961 , 0.4862745 ],\n               [0.5529412 , 0.28235295, 0.4509804 ],\n               [0.5411765 , 0.29411766, 0.5137255 ],\n               [0.5176471 , 0.24705882, 0.4509804 ],\n               [0.41960785, 0.14901961, 0.32156864],\n               [0.42352942, 0.15686275, 0.38431373],\n               [0.38431373, 0.12156863, 0.31764707],\n               [0.38039216, 0.10588235, 0.32156864]]], dtype=np.float32)\n\n\nclass RGBEquilisationTest(NiftyNetTestCase):\n    """"""\n    Test for RGBHistogramEquilisationLayer\n    """"""\n\n    def test_equilisation(self):\n        cv2 = require_module(\'cv2\', mandatory=False)\n\n        if cv2 is None:\n            self.skipTest(\'requires cv2 module\')\n            return\n\n        def _get_histogram(img):\n            inten = cv2.cvtColor(img[::-1], cv2.COLOR_BGR2YUV)[...,0]*255\n\n            return np.histogram(inten, 32, [0, 256])[0]\n\n        hist_before = _get_histogram(IMAGE_DATA)\n\n        layer = RGBHistogramEquilisationLayer(image_name=\'image\')\n        orig_shape = list(IMAGE_DATA.shape)\n        input_shape = orig_shape[:2] + [1]*2 + [3]\n        img, _ = layer(IMAGE_DATA.reshape(input_shape))\n\n        hist_after = _get_histogram(img.reshape(orig_shape))\n\n        self.assertGreater(hist_before.astype(np.float32).std(),\n                           hist_after.astype(np.float32).std())\n\n        img, _ = layer({\'image\': IMAGE_DATA.reshape(input_shape)})\n\n        hist_after = _get_histogram(img[\'image\'].reshape(orig_shape))\n\n        self.assertGreater(hist_before.astype(np.float32).std(),\n                           hist_after.astype(np.float32).std())\n\n        img = (255*IMAGE_DATA).astype(np.uint8)\n        img, _ = layer({\'image\': IMAGE_DATA.reshape(input_shape)})\n\n        hist_after = _get_histogram(img[\'image\'].reshape(orig_shape))\n\n        self.assertGreater(hist_before.astype(np.float32).std(),\n                           hist_after.astype(np.float32).std())\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/run_vars_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.application_iteration import IterationMessage, \\\n    IterationMessageGenerator\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.signal import \\\n    TRAIN, ITER_FINISHED, GRAPH_CREATED, SESS_STARTED\nfrom tests.niftynet_testcase import NiftyNetTestCase\nfrom tests.application_driver_test import get_initialised_driver\n\n\nclass DriverLoopTest(NiftyNetTestCase):\n    def test_interfaces(self):\n        msg = IterationMessage()\n        msg.current_iter = 0\n        self.assertEqual(msg.current_iter, 0)\n        self.assertEqual(msg.ops_to_run, {})\n        self.assertEqual(msg.data_feed_dict, {})\n        self.assertEqual(msg.current_iter_output, None)\n        self.assertEqual(msg.should_stop, None)\n        self.assertEqual(msg.phase, TRAIN)\n        self.assertEqual(msg.is_training, True)\n        self.assertEqual(msg.is_validation, False)\n        self.assertEqual(msg.is_inference, False)\n        msg.current_iter_output = {\'test\'}\n        self.assertEqual(msg.current_iter_output, {\'test\'})\n        self.assertGreater(msg.iter_duration, 0.0)\n        self.assertStartsWith(msg.to_console_string(), \'training\')\n        self.assertEqual(msg.to_tf_summary(0), None)\n\n    def test_set_fields(self):\n        msg = IterationMessage()\n\n        # setting iter will clear tic and iter output fields\n        msg.current_iter = 3\n        self.assertGreater(msg._current_iter_tic, 0.0)\n        self.assertEqual(msg._current_iter_output, None)\n\n        # setting iter output will update iter duration\n        msg.current_iter_output = {CONSOLE: {\'test\': \'test\'}}\n        self.assertEqual(msg.current_iter, 3)\n        self.assertGreater(msg.iter_duration, 0.0)\n        self.assertRegexpMatches(msg.to_console_string(), \'.*test=test.*\')\n\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            msg.current_iter = \'test\'\n\n    def test_run_vars(self):\n        app_driver = get_initialised_driver()\n        test_graph = app_driver.create_graph(app_driver.app, 1, True)\n        test_tensor = test_graph.get_tensor_by_name(\n            ""G/conv_bn_selu/conv_/w:0"")\n        train_eval_msgs = []\n        test_vals = []\n\n        def get_iter_msgs(_sender, **msg):\n            """""""" Captures iter_msg and model values for testing""""""\n            train_eval_msgs.append(msg[\'iter_msg\'])\n            test_vals.append(sess.run(test_tensor))\n            print(msg[\'iter_msg\'].to_console_string())\n\n        ITER_FINISHED.connect(get_iter_msgs)\n\n        with self.cached_session(graph=test_graph) as sess:\n            SESS_STARTED.send(app_driver.app, iter_msg=None)\n            iterations = IterationMessageGenerator(\n                initial_iter=0,\n                final_iter=3,\n                validation_every_n=2,\n                validation_max_iter=1,\n                is_training_action=True)\n            app_driver.loop(app_driver.app, iterations())\n\n            # Check sequence of iterations\n            self.assertRegexpMatches(\n                train_eval_msgs[0].to_console_string(), \'training\')\n            self.assertRegexpMatches(\n                train_eval_msgs[1].to_console_string(), \'training\')\n            self.assertRegexpMatches(\n                train_eval_msgs[2].to_console_string(), \'validation\')\n            self.assertRegexpMatches(\n                train_eval_msgs[3].to_console_string(), \'training\')\n\n            # Check durations\n            for iter_msg in train_eval_msgs:\n                self.assertGreater(iter_msg.iter_duration, 0.0)\n\n            # Check training changes test tensor\n            self.assertNotAlmostEqual(\n                np.mean(np.abs(test_vals[0] - test_vals[1])), 0.0)\n            self.assertNotAlmostEqual(\n                np.mean(np.abs(test_vals[2] - test_vals[3])), 0.0)\n\n            # Check validation doesn\'t change test tensor\n            self.assertAlmostEqual(\n                np.mean(np.abs(test_vals[1] - test_vals[2])), 0.0)\n\n            app_driver.app.stop()\n\n        ITER_FINISHED.disconnect(get_iter_msgs)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/sampler_balanced_v2_test.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window import N_SPATIAL\nfrom niftynet.engine.sampler_balanced_v2 import \\\n    BalancedSampler, balanced_spatial_coordinates\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'),\n                                 sampler=(\'T1\',))\n\nMOD_2D_DATA = {\n    \'ultrasound\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(10, 9, 1),\n        loader=None\n    ),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\',),\n                              sampler=(\'ultrasound\',))\n\nDYNAMIC_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    )\n}\nDYNAMIC_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'),\n                                   sampler=(\'FLAIR\',))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\nmod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\ndynamic_list = data_partitioner.initialise(DYNAMIC_MOD_DATA).get_file_list()\n\n\ndef get_3d_reader():\n    reader = ImageReader([\'image\', \'sampler\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    reader = ImageReader([\'image\', \'sampler\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_dynamic_window_reader():\n    reader = ImageReader([\'image\', \'sampler\'])\n    reader.initialise(DYNAMIC_MOD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return reader\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass BalancedSamplerTest(NiftyNetTestCase):\n    def test_3d_init(self):\n        sampler = BalancedSampler(reader=get_3d_reader(),\n                                  window_sizes=MULTI_MOD_DATA,\n                                  batch_size=2,\n                                  windows_per_image=10,\n                                  queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (2, 7, 10, 2, 2))\n        sampler.close_all()\n\n    def test_2d_init(self):\n        sampler = BalancedSampler(reader=get_2d_reader(),\n                                  window_sizes=MOD_2D_DATA,\n                                  batch_size=2,\n                                  windows_per_image=10,\n                                  queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (2, 10, 9, 1))\n        sampler.close_all()\n\n    def test_dynamic_init(self):\n        sampler = BalancedSampler(reader=get_dynamic_window_reader(),\n                                  window_sizes=DYNAMIC_MOD_DATA,\n                                  batch_size=2,\n                                  windows_per_image=10,\n                                  queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            #with self.assertRaisesRegexp(tf.errors.OutOfRangeError, """"):\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape[1:], (8, 2, 256, 2))\n\n    def test_ill_init(self):\n        with self.assertRaisesRegexp(ValueError, """"):\n            sampler = BalancedSampler(reader=get_3d_reader(),\n                                      window_sizes=MOD_2D_DATA,\n                                      batch_size=2,\n                                      windows_per_image=10,\n                                      queue_length=10)\n\n    def test_close_early(self):\n        sampler = BalancedSampler(reader=get_2d_reader(),\n                                  window_sizes=MOD_2D_DATA,\n                                  batch_size=2,\n                                  windows_per_image=10,\n                                  queue_length=10)\n        sampler.close_all()\n\n\nclass BalancedCoordinatesTest(NiftyNetTestCase):\n    def assertCoordinatesAreValid(self, coords, sampling_map):\n        for coord in coords:\n            for i in range(len(coord.shape)):\n                self.assertTrue(coord[i] >= 0)\n                self.assertTrue(coord[i] < sampling_map.shape[i])\n\n    def test_3d_coordinates(self):\n        img_size = (64, 15, 21, 1, 1)\n        win_size = (32, 13, 1, 1, 1)\n        sampling_map = np.zeros(img_size)\n        coords = balanced_spatial_coordinates(\n            32, img_size, win_size, sampling_map)\n\n        self.assertAllEqual(coords.shape, (32, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, sampling_map)\n\n        sampling_map[17, 7, 0, 0, 0] = 1.0\n        coords = balanced_spatial_coordinates(\n            500, img_size, win_size, sampling_map)\n        # better test?\n        self.assertTrue(np.sum(np.all(coords == [17, 7, 0], axis=1)) >= 200)\n\n    def test_2d_coordinates(self):\n        img_size = (23, 42, 1, 1, 1)\n        win_size = (22, 10, 1)\n        sampling_map = np.zeros(img_size)\n\n        coords = balanced_spatial_coordinates(\n            64, img_size, win_size, sampling_map)\n\n        self.assertAllEqual(coords.shape, (64, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, sampling_map)\n\n        sampling_map[11, 8, 0, 0, 0] = 1.0\n        coords = balanced_spatial_coordinates(\n            500, img_size, win_size, sampling_map)\n        # better test?\n        self.assertTrue(np.sum(np.all(coords == [11, 8, 0], axis=1)) >= 200)\n\n    def test_1d_coordinates(self):\n        img_size = (21, 1, 1, 1, 1)\n        win_size = (15, 1, 1)\n        sampling_map = np.zeros(img_size)\n        coords = balanced_spatial_coordinates(\n            10, img_size, win_size, sampling_map)\n\n        self.assertAllEqual(coords.shape, (10, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, sampling_map)\n\n        sampling_map[9, 0, 0, 0, 0] = 1.0\n        coords = balanced_spatial_coordinates(\n            500, img_size, win_size, sampling_map)\n        # better test?\n        self.assertTrue(np.sum(np.all(coords == [9, 0, 0], axis=1)) >= 200)\n\n    @unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\n    def test_classes_balances(self):\n        # Set the random state to prevent false positive\n        np.random.seed(0)\n\n        # Setting these too high inflats the run time\n        number_of_repetitions = 1000\n        samples_per_repetition = 10\n        num_classes = 3\n\n        # Create a map with almost all background, one pixel of each\n        # other label\n        img_size = (50, 25, 10, 1, 1)\n        win_size = (8, 7, 2, 1, 1)\n        sampling_map = np.zeros(img_size)\n        sampling_map[6, 5, 2:, 0, 0] = 1\n        sampling_map[11, 13:, 3, 0, 0] = 2\n\n        # Accumulate the number of times each class is sampled\n        accum = np.zeros((num_classes))\n        for _ in range(number_of_repetitions):\n            coords = balanced_spatial_coordinates(\n                samples_per_repetition, img_size, win_size, sampling_map)\n\n            # Be sure to sample the correct number\n            self.assertAllEqual(\n                coords.shape, (samples_per_repetition, N_SPATIAL))\n\n            # Convert to np.ndarry indexable\n            for coord in coords.astype(int):\n                x, y, z = coord\n                label = int(sampling_map[x, y, z])\n                accum[label] = accum[label] + 1\n\n        # Each class should be within 2 decimal places of 1.0/num_classes\n        accum = np.divide(accum, accum.sum())\n        self.assertAllClose(\n            accum,\n            np.ones((num_classes)) * 1.0 / num_classes, rtol=1e-2, atol=1e-2)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/sampler_csvpatch_v2_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.contrib.csv_reader.csv_reader import CSVReader\nfrom niftynet.contrib.csv_reader.sampler_csvpatch import CSVPatchSampler\nfrom niftynet.engine.image_window import N_SPATIAL\n# from niftynet.engine.sampler_uniform import UniformSampler\nfrom niftynet.engine.sampler_uniform_v2 import rand_spatial_coordinates\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nDYNAMIC_MOD_DATA = {\n    \'T1\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'data/csv_data\',\n        filename_contains=(),\n        filename_not_contains=(\'_\', \'csv\'),\n        interp_order=3,\n        csv_data_file=\'\',\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(69, 69, 69),\n        loader=None),\n    \'sampler\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'\',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file=\'data/csv_data/ICBMTest3.csv\')\n}\n\nDYNAMIC_MOD_TASK = ParserNamespace(\n    image=(\'T1\', ), label=(\'T1\', ), sampler=(\'sampler\', ))\n\nLARGE_MOD_DATA = {\n    \'T1\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'data/csv_data\',\n        filename_contains=(),\n        filename_not_contains=(\'_\', \'csv\'),\n        interp_order=3,\n        csv_data_file=\'\',\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(75, 75, 75),\n        loader=None),\n    \'sampler\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'\',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file=\'data/csv_data/ICBMTest2.csv\')\n}\nLARGE_MOD_DATA_2_ELEMENTS = {\n    \'T1\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'data/csv_data\',\n        filename_contains=(),\n        filename_not_contains=(\'_\', \'csv\'),\n        interp_order=3,\n        csv_data_file=\'\',\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(75, 75, 75),\n        loader=None),\n    \'sampler\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'\',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file=\'data/csv_data/ICBMTest4.csv\')\n}\nLARGE_MOD_TASK = ParserNamespace(\n    image=(\'T1\', ), label=(\'T1\', ), sampler=(\'sampler\', ))\n\nCSV_DATA = {\n    \'sampler\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'\',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file=\'data/csv_data/ICBMTest3.csv\')\n}\n\nCSV_DATA_TWO_ELEMENTS = {\n    \'sampler\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'\',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file=\'data/csv_data/ICBMTest4.csv\')\n}\n\nCSVBAD_DATA = {\n    \'sampler\':\n    ParserNamespace(\n        csv_file=\'\',\n        path_to_search=\'\',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file=\'data/csv_data/ICBMTest.csv\')\n}\n\ndata_partitioner = ImageSetsPartitioner()\n# multi_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\n# mod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\ndynamic_list = data_partitioner.initialise(DYNAMIC_MOD_DATA).get_file_list()\n\n# def get_3d_reader():\n#     reader = ImageReader([\'image\'])\n#     reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n#     return reader\n\n# def get_2d_reader():\n#     reader = ImageReader([\'image\'])\n#     reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n#     return reader\n\n\ndef get_dynamic_window_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(DYNAMIC_MOD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return reader\n\n\ndef get_large_window_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(LARGE_MOD_DATA, LARGE_MOD_TASK, dynamic_list)\n    return reader\n\n\ndef get_large_window_reader_two_elements():\n    reader = ImageReader([\'image\'])\n    reader.initialise(LARGE_MOD_DATA_2_ELEMENTS, LARGE_MOD_TASK, dynamic_list)\n    return reader\n\n\n# def get_concentric_window_reader():\n#     reader = ImageReader([\'image\', \'label\'])\n#     reader.initialise(MULTI_WINDOW_DATA, MULTI_WINDOW_TASK, multi_mod_list)\n#     return reader\n\n\ndef get_csvpatch_reader_two_elements():\n    csv_reader = CSVReader([\'sampler\'])\n    csv_reader.initialise(CSV_DATA_TWO_ELEMENTS, DYNAMIC_MOD_TASK,\n                          dynamic_list)\n    return csv_reader\n\n\ndef get_csvpatch_reader():\n    csv_reader = CSVReader([\'sampler\'])\n    csv_reader.initialise(CSV_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return csv_reader\n\n\ndef get_csvpatchbad_reader():\n    csv_reader = CSVReader([\'sampler\'])\n    csv_reader.initialise(CSVBAD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return csv_reader\n\n\nclass CSVPatchSamplerTest(NiftyNetTestCase):\n    def test_3d_csvsampler_init(self):\n        sampler = CSVPatchSampler(\n            reader=get_dynamic_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=DYNAMIC_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out[\'image_location\']\n            # print(img_loc)\n            self.assertAllClose(out[\'image\'].shape, (2, 69, 69, 69, 1))\n        sampler.close_all()\n\n    def test_pad_init(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3)\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out[\'image_location\']\n            # print(img_loc)\n            self.assertAllClose(out[\'image\'].shape[1:], (75, 75, 75, 1))\n        sampler.close_all()\n\n    def test_padd_volume(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out[\'image_location\']\n            # print(img_loc)\n            self.assertAllClose(out[\'image\'].shape[1:], (75, 75, 75, 1))\n        sampler.close_all()\n\n    def test_change_orientation(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out[\'image_location\']\n            # print(img_loc)\n            self.assertAllClose(out[\'image\'].shape[1:], (75, 75, 75, 1))\n        sampler.close_all()\n\n    def test_random_init(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3,\n            mode_correction=\'random\')\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out[\'image_location\']\n            # print(img_loc)\n            self.assertAllClose(out[\'image\'].shape[1:], (75, 75, 75, 1))\n        sampler.close_all()\n\n    def test_remove_element_two_elements(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader_two_elements(),\n            csv_reader=get_csvpatch_reader_two_elements(),\n            window_sizes=LARGE_MOD_DATA_2_ELEMENTS,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3,\n            mode_correction=\'remove\')\n        with self.cached_session() as sess:\n            sampler.set_num_threads(1)\n            try:\n                out = sess.run(sampler.pop_batch_op())\n                passed = True\n            except Exception:\n                passed = False\n            self.assertTrue(passed)\n\n    def test_remove_element_one_element(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3,\n            mode_correction=\'remove\')\n        with self.assertRaisesRegexp(Exception, """"):\n            with self.cached_session() as sess:\n                sampler.set_num_threads(1)\n                out = sess.run(sampler.pop_batch_op())\n\n    def test_ill_init(self):\n        with self.assertRaisesRegexp(Exception, """"):\n            sampler = \\\n                CSVPatchSampler(reader=get_dynamic_window_reader(),\n                                     csv_reader=get_csvpatchbad_reader(),\n                                     window_sizes=DYNAMIC_MOD_DATA,\n                                     batch_size=2,\n                                     windows_per_image=10,\n                                     queue_length=3)\n\n    #\n\n    # def test_close_early(self):\n    #     sampler = UniformSampler(reader=get_dynamic_window_reader(),\n    #                              window_sizes=DYNAMIC_MOD_DATA,\n    #                              batch_size=2,\n    #                              windows_per_image=10,\n    #                              queue_length=10)\n\n\nclass RandomCoordinatesTest(NiftyNetTestCase):\n    def assertCoordinatesAreValid(self, coords, img_size, win_size):\n        for coord in coords:\n            for i in range(len(coord.shape)):\n                self.assertTrue(coord[i] >= int(win_size[i] / 2))\n\n                self.assertTrue(coord[i] <= img_size[i] - int(win_size[i] / 2))\n\n    def test_3d_coordinates(self):\n        img_size = [8, 9, 10]\n        win_size = [7, 9, 4]\n        coords = rand_spatial_coordinates(32, img_size, win_size, None)\n        self.assertAllEqual(coords.shape, (32, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n    def test_2d_coordinates(self):\n\n        cropped_map = np.zeros((256, 512, 1))\n        img_size = [8, 9, 1]\n        win_size = [8, 8, 1]\n        coords = rand_spatial_coordinates(64, img_size, win_size, None)\n        self.assertAllEqual(coords.shape, (64, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n    def test_1d_coordinates(self):\n        cropped_map = np.zeros((1, 1, 1))\n        img_size = [4, 1, 1]\n        win_size = [2, 1, 1]\n        coords = rand_spatial_coordinates(20, img_size, win_size, None)\n        # print(coords)\n        self.assertAllEqual(coords.shape, (20, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/sampler_grid_v2_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.sampler_grid_v2 import \\\n        GridSampler, _enumerate_step_points, grid_spatial_coordinates\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 10, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 10, 2),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\nMOD_2D_DATA = {\n    \'ultrasound\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(10, 7, 1),\n        loader=None\n    ),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\',))\n\nDYNAMIC_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    )\n}\nDYNAMIC_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\nmod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\ndynamic_list = data_partitioner.initialise(DYNAMIC_MOD_DATA).get_file_list()\n\n\ndef get_3d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_dynamic_window_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(DYNAMIC_MOD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return reader\n\n\nclass GridSamplerTest(NiftyNetTestCase):\n    def test_3d_initialising(self):\n        sampler = GridSampler(reader=get_3d_reader(),\n                              window_sizes=MULTI_MOD_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(0, 0, 0),\n                              queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (10, 8, 10, 2, 2))\n        sampler.close_all()\n\n    def test_25d_initialising(self):\n        sampler = GridSampler(reader=get_3d_reader(),\n                              window_sizes=MULTI_MOD_DATA,\n                              batch_size=10,\n                              spatial_window_size=(1, 20, 15),\n                              window_border=(0, 0, 0),\n                              queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (10, 20, 15, 2))\n        sampler.close_all()\n\n    def test_2d_initialising(self):\n        sampler = GridSampler(reader=get_2d_reader(),\n                              window_sizes=MOD_2D_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(0, 0, 0),\n                              queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(1)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (10, 10, 7, 1))\n        sampler.close_all()\n\n    def test_dynamic_window_initialising(self):\n        sampler = GridSampler(reader=get_dynamic_window_reader(),\n                              window_sizes=DYNAMIC_MOD_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(0, 0, 0),\n                              queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(1)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (10, 8, 2, 256, 2))\n        sampler.close_all()\n\n    def test_name_mismatch(self):\n        with self.assertRaisesRegexp(ValueError, """"):\n            sampler = GridSampler(reader=get_dynamic_window_reader(),\n                                  window_sizes=MOD_2D_DATA,\n                                  batch_size=10,\n                                  spatial_window_size=None,\n                                  window_border=(0, 0, 0),\n                                  queue_length=10)\n        with self.assertRaisesRegexp(ValueError, """"):\n            sampler = GridSampler(reader=get_3d_reader(),\n                                  window_sizes=MOD_2D_DATA,\n                                  batch_size=10,\n                                  spatial_window_size=None,\n                                  window_border=(0, 0, 0),\n                                  queue_length=10)\n\n\nclass CoordinatesTest(NiftyNetTestCase):\n    def test_coordinates(self):\n        coords = grid_spatial_coordinates(\n            subject_id=1,\n            img_sizes={\'image\': (64, 64, 64, 1, 2),\n                       \'label\': (42, 42, 42, 1, 1)},\n            win_sizes={\'image\': (63, 63, 40),\n                       \'label\': (42, 41, 33)},\n            border_size=(2, 3, 4))\n        # first dim corresponds to subject id\n        expected_image = np.array(\n            [[1, 0, 0, 0, 63, 63, 40],\n             [1, 0, 0, 24, 63, 63, 64],\n             [1, 0, 0, 12, 63, 63, 52],\n             [1, 1, 0, 0, 64, 63, 40],\n             [1, 1, 0, 24, 64, 63, 64],\n             [1, 1, 0, 12, 64, 63, 52],\n             [1, 0, 1, 0, 63, 64, 40],\n             [1, 0, 1, 24, 63, 64, 64],\n             [1, 0, 1, 12, 63, 64, 52],\n             [1, 1, 1, 0, 64, 64, 40],\n             [1, 1, 1, 24, 64, 64, 64],\n             [1, 1, 1, 12, 64, 64, 52]], dtype=np.int32)\n        self.assertAllClose(coords[\'image\'], expected_image)\n        expected_label = np.array(\n            [[1, 0, 0, 0, 42, 41, 33],\n             [1, 0, 0, 9, 42, 41, 42],\n             [1, 0, 0, 4, 42, 41, 37],\n             [1, 0, 1, 0, 42, 42, 33],\n             [1, 0, 1, 9, 42, 42, 42],\n             [1, 0, 1, 4, 42, 42, 37]], dtype=np.int32)\n        self.assertAllClose(coords[\'label\'], expected_label)\n        pass\n\n    def test_2d_coordinates(self):\n        coords = grid_spatial_coordinates(\n            subject_id=1,\n            img_sizes={\'image\': (64, 64, 1, 1, 2),\n                       \'label\': (42, 42, 1, 1, 1)},\n            win_sizes={\'image\': (63, 63, 1),\n                       \'label\': (30, 32, 1)},\n            border_size=(2, 3, 4))\n        # first dim corresponds to subject id\n        expected_image = np.array(\n            [[1, 0, 0, 0, 63, 63, 1],\n             [1, 1, 0, 0, 64, 63, 1],\n             [1, 0, 1, 0, 63, 64, 1],\n             [1, 1, 1, 0, 64, 64, 1]], dtype=np.int32)\n        self.assertAllClose(coords[\'image\'], expected_image)\n        expected_label = np.array(\n            [[1, 0, 0, 0, 30, 32, 1],\n             [1, 12, 0, 0, 42, 32, 1],\n             [1, 6, 0, 0, 36, 32, 1],\n             [1, 0, 10, 0, 30, 42, 1],\n             [1, 12, 10, 0, 42, 42, 1],\n             [1, 6, 10, 0, 36, 42, 1],\n             [1, 0, 5, 0, 30, 37, 1],\n             [1, 12, 5, 0, 42, 37, 1],\n             [1, 6, 5, 0, 36, 37, 1]], dtype=np.int32)\n        self.assertAllClose(coords[\'label\'], expected_label)\n        pass\n\n    def test_nopadding_coordinates(self):\n        coords = grid_spatial_coordinates(\n            subject_id=1,\n            img_sizes={\'image\': (64, 64, 64, 1, 2),\n                       \'label\': (64, 64, 42, 1, 1)},\n            win_sizes={\'image\': (63, 63, 40),\n                       \'label\': (50, 62, 40)},\n            border_size=(-1, -1, -1))\n\n        coords_1 = grid_spatial_coordinates(\n            subject_id=1,\n            img_sizes={\'image\': (64, 64, 64, 1, 2),\n                       \'label\': (64, 64, 42, 1, 1)},\n            win_sizes={\'image\': (63, 63, 40),\n                       \'label\': (50, 62, 40)},\n            border_size=(0, 0, 0))\n        self.assertAllClose(coords[\'image\'], coords_1[\'image\'])\n        self.assertAllClose(coords[\'label\'], coords_1[\'label\'])\n        expected_image = np.array(\n            [[1, 0, 0, 0, 63, 63, 40],\n             [1, 0, 0, 24, 63, 63, 64],\n             [1, 0, 0, 12, 63, 63, 52],\n             [1, 1, 0, 0, 64, 63, 40],\n             [1, 1, 0, 24, 64, 63, 64],\n             [1, 1, 0, 12, 64, 63, 52],\n             [1, 0, 1, 0, 63, 64, 40],\n             [1, 0, 1, 24, 63, 64, 64],\n             [1, 0, 1, 12, 63, 64, 52],\n             [1, 1, 1, 0, 64, 64, 40],\n             [1, 1, 1, 24, 64, 64, 64],\n             [1, 1, 1, 12, 64, 64, 52]], dtype=np.int32)\n        self.assertAllClose(coords[\'image\'], expected_image)\n        expected_label = np.array(\n            [[1, 0, 0, 0, 50, 62, 40],\n             [1, 0, 0, 2, 50, 62, 42],\n             [1, 0, 0, 1, 50, 62, 41],\n             [1, 14, 0, 0, 64, 62, 40],\n             [1, 14, 0, 2, 64, 62, 42],\n             [1, 14, 0, 1, 64, 62, 41],\n             [1, 7, 0, 0, 57, 62, 40],\n             [1, 7, 0, 2, 57, 62, 42],\n             [1, 7, 0, 1, 57, 62, 41],\n             [1, 0, 2, 0, 50, 64, 40],\n             [1, 0, 2, 2, 50, 64, 42],\n             [1, 0, 2, 1, 50, 64, 41],\n             [1, 14, 2, 0, 64, 64, 40],\n             [1, 14, 2, 2, 64, 64, 42],\n             [1, 14, 2, 1, 64, 64, 41],\n             [1, 7, 2, 0, 57, 64, 40],\n             [1, 7, 2, 2, 57, 64, 42],\n             [1, 7, 2, 1, 57, 64, 41],\n             [1, 0, 1, 0, 50, 63, 40],\n             [1, 0, 1, 2, 50, 63, 42],\n             [1, 0, 1, 1, 50, 63, 41],\n             [1, 14, 1, 0, 64, 63, 40],\n             [1, 14, 1, 2, 64, 63, 42],\n             [1, 14, 1, 1, 64, 63, 41],\n             [1, 7, 1, 0, 57, 63, 40],\n             [1, 7, 1, 2, 57, 63, 42],\n             [1, 7, 1, 1, 57, 63, 41]], dtype=np.int32)\n        self.assertAllClose(coords[\'label\'], expected_label)\n        with self.assertRaisesRegexp(AssertionError, """"):\n            coords_1 = grid_spatial_coordinates(\n                subject_id=1,\n                img_sizes={\'image\': (64, 64, 64, 1, 2),\n                           \'label\': (42, 42, 42, 1, 1)},\n                win_sizes={\'image\': (63, 63, 40),\n                           \'label\': (80, 80, 33)},\n                border_size=(0, 0, 0))\n\n\nclass StepPointsTest(NiftyNetTestCase):\n    def test_steps(self):\n        loc = _enumerate_step_points(0, 10, 4, 1)\n        self.assertAllClose(loc, [0, 1, 2, 3, 4, 5, 6])\n\n        loc = _enumerate_step_points(0, 10, 4, 2)\n        self.assertAllClose(loc, [0, 2, 4, 6])\n\n        loc = _enumerate_step_points(0, 0, 4, 2)\n        self.assertAllClose(loc, [0])\n\n        loc = _enumerate_step_points(0, 0, 4, 2)\n        self.assertAllClose(loc, [0])\n\n        loc = _enumerate_step_points(0, 0, 0, -1)\n        self.assertAllClose(loc, [0])\n\n        loc = _enumerate_step_points(0, 10, 8, 8)\n        self.assertAllClose(loc, [0, 2, 1])\n\n        with self.assertRaisesRegexp(ValueError, """"):\n            loc = _enumerate_step_points(\'foo\', 0, 0, 10)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/sampler_linear_interpolate_v2_test.py,1,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.engine.sampler_linear_interpolate_v2 import LinearInterpolateSampler\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMULTI_MOD_DATA = {\n    'T1': ParserNamespace(\n        csv_file=os.path.join('testing_data', 'T1sampler.csv'),\n        path_to_search='testing_data',\n        filename_contains=('_o_T1_time',),\n        filename_not_contains=('Parcellation',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    ),\n    'FLAIR': ParserNamespace(\n        csv_file=os.path.join('testing_data', 'FLAIRsampler.csv'),\n        path_to_search='testing_data',\n        filename_contains=('FLAIR_',),\n        filename_not_contains=('Parcellation',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=('T1', 'FLAIR'))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\n\n\ndef get_3d_reader():\n    reader = ImageReader(['image'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\nclass LinearInterpolateSamplerTest(NiftyNetTestCase):\n    def test_init(self):\n        sampler = LinearInterpolateSampler(\n            reader=get_3d_reader(),\n            window_sizes=MULTI_MOD_DATA,\n            batch_size=1,\n            n_interpolations=8,\n            queue_length=1)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out['image'].shape, [1, 256, 168, 256, 2])\n        sampler.close_all()\n\n\nif __name__ == '__main__':\n    tf.test.main()\n"""
tests/sampler_random_vector_v2_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.sampler_random_vector_v2 import RandomVectorSampler\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass RandomVectorSamplerTest(NiftyNetTestCase):\n    def test_random_vector(self):\n        sampler = RandomVectorSampler(names=(\'test_vector\',),\n                                      vector_size=(100,),\n                                      batch_size=20,\n                                      repeat=None)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'test_vector\'].shape, (20, 100))\n        sampler.close_all()\n\n    def test_ill_init(self):\n        with self.assertRaisesRegexp(TypeError, """"):\n            sampler = RandomVectorSampler(names=(\'test_vector\',),\n                                          vector_size=10,\n                                          batch_size=20)\n        with self.assertRaisesRegexp(TypeError, """"):\n            sampler = RandomVectorSampler(names=0,\n                                          vector_size=(10,),\n                                          batch_size=20)\n\n    def test_repeat(self):\n        batch_size = 20\n        n_interpolations = 10\n        repeat = 4\n        sampler = RandomVectorSampler(names=(\'test_vector\',),\n                                      vector_size=(100,),\n                                      batch_size=batch_size,\n                                      n_interpolations=n_interpolations,\n                                      repeat=repeat)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(1)\n            n_output = 0\n            for _ in range(2):\n                out_vector = sess.run(sampler.pop_batch_op())\n                if np.all(out_vector[\'test_vector\'] == -1):\n                    break\n                n_output = n_output + batch_size\n                self.assertAllClose(out_vector[\'test_vector\'].shape,\n                                    (batch_size, 100))\n                self.assertAllClose(np.mean(out_vector[\'test_vector\']),\n                                    0.0, atol=0.5, rtol=0.5)\n                self.assertAllClose(np.std(out_vector[\'test_vector\']),\n                                    1.0, atol=1.0, rtol=1.0)\n            self.assertEquals(n_output, n_interpolations * repeat)\n        sampler.close_all()\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/sampler_resize_v2_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\nMOD_2D_DATA = {\n    \'ultrasound\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(10, 9, 1),\n        loader=None\n    ),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\',))\n\nDYNAMIC_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    )\n}\nDYNAMIC_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\nmod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\ndynamic_list = data_partitioner.initialise(DYNAMIC_MOD_DATA).get_file_list()\n\n\ndef get_3d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_dynamic_window_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(DYNAMIC_MOD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return reader\n\n\nclass ResizeSamplerTest(NiftyNetTestCase):\n    def test_3d_init(self):\n        sampler = ResizeSampler(\n            reader=get_3d_reader(),\n            window_sizes=MULTI_MOD_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=1)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, [1, 7, 10, 2, 2])\n        sampler.close_all()\n\n    def test_dynamic_init(self):\n        sampler = ResizeSampler(\n            reader=get_dynamic_window_reader(),\n            window_sizes=DYNAMIC_MOD_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=1)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, [1, 8, 2, 256, 2])\n        sampler.close_all()\n\n    def test_2d_init(self):\n        sampler = ResizeSampler(\n            reader=get_2d_reader(),\n            window_sizes=MOD_2D_DATA,\n            batch_size=1,\n            shuffle=True,\n            queue_length=1)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, [1, 10, 9, 1])\n        sampler.close_all()\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/sampler_uniform_v2_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\n# from niftynet.engine.sampler_uniform import UniformSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_uniform_v2 import rand_spatial_coordinates\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom niftynet.engine.image_window import N_SPATIAL\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\nMULTI_WINDOW_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(4, 10, 3),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 12, 2),\n        loader=None\n    )\n}\nMULTI_WINDOW_TASK = ParserNamespace(image=(\'T1\',), label=(\'FLAIR\',))\n\nMOD_2D_DATA = {\n    \'ultrasound\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(10, 9, 1),\n        loader=None\n    ),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\',))\n\nDYNAMIC_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    )\n}\nDYNAMIC_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\nmod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\ndynamic_list = data_partitioner.initialise(DYNAMIC_MOD_DATA).get_file_list()\n\n\ndef get_3d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_dynamic_window_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(DYNAMIC_MOD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return reader\n\ndef get_concentric_window_reader():\n    reader = ImageReader([\'image\', \'label\'])\n    reader.initialise(MULTI_WINDOW_DATA, MULTI_WINDOW_TASK, multi_mod_list)\n    return reader\n\n\nclass UniformSamplerTest(NiftyNetTestCase):\n    def test_3d_concentric_init(self):\n        sampler = UniformSampler(reader=get_concentric_window_reader(),\n                                 window_sizes=MULTI_WINDOW_DATA,\n                                 batch_size=2,\n                                 windows_per_image=10,\n                                 queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out[\'image_location\']\n            seg_loc = out[\'label_location\']\n            self.assertTrue(np.all(img_loc[:, 0] == seg_loc[:, 0]))\n            self.assertTrue(np.all((img_loc - seg_loc)[:, 1:4] == [1, 1, 0]))\n            self.assertTrue(np.all((img_loc - seg_loc)[:, 4:] == [-2, -1, 1]))\n            self.assertAllClose(out[\'image\'].shape, (2, 4, 10, 3, 1))\n            self.assertAllClose(out[\'label\'].shape, (2, 7, 12, 2, 1))\n        sampler.close_all()\n\n    def test_3d_init(self):\n        sampler = UniformSampler(reader=get_3d_reader(),\n                                 window_sizes=MULTI_MOD_DATA,\n                                 batch_size=2,\n                                 windows_per_image=10,\n                                 queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (2, 7, 10, 2, 2))\n        sampler.close_all()\n\n    def test_2d_init(self):\n        sampler = UniformSampler(reader=get_2d_reader(),\n                                 window_sizes=MOD_2D_DATA,\n                                 batch_size=2,\n                                 windows_per_image=10,\n                                 queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (2, 10, 9, 1))\n        sampler.close_all()\n\n    def test_dynamic_init(self):\n        sampler = UniformSampler(reader=get_dynamic_window_reader(),\n                                 window_sizes=DYNAMIC_MOD_DATA,\n                                 batch_size=2,\n                                 windows_per_image=10,\n                                 queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape[1:], (8, 2, 256, 2))\n        sampler.close_all()\n\n    def test_ill_init(self):\n        with self.assertRaisesRegexp(ValueError, """"):\n            sampler = UniformSampler(reader=get_3d_reader(),\n                                     window_sizes=MOD_2D_DATA,\n                                     batch_size=2,\n                                     windows_per_image=10,\n                                     queue_length=10)\n\n    def test_close_early(self):\n        sampler = UniformSampler(reader=get_dynamic_window_reader(),\n                                 window_sizes=DYNAMIC_MOD_DATA,\n                                 batch_size=2,\n                                 windows_per_image=10,\n                                 queue_length=10)\n        sampler.close_all()\n\n\nclass RandomCoordinatesTest(NiftyNetTestCase):\n    def assertCoordinatesAreValid(self, coords, img_size, win_size):\n        for coord in coords:\n            for i in range(len(coord.shape)):\n                self.assertTrue(coord[i] >= int(win_size[i] / 2))\n                self.assertTrue(coord[i] <= img_size[i] - int(win_size[i]/2))\n\n    def test_3d_coordinates(self):\n        img_size = [8, 9, 10]\n        win_size = [7, 9, 4]\n        coords = rand_spatial_coordinates(\n            32, img_size, win_size, None)\n        self.assertAllEqual(coords.shape, (32, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n    def test_2d_coordinates(self):\n        cropped_map=np.zeros((256, 512, 1))\n        img_size = [8, 9, 1]\n        win_size = [8, 8, 1]\n        coords = rand_spatial_coordinates(\n            64, img_size, win_size, None)\n        self.assertAllEqual(coords.shape, (64, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n    def test_1d_coordinates(self):\n        cropped_map=np.zeros((1, 1, 1))\n        img_size = [4, 1, 1]\n        win_size = [2, 1, 1]\n        coords = rand_spatial_coordinates(\n            20, img_size, win_size, None)\n        print(coords)\n        self.assertAllEqual(coords.shape, (20, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/sampler_weighted_v2_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window import N_SPATIAL\nfrom niftynet.engine.sampler_weighted_v2 import \\\n    WeightedSampler, weighted_spatial_coordinates\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'),\n                                 sampler=(\'T1\',))\n\nMOD_2D_DATA = {\n    \'ultrasound\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(10, 9, 1),\n        loader=None\n    ),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\',),\n                              sampler=(\'ultrasound\',))\n\nDYNAMIC_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    )\n}\nDYNAMIC_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'),\n                                   sampler=(\'FLAIR\',))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\nmod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\ndynamic_list = data_partitioner.initialise(DYNAMIC_MOD_DATA).get_file_list()\n\n\ndef get_3d_reader():\n    reader = ImageReader([\'image\', \'sampler\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    reader = ImageReader([\'image\', \'sampler\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_dynamic_window_reader():\n    reader = ImageReader([\'image\', \'sampler\'])\n    reader.initialise(DYNAMIC_MOD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return reader\n\n\nclass WeightedSamplerTest(NiftyNetTestCase):\n    def test_3d_init(self):\n        sampler = WeightedSampler(reader=get_3d_reader(),\n                                  window_sizes=MULTI_MOD_DATA,\n                                  batch_size=2,\n                                  windows_per_image=10,\n                                  queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (2, 7, 10, 2, 2))\n        sampler.close_all()\n\n    def test_2d_init(self):\n        sampler = WeightedSampler(reader=get_2d_reader(),\n                                  window_sizes=MOD_2D_DATA,\n                                  batch_size=2,\n                                  windows_per_image=10,\n                                  queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape, (2, 10, 9, 1))\n        sampler.close_all()\n\n    def test_dynamic_init(self):\n        sampler = WeightedSampler(reader=get_dynamic_window_reader(),\n                                  window_sizes=DYNAMIC_MOD_DATA,\n                                  batch_size=2,\n                                  windows_per_image=10,\n                                  queue_length=10)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertAllClose(out[\'image\'].shape[1:], (8, 2, 256, 2))\n\n    def test_ill_init(self):\n        with self.assertRaisesRegexp(ValueError, """"):\n            sampler = WeightedSampler(reader=get_3d_reader(),\n                                      window_sizes=MOD_2D_DATA,\n                                      batch_size=2,\n                                      windows_per_image=10,\n                                      queue_length=10)\n\n    def test_close_early(self):\n        sampler = WeightedSampler(reader=get_2d_reader(),\n                                  window_sizes=MOD_2D_DATA,\n                                  batch_size=2,\n                                  windows_per_image=10,\n                                  queue_length=10)\n        sampler.close_all()\n\n\nclass WeightedCoordinatesTest(NiftyNetTestCase):\n    def assertCoordinatesAreValid(self, coords, sampling_map):\n        for coord in coords:\n            for i in range(len(coord.shape)):\n                self.assertTrue(coord[i] >= 0)\n                self.assertTrue(coord[i] < sampling_map.shape[i])\n\n    def test_3d_coordinates(self):\n        img_size = (32, 16, 17, 1, 1)\n        win_size = (10, 16, 15)\n        sampling_map = np.zeros(img_size)\n\n        coords = weighted_spatial_coordinates(\n            32, img_size, win_size, sampling_map)\n        self.assertAllEqual(coords.shape, (32, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, sampling_map)\n\n        # testing high weight location (10, 8, 7, 0, 0)\n        sampling_map[10, 8, 7, 0, 0] = 1.0\n        coords = weighted_spatial_coordinates(\n            32, img_size, win_size, sampling_map)\n        self.assertAllEqual(coords.shape, (32, N_SPATIAL))\n        self.assertTrue(np.all(coords == [10, 8, 7]))\n\n    def test_2d_coordinates(self):\n        img_size = (32, 17, 1, 1, 1)\n        win_size = (31, 3, 1)\n        sampling_map = np.zeros(img_size)\n        coords = weighted_spatial_coordinates(\n            64, img_size, win_size, sampling_map)\n\n        self.assertAllEqual(coords.shape, (64, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, sampling_map)\n\n        # testing high weight location (15, 1, 1, 0, 0)\n        sampling_map[15, 1, 0, 0, 0] = 1.0\n        coords = weighted_spatial_coordinates(\n            64, img_size, win_size, sampling_map)\n        self.assertAllEqual(coords.shape, (64, N_SPATIAL))\n        self.assertTrue(np.all(coords == [15, 1, 0]))\n\n    def test_1d_coordinates(self):\n        img_size = (32, 1, 1, 1, 1)\n        win_size = (15, 1, 1)\n        sampling_map = np.zeros(img_size)\n        coords = weighted_spatial_coordinates(\n            10, img_size, win_size, sampling_map)\n        self.assertAllEqual(coords.shape, (10, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, sampling_map)\n\n        sampling_map[20, 0, 0] = 0.1\n        coords = weighted_spatial_coordinates(\n            10, img_size, win_size, sampling_map)\n        self.assertAllEqual(coords.shape, (10, N_SPATIAL))\n        self.assertTrue(np.all(coords == [20, 0, 0]))\n\n        sampling_map[9, 0, 0] = 0.1\n        coords = weighted_spatial_coordinates(\n            10, img_size, win_size, sampling_map)\n        self.assertAllEqual(coords.shape, (10, N_SPATIAL))\n        self.assertTrue(np.all((coords == [20, 0, 0]) | (coords == [9, 0, 0])))\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/scaleblock_test.py,13,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.scalenet import ScaleBlock\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass ScaleBlockTest(NiftyNetTestCase):\n    def get_2d_input(self):\n        input_shape = (2, 32, 32, 4)\n        x = tf.ones(input_shape)\n        x = tf.unstack(x, axis=-1)\n        for (idx, fea) in enumerate(x):\n            x[idx] = tf.expand_dims(fea, axis=-1)\n        x = tf.stack(x, axis=-1)\n        return x\n\n    def get_3d_input(self):\n        input_shape = (2, 32, 32, 32, 4)\n        x = tf.ones(input_shape)\n        x = tf.unstack(x, axis=-1)\n        for (idx, fea) in enumerate(x):\n            x[idx] = tf.expand_dims(fea, axis=-1)\n        x = tf.stack(x, axis=-1)\n        return x\n\n    def test_2d_shape(self):\n        x = self.get_2d_input()\n        scalenet_layer = ScaleBlock(\'AVERAGE\', n_layers=1)\n        out_1 = scalenet_layer(x, is_training=True)\n        print(scalenet_layer)\n\n        scalenet_layer = ScaleBlock(\'MAX\', n_layers=2)\n        out_2 = scalenet_layer(x, is_training=True)\n        print(scalenet_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 32, 32, 1), out_1.shape)\n            self.assertAllClose((2, 32, 32, 1), out_2.shape)\n\n    def test_2d_reg_shape(self):\n        x = self.get_2d_input()\n        scalenet_layer = ScaleBlock(\n            \'AVERAGE\',\n            n_layers=1,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n        out_1 = scalenet_layer(x, is_training=True)\n        print(scalenet_layer)\n\n        scalenet_layer = ScaleBlock(\'MAX\', n_layers=2)\n        out_2 = scalenet_layer(x, is_training=True)\n        print(scalenet_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 32, 32, 1), out_1.shape)\n            self.assertAllClose((2, 32, 32, 1), out_2.shape)\n\n    def test_3d_shape(self):\n        x = self.get_3d_input()\n        scalenet_layer = ScaleBlock(\'AVERAGE\', n_layers=1)\n        out_1 = scalenet_layer(x, is_training=True)\n        print(scalenet_layer)\n\n        scalenet_layer = ScaleBlock(\'MAX\', n_layers=2)\n        out_2 = scalenet_layer(x, is_training=True)\n        print(scalenet_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 32, 32, 32, 1), out_1.shape)\n            self.assertAllClose((2, 32, 32, 32, 1), out_2.shape)\n\n    def test_3d_reg_shape(self):\n        x = self.get_3d_input()\n        scalenet_layer = ScaleBlock(\n            \'AVERAGE\',\n            n_layers=1,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n        out_1 = scalenet_layer(x, is_training=True)\n        print(scalenet_layer)\n\n        scalenet_layer = ScaleBlock(\'MAX\', n_layers=2)\n        out_2 = scalenet_layer(x, is_training=True)\n        print(scalenet_layer)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 32, 32, 32, 1), out_1.shape)\n            self.assertAllClose((2, 32, 32, 32, 1), out_2.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/scalenet_test.py,9,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.scalenet import ScaleNet\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass ScaleNetTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 32, 32, 32, 4)\n        x = tf.ones(input_shape)\n\n        scalenet_layer = ScaleNet(num_classes=5)\n        out = scalenet_layer(x, is_training=True)\n        print(scalenet_layer.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 5), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 32, 32, 4)\n        x = tf.ones(input_shape)\n\n        scalenet_layer = ScaleNet(num_classes=5)\n        out = scalenet_layer(x, is_training=True)\n        print(scalenet_layer.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 5), out.shape)\n\n    def test_3d_reg_shape(self):\n        input_shape = (2, 32, 32, 32, 4)\n        x = tf.ones(input_shape)\n\n        scalenet_layer = ScaleNet(num_classes=5,\n                                  w_regularizer=regularizers.l2_regularizer(\n                                      0.3))\n        out = scalenet_layer(x, is_training=True)\n        print(scalenet_layer.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 5), out.shape)\n\n    def test_2d_reg_shape(self):\n        input_shape = (2, 32, 32, 4)\n        x = tf.ones(input_shape)\n\n        scalenet_layer = ScaleNet(num_classes=5,\n                                  w_regularizer=regularizers.l2_regularizer(\n                                      0.3))\n        out = scalenet_layer(x, is_training=True)\n        print(scalenet_layer.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 5), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/se_resnet_test.py,9,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.se_resnet import SE_ResNet\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass SeResNet3DTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 8, 16, 32, 1)\n        x = tf.ones(input_shape)\n\n        resnet_instance = SE_ResNet(num_classes=160)\n        out = resnet_instance(x, is_training=True)\n        print(resnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 160), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 8, 16, 1)\n        x = tf.ones(input_shape)\n\n        resnet_instance = SE_ResNet(num_classes=160)\n        out = resnet_instance(x, is_training=True)\n        print(resnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 160), out.shape)\n\n    def test_3d_reg_shape(self):\n        input_shape = (2, 8, 16, 24, 1)\n        x = tf.ones(input_shape)\n\n        resnet_instance = SE_ResNet(num_classes=160,\n                               w_regularizer=regularizers.l2_regularizer(0.4))\n        out = resnet_instance(x, is_training=True)\n        print(resnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 160), out.shape)\n\n    def test_2d_reg_shape(self):\n        input_shape = (2, 8, 16, 1)\n        x = tf.ones(input_shape)\n\n        resnet_instance = SE_ResNet(num_classes=160,\n                               w_regularizer=regularizers.l2_regularizer(0.4))\n        out = resnet_instance(x, is_training=True)\n        print(resnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 160), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/segmentation_evaluator_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.evaluation.segmentation_evaluator import SegmentationEvaluator\nfrom niftynet.io.misc_io import set_logger\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass SegmentationEvaluatorTest(NiftyNetTestCase):\n    def test_basic(self):\n        class NS(object):\n            def __init__(self, dict):\n                self.__dict__.update(dict)\n        segmentation_param=NS({\'evaluation_units\':\'foreground,cc\',\n                               \'num_classes\':2,\n                            \'output_prob\':False})\n        eval_param=NS({\'evaluations\':\'Dice\'})\n        mask = np.reshape(np.abs(np.linspace(0.,2.,64)-1)>.8,[4,4,4,1,1])\n        data_dict = {\'label\':mask,\'inferred\':mask}\n        interp_orders = {\'label\':0,\'inferred\':0}\n        image_folder = \'.\'\n        e = SegmentationEvaluator(None, segmentation_param, eval_param)\n        \n        def generator():\n            yield (\'test\',data_dict,interp_orders)\n\n        result_dict = e.evaluate_from_generator(generator())\n        self.assertIn((\'subject_id\', \'label\'), result_dict)\n        self.assertIn((\'subject_id\', \'cc_id\'), result_dict)\n        self.assertEqual(tuple(result_dict[(\'subject_id\', \'label\')].index.names),\n                         (\'subject_id\', \'label\'))\n        self.assertEqual(tuple(result_dict[(\'subject_id\', \'cc_id\')].index.names),\n                         (\'subject_id\', \'cc_id\'))\n        print(result_dict[(\'subject_id\', \'cc_id\')].to_dict(\'index\'))\n        self.assertEqual(result_dict[(\'subject_id\', \'label\')].to_dict(\'index\'),\n                      {(\'test\', 1): {\'dice\': 1.}})\n        self.assertEqual(result_dict[(\'subject_id\', \'cc_id\')].to_dict(\'index\'),\n                      {(\'test\', \'r1_s1\'): {\'dice\': 1.},\n                       (\'test\', \'r2_s2\'): {\'dice\': 1.}})\n\n\nif __name__ == ""__main__"":\n    set_logger()\n    # _run_test_application()\n    tf.test.main()\n'"
tests/simple_gan_test.py,7,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.simple_gan import SimpleGAN\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass SimpleGANTest(NiftyNetTestCase):\n    def test_3d_reg_shape(self):\n        input_shape = (2, 32, 32, 32, 1)\n        noise_shape = (2, 512)\n        x = tf.ones(input_shape)\n        r = tf.ones(noise_shape)\n\n        simple_gan_instance = SimpleGAN()\n        out = simple_gan_instance(r, x, is_training=True)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose(input_shape, out[0].shape)\n            self.assertAllClose((2, 1), out[1].shape)\n            self.assertAllClose((2, 1), out[2].shape)\n\n    def test_2d_reg_shape(self):\n        input_shape = (2, 64, 64, 1)\n        noise_shape = (2, 512)\n        x = tf.ones(input_shape)\n        r = tf.ones(noise_shape)\n\n        simple_gan_instance = SimpleGAN()\n        out = simple_gan_instance(r, x, is_training=True)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose(input_shape, out[0].shape)\n            self.assertAllClose((2, 1), out[1].shape)\n            self.assertAllClose((2, 1), out[2].shape)\n\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/spatial_gradient_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.spatial_gradient import SpatialGradientLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass SpatialGradientTest(NiftyNetTestCase):\n\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def _test_nd_gradient_output_shape(self,\n                                       rank,\n                                       param_dict,\n                                       expected_shape,\n                                       expected_value=None):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        elif rank == 3:\n            input_data = self.get_3d_input()\n\n        gradient_layer = SpatialGradientLayer(**param_dict)\n        output_data = gradient_layer(input_data)\n        print(gradient_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(output_data)\n            if expected_value is not None:\n                self.assertAllClose(expected_value, out)\n            self.assertAllClose(expected_shape, out.shape)\n\n    def test_cropping_shape(self):\n        for spatial_ind in [-1, 0, 1]:\n            input_param = {\'spatial_axis\': spatial_ind, \'do_cropping\': True}\n\n            expected_shape = (2, 14, 14, 14, 8)\n            expected_value = np.zeros(expected_shape)\n            self._test_nd_gradient_output_shape(\n                rank=3, param_dict=input_param,\n                expected_shape=expected_shape,\n                expected_value=expected_value)\n\n            expected_shape = (2, 14, 14, 8)\n            expected_value = np.zeros(expected_shape)\n            self._test_nd_gradient_output_shape(\n                rank=2, param_dict=input_param,\n                expected_shape=expected_shape,\n                expected_value=expected_value)\n\n    def test_no_cropping_shape(self):\n        input_param = {\'spatial_axis\': 0, \'do_cropping\': False}\n        expected_shape = (2, 14, 16, 16, 8)\n        expected_value = np.zeros(expected_shape)\n        self._test_nd_gradient_output_shape(\n            rank=3, param_dict=input_param,\n            expected_shape=expected_shape,\n            expected_value=expected_value)\n\n        input_param = {\'spatial_axis\': 0, \'do_cropping\': False}\n        expected_shape = (2, 14, 16, 8)\n        expected_value = np.zeros(expected_shape)\n        self._test_nd_gradient_output_shape(\n            rank=2, param_dict=input_param,\n            expected_shape=expected_shape,\n            expected_value=expected_value)\n\n        input_param = {\'spatial_axis\': 1, \'do_cropping\': False}\n        expected_shape = (2, 16, 14, 16, 8)\n        expected_value = np.zeros(expected_shape)\n        self._test_nd_gradient_output_shape(\n            rank=3, param_dict=input_param,\n            expected_shape=expected_shape,\n            expected_value=expected_value)\n\n        input_param = {\'spatial_axis\': 1, \'do_cropping\': False}\n        expected_shape = (2, 16, 14, 8)\n        expected_value = np.zeros(expected_shape)\n        self._test_nd_gradient_output_shape(\n            rank=2, param_dict=input_param,\n            expected_shape=expected_shape,\n            expected_value=expected_value)\n\n        input_param = {\'spatial_axis\': 2, \'do_cropping\': False}\n        expected_shape = (2, 16, 16, 14, 8)\n        expected_value = np.zeros(expected_shape)\n        self._test_nd_gradient_output_shape(\n            rank=3, param_dict=input_param,\n            expected_shape=expected_shape,\n            expected_value=expected_value)\n\n        input_param = {\'spatial_axis\': -1, \'do_cropping\': False}\n        self._test_nd_gradient_output_shape(\n            rank=3, param_dict=input_param,\n            expected_shape=expected_shape,\n            expected_value=expected_value)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/spatial_transformer_test.py,9,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.spatial_transformer import ResampledFieldGridWarperLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass ResamplerTest(NiftyNetTestCase):\n    def get_3d_input1(self):\n        return tf.expand_dims(tf.constant(\n            [[[[1, 2, -1], [3, 4, -2]], [[5, 6, -3], [7, 8, -4]]],\n             [[[9, 10, -5], [11, 12, -6]], [[13, 14, -7], [15, 16, -8]]]],\n            dtype=tf.float32), 4)\n\n    def get_3d_input2(self):\n        return tf.concat([self.get_3d_input1(), 100 + self.get_3d_input1()], 4)\n\n    def _test_correctness(self, input, grid, interpolation, boundary,\n                          expected_value):\n        resampler = ResamplerLayer(interpolation=interpolation,\n                                   boundary=boundary)\n        out = resampler(input, grid)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_value = sess.run(out)\n            self.assertAllClose(expected_value, out_value)\n\n    def test_interpolation_gridwarper_correctness(self):\n        gridL = ResampledFieldGridWarperLayer([2, 2, 3], [2, 2, 2], [2, 2, 2])\n        grid = gridL(\n            tf.constant([[[[[1, 1, 0], [1, 1, 1]], [[1, 0, 0], [1, 0, 1]]],\n                          [[[0, 1, 0], [0, 1, 1]], [[0, 0, 0], [0, 0, 1]]]],\n                         [[[[0, 0, 0.5], [0, 0, 1]], [[0, 1, 0.5], [0, 1, 1]]],\n                          [[[1, 0, 0.5], [1, 0, 1]],\n                           [[1, 1, 0.5], [1, 1, 1]]]]],\n                        dtype=tf.float32))\n        expected_value = [[[[[7], [8]], [[5], [6]]], [[[3], [4]], [[1], [2]]]],\n                          [[[[9.5], [10]], [[11.5], [12]]],\n                           [[[13.5], [14]], [[15.5], [16]]]]]\n        self._test_correctness(input=self.get_3d_input1(),\n                               grid=grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected_value)\n\n    def test_interpolation_gridwarper_correctness2(self):\n        field = tf.constant(\n            [[[[[0, 0, 0.5], [0, 0, 1]], [[0, 1, 0.5], [0, 1, 1]]],\n              [[[1, 0, 0.5], [1, 0, 1]], [[1, 1, 0.5], [1, 1, 1]]]]],\n            dtype=tf.float32)\n        grid = ResampledFieldGridWarperLayer([2, 2, 3], [3, 3, 3], [2, 2, 2])(\n            field)\n        expected_value = [[[[[9.5], [9.75], [10]], [[10.5], [10.75], [11]],\n                            [[11.5], [11.75], [12]]],\n                           [[[11.5], [11.75], [12]], [[12.5], [12.75], [13]],\n                            [[13.5], [13.75], [14]]],\n                           [[[13.5], [13.75], [14]], [[14.5], [14.75], [15]],\n                            [[15.5], [15.75], [16]]]]]\n        self._test_correctness(input=self.get_3d_input1()[1:, :, :, :, :],\n                               grid=grid,\n                               interpolation=\'LINEAR\',\n                               boundary=\'REPLICATE\',\n                               expected_value=expected_value)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/squeeze_excitation_test.py,35,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom niftynet.layer.squeeze_excitation import ChannelSELayer\nfrom niftynet.layer.squeeze_excitation import SpatialSELayer\nfrom niftynet.layer.squeeze_excitation import ChannelSpatialSELayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass SETest(NiftyNetTestCase):\n    def test_cSE_3d_shape(self):\n        input_shape = (2, 16, 16, 16, 32)\n        x = tf.ones(input_shape)\n        se_layer = ChannelSELayer()\n        out_se = se_layer(x)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            \n    def test_sSE_3d_shape(self):\n        input_shape = (2, 16, 16, 16, 32)\n        x = tf.ones(input_shape)\n        se_layer = SpatialSELayer()\n        out_se = se_layer(x)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            \n    def test_csSE_3d_shape(self):\n        input_shape = (2, 16, 16, 16, 32)\n        x = tf.ones(input_shape)\n        se_layer = ChannelSpatialSELayer()\n        out_se = se_layer(x)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n\n    def test_cSE_2d_shape(self):\n        input_shape = (2, 16, 16, 32)\n        x = tf.ones(input_shape)\n        se_layer = ChannelSELayer()\n        out_se = se_layer(x)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            \n    def test_sSE_2d_shape(self):\n        input_shape = (2, 16, 16, 32)\n        x = tf.ones(input_shape)\n        se_layer = SpatialSELayer()\n        out_se = se_layer(x)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n            \n    def test_csSE_2d_shape(self):\n        input_shape = (2, 16, 16, 32)\n        x = tf.ones(input_shape)\n        se_layer = ChannelSpatialSELayer()\n        out_se = se_layer(x)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            x_shape = tuple(x.shape.as_list())\n            self.assertAllClose(x_shape, out.shape)\n\n    def test_cSE_3d_excitation_op(self):\n        input_shape = (2, 16, 16, 16, 32)\n        x = tf.random_uniform(input_shape,seed=0)\n        se_layer = ChannelSELayer()\n        out_se = se_layer(x)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            x=sess.run(x)\n            x_0_0=float(x[0,0,0,0,0])\n            x_1_0=float(x[0,1,0,0,0])\n            x_0_1=float(x[0,0,0,0,1])\n            x_1_1=float(x[0,1,0,0,1])\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            out_0_0=float(out[0,0,0,0,0])\n            out_1_0=float(out[0,1,0,0,0])\n            out_0_1=float(out[0,0,0,0,1])\n            out_1_1=float(out[0,1,0,0,1])\n\n        div_0_0=out_0_0/x_0_0\n        div_1_0=out_1_0/x_1_0\n        div_0_1=out_0_1/x_0_1\n        div_1_1=out_1_1/x_1_1\n\n        with self.cached_session() as sess:\n            self.assertAlmostEqual(div_0_0, div_1_0,places=5)\n            self.assertAlmostEqual(div_0_1, div_1_1,places=5)\n            \n    def test_sSE_3d_excitation_op(self):\n        input_shape = (2, 16, 16, 16, 32)\n        x = tf.random_uniform(input_shape,seed=0)\n        se_layer = SpatialSELayer()\n        out_se = se_layer(x)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            x=sess.run(x)\n            x_0_0=float(x[0,0,0,0,0])\n            x_0_1=float(x[0,0,0,0,1])\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            out_0_0=float(out[0,0,0,0,0])\n            out_0_1=float(out[0,0,0,0,1])\n\n        div_0_0=out_0_0/x_0_0\n        div_0_1=out_0_1/x_0_1\n\n        with self.cached_session() as sess:\n            self.assertAlmostEqual(div_0_0, div_0_1,places=5)\n\n    def test_cSE_2d_excitation_op(self):\n        input_shape = (2, 16, 16, 32)\n        x = tf.random_uniform(input_shape,seed=0)\n        se_layer = ChannelSELayer()\n        out_se = se_layer(x)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            x=sess.run(x)\n            x_0_0=float(x[0,0,0,0])\n            x_1_0=float(x[0,1,0,0])\n            x_0_1=float(x[0,0,0,1])\n            x_1_1=float(x[0,1,0,1])\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            out_0_0=float(out[0,0,0,0])\n            out_1_0=float(out[0,1,0,0])\n            out_0_1=float(out[0,0,0,1])\n            out_1_1=float(out[0,1,0,1])\n\n        div_0_0=out_0_0/x_0_0\n        div_1_0=out_1_0/x_1_0\n        div_0_1=out_0_1/x_0_1\n        div_1_1=out_1_1/x_1_1\n\n        with self.cached_session() as sess:\n            self.assertAlmostEqual(div_0_0, div_1_0,places=5)\n            self.assertAlmostEqual(div_0_1, div_1_1,places=5)\n            \n    def test_sSE_2d_excitation_op(self):\n        input_shape = (2, 16, 16, 32)\n        x = tf.random_uniform(input_shape,seed=0)\n        se_layer = SpatialSELayer()\n        out_se = se_layer(x)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            x=sess.run(x)\n            x_0_0=float(x[0,0,0,0])\n            x_0_1=float(x[0,0,0,1])\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out_se)\n            out_0_0=float(out[0,0,0,0])\n            out_0_1=float(out[0,0,0,1])\n\n        div_0_0=out_0_0/x_0_0\n        div_0_1=out_0_1/x_0_1\n\n        with self.cached_session() as sess:\n            self.assertAlmostEqual(div_0_0, div_0_1,places=5)\n\n    def test_cSE_pooling_op_error(self):\n            with self.cached_session() as sess:\n                sess.run(tf.global_variables_initializer())\n\n                with self.assertRaises(ValueError):\n                    ChannelSELayer(func=\'ABC\')\n\n    def test_cSE_reduction_ratio_error(self):\n        input_shape = (2, 16, 16, 16, 33)\n        x = tf.ones(input_shape)\n        se_layer = ChannelSELayer()\n\n        with self.assertRaises(ValueError):\n            se_layer(x)\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/subpixel_test.py,4,"b'from __future__ import division, absolute_import, print_function\n\nimport functools as ft\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.subpixel import SubPixelLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass SubPixelTest(NiftyNetTestCase):\n    """"""\n    Test for niftynet.layer.subpixel.SubPixelLayer.\n    Mostly adapted from convolution_test.py\n    """"""\n\n    def get_3d_input(self, shape=(2, 16, 16, 16, 8)):\n        x_3d = tf.ones(shape)\n        return x_3d\n\n    def get_2d_input(self, shape=(2, 16, 16, 4)):\n        x_2d = tf.ones(shape)\n        return x_2d\n\n    def _make_output_shape(self, data, upsampling):\n        data_shape = data.shape.as_list()\n        output_shape = [data_shape[0]]\n        output_shape += [upsampling * d for d in data_shape[1:-1]]\n        output_shape += [data_shape[-1] // (upsampling ** (len(data_shape) - 2))]\n        return output_shape\n\n    def _test_subpixel_output_shape(self, input_data, param_dict, output_shape):\n        layer = SubPixelLayer(**param_dict)\n        output_data = layer(lr_image=input_data, is_training=True, keep_prob=1.0)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_value = sess.run(output_data)\n            self.assertAllClose(output_shape, output_value.shape)\n\n    def test_3d_default(self):\n        data = self.get_3d_input()\n\n        output_shape = self._make_output_shape(data, 2)\n\n        self._test_subpixel_output_shape(data, {}, output_shape)\n\n    def test_2d_default(self):\n        data = self.get_2d_input()\n\n        output_shape = self._make_output_shape(data, 2)\n\n        self._test_subpixel_output_shape(data, {}, output_shape)\n\n    def test_3d_bespoke(self):\n        upsampling = 4\n        data = self.get_3d_input(\n            shape=(2, 16, 16, 16, upsampling**3)\n        )\n\n        output_shape = self._make_output_shape(data, upsampling)\n\n        params = {""upsample_factor"": upsampling, ""kernel_size"": 4, ""padding"": ""SAME""}\n\n        self._test_subpixel_output_shape(data, params, output_shape)\n\n    def test_2d_bespoke(self):\n        upsampling = 6\n        data = self.get_2d_input(\n            shape=(2, 16, 16, upsampling ** 2)\n        )\n\n        output_shape = self._make_output_shape(data, upsampling)\n\n        params = {""upsample_factor"": upsampling, ""kernel_size"": 4, ""padding"": ""SAME""}\n\n        self._test_subpixel_output_shape(data, params, output_shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/test_model_zoo.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os, sys, unittest\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\nimport tensorflow as tf\n\nfrom niftynet.utilities.download import download\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\nfrom niftynet import main as niftynet_main\nfrom niftynet.application.base_application import SingletonApplication\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMODEL_HOME = NiftyNetGlobalConfig().get_niftynet_home_folder()\n\ndef net_run_with_sys_argv(argv):\n    # for gift-adelie\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n    SingletonApplication.clear()\n    cache = sys.argv\n    argv.extend([\'--cuda_devices\', \'0\'])\n    sys.argv = argv\n    niftynet_main()\n    sys.argv = cache\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass DenseVNetAbdominalCTModelZooTest(NiftyNetTestCase):\n    zoo_id = \'dense_vnet_abdominal_ct_model_zoo\'\n    location = \'dense_vnet_abdominal_ct\'\n    config = os.path.join(MODEL_HOME, \'extensions\', \'dense_vnet_abdominal_ct\', \'config.ini\')\n    application = \'net_segment\'\n    expected_output = os.path.join(\'segmentation_output\',\'window_seg_100___niftynet_out.nii.gz\')\n\n    def setUp(self):\n        NiftyNetTestCase.setUp(self)\n        download(self.zoo_id, download_if_already_existing=True, verbose=False)\n\n    def test_train_infer(self):\n        self._train()\n        self._infer()\n\n    def _train(self):\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'train\', \'--max_iter\', \'2\'])\n        checkpoint = os.path.join(MODEL_HOME, \'models\', self.location, \'models\', \'model.ckpt-2.index\')\n        self.assertTrue(os.path.exists(checkpoint), \'Expected {} to exist.\'.format(checkpoint))\n\n    def _infer(self):\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'inference\'])\n        output = os.path.join(MODEL_HOME, \'models\', self.location, self.expected_output)\n        self.assertTrue(os.path.exists(output), \'Expected {} to exist.\'.format(output))\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass UltrasoundSimulatorGanModelZooTest(NiftyNetTestCase):\n    zoo_id = \'ultrasound_simulator_gan_model_zoo\'\n    location = \'ultrasound_simulator_gan\'\n    config = os.path.join(MODEL_HOME, \'extensions\', \'ultrasound_simulator_gan\', \'config.ini\')\n    application = \'net_gan\'\n    expected_output = os.path.join(\'ultrasound_gan_simulated\',\'5_000053__window_image_niftynet_generated.nii.gz\')\n\n    def setUp(self):\n        NiftyNetTestCase.setUp(self)\n        download(self.zoo_id, download_if_already_existing=True, verbose=False)\n\n    def test_inference(self):\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'inference\'])\n        output = os.path.join(MODEL_HOME, \'models\', self.location, self.expected_output)\n        self.assertTrue(os.path.exists(output), \'Expected {} to exist.\'.format(output))\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass Highres3dnetBrainParcellationModelZooTest(NiftyNetTestCase):\n    zoo_id = \'highres3dnet_brain_parcellation_model_zoo\'\n    location = \'highres3dnet_brain_parcellation\'\n    config = os.path.join(MODEL_HOME, \'extensions\', \'highres3dnet_brain_parcellation\', \'highres3dnet_config_eval.ini\')\n    application = \'net_segment\'\n    expected_output = os.path.join(\'parcellation_output\',\'window_seg_OAS1_0145_MR2_mpr_n4_anon_sbj_111__niftynet_out.nii.gz\')\n\n    def setUp(self):\n        NiftyNetTestCase.setUp(self)\n        download(self.zoo_id, download_if_already_existing=True, verbose=False)\n\n    def test_inference(self):\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'inference\'])\n        output = os.path.join(MODEL_HOME, \'models\', self.location, self.expected_output)\n        self.assertTrue(os.path.exists(output), \'Expected {} to exist.\'.format(output))\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass AnisotropicNetsBratsChallengeModelZooTest(NiftyNetTestCase):\n    zoo_id = \'anisotropic_nets_brats_challenge_model_zoo\'\n    location = \'anisotropic_nets_brats_challenge\'\n    application = \'anisotropic_nets_brats_challenge.brats_seg_app.BRATSApp\'\n    expected_outputs = [os.path.join(\'model_whole_tumor_axial\',\'pred_whole_tumor_axial\',\'window_LGG71__niftynet_out.nii.gz\'),\n                        os.path.join(\'model_whole_tumor_coronal\',\'pred_whole_tumor_coronal\',\'window_LGG71__niftynet_out.nii.gz\'),\n                        os.path.join(\'model_whole_tumor_sagittal\',\'pred_whole_tumor_sagittal\',\'window_LGG71__niftynet_out.nii.gz\')]\n    configA = os.path.join(MODEL_HOME, \'extensions\', \'anisotropic_nets_brats_challenge\', \'whole_tumor_axial.ini\')\n    configC = os.path.join(MODEL_HOME, \'extensions\', \'anisotropic_nets_brats_challenge\', \'whole_tumor_coronal.ini\')\n    configS = os.path.join(MODEL_HOME, \'extensions\', \'anisotropic_nets_brats_challenge\', \'whole_tumor_sagittal.ini\')\n\n    def setUp(self):\n        NiftyNetTestCase.setUp(self)\n        download(self.zoo_id, download_if_already_existing=True, verbose=False)\n\n    def test_inference(self):\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.configA, \'inference\'])\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.configC, \'inference\'])\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.configS, \'inference\'])\n        for eo in self.expected_outputs:\n            output = os.path.join(MODEL_HOME, \'models\', self.location, eo)\n            self.assertTrue(os.path.exists(output), \'Expected {} to exist.\'.format(output))\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass MRCTRegressionModelZooTest(NiftyNetTestCase):\n    zoo_id = \'mr_ct_regression_model_zoo\'\n    location = \'mr_ct_regression\'\n    application = \'niftynet.contrib.regression_weighted_sampler.isample_regression.ISampleRegression\'\n    config = os.path.join(MODEL_HOME, \'extensions\', \'mr_ct_regression\',\'net_isampler.ini\')\n    expected_output_train = [\n        os.path.join(\'error_maps\',\'CRI.nii.gz\'),\n        ]\n\n    def setUp(self):\n        NiftyNetTestCase.setUp(self)\n        download(self.zoo_id, download_if_already_existing=True, verbose=False)\n\n    def test_train(self):\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'train\', \'--starting_iter\',\'0\',\'--max_iter\', \'2\'])\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'inference\', \'--inference_iter\',\'2\',\'--error_map\',\'True\'])\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'train\', \'--starting_iter\',\'2\',\'--max_iter\',\'4\'])\n\n        checkpoint = os.path.join(MODEL_HOME, \'models\', self.location, \'models\', \'model.ckpt-2.index\')\n        self.assertTrue(os.path.exists(checkpoint), \'Expected {} to exist.\'.format(checkpoint))\n        checkpoint = os.path.join(MODEL_HOME, \'models\', self.location, \'models\', \'model.ckpt-4.index\')\n        self.assertTrue(os.path.exists(checkpoint), \'Expected {} to exist.\'.format(checkpoint))\n        for eo in self.expected_output_train:\n            output = os.path.join(MODEL_HOME, \'models\', self.location, eo)\n            self.assertTrue(os.path.exists(output), \'Expected {} to exist.\'.format(output))\n\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"", \'Skipping slow tests\')\nclass AutoContextMRCTModelZooTest(NiftyNetTestCase):\n    zoo_id = \'autocontext_mr_ct_model_zoo\'\n    location = \'autocontext_mr_ct\'\n    application = \'net_regress\'\n    config = os.path.join(MODEL_HOME, \'extensions\', \'autocontext_mr_ct\',\'net_autocontext.ini\')\n    expected_output_train = [\n        os.path.join(\'error_maps\',\'WEB.nii.gz\'),\n        ]\n    expected_output_inference = [\n        os.path.join(\'autocontext_output\',\'window_reg_CHA_niftynet_out.nii.gz\'),\n        ]\n\n\n    def setUp(self):\n        NiftyNetTestCase.setUp(self)\n        download(self.zoo_id, download_if_already_existing=True, verbose=False)\n\n    def test_train_infer(self):\n        self._train()\n        self._infer()\n\n    def _train(self):\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'train\', \'--starting_iter\',\'0\',\'--max_iter\', \'2\'])\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'inference\', \'--inference_iter\',\'2\'])\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'train\', \'--starting_iter\',\'2\',\'--max_iter\',\'4\'])\n\n        checkpoint = os.path.join(MODEL_HOME, \'models\', self.location, \'models\', \'model.ckpt-2.index\')\n        self.assertTrue(os.path.exists(checkpoint))\n        checkpoint = os.path.join(MODEL_HOME, \'models\', self.location, \'models\', \'model.ckpt-4.index\')\n        self.assertTrue(os.path.exists(checkpoint))\n        for eo in self.expected_output_train:\n            output = os.path.join(MODEL_HOME, \'models\', self.location, eo)\n            self.assertTrue(os.path.exists(output), \'Expected {} to exist.\'.format(output))\n\n    def _infer(self):\n        net_run_with_sys_argv([\'net_run\', \'-a\', self.application, \'-c\', self.config, \'inference\', \'--inference_iter\',\'-1\'])\n        for eo in self.expected_output_inference:\n            output = os.path.join(MODEL_HOME, \'models\', self.location, eo)\n            self.assertTrue(os.path.exists(output), \'Expected {} to exist.\'.format(output))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/toy_application.py,24,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import ApplicationNetFactory\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.application_variables import TF_SUMMARIES\nfrom niftynet.engine.sampler_random_vector_v2 import RandomVectorSampler\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.fully_connected import FullyConnectedLayer\nfrom niftynet.network.base_net import BaseNet\n\nclass ToyApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""TOY""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting toy application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n        self.toy_param = None\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        self.toy_param = task_param\n        self.readers = []\n\n    def initialise_sampler(self):\n        self.sampler = [\n            [RandomVectorSampler(\n                names=(\'vectors\',),\n                vector_size=(self.toy_param.vector_size,),\n                batch_size=self.net_param.batch_size,\n                repeat=None,\n                mean=self.toy_param.mean,\n                stddev=self.toy_param.stddev)]]\n\n    def initialise_network(self):\n        self.net = ApplicationNetFactory.create(self.net_param.name)()\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        print(vars(self.action_param))\n        self.patience = self.action_param.patience\n        with tf.name_scope(\'Optimiser\'):\n            optimiser_class = OptimiserFactory.create(\n                name=self.action_param.optimiser)\n            self.optimiser = optimiser_class.get_instance(\n                learning_rate=self.action_param.lr)\n\n        fake_features, fake_logits, real_logits = self.feed_forward()\n\n        d_loss, g_loss = self.compute_loss(fake_logits, real_logits)\n\n        with tf.name_scope(\'ComputeGradients\'):\n            d_vars = tf.get_collection(\n                tf.GraphKeys.TRAINABLE_VARIABLES,\n                scope=self.net.d_net.layer_scope().name)\n            g_vars = tf.get_collection(\n                tf.GraphKeys.TRAINABLE_VARIABLES,\n                scope=self.net.g_net.layer_scope().name)\n            grads_d = self.optimiser.compute_gradients(\n                d_loss, var_list=d_vars)\n            grads_g = self.optimiser.compute_gradients(\n                g_loss, var_list=g_vars)\n            grads = [grads_d, grads_g]\n            gradients_collector.add_to_collection(grads)\n\n        self.collect_results(d_loss, fake_features, g_loss, outputs_collector)\n\n    def collect_results(self, d_loss, fake_features, g_loss, outputs_collector):\n        outputs_collector.add_to_collection(\n            var=d_loss, name=\'d_loss\', average_over_devices=True,\n            collection=TF_SUMMARIES)\n        outputs_collector.add_to_collection(\n            var=g_loss, name=\'g_loss\', average_over_devices=True,\n            collection=TF_SUMMARIES)\n        g_mean, g_var = tf.nn.moments(fake_features, axes=[0, 1, 2])\n        g_var = tf.sqrt(g_var)\n        outputs_collector.add_to_collection(\n            var=g_mean, name=\'mean\', average_over_devices=True,\n            collection=CONSOLE)\n        outputs_collector.add_to_collection(\n            var=g_var, name=\'var\', average_over_devices=True,\n            collection=CONSOLE)\n        outputs_collector.add_to_collection(\n            var=g_loss, name=\'total_loss\', average_over_devices=True,\n            collection=CONSOLE)\n        outputs_collector.add_to_collection(\n            var=g_mean, name=\'generated_mean\', average_over_devices=False,\n            collection=TF_SUMMARIES)\n        outputs_collector.add_to_collection(\n            var=g_var, name=\'generated_variance\', average_over_devices=False,\n            collection=TF_SUMMARIES)\n\n    def compute_loss(self, fake_logits, real_logits):\n        d_loss = tf.reduce_mean(real_logits - fake_logits)\n        g_loss = tf.reduce_mean(fake_logits)\n        return d_loss, g_loss\n\n    def feed_forward(self):\n        # a new pop_batch_op for each gpu tower\n        data_x = self.get_sampler()[0][0].pop_batch_op()\n        features = tf.cast(data_x[\'vectors\'], tf.float32, name=\'sampler_input\')\n        features = tf.expand_dims(features, axis=-1, name=\'feature_input\')\n        noise = tf.random_uniform(tf.shape(features), 0.0, 1.0)\n        real_logits, fake_logits, fake_features = self.net(features, noise)\n        return fake_features, fake_logits, real_logits\n\n    def interpret_output(self, batch_output):\n        return True\n\n\nclass ToyApplicationMultOpti(ToyApplication):\n\n    def __init__(self, net_param, action_param, action):\n        ToyApplication.__init__(self, net_param, action_param, action)\n        tf.logging.info(\'starting toy application using multiple optimiser\')\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        self.patience = self.action_param.patience\n        print(vars(self.action_param))\n        self.optimiser = dict()\n        with tf.name_scope(\'OptimiserGen\'):\n            optimiser_class = OptimiserFactory.create(\n                name=self.action_param.optimiser)\n            self.optimiser[\'gen\'] = optimiser_class.get_instance(\n                learning_rate=self.action_param.lr)\n\n        # 2nd optimiser could be initialized different\n        with tf.name_scope(\'OptimiserDis\'):\n            optimiser_class = OptimiserFactory.create(\n                name=self.action_param.optimiser)\n            self.optimiser[\'dis\'] = optimiser_class.get_instance(\n                learning_rate=self.action_param.lr)\n\n        fake_features, fake_logits, real_logits = self.feed_forward()\n\n        d_loss, g_loss = self.compute_loss(fake_logits, real_logits)\n\n        grads = dict()\n        with tf.name_scope(\'ComputeGradientsD\'):\n            d_vars = tf.get_collection(\n                tf.GraphKeys.TRAINABLE_VARIABLES,\n                scope=self.net.d_net.layer_scope().name)\n            grads[\'dis\'] = self.optimiser[\'dis\'].compute_gradients(\n                d_loss, var_list=d_vars)\n        with tf.name_scope(\'ComputeGradientsG\'):\n            g_vars = tf.get_collection(\n                tf.GraphKeys.TRAINABLE_VARIABLES,\n                scope=self.net.g_net.layer_scope().name)\n            grads[\'gen\'] = self.optimiser[\'gen\'].compute_gradients(\n                g_loss, var_list=g_vars)\n\n        gradients_collector.add_to_collection(grads)\n\n        self.collect_results(d_loss, fake_features, g_loss, outputs_collector)\n\n\nclass TinyNet(BaseNet):\n    def __init__(self):\n        BaseNet.__init__(self, name=\'tinynet\')\n        self.d_net = DNet()\n        self.g_net = GNet()\n\n    def layer_op(self, features, noise):\n        fake_features = self.g_net(noise)\n\n        real_logits = self.d_net(features)\n        fake_logits = self.d_net(fake_features)\n        return real_logits, fake_logits, fake_features\n\n\nclass DNet(BaseNet):\n    def __init__(self):\n        BaseNet.__init__(self, name=\'D\')\n\n    def layer_op(self, features):\n        batch_size = features.shape.as_list()[0]\n        conv_1 = ConvolutionalLayer(\n            20, 3, feature_normalization=None, with_bias=True, acti_func=\'relu\')\n        fc_1 = FullyConnectedLayer(\n            20, feature_normalization=None, with_bias=True, acti_func=\'relu\')\n        fc_2 = FullyConnectedLayer(\n            2, feature_normalization=None, with_bias=True)\n\n        hidden_feature = conv_1(features, is_training=True)\n        hidden_feature = tf.reshape(hidden_feature, [batch_size, -1])\n        hidden_feature = fc_1(hidden_feature, is_training=True)\n        logits = fc_2(hidden_feature, is_training=True)\n        return logits\n\n\nclass GNet(BaseNet):\n    def __init__(self):\n        BaseNet.__init__(self, name=\'G\')\n\n    def layer_op(self, noise):\n        n_chns = noise.shape[-1]\n        conv_1 = ConvolutionalLayer(\n            20, 10, feature_normalization=\'batch\', acti_func=\'selu\', with_bias=True)\n        conv_2 = ConvolutionalLayer(\n            20, 10, feature_normalization=\'batch\', acti_func=\'selu\', with_bias=True)\n        conv_3 = ConvolutionalLayer(\n            n_chns, 10, feature_normalization=None, with_bias=True)\n        hidden_feature = conv_1(noise, is_training=True)\n        hidden_feature = conv_2(hidden_feature, is_training=True)\n        fake_features = conv_3(hidden_feature, is_training=True)\n        return fake_features\n'"
tests/toynet_test.py,5,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.network.toynet import ToyNet\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass ToyNetTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 32, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        toynet_instance = ToyNet(num_classes=160)\n        out = toynet_instance(x, is_training=True)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 160), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        toynet_instance = ToyNet(num_classes=160)\n        out = toynet_instance(x, is_training=True)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 160), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/unet_2d_test.py,5,"b'from __future__ import absolute_import, print_function\n\nimport os\nimport unittest\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.unet_2d import UNet2D\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n@unittest.skipIf(os.environ.get(\'QUICKTEST\', """").lower() == ""true"",\n                 \'Skipping slow tests\')\nclass UNet3DTest(NiftyNetTestCase):\n    def test_2d_shape(self):\n        #input_shape = (2, 572, 572, 3)\n        input_shape = (2, 180, 180, 3)\n        x = tf.ones(input_shape)\n\n        unet_instance = UNet2D(num_classes=2)\n        out = unet_instance(x, is_training=True)\n        print(unet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            #self.assertAllClose((2, 388, 388, 2), out.shape)\n            self.assertAllClose((2, 4, 4, 2), out.shape)\n\n    def test_2d_reg_shape(self):\n        #input_shape = (2, 572, 572, 5)\n        input_shape = (2, 180, 180, 5)\n        x = tf.ones(input_shape)\n\n        unet_instance = UNet2D(num_classes=2,\n                               w_regularizer=regularizers.l2_regularizer(0.4))\n        out = unet_instance(x, is_training=True)\n        print(unet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            #self.assertAllClose((2, 388, 388, 2), out.shape)\n            self.assertAllClose((2, 4, 4, 2), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/unet_test.py,9,"b'from __future__ import absolute_import, print_function\n\nimport unittest\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.unet import UNet3D\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n@unittest.skip(\'Test currently disabled\')\nclass UNet3DTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 96, 96, 96, 1)\n        x = tf.ones(input_shape)\n\n        unet_instance = UNet3D(num_classes=160)\n        out = unet_instance(x, is_training=True)\n        print(unet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 8, 8, 8, 160), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 96, 96, 1)\n        x = tf.ones(input_shape)\n\n        unet_instance = UNet3D(num_classes=160)\n        out = unet_instance(x, is_training=True)\n        print(unet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 8, 8, 160), out.shape)\n\n    def test_3d_reg_shape(self):\n        input_shape = (2, 96, 96, 96, 1)\n        x = tf.ones(input_shape)\n\n        unet_instance = UNet3D(num_classes=160,\n                               w_regularizer=regularizers.l2_regularizer(0.4))\n        out = unet_instance(x, is_training=True)\n        print(unet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 8, 8, 8, 160), out.shape)\n\n    def test_2d_reg_shape(self):\n        input_shape = (2, 96, 96, 1)\n        x = tf.ones(input_shape)\n\n        unet_instance = UNet3D(num_classes=160,\n                               w_regularizer=regularizers.l2_regularizer(0.4))\n        out = unet_instance(x, is_training=True)\n        print(unet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 8, 8, 160), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/unetblock_test.py,7,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.unet import UNetBlock\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass UNetBlockTest(NiftyNetTestCase):\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def test_2d_shape(self):\n        x = self.get_2d_input()\n\n        unet_block_op = UNetBlock(\n            \'DOWNSAMPLE\', (32, 64), (3, 3), with_downsample_branch=True)\n        out_1, out_2 = unet_block_op(x, is_training=True)\n        print(unet_block_op)\n        print(out_1)\n        print(out_2)\n\n        unet_block_op = UNetBlock(\n            \'UPSAMPLE\', (32, 64), (3, 3), with_downsample_branch=False)\n        out_3, _ = unet_block_op(x, is_training=True)\n        print(unet_block_op)\n        print(out_3)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            self.assertAllClose((2, 8, 8, 64), out_1.shape)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 16, 16, 64), out_2.shape)\n            out_3 = sess.run(out_3)\n            self.assertAllClose((2, 32, 32, 64), out_3.shape)\n\n    def test_3d_shape(self):\n        x = self.get_3d_input()\n\n        unet_block_op = UNetBlock(\n            \'DOWNSAMPLE\', (32, 64), (3, 3), with_downsample_branch=True)\n        out_1, out_2 = unet_block_op(x, is_training=True)\n        print(unet_block_op)\n        print(out_1)\n        print(out_2)\n\n        unet_block_op = UNetBlock(\n            \'UPSAMPLE\', (32, 64), (3, 3), with_downsample_branch=False)\n        out_3, _ = unet_block_op(x, is_training=True)\n        print(unet_block_op)\n        print(out_3)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            self.assertAllClose((2, 8, 8, 8, 64), out_1.shape)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 16, 16, 16, 64), out_2.shape)\n            out_3 = sess.run(out_3)\n            self.assertAllClose((2, 32, 32, 32, 64), out_3.shape)\n\n    def test_2d_reg_shape(self):\n        x = self.get_2d_input()\n\n        unet_block_op = UNetBlock(\n            \'DOWNSAMPLE\', (32, 64), (3, 3), with_downsample_branch=True,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n        out_1, out_2 = unet_block_op(x, is_training=True)\n        print(unet_block_op)\n        print(out_1)\n        print(out_2)\n\n        unet_block_op = UNetBlock(\n            \'UPSAMPLE\', (32, 64), (3, 3), with_downsample_branch=False,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n        out_3, _ = unet_block_op(x, is_training=True)\n        print(unet_block_op)\n        print(out_3)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            self.assertAllClose((2, 8, 8, 64), out_1.shape)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 16, 16, 64), out_2.shape)\n            out_3 = sess.run(out_3)\n            self.assertAllClose((2, 32, 32, 64), out_3.shape)\n\n    def test_3d_reg_shape(self):\n        x = self.get_3d_input()\n\n        unet_block_op = UNetBlock(\n            \'DOWNSAMPLE\', (32, 64), (3, 3), with_downsample_branch=True,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n        out_1, out_2 = unet_block_op(x, is_training=True)\n        print(unet_block_op)\n        print(out_1)\n        print(out_2)\n\n        unet_block_op = UNetBlock(\n            \'UPSAMPLE\', (32, 64), (3, 3), with_downsample_branch=False,\n            w_regularizer=regularizers.l2_regularizer(0.3))\n        out_3, _ = unet_block_op(x, is_training=True)\n        print(unet_block_op)\n        print(out_3)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            self.assertAllClose((2, 8, 8, 8, 64), out_1.shape)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 16, 16, 16, 64), out_2.shape)\n            out_3 = sess.run(out_3)\n            self.assertAllClose((2, 32, 32, 32, 64), out_3.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/upsample_res_block_test.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.upsample_res_block import UpBlock\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass UpsampleResBlockTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def _test_nd_output_shape(self,\n                              rank,\n                              param_dict,\n                              output_shape):\n        if rank == 2:\n            input_data = self.get_2d_input()\n        else:\n            input_data = self.get_3d_input()\n\n        upsample_layer = UpBlock(**param_dict)\n        output_data = upsample_layer(input_data)\n        print(upsample_layer)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(output_data)\n            self.assertAllClose(output_shape, out.shape)\n\n    def test_3d_shape(self):\n        expected_shape = (2, 32, 32, 32, 4)\n        self._test_nd_output_shape(3, {}, expected_shape)\n\n        params = {\'n_output_chns\': 2,\n                  \'kernel_size\': 3, \'upsample_stride\': 2,\n                  \'acti_func\': \'relu\'}\n        expected_shape = (2, 32, 32, 32, 2)\n        self._test_nd_output_shape(3, params, expected_shape)\n\n        params = {\'n_output_chns\': 1,\n                  \'kernel_size\': 4, \'upsample_stride\': 3,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 48, 48, 1)\n        self._test_nd_output_shape(3, params, expected_shape)\n\n        params = {\'n_output_chns\': 1,\n                  \'kernel_size\': 4, \'upsample_stride\': (3, 2, 3),\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 32, 48, 1)\n        self._test_nd_output_shape(3, params, expected_shape)\n\n        params = {\'n_output_chns\': 1,\n                  \'kernel_size\': 4, \'upsample_stride\': (3, 2, 3),\n                  \'acti_func\': \'prelu\', \'is_residual_upsampling\': False}\n        expected_shape = (2, 48, 32, 48, 1)\n        self._test_nd_output_shape(3, params, expected_shape)\n\n    def test_2d_shape(self):\n        expected_shape = (2, 32, 32, 4)\n        self._test_nd_output_shape(2, {}, expected_shape)\n\n        params = {\'n_output_chns\': 2,\n                  \'kernel_size\': 3, \'upsample_stride\': 2,\n                  \'acti_func\': \'relu\'}\n        expected_shape = (2, 32, 32, 2)\n        self._test_nd_output_shape(2, params, expected_shape)\n\n        params = {\'n_output_chns\': 1,\n                  \'kernel_size\': 3, \'upsample_stride\': 3,\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 48, 1)\n        self._test_nd_output_shape(2, params, expected_shape)\n\n        params = {\'n_output_chns\': 1,\n                  \'kernel_size\': 3, \'upsample_stride\': (3, 2),\n                  \'acti_func\': \'prelu\'}\n        expected_shape = (2, 48, 32, 1)\n        self._test_nd_output_shape(2, params, expected_shape)\n\n        params = {\'n_output_chns\': 1,\n                  \'kernel_size\': 3, \'upsample_stride\': (3, 2),\n                  \'acti_func\': \'prelu\', \'is_residual_upsampling\': False}\n        expected_shape = (2, 48, 32, 1)\n        self._test_nd_output_shape(2, params, expected_shape)\n\n    def test_ill_params(self):\n        params = {\'n_output_chns\': 16,\n                  \'kernel_size\': 3, \'upsample_stride\': 2,\n                  \'acti_func\': \'relu\'}\n        expected_shape = (2, 32, 32, 32, 16)\n        with self.assertRaisesRegexp(AssertionError, \'\'):\n            self._test_nd_output_shape(3, params, expected_shape)\n\n        params = {\'n_output_chns\': 2,\n                  \'kernel_size\': 3, \'upsample_stride\': 2,\n                  \'acti_func\': \'relu\', \'type_string\': \'foo\'}\n        expected_shape = (2, 32, 32, 32, 16)\n        with self.assertRaisesRegexp(ValueError, \'\'):\n            self._test_nd_output_shape(3, params, expected_shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/upsample_test.py,4,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.upsample import UpSampleLayer\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass UpSampleTest(NiftyNetTestCase):\n    def get_3d_input(self):\n        input_shape = (4, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_2d_input(self):\n        input_shape = (4, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def _test_upsample_shape(self, rank, param_dict, output_shape):\n        if rank == 3:\n            input_data = self.get_3d_input()\n        elif rank == 2:\n            input_data = self.get_2d_input()\n\n        upsample_layer = UpSampleLayer(**param_dict)\n        output_data = upsample_layer(input_data)\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output = sess.run(output_data)\n            self.assertAllClose(output_shape, output.shape)\n\n    def test_3d_default_replicate(self):\n        input_param = {\'func\': \'REPLICATE\',\n                       \'kernel_size\': 3,\n                       \'stride\': 3}\n        self._test_upsample_shape(rank=3,\n                                  param_dict=input_param,\n                                  output_shape=(4, 48, 48, 48, 8))\n\n    def test_3d_default_channelwise_deconv(self):\n        input_param = {\'func\': \'CHANNELWISE_DECONV\',\n                       \'kernel_size\': 3,\n                       \'stride\': 3}\n        self._test_upsample_shape(rank=3,\n                                  param_dict=input_param,\n                                  output_shape=(4, 48, 48, 48, 8))\n\n    def test_3d_replicate(self):\n        input_param = {\'func\': \'REPLICATE\',\n                       \'kernel_size\': [3, 1, 3],\n                       \'stride\': [3, 1, 3]}\n        self._test_upsample_shape(rank=3,\n                                  param_dict=input_param,\n                                  output_shape=(4, 48, 16, 48, 8))\n\n    def test_3d_channelwise_deconv(self):\n        input_param = {\'func\': \'CHANNELWISE_DECONV\',\n                       \'kernel_size\': [1, 3, 2],\n                       \'stride\': [1, 2, 3]}\n        self._test_upsample_shape(rank=3,\n                                  param_dict=input_param,\n                                  output_shape=(4, 16, 32, 48, 8))\n\n    def test_2d_default_replicate(self):\n        input_param = {\'func\': \'REPLICATE\',\n                       \'kernel_size\': 3,\n                       \'stride\': 3}\n        self._test_upsample_shape(rank=2,\n                                  param_dict=input_param,\n                                  output_shape=(4, 48, 48, 8))\n\n    def test_2d_default_channelwise_deconv(self):\n        input_param = {\'func\': \'CHANNELWISE_DECONV\',\n                       \'kernel_size\': 3,\n                       \'stride\': 3}\n        self._test_upsample_shape(rank=2,\n                                  param_dict=input_param,\n                                  output_shape=(4, 48, 48, 8))\n\n    def test_2d_replicate(self):\n        input_param = {\'func\': \'REPLICATE\',\n                       \'kernel_size\': [3, 1],\n                       \'stride\': [3, 1]}\n        self._test_upsample_shape(rank=2,\n                                  param_dict=input_param,\n                                  output_shape=(4, 48, 16, 8))\n\n    def test_2d_channelwise_deconv(self):\n        input_param = {\'func\': \'CHANNELWISE_DECONV\',\n                       \'kernel_size\': [1, 3],\n                       \'stride\': [1, 2]}\n        self._test_upsample_shape(rank=2,\n                                  param_dict=input_param,\n                                  output_shape=(4, 16, 32, 8))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/user_parameters_default_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport argparse\n\nimport tensorflow as tf\n\nfrom niftynet.utilities.user_parameters_default import *\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass TestUserParameters(NiftyNetTestCase):\n    def test_list_all(self):\n        test_parser = argparse.ArgumentParser(conflict_handler=\'resolve\')\n        test_parser = add_application_args(test_parser)\n        test_parser = add_network_args(test_parser)\n        test_parser = add_training_args(test_parser)\n        test_parser = add_input_data_args(test_parser)\n        test_parser = add_inference_args(test_parser)\n\n        for opt in test_parser._actions:\n            print(opt_to_string(opt))\n\n\ndef opt_to_string(opt):\n    summary = \'opt: [{}]\\n\'.format(opt.dest)\n    summary += \'---- type: {}\\n\'.format(opt.type)\n    summary += \'---- default: {}\\n\'.format(opt.default)\n    summary += \'---- description: {}\\n\'.format(opt.help)\n    return summary\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/user_parameters_regex_test.py,1,"b'from __future__ import unicode_literals\n\nimport re\n\nimport tensorflow as tf\n\nfrom niftynet.utilities.user_parameters_regex import STATEMENT\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass UserParameterRegexTest(NiftyNetTestCase):\n    def run_match(self, string_to_match, expected_output):\n        regex = re.compile(STATEMENT)\n        matched_str = regex.match(string_to_match)\n        if matched_str:\n            filtered_groups = list(filter(None, matched_str.groups()))\n            if filtered_groups:\n                values = [v.strip() for v in filtered_groups[0].split(\',\')]\n                self.assertEqual(values, expected_output)\n        else:\n            self.assertEqual(expected_output, False)\n\n    def test_cases(self):\n        self.run_match(\'c:\\program files\', [u\'c:\\\\program files\'])\n        self.run_match(\'2.0, ( 6.0, 9.0\', False)\n        self.run_match(\'{   32.0, 32.0}\', [u\'32.0\', u\'32.0\'])\n        self.run_match(\'a, c, b, f, d, e\', [u\'a\', u\'c\', u\'b\', u\'f\', u\'d\', u\'e\'])\n        self.run_match(\'(), ()\', False)\n        self.run_match(\'{), (}\', False)\n        self.run_match(\'(),\', False)\n        self.run_match(\'()\', False)\n        self.run_match(\'{}\', False)\n        self.run_match(\'32, (32),\', False)\n        self.run_match(\'32, (),\', False)\n        self.run_match(\'32\', [u\'32\'])\n        self.run_match(\'({)\', False)\n        self.run_match(\'(()\', False)\n        self.run_match(\'\', False)\n        self.run_match(\'32, 32\', [u\'32\', u\'32\'])\n        self.run_match(\'32,\', False)\n        self.run_match(\'-32\', [u\'-32\'])\n        self.run_match(\'-32.0, a\', [u\'-32.0\', \'a\'])\n        self.run_match(\'-a, 32.0\', [u\'-a\', \'32.0\'])\n        self.run_match(\'(-32,a)\', [u\'-32\', u\'a\'])\n        self.run_match(\'-32.0\', [u\'-32.0\'])\n        self.run_match(\'(-a)\', [u\'-a\'])\n        self.run_match(\'(-32.0, 10.0, 2.99987, 5.6, 3.5, 5.6)\',\n                       [u\'-32.0\', u\'10.0\', u\'2.99987\', u\'5.6\', u\'3.5\', u\'5.6\'])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/util_import_test.py,1,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nfrom niftynet.utilities.util_import import require_module\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass OptionalPackageTest(NiftyNetTestCase):\n    def test_installed(self):\n        require_module(\'tensorflow\')\n\n    def test_installed_min_version(self):\n        require_module(\'tensorflow\', 1.0)\n\n    def test_no_package(self):\n        with self.assertRaisesRegexp(ImportError, \'\'):\n            require_module(\'foobar_wrong_case\', mandatory=True)\n\n    def test_wrong_version(self):\n        with self.assertRaisesRegexp(AssertionError, \'\'):\n            require_module(\'tensorflow\', 100, mandatory=True)\n\n    def test_self_version(self):\n        require_module(\'importlib\')\n\n    def test_no_version_info(self):\n        require_module(\'importlib\', 0)\n\n    def test_no_input(self):\n        with self.assertRaisesRegexp(ImportError, \'\'):\n            require_module([], mandatory=True)\n        with self.assertRaisesRegexp(ImportError, \'\'):\n            require_module(None, mandatory=True)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/versioning_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.utilities.versioning import check_pep_440\nfrom niftynet.utilities.versioning import get_niftynet_version_string\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\nclass VersioningTest(NiftyNetTestCase):\n    def test_version(self):\n        version_str = get_niftynet_version_string()\n        expected_string = ""NiftyNet version ""\n        self.assertEqual(version_str[:len(expected_string)], expected_string)\n\n        check_pep_440()\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/vnet_test.py,11,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.vnet import VNet\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass VNetTest(NiftyNetTestCase):\n    def test_3d_shape(self):\n        input_shape = (2, 32, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        # vnet_instance = VNet(num_classes=160)\n        vnet_instance = VNet(num_classes=160)\n        out = vnet_instance(x, is_training=True)\n        print(vnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 160), out.shape)\n\n    def test_2d_shape(self):\n        input_shape = (2, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        # vnet_instance = VNet(num_classes=160)\n        vnet_instance = VNet(num_classes=160)\n        out = vnet_instance(x, is_training=True)\n        print(vnet_instance.num_trainable_params())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 160), out.shape)\n\n    def test_3d_reg_shape(self):\n        input_shape = (2, 32, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        # vnet_instance = VNet(num_classes=160)\n        vnet_instance = VNet(\n            num_classes=160,\n            w_regularizer=regularizers.l2_regularizer(0.4),\n            b_regularizer=regularizers.l2_regularizer(0.4))\n        out = vnet_instance(x, is_training=True)\n        print(vnet_instance.num_trainable_params())\n        # print(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 32, 160), out.shape)\n\n    def test_2d_reg_shape(self):\n        input_shape = (2, 32, 32, 1)\n        x = tf.ones(input_shape)\n\n        # vnet_instance = VNet(num_classes=160)\n        vnet_instance = VNet(\n            num_classes=160,\n            w_regularizer=regularizers.l2_regularizer(0.4),\n            b_regularizer=regularizers.l2_regularizer(0.4))\n        out = vnet_instance(x, is_training=True)\n        print(vnet_instance.num_trainable_params())\n        # print(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n        # print(vnet_instance.regularizer_loss())\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out = sess.run(out)\n            self.assertAllClose((2, 32, 32, 160), out.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/vnetblock_test.py,7,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.network.vnet import VNetBlock\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nclass VNetBlockTest(NiftyNetTestCase):\n    def get_2d_data(self):\n        input_shape = (2, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def get_3d_data(self):\n        input_shape = (2, 16, 16, 16, 8)\n        x = tf.ones(input_shape)\n        return x\n\n    def test_3d_shape(self):\n        x = self.get_3d_data()\n        vnet_block_op = VNetBlock(\'DOWNSAMPLE\', 2, 16, 8)\n        out_1, out_2 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        vnet_block_op = VNetBlock(\'UPSAMPLE\', 2, 16, 8)\n        out_3, out_4 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        vnet_block_op = VNetBlock(\'SAME\', 2, 16, 8)\n        out_5, out_6 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            self.assertAllClose((2, 16, 16, 16, 16), out_1.shape)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 8, 8, 8, 8), out_2.shape)\n            out_3 = sess.run(out_3)\n            self.assertAllClose((2, 16, 16, 16, 16), out_3.shape)\n            out_4 = sess.run(out_4)\n            self.assertAllClose((2, 32, 32, 32, 8), out_4.shape)\n            out_5 = sess.run(out_5)\n            self.assertAllClose((2, 16, 16, 16, 16), out_5.shape)\n            out_6 = sess.run(out_6)\n            self.assertAllClose((2, 16, 16, 16, 8), out_6.shape)\n\n    def test_2d_shape(self):\n        x = self.get_2d_data()\n        vnet_block_op = VNetBlock(\'DOWNSAMPLE\', 2, 16, 8)\n        out_1, out_2 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        vnet_block_op = VNetBlock(\'UPSAMPLE\', 2, 16, 8)\n        out_3, out_4 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        vnet_block_op = VNetBlock(\'SAME\', 2, 16, 8)\n        out_5, out_6 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            self.assertAllClose((2, 16, 16, 16), out_1.shape)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 8, 8, 8), out_2.shape)\n            out_3 = sess.run(out_3)\n            self.assertAllClose((2, 16, 16, 16), out_3.shape)\n            out_4 = sess.run(out_4)\n            self.assertAllClose((2, 32, 32, 8), out_4.shape)\n            out_5 = sess.run(out_5)\n            self.assertAllClose((2, 16, 16, 16), out_5.shape)\n            out_6 = sess.run(out_6)\n            self.assertAllClose((2, 16, 16, 8), out_6.shape)\n\n    def test_3d_reg_shape(self):\n        x = self.get_3d_data()\n        vnet_block_op = VNetBlock(\'DOWNSAMPLE\', 2, 16, 8,\n                                  w_regularizer=regularizers.l2_regularizer(\n                                      0.2))\n        out_1, out_2 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        vnet_block_op = VNetBlock(\'UPSAMPLE\', 2, 16, 8,\n                                  w_regularizer=regularizers.l2_regularizer(\n                                      0.2))\n        out_3, out_4 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        vnet_block_op = VNetBlock(\'SAME\', 2, 16, 8,\n                                  w_regularizer=regularizers.l2_regularizer(\n                                      0.2))\n        out_5, out_6 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            self.assertAllClose((2, 16, 16, 16, 16), out_1.shape)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 8, 8, 8, 8), out_2.shape)\n            out_3 = sess.run(out_3)\n            self.assertAllClose((2, 16, 16, 16, 16), out_3.shape)\n            out_4 = sess.run(out_4)\n            self.assertAllClose((2, 32, 32, 32, 8), out_4.shape)\n            out_5 = sess.run(out_5)\n            self.assertAllClose((2, 16, 16, 16, 16), out_5.shape)\n            out_6 = sess.run(out_6)\n            self.assertAllClose((2, 16, 16, 16, 8), out_6.shape)\n\n    def test_2d_reg_shape(self):\n        x = self.get_2d_data()\n        vnet_block_op = VNetBlock(\'DOWNSAMPLE\', 2, 16, 8,\n                                  w_regularizer=regularizers.l2_regularizer(\n                                      0.2))\n        out_1, out_2 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        vnet_block_op = VNetBlock(\'UPSAMPLE\', 2, 16, 8,\n                                  w_regularizer=regularizers.l2_regularizer(\n                                      0.2))\n        out_3, out_4 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        vnet_block_op = VNetBlock(\'SAME\', 2, 16, 8,\n                                  w_regularizer=regularizers.l2_regularizer(\n                                      0.2))\n        out_5, out_6 = vnet_block_op(x, x)\n        print(vnet_block_op)\n\n        with self.cached_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_1 = sess.run(out_1)\n            self.assertAllClose((2, 16, 16, 16), out_1.shape)\n            out_2 = sess.run(out_2)\n            self.assertAllClose((2, 8, 8, 8), out_2.shape)\n            out_3 = sess.run(out_3)\n            self.assertAllClose((2, 16, 16, 16), out_3.shape)\n            out_4 = sess.run(out_4)\n            self.assertAllClose((2, 32, 32, 8), out_4.shape)\n            out_5 = sess.run(out_5)\n            self.assertAllClose((2, 16, 16, 16), out_5.shape)\n            out_6 = sess.run(out_6)\n            self.assertAllClose((2, 16, 16, 8), out_6.shape)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/windows_aggregator_grid_v2_test.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nibabel as nib\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\', \'23\'),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=(2.4, 5.0, 2.0),\n        axcodes=\'LSA\',\n        spatial_window_size=(23, 32, 15),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\', \'23\'),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=(2.4, 5.0, 2.0),\n        axcodes=\'LSA\',\n        spatial_window_size=(23, 32, 15),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\nMOD_2D_DATA = {\n    \'ultrasound\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(72, 83, 1),\n        loader=None\n    ),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\',))\n\nMOD_LABEL_DATA = {\n    \'parcellation\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'Parcelsampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'23_NeuroMorph_Parcellation\',),\n        filename_not_contains=(\'FLAIR\',),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(150, 140, 100),\n        loader=None\n    ),\n}\nMOD_LABEl_TASK = ParserNamespace(label=(\'parcellation\',))\n\nSINGLE_25D_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\', \'106\'),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=(3.0, 5.0, 5.0),\n        axcodes=\'ASL\',\n        spatial_window_size=(40, 30, 1),\n        loader=None\n    ),\n}\nSINGLE_25D_TASK = ParserNamespace(image=(\'T1\',))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\nmod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\nmod_label_list = data_partitioner.initialise(MOD_LABEL_DATA).get_file_list()\nsingle_25d_list = data_partitioner.initialise(SINGLE_25D_DATA).get_file_list()\n\n\ndef get_3d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_label_reader():\n    reader = ImageReader([\'label\'])\n    reader.initialise(MOD_LABEL_DATA, MOD_LABEl_TASK, mod_label_list)\n    label_normaliser = DiscreteLabelNormalisationLayer(\n        image_name=\'label\',\n        modalities=vars(SINGLE_25D_TASK).get(\'label\'),\n        model_filename=os.path.join(\'testing_data\', \'agg_test.txt\'))\n    reader.add_preprocessing_layers(label_normaliser)\n    pad_layer = PadLayer(image_name=(\'label\',), border=(5, 6, 7))\n    reader.add_preprocessing_layers([pad_layer])\n    return reader\n\n\ndef get_nonnormalising_label_reader():\n    reader = ImageReader([\'label\'])\n    reader.initialise(MOD_LABEL_DATA, MOD_LABEl_TASK, mod_label_list)\n    return reader\n\n\ndef get_25d_reader():\n    reader = ImageReader([\'image\'])\n    reader.initialise(SINGLE_25D_DATA, SINGLE_25D_TASK, single_25d_list)\n    return reader\n\n\nclass GridSamplesAggregatorTest(NiftyNetTestCase):\n    def test_3d_init(self):\n        reader = get_3d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MULTI_MOD_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\':out[\'image\']}, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(\n            nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        sampler.close_all()\n\n    def test_2d_init(self):\n        reader = get_2d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MOD_2D_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\':out[\'image\']}, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(\n            nib.load(output_file).shape, [128, 128])\n        sampler.close_all()\n\n    def test_25d_init(self):\n        reader = get_25d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=SINGLE_25D_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\':out[\'image\']}, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n        print(output_file)\n        self.assertAllClose(\n            nib.load(output_file).shape, [256, 168, 256],\n            rtol=1e-03, atol=1e-03)\n        sampler.close_all()\n\n    def test_3d_init_mo(self):\n        reader = get_3d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MULTI_MOD_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                out_flatten = np.reshape(np.asarray(out[\'image\']), [10, -1])\n                min_val = np.sum(np.reshape(np.asarray(out[\'image\']),\n                                            [10, -1]), 1)\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'], \'csv_sum\': min_val},\n                    out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(\n            nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [420, 9]\n        )\n        sampler.close_all()\n\n    def test_3d_init_mo_2im(self):\n        reader = get_3d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MULTI_MOD_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'], \'window_im2\': out[\'image\']},\n                    out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        outim2_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'window_im2_{}_niftynet_out.nii.gz\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(\n            nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        self.assertAllClose(\n            nib.load(outim2_filename).shape, (256, 168, 256, 1, 2))\n        sampler.close_all()\n\n    def test_init_3d_mo_3out(self):\n        reader = get_3d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MULTI_MOD_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                print(out[\'image\'].shape)\n                out_flatten = np.reshape(np.asarray(out[\'image\']), [10, -1])\n                min_val = np.sum(np.reshape(\n                    np.asarray(out[\'image\']), [10, -1]), 1)\n                stats_val = np.concatenate(\n                    [np.min(out_flatten, 1, keepdims=True),\n                     np.max(out_flatten, 1, keepdims=True),\n                     np.sum(out_flatten, 1, keepdims=True)], 1)\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'], \'csv_sum\': min_val,\n                     \'csv_stats\': stats_val},\n                    out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_stats_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(\n            nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [420, 9]\n        )\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(\n            stats_pd.shape, [420, 11]\n        )\n        sampler.close_all()\n\n    def test_init_3d_mo_bidimcsv(self):\n        reader = get_3d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MULTI_MOD_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                out_flatten = np.reshape(np.asarray(out[\'image\']), [10, -1])\n                min_val = np.sum(np.reshape(\n                    np.asarray(out[\'image\']), [10, -1]), 1)\n                stats_val = np.concatenate(\n                    [np.min(out_flatten, 1, keepdims=True),\n                     np.max(out_flatten, 1, keepdims=True),\n                     np.sum(out_flatten, 1, keepdims=True)], 1)\n                stats_val = np.expand_dims(stats_val, 1)\n                stats_val = np.concatenate([stats_val, stats_val], axis=1)\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'],\n                     \'csv_sum\': min_val,\n                     \'csv_stats_2d\': stats_val},\n                    out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_stats_2d_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(\n            nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [420, 9]\n        )\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(\n            stats_pd.shape, [420, 14]\n        )\n        sampler.close_all()\n\n    def test_init_2d_mo(self):\n        reader = get_2d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MOD_2D_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                out_flatten = np.reshape(np.asarray(out[\'image\']), [10, -1])\n                min_val = np.sum(np.reshape(\n                    np.asarray(out[\'image\']), [10, -1]), 1)\n                stats_val = np.concatenate(\n                    [np.min(out_flatten, 1, keepdims=True),\n                     np.max(out_flatten, 1, keepdims=True),\n                     np.sum(out_flatten, 1, keepdims=True)], 1)\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'], \'csv_sum\': min_val},\n                    out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(\n            nib.load(output_file).shape, (128, 128))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [10, 9]\n        )\n        sampler.close_all()\n\n    def test_init_2d_mo_3out(self):\n        reader = get_2d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MOD_2D_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                out_flatten = np.reshape(np.asarray(out[\'image\']), [10, -1])\n                min_val = np.sum(np.reshape(\n                    np.asarray(out[\'image\']), [10, -1]), 1)\n                stats_val = np.concatenate(\n                    [np.min(out_flatten, 1, keepdims=True),\n                     np.max(out_flatten, 1, keepdims=True),\n                     np.sum(out_flatten, 1, keepdims=True)], 1)\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'],\n                     \'csv_sum\': min_val,\n                     \'csv_stats\': stats_val},\n                    out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_stats_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(\n            nib.load(output_file).shape, (128, 128))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [10, 9]\n        )\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(\n            stats_pd.shape, [10, 11]\n        )\n        sampler.close_all()\n\n    def test_init_2d_mo_bidimcsv(self):\n        reader = get_2d_reader()\n        sampler = GridSampler(reader=reader,\n                              window_sizes=MOD_2D_DATA,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                out_flatten = np.reshape(np.asarray(out[\'image\']), [10, -1])\n                min_val = np.sum(np.reshape(\n                    np.asarray(out[\'image\']), [10, -1]), 1)\n                stats_val = np.concatenate(\n                    [np.min(out_flatten, 1, keepdims=True),\n                     np.max(out_flatten, 1, keepdims=True),\n                     np.sum(out_flatten, 1, keepdims=True)], 1)\n                stats_val = np.expand_dims(stats_val, 1)\n                stats_val = np.concatenate([stats_val, stats_val], axis=1)\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'],\n                     \'csv_sum\': min_val,\n                     \'csv_stats_2d\': stats_val},\n                    out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_stats_2d_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(\n            nib.load(output_file).shape, (128, 128))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [10, 9]\n        )\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(\n            stats_pd.shape, [10, 14]\n        )\n        sampler.close_all()\n\n    def test_inverse_mapping(self):\n        reader = get_label_reader()\n        data_param = MOD_LABEL_DATA\n        sampler = GridSampler(reader=reader,\n                              window_sizes=data_param,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=(3, 4, 5),\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'label\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=(3, 4, 5),\n            interp_order=0)\n        more_batch = True\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                more_batch = aggregator.decode_batch(\n                    {\'window_label\': out[\'label\']}, out[\'label_location\'])\n        output_filename = \'window_label_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\n            \'testing_data\', \'aggregated\', output_filename)\n        self.assertAllClose(\n            nib.load(output_file).shape, [256, 168, 256])\n        sampler.close_all()\n        output_data = nib.load(output_file).get_data()\n        expected_data = nib.load(\n            \'testing_data/T1_1023_NeuroMorph_Parcellation.nii.gz\').get_data()\n        self.assertAllClose(output_data, expected_data)\n\n    def test_filling(self):\n        reader = get_nonnormalising_label_reader()\n        test_constant = 0.5731\n        postfix = \'_niftynet_out_background\'\n        test_border = (10, 7, 8)\n        data_param = MOD_LABEL_DATA\n        sampler = GridSampler(reader=reader,\n                              window_sizes=data_param,\n                              batch_size=10,\n                              spatial_window_size=None,\n                              window_border=test_border,\n                              queue_length=50)\n        aggregator = GridSamplesAggregator(\n            image_reader=reader,\n            name=\'label\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            window_border=test_border,\n            interp_order=0,\n            postfix=postfix,\n            fill_constant=test_constant)\n        more_batch = True\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                out = sess.run(sampler.pop_batch_op())\n                more_batch = aggregator.decode_batch(\n                    {\'window_label\': out[\'label\']}, out[\'label_location\'])\n        output_filename = \'window_label_{}_{}.nii.gz\'.format(\n            sampler.reader.get_subject_id(0), postfix)\n        output_file = os.path.join(\n            \'testing_data\', \'aggregated\', output_filename)\n        output_data = nib.load(output_file).get_data()\n        output_shape = output_data.shape\n        for i in range(3):\n            def _test_background(idcs):\n                extract = output_data[idcs]\n                self.assertTrue((extract == test_constant).sum()\n                                == extract.size)\n\n            extract_idcs = [slice(None)]*3\n\n            extract_idcs[i] = slice(0, test_border[i])\n            _test_background(tuple(extract_idcs))\n\n            extract_idcs[i] = slice(output_shape[i] - test_border[i],\n                                    output_shape[i])\n            _test_background(tuple(extract_idcs))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/windows_aggregator_identity_v2_test.py,11,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nibabel as nib\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.windows_aggregator_identity import WindowAsImageAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nNEW_ORDER = (0, 1, 2, 4, 3)\nNEW_ORDER_2D = (0, 1, 3, 2)\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\', \'23\'),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=0,\n        pixdim=(2.4, 5.0, 2.0),\n        axcodes=\'LAS\',\n        spatial_window_size=(23, 32, 15),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\', \'23\'),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=0,\n        pixdim=(2.4, 5.0, 2.0),\n        axcodes=\'LAS\',\n        spatial_window_size=(23, 32, 15),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\nMOD_2D_DATA = {\n    \'ultrasound\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(72, 83, 1),\n        loader=None\n    ),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\',))\n\nMOD_LABEL_DATA = {\n    \'parcellation\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'Parcelsampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'23_NeuroMorph_Parcellation\',),\n        filename_not_contains=(\'FLAIR\',),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(150, 140, 100),\n        loader=None\n    ),\n}\nMOD_LABEl_TASK = ParserNamespace(label=(\'parcellation\',))\n\nSINGLE_25D_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\', \'106\'),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=0,\n        pixdim=(3.0, 5.0, 5.0),\n        axcodes=\'LAS\',\n        spatial_window_size=(40, 30, 1),\n        loader=None\n    ),\n}\nSINGLE_25D_TASK = ParserNamespace(image=(\'T1\',))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\nmod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\nmod_label_list = data_partitioner.initialise(MOD_LABEL_DATA).get_file_list()\nsingle_25d_list = data_partitioner.initialise(SINGLE_25D_DATA).get_file_list()\n\n\ndef get_3d_reader():\n    \'\'\'\n    define the 3d reader\n    :return: 3d reader\n    \'\'\'\n    reader = ImageReader([\'image\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    \'\'\'\n    define the 2d reader\n    :return: 2d reader\n    \'\'\'\n    reader = ImageReader([\'image\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_label_reader():\n    \'\'\'\n    define the label reader\n    :return: label reader\n    \'\'\'\n    reader = ImageReader([\'label\'])\n    reader.initialise(MOD_LABEL_DATA, MOD_LABEl_TASK, mod_label_list)\n    label_normaliser = DiscreteLabelNormalisationLayer(\n        image_name=\'label\',\n        modalities=vars(SINGLE_25D_TASK).get(\'label\'),\n        model_filename=os.path.join(\'testing_data\', \'agg_test.txt\'))\n    reader.add_preprocessing_layers(label_normaliser)\n    pad_layer = PadLayer(image_name=(\'label\',), border=(5, 6, 7))\n    reader.add_preprocessing_layers([pad_layer])\n    return reader\n\n\ndef get_25d_reader():\n    \'\'\'\n    define the 2.5 d reader\n    :return:\n    \'\'\'\n    reader = ImageReader([\'image\'])\n    reader.initialise(SINGLE_25D_DATA, SINGLE_25D_TASK, single_25d_list)\n    return reader\n\n\nclass IdentityAggregatorTest(NiftyNetTestCase):\n    def test_3d_init(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MULTI_MOD_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\'),\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\']}, out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape)\n        sampler.close_all()\n\n    def test_3d_init_mo(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MULTI_MOD_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\')\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                sum_val = np.sum(out[\'image\'])\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'], \'csv_sum\': sum_val},\n                    out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated_identity\',\n            \'{}_csv_sum_niftynet_generated.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape)\n        sum_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            sum_pd.shape, [1, 2]\n        )\n        sampler.close_all()\n\n    def test_3d_init_mo_2im(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MULTI_MOD_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\'),\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'], \'window_im2\': out[\'image\']},\n                    out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        outim2_filename = os.path.join(\n            \'testing_data\', \'aggregated_identity\',\n            \'{}_window_im2_niftynet_generated.nii.gz\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape)\n        self.assertAllClose(\n            nib.load(outim2_filename).shape, out_shape)\n        sampler.close_all()\n\n    def test_3d_init_mo_3out(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MULTI_MOD_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\')\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                sum_val = np.sum(out[\'image\'])\n                stats_val = [np.sum(out[\'image\']),\n                             np.min(out[\'image\']),\n                             np.max(out[\'image\'])]\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'], \'csv_sum\': sum_val,\n                     \'csv_stats\': stats_val},\n                    out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated_identity\',\n            \'{}_csv_sum_niftynet_generated.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated_identity\',\n            \'{}_csv_stats_niftynet_generated.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape)\n        sum_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            sum_pd.shape, [1, 2]\n        )\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(\n            stats_pd.shape, [1, 4]\n        )\n        sampler.close_all()\n\n    def test_init_3d_mo_bidimcsv(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MULTI_MOD_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\'),\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                min_val = np.sum((np.asarray(out[\'image\']).flatten()))\n                stats_val = [np.min(out[\'image\']), np.max(out[\'image\']), np.sum(\n                    out[\'image\'])]\n                stats_val = np.expand_dims(stats_val, 0)\n                stats_val = np.concatenate([stats_val, stats_val], axis=0)\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'],\n                     \'csv_sum\': min_val,\n                     \'csv_stats2d\': stats_val},\n                    out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated_identity\',\n            \'{}_csv_sum_niftynet_generated.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated_identity\',\n            \'{}_csv_stats2d_niftynet_generated.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape)\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [1, 2]\n        )\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(\n            stats_pd.shape, [1, 7]\n        )\n        sampler.close_all()\n\n    def test_2d_init(self):\n        reader = get_2d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MOD_2D_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\'),\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\']}, out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER_2D] + [1,]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape[:2])\n        sampler.close_all()\n\n    def test_init_2d_mo(self):\n        reader = get_2d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MOD_2D_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\'),\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                min_val = np.sum((np.asarray(out[\'image\']).flatten()))\n                stats_val = [np.min(out), np.max(out), np.sum(out)]\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'], \'csv_sum\': min_val},\n                    out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated_identity\',\n            \'{}_csv_sum_niftynet_generated.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER_2D] + [1,]\n\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape[:2])\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [1, 2]\n        )\n        sampler.close_all()\n\n    def test_init_2d_mo_3out(self):\n        reader = get_2d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MOD_2D_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\'),\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                min_val = np.sum((np.asarray(out[\'image\']).flatten()))\n                stats_val = [np.min(out[\'image\']), np.max(out[\'image\']), np.sum(\n                    out[\'image\'])]\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'],\n                     \'csv_sum\': min_val,\n                     \'csv_stats\': stats_val},\n                    out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\'testing_data\', \'aggregated_identity\',\n                                    \'{}_csv_sum_niftynet_generated.csv\'.format(\n                                        sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\'testing_data\', \'aggregated_identity\',\n                                      \'{}_csv_stats_niftynet_generated.csv\'.format(\n                                          sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER_2D] + [1,]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape[:2])\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [1, 2]\n        )\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(\n            stats_pd.shape, [1, 4]\n        )\n        sampler.close_all()\n\n    def test_init_2d_mo_bidimcsv(self):\n        reader = get_2d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=MOD_2D_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\'),\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                min_val = np.sum((np.asarray(out[\'image\']).flatten()))\n                stats_val = [np.min(out[\'image\']), np.max(out[\'image\']), np.sum(\n                    out[\'image\'])]\n                stats_val = np.expand_dims(stats_val, 0)\n                stats_val = np.concatenate([stats_val, stats_val], axis=0)\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\'],\n                     \'csv_sum\': min_val,\n                     \'csv_stats2d\': stats_val},\n                    out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\'testing_data\', \'aggregated_identity\',\n                                    \'{}_csv_sum_niftynet_generated.csv\'.format(\n                                        sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\'testing_data\', \'aggregated_identity\',\n                                      \'{}_csv_stats2d_niftynet_generated.csv\'.format(\n                                          sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n\n        out_shape = [out_shape[i] for i in NEW_ORDER_2D] + [1,]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape[:2])\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(\n            min_pd.shape, [1, 2]\n        )\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(\n            stats_pd.shape, [1, 7]\n        )\n        sampler.close_all()\n\n    def test_25d_init(self):\n        reader = get_25d_reader()\n        sampler = ResizeSampler(reader=reader,\n                                window_sizes=SINGLE_25D_DATA,\n                                batch_size=1,\n                                shuffle=False,\n                                queue_length=50)\n        aggregator = WindowAsImageAggregator(\n            image_reader=reader,\n\n            output_path=os.path.join(\'testing_data\', \'aggregated_identity\'),\n            )\n        more_batch = True\n        out_shape = []\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                    out_shape = out[\'image\'].shape[1:] + (1,)\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\']}, out[\'image_location\'])\n        output_filename = \'{}_window_image_niftynet_generated.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\',\n                                   \'aggregated_identity\',\n                                   output_filename)\n        out_shape = [out_shape[i] for i in NEW_ORDER_2D] + [1,]\n        self.assertAllClose(\n            nib.load(output_file).shape, out_shape[:2])\n        sampler.close_all()\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/windows_aggregator_resize_v2_test.py,12,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nibabel as nib\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nMULTI_MOD_DATA = {\n    \'T1\':\n    ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\', \'23\'),\n        filename_not_contains=(\'Parcellation\', ),\n        interp_order=3,\n        pixdim=(2.4, 5.0, 2.0),\n        axcodes=\'LAS\',\n        spatial_window_size=(23, 32, 15),\n        loader=None),\n    \'FLAIR\':\n    ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\', \'23\'),\n        filename_not_contains=(\'Parcellation\', ),\n        interp_order=3,\n        pixdim=(2.4, 5.0, 2.0),\n        axcodes=\'LAS\',\n        spatial_window_size=(23, 32, 15),\n        loader=None)\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'))\n\nMOD_2D_DATA = {\n    \'ultrasound\':\n    ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\', ),\n        filename_not_contains=(\'Parcellation\', ),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(72, 83, 1),\n        loader=None),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\', ))\n\nMOD_LABEL_DATA = {\n    \'parcellation\':\n    ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'Parcelsampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'23_NeuroMorph_Parcellation\', ),\n        filename_not_contains=(\'FLAIR\', ),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(150, 140, 100),\n        loader=None),\n}\nMOD_LABEl_TASK = ParserNamespace(label=(\'parcellation\', ))\n\nSINGLE_25D_DATA = {\n    \'T1\':\n    ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\', \'106\'),\n        filename_not_contains=(\'Parcellation\', ),\n        interp_order=3,\n        pixdim=(3.0, 5.0, 5.0),\n        axcodes=\'LAS\',\n        spatial_window_size=(40, 30, 1),\n        loader=None),\n}\nSINGLE_25D_TASK = ParserNamespace(image=(\'T1\', ))\n\ndata_partitioner = ImageSetsPartitioner()\nmulti_mod_list = data_partitioner.initialise(\n    MULTI_MOD_DATA,\n    data_split_file=\'testing_data/resize_split.csv\').get_file_list()\nmod_2d_list = data_partitioner.initialise(\n    MOD_2D_DATA,\n    data_split_file=\'testing_data/resize_split.csv\').get_file_list()\nmod_label_list = data_partitioner.initialise(\n    MOD_LABEL_DATA,\n    data_split_file=\'testing_data/resize_split.csv\').get_file_list()\nsingle_25d_list = data_partitioner.initialise(\n    SINGLE_25D_DATA,\n    data_split_file=\'testing_data/resize_split.csv\').get_file_list()\n\n\ndef get_3d_reader():\n    \'\'\'\n    define the 3d reader\n    :return: 3d reader\n    \'\'\'\n    reader = ImageReader([\'image\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    \'\'\'\n    define the 2d reader\n    :return: 2d reader\n    \'\'\'\n    reader = ImageReader([\'image\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_label_reader():\n    \'\'\'\n    define the label reader\n    :return: label reader\n    \'\'\'\n    reader = ImageReader([\'label\'])\n    reader.initialise(MOD_LABEL_DATA, MOD_LABEl_TASK, mod_label_list)\n    label_normaliser = DiscreteLabelNormalisationLayer(\n        image_name=\'label\',\n        modalities=vars(SINGLE_25D_TASK).get(\'label\'),\n        model_filename=os.path.join(\'testing_data\', \'agg_test.txt\'))\n    reader.add_preprocessing_layers(label_normaliser)\n    pad_layer = PadLayer(image_name=(\'label\', ), border=(5, 6, 7))\n    reader.add_preprocessing_layers([pad_layer])\n    return reader\n\n\ndef get_25d_reader():\n    \'\'\'\n    define the 2.5 d reader\n    :return:\n    \'\'\'\n    reader = ImageReader([\'image\'])\n    reader.initialise(SINGLE_25D_DATA, SINGLE_25D_TASK, single_25d_list)\n    return reader\n\n\nclass ResizeSamplesAggregatorTest(NiftyNetTestCase):\n    def test_3d_init(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MULTI_MOD_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\']}, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        sampler.close_all()\n\n    def test_3d_init_mo(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MULTI_MOD_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                sum_val = np.sum(out[\'image\'])\n                more_batch = aggregator.decode_batch(\n                    {\n                        \'window_image\': out[\'image\'],\n                        \'csv_sum\': sum_val\n                    }, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\', \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        sum_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(sum_pd.shape, [1, 2])\n        sampler.close_all()\n\n    def test_3d_init_mo_2im(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MULTI_MOD_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\n                        \'window_image\': out[\'image\'],\n                        \'window_im2\': out[\'image\']\n                    }, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        outim2_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'window_im2_{}_niftynet_out.nii.gz\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        self.assertAllClose(\n            nib.load(outim2_filename).shape, (256, 168, 256, 1, 2))\n        sampler.close_all()\n\n    def test_3d_init_mo_3out(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MULTI_MOD_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                sum_val = np.sum(out[\'image\'])\n                stats_val = [\n                    np.sum(out[\'image\']),\n                    np.min(out[\'image\']),\n                    np.max(out[\'image\'])\n                ]\n                more_batch = aggregator.decode_batch(\n                    {\n                        \'window_image\': out[\'image\'],\n                        \'csv_sum\': sum_val,\n                        \'csv_stats\': stats_val\n                    }, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\', \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_stats_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        sum_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(sum_pd.shape, [1, 2])\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(stats_pd.shape, [1, 4])\n        sampler.close_all()\n\n    def test_init_3d_mo_bidimcsv(self):\n        reader = get_3d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MULTI_MOD_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                min_val = np.sum((np.asarray(out[\'image\']).flatten()))\n                stats_val = [\n                    np.min(out[\'image\']),\n                    np.max(out[\'image\']),\n                    np.sum(out[\'image\'])\n                ]\n                stats_val = np.expand_dims(stats_val, 0)\n                stats_val = np.concatenate([stats_val, stats_val], axis=0)\n                more_batch = aggregator.decode_batch(\n                    {\n                        \'window_image\': out[\'image\'],\n                        \'csv_sum\': min_val,\n                        \'csv_stats_2d\': stats_val\n                    }, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\', \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_stats_2d_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(nib.load(output_file).shape, (256, 168, 256, 1, 2))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(min_pd.shape, [1, 2])\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(stats_pd.shape, [1, 7])\n        sampler.close_all()\n\n    def test_2d_init(self):\n        reader = get_2d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MOD_2D_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\']}, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(nib.load(output_file).shape, [128, 128])\n        sampler.close_all()\n\n    def test_init_2d_mo(self):\n        reader = get_2d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MOD_2D_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                min_val = np.sum((np.asarray(out[\'image\']).flatten()))\n                stats_val = [np.min(out), np.max(out), np.sum(out)]\n                more_batch = aggregator.decode_batch(\n                    {\n                        \'window_image\': out[\'image\'],\n                        \'csv_sum\': min_val\n                    }, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\', \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(nib.load(output_file).shape, (128, 128))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(min_pd.shape, [1, 2])\n        sampler.close_all()\n\n    def test_init_2d_mo_3out(self):\n        reader = get_2d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MOD_2D_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                min_val = np.sum((np.asarray(out[\'image\']).flatten()))\n                stats_val = [\n                    np.min(out[\'image\']),\n                    np.max(out[\'image\']),\n                    np.sum(out[\'image\'])\n                ]\n                more_batch = aggregator.decode_batch(\n                    {\n                        \'window_image\': out[\'image\'],\n                        \'csv_sum\': min_val,\n                        \'csv_stats\': stats_val\n                    }, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\', \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_stats_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(nib.load(output_file).shape, (128, 128))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(min_pd.shape, [1, 2])\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(stats_pd.shape, [1, 4])\n        sampler.close_all()\n\n    def test_init_2d_mo_bidimcsv(self):\n        reader = get_2d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MOD_2D_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                min_val = np.sum((np.asarray(out[\'image\']).flatten()))\n                stats_val = [\n                    np.min(out[\'image\']),\n                    np.max(out[\'image\']),\n                    np.sum(out[\'image\'])\n                ]\n                stats_val = np.expand_dims(stats_val, 0)\n                stats_val = np.concatenate([stats_val, stats_val], axis=0)\n                more_batch = aggregator.decode_batch(\n                    {\n                        \'window_image\': out[\'image\'],\n                        \'csv_sum\': min_val,\n                        \'csv_stats_2d\': stats_val\n                    }, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        sum_filename = os.path.join(\n            \'testing_data\', \'aggregated\', \'csv_sum_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        stats_filename = os.path.join(\n            \'testing_data\', \'aggregated\',\n            \'csv_stats_2d_{}_niftynet_out.csv\'.format(\n                sampler.reader.get_subject_id(0)))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n\n        self.assertAllClose(nib.load(output_file).shape, (128, 128))\n        min_pd = pd.read_csv(sum_filename)\n        self.assertAllClose(min_pd.shape, [1, 2])\n        stats_pd = pd.read_csv(stats_filename)\n        self.assertAllClose(stats_pd.shape, [1, 7])\n        sampler.close_all()\n\n    def test_25d_init(self):\n        reader = get_25d_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=SINGLE_25D_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'image\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=3)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\'window_image\': out[\'image\']}, out[\'image_location\'])\n        output_filename = \'window_image_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(nib.load(output_file).shape, [256, 168, 256])\n        sampler.close_all()\n\n    def test_inverse_mapping(self):\n        reader = get_label_reader()\n        sampler = ResizeSampler(\n            reader=reader,\n            window_sizes=MOD_LABEL_DATA,\n            batch_size=1,\n            shuffle=False,\n            queue_length=50)\n        aggregator = ResizeSamplesAggregator(\n            image_reader=reader,\n            name=\'label\',\n            output_path=os.path.join(\'testing_data\', \'aggregated\'),\n            interp_order=0)\n        more_batch = True\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            while more_batch:\n                try:\n                    out = sess.run(sampler.pop_batch_op())\n                except tf.errors.OutOfRangeError:\n                    break\n                more_batch = aggregator.decode_batch(\n                    {\'window_label\': out[\'label\']}, out[\'label_location\'])\n        output_filename = \'window_label_{}_niftynet_out.nii.gz\'.format(\n            sampler.reader.get_subject_id(0))\n        output_file = os.path.join(\'testing_data\', \'aggregated\',\n                                   output_filename)\n        self.assertAllClose(nib.load(output_file).shape, [256, 168, 256])\n        sampler.close_all()\n        # output_data = nib.load(output_file).get_data()[..., 0, 0]\n        # expected_data = nib.load(\n        #     \'testing_data/T1_1023_NeuroMorph_Parcellation.nii.gz\').get_data()\n        # self.assertAllClose(output_data, expected_data)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
data/PROMISE12/setup.py,0,"b'""""""\nUnzip data downloaded from challenge website:\nhttps://promise12.grand-challenge.org/\n\nThe outcome should be three folders named:\nTrainingData_Part1, TrainingData_Part2, TrainingData_Part3\neach folder contains multiple \'.mhd\' and \'.raw\' files\n""""""\nimport os\nimport zipfile\n\nzip_dir = \'.\'\ntarget_dir = \'.\'\nfor zip_filename in {\'TrainingData_Part1.zip\', \'TrainingData_Part2.zip\',\n                     \'TrainingData_Part3.zip\'}:\n    print(\'Extracting\', zip_filename, \'...\')\n    zip_ref = zipfile.ZipFile(os.path.join(zip_dir, zip_filename), \'r\')\n    zip_ref.extractall(os.path.basename(zip_filename.replace(\'.zip\', \'\')))\n    zip_ref.close()\n'"
demos/BRATS17/__init__.py,0,b''
demos/BRATS17/rename_crop_BRATS.py,0,"b'""""""\nThis script renames BRATS dataset to OUTPUT_path,\neach subject\'s images will be cropped and renamed to\n""TYPEindex_modality.nii.gz"".\n\noutput dataset folder will be created if not exists, and content\nin the created folder will be, for example:\n\nOUTPUT_path:\n   HGG100_Flair.nii.gz\n   HGG100_Label.nii.gz\n   HGG100_T1c.nii.gz\n   HGG100_T1.nii.gz\n   HGG100_T2.nii.gz\n   ...\n\nEach .nii.gz file in OUTPUT_path will be cropped with a tight bounding box\nusing function crop_zeros defined in this script.\n\nPlease change BRATS_path and OUTPUT_path accordingly to the preferred folder\n""""""\nimport os\n\nimport SimpleITK as sitk\nimport nibabel\nimport numpy as np\n\n# change here to the directory of downloaded BRATS data\')\nBRATS_path = os.path.join(\n    \'/Volumes\', \'Public\', \'Brats17TrainingData\')\n# change here to the directory of preferred output directory\')\nOUTPUT_path = os.path.join(\n    os.environ[\'HOME\'], \'Dataset\', \'Brats17Challenge_crop_renamed\')\n# Aff to use with BRATS dataset\nOUTPUT_AFFINE = np.array(\n    [[-1, 0, 0, 0],\n     [0, -1, 0, 239],\n     [0, 0, 1, 0],\n     [0, 0, 0, 1]])\nmod_names17 = [\'flair\', \'t1\', \'t1ce\', \'t2\']\nmod_names15 = [\'Flair\', \'T1\', \'T1c\', \'T2\']\n\n\ndef crop_zeros(img_array):\n    if len(img_array.shape) == 4:\n        img_array = np.amax(img_array, axis=3)\n    assert len(img_array.shape) == 3\n    x_dim, y_dim, z_dim = tuple(img_array.shape)\n    x_zeros, y_zeros, z_zeros = np.where(img_array == 0.)\n    # x-plans that are not uniformly equal to zeros\n    x_to_keep, = np.where(np.bincount(x_zeros) < y_dim * z_dim)\n    x_min = min(x_to_keep)\n    x_max = max(x_to_keep) + 1\n    y_to_keep, = np.where(np.bincount(y_zeros) < x_dim * z_dim)\n    y_min = min(y_to_keep)\n    y_max = max(y_to_keep) + 1\n    z_to_keep, = np.where(np.bincount(z_zeros) < x_dim * y_dim)\n    z_min = min(z_to_keep)\n    z_max = max(z_to_keep) + 1\n    return x_min, x_max, y_min, y_max, z_min, z_max\n\n\ndef load_scans_BRATS15(pat_folder, with_seg=False):\n    # Get path to nii files for a patient\n    VSD_id = None\n    nii_folders = [f_name for f_name in os.listdir(pat_folder)\n                   if f_name.startswith(\'VSD\')]\n    nii_paths = []\n    for nii_folder in nii_folders:\n        nii_path = os.path.join(pat_folder, nii_folder)\n        nii_fn = [f_n for f_n in os.listdir(nii_path)\n                  if f_n.endswith((\'.mha\')) and f_n.startswith(\'VSD\')]\n        assert len(nii_fn) == 1\n        nii_path = os.path.join(nii_path, nii_fn[0])\n        nii_paths.append(nii_path)\n        # Get VSD id (compulsory for online evaluation)\n        if \'Flair\' in nii_fn[0]:\n            VSD_id = nii_fn[0].split(\'.\')[-2]\n            print(\'VSD ID: %s\' % VSD_id)\n    assert VSD_id is not None\n    # Load data\n    img_data = []\n    for mod_n in mod_names15:\n        file_n = [f_n for f_n in nii_paths if (mod_n + \'.\') in f_n][0]\n        # mod_data = nibabel.load(os.path.join(pat_folder, file_n)).get_data()\n        mod_data = sitk.ReadImage(\n            os.path.join(pat_folder, file_n), sitk.sitkFloat32)\n        mod_data = sitk.GetArrayFromImage(mod_data)\n        img_data.append(mod_data)\n    img_data = np.stack(img_data, axis=-1)\n    if not with_seg:\n        return img_data, None, VSD_id\n    else:\n        file_n_list = [f_n for f_n in nii_paths if (\'OT.\') in f_n]\n        if len(file_n_list) != 0:\n            file_n = file_n_list[0]\n            # seg_data = nibabel.load(os.path.join(pat_folder, file_n)).get_data()\n            seg_data = sitk.ReadImage(\n                os.path.join(pat_folder, file_n), sitk.sitkFloat32)\n            seg_data = sitk.GetArrayFromImage(seg_data)\n        else:\n            seg_data = None\n        return img_data, seg_data, VSD_id\n\n\ndef load_scans_BRATS17(pat_folder, with_seg=False):\n    nii_fnames = [f_name for f_name in os.listdir(pat_folder)\n                  if f_name.endswith((\'.nii\', \'.nii.gz\'))]\n    img_data = []\n    for mod_n in mod_names17:\n        file_n = [f_n for f_n in nii_fnames if (mod_n + \'.\') in f_n][0]\n        mod_data = nibabel.load(os.path.join(pat_folder, file_n)).get_data()\n        img_data.append(mod_data)\n    img_data = np.stack(img_data, axis=-1)\n    if not with_seg:\n        return img_data, None\n    else:\n        file_n = [f_n for f_n in nii_fnames if (\'seg.\') in f_n][0]\n        seg_data = nibabel.load(os.path.join(pat_folder, file_n)).get_data()\n        return img_data, seg_data\n\n\ndef save_scans_BRATS15(pat_name, VSD_id, img_data, seg_data=None):\n    save_mod_names = [\'Flair\', \'T1\', \'T1c\', \'T2\']\n    save_seg_name = \'Label\'\n    assert img_data.shape[3] == 4\n    for mod_i in range(len(save_mod_names)):\n        save_name = \'%s.%s_%s.nii.gz\' % \\\n                    (pat_name, VSD_id, save_mod_names[mod_i])\n        save_path = os.path.join(OUTPUT_path, save_name)\n        mod_data_nii = nibabel.Nifti1Image(img_data[:, :, :, mod_i],\n                                           OUTPUT_AFFINE)\n        nibabel.save(mod_data_nii, save_path)\n    if seg_data is not None:\n        save_name = \'%s.%s_%s.nii.gz\' % \\\n                    (pat_name, VSD_id, save_seg_name)\n        save_path = os.path.join(OUTPUT_path, save_name)\n        seg_data_nii = nibabel.Nifti1Image(seg_data, OUTPUT_AFFINE)\n        nibabel.save(seg_data_nii, save_path)\n\n\ndef save_scans_BRATS17(pat_name, img_data, seg_data=None):\n    save_mod_names = [\'Flair\', \'T1\', \'T1c\', \'T2\']\n    save_seg_name = \'Label\'\n    assert img_data.shape[3] == 4\n    for mod_i in range(len(save_mod_names)):\n        save_name = \'%s_%s.nii.gz\' % (pat_name, save_mod_names[mod_i])\n        save_path = os.path.join(OUTPUT_path, save_name)\n        mod_data_nii = nibabel.Nifti1Image(img_data[:, :, :, mod_i],\n                                           OUTPUT_AFFINE)\n        nibabel.save(mod_data_nii, save_path)\n    print(\'saved to {}\'.format(OUTPUT_path))\n    if seg_data is not None:\n        save_name = \'%s_%s.nii.gz\' % (pat_name, save_seg_name)\n        save_path = os.path.join(OUTPUT_path, save_name)\n        seg_data_nii = nibabel.Nifti1Image(seg_data, OUTPUT_AFFINE)\n        nibabel.save(seg_data_nii, save_path)\n\n\ndef main(pat_category_list=(\'HGG\', \'LGG\'), dataset=\'BRATS17\', crop=False):\n    for pat_cat in pat_category_list:\n        pat_ID = 0\n        for pat_folder_name in os.listdir(os.path.join(BRATS_path, pat_cat)):\n            pat_ID += 1\n            # Load\n            pat_folder = os.path.join(BRATS_path, pat_cat, pat_folder_name)\n            try:\n                if dataset == \'BRATS17\':\n                    img_data, seg_data = load_scans_BRATS17(\n                        pat_folder, with_seg=True)\n                if dataset == \'BRATS15\':\n                    img_data, seg_data, VSD_id = load_scans_BRATS15(\n                        pat_folder, with_seg=True)\n            except OSError:\n                print(\'skipping %s\' % pat_folder)\n                continue\n                pass\n            print(""subject: {}, shape: {}"".format(pat_folder, img_data.shape))\n            # Cropping\n            if crop:\n                x_, _x, y_, _y, z_, _z = crop_zeros(img_data)\n                img_data = img_data[x_:_x, y_:_y, z_:_z, :]\n                seg_data = seg_data[x_:_x, y_:_y, z_:_z]\n                print(\'shape cropping: {}\'.format(img_data.shape))\n            # Save with name convention\n            pat_name = \'%s%d\' % (pat_cat, pat_ID)\n            # remove \'_\' from pat_name to match name convention\n            pat_name = pat_name.replace(\'_\', \'\')\n            if dataset == \'BRATS15\':\n                save_scans_BRATS15(pat_name, VSD_id, img_data, seg_data)\n            elif dataset == \'BRATS17\':\n                save_scans_BRATS17(pat_name, img_data, seg_data)\n\n\nif __name__ == \'__main__\':\n    if not os.path.exists(BRATS_path):\n        raise ValueError(\n            \'please change ""BRATS_path"" in this script to \'\n            \'the BRATS17 challenge dataset. \'\n            \'Dataset not found: {}\'.format(BRATS_path))\n    if not os.path.exists(OUTPUT_path):\n        os.makedirs(OUTPUT_path)\n    main(dataset=\'BRATS17\', crop=True)\n    # main([\'HGG\'], dataset=\'BRATS15\', crop=False)\n'"
demos/Learning_Rate_Decay/decay_lr_application.py,7,"b'import tensorflow as tf\n\nfrom niftynet.application.segmentation_application import \\\n    SegmentationApplication\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.application_variables import TF_SUMMARIES\nfrom niftynet.layer.loss_segmentation import LossFunction\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'weight\'])\n\n\nclass DecayLearningRateApplication(SegmentationApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, is_training):\n        SegmentationApplication.__init__(\n            self, net_param, action_param, is_training)\n        tf.logging.info(\'starting decay learning segmentation application\')\n        self.learning_rate = None\n        self.current_lr = action_param.lr\n        if self.action_param.validation_every_n > 0:\n            raise NotImplementedError(""validation process is not implemented ""\n                                      ""in this demo."")\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        data_dict = self.get_sampler()[0][0].pop_batch_op()\n        image = tf.cast(data_dict[\'image\'], tf.float32)\n        net_out = self.net(image, self.is_training)\n\n        if self.is_training:\n            with tf.name_scope(\'Optimiser\'):\n                self.learning_rate = tf.placeholder(tf.float32, shape=[])\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.learning_rate)\n            loss_func = LossFunction(\n                n_class=self.segmentation_param.num_classes,\n                loss_type=self.action_param.loss_type)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None),\n                weight_map=data_dict.get(\'weight\', None))\n\n            loss = data_loss\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            grads = self.optimiser.compute_gradients(loss)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.learning_rate, name=\'lr\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n        else:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            SegmentationApplication.connect_data_and_network(\n                self, outputs_collector, gradients_collector)\n\n    def set_iteration_update(self, iteration_message):\n        """"""\n        This function will be called by the application engine at each\n        iteration.\n        """"""\n        current_iter = iteration_message.current_iter\n        if iteration_message.is_training:\n            if current_iter > 0 and current_iter % 5 == 0:\n                self.current_lr = self.current_lr / 1.02\n            iteration_message.data_feed_dict[self.is_validation] = False\n        elif iteration_message.is_validation:\n            iteration_message.data_feed_dict[self.is_validation] = True\n        iteration_message.data_feed_dict[self.learning_rate] = self.current_lr\n'"
demos/PyTorchNiftyNet/segmentation.py,0,"b'import argparse\nimport time\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom libs import dataset_niftynet as dset_utils\nfrom libs import loss as loss_utils\nfrom libs import model as cnn_utils\nfrom torch.utils.data import DataLoader\n\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.signal import TRAIN, VALID, INFER\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.evaluation.pairwise_measures import PairwiseMeasures\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.layer.mean_variance_normalisation import MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\n\n\ndef get_reader(data_param, image_sets_partitioner, phase):\n    # Using Nifty Reader\n    if phase == \'training\':\n        image_reader = ImageReader().initialise(\n            data_param, file_list=image_sets_partitioner.get_file_list(TRAIN))\n\n    elif phase == \'validation\':\n        image_reader = ImageReader().initialise(\n            data_param, file_list=image_sets_partitioner.get_file_list(VALID))\n\n    elif phase == \'inference\':\n        image_reader = ImageReader().initialise(\n            data_param, file_list=image_sets_partitioner.get_file_list(INFER))\n    else:\n        raise Exception(\'Invalid phase choice: {}\'.format(\n            {\'phase\': [\'train\', \'validation\', \'inference\']}))\n\n    # Adding preprocessing layers\n    mean_variance_norm_layer = MeanVarNormalisationLayer(image_name=\'image\')\n    pad_layer = PadLayer(image_name=(\'image\', \'label\'), border=(8, 8, 8))\n    image_reader.add_preprocessing_layers([mean_variance_norm_layer])\n\n    if phase == \'inference\':\n        image_reader.add_preprocessing_layers([pad_layer])\n\n    return image_reader\n\n\ndef get_sampler(image_reader, patch_size, phase):\n    if phase in (\'training\', \'validation\'):\n        sampler = UniformSampler(image_reader,\n                                 window_sizes=patch_size,\n                                 windows_per_image=2)\n    elif phase == \'inference\':\n        sampler = GridSampler(image_reader,\n                              window_sizes=patch_size,\n                              window_border=(8, 8, 8),\n                              batch_size=1)\n    else:\n        raise Exception(\'Invalid phase choice: {}\'.format(\n            {\'phase\': [\'train\', \'validation\', \'inference\']}))\n\n    return sampler\n\n\ndef train(dsets, model, criterion, optimizer,\n          num_epochs, device, cp_path, batch_size):\n    since = time.time()\n\n    dataloaders = {\n        x: DataLoader(dsets[x], batch_size=batch_size,\n                      shuffle=True, num_workers=4)\n        for x in [\'training\', \'validation\']}\n\n    model = model.to(device)\n\n    for epoch in range(num_epochs):\n        print(\'Epoch {}/{}\'.format(epoch + 1, num_epochs))\n        print(\'-\' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in [\'training\', \'validation\']:\n            if phase == \'training\':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0.0\n            epoch_samples = 0\n\n            # Iterate over data\n            for iteration, (inputs, labels) in enumerate(dataloaders[phase], 1):\n\n                nbatches, wsize, nchannels, x, y, z, _ = inputs.size()\n\n                inputs = inputs.view(nbatches * wsize, nchannels, x, y, z)\n                labels = labels.view(nbatches * wsize, nchannels, x, y, z)\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == \'training\'):\n                    outputs = model(inputs)\n                    pred = (outputs > 0.5)\n\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == \'training\':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                epoch_samples += inputs.size(0)\n                running_loss += loss.item() * inputs.size(0)\n                measures = PairwiseMeasures(\n                    pred.cpu().numpy(), labels.cpu().numpy())\n                running_corrects += measures.dice_score() * inputs.size(0)\n\n            epoch_loss = running_loss / epoch_samples\n\n            epoch_acc = running_corrects / epoch_samples\n\n            print(\'{} Loss: {:.4f} Dice: {:.4f}\'.format(\n                phase, epoch_loss, epoch_acc))\n\n            if epoch == 0:\n                best_loss = epoch_loss\n                torch.save(model.state_dict(), cp_path.format(epoch + 1))\n\n            # deep copy the model\n            if phase == \'validation\' and epoch_loss < best_loss:\n                best_loss = epoch_loss\n                torch.save(model.state_dict(), cp_path)\n                print(\'Checkpoint {} saved!\'.format(epoch + 1))\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(\'Training complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n\n\ndef inference(sampler, model, device, pred_path, cp_path):\n    output = GridSamplesAggregator(image_reader=sampler.reader,\n                                   window_border=(8, 8, 8),\n                                   output_path=pred_path)\n    for _ in sampler():  # for each subject\n\n        model.load_state_dict(torch.load(cp_path))\n        model.to(device)\n        model.eval()\n\n        for batch_output in sampler():  # for each sliding window step\n            window = batch_output[\'image\']\n            # [...,0,:] eliminates time coordinate from NiftyNet Volume\n            window = window[..., 0, :]\n            window = np.transpose(window, (0, 4, 1, 2, 3))\n            window = torch.Tensor(window).to(device)\n\n            with torch.no_grad():\n                outputs = model(window)\n\n            outputs = outputs.cpu().numpy()\n            outputs = np.transpose(outputs, (0, 2, 3, 4, 1))\n            output.decode_batch({\'window_image\': outputs.astype(np.float32)},\n                                batch_output[\'image_location\'])\n\n\ndef main():\n    opt = parsing_data()\n\n    print(""[INFO]Reading data"")\n    # Dictionary with data parameters for NiftyNet Reader\n    if torch.cuda.is_available():\n        print(\'[INFO] GPU available.\')\n        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    else:\n        raise Exception(\n            ""[INFO] No GPU found or Wrong gpu id, please run without --cuda"")\n\n    # Dictionary with data parameters for NiftyNet Reader\n    data_param = {\n        \'image\': {\'path_to_search\': opt.image_path, \'filename_contains\': \'CC\'},\n        \'label\': {\'path_to_search\': opt.label_path, \'filename_contains\': \'CC\'}}\n\n    image_sets_partitioner = ImageSetsPartitioner().initialise(\n        data_param=data_param,\n        data_split_file=opt.data_split_file,\n        new_partition=False,\n        ratios=opt.ratios\n    )\n\n    readers = {x: get_reader(data_param, image_sets_partitioner, x)\n        for x in [\'training\', \'validation\', \'inference\']}\n    samplers = {x: get_sampler(readers[x], opt.patch_size, x)\n        for x in [\'training\', \'validation\', \'inference\']}\n\n    # Training stage only\n    dsets = {x: dset_utils.DatasetNiftySampler(sampler=samplers[x])\n             for x in [\'training\', \'validation\']}\n\n    print(""[INFO] Building model"")\n    model = cnn_utils.UNet3D(opt.in_channels, opt.n_classes)\n    criterion = loss_utils.SoftDiceLoss()\n    optimizer = optim.RMSprop(model.parameters(), lr=opt.lr)\n\n    print(""[INFO] Training"")\n    train(dsets, model, criterion, optimizer,\n          opt.num_epochs, device, opt.cp_path, opt.batch_size)\n\n    print(""[INFO] Inference"")\n    inference(samplers[\'inference\'], model, device, opt.pred_path, opt.cp_path)\n\n\ndef parsing_data():\n    parser = argparse.ArgumentParser(\n        description=\'3D Segmentation Using PyTorch and NiftyNet\')\n    parser.add_argument(\'-data_split_file\',\n                        default=\'train_val_infer_split.csv\',\n                        type=str, help=\'output csv filename\')\n    parser.add_argument(\'-patch_size\', default=(64, 64, 64),\n                        type=tuple, help=\'patch size\')\n    parser.add_argument(\'-in_channels\', default=1,\n                        type=int, help=\'# of data channels\')\n    parser.add_argument(\'-n_classes\', default=1,\n                        type=int, help=\'# of output classes\')\n    parser.add_argument(\'-num_epochs\', default=1,\n                        type=int, help=\'# of epochs\')\n    parser.add_argument(\'-lr\', default=1e-4,\n                        type=float, help=\'learning rate\')\n    parser.add_argument(\'-ratios\', default=[0.1, 0.1],\n                        type=list,\n                        help=\'ratio for validation and inference sets\')\n    parser.add_argument(\'-batch_size\', default=4,\n                        type=int, help=\'batch size\')\n    parser.add_argument(\'-cp_path\', default=\'./CP.pth\',\n                        type=str, help=\'checkpoint output filename\')\n    parser.add_argument(\'-image_path\',\n                        default=\'/home/oeslle/Documents/Datasets/CC359_NEW/Original\',\n                        type=str, help=\'image path\')\n    parser.add_argument(\'-label_path\',\n                        default=\'/home/oeslle/Documents/Datasets/CC359_NEW/STAPLE-binary\',\n                        type=str, help=\'label path\')\n    parser.add_argument(\'-pred_path\',\n                        default=\'/home/oeslle/Documents/pred_seg_brain\',\n                        type=str, help=\'output path for inferences\')\n\n    opt = parser.parse_args()\n\n    return opt\n\n\nif __name__ == \'__main__\':\n    main()\n'"
demos/module_examples/visualise_coordinates.py,2,"b'import sys\nimport matplotlib\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.collections import PatchCollection\n\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.windows_aggregator_base import ImageWindowsAggregator as IA\nfrom niftynet.layer.pad import PadLayer\n\n\ndef vis_coordinates(image, coordinates=None, saving_name=\'image.png\', dpi=50):\n    """"""\n    Plot image, and on top of it, draw boxes with the window coordinates\n    the figure is saved at `saving_name`\n    """"""\n    fig, ax = plt.subplots(1)\n    ax.imshow(image, cmap=\'gray\')\n    all_patch = []\n    if coordinates is not None:\n        for win in coordinates[::-1]:\n            patch = patches.Rectangle(\n                xy=(win[2], win[1]),\n                width=win[5] - win[2],\n                height=win[4] - win[1],\n                linewidth=1)\n            all_patch.append(patch)\n    if all_patch:\n        all_pc = PatchCollection(\n            all_patch, alpha=0.6, edgecolor=\'r\', facecolor=\'#f5e44c\')\n        ax.add_collection(all_pc)\n    # \xc2\xa0plt.show()\n    if saving_name:\n        fig.savefig(saving_name, bbox_inches=\'tight\', pad_inches=0, dpi=dpi)\n        return\n\n\n###\n# config parameters\n###\nspatial_window_size = (100, 100)\nborder = (12, 12)\nvolume_padding_size = (50, 50)\ndata_param = \\\n    {\'MR\': {\n        \'path_to_search\': \'~/Desktop/useful_scripts/visualise_windows\',\n        \'filename_contains\': \'example.png\'}}\n\n###\n# create an image reader\n###\nreader = ImageReader().initialise(data_param)\nreader.add_preprocessing_layers(  # add volume padding layer\n    [PadLayer(image_name=[\'MR\'],\n              border=volume_padding_size, mode=\'constant\')])\n\n###\n# show \'volume\' -- without window sampling\n###\nimage_2d = ImageWindowDataset(reader)()[\'MR\'][0, :, :, 0, 0, 0]\nvis_coordinates(image_2d, saving_name=\'output/image.png\')\n\n###\n# create & show uniform random samples\n###\nuniform_sampler = UniformSampler(\n    reader, spatial_window_size, windows_per_image=100)\nnext_window = uniform_sampler.pop_batch_op()\ncoords = []\nwith tf.Session() as sess:\n    for _ in range(20):\n        uniform_windows = sess.run(next_window)\n        coords.append(uniform_windows[\'MR_location\'])\ncoords = np.concatenate(coords, axis=0)\nvis_coordinates(image_2d, coords, \'output/uniform.png\')\n\n###\n# create & show all grid samples\n###\ngrid_sampler = GridSampler(\n    reader, spatial_window_size, window_border=border)\nnext_grid = grid_sampler.pop_batch_op()\ncoords = []\nwith tf.Session() as sess:\n    while True:\n        window = sess.run(next_grid)\n        if window[\'MR_location\'][0, 0] == -1:\n            break\n        coords.append(window[\'MR_location\'])\ncoords = np.concatenate(coords, axis=0)\nvis_coordinates(image_2d, coords, \'output/grid.png\')\n\n###\n# create & show cropped grid samples (in aggregator)\n###\nn_window = coords.shape[0]\ndummy_window = np.zeros((n_window, 800, 800, 1, 1))\n_, coords = IA.crop_batch(dummy_window, coords, border=border)\nvis_coordinates(image_2d, coords, \'output/grid_cropped.png\')\n\n'"
demos/unet/file_sorter.py,0,"b'import os\nfrom shutil import copy\nimport argparse\n\n\ndef get_user_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--file_dir\',\n                        default=os.path.join(\'data\', \'u-net\'),\n                        help=\'The directory containing the cell tracking data.\',\n                        )\n    parser.add_argument(\'--experiment_names\',\n                        default=[\'DIC-C2DH-HeLa\', \'PhC-C2DH-U373\'],\n                        help=\'The names of the cell tracking experiments.\',\n                        type=list\n                        )\n    return parser.parse_args()\n\n\ndef main():\n    args = get_user_args()\n\n    # for each specified folder\n    for experiment_name in args.experiment_names:\n        out_dir = os.path.join(args.file_dir, experiment_name, \'niftynet_data\')\n        if not os.path.isdir(out_dir):\n            os.makedirs(out_dir)\n\n        for root, _, files in os.walk(os.path.join(args.file_dir, experiment_name)):\n            for name in [f for f in files if \'track\' not in f]:\n                if \'niftynet_data\' not in root:  # don\'t look at the ones that are done already\n                    cell_id = root.split(os.sep)[root.split(\'/\').index(experiment_name) + 1][:2]\n                    out_name = name.replace(\'t0\', \'img_0\').replace(\'t1\', \'img_1\').replace(\'man_seg\', \'seg_\')\n                    out_name = \'\'.join([out_name.split(\'.\')[0] + \'_\', cell_id, \'.tif\'])\n                    out_path = os.path.join(out_dir, out_name)\n                    copy(os.path.join(root, name), out_path)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
demos/unet/generate_run_commands.py,0,"b'import itertools\n\n\nclass ExperimentProtocol(object):\n    def __init__(self, base_command, conditions, model_dir_prefix):\n        """"""\n        :param base_command: the base command used for NiftyNet\n        :param conditions: dictionary of experiment conditions,\n        {""name"" : {""--command_line_command"": ""values (iterable)""} }\n        e.g. {""elastic"": {""--do_elastic_deformation"": [""True"", ""False""]}}\n        :param model_dir_prefix: prefix for the experimental directory\n        """"""\n        self.base_command = base_command\n        self.conditions = conditions\n        self.model_dir_prefix = model_dir_prefix\n        self.commands = []\n\n    def add_condition(self, new_condition):\n        for cond in new_condition:\n            self.conditions[cond] = new_condition[cond]\n\n    def to_file(self, file_name):\n        with open(file_name, \'w\') as f:\n            for line in self.commands:\n                f.write(line + ""\\n"")\n\n    def generate_commands(self):\n        self.commands = []\n        combinations = list(dict_product(self.conditions))\n        for i, combo in enumerate(combinations):\n            str_command = [self.base_command]\n            for condition in combo:\n                str_command += [str(condition), combo[condition]]\n            str_command += [""--model_dir"",\n                            self.model_dir_prefix + \'_\' + str(i).zfill(len(combinations) // 10 + 1)]\n            self.commands += ["" "".join(str_command)]\n\n    def __str__(self):\n        return ""\\n"".join(self.commands)\n\n\ndef dict_product(dicts):\n    """"""\n    from https://stackoverflow.com/questions/5228158/cartesian-product-of-a-dictionary-of-lists\n    """"""\n    return (dict(zip(dicts, x)) for x in itertools.product(*dicts.values()))\n\n\ndef main():\n    base_command = ""python net_segment.py train -c ./demos/unet/U373.ini""\n    model_dir_prefix = ""./models/U373""\n    # note: these paths are relative to the model directory\n    d_split_files = [""../../demos/unet/u373_d_split_%i.csv"" % i for i in [1, 2]]\n    conditions = {""--do_elastic_deformation"": [""True"", ""False""],\n                  ""--random_flipping_axes"": [""\'0,1\'"", ""-1""],\n                  ""--dataset_split_file"": d_split_files}\n\n    u373_experiments = ExperimentProtocol(base_command, conditions, model_dir_prefix=model_dir_prefix)\n    u373_experiments.generate_commands()\n    u373_experiments.to_file(""./run_U373.sh"")\n\n    base_command = ""python net_segment.py train -c ./demos/unet/HeLa.ini""\n    model_dir_prefix = ""./models/HeLa""\n    # note: these paths are relative to the model directory\n    d_split_files = [""../../demos/unet/hela_d_split_%i.csv"" % i for i in [1, 2]]\n    conditions = {""--do_elastic_deformation"": [""True"", ""False""],\n                  ""--random_flipping_axes"": [""\'0,1\'"", ""-1""],\n                  ""--dataset_split_file"": d_split_files}\n\n    hela_experiments = ExperimentProtocol(base_command, conditions, model_dir_prefix=model_dir_prefix)\n    hela_experiments.generate_commands()\n    hela_experiments.to_file(""./run_hela.sh"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
demos/unet/make_cell_weights.py,0,"b'import os\nimport numpy as np\nimport matplotlib.pyplot as plt  # for visualising and debugging\nfrom scipy.ndimage.morphology import distance_transform_edt\nfrom skimage.io import imsave, imread\nfrom skimage.segmentation import find_boundaries\nfrom demos.unet.file_sorter import get_user_args\n\nW_0, SIGMA = 10, 5\n\n\ndef construct_weights_and_mask(img):\n    seg_boundaries = find_boundaries(img, mode=\'inner\')\n\n    bin_img = img > 0\n    # take segmentations, ignore boundaries\n    binary_with_borders = np.bitwise_xor(bin_img, seg_boundaries)\n\n    foreground_weight = 1 - binary_with_borders.sum() / binary_with_borders.size\n    background_weight = 1 - foreground_weight\n\n    # build euclidean distances maps for each cell:\n    cell_ids = [x for x in np.unique(img) if x > 0]\n    distances = np.zeros((img.shape[0], img.shape[1], len(cell_ids)))\n\n    for i, cell_id in enumerate(cell_ids):\n        distances[..., i] = distance_transform_edt(img != cell_id)\n\n    # we need to look at the two smallest distances\n    distances.sort(axis=-1)\n\n    weight_map = W_0 * np.exp(-(1 / (2 * SIGMA ** 2)) * ((distances[..., 0] + distances[..., 1]) ** 2))\n    weight_map[binary_with_borders] = foreground_weight\n    weight_map[~binary_with_borders] += background_weight\n\n    return weight_map, binary_with_borders\n\n\ndef main():\n    args = get_user_args()\n    for experiment_name in args.experiment_names:\n        file_dir = os.path.join(args.file_dir, experiment_name, \'niftynet_data\')\n        for f_name in [f for f in os.listdir(file_dir) if f.startswith(\'seg\') and f.endswith(\'.tif\')]:\n            img = imread(os.path.join(file_dir, f_name))\n            weight_map, binary_seg = construct_weights_and_mask(img)\n            imsave(os.path.join(file_dir, f_name).replace(\'seg\', \'weight\'), weight_map)\n            imsave(os.path.join(file_dir, f_name).replace(\'seg\', \'bin_seg\'), binary_seg.astype(np.uint8))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
demos/unet/unet_demo_utils.py,0,"b'import os\nimport nibabel as nib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nimport re\n\n\ndef dice_score(gt, img):\n    """"""Calculate the dice score for evaluation purposes""""""\n    gt, img = [x > 0 for x in (gt, img)]\n    num = 2 * np.sum(gt & img)\n    den = gt.sum() + img.sum()\n    return num / den\n\n\ndef results(ground_truths, est_dirs):\n    """"""Collates the dice scores from various experiments""""""\n    result = {e: [] for e in est_dirs}\n    result[\'ids\'] = []\n    for f in ground_truths:\n        r = re.search(\'.*bin_seg_(.*_\\d+)\', f)\n        if r:\n            gt = imread(f)\n            subj_id = r.group(1)\n            result[\'ids\'].append(subj_id)\n            for exp_name in est_dirs:\n                est_path = os.path.join(est_dirs[exp_name], subj_id + \'_niftynet_out.nii.gz\')\n                est = nib.load(est_path).get_data().squeeze()\n                result[exp_name].append(dice_score(gt, est))\n\n    df = pd.DataFrame(result)\n    return df\n\n\ndef results_long(df, csv_name):\n    """"""Labels the results as from train or validation datasets""""""\n    d_split = pd.read_csv(csv_name)\n    d_split.columns = (\'ids\', \'fold\')\n    merged_df = pd.merge(df, d_split)\n    df_long = pd.melt(merged_df, id_vars=[\'ids\', \'fold\'])\n    return df_long\n\n\ndef add_experiment_info_to_datasets(df, est_dirs):\n    """"""adds the experimental information from the training settings""""""\n    experiment_numbers, flipping, dataset_splits, deforming = [], [], [], []\n\n    for est_dir_key in est_dirs:\n        # getting the dataset_split file from the settings_train txt file:\n        train_settings = \' \'.join([l.strip() for l in open(est_dirs[est_dir_key] + \'../settings_train.txt\', \'r\')])\n\n        experiment_numbers.append(est_dir_key)\n\n        r = re.search(\'dataset_split_file:\\s.*(\\d).csv\', train_settings)\n        dataset_splits.append(r.group(1))\n\n        r = re.search(\'flipping_axes:\\s\\((.*?)\\)\', train_settings)\n        flip = \'False\' if \'-1\' in r.group(1) else \'True\'\n        flipping.append(flip)\n\n        r = re.search(\'elastic_deformation:\\s(\\w+)\', train_settings)\n        deforming.append(r.group(1))\n\n    data_dict = {\'variable\': experiment_numbers,\n                 \'flip\': flipping,\n                 \'deform\': deforming,\n                 \'train_split\': dataset_splits,\n                 \'augmentations\': [\'_\'.join([\'flip\', x[0], \'def\', y[0]]) for x, y in zip(flipping, deforming)]\n                 }\n\n    conditions_df = pd.DataFrame(data_dict)\n    combined_df = pd.merge(df, conditions_df)\n\n    return combined_df\n\n\ndef get_and_plot_results(ground_truths, est_dirs, subj_ids):\n    df = None\n    for est_dir_key in est_dirs:\n\n        # getting the dataset_split file from the settings_train txt file:\n        train_settings = [l.strip() for l in open(est_dirs[est_dir_key] + \'../settings_train.txt\', \'r\')]\n        dataset_split_file = [x.split(\':\')[1].strip() for x in train_settings if \'dataset_split\' in x][0]\n\n        new_df = results(ground_truths, {est_dir_key: est_dirs[est_dir_key]})\n        new_df_long = results_long(new_df, dataset_split_file)\n\n        f, axes = plt.subplots(2, 1, figsize=(9, 5))\n        f.suptitle(""Experiment %s"" % est_dir_key)\n        show_model_outputs(ground_truths, new_df_long, {est_dir_key: est_dirs[est_dir_key]}, subj_ids, axes)\n\n        if df is None:\n            df = new_df_long\n        else:\n            df = pd.concat([df, new_df_long])\n\n    combined_df = add_experiment_info_to_datasets(df, est_dirs)\n    return combined_df\n\n\ndef show_model_outputs(ground_truths, df, est_dirs, subj_ids, axes):\n    """"""Plots the results for visualisation""""""\n    for est_dir in est_dirs.values():\n        for i, sid in enumerate(subj_ids):\n            a = imread([f for f in ground_truths if sid in f][0])\n            b = nib.load(est_dir + \'/\' + sid + \'_niftynet_out.nii.gz\').get_data().squeeze()\n\n            axes[i].imshow(np.hstack([a, b, a - b]), cmap=\'gray\')\n            axes[i].set_axis_off()\n\n            train_or_val = df[df[\'ids\'] == sid][\'fold\'].values[0]\n            axes[i].set_title(\'{} Fold: Ground truth, Estimate and Difference. Dice Score = {:.2f}\'.format(\n                train_or_val, dice_score(a, b)))\n'"
doc/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# NiftyNet documentation build configuration file, created by\n# sphinx-quickstart on Wed Aug 30 14:13:50 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport subprocess\nimport os\nimport sys\nfrom recommonmark.parser import CommonMarkParser\n#import pip\n\n# sphinx readable theme\n# pip.main([\'install\', \'sphinx-readable-theme\'])\n# import sphinx_readable_theme as sp_theme\n\n# alternatively sphinx nameko theme\n# pip.main([\'install\', \'sphinx-nameko-theme\'])\nimport sphinx_nameko_theme as sp_theme\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\nworking_dir = os.path.abspath(os.path.dirname(__file__))\nroot_dir_rel = os.path.join(\'..\', \'..\')\nroot_dir_abs = os.path.abspath(root_dir_rel)\nmodule_path = root_dir_abs\nsys.path.insert(0, module_path)\nlogo_file = \'niftynet-logo.png\'\nstatic_images_folder = \'images\'\nlogo_path = os.path.join(\'..\', \'..\', logo_file)\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\n    \'tests\',\n    \'run_*\',\n    \'net_*\',\n    \'setup.py\',\n]\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nstatic_folder = \'_static\'\nhtml_static_path = [static_folder]\n\n\ndef generate_apidocs(*args):\n    """"""Generate API docs automatically by trawling the available modules""""""\n    global working_dir, module_path\n    output_path = working_dir\n    apidoc_command_path = \'sphinx-apidoc\'\n    if hasattr(sys, \'real_prefix\'):  # called from a virtualenv\n        apidoc_command_path = os.path.join(sys.prefix, \'bin\', \'sphinx-apidoc\')\n        apidoc_command_path = os.path.abspath(apidoc_command_path)\n    subprocess.check_call(\n        [apidoc_command_path, \'-f\', \'--separate\'] +\n        [\'-o\', output_path, module_path] +\n        [os.path.join(root_dir_abs, pattern) for pattern in exclude_patterns])\n\n\ndef setup(app):\n    # Hook to allow for automatic generation of API docs\n    # before doc deployment begins.\n    app.connect(\'builder-inited\', generate_apidocs)\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.imgmath\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx_markdown_tables\',\n]\nautodoc_default_flags = [\'members\', \'show-inheritance\']\nautodoc_member_order = \'bysource\'\n\n# Don\'t display module names before objects titles, it\'s more readable.\nadd_module_names = False\n\n# napoleon config ------------\nnapoleon_google_docstring = True\nnapoleon_include_init_with_doc = True\n# -------------------------\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_parsers = {\n    # \'.md\': CommonMarkParser\n    \'.md\': \'recommonmark.parser.CommonMarkParser\',\n}\nsource_suffix = [\'.rst\', \'.md\']\n#source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# This is for instance for niftynet.engine to be shown under the letter \'e\'\n# rather than \'n\'. That way the Python module index is displayed more nicely.\nmodindex_common_prefix = [\n    \'niftynet.\'\n]\n\n# General information about the project.\nproject = u\'NiftyNet\'\ncopyright = u\'2018, the NiftyNet Consortium\'\nauthor = u\'the NiftyNet Consortium\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\nfrom niftynet.utilities.versioning import get_niftynet_version\ntry:\n    # The short X.Y version.\n    version = get_niftynet_version().split(\'+\')[0]\nexcept (IndexError, AttributeError):\n    version = u\'\'\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n## The theme to use for HTML and HTML Help pages.  See the documentation for\n## a list of builtin themes.\n##\n#html_theme = \'classic\'\n#\n## Theme options are theme-specific and customize the look and feel of a theme\n## further.  For a list of options available for each theme, see the\n## documentation.\n##\nnn_black = \'#0A0A0A\'\nnn_red = \'#CC0000\'\nnn_gray = \'#DDDDDD\'\nnn_dark_red = \'#AE1111\'\nnn_white = \'#FEFEFE\'\n#html_theme_options = {\n#    \'footerbgcolor\': nn_gray,\n#    \'footertextcolor\': nn_black,\n#    \'sidebarbgcolor\': nn_white,\n#    \'sidebartextcolor\': nn_black,\n#    \'sidebarlinkcolor\': nn_red,\n#    \'relbarbgcolor\': nn_white,\n#    \'relbartextcolor\': nn_black,\n#    \'relbarlinkcolor\': nn_red,\n#    \'bgcolor\': nn_white,\n#    \'textcolor\': nn_black,\n#    \'linkcolor\': nn_red,\n#    \'visitedlinkcolor\': nn_dark_red,\n#    \'headbgcolor\': nn_white,\n#    \'headtextcolor\': nn_black,\n#    \'headlinkcolor\': nn_red,\n#    \'codebgcolor\': nn_gray,\n#    \'codetextcolor\': nn_black,\n#    \'stickysidebar\': \'true\',\n#}\n#\nhtml_logo = logo_path\nhtml_theme_path = [sp_theme.get_html_theme_path()]\n#html_theme = \'readable\'\nhtml_theme = \'nameko\'\nhtml_sidebars = {\n   \'**\': [\'badge.html\', \'localtoc.html\', \'globaltoc.html\', \'searchbox.html\']\n}\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NiftyNetdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NiftyNet.tex\', u\'NiftyNet Documentation\',\n     u\'NiftyNet Consortium\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'niftynet\', u\'NiftyNet Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NiftyNet\', u\'NiftyNet Documentation\',\n     author, \'NiftyNet\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
niftynet/application/__init__.py,0,"b'""""""\n\n.. module:: niftynet.application\n   :synopsis: Specific high-level NiftyNet applications.\n\n""""""\n'"
niftynet/application/autoencoder_application.py,15,"b'import tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import ApplicationNetFactory\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.application_variables import NETWORK_OUTPUT\nfrom niftynet.engine.application_variables import TF_SUMMARIES\nfrom niftynet.engine.sampler_linear_interpolate_v2 import LinearInterpolateSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.windows_aggregator_identity import WindowAsImageAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.loss_autoencoder import LossFunction\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_INPUT = set([\'image\', \'feature\'])\nSUPPORTED_INFERENCE = \\\n    set([\'encode\', \'encode-decode\', \'sample\', \'linear_interpolation\'])\n\n\nclass AutoencoderApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""AUTOENCODER""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting autoencoder application\')\n\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.autoencoder_param = None\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        self.data_param = data_param\n        self.autoencoder_param = task_param\n\n        if not self.is_training:\n            self._infer_type = look_up_operations(\n                self.autoencoder_param.inference_type, SUPPORTED_INFERENCE)\n        else:\n            self._infer_type = None\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        # read each line of csv files into an instance of Subject\n        if self.is_evaluation:\n            NotImplementedError(\'Evaluation is not yet \'\n                                \'supported in this application.\')\n        if self.is_training:\n            self.readers = []\n            for file_list in file_lists:\n                reader = ImageReader([\'image\'])\n                reader.initialise(data_param, task_param, file_list)\n                self.readers.append(reader)\n        if self._infer_type in (\'encode\', \'encode-decode\'):\n            self.readers = [ImageReader([\'image\'])]\n            self.readers[0].initialise(data_param, task_param, file_lists[0])\n        elif self._infer_type == \'sample\':\n            self.readers = []\n        elif self._infer_type == \'linear_interpolation\':\n            self.readers = [ImageReader([\'feature\'])]\n            self.readers[0].initialise(data_param, task_param, file_lists[0])\n        # if self.is_training or self._infer_type in (\'encode\', \'encode-decode\'):\n        #    mean_var_normaliser = MeanVarNormalisationLayer(image_name=\'image\')\n        #    self.reader.add_preprocessing_layers([mean_var_normaliser])\n\n    def initialise_sampler(self):\n        self.sampler = []\n        if self.is_training:\n            self.sampler.append([ResizeSampler(\n                reader=reader,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=1,\n                shuffle=True,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n            return\n        if self._infer_type in (\'encode\', \'encode-decode\'):\n            self.sampler.append([ResizeSampler(\n                reader=reader,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=1,\n                shuffle=False,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n            return\n        if self._infer_type == \'linear_interpolation\':\n            self.sampler.append([LinearInterpolateSampler(\n                reader=reader,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                n_interpolations=self.autoencoder_param.n_interpolations,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n            return\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            self.patience = self.action_param.patience\n            self.mode = self.action_param.early_stopping_mode\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(True),\n                                    lambda: switch_sampler(False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_output = self.net(image, is_training=self.is_training)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n\n            loss_func = LossFunction(loss_type=self.action_param.loss_type)\n            data_loss = loss_func(net_output)\n            loss = data_loss\n            if self.net_param.decay > 0.0:\n                reg_losses = tf.get_collection(\n                    tf.GraphKeys.REGULARIZATION_LOSSES)\n                if reg_losses:\n                    reg_loss = tf.reduce_mean(\n                        [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                    loss = loss + reg_loss\n\n            self.total_loss = loss\n            grads = self.optimiser.compute_gradients(\n                loss, colocate_gradients_with_ops=True)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'variational_lower_bound\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'variational_lower_bound\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            outputs_collector.add_to_collection(\n                var=net_output[4], name=\'Originals\',\n                average_over_devices=False, summary_type=\'image3_coronal\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=net_output[2], name=\'Means\',\n                average_over_devices=False, summary_type=\'image3_coronal\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=net_output[5], name=\'Variances\',\n                average_over_devices=False, summary_type=\'image3_coronal\',\n                collection=TF_SUMMARIES)\n        else:\n            if self._infer_type in (\'encode\', \'encode-decode\'):\n                data_dict = self.get_sampler()[0][0].pop_batch_op()\n                image = tf.cast(data_dict[\'image\'], dtype=tf.float32)\n                net_output = self.net(image, is_training=False)\n\n                outputs_collector.add_to_collection(\n                    var=data_dict[\'image_location\'], name=\'location\',\n                    average_over_devices=True, collection=NETWORK_OUTPUT)\n\n                if self._infer_type == \'encode-decode\':\n                    outputs_collector.add_to_collection(\n                        var=net_output[2], name=\'generated_image\',\n                        average_over_devices=True, collection=NETWORK_OUTPUT)\n                if self._infer_type == \'encode\':\n                    outputs_collector.add_to_collection(\n                        var=net_output[7], name=\'embedded\',\n                        average_over_devices=True, collection=NETWORK_OUTPUT)\n\n                self.output_decoder = WindowAsImageAggregator(\n                    image_reader=self.readers[0],\n                    output_path=self.action_param.save_seg_dir)\n                return\n            elif self._infer_type == \'sample\':\n                image_size = (self.net_param.batch_size,) + \\\n                             self.action_param.spatial_window_size + (1,)\n                dummy_image = tf.zeros(image_size)\n                net_output = self.net(dummy_image, is_training=False)\n                noise_shape = net_output[-1].shape.as_list()\n                stddev = self.autoencoder_param.noise_stddev\n                noise = tf.random_normal(shape=noise_shape,\n                                         mean=0.0,\n                                         stddev=stddev,\n                                         dtype=tf.float32)\n                partially_decoded_sample = self.net.shared_decoder(\n                    noise, is_training=False)\n                decoder_output = self.net.decoder_means(\n                    partially_decoded_sample, is_training=False)\n\n                outputs_collector.add_to_collection(\n                    var=decoder_output, name=\'generated_image\',\n                    average_over_devices=True, collection=NETWORK_OUTPUT)\n                self.output_decoder = WindowAsImageAggregator(\n                    image_reader=None,\n                    output_path=self.action_param.save_seg_dir)\n                return\n            elif self._infer_type == \'linear_interpolation\':\n                # construct the entire network\n                image_size = (self.net_param.batch_size,) + \\\n                             self.action_param.spatial_window_size + (1,)\n                dummy_image = tf.zeros(image_size)\n                net_output = self.net(dummy_image, is_training=False)\n                data_dict = self.get_sampler()[0][0].pop_batch_op()\n                real_code = data_dict[\'feature\']\n                real_code = tf.reshape(real_code, net_output[-1].get_shape())\n                partially_decoded_sample = self.net.shared_decoder(\n                    real_code, is_training=False)\n                decoder_output = self.net.decoder_means(\n                    partially_decoded_sample, is_training=False)\n\n                outputs_collector.add_to_collection(\n                    var=decoder_output, name=\'generated_image\',\n                    average_over_devices=True, collection=NETWORK_OUTPUT)\n                outputs_collector.add_to_collection(\n                    var=data_dict[\'feature_location\'], name=\'location\',\n                    average_over_devices=True, collection=NETWORK_OUTPUT)\n                self.output_decoder = WindowAsImageAggregator(\n                    image_reader=self.readers[0],\n                    output_path=self.action_param.save_seg_dir)\n            else:\n                raise NotImplementedError\n\n    def interpret_output(self, batch_output):\n        if self.is_training:\n            return True\n        else:\n            infer_type = look_up_operations(\n                self.autoencoder_param.inference_type,\n                SUPPORTED_INFERENCE)\n            if infer_type == \'encode\':\n                return self.output_decoder.decode_batch(\n                    {\'window_embedded\':batch_output[\'embedded\']},\n                    batch_output[\'location\'][:, 0:1])\n            if infer_type == \'encode-decode\':\n                return self.output_decoder.decode_batch(\n                    {\'window_generated_image\':batch_output[\'generated_image\']},\n                    batch_output[\'location\'][:, 0:1])\n            if infer_type == \'sample\':\n                return self.output_decoder.decode_batch(\n                    {\'window_generated_image\':batch_output[\'generated_image\']},\n                    None)\n            if infer_type == \'linear_interpolation\':\n                return self.output_decoder.decode_batch(\n                    {\'window_generated_image\':batch_output[\'generated_image\']},\n                    batch_output[\'location\'][:, :2])\n'"
niftynet/application/base_application.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nInterface of NiftyNet application\n""""""\nimport os\nfrom argparse import Namespace\n\nimport tensorflow as tf\nfrom six import with_metaclass\n\nfrom niftynet.engine.signal import TRAIN, INFER, EVAL\n\nAPP_INSTANCE = None  # global so it can be reset\n\n\n# pylint: disable=global-statement\nclass SingletonApplication(type):\n    """"""\n    Maintaining a global application instance.\n    """"""\n    def __call__(cls, *args, **kwargs):\n        global APP_INSTANCE\n        if APP_INSTANCE is None:\n            APP_INSTANCE = \\\n                super(SingletonApplication, cls).__call__(*args, **kwargs)\n        # else:\n        #     raise RuntimeError(\'application instance already started.\')\n        return APP_INSTANCE\n\n    @classmethod\n    def clear(mcs):\n        """"""\n        Remove the instance.\n        :return:\n        """"""\n        global APP_INSTANCE\n        APP_INSTANCE = None\n\n\nclass BaseApplication(with_metaclass(SingletonApplication, object)):\n    """"""\n    BaseApplication represents an interface.\n\n    Each application ``type_str`` should support to use\n    the standard training and inference driver.\n    """"""\n\n    # defines name of the customised configuration file section\n    # the section collects all application specific user parameters\n    REQUIRED_CONFIG_SECTION = None\n\n    SUPPORTED_PHASES = {TRAIN, INFER, EVAL}\n    _action = TRAIN\n    # TF placeholders for switching network on the fly\n    is_validation = None\n\n    # input of the network\n    readers = None\n    sampler = None\n\n    # the network\n    net = None\n\n    # training the network\n    optimiser = None\n    gradient_op = None\n\n    # interpret network output\n    output_decoder = None\n    outputs_collector = None\n    gradients_collector = None\n\n    # performance\n    total_loss = None\n    patience = None\n    performance_history = []\n    mode = None\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        """"""\n        this function initialise self.readers\n\n        :param data_param: input modality specifications\n        :param task_param: contains task keywords for grouping data_param\n        :param data_partitioner:\n                           specifies train/valid/infer splitting if needed\n        :return:\n        """"""\n        raise NotImplementedError\n\n    def initialise_sampler(self):\n        """"""\n        Samplers take ``self.reader`` as input and generates\n        sequences of ImageWindow that will be fed to the networks\n\n        This function sets ``self.sampler``.\n        """"""\n        raise NotImplementedError\n\n    def initialise_network(self):\n        """"""\n        This function create an instance of network and sets ``self.net``\n\n        :return: None\n        """"""\n        raise NotImplementedError\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        """"""\n        Adding sampler output tensor and network tensors to the graph.\n\n        :param outputs_collector:\n        :param gradients_collector:\n        :return:\n        """"""\n        raise NotImplementedError\n\n    def interpret_output(self, batch_output):\n        """"""\n        Implement output interpretations, e.g., save to hard drive\n        cache output windows.\n\n        :param batch_output: outputs by running the tf graph\n        :return: True indicates the driver should continue the loop\n            False indicates the drive should stop\n        """"""\n        raise NotImplementedError\n\n    def add_inferred_output_like(self, data_param, task_param, name):\n        """""" This function adds entries to parameter objects to enable\n        the evaluation action to automatically read in the output of a\n        previous inference run if inference is not explicitly specified.\n\n        This can be used in an application if there is a data section\n        entry in the configuration file that matches the inference output.\n        In supervised learning, the reference data section would often\n        match the inference output and could be used here. Otherwise,\n        a template data section could be used.\n\n        :param data_param:\n        :param task_param:\n        :param name:  name of input parameter to copy parameters from\n        :return: modified data_param and task_param\n        """"""\n        print(task_param)\n        # Add the data parameter\n        if \'inferred\' not in data_param:\n            data_name = vars(task_param)[name][0]\n            inferred_param = Namespace(**vars(data_param[data_name]))\n            inferred_param.csv_file = os.path.join(\n                self.action_param.save_seg_dir, \'inferred.csv\')\n            data_param[\'inferred\'] = inferred_param\n        # Add the task parameter\n        if \'inferred\' not in task_param or not task_param.inferred:\n            task_param.inferred = (\'inferred\',)\n        return data_param, task_param\n\n    def set_iteration_update(self, iteration_message):\n        """"""\n        At each iteration ``application_driver`` calls::\n\n            output = tf.session.run(variables_to_eval, feed_dict=data_dict)\n\n        to evaluate TF graph elements, where\n        ``variables_to_eval`` and ``data_dict`` are retrieved from\n        ``iteration_message.ops_to_run`` and\n        ``iteration_message.data_feed_dict``\n         (In addition to the variables collected by self.output_collector).\n\n        The output of `tf.session.run(...)` will be stored at\n        ``iteration_message.current_iter_output``, and can be accessed\n        from ``engine.handler_network_output.OutputInterpreter``.\n\n        override this function for more complex operations\n        (such as learning rate decay) according to\n        ``iteration_message.current_iter``.\n        """"""\n        if iteration_message.is_training:\n            iteration_message.data_feed_dict[self.is_validation] = False\n        elif iteration_message.is_validation:\n            iteration_message.data_feed_dict[self.is_validation] = True\n\n    def get_sampler(self):\n        """"""\n        Get samplers of the application\n\n        :return: ``niftynet.engine.sampler_*`` instances\n        """"""\n        return self.sampler\n\n    def add_validation_flag(self):\n        """"""\n        Add a TF placeholder for switching between train/valid graphs,\n        this function sets ``self.is_validation``. ``self.is_validation``\n        can be used by applications.\n\n        :return:\n        """"""\n        self.is_validation = \\\n            tf.placeholder_with_default(False, [], \'is_validation\')\n\n    @property\n    def action(self):\n        """"""\n        A string indicating the action in train/inference/evaluation\n\n        :return:\n        """"""\n        return self._action\n\n    @action.setter\n    def action(self, value):\n        """"""\n        The action should have at least two characters matching\n        the one of the phase string TRAIN, INFER, EVAL\n\n        :param value:\n        :return:\n        """"""\n        try:\n            self._action = value.lower()\n            assert len(self._action) >= 2\n        except (AttributeError, AssertionError):\n            tf.logging.fatal(\'Error setting application action: %s\', value)\n\n    @property\n    def is_training(self):\n        """"""\n\n        :return: boolean value indicating if the phase is training\n        """"""\n        return TRAIN.startswith(self.action)\n\n    @property\n    def is_inference(self):\n        """"""\n\n        :return: boolean value indicating if the phase is inference\n        """"""\n        return INFER.startswith(self.action)\n\n    @property\n    def is_evaluation(self):\n        """"""\n\n        :return: boolean value indicating if the action is evaluation\n        """"""\n        return EVAL.startswith(self.action)\n\n    @staticmethod\n    def stop():\n        """"""\n        remove application instance if there\'s any.\n\n        :return:\n        """"""\n        SingletonApplication.clear()\n'"
niftynet/application/classification_application.py,17,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines an image-level classification application\nthat maps from images to scalar, multi-class labels.\n\nThis class is instantiated and initalized by the application_driver.\n""""""\n\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.loss_classification import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.evaluation.classification_evaluator import ClassificationEvaluator\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'sampler\', \'inferred\'])\n\n\nclass ClassificationApplication(BaseApplication):\n    """"""This class defines an application for image-level classification\n    problems mapping from images to scalar labels.\n\n    This is the application class to be instantiated by the driver\n    and referred to in configuration files.\n\n    Although structurally similar to segmentation, this application\n    supports different samplers/aggregators (because patch-based\n    processing is not appropriate), and monitoring metrics.""""""\n\n    REQUIRED_CONFIG_SECTION = ""CLASSIFICATION""\n\n    def __init__(self, net_param, action_param, action):\n        super(ClassificationApplication, self).__init__()\n        tf.logging.info(\'starting classification application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.classification_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.classification_param = task_param\n\n        if self.is_training:\n            reader_names = (\'image\', \'label\', \'sampler\')\n        elif self.is_inference:\n            reader_names = (\'image\',)\n        elif self.is_evaluation:\n            reader_names = (\'image\', \'label\', \'inferred\')\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=self.net_param.foreground_type,\n            multimod_fusion=self.net_param.multimod_foreground_type,\n            threshold=0.0) \\\n            if self.net_param.normalise_foreground_only else None\n\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer) \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            binary_masking_func=foreground_masking_layer,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n\n        label_normaliser = DiscreteLabelNormalisationLayer(\n            image_name=\'label\',\n            modalities=vars(task_param).get(\'label\'),\n            model_filename=self.net_param.histogram_ref_file) \\\n            if (self.net_param.histogram_ref_file and\n                task_param.label_normalisation) else None\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n        if label_normaliser is not None:\n            normalisation_layers.append(label_normaliser)\n\n        augmentation_layers = []\n        if self.is_training:\n            train_param = self.action_param\n            if train_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=train_param.random_flipping_axes))\n            if train_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=train_param.scaling_percentage[0],\n                    max_percentage=train_param.scaling_percentage[1],\n                    antialiasing=train_param.antialiasing,\n                    isotropic=train_param.isotropic_scaling))\n            if train_param.rotation_angle or \\\n                    self.action_param.rotation_angle_x or \\\n                    self.action_param.rotation_angle_y or \\\n                    self.action_param.rotation_angle_z:\n                rotation_layer = RandomRotationLayer()\n                if train_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        train_param.rotation_angle)\n                else:\n                    rotation_layer.init_non_uniform_angle(\n                        self.action_param.rotation_angle_x,\n                        self.action_param.rotation_angle_y,\n                        self.action_param.rotation_angle_z)\n                augmentation_layers.append(rotation_layer)\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n             normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(normalisation_layers)\n\n        # Checking num_classes is set correctly\n        if self.classification_param.num_classes <= 1:\n            raise ValueError(""Number of classes must be at least 2 for classification"")\n        for preprocessor in self.readers[0].preprocessors:\n            if preprocessor.name == \'label_norm\':\n                if len(preprocessor.label_map[preprocessor.key[0]]) != self.classification_param.num_classes:\n                    raise ValueError(""Number of unique labels must be equal to ""\n                                     ""number of classes (check histogram_ref file)"")\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_aggregator(self):\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        else:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=self.classification_param.num_classes,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def add_confusion_matrix_summaries_(self,\n                                        outputs_collector,\n                                        net_out,\n                                        data_dict):\n        """""" This method defines several monitoring metrics that\n        are derived from the confusion matrix """"""\n        labels = tf.reshape(tf.cast(data_dict[\'label\'], tf.int64), [-1])\n        prediction = tf.reshape(tf.argmax(net_out, -1), [-1])\n        num_classes = self.classification_param.num_classes\n        conf_mat = tf.contrib.metrics.confusion_matrix(labels, prediction, num_classes)\n        conf_mat = tf.to_float(conf_mat)\n        if self.classification_param.num_classes == 2:\n            outputs_collector.add_to_collection(\n                var=conf_mat[1][1], name=\'true_positives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[1][0], name=\'false_negatives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[0][1], name=\'false_positives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[0][0], name=\'true_negatives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n        else:\n            outputs_collector.add_to_collection(\n                var=conf_mat[tf.newaxis, :, :, tf.newaxis],\n                name=\'confusion_matrix\',\n                average_over_devices=True, summary_type=\'image\',\n                collection=TF_SUMMARIES)\n\n        outputs_collector.add_to_collection(\n            var=tf.trace(conf_mat), name=\'accuracy\',\n            average_over_devices=True, summary_type=\'scalar\',\n            collection=TF_SUMMARIES)\n\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            self.patience = self.action_param.patience\n            self.mode = self.action_param.early_stopping_mode\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(\n                n_class=self.classification_param.num_classes,\n                loss_type=self.action_param.loss_type)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None))\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n\n            self.total_loss = loss\n\n            grads = self.optimiser.compute_gradients(\n                loss, colocate_gradients_with_ops=True)\n\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'data_loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'data_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            self.add_confusion_matrix_summaries_(outputs_collector,\n                                                 net_out,\n                                                 data_dict)\n        else:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n            tf.logging.info(\n                \'net_out.shape may need to be resized: %s\', net_out.shape)\n            output_prob = self.classification_param.output_prob\n            num_classes = self.classification_param.num_classes\n            if output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'SOFTMAX\', num_classes=num_classes)\n            elif not output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'ARGMAX\', num_classes=num_classes)\n            else:\n                post_process_layer = PostProcessingLayer(\n                    \'IDENTITY\', num_classes=num_classes)\n            net_out = post_process_layer(net_out)\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        if not self.is_training:\n            return self.output_decoder.decode_batch(\n                {\'csv\': batch_output[\'window\']},\n                batch_output[\'location\'])\n        return True\n\n    def initialise_evaluator(self, eval_param):\n        self.eval_param = eval_param\n        self.evaluator = ClassificationEvaluator(self.readers[0],\n                                                 self.classification_param,\n                                                 eval_param)\n\n    def add_inferred_output(self, data_param, task_param):\n        return self.add_inferred_output_like(data_param, task_param, \'label\')\n'"
niftynet/application/gan_application.py,19,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import ApplicationNetFactory\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_random_vector_v2 import RandomVectorSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.windows_aggregator_identity import WindowAsImageAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_gan import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\n\nSUPPORTED_INPUT = set([\'image\', \'conditioning\'])\n\n\nclass GANApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""GAN""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting GAN application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.gan_param = None\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        self.data_param = data_param\n        self.gan_param = task_param\n\n        if self.is_training:\n            reader_names = (\'image\', \'conditioning\')\n        elif self.is_inference:\n            # in the inference process use `conditioning` input only\n            reader_names = (\'conditioning\',)\n        elif self.is_evaluation:\n            tf.logging.fatal(\n                \'Evaluation is not yet supported in this application.\')\n            raise NotImplementedError\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        # initialise input preprocessing layers\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=self.net_param.foreground_type,\n            multimod_fusion=self.net_param.multimod_foreground_type,\n            threshold=0.0) \\\n            if self.net_param.normalise_foreground_only else None\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer) \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            binary_masking_func=foreground_masking_layer,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n\n        # initialise training data augmentation layers\n        augmentation_layers = []\n        if self.is_training:\n            if self.action_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=self.action_param.random_flipping_axes))\n            if self.action_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=self.action_param.scaling_percentage[0],\n                    max_percentage=self.action_param.scaling_percentage[1],\n                    antialiasing=self.action_param.antialiasing,\n                    isotropic=self.action_param.isotropic_scaling))\n            if self.action_param.rotation_angle:\n                augmentation_layers.append(RandomRotationLayer())\n                augmentation_layers[-1].init_uniform_angle(\n                    self.action_param.rotation_angle)\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n            normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(normalisation_layers)\n\n    def initialise_sampler(self):\n        self.sampler = []\n        if self.is_training:\n            self.sampler.append([ResizeSampler(\n                reader=reader,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=1,\n                shuffle=True,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n        else:\n            self.sampler.append([RandomVectorSampler(\n                names=(\'vector\',),\n                vector_size=(self.gan_param.noise_size,),\n                batch_size=self.net_param.batch_size,\n                n_interpolations=self.gan_param.n_interpolations,\n                repeat=None,\n                queue_length=self.net_param.queue_length) for _ in\n                self.readers])\n            # repeat each resized image n times, so that each\n            # image matches one random vector,\n            # (n = self.gan_param.n_interpolations)\n            self.sampler.append([ResizeSampler(\n                reader=reader,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=self.gan_param.n_interpolations,\n                shuffle=False,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n\n    def initialise_network(self):\n        self.net = ApplicationNetFactory.create(self.net_param.name)()\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        if self.is_training:\n            self.patience = self.action_param.patience\n            self.mode = self.action_param.early_stopping_mode\n            def switch_sampler(for_training):\n                with tf.name_scope(\'train\' if for_training else \'validation\'):\n                    sampler = self.get_sampler()[0][0 if for_training else -1]\n                    return sampler.pop_batch_op()\n\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            images = tf.cast(data_dict[\'image\'], tf.float32)\n            noise_shape = [self.net_param.batch_size,\n                           self.gan_param.noise_size]\n            noise = tf.random_normal(shape=noise_shape,\n                                     mean=0.0,\n                                     stddev=1.0,\n                                     dtype=tf.float32)\n            conditioning = data_dict[\'conditioning\']\n            net_output = self.net(\n                noise, images, conditioning, self.is_training)\n\n            loss_func = LossFunction(\n                loss_type=self.action_param.loss_type)\n            real_logits = net_output[1]\n            fake_logits = net_output[2]\n            lossG, lossD = loss_func(real_logits, fake_logits)\n            if self.net_param.decay > 0:\n                reg_losses = tf.get_collection(\n                    tf.GraphKeys.REGULARIZATION_LOSSES)\n                if reg_losses:\n                    reg_loss = tf.reduce_mean(\n                        [tf.reduce_mean(l_reg) for l_reg in reg_losses])\n                    lossD = lossD + reg_loss\n                    lossG = lossG + reg_loss\n\n            self.total_loss = lossD + lossG\n\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            # variables to display in STDOUT\n            outputs_collector.add_to_collection(\n                var=lossD, name=\'lossD\', average_over_devices=True,\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=lossG, name=\'lossG\', average_over_devices=False,\n                collection=CONSOLE)\n            # variables to display in tensorboard\n            outputs_collector.add_to_collection(\n                var=lossD, name=\'lossD\', average_over_devices=True,\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=lossG, name=\'lossG\', average_over_devices=False,\n                collection=TF_SUMMARIES)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n\n            with tf.name_scope(\'ComputeGradients\'):\n                # gradients of generator\n                generator_variables = tf.get_collection(\n                    tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'generator\')\n                generator_grads = self.optimiser.compute_gradients(\n                    lossG,\n                    var_list=generator_variables,\n                    colocate_gradients_with_ops=True)\n\n                # gradients of discriminator\n                discriminator_variables = tf.get_collection(\n                    tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'discriminator\')\n                discriminator_grads = self.optimiser.compute_gradients(\n                    lossD,\n                    var_list=discriminator_variables,\n                    colocate_gradients_with_ops=True)\n                grads = [generator_grads, discriminator_grads]\n\n                # add the grads back to application_driver\'s training_grads\n                gradients_collector.add_to_collection(grads)\n        else:\n            data_dict = self.get_sampler()[0][0].pop_batch_op()\n            conditioning_dict = self.get_sampler()[1][0].pop_batch_op()\n            conditioning = conditioning_dict[\'conditioning\']\n            image_size = conditioning.shape.as_list()[:-1]\n            dummy_image = tf.zeros(image_size + [1])\n            net_output = self.net(data_dict[\'vector\'],\n                                  dummy_image,\n                                  conditioning,\n                                  self.is_training)\n            outputs_collector.add_to_collection(\n                var=net_output[0],\n                name=\'image\',\n                average_over_devices=False,\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=conditioning_dict[\'conditioning_location\'],\n                name=\'location\',\n                average_over_devices=False,\n                collection=NETWORK_OUTPUT)\n\n            self.output_decoder = WindowAsImageAggregator(\n                image_reader=self.readers[0],\n                output_path=self.action_param.save_seg_dir)\n\n    def interpret_output(self, batch_output):\n        if self.is_training:\n            return True\n        return self.output_decoder.decode_batch(\n            {\'window_image\': batch_output[\'image\']},\n            batch_output[\'location\'])\n'"
niftynet/application/label_driven_registration.py,13,"b'""""""\nA preliminary re-implementation of:\n    Hu et al., Weakly-Supervised Convolutional Neural Networks for\n    Multimodal Image Registration, Medical Image Analysis (2018)\n    https://doi.org/10.1016/j.media.2018.07.002\n\nThe original implementation and tutorial is available at:\n    https://github.com/YipengHu/label-reg\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.contrib.sampler_pairwise.sampler_pairwise_uniform import \\\n    PairwiseUniformSampler\nfrom niftynet.contrib.sampler_pairwise.sampler_pairwise_resize import \\\n    PairwiseResizeSampler\nfrom niftynet.engine.application_factory import \\\n    OptimiserFactory, ApplicationNetFactory\nfrom niftynet.engine.application_variables import \\\n    NETWORK_OUTPUT, CONSOLE, TF_SUMMARIES\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\n\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.loss_segmentation import LossFunction\n\n\nSUPPORTED_INPUT = {\'moving_image\', \'moving_label\',\n                   \'fixed_image\', \'fixed_label\'}\n\n\nclass RegApp(BaseApplication):\n\n    REQUIRED_CONFIG_SECTION = ""REGISTRATION""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting label-driven registration\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.registration_param = None\n        self.data_param = None\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        self.data_param = data_param\n        self.registration_param = task_param\n\n        if self.is_evaluation:\n            NotImplementedError(\'Evaluation is not yet \'\n                                \'supported in this application.\')\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n\n        self.readers = []\n        for file_list in file_lists:\n            fixed_reader = ImageReader({\'fixed_image\', \'fixed_label\'})\n            fixed_reader.initialise(data_param, task_param, file_list)\n            self.readers.append(fixed_reader)\n\n            moving_reader = ImageReader({\'moving_image\', \'moving_label\'})\n            moving_reader.initialise(data_param, task_param, file_list)\n            self.readers.append(moving_reader)\n\n        # pad the fixed target only\n        # moving image will be resampled to match the targets\n        #volume_padding_layer = []\n        #if self.net_param.volume_padding_size:\n        #    volume_padding_layer.append(PadLayer(\n        #        image_name=(\'fixed_image\', \'fixed_label\'),\n        #        border=self.net_param.volume_padding_size))\n\n        #for reader in self.readers:\n        #    reader.add_preprocessing_layers(volume_padding_layer)\n\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.sampler = []\n            assert len(self.readers) >= 2, \'at least two readers are required\'\n            training_sampler = PairwiseUniformSampler(\n                reader_0=self.readers[0],\n                reader_1=self.readers[1],\n                data_param=self.data_param,\n                batch_size=self.net_param.batch_size)\n            self.sampler.append(training_sampler)\n            # adding validation readers if possible\n            if len(self.readers) >= 4:\n                validation_sampler = PairwiseUniformSampler(\n                    reader_0=self.readers[2],\n                    reader_1=self.readers[3],\n                    data_param=self.data_param,\n                    batch_size=self.net_param.batch_size)\n                self.sampler.append(validation_sampler)\n        else:\n            self.sampler = PairwiseResizeSampler(\n                reader_0=self.readers[0],\n                reader_1=self.readers[1],\n                data_param=self.data_param,\n                batch_size=self.net_param.batch_size)\n\n    def initialise_network(self):\n        decay = self.net_param.decay\n        self.net = ApplicationNetFactory.create(self.net_param.name)(decay)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_samplers(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0 if for_training else -1]\n                return sampler()  # returns image only\n\n        if self.is_training:\n            self.patience = self.action_param.patience\n            self.mode = self.action_param.early_stopping_mode\n            if self.action_param.validation_every_n > 0:\n                sampler_window = \\\n                    tf.cond(tf.logical_not(self.is_validation),\n                            lambda: switch_samplers(True),\n                            lambda: switch_samplers(False))\n            else:\n                sampler_window = switch_samplers(True)\n\n            image_windows, _ = sampler_window\n            # image_windows, locations = sampler_window\n\n            # decode channels for moving and fixed images\n            image_windows_list = [\n                tf.expand_dims(img, axis=-1)\n                for img in tf.unstack(image_windows, axis=-1)]\n            fixed_image, fixed_label, moving_image, moving_label = \\\n                image_windows_list\n\n            # estimate ddf\n            dense_field = self.net(fixed_image, moving_image)\n            if isinstance(dense_field, tuple):\n                dense_field = dense_field[0]\n\n            # transform the moving labels\n            resampler = ResamplerLayer(\n                interpolation=\'linear\', boundary=\'replicate\')\n            resampled_moving_label = resampler(moving_label, dense_field)\n\n            # compute label loss (foreground only)\n            loss_func = LossFunction(\n                n_class=1,\n                loss_type=self.action_param.loss_type,\n                softmax=False)\n            label_loss = loss_func(prediction=resampled_moving_label,\n                                   ground_truth=fixed_label)\n\n            dice_fg = 1.0 - label_loss\n            # appending regularisation loss\n            total_loss = label_loss\n            reg_loss = tf.get_collection(\'bending_energy\')\n            if reg_loss:\n                total_loss = total_loss + \\\n                    self.net_param.decay * tf.reduce_mean(reg_loss)\n\n            self.total_loss = total_loss\n\n            # compute training gradients\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            grads = self.optimiser.compute_gradients(\n                total_loss, colocate_gradients_with_ops=True)\n            gradients_collector.add_to_collection(grads)\n\n            metrics_dice = loss_func(\n                prediction=tf.to_float(resampled_moving_label >= 0.5),\n                ground_truth=tf.to_float(fixed_label >= 0.5))\n            metrics_dice = 1.0 - metrics_dice\n\n            # command line output\n            outputs_collector.add_to_collection(\n                var=dice_fg, name=\'one_minus_data_loss\',\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=tf.reduce_mean(reg_loss), name=\'bending_energy\',\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=total_loss, name=\'total_loss\', collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=metrics_dice, name=\'ave_fg_dice\', collection=CONSOLE)\n\n            # for tensorboard\n            outputs_collector.add_to_collection(\n                var=dice_fg,\n                name=\'data_loss\',\n                average_over_devices=True,\n                summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=total_loss,\n                name=\'total_loss\',\n                average_over_devices=True,\n                summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=metrics_dice,\n                name=\'averaged_foreground_Dice\',\n                average_over_devices=True,\n                summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            # for visualisation debugging\n            # resampled_moving_image = resampler(moving_image, dense_field)\n            # outputs_collector.add_to_collection(\n            #     var=fixed_image, name=\'fixed_image\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=fixed_label, name=\'fixed_label\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=moving_image, name=\'moving_image\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=moving_label, name=\'moving_label\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=resampled_moving_image, name=\'resampled_image\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=resampled_moving_label, name=\'resampled_label\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=dense_field, name=\'ddf\', collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=locations, name=\'locations\', collection=NETWORK_OUTPUT)\n\n            # outputs_collector.add_to_collection(\n            #     var=shift[0], name=\'a\', collection=CONSOLE)\n            # outputs_collector.add_to_collection(\n            #     var=shift[1], name=\'b\', collection=CONSOLE)\n        else:\n            image_windows, locations = self.sampler()\n            image_windows_list = [\n                tf.expand_dims(img, axis=-1)\n                for img in tf.unstack(image_windows, axis=-1)]\n            fixed_image, fixed_label, moving_image, moving_label = \\\n                image_windows_list\n\n            dense_field = self.net(fixed_image, moving_image)\n            if isinstance(dense_field, tuple):\n                dense_field = dense_field[0]\n\n            # transform the moving labels\n            resampler = ResamplerLayer(\n                interpolation=\'linear\', boundary=\'replicate\')\n            resampled_moving_image = resampler(moving_image, dense_field)\n            resampled_moving_label = resampler(moving_label, dense_field)\n\n            outputs_collector.add_to_collection(\n                var=fixed_image, name=\'fixed_image\',\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=moving_image, name=\'moving_image\',\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=resampled_moving_image,\n                name=\'resampled_moving_image\',\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=resampled_moving_label,\n                name=\'resampled_moving_label\',\n                collection=NETWORK_OUTPUT)\n\n            outputs_collector.add_to_collection(\n                var=fixed_label, name=\'fixed_label\',\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=moving_label, name=\'moving_label\',\n                collection=NETWORK_OUTPUT)\n            #outputs_collector.add_to_collection(\n            #    var=dense_field, name=\'field\',\n            #    collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=locations, name=\'locations\',\n                collection=NETWORK_OUTPUT)\n\n            self.output_decoder = ResizeSamplesAggregator(\n                image_reader=self.readers[0], # fixed image reader\n                name=\'fixed_image\',\n                output_path=self.action_param.save_seg_dir,\n                interp_order=self.action_param.output_interp_order)\n\n    def interpret_output(self, batch_output):\n        if self.is_training:\n            return True\n        return self.output_decoder.decode_batch(\n            {\'window_resampled\':batch_output[\'resampled_moving_image\']},\n            batch_output[\'locations\'])\n\n'"
niftynet/application/regression_application.py,14,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_weighted_v2 import WeightedSampler\nfrom niftynet.engine.sampler_balanced_v2 import BalancedSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.crop import CropLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_regression import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.layer.rgb_histogram_equilisation import \\\n    RGBHistogramEquilisationLayer\nfrom niftynet.evaluation.regression_evaluator import RegressionEvaluator\nfrom niftynet.layer.rand_elastic_deform import RandomElasticDeformationLayer\nfrom niftynet.engine.windows_aggregator_identity import WindowAsImageAggregator\n\nSUPPORTED_INPUT = set([\'image\', \'output\', \'weight\', \'sampler\', \'inferred\'])\n\n\nclass RegressionApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""REGRESSION""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting regression application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.regression_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'uniform\': (self.initialise_uniform_sampler,\n                        self.initialise_grid_sampler,\n                        self.initialise_grid_aggregator),\n            \'weighted\': (self.initialise_weighted_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler,\n                       self.initialise_resize_aggregator),\n            \'balanced\': (self.initialise_balanced_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.regression_param = task_param\n\n        # initialise input image readers\n        if self.is_training:\n            reader_names = (\'image\', \'output\', \'weight\', \'sampler\')\n        elif self.is_inference:\n            # in the inference process use `image` input only\n            reader_names = (\'image\',)\n        elif self.is_evaluation:\n            reader_names = (\'image\', \'output\', \'inferred\')\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        # initialise input preprocessing layers\n        mean_var_normaliser = MeanVarNormalisationLayer(image_name=\'image\') \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n        rgb_normaliser = RGBHistogramEquilisationLayer(\n            image_name=\'image\',\n            name=\'rbg_norm_layer\') if self.net_param.rgb_normalisation else None\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n        if rgb_normaliser is not None:\n            normalisation_layers.append(rgb_normaliser)\n\n        volume_padding_layer = [PadLayer(\n            image_name=SUPPORTED_INPUT,\n            border=self.net_param.volume_padding_size,\n            mode=self.net_param.volume_padding_mode,\n            pad_to=self.net_param.volume_padding_to_size)\n        ]\n\n        # initialise training data augmentation layers\n        augmentation_layers = []\n        if self.is_training:\n            train_param = self.action_param\n            if train_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=train_param.random_flipping_axes))\n            if train_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=train_param.scaling_percentage[0],\n                    max_percentage=train_param.scaling_percentage[1],\n                    antialiasing=train_param.antialiasing,\n                    isotropic=train_param.isotropic_scaling))\n            if train_param.rotation_angle:\n                rotation_layer = RandomRotationLayer()\n                if train_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        train_param.rotation_angle)\n                augmentation_layers.append(rotation_layer)\n            if train_param.do_elastic_deformation:\n                spatial_rank = list(self.readers[0].spatial_ranks.values())[0]\n                augmentation_layers.append(RandomElasticDeformationLayer(\n                    spatial_rank=spatial_rank,\n                    num_controlpoints=train_param.num_ctrl_points,\n                    std_deformation_sigma=train_param.deformation_sigma,\n                    proportion_to_augment=train_param.proportion_to_deform))\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n            volume_padding_layer + normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(\n                volume_padding_layer + normalisation_layers)\n\n    def initialise_uniform_sampler(self):\n        self.sampler = [[UniformSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_weighted_sampler(self):\n        self.sampler = [[WeightedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_sampler(self):\n        self.sampler = [[GridSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_balanced_sampler(self):\n        self.sampler = [[BalancedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_aggregator(self):\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix,\n            fill_constant=self.action_param.fill_constant)\n\n    def initialise_resize_aggregator(self):\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix)\n        \n    def initialise_identity_aggregator(self):\n        self.output_decoder = WindowAsImageAggregator(\n                image_reader=self.readers[0],\n                output_path=self.action_param.save_seg_dir,\n                postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        elif self.is_inference:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_aggregator(self):\n        if self.net_param.force_output_identity_resizing:\n            self.initialise_identity_aggregator()\n        else:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=1,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            self.patience = self.action_param.patience\n            self.mode = self.action_param.early_stopping_mode\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(loss_type=self.action_param.loss_type)\n\n            weight_map = data_dict.get(\'weight\', None)\n            border=self.regression_param.loss_border\n            if border == None or tf.reduce_sum(tf.abs(border)) == 0:\n                data_loss = loss_func(\n                        prediction=net_out,\n                        ground_truth=data_dict[\'output\'],\n                        weight_map=weight_map)\n            else:\n                crop_layer = CropLayer(border)\n                weight_map = None if weight_map is None else crop_layer(weight_map)\n                data_loss = loss_func(\n                        prediction=crop_layer(net_out),\n                        ground_truth=crop_layer(data_dict[\'output\']),\n                        weight_map=weight_map)\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n\n            # Get all vars\n            to_optimise = tf.trainable_variables()\n            vars_to_freeze = \\\n                self.action_param.vars_to_freeze or \\\n                self.action_param.vars_to_restore\n            if vars_to_freeze:\n                import re\n                var_regex = re.compile(vars_to_freeze)\n                # Only optimise vars that are not frozen\n                to_optimise = \\\n                    [v for v in to_optimise if not var_regex.search(v.name)]\n                tf.logging.info(\n                    ""Optimizing %d out of %d trainable variables, ""\n                    ""the other variables are fixed (--vars_to_freeze %s)"",\n                    len(to_optimise),\n                    len(tf.trainable_variables()),\n                    vars_to_freeze)\n\n            self.total_loss = loss\n\n            grads = self.optimiser.compute_gradients(\n                loss, var_list=to_optimise, colocate_gradients_with_ops=True)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n\n        elif self.is_inference:\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n            net_out = PostProcessingLayer(\'IDENTITY\')(net_out)\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        if self.is_inference:\n            return self.output_decoder.decode_batch(\n                {\'window_reg\':batch_output[\'window\']}, batch_output[\'location\'])\n        return True\n\n    def initialise_evaluator(self, eval_param):\n        self.eval_param = eval_param\n        self.evaluator = RegressionEvaluator(self.readers[0],\n                                             self.regression_param,\n                                             eval_param)\n\n    def add_inferred_output(self, data_param, task_param):\n        return self.add_inferred_output_like(data_param, task_param, \'output\')\n'"
niftynet/application/segmentation_application.py,26,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_weighted_v2 import WeightedSampler\nfrom niftynet.engine.sampler_balanced_v2 import BalancedSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_segmentation import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.layer.rgb_histogram_equilisation import \\\n    RGBHistogramEquilisationLayer\nfrom niftynet.evaluation.segmentation_evaluator import SegmentationEvaluator\nfrom niftynet.layer.rand_elastic_deform import RandomElasticDeformationLayer\n\nSUPPORTED_INPUT = set(\n    [\'image\', \'label\', \'weight\', \'sampler\', \'inferred\', \'value\'])\n\n\nclass SegmentationApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, action):\n        super(SegmentationApplication, self).__init__()\n        tf.logging.info(\'starting segmentation application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.segmentation_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'uniform\': (self.initialise_uniform_sampler,\n                        self.initialise_grid_sampler,\n                        self.initialise_grid_aggregator),\n            \'weighted\': (self.initialise_weighted_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler,\n                       self.initialise_resize_aggregator),\n            \'balanced\': (self.initialise_balanced_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.segmentation_param = task_param\n\n        # initialise input image readers\n        if self.is_training:\n            reader_names = (\'image\', \'label\', \'weight\', \'sampler\')\n        elif self.is_inference:\n            # in the inference process use `image` input only\n            reader_names = (\'image\',)\n        elif self.is_evaluation:\n            reader_names = (\'image\', \'label\', \'inferred\')\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        # initialise input preprocessing layers\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=self.net_param.foreground_type,\n            multimod_fusion=self.net_param.multimod_foreground_type,\n            threshold=0.0) \\\n            if self.net_param.normalise_foreground_only else None\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer) \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            binary_masking_func=foreground_masking_layer,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n        rgb_normaliser = RGBHistogramEquilisationLayer(\n            image_name=\'image\',\n            name=\'rbg_norm_layer\') if self.net_param.rgb_normalisation else None\n        label_normalisers = None\n        if self.net_param.histogram_ref_file and \\\n                task_param.label_normalisation:\n            label_normalisers = [DiscreteLabelNormalisationLayer(\n                image_name=\'label\',\n                modalities=vars(task_param).get(\'label\'),\n                model_filename=self.net_param.histogram_ref_file)]\n            if self.is_evaluation:\n                label_normalisers.append(\n                    DiscreteLabelNormalisationLayer(\n                        image_name=\'inferred\',\n                        modalities=vars(task_param).get(\'inferred\'),\n                        model_filename=self.net_param.histogram_ref_file))\n                label_normalisers[-1].key = label_normalisers[0].key\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if rgb_normaliser is not None:\n            normalisation_layers.append(rgb_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n        if task_param.label_normalisation and \\\n                (self.is_training or not task_param.output_prob):\n            normalisation_layers.extend(label_normalisers)\n\n        volume_padding_layer = [PadLayer(\n            image_name=SUPPORTED_INPUT,\n            border=self.net_param.volume_padding_size,\n            mode=self.net_param.volume_padding_mode,\n            pad_to=self.net_param.volume_padding_to_size)\n        ]\n        # initialise training data augmentation layers\n        augmentation_layers = []\n        if self.is_training:\n            train_param = self.action_param\n            self.patience = train_param.patience\n            self.mode = self.action_param.early_stopping_mode\n            if train_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=train_param.random_flipping_axes))\n            if train_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=train_param.scaling_percentage[0],\n                    max_percentage=train_param.scaling_percentage[1],\n                    antialiasing=train_param.antialiasing,\n                    isotropic=train_param.isotropic_scaling))\n            if train_param.rotation_angle or \\\n                    train_param.rotation_angle_x or \\\n                    train_param.rotation_angle_y or \\\n                    train_param.rotation_angle_z:\n                rotation_layer = RandomRotationLayer()\n                if train_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        train_param.rotation_angle)\n                else:\n                    rotation_layer.init_non_uniform_angle(\n                        train_param.rotation_angle_x,\n                        train_param.rotation_angle_y,\n                        train_param.rotation_angle_z)\n                augmentation_layers.append(rotation_layer)\n            if train_param.do_elastic_deformation:\n                spatial_rank = list(self.readers[0].spatial_ranks.values())[0]\n                augmentation_layers.append(RandomElasticDeformationLayer(\n                    spatial_rank=spatial_rank,\n                    num_controlpoints=train_param.num_ctrl_points,\n                    std_deformation_sigma=train_param.deformation_sigma,\n                    proportion_to_augment=train_param.proportion_to_deform))\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n            volume_padding_layer + normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(\n                volume_padding_layer + normalisation_layers)\n\n        # Checking num_classes is set correctly\n        if self.segmentation_param.num_classes <= 1:\n            raise ValueError(""Number of classes must be at least 2 for segmentation"")\n        for preprocessor in self.readers[0].preprocessors:\n            if preprocessor.name == \'label_norm\':\n                if len(preprocessor.label_map[preprocessor.key[0]]) != self.segmentation_param.num_classes:\n                    raise ValueError(""Number of unique labels must be equal to ""\n                                     ""number of classes (check histogram_ref file)"")\n\n    def initialise_uniform_sampler(self):\n        self.sampler = [[UniformSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_weighted_sampler(self):\n        self.sampler = [[WeightedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_sampler(self):\n        self.sampler = [[GridSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_balanced_sampler(self):\n        self.sampler = [[BalancedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_aggregator(self):\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix,\n            fill_constant=self.action_param.fill_constant)\n\n    def initialise_resize_aggregator(self):\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        elif self.is_inference:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_aggregator(self):\n        self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=self.segmentation_param.num_classes,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        def mixup_switch_sampler(for_training):\n            # get first set of samples\n            d_dict = switch_sampler(for_training=for_training)\n\n            mix_fields = (\'image\', \'weight\', \'label\')\n\n            if not for_training:\n                with tf.name_scope(\'nomix\'):\n                    # ensure label is appropriate for dense loss functions\n                    ground_truth = tf.cast(d_dict[\'label\'], tf.int32)\n                    one_hot = tf.one_hot(tf.squeeze(ground_truth, axis=-1),\n                                         depth=self.segmentation_param.num_classes)\n                    d_dict[\'label\'] = one_hot\n            else:\n                with tf.name_scope(\'mixup\'):\n                    # get the mixing parameter from the Beta distribution\n                    alpha = self.segmentation_param.mixup_alpha\n                    beta = tf.distributions.Beta(alpha, alpha)  # 1, 1: uniform:\n                    rand_frac = beta.sample()\n\n                    # get another minibatch\n                    d_dict_to_mix = switch_sampler(for_training=True)\n\n                    # look at binarised labels: sort them\n                    if self.segmentation_param.mix_match:\n                        # sum up the positive labels to sort by their volumes\n                        inds1 = tf.argsort(tf.map_fn(tf.reduce_sum, tf.cast(d_dict[\'label\'], tf.int64)))\n                        inds2 = tf.argsort(tf.map_fn(tf.reduce_sum, tf.cast(d_dict_to_mix[\'label\'] > 0, tf.int64)))\n                        for field in [field for field in mix_fields if field in d_dict]:\n                            d_dict[field] = tf.gather(d_dict[field], indices=inds1)\n                            # note: sorted for opposite directions for d_dict_to_mix\n                            d_dict_to_mix[field] = tf.gather(d_dict_to_mix[field], indices=inds2[::-1])\n\n                    # making the labels dense and one-hot\n                    for d in (d_dict, d_dict_to_mix):\n                        ground_truth = tf.cast(d[\'label\'], tf.int32)\n                        one_hot = tf.one_hot(tf.squeeze(ground_truth, axis=-1),\n                                             depth=self.segmentation_param.num_classes)\n                        d[\'label\'] = one_hot\n\n                    # do the mixing for any fields that are relevant and present\n                    mixed_up = {field: d_dict[field] * rand_frac + d_dict_to_mix[field] * (1 - rand_frac) for field\n                                in mix_fields if field in d_dict}\n                    # reassign all relevant values in d_dict\n                    d_dict.update(mixed_up)\n\n            return d_dict\n\n        if self.is_training:\n            if not self.segmentation_param.do_mixup:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                # mix up the samples if not in validation phase\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: mixup_switch_sampler(for_training=True),\n                                    lambda: mixup_switch_sampler(for_training=False))  # don\'t mix the validation\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(\n                n_class=self.segmentation_param.num_classes,\n                loss_type=self.action_param.loss_type,\n                softmax=self.segmentation_param.softmax)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None),\n                weight_map=data_dict.get(\'weight\', None))\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n\n            # Get all vars\n            to_optimise = tf.trainable_variables()\n            vars_to_freeze = \\\n                self.action_param.vars_to_freeze or \\\n                self.action_param.vars_to_restore\n            if vars_to_freeze:\n                import re\n                var_regex = re.compile(vars_to_freeze)\n                # Only optimise vars that are not frozen\n                to_optimise = \\\n                    [v for v in to_optimise if not var_regex.search(v.name)]\n                tf.logging.info(\n                    ""Optimizing %d out of %d trainable variables, ""\n                    ""the other variables fixed (--vars_to_freeze %s)"",\n                    len(to_optimise),\n                    len(tf.trainable_variables()),\n                    vars_to_freeze)\n\n            grads = self.optimiser.compute_gradients(\n                loss, var_list=to_optimise, colocate_gradients_with_ops=True)\n\n            self.total_loss = loss\n\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            # outputs_collector.add_to_collection(\n            #    var=image*180.0, name=\'image\',\n            #    average_over_devices=False, summary_type=\'image3_sagittal\',\n            #    collection=TF_SUMMARIES)\n\n            # outputs_collector.add_to_collection(\n            #    var=image, name=\'image\',\n            #    average_over_devices=False,\n            #    collection=NETWORK_OUTPUT)\n\n            # outputs_collector.add_to_collection(\n            #    var=tf.reduce_mean(image), name=\'mean_image\',\n            #    average_over_devices=False, summary_type=\'scalar\',\n            #    collection=CONSOLE)\n        elif self.is_inference:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            output_prob = self.segmentation_param.output_prob\n            num_classes = self.segmentation_param.num_classes\n            if output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'SOFTMAX\', num_classes=num_classes)\n            elif not output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'ARGMAX\', num_classes=num_classes)\n            else:\n                post_process_layer = PostProcessingLayer(\n                    \'IDENTITY\', num_classes=num_classes)\n            net_out = post_process_layer(net_out)\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        if self.is_inference:\n            return self.output_decoder.decode_batch(\n                {\'window_seg\': batch_output[\'window\']},\n                batch_output[\'location\'])\n\n        return True\n\n    def initialise_evaluator(self, eval_param):\n        self.eval_param = eval_param\n        self.evaluator = SegmentationEvaluator(self.readers[0],\n                                               self.segmentation_param,\n                                               eval_param)\n\n    def add_inferred_output(self, data_param, task_param):\n        return self.add_inferred_output_like(data_param, task_param, \'label\')\n'"
niftynet/contrib/__init__.py,0,"b'""""""\n\n.. module:: niftynet.contrib\n   :synopsis: Experimental code for new features.\n\n""""""\n'"
niftynet/engine/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\n\n.. module:: niftynet.engine\n   :synopsis: Application engine.\n\n""""""\n'"
niftynet/engine/application_driver.py,17,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines a general procedure for running applications.\n\nExample usage::\n    app_driver = ApplicationDriver()\n    app_driver.initialise_application(system_param, input_data_param)\n    app_driver.run_application()\n\n``system_param`` and ``input_data_param`` should be generated using:\n``niftynet.utilities.user_parameters_parser.run()``\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_factory import \\\n    ApplicationFactory, EventHandlerFactory, IteratorFactory\nfrom niftynet.engine.application_iteration import IterationMessage\nfrom niftynet.engine.application_variables import \\\n    GradientsCollector, OutputsCollector\nfrom niftynet.engine.signal import TRAIN, \\\n    ITER_STARTED, ITER_FINISHED, GRAPH_CREATED, SESS_FINISHED, SESS_STARTED\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.io.misc_io import infer_latest_model_file\nfrom niftynet.utilities.user_parameters_default import \\\n    DEFAULT_EVENT_HANDLERS, DEFAULT_ITERATION_GENERATOR\nfrom niftynet.utilities.util_common import \\\n    set_cuda_device, tf_config, device_string\nfrom niftynet.utilities.util_common import traverse_nested\n\n\n# pylint: disable=too-many-instance-attributes\nclass ApplicationDriver(object):\n    """"""\n    This class initialises an application by building a TF graph,\n    and maintaining a session. It controls the\n    starting/stopping of an application. Applications should be\n    implemented by inheriting ``niftynet.application.base_application``\n    to be compatible with this driver.\n    """"""\n\n    def __init__(self):\n        self.app = None\n\n        self.is_training_action = True\n        self.num_threads = 0\n        self.num_gpus = 0\n        self.model_dir = None\n\n        self.max_checkpoints = 2\n        self.save_every_n = 0\n        self.tensorboard_every_n = -1\n        self.vars_to_restore = \'\'\n\n        self.initial_iter = 0\n        self.final_iter = 0\n        self.validation_every_n = -1\n        self.validation_max_iter = 1\n\n        self.data_partitioner = ImageSetsPartitioner()\n\n        self._event_handlers = None\n        self._generator = None\n\n    def initialise_application(self, workflow_param, data_param=None):\n        """"""\n        This function receives all parameters from user config file,\n        create an instance of application.\n\n        :param workflow_param: a dictionary of user parameters,\n            keys correspond to sections in the config file\n        :param data_param: a dictionary of input image parameters,\n            keys correspond to data properties to be used by image_reader\n        :return:\n        """"""\n        try:\n            system_param = workflow_param.get(\'SYSTEM\', None)\n            net_param = workflow_param.get(\'NETWORK\', None)\n            train_param = workflow_param.get(\'TRAINING\', None)\n            infer_param = workflow_param.get(\'INFERENCE\', None)\n            app_param = workflow_param.get(\'CUSTOM\', None)\n        except AttributeError:\n            tf.logging.fatal(\'parameters should be dictionaries\')\n            raise\n\n        assert os.path.exists(system_param.model_dir), \\\n            \'Model folder not exists {}\'.format(system_param.model_dir)\n        self.model_dir = system_param.model_dir\n\n        self.is_training_action = TRAIN.startswith(system_param.action.lower())\n        # hardware-related parameters\n        self.num_threads = max(system_param.num_threads, 1) \\\n            if self.is_training_action else 1\n        self.num_gpus = system_param.num_gpus \\\n            if self.is_training_action else min(system_param.num_gpus, 1)\n        set_cuda_device(system_param.cuda_devices)\n\n        # set training params.\n        if self.is_training_action:\n            assert train_param, \'training parameters not specified\'\n            self.initial_iter = train_param.starting_iter\n            self.final_iter = max(train_param.max_iter, self.initial_iter)\n            self.save_every_n = train_param.save_every_n\n            self.tensorboard_every_n = train_param.tensorboard_every_n\n            self.max_checkpoints = max(self.max_checkpoints,\n                                       train_param.max_checkpoints)\n            self.validation_every_n = train_param.validation_every_n\n            self.vars_to_restore = train_param.vars_to_restore \\\n                if hasattr(train_param, \'vars_to_restore\') else \'\'\n            if self.validation_every_n > 0:\n                self.validation_max_iter = max(self.validation_max_iter,\n                                               train_param.validation_max_iter)\n            action_param = train_param\n        else:  # set inference params.\n            assert infer_param, \'inference parameters not specified\'\n            self.initial_iter = infer_param.inference_iter\n            action_param = infer_param\n\n        # infer the initial iteration from model files\n        if self.initial_iter < 0:\n            self.initial_iter = infer_latest_model_file(\n                os.path.join(self.model_dir, \'models\'))\n\n        # create an application instance\n        assert app_param, \'application specific param. not specified\'\n        app_module = ApplicationFactory.create(app_param.name)\n        self.app = app_module(net_param, action_param, system_param.action)\n\n        # clear the cached file lists\n        self.data_partitioner.reset()\n        if data_param:\n            do_new_partition = \\\n                self.is_training_action and \\\n                (not os.path.isfile(system_param.dataset_split_file)) and \\\n                (train_param.exclude_fraction_for_validation > 0 or\n                 train_param.exclude_fraction_for_inference > 0)\n            data_fractions = (train_param.exclude_fraction_for_validation,\n                              train_param.exclude_fraction_for_inference) \\\n                if do_new_partition else None\n\n            self.data_partitioner.initialise(\n                data_param=data_param,\n                new_partition=do_new_partition,\n                ratios=data_fractions,\n                data_split_file=system_param.dataset_split_file)\n            assert self.data_partitioner.has_validation or \\\n                self.validation_every_n <= 0, \\\n                \'validation_every_n is set to {}, \' \\\n                \'but train/validation splitting not available.\\nPlease \' \\\n                \'check dataset partition list {} \' \\\n                \'(remove file to generate a new dataset partition), \' \\\n                \'check ""exclude_fraction_for_validation"" \' \\\n                \'(current config value: {}).\\nAlternatively, \' \\\n                \'set ""validation_every_n"" to -1.\'.format(\n                    self.validation_every_n,\n                    system_param.dataset_split_file,\n                    train_param.exclude_fraction_for_validation)\n\n        # initialise readers\n        self.app.initialise_dataset_loader(\n            data_param, app_param, self.data_partitioner)\n\n        # make the list of initialised event handler instances.\n        self.load_event_handlers(\n            system_param.event_handler or DEFAULT_EVENT_HANDLERS)\n        self._generator = IteratorFactory.create(\n            system_param.iteration_generator or DEFAULT_ITERATION_GENERATOR)\n\n    def run(self, application, graph=None):\n        """"""\n        Initialise a TF graph, connect data sampler and network within\n        the graph context, run training loops or inference loops.\n\n        :param application: a niftynet application\n        :param graph: default base graph to run the application\n        :return:\n        """"""\n        if graph is None:\n            graph = ApplicationDriver.create_graph(\n                application=application,\n                num_gpus=self.num_gpus,\n                num_threads=self.num_threads,\n                is_training_action=self.is_training_action)\n\n        start_time = time.time()\n        loop_status = {\'current_iter\': self.initial_iter, \'normal_exit\': False}\n\n        with tf.Session(config=tf_config(), graph=graph):\n            try:\n                # broadcasting event of session started\n                SESS_STARTED.send(application, iter_msg=None)\n\n                # create a iteration message generator and\n                # iteratively run the graph (the main engine loop)\n                iteration_messages = self._generator(**vars(self))()\n                ApplicationDriver.loop(\n                    application=application,\n                    iteration_messages=iteration_messages,\n                    loop_status=loop_status)\n\n            except KeyboardInterrupt:\n                tf.logging.warning(\'User cancelled application\')\n            except (tf.errors.OutOfRangeError, EOFError):\n                if not loop_status.get(\'normal_exit\', False):\n                    # reached the end of inference Dataset\n                    loop_status[\'normal_exit\'] = True\n            except RuntimeError:\n                import sys\n                import traceback\n                exc_type, exc_value, exc_traceback = sys.exc_info()\n                traceback.print_exception(\n                    exc_type, exc_value, exc_traceback, file=sys.stdout)\n            finally:\n                tf.logging.info(\'cleaning up...\')\n                # broadcasting session finished event\n                iter_msg = IterationMessage()\n                iter_msg.current_iter = loop_status.get(\'current_iter\', -1)\n                SESS_FINISHED.send(application, iter_msg=iter_msg)\n\n        application.stop()\n        if not loop_status.get(\'normal_exit\', False):\n            # loop didn\'t finish normally\n            tf.logging.warning(\'stopped early, incomplete iterations.\')\n        tf.logging.info(\n            ""%s stopped (time in second %.2f)."",\n            type(application).__name__, (time.time() - start_time))\n\n    # pylint: disable=not-context-manager\n    @staticmethod\n    def create_graph(\n            application, num_gpus=1, num_threads=1, is_training_action=False):\n        """"""\n        Create a TF graph based on self.app properties\n        and engine parameters.\n\n        :return:\n        """"""\n        graph = tf.Graph()\n        main_device = device_string(num_gpus, 0, False, is_training_action)\n        outputs_collector = OutputsCollector(n_devices=max(num_gpus, 1))\n        gradients_collector = GradientsCollector(n_devices=max(num_gpus, 1))\n        # start constructing the graph, handling training and inference cases\n        with graph.as_default(), tf.device(main_device):\n            # initialise sampler\n            with tf.name_scope(\'Sampler\'):\n                application.initialise_sampler()\n                for sampler in traverse_nested(application.get_sampler()):\n                    sampler.set_num_threads(num_threads)\n\n            # initialise network, these are connected in\n            # the context of multiple gpus\n            application.initialise_network()\n            application.add_validation_flag()\n\n            # for data parallelism --\n            #     defining and collecting variables from multiple devices\n            for gpu_id in range(0, max(num_gpus, 1)):\n                worker_device = device_string(\n                    num_gpus, gpu_id, True, is_training_action)\n                scope_string = \'worker_{}\'.format(gpu_id)\n                with tf.name_scope(scope_string), tf.device(worker_device):\n                    # setup network for each of the multiple devices\n                    application.connect_data_and_network(\n                        outputs_collector, gradients_collector)\n            with tf.name_scope(\'MergeOutputs\'):\n                outputs_collector.finalise_output_op()\n            application.outputs_collector = outputs_collector\n            application.gradients_collector = gradients_collector\n            GRAPH_CREATED.send(application, iter_msg=None)\n        return graph\n\n    def load_event_handlers(self, names):\n        """"""\n        Import event handler modules and create a list of handler instances.\n        The event handler instances will be stored with this engine.\n\n        :param names: strings of event handlers\n        :return:\n        """"""\n        if not names:\n            return\n        if self._event_handlers:\n            # disconnect all handlers (assuming always weak connection)\n            for handler in list(self._event_handlers):\n                del self._event_handlers[handler]\n        self._event_handlers = {}\n        for name in set(names):\n            the_event_class = EventHandlerFactory.create(name)\n            # initialise all registered event handler classes\n            engine_config_dict = vars(self)\n            key = \'{}\'.format(the_event_class)\n            self._event_handlers[key] = the_event_class(**engine_config_dict)\n\n    @staticmethod\n    def loop(application,\n             iteration_messages=(),\n             loop_status=None):\n        """"""\n        Running ``loop_step`` with ``IterationMessage`` instances\n        generated by ``iteration_generator``.\n\n        This loop stops when any of the condition satisfied:\n            1. no more element from the ``iteration_generator``;\n            2. ``application.interpret_output`` returns False;\n            3. any exception raised.\n\n        Broadcasting SESS_* signals at the beginning and end of this method.\n\n        This function should be used in a context of\n        ``tf.Session`` or ``session.as_default()``.\n\n        :param application: a niftynet.application instance, application\n            will provides ``tensors`` to be fetched by ``tf.session.run()``.\n        :param iteration_messages:\n            a generator of ``engine.IterationMessage`` instances\n        :param loop_status: optional dictionary used to capture the loop status,\n            useful when the loop exited in an unexpected manner.\n        :return:\n        """"""\n        loop_status = loop_status or {}\n        for iter_msg in iteration_messages:\n            loop_status[\'current_iter\'] = iter_msg.current_iter\n\n            # run an iteration\n            ApplicationDriver.loop_step(application, iter_msg)\n\n            # Checking stopping conditions\n            if iter_msg.should_stop:\n                tf.logging.info(\'stopping -- event handler: %s.\',\n                                iter_msg.should_stop)\n                break\n        # loop finished without any exception\n        loop_status[\'normal_exit\'] = True\n\n    @staticmethod\n    def loop_step(application, iteration_message):\n        """"""\n        Calling ``tf.session.run`` with parameters encapsulated in\n        iteration message as an iteration.\n        Broadcasting ITER_* events before and afterward.\n\n        :param application:\n        :param iteration_message: an ``engine.IterationMessage`` instances\n        :return:\n        """"""\n        # broadcasting event of starting an iteration\n        ITER_STARTED.send(application, iter_msg=iteration_message)\n\n        # ``iter_msg.ops_to_run`` are populated with the ops to run in\n        # each iteration, fed into ``session.run()`` and then\n        # passed to the application (and observers) for interpretation.\n        sess = tf.get_default_session()\n        assert sess, \'method should be called within a TF session context.\'\n\n        iteration_message.current_iter_output = sess.run(\n            iteration_message.ops_to_run,\n            feed_dict=iteration_message.data_feed_dict)\n\n        # broadcasting event of finishing an iteration\n        ITER_FINISHED.send(application, iter_msg=iteration_message)\n'"
niftynet/engine/application_factory.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nLoading modules from a string representing the class name\nor a short name that matches the dictionary item defined\nin this module\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport importlib\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.utilities.util_common import \\\n    damerau_levenshtein_distance as edit_distance\n\n# pylint: disable=too-few-public-methods\nSUPPORTED_APP = {\n    \'net_regress\':\n        \'niftynet.application.regression_application.RegressionApplication\',\n    \'net_segment\':\n        \'niftynet.application.segmentation_application.SegmentationApplication\',\n    \'net_autoencoder\':\n        \'niftynet.application.autoencoder_application.AutoencoderApplication\',\n    \'net_gan\':\n        \'niftynet.application.gan_application.GANApplication\',\n    \'net_classify\':\n        \'niftynet.application.classification_application.\'\n        \'ClassificationApplication\',\n}\n\nSUPPORTED_NETWORK = {\n    # GAN\n    \'simulator_gan\':\n        \'niftynet.network.simulator_gan.SimulatorGAN\',\n    \'simple_gan\':\n        \'niftynet.network.simple_gan.SimpleGAN\',\n\n    # Segmentation\n    ""highres3dnet"":\n        \'niftynet.network.highres3dnet.HighRes3DNet\',\n    ""highres3dnet_small"":\n        \'niftynet.network.highres3dnet_small.HighRes3DNetSmall\',\n    ""highres3dnet_large"":\n        \'niftynet.network.highres3dnet_large.HighRes3DNetLarge\',\n    ""toynet"":\n        \'niftynet.network.toynet.ToyNet\',\n    ""unet"":\n        \'niftynet.network.unet.UNet3D\',\n    ""nonewnet"":\n        \'niftynet.network.no_new_net.UNet3D\',\n    ""vnet"":\n        \'niftynet.network.vnet.VNet\',\n    ""dense_vnet"":\n        \'niftynet.network.dense_vnet.DenseVNet\',\n    ""deepmedic"":\n        \'niftynet.network.deepmedic.DeepMedic\',\n    ""scalenet"":\n        \'niftynet.network.scalenet.ScaleNet\',\n    ""holisticnet"":\n        \'niftynet.network.holistic_net.HolisticNet\',\n    ""unet_2d"":\n        \'niftynet.network.unet_2d.UNet2D\',\n\n    # classification\n    ""resnet"": \'niftynet.network.resnet.ResNet\',\n    ""se_resnet"": \'niftynet.network.se_resnet.SE_ResNet\',\n\n    # autoencoder\n    ""vae"": \'niftynet.network.vae.VAE\'\n}\n\nSUPPORTED_LOSS_GAN = {\n    \'CrossEntropy\': \'niftynet.layer.loss_gan.cross_entropy\',\n}\n\nSUPPORTED_LOSS_SEGMENTATION = {\n    ""CrossEntropy"":\n        \'niftynet.layer.loss_segmentation.cross_entropy\',\n    ""CrossEntropy_Dense"":\n        \'niftynet.layer.loss_segmentation.cross_entropy_dense\',\n    ""Dice"":\n        \'niftynet.layer.loss_segmentation.dice\',\n    ""Dice_NS"":\n        \'niftynet.layer.loss_segmentation.dice_nosquare\',\n    ""Dice_Dense"":\n        \'niftynet.layer.loss_segmentation.dice_dense\',\n    ""Dice_Dense_NS"":\n        \'niftynet.layer.loss_segmentation.dice_dense_nosquare\',\n    ""Tversky"":\n        \'niftynet.layer.loss_segmentation.tversky\',\n    ""GDSC"":\n        \'niftynet.layer.loss_segmentation.generalised_dice_loss\',\n    ""DicePlusXEnt"":\n        \'niftynet.layer.loss_segmentation.dice_plus_xent_loss\',\n    ""WGDL"":\n        \'niftynet.layer.loss_segmentation.generalised_wasserstein_dice_loss\',\n    ""SensSpec"":\n        \'niftynet.layer.loss_segmentation.sensitivity_specificity_loss\',\n    ""VolEnforcement"":\n        \'niftynet.layer.loss_segmentation.volume_enforcement\',\n    # ""L1Loss"":\n    #     \'niftynet.layer.loss_segmentation.l1_loss\',\n    # ""L2Loss"":\n    #     \'niftynet.layer.loss_segmentation.l2_loss\',\n    # ""Huber"":\n    #     \'niftynet.layer.loss_segmentation.huber_loss\'\n}\n\nSUPPORTED_LOSS_REGRESSION = {\n    ""L1Loss"":\n        \'niftynet.layer.loss_regression.l1_loss\',\n    ""L2Loss"":\n        \'niftynet.layer.loss_regression.l2_loss\',\n    ""RMSE"":\n        \'niftynet.layer.loss_regression.rmse_loss\',\n    ""MAE"":\n        \'niftynet.layer.loss_regression.mae_loss\',\n    ""Huber"":\n        \'niftynet.layer.loss_regression.huber_loss\',\n    ""SmoothL1"":\n        \'niftynet.layer.loss_regression.smooth_l1_loss\',\n    ""Cosine"":\n        \'niftynet.layer.loss_regression.cosine_loss\'\n}\n\nSUPPORTED_LOSS_CLASSIFICATION = {\n    ""CrossEntropy"":\n        \'niftynet.layer.loss_classification.cross_entropy\',\n}\n\nSUPPORTED_LOSS_CLASSIFICATION_MULTI = {\n    ""ConfusionMatrix"":\n        \'niftynet.layer.loss_classification_multi.loss_confusion_matrix\',\n    ""Variability"":\n        \'niftynet.layer.loss_classification_multi.loss_variability\',\n    ""Consistency"":\n        \'niftynet.layer.loss_classification_multi.rmse_consistency\'\n}\n\n\nSUPPORTED_LOSS_AUTOENCODER = {\n    ""VariationalLowerBound"":\n        \'niftynet.layer.loss_autoencoder.variational_lower_bound\',\n}\n\nSUPPORTED_OPTIMIZERS = {\n    \'adam\': \'niftynet.engine.application_optimiser.Adam\',\n    \'gradientdescent\': \'niftynet.engine.application_optimiser.GradientDescent\',\n    \'momentum\': \'niftynet.engine.application_optimiser.Momentum\',\n    \'nesterov\': \'niftynet.engine.application_optimiser.NesterovMomentum\',\n\n    \'adagrad\': \'niftynet.engine.application_optimiser.Adagrad\',\n    \'rmsprop\': \'niftynet.engine.application_optimiser.RMSProp\',\n}\n\nSUPPORTED_INITIALIZATIONS = {\n    \'constant\': \'niftynet.engine.application_initializer.Constant\',\n    \'zeros\': \'niftynet.engine.application_initializer.Zeros\',\n    \'ones\': \'niftynet.engine.application_initializer.Ones\',\n    \'uniform_scaling\':\n        \'niftynet.engine.application_initializer.UniformUnitScaling\',\n    \'orthogonal\': \'niftynet.engine.application_initializer.Orthogonal\',\n    \'variance_scaling\':\n        \'niftynet.engine.application_initializer.VarianceScaling\',\n    \'glorot_normal\':\n        \'niftynet.engine.application_initializer.GlorotNormal\',\n    \'glorot_uniform\':\n        \'niftynet.engine.application_initializer.GlorotUniform\',\n    \'he_normal\': \'niftynet.engine.application_initializer.HeNormal\',\n    \'he_uniform\': \'niftynet.engine.application_initializer.HeUniform\'\n}\n\nSUPPORTED_EVALUATIONS = {\n    \'dice\': \'niftynet.evaluation.segmentation_evaluations.dice\',\n    \'jaccard\': \'niftynet.evaluation.segmentation_evaluations.jaccard\',\n    \'Dice\': \'niftynet.evaluation.segmentation_evaluations.dice\',\n    \'Jaccard\': \'niftynet.evaluation.segmentation_evaluations.jaccard\',\n    \'n_pos_ref\': \'niftynet.evaluation.segmentation_evaluations.n_pos_ref\',\n    \'n_neg_ref\': \'niftynet.evaluation.segmentation_evaluations.n_neg_ref\',\n    \'n_pos_seg\': \'niftynet.evaluation.segmentation_evaluations.n_pos_seg\',\n    \'n_neg_seg\': \'niftynet.evaluation.segmentation_evaluations.n_neg_seg\',\n    \'fp\': \'niftynet.evaluation.segmentation_evaluations.fp\',\n    \'fn\': \'niftynet.evaluation.segmentation_evaluations.fn\',\n    \'tp\': \'niftynet.evaluation.segmentation_evaluations.tp\',\n    \'tn\': \'niftynet.evaluation.segmentation_evaluations.tn\',\n    \'n_intersection\': \'niftynet.evaluation.segmentation_evaluations\'\n                      \'.n_intersection\',\n    \'n_union\': \'niftynet.evaluation.segmentation_evaluations.n_union\',\n    \'specificity\': \'niftynet.evaluation.segmentation_evaluations.specificity\',\n    \'sensitivity\': \'niftynet.evaluation.segmentation_evaluations.sensitivity\',\n    \'accuracy\': \'niftynet.evaluation.segmentation_evaluations.accuracy\',\n    \'false_positive_rate\': \'niftynet.evaluation.segmentation_evaluations\'\n                           \'.false_positive_rate\',\n    \'positive_predictive_values\': \'niftynet.evaluation.segmentation_evaluations\'\n                                  \'.positive_predictive_values\',\n    \'negative_predictive_values\': \'niftynet.evaluation.segmentation_evaluations\'\n                                  \'.negative_predictive_values\',\n    \'intersection_over_union\': \'niftynet.evaluation.segmentation_evaluations\'\n                               \'.intersection_over_union\',\n    \'informedness\': \'niftynet.evaluation.segmentation_evaluations.informedness\',\n    \'markedness\': \'niftynet.evaluation.segmentation_evaluations.markedness\',\n    \'vol_diff\': \'niftynet.evaluation.segmentation_evaluations.vol_diff\',\n    \'average_distance\': \'niftynet.evaluation.segmentation_evaluations\'\n                        \'.average_distance\',\n    \'hausdorff_distance\': \'niftynet.evaluation.segmentation_evaluations\'\n                          \'.hausdorff_distance\',\n    \'hausdorff95_distance\': \'niftynet.evaluation.segmentation_evaluations\'\n                            \'.hausdorff95_distance\',\n    \'com_ref\': \'niftynet.contrib.evaluation.segmentation_evaluations.com_ref\',\n    \'mse\': \'niftynet.evaluation.regression_evaluations.mse\',\n    \'rmse\': \'niftynet.evaluation.regression_evaluations.rmse\',\n    \'mae\': \'niftynet.evaluation.regression_evaluations.mae\',\n    # \'r2\': \'niftynet.contrib.evaluation.regression_evaluations.r2\',\n    \'classification_accuracy\': \'niftynet.evaluation.classification_evaluations\'\n                               \'.accuracy\',\n    \'roc_auc\': \'niftynet.contrib.evaluation.classification_evaluations.roc_auc\',\n    \'roc\': \'niftynet.contrib.evaluation.classification_evaluations.roc\',\n}\n\nSUPPORTED_EVENT_HANDLERS = {\n    \'model_restorer\':\n        \'niftynet.engine.handler_model.ModelRestorer\',\n    \'model_saver\':\n        \'niftynet.engine.handler_model.ModelSaver\',\n    \'sampler_threading\':\n        \'niftynet.engine.handler_sampler.SamplerThreading\',\n    \'apply_gradients\':\n        \'niftynet.engine.handler_gradient.ApplyGradients\',\n    \'output_interpreter\':\n        \'niftynet.engine.handler_network_output.OutputInterpreter\',\n    \'console_logger\':\n        \'niftynet.engine.handler_console.ConsoleLogger\',\n    \'tensorboard_logger\':\n        \'niftynet.engine.handler_tensorboard.TensorBoardLogger\',\n    \'performance_logger\':\n        \'niftynet.engine.handler_performance.PerformanceLogger\',\n    \'early_stopper\':\n        \'niftynet.engine.handler_early_stopping.EarlyStopper\',\n}\n\nSUPPORTED_ITERATION_GENERATORS = {\n    \'iteration_generator\':\n        \'niftynet.engine.application_iteration.IterationMessageGenerator\'\n}\n\n\ndef select_module(module_name, type_str, lookup_table=None):\n    """"""\n    This function first tries to find the absolute module name\n    by matching the static dictionary items, if not found, it\n    tries to import the module by splitting the input ``module_name``\n    as module name and class name to be imported.\n\n    :param module_name: string that matches the keys defined in lookup_table\n        or an absolute class name: module.name.ClassName\n    :param type_str: type of the module (used for better error display)\n    :param lookup_table: defines a set of shorthands for absolute class name\n    """"""\n    lookup_table = lookup_table or {}\n    module_name = \'{}\'.format(module_name)\n    is_external = True\n    if module_name in lookup_table:\n        module_name = lookup_table[module_name]\n        is_external = False\n    module_str, class_name = None, None\n    try:\n        module_str, class_name = module_name.rsplit(\'.\', 1)\n        the_module = importlib.import_module(module_str)\n        the_class = getattr(the_module, class_name)\n        if is_external:\n            # print location of external module\n            tf.logging.info(\'Import [%s] from %s.\',\n                            class_name, os.path.abspath(the_module.__file__))\n        return the_class\n    except (AttributeError, ValueError, ImportError) as not_imported:\n        tf.logging.fatal(repr(not_imported))\n        if \'.\' not in module_name:\n            err = \'Could not import {}: \' \\\n                  \'Incorrect module name ""{}""; \' \\\n                  \'expected ""module.object"".\'.format(type_str, module_name)\n        else:\n            err = \'{}: Could not import object\' \\\n                  \'""{}"" from ""{}""\'.format(type_str, class_name, module_str)\n        tf.logging.fatal(err)\n\n        if not lookup_table:\n            # no further guess\n            raise ValueError(err)\n\n        dists = dict(\n            (k, edit_distance(k, module_name)) for k in list(lookup_table))\n        closest = min(dists, key=dists.get)\n        if dists[closest] <= 3:\n            err = \'Could not import {2}: By ""{0}"", \' \\\n                  \'did you mean ""{1}""?\\n ""{0}"" is \' \\\n                  \'not a valid option. \'.format(module_name, closest, type_str)\n            tf.logging.fatal(err)\n        raise ValueError(err)\n\n\nclass ModuleFactory(object):\n    """"""\n    General interface for importing a class by its name.\n    """"""\n    SUPPORTED = None\n    type_str = \'object\'\n\n    @classmethod\n    def create(cls, name):\n        """"""\n        import a class by name\n        """"""\n        return select_module(name, cls.type_str, cls.SUPPORTED)\n\n\nclass ApplicationNetFactory(ModuleFactory):\n    """"""\n    Import a network from ``niftynet.network`` or from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_NETWORK\n    type_str = \'network\'\n\n\nclass ApplicationFactory(ModuleFactory):\n    """"""\n    Import an application from ``niftynet.application`` or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_APP\n    type_str = \'application\'\n\n\nclass LossGANFactory(ModuleFactory):\n    """"""\n    Import a GAN loss function from ``niftynet.layer`` or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_LOSS_GAN\n    type_str = \'GAN loss\'\n\n\nclass LossSegmentationFactory(ModuleFactory):\n    """"""\n    Import a segmentation loss function from ``niftynet.layer`` or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_LOSS_SEGMENTATION\n    type_str = \'segmentation loss\'\n\n\nclass LossRegressionFactory(ModuleFactory):\n    """"""\n    Import a regression loss function from ``niftynet.layer`` or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_LOSS_REGRESSION\n    type_str = \'regression loss\'\n\n\nclass LossClassificationFactory(ModuleFactory):\n    """"""\n    Import a classification loss function from niftynet.layer or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_LOSS_CLASSIFICATION\n    type_str = \'classification loss\'\n\n\nclass LossClassificationMultiFactory(ModuleFactory):\n    """"""\n    Import a classification loss function from niftynet.layer or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_LOSS_CLASSIFICATION_MULTI\n    type_str = \'classification multi loss\'\n\n\nclass LossAutoencoderFactory(ModuleFactory):\n    """"""\n    Import an autoencoder loss function from ``niftynet.layer`` or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_LOSS_AUTOENCODER\n    type_str = \'autoencoder loss\'\n\n\nclass OptimiserFactory(ModuleFactory):\n    """"""\n    Import an optimiser from ``niftynet.engine.application_optimiser`` or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_OPTIMIZERS\n    type_str = \'optimizer\'\n\n\nclass InitializerFactory(ModuleFactory):\n    """"""\n    Import an initializer from ``niftynet.engine.application_initializer`` or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_INITIALIZATIONS\n    type_str = \'initializer\'\n\n    @staticmethod\n    def get_initializer(name, args=None):\n        """"""\n        wrapper for getting the initializer.\n\n        :param name:\n        :param args: optional parameters for the initializer\n        :return:\n        """"""\n        init_class = InitializerFactory.create(name)\n        if args is None:\n            args = {}\n        return init_class.get_instance(args)\n\n\nclass EvaluationFactory(ModuleFactory):\n    """"""\n    Import an optimiser from niftynet.engine.application_optimiser or\n    from user specified string\n    """"""\n    SUPPORTED = SUPPORTED_EVALUATIONS\n    type_str = \'evaluation\'\n\n\nclass EventHandlerFactory(ModuleFactory):\n    """"""\n    Import an event handler such as niftynet.engine.handler_console\n    """"""\n    SUPPORTED = SUPPORTED_EVENT_HANDLERS\n    type_str = \'event handler\'\n\n\nclass IteratorFactory(ModuleFactory):\n    """"""\n    Import an iterative message generator for the main engine loop\n    """"""\n    SUPPORTED = SUPPORTED_ITERATION_GENERATORS\n    type_str = \'engine iterator\'\n'"
niftynet/engine/application_initializer.py,8,"b'# -*- coding: utf-8 -*-\n""""""\nLoading modules from a string representing the class name\nor a short name that matches the dictionary item defined\nin this module\n\nall classes and docs are taken from\nhttps://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/init_ops.py\n""""""\nimport tensorflow as tf\n\nSEED = 42\n\n\nclass Constant(object):\n    """"""\n    initialize with a constant value\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        """"""\n        create an instance of the initializer\n        """"""\n        value = float(args.get(\'value\', 0.0))\n        return tf.constant_initializer(value)\n\n\nclass Zeros(object):\n    """"""\n    initialize with zeros\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        # pylint: disable=unused-argument\n        """"""\n        create an instance of the initializer\n        """"""\n        return tf.constant_initializer(0.0)\n\n\nclass Ones(object):\n    """"""\n    initialize with ones\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        # pylint: disable=unused-argument\n        """"""\n        create an instance of the initializer\n        """"""\n        return tf.constant_initializer(1.0)\n\n\nclass UniformUnitScaling(object):\n    """"""\n    see also:\n        https://www.tensorflow.org/api_docs/python/tf/uniform_unit_scaling_initializer\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        """"""\n        create an instance of the initializer\n        """"""\n        factor = float(args.get(\'factor\', 1.0))\n        return tf.uniform_unit_scaling_initializer(factor, seed=SEED)\n\n\nclass Orthogonal(object):\n    """"""\n    see also:\n        https://www.tensorflow.org/api_docs/python/tf/orthogonal_initializer\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        """"""\n        create an instance of the initializer\n        """"""\n        gain = float(args.get(\'gain\', 1.0))\n        return tf.orthogonal_initializer(gain, seed=SEED)\n\n\nclass VarianceScaling(object):\n    """"""\n    see also:\n        https://www.tensorflow.org/api_docs/python/tf/variance_scaling_initializer\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        """"""\n        create an instance of the initializer\n        """"""\n        scale = float(args.get(\'scale\', 1.0))\n        mode = args.get(\'mode\', ""fan_in"")\n        assert (mode in [""fan_in"", ""fan_out"", ""fan_avg""])\n        distribution = args.get(\'distribution\', ""normal"")\n        assert (distribution in [""normal"", ""uniform""])\n        return tf.variance_scaling_initializer(scale,\n                                               mode,\n                                               distribution,\n                                               seed=SEED)\n\n\nclass GlorotNormal(object):\n    """"""\n    see also:\n        https://www.tensorflow.org/api_docs/python/tf/glorot_normal_initializer\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        # pylint: disable=unused-argument\n        """"""\n        create an instance of the initializer\n        """"""\n        return tf.glorot_normal_initializer(seed=SEED)\n\n\nclass GlorotUniform(object):\n    """"""\n    see also:\n        https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        # pylint: disable=unused-argument\n        """"""\n        create an instance of the initializer\n        """"""\n        return tf.glorot_uniform_initializer(seed=SEED)\n\n\nclass HeUniform(object):\n    """"""\n    He uniform variance scaling initializer.\n\n    It draws samples from a uniform distribution within [-limit, limit]\n    where ``limit`` is ``sqrt(6 / fan_in)``\n    where ``fan_in`` is the number of input units in the weight tensor.\n    # Arguments\n    seed: A Python integer. Used to seed the random generator.\n    # Returns\n    An initializer.\n    # References\n    He et al., https://arxiv.org/abs/1502.01852\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        # pylint: disable=unused-argument\n        """"""\n        create an instance of the initializer\n        """"""\n        if not args:\n            args = {""scale"": ""2."", ""mode"": ""fan_in"", ""distribution"": ""uniform""}\n        return VarianceScaling.get_instance(args)\n\n\nclass HeNormal(object):\n    """"""\n    He normal initializer.\n\n    It draws samples from a truncated normal distribution centered on 0\n    with ``stddev = sqrt(2 / fan_in)``\n    where ``fan_in`` is the number of input units in the weight tensor.\n    # Arguments\n    seed: A Python integer. Used to seed the random generator.\n    # Returns\n    An initializer.\n    # References\n    He et al., https://arxiv.org/abs/1502.01852\n    """"""\n\n    @staticmethod\n    def get_instance(args):\n        # pylint: disable=unused-argument\n        """"""\n        create an instance of the initializer\n        """"""\n        if not args:\n            args = {""scale"": ""2."", ""mode"": ""fan_in"", ""distribution"": ""normal""}\n        return VarianceScaling.get_instance(args)\n'"
niftynet/engine/application_iteration.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nMessage stores status info of the current iteration.\n""""""\nimport itertools\nimport time\n\nfrom niftynet.engine.application_variables import CONSOLE, TF_SUMMARIES\nfrom niftynet.engine.signal import TRAIN, VALID, INFER\nfrom niftynet.utilities.util_common import look_up_operations\n\nCONSOLE_FORMAT = ""{} iter {}, {} ({:3f}s)""\nSUPPORTED_PHASES = {TRAIN, VALID, INFER}\n\n\nclass IterationMessage(object):\n    """"""\n    This class consists of network variables and operations at each iteration.\n    It is generated by the application engine but can be modified by the\n    application as well.\n    """"""\n    _current_iter = 0\n    _current_iter_tic = 0\n    _current_iter_toc = 0\n    _current_iter_output = None\n\n    _data_feed_dict = None\n    _ops_to_run = None\n    _phase = TRAIN\n\n    _should_stop = None\n\n    @property\n    def current_iter(self):\n        """"""\n        Current iteration index\n        can be used to create complex schedule for the\n        iterative training/validation/inference procedure.\n\n        :return: integer of iteration\n        """"""\n        return self._current_iter\n\n    @current_iter.setter\n    def current_iter(self, value):\n        self._current_iter = int(value)\n        self._current_iter_tic = time.time()\n        self._current_iter_output = None\n\n    @property\n    def ops_to_run(self):\n        """"""\n        operations (tf graph elements) to be fed into\n        ``session.run(...)``. This is currently mainly used\n        for passing network gradient updates ops to ``session.run``.\n\n        To modify the operations, assigns ``self.ops_to_run``\n\n        :return: a copy of the operation dictionary\n        """"""\n        if self._ops_to_run is None:\n            self._ops_to_run = {}\n        assert isinstance(self._ops_to_run, dict), \\\n            \'ops to run should be a dictionary\'\n        return self._ops_to_run\n\n    @ops_to_run.setter\n    def ops_to_run(self, value):\n        self._ops_to_run = value\n\n    @property\n    def data_feed_dict(self):\n        """"""\n        A dictionary that maps graph elements to values\n        to be fed into ``session.run(...)`` as feed_dict parameter\n\n        :return: dictionary of operations\n        """"""\n        if self._data_feed_dict is None:\n            self._data_feed_dict = {}\n        return self._data_feed_dict\n\n    @data_feed_dict.setter\n    def data_feed_dict(self, value):\n        assert isinstance(value, dict), \\\n            \'data_feed_dict should a dictionary of placeholders:values\'\n        self._data_feed_dict = value\n\n    @property\n    def current_iter_output(self):\n        """"""\n        This property stores graph output received\n        by running ``session.run()``.\n\n        :return:\n        """"""\n        return self._current_iter_output\n\n    @current_iter_output.setter\n    def current_iter_output(self, value):\n        self._current_iter_output = value\n        self._current_iter_toc = time.time()\n\n    @property\n    def should_stop(self):\n        """"""\n        Engine check this property after each iteration\n\n        This could be modified in by application\n        ``application.set_iteration_update()``\n        to create training schedules such as early stopping.\n\n        :return: None or a handler that requested to stop the loop\n        """"""\n        return self._should_stop\n\n    @should_stop.setter\n    def should_stop(self, value):\n        self._should_stop = value\n\n    @property\n    def phase(self):\n        """"""\n        A string indicating the phase in train/validation/inference\n\n        :return:\n        """"""\n        return self._phase\n\n    @phase.setter\n    def phase(self, value):\n        self._phase = look_up_operations(value, SUPPORTED_PHASES)\n\n    @property\n    def is_training(self):\n        """"""\n\n        :return: boolean value indicating if the phase is training\n        """"""\n        return self.phase == TRAIN\n\n    @property\n    def is_validation(self):\n        """"""\n\n        :return: boolean value indicating if the phase is validation\n        """"""\n        return self.phase == VALID\n\n    @property\n    def is_inference(self):\n        """"""\n\n        :return: boolean value indicating if the phase is inference\n        """"""\n        return self.phase == INFER\n\n    @property\n    def iter_duration(self):\n        """"""\n        measuring time used\n        from setting self.current_iter to setting self.current_iter_output\n\n        :return: time duration of an iteration\n        """"""\n\n        current_toc = max(self._current_iter_toc, self._current_iter_tic)\n        return current_toc - self._current_iter_tic\n\n    def to_console_string(self):\n        """"""\n        converting current_iter_output to string, for console displaying\n\n        :return: summary string\n        """"""\n        summary_indentation = ""    "" if self.is_validation else """"\n        summary_format = summary_indentation + CONSOLE_FORMAT\n        try:\n            console_content = self.current_iter_output.get(CONSOLE, \'\')\n        except AttributeError:\n            console_content = ""print to console -- set current_iter_output "" \\\n                              ""to a dictionary of {CONSOLE: \'content\'}.""\n        result_str = _console_vars_to_str(console_content)\n        summary = summary_format.format(\n            self.phase, self.current_iter, result_str, self.iter_duration)\n        return summary\n\n    def to_tf_summary(self, writer=None):\n        """"""\n        converting current_iter_output to tf summary and write to ``writer``\n\n        :param writer: writer instance for summary output\n        :return:\n        """"""\n        if writer is None:\n            return\n        try:\n            summary = self.current_iter_output.get(TF_SUMMARIES, {})\n        except AttributeError:\n            summary = None\n\n        if not summary:\n            return\n        writer.add_summary(summary, self.current_iter)\n\n\nclass IterationMessageGenerator(object):\n    """"""\n    Classes provides an iteration message generator function.\n    The generator should yield IterationMessage instances.\n    """"""\n\n    def __init__(self,\n                 initial_iter=0,\n                 final_iter=0,\n                 validation_every_n=0,\n                 validation_max_iter=0,\n                 is_training_action=True,\n                 **_unused):\n        self.initial_iter = max(initial_iter, -1)\n        self.final_iter = max(final_iter, self.initial_iter)\n        self.validation_every_n = validation_every_n\n        self.validation_max_iter = validation_max_iter\n        self.is_training_action = is_training_action\n\n    def __call__(self):\n        if not self.is_training_action:\n            return _infer_iter_generator()\n        return _train_iter_generator(\n            initial_iter=self.initial_iter,\n            final_iter=self.final_iter,\n            validation_every_n=self.validation_every_n,\n            validation_max_iter=self.validation_max_iter)\n\n\ndef _infer_iter_generator():\n    """"""\n    This generator yields infinite number of infer iterations.\n\n    :return: iteration message instances\n    """"""\n    infer_iterations = _iter_msg_generator(itertools.count(), INFER)\n    for infer_iter_msg in infer_iterations:\n        yield infer_iter_msg\n\n\ndef _train_iter_generator(initial_iter=0,\n                          final_iter=0,\n                          validation_every_n=0,\n                          validation_max_iter=0):\n    """"""\n    This generator yields a sequence of interleaved training and validation\n    iterations.\n\n    :param initial_iter: starting iteration of the training sequence\n    :param final_iter: ending iteration of the training sequence\n    :param validation_every_n: validation at every n training\n    :param validation_max_iter: number of validation iterations\n    :return: iteration message instances\n    """"""\n    train_iterations = _iter_msg_generator(\n        range(initial_iter + 1, final_iter + 1), TRAIN)\n    for train_iter_msg in train_iterations:\n        yield train_iter_msg\n        current_iter = train_iter_msg.current_iter\n        if current_iter > 0 and validation_every_n > 0 and \\\n                current_iter % validation_every_n == 0:\n            # generating validation iterations without changing the current\n            # iteration number.\n            valid_iterations = _iter_msg_generator(\n                [current_iter] * validation_max_iter, VALID)\n            for valid_iter_msg in valid_iterations:\n                yield valid_iter_msg\n\n\ndef _iter_msg_generator(count_generator, phase):\n    """"""\n    Generate a numbered sequence of IterationMessage objects\n    with phase-appropriate signals.\n    count_generator is an iterable object yielding iteration numbers\n    phase is one of TRAIN, VALID or INFER\n    """"""\n    for iter_i in count_generator:\n        iter_msg = IterationMessage()\n        iter_msg.current_iter, iter_msg.phase = iter_i, phase\n        yield iter_msg\n\n\ndef _console_vars_to_str(console_dict):\n    """"""\n    Printing values of variable evaluations to command line output.\n    """"""\n    if not console_dict:\n        return \'\'\n    if isinstance(console_dict, dict):\n        console_str = \', \'.join(\'{}={}\'.format(key, val)\n                                for (key, val) in console_dict.items())\n    else:\n        console_str = \'{}\'.format(console_dict)\n    return console_str\n'"
niftynet/engine/application_optimiser.py,6,"b'# -*- coding: utf-8 -*-\n\n""""""\nTo customise optimisers including\nnew optimisation methods, learning rate decay schedule,\nor customise other optional parameters of the optimiser:\n\ncreate a `newclass.py` that has a class `NewOptimisor` and implement\n`get_instance()`.\nand set config parameter in config file or from command line\nspecify `--optimiser newclass.NewOptimisor`\n""""""\n\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\n\n# pylint: disable=too-few-public-methods\n\nclass Adam(object):\n    """"""\n    Adam optimiser with default hyper parameters\n    """"""\n\n    @staticmethod\n    def get_instance(learning_rate):\n        """"""\n        create an instance of the optimiser\n        """"""\n        return tf.train.AdamOptimizer(\n            learning_rate=learning_rate,\n            beta1=0.9,\n            beta2=0.999,\n            epsilon=1e-08,\n            use_locking=False, name=\'Adam\')\n\n\nclass Adagrad(object):\n    """"""\n    Adagrad optimiser with default hyper parameters\n    """"""\n\n    @staticmethod\n    def get_instance(learning_rate):\n        """"""\n        create an instance of the optimiser\n        """"""\n        return tf.train.AdagradOptimizer(\n            learning_rate=learning_rate,\n            initial_accumulator_value=0.1,\n            use_locking=False, name=\'Adagrad\')\n\n\nclass Momentum(object):\n    """"""\n    Momentum optimiser with default hyper parameters\n    """"""\n\n    @staticmethod\n    def get_instance(learning_rate):\n        """"""\n        create an instance of the optimiser\n        """"""\n        return tf.train.MomentumOptimizer(\n            learning_rate=learning_rate,\n            momentum=0.9,\n            use_locking=False,\n            name=\'Momentum\',\n            use_nesterov=False)\n\n\nclass NesterovMomentum(object):\n    """"""\n    Nesterov Momentum optimiser with default hyper parameters\n    """"""\n\n    @staticmethod\n    def get_instance(learning_rate):\n        """"""\n        create an instance of the optimiser\n        """"""\n        return tf.train.MomentumOptimizer(\n            learning_rate=learning_rate,\n            momentum=0.9,\n            use_locking=False,\n            name=\'Momentum\',\n            use_nesterov=True)\n\n\nclass RMSProp(object):\n    """"""\n    RMSProp optimiser with default hyper parameters\n    """"""\n\n    @staticmethod\n    def get_instance(learning_rate):\n        """"""\n        create an instance of the optimiser\n        """"""\n        return tf.train.RMSPropOptimizer(\n            learning_rate=learning_rate,\n            decay=0.9,\n            momentum=0.0,\n            epsilon=1e-10,\n            use_locking=False,\n            centered=False,\n            name=\'RMSProp\')\n\n\nclass GradientDescent(object):\n    """"""\n    Gradient Descent optimiser with default hyper parameters\n    """"""\n\n    @staticmethod\n    def get_instance(learning_rate):\n        """"""\n        create an instance of the optimiser\n        """"""\n        return tf.train.GradientDescentOptimizer(\n            learning_rate=learning_rate,\n            use_locking=False,\n            name=\'GradientDescent\')\n'"
niftynet/engine/application_variables.py,25,"b'# -*- coding: utf-8 -*-\n""""""\nManaging tf.Tensor variables for initialising/evaluating networks\n""""""\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.contrib.framework import list_variables\n\nfrom niftynet.io.misc_io import \\\n    image3_axial, image3_coronal, image3_sagittal, resolve_checkpoint\nfrom niftynet.utilities import util_common as util\nfrom niftynet.utilities.restore_initializer import restore_initializer\n\nRESTORABLE = \'NiftyNetObjectsToRestore\'\nNETWORK_OUTPUT = \'niftynetout\'\nCONSOLE = \'niftynetconsole\'\nTF_SUMMARIES = tf.GraphKeys.SUMMARIES\nSUPPORTED_SUMMARY = {\'scalar\': tf.summary.scalar,\n                     \'histogram\': tf.summary.histogram,\n                     \'image\': tf.summary.image,\n                     \'image3_sagittal\': image3_sagittal,\n                     \'image3_coronal\': image3_coronal,\n                     \'image3_axial\': image3_axial}\n\n\nclass GradientsCollector(object):\n    """"""\n    This collector has a list of all gradients, collected when\n    constructing the tf graph. The gradient from multiple GPUs\n    will be averaged, and the averaged op is added to graph\n    by application driver\n    """"""\n\n    def __init__(self, n_devices=1):\n        self._gradients = []\n        self.n_devices = n_devices\n\n    def add_to_collection(self, gradients):\n        """"""\n        Add gradient generated by optimiser.compute_gradients to a\n        dictionary. This will be retrieved during training.\n        The gradient can be a list of model updates,\n        application can choose to implement `set_iteration_update()` interface\n        to specify how the gradients are used at each iteration\n\n        :param gradients: generated by optimiser.compute_gradients(loss)\n        :return:\n        """"""\n        assert len(self._gradients) < self.n_devices, \\\n            ""call add_to_collection once per device""\n        self._gradients.append(gradients)\n\n    @property\n    def gradients(self):\n        """"""\n        this function returns averaged gradient over devices\n        used by application driver\n        :return: averaged gradients over devices\n        """"""\n        assert self._gradients, \\\n            ""Please add gradients to collector when constructing the graph""\n        return util.average_multi_opt_gradients(self._gradients)\n\n\nclass OutputsCollector(object):\n    """"""\n    Collect all tf.Tensor object, to be evaluated by tf.Session.run()\n    These objects are grouped into::\n\n        NETWORK_OUTPUT: to be decoded by an aggregator\n        CONSOLE: to be printed on command line\n        TF_SUMMARIES: to be added to tensorboard visualisation\n\n    """"""\n\n    def __init__(self, n_devices=1):\n        self.console_vars = {}\n        self.summary_vars = {}\n        self.output_vars = {}\n\n        self._merge_op = None\n        self.n_devices = n_devices\n\n    def _add_to_dict(self, var_dict, var, name, do_averaging=False):\n        """"""\n        update the dict, with item of either\n        {name: variable} or {name: list of variable}\n\n        return: key of the var_dict\n        """"""\n        assert isinstance(var, tf.Tensor), \\\n            ""only supports adding one tf.Tensor at a time,"" \\\n            ""but received {}"".format(var)\n\n        assert name is not None, \\\n            ""select a meaningful name for variable {}"" \\\n            ""received {}"".format(var, name)\n\n        if do_averaging and self.n_devices > 1:\n            # collecting variables across devices as a list\n            var_list = var_dict.get(name, [])\n            try:\n                var_list.append(var)\n            except AttributeError:\n                tf.logging.fatal(\n                    ""averaged variable name %s has been taken"", name)\n                raise\n            var_dict[name] = var_list\n            if len(var_list) > self.n_devices:\n                tf.logging.fatal(""averaged variable %s has been used ""\n                                 ""in the collector"", name)\n                raise ValueError\n            return name\n\n        # collecting variables and rename if exists\n        new_name = name\n        _uniq_id = 0\n        while new_name in var_dict:\n            _uniq_id += 1\n            new_name = \'{}_{}\'.format(name, _uniq_id)\n        var_dict[new_name] = var\n        return new_name\n\n    # pylint: disable=too-many-arguments\n    def add_to_collection(self,\n                          var,\n                          name,\n                          average_over_devices=False,\n                          collection=CONSOLE,\n                          summary_type=\'scalar\'):\n        """"""\n        add tf.Tensors to be evaluated to dictionary\n        The dictionaries will be retrieved and evaluated\n        by application driver in the train/infer loops\n\n        :param var: tf.Tensor to be evaluated by tf.Session()\n        :param name: name of the variable (for displaying purposes)\n        :param average_over_devices:\n        :param collection: in choices of\n            [CONSOLE, TF_SUMMARIES, NETWORK_OUTPUT]\n        :param summary_type: if adding to TF_SUMMARIES, there are\n            a few possible ways to visualise the Tensor value\n            see SUPPORTED_SUMMARY\n        :return:\n        """"""\n        if collection == CONSOLE:\n            self._add_to_console(var, name, average_over_devices)\n        elif collection == NETWORK_OUTPUT:\n            self._add_to_network_output(var, name, average_over_devices)\n        elif collection == TF_SUMMARIES:\n            self._add_to_tf_summary(\n                var, name, average_over_devices, summary_type)\n        else:\n            raise ValueError(\n                ""unknown variable collection {}."".format(collection))\n\n    def variables(self, collection=CONSOLE):\n        """"""\n        get tf.Tensors to be evaluated by tf.Session().run()\n\n        :param collection: in choices of\n            [CONSOLE, TF_SUMMARIES, NETWORK_OUTPUT]\n        :return: a variable dictionary\n        """"""\n        if collection == CONSOLE:\n            return self.console_vars\n        elif collection == TF_SUMMARIES:\n            return self._merge_op if self._merge_op is not None else {}\n        elif collection == NETWORK_OUTPUT:\n            return self.output_vars\n        else:\n            tf.logging.fatal(""unknown output %s"", collection)\n            raise ValueError\n\n    def finalise_output_op(self):\n        """"""\n        This function checks the dictionary, if the variable needs to\n        be averaged over devices, then a reduce_mean node is added to\n        the graph.\n        This function should be called in\n        `ApplicationDriver.create_graph` function\n        """"""\n        self._average_variables_over_devices(self.console_vars, False)\n        self._average_variables_over_devices(self.output_vars, False)\n        self._average_variables_over_devices(self.summary_vars, True)\n        self._merge_op = tf.summary.merge_all(key=TF_SUMMARIES)\n\n    def _add_to_network_output(self, var, name, average_over_devices=False):\n        """"""\n        add a variable to be decoded in application.interpret_output\n        """"""\n        self._add_to_dict(self.output_vars, var, name, average_over_devices)\n\n    def _add_to_console(self, var, name, average_over_devices=False):\n        """"""\n        add a variable to be displayed in command line\n        """"""\n        self._add_to_dict(self.console_vars, var, name, average_over_devices)\n\n    def _add_to_tf_summary(self, var, name,\n                           average_over_devices=False, summary_type=\'scalar\'):\n        """"""\n        add a variable to be displayed in tensorboard\n        """"""\n        name = self._add_to_dict(\n            self.summary_vars, var, name, average_over_devices)\n        # _add_to_dict might change the name parameter to avoid\n        # naming clash, here uses the returned new name for summary\n        values = self.summary_vars.get(name, None)\n        if isinstance(values, tf.Tensor):\n            summary_op = util.look_up_operations(summary_type,\n                                                 SUPPORTED_SUMMARY)\n            summary_op(name, values, collections=[TF_SUMMARIES])\n\n    @staticmethod\n    def _average_variables_over_devices(var_dict, create_tf_summary_op=False):\n        """"""\n        The last step of creating a tensorflow graph,\n        new ops are added by using reduce_mean over multi-devices\n        """"""\n        for var_name in var_dict:\n            values = var_dict.get(var_name, None)\n            if not isinstance(values, list):\n                continue\n            var_dict[var_name] = tf.reduce_mean(values, name=var_name)\n            if create_tf_summary_op:\n                # for the averaged variables use scalar summary only\n                tf.summary.scalar(name=\'{}_device_average_\'.format(var_name),\n                                  tensor=var_dict[var_name],\n                                  collections=[TF_SUMMARIES])\n\n\n# pylint: disable=too-many-locals,cell-var-from-loop\ndef global_vars_init_or_restore(var_list=None):\n    """"""\n    For any scope added to RESTORABLE collection:\n    variable will be restored from a checkpoint if it exists in the\n    specified checkpoint and no scope ancestor can restore it.\n    """"""\n    if var_list is None:\n        var_list = tf.global_variables()\n    restorable = sorted(tf.get_collection(RESTORABLE), key=lambda x: x[0])\n    restored_vars = {}\n    for scope, checkpoint_name, checkpoint_scope in restorable:\n        variables_in_scope = tf.get_collection(\n            tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n        checkpoint_file = resolve_checkpoint(checkpoint_name)\n        variables_in_file = [v for (v, _) in list_variables(checkpoint_file)]\n        rename = lambda x: x.replace(scope, checkpoint_scope).replace(\':0\', \'\')\n        to_restore = [v for v in variables_in_scope\n                      if v in var_list and rename(v.name) in variables_in_file]\n        for var in to_restore:\n            if var in restored_vars:\n                continue\n            if \'/\' in rename(var.name):\n                checkpoint_subscope, var_name = rename(var.name).rsplit(\'/\', 1)\n            else:\n                checkpoint_subscope, var_name = None, rename(var.name)\n            initializer = restore_initializer(\n                checkpoint_name, var_name, checkpoint_subscope)\n            restored_vars[var] = tf.assign(\n                var, initializer(var.shape, dtype=var.dtype))\n    init_others = tf.variables_initializer(\n        [v for v in var_list if v not in restored_vars])\n    restore_op = tf.group(init_others, *list(restored_vars.values()))\n    return restore_op\n'"
niftynet/engine/handler_console.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nThis module implements a console output writer.\n""""""\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.signal import ITER_STARTED, ITER_FINISHED\n\n\nclass ConsoleLogger(object):\n    """"""\n    This class handles iteration events to print output to the console.\n    """"""\n\n    def __init__(self, **_unused):\n        ITER_STARTED.connect(self.read_console_vars)\n        ITER_FINISHED.connect(self.print_console_vars)\n\n    def read_console_vars(self, sender, **msg):\n        """"""\n        Event handler to add all console output ops to the iteration message\n\n        :param sender: a niftynet.application instance\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        msg[\'iter_msg\'].ops_to_run[CONSOLE] = \\\n            sender.outputs_collector.variables(CONSOLE)\n\n    def print_console_vars(self, _sender, **msg):\n        """"""\n        Printing iteration message with ``tf.logging`` interface.\n\n        :param _sender:\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        tf.logging.info(msg[\'iter_msg\'].to_console_string())\n'"
niftynet/engine/handler_early_stopping.py,6,"b'# -*- coding: utf-8 -*-\n""""""\nThis module implements an early stopping handler\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.ndimage import median_filter\n\nfrom niftynet.engine.signal import ITER_FINISHED\n\n\nclass EarlyStopper(object):\n    """"""\n    This class handles iteration events to store the current performance as\n    an attribute of the sender (i.e. application).\n    """"""\n\n    def __init__(self, **_unused):\n        ITER_FINISHED.connect(self.check_criteria)\n\n    def check_criteria(self, _sender, **msg):\n        """"""\n        Printing iteration message with ``tf.logging`` interface.\n        :param _sender:\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        msg = msg[\'iter_msg\']\n        if len(_sender.performance_history) == _sender.patience:\n            # Value / threshold based methods:\n            # Check the latest value of the performance history against a\n            # threshold calculated based on the performance history\n            msg.should_stop = \\\n                check_should_stop(\n                    mode=_sender.mode,\n                    performance_history=_sender.performance_history)\n\n\ndef compute_generalisation_loss(validation_his):\n    """"""\n    This function computes the generalisation loss as\n        l[-1]-min(l)/max(l)-min(l)\n    :param validation_his: performance history\n    :return: generalisation loss\n    """"""\n    min_val_loss = np.min(np.array(validation_his))\n    max_val_loss = np.max(np.array(validation_his))\n    last = validation_his[-1]\n    if min_val_loss == 0:\n        return last\n    return (last-min_val_loss)/(max_val_loss - min_val_loss)\n\n\ndef check_should_stop(performance_history, mode=\'mean\', min_delta=0.03,\n                      kernel_size=5, k_splits=5):\n    """"""\n    This function takes in a mode, performance_history and patience and\n    returns True if the application should stop early.\n    :param mode: {\'mean\', \'robust_mean\', \'median\', \'generalisation_loss\',\n     \'median_smoothing\', \'validation_up\'} the default mode is \'mean\'\n    mean:\n        If your last loss is less than the average across the entire\n        performance history stop training\n    robust_mean:\n        Same as \'mean\' but only loss values within 5th and 95th\n        percentile are considered\n    median:\n        As in mode=\'mean\' but using the median\n    generalisation_loss:\n        Computes generalisation loss over the performance\n        history, and stops if it reaches an arbitrary threshold of 0.2.\n    validation_up:\n        This method check for performance increases in k sub-arrays of\n        length s, where k x s = patience. Because we cannot guarantee\n        patience to be divisible by both k and s, we define that k is\n        either 4 or 5, depending on which has the smallest remainder when\n        dividing.\n    :param performance_history: a list of size patience with the performance\n    history\n    :param min_delta: threshold for smoothness\n    :param kernel_size: hyperparameter for median smoothing\n    :param k_splits: number of splits if using \'validation_up\'\n    :return:\n    """"""\n    if mode == \'mean\':\n        performance_to_consider = performance_history[:-1]\n        thresh = np.mean(performance_to_consider)\n        tf.logging.info(""====Mean===="")\n        tf.logging.info(thresh)\n        tf.logging.info(performance_history[-1])\n        should_stop = performance_history[-1] > thresh\n\n    elif mode == \'robust_mean\':\n        performance_to_consider = performance_history[:-1]\n        perc = np.percentile(performance_to_consider, q=[5, 95])\n        temp = []\n        for perf_val in performance_to_consider:\n            if perc[0] < perf_val < perc[1]:\n                temp.append(perf_val)\n        should_stop = performance_history[-1] > np.mean(temp)\n\n    elif mode == \'median\':\n        performance_to_consider = performance_history[:-1]\n        should_stop = performance_history[-1] > np.median(\n            performance_to_consider)\n\n    elif mode == \'generalisation_loss\':\n        value = compute_generalisation_loss(performance_history)\n        should_stop = value > 0.2\n\n    elif mode == \'median_smoothing\':\n        smoothed = median_filter(performance_history[:-1],\n                                 size=kernel_size)\n        gradient = np.gradient(smoothed)\n        thresholded = np.where(gradient < min_delta, 1, 0)\n        value = np.sum(thresholded) * 1.0 / len(gradient)\n        should_stop = value < 0.5\n    elif mode == \'validation_up\':\n        remainder = len(performance_history) % k_splits\n        performance_to_consider = performance_history[remainder:]\n        strips = np.split(np.array(performance_to_consider), k_splits)\n        gl_increase = []\n        for strip in strips:\n            generalisation_loss = compute_generalisation_loss(strip)\n            gl_increase.append(generalisation_loss >= min_delta)\n        tf.logging.info(""====Validation_up===="")\n        tf.logging.info(gl_increase)\n        should_stop = False not in gl_increase\n    else:\n        raise Exception(\'Mode: {} provided is not supported\'.format(mode))\n    return should_stop\n'"
niftynet/engine/handler_gradient.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nThis module implements a network model updater with gradient ops.\n""""""\n\nimport tensorflow as tf\n\nfrom niftynet.engine.signal import ITER_STARTED, GRAPH_CREATED\nfrom niftynet.layer.bn import BN_COLLECTION\nfrom niftynet.utilities import util_common\n\nPRIMARY_NAME_SCOPE = \'worker_0\'\n\n\nclass ApplyGradients(object):\n    """"""\n    This class handles iteration events to update the model with gradient op\n    (by setting iteration message with a \'gradients\' op at the beginning of\n    each iteration).\n    """"""\n\n    def __init__(self, is_training_action=False, **_unused):\n        if not is_training_action:\n            return\n        GRAPH_CREATED.connect(self.make_gradients_op)\n        ITER_STARTED.connect(self.add_gradients)\n\n    def make_gradients_op(self, sender, **_unused):\n        """"""\n        Making ``optimiser.apply_gradients`` ops.\n\n        :param sender: a niftynet.application instance\n        :param _unused:\n        :return:\n        """"""\n        with tf.name_scope(\'ApplyGradients\'):\n            gradients = sender.gradients_collector.gradients\n            bn_ops = tf.get_collection(BN_COLLECTION, PRIMARY_NAME_SCOPE)\n            if not bn_ops:\n                sender.gradient_op = _apply_multiopt_gradients(\n                    sender.optimiser, gradients)\n            else:\n                with tf.get_default_graph().control_dependencies(bn_ops):\n                    sender.gradient_op = _apply_multiopt_gradients(\n                        sender.optimiser, gradients)\n\n    def add_gradients(self, sender, **msg):\n        """"""\n        Event handler to add gradients to iteration message ops_to_run.\n\n        See also\n        ``niftynet.application.base_application.set_network_gradient_op``\n\n        :param sender: a niftynet.application instance\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        if msg[\'iter_msg\'].is_training:\n            msg[\'iter_msg\'].ops_to_run[\'gradients\'] = sender.gradient_op\n\n\ndef _apply_multiopt_gradients(optimiser, gradients):\n    """"""\n    Apply gradients by using the corresponding optimiser.\n    This function sets ``self.gradient_op``.\n\n    :param optimiser: single optimiser or dict of optimisers\n        to be used to process the passed gradients\n    :param gradients: list or dict (having a reference to the optimiser used)\n        of processed gradients from the gradient_collector\n    :return:\n    """"""\n\n    if isinstance(gradients, dict):\n        ret = list()\n        for key in sorted(gradients):\n            optimiser_k = optimiser.get(key) \\\n                if isinstance(optimiser, dict) else optimiser\n            if not optimiser_k:\n                tf.logging.fatal(\'No optimizer found for %s\', key)\n                raise ValueError\n            ret.append(_apply_gradients(optimiser_k, gradients[key]))\n        return ret\n    return _apply_gradients(optimiser, gradients)\n\n\ndef _apply_gradients(optimiser, gradients):\n    """"""\n    Apply gradients op by ``optimiser.apply_gradients``.\n\n    :param optimiser: single optimiser processing the passed gradients\n    :param gradients: processed gradients from the gradient_collector\n    :return:\n    """"""\n\n    grad_list_depth = util_common.list_depth_count(gradients)\n    if grad_list_depth == 3:\n        # nested depth 3 means: gradients list is nested in terms of:\n        # list of networks -> list of network variables\n        return [optimiser.apply_gradients(grad) for grad in gradients]\n    elif grad_list_depth == 2:\n        # nested depth 2 means:\n        # gradients list is a list of variables\n        return optimiser.apply_gradients(gradients)\n    raise NotImplementedError(\n        \'This app supports updating a network, or a list of networks.\')\n'"
niftynet/engine/handler_model.py,19,"b'# -*- coding: utf-8 -*-\n""""""\nThis module implements a model checkpoint loader and writer.\n""""""\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_variables import global_vars_init_or_restore\nfrom niftynet.engine.signal import \\\n    ITER_FINISHED, SESS_FINISHED, SESS_STARTED\nfrom niftynet.io.misc_io import touch_folder\n\nFILE_PREFIX = \'model.ckpt\'\n\n\ndef make_model_name(model_dir):\n    """"""\n    Make the model checkpoint folder.\n    the checkpoint file will be located at `model_dir/models/` folder,\n    the filename will start with FILE_PREFIX.\n\n    :param model_dir: niftynet model folder\n    :return: a partial name of a checkpoint file `model_dir/model/FILE_PREFIX`\n    """"""\n    _model_dir = touch_folder(os.path.join(model_dir, \'models\'))\n    return os.path.join(_model_dir, FILE_PREFIX)\n\n\nclass ModelRestorer(object):\n    """"""\n    This class handles restoring the model at the beginning of a session.\n    """"""\n\n    def __init__(self,\n                 model_dir,\n                 initial_iter=0,\n                 is_training_action=True,\n                 vars_to_restore=None,\n                 **_unused):\n        self.initial_iter = initial_iter\n        self.vars_to_restore = vars_to_restore\n        self.file_name_prefix = make_model_name(model_dir)\n        # randomly initialise or restoring model\n        if is_training_action and initial_iter == 0:\n            SESS_STARTED.connect(self.rand_init_model)\n        else:\n            SESS_STARTED.connect(self.restore_model)\n\n    def rand_init_model(self, _sender, **_unused):\n        """"""\n        Randomly initialising all trainable variables defined in\n        the default session.\n\n        :param _sender:\n        :param _unused:\n        :return:\n        """"""\n        with tf.name_scope(\'Initialisation\'):\n            init_op = global_vars_init_or_restore()\n        tf.get_default_session().run(init_op)\n        tf.logging.info(\'Parameters from random initialisations ...\')\n\n    def restore_model(self, _sender, **_unused):\n        """"""\n        Loading checkpoint files as variable initialisations.\n\n        :param _sender:\n        :param _unused:\n        :return:\n        """"""\n        checkpoint = \'{}-{}\'.format(self.file_name_prefix, self.initial_iter)\n        to_restore = None  # tf.train.Saver\'s default value, restoring all\n\n        if self.vars_to_restore:\n            # partially restore (updating `to_restore` list)\n            tf.logging.info(""Finding variables to restore..."")\n            import re\n            # Determine which vars to\n            # restore using regex matching\n            var_regex = re.compile(self.vars_to_restore)\n            to_restore, to_randomise = [], []\n            for restorable in tf.global_variables():\n                if var_regex.search(restorable.name):\n                    to_restore.append(restorable)\n                else:\n                    to_randomise.append(restorable)\n\n            if not to_restore:\n                tf.logging.fatal(\n                    \'vars_to_restore specified: %s, but nothing matched.\',\n                    self.vars_to_restore)\n                assert to_restore, \'Nothing to restore (--vars_to_restore)\'\n\n            var_names = [  # getting first three item to print\n                var_restore.name for var_restore in to_restore[:3]]\n            tf.logging.info(\n                \'Restoring %s out of %s variables from %s: \\n%s, ...\',\n                len(to_restore),\n                len(tf.global_variables()),\n                checkpoint, \',\\n\'.join(var_names))\n            # Initialize vars to randomize\n            init_op = tf.variables_initializer(to_randomise)\n            tf.get_default_session().run(init_op)\n\n\n        try:\n            saver = tf.train.Saver(\n                var_list=to_restore, save_relative_paths=True)\n            saver.restore(tf.get_default_session(), checkpoint)\n        except tf.errors.NotFoundError:\n            tf.logging.fatal(\n                \'checkpoint %s not found or variables to restore do not \'\n                \'match the current application graph\', checkpoint)\n            dir_name = os.path.dirname(checkpoint)\n            if dir_name and not os.path.exists(dir_name):\n                tf.logging.fatal(\n                    ""Model folder not found %s, please check""\n                    ""config parameter: model_dir"", dir_name)\n            raise\n\n\nclass ModelSaver(object):\n    """"""\n    This class handles iteration events to save the model as checkpoint files.\n    """"""\n\n    def __init__(self,\n                 model_dir,\n                 save_every_n=0,\n                 max_checkpoints=1,\n                 is_training_action=True,\n                 **_unused):\n\n        self.save_every_n = save_every_n\n        self.max_checkpoints = max_checkpoints\n        self.file_name_prefix = make_model_name(model_dir)\n        self.saver = None\n\n        # initialise the saver after the graph finalised\n        SESS_STARTED.connect(self.init_saver)\n        # save the training model at a positive frequency\n        if self.save_every_n > 0:\n            ITER_FINISHED.connect(self.save_model_interval)\n        # always save the final training model before exiting\n        if is_training_action:\n            SESS_FINISHED.connect(self.save_model)\n\n    def init_saver(self, _sender, **_unused):\n        """"""\n        Initialise a model saver.\n\n        :param _sender:\n        :param _unused:\n        :return:\n        """"""\n        self.saver = tf.train.Saver(\n            max_to_keep=self.max_checkpoints, save_relative_paths=True)\n\n    def save_model(self, _sender, **msg):\n        """"""\n        Saving the model at the current iteration.\n\n        :param _sender:\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        iter_i = msg[\'iter_msg\'].current_iter\n        if iter_i >= 0:\n            self._save_at(iter_i)\n\n    def save_model_interval(self, _sender, **msg):\n        """"""\n        Saving the model according to the frequency of ``save_every_n``.\n\n        :param _sender:\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        if not msg[\'iter_msg\'].is_training:\n            return\n        iter_i = msg[\'iter_msg\'].current_iter\n        if iter_i > 0 and iter_i % self.save_every_n == 0:\n            self._save_at(iter_i)\n\n    def _save_at(self, iter_i):\n        """"""\n        Saving the model at iter i and print a console log.\n\n        : param iter_i: integer of the current iteration\n        : return:\n        """"""\n        if not self.saver:\n            return\n        self.saver.save(sess=tf.get_default_session(),\n                        save_path=self.file_name_prefix,\n                        global_step=iter_i)\n        tf.logging.info(\'iter %d saved: %s\', iter_i, self.file_name_prefix)\n'"
niftynet/engine/handler_network_output.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module implements a network output interpreter.\n""""""\nfrom niftynet.engine.application_variables import NETWORK_OUTPUT\nfrom niftynet.engine.signal import ITER_STARTED, ITER_FINISHED\n\n\nclass OutputInterpreter(object):\n    """"""\n    This class handles iteration events to interpret output.\n    """"""\n\n    def __init__(self, **_unused):\n        ITER_STARTED.connect(self.set_tensors_to_run)\n        ITER_FINISHED.connect(self.interpret_output)\n\n    def set_tensors_to_run(self, sender, **msg):\n        """"""\n        Event handler to add all tensors to evaluate to the iteration message.\n        The driver will fetch tensors\' values from\n        ``_iter_msg.ops_to_run``.\n\n        :param sender: a niftynet.application instance\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        _iter_msg = msg[\'iter_msg\']\n        _iter_msg.ops_to_run[NETWORK_OUTPUT] = \\\n            sender.outputs_collector.variables(NETWORK_OUTPUT)\n\n        # modifying `_iter_msg` using applications\'s set_iteration_update\n        sender.set_iteration_update(_iter_msg)\n\n    def interpret_output(self, sender, **msg):\n        """"""\n        Calling sender application to interpret evaluated tensors.\n        Set ``_iter_msg.should_stop`` to a True value\n        if it\'s an end of the engine loop.\n\n        See also:\n        ``niftynet.engine.application_driver.loop``\n\n        :param sender: a niftynet.application instance\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        _iter_msg = msg[\'iter_msg\']\n        waiting_for_more_output = sender.interpret_output(\n            _iter_msg.current_iter_output[NETWORK_OUTPUT])\n        if not waiting_for_more_output:\n            _iter_msg.should_stop = OutputInterpreter.__name__\n'"
niftynet/engine/handler_performance.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nThis module tracks model validation performance over training\n""""""\nimport tensorflow as tf\n\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.signal import ITER_FINISHED\n\n\nclass PerformanceLogger(object):\n    """"""\n    This class handles iteration events to store the current performance as\n    an attribute of the sender (i.e. application).\n    """"""\n\n    def __init__(self, **_unused):\n        ITER_FINISHED.connect(self.update_performance_history)\n\n    def update_performance_history(self, _sender, **msg):\n        """"""\n        Printing iteration message with ``tf.logging`` interface.\n        :param _sender:\n        :param msg: an iteration message instance\n        :return:\n        """"""\n        iter_msg = msg[\'iter_msg\']\n\n        if iter_msg.is_validation:\n            try:\n                console_content = iter_msg.current_iter_output.get(CONSOLE, \'\')\n                current_loss = console_content[\'total_loss\']\n\n                if len(_sender.performance_history) < _sender.patience:\n                    _sender.performance_history.append(current_loss)\n                else:\n                    _sender.performance_history = \\\n                        _sender.performance_history[1:] + [current_loss]\n            except (AttributeError, KeyError):\n                tf.logging.warning(""does not contain any performance field ""\n                                   ""called total loss."")\n'"
niftynet/engine/handler_sampler.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nThis module implements a sampler threads controller.\n""""""\nimport tensorflow as tf\n\n# from niftynet.engine.signal import SESS_STARTED\nfrom niftynet.engine.signal import SESS_FINISHED\nfrom niftynet.utilities.util_common import traverse_nested\n\n\nclass SamplerThreading(object):\n    """"""\n    This class handles iteration events to start/stop samplers\' threads.\n    """"""\n\n    def __init__(self, **_unused):\n        # SESS_STARTED.connect(self.start_sampler_threads)\n        SESS_FINISHED.connect(self.stop_sampler_threads)\n\n    def start_sampler_threads(self, _sender, **_unused_msg):\n        """"""\n        Get samplers from application and try to run sampler\'s threads.\n\n        (deprecating)\n\n        :param sender:\n        :param _unused_msg:\n        :return:\n        """"""\n        pass\n        # try:\n        #     for sampler in traverse_nested(sender.get_sampler()):\n        #         if sampler is None:\n        #             continue\n        #         sampler.run_threads(self.num_threads)\n        #     tf.logging.info(\'filling queues (this can take a few minutes).\')\n        # except (NameError, TypeError, AttributeError, IndexError):\n        #     tf.logging.fatal(\n        #         ""samplers not running, pop_batch_op operations ""\n        #         ""are blocked."")\n        #     raise\n\n    def stop_sampler_threads(self, sender, **_unused_msg):\n        """"""\n        Stop the sampler\'s threads\n\n        :param sender: an instance of niftynet.application\n        :param _unused_msg:\n        :return:\n        """"""\n        try:\n            tf.logging.info(\'stopping sampling threads\')\n            for sampler in traverse_nested(sender.get_sampler()):\n                if sampler is None:\n                    continue\n                sampler.close_all()\n        except (AttributeError, TypeError):\n            pass\n'"
niftynet/engine/handler_tensorboard.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nThis module implements a TensorBoard log writer.\n""""""\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_variables import TF_SUMMARIES\nfrom niftynet.engine.signal import \\\n    TRAIN, VALID, ITER_STARTED, ITER_FINISHED, GRAPH_CREATED\nfrom niftynet.io.misc_io import get_latest_subfolder\n\n\nclass TensorBoardLogger(object):\n    """"""\n    This class handles iteration events to log summaries to\n    the TensorBoard log.\n    """"""\n\n    def __init__(self,\n                 model_dir=None,\n                 initial_iter=0,\n                 tensorboard_every_n=0,\n                 **_unused):\n\n        self.tensorboard_every_n = tensorboard_every_n\n        # creating new summary subfolder if it\'s not finetuning\n        self.summary_dir = get_latest_subfolder(\n            os.path.join(model_dir, \'logs\'), create_new=initial_iter == 0)\n        self.writer_train = None\n        self.writer_valid = None\n\n        GRAPH_CREATED.connect(self.init_writer)\n        ITER_STARTED.connect(self.read_tensorboard_op)\n        ITER_FINISHED.connect(self.write_tensorboard)\n\n    def init_writer(self, _sender, **_unused_msg):\n        """"""\n        Initialise summary writers.\n\n        :param _sender:\n        :param msg:\n        :return:\n        """"""\n        # initialise summary writer\n        if not self.summary_dir or self.tensorboard_every_n <= 0:\n            return\n        self.writer_train = tf.summary.FileWriter(\n            os.path.join(self.summary_dir, TRAIN), tf.get_default_graph())\n        self.writer_valid = tf.summary.FileWriter(\n            os.path.join(self.summary_dir, VALID), tf.get_default_graph())\n\n    def read_tensorboard_op(self, sender, **msg):\n        """"""\n        Get TensorBoard summary_op from application at the\n        beginning of each iteration.\n\n        :param sender: a niftynet.application instance\n        :param msg: should contain an IterationMessage instance\n        """"""\n        _iter_msg = msg[\'iter_msg\']\n        if _iter_msg.is_inference:\n            return\n        if not self._is_writing(_iter_msg.current_iter):\n            return\n        tf_summary_ops = sender.outputs_collector.variables(TF_SUMMARIES)\n        _iter_msg.ops_to_run[TF_SUMMARIES] = tf_summary_ops\n\n    def write_tensorboard(self, _sender, **msg):\n        """"""\n        Write to tensorboard when received the iteration finished signal.\n\n        :param _sender:\n        :param msg:\n        """"""\n        _iter_msg = msg[\'iter_msg\']\n        if not self._is_writing(_iter_msg.current_iter):\n            return\n        if _iter_msg.is_training:\n            _iter_msg.to_tf_summary(self.writer_train)\n        elif _iter_msg.is_validation:\n            _iter_msg.to_tf_summary(self.writer_valid)\n\n    def _is_writing(self, c_iter):\n        """"""\n        Decide whether to save a TensorBoard log entry for a given iteration.\n\n        :param c_iter: Integer of the current iteration number\n        :return: boolean True if is writing at the current iteration\n        """"""\n        if self.writer_valid is None or self.writer_train is None:\n            return False\n        if not self.summary_dir:\n            return False\n        return c_iter % self.tensorboard_every_n == 0\n'"
niftynet/engine/image_window.py,13,"b'# -*- coding: utf-8 -*-\n""""""\nThis module provides an interface for data elements to be generated\nby an image sampler.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport copy\n\nimport numpy as np\nimport tensorflow as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.data.util import nest\n\nfrom niftynet.utilities.util_common import ParserNamespace\n\nN_SPATIAL = 3\nLOCATION_FORMAT = ""{}_location""\nBUFFER_POSITION_NP_TYPE = np.int32\nBUFFER_POSITION_DTYPE = tf.int32\n\n\nclass ImageWindow(object):\n    """"""\n    Each window is associated with a tuple of coordinates.\n    These data properties are used to create TF\n    placeholders or ``tf.data.Dataset`` when constructing a TF graph.\n    Sampler read the data specifications and fill the placeholder/dataset.\n    """"""\n\n    def __init__(self, shapes, dtypes):\n        """"""\n\n        :param shapes: A nested structure of tuple\n            corresponding to size of each image window\n        :param dtypes: A nested structure of `tf.DType` objects\n            corresponding to each image window\n        """"""\n        self._shapes = shapes\n        self._dtypes = dtypes\n        self._placeholders_dict = None\n\n        self.n_samples = 1\n        self.has_dynamic_shapes = self._check_dynamic_shapes()\n\n    @property\n    def names(self):\n        """"""\n\n        :return: a tuple of output modality names\n        """"""\n        return tuple(self._shapes)\n\n    @property\n    def shapes(self):\n        """"""\n\n        :return: a dictionary of image window and location shapes\n        """"""\n        shapes = {}\n        for name in list(self._shapes):\n            shapes[name] = tuple(\n                [self.n_samples] + list(self._shapes[name]))\n            shapes[LOCATION_FORMAT.format(name)] = tuple(\n                [self.n_samples] + [1 + N_SPATIAL * 2])\n        return shapes\n\n    @property\n    def tf_shapes(self):\n        """"""\n        :return: a dictionary of sampler output tensor shapes\n        """"""\n        output_shapes = nest.map_structure_up_to(\n            self.tf_dtypes, tf.TensorShape, self.shapes)\n        return output_shapes\n\n    @property\n    def tf_dtypes(self):\n        """"""\n        :return: tensorflow dtypes of the window.\n        """"""\n        dtypes = {}\n        for name in list(self._dtypes):\n            dtypes[name] = self._dtypes[name]\n            dtypes[LOCATION_FORMAT.format(name)] = BUFFER_POSITION_DTYPE\n        return dtypes\n\n    @classmethod\n    def from_data_reader_properties(cls,\n                                    source_names,\n                                    image_shapes,\n                                    image_dtypes,\n                                    window_sizes=None,\n                                    allow_dynamic=False):\n        """"""\n        Create a window instance with input data properties\n        each property is grouped into dict, with pairs of\n        image_name: data_value. Some input images is a\n        concatenated data array from multiple data sources.\n        example of input::\n\n            source_names={\n                \'image\': (u\'modality1\', u\'modality2\'),\n                \'label\': (u\'modality3\',)},\n            image_shapes={\n                \'image\': (192, 160, 192, 1, 2),\n                \'label\': (192, 160, 192, 1, 1)},\n            image_dtypes={\n                \'image\': tf.float32,\n                \'label\': tf.float32},\n            window_sizes={\n                \'image\': (10, 10, 2),\n                \'label\': (10, 10, 2)}\n\n        the ``window_sizes`` can also be::\n\n            window_sizes={\n                \'modality1\': (10, 10, 2),\n                \'modality3\': (10, 10, 2)}\n\n        or using a nested dictionary with \'spatial_window_size\' (deprecating)::\n\n            window_sizes={\n                \'modality1\': {\'spatial_window_size\': (10, 10, 2)},\n                \'modality2\': {\'spatial_window_size\': (10, 10, 2)},\n                \'modality3\': {\'spatial_window_size\': (5, 5, 1)}}\n\n        see ``niftynet.io.ImageReader`` for more details.\n\n        :param source_names: input image names\n        :param image_shapes: tuple of image window shapes\n        :param image_dtypes: tuple of image window data types\n        :param window_sizes: window sizes for the image image\n        :param allow_dynamic: if True, window_sizes negative or 0 indicates\n            dynamic window sizes; . Otherwise the dynamic sizes will be fixed\n            as the image shapes; this assumes the same image size across the\n            dataset.\n        :return: an ImageWindow instance\n        """"""\n        try:\n            image_shapes = nest.map_structure_up_to(\n                image_dtypes, tuple, image_shapes)\n        except KeyError:\n            tf.logging.fatal(\'window_sizes wrong format %s\', window_sizes)\n            raise\n        # create ImageWindow instance\n        window_instance = cls(shapes=image_shapes, dtypes=image_dtypes)\n\n        if not window_sizes:\n            # image window sizes not specified, defaulting to image sizes.\n            return window_instance\n\n        window_instance.set_spatial_shape(window_sizes, source_names)\n        if not allow_dynamic:\n            full_shape = window_instance.match_image_shapes(image_shapes)\n            window_instance.set_spatial_shape(full_shape)\n        return window_instance\n\n    def set_spatial_shape(self, spatial_window, source_names=None):\n        """"""\n        Set all spatial window of the window.\n\n        spatial_window should be a dictionary of window sizes tuples\n        or single window size tuple.  In the latter case the size\n        will be used by all output image windows.\n\n        :param spatial_window: tuple of integers specifying new shape\n        :param source_names: list/dictionary of input source names\n        :return:\n        """"""\n        win_sizes = copy.deepcopy(spatial_window)\n        if isinstance(spatial_window, dict):\n            for name in list(spatial_window):\n                window_size = spatial_window[name]\n                if isinstance(window_size,\n                              (ParserNamespace, argparse.Namespace)):\n                    window_size = vars(window_size)\n                if not isinstance(window_size, dict):\n                    win_sizes[name] = tuple(window_size)\n                elif \'spatial_window_size\' in window_size:\n                    win_sizes[name] = tuple(\n                        window_size[\'spatial_window_size\'])\n                else:\n                    raise ValueError(\n                        \'window_sizes should be a nested dictionary\')\n        elif isinstance(spatial_window, (list, tuple)):\n            # list or tuple of single window sizes\n            win_sizes = {name: spatial_window for name in list(self._dtypes)}\n\n        # complete window shapes based on user input and input_image sizes\n        if source_names:\n            spatial_shapes = _read_window_sizes(source_names, win_sizes)\n        else:\n            try:\n                spatial_shapes = {}\n                for name in list(self._dtypes):\n                    spatial_shapes[name] = \\\n                        tuple(int(win_size) for win_size in win_sizes[name])\n            except ValueError:\n                tf.logging.fatal(""spatial window should be an array of int"")\n                raise\n\n        spatial_shapes = nest.map_structure_up_to(\n            self._dtypes, tuple, spatial_shapes)\n\n        self._shapes = {\n            name: _complete_partial_window_sizes(spatial_shapes[name],\n                                                 self._shapes[name])\n            for name in list(self._shapes)}\n\n        # update based on the latest spatial shapes\n        self.has_dynamic_shapes = self._check_dynamic_shapes()\n        if self._placeholders_dict is not None:\n            self._update_placeholders_dict(n_samples=self.n_samples)\n\n    def placeholders_dict(self, n_samples=1):\n        """"""\n        This function create a dictionary with items of\n        ``{name: placeholders}``\n        name should match the queue input names\n        placeholders corresponds to the image window data\n        for each of these items an additional ``{location_name: placeholders}``\n        is created to hold the spatial location of the image window.\n        Used in the queue-based tensorflow APIs.\n\n        :param n_samples: specifies the number of image windows\n        :return: a dictionary with window data and locations placeholders\n        """"""\n\n        if self._placeholders_dict is not None:\n            return self._placeholders_dict\n        self._update_placeholders_dict(n_samples)\n        return self._placeholders_dict\n\n    def coordinates_placeholder(self, name):\n        """"""\n        Get coordinates placeholder, location name is formed\n        using ``LOCATION_FORMAT``.\n        Used in the queue-based tensorflow APIs.\n\n        :param name: input name string\n        :return: coordinates placeholder\n        """"""\n        try:\n            return self._placeholders_dict[LOCATION_FORMAT.format(name)]\n        except TypeError:\n            tf.logging.fatal(\'call placeholders_dict to initialise first\')\n            raise\n\n    def image_data_placeholder(self, name):\n        """"""\n        Get the image data placeholder by name.\n        Used in the queue-based tensorflow APIs.\n\n\n        :param name: input name string\n        :return: image placeholder\n        """"""\n        try:\n            return self._placeholders_dict[name]\n        except TypeError:\n            tf.logging.fatal(\'call placeholders_dict to initialise first\')\n            raise\n\n    def match_image_shapes(self, image_shapes):\n        """"""\n        If the window has dynamic shapes, this function\n        infers the fully specified shape from the image_shapes.\n\n        :param image_shapes:\n        :return: dict of fully specified window shapes\n        """"""\n        if not self.has_dynamic_shapes:\n            return self._shapes\n\n        static_window_shapes = self._shapes.copy()\n        # fill the None element in dynamic shapes using image_sizes\n        for name in list(self._shapes):\n            static_window_shapes[name] = tuple(\n                win_size if win_size else image_shape\n                for (win_size, image_shape) in\n                zip(list(self._shapes[name]), image_shapes[name]))\n        return static_window_shapes\n\n    def _update_placeholders_dict(self, n_samples=1):\n        """"""\n        Update the placeholders according to the new n_samples (batch_size).\n        Used in the queue-based tensorflow APIs.\n\n        :param n_samples:\n        :return:\n        """"""\n        # batch size=1 if the shapes are dynamic\n        self.n_samples = 1 if self.has_dynamic_shapes else n_samples\n\n        try:\n            self._placeholders_dict = {}\n            for name in list(self.tf_dtypes):\n                self._placeholders_dict[name] = tf.placeholder(\n                    dtype=self.tf_dtypes[name],\n                    shape=self.shapes[name],\n                    name=name)\n        except TypeError:\n            tf.logging.fatal(\n                \'shape should be defined as dict of iterable %s\', self.shapes)\n            raise\n\n    def _check_dynamic_shapes(self):\n        """"""\n        Check whether the shape of the window is fully specified.\n\n        :return: True indicates it\'s dynamic, False indicates\n            the window size is fully specified.\n        """"""\n        for shape in list(self._shapes.values()):\n            try:\n                for dim_length in shape:\n                    if not dim_length or dim_length < 0:\n                        return True\n            except TypeError:\n                return False\n        return False\n\n\ndef _read_window_sizes(input_mod_list, input_window_sizes):\n    """"""\n    Read window_size for each of the input image names defined\n    by input_mod_list.keys().\n\n    This function ensures that in the multimodality case\n    the spatial window sizes are the same across modalities.\n    For example::\n\n    # the input indicates `image` is a concatenation of `mr` and `ct`.\n    input_mod_list = {\'image\': (\'mr\', \'ct\')}\n    input_window_sizes = {\'mr\': (42, 42, 42)}\n    returns: {\'image\': (42, 42, 42)}\n\n    input_mod_list = (\'image\',)\n    input_window_sizes = {\'image\': (42, 42, 42)}\n    returns: {\'image\': (42, 42, 42)}\n\n    input_mod_list = (\'image\',)\n    input_window_sizes = (42, 42, 42)\n    returns: {\'image\': (42, 42, 42)}\n\n    # the input indicates a `image` and a `label` output.\n    input_mod_list = (\'image\',\'label\')\n    input_window_sizes = (42, 42, 42)\n    returns: {\'image\': (42, 42, 42), \'label\': (42, 42, 42)}\n\n    :param input_mod_list: list/dictionary of input source names\n    :param input_window_sizes: input source properties obtained\n        by parameters parser\n    :return: {\'output_name\': spatial window size} dictionary\n    """"""\n    window_sizes = {}\n    if isinstance(input_window_sizes, (tuple, list)):\n        try:\n            win_sizes = [int(win_size) for win_size in input_window_sizes]\n        except ValueError:\n            tf.logging.fatal(""spatial window should be an array of int"")\n            raise\n        # single window size for all inputs\n        for name in set(input_mod_list):\n            window_sizes[name] = win_sizes\n        return window_sizes\n\n    if isinstance(input_window_sizes, (ParserNamespace, argparse.Namespace)):\n        input_window_sizes = vars(input_window_sizes)\n\n    if not isinstance(input_window_sizes, dict):\n        raise ValueError(\'window sizes should be a list/tuple/dictionary\')\n\n    output_names = set(input_mod_list)\n    for name in output_names:\n        window_size = None\n        if name in input_window_sizes:\n            # resolve output window size as input_window_sizes spec.\n            window_size = input_window_sizes[name]\n        for mod in input_mod_list[name]:\n            # resolve output window size as input mod window size dict item\n            if mod in input_window_sizes:\n                window_size = input_window_sizes[mod]\n        if not window_size:\n            # input size not resolved\n            raise ValueError(\'Unknown output window size \'\n                             \'for input image {}\'.format(name))\n        if name in window_sizes:\n            assert window_size == window_sizes[name], \\\n                ""trying to use different window sizes for "" \\\n                ""the concatenated input {}"".format(name)\n        window_sizes[name] = window_size\n    return window_sizes\n\n\ndef _complete_partial_window_sizes(win_size, img_size):\n    """"""\n    Window size can be partially specified in the config.\n    This function complete the window size by making it\n    the same ndim as img_size, and set the not added dim\n    to size None. None values in window will be realised\n    when each image is loaded.\n\n    :param win_size: a tuple of (partial) window size\n    :param img_size: a tuple of image size\n    :return: a window size with the same ndim as image size,\n        with None components to be inferred at runtime\n    """"""\n    img_ndims = len(img_size)\n    # crop win_size list if it\'s longer than img_size\n    win_size = list(win_size[:img_ndims])\n    while len(win_size) < N_SPATIAL:\n        win_size.append(-1)\n    # complete win_size list if it\'s shorter than img_size\n    while len(win_size) < img_ndims:\n        win_size.append(img_size[len(win_size)])\n    # replace zero with full length in the n-th dim of image\n    win_size = [win if win > 0 else None for win in win_size]\n    return tuple(win_size)\n'"
niftynet/engine/image_window_dataset.py,21,"b'# -*- coding: utf-8 -*-\n""""""\nCreating ``tf.data.Dataset`` instance for image window sampler.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport inspect\n\nimport numpy as np\nimport tensorflow as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.data.util import nest\nfrom tensorflow.python.keras.utils import GeneratorEnqueuer\n\nfrom niftynet.engine.image_window import ImageWindow, N_SPATIAL, \\\n    LOCATION_FORMAT, BUFFER_POSITION_NP_TYPE\nfrom niftynet.io.misc_io import squeeze_spatial_temporal_dim\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.utilities.util_common import look_up_operations\n\n# when total number of window samples are not divisible by batch_size\n# the class supports different modes for the final batch\n#   \'drop\': drop the remainder batch\n#   \'pad\': padding the final smaller batch with -1s\n#   \'dynamic\': output the remainder directly (in this case the batch_size\n#              is undetermined at \'compile time\')\nSMALLER_FINAL_BATCH_MODE = {\'drop\', \'pad\', \'dynamic\'}\n\n\n# pylint: disable=too-many-instance-attributes\nclass ImageWindowDataset(Layer):\n    """"""\n    This class creates a ``tf.data.Dataset`` instance from\n    a sampler\'s layer_op function or generator.\n\n    If ``from_generator``, ``Dataset.from_generator`` interface will be used,\n    ``Dataset.map`` interface will be used otherwise::\n\n        if the windows are from a image reader,\n        the total number of windows produced\n        will be: `epoch x n_subjects x windows_per_image`\n\n        if the windows are from a generator,\n        the total number of windows produced\n        will be: ""iterations from the generator"" x num_threads\n\n    """"""\n\n    # pylint: disable=too-many-arguments\n    def __init__(self,\n                 reader=None,\n                 window_sizes=None,\n                 batch_size=1,\n                 windows_per_image=1,\n                 queue_length=10,\n                 shuffle=True,\n                 epoch=-1,\n                 smaller_final_batch_mode=\'pad\',\n                 seed=None,\n                 name=\'image_dataset\'):\n        Layer.__init__(self, name=name)\n\n        self._num_threads = 1\n        self._enqueuer = None\n        self._seed = seed\n\n        self.dataset = None\n        self.iterator = None\n        self.reader = reader\n\n        self.batch_size = batch_size\n        self.queue_length = int(max(queue_length, round(batch_size * 5.0)))\n        if self.queue_length > queue_length:\n            tf.logging.warning(\n                \'sampler queue_length should be larger than batch_size, \'\n                \'defaulting to batch_size * 5.0 (%s).\', self.queue_length)\n\n        self.from_generator = inspect.isgeneratorfunction(self.layer_op)\n        self.shuffle = shuffle\n        self.epoch = 1 if self.from_generator else epoch\n        self.smaller_final_batch_mode = look_up_operations(\n            smaller_final_batch_mode.lower(), SMALLER_FINAL_BATCH_MODE)\n\n        self.n_subjects = 1\n        self.window = None\n        if reader is not None:\n            self.window = ImageWindow.from_data_reader_properties(\n                reader.input_sources,\n                reader.shapes,\n                reader.tf_dtypes,\n                window_sizes or (-1, -1, -1))\n            self.n_subjects = reader.num_subjects\n            self.window.n_samples = windows_per_image\n\n    @property\n    def shapes(self):\n        """"""\n        the sampler output (value of ``layer_op``) is::\n\n            [windows_per_image, x, y, z, 1, channels]\n\n        returns a dictionary of sampler output shapes\n        """"""\n        assert self.window, \'Unknown output shapes: self.window not initialised\'\n        return self.window.shapes\n\n    @property\n    def tf_shapes(self):\n        """"""\n        returns a dictionary of sampler output tensor shapes\n        """"""\n        assert self.window, \'Unknown output shapes: self.window not initialised\'\n        return self.window.tf_shapes\n\n    @property\n    def tf_dtypes(self):\n        """"""\n        returns a dictionary of sampler output tensorflow dtypes\n        """"""\n        assert self.window, \'Unknown output dtypes: self.window not initialised\'\n        return self.window.tf_dtypes\n\n    def set_num_threads(self, num_threads):\n        """"""\n        Set number windows to generate in parallel.\n        """"""\n        self._num_threads = int(num_threads)\n\n    def layer_op(self, idx=None):\n        """"""\n        Generating each image as a window.\n        Overriding this function to create new image sampling strategies.\n\n        This function should either yield or return a dictionary\n        (of multiple windows per image)::\n\n            return a dictionary:\n            {\n             \'image_name\': a numpy array [n_samples, h, w, d, chn],\n             \'image_name_location\': [n_samples, 7]\n            }\n\n        where the 7-element location vector encode the image_id,\n        starting and ending coordinates of the image window.\n\n        Following the same notation, the dictionary can be extended\n        to multiple modalities; the keys will be::\n\n            {\'image_name_1\', \'image_name_1_location\',\n             \'image_name_2\', \'image_name_2_location\', ...}\n\n        :param idx: image_id used to load the image at the i-th row of\n            the input\n        :return: a image data dictionary\n        """"""\n        image_id, image_data, _ = self.reader(idx=idx)\n        for mod in list(image_data):\n            spatial_shape = image_data[mod].shape[:N_SPATIAL]\n            coords = self.dummy_coordinates(image_id, spatial_shape, 1)\n            image_data[LOCATION_FORMAT.format(mod)] = coords\n            image_data[mod] = image_data[mod][np.newaxis, ...]\n        return image_data\n\n        # # The following is a demo of generator as the layer_op\n        # # Often we don\'t know the total number of elements that\n        # # will be generated, epoch is always 1.\n        # for idx in range(100):\n        #     image_id, image_data, _ = self.reader()\n        #     for mod in list(image_data):\n        #         spatial_shape = image_data[mod].shape[:N_SPATIAL]\n        #         coords = self.dummy_coordinates(image_id, spatial_shape, 1)\n        #         image_data[LOCATION_FORMAT.format(mod)] = coords\n        #         image_data[mod] = image_data[mod][np.newaxis, ...]\n        #     yield image_data\n\n    def pop_batch_op(self):\n        """"""\n        This function is used when connecting a sampler output\n        to a network. e.g.::\n\n            data_dict = self.get_sampler()[0].pop_batch_op(device_id)\n            net_output = net_model(data_dict[\'image\'], is_training)\n\n        .. caution::\n\n            Note it squeezes the output tensor of 6 dims\n            ``[batch, x, y, z, time, modality]``\n            by removing all dims along which length is one.\n\n        :return: a dictionary of image window tensors.\n        """"""\n\n        if self.dataset is None or self.iterator is None:\n            # in case `run_threads` is not called,\n            # here we initialise the dataset and iterator\n            self.init_dataset()\n            self.iterator = self.dataset.make_one_shot_iterator()\n            # self.iterator = tf.data.Iterator.from_structure(\n            #     self.dataset.output_types, self.dataset.output_shapes)\n\n        window_output = self.iterator.get_next()\n        for name in window_output:\n            window_output[name] = squeeze_spatial_temporal_dim(\n                window_output[name])\n        return window_output\n\n    def init_dataset(self):\n        """"""\n        Make a window samples dataset from the reader and layer_op.\n        This function sets ``self.dataset``.\n\n        :return:\n        """"""\n        if not self.from_generator:\n            dataset = self._dataset_from_range()\n        else:\n            dataset = self._dataset_from_generator()\n        self.dataset = self.dataset_preprocessing(dataset)\n\n    def dataset_preprocessing(self, dataset):\n        """"""\n        dataset: batch and shuffle\n\n        :param dataset: a `tf.data.Dataset` instance\n        :return: a `tf.data.Dataset` instance\n        """"""\n        dataset = dataset.repeat(self.epoch)\n        dataset = dataset.prefetch(buffer_size=self.queue_length)\n        if self.shuffle:\n            # locally shuffle the buffer of image windows\n            dataset = dataset.shuffle(\n                buffer_size=self.queue_length, seed=self._seed)\n\n        if self.smaller_final_batch_mode == \'drop\':\n            # drop the remainder if there\'s not enough windows to\n            # form a batch, so that we have a fixed batch size.\n            # dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(\n            #     batch_size=self.batch_size))\n            # new API since TF 1.10\n            dataset = dataset.batch(batch_size=self.batch_size,\n                                    drop_remainder=True)\n            return dataset\n\n        dataset = dataset.batch(batch_size=self.batch_size)\n\n        if self.smaller_final_batch_mode == \'dynamic\' and self.batch_size > 1:\n            return dataset\n\n        # self.smaller_final_batch_mode is \'pad\'\n        # if self.batch_size == 1 no actual padding\n        # but this function will set the output shapes properly.\n        def _pad_batch(batch_size):\n            def _pad_batch_func(input_tensor_dict):\n                """"""\n                function to pad the batch dim to `batch_size`.\n                (assuming the input dataset is a dictionary-based one)\n                """"""\n                out_dict = {}\n                for in_name in list(input_tensor_dict):\n                    in_var = input_tensor_dict[in_name]\n                    var_shape = in_var.shape.as_list()\n                    if batch_size > 1:\n                        paddings = [[0, 0] for _ in in_var.shape]\n                        paddings[0][1] = batch_size - tf.shape(in_var)[0]\n                        in_var = tf.pad(\n                            in_var, paddings, ""CONSTANT"", constant_values=-1)\n                    var_shape[0] = batch_size\n                    in_var.set_shape(var_shape)\n                    out_dict[in_name] = in_var\n                return out_dict\n\n            return _pad_batch_func\n\n        dataset = dataset.map(_pad_batch(self.batch_size))\n        return dataset\n\n    # pylint: disable=redefined-variable-type\n    def _dataset_from_range(self):\n        """"""\n        This function maps a dataset of integers to a dataset of images.\n\n        :return: a `tf.data.Dataset`\n        """"""\n        # dataset: a list of integers\n        tf.logging.info(\n            \'Initialising Dataset from %s subjects...\', self.n_subjects)\n        dataset = tf.data.Dataset.range(self.n_subjects)\n        if self.shuffle:\n            # global shuffle of the entire set of subjects\n            dataset = dataset.shuffle(\n                buffer_size=self.n_subjects, seed=self._seed)\n\n        # dataset: map each integer i to n windows sampled from subject i\n        def _tf_wrapper(idx):\n            flattened_types = nest.flatten(self.tf_dtypes)\n            flattened_shapes = nest.flatten(self.tf_shapes)\n            flat_values = tf.py_func(\n                func=lambda subject_id: nest.flatten(self(subject_id)),\n                inp=[idx],\n                Tout=flattened_types)\n            for ret_t, shape in zip(flat_values, flattened_shapes):\n                # the actual returned numpy array shapes are not checked\n                ret_t.set_shape(shape)\n            return nest.pack_sequence_as(self.tf_dtypes, flat_values)\n\n        dataset = dataset.map(_tf_wrapper, num_parallel_calls=self._num_threads)\n\n        # dataset: slice the n-element window into n single windows\n        dataset = dataset.flat_map(map_func=tf.data.Dataset.from_tensor_slices)\n        return dataset\n\n    def _dataset_from_generator(self):\n        """"""\n        Create a `tf.data.Dataset` from a layer_op (as a generator).\n\n        :return: a `tf.data.Dataset`\n        """"""\n        tf.logging.info(\'Initialising dataset from generator...\')\n\n        if self._num_threads < 2 or not self.shuffle:\n            window_generator = self\n        else:\n            # self._enqueuer = GeneratorEnqueuer(\n            #     self(),\n            #     use_multiprocessing=True,\n            #     wait_time=0.01,\n            #     seed=self._seed)\n            self._enqueuer = GeneratorEnqueuer(\n                self(),\n                use_multiprocessing=True)\n            self._enqueuer.start(\n                workers=self._num_threads, max_queue_size=self.queue_length)\n            window_generator = self._enqueuer.get\n\n        # dataset from generator\n        dataset = tf.data.Dataset.from_generator(\n            generator=window_generator,\n            output_types=self.tf_dtypes,\n            output_shapes=self.tf_shapes)\n\n        # dataset: slice the n-element window into n single windows\n        dataset = dataset.flat_map(map_func=tf.data.Dataset.from_tensor_slices)\n        return dataset\n\n    def run_threads(self, *_args, **_kwargs):\n        """"""\n        This function is created for compatibility purposes\n\n        (Deprecating)\n\n        :param _args:\n        :param _kwargs:\n        :return:\n        """"""\n        pass\n        # if self.dataset is None or self.iterator is None:\n        #     self.init_dataset()\n        #     self.iterator = self.dataset.make_one_shot_iterator()\n\n        #     self.iterator = tf.data.Iterator.from_structure(\n        #         self.dataset.output_types, self.dataset.output_shapes)\n        # sess = session or tf.get_default_session()\n        # if sess is not None:\n        #     sess.run(self.iterator.make_initializer(self.dataset))\n\n    def close_all(self):\n        """"""\n        For compatibility with the queue-based sampler.\n        """"""\n        if self._enqueuer is not None:\n            self._enqueuer.stop()\n\n    @classmethod\n    def dummy_coordinates(cls, image_id, image_sizes, n_samples):\n        """"""\n        This function returns a set of image window coordinates\n        which are just spatially from 0 to `image_sizes`.\n\n        :return: a numpy array of `n_samples` spatial coordinates\n        """"""\n\n        starting_coordinates = [0, 0, 0]\n        image_spatial_shape = list(image_sizes[:N_SPATIAL])\n        coords = [image_id] + starting_coordinates + image_spatial_shape\n        coords = np.tile(np.asarray(coords), [n_samples, 1])\n        return coords.astype(BUFFER_POSITION_NP_TYPE)\n'"
niftynet/engine/sampler_balanced_v2.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nGenerate image windows from a balanced sampling map as if every label\nhad the same probability of occurrence.\n\nConsider a mask with three classes I, J, K with prevalence 0.1, 0.1, and\n0.8, respectively. If 100 samples are drawn from the balanced sampler, the\nclasses should be approximately 33 I, 33 J, and 33 K.\n\nThis can also be considered a ""balanced random cropping"" layer of the\ninput image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.image_window import N_SPATIAL\nfrom niftynet.engine.sampler_weighted_v2 import crop_sampling_map\n\n\nclass BalancedSampler(UniformSampler):\n    """"""\n    This class generators samples from a user provided frequency map for each\n    input volume. The sampling likelihood of each voxel is proportional its\n    intra class frequency. That is, if a given voxel is of class `A` and there\n    are 20 voxels with class `A`, the probability of selecting this voxel is\n    5%. If there are 10 classes, the probability becomes 10% * 5% = 0.5%.\n\n    In general, the likelihood of sampling a voxel is given by:\n        p(v) = (1)/(# of unique labels * # of voxels with same class as v)\n\n    This is done for balanced sampling. In the case of unbalanced labels,\n    this sampler should produce a roughly equal probability of sampling each\n    class.\n\n    This layer can be considered as a ""balanced random cropping"" layer of the\n    input image.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 window_sizes,\n                 batch_size=1,\n                 windows_per_image=1,\n                 queue_length=10,\n                 name=\'balanced_sampler\'):\n        UniformSampler.__init__(self,\n                                reader=reader,\n                                window_sizes=window_sizes,\n                                batch_size=batch_size,\n                                windows_per_image=windows_per_image,\n                                queue_length=queue_length,\n                                name=name)\n        tf.logging.info(\'Initialised balanced sampler window instance\')\n        self.window_centers_sampler = balanced_spatial_coordinates\n\n\ndef balanced_spatial_coordinates(\n        n_samples, img_spatial_size, win_spatial_size, sampler_map):\n    """"""\n    Perform balanced sampling.\n\n    Each label in the input tensor has an equal probability of\n    being sampled.\n\n    :param n_samples: number of random coordinates to generate\n    :param img_spatial_size: input image size\n    :param win_spatial_size: input window size\n    :param sampler_map: sampling prior map, it\'s spatial shape should be\n            consistent with `img_spatial_size`\n    :return: (n_samples, N_SPATIAL) coordinates representing sampling\n              window centres relative to img_spatial_size\n    """"""\n    assert sampler_map is not None, \\\n        \'sampling prior map is not specified, \' \\\n        \'please check `sampler=` option in the config.\'\n    assert np.all(img_spatial_size[:N_SPATIAL] ==\n                  sampler_map.shape[:N_SPATIAL]), \\\n        \'image and sampling map shapes do not match\'\n\n    # Find the number of unique labels\n    win_spatial_size = np.asarray(win_spatial_size, dtype=np.int32)\n    cropped_map = crop_sampling_map(sampler_map, win_spatial_size)\n\n    flatten_map = cropped_map.flatten()\n    unique_labels = np.unique(flatten_map)\n    if len(unique_labels) > 500:\n        tf.logging.warning(\n            ""unusual discrete volume: number of unique ""\n            ""labels: %s"", len(unique_labels))\n\n    # system parameter?\n    class_probs = [1.0 / len(unique_labels)] * len(unique_labels)\n    label_counts = np.random.multinomial(n_samples, class_probs)\n    # Look inside each label and sample `count`. Add the middle_coord of\n    # each sample to `middle_coords`\n    middle_coords = np.zeros((n_samples, N_SPATIAL), dtype=np.int32)\n    sample_count = 0\n    for label, count in zip(unique_labels, label_counts):\n        # Get indices where(cropped_map == label)\n        valid_locations = np.where(flatten_map == label)[0]\n\n        # Sample `count` from those indices. Need replace=True. Consider the\n        # case where all pixels are background except for one pixel which is\n        # foreground. We ask for 10 samples. We should get 5 samples from\n        # background and the foreground pixel sampled 5 times (give or take\n        # random fluctuation).\n        try:\n            samples = np.random.choice(\n                valid_locations,\n                size=count,\n                replace=True)\n        except ValueError:\n            tf.logging.fatal(""unable to choose sampling window based on ""\n                             ""the current frequency map."")\n            raise\n\n        assert count == samples.size, ""Unable to sample from the image""\n\n        # Place into `middle_coords`\n        for sample in samples:\n            middle_coords[sample_count, :N_SPATIAL] = \\\n                np.unravel_index(sample, cropped_map.shape)[:N_SPATIAL]\n            sample_count += 1\n\n    # re-shift coords due to the crop\n    half_win = np.floor(win_spatial_size / 2).astype(np.int32)\n    middle_coords[:, :N_SPATIAL] = \\\n        middle_coords[:, :N_SPATIAL] + half_win[:N_SPATIAL]\n    return middle_coords\n'"
niftynet/engine/sampler_grid_v2.py,9,"b'# -*- coding: utf-8 -*-\n""""""\nSampling image by a sliding window.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\n\n\n# pylint: disable=too-many-locals\nclass GridSampler(ImageWindowDataset):\n    """"""\n    This class generators ND image samples with a sliding window.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 window_sizes,\n                 batch_size=1,\n                 spatial_window_size=None,\n                 window_border=None,\n                 queue_length=10,\n                 smaller_final_batch_mode=\'pad\',\n                 name=\'grid_sampler\'):\n\n        # override all spatial window defined in input\n        # modalities sections\n        # this is useful when do inference with a spatial window\n        # which is different from the training specifications\n        ImageWindowDataset.__init__(\n            self,\n            reader=reader,\n            window_sizes=spatial_window_size or window_sizes,\n            batch_size=batch_size,\n            windows_per_image=1,\n            queue_length=queue_length,\n            shuffle=False,\n            epoch=1,\n            smaller_final_batch_mode=smaller_final_batch_mode,\n            name=name)\n\n        self.border_size = window_border or (0, 0, 0)\n        assert isinstance(self.border_size, (list, tuple)), \\\n            ""window_border should be a list or tuple""\n        while len(self.border_size) < N_SPATIAL:\n            self.border_size = tuple(self.border_size) + \\\n                               (self.border_size[-1],)\n        self.border_size = self.border_size[:N_SPATIAL]\n        tf.logging.info(\'initialised window instance\')\n        tf.logging.info(""initialised grid sampler %s"", self.window.shapes)\n\n    def layer_op(self):\n        while True:\n            image_id, data, _ = self.reader(idx=None, shuffle=False)\n            if not data:\n                break\n            image_shapes = {name: data[name].shape\n                            for name in self.window.names}\n            static_window_shapes = self.window.match_image_shapes(image_shapes)\n            coordinates = grid_spatial_coordinates(\n                image_id, image_shapes, static_window_shapes, self.border_size)\n\n            # extend the number of sampling locations to be divisible\n            # by batch size\n            n_locations = list(coordinates.values())[0].shape[0]\n            extra_locations = 0\n            if (n_locations % self.batch_size) > 0:\n                extra_locations = \\\n                    self.batch_size - n_locations % self.batch_size\n            total_locations = n_locations + extra_locations\n\n            tf.logging.info(\n                \'grid sampling image sizes: %s\', image_shapes)\n            tf.logging.info(\n                \'grid sampling window sizes: %s\', static_window_shapes)\n            if extra_locations > 0:\n                tf.logging.info(\n                    ""yielding %s locations from image, ""\n                    ""extended to %s to be divisible by batch size %s"",\n                    n_locations, total_locations, self.batch_size)\n            else:\n                tf.logging.info(\n                    ""yielding %s locations from image"", n_locations)\n            for i in range(total_locations):\n                idx = i % n_locations\n                # \xc2\xa0initialise output dict\n                output_dict = {}\n                for name in list(data):\n                    assert coordinates[name].shape[0] == n_locations, \\\n                        ""different number of grid samples from the input"" \\\n                        ""images, don\'t know how to combine them in the queue""\n                    x_start, y_start, z_start, x_end, y_end, z_end = \\\n                        coordinates[name][idx, 1:]\n                    try:\n                        image_window = data[name][\n                            x_start:x_end, y_start:y_end, z_start:z_end, ...]\n                    except ValueError:\n                        tf.logging.fatal(\n                            ""dimensionality miss match in input volumes, ""\n                            ""please specify spatial_window_size with a ""\n                            ""3D tuple and make sure each element is ""\n                            ""smaller than the image length in each dim."")\n                        raise\n                    # fill output dict with data\n                    coord_key = LOCATION_FORMAT.format(name)\n                    image_key = name\n                    output_dict[coord_key] = coordinates[name][idx:idx+1, ...]\n                    output_dict[image_key] = image_window[np.newaxis, ...]\n                yield output_dict\n\n        # this is needed because otherwise reading beyond the last element\n        # raises an out-of-range error, and the last grid sample\n        # will not be processed properly.\n        try:\n            for _ in range(1):\n                for name in list(output_dict):\n                    output_dict[name] = np.ones_like(output_dict[name]) * -1\n                yield output_dict\n        except (NameError, KeyError):\n            tf.logging.fatal(""No feasible samples from %s"", self)\n            raise\n\n\ndef grid_spatial_coordinates(subject_id, img_sizes, win_sizes, border_size):\n    """"""\n    This function generates all coordinates of feasible windows, with\n    step sizes specified in grid_size parameter.\n\n    The border size changes the sampling locations but not the\n    corresponding window sizes of the coordinates.\n\n    :param subject_id: integer value indicates the position of of this\n        image in ``image_reader.file_list``\n    :param img_sizes: a dictionary of image shapes, ``{input_name: shape}``\n    :param win_sizes: a dictionary of window shapes, ``{input_name: shape}``\n    :param border_size: size of padding on both sides of each dim\n    :return:\n    """"""\n    all_coordinates = {}\n    for name, image_shape in img_sizes.items():\n        window_shape = win_sizes[name]\n        grid_size = [max(win_size - 2 * border, 0)\n                     for (win_size, border) in zip(window_shape, border_size)]\n        assert len(image_shape) >= N_SPATIAL, \\\n            \'incompatible image shapes in grid_spatial_coordinates\'\n        assert len(window_shape) >= N_SPATIAL, \\\n            \'incompatible window shapes in grid_spatial_coordinates\'\n        assert len(grid_size) >= N_SPATIAL, \\\n            \'incompatible border sizes in grid_spatial_coordinates\'\n        steps_along_each_dim = [\n            _enumerate_step_points(starting=0,\n                                   ending=image_shape[i],\n                                   win_size=window_shape[i],\n                                   step_size=grid_size[i])\n            for i in range(N_SPATIAL)]\n        starting_coords = np.asanyarray(np.meshgrid(*steps_along_each_dim))\n        starting_coords = starting_coords.reshape((N_SPATIAL, -1)).T\n        n_locations = starting_coords.shape[0]\n        # prepare the output coordinates matrix\n        spatial_coords = np.zeros((n_locations, N_SPATIAL * 2), dtype=np.int32)\n        spatial_coords[:, :N_SPATIAL] = starting_coords\n        for idx in range(N_SPATIAL):\n            spatial_coords[:, N_SPATIAL + idx] = \\\n                starting_coords[:, idx] + window_shape[idx]\n        max_coordinates = np.max(spatial_coords, axis=0)[N_SPATIAL:]\n        assert np.all(max_coordinates <= image_shape[:N_SPATIAL]), \\\n            ""window size greater than the spatial coordinates {} : {}"".format(\n                max_coordinates, image_shape)\n        subject_list = np.ones((n_locations, 1), dtype=np.int32) * subject_id\n        spatial_coords = np.append(subject_list, spatial_coords, axis=1)\n        all_coordinates[name] = spatial_coords\n    return all_coordinates\n\n\ndef _enumerate_step_points(starting, ending, win_size, step_size):\n    """"""\n    generate all possible sampling size in between starting and ending.\n\n    :param starting: integer of starting value\n    :param ending: integer of ending value\n    :param win_size: integer of window length\n    :param step_size: integer of distance between two sampling points\n    :return: a set of unique sampling points\n    """"""\n    try:\n        starting = max(int(starting), 0)\n        ending = max(int(ending), 0)\n        win_size = max(int(win_size), 1)\n        step_size = max(int(step_size), 1)\n    except (TypeError, ValueError):\n        tf.logging.fatal(\n            \'step points should be specified by integers, received:\'\n            \'%s, %s, %s, %s\', starting, ending, win_size, step_size)\n        raise ValueError\n    if starting > ending:\n        starting, ending = ending, starting\n    sampling_point_set = []\n    while (starting + win_size) <= ending:\n        sampling_point_set.append(starting)\n        starting = starting + step_size\n    additional_last_point = ending - win_size\n    sampling_point_set.append(max(additional_last_point, 0))\n    sampling_point_set = np.unique(sampling_point_set).flatten()\n    if len(sampling_point_set) == 2:\n        # in case of too few samples, adding\n        # an additional sampling point to\n        # the middle between starting and ending\n        sampling_point_set = np.append(\n            sampling_point_set, np.round(np.mean(sampling_point_set)))\n    _, uniq_idx = np.unique(sampling_point_set, return_index=True)\n    return sampling_point_set[np.sort(uniq_idx)]\n'"
niftynet/engine/sampler_linear_interpolate_v2.py,1,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating samples by linearly combining two input images.\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\n\n\nclass LinearInterpolateSampler(ImageWindowDataset):\n    """"""\n    This class reads two feature vectors from files (often generated\n    by running feature extractors on images in advance)\n    and returns n linear combinations of the vectors.\n    The coefficients are generated by::\n\n        np.linspace(0, 1, n_interpolations)\n    """"""\n\n    def __init__(self,\n                 reader,\n                 window_sizes,\n                 batch_size=10,\n                 n_interpolations=10,\n                 queue_length=10,\n                 name=\'linear_interpolation_sampler\'):\n        ImageWindowDataset.__init__(\n            self,\n            reader,\n            window_sizes=window_sizes,\n            batch_size=batch_size,\n            queue_length=queue_length,\n            shuffle=False,\n            epoch=1,\n            smaller_final_batch_mode=\'drop\',\n            name=name)\n        self.n_interpolations = n_interpolations\n        # only try to use the first spatial shape available\n        image_spatial_shape = list(self.reader.shapes.values())[0][:3]\n        self.window.set_spatial_shape(image_spatial_shape)\n        tf.logging.info(\n            ""initialised linear interpolation sampler %s "", self.window.shapes)\n        assert not self.window.has_dynamic_shapes, \\\n            ""dynamic shapes not supported, please specify "" \\\n            ""spatial_window_size = (1, 1, 1)""\n\n    def layer_op(self, *_unused_args, **_unused_kwargs):\n        """"""\n        This function first reads two vectors, and interpolates them\n        with self.n_interpolations mixing coefficients.\n\n        Location coordinates are set to ``np.ones`` for all the vectors.\n        """"""\n        while True:\n            image_id_x, data_x, _ = self.reader(idx=None, shuffle=False)\n            image_id_y, data_y, _ = self.reader(idx=None, shuffle=True)\n            if not data_x or not data_y:\n                break\n            if image_id_x == image_id_y:\n                continue\n            embedding_x = data_x[self.window.names[0]]\n            embedding_y = data_y[self.window.names[0]]\n\n            steps = np.linspace(0, 1, self.n_interpolations)\n            for (_, mixture) in enumerate(steps):\n                output_vector = \\\n                    embedding_x * mixture + embedding_y * (1 - mixture)\n                coordinates = np.ones((1, N_SPATIAL * 2 + 1), dtype=np.int32)\n                coordinates[0, 0:2] = [image_id_x, image_id_y]\n                output_dict = {}\n                for name in self.window.names:\n                    coordinates_key = LOCATION_FORMAT.format(name)\n                    image_data_key = name\n                    output_dict[coordinates_key] = coordinates\n                    output_dict[image_data_key] = output_vector[np.newaxis, ...]\n                yield output_dict\n'"
niftynet/engine/sampler_random_vector_v2.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating sample arrays from random distributions.\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.engine.image_window import \\\n    N_SPATIAL, LOCATION_FORMAT, ImageWindow\n\n\nclass RandomVectorSampler(ImageWindowDataset):\n    """"""\n    This class generates two samples from the standard normal\n    distribution.  These two samples are mixed with n\n    mixing coefficients. The coefficients are generated\n    by ``np.linspace(0, 1, n_interpolations)``\n    """"""\n\n    def __init__(self,\n                 names=(\'vector\',),\n                 vector_size=(100,),\n                 batch_size=10,\n                 n_interpolations=10,\n                 mean=0.0,\n                 stddev=1.0,\n                 repeat=1,\n                 queue_length=10,\n                 name=\'random_vector_sampler\'):\n        # repeat=None for infinite loops\n        self.n_interpolations = max(n_interpolations, 1)\n        self.mean = mean\n        self.stddev = stddev\n        self.repeat = repeat\n        self.names = names\n\n        ImageWindowDataset.__init__(\n            self,\n            reader=None,\n            window_sizes={names[0]: {\'spatial_window_size\': vector_size}},\n            batch_size=batch_size,\n            queue_length=queue_length,\n            shuffle=False,\n            epoch=1,\n            smaller_final_batch_mode=\'drop\',\n            name=name)\n        self.window = ImageWindow(shapes={names[0]: vector_size},\n                                  dtypes={names[0]: tf.float32})\n        tf.logging.info(""initialised sampler output %s "", self.window.shapes)\n\n    def layer_op(self):\n        """"""\n        This function first draws two samples, and interpolates them\n        with self.n_interpolations mixing coefficients.\n\n        Location coordinates are set to ``np.ones`` for all the vectors.\n        """"""\n        total_iter = self.repeat if self.repeat is not None else 1\n        while total_iter > 0:\n            total_iter = total_iter - 1 if self.repeat is not None else 1\n            embedding_x = np.random.normal(\n                self.mean,\n                self.stddev,\n                self.window.shapes[self.window.names[0]])\n            embedding_y = np.random.normal(\n                self.mean,\n                self.stddev,\n                self.window.shapes[self.window.names[0]])\n            steps = np.linspace(0, 1, self.n_interpolations)\n            for (_, mixture) in enumerate(steps):\n                output_vector = \\\n                    embedding_x * mixture + embedding_y * (1 - mixture)\n                coordinates = np.ones((1, N_SPATIAL * 2 + 1), dtype=np.int32)\n                output_dict = {}\n                for name in self.window.names:\n                    coordinates_key = LOCATION_FORMAT.format(name)\n                    image_data_key = name\n                    output_dict[coordinates_key] = coordinates\n                    output_dict[image_data_key] = output_vector\n                yield output_dict\n'"
niftynet/engine/sampler_resize_v2.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nResize input image as output window.\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport scipy.ndimage\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.engine.image_window import LOCATION_FORMAT\n\n\nclass ResizeSampler(ImageWindowDataset):\n    """"""\n    This class generates samples by rescaling\n    the whole image to the desired size\n    Assuming the reader\'s output is 5d:\n    ``Height x Width x Depth x time x Modality``\n    """"""\n\n    def __init__(self,\n                 reader,\n                 window_sizes,\n                 batch_size=1,\n                 spatial_window_size=None,\n                 windows_per_image=1,\n                 shuffle=True,\n                 queue_length=10,\n                 smaller_final_batch_mode=\'pad\',\n                 name=\'resize_sampler_v2\'):\n        tf.logging.info(\'reading size of preprocessed images\')\n        ImageWindowDataset.__init__(\n            self,\n            reader=reader,\n            window_sizes=window_sizes,\n            batch_size=batch_size,\n            windows_per_image=windows_per_image,\n            queue_length=queue_length,\n            shuffle=shuffle,\n            epoch=-1 if shuffle else 1,\n            smaller_final_batch_mode=smaller_final_batch_mode,\n            name=name)\n        if spatial_window_size:\n            # override all spatial window defined in input\n            # modalities sections\n            # this is useful when do inference with a spatial window\n            # which is different from the training specifications\n            self.window.set_spatial_shape(spatial_window_size)\n        tf.logging.info(""initialised resize sampler %s "", self.window.shapes)\n\n    def layer_op(self, idx=None):\n        """"""\n        This function generates sampling windows to the input buffer\n        image data are from ``self.reader()``.\n\n        It first completes window shapes based on image data,\n        then resize each image as window and output\n        a dictionary (required by input buffer)\n\n        :return: output data dictionary ``{\'image_modality\': data_array}``\n        """"""\n        image_id, data, interp_orders = self.reader(idx=idx)\n        image_shapes = \\\n            dict((name, data[name].shape) for name in self.window.names)\n        # window shapes can be dynamic, here they\n        # are converted to static ones\n        # as now we know the image shapes\n        static_window_shapes = self.window.match_image_shapes(image_shapes)\n\n        # for resize sampler the coordinates are not used\n        # simply use the spatial dims of the input image\n        output_dict = {}\n        for name in list(data):\n            # prepare output dictionary keys\n            coordinates_key = LOCATION_FORMAT.format(name)\n            image_data_key = name\n\n            output_dict[coordinates_key] = self.dummy_coordinates(\n                image_id, static_window_shapes[name], self.window.n_samples)\n            image_array = []\n            for _ in range(self.window.n_samples):\n                # prepare image data\n                image_shape = image_shapes[name]\n                window_shape = static_window_shapes[name]\n\n                if image_shape == window_shape or interp_orders[name][0] < 0:\n                    # already in the same shape\n                    image_window = data[name]\n                else:\n                    zoom_ratio = [float(p) / float(d) for p, d in\n                                  zip(window_shape, image_shape)]\n                    image_window = zoom_3d(image=data[name],\n                                           ratio=zoom_ratio,\n                                           interp_order=interp_orders[name][0])\n                image_array.append(image_window[np.newaxis, ...])\n            if len(image_array) > 1:\n                output_dict[image_data_key] = \\\n                    np.concatenate(image_array, axis=0)\n            else:\n                output_dict[image_data_key] = image_array[0]\n        # the output image shape should be\n        # [enqueue_batch_size, x, y, z, time, modality]\n        # here enqueue_batch_size = 1 as we only have one sample\n        # per image\n        return output_dict\n\n\ndef zoom_3d(image, ratio, interp_order):\n    """"""\n    Taking 5D image as input, and zoom each 3D slice independently\n    """"""\n    assert image.ndim == 5, ""input images should be 5D array""\n    output = []\n    for time_pt in range(image.shape[3]):\n        output_mod = []\n        for mod in range(image.shape[4]):\n            zoomed = scipy.ndimage.zoom(\n                image[..., time_pt, mod], ratio[:3], order=interp_order)\n            output_mod.append(zoomed[..., np.newaxis, np.newaxis])\n        output.append(np.concatenate(output_mod, axis=-1))\n    return np.concatenate(output, axis=-2)\n'"
niftynet/engine/sampler_uniform_v2.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating uniformly distributed image window from input image\nThis can also be considered as a ""random cropping"" layer of the\ninput image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\n\n\nclass UniformSampler(ImageWindowDataset):\n    """"""\n    This class generates samples by uniformly sampling each input volume\n    currently the coordinates are randomised for spatial dims only,\n    i.e., the first three dims of image.\n\n    This layer can be considered as a ""random cropping"" layer of the\n    input image.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 window_sizes,\n                 batch_size=1,\n                 windows_per_image=1,\n                 queue_length=10,\n                 name=\'uniform_sampler_v2\'):\n        ImageWindowDataset.__init__(\n            self,\n            reader=reader,\n            window_sizes=window_sizes,\n            batch_size=batch_size,\n            windows_per_image=windows_per_image,\n            queue_length=queue_length,\n            shuffle=True,\n            epoch=-1,\n            smaller_final_batch_mode=\'drop\',\n            name=name)\n\n        tf.logging.info(""initialised uniform sampler %s "", self.window.shapes)\n        self.window_centers_sampler = rand_spatial_coordinates\n\n    # pylint: disable=too-many-locals\n    def layer_op(self, idx=None):\n        """"""\n        This function generates sampling windows to the input buffer\n        image data are from ``self.reader()``\n\n        It first completes window shapes based on image data,\n        then finds random coordinates based on the window shapes\n        finally extract window with the coordinates and output\n        a dictionary (required by input buffer).\n\n        :return: output data dictionary\n            ``{image_modality: data_array, image_location: n_samples * 7}``\n        """"""\n        image_id, data, _ = self.reader(idx=idx, shuffle=True)\n        image_shapes = dict(\n            (name, data[name].shape) for name in self.window.names)\n        static_window_shapes = self.window.match_image_shapes(image_shapes)\n\n        # find random coordinates based on window and image shapes\n        coordinates = self._spatial_coordinates_generator(\n            subject_id=image_id,\n            data=data,\n            img_sizes=image_shapes,\n            win_sizes=static_window_shapes,\n            n_samples=self.window.n_samples)\n\n        # initialise output dict, placeholders as dictionary keys\n        # this dictionary will be used in\n        # enqueue operation in the form of: `feed_dict=output_dict`\n        output_dict = {}\n        # fill output dict with data\n        for name in list(data):\n            coordinates_key = LOCATION_FORMAT.format(name)\n            image_data_key = name\n\n            # fill the coordinates\n            location_array = coordinates[name]\n            output_dict[coordinates_key] = location_array\n\n            # fill output window array\n            image_array = []\n            for window_id in range(self.window.n_samples):\n                x_start, y_start, z_start, x_end, y_end, z_end = \\\n                    location_array[window_id, 1:]\n                try:\n                    image_window = data[name][\n                        x_start:x_end, y_start:y_end, z_start:z_end, ...]\n                    image_array.append(image_window[np.newaxis, ...])\n                except ValueError:\n                    tf.logging.fatal(\n                        ""dimensionality miss match in input volumes, ""\n                        ""please specify spatial_window_size with a ""\n                        ""3D tuple and make sure each element is ""\n                        ""smaller than the image length in each dim. ""\n                        ""Current coords %s"", location_array[window_id])\n                    raise\n            if len(image_array) > 1:\n                output_dict[image_data_key] = \\\n                    np.concatenate(image_array, axis=0)\n            else:\n                output_dict[image_data_key] = image_array[0]\n        # the output image shape should be\n        # [enqueue_batch_size, x, y, z, time, modality]\n        # where enqueue_batch_size = windows_per_image\n        return output_dict\n\n    def _spatial_coordinates_generator(self,\n                                       subject_id,\n                                       data,\n                                       img_sizes,\n                                       win_sizes,\n                                       n_samples=1):\n        """"""\n        Generate spatial coordinates for sampling.\n\n        Values in ``win_sizes`` could be different --\n        for example in a segmentation network ``win_sizes`` could be\n        ``{\'training_image_spatial_window\': (32, 32, 10),\n           \'Manual_label_spatial_window\': (16, 16, 10)}``\n        (the network reduces x-y plane spatial resolution).\n\n        This function handles this situation by first find the largest\n        window across these window definitions, and generate the coordinates.\n        These coordinates are then adjusted for each of the\n        smaller window sizes (the output windows are almost concentric).\n        """"""\n\n        assert data is not None, ""No input from image reader. Please check"" \\\n                                 ""the configuration file.""\n\n        # infer the largest spatial window size and check image spatial shapes\n        img_spatial_size, win_spatial_size = \\\n            _infer_spatial_size(img_sizes, win_sizes)\n\n        sampling_prior_map = None\n        try:\n            sampling_prior_map = data.get(\'sampler\', None)\n        except AttributeError:\n            pass\n\n        n_samples = max(n_samples, 1)\n        window_centres = self.window_centers_sampler(\n            n_samples, img_spatial_size, win_spatial_size, sampling_prior_map)\n        assert window_centres.shape == (n_samples, N_SPATIAL), \\\n            ""the coordinates generator should return "" \\\n            ""{} samples of rank {} locations"".format(n_samples, N_SPATIAL)\n\n        # adjust spatial coordinates based on each mod spatial window size\n        all_coordinates = {}\n        for mod in list(win_sizes):\n            win_size = np.asarray(win_sizes[mod][:N_SPATIAL])\n            half_win = np.floor(win_size / 2.0).astype(int)\n\n            # Make starting coordinates of the window\n            spatial_coords = np.zeros(\n                (n_samples, N_SPATIAL * 2), dtype=np.int32)\n            spatial_coords[:, :N_SPATIAL] = np.maximum(\n                window_centres[:, :N_SPATIAL] - half_win[:N_SPATIAL], 0)\n\n            # Make the opposite corner of the window is\n            # just adding the mod specific window size\n            spatial_coords[:, N_SPATIAL:] = \\\n                spatial_coords[:, :N_SPATIAL] + win_size[:N_SPATIAL]\n            assert np.all(spatial_coords[:, N_SPATIAL:] <= img_spatial_size), \\\n                \'spatial coords: out of bounds.\'\n\n            # include subject id as the 1st column of all_coordinates values\n            subject_id = np.ones((n_samples,), dtype=np.int32) * subject_id\n            spatial_coords = np.append(\n                subject_id[:, None], spatial_coords, axis=1)\n            all_coordinates[mod] = spatial_coords\n\n        return all_coordinates\n\n\ndef rand_spatial_coordinates(\n        n_samples, img_spatial_size, win_spatial_size, sampler_map):\n    """"""\n    Generate spatial coordinates from a discrete uniform distribution.\n\n    :param n_samples: number of random coordinates to generate\n    :param img_spatial_size: input image size\n    :param win_spatial_size: input window size\n    :param sampler_map: sampling prior map (not in use)\n    :return: (n_samples, N_SPATIAL) coordinates representing sampling\n              window centres relative to img_spatial_size\n    """"""\n    tf.logging.debug(\'uniform sampler, prior %s ignored\', sampler_map)\n\n    # Sample coordinates at random\n    half_win = np.floor(np.asarray(win_spatial_size) / 2.0).astype(np.int32)\n    max_coords = np.zeros((n_samples, N_SPATIAL), dtype=np.int32)\n    for (idx, (img, win)) in enumerate(\n            zip(img_spatial_size[:N_SPATIAL], win_spatial_size[:N_SPATIAL])):\n        max_coords[:, idx] = np.random.randint(\n            0, max(img - win + 1, 1), n_samples)\n    max_coords[:, :N_SPATIAL] = \\\n        max_coords[:, :N_SPATIAL] + half_win[:N_SPATIAL]\n    return max_coords\n\n\ndef _infer_spatial_size(img_sizes, win_sizes):\n    """"""\n    Utility function to find the spatial size of image,\n    and the largest spatial window size across input sections.\n\n    Raises NotImplementedError if the images have\n    different spatial dimensions.\n\n    :param img_sizes: dictionary of {\'input_name\': (img_size_x, img_size,y,...)}\n    :param win_sizes: dictionary of {\'input_name\': (win_size_x, win_size_y,...)}\n    :return: (image_spatial_size, window_largest_spatial_size)\n    """"""\n    uniq_spatial_size = \\\n        set([img_size[:N_SPATIAL] for img_size in list(img_sizes.values())])\n    if len(uniq_spatial_size) != 1:\n        tf.logging.fatal(""Don\'t know how to generate sampling ""\n                         ""locations: Spatial dimensions of the ""\n                         ""grouped input sources are not ""\n                         ""consistent. %s"", uniq_spatial_size)\n        raise NotImplementedError\n    img_spatial_size = np.asarray(uniq_spatial_size.pop(), dtype=np.int32)\n\n    # find the largest spatial window across input sections\n    _win_spatial_sizes = \\\n        [win_size[:N_SPATIAL] for win_size in win_sizes.values()]\n    _win_spatial_sizes = np.asarray(_win_spatial_sizes, dtype=np.int32)\n    win_spatial_size = np.max(_win_spatial_sizes, axis=0)\n\n    assert all([img_spatial_size[i] >= win_spatial_size[i]\n                for i in range(N_SPATIAL)]), \\\n        ""window size {} is larger than image size {}"".format(\n            win_spatial_size, img_spatial_size)\n\n    return img_spatial_size, win_spatial_size\n'"
niftynet/engine/sampler_weighted_v2.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating image window by weighted sampling map from input image\nThis can also be considered as a ""weighted random cropping"" layer of the\ninput image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.image_window import N_SPATIAL\n\n\nclass WeightedSampler(UniformSampler):\n    """"""\n    This class generators samples from a user provided\n    frequency map for each input volume\n    The sampling likelihood of each voxel (and window around)\n    is proportional to its frequency\n\n    This is implemented in a closed form using cumulative histograms\n    for efficiency purposes i.e., the first three dims of image.\n\n    This layer can be considered as a ""weighted random cropping"" layer of the\n    input image.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 window_sizes,\n                 batch_size=1,\n                 windows_per_image=1,\n                 queue_length=10,\n                 name=\'weighted_sampler\'):\n        UniformSampler.__init__(self,\n                                reader=reader,\n                                window_sizes=window_sizes,\n                                batch_size=batch_size,\n                                windows_per_image=windows_per_image,\n                                queue_length=queue_length,\n                                name=name)\n        tf.logging.info(\'Initialised weighted sampler window instance\')\n        self.window_centers_sampler = weighted_spatial_coordinates\n\n\ndef weighted_spatial_coordinates(\n        n_samples, img_spatial_size, win_spatial_size, sampler_map):\n    """"""\n    Weighted sampling from a map.\n    This function uses a cumulative histogram for fast sampling.\n\n    see also `sampler_uniform.rand_spatial_coordinates`\n\n    :param n_samples: number of random coordinates to generate\n    :param img_spatial_size: input image size\n    :param win_spatial_size: input window size\n    :param sampler_map: sampling prior map, it\'s spatial shape should be\n            consistent with `img_spatial_size`\n    :return: (n_samples, N_SPATIAL) coordinates representing sampling\n              window centres relative to img_spatial_size\n    """"""\n    assert sampler_map is not None, \\\n        \'sampling prior map is not specified, \' \\\n        \'please check `sampler=` option in the config.\'\n    # Get the cumulative sum of the normalised sorted intensities\n    # i.e. first sort the sampling frequencies, normalise them\n    # to sum to one, and then accumulate them in order\n    assert np.all(img_spatial_size[:N_SPATIAL] ==\n                  sampler_map.shape[:N_SPATIAL]), \\\n        \'image and sampling map shapes do not match\'\n    win_spatial_size = np.asarray(win_spatial_size, dtype=np.int32)\n    cropped_map = crop_sampling_map(sampler_map, win_spatial_size)\n    flatten_map = cropped_map.flatten()\n    flatten_map_min = np.min(flatten_map)\n    if flatten_map_min < 0:\n        flatten_map -= flatten_map_min\n    normaliser = flatten_map.sum()\n    # get the sorting indexes to that we can invert the sorting later on.\n    sorted_indexes = np.argsort(flatten_map)\n    sorted_data = np.cumsum(\n        np.true_divide(flatten_map[sorted_indexes], normaliser))\n\n    middle_coords = np.zeros((n_samples, N_SPATIAL), dtype=np.int32)\n    for sample in range(n_samples):\n        # get n_sample from the cumulative histogram, spaced by 1/n_samples,\n        # plus a random perturbation to give us a stochastic sampler\n        sample_ratio = 1 - (np.random.random() + sample) / n_samples\n        # find the index where the cumulative it above the sample threshold\n        try:\n            if normaliser == 0:\n                # constant map? reducing to a uniform sampling\n                sample_index = np.random.randint(len(sorted_data))\n            else:\n                sample_index = np.argmax(sorted_data >= sample_ratio)\n        except ValueError:\n            tf.logging.fatal(""unable to choose sampling window based on ""\n                             ""the current frequency map."")\n            raise\n        # invert the sample index to the pre-sorted index\n        inverted_sample_index = sorted_indexes[sample_index]\n        # get the x,y,z coordinates on the cropped_map\n        middle_coords[sample, :N_SPATIAL] = np.unravel_index(\n            inverted_sample_index, cropped_map.shape)[:N_SPATIAL]\n\n    # re-shift coords due to the crop\n    half_win = np.floor(win_spatial_size / 2).astype(np.int32)\n    middle_coords[:, :N_SPATIAL] = \\\n        middle_coords[:, :N_SPATIAL] + half_win[:N_SPATIAL]\n    return middle_coords\n\n\ndef crop_sampling_map(input_map, win_spatial_size):\n    """"""\n    Utility function for generating a cropped version of the\n    input sampling prior map (the input weight map where the centre of\n    the window might be). If the centre of the window was outside of\n    this crop area, the patch would be outside of the field of view\n\n    :param input_map: the input weight map where the centre of\n                      the window might be\n    :param win_spatial_size: size of the borders to be cropped\n    :return: cropped sampling map\n    """"""\n\n    # prepare cropping indices\n    _start, _end = [], []\n    for win_size, img_size in \\\n            zip(win_spatial_size[:N_SPATIAL], input_map.shape[:N_SPATIAL]):\n        # cropping floor of the half window\n        d_start = int(win_size / 2.0)\n        # using ceil of half window\n        d_end = img_size - win_size + int(win_size / 2.0 + 0.6)\n\n        _start.append(d_start)\n        _end.append(d_end + 1 if d_start == d_end else d_end)\n\n    try:\n        assert len(_start) == 3\n        cropped_map = input_map[\n            _start[0]:_end[0], _start[1]:_end[1], _start[2]:_end[2], 0, 0]\n        assert np.all(cropped_map.shape) > 0\n    except (IndexError, KeyError, TypeError, AssertionError):\n        tf.logging.fatal(\n            ""incompatible map: %s and window size: %s\\n""\n            ""try smaller (fully-specified) spatial window sizes?"",\n            input_map.shape, win_spatial_size)\n        raise\n    return cropped_map\n'"
niftynet/engine/signal.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines default signals supported by NiftyNet engine.\n\nBy design, all connected functions (event handlers) have access\nto TF session and graph\nby ``tf.get_default_session()`` and ``tf.get_default_graph()``.\n""""""\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom blinker import Namespace\n\n# possible phases of the engine, used throughout the project.\nTRAIN = \'training\'\nVALID = \'validation\'\nINFER = \'inference\'\nEVAL = \'evaluation\'\nALL = \'all\'\n\n#: namespace of NiftyNet\'s default signals.\nNIFTYNET = Namespace()\n\n#: Signal emitted immediately after the application\'s graph is created\nGRAPH_CREATED = NIFTYNET.signal(\n    \'graph_started\',\n    doc=""emitted when application\'s graph is created"")\n\n#: Signal emitted at the beginning of a training/inference process\n#: (after the creation of both graph and session.)\nSESS_STARTED = NIFTYNET.signal(\n    \'session_started\',\n    doc=\'signal emitted at the beginning of the training/inference loop.\')\n\n#: Signal emitted before the end of a training/inference process\n#: (after the creation of both graph and session.)\nSESS_FINISHED = NIFTYNET.signal(\n    \'session_finished\',\n    doc=\'signal emitted at the end of the training/inference loop.\')\n\n#: Signal emitted immediately when each iteration starts\n#: (after the creation of both graph and session.)\nITER_STARTED = NIFTYNET.signal(\n    \'iteration_started\',\n    doc=\'emitted when every iteration starts, before ``tf.session.run(...)``.\')\n\n#: Signal emitted before the end of each iteration\n#: (after the creation of both graph and session.)\nITER_FINISHED = NIFTYNET.signal(\n    \'iteration_finished\',\n    doc=\'emitted at the end of each iteration, after ``tf.session.run(...)``.\')\n\n\n# EPOCH_STARTED = NIFTYNET.signal(\n#     \'epoch_started\',\n#     doc=\'emitted at the beginning of each training epoch\')\n# EPOCH_FINISHED = NIFTYNET.signal(\n#     \'epoch_finished\',\n#     doc=\'emitted at the end of each training epoch\')\n'"
niftynet/engine/windows_aggregator_base.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nThis module is used to cache window-based network outputs,\nform a image-level output,\nwrite the cached the results to hard drive.\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom niftynet.engine.image_window import N_SPATIAL\n\n\nclass ImageWindowsAggregator(object):\n    """"""\n    Image windows are retrieved and analysed by the\n    tensorflow graph, this windows aggregator receives\n    output window data in numpy array. To access image-level\n    information the reader is needed.\n    """"""\n\n    def __init__(self, image_reader=None, output_path=\'.\'):\n        self.reader = image_reader\n        self._image_id = None\n        self.postfix = \'\'\n        self.output_path = os.path.abspath(output_path)\n        if not os.path.exists(self.output_path):\n            os.makedirs(self.output_path)\n        self.inferred_cleared = False\n\n    @property\n    def input_image(self):\n        """"""\n        Get the corresponding input image of these batch data.\n        So that the batch data can be stored correctly in\n        terms of interpolation order, orientation, pixdims.\n\n        :return: an image object from image reader\n        """"""\n        if self.image_id is not None and self.reader:\n            return self.reader.output_list[self.image_id]\n        return None\n\n    @property\n    def image_id(self):\n        """"""\n        Index of the image in the output image list maintained by\n        image reader.\n\n        :return: integer of the position in image list\n        """"""\n        return self._image_id\n\n    @image_id.setter\n    def image_id(self, current_id):\n        try:\n            self._image_id = int(current_id)\n        except (ValueError, TypeError):\n            tf.logging.fatal(""unknown image id format (should be an integer)"")\n\n    def decode_batch(self, *args, **kwargs):\n        """"""\n        The implementation of caching and writing batch output\n        goes here. This function should return False when the\n        location vector is stopping signal, to notify the\n        inference loop to terminate.\n\n        :param args:\n        :param kwargs:\n        :return: True if more batch data are expected, False otherwise\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def _is_stopping_signal(location_vector):\n        if location_vector is None:\n            return True\n        return np.any(location_vector < 0)\n\n    @staticmethod\n    def crop_batch(window, location, border=None):\n        """"""\n        This utility function removes two borders along each\n        spatial dim of the output image window data,\n        adjusts window spatial coordinates accordingly.\n\n        :param window:\n        :param location:\n        :param border:\n        :return:\n        """"""\n        if not border:\n            return window, location\n        assert isinstance(border, (list, tuple)), \\\n            ""border should be a list or tuple""\n        while len(border) < N_SPATIAL:\n            border = tuple(border) + (border[-1],)\n        border = border[:N_SPATIAL]\n\n        location = location.astype(np.int)\n        window_shape = window.shape\n        spatial_shape = window_shape[1:-1]\n        n_spatial = len(spatial_shape)\n        for idx in range(n_spatial):\n            location[:, idx + 1] = location[:, idx + 1] + border[idx]\n            location[:, idx + 4] = location[:, idx + 4] - border[idx]\n        if np.any(location < 0):\n            return window, location\n\n        cropped_shape = np.max(location[:, 4:7] - location[:, 1:4], axis=0)\n        left = np.floor(\n            (spatial_shape - cropped_shape[:n_spatial])/2.0).astype(np.int)\n        if np.any(left < 0):\n            tf.logging.fatal(\n                \'network output window can be \'\n                \'cropped by specifying the border parameter in config file, \'\n                \'but here the output window content size %s is smaller \'\n                \'than the window coordinate size: %s -- \'\n                \'computed by input window size minus border size (%s)\'\n                \'not supported by this aggregator. Please try larger border.)\',\n                spatial_shape, cropped_shape, border)\n            raise ValueError\n        if n_spatial == 1:\n            window = window[:,\n                            left[0]:(left[0] + cropped_shape[0]),\n                            np.newaxis, np.newaxis, ...]\n        elif n_spatial == 2:\n            window = window[:,\n                            left[0]:(left[0] + cropped_shape[0]),\n                            left[1]:(left[1] + cropped_shape[1]),\n                            np.newaxis, ...]\n        elif n_spatial == 3:\n            window = window[:,\n                            left[0]:(left[0] + cropped_shape[0]),\n                            left[1]:(left[1] + cropped_shape[1]),\n                            left[2]:(left[2] + cropped_shape[2]),\n                            ...]\n        else:\n            tf.logging.fatal(\n                \'unknown output format: shape %s\'\n                \' spatial dims are: %s\', window_shape, spatial_shape)\n            raise NotImplementedError\n        return window, location\n\n    def log_inferred(self, subject_name, filename):\n        """"""\n        This function writes out a csv of inferred files\n\n        :param subject_name: subject name corresponding to output\n        :param filename: filename of output\n        :return:\n        """"""\n        inferred_csv = os.path.join(self.output_path, \'inferred.csv\')\n        if not self.inferred_cleared:\n            if os.path.exists(inferred_csv):\n                os.remove(inferred_csv)\n            self.inferred_cleared = True\n            if not os.path.exists(self.output_path):\n                os.makedirs(self.output_path)\n        with open(inferred_csv, \'a+\') as csv_file:\n            filename = os.path.join(self.output_path, filename)\n            csv_file.write(\'{},{}\\n\'.format(subject_name, filename))\n'"
niftynet/engine/windows_aggregator_grid.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nwindows aggregator decode sampling grid coordinates and image id from\nbatch data, forms image level output and write to hard drive.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\n\n# pylint: disable=too-many-nested-blocks\n# pylint: disable=too-many-branches\nimport niftynet.io.misc_io as misc_io\nfrom niftynet.engine.windows_aggregator_base import ImageWindowsAggregator\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\n\n\nclass GridSamplesAggregator(ImageWindowsAggregator):\n    """"""\n    This class keeps record of the currently cached image,\n    initialised as all zeros, and the values are replaced\n    by image window data decoded from batch.\n    """"""\n\n    def __init__(self,\n                 image_reader,\n                 name=\'image\',\n                 output_path=os.path.join(\'.\', \'output\'),\n                 window_border=(),\n                 interp_order=0,\n                 postfix=\'niftynet_out\',\n                 fill_constant=0.0):\n        ImageWindowsAggregator.__init__(\n            self, image_reader=image_reader, output_path=output_path)\n        self.name = name\n        self.image_out = None\n        self.csv_out = None\n        self.window_border = window_border\n        self.output_interp_order = interp_order\n        self.postfix = postfix\n        self.fill_constant = fill_constant\n\n    def decode_batch(self, window, location):\n        """"""\n        Function used to save multiple outputs listed in the window\n        dictionary. For the fields that have the keyword \'window\' in the\n        dictionary key, it will be saved as image. The rest will be saved as\n        csv. CSV files will contain at saving a first line of 0 (to be\n        changed into the header by the user), the first column being the\n        index of the window, followed by the list of output and the location\n        array for each considered window\n\n        :param window: dictionary of output\n        :param location: location of the input\n        :return:\n        """"""\n        n_samples = location.shape[0]\n        location_cropped = {}\n        for key in window:\n            if \'window\' in key:  # all outputs to be created as images should\n                # contained the keyword ""window""\n                window[key], location_cropped[key] = self.crop_batch(\n                    window[key], location, self.window_border)\n\n        for batch_id in range(n_samples):\n            image_id = location[batch_id, 0]\n            if image_id != self.image_id:\n                # image name changed:\n                # save current result and create an empty result file\n                self._save_current_image()\n                self._save_current_csv()\n                if self._is_stopping_signal(location[batch_id]):\n                    return False\n\n                self.image_out, self.csv_out = {}, {}\n                for key in window:\n                    if \'window\' in key:\n                        # to be saved as image\n                        self.image_out[key] = self._initialise_empty_image(\n                            image_id=image_id,\n                            n_channels=window[key].shape[-1],\n                            dtype=window[key].dtype)\n                    else:\n                        # to be saved as csv file\n                        n_elements = np.int64(\n                            np.asarray(window[key]).size / n_samples)\n                        table_header = [\n                            \'{}_{}\'.format(key, idx)\n                            for idx in range(n_elements)\n                        ] if n_elements > 1 else [\'{}\'.format(key)]\n                        table_header += [\n                            \'coord_{}\'.format(idx)\n                            for idx in range(location.shape[-1])\n                        ]\n                        self.csv_out[key] = self._initialise_empty_csv(\n                            key_names=table_header)\n\n            for key in window:\n                if \'window\' in key:\n                    x_start, y_start, z_start, x_end, y_end, z_end = \\\n                        location_cropped[key][batch_id, 1:]\n                    self.image_out[key][\n                        x_start:x_end, y_start:y_end, z_start:z_end, ...] = \\\n                        window[key][batch_id, ...]\n                else:\n                    window[key] = np.asarray(window[key]).reshape(\n                        [n_samples, -1])\n                    window_save = window[key][batch_id:batch_id + 1, :]\n                    window_loc = location[batch_id:batch_id + 1, :]\n                    csv_row = np.concatenate([window_save, window_loc], 1)\n                    csv_row = csv_row.ravel()\n                    key_names = self.csv_out[key].columns\n                    self.csv_out[key] = self.csv_out[key].append(\n                        OrderedDict(zip(key_names, csv_row)),\n                        ignore_index=True)\n        return True\n\n    def _initialise_empty_image(self, image_id, n_channels, dtype=np.float):\n        """"""\n        Initialise an empty image in which to populate the output\n        :param image_id: image_id to be used in the reader\n        :param n_channels: numbers of channels of the saved output (for\n        multimodal output)\n        :param dtype: datatype used for the saving\n        :return: the initialised empty image\n        """"""\n        self.image_id = image_id\n        spatial_shape = self.input_image[self.name].shape[:3]\n        output_image_shape = spatial_shape + (n_channels, )\n        empty_image = np.zeros(output_image_shape, dtype=dtype)\n        for layer in self.reader.preprocessors:\n            if isinstance(layer, PadLayer):\n                empty_image, _ = layer(empty_image)\n\n        if self.fill_constant != 0.0:\n            empty_image[:] = self.fill_constant\n\n        return empty_image\n\n    def _initialise_empty_csv(self, key_names):\n        """"""\n        Initialise a csv output file with a first line of zeros\n\n        :param n_channel: number of saved fields\n        :return: empty first line of the array to be saved as csv\n        """"""\n        return pd.DataFrame(columns=key_names)\n\n    def _save_current_image(self):\n        """"""\n        For all the outputs to be saved as images, go through the dictionary\n        and save the resulting output after reversing the initial preprocessing\n        :return:\n        """"""\n        if self.input_image is None:\n            return\n        for layer in reversed(self.reader.preprocessors):\n            if isinstance(layer, PadLayer):\n                for i in self.image_out:\n                    self.image_out[i], _ = layer.inverse_op(self.image_out[i])\n            if isinstance(layer, DiscreteLabelNormalisationLayer):\n                for i in self.image_out:\n                    self.image_out[i], _ = layer.inverse_op(self.image_out[i])\n        subject_name = self.reader.get_subject_id(self.image_id)\n        for i in self.image_out:\n            filename = ""{}_{}_{}.nii.gz"".format(i, subject_name, self.postfix)\n            source_image_obj = self.input_image[self.name]\n            misc_io.save_data_array(self.output_path, filename,\n                                    self.image_out[i], source_image_obj,\n                                    self.output_interp_order)\n            self.log_inferred(subject_name, filename)\n        return\n\n    def _save_current_csv(self):\n        """"""\n        For all output to be saved as csv, loop through the dictionary of\n        output and create the csv\n        :return:\n        """"""\n        if self.input_image is None:\n            return\n        subject_name = self.reader.get_subject_id(self.image_id)\n        for i in self.csv_out:\n            filename = ""{}_{}_{}.csv"".format(i, subject_name, self.postfix)\n            misc_io.save_csv_array(self.output_path, filename, self.csv_out[i])\n            self.log_inferred(subject_name, filename)\n        return\n'"
niftynet/engine/windows_aggregator_identity.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nwindows aggregator saves each item in a batch output as an image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\n\nimport niftynet.io.misc_io as misc_io\nfrom niftynet.engine.windows_aggregator_base import ImageWindowsAggregator\n\n# pylint: disable=too-many-branches\n\n\nclass WindowAsImageAggregator(ImageWindowsAggregator):\n    """"""\n    This class saves each item in a batch output to images,\n    the output filenames can be defined in three ways:\n\n        1. location is None (input image from a random distribution):\n        a uuid is generated as output filename.\n\n        2. the length of the location array is 2:\n        (indicates output image is from an\n        interpolation of two input images):\n\n                - ``location[batch_id, 0]`` is used as a ``base_name``,\n                - ``location[batch_id, 0]`` is used as a ``relative_id``\n\n        output file name is ``""{}_{}""%(base_name, relative_id)``.\n\n        3. the length of the location array is greater than 2:\n        (indicates output image is from single input image)\n        ``location[batch_id, 0]`` is used as the file name\n    """"""\n\n    def __init__(self,\n                 image_reader=None,\n                 output_path=os.path.join(\'.\', \'output\'),\n                 postfix=\'_niftynet_generated\'):\n        ImageWindowsAggregator.__init__(\n            self, image_reader=image_reader, output_path=output_path)\n        self.output_path = os.path.abspath(output_path)\n        self.inferred_csv = os.path.join(self.output_path, \'inferred.csv\')\n        self.output_id = {\'base_name\': None, \'relative_id\': 0}\n        self.postfix = postfix\n        if os.path.exists(self.inferred_csv):\n            os.remove(self.inferred_csv)\n\n    def _decode_subject_name(self, location=None):\n        if self.reader:\n            image_id = int(location)\n            return self.reader.get_subject_id(image_id)\n        import uuid\n        return str(uuid.uuid4())\n\n    def decode_batch(self, window, location=None):\n        if location is not None:\n            n_samples = location.shape[0]\n        else:\n            n_samples = window[sorted(window)[0]].shape[0]\n\n        for batch_id in range(n_samples):\n            location_b = location[batch_id] if (location is not None) else None\n            if self._is_stopping_signal(location_b):\n                return False\n            filename = self._decode_subject_name(location_b[0]) \\\n                if (location_b is not None) else self._decode_subject_name()\n            # if base file name changed, reset relative name index\n            if filename != self.output_id[\'base_name\']:\n                self.output_id[\'base_name\'] = filename\n                self.output_id[\'relative_id\'] = 0\n            # when location has two component, the name should\n            # be constructed as a composite of two input filenames\n            if (location_b is not None) and (len(location_b) == 2):\n                output_name = \'{}_{}\'.format(\n                    self.output_id[\'base_name\'],\n                    self._decode_subject_name(location_b[1]))\n            else:\n                output_name = self.output_id[\'base_name\']\n\n            for key in window:\n                output_name_k = \'{}_{}\'.format(output_name, key)\n                if \'window\' in key:\n                    self._save_current_image(self.output_id[\'relative_id\'],\n                                             output_name_k,\n                                             window[key][batch_id, ...])\n                else:\n                    window[key] = np.asarray(window[key]).reshape(\n                        [n_samples, -1])\n                    n_elements = window[key].shape[-1]\n                    table_header = [\n                        \'{}_{}\'.format(key, idx) for idx in range(n_elements)\n                    ] if n_elements > 1 else [\'{}\'.format(key)]\n                    csv_table = pd.DataFrame(columns=table_header)\n                    csv_table = csv_table.append(\n                        OrderedDict(zip(table_header, window[key].ravel())),\n                        ignore_index=True)\n                    self._save_current_csv(self.output_id[\'relative_id\'],\n                                           output_name_k, csv_table)\n            self.output_id[\'relative_id\'] += 1\n        return True\n\n    def _save_current_image(self, idx, filename, image):\n        if image is None:\n            return\n        if idx == 0:\n            uniq_name = ""{}{}.nii.gz"".format(filename, self.postfix)\n        else:\n            uniq_name = ""{}_{}{}.nii.gz"".format(idx, filename, self.postfix)\n        misc_io.save_data_array(self.output_path, uniq_name, image, None)\n        with open(self.inferred_csv, \'a\') as csv_file:\n            filename = os.path.join(self.output_path, filename)\n            csv_file.write(\'{},{}\\n\'.format(idx, filename))\n        return\n\n    def _save_current_csv(self, idx, filename, csv_data):\n        """"""\n        Save all csv output present in the dictionary of csv_output.\n        :return:\n        """"""\n\n        if csv_data is None:\n            return\n        if idx == 0:\n            uniq_name = ""{}{}.csv"".format(filename, self.postfix)\n        else:\n            uniq_name = ""{}_{}{}.csv"".format(idx, filename, self.postfix)\n        misc_io.save_csv_array(self.output_path, uniq_name, csv_data)\n        with open(self.inferred_csv, \'a\') as csv_file:\n            filename = os.path.join(self.output_path, filename)\n            csv_file.write(\'{},{}\\n\'.format(idx, filename))\n        return\n'"
niftynet/engine/windows_aggregator_resize.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nWindows aggregator resize each item\nin a batch output and save as an image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\n\nimport niftynet.io.misc_io as misc_io\nfrom niftynet.engine.sampler_resize_v2 import zoom_3d\nfrom niftynet.engine.windows_aggregator_base import ImageWindowsAggregator\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\n\n\nclass ResizeSamplesAggregator(ImageWindowsAggregator):\n    """"""\n    This class decodes each item in a batch by resizing each image\n    window and save as a new image volume. Multiple output image can be\n    proposed and csv output can be performed as well\n    """"""\n\n    def __init__(self,\n                 image_reader,\n                 name=\'image\',\n                 output_path=os.path.join(\'.\', \'output\'),\n                 window_border=(),\n                 interp_order=0,\n                 postfix=\'niftynet_out\'):\n        ImageWindowsAggregator.__init__(\n            self, image_reader=image_reader, output_path=output_path)\n        self.name = name\n        self.image_out = None\n        self.csv_out = None\n        self.window_border = window_border\n        self.output_interp_order = interp_order\n        self.postfix = postfix\n        self.current_out = {}\n\n    def decode_batch(self, window, location):\n        """"""\n        Resizing each output image window in the batch as an image volume\n        location specifies the original input image (so that the\n        interpolation order, original shape information retained in the\n\n        generated outputs).For the fields that have the keyword \'window\' in the\n        dictionary key, it will be saved as image. The rest will be saved as\n        csv. CSV files will contain at saving a first line of 0 (to be\n        changed into the header by the user), the first column being the\n        index of the window, followed by the list of output.\n\n        """"""\n        n_samples = location.shape[0]\n        for batch_id in range(n_samples):\n            if self._is_stopping_signal(location[batch_id]):\n                return False\n            self.image_id = location[batch_id, 0]\n            self.image_out, self.csv_out = {}, {}\n            for key in window:\n                if \'window\' in key:\n                    # saving image output\n                    while window[key].ndim < 5:\n                        window[key] = window[key][..., np.newaxis, :]\n                    self.image_out[key] = window[key][batch_id, ...]\n                else:\n                    # saving csv output\n                    window[key] = np.asarray(window[key]).reshape(\n                        [n_samples, -1])\n                    n_elements = window[key].shape[-1]\n                    table_header = [\n                        \'{}_{}\'.format(key, idx) for idx in range(n_elements)\n                    ] if n_elements > 1 else [\'{}\'.format(key)]\n                    self.csv_out[key] = self._initialise_empty_csv(\n                        key_names=table_header)\n                    csv_row = window[key][batch_id:batch_id + 1, :].ravel()\n                    self.csv_out[key] = self.csv_out[key].append(\n                        OrderedDict(zip(table_header, csv_row)),\n                        ignore_index=True)\n            self._save_current_image()\n            self._save_current_csv()\n\n        return True\n\n    def _initialise_image_shape(self, image_id, n_channels):\n        """"""\n        Return the shape of the empty image to be saved\n        :param image_id: index to find the appropriate input image from the\n        reader\n        :param n_channels: number of channels of the image\n        :return:  shape of the empty image\n        """"""\n        self.image_id = image_id\n        spatial_shape = self.input_image[self.name].shape[:3]\n        output_image_shape = spatial_shape + (1, n_channels)\n        empty_image = np.zeros(output_image_shape, dtype=np.bool)\n        for layer in self.reader.preprocessors:\n            if isinstance(layer, PadLayer):\n                empty_image, _ = layer(empty_image)\n        return empty_image.shape\n\n    def _save_current_image(self):\n        """"""\n        Loop through the dictionary of images output and resize and reverse\n        the preprocessing prior to saving\n        :return:\n        """"""\n        if self.input_image is None:\n            return\n\n        self.current_out = {}\n        for i in self.image_out:\n            resize_to_shape = self._initialise_image_shape(\n                image_id=self.image_id, n_channels=self.image_out[i].shape[-1])\n            window_shape = resize_to_shape\n            current_out = self.image_out[i]\n            while current_out.ndim < 5:\n                current_out = current_out[..., np.newaxis, :]\n            if self.window_border and any([b > 0 for b in self.window_border]):\n                np_border = self.window_border\n                while len(np_border) < 5:\n                    np_border = np_border + (0, )\n                np_border = [(b, ) for b in np_border]\n                current_out = np.pad(current_out, np_border, mode=\'edge\')\n            image_shape = current_out.shape\n            zoom_ratio = \\\n                [float(p) / float(d) for p, d in zip(window_shape, image_shape)]\n            image_shape = list(image_shape[:3]) + [1, image_shape[-1]]\n            current_out = np.reshape(current_out, image_shape)\n            current_out = zoom_3d(\n                image=current_out,\n                ratio=zoom_ratio,\n                interp_order=self.output_interp_order)\n            self.current_out[i] = current_out\n\n        for layer in reversed(self.reader.preprocessors):\n            if isinstance(layer, PadLayer):\n                for i in self.image_out:\n                    self.current_out[i], _ = layer.inverse_op(\n                        self.current_out[i])\n            if isinstance(layer, DiscreteLabelNormalisationLayer):\n                for i in self.image_out:\n                    self.image_out[i], _ = layer.inverse_op(self.image_out[i])\n        subject_name = self.reader.get_subject_id(self.image_id)\n        for i in self.image_out:\n            filename = ""{}_{}_{}.nii.gz"".format(i, subject_name, self.postfix)\n            source_image_obj = self.input_image[self.name]\n            misc_io.save_data_array(self.output_path, filename,\n                                    self.current_out[i], source_image_obj,\n                                    self.output_interp_order)\n            self.log_inferred(subject_name, filename)\n        return\n\n    def _save_current_csv(self):\n        """"""\n        Save all csv output present in the dictionary of csv_output.\n        :return:\n        """"""\n        if self.input_image is None:\n            return\n        subject_name = self.reader.get_subject_id(self.image_id)\n        for i in self.csv_out:\n            filename = ""{}_{}_{}.csv"".format(i, subject_name, self.postfix)\n            misc_io.save_csv_array(self.output_path, filename, self.csv_out[i])\n            self.log_inferred(subject_name, filename)\n        return\n\n    def _initialise_empty_csv(self, key_names):\n        """"""\n        Initialise the array to be saved as csv as a line of zeros according\n        to the number of elements to be saved\n        :param n_channel:\n        :return:\n        """"""\n        return pd.DataFrame(columns=key_names)\n'"
niftynet/evaluation/__init__.py,0,"b'""""""\n\n.. module:: niftynet.evaluation\n   :synopsis: Evaluation metrics for network outputs.\n\n""""""\n'"
niftynet/evaluation/base_evaluations.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines basic interfaces for NiftyNet evaluations\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport pandas as pd\n\nfrom niftynet.layer.base_layer import Layer\n\nclass BaseEvaluation(Layer):\n    """"""\n    Minimal interface for a NiftyNet evaluation\n    """"""\n    def __init__(self, reader, app_param, eval_param):\n        super(BaseEvaluation, self).__init__()\n        self.reader = reader\n        self.app_param = app_param\n        self.eval_param = eval_param\n\n    def get_aggregations(self):\n        """"""\n        Returns aggregations to compute for the metric. Each aggregation is\n        a callable that computes a list of DataFrames from a dictionary of \n        metric dataframes (index by the DataFrame index). See\n        BaseEvaluator.ScalarAggregator for an example.\n\n        :return: list of aggregation callables\n        """"""\n        return []\n\n    def layer_op(self, subject_id, data):\n        """"""\n        Perform one evaluation calculation for one subject\n        :param subject_id:  subject identifier string\n        :param data:  a data dictionary as built by ImageReader\n        :return: a list of pandas.DataFrame objects\n        """"""\n        raise NotImplementedError(\'not implemented in abstract base class\')\n\nclass CachedSubanalysisEvaluation(BaseEvaluation):\n    """"""\n    Interface for NiftyNet evaluations used with\n    CachedSubanalysisEvaluator so that evaluations are run in a way that is\n    friendly for caching intermediate computations. Each evaluation defines\n    sub-analyses to run, and all subanalysis are run at the same time then\n    the cache is cleared\n    """"""\n    def subanalyses(self, subject_id, data):\n        """"""\n        This function defines the sub-analyses to run. All evaluations with\n        matching sub-analyses will be run in sequence, before clearing the cache\n        :param subject_id:  subject identifier string\n        :param data:  a data dictionary as built by ImageReader\n        :return: list of dictionaries, each containing information specifyng\n        the analysis to run. Elements will be passed to layer_op one at a\n        time in a cache friendly order\n        """"""\n        raise NotImplementedError(\'not implemented in abstract class\')\n\n    def layer_op(self, subject_id, data, subanalysis):\n        """"""\n        Performs one sub-analysis\n\n        :param subject_id: subject identifier string\n        :param data: a data dictionary as built by ImageReader\n        :param subanalysis: dictionary containing information specifying the\n        analysis to run\n        :return: a list of pandas.DataFrame objects\n        """"""\n        raise NotImplementedError(\'not implemented in abstract class\')\n'"
niftynet/evaluation/base_evaluator.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines base classes for Evaluator classes which define the\nlogic for iterating through the subjects and requested metrics needed for\nevaluation\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport itertools\nfrom itertools import groupby\nfrom collections import defaultdict\n\nimport pandas as pd\n\nfrom niftynet.engine.application_factory import EvaluationFactory\nfrom niftynet.utilities.util_common import cache\n\nclass BaseEvaluator(object):\n    """"""\n    The base evaluator defines a simple evaluations that iterates through\n    subjects and computes each metric in sequence\n\n    Sub-classes should overload the default_evaluation_list with\n    application-specific metrics\n    If a particular ordering of computations per subject is needed, sub-class\n    can override the evaluate_next method; if a particular ordering of\n    subjects is needed, subclasses can override the evaluate method.\n    """"""\n    def __init__(self, reader, app_param, eval_param):\n        self.reader = reader\n        self.app_param = app_param\n        self.eval_param = eval_param\n        if eval_param.evaluations:\n            eval_list = eval_param.evaluations.split(\',\')\n        else:\n            eval_list = self.default_evaluation_list()\n        evaluation_classes = [EvaluationFactory.create(e) for e in eval_list]\n        self.evaluations = [e(reader, app_param, eval_param) for e in\n                            evaluation_classes]\n\n    def evaluate(self):\n        """"""\n        This method loops through all subjects and computes the metrics for\n        each subject.\n\n        :return: a dictionary of pandas.DataFrame objects\n        """"""\n        def generator_from_reader(reader):\n            while True:\n                image_id, data, interp_orders = reader(shuffle=False)\n                if image_id < 0:\n                    break\n                subject_id = self.reader.get_subject_id(image_id)\n                yield (subject_id, data,interp_orders)\n                \n        generator = generator_from_reader(self.reader)\n        return self.evaluate_from_generator(generator)\n\n    def evaluate_from_generator(self, generator):\n        """"""\n        This method loops through all subjects and computes the metrics for\n        each subject.\n\n        :return: a dictionary of pandas.DataFrame objects\n        """"""\n        all_results = []\n        for subject_id, data,interp_orders in generator:\n            all_results += self.evaluate_next(subject_id, data, interp_orders)\n        return self.aggregate(all_results)\n\n    def evaluate_next(self, subject_id, data, interp_orders):\n        """"""\n        Computes metrics for one subject.\n\n        :param subject_id:\n        :param data: data dictionary passed to each evaluation\n        :param interp_orders: metadata for the data dictionary\n               [currently not used]\n        :return: a list of pandas.DataFrame objects\n        """"""\n        metrics = []\n        for evaluation in self.evaluations:\n            metrics += evaluation(subject_id, data)\n        return metrics\n\n    def aggregate(self, dataframes):\n        """"""\n        Apply all of the iterations requested by the evaluations\n\n        :param dataframes: a list of pandas.DataFrame objects\n        :return: a dictionary of pandas.DataFrame objects after aggregation\n        """"""\n        result_dict = defaultdict(lambda: pd.DataFrame())\n        for pdf in dataframes:\n            key = tuple(pdf.index.names)\n            result_dict[key] = pdf if key not in result_dict else result_dict[key].combine_first(pdf)\n\n        aggregations = []\n        for evaluation in self.evaluations:\n            agg_list = evaluation.get_aggregations()\n            aggregations.extend(agg_list)\n        for aggregation in aggregations:\n            for pdf in aggregation(result_dict):\n                key = tuple(pdf.index.names)\n                result_dict[key] = pdf if key not in result_dict else result_dict[key].combine_first(pdf)\n        return result_dict\n\n    def default_evaluation_list(self):\n        """"""\n        :return: List of EvaluationFactory strings defining the evaluations\n        to compute if no evaluations are specified in the configuration\n        """"""\n        raise NotImplementedError(\'not implemented in abstract class\')\n\nclass CachedSubanalysisEvaluator(BaseEvaluator):\n    """"""\n    This evaluator sequences evaluations in a way that is friendly for\n    caching intermediate computations. Each evaluation defines sub-analyses\n    to run, and all subanalysis are run at the same time then the cache is\n    cleared\n    """"""\n\n    def evaluate_next(self, subject_id, data, interp_orders):\n        """"""\n        Computes metrics for one subject. Instead of iterating through the\n        metrics in order, this method first identifies sub-analyses that should\n        be run together (for caching reasons) and iterates through the\n        sub-analyses in sequence, calculating the metrics for each\n        sub-analysis together\n\n        :param subject_id:\n        :param data: data dictionary passed to each evaluation\n        :param interp_orders: metadata for the data dictionary\n               [currently not used]\n        :return: a list of pandas.DataFrame objects\n        """"""\n        # First go through evaluations to find those with subanalyses\n        evaluations = {\'normal\': [], \'subanalyses\':[]}\n        for evl in self.evaluations:\n            if hasattr(evl, \'subanalyses\'):\n                sub = evl.subanalyses(subject_id, data)\n                evaluations[\'subanalyses\'].extend([(evl, s) for s in sub])\n            else:\n                evaluations[\'normal\'].append(evl)\n\n        # Run normal evaluations\n        metrics = []\n        for evaluation in evaluations[\'normal\']:\n            metrics += evaluation(subject_id, data)\n\n        # group sub-analysis evaluations by subanalysis\n        def keyfunc(sub):\n            return str(sub[1])\n        tasks = sorted(evaluations[\'subanalyses\'], key=keyfunc)\n        tasksets = groupby(tasks, keyfunc)\n        # run grouped evaluations\n        for _, evaluationset in tasksets:\n            for evaluation, sub in evaluationset:\n                metrics += evaluation(subject_id, data, sub)\n            cache.clear()\n        return metrics\n\nclass DataFrameAggregator(object):\n    """"""\n    This class defines a simple aggregator that operates on groups of entries \n    in a pandas dataframe\n    \n    `func` should accept a dataframe and return a list of dataframes with appropriate indices\n    """"""\n    def __init__(self, group_by, func):\n        """"""\n        :param group_by: level at which original metric was computed,\n            e.g. (\'subject_id\', \'label\')\n        :param func: function (dataframe=>dataframe) to aggregate the collected\n                     metrics\n        """"""\n        self.group_by = group_by\n        self.func = func\n\n    def __call__(self, result_dict):\n        return self.func(result_dict[self.group_by])\n\nclass ScalarAggregator(DataFrameAggregator):\n    """"""\n    This class defines a simple aggregator that groups metrics and applies an\n    aggregating function. Grouping is determined by the set difference\n    between an original `group_by` term and a subset `new_group_py` term.\n    """"""\n    def __init__(self, key, group_by, new_group_by, func, name):\n        """"""\n\n        :param key: metric heading name with values to aggregate\n        :param group_by: level at which original metric was computed,\n            e.g. (\'subject_id\', \'label\')\n        :param new_group_by: level at which metric after aggregation is\n            computed, e.g. (\'label\')\n        :param func: function (iterable=>scalar) to aggregate the collected\n        values e.g., np.mean\n        :param name: new heading name for the aggregated metric\n        """"""\n        self.key = key\n        self.name = name\n        self.group_by = group_by\n        self.new_group_by = new_group_by\n        self.scalar_func = func\n        super(ScalarAggregator, self).__init__(group_by, self.scalar_wrapper_)\n\n    def scalar_wrapper_(self, pdf):\n        """""" For each unique value of pdf.loc[:,new_group_by], aggregate\n        the values using self.func """"""\n        group = pdf.reset_index().groupby(by=self.new_group_by, group_keys=False)\n        def func(pdf):\n            agg = self.scalar_func(list(pdf.loc[:,self.key]))\n            return pd.Series({self.name:agg})\n        return [group.apply(func)]\n'"
niftynet/evaluation/classification_evaluations.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines built-in evaluation functions for classification \napplications\n\nMany classification metrics only make sense computed over all subjects,\nso aggregation is used.\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport pandas as pd\n\nfrom niftynet.evaluation.base_evaluations import BaseEvaluation\nfrom niftynet.evaluation.base_evaluator import ScalarAggregator,\\\n                                               DataFrameAggregator\n\nclass accuracy(BaseEvaluation):\n    def layer_op(self, subject_id, data):\n        metric_name = \'accuracy_\'\n        if self.app_param.output_prob:\n            inferred_label = np.argmax(data[\'inferred\'][0,0,0,0,:])\n        else:\n            inferred_label = data[\'inferred\'][0,0,0,0,0]\n        pdf = pd.DataFrame.from_records([{\'subject_id\':subject_id,\n                                          \'acc_i\':inferred_label,\n                                          \'acc_l\':data[\'label\'][0,0,0,0,0]}],\n                                        index=(\'subject_id\',))\n        return [pdf]\n        \n    def aggregate(self, df):\n        print(df)\n        agg = pd.DataFrame.from_records([{\'accuracy\':(df.acc_i==df.acc_l).mean()}])\n        return [agg]\n\n    def get_aggregations(self):\n        return [DataFrameAggregator((\'subject_id\',), self.aggregate)]\n'"
niftynet/evaluation/classification_evaluator.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines the specialized Evaluator for classification applications\nAll logic except default metrics is delegated to the parent class\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom niftynet.evaluation.base_evaluator import BaseEvaluator\n\nclass ClassificationEvaluator(BaseEvaluator):\n    """"""\n    Evaluator for ClassificationApplication\n    """"""\n    def default_evaluation_list(self):\n        """"""\n        :return:  list of metric names to compute by default\n        """"""\n        return [\'classification_accuracy\', \'roc_auc\', \'roc\']\n'"
niftynet/evaluation/evaluation_application_driver.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines a general procedure for running evaluations\nExample usage:\n    app_driver = EvaluationApplicationDriver()\n    app_driver.initialise_application(system_param, input_data_param)\n    app_driver.run_application()\n\nsystem_param and input_data_param should be generated using:\nniftynet.utilities.user_parameters_parser.run()\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nimport itertools\n\nimport pandas as pd\nimport tensorflow as tf\n\nfrom niftynet.engine.application_factory import ApplicationFactory\nfrom niftynet.io.misc_io import touch_folder\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\n\nFILE_PREFIX = \'model.ckpt\'\n\n\nclass EvaluationApplicationDriver(object):\n    """"""\n    This class represents the application logic for evaluating a set of\n    results inferred within NiftyNet (or externally generated)\n    """"""\n    def __init__(self):\n        self.app = None\n        self.model_dir = None\n        self.summary_dir = None\n        self.session_prefix = None\n\n        self.outputs_collector = None\n        self.gradients_collector = None\n\n    def initialise_application(self, workflow_param, data_param):\n        """"""\n        This function receives all parameters from user config file,\n        create an instance of application.\n        :param workflow_param: a dictionary of user parameters,\n        keys correspond to sections in the config file\n        :param data_param: a dictionary of input image parameters,\n        keys correspond to data properties to be used by image_reader\n        :return:\n        """"""\n        try:\n            system_param = workflow_param.get(\'SYSTEM\', None)\n            net_param = workflow_param.get(\'NETWORK\', None)\n            infer_param = workflow_param.get(\'INFERENCE\', None)\n            eval_param = workflow_param.get(\'EVALUATION\', None)\n            app_param = workflow_param.get(\'CUSTOM\', None)\n        except AttributeError:\n            tf.logging.fatal(\'parameters should be dictionaries\')\n            raise\n        self.num_threads = 1\n        # self.num_threads = max(system_param.num_threads, 1)\n        # self.num_gpus = system_param.num_gpus\n        # set_cuda_device(system_param.cuda_devices)\n\n        # set output TF model folders\n        self.model_dir = touch_folder(\n            os.path.join(system_param.model_dir, \'models\'))\n        self.session_prefix = os.path.join(self.model_dir, FILE_PREFIX)\n\n        assert infer_param, \'inference parameters not specified\'\n\n        # create an application instance\n        assert app_param, \'application specific param. not specified\'\n        self.app_param = app_param\n        app_module = ApplicationFactory.create(app_param.name)\n        self.app = app_module(net_param, infer_param, system_param.action)\n\n        self.eval_param = eval_param\n\n        data_param, self.app_param = \\\n            self.app.add_inferred_output(data_param, self.app_param)\n        # initialise data input\n        data_partitioner = ImageSetsPartitioner()\n        # clear the cached file lists\n        data_partitioner.reset()\n        if data_param:\n            data_partitioner.initialise(\n                data_param=data_param,\n                new_partition=False,\n                ratios=None,\n                data_split_file=system_param.dataset_split_file)\n\n        # initialise data input\n        self.app.initialise_dataset_loader(data_param, self.app_param,\n                                           data_partitioner)\n        self.app.initialise_evaluator(eval_param)\n\n    def run(self, application):\n        """"""\n        This is the main application logic for evaluation.\n        Computation of all metrics for all subjects is delegated to an\n        Evaluator objects owned by the application object. The resulting\n        metrics are aggregated as defined by the evaluation classes and\n        output to one or more csv files (based on their \'group_by\' headings).\n        For example, per-subject metrics will be in one file, per-label-class\n        metrics will be in another and per-subject-per-class will be in a\n        third.\n        :return:\n        """"""\n        start_time = time.time()\n        try:\n            if not os.path.exists(self.eval_param.save_csv_dir):\n                os.makedirs(self.eval_param.save_csv_dir)\n            # iteratively run the graph\n            all_results = application.evaluator.evaluate()\n            for group_by, data_frame in all_results.items():\n                if group_by == (None,):\n                    csv_id = \'\'\n                else:\n                    csv_id = \'_\'.join(group_by)\n\n                with open(os.path.join(self.eval_param.save_csv_dir,\n                                       \'eval_\' + csv_id + \'.csv\'), \'w\') as csv:\n                    csv.write(data_frame.reset_index().to_csv(index=False))\n        except KeyboardInterrupt:\n            tf.logging.warning(\'User cancelled application\')\n        except RuntimeError:\n            import sys\n            import traceback\n            exc_type, exc_value, exc_traceback = sys.exc_info()\n            traceback.print_exception(\n                exc_type, exc_value, exc_traceback, file=sys.stdout)\n        finally:\n            tf.logging.info(\'Cleaning up...\')\n            tf.logging.info(\n                ""%s stopped (time in second %.2f)."",\n                type(application).__name__, (time.time() - start_time))\n'"
niftynet/evaluation/pairwise_measures.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nfrom scipy import ndimage\n\nfrom niftynet.utilities.util_common import MorphologyOps, CacheFunctionOutput\n\n\nclass PairwiseMeasures(object):\n    def __init__(self, seg_img, ref_img,\n                 measures=None, num_neighbors=8, pixdim=(1, 1, 1),\n                 empty=False, list_labels=None):\n\n        self.m_dict = {\n            \'ref volume\': (self.n_pos_ref, \'Volume (Ref)\'),\n            \'seg volume\': (self.n_pos_seg, \'Volume (Seg)\'),\n            \'ref bg volume\': (self.n_neg_ref, \'Volume (Ref bg)\'),\n            \'seg bg volume\': (self.n_neg_seg, \'Volume (Seg bg)\'),\n            \'list_labels\': (self.list_labels, \'List Labels Seg\'),\n            \'fp\': (self.fp, \'FP\'),\n            \'fn\': (self.fn, \'FN\'),\n            \'tp\': (self.tp, \'TP\'),\n            \'tn\': (self.tn, \'TN\'),\n            \'n_intersection\': (self.n_intersection, \'Intersection\'),\n            \'n_union\': (self.n_union, \'Union\'),\n            \'sensitivity\': (self.sensitivity, \'Sens\'),\n            \'specificity\': (self.specificity, \'Spec\'),\n            \'accuracy\': (self.accuracy, \'Acc\'),\n            \'fpr\': (self.false_positive_rate, \'FPR\'),\n            \'ppv\': (self.positive_predictive_values, \'PPV\'),\n            \'npv\': (self.negative_predictive_values, \'NPV\'),\n            \'dice\': (self.dice_score, \'Dice\'),\n            \'IoU\': (self.intersection_over_union, \'IoU\'),\n            \'jaccard\': (self.jaccard, \'Jaccard\'),\n            \'informedness\': (self.informedness, \'Informedness\'),\n            \'markedness\': (self.markedness, \'Markedness\'),\n            \'vol_diff\': (self.vol_diff, \'VolDiff\'),\n            \'ave_dist\': (self.measured_average_distance, \'AveDist\'),\n            \'haus_dist\': (self.measured_hausdorff_distance, \'HausDist\'),\n            \'connected_elements\': (self.connected_elements, \'TPc,FPc,FNc\'),\n            \'outline_error\': (self.outline_error, \'OER,OEFP,OEFN\'),\n            \'detection_error\': (self.detection_error, \'DE,DEFP,DEFN\'),\n            \'com_dist\': (self.com_dist, \'COM distance\'),\n            \'com_ref\': (self.com_ref, \'COM reference\'),\n            \'com_seg\': (self.com_seg, \'COM segmentation\')\n        }\n        self.seg = seg_img\n        self.ref = ref_img\n        self.list_labels = list_labels\n        self.flag_empty = empty\n        self.measures = measures if measures is not None else self.m_dict\n        self.neigh = num_neighbors\n        self.pixdim = pixdim\n\n    def check_binary(self):\n        """"""\n        Checks whether self.seg and self.ref are binary. This is to enable\n        measurements such as \'false positives\', which only have meaning in \n        the binary case (what is positive/negative for multiple class?)\n        """"""\n\n        is_seg_binary, is_ref_binary = [((x > 0.5) == x).all()\n                                        for x in [self.seg, self.ref]]\n        if (not is_ref_binary) or (not is_seg_binary):\n            raise ValueError(""The input segmentation/reference images""\n                             "" must be binary for this function."")\n\n    def __FPmap(self):\n        """"""\n        This function calculates the false positive map from binary \n        segmentation and reference maps\n\n        :return: FP map\n        """"""\n        self.check_binary()\n        return np.asarray((self.seg - self.ref) > 0.0, dtype=np.float32)\n\n    def __FNmap(self):\n        """"""\n        This function calculates the false negative map\n\n        :return: FN map\n        """"""\n        self.check_binary()\n        return np.asarray((self.ref - self.seg) > 0.0, dtype=np.float32)\n\n    def __TPmap(self):\n        """"""\n        This function calculates the true positive map (i.e. how many \n        reference voxels are positive)\n\n        :return: TP map\n        """"""\n        self.check_binary()\n        return np.logical_and(self.ref > 0.5, self.seg > 0.5).astype(float)\n\n    def __TNmap(self):\n        """"""\n        This function calculates the true negative map\n\n        :return: TN map\n        """"""\n        self.check_binary()\n        return np.logical_and(self.ref < 0.5, self.seg < 0.5).astype(float)\n\n    def __union_map(self):\n        """"""\n        This function calculates the union map between segmentation and\n        reference image\n\n        :return: union map\n        """"""\n        self.check_binary()\n        return np.logical_or(self.ref, self.seg).astype(float)\n\n    def __intersection_map(self):\n        """"""\n        This function calculates the intersection between segmentation and\n        reference image\n\n        :return: intersection map\n        """"""\n        self.check_binary()\n        return np.multiply(self.ref, self.seg)\n\n    @CacheFunctionOutput\n    def n_pos_ref(self):\n        return np.sum(self.ref)\n\n    @CacheFunctionOutput\n    def n_neg_ref(self):\n        self.check_binary()\n        return np.sum(self.ref == 0)\n\n    @CacheFunctionOutput\n    def n_pos_seg(self):\n        return np.sum(self.seg)\n\n    @CacheFunctionOutput\n    def n_neg_seg(self):\n        return np.sum(1 - self.seg)\n\n    @CacheFunctionOutput\n    def fp(self):\n        return np.sum(self.__FPmap())\n\n    @CacheFunctionOutput\n    def fn(self):\n        return np.sum(self.__FNmap())\n\n    @CacheFunctionOutput\n    def tp(self):\n        return np.sum(self.__TPmap())\n\n    @CacheFunctionOutput\n    def tn(self):\n        return np.sum(self.__TNmap())\n\n    @CacheFunctionOutput\n    def n_intersection(self):\n        return np.sum(self.__intersection_map())\n\n    @CacheFunctionOutput\n    def n_union(self):\n        return np.sum(self.__union_map())\n\n    def sensitivity(self):\n        return self.tp() / self.n_pos_ref()\n\n    def specificity(self):\n        return self.tn() / self.n_neg_ref()\n\n    def accuracy(self):\n        return (self.tn() + self.tp()) / \\\n               (self.tn() + self.tp() + self.fn() + self.fp())\n\n    def false_positive_rate(self):\n        return self.fp() / self.n_neg_ref()\n\n    def positive_predictive_values(self):\n        if self.flag_empty:\n            return -1\n        return self.tp() / (self.tp() + self.fp())\n\n    def negative_predictive_values(self):\n        """"""\n        This function calculates the negative predictive value ratio between\n        the number of true negatives and the total number of negative elements\n\n        :return:\n        """"""\n        return self.tn() / (self.fn() + self.tn())\n\n    def dice_score(self):\n        """"""\n        This function returns the dice score coefficient between a reference\n        and segmentation images\n\n        :return: dice score\n        """"""\n        return 2 * self.tp() / np.sum(self.ref + self.seg)\n\n    def intersection_over_union(self):\n        """"""\n        This function the intersection over union ratio - Definition of\n        jaccard coefficient\n\n        :return:\n        """"""\n        return self.n_intersection() / self.n_union()\n\n    def jaccard(self):\n        """"""\n        This function returns the jaccard coefficient (defined as\n        intersection over union)\n\n        :return: jaccard coefficient\n        """"""\n        return self.intersection_over_union()\n\n    def informedness(self):\n        """"""\n        This function calculates the informedness between the segmentation\n        and the reference\n\n        :return: informedness\n        """"""\n        return self.sensitivity() + self.specificity() - 1\n\n    def markedness(self):\n        """"""\n        This functions calculates the markedness\n        :return:\n        """"""\n        return self.positive_predictive_values() + \\\n               self.negative_predictive_values() - 1\n\n    def com_dist(self):\n        """"""\n        This function calculates the euclidean distance between the centres\n        of mass of the reference and segmentation.\n\n        :return:\n        """"""\n        if self.flag_empty:\n            return -1\n        com_ref = ndimage.center_of_mass(self.ref)\n        com_seg = ndimage.center_of_mass(self.seg)\n        com_dist = np.sqrt(np.dot(np.square(np.asarray(com_ref) -\n                                            np.asarray(com_seg)), np.square(\n            self.pixdim)))\n        return com_dist\n\n    def com_ref(self):\n        """"""\n        This function calculates the centre of mass of the reference\n        segmentation\n\n        :return:\n        """"""\n        return ndimage.center_of_mass(self.ref) * np.array(self.pixdim)\n\n    def com_seg(self):\n        """"""\n        This functions provides the centre of mass of the segmented element\n\n        :return:\n        """"""\n        if self.flag_empty:\n            return -1\n        return ndimage.center_of_mass(self.seg)\n\n    def list_labels(self):\n        if self.list_labels is None:\n            return ()\n        return tuple(np.unique(self.list_labels))\n\n    def vol_diff(self):\n        """"""\n        This function calculates the ratio of difference in volume between\n        the reference and segmentation images.\n\n        :return: vol_diff\n        """"""\n        return np.abs(self.n_pos_ref() - self.n_pos_seg()) / self.n_pos_ref()\n\n    # @CacheFunctionOutput\n    # def _boundaries_dist_mat(self):\n    #     dist = DistanceMetric.get_metric(\'euclidean\')\n    #     border_ref = MorphologyOps(self.ref, self.neigh).border_map()\n    #     border_seg = MorphologyOps(self.seg, self.neigh).border_map()\n    #     coord_ref = np.multiply(np.argwhere(border_ref > 0), self.pixdim)\n    #     coord_seg = np.multiply(np.argwhere(border_seg > 0), self.pixdim)\n    #     pairwise_dist = dist.pairwise(coord_ref, coord_seg)\n    #     return pairwise_dist\n\n    @CacheFunctionOutput\n    def border_distance(self):\n        """"""\n        This functions determines the map of distance from the borders of the\n        segmentation and the reference and the border maps themselves\n\n        :return: distance_border_ref, distance_border_seg, border_ref,\n            border_seg\n        """"""\n        border_ref = MorphologyOps(self.ref, self.neigh).border_map()\n        border_seg = MorphologyOps(self.seg, self.neigh).border_map()\n        oppose_ref = 1 - self.ref\n        oppose_seg = 1 - self.seg\n        # euclidean distance transform\n        distance_ref = ndimage.distance_transform_edt(oppose_ref)\n        distance_seg = ndimage.distance_transform_edt(oppose_seg)\n        distance_border_seg = border_ref * distance_seg\n        distance_border_ref = border_seg * distance_ref\n        return distance_border_ref, distance_border_seg, border_ref, border_seg\n\n    def measured_distance(self):\n        """"""\n        This functions calculates the average symmetric distance and the\n        hausdorff distance between a segmentation and a reference image\n\n        :return: hausdorff distance and average symmetric distance\n        """"""\n        ref_border_dist, seg_border_dist, ref_border, \\\n            seg_border = self.border_distance()\n        average_distance = (np.sum(ref_border_dist) + np.sum(\n            seg_border_dist)) / (np.sum(self.ref + self.seg))\n        hausdorff_distance = np.max(\n            [np.max(ref_border_dist), np.max(seg_border_dist)])\n        return hausdorff_distance, average_distance\n\n    def measured_average_distance(self):\n        """"""\n        This function returns only the average distance when calculating the\n        distances between segmentation and reference\n\n        :return:\n        """"""\n        return self.measured_distance()[1]\n\n    def measured_hausdorff_distance(self):\n        """"""\n        This function returns only the hausdorff distance when calculated the\n        distances between segmentation and reference\n\n        :return:\n        """"""\n        return self.measured_distance()[0]\n\n    # def average_distance(self):\n    #     pairwise_dist = self._boundaries_dist_mat()\n    #     return (np.sum(np.min(pairwise_dist, 0)) + \\\n    #             np.sum(np.min(pairwise_dist, 1))) / \\\n    #            (np.sum(self.ref + self.seg))\n    #\n    # def hausdorff_distance(self):\n    #     pairwise_dist = self._boundaries_dist_mat()\n    #     return np.max((np.max(np.min(pairwise_dist, 0)),\n    #                    np.max(np.min(pairwise_dist, 1))))\n\n    @CacheFunctionOutput\n    def _connected_components(self):\n        """"""\n        This function creates the maps of connected component for the\n        reference and the segmentation image using the neighborhood defined\n        in self.neigh\n\n        :return:\n            blobs_ref: connected labeling for the reference image,\n            blobs_seg: connected labeling for the segmentation image,\n            init: intersection between segmentation and reference\n        """"""\n        init = np.multiply(self.seg, self.ref)\n        blobs_ref = MorphologyOps(self.ref, self.neigh).foreground_component()\n        blobs_seg = MorphologyOps(self.seg, self.neigh).foreground_component()\n        return blobs_ref, blobs_seg, init\n\n    def connected_elements(self):\n        """"""\n        This function returns the number of FP FN and TP in terms of\n        connected components.\n\n        :return: Number of true positive connected components, Number of\n            false positives connected components, Number of false negatives\n            connected components\n        """"""\n        blobs_ref, blobs_seg, init = self._connected_components()\n        list_blobs_ref = range(1, blobs_ref[1])\n        list_blobs_seg = range(1, blobs_seg[1])\n        mul_blobs_ref = np.multiply(blobs_ref[0], init)\n        mul_blobs_seg = np.multiply(blobs_seg[0], init)\n        list_TP_ref = np.unique(mul_blobs_ref[mul_blobs_ref > 0])\n        list_TP_seg = np.unique(mul_blobs_seg[mul_blobs_seg > 0])\n\n        list_FN = [x for x in list_blobs_ref if x not in list_TP_ref]\n        list_FP = [x for x in list_blobs_seg if x not in list_TP_seg]\n        return len(list_TP_ref), len(list_FP), len(list_FN)\n\n    @CacheFunctionOutput\n    def connected_errormaps(self):\n        """"""\n        This functions calculates the error maps from the connected components\n\n        :return:\n        """"""\n        blobs_ref, blobs_seg, init = self._connected_components()\n        list_blobs_ref = range(1, blobs_ref[1])\n        list_blobs_seg = range(1, blobs_seg[1])\n        mul_blobs_ref = np.multiply(blobs_ref[0], init)\n        mul_blobs_seg = np.multiply(blobs_seg[0], init)\n        list_TP_ref = np.unique(mul_blobs_ref[mul_blobs_ref > 0])\n        list_TP_seg = np.unique(mul_blobs_seg[mul_blobs_seg > 0])\n\n        list_FN = [x for x in list_blobs_ref if x not in list_TP_ref]\n        list_FP = [x for x in list_blobs_seg if x not in list_TP_seg]\n        # print(np.max(blobs_ref),np.max(blobs_seg))\n        tpc_map = np.zeros_like(blobs_ref[0])\n        fpc_map = np.zeros_like(blobs_ref[0])\n        fnc_map = np.zeros_like(blobs_ref[0])\n        for i in list_TP_ref:\n            tpc_map[blobs_ref[0] == i] = 1\n        for i in list_TP_seg:\n            tpc_map[blobs_seg[0] == i] = 1\n        for i in list_FN:\n            fnc_map[blobs_ref[0] == i] = 1\n        for i in list_FP:\n            fpc_map[blobs_seg[0] == i] = 1\n        return tpc_map, fnc_map, fpc_map\n\n    def outline_error(self):\n        """"""\n        This function calculates the outline error as defined in Wack et al.\n\n        :return: OER: Outline error ratio, OEFP: number of false positive\n            outlier error voxels, OEFN: number of false negative outline error\n            elements\n        """"""\n        TPcMap, _, _ = self.connected_errormaps()\n        OEFMap = self.ref - np.multiply(TPcMap, self.seg)\n        unique, counts = np.unique(OEFMap, return_counts=True)\n        # print(counts)\n        OEFN = counts[unique == 1]\n        OEFP = counts[unique == -1]\n        OEFN = 0 if len(OEFN) == 0 else OEFN[0]\n        OEFP = 0 if len(OEFP) == 0 else OEFP[0]\n        OER = 2 * (OEFN + OEFP) / (self.n_pos_seg() + self.n_pos_ref())\n        return OER, OEFP, OEFN\n\n    def detection_error(self):\n        """"""\n        This function calculates the volume of detection error as defined in\n        Wack et al.\n\n        :return: DE: Total volume of detection error, DEFP: Detection error\n            false positives, DEFN: Detection error false negatives\n        """"""\n        TPcMap, FNcMap, FPcMap = self.connected_errormaps()\n        DEFN = np.sum(FNcMap)\n        DEFP = np.sum(FPcMap)\n        return DEFN + DEFP, DEFP, DEFN\n\n    def header_str(self):\n        result_str = [self.m_dict[key][1] for key in self.measures]\n        result_str = \',\' + \',\'.join(result_str)\n        return result_str\n\n    def to_string(self, fmt=\'{:.4f}\'):\n        result_str = """"\n        list_space = [\'com_ref\', \'com_seg\', \'list_labels\']\n        for key in self.measures:\n            result = self.m_dict[key][0]()\n            if key in list_space:\n                result_str += \' \'.join(fmt.format(x) for x in result) \\\n                    if isinstance(result, tuple) else fmt.format(result)\n            else:\n                result_str += \',\'.join(fmt.format(x) for x in result) \\\n                    if isinstance(result, tuple) else fmt.format(result)\n            result_str += \',\'\n        return result_str[:-1]  # trim the last comma\n\n\nclass PairwiseMeasuresRegression(object):\n    def __init__(self, reg_img, ref_img, measures=None):\n        self.reg = reg_img\n        self.ref = ref_img\n        self.measures = measures\n\n        self.m_dict = {\n            \'mse\': (self.mse, \'MSE\'),\n            \'rmse\': (self.rmse, \'RMSE\'),\n            \'mae\': (self.mae, \'MAE\'),\n            \'r2\': (self.r2, \'R2\')\n        }\n\n    def mse(self):\n        return np.mean(np.square(self.reg - self.ref))\n\n    def rmse(self):\n        return np.sqrt(self.mse())\n\n    def mae(self):\n        return np.mean(np.abs(self.ref - self.reg))\n\n    def r2(self):\n        ref_var = np.sum(np.square(self.ref - np.mean(self.ref)))\n        reg_var = np.sum(np.square(self.reg - np.mean(self.reg)))\n        cov_refreg = np.sum(\n            (self.reg - np.mean(self.reg)) * (self.ref - np.mean(\n                self.ref)))\n        return np.square(cov_refreg / np.sqrt(ref_var * reg_var + 0.00001))\n\n    def header_str(self):\n        result_str = [self.m_dict[key][1] for key in self.measures]\n        result_str = \',\' + \',\'.join(result_str)\n        return result_str\n\n    def to_string(self, fmt=\'{:.4f}\'):\n        result_str = """"\n        for key in self.measures:\n            result = self.m_dict[key][0]()\n            result_str += \',\'.join(fmt.format(x) for x in result) \\\n                if isinstance(result, tuple) else fmt.format(result)\n            result_str += \',\'\n        return result_str[:-1]  # trim the last comma\n'"
niftynet/evaluation/region_properties.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport numpy.ma as ma\nimport math\nimport scipy.stats.mstats as mstats\nimport scipy.ndimage as ndimage\n\nfrom niftynet.utilities.util_common import MorphologyOps, CacheFunctionOutput\n\n\nclass RegionProperties(object):\n    """"""This class enables the extraction of image features (Harilick\n    features) and basic statistics over a segmentation""""""\n    def __init__(self, seg, img, measures,\n                 num_neighbors=6, threshold=0, pixdim=(1, 1, 1)):\n\n        self.seg = seg\n        self.bin = 100\n        self.mul = 100\n        self.trans = 0\n        self.img = img\n        self.img_channels = self.img.shape[4] if img.ndim >= 4 else 1\n        img_id = range(0, self.img_channels)\n        self.neigh = num_neighbors\n        self.harilick_m = np.atleast_2d(self.harilick_matrix())\n        self.m_dict = {\n            \'centre of mass\': (self.centre_of_mass, [\'CoMx\',\n                                                     \'CoMy\',\n                                                     \'CoMz\']),\n            \'volume\': (self.volume,\n                       [\'NVoxels\', \'NVoxelsBinary\', \'Vol\', \'VolBinary\']),\n            \'surface\': (self.surface, [\'NSurface\',\n                                       \'NSurfaceBinary\',\n                                       \'SurfaceVol\',\n                                       \'SurfaceVolBinary\']),\n            \'surface volume ratio\': (self.sav, [\'SAVNumb\',\n                                                \'SAVNumBinary\',\n                                                \'SAV\',\n                                                \'SAVBinary\']),\n            \'compactness\': (self.compactness, [\'CompactNumb\',\n                                               \'CompactNumbBinary\',\n                                               \'Compactness\',\n                                               \'CompactnessBinary\']),\n            \'mean\': (self.mean_, [\'Mean_%d\' % i for i in img_id]),\n            \'weighted_mean\': (self.weighted_mean_,\n                              [\'Weighted_mean_%d\' % i for i in img_id]),\n            \'median\': (self.median_, [\'Median_%d\' % i for i in img_id]),\n            \'skewness\': (self.skewness_, [\'Skewness_%d\' % i for i in img_id]),\n            \'kurtosis\': (self.kurtosis_, [\'Kurtosis_%d\' % i for i in img_id]),\n            \'min\': (self.min_, [\'Min_%d\' % i for i in img_id]),\n            \'max\': (self.max_, [\'Max_%d\' % i for i in img_id]),\n            \'quantile_25\': (self.quantile_25,\n                            [\'P25_%d\' % i for i in img_id]),\n            \'quantile_50\': (self.median_,\n                            [\'P50_%d\' % i for i in img_id]),\n            \'quantile_75\': (self.quantile_75,\n                            [\'P75_%d\' % i for i in img_id]),\n            \'std\': (self.std_, [\'STD_%d\' % i for i in img_id]),\n            \'asm\': (self.call_asm,\n                    [\'asm%d\' % i for i in\n                     img_id]),\n\n            \'contrast\': (self.call_contrast, [\'contrast%d\' % i for i in\n                                              img_id]),\n            \'correlation\': (self.call_correlation, [\'correlation%d\' % i\n                                                    for i\n                                                    in\n                                                    img_id]),\n            \'sumsquare\': (self.call_sum_square, [\'sumsquare%d\' % i for i in\n                                                 img_id]),\n            \'sum_average\': (self.call_sum_average, [\'sum_average%d\' % i\n                                                    for i in\n                                                    img_id]),\n            \'idifferentmomment\': (self.call_idifferent_moment,\n                                  [\'idifferentmomment%d\' % i for i in\n                                   img_id]),\n            \'sumentropy\': (self.call_sum_entropy, [\'sumentropy%d\' % i for i in\n                                                   img_id]),\n            \'entropy\': (self.call_entropy, [\'entropy%d\' % i for i in\n                                            img_id]),\n            \'differencevariance\': (self.call_difference_variance,\n                                   [\'differencevariance%d\' % i for i in\n                                    img_id]),\n            \'differenceentropy\': (self.call_difference_entropy,\n                                  [\'differenceentropy%d\'\n                                   % i for i in\n                                   img_id]),\n            \'sumvariance\': (self.call_sum_variance, [\'sumvariance%d\' % i for i\n                                                     in\n                                                     img_id]),\n            \'imc1\': (self.call_imc1, [\'imc1%d\' % i for i in img_id]),\n            \'imc2\': (self.call_imc2, [\'imc2%d\' % i for i in img_id])\n\n        }\n        self.measures = measures\n\n        self.pixdim = pixdim\n        self.threshold = threshold\n        if self.seg is not None:\n            self.masked_img, self.masked_seg = self.__compute_mask()\n        self.vol_vox = np.prod(pixdim)\n\n    def __compute_mask(self):\n        # TODO: check whether this works for probabilities type_str\n        foreground_selector = np.where((self.seg > 0).reshape(-1))[0]\n        probs = self.seg.reshape(-1)[foreground_selector]\n        regions = np.zeros((foreground_selector.shape[0], self.img_channels))\n        for i in np.arange(self.img_channels):\n            regions[:, i] = self.img[..., 0, i].reshape(-1)[foreground_selector]\n        return regions, probs\n\n    def centre_of_mass(self):\n        """"""\n        Calculates the centre of mass of the segmentation using the threshold\n        to binarise the initial map\n\n        :return:\n        """"""\n        return np.mean(np.argwhere(self.seg > self.threshold), 0)\n\n    @CacheFunctionOutput\n    def volume(self):\n        """"""\n        this calculates the integral of the segmentation (probability maps),\n        the corresponding binary number of elements, the probabilistic volume\n        and the binarised volume\n\n        :return:\n        """"""\n        numb_seg = np.sum(self.seg)\n        numb_seg_bin = np.sum(self.seg > 0)\n        return numb_seg, numb_seg_bin, \\\n               numb_seg * self.vol_vox, numb_seg_bin * self.vol_vox\n\n    @CacheFunctionOutput\n    def surface(self):\n        """"""\n        Similarly to the volume function, returns probabilistic sum, binary\n        number, probabilistic volume and binarised volume of the segmentation\n        border\n\n        :return:\n        """"""\n        border_seg = MorphologyOps(self.seg, self.neigh).border_map()\n        numb_border_seg_bin = np.sum(border_seg > 0)\n        numb_border_seg = np.sum(border_seg)\n        return numb_border_seg, numb_border_seg_bin, \\\n               numb_border_seg * self.vol_vox, numb_border_seg_bin * self.vol_vox\n\n    def glcm(self):\n        """"""\n        Creation of the grey level co-occurrence matrix. The neighbourhood\n        distance is set to 1 in this instance. All neighborhood shifts are\n        calculated for each modality\n\n        :return: multi_mod_glcm list of m (number of modalities) matrices of\n            size bin x bin x neigh\n        """"""\n        shifts = [[0, 0, 0],\n                  [1, 0, 0],\n                  [-1, 0, 0],\n                  [0, 1, 0],\n                  [0, -1, 0],\n                  [0, 0, 1],\n                  [0, 0, -1],\n                  [1, 1, 0],\n                  [-1, -1, 0],\n                  [-1, 1, 0],\n                  [1, -1, 0],\n                  [1, 1, 0],\n                  [0, -1, -1],\n                  [0, -1, 1],\n                  [0, 1, -1],\n                  [1, 0, 1],\n                  [-1, 0, -1],\n                  [-1, 0, 1],\n                  [1, 0, -1],\n                  [1, 1, 1],\n                  [-1, 1, -1],\n                  [-1, 1, 1],\n                  [1, 1, -1],\n                  [1, -1, 1],\n                  [-1, -1, -1],\n                  [-1, -1, 1],\n                  [1, -1, -1]]\n        bins = np.arange(0, self.bin)\n        multi_mod_glcm = []\n        if self.seg is None:\n            return None\n        for m in range(0, self.img.shape[4]):\n            shifted_image = []\n            for n in range(0, self.neigh + 1):\n\n                new_img = self.seg * self.img[:, :, :, 0, m]\n                print(np.max(self.img), \'is max img\')\n                new_img = ndimage.shift(new_img, shifts[n], order=0)\n                print(np.max(self.seg), \'is max shift\')\n                if np.count_nonzero(new_img) > 0:\n                    flattened_new = new_img.flatten()\n                    flattened_seg = self.seg.flatten()\n                    affine = np.round(flattened_new * self.mul + self.trans)\n                    # select = [round(flattened_new[i] * self.mul+self.trans) for i in\n                    #                 range(0, new_img.size) if\n                    #           flattened_seg[i]>0]\n\n                    select_new = np.digitize(affine[flattened_seg == 1], bins)\n                    select_new[select_new >= self.bin] = self.bin - 1\n                    print(np.max(select_new), \' is max bin\', np.max(affine))\n                    shifted_image.append(select_new)\n            glcm = np.zeros([self.bin, self.bin, self.neigh])\n            for n in range(0, self.neigh):\n                for i in range(0, shifted_image[0].size):\n                    glcm[shifted_image[0][i], shifted_image[n + 1][i], n] += 1\n                glcm[:, :, n] = glcm[:, :, n] / np.sum(glcm[:, :, n])\n            multi_mod_glcm.append(glcm)\n        return multi_mod_glcm\n\n    def harilick_matrix(self):\n        """"""\n        this function populates the matrix of harilick features for each\n        image modality and neighborhood shift and average over the neighbours\n\n        :return:\n        """"""\n        multi_mod_glcm = self.glcm()\n        matrix_harilick = np.zeros([13, self.neigh, self.img_channels])\n        if multi_mod_glcm is None:\n            return np.average(matrix_harilick, axis=1)\n\n        for i in range(0, self.img_channels):\n            for j in range(0, self.neigh):\n                matrix = multi_mod_glcm[i][..., j]\n                harilick_vector = self.harilick(matrix)\n                for index, elem in enumerate(harilick_vector):\n                    matrix_harilick[index, j, i] = elem\n        return np.average(matrix_harilick, axis=1)\n\n    def call_asm(self):\n        """"""\n        Extracts the angular second moment features from the harilick matrix of\n        features. Length of the output is the number of modalities\n\n        :return:\n        """"""\n        return self.harilick_m[0, :]\n\n    def call_contrast(self):\n        """"""\n        Extracts the contrast feature from the harilick matrix of\n        features\n\n        :return:\n        """"""\n        return self.harilick_m[1, :]\n\n    def call_correlation(self):\n        """"""\n        Extracts the correlation feature from the harilick matrix of\n        features\n\n        :return:\n        """"""\n        return self.harilick_m[2, :]\n\n    def call_sum_square(self):\n        """"""\n        Extracts the sum square feature from the harilick matrix of\n        features\n\n        :return:\n        """"""\n        return self.harilick_m[3, :]\n\n    def call_sum_average(self):\n        """"""\n        Extracts the sum average feature from the harilick matrix of\n        features\n\n        :return:\n        """"""\n        return self.harilick_m[4, :]\n\n    def call_idifferent_moment(self):\n        """"""\n        Extracts the inverse difference of moment feature from the\n        harilick matrix of features\n\n        :return:\n        """"""\n        return self.harilick_m[5, :]\n\n    def call_sum_entropy(self):\n        """"""\n        Extracts the sum entropy features from the harilick matrix of features\n\n        :return:\n        """"""\n        return self.harilick_m[6, :]\n\n    def call_entropy(self):\n        """"""\n        Extracts the entropy features from the harilick matrix of features\n\n        :return:\n        """"""\n        return self.harilick_m[7, :]\n\n    def call_difference_variance(self):\n        """"""\n        Extracts the difference variance from the harilick matrix of features\n\n        :return:\n        """"""\n        return self.harilick_m[8, :]\n\n    def call_difference_entropy(self):\n        """"""\n        Extracts the difference entropy features from the harilic matrix of\n        features\n\n        :return:\n        """"""\n        return self.harilick_m[9, :]\n\n    def call_sum_variance(self):\n        """"""\n        Extracts the difference entropy features from the harilick matrix of\n        features\n\n        :return:\n        """"""\n        return self.harilick_m[10, :]\n\n    def call_imc1(self):\n        """"""\n        Extracts the first information measure of correlation from the\n        harilick matrix of features\n\n        :return:\n        """"""\n        return self.harilick_m[11, :]\n\n    def call_imc2(self):\n        """"""\n        Extracts the second information measure of correlation from the\n        harilick matrix of features\n\n        :return:\n        """"""\n        return self.harilick_m[12, :]\n\n    def harilick(self, matrix):\n        """"""\n        Creates the vector of harilick features for one glcm matrix.\n        Definition of the Harilick features can be found in\n\n            Textural features for image classification\n            Robert Harilick K, Shanmugam and Its\'Hak Dinstein\n            in IEEE Transactions on systems, man and cybernetics\n            Vol SMC-3 issue 6 pp610-621\n\n        :param matrix: glcm matrix on which to calculates the Harilick features\n        :return:\n        """"""\n        vector_harilick = []\n        asm = self.angular_second_moment(matrix)\n        contrast = self.contrast(matrix)\n        correlation = self.correlation(matrix)\n        sum_square = self.sum_square_variance(matrix)\n        sum_average = self.sum_average(matrix)\n        idifferentmomment = self.inverse_difference_moment(matrix)\n        sum_entropy = self.sum_entropy(matrix)\n        entropy = self.entropy(matrix)\n        differencevariance, differenceentropy = \\\n            self.difference_variance_entropy(matrix)\n        sum_variance = self.sum_variance(matrix)\n        imc1, imc2 = self.information_measure_correlation(matrix)\n        vector_harilick.append(asm)\n        vector_harilick.append(contrast)\n        vector_harilick.append(correlation)\n        vector_harilick.append(sum_square)\n        vector_harilick.append(sum_average)\n        vector_harilick.append(idifferentmomment)\n        vector_harilick.append(sum_entropy)\n        vector_harilick.append(entropy)\n        vector_harilick.append(differencevariance)\n        vector_harilick.append(differenceentropy)\n        vector_harilick.append(sum_variance)\n        vector_harilick.append(imc1)\n        vector_harilick.append(imc2)\n        return vector_harilick\n\n    def angular_second_moment(self, matrix):\n        """"""\n        Calculates the angular second moment\n\n        :param matrix:\n        :return:\n        """"""\n        asm = 0\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                asm += matrix[i, j] ** 2\n        return asm\n\n    def contrast(self, matrix):\n        """"""\n        Calculates the angular second moment\n\n        :param matrix:\n        :return:\n        """"""\n        contrast = 0\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[1]):\n                contrast += (j - i) ** 2 * matrix[i, j]\n        return contrast\n\n    def homogeneity(self, matrix):\n        """"""\n        Calculates the homogeneity over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        homogeneity = 0\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[1]):\n                homogeneity += matrix[i, j] / (1 - abs(i - j))\n        return homogeneity\n\n    def energy(self, matrix):\n        """"""\n        Calculates the energy over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        energy = 0\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                energy += matrix[i, j] ** 2\n        return energy\n\n    def entropy(self, matrix):\n        """"""\n        Calculates the entropy over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        entropy = 0\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                if matrix[i, j] > 0:\n                    entropy += matrix[i, j] * math.log(matrix[i, j])\n        return entropy\n\n    def correlation(self, matrix):\n        """"""\n        Calculates the correlation over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        range_values = np.arange(0, matrix.shape[0])\n        matrix_range = np.tile(range_values, [matrix.shape[0], 1])\n        mean_matrix = np.sum(matrix_range * matrix, axis=0)\n        sd_matrix = np.sqrt(np.sum((matrix_range - np.tile(mean_matrix,\n                                                           [matrix.shape[\n                                                                0], 1])) ** 2 * matrix, axis=0))\n        correlation = 0\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                if sd_matrix[i] > 0 and sd_matrix[j] > 0:\n                    correlation += (i * j * matrix[i, j] - mean_matrix[i] * mean_matrix[\n                        j]) / (sd_matrix[i] * sd_matrix[j])\n        return correlation\n\n    def inverse_difference_moment(self, matrix):\n        """"""\n        Calculates the inverse difference moment over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        idm = 0\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                idm += 1.0 / (1 + (i - j) ** 2) * matrix[i, j]\n        return idm\n\n    def sum_average(self, matrix):\n        """"""\n        Calculates the sum average over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        sa = 0\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                sa += (i + j) * matrix[i, j]\n        return sa\n\n    def sum_entropy(self, matrix):\n        """"""\n        Calculates the sum entropy over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        se = 0\n        matrix_bis = np.zeros([2 * matrix.shape[0]])\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                matrix_bis[i + j] += matrix[i, j]\n        for v in matrix_bis:\n            if v > 0:\n                se += v * math.log(v)\n        return se\n\n    def sum_variance(self, matrix):\n        """"""\n        Calculates the sum variance over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        sv = 0\n        se = self.sum_entropy(matrix)\n        matrix_bis = np.zeros([2 * matrix.shape[0]])\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                matrix_bis[i + j] += matrix[i, j]\n        for i in range(0, matrix_bis.size):\n            sv += (i - se) ** 2 * matrix_bis[i]\n        return sv\n\n    def difference_variance_entropy(self, matrix):\n        """"""\n        Calculates the difference of variance entropy over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        dv = 0\n        de = 0\n        matrix_bis = np.zeros([matrix.shape[0]])\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                matrix_bis[abs(i - j)] += matrix[i, j]\n        for i in range(0, matrix.shape[0]):\n            dv += matrix_bis[i] * i ** 2\n            if matrix_bis[i] > 0:\n                de -= math.log(matrix_bis[i]) * matrix_bis[i]\n        return dv, de\n\n    def information_measure_correlation(self, matrix):\n        """"""\n        Calculates the two measures of information measure of correlation\n        over the glcm matrix\n\n        :param matrix:\n        :return: ic_1, ic_2\n        """"""\n        hxy = self.entropy(matrix)\n        sum_row = np.sum(matrix, axis=0)\n        hxy_1 = 0\n        hxy_2 = 0\n        hx = 0\n        for i in range(0, matrix.shape[0]):\n            hx -= sum_row[i] * math.log(sum_row[i] + 0.001)\n            for j in range(0, matrix.shape[0]):\n                hxy_1 -= matrix[i, j] * math.log(sum_row[i] * sum_row[j] + 0.001)\n                hxy_2 -= sum_row[i] * sum_row[j] * math.log(sum_row[i] *\n                                                            sum_row[j] + 0.001)\n        ic_1 = (hxy - hxy_1) / (hx)\n        if hxy == 0:\n            ic_2 = 0\n        else:\n            ic_2 = math.sqrt(1 - math.exp(-2 * (hxy_2 - hxy)))\n        return ic_1, ic_2\n\n    def sum_square_variance(self, matrix):\n        """"""\n        Calculates the sum of square variance over the glcm matrix\n\n        :param matrix:\n        :return:\n        """"""\n        ssv = 0\n        range_values = np.arange(0, matrix.shape[0])\n        matrix_range = np.tile(range_values, [matrix.shape[0], 1])\n        mean = np.average(matrix_range * matrix)\n        for i in range(0, matrix.shape[0]):\n            for j in range(0, matrix.shape[0]):\n                ssv += (i - mean) ** 2 * matrix[i, j]\n        return ssv\n\n    def sav(self):\n        """"""\n        Calculates the Surface area / Volume ratio in terms of Probabilistic\n        Count, Binarised count, Probabilistic Volume, Binarised Volume\n\n        :return:\n        """"""\n        Sn, Snb, Sv, Svb = self.surface()\n        Vn, Vnb, Vv, Vvb = self.volume()\n        return Sn / Vn, Snb / Vnb, Sv / Vv, Svb / Vvb\n\n    def compactness(self):\n        """"""\n        Calculates the compactness S^1.5/V in terms of probabilistic count,\n        binarised count, probabilistic volume, binarised volume\n\n        :return:\n        """"""\n        Sn, Snb, Sv, Svb = self.surface()\n        Vn, Vnb, Vv, Vvb = self.volume()\n        return np.power(Sn, 1.5) / Vn, np.power(Snb, 1.5) / Vnb, \\\n               np.power(Sv, 1.5) / Vv, np.power(Svb, 1.5) / Vvb\n\n    def min_(self):\n        """"""\n        Calculates the minimum of the image over the segmentation\n\n        :return:\n        """"""\n        return ma.min(self.masked_img, 0)\n\n    def max_(self):\n        """"""\n        Calculates the maximum of the image over the segmentation\n\n        :return:\n        """"""\n        return ma.max(self.masked_img, 0)\n\n    def weighted_mean_(self):\n        """"""\n        Calculates the weighted mean of the image given the probabilistic\n        segmentation. If binary, mean and weighted mean will give the same\n        result\n\n        :return:\n        """"""\n        masked_seg = np.tile(self.masked_seg, [self.img_channels, 1]).T\n        return ma.average(self.masked_img, axis=0, weights=masked_seg).flatten()\n\n    def mean_(self):\n        """"""\n        Calculates the mean of the image over the segmentation\n\n        :return:\n        """"""\n        return ma.mean(self.masked_img, 0)\n\n    def skewness_(self):\n        """"""\n        Calculates the skewness of the image over the binarised segmentation\n\n        :return:\n        """"""\n        return mstats.skew(self.masked_img, 0)\n\n    def std_(self):\n        """"""\n        calculates the standard deviation of the image over the binarised\n        segmentation\n\n        :return:\n        """"""\n        return ma.std(self.masked_img, 0)\n\n    def kurtosis_(self):\n        """"""\n        calculates the kurtosis of the image over the binarised segmentation\n\n        :return:\n        """"""\n        return mstats.kurtosis(self.masked_img, 0)\n\n    def median_(self):\n        """"""\n        calculates the median of the image over the binarised segmentation\n\n        :return:\n        """"""\n        return ma.median(self.masked_img, 0)\n\n    def quantile_25(self):\n        """"""\n        calculates the first quartile of the image over the binarised\n        segmentation\n\n        :return:\n        """"""\n        return mstats.mquantiles(self.masked_img, prob=0.25, axis=0).flatten()\n\n    def quantile_75(self):\n        """"""\n        calculates the third quartile of the image over the binarised\n        segmentation\n\n        :return:\n        """"""\n        return mstats.mquantiles(self.masked_img, prob=0.75, axis=0).flatten()\n\n    def header_str(self):\n        """"""\n        creates the header string to be output as part of the result report\n\n        :return:\n        """"""\n        result_str = [j for i in self.measures for j in self.m_dict[i][1]]\n        result_str = \',\' + \',\'.join(result_str)\n        return result_str\n\n    def to_string(self, fmt=\'{:4f}\'):\n        """"""\n        transforms the result dictionary into a string according to a\n        specified format to be written on the result report\n\n        :param fmt: Format under which the result will be written e.g \'{:4f}\'\n        :return:\n        """"""\n        result_str = """"\n        for i in self.measures:\n            for j in self.m_dict[i][0]():\n                try:\n                    result_str += \',\' + fmt.format(j)\n                except ValueError:  # some functions give strings e.g., ""--""\n                    print(i, j)\n                    result_str += \',{}\'.format(j)\n        return result_str\n'"
niftynet/evaluation/regression_evaluations.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines built-in evaluation functions for regression applications\n\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport pandas as pd\n\nfrom niftynet.evaluation.base_evaluations import BaseEvaluation\n\n\nclass BaseRegressionEvaluation(BaseEvaluation):\n    """""" Interface for scalar regression metrics """"""\n    def layer_op(self, subject_id, data):\n        metric_name = self.__class__.__name__\n        metric_value = self.metric(data[\'inferred\'], data[\'output\'])\n        pdf = pd.DataFrame.from_records([{\'subject_id\':subject_id,\n                                          metric_name:metric_value}],\n                                        (\'subject_id\',))\n        return [pdf]\n\n    def metric(self, reg, ref):\n        """"""\n        Computes a scalar value for the metric\n        :param reg: np.array with inferred regression\n        :param ref: np array with the reference output\n        :return: scalar metric value\n        """"""\n        raise NotImplementedError\n\n#pylint: disable=invalid-name\nclass mse(BaseRegressionEvaluation):\n    """""" Computes mean squared error """"""\n    def metric(self, reg, ref):\n        return np.mean(np.square(reg - ref))\n\n\nclass rmse(BaseRegressionEvaluation):\n    """""" Computes root mean squared error """"""\n    def metric(self, reg, ref):\n        return  np.sqrt(np.mean(np.square(reg - ref)))\n\n\nclass mae(BaseRegressionEvaluation):\n    """""" Computes mean absolute error """"""\n    def metric(self, reg, ref):\n        return np.mean(np.abs(ref - reg))\n'"
niftynet/evaluation/regression_evaluator.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines the specialized Evaluator for segmentation applications\nAll logic except default metrics is delegated to the parent class\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom niftynet.evaluation.base_evaluator import BaseEvaluator\n\nclass RegressionEvaluator(BaseEvaluator):\n    """"""\n    Evaluator for RegressionApplication\n    """"""\n    def default_evaluation_list(self):\n        """"""\n        :return:  list of metric names to compute by default\n        """"""\n        return [\'mse\', \'rmse\', \'mae\']\n'"
niftynet/evaluation/segmentation_evaluations.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines built-in evaluation functions for segmentation applications\n\nSegmentations can be evaluated at several scales:\n\'foreground\' refering to metrics computed once for a foreground label\n\'label\' refering to metrics computed once for each label (including background)\n\'cc\' referring to metrics computed once for each connected component set\n    Connected components are defined by one-or-more connected\n    components on the reference segmentation and one-or-more connected\n    components on the infered segmentation.\n    These sets are defined by a cc_func. Currently\n    this is hard coded to be union_of_seg_for_each_ref_cc which takes each\n    connected component on the reference segmentation and all connected\n    components on the infered segmentation with any overlap. This will\n    eventually be made into a factory option for different cc set definitions\n\nOverlap and distance measures can be computed at each of these levels by\nderiving from PerComponentEvaluation, which handles the logic of identifying\nwhich comparisons need to be done for each scale.\n\nOverlap and distance measures are computed in two convenience functions\n(compute_many_overlap_metrics and compute_many_distance_metrics) and wrapped\nby Evaluation classes\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import ndimage\n\nfrom niftynet.evaluation.base_evaluations import CachedSubanalysisEvaluation\nfrom niftynet.utilities.util_common import MorphologyOps, \\\n    CachedFunction, CachedFunctionByID\nfrom niftynet.evaluation.base_evaluator import ScalarAggregator\n\n\nclass PerComponentEvaluation(CachedSubanalysisEvaluation):\n    """"""\n    This class represents evaluations performed on binary segmentation\n    components computed per label or per connected component. It encodes the\n    generation of evaluation tasks. Derived classes should define the\n    metric_name constant and the function metric_from_binarized()\n    """"""\n\n    def subanalyses(self, subject_id, data):\n        analyses = self.app_param.evaluation_units.split(\',\')\n        tasks = []\n        for analysis in analyses:\n            if analysis in [\'foreground\', \'label\']:\n                labels = list(range(self.app_param.num_classes))\n                if analysis == \'foreground\':\n                    labels.remove(0)\n                for label in labels:\n                    tasks.append({\'subject_id\': subject_id, \'label\': label})\n            elif analysis in [\'cc\']:\n                cc_seg, cc_ref = \\\n                    connected_components(data[\'inferred\'], data[\'label\'],\n                                         self.app_param.output_prob)\n                cc_func = union_of_seg_for_each_ref_cc  # TODO make into factory\n                conncomps = cc_func(cc_seg, cc_ref)\n                for conncomp in conncomps:\n                    tasks.append({\'subject_id\': subject_id,\n                                  \'cc_labels\': conncomps[conncomp]})\n                    # TODO save an index image from blobs_ref[0]\n        return tasks\n\n    def layer_op(self, subject_id, data, task):\n        # We use a cached binarizer function so that the binarized\n        # segmentation have the same python id\n        if \'label\' in task:\n            binarizer = cached_label_binarizer(task[\'label\'],\n                                               self.app_param.output_prob)\n            seg, ref = binarizer(data)\n            metric_dict = {\'subject_id\': subject_id, \'label\': task[\'label\']}\n            metric_dict.update(self.metric_dict_from_binarized(seg, ref))\n            pdf = pd.DataFrame.from_records([metric_dict], (\'subject_id\', \'label\'))\n            return [pdf]\n        elif \'cc_labels\' in task:\n            binarizer = cached_cc_binarizer(task[\'cc_labels\'],\n                                            self.app_param.output_prob)\n            seg, ref = binarizer(data)\n            r_str = \'&\'.join([str(l) for l in task[\'cc_labels\'][1]])\n            s_str = \'&\'.join([str(l) for l in task[\'cc_labels\'][0]])\n            cc_id = \'r%s_s%s\' % (r_str, s_str)\n            metric_dict = {\'subject_id\': subject_id, \'cc_id\': cc_id}\n            metric_dict.update(self.metric_dict_from_binarized(seg, ref))\n            pdf = pd.DataFrame.from_records([metric_dict], (\'subject_id\', \'cc_id\'))\n            return [pdf]\n        return []\n\n\n    def metric_dict_from_binarized(self, seg, ref):\n        """"""\n        Computes a metric from a binarized mask\n        :param seg: numpy array with binary mask from inferred segmentation\n        :param ref: numpy array with binary mask from reference segmentation\n        :return: a dictionary of metric_name:metric_value\n        """"""\n        raise NotImplementedError(\'Not implemented in abstract base class\')\n\n\nclass PerComponentScalarEvaluation(PerComponentEvaluation):\n    """""" This class simplifies the implementation when the metric just returns a\n    single scalar with the same name as the class name""""""\n    def __init__(self, *args, **kwargs):\n        super(PerComponentScalarEvaluation, self).__init__(*args,\n                                                           **kwargs)\n        self.metric_name = self.__class__.__name__\n\n    def metric_dict_from_binarized(self, seg, ref):\n        """""" Wrap computed metric in dictionary for parent class """"""\n        metric_value = self.metric_from_binarized(seg, ref)\n        return {self.metric_name: metric_value}\n\n    def metric_from_binarized(self, seg, ref):\n        """"""\n        Computer scalar metric value\n        :param seg: numpy array with binary mask from inferred segmentation\n        :param ref: numpy array with binary mask from reference segmentation\n        :return: scalar metric value\n        """"""\n\n    def get_aggregations(self):\n        aggregations = []\n        analyses = self.app_param.evaluation_units.split(\',\')\n        for analysis in analyses:\n            if analysis in [\'foreground\', \'label\']:\n                mean_agg = ScalarAggregator(self.metric_name,\n                                            (\'subject_id\', \'label\'),\n                                            (\'label\',), np.mean,\n                                            \'mean_\' + self.metric_name)\n                std_agg = ScalarAggregator(self.metric_name,\n                                           (\'subject_id\', \'label\'),\n                                           (\'label\',), np.std,\n                                           \'stdev_\' + self.metric_name)\n                aggregations.extend([mean_agg, std_agg])\n            elif analysis in [\'cc\']:\n                pass\n        return aggregations\n\nclass BuiltinOverlapEvaluation(PerComponentScalarEvaluation):\n    """"""\n    Wrapper class to encode many similar overlap metrics that can be computed\n    from a confusion matrix\n    Metrics computed in compute_many_overlap_metrics can be wrapped by\n    overriding self.metric_name\n    """"""\n    def metric_from_binarized(self, seg, ref):\n        """"""\n        Computes a metric from a binarized mask by computing a confusion\n        matrix and then delegating the metric computation\n        :param seg: numpy array with binary mask from inferred segmentation\n        :param ref: numpy array with binary mask from reference segmentation\n        :return: scalar metric value\n        """"""\n        lnot = np.logical_not\n        land = np.logical_and\n        conf_mat = np.array([[np.sum(land(lnot(seg), lnot(ref))),\n                              np.sum(land(lnot(seg), (ref)))],\n                             [np.sum(land((seg), lnot(ref))),\n                              np.sum(land((seg), (ref)))]])\n        return self.metric_from_confusion_matrix(conf_mat)\n\n    def metric_from_confusion_matrix(self, confusion_matrix):\n        """"""\n        Compute metrics from a 2x2 confusion matrix\n        :param confusion_matrix: 2x2 numpy array\n        :return: scalar metric value\n        """"""\n\n\n#pylint: disable=missing-docstring,invalid-name\nclass n_pos_ref(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[0, 1] + M[1, 1]\n\n\nclass n_neg_ref(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[0, 0] + M[1, 0]\n\n\nclass n_pos_seg(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 0] + M[1, 1]\n\n\nclass n_neg_seg(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[0, 0] + M[0, 1]\n\n\nclass fp(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 0]\n\n\nclass fn(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[0, 1]\n\n\nclass tp(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 1]\n\n\nclass tn(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[0, 0]\n\n\nclass n_intersection(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 1]\n\n\nclass n_union(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[0, 1] + M[1, 0] + M[1, 1]\n\n\nclass specificity(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[0, 0] / (M[0, 0] + M[1, 0])\n\n\nclass sensitivity(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 1] / (M[0, 1] + M[1, 1])\n\n\nclass accuracy(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return (M[1, 1] + M[0, 0]) / sum(sum(M))\n\n\nclass false_positive_rate(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 0] / (M[0, 0] + M[1, 0])\n\n\nclass positive_predictive_values(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 1] / (M[1, 0] + M[1, 1])\n\n\nclass negative_predictive_values(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[0, 0] / (M[0, 0] + M[0, 1])\n\n\nclass dice(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return 2 * M[1, 1] / (M[1, 1] * 2 + M[1, 0] + M[0, 1])\n\n\nDice = dice\n\n\nclass jaccard(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 1] / (M[0, 1] + M[1, 0] + M[1, 1])\n\n\nintersection_over_union = jaccard\nJaccard = jaccard\n\n\nclass informedness(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 1] / (M[0, 1] + M[1, 1]) + \\\n                             M[0, 0] / (M[0, 0] + M[1, 0]) - 1\n\n\nclass markedness(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return M[1, 1] / (M[1, 0] + M[1, 1]) + \\\n                           M[0, 0] / (M[0, 0] + M[0, 1]) - 1\n\n\nclass vol_diff(BuiltinOverlapEvaluation):\n    def metric_from_confusion_matrix(self, M):\n        return (M[1, 1] + M[1, 0]) / (M[0, 1] + M[1, 1])\n\n\n# Distance metrics as e.g. in 10.3978/j.issn.2223-4292.2015.08.02\nclass average_distance(PerComponentScalarEvaluation):\n    def metric_from_binarized(self, seg, ref):\n        ref_border_dist, seg_border_dist = border_distance(seg, ref, 8)\n        border_ref, border_seg = borders(seg, ref, 8)\n        return (np.sum(ref_border_dist) + np.sum(\n            seg_border_dist)) / (np.sum(border_ref + border_seg))\n\n\nclass hausdorff_distance(PerComponentScalarEvaluation):\n    def metric_from_binarized(self, seg, ref):\n        ref_border_dist, seg_border_dist = border_distance(seg, ref, 8)\n        return np.max([np.max(ref_border_dist), np.max(seg_border_dist)])\n\n\nclass hausdorff95_distance(PerComponentScalarEvaluation):\n    def metric_from_binarized(self, seg, ref):\n        ref_border_dist, seg_border_dist = border_distance(seg, ref, 8)\n        border_ref, border_seg = borders(seg, ref, 8)\n        seg_values = ref_border_dist[border_seg > 0]\n        ref_values = seg_border_dist[border_ref > 0]\n        if seg_values.size == 0 or ref_values.size == 0:\n            return np.nan\n        return np.max([np.percentile(seg_values, 95),\n                       np.percentile(ref_values, 95)])\n\n\n#pylint: enable=missing-docstring,invalid-name\n# Helper functions\n@CachedFunction\ndef cached_label_binarizer(label, output_prob):\n    """"""\n    This class returns a function for binarizing an inferred segmentation\n    for a specified label.\n    This function is carefully designed to allow caching of unhashable numpy\n    objects. Specifically, each call to cached_label_binarizer with the same\n    (by-value) parameters will return the same (by python id) function\n    object. This enables two calls to\n    cached_label_binarizer(...)(numpy_object_1)\n    with the same parameters from different metrics to use the cached result\n    :param label:  Which label to make foreground in the binary mask\n    :param output_prob: Is the segmentation probabilistic (if so,\n    argmax is used first to compute a label map)\n    :return: a function for computing a binary label map\n    """"""\n    @CachedFunctionByID\n    def binarizer(data):\n        """"""\n        This function binarizes a segmentation based on a specified\n        label (defined by outer function)\n        :param data: a data dictionary as built by ImageReader\n        :return: a numpy array representing a binary label map\n        """"""\n        if output_prob:\n            out = np.argmax(data[\'inferred\'], -1)\n        else:\n            out = data[\'inferred\']\n        return out == label, data[\'label\'] == label\n\n    return binarizer\n\n\n@CachedFunction\ndef cached_cc_binarizer(cc_labels, output_prob):\n    """"""\n    This class returns a function for binarizing inferred and reference\n    segmentations for a specified connected component set.\n    This function is carefully designed to allow caching of unhashable numpy\n    objects. Specifically, each call to cached_label_binarizer with the same\n    (by-value) parameters will return the same (by python id) function\n    object. This enables two calls to\n    cached_cc_binarizer(...)(numpy_object_1)\n    with the same parameters from different metrics to use the cached result\n    :param cc_labels:  [seg_label_list, ref_label_list] where each is a\n        list of values to be considered foreground for this cc set\n    :param output_prob: Is the segmentation probabilistic (if so,\n        argmax is used first to compute a label map)\n    :return: a function for computing a binary label map pair\n\n    """"""\n    @CachedFunctionByID\n    def binarizer(data):\n        """"""\n        This function binarizes a multi-object segmentation and reference\n        into a specified connected component set (defined by outer function)\n        :param data: a data dictionary as built by ImageReader\n        :return: two numpy arrays representing binary masks (from\n        inferred and reference segmentations) for a connected component set\n        """"""\n        cc_func = connected_components\n        cc_seg, cc_ref = cc_func(data[\'inferred\'], data[\'label\'], output_prob)\n        cc_seg_in = np.zeros_like(cc_seg[0])\n        cc_ref_in = np.zeros_like(cc_ref[0])\n        for i in cc_labels[0]:\n            cc_seg_in[cc_seg[0] == i] = 1\n        for i in cc_labels[1]:\n            cc_ref_in[cc_ref[0] == i] = 1\n        return cc_seg_in, cc_ref_in\n\n    return binarizer\n\ndef union_of_seg_for_each_ref_cc(blobs_seg, blobs_ref):\n    """"""\n    Constructs connected component sets to compute metrics for. Each\n    reference connected component is paired with the union of inferred\n    segmentation connected components with any overlap\n    :param blobs_seg: tuple (numbered_cc_array, number_of_ccs)\n    :param blobs_ref: tuple (numbered_cc_array, number_of_ccs)\n    :return: dictionary {label:(ref_label_list, seg_label_list)}\n    """"""\n    keys = {}\n    for cc_id in range(1, blobs_ref[1] + 1):\n        seg_idx = list(np.unique(blobs_seg[0][blobs_ref[0] == cc_id]))\n        if 0 in seg_idx:\n            seg_idx.remove(0)\n        key = \'r\' + str(cc_id) + \'_c\' + \'_\'.join([str(s) for s in seg_idx])\n        keys[key] = ((cc_id,), tuple(seg_idx))\n    return keys\n\n\n@CachedFunctionByID\ndef borders(seg, ref, neigh=8):\n    """"""\n    This function determines the points that lie on the border of the\n    inferred and reference segmentations\n    :param seg: numpy array with binary mask from inferred segmentation\n    :param ref: numpy array with binary mask from reference segmentation\n    :param neigh: connectivity 4 or 8\n    :return: numpy arrays of reference and inferred segmentation borders\n    """"""\n    border_ref = MorphologyOps(ref[:, :, :, 0, 0], neigh).border_map()\n    border_seg = MorphologyOps(seg[:, :, :, 0, 0], neigh).border_map()\n    return border_ref, border_seg\n\n\n@CachedFunctionByID\ndef border_distance(seg, ref, neigh=8):\n    """"""\n    This functions determines the distance at each seg border point to the\n    nearest ref border point and vice versa\n    :param seg: numpy array with binary mask from inferred segmentation\n    :param ref: numpy array with binary mask from reference segmentation\n    :param neigh: connectivity 4 or 8\n    :return: numpy arrays for distance_from_ref_border, distance_from\n    seg_border\n    """"""\n    border_ref, border_seg = borders(seg, ref, neigh)\n    distance_ref = ndimage.distance_transform_edt(1 - border_ref)\n    distance_seg = ndimage.distance_transform_edt(1 - border_seg)\n    distance_border_seg = border_ref * distance_seg\n    distance_border_ref = border_seg * distance_ref\n    return distance_border_ref, distance_border_seg\n\n\n@CachedFunctionByID\ndef connected_components(seg, ref, output_prob, neigh=8):\n    """"""\n    Numbers connected components in the reference and inferred segmentations\n    :param seg: numpy array with binary mask from inferred segmentation\n    :param ref: numpy array with binary mask from reference segmentation\n    :param output_prob: Is the segmentation probabilistic (if so,\n    argmax is used first to compute a label map)\n    :param neigh: connectivity 4 or 8\n    :return: (cc_map_ref, count) numbered connected components from reference\n    :return: (cc_map_seg, count) numbered connected components from inferred\n    """"""\n    if output_prob:\n        seg = np.argmax(seg, -1)\n    blobs_ref = MorphologyOps(ref[:, :, :, 0, 0], neigh).foreground_component()\n    blobs_seg = MorphologyOps(seg[:, :, :, 0, 0], neigh).foreground_component()\n\n    return (blobs_ref[0][:, :, :, np.newaxis, np.newaxis], blobs_ref[1]), \\\n           (blobs_seg[0][:, :, :, np.newaxis, np.newaxis], blobs_seg[1]),\n\n\n# TODO\n# per subject connected component related metrics\n# \'connected_elements\': (self.connected_elements, \'TPc,FPc,FNc\'),\n# \'outline_error\': (self.outline_error, \'OER,OEFP,OEFN\'),\n# \'detection_error\': (self.detection_error, \'DE,DEFP,DEFN\'),\n# list_labels\n# list connected components\n# TODO image_map outputs\n'"
niftynet/evaluation/segmentation_evaluator.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines the specialized Evaluator for segmentation applications\nAll logic except default metrics is delegated to the parent class\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom niftynet.evaluation.base_evaluator import CachedSubanalysisEvaluator\n\nclass SegmentationEvaluator(CachedSubanalysisEvaluator):\n    """"""\n    Evaluator for SegmentationApplication\n    Supports caching of intermediate results which is\n    important for boundary error calculations\n    """"""\n    def default_evaluation_list(self):\n        """"""\n        :return:  list of metric names to compute by default\n        """"""\n        return [\'dice\', \'jaccard\', \'average_distance\']\n'"
niftynet/io/__init__.py,0,"b'""""""\n\n.. module:: niftynet.io\n   :synopsis: High-level input / output operations.\n\n""""""\n'"
niftynet/io/image_loader.py,3,"b'# -*- coding: utf-8 -*-\n""""""Imports images of multiple types (2D or 3D) as `nib.Nifti1Image`""""""\n\nfrom collections import OrderedDict\n\nimport nibabel as nib\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.utilities.util_import import require_module\n\nSUPPORTED_LOADERS = OrderedDict()\nAVAILABLE_LOADERS = OrderedDict()\n\n\n###############################################################################\n# Utility Image Loader Funtions\n###############################################################################\n\ndef register_image_loader(name, requires, min_version=None, auto_discover=True):\n    """"""\n    Function decorator to register an image loader.\n\n    SUPPORTED_LOADERS:\n        Ordered dictionary were each entry is a function decorated with\n        `@register_image_loader`. This is, every loader that NiftyNet supports.\n        This dictionary will be dynamically filled and will be identical for\n        every NiftyNet installation.\n\n        Used only for information or error messages and logging purposes.\n\n    AVAILABLE_LOADERS:\n        A subset of the `SUPPORTED_LOADERS` that contain only the loaders that\n        have the required library/module installed on the system. Dynamically\n        filled from every function decorated with `@register_image_loader` that\n        passes the import check. This list will be different for every\n        installation, as it is platform dependant.\n\n        Inspedted and used to load images in runtime.\n\n    Adding a new loader only requires to decorate a function with\n    `@register_image_loader` and it will populate SUPPORTED_LOADERS and\n    AVAILABLE_LOADERS accordingly in runtime. The function will receive\n    a filename as its only parameter, and will return an image and its\n    `4x4` affinity matrix. Dummy example:\n\n        @register_image_loader(\'fake\', requires=\'numpy\', min_version=\'1.13.3\',\n                               auto_discover=False)\n        def imread_numpy(filename):\n            np = require_module(\'numpy\')\n            return image2nibabel(np.random.rand(100, 100, 3), np.eye(4))\n\n    It registers a loader named \'fake\' that requires `numpy` version >= \'1.13.3\'\n    to be installed. It will first dynamically load numpy library and then\n    return a `(100, 100, 3)` fake color image and an identity `(4, 4)`\n    affinity matrix. `loader = fake` in the data section of a config file will\n    select this loader and generate fake data.\n\n    When `auto_discover=True` (default) the method will be available to be\n    automatically discovered and used if `loader` is not provided in the\n    config file. This is, if no loader is specified, all the loaders\n    registered with `auto_discover=True` will be looped in priority order.\n    """"""\n\n    def _wrapper(func):\n        """"""Wrapper that registers a function if it satisfies requirements.""""""\n        try:\n            auto_d = auto_discover\n            require_module(requires, min_version=min_version, mandatory=True)\n            AVAILABLE_LOADERS[name] = dict(func=func, auto_discover=auto_d)\n        except (ImportError, AssertionError):\n            pass\n        SUPPORTED_LOADERS[name] = (requires, min_version)\n        return func\n\n    return _wrapper\n\n\ndef load_image_obj(filename, loader=None):\n    """"""\n    Loads an image from a given loader or checking multiple loaders.\n\n    If `loader` is specified the selected loader will be used if it exists in\n    `AVAILABLE_LOADERS` (see above).\n\n    If no loader is specified, all the loaders registered with\n    `auto_discover=True` (default) will be looped in priority order.\n    """"""\n    if loader and loader in SUPPORTED_LOADERS:\n        if loader not in AVAILABLE_LOADERS:\n            raise ValueError(\'Image Loader {} supported but library not found.\'\n                             \' Required libraries: {}\'\n                             .format(loader, SUPPORTED_LOADERS[loader]))\n        tf.logging.debug(\'Using requested loader: {}\'.format(loader))\n        loader_params = AVAILABLE_LOADERS[loader]\n        return loader_params[\'func\'](filename)\n    if loader:\n        raise ValueError(\'Image Loader {} not supported. Supported loaders: {}\'\n                         .format(loader, list(SUPPORTED_LOADERS.keys())))\n\n    for name, loader_params in AVAILABLE_LOADERS.items():\n        if not loader_params[\'auto_discover\']:\n            continue\n\n        try:\n            img = loader_params[\'func\'](filename)\n            tf.logging.debug(\'Using Image Loader {}.\'.format(name))\n            return img\n        except IOError:\n            # e.g. Nibabel cannot load standard 2D images\n            # e.g. PIL cannot load 16bit TIF images\n            pass\n\n    raise ValueError(\'No available loader could load file: {}.\'\n                     \' Available loaders: {}. Supported Loaders: {}\'\n                     .format(filename, list(AVAILABLE_LOADERS.keys()),\n                             list(SUPPORTED_LOADERS.keys())))\n\n\n###############################################################################\n# All supported Image Loaders -- In Priority Order\n###############################################################################\n\n@register_image_loader(\'nibabel\', requires=\'nibabel\')\ndef imread_nibabel(filename):\n    """"""Default nibabel loader for NiftyNet.""""""\n    try:\n        return nib.load(filename)\n    except nib.filebasedimages.ImageFileError:\n        raise IOError(\'Nibabel could not load image file: {}\'.format(filename))\n\n\n@register_image_loader(\'opencv\', requires=\'cv2\')\ndef imread_opencv(filename):\n    """"""OpenCV image loader with identity 2D affine.""""""\n    cv2 = require_module(\'cv2\')\n    img = cv2.imread(filename, flags=-1)\n    if img is None:\n        raise IOError(filename)\n    return image2nibabel(img[..., ::-1])\n\n\n@register_image_loader(\'skimage\', requires=\'skimage.io\', min_version=(0, 13))\ndef imread_skimage(filename):\n    """"""Scikit-image loader with an identity affine matrix.""""""\n    skio = require_module(\'skimage.io\')\n    img = skio.imread(filename)\n    return image2nibabel(img)\n\n\n@register_image_loader(\'pillow\', requires=\'PIL.Image\')\ndef imread_pillow(filename):\n    """"""PIL (Pillow) image loader with an identity affine matrix.""""""\n    pil = require_module(\'PIL.Image\')\n    img = np.asarray(pil.open(filename))\n    return image2nibabel(img)\n\n\n@register_image_loader(\'simpleitk\', requires=\'SimpleITK\')\ndef imread_sitk(filename):\n    """"""SimpleITK requires two function calls to retrieve a numpy array.""""""\n    sitk = require_module(\'SimpleITK\')\n    try:\n        simg = sitk.ReadImage(filename)\n    except RuntimeError:\n        raise IOError(filename)\n    img = sitk.GetArrayFromImage(simg)\n    if simg.GetDimension() > 2:\n        img = img.transpose()\n    return image2nibabel(img, affine=make_affine_from_sitk(simg))\n\n\n@register_image_loader(\'dummy\', requires=\'numpy\', auto_discover=False)\ndef imread_numpy(filename=None):\n    """"""Fake loader to load random data with numpy""""""\n    fake_img = np.random.randint(255, size=(100, 100, 3)).astype(np.uint8)\n    print(\'test case {}\', filename)\n    return image2nibabel(fake_img, affine=np.eye(4))\n\n\ntf.logging.info(\n    \'Available Image Loaders:\\n{}.\'.format(list(AVAILABLE_LOADERS.keys())))\n\n\n###############################################################################\n# Auxiliary functions\n###############################################################################\n\ndef image2nibabel(img, affine=np.eye(4)):\n    """"""\n    Loads a RGB or Grayscale Image from a file and stores it in a 5D array,\n    moving the color channels to the last axis for color images.\n    """"""\n    return ImageAsNibabel(img, affine)\n\n\nclass ImageAsNibabel(nib.Nifti1Image):\n    """"""\n    Wrapper class around a Nibabel file format. Loads an image using PIL\n    (or scikit-image if available) and transforms it to a `nib.Nifti1Image`.\n\n    The resulting 2D color image is already translated to a 5D array,\n    swapping the channels to the last axis.\n    """"""\n\n    def __init__(self, img, affine):\n        if img.ndim == 3 and img.shape[2] <= 4:  # Color Image\n            img = img[:, :, None, None, :]\n\n        if img.dtype == np.bool:  # bool is not a supported datatype by nibabel\n            img = img.astype(np.uint8)\n\n        nib.Nifti1Image.__init__(self, img, affine)\n\n\ndef make_affine_from_sitk(sitk_img):\n    """"""Get affine transform in LPS""""""\n    if sitk_img.GetDepth() <= 0:\n        return np.eye(4)\n\n    rot = [sitk_img.TransformContinuousIndexToPhysicalPoint(p)\n           for p in ((1, 0, 0),\n                     (0, 1, 0),\n                     (0, 0, 1),\n                     (0, 0, 0))]\n    rot = np.array(rot)\n    affine = np.concatenate([\n        np.concatenate([rot[0:3] - rot[3:], rot[3:]], axis=0),\n        [[0.], [0.], [0.], [1.]]\n    ], axis=1)\n    affine = np.transpose(affine)\n    # convert to RAS to match nibabel\n    affine = np.matmul(np.diag([-1., -1., 1., 1.]), affine)\n    return affine\n'"
niftynet/io/image_reader.py,17,"b'# -*- coding: utf-8 -*-\n""""""This module loads images from csv files and outputs numpy arrays.""""""\nfrom __future__ import absolute_import, division, print_function\n\nfrom copy import deepcopy\nimport argparse\nimport numpy as np\nimport pandas\nimport tensorflow as tf\nfrom six import string_types\n\nfrom niftynet.io.misc_io import dtype_casting\nfrom niftynet.io.image_sets_partitioner import COLUMN_UNIQ_ID\nfrom niftynet.io.image_type import ImageFactory\nfrom niftynet.layer.base_layer import Layer, DataDependentLayer, RandomisedLayer\nfrom niftynet.utilities.user_parameters_helper import make_input_tuple\nfrom niftynet.utilities.util_common import print_progress_bar, ParserNamespace\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import look_up_operations\n\nDEFAULT_INTERP_ORDER = 1\nSUPPORTED_DATA_SPEC = {\n\n    \'csv_file\', \'path_to_search\', \'csv_data_file\', \'filename_removefromid\',\n    \'filename_contains\', \'filename_not_contains\', \'to_ohe\',\n    \'interp_order\', \'loader\', \'pixdim\', \'axcodes\', \'spatial_window_size\'}\n\n\ndef infer_tf_dtypes(image_array):\n    """"""\n    Choosing a suitable tf dtype based on the dtype of input numpy array.\n    """"""\n    return dtype_casting(\n        image_array.dtype[0], image_array.interp_order[0], as_tf=True)\n\n\nclass ImageReader(Layer):\n    """"""\n    For a concrete example::\n\n        _input_sources define multiple modality mappings, e.g.,\n        _input_sources {\'image\': (\'T1\', \'T2\'), \'label\': (\'manual_map\',)}\n\n    means:\n\n    \'image\' consists of two components, formed by\n    concatenating \'T1\' and \'T2\' input source images.\n    \'label\' consists of one component, loading from \'manual_map\'\n\n    :param self._names: a tuple of the output names of this reader.\n        ``(\'image\', \'labels\')``\n\n    :param self._shapes: the shapes after combining input sources\n        ``{\'image\': (192, 160, 192, 1, 2), \'label\': (192, 160, 192, 1, 1)}``\n\n    :param self._dtypes: store the dictionary of tensorflow shapes\n        ``{\'image\': tf.float32, \'label\': tf.float32}``\n\n    :param self.output_list: a list of dictionaries, with each item::\n\n        {\'image\': <niftynet.io.image_type.SpatialImage4D object>,\n        \'label\': <niftynet.io.image_type.SpatialImage3D object>}\n\n    """"""\n\n    def __init__(self, names=None):\n        # list of file names\n        self._file_list = None\n        self._input_sources = None\n        self._spatial_ranks = None\n        self._shapes = None\n        self._dtypes = None\n        self._names = None\n        if names:\n            self.names = names\n\n        # list of image objects\n        self.output_list = None\n        self.current_id = -1\n\n        self.preprocessors = []\n        super(ImageReader, self).__init__(name=\'image_reader\')\n\n    def initialise(self, data_param, task_param=None, file_list=None):\n        """"""\n        ``task_param`` specifies how to combine user input modalities.\n        e.g., for multimodal segmentation \'image\' corresponds to multiple\n        modality sections, \'label\' corresponds to one modality section\n\n        This function converts elements of ``file_list`` into\n        dictionaries of image objects, and save them to ``self.output_list``.\n        e.g.::\n\n             data_param = {\'T1\': {\'path_to_search\': \'path/to/t1\'}\n                           \'T2\': {\'path_to_search\': \'path/to/t2\'}}\n\n        loads pairs of T1 and T1 images (grouped by matching the filename).\n        The reader\'s output is in the form of\n        ``{\'T1\': np.array, \'T2\': np.array}``.\n        If the (optional) ``task_param`` is specified::\n\n             task_param = {\'image\': (\'T1\', \'T2\')}\n\n        the reader loads pairs of T1 and T2 and returns the concatenated\n        image (both modalities should have the same spatial dimensions).\n        The reader\'s output is in the form of ``{\'image\': np.array}``.\n\n\n        :param data_param: dictionary of input sections\n        :param task_param: dictionary of grouping\n        :param file_list: a dataframe generated by ImagePartitioner\n            for cross validation, so\n            that the reader only loads files in training/inference phases.\n        :return: the initialised reader instance\n        """"""\n        data_param = param_to_dict(data_param)\n\n        if not task_param:\n            task_param = {mod: (mod,) for mod in list(data_param)}\n        try:\n            if not isinstance(task_param, dict):\n                task_param = vars(task_param)\n        except ValueError:\n            tf.logging.fatal(\n                ""To concatenate multiple input data arrays,\\n""\n                ""task_param should be a dictionary in the form:\\n""\n                ""{\'new_modality_name\': [\'modality_1\', \'modality_2\',...]}."")\n            raise\n        if file_list is None:\n            # defaulting to all files detected by the input specification\n            file_list = ImageSetsPartitioner().initialise(data_param).all_files\n        if not self.names:\n            # defaulting to load all sections defined in the task_param\n            self.names = list(task_param)\n        valid_names = [name for name in self.names\n                       if task_param.get(name, None)]\n        if not valid_names:\n            tf.logging.fatal(""Reader requires task input keywords %s, but ""\n                             ""not exist in the config file.\\n""\n                             ""Available task keywords: %s"",\n                             self.names, list(task_param))\n            raise ValueError\n        self.names = valid_names\n\n        self._input_sources = dict((name, task_param.get(name))\n                                   for name in self.names)\n        required_sections = \\\n            sum([list(task_param.get(name)) for name in self.names], [])\n\n        for required in required_sections:\n            try:\n                if (file_list is None) or \\\n                        (required not in list(file_list)) or \\\n                        (file_list[required].isnull().all()):\n                    tf.logging.fatal(\'Reader required input section \'\n                                     \'name [%s], but in the filename list \'\n                                     \'the column is empty.\', required)\n                    raise ValueError\n            except (AttributeError, TypeError, ValueError):\n                tf.logging.fatal(\n                    \'file_list parameter should be a \'\n                    \'pandas.DataFrame instance and has input \'\n                    \'section name [%s] as a column name.\', required)\n                if required_sections:\n                    tf.logging.fatal(\'Reader requires section(s): %s\',\n                                     required_sections)\n                if file_list is not None:\n                    tf.logging.fatal(\'Configuration input sections are: %s\',\n                                     list(file_list))\n                raise\n\n        self.output_list, self._file_list = _filename_to_image_list(\n            file_list, self._input_sources, data_param)\n        for name in self.names:\n            tf.logging.info(\n                \'Image reader: loading %d subjects \'\n                \'from sections %s as input [%s]\',\n                len(self.output_list), self.input_sources[name], name)\n        return self\n\n    def prepare_preprocessors(self):\n        """"""\n        Some preprocessors requires an initial step to initialise\n        data dependent internal parameters.\n\n        This function find these preprocessors and run the initialisations.\n        """"""\n        for layer in self.preprocessors:\n            if isinstance(layer, DataDependentLayer):\n                layer.train(self.output_list)\n\n    def add_preprocessing_layers(self, layers):\n        """"""\n        Adding a ``niftynet.layer`` or a list of layers as preprocessing steps.\n        """"""\n        assert self.output_list is not None, \\\n            \'Please initialise the reader first, \' \\\n            \'before adding preprocessors.\'\n        if isinstance(layers, Layer):\n            self.preprocessors.append(layers)\n        else:\n            self.preprocessors.extend(layers)\n        self.prepare_preprocessors()\n\n    # pylint: disable=arguments-differ,too-many-branches\n    def layer_op(self, idx=None, shuffle=True):\n        """"""\n        this layer returns dictionaries::\n\n            keys: self.output_fields\n            values: image volume array\n\n        """"""\n        if idx is None:\n            if shuffle:\n                # training, with random list output\n                idx = np.random.randint(len(self.output_list))\n            else:\n                # testing, with sequential output\n                # accessing self.current_id, not suitable for multi-thread\n                idx = self.current_id + 1\n                self.current_id = idx\n\n        try:\n            image_dict = self.output_list[idx]\n        except (IndexError, TypeError):\n            return -1, None, None\n\n        image_data_dict = \\\n            {field: image.get_data() for (field, image) in image_dict.items()}\n        interp_order_dict = \\\n            {field: image.interp_order for (\n                field, image) in image_dict.items()}\n\n        preprocessors = [deepcopy(layer) for layer in self.preprocessors]\n        # dictionary of masks is cached\n        mask = None\n        for layer in preprocessors:\n            # import time; local_time = time.time()\n            if layer is None:\n                continue\n            if isinstance(layer, RandomisedLayer):\n                if ""random_elastic_deformation"" not in layer.name:\n                    layer.randomise()\n                else:\n                    layer.randomise(image_data_dict)\n\n                image_data_dict = layer(image_data_dict, interp_order_dict)\n            elif isinstance(layer, Layer):\n                image_data_dict, mask = layer(image_data_dict, mask)\n                # print(\'%s, %.3f sec\'%(layer, -local_time + time.time()))\n        return idx, image_data_dict, interp_order_dict\n\n    @property\n    def spatial_ranks(self):\n        """"""\n        Number of spatial dimensions of the images.\n\n        :return: integers of spatial rank\n        """"""\n        if not self.output_list:\n            tf.logging.fatal(""Please initialise the reader first."")\n            raise RuntimeError\n        if not self._spatial_ranks:\n            first_image = self.output_list[0]\n            self._spatial_ranks = {field: first_image[field].spatial_rank\n                                   for field in self.names}\n        return self._spatial_ranks\n\n    @property\n    def shapes(self):\n        """"""\n        Image shapes before any preprocessing.\n\n        :return: tuple of integers as image shape\n\n\n        .. caution::\n\n            To have fast access, the spatial dimensions are not accurate\n\n                1. only read from the first image in list\n                2. not considering effects of random augmentation layers\n                    but time and modality dimensions should be correct\n        """"""\n        if not self.output_list:\n            tf.logging.fatal(""Please initialise the reader first."")\n            raise RuntimeError\n        if not self._shapes:\n            first_image = self.output_list[0]\n            self._shapes = {field: first_image[field].shape\n                            for field in self.names}\n        return self._shapes\n\n    @property\n    def tf_dtypes(self):\n        """"""\n        Infer input data dtypes in TF\n        (using the first image in the file list).\n        """"""\n        if not self.output_list:\n            tf.logging.fatal(""Please initialise the reader first."")\n            raise RuntimeError\n        if not self._dtypes:\n            first_image = self.output_list[0]\n            self._dtypes = {field: infer_tf_dtypes(first_image[field])\n                            for field in self.names}\n        return self._dtypes\n\n    @property\n    def input_sources(self):\n        """"""\n        returns mapping of input keywords and input sections\n        e.g., input_sources::\n\n            {\'image\': (\'T1\', \'T2\'),\n             \'label\': (\'manual_map\',)}\n\n        map task parameter keywords ``image`` and ``label`` to\n        section names ``T1``, ``T2``, and ``manual_map`` respectively.\n        """"""\n        if not self._input_sources:\n            tf.logging.fatal(""Please initialise the reader first."")\n            raise RuntimeError\n        return self._input_sources\n\n    @property\n    def names(self):\n        """"""\n\n        :return: the keys of ``self.input_sources`` dictionary\n        """"""\n        return self._names\n\n    @names.setter\n    def names(self, fields_tuple):\n        """"""\n        output_fields is a sequence of output names\n        each name might correspond to a list of multiple input sources\n        this should be specified in CUSTOM section in the config\n        """"""\n        self._names = make_input_tuple(fields_tuple, string_types)\n\n    @property\n    def num_subjects(self):\n        """"""\n\n        :return: number of subjects in the reader\n        """"""\n        if not self.output_list:\n            return 0\n        return len(self.output_list)\n\n    def get_subject_id(self, image_index):\n        """"""\n        Given an integer id returns the subject id.\n        """"""\n        try:\n            return self._file_list.iloc[image_index][COLUMN_UNIQ_ID]\n        except KeyError:\n            tf.logging.warning(\'Unknown subject id in reader file list.\')\n            raise\n\n    def get_image_index(self, subject_id):\n        """"""\n        Given a subject id, return the file_list index\n        :param subject_id: a string with the subject id\n        :return: an int with the file list index\n        """"""\n        return np.flatnonzero(self._file_list[\'subject_id\'] == subject_id)[0]\n\n    def get_subject(self, image_index=None):\n        """"""\n        Given an integer id returns the corresponding row of the file list.\n        returns: a dictionary of the row\n        """"""\n        try:\n            if image_index is None:\n                return self._file_list.iloc[:].to_dict()\n            return self._file_list.iloc[image_index].to_dict()\n        except (KeyError, AttributeError):\n            tf.logging.warning(\'Unknown subject id in reader file list.\')\n            raise\n\n\ndef _filename_to_image_list(file_list, mod_dict, data_param):\n    """"""\n    Converting a list of filenames to a list of image objects,\n    Properties (e.g. interp_order) are added to each object\n    """"""\n    volume_list = []\n    valid_idx = []\n    for idx in range(len(file_list)):\n        # create image instance for each subject\n        print_progress_bar(idx, len(file_list),\n                           prefix=\'reading datasets headers\',\n                           decimals=1, length=10, fill=\'*\')\n\n        # combine fieldnames and volumes as a dictionary\n        _dict = {}\n        for field, modalities in mod_dict.items():\n            _dict[field] = _create_image(\n                file_list, idx, modalities, data_param)\n\n        # skipping the subject if there\'re missing image components\n        if _dict and None not in list(_dict.values()):\n            volume_list.append(_dict)\n            valid_idx.append(idx)\n\n    if not volume_list:\n        tf.logging.fatal(\n            ""Empty filename lists, please check the csv ""\n            ""files. (removing csv_file keyword if it is""\n            "" in the config file ""\n            ""to automatically search folders and generate new csv ""\n            ""files again)\\n\\n""\n            ""Please note in the matched file names, each subject id are ""\n            ""created by removing all keywords listed `filename_contains` ""\n            ""in the config.\\n\\n""\n            ""E.g., `filename_contains=foo, bar` will match file ""\n            ""foo_subject42_bar.nii.gz, and the subject id is _subject42_."")\n        raise IOError\n    return volume_list, file_list.iloc[valid_idx]\n\n\ndef _create_image(file_list, idx, modalities, data_param):\n    """"""\n    data_param consists of description of each modality\n    This function combines modalities according to the \'modalities\'\n    parameter and create <niftynet.io.input_type.SpatialImage*D>\n    """"""\n    try:\n        file_path = tuple(file_list.iloc[idx][mod] for mod in modalities)\n        any_missing = any([pandas.isnull(file_name) or not bool(file_name)\n                           for file_name in file_path])\n        if any_missing:\n            # todo: enable missing modalities again\n            # the file_path of a multimodal image will contain `nan`, e.g.\n            # this should be handled by `ImageFactory.create_instance`\n            # (\'testT1.nii.gz\', \'testT2.nii.gz\', nan, \'testFlair.nii.gz\')\n            return None\n\n        interp_order, pixdim, axcodes, loader = [], [], [], []\n        for mod in modalities:\n            mod_spec = data_param[mod] \\\n                if isinstance(data_param[mod], dict) else vars(data_param[mod])\n            interp_order.append(mod_spec.get(\'interp_order\',\n                                             DEFAULT_INTERP_ORDER))\n            pixdim.append(mod_spec.get(\'pixdim\', None))\n            axcodes.append(mod_spec.get(\'axcodes\', None))\n            loader.append(mod_spec.get(\'loader\', None))\n\n    except KeyError:\n        tf.logging.fatal(\n            ""Specified modality names %s ""\n            ""not found in config: input sections %s."",\n            modalities, list(data_param))\n        raise\n    except AttributeError:\n        tf.logging.fatal(\n            ""Data params must contain: interp_order, pixdim, axcodes.\\n""\n            ""Reader must be initialised with a dataframe as file_list."")\n        raise\n\n    image_properties = {\'file_path\': file_path,\n                        \'name\': modalities,\n                        \'interp_order\': interp_order,\n                        \'output_pixdim\': pixdim,\n                        \'output_axcodes\': axcodes,\n                        \'loader\': loader}\n    return ImageFactory.create_instance(**image_properties)\n\n\ndef param_to_dict(input_data_param):\n    """"""\n    Validate the user input ``input_data_param``\n    raise an error if it\'s invalid.\n\n    :param input_data_param:\n    :return: input data specifications as a nested dictionary\n    """"""\n    error_msg = \'Unknown ``data_param`` type. \' \\\n                \'It should be a nested dictionary: \'\\\n                \'{""modality_name"": {""input_property"": value}} \'\\\n                \'or a dictionary of: {""modality_name"": \'\\\n                \'niftynet.utilities.util_common.ParserNamespace}\'\n    data_param = deepcopy(input_data_param)\n    if isinstance(data_param, (ParserNamespace, argparse.Namespace)):\n        data_param = vars(data_param)\n    if not isinstance(data_param, dict):\n        raise ValueError(error_msg)\n    for mod in data_param:\n        mod_param = data_param[mod]\n        if isinstance(mod_param, (ParserNamespace, argparse.Namespace)):\n            dict_param = vars(mod_param)\n        elif isinstance(mod_param, dict):\n            dict_param = mod_param\n        else:\n            raise ValueError(error_msg)\n        for data_key in dict_param:\n            look_up_operations(data_key, SUPPORTED_DATA_SPEC)\n        data_param[mod] = dict_param\n    return data_param\n'"
niftynet/io/image_sets_partitioner.py,29,"b'# -*- coding: utf-8 -*-\n""""""\nThis module manages a table of subject ids and\ntheir associated image file names.\nA subset of the table can be retrieved by partitioning the set of images into\nsubsets of ``Train``, ``Validation``, ``Inference``.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport shutil\n\nimport pandas\nimport tensorflow as tf  # to use the system level logging\n\nfrom niftynet.engine.signal import TRAIN, VALID, INFER, ALL\nfrom niftynet.utilities.decorators import singleton\nfrom niftynet.utilities.filename_matching import KeywordsMatching\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\nfrom niftynet.utilities.util_common import look_up_operations\nfrom niftynet.utilities.util_csv import match_and_write_filenames_to_csv\nfrom niftynet.utilities.util_csv import write_csv\n\nCOLUMN_UNIQ_ID = \'subject_id\'\nCOLUMN_PHASE = \'phase\'\nSUPPORTED_PHASES = {TRAIN, VALID, INFER, ALL}\n\n\n@singleton\nclass ImageSetsPartitioner(object):\n    """"""\n    This class maintains a pandas.dataframe of filenames for all input sections.\n    The list of filenames are obtained by searching the specified folders\n    or loading from an existing csv file.\n\n    Users can query a subset of the dataframe by train/valid/infer partition\n    label and input section names.\n    """"""\n\n    # dataframe (table) of file names in a shape of subject x modality\n    _file_list = None\n    # dataframes of subject_id:phase_id\n    _partition_ids = None\n\n    data_param = None\n    ratios = None\n    new_partition = False\n\n    # for saving the splitting index\n    data_split_file = """"\n    # default parent folder location for searching the image files\n    default_image_file_location = \\\n        NiftyNetGlobalConfig().get_niftynet_home_folder()\n\n    def initialise(self,\n                   data_param,\n                   new_partition=False,\n                   data_split_file=None,\n                   ratios=None):\n        """"""\n        Set the data partitioner parameters\n\n        :param data_param: corresponding to all config sections\n        :param new_partition: bool value indicating whether to generate new\n            partition ids and overwrite csv file\n            (this class will write partition file iff new_partition)\n        :param data_split_file: location of the partition id file\n        :param ratios: a tuple/list with two elements:\n            ``(fraction of the validation set, fraction of the inference set)``\n            initialise to None will disable data partitioning\n            and get_file_list always returns all subjects.\n        """"""\n        self.data_param = data_param\n        if data_split_file is None:\n            self.data_split_file = os.path.join(\'.\', \'dataset_split.csv\')\n        else:\n            self.data_split_file = data_split_file\n        self.ratios = ratios\n\n        self._file_list = None\n        self._partition_ids = None\n\n        self.load_data_sections_by_subject()\n        self.new_partition = new_partition\n        self.randomly_split_dataset(overwrite=new_partition)\n        tf.logging.info(self)\n        return self\n\n    def number_of_subjects(self, phase=ALL):\n        """"""\n        query number of images according to phase.\n\n        :param phase:\n        :return:\n        """"""\n        if self._file_list is None:\n            return 0\n        try:\n            phase = look_up_operations(phase.lower(), SUPPORTED_PHASES)\n        except (ValueError, AttributeError):\n            tf.logging.fatal(\'Unknown phase argument.\')\n            raise\n\n        if phase == ALL:\n            return self._file_list[COLUMN_UNIQ_ID].count()\n        if self._partition_ids is None:\n            return 0\n        selector = self._partition_ids[COLUMN_PHASE] == phase\n        selected = self._partition_ids[selector][[COLUMN_UNIQ_ID]]\n        subset = pandas.merge(\n            self._file_list, selected, on=COLUMN_UNIQ_ID, sort=True)\n        return subset.count()[COLUMN_UNIQ_ID]\n\n    def get_file_list(self, phase=ALL, *section_names):\n        """"""\n        get file names as a dataframe, by partitioning phase and section names\n        set phase to ALL to load all subsets.\n\n        :param phase: the label of the subset generated by self._partition_ids\n                    should be one of the SUPPORTED_PHASES\n        :param section_names: one or multiple input section names\n        :return: a pandas.dataframe of file names\n        """"""\n        if self._file_list is None:\n            tf.logging.warning(\'Empty file list, please initialise\'\n                               \'ImageSetsPartitioner first.\')\n            return []\n        try:\n            phase = look_up_operations(phase.lower(), SUPPORTED_PHASES)\n        except (ValueError, AttributeError):\n            tf.logging.fatal(\'Unknown phase argument.\')\n            raise\n\n        for name in section_names:\n            try:\n                look_up_operations(name, set(self._file_list))\n            except ValueError:\n                tf.logging.fatal(\n                    \'Requesting files under input section [%s],\\n\'\n                    \'however the section does not exist in the config.\', name)\n                raise\n        if phase == ALL:\n            self._file_list = self._file_list.sort_values(COLUMN_UNIQ_ID)\n            if section_names:\n                section_names = [COLUMN_UNIQ_ID] + list(section_names)\n                return self._file_list[section_names]\n            return self._file_list\n        if self._partition_ids is None or self._partition_ids.empty:\n            tf.logging.fatal(\'No partition ids available.\')\n            if self.new_partition:\n                tf.logging.fatal(\'Unable to create new partitions,\'\n                                 \'splitting ratios: %s, writing file %s\',\n                                 self.ratios, self.data_split_file)\n            elif os.path.isfile(self.data_split_file):\n                tf.logging.fatal(\n                    \'Unable to load %s, initialise the\'\n                    \'ImageSetsPartitioner with `new_partition=True`\'\n                    \'to overwrite the file.\',\n                    self.data_split_file)\n            raise ValueError\n\n        selector = self._partition_ids[COLUMN_PHASE] == phase\n        selected = self._partition_ids[selector][[COLUMN_UNIQ_ID]]\n        if selected.empty:\n            tf.logging.warning(\n                \'Empty subset for phase [%s], returning None as file list. \'\n                \'Please adjust splitting fractions.\', phase)\n            return None\n        subset = pandas.merge(\n            self._file_list, selected, on=COLUMN_UNIQ_ID, sort=True)\n        if subset.empty:\n            tf.logging.warning(\n                \'No subject id matched in between file names and \'\n                \'partition files.\\nPlease check the partition files %s,\\nor \'\n                \'removing it to generate a new file automatically.\',\n                self.data_split_file)\n        if section_names:\n            section_names = [COLUMN_UNIQ_ID] + list(section_names)\n            return subset[section_names]\n        return subset\n\n    def load_data_sections_by_subject(self):\n        """"""\n        Go through all input data sections, converting each section\n        to a list of file names.\n\n        These lists are merged on ``COLUMN_UNIQ_ID``.\n\n        This function sets ``self._file_list``.\n        """"""\n        if not self.data_param:\n            tf.logging.fatal(\n                \'Nothing to load, please check input sections in the config.\')\n            raise ValueError\n        self._file_list = None\n        for section_name in self.data_param:\n\n            if isinstance(self.data_param[section_name], dict):\n                mod_spec = self.data_param[section_name]\n            else:\n                mod_spec = vars(self.data_param[section_name])\n            if mod_spec.get(\'csv_data_file\', None):  # has csv_data_file\n                # skip file search\n                continue\n\n            modality_file_list = self.grep_files_by_data_section(section_name)\n            if self._file_list is None:\n                # adding all rows of the first modality\n                self._file_list = modality_file_list\n                continue\n            n_rows = self._file_list[COLUMN_UNIQ_ID].count()\n            self._file_list = pandas.merge(self._file_list,\n                                           modality_file_list,\n                                           how=\'outer\',\n                                           on=COLUMN_UNIQ_ID)\n            if self._file_list[COLUMN_UNIQ_ID].count() < n_rows:\n                tf.logging.warning(\'rows not matched in section [%s]\',\n                                   section_name)\n\n        if self._file_list is None or self._file_list.size == 0:\n            tf.logging.fatal(\n                ""Empty filename lists, please check the csv ""\n                ""files (removing csv_file keyword if it is in the config file ""\n                ""to automatically search folders and generate new csv ""\n                ""files again).\\n\\n""\n                ""Please note in the matched file names, each subject id are ""\n                ""created by removing all keywords listed `filename_contains` ""\n                ""in the config.\\n""\n                ""E.g., `filename_contains=foo, bar` will match file ""\n                ""foo_subject42_bar.nii.gz, and the subject id is ""\n                ""_subject42_.\\n\\n"")\n            raise IOError\n\n    def grep_files_by_data_section(self, modality_name):\n        """"""\n        list all files by a given input data section::\n            if the ``csv_file`` property of ``data_param[modality_name]``\n            corresponds to a file, read the list from the file;\n            otherwise\n                write the list to ``csv_file``.\n\n        :return: a table with two columns,\n                 the column names are ``(COLUMN_UNIQ_ID, modality_name)``.\n        """"""\n        if modality_name not in self.data_param:\n            tf.logging.fatal(\'unknown section name [%s], \'\n                             \'current input section names: %s.\',\n                             modality_name, list(self.data_param))\n            raise ValueError\n\n        # input data section must have a ``csv_file`` section for loading\n        # or writing filename lists\n        if isinstance(self.data_param[modality_name], dict):\n            mod_spec = self.data_param[modality_name]\n        else:\n            mod_spec = vars(self.data_param[modality_name])\n\n        #########################\n        # guess the csv_file path\n        #########################\n        temp_csv_file = None\n        try:\n            csv_file = os.path.expanduser(mod_spec.get(\'csv_file\', None))\n            if not os.path.isfile(csv_file):\n                # writing to the same folder as data_split_file\n                default_csv_file = os.path.join(\n                    os.path.dirname(self.data_split_file),\n                    \'{}.csv\'.format(modality_name))\n                tf.logging.info(\'`csv_file = %s` not found, \'\n                                \'writing to ""%s"" instead.\',\n                                csv_file, default_csv_file)\n                csv_file = default_csv_file\n                if os.path.isfile(csv_file):\n                    tf.logging.info(\'Overwriting existing: ""%s"".\', csv_file)\n            csv_file = os.path.abspath(csv_file)\n        except (AttributeError, KeyError, TypeError):\n            tf.logging.debug(\'`csv_file` not specified, writing the list of \'\n                             \'filenames to a temporary file.\')\n            import tempfile\n            temp_csv_file = os.path.join(\n                tempfile.mkdtemp(), \'{}.csv\'.format(modality_name))\n            csv_file = temp_csv_file\n\n        #############################################\n        # writing csv file if path_to_search specified\n        ##############################################\n        if mod_spec.get(\'path_to_search\', None):\n            if not temp_csv_file:\n                tf.logging.info(\n                    \'[%s] search file folders, writing csv file %s\',\n                    modality_name, csv_file)\n            # grep files by section properties and write csv\n            try:\n                matcher = KeywordsMatching.from_dict(\n                    input_dict=mod_spec,\n                    default_folder=self.default_image_file_location)\n                match_and_write_filenames_to_csv([matcher], csv_file)\n            except (IOError, ValueError) as reading_error:\n                tf.logging.warning(\'Ignoring input section: [%s], \'\n                                   \'due to the following error:\',\n                                   modality_name)\n                tf.logging.warning(repr(reading_error))\n                return pandas.DataFrame(\n                    columns=[COLUMN_UNIQ_ID, modality_name])\n        else:\n            tf.logging.info(\n                \'[%s] using existing csv file %s, skipped filenames search\',\n                modality_name, csv_file)\n\n        if not os.path.isfile(csv_file):\n            tf.logging.fatal(\n                \'[%s] csv file %s not found.\', modality_name, csv_file)\n            raise IOError\n        ###############################\n        # loading the file as dataframe\n        ###############################\n        try:\n            csv_list = pandas.read_csv(\n                csv_file,\n                header=None,\n                dtype=(str, str),\n                names=[COLUMN_UNIQ_ID, modality_name],\n                skipinitialspace=True)\n        except Exception as csv_error:\n            tf.logging.fatal(repr(csv_error))\n            raise\n\n        if temp_csv_file:\n            shutil.rmtree(os.path.dirname(temp_csv_file), ignore_errors=True)\n\n        return csv_list\n\n    # pylint: disable=broad-except\n    def randomly_split_dataset(self, overwrite=False):\n        """"""\n        Label each subject as one of the ``TRAIN``, ``VALID``, ``INFER``,\n        use ``self.ratios`` to compute the size of each set.\n\n        The results will be written to ``self.data_split_file`` if overwrite\n        otherwise it tries to read partition labels from it.\n\n        This function sets ``self._partition_ids``.\n        """"""\n        if overwrite:\n            try:\n                valid_fraction, infer_fraction = self.ratios\n                valid_fraction = max(min(1.0, float(valid_fraction)), 0.0)\n                infer_fraction = max(min(1.0, float(infer_fraction)), 0.0)\n            except (TypeError, ValueError):\n                tf.logging.fatal(\n                    \'Unknown format of faction values %s\', self.ratios)\n                raise\n\n            if (valid_fraction + infer_fraction) <= 0:\n                tf.logging.warning(\n                    \'To split dataset into training/validation, \'\n                    \'please make sure \'\n                    \'""exclude_fraction_for_validation"" parameter is set to \'\n                    \'a float in between 0 and 1. Current value: %s.\',\n                    valid_fraction)\n                # raise ValueError\n\n            n_total = self.number_of_subjects()\n            n_valid = int(math.ceil(n_total * valid_fraction))\n            n_infer = int(math.ceil(n_total * infer_fraction))\n            n_train = int(n_total - n_infer - n_valid)\n            phases = [TRAIN] * n_train + [VALID] * n_valid + [INFER] * n_infer\n            if len(phases) > n_total:\n                phases = phases[:n_total]\n            random.shuffle(phases)\n            write_csv(self.data_split_file,\n                      zip(self._file_list[COLUMN_UNIQ_ID], phases))\n        elif os.path.isfile(self.data_split_file):\n            tf.logging.warning(\n                \'Loading from existing partitioning file %s, \'\n                \'ignoring partitioning ratios.\', self.data_split_file)\n\n        if os.path.isfile(self.data_split_file):\n            try:\n                self._partition_ids = pandas.read_csv(\n                    self.data_split_file,\n                    header=None,\n                    dtype=(str, str),\n                    names=[COLUMN_UNIQ_ID, COLUMN_PHASE],\n                    skipinitialspace=True)\n                assert not self._partition_ids.empty, \\\n                    ""partition file is empty.""\n            except Exception as csv_error:\n                tf.logging.warning(\n                    ""Unable to load the existing partition file %s, %s"",\n                    self.data_split_file, repr(csv_error))\n                self._partition_ids = None\n\n            try:\n                phase_strings = self._partition_ids[COLUMN_PHASE]\n                phase_strings = phase_strings.astype(str).str.lower()\n                is_valid_phase = phase_strings.isin(SUPPORTED_PHASES)\n                assert is_valid_phase.all(), \\\n                    ""Partition file contains unknown phase id.""\n                self._partition_ids[COLUMN_PHASE] = phase_strings\n            except (TypeError, AssertionError):\n                tf.logging.warning(\n                    \'Please make sure the values of the second column \'\n                    \'of data splitting file %s, in the set of phases: %s.\\n\'\n                    \'Remove %s to generate random data partition file.\',\n                    self.data_split_file,\n                    SUPPORTED_PHASES,\n                    self.data_split_file)\n                raise ValueError\n\n    def __str__(self):\n        return self.to_string()\n\n    def to_string(self):\n        """"""\n        Print summary of the partitioner.\n        """"""\n        n_subjects = self.number_of_subjects()\n        summary_str = \'\\n\\nNumber of subjects {}, \'.format(n_subjects)\n        if self._file_list is not None:\n            summary_str += \'input section names: {}\\n\'.format(\n                list(self._file_list))\n        if self._partition_ids is not None and n_subjects > 0:\n            n_train = self.number_of_subjects(TRAIN)\n            n_valid = self.number_of_subjects(VALID)\n            n_infer = self.number_of_subjects(INFER)\n            summary_str += \\\n                \'Dataset partitioning:\\n\' \\\n                \'-- {} {} cases ({:.2f}%),\\n\' \\\n                \'-- {} {} cases ({:.2f}%),\\n\' \\\n                \'-- {} {} cases ({:.2f}%).\\n\'.format(\n                    TRAIN, n_train, float(n_train) / float(n_subjects) * 100.0,\n                    VALID, n_valid, float(n_valid) / float(n_subjects) * 100.0,\n                    INFER, n_infer, float(n_infer) / float(n_subjects) * 100.0)\n        else:\n            summary_str += \'-- using all subjects \' \\\n                           \'(without data partitioning).\\n\'\n        return summary_str\n\n    def has_phase(self, phase):\n        """"""\n\n        :return: True if the `phase` subset of images is not empty.\n        """"""\n        if self._partition_ids is None or self._partition_ids.empty:\n            return False\n        selector = self._partition_ids[COLUMN_PHASE] == phase\n        if not selector.any():\n            return False\n        selected = self._partition_ids[selector][[COLUMN_UNIQ_ID]]\n        subset = pandas.merge(\n            left=self._file_list, right=selected,\n            on=COLUMN_UNIQ_ID, sort=False)\n        return not subset.empty\n\n    @property\n    def has_training(self):\n        """"""\n\n        :return: True if the TRAIN subset of images is not empty.\n        """"""\n        return self.has_phase(TRAIN)\n\n    @property\n    def has_inference(self):\n        """"""\n\n        :return: True if the INFER subset of images is not empty.\n        """"""\n        return self.has_phase(INFER)\n\n    @property\n    def has_validation(self):\n        """"""\n\n        :return: True if the VALID subset of images is not empty.\n        """"""\n        return self.has_phase(VALID)\n\n    @property\n    def validation_files(self):\n        """"""\n\n        :return: the list of validation filenames.\n        """"""\n        if self.has_validation:\n            return self.get_file_list(VALID)\n        return self.all_files\n\n    @property\n    def train_files(self):\n        """"""\n\n        :return: the list of training filenames.\n        """"""\n        if self.has_training:\n            return self.get_file_list(TRAIN)\n        return self.all_files\n\n    @property\n    def inference_files(self):\n        """"""\n\n        :return: the list of inference filenames\n            (defaulting to list of all filenames if no partition definition)\n        """"""\n        if self.has_inference:\n            return self.get_file_list(INFER)\n        return self.all_files\n\n    @property\n    def all_files(self):\n        """"""\n\n        :return: list of all filenames\n        """"""\n        return self.get_file_list()\n\n    def get_file_lists_by(self, phase=None, action=\'train\'):\n        """"""\n        Get file lists by action and phase.\n\n        This function returns file lists for training/validation/inference\n        based on the phase or action specified by the user.\n\n        ``phase`` has a higher priority:\n        If `phase` specified, the function returns the corresponding\n        file list (as a list).\n\n        otherwise, the function checks ``action``:\n        it returns train and validation file lists if it\'s training action,\n        otherwise returns inference file list.\n\n        :param action: an action\n        :param phase: an element from ``{TRAIN, VALID, INFER, ALL}``\n        :return:\n        """"""\n        if phase:\n            try:\n                return [self.get_file_list(phase=phase)]\n            except (ValueError, AttributeError):\n                tf.logging.warning(\'phase `parameter` %s ignored\', phase)\n\n        if action and TRAIN.startswith(action):\n            file_lists = [self.train_files]\n            if self.has_validation:\n                file_lists.append(self.validation_files)\n            return file_lists\n\n        return [self.inference_files]\n\n    def reset(self):\n        """"""\n        reset all fields of this singleton class.\n        """"""\n        self._file_list = None\n        self._partition_ids = None\n        self.data_param = None\n        self.ratios = None\n        self.new_partition = False\n\n        self.data_split_file = """"\n        self.default_image_file_location = \\\n            NiftyNetGlobalConfig().get_niftynet_home_folder()\n'"
niftynet/io/image_type.py,17,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines images used by image reader, image properties\nare set by user or read from image header.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom abc import ABCMeta, abstractmethod\n\nimport nibabel as nib\nimport numpy as np\nimport tensorflow as tf\nfrom six import with_metaclass, string_types\n\nimport niftynet.io.misc_io as misc\nfrom niftynet.io.image_loader import load_image_obj\nfrom niftynet.io.misc_io import resolve_file_name, dtype_casting\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\n\n\nclass Loadable(with_metaclass(ABCMeta, object)):\n    """"""\n    interface of loadable data\n    """"""\n\n    @abstractmethod\n    def get_data(self):\n        """"""\n        loads a numpy array from the image object\n        if the array has less than 5 dimensions\n        it extends the array to 5d\n        (corresponding to 3 spatial dimensions, temporal dim, modalities)\n        ndims > 5 not currently supported\n        """"""\n        raise NotImplementedError\n\n\nclass DataFromFile(Loadable):\n    """"""\n    Data from file should have a valid file path\n    (are files on hard drive) and a name.\n    """"""\n\n    def __init__(self, file_path, name=(\'loadable_data\',), loader=None):\n        self._name = None\n        self._file_path = None\n        self._dtype = None\n        self._loader = None\n\n        # assigning using property setters\n        self.file_path = file_path\n        self.name = name\n        self.loader = loader\n\n    @property\n    def dtype(self):\n        """"""\n        data type property of the input images.\n\n        :return: a tuple of input image data types\n            ``len(self.dtype) == len(self.file_path)``\n        """"""\n        if not self._dtype:\n            try:\n                self._dtype = tuple(\n                    load_image_obj(_file, _loader).header.get_data_dtype()\n                    for _file, _loader in zip(self.file_path, self.loader))\n            except (IOError, TypeError, AttributeError):\n                tf.logging.warning(\'could not decide image data type\')\n                self._dtype = (np.dtype(np.float32),) * len(self.file_path)\n        return self._dtype\n\n    @property\n    def file_path(self):\n        """"""\n        A tuple of valid image filenames, this property always returns\n        a tuple, length of the tuple is one for single image,\n        length of the tuple is larger than one for single image from\n        multiple files.\n\n        :return: a tuple of file paths\n        """"""\n        return self._file_path\n\n    @file_path.setter\n    def file_path(self, path_array):\n        if isinstance(path_array, string_types):\n            path_array = (path_array,)\n        home_folder = NiftyNetGlobalConfig().get_niftynet_home_folder()\n        try:\n            self._file_path = tuple(resolve_file_name(path, (\'.\', home_folder))\n                                    for path in path_array)\n        except (TypeError, AssertionError, AttributeError, IOError):\n            tf.logging.fatal(\n                ""unrecognised file path format, should be a valid filename,""\n                ""or a sequence of filenames %s"", path_array)\n            raise IOError\n\n    @property\n    def loader(self):\n        """"""A tuple of valid image loaders. Always returns a tuple""""""\n        return self._loader\n\n    @loader.setter\n    def loader(self, loader):\n        """"""Makes sure loader is always a tuple of length = #modalities""""""\n        try:\n            if len(self.file_path) == len(loader):\n                self._loader = loader\n                return\n        except TypeError:\n            pass\n        self._loader = (loader,) * len(self.file_path)\n\n    @property\n    def name(self):\n        """"""\n        A tuple of image names, this property always returns\n        a tuple, length of the tuple is one for single image,\n        length of the tuple is larger than one for single image from\n        multiple files.\n\n        :return: a tuple of image name tags\n        """"""\n        return self._name\n\n    @name.setter\n    def name(self, name_array):\n        try:\n            if len(self.file_path) == len(name_array):\n                self._name = name_array\n                return\n        except TypeError:\n            pass\n        self._name = (name_array,)\n\n    def get_data(self):\n        raise NotImplementedError\n\n\nclass SpatialImage2D(DataFromFile):\n    """"""\n    2D images, axcodes specifications are ignored when\n    loading. (Resampling to new pixdims is currently not supported).\n    """"""\n\n    def __init__(self,\n                 file_path,\n                 name,\n                 interp_order,\n                 output_pixdim,\n                 output_axcodes,\n                 loader):\n        DataFromFile.__init__(\n            self, file_path=file_path, name=name, loader=loader)\n        self._original_affine = None\n        self._original_pixdim = None\n        self._original_shape = None\n        self._interp_order = None\n        self._output_pixdim = None\n        self._output_axcodes = None\n\n        # assigning with property setters\n        self.interp_order = interp_order\n        self.output_pixdim = output_pixdim\n        self.output_axcodes = output_axcodes\n\n        self._load_header()\n\n    @property\n    def spatial_rank(self):\n        """"""\n        volume [x, y, 1, m, n] will have a spatial rank 2\n        volume [x, y, z, m, n] will have a spatial rank 3\n           if z > 1\n\n        (resampling/reorientation will not be done when spatial rank is 2).\n\n        """"""\n        return int(np.sum([dim > 1 for dim in self.shape[:3]]))\n\n    @property\n    def original_shape(self):\n        """"""\n        Shape with multi-modal concatenation, before any resampling.\n\n        :return: a tuple of integers as the original image shape\n        """"""\n        return self._original_shape\n\n    @property\n    def shape(self):\n        """"""\n        This function read image shape info from the headers\n        The lengths in the fifth dim of multiple images are summed\n        as a multi-mod representation.\n        The fourth dim corresponding to different time sequences\n        is ignored.\n\n        :return: a tuple of integers as image shape\n        """"""\n        if self._original_shape is None:\n            try:\n                self._original_shape = tuple(\n                    load_image_obj(_file, _loader).header[\'dim\'][1:6]\n                    for _file, _loader in zip(self.file_path, self.loader))\n            except (IOError, KeyError, AttributeError, IndexError):\n                tf.logging.fatal(\n                    \'unknown image shape from header %s\', self.file_path)\n                raise ValueError\n            try:\n                non_modality_shapes = \\\n                    set([tuple(shape[:4].tolist())\n                         for shape in self._original_shape])\n                assert len(non_modality_shapes) == 1\n            except (TypeError, IndexError, AssertionError):\n                tf.logging.fatal(""could not combining multimodal images: ""\n                                 ""shapes not consistent %s -- %s"",\n                                 self.file_path, self._original_shape)\n                raise ValueError\n            n_modalities = \\\n                np.sum([int(shape[4]) for shape in self._original_shape])\n            self._original_shape = non_modality_shapes.pop() + (n_modalities,)\n            self._original_shape = \\\n                tuple([shape_i if shape_i > 0 else 1\n                       for shape_i in self._original_shape])\n        return self._original_shape\n\n    def _load_header(self):\n        """"""\n        read original header for pixdim and affine info\n\n        :return:\n        """"""\n        self._original_pixdim = []\n        self._original_affine = []\n        for file_i, loader_i in zip(self.file_path, self.loader):\n            image_obj = load_image_obj(file_i, loader_i)\n            try:\n                misc.correct_image_if_necessary(image_obj)\n                self._original_pixdim.append(image_obj.header.get_zooms()[:3])\n                self._original_affine.append(image_obj.affine)\n            except (TypeError, IndexError, AttributeError):\n                tf.logging.fatal(\'could not read header from %s\', file_i)\n                raise ValueError\n                # self._original_pixdim = tuple(self._original_pixdim)\n                # self._original_affine = tuple(self._original_affine)\n\n    @property\n    def original_pixdim(self):\n        """"""\n        pixdim info from the image header.\n\n        :return: a tuple of pixdims, with each element as pixdims\n            of an image file\n        """"""\n        try:\n            assert self._original_pixdim[0] is not None\n        except (IndexError, AssertionError):\n            self._load_header()\n        return self._original_pixdim\n\n    @property\n    def original_affine(self):\n        """"""\n        affine info from the image header.\n\n        :return: a tuple of affine, with each element as an affine\n            matrix of an image file\n        """"""\n        try:\n            assert self._original_affine[0] is not None\n        except (IndexError, AssertionError):\n            self._load_header()\n        return self._original_affine\n\n    @property\n    def original_axcodes(self):\n        """"""\n        axcodes info from the image header\n        more info: http://nipy.org/nibabel/image_orientation.html\n\n        :return: a tuple of axcodes, with each element as axcodes\n            of an image file\n        """"""\n        try:\n            return tuple(nib.aff2axcodes(affine)\n                         for affine in self.original_affine)\n        except IndexError:\n            tf.logging.fatal(\'unknown affine in header %s: %s\',\n                             self.file_path, self.original_affine)\n            raise\n\n    @property\n    def interp_order(self):\n        """"""\n        interpolation order specified by user.\n\n        :return: a tuple of integers, with each element as an\n            interpolation order of an image file\n        """"""\n        return self._interp_order\n\n    @interp_order.setter\n    def interp_order(self, interp_order):\n        try:\n            if len(interp_order) == len(self.file_path):\n                self._interp_order = \\\n                    tuple(int(order) for order in interp_order)\n                return\n        except (TypeError, ValueError):\n            pass\n        try:\n            interp_order = int(interp_order)\n            self._interp_order = (int(interp_order),) * len(self.file_path)\n        except (TypeError, ValueError):\n            tf.logging.fatal(\n                ""output interp_order should be an integer or ""\n                ""a sequence of integers that matches len(self.file_path)"")\n            raise ValueError\n\n    @property\n    def dtype(self):\n        """"""\n        data type property of the input images.\n\n        :return: a tuple of input image data types\n            ``len(self.dtype) == len(self.file_path)``\n        """"""\n        if not self._dtype:\n            self._dtype = super(SpatialImage2D, self).dtype\n            self._dtype = tuple(\n                dtype_casting(dtype, interp_order)\n                for dtype, interp_order in zip(self._dtype, self.interp_order))\n        return self._dtype\n\n    @property\n    def output_pixdim(self):\n        """"""\n        output pixdim info specified by user\n        set to None for using the original pixdim in image header\n        otherwise get_data() transforms image array according to this value.\n\n        :return: a tuple of pixdims, with each element as pixdims\n            of an image file\n        """"""\n        tf.logging.warning(""resampling 2D images not implemented"")\n        return (None,) * len(self.file_path)\n\n    @output_pixdim.setter\n    def output_pixdim(self, output_pixdim):\n        try:\n            if len(output_pixdim) == len(self.file_path):\n                self._output_pixdim = []\n                for i, _ in enumerate(self.file_path):\n                    if output_pixdim[i] is None:\n                        self._output_pixdim.append(None)\n                    else:\n                        self._output_pixdim.append(\n                            tuple(float(pixdim) for pixdim in output_pixdim[i]))\n                # self._output_pixdim = tuple(self._output_pixdim)\n                return\n        except (TypeError, ValueError):\n            pass\n        try:\n            if output_pixdim is not None:\n                output_pixdim = \\\n                    tuple(float(pixdim) for pixdim in output_pixdim)\n            self._output_pixdim = (output_pixdim,) * len(self.file_path)\n        except (TypeError, ValueError):\n            tf.logging.fatal(\n                \'could not set output pixdim \'\n                \'%s for %s\', output_pixdim, self.file_path)\n            raise\n\n    @property\n    def output_axcodes(self):\n        """"""\n        output axcodes info specified by user\n        set to None for using the original axcodes in image header,\n        otherwise get_data() change axes of the image array\n        according to this value.\n\n        :return: a tuple of pixdims, with each element as pixdims\n            of an image file\n        """"""\n        tf.logging.warning(""reorienting 2D images not implemented"")\n        return (None,) * len(self.file_path)\n\n    @output_axcodes.setter\n    def output_axcodes(self, output_axcodes):\n        try:\n            if len(output_axcodes) == len(self.file_path):\n                self._output_axcodes = []\n                for i, _ in enumerate(self.file_path):\n                    if output_axcodes[i] is None:\n                        self._output_axcodes.append(None)\n                    else:\n                        self._output_axcodes.append(tuple(output_axcodes[i]))\n                return\n        except (TypeError, ValueError):\n            pass\n        try:\n            if output_axcodes is None:\n                output_axcodes = (None,)\n            else:\n                output_axcodes = (output_axcodes,)\n            self._output_axcodes = output_axcodes * len(self.file_path)\n        except (TypeError, ValueError):\n            tf.logging.fatal(\n                \'could not set output pixdim \'\n                \'%s for %s\', output_axcodes, self.file_path)\n            raise\n\n    @classmethod\n    def _load_single_file(cls, file_path, loader, dtype=np.float32):\n        image_obj = load_image_obj(file_path, loader)\n        image_data = image_obj.get_data()  # new API: get_fdata()\n        image_data = misc.expand_to_5d(image_data)\n        return image_data.astype(dtype)\n\n    def get_data(self):\n        if len(self._file_path) > 1:\n            image_data = []\n            for file_path, loader, dtype in \\\n                    zip(self._file_path, self.loader, self.dtype):\n                data_array = self._load_single_file(file_path, loader, dtype)\n                image_data.append(data_array)\n            try:\n                return np.concatenate(image_data, axis=4)\n            except ValueError:\n                tf.logging.fatal(\n                    ""multi-modal data shapes not consistent -- trying to ""\n                    ""concat {}."".format([mod.shape for mod in image_data]))\n                raise\n        image_data = self._load_single_file(\n            self.file_path[0], self.loader[0], self.dtype[0])\n        return image_data\n\n\nclass SpatialImage3D(SpatialImage2D):\n    """"""\n    3D image from a single, supports resampling and reorientation\n    (3D image from a set of 2D slices is currently not supported).\n    """"""\n\n    def __init__(self,\n                 file_path,\n                 name,\n                 interp_order,\n                 output_pixdim,\n                 output_axcodes,\n                 loader):\n        SpatialImage2D.__init__(self,\n                                file_path=file_path,\n                                name=name,\n                                interp_order=interp_order,\n                                output_pixdim=output_pixdim,\n                                output_axcodes=output_axcodes,\n                                loader=loader)\n        self._load_header()\n\n    # pylint: disable=no-member\n    @SpatialImage2D.output_pixdim.getter\n    def output_pixdim(self):\n        if self._output_pixdim is None:\n            self.output_pixdim = None\n        return tuple(self._output_pixdim)\n\n    # pylint: disable=no-member\n    @SpatialImage2D.output_axcodes.getter\n    def output_axcodes(self):\n        if self._output_axcodes is None:\n            self.output_axcodes = None\n        return tuple(self._output_axcodes)\n\n    @property\n    def shape(self):\n        image_shape = super(SpatialImage3D, self).shape\n        spatial_shape = image_shape[:3]\n        rest_shape = image_shape[3:]\n\n        if int(np.sum([dim > 1 for dim in spatial_shape])) < 3:\n            # skip resampling and reorientation for spatially 2D\n            return image_shape\n        pixdim = tuple(self.original_pixdim[0])\n        if self.original_axcodes[0] and self.output_axcodes[0]:\n            transf, _, _ = misc.compute_orientation(\n                self.output_axcodes[0], self.original_axcodes[0])\n            spatial_shape = tuple(\n                spatial_shape[k] for k in transf[:, 0].astype(np.int))\n            if pixdim:\n                pixdim = tuple(pixdim[k] for k in transf[:, 0].astype(np.int))\n\n        if pixdim and self.output_pixdim[0]:\n            try:\n                zoom_ratio = np.divide(pixdim[:3], self.output_pixdim[0][:3])\n                spatial_shape = tuple(\n                    int(round(ii * jj))\n                    for ii, jj in zip(spatial_shape, zoom_ratio))\n            except (ValueError, IndexError):\n                tf.logging.fatal(\n                    \'unknown pixdim %s: %s\',\n                    self.original_pixdim, self.output_pixdim)\n                raise ValueError\n        return spatial_shape + rest_shape\n\n    def _load_single_file(self, file_path, loader, dtype=np.float32):\n        image_data = SpatialImage2D._load_single_file(file_path, loader, dtype)\n\n        if self.spatial_rank < 3:\n            return image_data\n\n        pixdim = self.original_pixdim[0]\n        if self.original_axcodes[0] and self.output_axcodes[0]:\n            image_data = misc.do_reorientation(\n                image_data, self.original_axcodes[0], self.output_axcodes[0])\n            transf, _, _ = misc.compute_orientation(\n                self.output_axcodes[0], self.original_axcodes[0])\n            if pixdim:\n                pixdim = tuple(pixdim[k] for k in transf[:, 0].astype(np.int))\n\n        if pixdim and self.output_pixdim[0]:\n            # verbose: warning when interpolate_order>1 for integers\n            image_data = misc.do_resampling(image_data,\n                                            pixdim,\n                                            self.output_pixdim[0],\n                                            self.interp_order[0])\n        return image_data\n\n\nclass SpatialImage4D(SpatialImage3D):\n    """"""\n    4D image from a set of 3D volumes,\n    supports resampling and reorientation.\n\n    The 3D volumes are concatenated in the fifth dim (modality dim)\n    """"""\n\n    def __init__(self,\n                 file_path,\n                 name,\n                 interp_order,\n                 output_pixdim,\n                 output_axcodes,\n                 loader):\n        SpatialImage3D.__init__(self,\n                                file_path=file_path,\n                                name=name,\n                                interp_order=interp_order,\n                                output_pixdim=output_pixdim,\n                                output_axcodes=output_axcodes,\n                                loader=loader)\n\n    @property\n    def spatial_rank(self):\n        """"""\n        Inferring spatial rank from array shape.\n\n        In the case of concatenating ``M`` volumes of ``[x, y, 1]``\n        the outcome ``[x, y, 1, 1, M]`` will have a spatial rank 2\n        (resampling/reorientation will not be done in this case).\n\n        :return: an integer\n        """"""\n        return int(np.sum([dim > 1 for dim in self.shape[:3]]))\n\n    def get_data(self):\n        if len(self.file_path) == 1:\n            # 4D image from a single file ()\n            return SpatialImage3D._load_single_file(\n                self, self.file_path[0], self.loader[0])\n        # assuming len(self._file_path) > 1\n        mod_list = []\n        for mod in range(len(self.file_path)):\n            mod_3d = SpatialImage3D(file_path=(self.file_path[mod],),\n                                    name=(self.name[mod],),\n                                    interp_order=(self.interp_order[mod],),\n                                    output_pixdim=(self.output_pixdim[mod],),\n                                    output_axcodes=(self.output_axcodes[mod],),\n                                    loader=(self.loader[mod],))\n            mod_data_5d = mod_3d.get_data()\n            mod_list.append(mod_data_5d)\n        try:\n            image_data = np.concatenate(mod_list, axis=4)\n        except ValueError:\n            tf.logging.fatal(\n                ""multi-modal data shapes not consistent -- trying to ""\n                ""concatenate {}."".format([mod.shape for mod in mod_list]))\n            raise\n        return image_data\n\n\nclass SpatialImage5D(SpatialImage4D):\n    """"""\n    5D image from a single file,\n    resampling and reorientation are implemented as\n    operations on each 3D slice individually.\n\n    (5D image from a set of 4D files is currently not supported)\n    """"""\n\n    def __init__(self,\n                 file_path,\n                 name,\n                 interp_order,\n                 output_pixdim,\n                 output_axcodes,\n                 loader):\n        SpatialImage4D.__init__(self,\n                                file_path=file_path,\n                                name=name,\n                                interp_order=interp_order,\n                                output_pixdim=output_pixdim,\n                                output_axcodes=output_axcodes,\n                                loader=loader)\n\n\nclass ImageFactory(object):\n    """"""\n    Create image instance according to number of dimensions\n    specified in image headers.\n    """"""\n    INSTANCE_DICT = {\n        2: SpatialImage2D,\n        3: SpatialImage3D,\n        4: SpatialImage4D,\n        5: SpatialImage5D,\n        6: SpatialImage5D}\n\n    @classmethod\n    def create_instance(cls, file_path, **kwargs):\n        """"""\n        Read image headers and create image instance.\n\n        :param file_path: a file path or a sequence of file paths\n        :param kwargs: output properties for transforming the image data\n            array into a desired format\n        :return: an image instance\n        """"""\n        if file_path is None:\n            tf.logging.fatal(\'No file_path provided, \'\n                             \'please check input sources in config file\')\n            raise ValueError\n\n        ndims = 0\n        image_type = None\n        home_folder = NiftyNetGlobalConfig().get_niftynet_home_folder()\n        try:\n            file_path = resolve_file_name(file_path, (\'.\', home_folder))\n            if os.path.isfile(file_path):\n                loader = kwargs.get(\'loader\', None) or None\n                ndims = misc.infer_ndims_from_file(file_path, loader)\n                image_type = cls.INSTANCE_DICT.get(ndims, None)\n        except (TypeError, IOError, AttributeError):\n            pass\n\n        if image_type is None:\n            try:\n                file_path = [\n                    resolve_file_name(path, (\'.\', home_folder))\n                    for path in file_path]\n                loader = kwargs.get(\'loader\', None) or (None,)\n                ndims = misc.infer_ndims_from_file(file_path[0], loader[0])\n                ndims = ndims + (1 if len(file_path) > 1 else 0)\n                image_type = cls.INSTANCE_DICT.get(ndims, None)\n            except (AssertionError, TypeError, IOError, AttributeError):\n                tf.logging.fatal(\'Could not load file: %s\', file_path)\n                raise IOError\n        if image_type is None:\n            tf.logging.fatal(\'Not supported image type from:\\n%s\', file_path)\n            raise NotImplementedError(\n                ""unrecognised spatial rank {}"".format(ndims))\n        return image_type(file_path, **kwargs)\n'"
niftynet/io/misc_io.py,35,"b'# -*- coding: utf-8 -*-\n""""""Utilities functions for file and path management""""""\nfrom __future__ import absolute_import, print_function, unicode_literals\n\nimport errno\nimport importlib\nimport logging as log\nimport os\nimport re\nimport sys\nimport warnings\n\nimport nibabel as nib\nimport numpy as np\nimport pandas as pd\nimport scipy.ndimage\nimport tensorflow as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.core.framework import summary_pb2\n\nfrom niftynet.io.image_loader import load_image_obj\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\nfrom niftynet.utilities.util_import import require_module\n\nIS_PYTHON2 = sys.version_info[0] == 2\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\nFILE_EXTENSIONS = ["".nii.gz"", "".tar.gz""]\nCONSOLE_LOG_FORMAT = ""\\033[1m%(levelname)s:niftynet:\\033[0m %(message)s""\nFILE_LOG_FORMAT = ""%(levelname)s:niftynet:%(asctime)s: %(message)s""\n\n# utilities for file headers #\n\n\ndef infer_ndims_from_file(file_path, loader=None):\n    """"""\n    Get spatial rank of the image file.\n\n    :param file_path:\n    :param loader:\n    :return:\n    """"""\n    image_header = load_image_obj(file_path, loader).header\n    try:\n        return int(image_header[\'dim\'][0])\n    except (TypeError, KeyError, IndexError):\n        pass\n    try:\n        return int(len(image_header.get_data_shape()))\n    except (TypeError, AttributeError):\n        pass\n\n    tf.logging.fatal(\'unsupported file header in: {}\'.format(file_path))\n    raise IOError(\'could not get ndims from file header, please \'\n                  \'consider convert image files to NifTI format.\')\n\n\ndef dtype_casting(original_dtype, interp_order, as_tf=False):\n    """"""\n    Making image dtype based on user specified interp order and\n    best compatibility with Tensorflow.\n\n    (if interp_order > 1, all values are promoted to float32,\n     this avoids errors when the input images have different dtypes)\n\n     The image preprocessing steps such as normalising intensities to [-1, 1]\n     will cast input into floats. We therefore cast\n     almost everything to float32 in the reader. Potentially more\n     complex casting rules are needed here.\n\n    :param original_dtype: an input datatype\n    :param interp_order: an integer of interpolation order\n    :param as_tf: boolean\n    :return: normalised numpy dtype if not `as_tf` else tensorflow dtypes\n    """"""\n\n    dkind = np.dtype(original_dtype).kind\n    if dkind in \'biu\':  # handling integers\n        if interp_order < 0:\n            return np.int32 if not as_tf else tf.int32\n        return np.float32 if not as_tf else tf.float32\n    if dkind == \'f\':  # handling floats\n        return np.float32 if not as_tf else tf.float32\n\n    if as_tf:\n        return tf.float32  # fallback to float32 for tensorflow\n    return original_dtype  # do nothing for numpy array\n\n\ndef create_affine_pixdim(affine, pixdim):\n    """"""\n    Given an existing affine transformation and the pixel dimension to apply,\n    create a new affine matrix that satisfies the new pixel dimension.\n\n    :param affine: original affine matrix\n    :param pixdim: pixel dimensions to apply\n    :return:\n    """"""\n    norm_affine = np.sqrt(np.sum(np.square(affine[:, 0:3]), 0))\n    to_divide = np.tile(\n        np.expand_dims(np.append(norm_affine, 1), axis=1), [1, 4])\n    to_multiply = np.tile(\n        np.expand_dims(np.append(np.asarray(pixdim), 1), axis=1), [1, 4])\n    return np.multiply(np.divide(affine, to_divide.T), to_multiply.T)\n\n\ndef correct_image_if_necessary(img):\n    """"""\n    Check image object header\'s format,\n    update the object if necessary\n\n    :param img:\n    :return:\n    """"""\n    if img.header[\'dim\'][0] == 5:\n        # do nothing for high-dimensional array\n        return img\n    # Check that affine matches zooms\n    pixdim = img.header.get_zooms()\n    if not np.array_equal(\n            np.sqrt(np.sum(np.square(img.affine[0:3, 0:3]), 0)),\n            np.asarray(pixdim)):\n        if hasattr(img, \'get_sform\'):\n            # assume it is a malformed NIfTI and try to fix it\n            img = rectify_header_sform_qform(img)\n    return img\n\n\ndef rectify_header_sform_qform(img_nii):\n    """"""\n    Look at the sform and qform of the nifti object and\n    correct it if any incompatibilities with pixel dimensions\n\n    :param img_nii:\n    :return:\n    """"""\n    pixdim = img_nii.header.get_zooms()\n    sform = img_nii.get_sform()\n    qform = img_nii.get_qform()\n    norm_sform = np.sqrt(np.sum(np.square(sform[0:3, 0:3]), 0))\n    norm_qform = np.sqrt(np.sum(np.square(qform[0:3, 0:3]), 0))\n    flag_sform_problem = False\n    flag_qform_problem = False\n    if not np.array_equal(norm_sform, np.asarray(pixdim)):\n        flag_sform_problem = True\n    if not np.array_equal(norm_qform, np.asarray(pixdim)):\n        flag_qform_problem = True\n\n    if img_nii.header[\'sform_code\'] > 0:\n        if not flag_sform_problem:\n            return img_nii\n        if not flag_qform_problem:\n            # recover by copying the qform over the sform\n            img_nii.set_sform(np.copy(img_nii.get_qform()))\n            return img_nii\n    elif img_nii.header[\'qform_code\'] > 0:\n        if not flag_qform_problem:\n            return img_nii\n        if not flag_sform_problem:\n            # recover by copying the sform over the qform\n            img_nii.set_qform(np.copy(img_nii.get_sform()))\n            return img_nii\n    affine = img_nii.affine\n    pixdim = img_nii.header.get_zooms()\n    while len(pixdim) < 3:\n        pixdim = pixdim + (1.0, )\n    # assuming 3 elements\n    new_affine = create_affine_pixdim(affine, pixdim[:3])\n    img_nii.set_sform(new_affine)\n    img_nii.set_qform(new_affine)\n    return img_nii\n\n\n# end of utilities for file headers #\n\n\ndef compute_orientation(init_axcodes, final_axcodes):\n    """"""\n    A thin wrapper around ``nib.orientations.ornt_transform``\n\n    :param init_axcodes: Initial orientation codes\n    :param final_axcodes: Target orientation codes\n    :return: orientations array, start_ornt, end_ornt\n    """"""\n    ornt_init = nib.orientations.axcodes2ornt(init_axcodes)\n    ornt_fin = nib.orientations.axcodes2ornt(final_axcodes)\n    if np.any(np.isnan(ornt_init)) or np.any(np.isnan(ornt_fin)):\n        tf.logging.fatal(""unknown axcodes %s, %s"", ornt_init, ornt_fin)\n        raise ValueError\n    try:\n        ornt_transf = nib.orientations.ornt_transform(ornt_init, ornt_fin)\n        return ornt_transf, ornt_init, ornt_fin\n    except (ValueError, IndexError):\n        tf.logging.fatal(\'reorientation transform error: %s, %s\', ornt_init,\n                         ornt_fin)\n        raise ValueError\n\n\ndef do_resampling_idx(idx_array, init_pixdim, fin_pixdim):\n    """"""\n    Performs the transformation of indices (for csv sampler) when resampling\n    is required (change of resolution enforced\n    :param idx_array: array of indices to modify\n    :param init_pixdim: initial pixdim\n    :param fin_pixdim: target pixdim\n    :return: new_idx transformed array of indices according to resampling\n    """"""\n    if fin_pixdim is None:\n        new_idx = idx_array\n        return new_idx\n    factor_mult = np.asarray(init_pixdim) / np.asarray(fin_pixdim)\n    new_idx = idx_array.astype(np.float32) * factor_mult\n    new_idx = new_idx.astype(np.int32)\n    return new_idx\n\n\ndef do_reorientation_idx(idx_array, init_axcodes, final_axcodes,\n                         init_spatial_size):\n    """"""\n    Perform the indices change based on the the orientation transformation\n    :param idx_array: array of indices to transform when reorienting\n    :param init_axcodes: initial orientation codes\n    :param final_axcodes: target orientation codes\n    :param init_spatial_size: initial image spatial size\n    :return: new_idx the array of transformed indices and orientation transform\n    """"""\n    new_idx = idx_array\n    if final_axcodes is None:\n        ornt_transf, ornt_init, ornt_fin = compute_orientation(init_axcodes,\n                                                               init_axcodes)\n        return new_idx, ornt_transf\n    ornt_transf, ornt_init, ornt_fin = compute_orientation(init_axcodes,\n                                                           final_axcodes)\n    # print(ornt_transf, ornt_init, ""orientation prior idx transfo"",\n    #       idx_array.shape, init_spatial_size)\n    if np.array_equal(ornt_init, ornt_fin):\n        return new_idx, ornt_transf\n    else:\n        # print(ornt_transf[:, 0])\n        # print(idx_array[:, np.asarray(ornt_transf[:,0],dtype=np.int32)])\n        new_idx = idx_array[:, np.asarray(ornt_transf[:, 0], dtype=np.int32)]\n        # print(""obtained new first"", init_spatial_size)\n        reorder_axes = np.squeeze(np.asarray(ornt_transf[:, 0], dtype=np.int32))\n        fin_spatial_size = [init_spatial_size[k] for k in reorder_axes]\n        for i in range(ornt_transf.shape[0]):\n            if ornt_transf[i, 1] < 0:\n                new_idx[:, i] = fin_spatial_size[i] - np.asarray(new_idx[:, i])\n                print(""Updated idx for %d"" % i)\n        return new_idx, ornt_transf\n\n\ndef do_reorientation(data_array, init_axcodes, final_axcodes):\n    """"""\n    Performs the reorientation (changing order of axes)\n\n    :param data_array: 5D Array to reorient\n    :param init_axcodes: Initial orientation\n    :param final_axcodes: Target orientation\n    :return data_reoriented: New data array in its reoriented form\n    """"""\n    ornt_transf, ornt_init, ornt_fin = \\\n        compute_orientation(init_axcodes, final_axcodes)\n    # print(ornt_transf, init_axcodes, final_axcodes)\n    if np.array_equal(ornt_init, ornt_fin):\n        return data_array\n    try:\n        return nib.orientations.apply_orientation(data_array, ornt_transf)\n    except (ValueError, IndexError):\n        tf.logging.fatal(\'reorientation undecided %s, %s\', ornt_init, ornt_fin)\n        raise ValueError\n\n\ndef do_resampling(data_array, pixdim_init, pixdim_fin, interp_order):\n    """"""\n    Performs the resampling\n    Perform the resampling of the data array given the initial and final pixel\n    dimensions and the interpolation order\n    this function assumes the same interp_order for multi-modal images\n\n    :param data_array: 5D Data array to resample\n    :param pixdim_init: Initial pixel dimension\n    :param pixdim_fin: Targeted pixel dimension\n    :param interp_order: Interpolation order applied\n    :return data_resampled: Array containing the resampled data\n    """"""\n    if data_array is None:\n        return None\n    if np.array_equal(pixdim_fin, pixdim_init):\n        return data_array\n    try:\n        assert len(pixdim_init) <= len(pixdim_fin)\n    except (TypeError, AssertionError):\n        tf.logging.fatal(""unknown pixdim format original %s output %s"",\n                         pixdim_init, pixdim_fin)\n        raise\n    to_multiply = np.divide(pixdim_init, pixdim_fin[:len(pixdim_init)])\n    data_shape = data_array.shape\n    if len(data_shape) != 5:\n        raise ValueError(""only supports 5D array resampling, ""\n                         ""input shape {}"".format(data_shape))\n    data_resampled = []\n    for time_point in range(0, data_shape[3]):\n        data_mod = []\n        for mod in range(0, data_shape[4]):\n            data_new = scipy.ndimage.zoom(\n                data_array[..., time_point, mod],\n                to_multiply[0:3],\n                order=interp_order)\n            data_mod.append(data_new[..., np.newaxis, np.newaxis])\n        data_resampled.append(np.concatenate(data_mod, axis=-1))\n    return np.concatenate(data_resampled, axis=-2)\n\n\ndef save_csv_array(filefolder, filename, array_to_save):\n    """"""\n    Save a np array as a csv\n    :param filefolder: Path to the folder where to save\n    :param filename: Name of the file to save\n    :param array_to_save: Array to save\n    :return:\n    """"""\n    if array_to_save is None:\n        return\n    if not isinstance(array_to_save, pd.DataFrame):\n        array_to_save = pd.DataFrame(array_to_save)\n    touch_folder(filefolder)\n    output_name = os.path.join(filefolder, filename)\n    try:\n        if os.path.isfile(output_name):\n            tf.logging.warning(\'File %s exists, overwriting the file.\',\n                               output_name)\n        array_to_save.to_csv(output_name)\n    except OSError:\n        tf.logging.fatal(""writing failed {}"".format(output_name))\n        raise\n    tf.logging.info(\'Saved {}\'.format(output_name))\n\n\ndef save_data_array(filefolder,\n                    filename,\n                    array_to_save,\n                    image_object=None,\n                    interp_order=3,\n                    reshape=True):\n    """"""\n    write image data array to hard drive using image_object\n    properties such as affine, pixdim and axcodes.\n\n    :param filefolder:\n    :param filename:\n    :param array_to_save:\n    :param image_object:\n    :param interp_order:\n    :param reshape:\n    :return:\n    """"""\n    if image_object is not None:\n        affine = image_object.original_affine[0]\n        image_pixdim = image_object.output_pixdim[0]\n        image_axcodes = image_object.output_axcodes[0]\n        dst_pixdim = image_object.original_pixdim[0]\n        dst_axcodes = image_object.original_axcodes[0]\n        original_shape = image_object.original_shape\n    else:\n        affine = np.eye(4)\n        image_pixdim, dst_pixdim = (), ()\n        image_axcodes, dst_axcodes = (), ()\n        original_shape = ()\n\n    if reshape:\n        input_ndim = array_to_save.ndim\n        if input_ndim == 1:\n            # feature vector, should be saved with shape (1, 1, 1, 1, mod)\n            while array_to_save.ndim < 5:\n                array_to_save = np.expand_dims(array_to_save, axis=0)\n        elif input_ndim in (2, 3):\n            # 2D or 3D images should be saved with shape (x, y, z, 1, 1)\n            while array_to_save.ndim < 5:\n                array_to_save = np.expand_dims(array_to_save, axis=-1)\n        elif input_ndim == 4:\n            # recover a time dimension for nifti format output\n            array_to_save = np.expand_dims(array_to_save, axis=3)\n\n    if image_pixdim and dst_pixdim:\n        if original_shape:\n            # generating image_pixdim from original shape\n            # so that `do_resampling` returns deterministic shape\n            spatial_shape = np.asarray(array_to_save.shape[:3], dtype=np.float)\n            original_shape = np.asarray(original_shape[:3], dtype=np.float)\n            if image_axcodes and dst_axcodes:\n                transf, _, _ = compute_orientation(image_axcodes, dst_axcodes)\n                original_shape = tuple(\n                    original_shape[k] for k in transf[:, 0].astype(np.int))\n            image_pixdim = dst_pixdim * np.divide(original_shape,\n                                                  spatial_shape)\n        array_to_save = do_resampling(array_to_save, image_pixdim, dst_pixdim,\n                                      interp_order)\n\n    if image_axcodes and dst_axcodes:\n        array_to_save = do_reorientation(array_to_save, image_axcodes,\n                                         dst_axcodes)\n    save_volume_5d(array_to_save, filename, filefolder, affine)\n\n\ndef expand_to_5d(img_data):\n    """"""\n    Expands an array up to 5d if it is not the case yet;\n    The first three spatial dims are rearranged so that\n    1-d is always [X, 1, 1]\n    2-d is always [X, y, 1]\n    :param img_data:\n    :return:\n    """"""\n    while img_data.ndim < 5:\n        img_data = np.expand_dims(img_data, axis=-1)\n\n    spatial_dims = img_data.shape[:3]\n    spatial_rank = np.sum([dim > 1 for dim in spatial_dims])\n    if spatial_rank == 1:\n        return np.swapaxes(img_data, 0, np.argmax(spatial_dims))\n    if spatial_rank == 2:\n        return np.swapaxes(img_data, 2, np.argmin(spatial_dims))\n    return img_data\n\n\ndef save_volume_5d(img_data, filename, save_path, affine=np.eye(4)):\n    """"""\n    Save the img_data to nifti image, if the final dimensions of the 5D array\n    are 1\'s, save the lower dimensional image to disk by squeezing the trailing\n    single dimensional spaces away.\n\n    :param img_data: 5d img to save\n    :param filename: filename under which to save the img_data\n    :param save_path:\n    :param affine: an affine matrix.\n    :return:\n    """"""\n    if img_data is None:\n        return\n    # 5D images are not well supported by many image processing tools\n    # (or are assumed to be time series)\n    # Squeeze 5d processing space into smaller image spatial size (3d or 2d)\n    # for improved compatibility with\n    # external visualization/processing tools like Slicer3D, ITK,\n    # SimpleITK, etc ...\n    sqeezed_shape = img_data.shape\n    while sqeezed_shape[-1] == 1:\n        sqeezed_shape = sqeezed_shape[0:-1]\n    img_data.shape = sqeezed_shape\n    touch_folder(save_path)\n    img_nii = nib.Nifti1Image(img_data, affine)\n    # img_nii.set_data_dtype(np.dtype(np.float32))\n    output_name = os.path.join(save_path, filename)\n    try:\n        if os.path.isfile(output_name):\n            tf.logging.warning(\'File %s exists, overwriting the file.\',\n                               output_name)\n        nib.save(img_nii, output_name)\n    except OSError:\n        tf.logging.fatal(""writing failed {}"".format(output_name))\n        raise\n    tf.logging.info(\'Saved {}\'.format(output_name))\n\n\ndef split_filename(file_name):\n    """"""\n    split `file_name` into folder path name, basename, and extension name.\n\n    :param file_name:\n    :return:\n    """"""\n    pth = os.path.dirname(file_name)\n    fname = os.path.basename(file_name)\n\n    ext = None\n    for special_ext in FILE_EXTENSIONS:\n        ext_len = len(special_ext)\n        if fname[-ext_len:].lower() == special_ext:\n            ext = fname[-ext_len:]\n            fname = fname[:-ext_len] if len(fname) > ext_len else \'\'\n            break\n    if not ext:\n        fname, ext = os.path.splitext(fname)\n    return pth, fname, ext\n\n\ndef squeeze_spatial_temporal_dim(tf_tensor):\n    """"""\n    Given a tensorflow tensor, ndims==6 means::\n\n        [batch, x, y, z, time, modality]\n\n    this function removes x, y, z, and time dims if\n    the length along the dims is one.\n\n    :return: squeezed tensor\n    """"""\n    if tf_tensor.shape.ndims != 6:\n        return tf_tensor\n    if tf_tensor.shape[4] != 1:\n        if tf_tensor.shape[5] > 1:\n            raise NotImplementedError(""time sequences not currently supported"")\n        # input shape [batch, x, y, z, t, 1]: swapping \'t\' and 1\n        tf_tensor = tf.transpose(tf_tensor, [0, 1, 2, 3, 5, 4])\n    axis_to_squeeze = []\n    for (idx, axis) in enumerate(tf_tensor.shape.as_list()):\n        if idx in (0, 5):\n            continue\n        if axis == 1:\n            axis_to_squeeze.append(idx)\n    return tf.squeeze(tf_tensor, axis=axis_to_squeeze)\n\n\ndef touch_folder(model_dir):\n    """"""\n    This function returns the absolute path of `model_dir` if exists\n    otherwise try to create the folder and returns the absolute path.\n    """"""\n    model_dir = os.path.expanduser(model_dir)\n    if not os.path.exists(model_dir):\n        try:\n            os.makedirs(model_dir)\n        except (OSError, TypeError):\n            tf.logging.fatal(\'could not create model folder: %s\', model_dir)\n            raise\n    absolute_dir = os.path.abspath(model_dir)\n    # tf.logging.info(\'accessing output folder: {}\'.format(absolute_dir))\n    return absolute_dir\n\n\ndef resolve_module_dir(module_dir_str, create_new=False):\n    """"""\n    Interpret `module_dir_str` as an absolute folder path.\n    create the folder if `create_new`\n\n    :param module_dir_str:\n    :param create_new:\n    :return:\n    """"""\n    try:\n        # interpret input as a module string\n        module_from_string = importlib.import_module(module_dir_str)\n        folder_path = os.path.dirname(module_from_string.__file__)\n        return os.path.abspath(folder_path)\n    except (ImportError, AttributeError, TypeError):\n        pass\n\n    try:\n        # interpret last part of input as a module string\n        string_last_part = module_dir_str.rsplit(\'.\', 1)\n        module_from_string = importlib.import_module(string_last_part[-1])\n        folder_path = os.path.dirname(module_from_string.__file__)\n        return os.path.abspath(folder_path)\n    except (ImportError, AttributeError, IndexError, TypeError):\n        pass\n\n    module_dir_str = os.path.expanduser(module_dir_str)\n    try:\n        # interpret input as a file folder path string\n        if os.path.isdir(module_dir_str):\n            return os.path.abspath(module_dir_str)\n    except TypeError:\n        pass\n\n    try:\n        # interpret input as a file path string\n        if os.path.isfile(module_dir_str):\n            return os.path.abspath(os.path.dirname(module_dir_str))\n    except TypeError:\n        pass\n\n    try:\n        # interpret input as a path string relative to the global home\n        home_location = NiftyNetGlobalConfig().get_niftynet_home_folder()\n        possible_dir = os.path.join(home_location, module_dir_str)\n        if os.path.isdir(possible_dir):\n            return os.path.abspath(possible_dir)\n    except (TypeError, ImportError, AttributeError):\n        pass\n\n    if create_new:\n        # try to create the folder\n        folder_path = touch_folder(module_dir_str)\n        init_file = os.path.join(folder_path, \'__init__.py\')\n        try:\n            file_ = os.open(init_file, os.O_CREAT | os.O_EXCL | os.O_WRONLY)\n        except OSError as sys_error:\n            if sys_error.errno == errno.EEXIST:\n                pass\n            else:\n                tf.logging.fatal(\n                    ""trying to use \'{}\' as NiftyNet writing path, ""\n                    ""however cannot write \'{}\'"".format(folder_path, init_file))\n                raise\n        else:\n            with os.fdopen(file_, \'w\') as file_object:\n                file_object.write(""# Created automatically\\n"")\n        return folder_path\n    raise ValueError(\n        ""Could not resolve [{}].\\nMake sure it is a valid folder path ""\n        ""or a module name.\\nIf it is string representing a module, ""\n        ""the parent folder of [{}] should be on ""\n        ""the system path.\\n\\nCurrent system path {}."".format(\n            module_dir_str, module_dir_str, sys.path))\n\n\ndef to_absolute_path(input_path, model_root):\n    """"""\n    Convert `input_path` into a relative path to model_root\n    (model_root/input_path) if `input_path` is not an absolute one.\n\n    :param input_path:\n    :param model_root:\n    :return:\n    """"""\n    try:\n        input_path = os.path.expanduser(input_path)\n        model_root = os.path.expanduser(model_root)\n        if os.path.isabs(input_path):\n            return input_path\n    except TypeError:\n        pass\n    return os.path.abspath(os.path.join(model_root, input_path))\n\n\ndef resolve_file_name(file_name, paths):\n    """"""\n    check if `file_name` exists, if not,\n    go though the list of [path + file_name for path in paths].\n    raises IOError if all options don\'t exist\n\n    :param file_name:\n    :param paths:\n    :return:\n    """"""\n    try:\n        assert file_name\n        if os.path.isfile(file_name):\n            return os.path.abspath(file_name)\n        for path in paths:\n            path_file_name = os.path.join(path, file_name)\n            if os.path.isfile(path_file_name):\n                tf.logging.info(\'Resolving {} as {}\'.format(\n                    file_name, path_file_name))\n                return os.path.abspath(path_file_name)\n        raise IOError(\'Could not resolve file name\')\n    except (TypeError, AssertionError, IOError):\n        raise IOError(\'Could not resolve {}\'.format(file_name))\n\n\ndef resolve_checkpoint(checkpoint_name):\n    """"""\n    Find the abosolute path of `checkpoint_name`\n\n    For now only supports checkpoint_name where\n    checkpoint_name.index is in the file system\n    eventually will support checkpoint names that can be referenced\n    in a path file.\n\n    :param checkpoint_name:\n    :return:\n    """"""\n\n    if os.path.isfile(checkpoint_name + \'.index\'):\n        return checkpoint_name\n    home_folder = NiftyNetGlobalConfig().get_niftynet_home_folder()\n    checkpoint_name = to_absolute_path(\n        input_path=checkpoint_name, model_root=home_folder)\n    if os.path.isfile(checkpoint_name + \'.index\'):\n        return checkpoint_name\n    raise ValueError(\'Invalid checkpoint {}\'.format(checkpoint_name))\n\n\ndef get_latest_subfolder(parent_folder, create_new=False):\n    """"""\n    Automatically determine the latest folder n if there are n folders\n    in `parent_folder`, and create the (n+1)th folder if required.\n    This is used in accessing/creating log folders of multiple runs.\n\n    :param parent_folder:\n    :param create_new:\n    :return:\n    """"""\n    parent_folder = touch_folder(parent_folder)\n    try:\n        log_sub_dirs = os.listdir(parent_folder)\n    except OSError:\n        tf.logging.fatal(\'not a directory {}\'.format(parent_folder))\n        raise OSError\n    log_sub_dirs = [\n        name for name in log_sub_dirs if re.findall(\'^[0-9]+$\', name)\n    ]\n    if log_sub_dirs and create_new:\n        latest_id = max([int(name) for name in log_sub_dirs])\n        log_sub_dir = \'{}\'.format(latest_id + 1)\n    elif log_sub_dirs and not create_new:\n        latest_valid_id = max([\n            int(name) for name in log_sub_dirs\n            if os.path.isdir(os.path.join(parent_folder, name))\n        ])\n        log_sub_dir = \'{}\'.format(latest_valid_id)\n    else:\n        log_sub_dir = \'{}\'.format(0)\n    return os.path.join(parent_folder, log_sub_dir)\n\n\n# pylint: disable=invalid-name\ndef _image3_animated_gif(tag, ims):\n    PIL = require_module(\'PIL\')\n    from PIL.GifImagePlugin import Image as GIF\n\n    # x=numpy.random.randint(0,256,[10,10,10],numpy.uint8)\n    ims = [\n        np.asarray((ims[i, :, :]).astype(np.uint8))\n        for i in range(ims.shape[0])\n    ]\n    ims = [GIF.fromarray(im) for im in ims]\n    img_str = b\'\'\n    for b_data in PIL.GifImagePlugin.getheader(ims[0])[0]:\n        img_str += b_data\n    img_str += b\'\\x21\\xFF\\x0B\\x4E\\x45\\x54\\x53\\x43\\x41\\x50\' \\\n         b\'\\x45\\x32\\x2E\\x30\\x03\\x01\\x00\\x00\\x00\'\n    for i in ims:\n        for b_data in PIL.GifImagePlugin.getdata(i):\n            img_str += b_data\n    img_str += b\'\\x3B\'\n    if IS_PYTHON2:\n        img_str = str(img_str)\n    summary_image_str = summary_pb2.Summary.Image(\n        height=10, width=10, colorspace=1, encoded_image_string=img_str)\n    image_summary = summary_pb2.Summary.Value(tag=tag, image=summary_image_str)\n    return [summary_pb2.Summary(value=[image_summary]).SerializeToString()]\n\n\ndef image3(name,\n           tensor,\n           max_out=3,\n           collections=(tf.GraphKeys.SUMMARIES, ),\n           animation_axes=(1, ),\n           image_axes=(2, 3),\n           other_indices=None):\n    """"""\n    Summary for higher dimensional images\n\n    Parameters:\n\n        name: string name for the summary\n        tensor: tensor to summarize. Should be in the range 0..255.\n            By default, assumes tensor is NDHWC, and animates (through D)\n            HxW slices of the 1st channel.\n        collections: list of strings collections to add the summary to\n        animation_axes=[1],image_axes=[2,3]\n\n    """"""\n\n    if max_out == 1:\n        suffix = \'/image\'\n    else:\n        suffix = \'/image/{}\'\n    if other_indices is None:\n        other_indices = {}\n    axis_order = [0] + animation_axes + image_axes\n    # slice tensor\n    slicing = []\n    for i in range(len(tensor.shape)):\n        if i in axis_order:\n            slicing.append(slice(None))\n        else:\n            other_ind = other_indices.get(i, 0)\n            slicing.append(slice(other_ind, other_ind + 1))\n    tensor = tensor[tuple(slicing)]\n    axis_order_all = \\\n        axis_order + [i for i in range(len(tensor.shape.as_list()))\n                      if i not in axis_order]\n    original_shape = tensor.shape.as_list()\n    new_shape = [\n        original_shape[0], -1, original_shape[axis_order[-2]],\n        original_shape[axis_order[-1]]\n    ]\n    transposed_tensor = tf.transpose(tensor, axis_order_all)\n    transposed_tensor = tf.reshape(transposed_tensor, new_shape)\n    # split images\n    with tf.device(\'/cpu:0\'):\n        for it_i in range(min(max_out, transposed_tensor.shape.as_list()[0])):\n            inp = [\n                name + suffix.format(it_i), transposed_tensor[it_i, :, :, :]\n            ]\n            summary_op = tf.py_func(_image3_animated_gif, inp, tf.string)\n            for c in collections:\n                tf.add_to_collection(c, summary_op)\n    return summary_op\n\n\ndef image3_sagittal(name,\n                    tensor,\n                    max_outputs=3,\n                    collections=(tf.GraphKeys.SUMMARIES, )):\n    """"""\n    Create 2D image summary in the sagittal view.\n\n    :param name:\n    :param tensor:\n    :param max_outputs:\n    :param collections:\n    :return:\n    """"""\n    return image3(name, tensor, max_outputs, collections, [1], [2, 3])\n\n\ndef image3_coronal(name,\n                   tensor,\n                   max_outputs=3,\n                   collections=(tf.GraphKeys.SUMMARIES, )):\n    """"""\n    Create 2D image summary in the coronal view.\n\n    :param name:\n    :param tensor:\n    :param max_outputs:\n    :param collections:\n    :return:\n    """"""\n    return image3(name, tensor, max_outputs, collections, [2], [1, 3])\n\n\ndef image3_axial(name,\n                 tensor,\n                 max_outputs=3,\n                 collections=(tf.GraphKeys.SUMMARIES, )):\n    """"""\n    Create 2D image summary in the axial view.\n\n    :param name:\n    :param tensor:\n    :param max_outputs:\n    :param collections:\n    :return:\n    """"""\n    return image3(name, tensor, max_outputs, collections, [3], [1, 2])\n\n\ndef set_logger(file_name=None):\n    """"""\n    Writing logs to a file if file_name,\n    the handler needs to be closed by `close_logger()` after use.\n\n    :param file_name:\n    :return:\n    """"""\n    # pylint: disable=no-name-in-module\n    # This is done so if the user had TF 1.12.1 or a new version the code\n    # does not brake. First part of the try is renaming the TF 1.12.1 to\n    # fit the TF 1.13.1>= naming scheme, while the second is just a normal\n    # import for TF 1.13.1>=\n    try:\n        # pylint: disable=no-name-in-module\n        from tensorflow.python.platform.tf_logging import \\\n            _get_logger as get_logger\n    except ImportError:\n        from tensorflow.python.platform.tf_logging import get_logger\n\n    logger = get_logger()\n    tf.logging.set_verbosity(tf.logging.INFO)\n    logger.handlers = []\n\n    # adding console output\n    f = log.Formatter(CONSOLE_LOG_FORMAT)\n    std_handler = log.StreamHandler(sys.stdout)\n    std_handler.setFormatter(f)\n    logger.addHandler(std_handler)\n\n    if file_name:\n        # adding file output\n        f = log.Formatter(FILE_LOG_FORMAT)\n        file_handler = log.FileHandler(file_name)\n        file_handler.setFormatter(f)\n        logger.addHandler(file_handler)\n\n\ndef close_logger():\n    """"""\n    Close file-based outputs\n\n    :return:\n    """"""\n    # pylint: disable=no-name-in-module\n    # This is done so if the user had TF 1.12.1 or a new version the code\n    # does not brake. First part of the try is renaming the TF 1.12.1 to\n    # fit the TF 1.13.1>= naming scheme, while the second is just a normal\n    # import for TF 1.13.1>=\n    try:\n        # pylint: disable=no-name-in-module\n        from tensorflow.python.platform.tf_logging import \\\n            _get_logger as get_logger\n    except ImportError:\n        from tensorflow.python.platform.tf_logging import get_logger\n\n    logger = get_logger()\n    for handler in reversed(logger.handlers):\n        try:\n            handler.flush()\n            handler.close()\n            logger.removeHandler(handler)\n        except (OSError, ValueError):\n            pass\n\n\ndef infer_latest_model_file(model_dir):\n    """"""\n    Infer initial iteration number from model_dir/checkpoint.\n\n    :param model_dir: model folder to search\n    :return:\n    """"""\n    ckpt_state = tf.train.get_checkpoint_state(model_dir)\n    try:\n        assert ckpt_state, ""{}/checkpoint not found, please check "" \\\n                           ""config parameter: model_dir"".format(model_dir)\n        checkpoint = ckpt_state.model_checkpoint_path\n        assert checkpoint, \'checkpoint path not found \' \\\n                           \'in {}/checkpoint\'.format(model_dir)\n        initial_iter = int(checkpoint.rsplit(\'-\')[-1])\n        tf.logging.info(\'set initial_iter to %d based \'\n                        \'on checkpoints\', initial_iter)\n        return initial_iter\n    except (ValueError, AttributeError, AssertionError):\n        tf.logging.fatal(\n            \'Failed to get iteration number \'\n            \'from checkpoint path (%s),\\n\'\n            \'please check config parameter: \'\n            \'model_dir, and initial_iter\', model_dir)\n        raise\n'"
niftynet/layer/__init__.py,0,"b'""""""\n\n.. module:: niftynet.layer\n   :synopsis: Building blocks for neural network layers.\n\n""""""\n'"
niftynet/layer/activation.py,13,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\n\ndef prelu(f_in, channelwise_params):\n    pos = tf.nn.relu(f_in)\n    neg = channelwise_params * (f_in - tf.abs(f_in)) * 0.5\n    return pos + neg\n\n\ndef selu(x, name):\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    return scale * tf.where(x >= 0.0, x, alpha * tf.nn.elu(x))\n\n\ndef leaky_relu(x, name):\n    half_alpha = 0.01\n    return (0.5 + half_alpha) * x + (0.5 - half_alpha) * abs(x)\n\n\nSUPPORTED_OP = {\'relu\': tf.nn.relu,\n                \'relu6\': tf.nn.relu6,\n                \'elu\': tf.nn.elu,\n                \'softplus\': tf.nn.softplus,\n                \'softsign\': tf.nn.softsign,\n                \'sigmoid\': tf.nn.sigmoid,\n                \'tanh\': tf.nn.tanh,\n                \'prelu\': prelu,\n                \'selu\': selu,\n                \'leakyrelu\': leaky_relu,\n                \'dropout\': tf.nn.dropout}\n\n\nclass ActiLayer(TrainableLayer):\n    """"""\n    Apply an element-wise non-linear activation function.\n    \'Prelu\' uses trainable parameters and those are initialised to zeros\n    Dropout function is also supported\n    """"""\n\n    def __init__(self, func, regularizer=None, name=\'activation\'):\n        self.func = func.lower()\n        self.layer_name = \'{}_{}\'.format(self.func, name)\n\n        super(ActiLayer, self).__init__(name=self.layer_name)\n\n        # these are used for prelu variables\n        self.initializers = {\'alpha\': tf.constant_initializer(0.0)}\n        self.regularizers = {\'alpha\': regularizer}\n\n    def layer_op(self, input_tensor, keep_prob=None):\n        func_ = look_up_operations(self.func, SUPPORTED_OP)\n        if self.func == \'prelu\':\n            alphas = tf.get_variable(\n                \'alpha\', input_tensor.shape[-1],\n                initializer=self.initializers[\'alpha\'],\n                regularizer=self.regularizers[\'alpha\'])\n            output_tensor = func_(input_tensor, alphas)\n        elif self.func == \'dropout\':\n            output_tensor = func_(input_tensor,\n                                  keep_prob=keep_prob,\n                                  name=\'dropout\')\n        else:\n            output_tensor = func_(input_tensor, name=\'acti\')\n        return output_tensor\n'"
niftynet/layer/additive_upsample.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer as ResizingLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer as Deconv\nfrom niftynet.layer.layer_util import check_divisible_channels\nfrom niftynet.layer.elementwise import ElementwiseLayer\n\n\nclass AdditiveUpsampleLayer(Layer):\n    """"""\n    Implementation of bilinear (or trilinear) additive upsampling layer,\n    described in paper:\n\n        Wojna et al., The devil is in the decoder,\n        https://arxiv.org/abs/1707.05847\n\n    In the paper 2D images are upsampled by a factor of 2 and ``n_splits = 4``\n    """"""\n\n    def __init__(self, new_size, n_splits, name=\'linear_additive_upsample\'):\n        """"""\n\n        :param new_size: integer or a list of integers set the output\n            2D/3D spatial shape.  If the parameter is an integer ``d``,\n            it\'ll be expanded to ``(d, d)`` and ``(d, d, d)`` for 2D and\n            3D inputs respectively.\n        :param n_splits: integer, the output tensor will have ``C / n_splits``\n            channels, where ``C`` is the number of channels of the input.\n            (``n_splits`` must evenly divide ``C``.)\n        :param name: (optional) name of the layer\n        """"""\n        super(AdditiveUpsampleLayer, self).__init__(name=name)\n        self.new_size = new_size\n        self.n_splits = int(n_splits)\n\n    def layer_op(self, input_tensor):\n        """"""\n        If the input has the shape ``batch, X, Y,[ Z,] Channels``,\n        the output will be\n        ``batch, new_size_x, new_size_y,[ new_size_z,] channels/n_splits``.\n\n        :param input_tensor: 2D/3D image tensor, with shape:\n            ``batch, X, Y,[ Z,] Channels``\n        :return: linearly additively upsampled volumes\n        """"""\n        check_divisible_channels(input_tensor, self.n_splits)\n\n        resizing_layer = ResizingLayer(self.new_size)\n        split = tf.split(resizing_layer(input_tensor), self.n_splits, axis=-1)\n        split_tensor = tf.stack(split, axis=-1)\n        output_tensor = tf.reduce_sum(split_tensor, axis=-1)\n        return output_tensor\n\n\nclass ResidualUpsampleLayer(TrainableLayer):\n    """"""\n    Implementation of the upsampling layer with residual like connections,\n    described in paper:\n\n        Wojna et al., The devil is in the decoder,\n        https://arxiv.org/abs/1707.05847\n\n    """"""\n\n    def __init__(self,\n                 kernel_size=3,\n                 stride=2,\n                 n_splits=2,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'residual_additive_upsample\'):\n        TrainableLayer.__init__(self, name=name)\n        self.n_splits = n_splits\n        self.deconv_param = {\'w_initializer\': w_initializer,\n                             \'w_regularizer\': w_regularizer,\n                             \'kernel_size\': kernel_size,\n                             \'acti_func\': acti_func,\n                             \'stride\': stride}\n\n    def layer_op(self, input_tensor, is_training=True):\n        """"""\n        output is an elementwise sum of deconvolution and additive upsampling::\n\n            --(inputs)--o--deconvolution-------+--(outputs)--\n                        |                      |\n                        o--additive upsampling-o\n        :param input_tensor:\n        :param is_training:\n        :return: an upsampled tensor with ``n_input_channels/n_splits``\n            feature channels.\n        """"""\n        n_output_chns = check_divisible_channels(input_tensor, self.n_splits)\n        # deconvolution path\n        deconv_output = Deconv(n_output_chns=n_output_chns,\n                               with_bias=False, feature_normalization=\'batch\',\n                               **self.deconv_param)(input_tensor, is_training)\n\n        # additive upsampling path\n        additive_output = AdditiveUpsampleLayer(\n            new_size=deconv_output.get_shape().as_list()[1:-1],\n            n_splits=self.n_splits)(input_tensor)\n\n        output_tensor = ElementwiseLayer(\'SUM\')(deconv_output, additive_output)\n        return output_tensor\n'"
niftynet/layer/affine_augmentation.py,9,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.layer_util import infer_spatial_rank\nfrom niftynet.layer.resampler import ResamplerLayer\n\n\nclass AffineAugmentationLayer(Layer):\n    """"""\n    This layer applies a small random (per-iteration) affine\n    transformation to an image. The distribution of transformations\n    generally results in scaling the image up, with minimal sampling\n    outside the original image.\n    """"""\n\n    def __init__(self,\n                 scale,\n                 interpolation=\'linear\',\n                 boundary=\'zero\',\n                 transform=None,\n                 name=\'AffineAugmentation\'):\n        """"""\n\n        :param scale: how extreme the perturbation is, with 0. meaning\n            no perturbation and 1.0 giving largest perturbations.\n        :param interpolation: the image value interpolation used by\n            the resampling.\n        :param boundary: the boundary handling used by the resampling\n        :param name: string name of the layer.\n        """"""\n        Layer.__init__(self, name=name)\n\n        self.scale = min(max(float(scale), 0.0), 1.0)\n        self.interpolation = interpolation\n        self.boundary = boundary\n\n        self._transform = None\n        if transform is not None:\n            self._transform = transform\n\n    def _random_transform(self, batch_size, spatial_rank):\n        """"""\n        computes a relative transformation\n        mapping <-1..1, -1..1, -1..1> to <-1..1, -1..1, -1..1> (in 3D)\n        or <-1..1, -1..1> to <-1..1, -1..1> (in 2D).\n\n        :param batch_size: number of different random transformations\n        :param spatial_rank: number of spatial dimensions\n        :return:\n        """"""\n        output_corners = get_relative_corners(spatial_rank)\n        output_corners = tf.tile([output_corners], [batch_size, 1, 1])\n\n        # make randomised output corners\n        random_size = [batch_size, 2 ** spatial_rank, spatial_rank]\n        random_scale = tf.random_uniform(random_size, 1. - self.scale, 1.0)\n        source_corners = output_corners * random_scale\n\n        # make homogeneous corners\n        batch_ones = tf.ones_like(output_corners[..., 0:1])\n        source_corners = tf.concat([source_corners, batch_ones], -1)\n        output_corners = tf.concat([output_corners, batch_ones], -1)\n\n        ls_transform = tf.matrix_solve_ls(output_corners, source_corners)\n        return tf.transpose(ls_transform, [0, 2, 1])\n\n    def layer_op(self, input_tensor):\n        input_shape = input_tensor.shape.as_list()\n        batch_size = input_shape[0]\n        spatial_shape = input_shape[1:-1]\n        spatial_rank = infer_spatial_rank(input_tensor)\n\n        if self._transform is None:\n            relative_transform = self._random_transform(\n                batch_size, spatial_rank)\n            self._transform = relative_transform\n        else:\n            relative_transform = self._transform\n\n        grid_warper = AffineGridWarperLayer(spatial_shape, spatial_shape)\n        resampler = ResamplerLayer(\n            interpolation=self.interpolation, boundary=self.boundary)\n        warp_parameters = tf.reshape(\n            relative_transform[:, :spatial_rank, :], [batch_size, -1])\n        grid = grid_warper(warp_parameters)\n        resampled = resampler(input_tensor, grid)\n        return resampled\n\n    def inverse(self, interpolation=None, boundary=None, name=None):\n        """"""\n        create a new layer that will apply the inversed version of\n        self._transform. This function write this instance members.\n        (calling `self()` after `self.inverse()` might give unexpected results.)\n\n        :param interpolation:\n        :param boundary:\n        :param name:\n        :return: a niftynet layer that inverses the transformation of  `self`.\n        """"""\n        if interpolation is None:\n            interpolation = self.interpolation\n        if boundary is None:\n            boundary = self.boundary\n        if name is None:\n            name = self.name + \'_inverse\'\n\n        inverse_layer = AffineAugmentationLayer(\n            self.scale,\n            interpolation,\n            boundary,\n            tf.matrix_inverse(self._transform),\n            name)\n        return inverse_layer\n\n\ndef get_relative_corners(spatial_rank):\n    """"""\n    compute relative corners of the spatially n-d tensor::\n\n        1-D: [[-1], [1]]\n        2-D: [[-1, -1], [-1, 1], [1, -1], [1, 1]]\n        3-D: [[-1, -1, -1], [-1, -1, 1],\n              [-1, 1, -1],  [-1, 1, 1],\n              [1, -1, -1],  [1, -1, 1],\n              [1, 1, -1],   [1, 1, 1]]\n\n    :param spatial_rank: integer of number of spatial dimensions\n    :return: [2**spatial_rank, spatial_rank] matrix\n    """"""\n    return [\n        [int(c) * 2.0 - 1.0 for c in format(i, \'0%ib\' % spatial_rank)]\n        for i in range(2 ** spatial_rank)]\n'"
niftynet/layer/approximated_smoothing.py,10,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.layer_util import \\\n    expand_spatial_params, infer_spatial_rank\nfrom niftynet.utilities.util_common import look_up_operations\n\n""""""\nThis class approximates image smoothing using separable 1D kernels.\n(This layer is not trainable.)\n""""""\n\n\ndef gaussian_1d(sigma, truncated=3.0):\n    if sigma <= 0:\n        return tf.constant(0.0)\n\n    tail = int(sigma * truncated + 0.5)\n    sigma_square = sigma * sigma\n    k = [(-0.5 * x * x) / sigma_square for x in range(-tail, tail + 1)]\n    k = tf.exp(k)\n    k = k / tf.reduce_sum(k)\n    return k\n\n\ndef cauchy_1d(sigma, truncated=5.0):\n    if sigma <= 0:\n        return tf.constant(0.0)\n\n    tail = int(sigma * truncated + 0.5)\n    k = [((float(x) / sigma) ** 2 + 1.0) for x in range(-tail, tail + 1)]\n    k = tf.reciprocal(k)\n    k = k / tf.reduce_sum(k)\n    return k\n\n\nSUPPORTED_KERNELS = {\'gaussian\': gaussian_1d, \'cauchy\': cauchy_1d}\n\n\nclass SmoothingLayer(Layer):\n    """"""\n    computing 1d convolution one each spatial dimension of the input\n    using one-dimensional filter.\n    """"""\n\n    def __init__(self, sigma=1, truncate=3.0, type_str=\'gaussian\'):\n        """"""\n\n        :param sigma: standard deviation\n        :param truncate: Truncate the filter at this many standard deviations\n        :param type_str: type of kernels\n        """"""\n        Layer.__init__(self, name=\'approximated_smoothing\')\n        self.kernel_func = look_up_operations(\n            type_str.lower(), SUPPORTED_KERNELS)\n        self.sigma = sigma\n        self.truncate = truncate\n\n    def layer_op(self, image):\n        """"""\n\n        :param image: in shape `(batch, x[, y, z], feature_channels)`\n        :return: spatially smoothed image\n        """"""\n        spatial_rank = infer_spatial_rank(image)\n        _sigmas = expand_spatial_params(input_param=self.sigma,\n                                        spatial_rank=spatial_rank,\n                                        param_type=float)\n        _truncate = expand_spatial_params(input_param=self.truncate,\n                                          spatial_rank=spatial_rank,\n                                          param_type=float)\n        if not all(_sigmas):\n            # return the original image if any sigma is zero\n            return image\n\n        def do_conv(input_tensor, dim):\n            assert dim < spatial_rank\n            if dim < 0:\n                return input_tensor\n\n            # squeeze the kernel to be along the \'dim\'\n            new_kernel_shape = [1] * (spatial_rank + 2)\n            new_kernel_shape[dim] = -1\n            kernel_tensor = self.kernel_func(\n                sigma=_sigmas[dim], truncated=_truncate[dim])\n            kernel_tensor = tf.reshape(kernel_tensor, new_kernel_shape)\n\n            # split channels and do smoothing respectively\n            chn_wise_list = tf.unstack(do_conv(input_tensor, dim - 1), axis=-1)\n            output_tensor = [\n                tf.nn.convolution(input=tf.expand_dims(chn, axis=-1),\n                                  filter=kernel_tensor,\n                                  padding=\'SAME\',\n                                  strides=[1] * spatial_rank)\n                for chn in chn_wise_list]\n            return tf.concat(output_tensor, axis=-1)\n\n        return do_conv(image, spatial_rank - 1)\n'"
niftynet/layer/base_layer.py,7,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom abc import ABCMeta, abstractmethod\n\nimport numpy as np\nimport tensorflow as tf\nfrom six import with_metaclass\n\nfrom niftynet.engine.application_variables import RESTORABLE\n\n\nclass Invertible(with_metaclass(ABCMeta, object)):\n    """"""\n    interface of Invertible data\n    """"""\n\n    @abstractmethod\n    def inverse_op(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass Layer(object):\n    def __init__(self, name=\'untitled_op\'):\n        self.name = name\n        self._op = tf.make_template(name, self.layer_op, create_scope_now_=True)\n\n    def layer_op(self, *args, **kwargs):\n        msg = \'method \\\'layer_op\\\' in \\\'{}\\\'\'.format(type(self).__name__)\n        tf.logging.fatal(msg)\n        raise NotImplementedError\n\n    def __call__(self, *args, **kwargs):\n        return self._op(*args, **kwargs)\n\n    def __str__(self):\n        return self.to_string()\n\n    def layer_scope(self):\n        return self._op.variable_scope\n\n    def to_string(self):\n        layer_scope_name = self.layer_scope().name\n        out_str = ""\\033[42m[Layer]\\033[0m {}"".format(layer_scope_name)\n        if not self._op._variables_created:\n            out_str += \' \\033[46m(input undecided)\\033[0m\'\n            return out_str\n        return out_str\n\n\nclass TrainableLayer(Layer):\n    """"""\n    Extends the Layer object to have trainable parameters,\n    adding initializers and regularizers.\n    """"""\n\n    def __init__(self, name=\'trainable_op\'):\n        super(TrainableLayer, self).__init__(name=name)\n\n        self._initializers = None\n        self._regularizers = None\n\n    def trainable_variables(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                 self.layer_scope().name)\n\n    def restore_from_checkpoint(self, checkpoint_name, scope=None):\n        if scope is None:\n            scope = self.layer_scope().name\n        tf.add_to_collection(RESTORABLE, (self.layer_scope().name,\n                                          checkpoint_name, scope))\n\n    def regularizer_loss(self):\n        return tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n                                 self.layer_scope().name)\n\n    def num_trainable_params(self):\n        n = tf.Dimension(0)\n        for x in self.trainable_variables():\n            n += np.prod(x.get_shape())\n        return int(n)\n\n    def to_string(self):\n        out_str = Layer.to_string(self)\n        # try to add trainable variable info to the string\n        layer_variables = self.trainable_variables()\n        if not layer_variables:\n            return out_str\n        # including name of parameters\n        out_str += \' \\033[92m[Trainable]\\033[0m \'\n        out_str += \', \'.join(\n            [v.name.split(\':\')[0][len(self.layer_scope().name) + 1:]\n             for v in layer_variables])\n        # including number of parameters\n        out_str += \' ({})\'.format(self.num_trainable_params())\n        return out_str\n\n    @property\n    def initializers(self):\n        return self._initializers\n\n    @property\n    def regularizers(self):\n        return self._regularizers\n\n    @initializers.setter\n    def initializers(self, value):\n        assert isinstance(value, dict)\n        self._initializers = value\n\n    @regularizers.setter\n    def regularizers(self, value):\n        assert isinstance(value, dict)\n        self._regularizers = value\n\n\nclass DataDependentLayer(Layer):\n    """"""\n    Some layers require a one-pass training through the training set\n    to determine their internal models, this abstract provides\n    interfaces for training these internal models and querying the\n    status.\n    """"""\n\n    def __init__(self, name=\'data_dependent_op\'):\n        super(DataDependentLayer, self).__init__(name=name)\n\n    def is_ready(self):\n        raise NotImplementedError\n\n    def train(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass RandomisedLayer(Layer):\n    """"""\n    The layers require a randomisation process, to randomly\n    change some of the layer\'s states on the fly.\n    """"""\n\n    def __init__(self, name=\'randomised_op\'):\n        super(RandomisedLayer, self).__init__(name=name)\n\n    def randomise(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass LayerFromCallable(Layer):\n    """"""\n    Module wrapping a function provided by the user.\n    Analogous to snt.Module\n    """"""\n\n    def __init__(self, layer_op, name=\'from_callable_op\'):\n        super(LayerFromCallable, self).__init__(name=name)\n        if not callable(layer_op):\n            tf.logging.fatal(""layer_op must be callable."")\n            raise TypeError\n        self._layer_op = layer_op\n\n    def layer_op(self, *args, **kwargs):\n        return self._layer_op(*args, **kwargs)\n'"
niftynet/layer/binary_masking.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport scipy.ndimage as ndimg\nfrom scipy.ndimage.morphology import binary_fill_holes as fill_holes\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.utilities.util_common import look_up_operations\nfrom niftynet.utilities.util_common import otsu_threshold\n\n""""""\nThis class defines methods to generate a binary image from an input image.\nThe binary image can be used as an automatic foreground selector, so that later\nprocessing layers can only operate on the `True` locations within the image.\n""""""\nSUPPORTED_MASK_TYPES = set([\'threshold_plus\', \'threshold_minus\',\n                            \'otsu_plus\', \'otsu_minus\', \'mean_plus\'])\n\nSUPPORTED_MULTIMOD_MASK_TYPES = set([\'or\', \'and\', \'multi\'])\n\n\nclass BinaryMaskingLayer(Layer):\n    def __init__(self,\n                 type_str=\'otsu_plus\',\n                 multimod_fusion=\'or\',\n                 threshold=0.0):\n\n        super(BinaryMaskingLayer, self).__init__(name=\'binary_masking\')\n        self.type_str = look_up_operations(\n            type_str.lower(), SUPPORTED_MASK_TYPES)\n        self.multimod_fusion = look_up_operations(\n            multimod_fusion.lower(), SUPPORTED_MULTIMOD_MASK_TYPES)\n\n        self.threshold = threshold\n\n    def __make_mask_3d(self, image):\n        assert image.ndim == 3\n        image_shape = image.shape\n        image = image.reshape(-1)\n        mask = np.zeros_like(image, dtype=np.bool)\n        thr = self.threshold\n        if self.type_str == \'threshold_plus\':\n            mask[image > thr] = True\n        elif self.type_str == \'threshold_minus\':\n            mask[image < thr] = True\n        elif self.type_str == \'otsu_plus\':\n            thr = otsu_threshold(image) if np.any(image) else thr\n            mask[image > thr] = True\n        elif self.type_str == \'otsu_minus\':\n            thr = otsu_threshold(image) if np.any(image) else thr\n            mask[image < thr] = True\n        elif self.type_str == \'mean_plus\':\n            thr = np.mean(image)\n            mask[image > thr] = True\n        mask = mask.reshape(image_shape)\n        mask = ndimg.binary_dilation(mask, iterations=2)\n        mask = fill_holes(mask)\n        # foreground should not be empty\n        assert np.any(mask == True), \\\n            ""no foreground based on the specified combination parameters, "" \\\n            ""please change choose another `mask_type` or double-check all "" \\\n            ""input images""\n        return mask\n\n    def layer_op(self, image):\n        if image.ndim == 3:\n            return self.__make_mask_3d(image)\n\n        if image.ndim == 5:\n            mod_to_mask = [m for m in range(image.shape[4])\n                           if np.any(image[..., :, m])]\n            mask = np.zeros_like(image, dtype=bool)\n            mod_mask = None\n            for mod in mod_to_mask:\n                for t in range(image.shape[3]):\n                    mask[..., t, mod] = self.__make_mask_3d(image[..., t, mod])\n                # combine masks across the modalities dim\n                if self.multimod_fusion == \'or\':\n                    if mod_mask is None:\n                        mod_mask = np.zeros(image.shape[:4], dtype=bool)\n                    mod_mask = np.logical_or(mod_mask, mask[..., mod])\n                elif self.multimod_fusion == \'and\':\n                    if mod_mask is None:\n                        mod_mask = np.ones(image.shape[:4], dtype=bool)\n                    mod_mask = np.logical_and(mod_mask, mask[..., mod])\n            for mod in mod_to_mask:\n                mask[..., mod] = mod_mask\n            return mask\n        else:\n            raise ValueError(""unknown input format"")\n'"
niftynet/layer/bn.py,27,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.training import moving_averages\n\nfrom niftynet.layer.base_layer import TrainableLayer\n\nBN_COLLECTION = tf.GraphKeys.UPDATE_OPS\n\n\nclass BNLayer(TrainableLayer):\n    """"""\n    Batch normalisation layer, with trainable mean value \'beta\' and\n    std \'gamma\'.  \'beta\' is initialised to 0.0 and \'gamma\' is initialised\n    to 1.0.  This class assumes \'beta\' and \'gamma\' share the same type_str of\n    regulariser.\n    """"""\n\n    def __init__(self,\n                 regularizer=None,\n                 moving_decay=0.9,\n                 eps=1e-5,\n                 name=\'batch_norm\'):\n        super(BNLayer, self).__init__(name=name)\n        self.eps = eps\n        self.moving_decay = moving_decay\n\n        self.initializers = {\'beta\': tf.constant_initializer(0.0),\n                             \'gamma\': tf.constant_initializer(1.0),\n                             \'moving_mean\': tf.constant_initializer(0.0),\n                             \'moving_variance\': tf.constant_initializer(1.0)}\n\n        self.regularizers = {\'beta\': regularizer, \'gamma\': regularizer}\n\n    def layer_op(self, inputs, is_training, use_local_stats=False):\n        input_shape = inputs.shape\n\n        # operates on all dims except the last dim\n        params_shape = input_shape[-1:]\n        axes = list(range(input_shape.ndims - 1))\n\n        # create trainable variables and moving average variables\n        beta = tf.get_variable(\n            \'beta\',\n            shape=params_shape,\n            initializer=self.initializers[\'beta\'],\n            regularizer=self.regularizers[\'beta\'],\n            dtype=tf.float32, trainable=True)\n        gamma = tf.get_variable(\n            \'gamma\',\n            shape=params_shape,\n            initializer=self.initializers[\'gamma\'],\n            regularizer=self.regularizers[\'gamma\'],\n            dtype=tf.float32, trainable=True)\n\n        collections = [tf.GraphKeys.GLOBAL_VARIABLES]\n        moving_mean = tf.get_variable(\n            \'moving_mean\',\n            shape=params_shape,\n            initializer=self.initializers[\'moving_mean\'],\n            dtype=tf.float32, trainable=False, collections=collections)\n        moving_variance = tf.get_variable(\n            \'moving_variance\',\n            shape=params_shape,\n            initializer=self.initializers[\'moving_variance\'],\n            dtype=tf.float32, trainable=False, collections=collections)\n\n        # mean and var\n        mean, variance = tf.nn.moments(inputs, axes)\n        update_moving_mean = moving_averages.assign_moving_average(\n            moving_mean, mean, self.moving_decay).op\n        update_moving_variance = moving_averages.assign_moving_average(\n            moving_variance, variance, self.moving_decay).op\n        tf.add_to_collection(BN_COLLECTION, update_moving_mean)\n        tf.add_to_collection(BN_COLLECTION, update_moving_variance)\n\n        # call the normalisation function\n        if is_training or use_local_stats:\n            # with tf.control_dependencies(\n            #         [update_moving_mean, update_moving_variance]):\n            outputs = tf.nn.batch_normalization(\n                inputs, mean, variance,\n                beta, gamma, self.eps, name=\'batch_norm\')\n        else:\n            outputs = tf.nn.batch_normalization(\n                inputs, moving_mean, moving_variance,\n                beta, gamma, self.eps, name=\'batch_norm\')\n        outputs.set_shape(inputs.get_shape())\n        return outputs\n\n        # # Regularizers are not currently supported for fused batch norm.\n        # return tf.contrib.layers.batch_norm(\n        #     inputs,\n        #     decay=self.moving_decay,\n        #     center=True,\n        #     scale=True,\n        #     epsilon=self.eps,\n        #     activation_fn=None,\n        #     param_initializers=self.initializers,\n        #     param_regularizers=self.regularizers,\n        #     updates_collections=tf.GraphKeys.UPDATE_OPS,\n        #     is_training=is_training,\n        #     reuse=None,\n        #     variables_collections=[tf.GraphKeys.MOVING_AVERAGE_VARIABLES,\n        #                            tf.GraphKeys.GLOBAL_VARIABLES],\n        #     outputs_collections=None,\n        #     trainable=True,\n        #     batch_weights=None,\n        #     fused=False,\n        #     data_format=\'NHWC\',\n        #     zero_debias_moving_mean=False,\n        #     scope=None)\n\n\nclass InstanceNormLayer(TrainableLayer):\n    """"""\n    Instance normalisation layer, wrapper of `tf.contrib.layers.instance_norm`.\n    """"""\n    def __init__(self, eps=1e-6, gamma_initializer=None, name=\'instance_norm\'):\n        TrainableLayer.__init__(self, name=name)\n        self.eps = eps\n        self.gamma_initializer = gamma_initializer\n\n    def layer_op(self, inputs):\n        if self.gamma_initializer is None:\n            self.gamma_initializer = tf.constant_initializer(1.0)\n        return tf.contrib.layers.instance_norm(\n            inputs,\n            center=True,\n            scale=True,\n            epsilon=self.eps,\n            param_initializers={\'gamma\': self.gamma_initializer},\n            reuse=None,\n            variables_collections=None,\n            outputs_collections=None,\n            trainable=True,\n            data_format=\'NHWC\',\n            scope=None)\n'"
niftynet/layer/channel_sparse_convolution.py,52,"b'# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import, print_function\r\n\r\nimport math\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.training import moving_averages\r\n\r\nimport niftynet.layer.bn\r\nimport niftynet.layer.convolution\r\nimport niftynet.layer.deconvolution\r\nfrom niftynet.layer import layer_util\r\nfrom niftynet.layer.activation import ActiLayer\r\nfrom niftynet.layer.base_layer import TrainableLayer\r\nfrom niftynet.layer.deconvolution import infer_output_dims\r\n\r\nSUPPORTED_OP = {\'2D\': tf.nn.conv2d_transpose,\r\n                \'3D\': tf.nn.conv3d_transpose}\r\n\r\n\r\nclass ChannelSparseDeconvLayer(niftynet.layer.deconvolution.DeconvLayer):\r\n    """"""\r\n    Channel sparse convolutions perform convolutions over\r\n    a subset of image channels and generate a subset of output\r\n    channels. This enables spatial dropout without wasted computations\r\n    """"""\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super(ChannelSparseDeconvLayer, self).__init__(*args, **kwargs)\r\n\r\n    def layer_op(self, input_tensor, input_mask=None, output_mask=None):\r\n        """"""\r\n\r\n        :param input_tensor: image to convolve with kernel\r\n        :param input_mask: 1-Tensor with a binary mask of input channels to use\r\n            If this is None, all channels are used.\r\n        :param output_mask: 1-Tensor with a binary mask of output channels to\r\n            generate. If this is None, all channels are used and the number\r\n            of output channels is set at graph-creation time.\r\n        :return:\r\n        """"""\r\n\r\n        input_shape = input_tensor.shape.as_list()\r\n        if input_mask is None:\r\n            _input_mask = tf.ones([input_shape[-1]]) > 0\r\n        else:\r\n            _input_mask = input_mask\r\n\r\n        if output_mask is None:\r\n            n_sparse_output_chns = self.n_output_chns\r\n            _output_mask = tf.ones([self.n_output_chns]) > 0\r\n        else:\r\n            n_sparse_output_chns = tf.reduce_sum(\r\n                tf.cast(output_mask, tf.float32))\r\n            _output_mask = output_mask\r\n\r\n        n_full_input_chns = _input_mask.shape.as_list()[0]\r\n        spatial_rank = layer_util.infer_spatial_rank(input_tensor)\r\n\r\n        # initialize conv kernels/strides and then apply\r\n        w_full_size = np.vstack((\r\n            [self.kernel_size] * spatial_rank,\r\n            self.n_output_chns, n_full_input_chns)).flatten()\r\n        full_stride = np.vstack((\r\n            1, [self.stride] * spatial_rank, 1)).flatten()\r\n        deconv_kernel = tf.get_variable(\r\n            \'w\', shape=w_full_size.tolist(),\r\n            initializer=self.initializers[\'w\'],\r\n            regularizer=self.regularizers[\'w\'])\r\n        if spatial_rank == 2:\r\n            op_ = SUPPORTED_OP[\'2D\']\r\n        elif spatial_rank == 3:\r\n            op_ = SUPPORTED_OP[\'3D\']\r\n        else:\r\n            raise ValueError(\r\n                ""Only 2D and 3D spatial deconvolutions are supported"")\r\n\r\n        output_dim = infer_output_dims(input_shape[1],\r\n                                       self.stride,\r\n                                       self.kernel_size,\r\n                                       self.padding)\r\n        sparse_output_size = \\\r\n            [input_shape[0], [output_dim] * spatial_rank, n_sparse_output_chns]\r\n        sparse_output_size = tf.stack(sparse_output_size, 0)\r\n        output_tensor = op_(value=input_tensor,\r\n                            filter=deconv_kernel,\r\n                            output_shape=sparse_output_size,\r\n                            strides=full_stride.tolist(),\r\n                            padding=self.padding,\r\n                            name=\'deconv\')\r\n        if output_mask is None:\r\n            # If all output channels are used, we can specify\r\n            # the number of output channels which is useful for later layers\r\n            old_shape = output_tensor.shape.as_list()\r\n            old_shape[-1] = self.n_output_chns\r\n            output_tensor.set_shape(old_shape)\r\n        if not self.with_bias:\r\n            return output_tensor\r\n\r\n        # adding the bias term\r\n        bias_full_size = (self.n_output_chns,)\r\n        bias_term = tf.get_variable(\r\n            \'b\', shape=bias_full_size,\r\n            initializer=self.initializers[\'b\'],\r\n            regularizer=self.regularizers[\'b\'])\r\n        sparse_bias = tf.boolean_mask(bias_term, _output_mask)\r\n\r\n        output_tensor = tf.nn.bias_add(\r\n            output_tensor, sparse_bias, name=\'add_bias\')\r\n        return output_tensor\r\n\r\n\r\nclass ChannelSparseConvLayer(niftynet.layer.convolution.ConvLayer):\r\n    """"""\r\n    Channel sparse convolutions perform convolutions over\r\n    a subset of image channels and generate a subset of output\r\n    channels. This enables spatial dropout without wasted computations.\r\n    """"""\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super(ChannelSparseConvLayer, self).__init__(*args, **kwargs)\r\n\r\n    def layer_op(self, input_tensor, input_mask, output_mask):\r\n        """"""\r\n\r\n        :param input_tensor: image to convolve with kernel\r\n        :param input_mask: 1-Tensor with a binary mask of input channels to use\r\n            If this is None, all channels are used.\r\n        :param output_mask: 1-Tensor with a binary mask of output channels to\r\n            generate. If this is None, all channels are used and\r\n            the number of output channels is set at graph-creation time.\r\n        :return:\r\n        """"""\r\n        sparse_input_shape = input_tensor.shape.as_list()\r\n        if input_mask is None:\r\n            _input_mask = tf.ones([sparse_input_shape[-1]]) > 0\r\n        else:\r\n            _input_mask = input_mask\r\n        if output_mask is None:\r\n            _output_mask = tf.ones([self.n_output_chns]) > 0\r\n        else:\r\n            _output_mask = output_mask\r\n        n_full_input_chns = _input_mask.shape.as_list()[0]\r\n        spatial_rank = layer_util.infer_spatial_rank(input_tensor)\r\n        # initialize conv kernels/strides and then apply\r\n        w_full_size = layer_util.expand_spatial_params(\r\n            self.kernel_size, spatial_rank)\r\n        # expand kernel size to include number of features\r\n        w_full_size = w_full_size + (n_full_input_chns, self.n_output_chns)\r\n\r\n        full_stride = layer_util.expand_spatial_params(\r\n            self.stride, spatial_rank)\r\n\r\n        full_dilation = layer_util.expand_spatial_params(\r\n            self.dilation, spatial_rank)\r\n\r\n        conv_kernel = tf.get_variable(\r\n            \'w\', shape=w_full_size,\r\n            initializer=self.initializers[\'w\'],\r\n            regularizer=self.regularizers[\'w\'])\r\n\r\n        if spatial_rank == 2:\r\n            transpositions = [[3, 2, 1, 0], [1, 0, 2, 3], [3, 2, 0, 1]]\r\n        elif spatial_rank == 3:\r\n            transpositions = [[4, 3, 2, 1, 0], [1, 0, 2, 3, 4], [4, 3, 2, 0, 1]]\r\n        else:\r\n            raise NotImplementedError(""spatial rank not supported"")\r\n\r\n        sparse_kernel = tf.transpose(conv_kernel, transpositions[0])\r\n        sparse_kernel = tf.boolean_mask(sparse_kernel, _output_mask)\r\n        sparse_kernel = tf.transpose(sparse_kernel, transpositions[1])\r\n        sparse_kernel = tf.boolean_mask(sparse_kernel, _input_mask)\r\n        sparse_kernel = tf.transpose(sparse_kernel, transpositions[2])\r\n\r\n        output_tensor = tf.nn.convolution(input=input_tensor,\r\n                                          filter=sparse_kernel,\r\n                                          strides=full_stride,\r\n                                          dilation_rate=full_dilation,\r\n                                          padding=self.padding,\r\n                                          name=\'conv\')\r\n        if output_mask is None:\r\n            # If all output channels are used, we can specify\r\n            # the number of output channels which is useful for later layers\r\n            old_shape = output_tensor.shape.as_list()\r\n            old_shape[-1] = self.n_output_chns\r\n            output_tensor.set_shape(old_shape)\r\n\r\n        if not self.with_bias:\r\n            return output_tensor\r\n\r\n        # adding the bias term\r\n        bias_term = tf.get_variable(\r\n            \'b\', shape=self.n_output_chns,\r\n            initializer=self.initializers[\'b\'],\r\n            regularizer=self.regularizers[\'b\'])\r\n        sparse_bias = tf.boolean_mask(bias_term, output_mask)\r\n        output_tensor = tf.nn.bias_add(\r\n            output_tensor, sparse_bias, name=\'add_bias\')\r\n        return output_tensor\r\n\r\n\r\nclass ChannelSparseBNLayer(niftynet.layer.bn.BNLayer):\r\n    """"""\r\n    Channel sparse convolutions perform convolutions over\r\n    a subset of image channels and generate a subset of output\r\n    channels. This enables spatial dropout without wasted computations\r\n    """"""\r\n\r\n    def __init__(self, n_dense_channels, *args, **kwargs):\r\n        self.n_dense_channels = n_dense_channels\r\n        super(ChannelSparseBNLayer, self).__init__(*args, **kwargs)\r\n\r\n    def layer_op(self, inputs, is_training, mask, use_local_stats=False):\r\n        """"""\r\n\r\n        :param inputs: image to normalize. This typically represents a sparse\r\n            subset of channels from a sparse convolution.\r\n        :param is_training: boolean that is True during training.\r\n            When True, the layer uses batch statistics for normalization and\r\n            records a moving average of means and variances.\r\n            When False, the layer uses previously computed moving averages\r\n            for normalization.\r\n        :param mask: 1-Tensor with a binary mask identifying the sparse\r\n            channels represented in inputs\r\n        :param use_local_stats:\r\n        :return:\r\n        """"""\r\n\r\n        if mask is None:\r\n            mask = tf.ones([self.n_dense_channels]) > 0\r\n        else:\r\n            mask = mask\r\n\r\n        input_shape = inputs.shape\r\n        mask_shape = mask.shape\r\n        # operates on all dims except the last dim\r\n        params_shape = mask_shape[-1:]\r\n        assert params_shape[0] == self.n_dense_channels, \\\r\n            \'Mask size {} must match n_dense_channels {}.\'.format(\r\n                params_shape[0], self.n_dense_channels)\r\n        axes = list(range(input_shape.ndims - 1))\r\n        # create trainable variables and moving average variables\r\n        beta = tf.get_variable(\r\n            \'beta\',\r\n            shape=params_shape,\r\n            initializer=self.initializers[\'beta\'],\r\n            regularizer=self.regularizers[\'beta\'],\r\n            dtype=tf.float32, trainable=True)\r\n        gamma = tf.get_variable(\r\n            \'gamma\',\r\n            shape=params_shape,\r\n            initializer=self.initializers[\'gamma\'],\r\n            regularizer=self.regularizers[\'gamma\'],\r\n            dtype=tf.float32, trainable=True)\r\n        beta = tf.boolean_mask(beta, mask)\r\n        gamma = tf.boolean_mask(gamma, mask)\r\n\r\n        collections = [tf.GraphKeys.GLOBAL_VARIABLES]\r\n        moving_mean = tf.get_variable(\r\n            \'moving_mean\',\r\n            shape=params_shape,\r\n            initializer=self.initializers[\'moving_mean\'],\r\n            dtype=tf.float32, trainable=False, collections=collections)\r\n        moving_variance = tf.get_variable(\r\n            \'moving_variance\',\r\n            shape=params_shape,\r\n            initializer=self.initializers[\'moving_variance\'],\r\n            dtype=tf.float32, trainable=False, collections=collections)\r\n\r\n        # mean and var\r\n        mean, variance = tf.nn.moments(inputs, axes)\r\n        # only update masked moving averages\r\n        mean_update = tf.dynamic_stitch(\r\n            [tf.to_int32(tf.where(mask)[:, 0]),\r\n             tf.to_int32(tf.where(~mask)[:, 0])],\r\n            [mean,\r\n             tf.boolean_mask(moving_mean, ~mask)])\r\n        variance_update = tf.dynamic_stitch(\r\n            [tf.to_int32(tf.where(mask)[:, 0]),\r\n             tf.to_int32(tf.where(~mask)[:, 0])],\r\n            [variance,\r\n             tf.boolean_mask(moving_variance, ~mask)])\r\n        update_moving_mean = moving_averages.assign_moving_average(\r\n            moving_mean, mean_update, self.moving_decay).op\r\n        update_moving_variance = moving_averages.assign_moving_average(\r\n            moving_variance, variance_update, self.moving_decay).op\r\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_mean)\r\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_variance)\r\n\r\n        # call the normalisation function\r\n        if is_training or use_local_stats:\r\n            outputs = tf.nn.batch_normalization(\r\n                inputs, mean, variance,\r\n                beta, gamma, self.eps, name=\'batch_norm\')\r\n        else:\r\n            outputs = tf.nn.batch_normalization(\r\n                inputs,\r\n                tf.boolean_mask(moving_mean, mask),\r\n                tf.boolean_mask(moving_variance, mask),\r\n                beta, gamma, self.eps, name=\'batch_norm\')\r\n        outputs.set_shape(inputs.get_shape())\r\n        return outputs\r\n\r\n\r\nclass ChannelSparseConvolutionalLayer(TrainableLayer):\r\n    """"""\r\n    This class defines a composite layer with optional components::\r\n\r\n      channel sparse convolution ->\r\n      batchwise-spatial dropout ->\r\n      batch_norm ->\r\n      activation\r\n\r\n    The b_initializer and b_regularizer are applied to\r\n    the ChannelSparseConvLayer, the w_initializer and w_regularizer\r\n    are applied to the ChannelSparseConvLayer, the batch normalisation\r\n    layer, and the activation layer (for \'prelu\')\r\n    """"""\r\n\r\n    def __init__(self,\r\n                 n_output_chns,\r\n                 kernel_size=3,\r\n                 stride=1,\r\n                 dilation=1,\r\n                 padding=\'SAME\',\r\n                 with_bias=False,\r\n                 feature_normalization=\'batch\',\r\n                 acti_func=None,\r\n                 w_initializer=None,\r\n                 w_regularizer=None,\r\n                 b_initializer=None,\r\n                 b_regularizer=None,\r\n                 moving_decay=0.9,\r\n                 eps=1e-5,\r\n                 name=""conv""):\r\n\r\n        self.acti_func = acti_func\r\n        self.feature_normalization = feature_normalization\r\n        self.layer_name = \'{}\'.format(name)\r\n        if self.feature_normalization == \'batch\':\r\n            self.layer_name += \'_bn\'\r\n        if self.acti_func is not None:\r\n            self.layer_name += \'_{}\'.format(self.acti_func)\r\n        super(ChannelSparseConvolutionalLayer, self).__init__(\r\n            name=self.layer_name)\r\n\r\n        # for ConvLayer\r\n        self.n_output_chns = n_output_chns\r\n        self.kernel_size = kernel_size\r\n        self.stride = stride\r\n        self.dilation = dilation\r\n        self.padding = padding\r\n        self.with_bias = with_bias\r\n\r\n        # for BNLayer\r\n        self.moving_decay = moving_decay\r\n        self.eps = eps\r\n\r\n        self.initializers = {\r\n            \'w\': w_initializer if w_initializer else\r\n            niftynet.layer.convolution.default_w_initializer(),\r\n            \'b\': b_initializer if b_initializer else\r\n            niftynet.layer.convolution.default_b_initializer()}\r\n\r\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\r\n\r\n    def layer_op(self,\r\n                 input_tensor,\r\n                 input_mask=None,\r\n                 is_training=None,\r\n                 keep_prob=None):\r\n        conv_layer = ChannelSparseConvLayer(\r\n            n_output_chns=self.n_output_chns,\r\n            kernel_size=self.kernel_size,\r\n            stride=self.stride,\r\n            dilation=self.dilation,\r\n            padding=self.padding,\r\n            with_bias=self.with_bias,\r\n            w_initializer=self.initializers[\'w\'],\r\n            w_regularizer=self.regularizers[\'w\'],\r\n            b_initializer=self.initializers[\'b\'],\r\n            b_regularizer=self.regularizers[\'b\'],\r\n            name=\'conv_\')\r\n        if keep_prob is not None:\r\n            output_mask = \\\r\n                tf.to_float(tf.random_shuffle(tf.range(self.n_output_chns))) \\\r\n                < keep_prob * self.n_output_chns\r\n            n_output_ch = math.ceil(keep_prob * self.n_output_chns)\r\n        else:\r\n            output_mask = tf.ones([self.n_output_chns]) > 0\r\n            n_output_ch = self.n_output_chns\r\n\r\n        output_tensor = conv_layer(input_tensor, input_mask, output_mask)\r\n        output_tensor.set_shape(\r\n            output_tensor.shape.as_list()[:-1] + [n_output_ch])\r\n\r\n        if self.feature_normalization == \'batch\':\r\n            if is_training is None:\r\n                raise ValueError(\'For batch norm, you must set the `is_training` argument.\')\r\n            bn_layer = ChannelSparseBNLayer(\r\n                self.n_output_chns,\r\n                regularizer=self.regularizers[\'w\'],\r\n                moving_decay=self.moving_decay,\r\n                eps=self.eps,\r\n                name=\'bn_\')\r\n            output_tensor = bn_layer(output_tensor, is_training, output_mask)\r\n\r\n        if self.acti_func is not None:\r\n            acti_layer = ActiLayer(\r\n                func=self.acti_func,\r\n                regularizer=self.regularizers[\'w\'],\r\n                name=\'acti_\')\r\n            output_tensor = acti_layer(output_tensor)\r\n        return output_tensor, output_mask\r\n'"
niftynet/layer/convolution.py,14,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.activation import ActiLayer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer, InstanceNormLayer\nfrom niftynet.layer.gn import GNLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_PADDING = set([\'SAME\', \'VALID\', \'REFLECT\', \'SYMMETRIC\', \'CONSTANT\'])\n\n\ndef default_w_initializer():\n    def _initializer(shape, dtype, partition_info):\n        stddev = np.sqrt(2.0 / np.prod(shape[:-1]))\n        from tensorflow.python.ops import random_ops\n        return random_ops.truncated_normal(shape, 0.0, stddev, dtype=tf.float32)\n        # return tf.truncated_normal_initializer(\n        #    mean=0.0, stddev=stddev, dtype=tf.float32)\n\n    return _initializer\n\n\ndef default_b_initializer():\n    return tf.constant_initializer(0.0)\n\n\nclass ConvLayer(TrainableLayer):\n    """"""\n    This class defines a simple convolution with an optional bias term.\n    Please consider ``ConvolutionalLayer`` if batch_norm and activation\n    are also used.\n    """"""\n\n    def __init__(self,\n                 n_output_chns,\n                 kernel_size=3,\n                 stride=1,\n                 dilation=1,\n                 padding=\'SAME\',\n                 with_bias=False,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 padding_constant=0,\n                 name=\'conv\'):\n        """"""\n        :param padding_constant: a constant applied in padded convolution\n        (see also tf.pad)\n        """"""\n\n        super(ConvLayer, self).__init__(name=name)\n\n        self.padding = look_up_operations(padding.upper(), SUPPORTED_PADDING)\n        self.n_output_chns = int(n_output_chns)\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.with_bias = with_bias\n        self.padding_constant = padding_constant\n\n        self.initializers = {\n            \'w\': w_initializer if w_initializer else default_w_initializer(),\n            \'b\': b_initializer if b_initializer else default_b_initializer()}\n\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, input_tensor):\n        input_shape = input_tensor.shape.as_list()\n        n_input_chns = input_shape[-1]\n        spatial_rank = layer_util.infer_spatial_rank(input_tensor)\n\n        # initialize conv kernels/strides and then apply\n        w_full_size = layer_util.expand_spatial_params(\n            self.kernel_size, spatial_rank)\n        # expand kernel size to include number of features\n        w_full_size = w_full_size + (n_input_chns, self.n_output_chns)\n        full_stride = layer_util.expand_spatial_params(\n            self.stride, spatial_rank)\n        full_dilation = layer_util.expand_spatial_params(\n            self.dilation, spatial_rank)\n\n        conv_kernel = tf.get_variable(\n            \'w\', shape=w_full_size,\n            initializer=self.initializers[\'w\'],\n            regularizer=self.regularizers[\'w\'])\n        if self.padding in (\'VALID\', \'SAME\'):\n            output_tensor = tf.nn.convolution(input=input_tensor,\n                                              filter=conv_kernel,\n                                              strides=full_stride,\n                                              dilation_rate=full_dilation,\n                                              padding=self.padding,\n                                              name=\'conv\')\n        else:\n            output_tensor = _extended_convolution(\n                input_tensor,\n                conv_kernel,\n                full_stride,\n                full_dilation,\n                self.padding,\n                constant=self.padding_constant)\n\n        if not self.with_bias:\n            return output_tensor\n\n        # adding the bias term\n        bias_term = tf.get_variable(\n            \'b\', shape=self.n_output_chns,\n            initializer=self.initializers[\'b\'],\n            regularizer=self.regularizers[\'b\'])\n        output_tensor = tf.nn.bias_add(output_tensor,\n                                       bias_term,\n                                       name=\'add_bias\')\n        return output_tensor\n\n\nclass ConvolutionalLayer(TrainableLayer):\n    """"""\n    This class defines a composite layer with optional components::\n\n        convolution -> feature_normalization (default batch norm) -> activation -> dropout\n\n    The b_initializer and b_regularizer are applied to the ConvLayer\n    The w_initializer and w_regularizer are applied to the ConvLayer,\n    the feature normalization layer, and the activation layer (for \'prelu\')\n    """"""\n\n    def __init__(self,\n                 n_output_chns,\n                 kernel_size=3,\n                 stride=1,\n                 dilation=1,\n                 padding=\'SAME\',\n                 with_bias=False,\n                 feature_normalization=\'batch\',\n                 group_size=-1,\n                 acti_func=None,\n                 preactivation=False,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 moving_decay=0.9,\n                 eps=1e-5,\n                 padding_constant=0,\n                 name=""conv""):\n        """"""\n        :param padding_constant: constant applied with CONSTANT padding\n        """"""\n\n        self.acti_func = acti_func\n        self.feature_normalization = feature_normalization\n        self.group_size = group_size\n        self.preactivation = preactivation\n        self.layer_name = \'{}\'.format(name)\n        if self.feature_normalization != \'group\' and group_size > 0:\n            raise ValueError(\'You cannot have a group_size > 0 if not using group norm\')\n        elif self.feature_normalization == \'group\' and group_size <= 0:\n            raise ValueError(\'You cannot have a group_size <= 0 if using group norm\')\n\n        if self.feature_normalization == \'batch\':\n            self.layer_name += \'_bn\'\n        elif self.feature_normalization == \'group\':\n            self.layer_name += \'_gn\'\n        elif self.feature_normalization == \'instance\':\n            self.layer_name += \'_in\'\n        if self.acti_func is not None:\n            self.layer_name += \'_{}\'.format(self.acti_func)\n        super(ConvolutionalLayer, self).__init__(name=self.layer_name)\n\n        # for ConvLayer\n        self.n_output_chns = n_output_chns\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.padding = padding\n        self.with_bias = with_bias\n        self.padding_constant = padding_constant\n\n        # for BNLayer\n        self.moving_decay = moving_decay\n        self.eps = eps\n\n        self.initializers = {\n            \'w\': w_initializer if w_initializer else default_w_initializer(),\n            \'b\': b_initializer if b_initializer else default_b_initializer()}\n\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, input_tensor, is_training=None, keep_prob=None):\n        conv_layer = ConvLayer(n_output_chns=self.n_output_chns,\n                               kernel_size=self.kernel_size,\n                               stride=self.stride,\n                               dilation=self.dilation,\n                               padding=self.padding,\n                               with_bias=self.with_bias,\n                               w_initializer=self.initializers[\'w\'],\n                               w_regularizer=self.regularizers[\'w\'],\n                               b_initializer=self.initializers[\'b\'],\n                               b_regularizer=self.regularizers[\'b\'],\n                               padding_constant=self.padding_constant,\n                               name=\'conv_\')\n\n        if self.feature_normalization == \'batch\':\n            if is_training is None:\n                raise ValueError(\'is_training argument should be \'\n                                 \'True or False unless feature_normalization is False\')\n            bn_layer = BNLayer(\n                regularizer=self.regularizers[\'w\'],\n                moving_decay=self.moving_decay,\n                eps=self.eps,\n                name=\'bn_\')\n        elif self.feature_normalization == \'instance\':\n            in_layer = InstanceNormLayer(eps=self.eps, name=\'in_\')\n        elif self.feature_normalization == \'group\':\n            gn_layer = GNLayer(\n                regularizer=self.regularizers[\'w\'],\n                group_size=self.group_size,\n                eps=self.eps,\n                name=\'gn_\')\n        if self.acti_func is not None:\n            acti_layer = ActiLayer(\n                func=self.acti_func,\n                regularizer=self.regularizers[\'w\'],\n                name=\'acti_\')\n\n        if keep_prob is not None:\n            dropout_layer = ActiLayer(func=\'dropout\', name=\'dropout_\')\n\n        def activation(output_tensor):\n            if self.feature_normalization == \'batch\':\n                output_tensor = bn_layer(output_tensor, is_training)\n            elif self.feature_normalization == \'instance\':\n                output_tensor = in_layer(output_tensor)\n            elif self.feature_normalization == \'group\':\n                output_tensor = gn_layer(output_tensor)\n            if self.acti_func is not None:\n                output_tensor = acti_layer(output_tensor)\n            if keep_prob is not None:\n                output_tensor = dropout_layer(output_tensor,\n                                              keep_prob=keep_prob)\n            return output_tensor\n\n        if self.preactivation:\n            output_tensor = conv_layer(activation(input_tensor))\n        else:\n            output_tensor = activation(conv_layer(input_tensor))\n\n        return output_tensor\n\n\ndef _compute_pad_size(input_dim_size, output_dim_size, kernel_dim_size,\n                      stride, dilation):\n    """"""\n    Computes the size of the pad using the formula given in TF\'s conv_ops.cc.\n    :return: the one-sided pad size\n    """"""\n\n    return ((output_dim_size - 1)*stride + (kernel_dim_size - 1)*dilation + 2\n            - input_dim_size)//2\n\n\ndef _extended_convolution(input_tensor,\n                          kernel,\n                          strides,\n                          dilations,\n                          padding,\n                          constant=0,\n                          name=\'extended_convolution\'):\n    """"""\n    A simple wrapper for tf.nn.convolution that first expands the input tensor\n    by sampling at discrete locations in the original tensor then invokes\n    the original convolution operation on the expanded tensor, and finally\n    extracts a suitable output tensor from the output of the convolution of\n    the expanded tensor.\n    :param input_tensor: original convolution input tensor\n    :param kernel: convolution kernel\n    :param strides: strided convolution strides (one per spatial dimension)\n    :param dilations: dilated convolution dilation factors\n    (one per spatial dimension)\n    :param padding: a string specifying the type of padding to apply\n    :param constant: a padding constant (only read in the case of constant\n    padding)\n    :param name: a name for the operation\n    :return: a convolution result of the same size as the input tensor\n    """"""\n\n    input_shape = input_tensor.shape.as_list()\n    batch_size = input_shape[0]\n    input_shape = input_shape[1:-1]\n    kernel_shape = kernel.shape.as_list()\n    nof_output_features = kernel_shape[-1]\n    kernel_shape = kernel_shape[:-2]\n\n    if any(i is None or i < 0 or k is None or k < 0\n           for i, k in zip(input_shape, kernel_shape)):\n        raise ValueError(\'The dimensions of the input tensor and the filter\'\n                         \' must be known in advance for this operation to \'\n                         \'work.\')\n\n    output_shape = [int(math.ceil(i/s)) for i, s in zip(input_shape, strides)]\n    output_shape = [batch_size] + output_shape + [nof_output_features]\n\n    dimpads = [0]\n    for i, k, s, d in zip(input_shape, kernel_shape, strides, dilations):\n        pad = _compute_pad_size(i, int(math.ceil(i/s)), k, s, d)\n        dimpads.append(pad)\n    dimpads += [0]\n\n    # Cannot pad by more than 1 dimension size => repeatedly pad\n    if padding in (\'REFLECT\', \'SYMMETRIC\'):\n        padded_input = input_tensor\n        offset = int(padding == \'REFLECT\')\n\n        while min(o - i - 2*p for o, i, p in zip(\n                padded_input.shape.as_list()[1:-1],\n                input_shape,\n                dimpads[1:-1])) < 0:\n            effective_pad = [(0, 0)]\n            padded_shape = padded_input.shape.as_list()[1:-1]\n            for i in range(len(input_shape)):\n                epad = min((input_shape[i] + 2*dimpads[1+i] - padded_shape[i])//2,\n                           padded_shape[i] - offset)\n                epad = max(epad, 0)\n                effective_pad.append((epad, epad))\n            effective_pad += [(0, 0)]\n\n            assert max(e for e, _ in effective_pad) > 0\n\n            padded_input = tf.pad(padded_input,\n                                  effective_pad,\n                                  mode=padding)\n    else:\n        padded_input = tf.pad(input_tensor,\n                              [(d, d) for d in dimpads],\n                              mode=padding,\n                              constant_values=constant)\n\n    conv_output = tf.nn.convolution(input=padded_input,\n                                    filter=kernel,\n                                    strides=strides,\n                                    dilation_rate=dilations,\n                                    padding=\'SAME\',\n                                    name=\'conv_\' + name)\n\n    conv_output_shape = conv_output.shape.as_list()\n    out_pad = [0]\n    out_pad += [(o - i)//2 for i, o in zip(output_shape[1:-1], conv_output_shape[1:-1])]\n    out_pad += [0]\n\n    return tf.slice(conv_output, out_pad, output_shape) if max(out_pad) > 0 \\\n        else conv_output\n'"
niftynet/layer/crf.py,81,"b'# -*- coding: utf-8 -*-\n""""""\nRe-implementation of [1] in Tensorflow for volumetric image processing.\n\n[1] Zheng et al.\n""Conditional random fields as recurrent neural networks."" ICCV 2015.\nhttps://arxiv.org/abs/1502.03240\n""""""\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.layer_util import infer_spatial_rank, expand_spatial_params\n\n\nclass CRFAsRNNLayer(TrainableLayer):\n    """"""\n    This class defines a layer implementing CRFAsRNN described in [1] using\n    a bilateral and a spatial kernel as in [2].\n    Essentially, this layer smooths its input based on a distance in a feature\n    space comprising spatial and feature dimensions.\n    High-dimensional Gaussian filtering adapted from [3].\n\n    [1] Zheng et al., https://arxiv.org/abs/1502.03240\n    [2] Krahenbuhl and Koltun, https://arxiv.org/pdf/1210.5644.pdf\n    [3] Adam et al., https://graphics.stanford.edu/papers/permutohedral/\n    """"""\n\n    def __init__(self,\n                 alpha=5.,\n                 beta=5.,\n                 gamma=5.,\n                 T=5,\n                 aspect_ratio=None,\n                 mu_init=None,\n                 w_init=None,\n                 name=""crf_as_rnn""):\n        """"""\n        Currently this layer supports spatial ND dense CRF with CPU only.\n        To place the layer on CPU::\n\n            with tf.device(\'/cpu:0\'):\n                crf_layer = CRFAsRNNLayer()\n                crf_output = crf_layer(features, raw_logits)\n\n        To ensure backpropagations during training are placed on CPU as well,\n        the optimiser should be used with argument\n        ``colocate_gradients_with_ops=True``, e.g.,::\n\n            train_op = tf.train.GradientDescentOptimizer(.5).minimise(\n                training_loss, colocate_gradients_with_ops=True)\n\n\n\n        :param alpha: bandwidth for spatial coordinates in bilateral kernel.\n                      Higher values cause more spatial blurring\n        :param beta: bandwidth for feature coordinates in bilateral kernel\n                      Higher values cause more feature blurring\n        :param gamma: bandwidth for spatial coordinates in spatial kernel\n                      Higher values cause more spatial blurring\n        :param T: number of stacked layers in the RNN\n        :param aspect_ratio: spacing of adjacent voxels\n            (allows isotropic spatial smoothing when voxels are not isotropic)\n        :param mu_init: initial compatibility matrix [n_classes x n_classes]\n            default value: `-1.0 * eye(n_classes)`\n        :param w_init: initial kernel weights [2 x n_classes]\n            where w_init[0] are the weights for the bilateral kernel,\n                  w_init[1] are the weights for the spatial kernel.\n            default value: `[ones(n_classes), ones(n_classes)]`\n        :param name:\n        """"""\n\n        super(CRFAsRNNLayer, self).__init__(name=name)\n        self._alpha = alpha\n        self._beta = beta\n        self._gamma = gamma\n        self._T = T\n        self._aspect_ratio = aspect_ratio\n        self._mu_init = mu_init\n        self._w_init = w_init\n\n        assert self._alpha > 0, \'alpha should be positive\'\n        assert self._beta > 0, \'beta should be positive\'\n        assert self._gamma > 0, \'gamma should be positive\'\n\n    def layer_op(self, I, U):\n        """"""\n        Compute `T` iterations of mean field update given a dense CRF.\n\n        This layer maintains trainable CRF model parameters\n        (a compatibility function and `m` kernel weights).\n\n        :param I: feature maps used in the dense pairwise term of CRF\n        :param U: activation maps used in the unary term of CRF (before softmax)\n        :return: Maximum a posteriori labeling (before softmax)\n        """"""\n\n        spatial_rank = infer_spatial_rank(U)\n        all_shape = U.shape.as_list()\n        batch_size, spatial_shape, n_ch = \\\n            all_shape[0], all_shape[1:-1], all_shape[-1]\n        n_feat = I.shape.as_list()[-1]\n        if self._aspect_ratio is None:\n            self._aspect_ratio = [1.] * spatial_rank\n        self._aspect_ratio = expand_spatial_params(\n            self._aspect_ratio, spatial_rank, float)\n\n        # constructing the scaled regular grid\n        spatial_grid = tf.meshgrid(\n            *[np.arange(i, dtype=np.float32) * a\n                for i, a in zip(spatial_shape, self._aspect_ratio)],\n            indexing=\'ij\')\n        spatial_coords = tf.stack(spatial_grid[::-1], spatial_rank)\n        spatial_coords = tf.tile(\n            tf.expand_dims(spatial_coords, 0),\n            [batch_size] + [1] * spatial_rank + [1])\n\n        # concatenating spatial coordinates and features\n        # (and squeeze spatially)\n        # for the bilateral kernel\n        bilateral_coords = tf.reshape(\n            tf.concat([spatial_coords / self._alpha, I / self._beta], -1),\n            [batch_size, -1, n_feat + spatial_rank])\n        # for the spatial kernel\n        spatial_coords = tf.reshape(\n            spatial_coords / self._gamma,\n            [batch_size, -1, spatial_rank])\n\n        # Build permutohedral structures for smoothing\n        permutohedrals = [\n            permutohedral_prepare(coords)\n            for coords in (bilateral_coords, spatial_coords)]\n\n        # squeeze the spatial shapes and recover them in the end\n        U = tf.reshape(U, [batch_size, -1, n_ch])\n        n_voxels = U.shape.as_list()[1]\n        # normalisation factor\n        norms = []\n        for idx, permutohedral in enumerate(permutohedrals):\n            spatial_norm = _permutohedral_gen(\n                permutohedral,\n                tf.ones((batch_size, n_voxels, 1)),\n                \'spatial_norms\' + str(idx))\n            spatial_norm.set_shape([batch_size, n_voxels, 1])\n            spatial_norm = 1.0 / tf.sqrt(spatial_norm + 1e-20)\n            norms.append(spatial_norm)\n\n        # trainable compatibility matrix mu (initialised as identity * -1)\n        mu_shape = [n_ch, n_ch]\n        if self._mu_init is None:\n            self._mu_init = -np.eye(n_ch)\n        self._mu_init = np.reshape(self._mu_init, mu_shape)\n        mu = tf.get_variable(\n            \'Compatibility\',\n            initializer=tf.constant(self._mu_init, dtype=tf.float32))\n\n        # trainable kernel weights\n        weight_shape = [n_ch]\n        if self._w_init is None:\n            self._w_init = [np.ones(n_ch), np.ones(n_ch)]\n        self._w_init = [\n            np.reshape(_w, weight_shape) for _w in self._w_init]\n        kernel_weights = [tf.get_variable(\n            \'FilterWeights{}\'.format(idx),\n            initializer=tf.constant(self._w_init[idx], dtype=tf.float32))\n            for idx, k in enumerate(permutohedrals)]\n\n        H1 = U\n        for t in range(self._T):\n            H1 = ftheta(U, H1, permutohedrals, mu, kernel_weights, norms,\n                        name=\'{}{}\'.format(self.name, t))\n        return tf.reshape(H1, all_shape)\n\n\ndef ftheta(U, H1, permutohedrals, mu, kernel_weights, norms, name):\n    """"""\n    A mean-field update\n\n    :param U: the unary potentials (before softmax)\n    :param H1: the previous mean-field approximation to be updated\n    :param permutohedrals: fixed position vectors for fast filtering\n    :param mu: compatibility function\n    :param kernel_weights: weights bilateral/spatial kernels\n    :param norms: precomputed normalisation factor\n    :param name: layer name\n    :return: updated mean-field distribution\n    """"""\n    unary_shape = U.shape.as_list()\n    n_ch = unary_shape[-1]\n    H1 = tf.nn.softmax(H1)\n    Q1 = 0\n    for idx, permutohedral in enumerate(permutohedrals):\n        # Message Passing\n        Q = _permutohedral_gen(permutohedral, H1 * norms[idx], name + str(idx))\n        Q.set_shape(unary_shape)\n        # Weighting Filtered Outputs\n        Q1 += Q * kernel_weights[idx] * norms[idx]\n\n    # Compatibility Transform, Adding Unary Potentials\n    # output logits, not the softmax\n    Q1 = tf.reshape(tf.matmul(tf.reshape(Q1, [-1, n_ch]), mu), unary_shape)\n    return U - Q1\n\n\ndef permutohedral_prepare(position_vectors):\n    """"""\n    Embedding the position vectors in a high-dimensional space,\n    the lattice points are stored in hash tables.\n\n    The function computes:\n    - translation by the nearest reminder-0\n    - ranking permutation to the canonical simplex\n    - barycentric weights in the canonical simplex\n\n    :param position_vectors: N x d position\n    :return: barycentric weights, blur neighbours points in the hyperplane\n    """"""\n    batch_size, n_voxels, n_ch = position_vectors.shape.as_list()\n    n_ch_1 = n_ch + 1\n\n    # reshaping batches and voxels into one dimension\n    # means we can use 1D gather and hashing easily\n    position_vectors = tf.reshape(position_vectors, [-1, n_ch])\n\n    # Generate position vectors in lattice space\n    # first rotate position into the (n_ch+1)-dimensional hyperplane\n    inv_std_dev = np.sqrt(2 / 3.) * n_ch_1\n    scale_factor = tf.constant([\n        inv_std_dev / np.sqrt((i + 1) * (i + 2)) for i in range(n_ch)])\n    Ex = [None] * n_ch_1\n    cum_sum = 0.0\n    for dit in range(n_ch, 0, -1):\n        scaled_vectors = position_vectors[:, dit - 1] * scale_factor[dit - 1]\n        Ex[dit] = cum_sum - scaled_vectors * dit\n        cum_sum += scaled_vectors\n    Ex[0] = cum_sum\n    Ex = tf.stack(Ex, -1)\n\n    # Compute coordinates\n    # Get closest remainder-0 point\n    v = tf.to_int32(tf.round(Ex / float(n_ch_1)))\n    rem0 = v * n_ch_1\n\n    # Find the simplex we are in and store it in rank\n    # (where rank describes what position coordinate i has\n    # in the sorted order of the features values).\n    # This can be done more efficiently\n    # if necessary following the permutohedral paper.\n    index = tf.nn.top_k(Ex - tf.to_float(rem0), n_ch_1, sorted=True).indices\n    rank = tf.nn.top_k(-index, n_ch_1, sorted=True).indices\n\n    # if the point doesn\'t lie on the plane (sum != 0) bring it back\n    # (sum(v) != 0)  meaning off the plane\n    rank = rank + tf.reduce_sum(v, 1, True)\n    add_minus_sub = tf.to_int32(rank < 0) - tf.to_int32(rank > n_ch)\n    add_minus_sub *= n_ch_1\n    rem0 = rem0 + add_minus_sub\n    rank = rank + add_minus_sub\n\n    # Compute the barycentric coordinates (p.10 in [Adams et al 2010])\n    v2 = (Ex - tf.to_float(rem0)) / float(n_ch_1)\n    # CRF2RNN uses the calculated ranks to get v2 sorted in O(n_ch) time\n    # We cheat here by using the easy to implement\n    # but slower method of sorting again in O(n_ch log n_ch)\n    # we might get this even more efficient\n    # if we correct the original sorted data above\n    v_sorted = -tf.nn.top_k(-v2, n_ch_1, sorted=True).values\n    # weighted against the canonical simplex vertices\n    barycentric = \\\n        v_sorted - tf.concat([v_sorted[:, -1:] - 1., v_sorted[:, :-1]], 1)\n\n    # Compute all vertices and their offset\n    def _simple_hash(key):\n        # WARNING: This hash function does not guarantee\n        # uniqueness of different position_vectors\n        hash_vector = np.power(\n            int(np.floor(np.power(tf.int64.max, 1. / (n_ch + 2)))),\n            [range(1, n_ch_1)])\n        hash_vector = tf.constant(hash_vector, dtype=tf.int64)\n        return tf.reduce_sum(tf.to_int64(key) * hash_vector, 1)\n\n    # This is done so if the user had TF 1.12.1 or a new version the code\n    # does not brake. First part of the try is for TF 1.12.1 where the\n    # deleted_key keyword was missing, while the second is just a normal\n    # usage for TF 1.13.1>=\n    try:\n        hash_table = tf.contrib.lookup.MutableDenseHashTable(\n            tf.int64, tf.int64,\n            default_value=tf.constant([-1] * 100, dtype=tf.int64),\n            empty_key=-2,\n            initial_num_buckets=8,\n            checkpoint=False\n        )\n    except TypeError:\n        hash_table = tf.contrib.lookup.MutableDenseHashTable(\n            tf.int64, tf.int64,\n            default_value=tf.constant([-1] * n_ch, dtype=tf.int64),\n            empty_key=-3,\n            deleted_key=-2,\n            initial_num_buckets=8,\n            checkpoint=False\n        )\n    try:\n        index_table = tf.contrib.lookup.MutableDenseHashTable(\n            tf.int64, tf.int64,\n            default_value=0,\n            empty_key=-1,\n            initial_num_buckets=8,\n            checkpoint=False\n        )\n    except TypeError:\n        index_table = tf.contrib.lookup.MutableDenseHashTable(\n            tf.int64, tf.int64,\n            default_value=0,\n            empty_key=-2,\n            deleted_key=-1,\n            initial_num_buckets=8,\n            checkpoint=False\n        )\n\n    # canonical simplex (p.4 in [Adams et al 2010])\n    canonical = \\\n        [[i] * (n_ch_1 - i) + [i - n_ch - 1] * i for i in range(n_ch_1)]\n\n    insert_ops = []\n    loc = [None] * n_ch_1\n    loc_hash = [None] * n_ch_1\n    for scit in range(n_ch_1):\n        # Compute the location of the lattice point explicitly\n        # (all but the last coordinate -\n        #  it\'s redundant because they sum to zero)\n        loc[scit] = tf.gather(canonical[scit], rank[:, :-1]) + rem0[:, :-1]\n        loc_hash[scit] = _simple_hash(loc[scit])\n        insert_ops.append(\n            hash_table.insert(loc_hash[scit], tf.to_int64(loc[scit])))\n\n    with tf.control_dependencies(insert_ops):\n        fused_loc_hash, fused_loc = hash_table.export()\n        is_good_key = tf.where(tf.not_equal(fused_loc_hash, -2))[:, 0]\n        fused_loc = tf.gather(fused_loc, is_good_key)\n        fused_loc_hash = tf.gather(fused_loc_hash, is_good_key)\n\n    # The additional index hash table is used to\n    # linearise the hash table so that we can `tf.scatter` and `tf.gather`\n    # (range_id 0 reserved for the indextable\'s default value)\n    range_id = tf.range(\n        1, tf.size(fused_loc_hash, out_type=tf.int64) + 1, dtype=tf.int64)\n    range_id = tf.expand_dims(range_id, 1)\n    insert_indices = index_table.insert(fused_loc_hash, range_id)\n\n    # linearised [batch, spatial_dim] indices\n    # where in the splat variable each simplex vertex is\n    batch_index = tf.range(batch_size, dtype=tf.int32)\n    batch_index = tf.expand_dims(batch_index, 1)\n    batch_index = tf.tile(batch_index, [1, n_voxels])\n    batch_index = tf.to_int64(tf.reshape(batch_index, [-1]))\n\n    indices = [None] * n_ch_1\n    blur_neighbours1 = [None] * n_ch_1\n    blur_neighbours2 = [None] * n_ch_1\n    with tf.control_dependencies([insert_indices]):\n        for dit in range(n_ch_1):\n            # the neighbors along each axis.\n            offset = [n_ch if i == dit else -1 for i in range(n_ch)]\n            offset = tf.constant(offset, dtype=tf.int64)\n            blur_neighbours1[dit] = \\\n                index_table.lookup(_simple_hash(fused_loc + offset))\n            blur_neighbours2[dit] = \\\n                index_table.lookup(_simple_hash(fused_loc - offset))\n            indices[dit] = tf.stack([\n                index_table.lookup(loc_hash[dit]), batch_index], 1)\n\n    return barycentric, blur_neighbours1, blur_neighbours2, indices\n\n\ndef permutohedral_compute(data_vectors,\n                          barycentric,\n                          blur_neighbours1,\n                          blur_neighbours2,\n                          indices,\n                          name,\n                          reverse):\n    """"""\n    Splat, Gaussian blur, and slice\n\n    :param data_vectors: value map to be filtered\n    :param barycentric: embedding coordinates\n    :param blur_neighbours1: first neighbours\' coordinates relative to indices\n    :param blur_neighbours2: second neighbours\' coordinates relative to indices\n    :param indices: corresponding locations of data_vectors\n    :param name: layer name\n    :param reverse: transpose the Gaussian kernel if True\n    :return: filtered data_vectors (sliced to the original space)\n    """"""\n\n    num_simplex_corners = barycentric.shape.as_list()[-1]\n    n_ch = num_simplex_corners - 1\n    batch_size, n_voxels, n_ch_data = data_vectors.shape.as_list()\n    data_vectors = tf.reshape(data_vectors, [-1, n_ch_data])\n\n    # Splatting\n    with tf.variable_scope(name):\n        splat = tf.contrib.framework.local_variable(\n            tf.constant(0.0), validate_shape=False, name=\'splatbuffer\')\n\n    # with tf.control_dependencies([splat.initialized_value()]):\n    initial_splat = tf.zeros(\n        [tf.shape(blur_neighbours1[0])[0] + 1, batch_size, n_ch_data])\n    reset_splat = tf.assign(splat, initial_splat, validate_shape=False)\n    with tf.control_dependencies([reset_splat]):\n        for scit in range(num_simplex_corners):\n            data = data_vectors * barycentric[:, scit:scit + 1]\n            splat = tf.scatter_nd_add(splat, indices[scit], data)\n\n    # Blur with 1D kernels\n    for dit in range(n_ch, -1, -1) if reverse else range(n_ch + 1):\n        b1 = tf.gather(splat, blur_neighbours1[dit])\n        b3 = tf.gather(splat, blur_neighbours2[dit])\n        splat = tf.concat([\n            splat[:1, ...], splat[1:, ...] + 0.5 * (b1 + b3)], 0)\n\n    # Slice\n    sliced = 0.0\n    # Alpha is a magic scaling constant from CRFAsRNN code\n    alpha = 1. / (1. + np.power(2., -n_ch))\n    for scit in range(0, num_simplex_corners):\n        sliced += tf.gather_nd(splat, indices[scit]) * \\\n                  barycentric[:, scit:scit + 1] * alpha\n    sliced = tf.reshape(sliced, [batch_size, n_voxels, n_ch_data])\n    return sliced\n\n\ndef _py_func_with_grads(func, inp, Tout, stateful=True, name=None, grad=None):\n    """"""\n    To get this to work with automatic differentiation\n    we use a hack attributed to Sergey Ioffe\n    mentioned here: http://stackoverflow.com/questions/36456436\n\n    Define custom _py_func_with_grads which takes also a grad op as argument:\n    from https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342\n\n    :param func:\n    :param inp:\n    :param Tout:\n    :param stateful:\n    :param name:\n    :param grad:\n    :return:\n    """"""\n    # Need to generate a unique name to avoid duplicates:\n    import uuid\n    rnd_name = \'PyFuncGrad\' + str(uuid.uuid4())\n    # tf.logging.info(\'CRFasRNN layer iteration {}\'.format(rnd_name))\n    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n    with tf.get_default_graph().gradient_override_map({""PyFunc"": rnd_name}):\n        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)[0]\n\n\ndef _gradient_stub(data_vectors,\n                   barycentric,\n                   blur_neighbours1,\n                   blur_neighbours2,\n                   indices,\n                   name):\n    """"""\n    This is a stub operator whose purpose is\n    to allow us to overwrite the gradient.\n    The forward pass gives zeros and\n    the backward pass gives the correct gradients\n    for the permutohedral_compute function\n\n    :param data_vectors:\n    :param barycentric:\n    :param blur_neighbours1:\n    :param blur_neighbours2:\n    :param indices:\n    :param name:\n    :return:\n    """"""\n\n    def _dummy_wrapper(*_unused):\n        return np.float32(0)\n\n    def _permutohedral_grad_wrapper(op, grad):\n        # Differentiation can be done using permutohedral lattice\n        # with Gaussian filter order reversed\n        filtering_grad = permutohedral_compute(\n            grad, op.inputs[1], op.inputs[2], op.inputs[3], op.inputs[4],\n            name, reverse=True)\n        return [filtering_grad] + [None for i in op.inputs[1:]]\n\n    _inputs = [\n        data_vectors, barycentric, blur_neighbours1, blur_neighbours2, indices]\n\n    partial_grads_func = _py_func_with_grads(\n        _dummy_wrapper,\n        _inputs,\n        [tf.float32],\n        name=name,\n        grad=_permutohedral_grad_wrapper)\n    partial_grads_func.set_shape(data_vectors.shape.as_list())\n    return partial_grads_func\n\n\ndef _permutohedral_gen(permutohedral, data_vectors, name):\n    """"""\n    a wrapper combines permutohedral_compute and a customised gradient op.\n\n    :param permutohedral:\n    :param data_vectors:\n    :param name:\n    :return:\n    """"""\n    barycentric, blur_neighbours1, blur_neighbours2, indices = permutohedral\n    backward_branch = _gradient_stub(\n        data_vectors,\n        barycentric,\n        blur_neighbours1,\n        blur_neighbours2,\n        indices,\n        name)\n    forward_branch = permutohedral_compute(\n        data_vectors,\n        barycentric,\n        blur_neighbours1,\n        blur_neighbours2,\n        indices,\n        name,\n        reverse=False)\n    return backward_branch + tf.stop_gradient(forward_branch)\n'"
niftynet/layer/crop.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.base_layer import Layer\n\n\nclass CropLayer(Layer):\n    """"""\n    This class defines a cropping operation:\n    Removing ``2*border`` pixels from each spatial dim of the input,\n    and return the spatially centred elements extracted from the input.\n    """"""\n\n    def __init__(self, border, name=\'crop\'):\n        super(CropLayer, self).__init__(name=name)\n        self.border = border\n\n    def layer_op(self, inputs):\n        spatial_rank = layer_util.infer_spatial_rank(inputs)\n        offsets = [0] + [int(self.border)] * spatial_rank + [0]\n        # inferring the shape of the output by subtracting the border dimension\n        out_shape = [\n            int(d) - 2 * int(self.border)\n            for d in list(inputs.shape)[1:-1]]\n        out_shape = [-1] + out_shape + [-1]\n        output_tensor = tf.slice(inputs, offsets, out_shape)\n        return output_tensor\n'"
niftynet/layer/deconvolution.py,12,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.activation import ActiLayer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer, InstanceNormLayer\nfrom niftynet.layer.gn import GNLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_OP = {\n    \'2D\': tf.nn.conv2d_transpose,\n    \'3D\': tf.nn.conv3d_transpose}\nSUPPORTED_PADDING = set([\'SAME\', \'VALID\'])\n\n\ndef default_w_initializer():\n    def _initializer(shape, dtype, partition_info):\n        stddev = np.sqrt(2.0 / (np.prod(shape[:-2]) * shape[-1]))\n        from tensorflow.python.ops import random_ops\n        return random_ops.truncated_normal(shape, 0.0, stddev, dtype=tf.float32)\n        # return tf.truncated_normal_initializer(\n        #    mean=0.0, stddev=stddev, dtype=tf.float32)\n\n    return _initializer\n\n\ndef default_b_initializer():\n    return tf.constant_initializer(0.0)\n\n\ndef infer_output_dims(input_dims, strides, kernel_sizes, padding):\n    """"""\n    infer output dims from list,\n    the dim can be different in different directions.\n    Note: dilation is not considered here.\n    """"""\n    assert len(input_dims) == len(strides)\n    assert len(input_dims) == len(kernel_sizes)\n    output_dims = []\n    for (i, dim) in enumerate(input_dims):\n        if dim is None:\n            output_dims.append(None)\n            continue\n        if padding == \'VALID\':\n            output_dims.append(\n                dim * strides[i] + max(kernel_sizes[i] - strides[i], 0))\n        else:\n            output_dims.append(dim * strides[i])\n    return output_dims\n\n\nclass DeconvLayer(TrainableLayer):\n    """"""\n    This class defines a simple deconvolution with an optional bias term.\n    Please consider ``DeconvolutionalLayer`` if batch_norm and activation\n    are also used.\n    """"""\n\n    def __init__(self,\n                 n_output_chns,\n                 kernel_size=3,\n                 stride=1,\n                 padding=\'SAME\',\n                 with_bias=False,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 name=\'deconv\'):\n\n        super(DeconvLayer, self).__init__(name=name)\n\n        self.padding = look_up_operations(padding.upper(), SUPPORTED_PADDING)\n        self.n_output_chns = int(n_output_chns)\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.with_bias = with_bias\n\n        self.initializers = {\n            \'w\': w_initializer if w_initializer else default_w_initializer(),\n            \'b\': b_initializer if b_initializer else default_b_initializer()}\n\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, input_tensor):\n        input_shape = input_tensor.shape.as_list()\n        n_input_chns = input_shape[-1]\n        spatial_rank = layer_util.infer_spatial_rank(input_tensor)\n\n        # initialize conv kernels/strides and then apply\n        kernel_size_all_dim = layer_util.expand_spatial_params(\n            self.kernel_size, spatial_rank)\n        w_full_size = kernel_size_all_dim + (self.n_output_chns, n_input_chns)\n        stride_all_dim = layer_util.expand_spatial_params(\n            self.stride, spatial_rank)\n        full_stride = (1,) + stride_all_dim + (1,)\n\n        deconv_kernel = tf.get_variable(\n            \'w\', shape=w_full_size,\n            initializer=self.initializers[\'w\'],\n            regularizer=self.regularizers[\'w\'])\n        if spatial_rank == 2:\n            op_ = SUPPORTED_OP[\'2D\']\n        elif spatial_rank == 3:\n            op_ = SUPPORTED_OP[\'3D\']\n        else:\n            raise ValueError(\n                ""Only 2D and 3D spatial deconvolutions are supported"")\n\n        spatial_shape = []\n        for (i, dim) in enumerate(input_shape[:-1]):\n            if i == 0:\n                continue\n            if dim is None:\n                spatial_shape.append(tf.shape(input_tensor)[i])\n            else:\n                spatial_shape.append(dim)\n        output_dims = infer_output_dims(spatial_shape,\n                                        stride_all_dim,\n                                        kernel_size_all_dim,\n                                        self.padding)\n        if input_tensor.shape.is_fully_defined():\n            full_output_size = \\\n                [input_shape[0]] + output_dims + [self.n_output_chns]\n        else:\n            batch_size = tf.shape(input_tensor)[0]\n            full_output_size = tf.stack(\n                [batch_size] + output_dims + [self.n_output_chns])\n        output_tensor = op_(value=input_tensor,\n                            filter=deconv_kernel,\n                            output_shape=full_output_size,\n                            strides=full_stride,\n                            padding=self.padding,\n                            name=\'deconv\')\n        if not self.with_bias:\n            return output_tensor\n\n        # adding the bias term\n        bias_full_size = (self.n_output_chns,)\n        bias_term = tf.get_variable(\n            \'b\', shape=bias_full_size,\n            initializer=self.initializers[\'b\'],\n            regularizer=self.regularizers[\'b\'])\n        output_tensor = tf.nn.bias_add(output_tensor,\n                                       bias_term,\n                                       name=\'add_bias\')\n        return output_tensor\n\n\nclass DeconvolutionalLayer(TrainableLayer):\n    """"""\n    This class defines a composite layer with optional components::\n\n        deconvolution -> batch_norm -> activation -> dropout\n\n    The b_initializer and b_regularizer are applied to the DeconvLayer\n    The w_initializer and w_regularizer are applied to the DeconvLayer,\n    the batch normalisation layer, and the activation layer (for \'prelu\')\n    """"""\n\n    def __init__(self,\n                 n_output_chns,\n                 kernel_size=3,\n                 stride=1,\n                 padding=\'SAME\',\n                 with_bias=False,\n                 feature_normalization=\'batch\',\n                 group_size=-1,\n                 acti_func=None,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 moving_decay=0.9,\n                 eps=1e-5,\n                 name=""deconv""):\n\n        self.acti_func = acti_func\n        self.feature_normalization = feature_normalization\n        self.group_size = group_size\n        self.layer_name = \'{}\'.format(name)\n        if self.feature_normalization != \'group\' and group_size > 0:\n            raise ValueError(\'You cannot have a group_size > 0 if not using group norm\')\n        elif self.feature_normalization == \'group\' and group_size <= 0:\n            raise ValueError(\'You cannot have a group_size <= 0 if using group norm\')\n\n        if self.feature_normalization is not None:\n            # appending, for example, \'_bn\' to the name \n            self.layer_name += \'_\' + self.feature_normalization[0] + \'n\'\n        if self.acti_func is not None:\n            self.layer_name += \'_{}\'.format(self.acti_func)\n        super(DeconvolutionalLayer, self).__init__(name=self.layer_name)\n\n        # for DeconvLayer\n        self.n_output_chns = n_output_chns\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.with_bias = with_bias\n\n        # for BNLayer\n        self.moving_decay = moving_decay\n        self.eps = eps\n\n        self.initializers = {\n            \'w\': w_initializer if w_initializer else default_w_initializer(),\n            \'b\': b_initializer if b_initializer else default_b_initializer()}\n\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, input_tensor, is_training=None, keep_prob=None):\n        # init sub-layers\n        deconv_layer = DeconvLayer(n_output_chns=self.n_output_chns,\n                                   kernel_size=self.kernel_size,\n                                   stride=self.stride,\n                                   padding=self.padding,\n                                   with_bias=self.with_bias,\n                                   w_initializer=self.initializers[\'w\'],\n                                   w_regularizer=self.regularizers[\'w\'],\n                                   b_initializer=self.initializers[\'b\'],\n                                   b_regularizer=self.regularizers[\'b\'],\n                                   name=\'deconv_\')\n        output_tensor = deconv_layer(input_tensor)\n\n        if self.feature_normalization == \'batch\':\n            if is_training is None:\n                raise ValueError(\'is_training argument should be \'\n                                 \'True or False unless feature_normalization is False\')\n            bn_layer = BNLayer(\n                regularizer=self.regularizers[\'w\'],\n                moving_decay=self.moving_decay,\n                eps=self.eps,\n                name=\'bn_\')\n            output_tensor = bn_layer(output_tensor, is_training)\n        elif self.feature_normalization == \'instance\':\n            in_layer = InstanceNormLayer(eps=self.eps, name=\'in_\')\n            output_tensor = in_layer(output_tensor)\n        elif self.feature_normalization == \'group\':\n            gn_layer = GNLayer(\n                regularizer=self.regularizers[\'w\'],\n                group_size=self.group_size,\n                eps=self.eps,\n                name=\'gn_\')\n            output_tensor = gn_layer(output_tensor)\n\n        if self.acti_func is not None:\n            acti_layer = ActiLayer(\n                func=self.acti_func,\n                regularizer=self.regularizers[\'w\'],\n                name=\'acti_\')\n            output_tensor = acti_layer(output_tensor)\n\n        if keep_prob is not None:\n            dropout_layer = ActiLayer(func=\'dropout\', name=\'dropout_\')\n            output_tensor = dropout_layer(output_tensor, keep_prob=keep_prob)\n\n        return output_tensor\n'"
niftynet/layer/dilatedcontext.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\n\n\nclass DilatedTensor(object):\n    """"""\n    This context manager makes a wrapper of input_tensor\n    When created, the input_tensor is dilated,\n    the input_tensor resumes to original space when exiting the context.\n    """"""\n\n    def __init__(self, input_tensor, dilation_factor):\n        assert (layer_util.check_spatial_dims(\n            input_tensor, lambda x: x % dilation_factor == 0))\n        self._tensor = input_tensor\n        self.dilation_factor = dilation_factor\n        # parameters to transform input tensor\n        self.spatial_rank = layer_util.infer_spatial_rank(self._tensor)\n        self.zero_paddings = [[0, 0]] * self.spatial_rank\n        self.block_shape = [dilation_factor] * self.spatial_rank\n\n    def __enter__(self):\n        if self.dilation_factor > 1:\n            self._tensor = tf.space_to_batch_nd(self._tensor,\n                                                self.block_shape,\n                                                self.zero_paddings,\n                                                name=\'dilated\')\n        return self\n\n    def __exit__(self, *args):\n        if self.dilation_factor > 1:\n            self._tensor = tf.batch_to_space_nd(self._tensor,\n                                                self.block_shape,\n                                                self.zero_paddings,\n                                                name=\'de-dilate\')\n\n    @property\n    def tensor(self):\n        return self._tensor\n\n    @tensor.setter\n    def tensor(self, value):\n        self._tensor = value\n'"
niftynet/layer/discrete_label_normalisation.py,6,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nimport niftynet.utilities.histogram_standardisation as hs\nfrom niftynet.layer.base_layer import DataDependentLayer\nfrom niftynet.layer.base_layer import Invertible\nfrom niftynet.utilities.user_parameters_helper import standardise_string\nfrom niftynet.utilities.util_common import print_progress_bar\n\n\nclass DiscreteLabelNormalisationLayer(DataDependentLayer, Invertible):\n    def __init__(self,\n                 image_name,\n                 modalities,\n                 model_filename=None,\n                 name=\'label_norm\'):\n\n        super(DiscreteLabelNormalisationLayer, self).__init__(name=name)\n        # mapping is a complete cache of the model file, the total number of\n        # modalities are listed in self.modalities\n        self.image_name = image_name\n        self.modalities = None\n        if isinstance(modalities, (list, tuple)):\n            if len(modalities) > 1:\n                raise NotImplementedError(\n                    ""Currently supports single modality discrete labels."")\n            self.modalities = modalities\n        else:\n            self.modalities = (modalities,)\n        if model_filename is None:\n            model_filename = os.path.join(\'.\', \'histogram_ref_file.txt\')\n        self.model_file = os.path.abspath(model_filename)\n        self._key=None\n        assert not os.path.isdir(self.model_file), \\\n            ""model_filename is a directory, "" \\\n            ""please change histogram_ref_file to a filename.""\n        self.label_map = hs.read_mapping_file(self.model_file)\n\n    @property\n    def key(self):\n        if self._key:\n            return self._key\n        # provide a readable key for the label mapping item\n        name1 = self.image_name\n        name2 = self.image_name if not self.modalities else self.modalities[0]\n        key_from = ""{}_{}-from"".format(name1, name2)\n        key_to = ""{}_{}-to"".format(name1, name2)\n        return standardise_string(key_from), standardise_string(key_to)\n\n    @key.setter\n    def key(self, value):\n        # Allows the key to be overridden\n        self._key=value\n\n    def layer_op(self, image, mask=None):\n        assert self.is_ready(), \\\n            ""discrete_label_normalisation layer needs to be trained first.""\n        # mask is not used for label mapping\n        if isinstance(image, dict):\n            if self.image_name not in image:\n                return image, mask\n            label_data = np.asarray(image[self.image_name])\n        else:\n            label_data = np.asarray(image)\n\n        mapping_from = self.label_map[self.key[0]]\n        mapping_to = self.label_map[self.key[1]]\n\n        image_shape = label_data.shape\n        label_data = label_data.reshape(-1)\n        mapped_data = np.zeros_like(label_data)\n        for (original, new_id) in zip(mapping_from, mapping_to):\n            mapped_data[label_data == original] = new_id\n        label_data = mapped_data.reshape(image_shape)\n\n        if isinstance(image, dict):\n            image[self.image_name] = label_data\n            return image, mask\n        return label_data, mask\n\n    def inverse_op(self, image, mask=None):\n        assert self.is_ready(), \\\n            ""discrete_label_normalisation layer needs to be trained first.""\n        # mask is not used for label mapping\n        if isinstance(image, dict):\n            label_data = np.asarray(image[self.image_name])\n        else:\n            label_data = np.asarray(image)\n\n        mapping_from = self.label_map[self.key[0]]\n        mapping_to = self.label_map[self.key[1]]\n\n        image_shape = label_data.shape\n        label_data = label_data.reshape(-1)\n        mapped_data = np.zeros_like(label_data)\n        for (new_id, original) in zip(mapping_from, mapping_to):\n            mapped_data[label_data == original] = new_id\n        label_data = mapped_data.reshape(image_shape)\n        if isinstance(image, dict):\n            image[self.image_name] = label_data\n            return image, mask\n        return label_data, mask\n\n    def is_ready(self):\n        mapping_from = self.label_map.get(self.key[0], None)\n        if mapping_from is None:\n            # tf.logging.warning(\'could not find mapping key %s\', self.key[0])\n            return False\n        mapping_to = self.label_map.get(self.key[1], None)\n        if mapping_to is None:\n            # tf.logging.warning(\'could not find mapping key %s\', self.key[1])\n            return False\n        assert len(mapping_from) == len(mapping_to), \\\n            ""mapping is not one-to-one, "" \\\n            ""corrupted mapping file? {}"".format(self.model_file)\n        return True\n\n    def train(self, image_list):\n        # check modalities to train, using the first subject in subject list\n        # to find input modality list\n        assert image_list is not None, ""nothing to training for this layer""\n        if self.is_ready():\n            tf.logging.info(\n                ""label mapping ready for {}:{}, {} classes"".format(\n                    self.image_name,\n                    self.modalities,\n                    len(self.label_map[self.key[0]])))\n            return\n        tf.logging.info(\n            ""Looking for the set of unique discrete labels from input {}""\n            "" using {} subjects"".format(self.image_name, len(image_list)))\n        label_map = find_set_of_labels(image_list, self.image_name, self.key)\n        # merging trained_mapping dict and self.mapping dict\n        self.label_map.update(label_map)\n        all_maps = hs.read_mapping_file(self.model_file)\n        all_maps.update(self.label_map)\n        hs.write_all_mod_mapping(self.model_file, all_maps)\n\n\ndef find_set_of_labels(image_list, field, output_key):\n    label_set = set()\n    if field in image_list[0] :\n        for idx, image in enumerate(image_list):\n            assert field in image, \\\n                ""label normalisation layer requires {} input, "" \\\n                ""however it is not provided in the config file.\\n"" \\\n                ""Please consider setting "" \\\n                ""label_normalisation to False."".format(field)\n            print_progress_bar(idx, len(image_list),\n                               prefix=\'searching unique labels from files\',\n                               decimals=1, length=10, fill=\'*\')\n            unique_label = np.unique(image[field].get_data())\n            if len(unique_label) > 500 or len(unique_label) <= 1:\n                tf.logging.warning(\n                    \'unusual discrete values: number of unique \'\n                    \'labels to normalise %s\', len(unique_label))\n            label_set.update(set(unique_label))\n        label_set = list(label_set)\n        label_set.sort()\n    try:\n        mapping_from_to = dict()\n        mapping_from_to[output_key[0]] = tuple(label_set)\n        mapping_from_to[output_key[1]] = tuple(range(0, len(label_set)))\n    except (IndexError, ValueError):\n        tf.logging.fatal(""unable to create mappings keys: %s, image name %s"",\n                         output_key, field)\n        raise\n    return mapping_from_to\n'"
niftynet/layer/downsample.py,6,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_OP = set(['AVG', 'MAX', 'CONSTANT'])\nSUPPORTED_PADDING = set(['SAME', 'VALID'])\n\n\nclass DownSampleLayer(Layer):\n    def __init__(self,\n                 func,\n                 kernel_size=3,\n                 stride=2,\n                 padding='SAME',\n                 name='pooling'):\n        self.func = func.upper()\n        self.layer_name = '{}_{}'.format(self.func.lower(), name)\n        super(DownSampleLayer, self).__init__(name=self.layer_name)\n\n        self.padding = padding.upper()\n        look_up_operations(self.padding, SUPPORTED_PADDING)\n\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n    def layer_op(self, input_tensor):\n        spatial_rank = layer_util.infer_spatial_rank(input_tensor)\n        look_up_operations(self.func, SUPPORTED_OP)\n        kernel_size_all_dims = layer_util.expand_spatial_params(\n            self.kernel_size, spatial_rank)\n        stride_all_dims = layer_util.expand_spatial_params(\n            self.stride, spatial_rank)\n        if self.func == 'CONSTANT':\n            full_kernel_size = kernel_size_all_dims + (1, 1)\n            np_kernel = layer_util.trivial_kernel(full_kernel_size)\n            kernel = tf.constant(np_kernel, dtype=tf.float32)\n            output_tensor = [tf.expand_dims(x, -1)\n                             for x in tf.unstack(input_tensor, axis=-1)]\n            output_tensor = [\n                tf.nn.convolution(\n                    input=inputs,\n                    filter=kernel,\n                    strides=stride_all_dims,\n                    padding=self.padding,\n                    name='conv')\n                for inputs in output_tensor]\n            output_tensor = tf.concat(output_tensor, axis=-1)\n        else:\n            output_tensor = tf.nn.pool(\n                input=input_tensor,\n                window_shape=kernel_size_all_dims,\n                pooling_type=self.func,\n                padding=self.padding,\n                dilation_rate=[1] * spatial_rank,\n                strides=stride_all_dims,\n                name=self.layer_name)\n        return output_tensor\n"""
niftynet/layer/downsample_res_block.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer as Conv\nfrom niftynet.layer.downsample import DownSampleLayer as Down\nfrom niftynet.layer.residual_unit import ResidualUnit as ResUnit\n\n\nclass DownBlock(TrainableLayer):\n    def __init__(self,\n                 n_output_chns=4,\n                 kernel_size=3,\n                 downsample_kernel_size=2,\n                 downsample_stride=2,\n                 acti_func=\'relu\',\n                 w_initializer=None,\n                 w_regularizer=None,\n                 type_string=\'bn_acti_conv\',\n                 name=\'res-downsample\'):\n        super(TrainableLayer, self).__init__(name=name)\n        self.n_output_chns = n_output_chns\n        self.kernel_size = kernel_size\n        self.downsample_kernel_size = downsample_kernel_size\n        self.downsample_stride = downsample_stride\n        self.acti_func = acti_func\n        self.conv_param = {\'w_initializer\': w_initializer,\n                           \'w_regularizer\': w_regularizer}\n        self.type_string = type_string\n\n    def layer_op(self, inputs, is_training=True):\n        """"""\n        Consists of::\n\n            (inputs)--conv_0-o-conv_1--conv_2-+-(conv_res)--down_sample--\n                             |                |\n                             o----------------o\n\n        conv_0, conv_res is also returned for feature forwarding purpose\n        """"""\n        conv_0 = Conv(n_output_chns=self.n_output_chns,\n                      kernel_size=self.kernel_size,\n                      acti_func=self.acti_func,\n                      with_bias=False, feature_normalization=\'batch\',\n                      **self.conv_param)(inputs, is_training)\n        conv_res = ResUnit(n_output_chns=self.n_output_chns,\n                           kernel_size=self.kernel_size,\n                           acti_func=self.acti_func,\n                           type_string=self.type_string,\n                           **self.conv_param)(conv_0, is_training)\n        conv_down = Down(\'Max\',\n                         kernel_size=self.downsample_kernel_size,\n                         stride=self.downsample_stride)(conv_res)\n        return conv_down, conv_0, conv_res\n'"
niftynet/layer/elementwise.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_OP = set([\'SUM\', \'CONCAT\'])\n\n\nclass ElementwiseLayer(TrainableLayer):\n    """"""\n    This class takes care of the elementwise sum in a residual connection\n    It matches the channel dims from two branch flows,\n    by either padding or projection if necessary.\n    """"""\n\n    def __init__(self,\n                 func,\n                 initializer=None,\n                 regularizer=None,\n                 name=\'residual\'):\n\n        self.func = look_up_operations(func.upper(), SUPPORTED_OP)\n        self.layer_name = \'{}_{}\'.format(name, self.func.lower())\n\n        super(ElementwiseLayer, self).__init__(name=self.layer_name)\n        self.initializers = {\'w\': initializer}\n        self.regularizers = {\'w\': regularizer}\n\n    def layer_op(self, param_flow, bypass_flow):\n        n_param_flow = param_flow.shape[-1]\n        n_bypass_flow = bypass_flow.shape[-1]\n        spatial_rank = layer_util.infer_spatial_rank(param_flow)\n\n        output_tensor = param_flow\n        if self.func == \'SUM\':\n            if n_param_flow > n_bypass_flow:  # pad the channel dim\n                pad_1 = np.int((n_param_flow - n_bypass_flow) // 2)\n                pad_2 = np.int(n_param_flow - n_bypass_flow - pad_1)\n                padding_dims = np.vstack(([[0, 0]],\n                                          [[0, 0]] * spatial_rank,\n                                          [[pad_1, pad_2]]))\n                bypass_flow = tf.pad(tensor=bypass_flow,\n                                     paddings=padding_dims.tolist(),\n                                     mode=\'CONSTANT\')\n            elif n_param_flow < n_bypass_flow:  # make a projection\n                projector = ConvLayer(n_output_chns=n_param_flow,\n                                      kernel_size=1,\n                                      stride=1,\n                                      padding=\'SAME\',\n                                      w_initializer=self.initializers[\'w\'],\n                                      w_regularizer=self.regularizers[\'w\'],\n                                      name=\'proj\')\n                bypass_flow = projector(bypass_flow)\n\n            # element-wise sum of both paths\n            output_tensor = param_flow + bypass_flow\n\n        elif self.func == \'CONCAT\':\n            output_tensor = tf.concat([param_flow, bypass_flow], axis=-1)\n\n        return output_tensor\n'"
niftynet/layer/fully_connected.py,9,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n# from utilities.misc_common import look_up_operations\n# from . import layer_util\nfrom niftynet.layer.activation import ActiLayer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer, InstanceNormLayer\nfrom niftynet.layer.gn import GNLayer\n\n\ndef default_w_initializer():\n    def _initializer(shape, dtype, partition_info):\n        stddev = np.sqrt(2.0 / np.prod(shape[:-1]))\n        from tensorflow.python.ops import random_ops\n        return random_ops.truncated_normal(\n            shape, 0.0, stddev, dtype=tf.float32)\n        # return tf.truncated_normal_initializer(\n        #    mean=0.0, stddev=stddev, dtype=tf.float32)\n\n    return _initializer\n\n\ndef default_b_initializer():\n    return tf.constant_initializer(0.0)\n\n\nclass FCLayer(TrainableLayer):\n    """"""\n    This class defines a simple fully connected layer with\n    an optional bias term.\n    Please consider ``FullyConnectedLayer`` if batch_norm and activation\n    are also used.\n    """"""\n\n    def __init__(self,\n                 n_output_chns,\n                 with_bias=True,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 name=\'fc\'):\n        super(FCLayer, self).__init__(name=name)\n\n        self.n_output_chns = n_output_chns\n        self.with_bias = with_bias\n\n        self.initializers = {\n            \'w\': w_initializer if w_initializer else default_w_initializer(),\n            \'b\': b_initializer if b_initializer else default_b_initializer()}\n\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, input_tensor):\n        input_shape = input_tensor.shape.as_list()\n        if len(input_shape) > 2:\n            batch_size = input_shape[0]\n            input_tensor = tf.reshape(input_tensor, [batch_size, -1])\n            input_shape = input_tensor.shape.as_list()\n        n_input_chns = input_shape[-1]\n\n        # initialize weight matrix and then apply\n        weight_matrix = tf.get_variable(\n            \'w\', shape=[n_input_chns, self.n_output_chns],\n            initializer=self.initializers[\'w\'],\n            regularizer=self.regularizers[\'w\'])\n        output_tensor = tf.matmul(input_tensor,\n                                  weight_matrix,\n                                  name=\'fc\')\n        if not self.with_bias:\n            return output_tensor\n\n        # adding the bias term\n        bias_term = tf.get_variable(\n            \'b\', shape=self.n_output_chns,\n            initializer=self.initializers[\'b\'],\n            regularizer=self.regularizers[\'b\'])\n        output_tensor = tf.nn.bias_add(\n            output_tensor, bias_term, name=\'add_bias\')\n        return output_tensor\n\n\nclass FullyConnectedLayer(TrainableLayer):\n    """"""\n    This class defines a composite layer with optional components::\n\n        fully connected layer -> batch_norm -> activation -> dropout\n\n    The b_initializer and b_regularizer are applied to the FCLayer\n    The w_initializer and w_regularizer are applied to the FCLayer,\n    the batch normalisation layer, and the activation layer (for \'prelu\')\n    """"""\n\n    def __init__(self,\n                 n_output_chns,\n                 with_bias=True,\n                 feature_normalization=\'batch\',\n                 group_size=-1,\n                 acti_func=None,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 moving_decay=0.9,\n                 eps=1e-5,\n                 name=""fc""):\n\n        self.acti_func = acti_func\n        self.feature_normalization = feature_normalization\n        self.group_size = group_size\n        self.layer_name = \'{}\'.format(name)\n\n        if self.feature_normalization != \'group\' and group_size > 0:\n            raise ValueError(\'You cannot have a group_size > 0 if not using group norm\')\n        elif self.feature_normalization == \'group\' and group_size <= 0:\n            raise ValueError(\'You cannot have a group_size <= 0 if using group norm\')\n\n        if self.feature_normalization is not None:\n            # to append batch_norm as _bn and likewise for other norms\n            self.layer_name += \'_\' + self.feature_normalization[0] + \'n\'\n        if self.acti_func is not None:\n            self.layer_name += \'_{}\'.format(self.acti_func)\n        super(FullyConnectedLayer, self).__init__(name=self.layer_name)\n\n        # for FCLayer\n        self.n_output_chns = n_output_chns\n        self.with_bias = with_bias\n\n        # for BNLayer\n        self.moving_decay = moving_decay\n        self.eps = eps\n\n        self.initializers = {\n            \'w\': w_initializer if w_initializer else default_w_initializer(),\n            \'b\': b_initializer if b_initializer else default_b_initializer()}\n\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, input_tensor, is_training=None, keep_prob=None):\n        fc_layer = FCLayer(n_output_chns=self.n_output_chns,\n                           with_bias=self.with_bias,\n                           w_initializer=self.initializers[\'w\'],\n                           w_regularizer=self.regularizers[\'w\'],\n                           b_initializer=self.initializers[\'b\'],\n                           b_regularizer=self.regularizers[\'b\'],\n                           name=\'fc_\')\n        output_tensor = fc_layer(input_tensor)\n\n        if self.feature_normalization == \'batch\':\n            if is_training is None:\n                raise ValueError(\'is_training argument should be \'\n                                 \'True or False unless feature_normalization is False\')\n            bn_layer = BNLayer(\n                regularizer=self.regularizers[\'w\'],\n                moving_decay=self.moving_decay,\n                eps=self.eps,\n                name=\'bn_\')\n            output_tensor = bn_layer(output_tensor, is_training)\n        elif self.feature_normalization == \'instance\':\n            in_layer = InstanceNormLayer(eps=self.eps, name=\'in_\')\n            output_tensor = in_layer(output_tensor)\n        elif self.feature_normalization == \'group\':\n            gn_layer = GNLayer(\n                regularizer=self.regularizers[\'w\'],\n                group_size=self.group_size,\n                eps=self.eps,\n                name=\'gn_\')\n            output_tensor = gn_layer(output_tensor)\n\n        if self.acti_func is not None:\n            acti_layer = ActiLayer(\n                func=self.acti_func,\n                regularizer=self.regularizers[\'w\'],\n                name=\'acti_\')\n            output_tensor = acti_layer(output_tensor)\n\n        if keep_prob is not None:\n            dropout_layer = ActiLayer(func=\'dropout\', name=\'dropout_\')\n            output_tensor = dropout_layer(output_tensor, keep_prob=keep_prob)\n\n        return output_tensor\n'"
niftynet/layer/gan_blocks.py,3,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import TrainableLayer\n\n\nclass GANImageBlock(TrainableLayer):\n    def __init__(self,\n                 generator,\n                 discriminator,\n                 clip=None,\n                 name='GAN_image_block'):\n        self._generator = generator\n        self._discriminator = discriminator\n        self.clip = clip\n        super(GANImageBlock, self).__init__(name=name)\n\n    def layer_op(self,\n                 random_source,\n                 training_image,\n                 conditioning,\n                 is_training):\n        shape_to_generate = training_image.shape.as_list()[1:]\n        fake_image = self._generator(random_source,\n                                     shape_to_generate,\n                                     conditioning,\n                                     is_training)\n\n        fake_logits = self._discriminator(fake_image,\n                                          conditioning,\n                                          is_training)\n        if self.clip:\n            with tf.name_scope('clip_real_images'):\n                training_image = tf.maximum(\n                    -self.clip,\n                    tf.minimum(self.clip, training_image))\n        real_logits = self._discriminator(training_image,\n                                          conditioning,\n                                          is_training)\n        return fake_image, real_logits, fake_logits\n\n\nclass BaseGenerator(TrainableLayer):\n    def __init__(self, name='generator', *args, **kwargs):\n        super(BaseGenerator, self).__init__(name=name)\n\n\nclass BaseDiscriminator(TrainableLayer):\n    def __init__(self, name='discriminator', *args, **kwargs):\n        super(BaseDiscriminator, self).__init__(name=name)\n"""
niftynet/layer/gn.py,11,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import TrainableLayer\n\n\nclass GNLayer(TrainableLayer):\n    """"""\n    Group normalisation layer, with trainable mean value \'beta\' and\n    std \'gamma\'.  \'beta\' is initialised to 0.0 and \'gamma\' is initialised\n    to 1.0.  This class assumes \'beta\' and \'gamma\' share the same type_str of\n    regulariser.\n\n    Reimplementation of\n    Wu and He, Group Normalization, arXiv:1803.08494 (2018)\n    """"""\n\n    def __init__(self,\n                 group_size=32,\n                 regularizer=None,\n                 eps=1e-5,\n                 name=\'group_norm\'):\n        super(GNLayer, self).__init__(name=name)\n        self.group_size = group_size\n        self.eps = eps\n        self.initializers = {\n            \'beta\': tf.constant_initializer(0.0),\n            \'gamma\': tf.constant_initializer(1.0)}\n        self.regularizers = {\'beta\': regularizer, \'gamma\': regularizer}\n\n    def layer_op(self, inputs):\n        input_shape = inputs.shape\n        group_size = max(min(self.group_size, input_shape[-1]), 1)\n\n        assert input_shape[-1] % group_size == 0, \\\n            \'number of input channels should be divisible by group size.\'\n        grouped_shape = tf.stack(\n            list(input_shape[:-1]) +\n            [group_size, input_shape[-1] // group_size])\n        grouped_inputs = tf.reshape(inputs, grouped_shape)\n\n        # operates on all dims except the batch and grouped dim\n        axes = list(range(1, input_shape.ndims - 1)) + [input_shape.ndims]\n\n        # create the shape of trainable variables\n        param_shape = [1] * (input_shape.ndims - 1) + [input_shape[-1]]\n\n        # create trainable variables\n        beta = tf.get_variable(\n            \'beta\',\n            shape=param_shape,\n            initializer=self.initializers[\'beta\'],\n            regularizer=self.regularizers[\'beta\'],\n            dtype=tf.float32, trainable=True)\n        gamma = tf.get_variable(\n            \'gamma\',\n            shape=param_shape,\n            initializer=self.initializers[\'gamma\'],\n            regularizer=self.regularizers[\'gamma\'],\n            dtype=tf.float32, trainable=True)\n\n        # mean and var\n        mean, variance = tf.nn.moments(grouped_inputs, axes, keep_dims=True)\n\n        outputs = (grouped_inputs - mean) / tf.sqrt(variance + self.eps)\n        outputs = tf.reshape(outputs, input_shape) * gamma + beta\n        return outputs\n'"
niftynet/layer/grid_warper.py,31,"b'# -*- coding: utf-8 -*-\n# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n# Modifications copyright 2018 The NiftyNet Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""\nGrid warper layer and utilities\nadapted from\nhttps://github.com/deepmind/sonnet/blob/v1.13/sonnet/python/modules/spatial_transformer.py\nhttps://github.com/niftk/NiftyNet/blob/v0.2.0.post1/niftynet/layer/spatial_transformer.py\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nfrom itertools import chain\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer, LayerFromCallable, Invertible\n\n\nclass GridWarperLayer(Layer):\n    """"""\n    Grid warper interface class.\n\n    An object implementing the `GridWarper` interface\n    generates a reference grid of feature points at construction time,\n    and warps it via a parametric transformation model,\n    specified at run time by an input parameter Tensor.\n    Grid warpers must then implement a `create_features` function\n    used to generate the reference grid to be warped\n    in the forward pass (according to a determined warping model).\n    """"""\n\n    def __init__(self,\n                 source_shape,\n                 output_shape,\n                 coeff_shape,\n                 name,\n                 **kwargs):\n        """"""\n        Constructs a GridWarper module and\n        initializes the source grid params.\n\n        `source_shape` and `output_shape` defines the size of the source\n        and output signal domains.\n\n        For example,\n        for an image of size `width=W` and `height=H`,\n        `{source,output}_shape=[H, W]`;\n        for a volume of size `width=W`, `height=H`\n        and `depth=D`, `{source,output}_shape=[H, W, D]`.\n\n        Args:\n          source_shape: Iterable of integers determining\n            the size of the source signal domain.\n          output_shape: Iterable of integers determining\n            the size of the destination resampled signal domain.\n          coeff_shape: Shape of coefficients parameterizing the grid warp.\n            For example, a 2D affine transformation will be defined by the [6]\n            parameters populating the corresponding 2x3 affine matrix.\n          name: Name of Module.\n          **kwargs: Extra kwargs to be forwarded to\n            the `create_features` function,\n            instantiating the source grid parameters.\n\n        Raises:\n          Error: If `len(output_shape) > len(source_shape)`.\n          TypeError: If `output_shape` and `source_shape`\n            are not both iterable.\n        """"""\n        super(GridWarperLayer, self).__init__(name=name)\n\n        self._source_shape = tuple(source_shape)\n        self._output_shape = tuple(output_shape)\n        if len(self._output_shape) > len(self._source_shape):\n            tf.logging.fatal(\n                \'Output domain dimensionality (%s) must be equal or \'\n                \'smaller than source domain dimensionality (%s)\',\n                len(self._output_shape), len(self._source_shape))\n            raise ValueError\n\n        self._coeff_shape = coeff_shape\n        self._psi = self._create_features(**kwargs)\n\n    def _create_features(self, **kwargs):\n        """"""\n        Precomputes features\n        (e.g. sampling patterns, unconstrained feature matrices).\n        """"""\n        tf.logging.fatal(\'_create_features() should be implemented\')\n        raise NotImplementedError\n\n    def layer_op(self, *args, **kwargs):\n        tf.logging.fatal(\'layer_op() should be implemented to warp self._psi\')\n        raise NotImplementedError\n\n    @property\n    def coeff_shape(self):\n        """"""Returns number of coefficients of warping function.""""""\n        return self._coeff_shape\n\n    @property\n    def psi(self):\n        """"""Returns a list of features used to compute the grid warp.""""""\n        return self._psi\n\n    @property\n    def source_shape(self):\n        """"""Returns a tuple containing the shape of the source signal.""""""\n        return self._source_shape\n\n    @property\n    def output_shape(self):\n        """"""Returns a tuple containing the shape of the output grid.""""""\n        return self._output_shape\n\n\nclass AffineGridWarperLayer(GridWarperLayer, Invertible):\n    """"""\n    Affine Grid Warper class.\n\n    The affine grid warper generates a reference grid of n-dimensional points\n    and warps it via an affine transformation model determined by an input\n    parameter Tensor. Some of the transformation parameters can be fixed at\n    construction time via an `AffineWarpConstraints` object.\n    """"""\n\n    def __init__(self,\n                 source_shape,\n                 output_shape,\n                 constraints=None,\n                 name=\'affine_grid_warper\'):\n        """"""Constructs an AffineGridWarper.\n\n        `source_shape` and `output_shape` are used to define shape of source\n        and output signal domains, as opposed to the shape of the respective\n        Tensors.\n        For example, for an image of size `width=W` and `height=H`,\n        `{source,output}_shape=[H, W]`;\n        for a volume of size `width=W`, `height=H` and `depth=D`,\n        `{source,output}_shape=[H, W, D]`.\n\n        Args:\n          source_shape: Iterable of integers determining shape of source\n            signal domain.\n          output_shape: Iterable of integers determining shape of destination\n            resampled signal domain.\n          constraints: Either a double list of shape `[N, N+1]`\n            defining constraints\n            on the entries of a matrix defining an affine transformation in N\n            dimensions, or an `AffineWarpConstraints` object.\n            If the double list is passed, a numeric value bakes\n            in a constraint on the corresponding\n            entry in the transformation matrix, whereas `None` implies that the\n            corresponding entry will be specified at run time.\n          name: Name of module.\n\n        Raises:\n          Error: If constraints fully define the affine transformation; or if\n            input grid shape and constraints have different dimensionality.\n          TypeError: If output_shape and source_shape are not both iterable.\n        """"""\n        self._source_shape = tuple(source_shape)\n        self._output_shape = tuple(output_shape)\n        num_dim = len(source_shape)\n        if isinstance(constraints, AffineWarpConstraints):\n            self._constraints = constraints\n        elif constraints is None:\n            self._constraints = AffineWarpConstraints.no_constraints(num_dim)\n        else:\n            self._constraints = AffineWarpConstraints(constraints=constraints)\n\n        if self._constraints.num_free_params == 0:\n            tf.logging.fatal(\'Transformation is fully constrained.\')\n            raise ValueError\n\n        if self._constraints.num_dim != num_dim:\n            tf.logging.fatal(\'Incompatible set of constraints provided: \'\n                             \'input grid shape and constraints have different \'\n                             \'dimensionality.\')\n            raise ValueError\n\n        GridWarperLayer.__init__(\n            self,\n            source_shape=source_shape,\n            output_shape=output_shape,\n            coeff_shape=[6],\n            name=name,\n            constraints=self._constraints)\n\n    def _create_features(self, constraints):\n        """"""\n        Creates all the matrices needed to compute the output warped grids.\n        """"""\n        affine_warp_constraints = constraints\n        if not isinstance(affine_warp_constraints, AffineWarpConstraints):\n            affine_warp_constraints = AffineWarpConstraints(constraints)\n        psi = _create_affine_features(output_shape=self._output_shape,\n                                      source_shape=self._source_shape,\n                                      relative=True)\n        psi = np.asarray(psi)\n        scales = [(x - 1.0) * .5 for x in self._source_shape]\n        offsets = scales\n\n        # Transforming a point x\'s i-th coordinate via an affine transformation\n        # is performed via the following dot product:\n        #\n        #  x_i\' = s_i * (T_i * x) + t_i                                    (1)\n        #\n        # where Ti is the i-th row of an affine matrix, and the scalars\n        # s_i and t_i define a decentering and global scaling into\n        # the source space.\n        #\n        # In the AffineGridWarper some of the entries of Ti are provided via the\n        # input, some others are instead fixed, according to the constraints\n        # assigned in the constructor.\n        # In create_features the internal dot product (1) is accordingly\n        # broken down into two parts:\n        #\n        # x_i\' = Ti[uncon_i] * x[uncon_i, :] + offset(con_var)             (2)\n        #\n        # i.e. the sum of the dot product of the free parameters (coming\n        # from the input) indexed by uncond_i and an offset obtained by\n        # precomputing the fixed part of (1) according to the constraints.\n        # This step is implemented by analyzing row by row\n        # the constraints matrix and saving into a list\n        # the x[uncon_i] and offset(con_var) data matrices\n        # for each output dimension.\n        #\n        # constraint -- None, indicates dynamic element of in the affine mat.\n\n        spatial_rank = len(self._source_shape)\n        features = []\n        # computes dynamic elements in the affine\n        for i in range(spatial_rank):\n            is_fixed = affine_warp_constraints[i]\n            x_i = np.array(\n                [x for x, fixed_var in zip(psi, is_fixed) if fixed_var is None])\n            features.append(x_i * scales[i] if len(x_i) else None)\n\n        # computes fixed elements in the affine\n        for i in range(spatial_rank):\n            all_elements = np.asarray(affine_warp_constraints[i])\n            dynamic_elements = all_elements == np.array(None)\n            if np.all(dynamic_elements):\n                x_i = None\n            else:\n                all_elements[dynamic_elements] = 0.0\n                x_i = np.dot(all_elements, psi) * scales[i]\n            features.append(x_i)\n\n        # appending global offsets to the list\n        features = features + offsets\n        return features\n\n    @property\n    def constraints(self):\n        return self._constraints\n\n    def layer_op(self, inputs):\n        """"""Assembles the module network and adds it to the graph.\n\n        The internal computation graph is assembled according to the set of\n        constraints provided at construction time.\n\n        inputs shape: batch_size x num_free_params\n\n        Args:\n          inputs: Tensor containing a batch of transformation parameters.\n\n        Returns:\n          A batch of warped grids.\n\n        Raises:\n          Error: If the input tensor size is not consistent\n            with the constraints passed at construction time.\n        """"""\n        inputs = tf.to_float(inputs)\n        batch_size, number_of_params = list(inputs.shape)\n        input_dtype = inputs.dtype.as_numpy_dtype\n        if number_of_params != self._constraints.num_free_params:\n            tf.logging.fatal(\n                \'Input size is not consistent with constraint \'\n                \'definition: (N, %s) parameters expected \'\n                \'(where N is the batch size; > 1), but %s provided.\',\n                self._constraints.num_free_params, inputs.shape)\n            raise ValueError\n        spatial_rank = len(self._source_shape)\n\n        warped_grid = []\n        var_index_offset = 0\n        for i in range(spatial_rank):\n            if self._psi[i] is not None:\n                # The i-th output dimension is not fully specified\n                # by the constraints, the graph is setup to perform\n                # matrix multiplication in batch mode.\n                grid_coord = self._psi[i].astype(input_dtype)\n\n                num_active_vars = self._psi[i].shape[0]\n                var_start = var_index_offset\n                var_index_offset += num_active_vars\n                warped_coord = tf.matmul(\n                    inputs[:, var_start:var_index_offset], grid_coord)\n                offset = self._psi[spatial_rank + i]\n                if offset is not None:\n                    offset = offset.astype(input_dtype)\n                    # Some entries in the i-th row\n                    # of the affine matrix were constrained\n                    # and the corresponding matrix\n                    # multiplications have been precomputed.\n                    tiling_params = tf.concat([\n                        [batch_size], tf.ones_like(offset.shape)], 0)\n                    offset = np.expand_dims(offset, 0)\n                    warped_coord += tf.tile(offset, tiling_params)\n\n            else:\n                # The i-th output dimension is fully specified\n                # by the constraints, and the corresponding matrix\n                # multiplications have been precomputed.\n                warped_coord = \\\n                    self._psi[spatial_rank + i].astype(input_dtype)\n                tiling_params = tf.concat([\n                    [batch_size], tf.ones_like(warped_coord.shape)], 0)\n                warped_coord = np.expand_dims(warped_coord, 0)\n                warped_coord = tf.tile(warped_coord, tiling_params)\n\n            # update global offset\n            warped_coord = warped_coord + self._psi[i + 2 * spatial_rank]\n            # Need to help TF figuring out shape inference\n            # since tiling information\n            # is held in Tensors which are not known until run time.\n            warped_coord.set_shape([batch_size, np.prod(self._output_shape)])\n            warped_grid.append(warped_coord)\n\n        # Reshape all the warped coordinates tensors to\n        # match the specified output\n        # shape and concatenate into a single matrix.\n        warped_grid = [tf.reshape(grid, (batch_size,) + self._output_shape)\n                       for grid in warped_grid]\n        return tf.stack(warped_grid, -1)\n\n    def inverse_op(self, name=None):\n        """"""\n        Returns a layer to compute inverse affine transforms.\n\n          The function first assembles a network that\n          given the constraints of the\n          current AffineGridWarper and a set of input parameters,\n          retrieves the coefficients of the corresponding inverse\n          affine transform, then feeds its output into a new\n          AffineGridWarper setup to correctly warp the `output`\n          space into the `source` space.\n\n        Args:\n          name: Name of module implementing the inverse grid transformation.\n\n        Returns:\n          A `sonnet` module performing the inverse affine transform\n          of a reference grid of points via an AffineGridWarper module.\n\n        Raises:\n          tf.errors.UnimplementedError: If the function is called on a non 2D\n            instance of AffineGridWarper.\n        """"""\n        if self._coeff_shape != [6]:\n            tf.logging.fatal(\'AffineGridWarper currently supports\'\n                             \'inversion only for the 2D case.\')\n            raise NotImplementedError\n\n        def _affine_grid_warper_inverse(inputs):\n            """"""Assembles network to compute inverse affine transformation.\n\n            Each `inputs` row potentially contains [a, b, tx, c, d, ty]\n            corresponding to an affine matrix:\n\n              A = [a, b, tx],\n                  [c, d, ty]\n\n            We want to generate a tensor containing the coefficients of the\n            corresponding inverse affine transformation in a constraints-aware\n            fashion.\n            Calling M:\n\n              M = [a, b]\n                  [c, d]\n\n            the affine matrix for the inverse transform is:\n\n               A_in = [M^(-1), M^-1 * [-tx, -tx]^T]\n\n            where\n\n              M^(-1) = (ad - bc)^(-1) * [ d, -b]\n                                        [-c,  a]\n\n            Args:\n              inputs: Tensor containing a batch of transformation parameters.\n\n            Returns:\n              A tensorflow graph performing the inverse affine transformation\n              parametrized by the input coefficients.\n            """"""\n            batch_size = tf.expand_dims(tf.shape(inputs)[0], 0)\n            constant_shape = tf.concat(\n                [batch_size, tf.convert_to_tensor((1,))], 0)\n\n            index = iter(range(6))\n\n            def get_variable(constraint):\n                if constraint is None:\n                    i = next(index)\n                    return inputs[:, i:i + 1]\n                else:\n                    return tf.fill(constant_shape,\n                                   tf.constant(constraint, dtype=inputs.dtype))\n\n            constraints = chain.from_iterable(self.constraints)\n            a, b, tx, c, d, ty = (get_variable(constr) for constr in\n                                  constraints)\n\n            det = a * d - b * c\n            a_inv = d / det\n            b_inv = -b / det\n            c_inv = -c / det\n            d_inv = a / det\n\n            m_inv = tf.reshape(\n                tf.concat([a_inv, b_inv, c_inv, d_inv], 1), [-1, 2, 2])\n\n            txy = tf.expand_dims(tf.concat([tx, ty], 1), 2)\n\n            txy_inv = tf.reshape(tf.matmul(m_inv, txy), [-1, 2])\n            tx_inv = txy_inv[:, 0:1]\n            ty_inv = txy_inv[:, 1:2]\n\n            inverse_gw_inputs = tf.concat(\n                [a_inv, b_inv, -tx_inv, c_inv, d_inv, -ty_inv], 1)\n\n            agw = AffineGridWarperLayer(self.output_shape, self.source_shape)\n\n            return agw(inverse_gw_inputs)  # pylint: disable=not-callable\n\n        if name is None:\n            name = self.name + \'_inverse\'\n        return LayerFromCallable(_affine_grid_warper_inverse, name=name)\n\n\nclass AffineWarpConstraints(object):\n    """"""Affine warp constraints class.\n\n    `AffineWarpConstraints` allow for\n    very succinct definitions of constraints on\n    the values of entries in affine transform matrices.\n    """"""\n\n    def __init__(self, constraints=((None,) * 3,) * 2):\n        """"""Creates a constraint definition for an affine transformation.\n\n        Args:\n          constraints: A doubly-nested iterable of shape `[N, N+1]`\n          defining constraints on the entries of a matrix that\n          represents an affine transformation in `N` dimensions.\n          A numeric value bakes in a constraint on the corresponding\n          entry in the transformation matrix, whereas `None` implies that\n          the corresponding entry will be specified at run time.\n\n        Raises:\n          TypeError: If `constraints` is not a nested iterable.\n          ValueError: If the double iterable `constraints` has inconsistent\n            dimensions.\n        """"""\n        try:\n            self._constraints = tuple(tuple(x) for x in constraints)\n        except TypeError:\n            tf.logging.fatal(\'constraints must be a nested iterable.\')\n            raise TypeError\n\n        # Number of rows\n        self._num_dim = len(self._constraints)\n        expected_num_cols = self._num_dim + 1\n        if any(len(x) != expected_num_cols for x in self._constraints):\n            tf.logging.fatal(\n                \'The input list must define a Nx(N+1) matrix of constraints.\')\n            raise ValueError\n\n    def _calc_num_free_params(self):\n        """"""Computes number of non constrained parameters.""""""\n        return sum(row.count(None) for row in self._constraints)\n\n    @property\n    def num_free_params(self):\n        return self._calc_num_free_params()\n\n    @property\n    def constraints(self):\n        return self._constraints\n\n    @property\n    def num_dim(self):\n        return self._num_dim\n\n    def __getitem__(self, i):\n        """"""\n        Returns the list of constraints\n        for the i-th row of the affine matrix.\n        """"""\n        return self._constraints[i]\n\n    def _combine(self, x, y):\n        """"""\n        Combines two constraints,\n        raising an error if they are not compatible.\n        """"""\n        if x is None or y is None:\n            return x or y\n        if x != y:\n            tf.logging.fatal(\'Incompatible set of constraints provided.\')\n            raise ValueError\n        return x\n\n    def __and__(self, rhs):\n        """"""Combines two sets of constraints into a coherent single set.""""""\n        return self.combine_with(rhs)\n\n    def combine_with(self, additional_constraints):\n        """"""Combines two sets of constraints into a coherent single set.""""""\n        x = additional_constraints\n        if not isinstance(additional_constraints, AffineWarpConstraints):\n            x = AffineWarpConstraints(additional_constraints)\n        new_constraints = []\n        for left, right in zip(self._constraints, x.constraints):\n            new_constraints.append(\n                [self._combine(x, y) for x, y in zip(left, right)])\n        return AffineWarpConstraints(new_constraints)\n\n    # Collection of utilities to initialize an AffineGridWarper in 2D and 3D.\n    @classmethod\n    def no_constraints(cls, num_dim=2):\n        """"""\n        Empty set of constraints for a num_dim affine transform.\n        """"""\n        return cls(((None,) * (num_dim + 1),) * num_dim)\n\n    @classmethod\n    def translation_2d(cls, x=None, y=None):\n        """"""\n        Assign constraints on translation components of\n        affine transform in 2d.\n        """"""\n        return cls([[None, None, x],\n                    [None, None, y]])\n\n    @classmethod\n    def translation_3d(cls, x=None, y=None, z=None):\n        """"""\n        Assign constraints on translation components of\n        affine transform in 3d.\n        """"""\n        return cls([[None, None, None, x],\n                    [None, None, None, y],\n                    [None, None, None, z]])\n\n    @classmethod\n    def scale_2d(cls, x=None, y=None):\n        """"""\n        Assigns constraints on scaling components of\n        affine transform in 2d.\n        """"""\n        return cls([[x, None, None],\n                    [None, y, None]])\n\n    @classmethod\n    def scale_3d(cls, x=None, y=None, z=None):\n        """"""\n        Assigns constraints on scaling components of\n        affine transform in 3d.\n        """"""\n        return cls([[x, None, None, None],\n                    [None, y, None, None],\n                    [None, None, z, None]])\n\n    @classmethod\n    def shear_2d(cls, x=None, y=None):\n        """"""\n        Assigns constraints on shear components of\n        affine transform in 2d.\n        """"""\n        return cls([[None, x, None],\n                    [y, None, None]])\n\n    @classmethod\n    def no_shear_2d(cls):\n        return cls.shear_2d(x=0, y=0)\n\n    @classmethod\n    def no_shear_3d(cls):\n        """"""\n        Assigns constraints on shear components of\n        affine transform in 3d.\n        """"""\n        return cls([[None, 0, 0, None],\n                    [0, None, 0, None],\n                    [0, 0, None, None]])\n\n\ndef _create_affine_features(output_shape, source_shape, relative=False):\n    """"""\n    Generates n-dimensional homogeneous coordinates\n    for a given grid definition.\n    `source_shape` and `output_shape` are used to\n    define the size of the source and output signal domains.\n\n    For example,\n    for an image of size `width=W` and `height=H`,\n    `{source,output}_shape=[H, W]`;\n    for a volume of size `width=W`, `height=H` and `depth=D`,\n    `{source,output}_shape=[H, W, D]`.\n\n    Note returning in Matrix indexing \'ij\'\n\n    Args:\n      output_shape: Iterable of integers determining\n        the shape of the grid to be warped.\n      source_shape: Iterable of integers determining\n        the domain of the signal to be resampled.\n    Returns:\n      List of flattened numpy arrays of coordinates\n      When the dimensionality of `output_shape` is smaller that that of\n      `source_shape` the last rows before [1, ..., 1] will be filled with -1.\n    """"""\n    dim_gap = len(source_shape) - len(output_shape)\n    embedded_output_shape = list(output_shape) + [1] * dim_gap\n    if not relative:\n        ranges = [np.arange(dim, dtype=np.float32)\n                  for dim in embedded_output_shape]\n    else:\n        ranges = [np.linspace(-1., 1., x, dtype=np.float32)\n                  for x in embedded_output_shape]\n    ranges.append(np.array([1.0]))\n    return [x.ravel() for x in np.meshgrid(*ranges, indexing=\'ij\')]\n'"
niftynet/layer/histogram_normalisation.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nThis class computes histogram based normalisation. A `training`\nprocess is first used to find an averaged histogram mapping\nfrom all training volumes.  This layer maintains the mapping array,\nand the layer_op maps the intensity of new volumes to a normalised version.\nThe histogram is computed from foreground if a definition is provided for\nforeground (by `binary_masking_func` or a `mask` matrix)\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nimport niftynet.utilities.histogram_standardisation as hs\nfrom niftynet.layer.base_layer import DataDependentLayer\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\n\n\nclass HistogramNormalisationLayer(DataDependentLayer):\n    def __init__(self,\n                 image_name,\n                 modalities,\n                 model_filename=None,\n                 binary_masking_func=None,\n                 norm_type=\'percentile\',\n                 cutoff=(0.05, 0.95),\n                 name=\'hist_norm\'):\n        """"""\n\n        :param image_name:\n        :param modalities:\n        :param model_filename:\n        :param binary_masking_func: set to None for global mapping\n        :param norm_type:\n        :param cutoff:\n        :param name:\n        """"""\n\n        super(HistogramNormalisationLayer, self).__init__(name=name)\n        if model_filename is None:\n            model_filename = os.path.join(\'.\', \'histogram_ref_file.txt\')\n        self.model_file = os.path.abspath(model_filename)\n        assert not os.path.isdir(self.model_file), \\\n            ""model_filename is a directory, "" \\\n            ""please change histogram_ref_file to a filename.""\n\n        if binary_masking_func:\n            assert isinstance(binary_masking_func, BinaryMaskingLayer)\n            self.binary_masking_func = binary_masking_func\n        else:\n            self.binary_masking_func = None\n        self.norm_type = norm_type\n        self.cutoff = cutoff\n\n        # mapping is a complete cache of the model file, the total number of\n        # modalities are listed in self.modalities tuple\n        self.image_name = image_name\n        self.modalities = modalities\n        self.mapping = hs.read_mapping_file(self.model_file)\n\n    def layer_op(self, image, mask=None):\n        assert self.is_ready(), \\\n            ""histogram normalisation layer needs to be trained first.""\n        if isinstance(image, dict):\n            image_5d = np.asarray(image[self.image_name], dtype=np.float32)\n        else:\n            image_5d = np.asarray(image, dtype=np.float32)\n\n        if isinstance(mask, dict):\n            image_mask = mask.get(self.image_name, None)\n        elif mask is not None:\n            image_mask = mask\n        elif self.binary_masking_func is not None:\n            image_mask = self.binary_masking_func(image_5d)\n        else:\n            # no access to mask, default to all image\n            image_mask = np.ones_like(image_5d, dtype=np.bool)\n\n        normalised = self._normalise_5d(image_5d, image_mask)\n\n        if isinstance(image, dict):\n            image[self.image_name] = normalised\n            if isinstance(mask, dict):\n                mask[self.image_name] = image_mask\n            else:\n                mask = {self.image_name: image_mask}\n            return image, mask\n        else:\n            return normalised, image_mask\n\n    def __check_modalities_to_train(self):\n        modalities_to_train = [mod for mod in self.modalities\n                               if mod not in self.mapping]\n        return set(modalities_to_train)\n\n    def is_ready(self):\n        mod_to_train = self.__check_modalities_to_train()\n        return False if mod_to_train else True\n\n    def train(self, image_list):\n        # check modalities to train, using the first subject in subject list\n        # to find input modality list\n        if self.is_ready():\n            tf.logging.info(\n                ""normalisation histogram reference models ready""\n                "" for {}:{}"".format(self.image_name, self.modalities))\n            return\n        mod_to_train = self.__check_modalities_to_train()\n        tf.logging.info(\n            ""training normalisation histogram references ""\n            ""for {}:{}, using {} subjects"".format(\n                self.image_name, mod_to_train, len(image_list)))\n        trained_mapping = hs.create_mapping_from_multimod_arrayfiles(\n            image_list,\n            self.image_name,\n            self.modalities,\n            mod_to_train,\n            self.cutoff,\n            self.binary_masking_func)\n\n        # merging trained_mapping dict and self.mapping dict\n        self.mapping.update(trained_mapping)\n        all_maps = hs.read_mapping_file(self.model_file)\n        all_maps.update(self.mapping)\n        hs.write_all_mod_mapping(self.model_file, all_maps)\n\n    def _normalise_5d(self, data_array, mask_array):\n        assert self.modalities\n        assert data_array.ndim == 5\n        assert data_array.shape[4] <= len(self.modalities)\n\n        if not self.mapping:\n            tf.logging.fatal(\n                ""calling normaliser with empty mapping,""\n                ""probably {} is not loaded"".format(self.model_file))\n            raise RuntimeError\n        mask_array = np.asarray(mask_array, dtype=np.bool)\n        for mod_id, mod_name in enumerate(self.modalities):\n            if not np.any(data_array[..., mod_id]):\n                continue  # missing modality\n            data_array[..., mod_id] = self.__normalise(\n                data_array[..., mod_id],\n                mask_array[..., mod_id],\n                self.mapping[mod_name])\n        return data_array\n\n    def __normalise(self, img_data, mask, mapping):\n        return hs.transform_by_mapping(\n            img_data, mask, mapping, self.cutoff, self.norm_type)\n'"
niftynet/layer/layer_util.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\n\n\ndef check_spatial_dims(input_tensor, criteria):\n    """"""\n    valid each of the spatial dims against `criteria`\n    criteria can be a lambda function\n    e.g. lambda x : x > 10 checks whether each dim is greater than 10\n    """"""\n    input_shape = input_tensor.shape\n    if not input_shape.is_fully_defined():\n        # skip checking if the input has dynamic shapes\n        return True\n    input_shape.with_rank_at_least(3)\n    spatial_dims = input_shape[1:-1].as_list()\n    all_dims_satisfied = np.all([criteria(x) for x in spatial_dims])\n    if not all_dims_satisfied:\n        import inspect\n        raise ValueError(""input tensor\'s spatial dimensionality""\n                         "" not compatible, please tune ""\n                         ""the input window sizes. ""\n                         ""(e.g. lambda x : x % 8 == 0 checks whether each dimension is divisible by 8)\\n{}"".format(\n            inspect.getsource(criteria)))\n    return all_dims_satisfied\n\n\ndef infer_spatial_rank(input_tensor):\n    """"""\n    e.g. given an input tensor [Batch, X, Y, Z, Feature] the spatial rank is 3\n    """"""\n    input_shape = input_tensor.shape\n    input_shape.with_rank_at_least(3)\n    #dims = input_tensor.get_shape().ndims - 2\n    #assert dims > 0, ""input tensor should have at least one spatial dim, "" \\\n    #                 ""in addition to batch and channel dims""\n    return int(input_shape.ndims - 2)\n\n\ndef trivial_kernel(kernel_shape):\n    """"""\n    This function generates a trivial kernel with all 0s except for the\n    element in its spatial center\n    e.g. trivial_kernel((3, 3, 1, 1,)) returns a kernel of::\n\n        [[[[0]], [[0]], [[0]]],\n         [[[0]], [[1]], [[0]]],\n         [[[0]], [[0]], [[0]]]]\n\n    kernel_shape[-1] and kernel_shape[-2] should be 1, so that it operates\n    on the spatial dims only.  However, there is no exact spatial centre\n    if np.any((kernel_shape % 2) == 0). This is fine in many cases\n    as np.sum(trivial_kernel(kernel_shape)) == 1\n    """"""\n    assert kernel_shape[-1] == 1\n    assert kernel_shape[-2] == 1\n    # assert np.all((kernel_shape % 2) == 1)\n    kernel = np.zeros(kernel_shape)\n    flattened = kernel.reshape(-1)\n    flattened[np.prod(kernel_shape) // 2] = 1\n    return flattened.reshape(kernel_shape)\n\n\ndef expand_spatial_params(input_param, spatial_rank, param_type=int):\n    """"""\n    Expand input parameter\n    e.g., ``kernel_size=3`` is converted to ``kernel_size=[3, 3, 3]``\n    for 3D images (when ``spatial_rank == 3``).\n    """"""\n    spatial_rank = int(spatial_rank)\n    try:\n        if param_type == int:\n            input_param = int(input_param)\n        else:\n            input_param = float(input_param)\n        return (input_param,) * spatial_rank\n    except (ValueError, TypeError):\n        pass\n    try:\n        if param_type == int:\n            input_param = \\\n                np.asarray(input_param).flatten().astype(np.int).tolist()\n        else:\n            input_param = \\\n                np.asarray(input_param).flatten().astype(np.float).tolist()\n    except (ValueError, TypeError):\n        # skip type casting if it\'s a TF tensor\n        pass\n    assert len(input_param) >= spatial_rank, \\\n        \'param length should be at least have the length of spatial rank\'\n    return tuple(input_param[:spatial_rank])\n\n# class RequireKeywords(object):\n#    def __init__(self, *list_of_keys):\n#        self.keys = list_of_keys\n#\n#    def __call__(self, f):\n#        def wrapped(*args, **kwargs):\n#            for key in self.keys:\n#                if key not in kwargs:\n#                    raise ValueError(""{}: specify keywords: \'{}\'"".format(\n#                        args[0].layer_scope().name, self.keys))\n#            return f(*args, **kwargs)\n#        return wrapped\n\n\ndef check_divisible_channels(input_tensor, n_channel_splits):\n    """"""\n    Check if the number of channels (last dim) of the input tensor\n    is divisible by ``n_channel_splits``. If True, returns\n    ``n_input_channels / n_channel_splits``, raises AssertionError otherwise\n\n    :param input_tensor:\n    :param n_channel_splits:\n    :return: n_input_channels / n_channel_splits\n    """"""\n\n    n_input_channels = int(input_tensor.shape.as_list()[-1])\n    n_channel_splits = int(n_channel_splits)\n    assert n_channel_splits > 0 and n_input_channels % n_channel_splits == 0, \\\n        ""Number of feature channels should be divisible by "" \\\n        ""n_channel_splits {}, so that given an input with n_input_channels, "" \\\n        ""the output tensor will have "" \\\n        ""n_input_channels / n_channel_splits."".format(n_channel_splits)\n    return n_input_channels / n_channel_splits\n'"
niftynet/layer/linear_resize.py,10,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.layer_util import expand_spatial_params\nfrom niftynet.layer.layer_util import infer_spatial_rank\n\n\nclass LinearResizeLayer(Layer):\n    """"""\n    Resize 2D/3D images using ``tf.image.resize_bilinear``\n    (without trainable parameters).\n    """"""\n\n    def __init__(self, new_size, name=\'trilinear_resize\'):\n        """"""\n\n        :param new_size: integer or a list of integers set the output\n            2D/3D spatial shape.  If the parameter is an integer ``d``,\n            it\'ll be expanded to ``(d, d)`` and ``(d, d, d)`` for 2D and\n            3D inputs respectively.\n        :param name: layer name string\n        """"""\n        super(LinearResizeLayer, self).__init__(name=name)\n        self.new_size = new_size\n\n    def layer_op(self, input_tensor):\n        """"""\n        Resize the image by linearly interpolating the input\n        using TF ``resize_bilinear`` function.\n\n        :param input_tensor: 2D/3D image tensor, with shape:\n            ``batch, X, Y, [Z,] Channels``\n        :return: interpolated volume\n        """"""\n\n        input_spatial_rank = infer_spatial_rank(input_tensor)\n        assert input_spatial_rank in (2, 3), \\\n            ""linearly interpolation layer can only be applied to "" \\\n            ""2D/3D images (4D or 5D tensor).""\n        self.new_size = expand_spatial_params(self.new_size, input_spatial_rank)\n\n        if input_spatial_rank == 2:\n            return tf.image.resize_bilinear(input_tensor, self.new_size)\n\n        b_size, x_size, y_size, z_size, c_size = \\\n            input_tensor.shape.as_list()\n        x_size_new, y_size_new, z_size_new = self.new_size\n\n        if (x_size == x_size_new) and (y_size == y_size_new) and (\n                z_size == z_size_new):\n            # already in the target shape\n            return input_tensor\n\n        # resize y-z\n        squeeze_b_x = tf.reshape(\n            input_tensor, [-1, y_size, z_size, c_size])\n        resize_b_x = tf.image.resize_bilinear(\n            squeeze_b_x, [y_size_new, z_size_new])\n        resume_b_x = tf.reshape(\n            resize_b_x, [b_size, x_size, y_size_new, z_size_new, c_size])\n\n        # resize x\n        #   first reorient\n        reoriented = tf.transpose(resume_b_x, [0, 3, 2, 1, 4])\n        #   squeeze and 2d resize\n        squeeze_b_z = tf.reshape(\n            reoriented, [-1, y_size_new, x_size, c_size])\n        resize_b_z = tf.image.resize_bilinear(\n            squeeze_b_z, [y_size_new, x_size_new])\n        resume_b_z = tf.reshape(\n            resize_b_z, [b_size, z_size_new, y_size_new, x_size_new, c_size])\n\n        output_tensor = tf.transpose(resume_b_z, [0, 3, 2, 1, 4])\n        return output_tensor\n'"
niftynet/layer/loss_autoencoder.py,10,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_factory import LossAutoencoderFactory\nfrom niftynet.layer.base_layer import Layer\n\n\nclass LossFunction(Layer):\n    def __init__(self,\n                 loss_type=\'VariationalLowerBound\',\n                 loss_func_params=None,\n                 name=\'loss_function\'):\n\n        super(LossFunction, self).__init__(name=name)\n        if loss_func_params is not None:\n            self._loss_func_params = loss_func_params\n        else:\n            self._loss_func_params = {}\n        self._data_loss_func = None\n        self.make_callable_loss_func(loss_type)\n\n    def make_callable_loss_func(self, type_str):\n        self._data_loss_func = LossAutoencoderFactory.create(type_str)\n\n    def layer_op(self, prediction):\n        with tf.device(\'/cpu:0\'):\n            return self._data_loss_func(prediction, **self._loss_func_params)\n\n\ndef variational_lower_bound(prediction):\n    """"""\n    This is the variational lower bound derived in\n    Auto-Encoding Variational Bayes, Kingma & Welling, 2014\n\n    :param prediction: [posterior_means, posterior_logvar,\n        data_means, data_logvar, originals]\n\n        posterior_means: predicted means for the posterior\n\n        posterior_logvar: predicted log variances for the posterior\n        data_means: predicted mean parameter\n        for the voxels modelled as Gaussians\n\n        data_logvar: predicted log variance parameter\n        for the voxels modelled as Gaussians\n\n        originals: the original inputs\n    :return:\n    """"""\n\n    # log_2pi = np.log(2*np.pi)\n    log_2pi = 1.837877\n    assert len(prediction) >= 5, \\\n        ""please see the returns of network/vae.py"" \\\n        ""for the prediction list format""\n    posterior_means, posterior_logvar = prediction[:2]\n    data_means, data_logvar = prediction[2:4]\n    originals = prediction[4]\n\n    squared_diff = tf.square(data_means - originals)\n    log_likelihood = \\\n        data_logvar + log_2pi + tf.exp(-data_logvar) * squared_diff\n    # batch_size = tf.shape(log_likelihood)[0]\n    batch_size = log_likelihood.shape.as_list()[0]\n    log_likelihood = tf.reshape(log_likelihood, shape=[batch_size, -1])\n    log_likelihood = -0.5 * tf.reduce_sum(log_likelihood, axis=[1])\n\n    KL_divergence = 1 + posterior_logvar \\\n                    - tf.square(posterior_means) \\\n                    - tf.exp(posterior_logvar)\n    KL_divergence = -0.5 * tf.reduce_sum(KL_divergence, axis=[1])\n    return tf.reduce_mean(KL_divergence - log_likelihood)\n'"
niftynet/layer/loss_classification.py,6,"b'# -*- coding: utf-8 -*-\n""""""\nLoss functions for multi-class classification\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.application_factory import LossClassificationFactory\nfrom niftynet.layer.base_layer import Layer\n\n\nclass LossFunction(Layer):\n    def __init__(self,\n                 n_class,\n                 loss_type=\'CrossEntropy\',\n                 loss_func_params=None,\n                 name=\'loss_function\'):\n\n        super(LossFunction, self).__init__(name=name)\n        self._num_classes = n_class\n        if loss_func_params is not None:\n            self._loss_func_params = loss_func_params\n        else:\n            self._loss_func_params = {}\n        self._data_loss_func = None\n        self.make_callable_loss_func(loss_type)\n\n    def make_callable_loss_func(self, type_str):\n        self._data_loss_func = LossClassificationFactory.create(type_str)\n\n    def layer_op(self,\n                 prediction,\n                 ground_truth=None,\n                 var_scope=None, ):\n        """"""\n        Compute loss from `prediction` and `ground truth`,\n\n        if `prediction `is list of tensors, each element of the list\n        will be compared against `ground_truth`.\n\n        :param prediction: input will be reshaped into (N, num_classes)\n        :param ground_truth: input will be reshaped into (N,)\n        :param var_scope:\n        :return:\n        """"""\n\n        with tf.device(\'/cpu:0\'):\n            if ground_truth is not None:\n                ground_truth = tf.reshape(ground_truth, [-1])\n\n            if not isinstance(prediction, (list, tuple)):\n                prediction = [prediction]\n            # prediction should be a list for holistic networks\n            if self._num_classes > 0:\n                # reshape the prediction to [n_voxels , num_classes]\n                prediction = [tf.reshape(pred, [-1, self._num_classes])\n                              for pred in prediction]\n\n            data_loss = []\n            for pred in prediction:\n                if self._loss_func_params:\n                    data_loss.append(self._data_loss_func(\n                        pred, ground_truth,\n                        **self._loss_func_params))\n                else:\n                    data_loss.append(self._data_loss_func(\n                        pred, ground_truth))\n            return tf.reduce_mean(data_loss)\n\n\ndef cross_entropy(prediction,\n                  ground_truth):\n    """"""\n    Function to calculate the cross entropy loss\n    :param prediction: the logits (before softmax)\n    :param ground_truth: the classification ground truth\n    :return: the loss\n    """"""\n    ground_truth = tf.to_int64(ground_truth)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prediction, labels=ground_truth)\n    return loss\n'"
niftynet/layer/loss_classification_multi.py,69,"b'# -*- coding: utf-8 -*-\n""""""\nLoss functions for multi-class classification\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.application_factory import LossClassificationMultiFactory\nfrom niftynet.layer.base_layer import Layer\n#from niftynet.layer.loss_segmentation import labels_to_one_hot\n\n\nclass LossFunction(Layer):\n    def __init__(self,\n                 n_class,\n                 n_rater,\n                 loss_type=\'CrossEntropy\',\n                 loss_func_params=None,\n                 name=\'loss_function\'):\n\n        super(LossFunction, self).__init__(name=name)\n        self._num_classes = n_class\n        self._num_raters = n_rater\n        if loss_func_params is not None:\n            self._loss_func_params = loss_func_params\n        else:\n            self._loss_func_params = {}\n        self._data_loss_func = None\n        self.make_callable_loss_func(loss_type)\n\n    def make_callable_loss_func(self, type_str):\n        self._data_loss_func = LossClassificationMultiFactory.create(type_str)\n\n    def layer_op(self,\n                 pred_ave=None,\n                 pred_multi=None,\n                 ground_truth=None,\n                 weight_batch=None,\n                 var_scope=None, ):\n        \'\'\'\n        Compute the losses in the case of a multirater setting\n        :param pred_ave: average of the predictions over the different raters\n        :param pred_multi: prediction for each individual rater\n        :param ground_truth: ground truth classification for each individual\n        rater\n        :param weight_batch:\n        :param var_scope:\n        :return:\n        \'\'\'\n\n\n        with tf.device(\'/cpu:0\'):\n            if ground_truth is not None:\n                ground_truth = tf.reshape(ground_truth, [-1,\n                                                         self._num_raters,\n                                                         self._num_classes])\n            if pred_ave is not None:\n                if not isinstance(pred_ave, (list, tuple)):\n                    pred_ave = [pred_ave]\n                if self._num_classes > 0 and pred_ave is not None:\n                        # reshape the prediction to [n_voxels , num_classes]\n                    pred_ave = [tf.reshape(pred, [-1, self._num_classes])\n                                for pred in pred_ave]\n            if pred_multi is not None and not isinstance(pred_multi, (list, \\\n                    tuple)):\n                pred_multi = [pred_multi]\n                if self._num_classes > 0 and pred_multi is not None:\n\n                    pred_multi = [tf.reshape(pred, [-1,\n                                                self._num_raters,\n                                                self._num_classes])\n                                    for pred in pred_multi]\n\n\n            data_loss = []\n            if ground_truth is not None:\n                if pred_multi is not None:\n                    if pred_ave is not None:\n                        for pred, pred_mul in zip(pred_ave, pred_multi):\n                            if self._loss_func_params:\n                                data_loss.append(\n                                    self._data_loss_func(ground_truth,\n                                                         pred, pred_mul,\n                                                         **self._loss_func_params))\n                            else:\n                                data_loss.append(self._data_loss_func(\n                                    ground_truth, pred, pred_mul))\n                    else:\n                        for pred_mul in pred_multi:\n                            if self._loss_func_params:\n                                data_loss.append(\n                                    self._data_loss_func(ground_truth,\n                                                         pred_mul,\n                                                         **self._loss_func_params))\n                            else:\n                                data_loss.append(self._data_loss_func(\n                                    ground_truth, pred_mul))\n                else:\n                    for pred in pred_ave:\n\n                        if self._loss_func_params:\n                            data_loss.append(self._data_loss_func(\n                                pred, ground_truth,\n                                **self._loss_func_params))\n                        else:\n                            data_loss.append(self._data_loss_func(\n                                pred, ground_truth))\n            elif pred_multi is not None:\n                for pred, pred_mul in zip(pred_ave, pred_multi):\n                    if self._loss_func_params:\n                        data_loss.append(self._data_loss_func(\n                            pred, pred_mul,\n                            **self._loss_func_params))\n                    else:\n                        data_loss.append(self._data_loss_func(\n                            pred, pred_mul))\n\n            if weight_batch is not None:\n                return tf.reduce_mean(weight_batch/tf.reduce_sum(\n                    weight_batch) * data_loss[0])\n            else:\n                return tf.reduce_mean(data_loss)\n\n#\ndef labels_to_one_hot(ground_truth, num_classes=1):\n    """"""\n    Converts ground truth labels to one-hot, sparse tensors.\n    Used extensively in segmentation losses.\n\n    :param ground_truth: ground truth categorical labels (rank `N`)\n    :param num_classes: A scalar defining the depth of the one hot dimension\n        (see `depth` of `tf.one_hot`)\n    :return: one-hot sparse tf tensor\n        (rank `N+1`; new axis appended at the end) and the output shape\n    """"""\n    # read input/output shapes\n    if isinstance(num_classes, tf.Tensor):\n        num_classes_tf = tf.to_int32(num_classes)\n    else:\n        num_classes_tf = tf.constant(num_classes, tf.int32)\n    input_shape = tf.shape(ground_truth)\n    output_shape = tf.concat(\n        [input_shape, tf.reshape(num_classes_tf, (1,))], 0)\n\n    if num_classes == 1:\n        # need a sparse representation?\n        print(\'no need\')\n        return tf.reshape(ground_truth, output_shape), output_shape\n\n    # squeeze the spatial shape\n\n    ground_truth = tf.reshape(ground_truth, (-1,))\n    # shape of squeezed output\n    dense_shape = tf.stack([tf.shape(ground_truth)[0],\n                            num_classes_tf], 0)\n    dense_shape = tf.Print(tf.cast(dense_shape, tf.int64), [dense_shape,\n                           output_shape], message=\'check_shape_lohe\')\n    # create a rank-2 sparse tensor\n    ground_truth = tf.to_int64(ground_truth)\n    ids = tf.range(tf.to_int64(tf.shape(ground_truth)[0]), dtype=tf.int64)\n    ids = tf.stack([ids, ground_truth], axis=1)\n    one_hot = tf.SparseTensor(\n        indices=ids,\n        values=tf.ones_like(ground_truth, dtype=tf.float32),\n        dense_shape=tf.to_int64(dense_shape))\n\n    # resume the spatial dims\n    one_hot = tf.sparse_reshape(one_hot, output_shape)\n    return one_hot, output_shape\n\n\ndef loss_confusion_matrix(ground_truth, pred_multi, num_classes=2, nrater=6):\n    \'\'\'\n    Creates a loss over the two multi rater confusion matrices between the rater\n    :param ground_truth: multi rater classification\n    :param pred_multi: multi rater prediction (1 pred per class for each\n    rater and each observation - A softmax is performed during the loss\n    calculation\n    :param nrater: number of raters\n    :return: integration over the absolute differences between the confusion\n    matrices divided by number of raters\n    \'\'\'\n\n    one_hot_gt, output_shape = labels_to_one_hot(ground_truth, num_classes)\n    dense_one_hot = tf.reshape(tf.sparse_tensor_to_dense(one_hot_gt),\n                               output_shape)\n    dense_one_hot = tf.reshape(dense_one_hot, tf.shape(pred_multi))\n    nclasses=tf.shape(pred_multi)[-1]\n    nn_pred = tf.nn.softmax(pred_multi,-1)\n    error_fin = tf.zeros([nclasses, nclasses])\n    error_fin = tf.Print(tf.cast(error_fin, tf.float32), [nn_pred, tf.shape(\n        pred_multi)], message=\'error\')\n    nn_pred = tf.Print(tf.cast(nn_pred, tf.float32), [tf.shape(\n        dense_one_hot), nclasses, tf.shape(ground_truth), tf.shape(nn_pred)],\n                       message=\'check_conf\')\n    for i in range(0, nrater):\n        for j in range(i+1, nrater):\n\n            confusion_pred = tf.matmul(tf.transpose(nn_pred[:, i, :]),\n                                       nn_pred[:, j, :])\n            confusion_gt = tf.matmul(tf.transpose(dense_one_hot[:, i, :]),\n                                     dense_one_hot[:, j, :])\n            error = tf.divide(tf.abs(confusion_gt - confusion_pred), tf.cast(\n                tf.shape(ground_truth)[0], tf.float32))\n            error_fin += error\n            error_fin = tf.Print(tf.cast(error,tf.float32), [tf.reduce_sum(\n                error_fin), tf.reduce_max(error_fin)], message=\'build_error\')\n    return tf.reduce_sum(error_fin)/tf.cast(nrater, tf.float32)\n\n\ndef variability(pred_multi, num_classes=2, nrater=2):\n    one_hot_gt, output_shape = labels_to_one_hot(tf.cast(pred_multi, tf.int64),\n                                           num_classes)\n    dense_one_hot = tf.sparse_tensor_to_dense(one_hot_gt)\n    freq = tf.divide(tf.reduce_sum(dense_one_hot, 1), tf.cast(tf.shape(\n        pred_multi)[1],tf.float32))\n    variability = tf.reduce_sum(tf.square(freq), -1)\n    return 1 - variability\n\n\ndef loss_variability(ground_truth, pred_multi, weight_map=None):\n    one_hot_gt, output_shape = labels_to_one_hot(tf.cast(ground_truth,\n                                                         tf.int64),\n                                   tf.shape(pred_multi)[-1])\n    dense_gt = tf.sparse_tensor_to_dense(one_hot_gt)\n    pred_hard = tf.argmax(pred_multi, -1)\n\n    one_hot_pred, _ = labels_to_one_hot(tf.cast(pred_hard, tf.int64),\n                                   tf.shape(pred_multi)[-1])\n    dense_pred = tf.sparse_tensor_to_dense(one_hot_pred)\n    freq_pred = tf.divide(tf.reduce_sum(dense_pred, 1),\n                          tf.cast(tf.shape(pred_multi)[1],tf.float32))\n    variability_pred = tf.reduce_sum(tf.square(freq_pred), -1)\n    freq_gt = tf.divide(tf.reduce_sum(dense_gt, 1),\n                        tf.cast(tf.shape(pred_multi)[1],tf.float32))\n    variability_gt = tf.reduce_sum(tf.square(freq_gt), -1)\n\n    diff_square = tf.square(variability_gt-variability_pred)\n    if weight_map is not None:\n        diff_square = weight_map * diff_square\n    loss = tf.sqrt(tf.reduce_mean(diff_square))\n    return loss\n\n\ndef rmse_consistency(pred_ave,\n                     pred_multi, weight_map=None):\n    pred_multi  = tf.nn.softmax(pred_multi, -1)\n    pred_multi_ave = tf.reduce_mean(pred_multi, axis=1)\n    pred_multi_ave = tf.Print(tf.cast(pred_multi_ave, tf.float32), [pred_ave[0],\n                                                        pred_multi_ave[0],\n                                                                    tf.shape(\n                                                                        pred_ave), tf.shape(pred_multi_ave),\n                              tf.reduce_max(pred_ave-pred_multi_ave)],\n                              message=\'rmse_test\')\n    diff_square = tf.square(pred_ave-pred_multi_ave)\n    if weight_map is not None:\n        diff_square = tf.multiply(weight_map, diff_square) / tf.reduce_sum(\n            weight_map)\n\n    return tf.sqrt(tf.reduce_mean(diff_square))\n\n\n'"
niftynet/layer/loss_gan.py,5,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_factory import LossGANFactory\nfrom niftynet.layer.base_layer import Layer\n\n\nclass LossFunction(Layer):\n    def __init__(self,\n                 loss_type='CrossEntropy',\n                 loss_func_params=None,\n                 name='loss_function'):\n\n        super(LossFunction, self).__init__(name=name)\n        if loss_func_params is not None:\n            self._loss_func_params = loss_func_params\n        else:\n            self._loss_func_params = {}\n        self._data_loss_func = None\n        self.make_callable_loss_func(loss_type)\n\n    def make_callable_loss_func(self, type_str):\n        self._data_loss_func = LossGANFactory.create(type_str)\n\n    def layer_op(self, pred_real, pred_fake, var_scope=None):\n        with tf.device('/cpu:0'):\n            g_loss = self._data_loss_func['g'](\n                pred_fake,\n                **self._loss_func_params)\n            d_fake = self._data_loss_func['d_fake'](\n                pred_fake,\n                **self._loss_func_params)\n            d_real = self._data_loss_func['d_real'](\n                pred_real,\n                **self._loss_func_params)\n        return g_loss, (d_fake + d_real)\n\n\ndef cross_entropy_function(is_real, softness=.1):\n    def cross_entropy_op(pred, **kwargs):\n        if is_real:\n            target = (1. - softness) * tf.ones_like(pred)\n        else:\n            target = softness * tf.ones_like(pred)\n        entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=pred,\n                                                          labels=target)\n        return tf.reduce_mean(entropy)\n\n    return cross_entropy_op\n\n\ncross_entropy = {'g': cross_entropy_function(True, 0),\n                 'd_fake': cross_entropy_function(False, 0),\n                 'd_real': cross_entropy_function(True, .1)}\n"""
niftynet/layer/loss_regression.py,72,"b'# -*- coding: utf-8 -*-\n""""""\nLoss functions for regression\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\n\nfrom niftynet.engine.application_factory import LossRegressionFactory\nfrom niftynet.layer.base_layer import Layer\n\n\nclass LossFunction(Layer):\n    def __init__(self,\n                 loss_type=\'L2Loss\',\n                 loss_func_params=None,\n                 name=\'loss_function\'):\n\n        super(LossFunction, self).__init__(name=name)\n\n        # set loss function and function-specific additional params.\n        self._data_loss_func = LossRegressionFactory.create(loss_type)\n        self._loss_func_params = \\\n            loss_func_params if loss_func_params is not None else {}\n        self._reshape = True\n        if loss_type == \'Cosine\':\n            self._reshape = False\n\n    def layer_op(self,\n                 prediction,\n                 ground_truth=None,\n                 weight_map=None):\n        """"""\n        Compute loss from ``prediction`` and ``ground truth``,\n        the computed loss map are weighted by ``weight_map``.\n\n        if ``prediction`` is list of tensors, each element of the list\n        will be compared against ``ground_truth` and the weighted by\n        ``weight_map``.\n\n        :param prediction: input will be reshaped into\n            ``(batch_size, N_voxels, num_classes)``\n        :param ground_truth: input will be reshaped into\n            ``(batch_size, N_voxels)``\n        :param weight_map: input will be reshaped into\n            ``(batch_size, N_voxels)``\n        :return:\n        """"""\n\n        with tf.device(\'/cpu:0\'):\n            batch_size = ground_truth.shape[0].value\n            dir_size = 1\n            if self._reshape:\n                ground_truth = tf.reshape(ground_truth, [batch_size, -1])\n                if weight_map is not None:\n                    weight_map = tf.reshape(weight_map, [batch_size, -1])\n            else:\n\n                dir_size = ground_truth.shape[-1].value\n                ground_truth = tf.reshape(ground_truth, [batch_size, -1,\n                                                         dir_size])\n            if not isinstance(prediction, (list, tuple)):\n                prediction = [prediction]\n\n            data_loss = []\n            for ind, pred in enumerate(prediction):\n                # go through each scale\n                def _batch_i_loss(*args):\n\n                    # go through each image in a batch\n                    if len(args[0]) == 2:\n                        pred_b, ground_truth_b = args[0]\n                        weight_map_b = None\n                    else:\n                        pred_b, ground_truth_b, weight_map_b = args[0]\n                    pred_b = tf.reshape(pred_b, tf.shape(ground_truth_b))\n                    # pred_b = tf.reshape(pred_b, [-1])\n                    # pred_b = tf.Print(tf.cast(pred_b, tf.float32),\n                    #                       [tf.shape(\n                    #                           pred_b), tf.shape(\n                    #                           ground_truth_b)],\n                    #                       message=\'pred_b_shape\')\n\n\n                    loss_params = {\n                        \'prediction\': pred_b,\n                        \'ground_truth\': ground_truth_b,\n                        \'weight_map\': weight_map_b}\n                    if self._loss_func_params:\n                        loss_params.update(self._loss_func_params)\n\n                    return tf.to_float(self._data_loss_func(**loss_params))\n\n                if weight_map is not None:\n                    elements = (pred, ground_truth, weight_map)\n                else:\n                    elements = (pred, ground_truth)\n\n                loss_batch = tf.map_fn(\n                    fn=_batch_i_loss,\n                    elems=elements,\n                    dtype=tf.float32,\n                    parallel_iterations=1)\n                data_loss.append(tf.reduce_mean(loss_batch))\n            return tf.reduce_mean(data_loss)\n\n\ndef l1_loss(prediction, ground_truth, weight_map=None):\n    """"""\n    :param prediction: the current prediction of the ground truth.\n    :param ground_truth: the measurement you are approximating with regression.\n    :return: mean of the l1 loss across all voxels.\n    """"""\n    absolute_residuals = tf.abs(tf.subtract(prediction, ground_truth))\n    if weight_map is not None:\n        absolute_residuals = tf.multiply(absolute_residuals, weight_map)\n        sum_residuals = tf.reduce_sum(absolute_residuals)\n        sum_weights = tf.reduce_sum(weight_map)\n    else:\n        sum_residuals = tf.reduce_sum(absolute_residuals)\n        sum_weights = tf.size(absolute_residuals)\n    return tf.truediv(tf.cast(sum_residuals, dtype=tf.float32),\n                      tf.cast(sum_weights, dtype=tf.float32))\n\n\ndef l2_loss(prediction, ground_truth, weight_map=None):\n    """"""\n    :param prediction: the current prediction of the ground truth.\n    :param ground_truth: the measurement you are approximating with regression.\n    :return: sum(differences squared) / 2 - Note, no square root\n    """"""\n\n    residuals = tf.subtract(prediction, ground_truth)\n    if weight_map is not None:\n        residuals = \\\n            tf.multiply(residuals, weight_map) / tf.reduce_sum(weight_map)\n    return tf.nn.l2_loss(residuals)\n\n\ndef rmse_loss(prediction, ground_truth, weight_map=None):\n    """"""\n    :param prediction: the current prediction of the ground truth.\n    :param ground_truth: the measurement you are approximating with regression.\n    :param weight_map: a weight map for the cost function. .\n    :return: sqrt(mean(differences squared))\n    """"""\n    if weight_map is not None:\n        residuals = tf.subtract(prediction, ground_truth)\n        residuals = tf.multiply(residuals, residuals)\n        residuals = tf.multiply(residuals, weight_map)\n        return tf.sqrt(tf.reduce_mean(residuals) / tf.reduce_mean(weight_map))\n    else:\n        return tf.sqrt(tf.losses.mean_squared_error(prediction, ground_truth))\n\n\ndef mae_loss(prediction, ground_truth, weight_map=None):\n    """"""\n    :param prediction: the current prediction of the ground truth.\n    :param ground_truth: the measurement you are approximating with regression.\n    :param weight_map: a weight map for the cost function. .\n    :return: mean(abs(ground_truth-prediction))\n    """"""\n    if weight_map is not None:\n        residuals = tf.subtract(prediction, ground_truth)\n        residuals = tf.abs(residuals)\n        residuals = tf.multiply(residuals, weight_map)\n        return tf.reduce_mean(residuals) / tf.reduce_mean(weight_map)\n    else:\n        return tf.reduce_mean(tf.abs(tf.subtract(prediction, ground_truth)))\n\n\ndef huber_loss(prediction, ground_truth, delta=1.0, weight_map=None):\n    """"""\n    The Huber loss is a smooth piecewise loss function\n    that is quadratic for ``|x| <= delta``, and linear for ``|x|> delta``\n    See https://en.wikipedia.org/wiki/Huber_loss .\n\n    :param prediction: the current prediction of the ground truth.\n    :param ground_truth: the measurement you are approximating with regression.\n    :param delta: the point at which quadratic->linear transition happens.\n    :return:\n    """"""\n    absolute_residuals = tf.abs(tf.subtract(prediction, ground_truth))\n    residual_is_outside_delta = tf.less(delta, absolute_residuals)\n    quadratic_residual = 0.5 * absolute_residuals ** 2\n    linear_residual = delta * (absolute_residuals - delta / 2)\n\n    voxelwise_loss = tf.where(residual_is_outside_delta,\n                              linear_residual,\n                              quadratic_residual)\n    if weight_map is not None:\n        voxelwise_loss = tf.multiply(voxelwise_loss, weight_map)\n        sum_weights = tf.reduce_sum(weight_map)\n    else:\n        sum_weights = tf.to_float(tf.size(absolute_residuals))\n    sum_loss = tf.reduce_sum(voxelwise_loss)\n    return tf.truediv(sum_loss, sum_weights)\n\n\ndef smooth_l1_loss(prediction, ground_truth, weight_map=None, value_thresh=0.5):\n    """"""\n    Similarly to the Huber loss, the residuals are squared below a threshold\n    value. In addition they are square above the inverse of this threshold\n    :param prediction: the current prediction of the ground truth.\n    :param ground_truth: the measurement you are approximating with regression.\n    :param weight_map:\n    :return: mean of the l1 loss across all voxels.\n    """"""\n    # Definition of thresholds\n    if value_thresh>1:\n        value_thresh_max = value_thresh\n        value_thresh = 1.0/value_thresh\n    else:\n        value_thresh_max = 1.0 / value_thresh\n\n    value_correction = value_thresh ** 3 - value_thresh\n\n    value_correction_max = value_thresh_max - value_thresh_max ** 2\n\n    prediction = tf.cast(prediction, dtype=tf.float32)\n\n    ground_truth = tf.cast(ground_truth, dtype=tf.float32)\n\n    absolute_residuals = tf.cast(tf.abs(tf.subtract(prediction,\n                                                             ground_truth)),\n                                 dtype=tf.float32)\n\n    absolute_residuals = tf.where(absolute_residuals < value_thresh,\n                                  value_thresh *\n                                           tf.square(absolute_residuals),\n                                           absolute_residuals + value_correction)\n\n    absolute_residuals = tf.where(tf.greater(absolute_residuals,value_thresh_max),\n                                  tf.square(\n        absolute_residuals) + value_correction_max, absolute_residuals)\n    if weight_map is not None:\n\n        absolute_residuals = tf.multiply(absolute_residuals, weight_map)\n        sum_residuals = tf.reduce_sum(absolute_residuals)\n\n        sum_weights = tf.reduce_sum(weight_map)\n\n    else:\n        sum_residuals = tf.reduce_sum(absolute_residuals)\n        sum_weights = tf.size(absolute_residuals)\n    return tf.truediv(tf.cast(sum_residuals, dtype=tf.float32),\n                      tf.cast(sum_weights, dtype=tf.float32))\n\n\ndef cosine_loss(prediction, ground_truth, weight_map=None, to_complete=True):\n    \'\'\'\n    Cosine loss between predicted and ground_truth vectors. The predicted and\n     targeted vectors should be unit vectors\n    :param prediction:\n    :param ground_truth:\n    :param weight_map:\n    :param to_complete: if the unit vector is to be completed\n    :return:\n    \'\'\'\n    if to_complete:\n        prediction_complete = tf.reshape(tf.sqrt(1 - tf.minimum(tf.reduce_sum(\n            tf.square(\n            prediction),-1),1)), [tf.shape(prediction)[0],1])\n        ground_truth_complete = tf.reshape(tf.sqrt(1 - tf.minimum(tf.reduce_sum(\n            tf.square(\n            ground_truth),-1),1)),[tf.shape(prediction)[0],1])\n\n        pred_vect = tf.concat([prediction, prediction_complete], -1)\n\n        gt_vect = tf.concat([ground_truth, ground_truth_complete], -1)\n    else:\n        pred_vect = prediction\n        gt_vect = ground_truth\n\n    if weight_map is None:\n        weight_map = tf.ones([tf.shape(prediction)[0]])\n    else:\n        weight_map = tf.reshape(weight_map, [tf.shape(prediction)[0]])\n\n    pred_vect = pred_vect / tf.maximum(tf.norm(\n        pred_vect,ord=\'euclidean\',axis=-1, keep_dims=True), 0.00001)\n    gt_vect = gt_vect /tf.maximum(tf.norm(\n        gt_vect,ord=\'euclidean\',axis=-1, keep_dims=True), 0.00001)\n    loss_init = 1 -tf.reduce_sum(gt_vect * pred_vect, -1)\n    weighted_loss = loss_init * weight_map\n    loss = tf.reduce_sum(weighted_loss) / tf.reduce_sum(weight_map)\n\n    return loss\n'"
niftynet/layer/loss_segmentation.py,178,"b'# -*- coding: utf-8 -*-\n""""""\nLoss functions for multi-class segmentation\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.application_factory import LossSegmentationFactory\nfrom niftynet.layer.base_layer import Layer\n\nM_tree = np.array([[0., 1., 1., 1., 1.],\n                   [1., 0., 0.6, 0.2, 0.5],\n                   [1., 0.6, 0., 0.6, 0.7],\n                   [1., 0.2, 0.6, 0., 0.5],\n                   [1., 0.5, 0.7, 0.5, 0.]], dtype=np.float64)\n\n\nclass LossFunction(Layer):\n    def __init__(self,\n                 n_class,\n                 loss_type=\'Dice\',\n                 softmax=True,\n                 loss_func_params=None,\n                 name=\'loss_function\'):\n\n        super(LossFunction, self).__init__(name=name)\n        assert n_class > 0, \\\n            ""Number of classes for segmentation loss should be positive.""\n        self._num_classes = n_class\n\n        self._softmax = bool(softmax)\n        # set loss function and function-specific additional params.\n        self._data_loss_func = LossSegmentationFactory.create(loss_type)\n        self._loss_func_params = \\\n            loss_func_params if loss_func_params is not None else dict()\n\n        data_loss_function_name = self._data_loss_func.__name__\n        if data_loss_function_name.startswith(\'cross_entropy\') \\\n                or \'xent\' in data_loss_function_name:\n            tf.logging.info(\n                \'Cross entropy loss function calls \'\n                \'tf.nn.sparse_softmax_cross_entropy_with_logits \'\n                \'which always performs a softmax internally.\')\n            self._softmax = False\n\n    def layer_op(self, prediction, ground_truth, weight_map=None):\n        """"""\n        Compute loss from `prediction` and `ground truth`,\n        the computed loss map are weighted by `weight_map`.\n\n        if `prediction `is list of tensors, each element of the list\n        will be compared against `ground_truth` and the weighted by\n        `weight_map`. (Assuming the same gt and weight across scales)\n\n        :param prediction: input will be reshaped into\n            ``(batch_size, N_voxels, num_classes)``\n        :param ground_truth: input will be reshaped into\n            ``(batch_size, N_voxels, ...)``\n        :param weight_map: input will be reshaped into\n            ``(batch_size, N_voxels, ...)``\n        :return:\n        """"""\n\n        with tf.device(\'/cpu:0\'):\n\n            # prediction should be a list for multi-scale losses\n            # single scale ``prediction`` is converted to ``[prediction]``\n            if not isinstance(prediction, (list, tuple)):\n                prediction = [prediction]\n\n            data_loss = []\n            for ind, pred in enumerate(prediction):\n\n                # go through each scale\n                def _batch_i_loss(*args):\n                    """"""\n                    loss for the `b_id`-th batch (over spatial dimensions)\n\n                    :param b_id:\n                    :return:\n                    """"""\n                    # unpacking input from map_fn elements\n                    if len(args[0]) == 2:\n                        # pred and ground_truth\n                        pred_b, ground_truth_b = args[0]\n                        weight_b = None\n                    else:\n                        pred_b, ground_truth_b, weight_b = args[0]\n\n                    pred_b = tf.reshape(pred_b, [-1, self._num_classes])\n                    # performs softmax if required\n                    if self._softmax:\n                        pred_b = tf.cast(pred_b, dtype=tf.float32)\n                        pred_b = tf.nn.softmax(pred_b)\n\n                    # reshape pred, ground_truth, weight_map to the same\n                    # size: (n_voxels, num_classes)\n                    # if the ground_truth has only one channel, the shape\n                    # becomes: (n_voxels,)\n                    if not pred_b.shape.is_fully_defined():\n                        ref_shape = tf.stack(\n                            [tf.shape(pred_b)[0], tf.constant(-1)], 0)\n                    else:\n                        ref_shape = pred_b.shape.as_list()[:-1] + [-1]\n\n                    ground_truth_b = tf.reshape(ground_truth_b, ref_shape)\n                    if ground_truth_b.shape.as_list()[-1] == 1:\n                        ground_truth_b = tf.squeeze(ground_truth_b, axis=-1)\n\n                    if weight_b is not None:\n                        weight_b = tf.reshape(weight_b, ref_shape)\n                        if weight_b.shape.as_list()[-1] == 1:\n                            weight_b = tf.squeeze(weight_b, axis=-1)\n\n                    # preparing loss function parameters\n                    loss_params = {\n                        \'prediction\': pred_b,\n                        \'ground_truth\': ground_truth_b,\n                        \'weight_map\': weight_b}\n                    if self._loss_func_params:\n                        loss_params.update(self._loss_func_params)\n\n                    return tf.to_float(self._data_loss_func(**loss_params))\n\n                if weight_map is not None:\n                    elements = (pred, ground_truth, weight_map)\n                else:\n                    elements = (pred, ground_truth)\n\n                loss_batch = tf.map_fn(\n                    fn=_batch_i_loss,\n                    elems=elements,\n                    dtype=tf.float32,\n                    parallel_iterations=1)\n\n                # loss averaged over batch\n                data_loss.append(tf.reduce_mean(loss_batch))\n            # loss averaged over multiple scales\n            return tf.reduce_mean(data_loss)\n\n\ndef labels_to_one_hot(ground_truth, num_classes=1):\n    """"""\n    Converts ground truth labels to one-hot, sparse tensors.\n    Used extensively in segmentation losses.\n\n    :param ground_truth: ground truth categorical labels (rank `N`)\n    :param num_classes: A scalar defining the depth of the one hot dimension\n        (see `depth` of `tf.one_hot`)\n    :return: one-hot sparse tf tensor\n        (rank `N+1`; new axis appended at the end)\n    """"""\n    # read input/output shapes\n    if isinstance(num_classes, tf.Tensor):\n        num_classes_tf = tf.to_int32(num_classes)\n    else:\n        num_classes_tf = tf.constant(num_classes, tf.int32)\n    input_shape = tf.shape(ground_truth)\n    output_shape = tf.concat(\n        [input_shape, tf.reshape(num_classes_tf, (1,))], 0)\n\n    if num_classes == 1:\n        # need a sparse representation?\n        return tf.reshape(ground_truth, output_shape)\n\n    # squeeze the spatial shape\n    ground_truth = tf.reshape(ground_truth, (-1,))\n    # shape of squeezed output\n    dense_shape = tf.stack([tf.shape(ground_truth)[0], num_classes_tf], 0)\n\n    # create a rank-2 sparse tensor\n    ground_truth = tf.to_int64(ground_truth)\n    ids = tf.range(tf.to_int64(dense_shape[0]), dtype=tf.int64)\n    ids = tf.stack([ids, ground_truth], axis=1)\n    one_hot = tf.SparseTensor(\n        indices=ids,\n        values=tf.ones_like(ground_truth, dtype=tf.float32),\n        dense_shape=tf.to_int64(dense_shape))\n\n    # resume the spatial dims\n    one_hot = tf.sparse_reshape(one_hot, output_shape)\n    return one_hot\n\n\ndef undecided_loss(prediction, ground_truth, weight_map=None):\n    """"""\n\n    :param prediction:\n    :param ground_truth:\n    :param weight_map:\n    :return:\n    """"""\n    ratio_undecided = 1.0/tf.cast(tf.shape(prediction)[-1], tf.float32)\n    res_undecided = tf.reciprocal(tf.reduce_mean(tf.abs(prediction -\n                                                 ratio_undecided), -1) + 0.0001)\n    if weight_map is None:\n        return tf.reduce_mean(res_undecided)\n    else:\n        res_undecided = tf.Print(tf.cast(res_undecided, tf.float32), [tf.shape(\n            res_undecided), tf.shape(weight_map), tf.shape(\n                res_undecided*weight_map)], message=\'test_printshape_und\')\n        return tf.reduce_sum(res_undecided * weight_map /\n                             tf.reduce_sum(weight_map))\n\n\ndef volume_enforcement(prediction, ground_truth, weight_map=None, eps=0.001,\n                       hard=False):\n    """"""\n    Computing a volume enforcement loss to ensure that the obtained volumes are\n    close and avoid empty results when something is expected\n    :param prediction:\n    :param ground_truth: labels\n    :param weight_map: potential weight map to apply\n    :param eps: epsilon to use as regulariser\n    :return:\n    """"""\n\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    gt_red = tf.sparse_reduce_sum(one_hot, 0)\n    pred_red = tf.reduce_sum(prediction, 0)\n    if hard:\n        pred_red  = tf.sparse_reduce_sum(labels_to_one_hot(tf.argmax(\n            prediction,-1),tf.shape(prediction)[-1]), 0)\n\n    if weight_map is not None:\n        n_classes = prediction.shape[1].value\n        weight_map_nclasses = tf.tile(tf.expand_dims(tf.reshape(weight_map,\n                                                                [-1]), 1),\n                                      [1, n_classes])\n        gt_red = tf.sparse_reduce_sum(weight_map_nclasses * one_hot,\n                                      reduction_axes=[0])\n        pred_red = tf.reduce_sum(weight_map_nclasses * prediction, 0)\n\n    return tf.reduce_mean(tf.sqrt(tf.square((gt_red+eps)/(pred_red+eps) -\n                                            (pred_red+eps)/(gt_red+eps))))\n\n\ndef volume_enforcement_fin(prediction, ground_truth, weight_map=None,\n                           eps=0.001):\n    """"""\n    Computing a volume enforcement loss to ensure that the obtained volumes are\n     close and avoid empty results when something is expected\n    :param prediction:\n    :param ground_truth:\n    :param weight_map:\n    :param eps:\n    :return:\n    """"""\n\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n    gt_red = tf.sparse_reduce_sum(one_hot, 0)\n    pred_red = tf.sparse_reduce_sum(labels_to_one_hot(tf.argmax(\n            prediction,-1),tf.shape(prediction)[-1]), 0)\n\n    if weight_map is not None:\n        n_classes = prediction.shape[1].value\n        weight_map_nclasses = tf.tile(tf.expand_dims(tf.reshape(weight_map,\n                                                                [-1]), 1),\n                                      [1, n_classes])\n        gt_red = tf.sparse_reduce_sum(weight_map_nclasses * one_hot,\n                                      reduction_axes=[0])\n        pred_red = tf.sparse_reduce_sum(labels_to_one_hot(tf.argmax(\n            prediction, -1), tf.shape(prediction)[-1]) * weight_map_nclasses, 0)\n\n    return tf.reduce_mean(tf.sqrt(tf.square((gt_red+eps)/(pred_red+eps)\n                                            - (pred_red+eps)/(gt_red+eps))))\n\n\n\ndef generalised_dice_loss(prediction,\n                          ground_truth,\n                          weight_map=None,\n                          type_weight=\'Square\'):\n    """"""\n    Function to calculate the Generalised Dice Loss defined in\n        Sudre, C. et. al. (2017) Generalised Dice overlap as a deep learning\n        loss function for highly unbalanced segmentations. DLMIA 2017\n\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground truth\n    :param weight_map:\n    :param type_weight: type of weighting allowed between labels (choice\n        between Square (square of inverse of volume),\n        Simple (inverse of volume) and Uniform (no weighting))\n    :return: the loss\n    """"""\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    if weight_map is not None:\n        num_classes = prediction.shape[1].value\n        # weight_map_nclasses = tf.reshape(\n        #     tf.tile(weight_map, [num_classes]), prediction.get_shape())\n        weight_map_nclasses = tf.tile(\n            tf.expand_dims(tf.reshape(weight_map, [-1]), 1), [1, num_classes])\n        ref_vol = tf.sparse_reduce_sum(\n            weight_map_nclasses * one_hot, reduction_axes=[0])\n\n        intersect = tf.sparse_reduce_sum(\n            weight_map_nclasses * one_hot * prediction, reduction_axes=[0])\n        seg_vol = tf.reduce_sum(\n            tf.multiply(weight_map_nclasses, prediction), 0)\n    else:\n        ref_vol = tf.sparse_reduce_sum(one_hot, reduction_axes=[0])\n        intersect = tf.sparse_reduce_sum(one_hot * prediction,\n                                         reduction_axes=[0])\n        seg_vol = tf.reduce_sum(prediction, 0)\n    if type_weight == \'Square\':\n        weights = tf.reciprocal(tf.square(ref_vol))\n    elif type_weight == \'Simple\':\n        weights = tf.reciprocal(ref_vol)\n    elif type_weight == \'Uniform\':\n        weights = tf.ones_like(ref_vol)\n    else:\n        raise ValueError(""The variable type_weight \\""{}\\""""\n                         ""is not defined."".format(type_weight))\n    new_weights = tf.where(tf.is_inf(weights), tf.zeros_like(weights), weights)\n    weights = tf.where(tf.is_inf(weights), tf.ones_like(weights) *\n                       tf.reduce_max(new_weights), weights)\n    generalised_dice_numerator = \\\n        2 * tf.reduce_sum(tf.multiply(weights, intersect))\n    # generalised_dice_denominator = \\\n    #     tf.reduce_sum(tf.multiply(weights, seg_vol + ref_vol)) + 1e-6\n    generalised_dice_denominator = tf.reduce_sum(\n        tf.multiply(weights, tf.maximum(seg_vol + ref_vol, 1)))\n    generalised_dice_score = \\\n        generalised_dice_numerator / generalised_dice_denominator\n    generalised_dice_score = tf.where(tf.is_nan(generalised_dice_score), 1.0,\n                                      generalised_dice_score)\n    return 1 - generalised_dice_score\n\n\ndef dice_plus_xent_loss(prediction, ground_truth, weight_map=None):\n    """"""\n    Function to calculate the loss used in https://arxiv.org/pdf/1809.10486.pdf,\n    no-new net, Isenseee et al (used to win the Medical Imaging Decathlon).\n\n    It is the sum of the cross-entropy and the Dice-loss.\n\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground truth\n    :param weight_map:\n    :return: the loss (cross_entropy + Dice)\n\n    """"""\n    num_classes = tf.shape(prediction)[-1]\n\n    prediction = tf.cast(prediction, tf.float32)\n    loss_xent = cross_entropy(prediction, ground_truth, weight_map=weight_map)\n\n    # Dice as according to the paper:\n    one_hot = labels_to_one_hot(ground_truth, num_classes=num_classes)\n    softmax_of_logits = tf.nn.softmax(prediction)\n\n    if weight_map is not None:\n        weight_map_nclasses = tf.tile(\n            tf.reshape(weight_map, [-1, 1]), [1, num_classes])\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            weight_map_nclasses * one_hot * softmax_of_logits,\n            reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(weight_map_nclasses * softmax_of_logits,\n                          reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(one_hot * weight_map_nclasses,\n                                 reduction_axes=[0])\n    else:\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            one_hot * softmax_of_logits, reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(softmax_of_logits, reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(one_hot, reduction_axes=[0])\n\n    epsilon = 0.00001\n    loss_dice = -(dice_numerator + epsilon) / (dice_denominator + epsilon)\n    dice_numerator = tf.Print(\n        dice_denominator, [dice_numerator, dice_denominator, loss_dice])\n\n    return loss_dice + loss_xent\n\n\ndef sensitivity_specificity_loss(prediction,\n                                 ground_truth,\n                                 weight_map=None,\n                                 r=0.05):\n    """"""\n    Function to calculate a multiple-ground_truth version of\n    the sensitivity-specificity loss defined in ""Deep Convolutional\n    Encoder Networks for Multiple Sclerosis Lesion Segmentation"",\n    Brosch et al, MICCAI 2015,\n    https://link.springer.com/chapter/10.1007/978-3-319-24574-4_1\n\n    error is the sum of r(specificity part) and (1-r)(sensitivity part)\n\n    :param prediction: the logits\n    :param ground_truth: segmentation ground_truth.\n    :param r: the \'sensitivity ratio\'\n        (authors suggest values from 0.01-0.10 will have similar effects)\n    :return: the loss\n    """"""\n    if weight_map is not None:\n        # raise NotImplementedError\n        tf.logging.warning(\'Weight map specified but not used.\')\n\n    prediction = tf.cast(prediction, tf.float32)\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    one_hot = tf.sparse_tensor_to_dense(one_hot)\n    # value of unity everywhere except for the previous \'hot\' locations\n    one_cold = 1 - one_hot\n\n    # chosen region may contain no voxels of a given label. Prevents nans.\n    epsilon = 1e-5\n\n    squared_error = tf.square(one_hot - prediction)\n    specificity_part = tf.reduce_sum(\n        squared_error * one_hot, 0) / \\\n                       (tf.reduce_sum(one_hot, 0) + epsilon)\n    sensitivity_part = \\\n        (tf.reduce_sum(tf.multiply(squared_error, one_cold), 0) /\n         (tf.reduce_sum(one_cold, 0) + epsilon))\n\n    return tf.reduce_sum(r * specificity_part + (1 - r) * sensitivity_part)\n\n\ndef cross_entropy(prediction, ground_truth, weight_map=None):\n    """"""\n    Function to calculate the cross-entropy loss function\n\n    :param prediction: the logits (before softmax)\n    :param ground_truth: the segmentation ground truth\n    :param weight_map:\n    :return: the cross-entropy loss\n    """"""\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n\n    # TODO trace this back:\n    ground_truth = tf.cast(ground_truth, tf.int32)\n\n    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=prediction, labels=ground_truth)\n\n    if weight_map is None:\n        return tf.reduce_mean(entropy)\n\n    weight_sum = tf.maximum(tf.reduce_sum(weight_map), 1e-6)\n    return tf.reduce_sum(entropy * weight_map / weight_sum)\n\n\ndef cross_entropy_dense(prediction, ground_truth, weight_map=None):\n    if weight_map is not None:\n        raise NotImplementedError\n\n    entropy = tf.nn.softmax_cross_entropy_with_logits(\n        logits=prediction, labels=ground_truth)\n    return tf.reduce_mean(entropy)\n\n\ndef wasserstein_disagreement_map(\n        prediction, ground_truth, weight_map=None, M=None):\n    """"""\n    Function to calculate the pixel-wise Wasserstein distance between the\n    flattened prediction and the flattened labels (ground_truth) with respect\n    to the distance matrix on the label space M.\n\n    :param prediction: the logits after softmax\n    :param ground_truth: segmentation ground_truth\n    :param M: distance matrix on the label space\n    :return: the pixelwise distance map (wass_dis_map)\n    """"""\n    if weight_map is not None:\n        # raise NotImplementedError\n        tf.logging.warning(\'Weight map specified but not used.\')\n\n    assert M is not None, ""Distance matrix is required.""\n    # pixel-wise Wassertein distance (W) between flat_pred_proba and flat_labels\n    # wrt the distance matrix on the label space M\n    num_classes = prediction.shape[1].value\n    ground_truth.set_shape(prediction.shape)\n    unstack_labels = tf.unstack(ground_truth, axis=-1)\n    unstack_labels = tf.cast(unstack_labels, dtype=tf.float64)\n    unstack_pred = tf.unstack(prediction, axis=-1)\n    unstack_pred = tf.cast(unstack_pred, dtype=tf.float64)\n    # print(""shape of M"", M.shape, ""unstacked labels"", unstack_labels,\n    #       ""unstacked pred"" ,unstack_pred)\n    # W is a weighting sum of all pairwise correlations (pred_ci x labels_cj)\n    pairwise_correlations = []\n    for i in range(num_classes):\n        for j in range(num_classes):\n            pairwise_correlations.append(\n                M[i, j] * tf.multiply(unstack_pred[i], unstack_labels[j]))\n    wass_dis_map = tf.add_n(pairwise_correlations)\n    return wass_dis_map\n\n\ndef generalised_wasserstein_dice_loss(prediction,\n                                      ground_truth,\n                                      weight_map=None):\n    """"""\n    Function to calculate the Generalised Wasserstein Dice Loss defined in\n\n        Fidon, L. et. al. (2017) Generalised Wasserstein Dice Score\n        for Imbalanced Multi-class Segmentation using Holistic\n        Convolutional Networks.MICCAI 2017 (BrainLes)\n\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground_truth\n    :param weight_map:\n    :return: the loss\n    """"""\n    if weight_map is not None:\n        # raise NotImplementedError\n        tf.logging.warning(\'Weight map specified but not used.\')\n\n    prediction = tf.cast(prediction, tf.float32)\n    num_classes = prediction.shape[1].value\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    one_hot = tf.sparse_tensor_to_dense(one_hot)\n    # M = tf.cast(M, dtype=tf.float64)\n    # compute disagreement map (delta)\n    M = M_tree\n    delta = wasserstein_disagreement_map(prediction, one_hot, M=M)\n    # compute generalisation of all error for multi-class seg\n    all_error = tf.reduce_sum(delta)\n    # compute generalisation of true positives for multi-class seg\n    one_hot = tf.cast(one_hot, dtype=tf.float64)\n    true_pos = tf.reduce_sum(\n        tf.multiply(tf.constant(M[0, :num_classes], dtype=tf.float64), one_hot),\n        axis=1)\n    true_pos = tf.reduce_sum(tf.multiply(true_pos, 1. - delta), axis=0)\n    WGDL = 1. - (2. * true_pos) / (2. * true_pos + all_error)\n    return tf.cast(WGDL, dtype=tf.float32)\n\n\ndef dice(prediction, ground_truth, weight_map=None):\n    """"""\n    Function to calculate the dice loss with the definition given in\n\n        Milletari, F., Navab, N., & Ahmadi, S. A. (2016)\n        V-net: Fully convolutional neural\n        networks for volumetric medical image segmentation. 3DV 2016\n\n    using a square in the denominator\n\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground_truth\n    :param weight_map:\n    :return: the loss\n    """"""\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    if weight_map is not None:\n        num_classes = prediction.shape[1].value\n        weight_map_nclasses = tf.tile(tf.expand_dims(\n            tf.reshape(weight_map, [-1]), 1), [1, num_classes])\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            weight_map_nclasses * one_hot * prediction, reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(weight_map_nclasses * tf.square(prediction),\n                          reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(one_hot * weight_map_nclasses,\n                                 reduction_axes=[0])\n    else:\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            one_hot * prediction, reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(tf.square(prediction), reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(one_hot, reduction_axes=[0])\n    epsilon = 0.00001\n\n    dice_score = (dice_numerator + epsilon) / (dice_denominator + epsilon)\n    # dice_score.set_shape([num_classes])\n    # minimising (1 - dice_coefficients)\n    return 1.0 - tf.reduce_mean(dice_score)\n\n\ndef dice_nosquare(prediction, ground_truth, weight_map=None):\n    """"""\n    Function to calculate the classical dice loss\n\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground_truth\n    :param weight_map:\n    :return: the loss\n    """"""\n    prediction = tf.cast(prediction, tf.float32)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n\n    # dice\n    if weight_map is not None:\n        num_classes = prediction.shape[1].value\n        weight_map_nclasses = tf.tile(tf.expand_dims(\n            tf.reshape(weight_map, [-1]), 1), [1, num_classes])\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(\n            weight_map_nclasses * one_hot * prediction, reduction_axes=[0])\n        dice_denominator = \\\n            tf.reduce_sum(prediction * weight_map_nclasses,\n                          reduction_indices=[0]) + \\\n            tf.sparse_reduce_sum(weight_map_nclasses * one_hot,\n                                 reduction_axes=[0])\n    else:\n        dice_numerator = 2.0 * tf.sparse_reduce_sum(one_hot * prediction,\n                                                    reduction_axes=[0])\n        dice_denominator = tf.reduce_sum(prediction, reduction_indices=[0]) + \\\n                           tf.sparse_reduce_sum(one_hot, reduction_axes=[0])\n    epsilon = 0.00001\n\n    dice_score = (dice_numerator + epsilon) / (dice_denominator + epsilon)\n    # dice_score.set_shape([num_classes])\n    # minimising (1 - dice_coefficients)\n    return 1.0 - tf.reduce_mean(dice_score)\n\n\ndef tversky(prediction, ground_truth, weight_map=None, alpha=0.5, beta=0.5):\n    """"""\n    Function to calculate the Tversky loss for imbalanced data\n\n        Sadegh et al. (2017)\n\n        Tversky loss function for image segmentation\n        using 3D fully convolutional deep networks\n\n    :param prediction: the logits\n    :param ground_truth: the segmentation ground_truth\n    :param alpha: weight of false positives\n    :param beta: weight of false negatives\n    :param weight_map:\n    :return: the loss\n    """"""\n    prediction = tf.to_float(prediction)\n    if len(ground_truth.shape) == len(prediction.shape):\n        ground_truth = ground_truth[..., -1]\n    one_hot = labels_to_one_hot(ground_truth, tf.shape(prediction)[-1])\n    one_hot = tf.sparse_tensor_to_dense(one_hot)\n\n    p0 = prediction\n    p1 = 1 - prediction\n    g0 = one_hot\n    g1 = 1 - one_hot\n\n    if weight_map is not None:\n        num_classes = prediction.shape[1].value\n        weight_map_flattened = tf.reshape(weight_map, [-1])\n        weight_map_expanded = tf.expand_dims(weight_map_flattened, 1)\n        weight_map_nclasses = tf.tile(weight_map_expanded, [1, num_classes])\n    else:\n        weight_map_nclasses = 1\n\n    tp = tf.reduce_sum(weight_map_nclasses * p0 * g0)\n    fp = alpha * tf.reduce_sum(weight_map_nclasses * p0 * g1)\n    fn = beta * tf.reduce_sum(weight_map_nclasses * p1 * g0)\n\n    EPSILON = 0.00001\n    numerator = tp\n    denominator = tp + fp + fn + EPSILON\n    score = numerator / denominator\n    return 1.0 - tf.reduce_mean(score)\n\n\ndef dice_dense(prediction, ground_truth, weight_map=None):\n    """"""\n    Computing mean-class Dice similarity.\n\n    :param prediction: last dimension should have ``num_classes``\n    :param ground_truth: segmentation ground truth (encoded as a binary matrix)\n        last dimension should be ``num_classes``\n    :param weight_map:\n    :return: ``1.0 - mean(Dice similarity per class)``\n    """"""\n\n    if weight_map is not None:\n        raise NotImplementedError\n    prediction = tf.cast(prediction, dtype=tf.float32)\n    ground_truth = tf.cast(ground_truth, dtype=tf.float32)\n    ground_truth = tf.reshape(ground_truth, prediction.shape)\n    # computing Dice over the spatial dimensions\n    reduce_axes = list(range(len(prediction.shape) - 1))\n    dice_numerator = 2.0 * tf.reduce_sum(\n        prediction * ground_truth, axis=reduce_axes)\n    dice_denominator = \\\n        tf.reduce_sum(tf.square(prediction), axis=reduce_axes) + \\\n        tf.reduce_sum(tf.square(ground_truth), axis=reduce_axes)\n\n    epsilon = 0.00001\n\n    dice_score = (dice_numerator + epsilon) / (dice_denominator + epsilon)\n    return 1.0 - tf.reduce_mean(dice_score)\n\n\ndef dice_dense_nosquare(prediction, ground_truth, weight_map=None):\n    """"""\n    Computing mean-class Dice similarity with no square terms in the denominator\n\n    :param prediction: last dimension should have ``num_classes``\n    :param ground_truth: segmentation ground truth (encoded as a binary matrix)\n        last dimension should be ``num_classes``\n    :param weight_map:\n    :return: ``1.0 - mean(Dice similarity per class)``\n    """"""\n\n    if weight_map is not None:\n        raise NotImplementedError\n    prediction = tf.cast(prediction, dtype=tf.float32)\n    ground_truth = tf.cast(ground_truth, dtype=tf.float32)\n    ground_truth = tf.reshape(ground_truth, prediction.shape)\n    # computing Dice over the spatial dimensions\n    reduce_axes = list(range(len(prediction.shape) - 1))\n    dice_numerator = 2.0 * tf.reduce_sum(\n        prediction * ground_truth, axis=reduce_axes)\n    dice_denominator = \\\n        tf.reduce_sum(prediction, axis=reduce_axes) + \\\n        tf.reduce_sum(ground_truth, axis=reduce_axes)\n    epsilon = 0.00001\n\n    dice_score = (dice_numerator + epsilon) / (dice_denominator + epsilon)\n    return 1.0 - tf.reduce_mean(dice_score)\n'"
niftynet/layer/mean_variance_normalisation.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport numpy.ma as ma\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\n\n\nclass MeanVarNormalisationLayer(Layer):\n    """"""\n    This class defines image-level normalisation by subtracting\n    foreground mean intensity value and dividing by standard deviation\n    """"""\n\n    def __init__(self, image_name, binary_masking_func=None):\n\n        self.image_name = image_name\n        super(MeanVarNormalisationLayer, self).__init__(name=\'mean_var_norm\')\n        self.binary_masking_func = None\n        if binary_masking_func is not None:\n            assert isinstance(binary_masking_func, BinaryMaskingLayer)\n            self.binary_masking_func = binary_masking_func\n\n    def layer_op(self, image, mask=None):\n        if isinstance(image, dict):\n            image_data = np.asarray(image[self.image_name], dtype=np.float32)\n        else:\n            image_data = np.asarray(image, dtype=np.float32)\n\n        if isinstance(mask, dict):\n            image_mask = mask.get(self.image_name, None)\n        elif mask is not None:\n            image_mask = mask\n        elif self.binary_masking_func is not None:\n            image_mask = self.binary_masking_func(image_data)\n        else:\n            # no access to mask, default to the entire image\n            image_mask = np.ones_like(image_data, dtype=np.bool)\n\n        if image_data.ndim == 3:\n            image_data = whitening_transformation(image_data, image_mask)\n        if image_data.ndim == 5:\n            for m in range(image_data.shape[4]):\n                for t in range(image_data.shape[3]):\n                    image_data[..., t, m] = whitening_transformation(\n                        image_data[..., t, m], image_mask[..., t, m])\n\n        if isinstance(image, dict):\n            image[self.image_name] = image_data\n            if isinstance(mask, dict):\n                mask[self.image_name] = image_mask\n            else:\n                mask = {self.image_name: image_mask}\n            return image, mask\n        else:\n            return image_data, image_mask\n\n\ndef whitening_transformation(image, mask):\n    # make sure image is a monomodal volume\n    masked_img = ma.masked_array(image, np.logical_not(mask))\n    image = (image - masked_img.mean()) / max(masked_img.std(), 1e-5)\n    return image\n'"
niftynet/layer/pad.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer, Invertible\n\n\nclass PadLayer(Layer, Invertible):\n    """"""\n    This class defines a padding operation:\n    pad `2*border` pixels from spatial dims of the input (numpy array),\n    and return the padded input.\n\n    This function is used at volume level (as a preprocessor in image reader)\n    therefore assumes the input has at least three spatial dims.\n    """"""\n\n    def __init__(self, image_name, border, name=\'pad\', mode=\'minimum\', pad_to=(0,)):\n        """"""\n        :param image_name: the name of the relevant key in the data dictionary\n        :param border: the dimensions of the desired border around the image.\n        :param name: name of the PadLayer in the tensorflow graph.\n        :param mode: how to choose the padding values for the np.pad operation.\n        :param pad_to: this determines a desired size of the padded image (useful\n            for inconsistent input sizes or for making inference efficient). If\n            it == (0, ) (DEFAULT), it will use the constant padding mode\n            determined by \'border\'\n        """"""\n        super(PadLayer, self).__init__(name=name)\n        try:\n            spatial_border = tuple(map(lambda x: (x,), border))\n        except (ValueError, TypeError):\n            tf.logging.fatal(""Unknown padding param. {}"".format(border))\n            raise\n        self.border = spatial_border\n        self.image_name = set(image_name)\n        self.mode = mode\n        self.pad_to = pad_to\n        self.full_border = None\n\n    def layer_op(self, input_image, mask=None):\n        if not isinstance(input_image, dict):\n            self._set_full_border(input_image)\n            input_image = np.pad(input_image, self.full_border, mode=self.mode)\n            return input_image, mask\n\n        for name, image in input_image.items():\n            self._set_full_border(image)\n            if name not in self.image_name:\n                tf.logging.warning(\'could not pad, dict name %s not in %s\', name, self.image_name)\n                continue\n            input_image[name] = np.pad(image, self.full_border, mode=self.mode)\n        return input_image, mask\n\n    def inverse_op(self, input_image, mask=None):\n        if not isinstance(input_image, dict):\n            # you can run the cropping op without running the padding op, but only if you\n            # pad with a constant amount (not pad_to)\n            if self.full_border is None and self.pad_to == (0,):\n                self._set_full_border(input_image)\n\n            outputs = self._crop_numpy_array(input_image, self.full_border)\n            return outputs, mask\n\n        for name, image in input_image.items():\n            # you can run the cropping op without running the padding op, but only if you\n            # pad with a constant amount (not pad_to)\n            if self.full_border is None and self.pad_to == (0,):\n                self._set_full_border(image)\n\n            if name not in self.image_name:\n                continue\n            input_image[name] = self._crop_numpy_array(image, self.full_border)\n        return input_image, mask\n\n    @staticmethod\n    def _crop_numpy_array(image, border):\n        try:\n            assert image.ndim >= 3, ""input image must have at least 3 spatial dims""\n            if np.shape(border)[-1] < 2:\n                # same amount cropped from each side of array\n                border = np.hstack([np.array(border), np.array(border)])\n\n            x_ = border[0][0] if image.shape[0] / 2 > border[0][0] > 0 else 0\n            y_ = border[1][0] if image.shape[1] / 2 > border[1][0] > 0 else 0\n            z_ = border[2][0] if image.shape[2] / 2 > border[2][0] > 0 else 0\n            _x = -border[0][1] if image.shape[0] / 2 > border[0][1] > 0 else image.shape[0]\n            _y = -border[1][1] if image.shape[1] / 2 > border[1][1] > 0 else image.shape[1]\n            _z = -border[2][1] if image.shape[2] / 2 > border[2][1] > 0 else image.shape[2]\n            return image[x_:_x, y_:_y, z_:_z, ...]\n        except (IndexError, AssertionError):\n            tf.logging.fatal(\n                ""Unable to invert the padding. Input: {}, pad param. {}"".format(image.shape, border))\n            raise\n\n    def _set_full_border(self, image):\n        """"""\n        To calculate and set the border that is used to a) pad the image and b) invert the padding\n        :param image: the input image\n        """"""\n        if self.pad_to == (0,):\n            full_border = self.border\n            while len(full_border) < image.ndim:\n                # here, we extend the tuple with zeros as all padding is symmetric (for each\n                # dimension, we pad \'in front\' and \'behind\' with the same number of values).\n                full_border = full_border + ((0,),)\n        else:\n            necessary_padding = np.array(self.pad_to) - image.shape[:len(self.pad_to)]\n            full_border = tuple()\n\n            # do not pad if the dimension is bigger than self.pad_to\n            necessary_padding[necessary_padding < 0] = 0\n            for pad in necessary_padding:\n                full_border += ((pad // 2, (pad + 1) // 2),)\n\n            # no padding on the channel dimensions\n            while len(full_border) < image.ndim:\n                # in pad_to mode, we explicitly determine the padding at the front and back.\n                full_border += ((0, 0),)\n\n        self.full_border = full_border\n'"
niftynet/layer/post_processing.py,4,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_OPS = set([""SOFTMAX"", ""ARGMAX"", ""IDENTITY""])\n\n\nclass PostProcessingLayer(Layer):\n    """"""\n    This layer operation converts the raw network outputs into final inference\n    results.\n    """"""\n\n    def __init__(self, func=\'\', num_classes=0, name=\'post_processing\'):\n        super(PostProcessingLayer, self).__init__(name=name)\n        self.func = look_up_operations(func.upper(), SUPPORTED_OPS)\n        self.num_classes = num_classes\n\n    def num_output_channels(self):\n        assert self._op._variables_created\n        if self.func == ""SOFTMAX"":\n            return self.num_classes\n        else:\n            return 1\n\n    def layer_op(self, inputs):\n        if self.func == ""SOFTMAX"":\n            output_tensor = tf.cast(tf.nn.softmax(inputs), tf.float32)\n        elif self.func == ""ARGMAX"":\n            output_tensor = tf.cast(tf.argmax(inputs, -1), tf.int32)\n            output_tensor = tf.expand_dims(output_tensor, axis=-1)\n        elif self.func == ""IDENTITY"":\n            output_tensor = tf.cast(inputs, tf.float32)\n        return output_tensor\n'"
niftynet/layer/rand_bias_field.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\n\nfrom niftynet.layer.base_layer import RandomisedLayer\n\n\nclass RandomBiasFieldLayer(RandomisedLayer):\n    """"""\n    generate randomised bias field transformation for data augmentation\n    """"""\n\n    def __init__(self, name=\'random_bias_field\'):\n        super(RandomBiasFieldLayer, self).__init__(name=name)\n        self._bf_coeffs = None\n        self.min_coeff = -10.0\n        self.max_coeff = 10.0\n        self.order = 3\n\n    def init_uniform_coeff(self, coeff_range=(-10.0, 10.0)):\n        assert coeff_range[0] < coeff_range[1]\n        self.min_coeff = float(coeff_range[0])\n        self.max_coeff = float(coeff_range[1])\n\n    def init_order(self, order=3):\n        self.order = int(order)\n\n    def randomise(self, spatial_rank=3):\n        self._generate_bias_field_coeffs(spatial_rank)\n\n    def _generate_bias_field_coeffs(self, spatial_rank):\n        """"""\n        Sampling of the appropriate number of coefficients for the creation\n        of the bias field map\n        :param spatial_rank: spatial rank of the image to modify\n        :return:\n        """"""\n        rand_coeffs = []\n        if spatial_rank == 3:\n            for order_x in range(0, self.order + 1):\n                for order_y in range(0, self.order + 1 - order_x):\n                    for order_z in range(0,\n                                         self.order + 1 - (order_x + order_y)):\n                        rand_coeff_new = np.random.uniform(self.min_coeff,\n                                                           self.max_coeff)\n                        rand_coeffs.append(rand_coeff_new)\n        else:\n            for order_x in range(0, self.order + 1):\n                for order_y in range(0, self.order + 1 - order_x):\n                    rand_coeff_new = np.random.uniform(self.min_coeff,\n                                                       self.max_coeff)\n                    rand_coeffs.append(rand_coeff_new)\n        self._bf_coeffs = rand_coeffs\n\n    def _generate_bias_field_map(self, shape):\n        """"""\n        Create the bias field map using a linear combination polynomial\n        functions and the coefficients previously sampled\n        :param shape: shape of the image in order to create the polynomial\n            functions\n        :return: bias field map to apply\n        """"""\n        spatial_rank = len(shape)\n        x_range = np.arange(-shape[0] / 2, shape[0] / 2)\n        y_range = np.arange(-shape[1] / 2, shape[1] / 2)\n        bf_map = np.zeros(shape)\n        i = 0\n        if spatial_rank == 3:\n            z_range = np.arange(-shape[2] / 2, shape[2] / 2)\n            x_mesh, y_mesh, z_mesh = np.asarray(\n                np.meshgrid(x_range, y_range, z_range), dtype=float)\n            x_mesh /= float(np.max(x_mesh))\n            y_mesh /= float(np.max(y_mesh))\n            z_mesh /= float(np.max(z_mesh))\n            for order_x in range(self.order + 1):\n                for order_y in range(self.order + 1 - order_x):\n                    for order_z in range(self.order + 1 - (order_x + order_y)):\n                        rand_coeff = self._bf_coeffs[i]\n                        new_map = rand_coeff * \\\n                                  np.power(x_mesh, order_x) * \\\n                                  np.power(y_mesh, order_y) * \\\n                                  np.power(z_mesh, order_z)\n                        bf_map += np.transpose(new_map, (1, 0, 2))\n                        i += 1\n        if spatial_rank == 2:\n            x_mesh, y_mesh = np.asarray(\n                np.meshgrid(x_range, y_range), dtype=float)\n            x_mesh /= np.max(x_mesh)\n            y_mesh /= np.max(y_mesh)\n            for order_x in range(self.order + 1):\n                for order_y in range(self.order + 1 - order_x):\n                    rand_coeff = self._bf_coeffs[i]\n                    new_map = rand_coeff * \\\n                              np.power(x_mesh, order_x) * \\\n                              np.power(y_mesh, order_y)\n                    bf_map += np.transpose(new_map, (1, 0))\n                    i += 1\n        return np.exp(bf_map)\n\n    def _apply_transformation(self, image):\n        """"""\n        Create the bias field map based on the randomly sampled coefficients\n        and apply it (multiplicative) to the image to augment\n        :param image: image on which to apply the bias field augmentation\n        :return: modified image\n        """"""\n        assert self._bf_coeffs is not None\n        bf_map = self._generate_bias_field_map(image.shape)\n        bf_image = image * bf_map\n        return bf_image\n\n    def layer_op(self, inputs, interp_orders, *args, **kwargs):\n        if inputs is None:\n            return inputs\n        for (field, image) in inputs.items():\n            if field == \'image\':\n                for mod_i in range(image.shape[-1]):\n                    if image.ndim == 4:\n                        inputs[field][..., mod_i] = \\\n                            self._apply_transformation(image[..., mod_i])\n                    elif image.ndim == 5:\n                        for t in range(image.shape[-2]):\n                            inputs[field][..., t, mod_i] = \\\n                                self._apply_transformation(image[..., t, mod_i])\n                    else:\n                        raise NotImplementedError(""unknown input format"")\n        return inputs\n'"
niftynet/layer/rand_elastic_deform.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nData augmentation using elastic deformations as used by:\nMilletari,F., Navab, N., & Ahmadi, S. A. (2016) V-net:\nFully convolutional neural networks for volumetric medical\nimage segmentation\n""""""\n\nfrom __future__ import absolute_import, print_function\n\nimport warnings\n\nimport numpy as np\n\nfrom niftynet.layer.base_layer import RandomisedLayer\nfrom niftynet.utilities.util_import import require_module\n\nsitk = require_module(\'SimpleITK\')\n\nwarnings.simplefilter(""ignore"", UserWarning)\nwarnings.simplefilter(""ignore"", RuntimeWarning)\n\n\nclass RandomElasticDeformationLayer(RandomisedLayer):\n    """"""\n    generate randomised elastic deformations\n    along each dim for data augmentation\n    """"""\n\n    def __init__(self,\n                 num_controlpoints=4,\n                 std_deformation_sigma=15,\n                 proportion_to_augment=0.5,\n                 spatial_rank=3):\n        """"""\n        This layer elastically deforms the inputs,\n        for data-augmentation purposes.\n\n        :param num_controlpoints:\n        :param std_deformation_sigma:\n        :param proportion_to_augment: what fraction of the images\n            to do augmentation on\n        :param name: name for tensorflow graph\n        (may be computationally expensive).\n        """"""\n\n        super(RandomElasticDeformationLayer, self).__init__(\n            name=\'random_elastic_deformation\')\n\n        self._bspline_transformation = None\n        self.num_controlpoints = max(num_controlpoints, 2)\n        self.std_deformation_sigma = max(std_deformation_sigma, 1)\n        self.proportion_to_augment = proportion_to_augment\n        if not sitk:\n            self.proportion_to_augment = -1\n        self.spatial_rank = spatial_rank\n\n    def randomise(self, image_dict):\n        images = list(image_dict.values())\n        equal_shapes = np.all(\n            [images[0].shape[:self.spatial_rank] == image.shape[:self.spatial_rank] for image in images])\n        if equal_shapes and self.proportion_to_augment >= 0:\n            self._randomise_bspline_transformation(images[0].shape)\n        else:\n            # currently not supported spatial rank for elastic deformation\n            # should support classification in the future\n            print(""randomising elastic deformation FAILED"")\n            pass\n\n    def _randomise_bspline_transformation(self, shape):\n        # generate transformation\n        if len(shape) == 5:  # for niftynet reader outputs\n            squeezed_shape = [dim for dim in shape[:3] if dim > 1]\n        else:\n            squeezed_shape = shape[:self.spatial_rank]\n        itkimg = sitk.GetImageFromArray(np.zeros(squeezed_shape))\n        trans_from_domain_mesh_size = \\\n            [self.num_controlpoints] * itkimg.GetDimension()\n        self._bspline_transformation = sitk.BSplineTransformInitializer(\n            itkimg, trans_from_domain_mesh_size)\n\n        params = self._bspline_transformation.GetParameters()\n        params_numpy = np.asarray(params, dtype=float)\n        params_numpy = params_numpy + np.random.randn(\n            params_numpy.shape[0]) * self.std_deformation_sigma\n\n        # remove z deformations! The resolution in z is too bad\n        # params_numpy[0:int(len(params) / 3)] = 0\n\n        params = tuple(params_numpy)\n        self._bspline_transformation.SetParameters(params)\n\n    def _apply_bspline_transformation(self, image, interp_order=3):\n        """"""\n        Apply randomised transformation to 2D or 3D image\n\n        :param image: 2D or 3D array\n        :param interp_order: order of interpolation\n        :return: the transformed image\n        """"""\n        resampler = sitk.ResampleImageFilter()\n        if interp_order > 1:\n            resampler.SetInterpolator(sitk.sitkBSpline)\n        elif interp_order == 1:\n            resampler.SetInterpolator(sitk.sitkLinear)\n        elif interp_order == 0:\n            resampler.SetInterpolator(sitk.sitkNearestNeighbor)\n        else:\n            return image\n\n        squeezed_image = np.squeeze(image)\n        while squeezed_image.ndim < self.spatial_rank:\n            # pad to the required number of dimensions\n            squeezed_image = squeezed_image[..., None]\n        sitk_image = sitk.GetImageFromArray(squeezed_image)\n        \n        resampler.SetReferenceImage(sitk_image)\n        resampler.SetDefaultPixelValue(0)\n        resampler.SetTransform(self._bspline_transformation)\n        out_img_sitk = resampler.Execute(sitk_image)\n        out_img = sitk.GetArrayFromImage(out_img_sitk)\n        return out_img.reshape(image.shape)\n\n    def layer_op(self, inputs, interp_orders, *args, **kwargs):\n        if inputs is None:\n            return inputs\n\n        # only do augmentation with a probability `proportion_to_augment`\n        do_augmentation = np.random.rand() < self.proportion_to_augment\n        if not do_augmentation:\n            return inputs\n\n        if isinstance(inputs, dict) and isinstance(interp_orders, dict):\n            for (field, image) in inputs.items():\n                assert image.shape[-1] == len(interp_orders[field]), \\\n                    ""interpolation orders should be"" \\\n                    ""specified for each inputs modality""\n                for mod_i, interp_order in enumerate(interp_orders[field]):\n                    if image.ndim in (3, 4):  # for 2/3d images\n                        inputs[field][..., mod_i] = \\\n                            self._apply_bspline_transformation(\n                                image[..., mod_i], interp_order)\n                    elif image.ndim == 5:\n                        for t in range(image.shape[-2]):\n                            inputs[field][..., t, mod_i] = \\\n                                self._apply_bspline_transformation(\n                                    image[..., t, mod_i], interp_order)\n                    else:\n                        raise NotImplementedError(""unknown input format"")\n\n        else:\n            raise NotImplementedError(""unknown input format"")\n        return inputs\n'"
niftynet/layer/rand_flip.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport warnings\n\nimport numpy as np\n\nfrom niftynet.layer.base_layer import RandomisedLayer\n\nwarnings.simplefilter(""ignore"", UserWarning)\nwarnings.simplefilter(""ignore"", RuntimeWarning)\n\n\nclass RandomFlipLayer(RandomisedLayer):\n    """"""\n    Add a random flipping layer as pre-processing.\n    """"""\n\n    def __init__(self,\n                 flip_axes,\n                 flip_probability=0.5,\n                 name=\'random_flip\'):\n        """"""\n\n        :param flip_axes: a list of indices over which to flip\n        :param flip_probability: the probability of performing the flip\n            (default = 0.5)\n        :param name:\n        """"""\n        super(RandomFlipLayer, self).__init__(name=name)\n        self._flip_axes = flip_axes\n        self._flip_probability = flip_probability\n        self._rand_flip = None\n\n    def randomise(self, spatial_rank=3):\n        spatial_rank = int(np.floor(spatial_rank))\n        self._rand_flip = np.random.random(\n            size=spatial_rank) < self._flip_probability\n\n    def _apply_transformation(self, image):\n        assert self._rand_flip is not None, ""Flip is unset -- Error!""\n        for axis_number, do_flip in enumerate(self._rand_flip):\n            if axis_number in self._flip_axes and do_flip:\n                image = np.flip(image, axis=axis_number)\n        return image\n\n    def layer_op(self, inputs, interp_orders=None, *args, **kwargs):\n        if inputs is None:\n            return inputs\n        if isinstance(inputs, dict) and isinstance(interp_orders, dict):\n            for (field, image_data) in inputs.items():\n                assert (all([i < 0 for i in interp_orders[field]]) or\n                    all([i >= 0 for i in interp_orders[field]])), \\\n                    \'Cannot combine interpolatable and non-interpolatable data\'\n                if interp_orders[field][0]<0:\n                    continue\n                inputs[field] = self._apply_transformation(image_data)\n        else:\n            inputs = self._apply_transformation(inputs)\n        return inputs\n'"
niftynet/layer/rand_rotation.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport scipy.ndimage\n\nfrom niftynet.layer.base_layer import RandomisedLayer\n\n\nclass RandomRotationLayer(RandomisedLayer):\n    """"""\n    generate randomised rotation matrix for data augmentation\n    """"""\n\n    def __init__(self, name=\'random_rotation\'):\n        super(RandomRotationLayer, self).__init__(name=name)\n        self._transform = None\n        self.min_angle = None\n        self.max_angle = None\n        self.rotation_angle_x = None\n        self.rotation_angle_y = None\n        self.rotation_angle_z = None\n\n    def init_uniform_angle(self, rotation_angle=(-10.0, 10.0)):\n        assert rotation_angle[0] < rotation_angle[1]\n        self.min_angle = float(rotation_angle[0])\n        self.max_angle = float(rotation_angle[1])\n\n    def init_non_uniform_angle(self,\n                               rotation_angle_x,\n                               rotation_angle_y,\n                               rotation_angle_z):\n        if len(rotation_angle_x):\n            assert rotation_angle_x[0] < rotation_angle_x[1]\n        if len(rotation_angle_y):\n            assert rotation_angle_y[0] < rotation_angle_y[1]\n        if len(rotation_angle_z):\n            assert rotation_angle_z[0] < rotation_angle_z[1]\n        self.rotation_angle_x = [float(e) for e in rotation_angle_x]\n        self.rotation_angle_y = [float(e) for e in rotation_angle_y]\n        self.rotation_angle_z = [float(e) for e in rotation_angle_z]\n\n    def randomise(self, spatial_rank=3):\n        if spatial_rank == 3:\n            self._randomise_transformation_3d()\n        else:\n            # currently not supported spatial rank for rand rotation\n            pass\n\n    def _randomise_transformation_3d(self):\n        angle_x = 0.0\n        angle_y = 0.0\n        angle_z = 0.0\n        if self.min_angle is None and self.max_angle is None:\n            # generate transformation\n            if len(self.rotation_angle_x) >= 2:\n                angle_x = np.random.uniform(\n                    self.rotation_angle_x[0],\n                    self.rotation_angle_x[1]) * np.pi / 180.0\n\n            if len(self.rotation_angle_y) >= 2:\n                angle_y = np.random.uniform(\n                    self.rotation_angle_y[0],\n                    self.rotation_angle_y[1]) * np.pi / 180.0\n\n            if len(self.rotation_angle_z) >= 2:\n                angle_z = np.random.uniform(\n                    self.rotation_angle_z[0],\n                    self.rotation_angle_z[1]) * np.pi / 180.0\n        else:\n            # generate transformation\n            angle_x = np.random.uniform(\n                self.min_angle, self.max_angle) * np.pi / 180.0\n            angle_y = np.random.uniform(\n                self.min_angle, self.max_angle) * np.pi / 180.0\n            angle_z = np.random.uniform(\n                self.min_angle, self.max_angle) * np.pi / 180.0\n\n        transform_x = np.array([[np.cos(angle_x), -np.sin(angle_x), 0.0],\n                                [np.sin(angle_x), np.cos(angle_x), 0.0],\n                                [0.0, 0.0, 1.0]])\n        transform_y = np.array([[np.cos(angle_y), 0.0, np.sin(angle_y)],\n                                [0.0, 1.0, 0.0],\n                                [-np.sin(angle_y), 0.0, np.cos(angle_y)]])\n        transform_z = np.array([[1.0, 0.0, 0.0],\n                                [0.0, np.cos(angle_z), -np.sin(angle_z)],\n                                [0.0, np.sin(angle_z), np.cos(angle_z)]])\n        transform = np.dot(transform_z, np.dot(transform_x, transform_y))\n        self._transform = transform\n\n    def _apply_transformation_3d(self, image_3d, interp_order=3):\n        if interp_order < 0:\n            return image_3d\n        assert image_3d.ndim == 3\n        assert self._transform is not None\n        assert all([dim > 1 for dim in image_3d.shape]), \\\n            \'random rotation supports 3D inputs only\'\n        center_ = 0.5 * np.asarray(image_3d.shape, dtype=np.int64)\n        c_offset = center_ - center_.dot(self._transform)\n        image_3d[...] = scipy.ndimage.affine_transform(\n            image_3d[...], self._transform.T, c_offset, order=interp_order)\n        return image_3d\n\n    def layer_op(self, inputs, interp_orders, *args, **kwargs):\n        if inputs is None:\n            return inputs\n\n        if isinstance(inputs, dict) and isinstance(interp_orders, dict):\n            for (field, image) in inputs.items():\n                interp_order = interp_orders[field][0]\n                for channel_idx in range(image.shape[-1]):\n                    if image.ndim == 4:\n                        inputs[field][..., channel_idx] = \\\n                            self._apply_transformation_3d(\n                                image[..., channel_idx], interp_order)\n                    elif image.ndim == 5:\n                        for t in range(image.shape[-2]):\n                            inputs[field][..., t, channel_idx] = \\\n                                self._apply_transformation_3d(\n                                    image[..., t, channel_idx], interp_order)\n                    else:\n                        raise NotImplementedError(""unknown input format"")\n            # shapes = []\n            # for (field, image) in inputs.items():\n            #     shapes.append(image.shape)\n            # assert(len(shapes) == 2 and shapes[0][0:4] == shapes[1][0:4]), shapes\n        else:\n            raise NotImplementedError(""unknown input format"")\n        return inputs\n\n        # if inputs.spatial_rank == 3:\n        #    if inputs.data.ndim == 4:\n        #        for mod_i in range(inputs.data.shape[-1]):\n        #            inputs.data[..., mod_i] = self._apply_transformation_3d(\n        #                inputs.data[..., mod_i], inputs.interp_order)\n        #    if inputs.data.ndim == 5:\n        #        for t in range(inputs.data.shape[-1]):\n        #            for mod_i in range(inputs.data.shape[-2]):\n        #                inputs.data[..., mod_i, t] = \\\n        #                    self._apply_transformation_3d(\n        #                      inputs.data[..., mod_i, t], inputs.interp_order)\n        #    if inputs.interp_order > 0:\n        #        inputs.data = inputs.data.astype(np.float)\n        #    elif inputs.interp_order == 0:\n        #        inputs.data = inputs.data.astype(np.int64)\n        #    else:\n        #        raise ValueError(\'negative interpolation order\')\n        #    return inputs\n        # else:\n        #    # TODO: rotation for spatial_rank is 2\n        #    # currently not supported 2/2.5D rand rotation\n        #    return inputs\n'"
niftynet/layer/rand_spatial_scaling.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport warnings\n\nimport numpy as np\nimport scipy.ndimage as ndi\n\nfrom niftynet.layer.base_layer import RandomisedLayer\n\nwarnings.simplefilter(""ignore"", UserWarning)\nwarnings.simplefilter(""ignore"", RuntimeWarning)\n\n\nclass RandomSpatialScalingLayer(RandomisedLayer):\n    """"""\n    generate randomised scaling along each dim for data augmentation\n    """"""\n\n    def __init__(self,\n                 min_percentage=-10.0,\n                 max_percentage=10.0,\n                 antialiasing=True,\n                 isotropic=False,\n                 name=\'random_spatial_scaling\'):\n        super(RandomSpatialScalingLayer, self).__init__(name=name)\n        assert min_percentage <= max_percentage\n        self._min_percentage = max(min_percentage, -99.9)\n        self._max_percentage = max_percentage\n        self.antialiasing = antialiasing\n        self.isotropic = isotropic\n        self._rand_zoom = None\n\n    def randomise(self, spatial_rank=3):\n        spatial_rank = int(np.floor(spatial_rank))\n        if self.isotropic:\n            one_rand_zoom = np.random.uniform(low=self._min_percentage,\n                                              high=self._max_percentage)\n            rand_zoom = np.repeat(one_rand_zoom, spatial_rank)\n        else:\n            rand_zoom = np.random.uniform(low=self._min_percentage,\n                                          high=self._max_percentage,\n                                          size=(spatial_rank,))\n        self._rand_zoom = (rand_zoom + 100.0) / 100.0\n\n    def _get_sigma(self, zoom):\n        """"""\n        Compute optimal standard deviation for Gaussian kernel.\n\n            Cardoso et al., ""Scale factor point spread function matching:\n            beyond aliasing in image resampling"", MICCAI 2015\n        """"""\n        k = 1 / zoom\n        variance = (k ** 2 - 1 ** 2) * (2 * np.sqrt(2 * np.log(2))) ** (-2)\n        sigma = np.sqrt(variance)\n        return sigma\n\n    def _apply_transformation(self, image, interp_order=3):\n        if interp_order < 0:\n            return image\n        assert self._rand_zoom is not None\n        full_zoom = np.array(self._rand_zoom)\n        while len(full_zoom) < image.ndim:\n            full_zoom = np.hstack((full_zoom, [1.0]))\n        is_undersampling = all(full_zoom[:3] < 1)\n        run_antialiasing_filter = self.antialiasing and is_undersampling\n        if run_antialiasing_filter:\n            sigma = self._get_sigma(full_zoom[:3])\n        if image.ndim == 4:\n            output = []\n            for mod in range(image.shape[-1]):\n                to_scale = ndi.gaussian_filter(image[..., mod], sigma) if \\\n                    run_antialiasing_filter else image[..., mod]\n                scaled = ndi.zoom(to_scale, full_zoom[:3], order=interp_order)\n                output.append(scaled[..., np.newaxis])\n            return np.concatenate(output, axis=-1)\n        elif image.ndim == 3:\n            to_scale = ndi.gaussian_filter(image, sigma) \\\n                if run_antialiasing_filter else image\n            scaled = ndi.zoom(\n                to_scale, full_zoom[:3], order=interp_order)\n            return scaled[..., np.newaxis]\n        else:\n            raise NotImplementedError(\'not implemented random scaling\')\n\n    def layer_op(self, inputs, interp_orders, *args, **kwargs):\n        if inputs is None:\n            return inputs\n\n        if isinstance(inputs, dict) and isinstance(interp_orders, dict):\n\n            for (field, image) in inputs.items():\n                transformed_data = []\n                interp_order = interp_orders[field][0]\n                for mod_i in range(image.shape[-1]):\n                    scaled_data = self._apply_transformation(\n                        image[..., mod_i], interp_order)\n                    transformed_data.append(scaled_data[..., np.newaxis])\n                inputs[field] = np.concatenate(transformed_data, axis=-1)\n            # shapes = []\n            # for (field, image) in inputs.items():\n            #     shapes.append(image.shape)\n            # assert(len(shapes) == 2 and shapes[0][0:4] == shapes[1][0:4]), shapes\n        else:\n            raise NotImplementedError(""unknown input format"")\n        return inputs\n'"
niftynet/layer/resampler.py,116,"b'# -*- coding: utf-8 -*-\n""""""\nResampler layer initially implemented in\nhttps://github.com/niftk/NiftyNet/blob/v0.2.0.post1/niftynet/layer/spatial_transformer.py\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.layer_util import infer_spatial_rank\nfrom niftynet.utilities.util_common import look_up_operations\n\nCOORDINATES_TYPE = tf.int32\nEPS = 1e-6\n\n\nclass ResamplerLayer(Layer):\n    """"""\n    resample inputs according to ``sample_coords``\n    """"""\n\n    def __init__(self,\n                 interpolation=""LINEAR"",\n                 boundary=""REPLICATE"",\n                 name=""resampler"",\n                 implementation=""Fast""):\n        super(ResamplerLayer, self).__init__(name=name)\n        self.boundary = boundary.upper()\n        self.boundary_func = look_up_operations(\n            self.boundary, SUPPORTED_BOUNDARY)\n        self.interpolation = look_up_operations(\n            interpolation.upper(), SUPPORTED_INTERPOLATION)\n\n        if self.boundary == \'ZERO\' and self.interpolation == \'BSPLINE\':\n            tf.logging.fatal(\'Zero padding is not supported for BSPLINE mode\')\n            raise NotImplementedError\n\n        if self.boundary == \'ZERO\' and self.interpolation == \'IDW\':\n            tf.logging.warning(\'Zero padding is not supported for IDW mode\')\n            # raise NotImplementedError\n\n        self.FastResamplerLayer = None  #\n        if implementation.lower() in [\'niftyreg\', \'fast\']:\n            # check if niftyreg_resampling_layer is installed\n            try:\n                from niftyreg_image_resampling import NiftyregImageResamplingLayer\n                import niftyreg_image_resampling as resampler_module\n            except ImportError:\n                tf.logging.warning(\'\'\'\n                    niftyreg_image_resampling is not installed; falling back onto\n                    niftynet.layer.resampler.ResamplerLayer. To allow fast resampling,\n                    please see installation instructions in\n                    niftynet/contrib/niftyreg_image_resampling/README.md\n                    \'\'\')\n                return\n\n            # Passthrough of supported boundary types for  resampling\n            SUPPORTED_BOUNDARY_FAST = resampler_module.SUPPORTED_BOUNDARY\n\n            # Passthrough of supported interpolation types for NiftyReg resampling\n            SUPPORTED_INTERPOLATION_FAST = resampler_module.SUPPORTED_INTERPOLATION\n            # check compatibility of the resampling options with niftyreg_image_resampling\n            try:\n                boundary_fast = look_up_operations(self.boundary, SUPPORTED_BOUNDARY_FAST)\n                interp_fast = look_up_operations(self.interpolation, SUPPORTED_INTERPOLATION_FAST)\n                self.FastResamplerLayer = NiftyregImageResamplingLayer(interp_fast, boundary_fast)\n                tf.logging.info(\'\'\'NiftyReg image resampling is used.\'\'\')\n            except ValueError as e:\n                tf.logging.warning(e)\n                tf.logging.warning(\'\'\'Falling back onto niftynet.layer.resampler.ResamplerLayer.\'\'\')\n\n    def layer_op(self, inputs, sample_coords):\n        """"""\n        This layer resamples 2D or 3D data given the coordinates.\n\n        In terms of 3D inputs,\n\n        when the shape of ``inputs`` is ``[batch, x, y, z, num_channels]``,\n        the shape of ``sample_coords`` can be\n        ``[1, d0, d1, ..., 3]`` or ``[batch, d0, d1, ..., 3]``.\n\n        The output shape would be ``[batch, d0, d1, ..., num_channels]``.\n\n        Similarly, in 2D,\n\n        when the shape of ``inputs`` is ``[batch, x, y, num_channels]``,\n        the shape of ``sample_coords`` can be\n        ``[1, d0, d1, ..., 2]`` or ``[batch, d0, d1, ..., 2]``.\n\n        The output shape would be ``[batch, d0, d1, ... num_channels]``\n\n        (If the shape of ``inputs`` is not fully specified, ``sample_coords``\n        must be checked before using this function, to make sure the\n        coordinates are pointing to locations within the inputs.)\n\n        (Resampling 2D inputs is implemented by calling\n        ``tf.contrib.resampler.resampler``. The interpretaion of coordinates is\n        different in between this function and\n        ``tf.contrib.resampler.resampler``:\n        using ``self.layer_op(inputs, sample_coords)`` for 2D data\n        is equivalent to (apart from the batch size broadcasting feature)::\n\n            tf.contrib.resampler.resampler(\n                tf.transpose(inputs, [0, 2, 1, 3]), sample_coords)\n\n        (No gradient is computed for ``NEAREST`` method, and\n         some of the padding modes.)\n        """"""\n\n        # check the input dims\n        try:\n            batch_inputs = int(inputs.shape[0])\n            batch_sample_coords = int(sample_coords.shape[0])\n        except (TypeError, ValueError):\n            tf.logging.fatal(\'Unknown input shape, at least batch size \'\n                             \'needs to be specified.\')\n            raise\n\n        if batch_inputs != batch_sample_coords and batch_sample_coords > 1:\n            tf.logging.fatal(\n                \'\\nOnly the following two cases are currently supported:\\n\'\n                \'    - batch size of inputs == batch size of sample_coords\\n\'\n                \'    - batch size of sample_coords == 1\\n\'\n                \'In the second case, sample_coords will be applied to each of \'\n                \'the batch component of the inputs.\')\n            raise ValueError\n\n        # input_spatial_rank = infer_spatial_rank(inputs)\n        # if input_spatial_rank != 2 and input_spatial_rank != 3:\n        #     tf.logging.fatal(\'Only 2D or 3D inputs are supported.\')\n        #     raise ValueError\n\n        try:\n            coords_n_dim = int(sample_coords.shape[-1])\n        except (TypeError, ValueError):\n            tf.logging.fatal(\n                \'The last dim of the coordinates must have 2 or 3 elements.\')\n            raise\n\n        if infer_spatial_rank(inputs) != coords_n_dim:\n            tf.logging.fatal(\n                \'sample_coords.shape[-1] must be the same as the spatial rank \'\n                \'of the inputs.\')\n            raise ValueError\n\n        # currently converting everything to floats\n        if inputs.dtype not in SUPPORTED_INPUT_DTYPE:\n            inputs = tf.to_float(inputs)\n        if sample_coords.dtype not in SUPPORTED_INPUT_DTYPE:\n            sample_coords = tf.to_float(sample_coords)\n\n        # use fast resampling layer if available\n        if self.FastResamplerLayer is not None:\n            return self.FastResamplerLayer.layer_op(inputs, sample_coords)\n        # otherwise compatibility resampling layer is used\n        if self.interpolation == \'LINEAR\':\n            return self._resample_linear(inputs, sample_coords)\n        if self.interpolation == \'NEAREST\':\n            return self._resample_nearest(inputs, sample_coords)\n        if self.interpolation == \'BSPLINE\':\n            return self._resample_bspline(inputs, sample_coords)\n        if self.interpolation == \'IDW\':\n            return self._resample_inv_dst_weighting(inputs, sample_coords)\n        tf.logging.fatal(\'interpolation method not implmented\')\n        raise NotImplementedError\n\n    def _resample_nearest(self, inputs, sample_coords):\n        # This is forward only as no gradient for tf.round\n\n        # read input shape\n        in_size = inputs.shape\n        partial_shape = not in_size.is_fully_defined()\n        in_spatial_size = None\n\n        try:\n            batch_size = int(in_size[0])\n            n_coords = int(sample_coords.shape[0])\n        except (TypeError, AssertionError, ValueError):\n            tf.logging.fatal(\'Unknown input shape, at least batch size \'\n                             \'and rank of the inputs are required.\')\n            raise\n\n        # quantise coordinates\n        spatial_coords = tf.round(sample_coords)\n        if not partial_shape:\n            in_spatial_size = in_size.as_list()[1:-1]\n            spatial_coords = self.boundary_func(\n                spatial_coords, in_spatial_size)\n        spatial_coords = tf.cast(spatial_coords, COORDINATES_TYPE)\n\n        if batch_size == n_coords:\n            batch_inputs = tf.unstack(inputs)\n            batch_coords = tf.unstack(spatial_coords)\n            gathered_image = [tf.gather_nd(img, coord) for (img, coord) in\n                              zip(batch_inputs, batch_coords)]\n        elif n_coords == 1 and batch_size > 1:\n            gathered_image = [tf.gather_nd(img, spatial_coords[0])\n                              for img in tf.unstack(inputs)]\n        else:\n            raise NotImplementedError\n        output = tf.stack(gathered_image, axis=0)\n\n        if self.boundary == \'ZERO\' and in_spatial_size:\n            scale = 1. / (tf.constant(in_spatial_size, dtype=tf.float32) - 1.)\n            mask = tf.logical_and(\n                tf.reduce_all(sample_coords > 0, -1, True),\n                tf.reduce_all(scale * sample_coords < 1, -1, True))\n            return output * tf.to_float(mask)\n        return output\n\n    def _resample_linear(self, inputs, sample_coords):\n        """"""\n        Bilinear or trilinear resampling.\n\n        :param inputs:\n        :param sample_coords:\n        :return:\n        """"""\n\n        # read input shape\n        in_size = inputs.shape\n        partial_shape = not in_size.is_fully_defined()\n\n        try:\n            batch_size = int(in_size[0])\n            n_coords = int(sample_coords.shape[0])\n            in_spatial_rank = infer_spatial_rank(inputs)\n            in_spatial_size = \\\n                None if partial_shape else in_size.as_list()[1:-1]\n        except (TypeError, AssertionError, ValueError):\n            tf.logging.fatal(\'Unknown input shape, at least batch size \'\n                             \'and rank of the inputs are required.\')\n            raise\n\n        # read output shape\n        out_spatial_rank = infer_spatial_rank(sample_coords)\n        out_spatial_size = sample_coords.shape.as_list()[1:-1]\n\n        if in_spatial_rank == 2 and self.boundary == \'ZERO\':\n            # calling TF\'s resampler\n            inputs = tf.transpose(inputs, [0, 2, 1, 3])\n            if batch_size == n_coords:\n                return tf.contrib.resampler.resampler(inputs, sample_coords)\n            outputs = [\n                tf.contrib.resampler.resampler(\n                    tf.expand_dims(img), sample_coords)\n                for img in tf.unstack(inputs)]\n            return tf.concat(outputs, axis=0)\n\n        xy = tf.unstack(sample_coords, axis=-1)\n        base_coords = [tf.floor(coords) for coords in xy]\n        if partial_shape:\n            # if input shape is not defined, unable to compute\n            # boundary elements\n            floor_coords = [coord for coord in base_coords]\n            ceil_coords = [coord + 1.0 for coord in base_coords]\n        else:\n            floor_coords = [self.boundary_func(x, in_spatial_size[idx])\n                            for (idx, x) in enumerate(base_coords)]\n            ceil_coords = [self.boundary_func(x + 1.0, in_spatial_size[idx])\n                           for (idx, x) in enumerate(base_coords)]\n\n        if self.boundary == \'ZERO\':\n            weight_0 = [tf.expand_dims(x - i, -1)\n                        for (x, i) in zip(xy, floor_coords)]\n            weight_1 = [tf.expand_dims(i - x, -1)\n                        for (x, i) in zip(xy, ceil_coords)]\n        else:\n            weight_0 = [tf.expand_dims(x - i, -1)\n                        for (x, i) in zip(xy, base_coords)]\n            weight_1 = [1.0 - w for w in weight_0]\n\n        sc = (tf.cast(floor_coords, COORDINATES_TYPE),\n              tf.cast(ceil_coords, COORDINATES_TYPE))\n\n        if n_coords == 1 and batch_size > 1:\n            # fetch neighbours with the same coordinates across the input batch\n            inputs = tf.unstack(inputs)\n\n            def _get_knot(bc):\n                coord = [sc[c][i] for i, c in enumerate(bc)]\n                coord = tf.stack(coord, axis=-1)\n                batch_samples = [tf.gather_nd(img, coord) for img in inputs]\n                batch_samples = tf.concat(batch_samples, axis=0)\n                return batch_samples\n\n        elif n_coords == batch_size:\n            batch_ids = tf.reshape(\n                tf.range(batch_size), [batch_size] + [1] * out_spatial_rank)\n            batch_ids = tf.tile(batch_ids, [1] + out_spatial_size)\n\n            def _get_knot(bc):\n                coord = [batch_ids] + [sc[c][i] for i, c in enumerate(bc)]\n                coord = tf.stack(coord, axis=-1)\n                return tf.gather_nd(inputs, coord)\n        else:\n            raise NotImplementedError\n\n        def _pyramid_combination(samples, w_0, w_1):\n            # the case where n_coords = 1 and batch_size > 1 is handled by\n            # shape broadcasting\n            if len(w_0) == 1:\n                return samples[0] * w_1[0] + samples[1] * w_0[0]\n            f_0 = _pyramid_combination(samples[::2], w_0[:-1], w_1[:-1])\n            f_1 = _pyramid_combination(samples[1::2], w_0[:-1], w_1[:-1])\n            return f_0 * w_1[-1] + f_1 * w_0[-1]\n\n        binary_neighbour_ids = _binary_neighbour_ids(in_spatial_rank)\n        samples = [_get_knot(bc) for bc in binary_neighbour_ids]\n\n        return _pyramid_combination(samples, weight_0, weight_1)\n\n    def _resample_bspline(self, inputs, sample_coords):\n        assert inputs.shape.is_fully_defined(), \\\n            ""input shape should be fully defined for bspline interpolation""\n        in_size = inputs.shape.as_list()\n        batch_size = in_size[0]\n        in_spatial_size = in_size[1:-1]\n        in_spatial_rank = infer_spatial_rank(inputs)\n\n        out_spatial_rank = infer_spatial_rank(sample_coords)\n        if in_spatial_rank == 2:\n            raise NotImplementedError(\n                \'bspline interpolation not implemented for 2d yet\')\n        assert batch_size == int(sample_coords.get_shape()[0])\n        floor_coords = tf.floor(sample_coords)\n\n        # Compute voxels to use for interpolation\n        grid = tf.meshgrid([-1., 0., 1., 2.],\n                           [-1., 0., 1., 2.],\n                           [-1., 0., 1., 2.],\n                           indexing=\'ij\')\n        offset_shape = [1, -1] + [1] * out_spatial_rank + [in_spatial_rank]\n        offsets = tf.reshape(tf.stack(grid, 3), offset_shape)\n        spatial_coords = offsets + tf.expand_dims(floor_coords, 1)\n        spatial_coords = self.boundary_func(spatial_coords, in_spatial_size)\n        spatial_coords = tf.cast(spatial_coords, COORDINATES_TYPE)\n        knot_size = spatial_coords.shape.as_list()\n\n        # Compute weights for each voxel\n        def build_coef(u, d):\n            coeff_list = [tf.pow(1 - u, 3),\n                          3 * tf.pow(u, 3) - 6 * tf.pow(u, 2) + 4,\n                          -3 * tf.pow(u, 3) + 3 * tf.pow(u, 2) + 3 * u + 1,\n                          tf.pow(u, 3)]\n            return tf.concat(coeff_list, d) / 6\n\n        weight = tf.reshape(sample_coords - floor_coords, [batch_size, -1, 3])\n        coef_shape = [batch_size, 1, 1, 1, -1]\n        Bu = build_coef(tf.reshape(weight[:, :, 0], coef_shape), 1)\n        Bv = build_coef(tf.reshape(weight[:, :, 1], coef_shape), 2)\n        Bw = build_coef(tf.reshape(weight[:, :, 2], coef_shape), 3)\n        all_weights = tf.reshape(Bu * Bv * Bw,\n                                 [batch_size] + knot_size[1:-1] + [1])\n\n        # Gather voxel values and compute weighted sum\n        batch_coords = tf.reshape(\n            tf.range(batch_size), [batch_size] + [1] * (len(knot_size) - 1))\n        batch_coords = tf.tile(batch_coords, [1] + knot_size[1:-1] + [1])\n        raw_samples = tf.gather_nd(\n            inputs, tf.concat([batch_coords, spatial_coords], -1))\n        return tf.reduce_sum(all_weights * raw_samples, reduction_indices=1)\n\n    def _resample_inv_dst_weighting(self, inputs, sample_coords):\n        # inverse distance weighting using 2^(sptial_rank) neighbours\n        in_size = inputs.shape\n        partial_shape = not in_size.is_fully_defined()\n        try:\n            batch_size = int(in_size[0])\n            n_coords = int(sample_coords.shape[0])\n            in_spatial_rank = infer_spatial_rank(inputs)\n            in_spatial_size = \\\n                None if partial_shape else in_size.as_list()[1:-1]\n        except (TypeError, AssertionError, ValueError):\n            tf.logging.fatal(\'Unknown input shape, at least batch size \'\n                             \'and rank of the inputs are required.\')\n            raise\n\n        out_rank = len(sample_coords.get_shape())\n        binary_neighbour_ids = _binary_neighbour_ids(in_spatial_rank)\n        weight_id = [[[c, i] for i, c in enumerate(bc)]\n                     for bc in binary_neighbour_ids]\n        sample_coords_shape = [out_rank - 1, 0] + list(range(1, out_rank - 1))\n        sample_coords = tf.transpose(sample_coords, sample_coords_shape)\n\n        if partial_shape or in_spatial_size is None:\n            all_coords_f = tf.stack(\n                [tf.floor(sample_coords), tf.ceil(sample_coords)])\n        else:\n            # broadcasting input spatial size for boundary functions\n            expanded_spatial_size = \\\n                [len(in_spatial_size)] + [1] * (out_rank - 1)\n            b_size = tf.reshape(in_spatial_size, expanded_spatial_size)\n            # find floor and ceil coordinates\n            all_coords_f = tf.stack([\n                self.boundary_func(tf.floor(sample_coords), b_size),\n                self.boundary_func(tf.ceil(sample_coords), b_size)])\n\n        # find N weights associated to each output point\n        diff = tf.stack(\n            [tf.squared_difference(sample_coords - EPS, all_coords_f[0]),\n             tf.squared_difference(sample_coords + EPS, all_coords_f[1])])\n\n        # gather_nd for both matrices, the same as:\n        # point_weights = tf.gather_nd(diff, weight_id)\n        # knots_id = tf.gather_nd(all_coords_f, weight_id)\n        n_val = tf.gather_nd(\n            tf.stack([diff, all_coords_f], axis=-1), weight_id)\n        n_val = tf.unstack(n_val, axis=-1)\n        point_weights, knots_id = n_val[0], n_val[1]\n\n        # inverse distance weighting\n        # sum_i (w_i*p_i/(sum_j w_j)) where w_i = 1/((p-p_i)^2)\n        # point_weights has the shape:100\n        # `[N, input_rank, b, sp_dim_0, ..., sp_dim_K]`\n        # where:\n        #  `N` is 2**source data spatial rank\n        #  `b` is batch size,\n        #  `sp_dim_0` is the output spatial output 0,\n        #\n        # `point_weights` represents (p - p_i)^2\n        #      with i= 0...2**source_rank neighbours\n        # (to do: these operations could be refactored as a resampling kernel)\n        point_weights = tf.reduce_sum(point_weights, axis=1)\n        # skip this as power = 2.0:\n        # self.power = 2.0\n        # point_weights = tf.pow(point_weights, self.power / 2.0)\n        point_weights = tf.reciprocal(point_weights)\n        point_weights = point_weights / tf.reduce_sum(point_weights, axis=0)\n\n        # find N neighbours associated to each output point\n        # knots_shape = tf.concat([[0], tf.range(2, out_rank + 1), [1]], 0)\n        knots_shape = [0] + list(range(2, out_rank + 1)) + [1]\n        knots_id = tf.cast(knots_id, COORDINATES_TYPE)\n        knots_id = tf.transpose(knots_id, knots_shape)\n\n        # get values of N neighbours\n        batch_inputs = tf.unstack(inputs, axis=0)\n        batch_knots = tf.unstack(knots_id, axis=1)\n        if batch_size == n_coords:\n            samples = [tf.gather_nd(img, knot)\n                       for (img, knot) in zip(batch_inputs, batch_knots)]\n        elif n_coords == 1 and batch_size > 1:\n            samples = [tf.gather_nd(img, batch_knots[0])\n                       for img in batch_inputs]\n        else:\n            raise NotImplementedError\n        samples = tf.stack(samples, axis=1)\n        # weighted average over N neighbours\n        return tf.reduce_sum(\n            samples * tf.expand_dims(point_weights, axis=-1), axis=0)\n\n\ndef _boundary_replicate(sample_coords, input_size):\n    sample_coords, input_size = _param_type_and_shape(sample_coords, input_size)\n    return tf.maximum(tf.minimum(sample_coords, input_size - 1), 0)\n\n\ndef _boundary_circular(sample_coords, input_size):\n    sample_coords, input_size = _param_type_and_shape(sample_coords, input_size)\n    return tf.mod(tf.mod(sample_coords, input_size) + input_size, input_size)\n\n\ndef _boundary_symmetric(sample_coords, input_size):\n    sample_coords, input_size = _param_type_and_shape(sample_coords, input_size)\n    circular_size = input_size + input_size - 2\n    return (input_size - 1) - tf.abs(\n        (input_size - 1) - _boundary_circular(sample_coords, circular_size))\n\n\ndef _param_type_and_shape(sample_coords, input_size):\n    # sample_coords = tf.cast(sample_coords, COORDINATES_TYPE)\n    try:\n        # if input_size is a numpy array\n        input_size = tf.constant(input_size, dtype=sample_coords.dtype)\n    except (TypeError, AttributeError):\n        pass\n    try:\n        # if input_size is a TF Tensor\n        input_size = tf.cast(input_size, dtype=sample_coords.dtype)\n    except (TypeError, AttributeError):\n        pass\n    return sample_coords, input_size\n\n\ndef _binary_neighbour_ids(spatial_rank):\n    """"""\n    returns combinatorial binary indices::\n\n        1-D: [[0], [1]]\n        2-D: [[0, 0], [0, 1], [1, 0], [1, 1]]\n        3-D: [[0, 0, 0], [0, 0, 1], [0, 1, 0],\n              [0, 1, 1], [1, 0, 0], [1, 0, 1],\n              [1, 1, 0], [1, 1, 1]]\n\n    """"""\n    return [[int(c) for c in format(i, \'0%ib\' % spatial_rank)]\n            for i in range(2 ** spatial_rank)]\n\n\ntry:  # Some tf versions have this defined already\n    @tf.RegisterGradient(\'FloorMod\')\n    def _floormod_grad(op, grad):\n        return [None, None]\nexcept:\n    pass\n\nSUPPORTED_INTERPOLATION = {\'BSPLINE\', \'LINEAR\', \'NEAREST\', \'IDW\'}\n\nSUPPORTED_BOUNDARY = {\n    \'ZERO\': _boundary_replicate,\n    \'REPLICATE\': _boundary_replicate,\n    \'CIRCULAR\': _boundary_circular,\n    \'SYMMETRIC\': _boundary_symmetric}\n\nSUPPORTED_INPUT_DTYPE = {tf.float32}\n'"
niftynet/layer/residual_unit.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer.activation import ActiLayer as Acti\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer as Conv\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_OP = set([\'original\', \'conv_bn_acti\', \'acti_conv_bn\', \'bn_acti_conv\'])\n\n\nclass ResidualUnit(TrainableLayer):\n    def __init__(self,\n                 n_output_chns=1,\n                 kernel_size=3,\n                 dilation=1,\n                 acti_func=\'relu\',\n                 w_initializer=None,\n                 w_regularizer=None,\n                 moving_decay=0.9,\n                 eps=1e-5,\n                 type_string=\'bn_acti_conv\',\n                 name=\'res-downsample\'):\n        """"""\n        Implementation of residual unit presented in:\n\n            [1] He et al., Identity mapping in deep residual networks, ECCV 2016\n            [2] He et al., Deep residual learning for image recognition, CVPR 2016\n\n        The possible types of connections are::\n\n            \'original\': residual unit presented in [2]\n            \'conv_bn_acti\': ReLU before addition presented in [1]\n            \'acti_conv_bn\': ReLU-only pre-activation presented in [1]\n            \'bn_acti_conv\': full pre-activation presented in [1]\n\n        [1] recommends \'bn_acti_conv\'\n\n        :param n_output_chns: number of output feature channels\n            if this doesn\'t match the input, a 1x1 projection will be created.\n        :param kernel_size:\n        :param dilation:\n        :param acti_func:\n        :param w_initializer:\n        :param w_regularizer:\n        :param moving_decay:\n        :param eps:\n        :param type_string:\n        :param name:\n        """"""\n\n        super(TrainableLayer, self).__init__(name=name)\n        self.type_string = look_up_operations(type_string.lower(), SUPPORTED_OP)\n        self.acti_func = acti_func\n        self.conv_param = {\'w_initializer\': w_initializer,\n                           \'w_regularizer\': w_regularizer,\n                           \'kernel_size\': kernel_size,\n                           \'dilation\': dilation,\n                           \'n_output_chns\': n_output_chns}\n        self.bn_param = {\'regularizer\': w_regularizer,\n                         \'moving_decay\': moving_decay,\n                         \'eps\': eps}\n\n    def layer_op(self, inputs, is_training=True):\n        """"""\n        The general connections is::\n\n            (inputs)--o-conv_0--conv_1-+-- (outputs)\n                      |                |\n                      o----------------o\n\n        ``conv_0``, ``conv_1`` layers are specified by ``type_string``.\n        """"""\n        conv_flow = inputs\n        # batch normalisation layers\n        bn_0 = BNLayer(**self.bn_param)\n        bn_1 = BNLayer(**self.bn_param)\n        # activation functions //regularisers?\n        acti_0 = Acti(func=self.acti_func)\n        acti_1 = Acti(func=self.acti_func)\n        # convolutions\n        conv_0 = Conv(acti_func=None, with_bias=False, feature_normalization=None,\n                      **self.conv_param)\n        conv_1 = Conv(acti_func=None, with_bias=False, feature_normalization=None,\n                      **self.conv_param)\n\n        if self.type_string == \'original\':\n            conv_flow = acti_0(bn_0(conv_0(conv_flow), is_training))\n            conv_flow = bn_1(conv_1(conv_flow), is_training)\n            conv_flow = ElementwiseLayer(\'SUM\')(conv_flow, inputs)\n            conv_flow = acti_1(conv_flow)\n            return conv_flow\n\n        if self.type_string == \'conv_bn_acti\':\n            conv_flow = acti_0(bn_0(conv_0(conv_flow), is_training))\n            conv_flow = acti_1(bn_1(conv_1(conv_flow), is_training))\n            return ElementwiseLayer(\'SUM\')(conv_flow, inputs)\n\n        if self.type_string == \'acti_conv_bn\':\n            conv_flow = bn_0(conv_0(acti_0(conv_flow)), is_training)\n            conv_flow = bn_1(conv_1(acti_1(conv_flow)), is_training)\n            return ElementwiseLayer(\'SUM\')(conv_flow, inputs)\n\n        if self.type_string == \'bn_acti_conv\':\n            conv_flow = conv_0(acti_0(bn_0(conv_flow, is_training)))\n            conv_flow = conv_1(acti_1(bn_1(conv_flow, is_training)))\n            return ElementwiseLayer(\'SUM\')(conv_flow, inputs)\n\n        raise ValueError(\'Unknown type string\')\n'"
niftynet/layer/rgb_histogram_equilisation.py,0,"b'from __future__ import absolute_import, print_function\n\nimport numpy as np\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.utilities.util_import import require_module\n\nclass RGBHistogramEquilisationLayer(Layer):\n    """"""\n    RGB histogram equilisation. Unlike the multi-modality general\n    histogram normalisation this is done conventionally, on a\n    per-image basis. This layer requires OpenCV.\n    """"""\n\n    def __init__(self,\n                 image_name,\n                 name=\'rgb_normaliser\'):\n        super(RGBHistogramEquilisationLayer, self).__init__(name=name)\n\n        self.image_name = image_name\n\n    def _normalise_image(self, image):\n        """"""\n        Normalises a 2D RGB image, if necessary performs any type casting\n        and reshaping operations.\n        :param image: a 2D RGB image, possibly given as a 5D tensor\n        :return: the normalised image in its original shape\n        """"""\n\n        if isinstance(image.dtype, np.floating) and image.dtype != np.float32:\n            image = image.astype(np.float32)\n        elif isinstance(image.dtype, np.uint):\n            image = image.astype(np.float32)/255\n\n        orig_shape = list(image.shape)\n        if len(orig_shape) == 5 and (orig_shape[2] > 1 or orig_shape[3] > 1):\n            raise ValueError(\'Can only process 2D images.\')\n\n        if len(image.shape) != 3:\n            image = image.reshape(orig_shape[:2] + [orig_shape[-1]])\n\n        image = image[...,::-1]\n\n        cv2 = require_module(\'cv2\')\n        yuv_image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n        intensity = (255*yuv_image[...,0]).astype(np.uint8)\n        yuv_image[...,0] = cv2.equalizeHist(intensity).astype(np.float32)/255\n\n        return cv2.cvtColor(yuv_image, cv2.COLOR_YUV2BGR)[...,::-1]\\\n                  .reshape(orig_shape)\n\n    def layer_op(self, image, mask=None):\n        """"""\n        :param image: a 3-channel tensor assumed to be an image in floating-point\n        RGB format (each channel in [0, 1])\n        :return: the equilised image\n        """"""\n\n        if isinstance(image, dict):\n            image[self.image_name] = self._normalise_image(\n                image[self.image_name])\n\n            return image, mask\n        else:\n            return self._normalise_image(image), mask\n'"
niftynet/layer/spatial_gradient.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.layer_util import infer_spatial_rank\n\n\nclass SpatialGradientLayer(Layer):\n    """"""\n    Computing image spatial gradients.\n    """"""\n\n    def __init__(self,\n                 spatial_axis=0,\n                 do_cropping=True,\n                 name=\'spatial_gradient\'):\n\n        Layer.__init__(self, name=name)\n        self.spatial_axis = int(spatial_axis)\n        self.do_cropping = do_cropping\n\n    def layer_op(self, input_tensor):\n        """"""\n        Computing spatial gradient of ``input_tensor`` along\n        ``self.spatial_axis``.\n\n        output is equivalent to convolve along ``spatial_axis`` with a\n         kernel: ``[-1, 0, 1]``\n\n        This layer assumes the first and the last dimension of the input\n        tensor represent batch and feature channels.\n        Therefore ``spatial_axis=1`` is computing gradient along the\n        third dimension of input tensor, i.e., ``input_tensor[:, :, y, ...]``\n\n        Given the input with shape ``[B, X, Y, Z, C]``, and ``spatial_axis=1``\n        the output shape is::\n            [B, X-2, Y-2, Z-2, C] if do_scropping is True\n            [B, X, Y-2, Z, C] otherwise\n\n        Setting do_cropping to True makes the output tensor has the same\n        dimensionality for different ``spatial_axis``.\n\n        :param input_tensor: a batch of images with a shape of\n            ``[Batch, x[, y, z, ... ], Channel]``\n        :return: spatial gradients of ``input_tensor``\n        """"""\n\n        spatial_rank = infer_spatial_rank(input_tensor)\n        spatial_size = input_tensor.get_shape().as_list()[1:-1]\n        if self.do_cropping:\n            # remove two elements in all spatial dims\n            spatial_size = [size_x - 2 for size_x in spatial_size]\n            spatial_begins = [1] * spatial_rank\n        else:\n            # remove two elements along the gradient dim only\n            spatial_size[self.spatial_axis] = spatial_size[self.spatial_axis] -2\n            spatial_begins = [0] * spatial_rank\n\n        spatial_begins[self.spatial_axis] = 2\n        begins_0 = [0] + spatial_begins + [0]\n\n        spatial_begins[self.spatial_axis] = 0\n        begins_1 = [0] + spatial_begins + [0]\n\n        sizes_0 = [-1] + spatial_size + [-1]\n        sizes_1 = [-1] + spatial_size + [-1]\n\n        image_gradients = \\\n            tf.slice(input_tensor, begins_0, sizes_0) - \\\n            tf.slice(input_tensor, begins_1, sizes_1)\n        return image_gradients\n'"
niftynet/layer/spatial_transformer.py,23,"b'# -*- coding: utf-8 -*-\n# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n# Modifications copyright 2018 The NiftyNet Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""""Implementation of Spatial Transformer networks core components.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.grid_warper import GridWarperLayer\nfrom niftynet.layer.resampler import ResamplerLayer\n\nSUPPORTED_INTERPOLATION = set([\'BSPLINE\', \'LINEAR\', \'NEAREST\'])\nSUPPORTED_BOUNDARY = set([\'ZERO\', \'REPLICATE\', \'CIRCULAR\', \'SYMMETRIC\'])\n\n\nclass BSplineFieldImageGridWarperLayer(GridWarperLayer):\n  """""" The fast BSpline Grid Warper defines a grid based on\n      sampling coordinate values from a spatially varying displacement\n      field  (passed as a tensor input) along a regular cartesian grid\n      pattern aligned with the field. Specifically,\n  this class defines a grid based on BSpline smoothing, as described by Rueckert et al.\n      To ensure that it can be done efficiently, several assumptions are made:\n      1) The grid is a cartesian grid aligned with the field.\n      2) Knots occur every M,N,O grid points (in X,Y,Z) This allows the\n         smoothing to be represented as a 4x4x4 convolutional kernel with MxNxO channels\n  """"""\n  def __init__(self,\n               source_shape,\n               output_shape,\n               knot_spacing,\n               name=\'interpolated_spline_grid_warper_layer\'):\n    """"""Constructs an BSplineFieldImageGridWarperLayer.\n    Args:\n      source_shape: Iterable of integers determining the size of the source\n        signal domain.\n      output_shape: Iterable of integers determining the size of the destination\n        resampled signal domain.\n      knot_spacing: List of intervals (in voxels) in each dimension where\n        displacements are defined in the field.\n      interpolation: type_str of interpolation as used by tf.image.resize_images\n      name: Name of Module.""""""\n    coeff_shape=[4+(n-1)//k for n,k in zip(output_shape,knot_spacing)]\n    self._knot_spacing=knot_spacing\n    super(BSplineFieldImageGridWarperLayer, self).__init__(source_shape=source_shape,\n                                           output_shape=output_shape,\n                                           coeff_shape=coeff_shape,\n                                           name=name)\n  def _create_features(self):\n    """""" Creates the convolutional kernel""""""\n    build_coefficient = lambda u,d: np.reshape(np.stack([(np.power(1-u,3))/6,\n                                     (3*np.power(u,3) - 6*np.power(u,2) + 4)/6,\n                                     (-3*np.power(u,3) + 3*np.power(u,2) + 3*u + 1)/6,\n                                      np.power(u,3)/6],0),np.roll([4,1,1,len(u),1,1],d))\n    coeffs = [build_coefficient(np.arange(k)/k,d) for d,k in enumerate(self._knot_spacing)]\n    kernels = tf.constant(np.reshape(np.prod(coeffs),[4,4,4,1,-1]),dtype=tf.float32)\n    return kernels\n  def layer_op(self,field):\n    batch_size=int(field.shape.as_list()[0])\n    spatial_rank = int(field.shape.as_list()[-1])\n    resampled_list=[tf.nn.conv3d(field[:, :, :, :, d:d + 1], self._psi, strides=[1]*5, padding=\'VALID\')\n                    for d in [0, 1, 2]]\n    resampled=tf.stack(resampled_list,5)\n    permuted_shape=[batch_size]+[f-3 for f in self._coeff_shape]+self._knot_spacing+[spatial_rank]\n    permuted=tf.transpose(tf.reshape(resampled,permuted_shape),[0,1,4,2,5,3,6,7])\n    valid_size=[(f-3)*k for f,k in zip(self._coeff_shape,self._knot_spacing)]\n    reshaped=tf.reshape(permuted,[batch_size]+valid_size+[spatial_rank])\n    cropped = reshaped[:,:self._output_shape[0],:self._output_shape[1],:self._output_shape[2],:]\n    return cropped\n\nclass RescaledFieldImageGridWarperLayer(GridWarperLayer):\n  """""" The rescaled field grid warper defines a grid based on\n      sampling coordinate values from a spatially varying displacement\n      field  (passed as a tensor input) along a regular cartesian grid\n      pattern aligned with the field. Specifically, this class defines\n      a grid by resampling the field (using tf.rescale_images with\n      align_corners=False) to the output_shape.\n  """"""\n  def __init__(self,\n               source_shape,\n               output_shape,\n               coeff_shape,\n               interpolation=tf.image.ResizeMethod.BICUBIC,\n               name=\'rescaling_interpolated_spline_grid_warper_layer\'):\n    """""" Constructs an RescaledFieldImageGridWarperLayer.\n    Args:\n      source_shape: Iterable of integers determining the size of the source\n        signal domain.\n      output_shape: Iterable of integers determining the size of the destination\n        resampled signal domain.\n      coeff_shape: Shape of displacement field.\n      interpolation: type_str of interpolation as used by tf.image.resize_images\n      name: Name of Module.\n\n    """"""\n    self._interpolation=interpolation\n    if self._interpolation==\'LINEAR\':\n      self._interpolation=tf.image.ResizeMethod.BILINEAR\n    elif self._interpolation==\'CUBIC\':\n      self._interpolation=tf.image.ResizeMethod.BICUBIC\n\n    super(RescaledFieldImageGridWarperLayer, self).__init__(source_shape=source_shape,\n                                           output_shape=output_shape,\n                                           coeff_shape=coeff_shape,\n                                           name=name)\n  def layer_op(self,field):\n    input_shape = tf.shape(field)\n    input_dtype = field.dtype.as_numpy_dtype\n    batch_size = int(field.shape[0])\n    reshaped_field=tf.reshape(field, [batch_size, self._coeff_shape[0], self._coeff_shape[1], -1])\n    coords_intermediate = tf.image.resize_images(reshaped_field,self._output_shape[0:2],\n                                                 self._interpolation,align_corners=False)\n    sz_xy_z1=[batch_size,self._output_shape[0]*self._output_shape[1],self._coeff_shape[2],-1]\n    tmp=tf.reshape(coords_intermediate,sz_xy_z1)\n    final_sz=[batch_size]+list(self._output_shape)+[-1]\n    sz_xy_z2=[self._output_shape[0]*self._output_shape[1],self._output_shape[2]]\n    coords=tf.reshape(tf.image.resize_images(tmp,sz_xy_z2,self._interpolation,align_corners=False),final_sz)\n    return coords\n\nclass ResampledFieldGridWarperLayer(GridWarperLayer):\n  """""" The resampled field grid warper defines a grid based on\n      sampling coordinate values from a spatially varying displacement\n      field  (passed as a tensor input) along an affine grid pattern\n      in the field.\n      This enables grids representing small patches of a larger transform,\n      as well as the composition of multiple transforms before sampling.\n  """"""\n  def __init__(self,\n               source_shape,\n               output_shape,\n               coeff_shape,\n               field_transform=None,\n               resampler=None,\n               name=\'resampling_interpolated_spline_grid_warper\'):\n    """"""Constructs an ResampledFieldingGridWarperLayer.\n    Args:\n      source_shape: Iterable of integers determining the size of the source\n        signal domain.\n      output_shape: Iterable of integers determining the size of the destination\n        resampled signal domain.\n      coeff_shape: Shape of displacement field.\n      interpolation: type_str of interpolation as used by tf.image.resize_images\n      name: Name of Module.\n      field_transform: an object defining the spatial relationship between the\n        output_grid and the field.\n        batch_size x4x4 tensor: per-image transform matrix from output coords to field coords\n        None (default):         corners of output map to corners of field with an allowance for\n                                  interpolation (1 for bspline, 0 for linear)\n      resampler: a ResamplerLayer used to interpolate the\n        deformation field\n      name: Name of module.\n\n    Raises:\n      TypeError: If output_shape and source_shape are not both iterable.\n    """"""\n    if resampler==None:\n      self._resampler=ResamplerLayer(interpolation=\'LINEAR\',boundary=\'REPLICATE\')\n      self._interpolation = \'LINEAR\'\n    else:\n      self._resampler=resampler\n      self._interpolation = self._resampler.interpolation\n\n    self._field_transform = field_transform\n\n    super(ResampledFieldGridWarperLayer, self).__init__(source_shape=source_shape,\n                                           output_shape=output_shape,\n                                           coeff_shape=coeff_shape,\n                                           name=name)\n\n  def _create_features(self):\n    """"""Creates the coordinates for resampling. If field_transform is\n    None, these are constant and are created in field space; otherwise,\n    the final coordinates will be transformed by an input tensor\n    representing a transform from output coordinates to field\n    coordinates, so they are created are created in output coordinate\n    space\n    """"""\n    embedded_output_shape = list(self._output_shape)+[1]*(len(self._source_shape) - len(self._output_shape))\n    embedded_coeff_shape = list(self._coeff_shape)+[1]*(len(self._source_shape) - len(self._output_shape))\n    if self._field_transform==None and self._interpolation == \'BSPLINE\':\n      range_func= lambda f,x: tf.linspace(1.,f-2.,x)\n    elif self._field_transform==None and self._interpolation != \'BSPLINE\':\n      range_func= lambda f,x: tf.linspace(0.,f-1.,x)\n    else:\n      range_func= lambda f,x: np.arange(x,dtype=np.float32)\n      embedded_output_shape+=[1] # make homogeneous\n      embedded_coeff_shape+=[1]\n    ranges = [range_func(f,x) for f,x in zip(embedded_coeff_shape,embedded_output_shape)]\n    coords= tf.stack([tf.reshape(x,[1,-1]) for x in tf.meshgrid(*ranges, indexing=\'ij\')],2)\n    return coords\n\n  def layer_op(self, field):\n    """"""Assembles the module network and adds it to the graph.\n\n    The internal computation graph is assembled according to the set of\n    constraints provided at construction time.\n\n    Args:\n      field: Tensor containing a batch of transformation parameters.\n\n    Returns:\n      A batch of warped grids.\n\n    Raises:\n      Error: If the input tensor size is not consistent with the constraints\n        passed at construction time.\n    """"""\n    input_shape = tf.shape(field)\n    input_dtype = field.dtype.as_numpy_dtype\n    batch_size = int(field.shape[0])\n\n    # transform grid into field coordinate space if necessary\n    if self._field_transform==None:\n      coords=self._psi\n    else:\n      coords = tf.matmul(self._psi,self._field_transform[:,:,1:3])\n    # resample\n    coords = tf.reshape(tf.tile(coords,[batch_size,1,1]),[-1]+list(self._output_shape)+[len(self._source_shape)])\n    resampled_coords = self._resampler(field, coords)\n    return resampled_coords\n\n'"
niftynet/layer/squeeze_excitation.py,6,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.fully_connected import FullyConnectedLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_OP = set([\'AVG\', \'MAX\'])\n\n\nclass ChannelSELayer(Layer):\n    """"""\n    Re-implementation of Squeeze-and-Excitation (SE) block described in::\n\n         Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507\n    """"""\n    def __init__(self,\n                 func=\'AVG\',\n                 reduction_ratio=16,\n                 name=\'channel_squeeze_excitation\'):\n        self.func = func.upper()\n        self.reduction_ratio = reduction_ratio\n        super(ChannelSELayer, self).__init__(name=name)\n\n        look_up_operations(self.func, SUPPORTED_OP)\n\n    def layer_op(self, input_tensor):\n        # spatial squeeze\n        input_rank = len(input_tensor.shape)\n        reduce_indices = list(range(input_rank))[1:-1]\n        if self.func == \'AVG\':\n            squeeze_tensor = tf.reduce_mean(input_tensor, axis=reduce_indices)\n        elif self.func == \'MAX\':\n            squeeze_tensor = tf.reduce_max(input_tensor, axis=reduce_indices)\n        else:\n            raise NotImplementedError(""pooling function not supported"")\n\n        # channel excitation\n        num_channels = int(squeeze_tensor.shape[-1])\n        reduction_ratio = self.reduction_ratio\n        if num_channels % reduction_ratio != 0:\n            raise ValueError(\n                ""reduction ratio incompatible with ""\n                ""number of input tensor channels"")\n\n        num_channels_reduced = num_channels / reduction_ratio\n        fc1 = FullyConnectedLayer(num_channels_reduced,\n                                  with_bias=False,\n                                  feature_normalization=None,\n                                  acti_func=\'relu\',\n                                  name=\'se_fc_1\')\n        fc2 = FullyConnectedLayer(num_channels,\n                                  with_bias=False,\n                                  feature_normalization=None,\n                                  acti_func=\'sigmoid\',\n                                  name=\'se_fc_2\')\n\n        fc_out_1 = fc1(squeeze_tensor)\n        fc_out_2 = fc2(fc_out_1)\n\n        while len(fc_out_2.shape) < input_rank:\n            fc_out_2 = tf.expand_dims(fc_out_2, axis=1)\n\n        output_tensor = tf.multiply(input_tensor, fc_out_2)\n\n        return output_tensor\n\n\nclass SpatialSELayer(Layer):\n    """"""\n    Re-implementation of SE block -- squeezing spatially\n    and exciting channel-wise described in::\n\n        Roy et al., Concurrent Spatial and Channel Squeeze & Excitation\n        in Fully Convolutional Networks, arXiv:1803.02579\n\n    """"""\n    def __init__(self,\n                 name=\'spatial_squeeze_excitation\'):\n        super(SpatialSELayer, self).__init__(name=name)\n\n    def layer_op(self, input_tensor):\n        # channel squeeze\n        conv = ConvolutionalLayer(n_output_chns=1,\n                                  kernel_size=1,\n                                  feature_normalization=None,\n                                  acti_func=\'sigmoid\',\n                                  name=""se_conv"")\n\n        squeeze_tensor = conv(input_tensor)\n\n        # spatial excitation\n        output_tensor = tf.multiply(input_tensor, squeeze_tensor)\n\n        return output_tensor\n\n\nclass ChannelSpatialSELayer(Layer):\n    """"""\n    Re-implementation of concurrent spatial and channel\n    squeeze & excitation::\n\n        Roy et al., Concurrent Spatial and Channel Squeeze & Excitation\n        in Fully Convolutional Networks, arXiv:1803.02579\n\n    """"""\n    def __init__(self,\n                 func=\'AVG\',\n                 reduction_ratio=16,\n                 name=\'channel_spatial_squeeze_excitation\'):\n        self.func = func.upper()\n        self.reduction_ratio = reduction_ratio\n        super(ChannelSpatialSELayer, self).__init__(name=name)\n\n        look_up_operations(self.func, SUPPORTED_OP)\n\n    def layer_op(self, input_tensor):\n        cSE = ChannelSELayer(func=self.func,\n                             reduction_ratio=self.reduction_ratio,\n                             name=\'cSE\')\n        sSE = SpatialSELayer(name=\'sSE\')\n\n        output_tensor = tf.add(cSE(input_tensor), sSE(input_tensor))\n\n        return output_tensor\n'"
niftynet/layer/subpixel.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.downsample import DownSampleLayer\n\n\nclass SubPixelLayer(TrainableLayer):\n    """"""\n    Implementation of:\n\n    SubPixel Convolution initialised with ICNR initialisation\n    and followed by an AveragePooling\n\n    Limitations:\n\n    If ICNR initialization is used then the upsample factor\n    MUST be an integer\n\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Shi, W., Caballero, J., Husz\xc3\xa1r, F., Totz, J., Aitken, A.P.,\n    Bishop, R., Rueckert, D. and Wang, Z., 2016.\n\n    Real-time single image and video super-resolution using an\n    efficient sub-pixel convolutional neural network.\n\n    In Proceedings of the IEEE conference on computer vision\n    and pattern recognition (pp. 1874-1883).\n\n    https://www.cv-foundation.org/openaccess/content_cvpr_2016/\n    papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Aitken, A., Ledig, C., Theis, L., Caballero, J., Wang, Z.\n    and Shi, W., 2017.\n\n    Checkerboard artifact free sub-pixel convolution: A note on\n    sub-pixel convolution, resize convolution and convolution\n    resize.\n\n    arXiv preprint arXiv:1707.02937.\n\n    https://arxiv.org/pdf/1707.02937.pdf\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Sugawara, Y., Shiota, S. and Kiya, H., 2018, October.\n\n    Super-resolution using convolutional neural networks without\n    any checkerboard artifacts.\n\n    In 2018 25th IEEE International Conference on Image Processing\n    (ICIP) (pp. 66-70). IEEE.\n\n    https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451141\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    """"""\n\n    def __init__(\n        self,\n        upsample_factor=2,\n        n_output_chns=1,\n        kernel_size=3,\n        acti_func=""tanh"",\n        feature_normalization=None,\n        group_size=-1,\n        with_bias=True,\n        padding=""REFLECT"",\n        use_icnr=False,\n        use_avg=False,\n        w_initializer=None,\n        w_regularizer=None,\n        b_initializer=None,\n        b_regularizer=None,\n        name=""subpixel_cnn"",\n    ):\n        """"""\n        :param upsample_factor: zoom-factor/image magnification factor\n        :param n_output_chns: the desired ammount of channels for the\n        upsampled image\n        :param kernel_size: the size of the convolutional kernels\n        :param acti_func: activation function applied to first N - 1 layers\n        :param feature_normalization: the type of feature normalization (e.g.\n        batch, instance or group norm. Default None.\n        :param group_size: size of the groups if groupnorm is chosen.\n        :param with_bias: incorporate bias parameters in convolutional layers\n        :param padding: padding applied in convolutional layers\n        :param use_icnr: whether to use Aitken et al. initialization\n        :param use_avg: whether to use Sugawara et al. post-processing\n        """"""\n\n        super(SubPixelLayer, self).__init__(name=name)\n\n        if upsample_factor <= 0:\n            raise ValueError(""The upsampling factor must be strictly positive."")\n        if int(upsample_factor) != float(upsample_factor) and use_icnr:\n            raise ValueError(\n                ""If ICNR initialization is used the sample factor must be an integer""\n            )\n        if w_initializer is None and use_icnr:\n            raise ValueError(\n                ""If ICNR initialization is used the weights initializer must be specified""\n            )\n\n        self.upsample_factor = upsample_factor\n        self.kernel_size = kernel_size\n        self.acti_func = acti_func\n        self.use_avg = use_avg\n        self.n_output_chns = n_output_chns\n\n        self.conv_layer_params = {\n            ""with_bias"": with_bias,\n            ""feature_normalization"": feature_normalization,\n            ""group_size"": group_size,\n            ""padding"": padding,\n            ""w_initializer"": w_initializer\n            if not use_icnr\n            else _ICNR(\n                initializer=tf.keras.initializers.get(w_initializer),\n                upsample_factor=upsample_factor,\n            ),\n            ""b_initializer"": b_initializer,\n            ""w_regularizer"": w_regularizer,\n            ""b_regularizer"": b_regularizer,\n        }\n\n    def layer_op(self, lr_image, is_training=True, keep_prob=1.0):\n        input_shape = lr_image.get_shape().as_list()\n        batch_size = input_shape.pop(0)\n\n        if batch_size is None:\n            raise ValueError(""The batch size must be known and fixed."")\n        if any(i is None or i <= 0 for i in input_shape):\n            raise ValueError(""The image shape must be known in advance."")\n\n        # Making sure there are enough features channels for the\n        # periodic shuffling\n        features = ConvolutionalLayer(\n            n_output_chns=(\n                self.n_output_chns * self.upsample_factor ** (len(input_shape) - 1)\n            ),\n            kernel_size=self.kernel_size,\n            acti_func=None,\n            name=""subpixel_conv"",\n            **self.conv_layer_params\n        )(input_tensor=lr_image, is_training=is_training, keep_prob=keep_prob)\n\n        # Setting the number of output features to the known value\n        # obtained from the input shape results in a ValueError as\n        # of TF 1.12\n        sr_image = tf.contrib.periodic_resample.periodic_resample(\n            values=features,\n            shape=(\n                [batch_size]\n                + [self.upsample_factor * i for i in input_shape[:-1]]\n                + [None]\n            ),\n            name=""periodic_shuffle"",\n        )\n\n        # Averaging out the values without downsampling to counteract\n        # the periodicity of periodic shuffling as per Sugawara et al.\n        if self.use_avg:\n            sr_image = DownSampleLayer(\n                func=""AVG"",\n                kernel_size=self.upsample_factor,\n                stride=1,\n                padding=""SAME"",\n                name=""averaging"",\n            )(input_tensor=sr_image)\n\n        return sr_image\n\n\nclass _ICNR:\n    def __init__(self, initializer, upsample_factor=1):\n        """"""\n        :param initializer:  initializer used for sub kernels (orthogonal, glorot uniform, etc.)\n        :param upsample_factor: upsample factor of sub pixel convolution\n        """"""\n        self.upsample_factor = upsample_factor\n        self.initializer = initializer\n\n    def __call__(self, shape, dtype, partition_info=None):\n        shape = list(shape)\n        if self.upsample_factor == 1:\n            return self.initializer(shape)\n\n        # Initializing W0 (enough kernels for one output channel)\n        new_shape = shape[:-1] + [\n            shape[-1] // (self.upsample_factor ** (len(shape) - 2))\n        ]\n        x = self.initializer(new_shape, dtype, partition_info)\n\n        # Repeat the elements along the output dimension\n        x = tf.keras.backend.repeat_elements(\n            x=x, rep=self.upsample_factor ** (len(shape) - 2), axis=-1\n        )\n\n        return x\n'"
niftynet/layer/upsample.py,5,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.deconvolution import DeconvLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_OP = set([\'REPLICATE\', \'CHANNELWISE_DECONV\'])\n\n\nclass UpSampleLayer(TrainableLayer):\n    """"""\n    This class defines channel-wise upsampling operations.\n    Different from ``DeconvLayer``,\n    the elements are not mixed in the channel dim.\n\n    ``REPLICATE`` mode replicates each spatial_dim into\n    ``spatial_dim*kernel_size``\n    `CHANNELWISE_DECONV`` mode makes a projection using a kernel.\n    e.g., With 2D input (without loss of generality), given input\n    ``[N, X, Y, C]``, the output is ``[N, X*kernel_size, Y*kernel_size, C]``.\n    """"""\n\n    def __init__(self,\n                 func,\n                 kernel_size=3,\n                 stride=2,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 with_bias=False,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 name=\'upsample\'):\n        self.func = look_up_operations(func.upper(), SUPPORTED_OP)\n        self.layer_name = \'{}_{}\'.format(self.func.lower(), name)\n        super(UpSampleLayer, self).__init__(name=self.layer_name)\n\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.with_bias = with_bias\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, input_tensor):\n        spatial_rank = layer_util.infer_spatial_rank(input_tensor)\n        output_tensor = input_tensor\n        if self.func == \'REPLICATE\':\n            if self.kernel_size != self.stride:\n                raise ValueError(\n                    ""`kernel_size` != `stride` currently not""\n                    ""supported in `REPLICATE` mode. Please""\n                    ""consider using `CHANNELWISE_DECONV` operation."")\n            # simply replicate input values to\n            # local regions of (kernel_size ** spatial_rank) element\n            kernel_size_all_dims = layer_util.expand_spatial_params(\n                self.kernel_size, spatial_rank)\n            pixel_num = np.prod(kernel_size_all_dims)\n            repmat = np.hstack((pixel_num, [1] * spatial_rank, 1)).flatten()\n            output_tensor = tf.tile(input=input_tensor, multiples=repmat)\n            output_tensor = tf.batch_to_space_nd(\n                input=output_tensor,\n                block_shape=kernel_size_all_dims,\n                crops=[[0, 0]] * spatial_rank)\n\n        elif self.func == \'CHANNELWISE_DECONV\':\n            output_tensor = [tf.expand_dims(x, -1)\n                             for x in tf.unstack(input_tensor, axis=-1)]\n            output_tensor = [DeconvLayer(n_output_chns=1,\n                                         kernel_size=self.kernel_size,\n                                         stride=self.stride,\n                                         padding=\'SAME\',\n                                         with_bias=self.with_bias,\n                                         w_initializer=self.initializers[\'w\'],\n                                         w_regularizer=self.regularizers[\'w\'],\n                                         b_initializer=self.initializers[\'b\'],\n                                         b_regularizer=self.regularizers[\'b\'],\n                                         name=\'deconv_{}\'.format(i))(x)\n                             for (i, x) in enumerate(output_tensor)]\n            output_tensor = tf.concat(output_tensor, axis=-1)\n        return output_tensor\n'"
niftynet/layer/upsample_res_block.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer.additive_upsample import ResidualUpsampleLayer as ResUp\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer as Deconv\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.layer.residual_unit import ResidualUnit as ResUnit\n\n\nclass UpBlock(TrainableLayer):\n    def __init__(self,\n                 n_output_chns=4,\n                 kernel_size=3,\n                 upsample_stride=2,\n                 acti_func=\'relu\',\n                 w_initializer=None,\n                 w_regularizer=None,\n                 is_residual_upsampling=True,\n                 type_string=\'bn_acti_conv\',\n                 name=\'res-upsample\'):\n        super(TrainableLayer, self).__init__(name=name)\n        self.type_string = type_string\n        self.n_output_chns = n_output_chns\n        self.kernel_size = kernel_size\n        self.acti_func = acti_func\n        self.upsample_stride = upsample_stride\n        self.conv_param = {\'w_initializer\': w_initializer,\n                           \'w_regularizer\': w_regularizer}\n        self.is_residual_upsampling = is_residual_upsampling\n\n    def layer_op(self, inputs, forwarding=None, is_training=True):\n        """"""\n        Consists of::\n\n            (inputs)--upsampling-+-o--conv_1--conv_2--+--(conv_res)--\n                                 | |                  |\n            (forwarding)---------o o------------------o\n\n        where upsampling method could be ``DeconvolutionalLayer``\n        or ``ResidualUpsampleLayer``\n        """"""\n        if self.is_residual_upsampling:\n            n_input_channels = inputs.get_shape().as_list()[-1]\n            n_splits = float(n_input_channels) / float(self.n_output_chns)\n            upsampled = ResUp(kernel_size=self.kernel_size,\n                              stride=self.upsample_stride,\n                              n_splits=n_splits,\n                              acti_func=self.acti_func,\n                              **self.conv_param)(inputs, is_training)\n        else:\n            upsampled = Deconv(n_output_chns=self.n_output_chns,\n                               kernel_size=self.kernel_size,\n                               stride=self.upsample_stride,\n                               acti_func=self.acti_func,\n                               with_bias=False, feature_normalization=\'batch\',\n                               **self.conv_param)(inputs, is_training)\n\n        if forwarding is None:\n            conv_0 = upsampled\n        else:\n            conv_0 = ElementwiseLayer(\'SUM\')(upsampled, forwarding)\n\n        conv_res = ResUnit(n_output_chns=self.n_output_chns,\n                           kernel_size=self.kernel_size,\n                           acti_func=self.acti_func,\n                           type_string=self.type_string,\n                           **self.conv_param)(conv_0, is_training)\n        return conv_res\n'"
niftynet/network/__init__.py,0,"b'""""""\n\n.. module:: niftynet.network\n   :synopsis: Neural network (re-)implementations.\n\n""""""\n'"
niftynet/network/base_net.py,1,"b'from __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nfrom niftynet.layer.base_layer import TrainableLayer\n\n\nclass BaseNet(TrainableLayer):\n    """"""\n    Template for networks\n    """"""\n\n    def __init__(self,\n                 num_classes=0,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'prelu\',\n                 name=""net_template""):\n\n        super(BaseNet, self).__init__(name=name)\n\n        self.num_classes = num_classes\n        self.acti_func = acti_func\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n        tf.logging.info(\'using {}\'.format(name))\n'"
niftynet/network/deepmedic.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.crop import CropLayer\nfrom niftynet.layer.downsample import DownSampleLayer\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.layer.upsample import UpSampleLayer\nfrom niftynet.network.base_net import BaseNet\n\n\nclass DeepMedic(BaseNet):\n    """"""\n    ### Description\n    reimplementation of DeepMedic:\n        Kamnitsas et al., ""Efficient multi-scale 3D CNN with fully connected\n        CRF for accurate brain lesion segmentation"", MedIA \'17\n\n    ### Building blocks\n    [CONV]          - 3x3x3 convolutional layer\n    [denseCONV]     - 1x1x1 convolutional layer\n\n    ### Diagram\n    INPUT --> CROP -------> [CONV]x8 ------> [SUM] ----> [denseCONV]x3 --> OUTPUT\n                |                             |\n            DOWNSAMPLE ---> [CONV]x8 ---> UPSAMPLE\n\n\n    ### Constraints:\n    - The downsampling factor (d_factor) should be odd\n    - Label size = [(image_size / d_factor) - 16]* d_factor\n    - Image size should be divisible by d_factor\n\n    # Examples:\n    - Appropriate configuration for training:\n    image spatial window size = 57, label spatial window size = 9, d_ factor = 3\n    - Appropriate configuration for inference:\n    image spatial window size = 105, label spatial window size = 57, d_ factor = 3\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'prelu\',\n                 name=""DeepMedic""):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(DeepMedic, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.d_factor = 3  # downsampling factor - should be odd\n        self.crop_diff = ((self.d_factor - 1) * 16) // 2\n        self.conv_features = [30, 30, 40, 40, 40, 40, 50, 50]\n        self.fc_features = [150, 150, num_classes]\n\n    def layer_op(self, images, is_training, layer_id=-1, **unused_kwargs):\n        """"""\n\n        :param images: tensor, input to the network, size should be divisible by d_factor\n        :param is_training: boolean, True if network is in training mode\n        :param layer_id: not in use\n        :param unused_kwargs:\n        :return: tensor, network output\n        """"""\n        # image_size is defined as the largest context, then:\n        #   downsampled path size: image_size / d_factor\n        #   downsampled path output: image_size / d_factor - 16\n\n        # to make sure same size of feature maps from both pathways:\n        #   normal path size: (image_size / d_factor - 16) * d_factor + 16\n        #   normal path output: (image_size / d_factor - 16) * d_factor\n\n        # where 16 is fixed by the receptive field of conv layers\n        # TODO: make sure label_size = image_size/d_factor - 16\n\n        # image_size has to be an odd number and divisible by 3 and\n        # smaller than the smallest image size of the input volumes\n\n        # label_size should be (image_size/d_factor - 16) * d_factor\n\n        assert self.d_factor % 2 == 1  # to make the downsampling centered\n        assert (layer_util.check_spatial_dims(\n            images, lambda x: x % self.d_factor == 0))\n        assert (layer_util.check_spatial_dims(\n            images, lambda x: x % 2 == 1))  # to make the crop centered\n        assert (layer_util.check_spatial_dims(\n            images,\n            lambda x: x > self.d_factor * 16))  # required by receptive field\n\n        # crop 25x25x25 from 57x57x57\n        crop_op = CropLayer(border=self.crop_diff, name=\'cropping_input\')\n        normal_path = crop_op(images)\n        print(crop_op)\n\n        # downsample 19x19x19 from 57x57x57\n        downsample_op = DownSampleLayer(func=\'CONSTANT\',\n                                        kernel_size=self.d_factor,\n                                        stride=self.d_factor,\n                                        padding=\'VALID\',\n                                        name=\'downsample_input\')\n        downsample_path = downsample_op(images)\n        print(downsample_op)\n\n        # convolutions for both pathways\n        for n_features in self.conv_features:\n            # normal pathway convolutions\n            conv_path_1 = ConvolutionalLayer(\n                n_output_chns=n_features,\n                kernel_size=3,\n                padding=\'VALID\',\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                acti_func=self.acti_func,\n                name=\'normal_conv\')\n            normal_path = conv_path_1(normal_path, is_training)\n            print(conv_path_1)\n\n            # downsampled pathway convolutions\n            conv_path_2 = ConvolutionalLayer(\n                n_output_chns=n_features,\n                kernel_size=3,\n                padding=\'VALID\',\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                acti_func=self.acti_func,\n                name=\'downsample_conv\')\n            downsample_path = conv_path_2(downsample_path, is_training)\n            print(conv_path_2)\n\n        # upsampling the downsampled pathway\n        downsample_path = UpSampleLayer(\'REPLICATE\',\n                                        kernel_size=self.d_factor,\n                                        stride=self.d_factor)(downsample_path)\n\n        # concatenate both pathways\n        output_tensor = ElementwiseLayer(\'CONCAT\')(normal_path, downsample_path)\n\n        # 1x1x1 convolution layer\n        for n_features in self.fc_features:\n            conv_fc = ConvolutionalLayer(\n                n_output_chns=n_features,\n                kernel_size=1,\n                acti_func=self.acti_func,\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                name=\'conv_1x1x1_{}\'.format(n_features))\n            output_tensor = conv_fc(output_tensor, is_training)\n            print(conv_fc)\n\n        return output_tensor\n'"
niftynet/network/dense_vnet.py,28,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.io.misc_io import image3_axial\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.affine_augmentation import AffineAugmentationLayer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer\nfrom niftynet.layer.channel_sparse_convolution \\\n    import ChannelSparseConvolutionalLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer\nfrom niftynet.layer.downsample import DownSampleLayer\nfrom niftynet.network.base_net import BaseNet\n\nfrom collections import namedtuple\n\n\nclass DenseVNet(BaseNet):\n    """"""\n    ### Description\n    implementation of Dense-V-Net:\n       Gibson et al., ""Automatic multi-organ segmentation on abdominal CT with\n       dense V-networks"" 2018\n\n    ### Diagram\n\n    DFS = Dense Feature Stack Block\n\n    - Initial image is first downsampled to a given size.\n    - Each DFS+SD outputs a skip link + a downsampled output.\n    - All outputs are upscaled to the initial downsampled size.\n    - If initial prior is given add it to the output prediction.\n\n    Input\n      |\n      --[ DFS ]-----------------------[ Conv ]------------[ Conv ]------[+]-->\n           |                                       |  |              |\n           -----[ DFS ]---------------[ Conv ]------  |              |\n                   |                                  |              |\n                   -----[ DFS ]-------[ Conv ]---------              |\n                                                          [ Prior ]---\n\n    The layer DenseFeatureStackBlockWithSkipAndDownsample layer implements\n    [DFS + Conv + Downsampling] in a single module, and outputs 2 elements:\n        - Skip layer:          [ DFS + Conv]\n        - Downsampled output:  [ DFS + Down]\n\n    ### Constraints\n    - Input size has to be divisible by 2*dilation_rates\n\n    """"""\n\n    """""" Default network hyperparameters\n\n    Params:\n        prior_size (): size of spatial prior\n        n_dense_channels (): num dense channels in each block\n        n_seg_channels (): num of segmentation channels\n        n_initial_conv_channels (): num of channels in inital convolution\n        n_down_channels (): num of downsampling channels\n        dilation_rate (): dilation rate of each layer in each vblock\n        seg_kernel_size (): kernel size of final conv segmentation\n        augmentation_scale (): determines extent of the affine perturbation.\n            0.0 gives no perturbation and 1.0 gives the largest perturbation\n        use_bdo (): use batch-wise dropout\n        use_prior (): use spatial prior\n        use_dense_connections (): densely connect layers of each vblock\n        use_coords (): use image coordinate augmentation\n    """"""\n    __hyper_params__ = dict(\n        prior_size=24,\n        n_dense_channels=[4, 8, 16],\n        n_seg_channels=[12, 24, 24],\n        n_initial_conv_channels=24,\n        n_down_channels=[24, 24, None],\n        dilation_rates=[[1] * 5, [1] * 10, [1] * 10],\n        seg_kernel_size=3,\n        augmentation_scale=0.1,\n        use_bdo=False,\n        use_prior=False,\n        use_dense_connections=True,\n        use_coords=False)\n\n    def __init__(self,\n                 num_classes,\n                 hyperparams={},\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'DenseVNet\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param hyperparams: dictionary, network hyperparameters\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(DenseVNet, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        # Override default Hyperparameters\n        self.hyperparams = self.__hyper_params__\n        self.hyperparams.update(hyperparams)\n\n        # Check for dilation rates\n        if any([d != 1 for ds in self.hyperparams[\'dilation_rates\']\n                for d in ds]):\n            raise NotImplementedError(\n                \'Dilated convolutions are not yet implemented\')\n        # Check available modes\n        if self.hyperparams[\'use_dense_connections\'] is False:\n            raise NotImplementedError(\n                \'Non-dense connections are not yet implemented\')\n        if self.hyperparams[\'use_coords\'] is True:\n            raise NotImplementedError(\n                \'Image coordinate augmentation is not yet implemented\')\n\n    def create_network(self):\n\n        hyperparams = self.hyperparams\n\n        # Create initial convolutional layer\n        initial_conv = ConvolutionalLayer(\n            hyperparams[\'n_initial_conv_channels\'],\n            kernel_size=5,\n            stride=2)\n        # name=\'initial_conv\')\n\n        # Create dense vblocks\n        num_blocks = len(hyperparams[""n_dense_channels""])  # Num dense blocks\n        dense_ch = hyperparams[""n_dense_channels""]\n        seg_ch = hyperparams[""n_seg_channels""]\n        down_ch = hyperparams[""n_down_channels""]\n        dil_rate = hyperparams[""dilation_rates""]\n        use_bdo = hyperparams[\'use_bdo\']\n\n        dense_vblocks = []\n        for i in range(num_blocks):\n            vblock = DenseFeatureStackBlockWithSkipAndDownsample(\n                n_dense_channels=dense_ch[i],\n                kernel_size=3,\n                dilation_rates=dil_rate[i],\n                n_seg_channels=seg_ch[i],\n                n_down_channels=down_ch[i],\n                use_bdo=use_bdo,\n                acti_func=\'relu\')\n            dense_vblocks.append(vblock)\n\n        # Create final convolutional layer\n        final_conv = ConvolutionalLayer(\n            self.num_classes,\n            kernel_size=hyperparams[\'seg_kernel_size\'],\n            feature_normalization=None,\n            with_bias=True)\n        # \xc2\xa0name=\'final_conv\')\n\n        # Create a structure with all the fields of a DenseVNet\n        dense_vnet = namedtuple(\'DenseVNet\',\n                                [\'initial_conv\', \'dense_vblocks\', \'final_conv\'])\n\n        return dense_vnet(initial_conv=initial_conv,\n                          dense_vblocks=dense_vblocks,\n                          final_conv=final_conv)\n\n    def layer_op(self,\n                 input_tensor,\n                 is_training=True,\n                 layer_id=-1,\n                 keep_prob=0.5,\n                 **unused_kwargs):\n        """"""\n\n        :param input_tensor: tensor to input to the network, size has to be divisible by 2*dilation_rates\n        :param is_training: boolean, True if network is in training mode\n        :param layer_id: not in use\n        :param keep_prob: double, percentage of nodes to keep for drop-out\n        :param unused_kwargs:\n        :return: network prediction\n        """"""\n        hyperparams = self.hyperparams\n\n        # Validate that dilation rates are compatible with input dimensions\n        modulo = 2 ** (len(hyperparams[\'dilation_rates\']))\n        assert layer_util.check_spatial_dims(input_tensor,\n                                             lambda x: x % modulo == 0)\n\n        # Perform on the fly data augmentation\n        if is_training and hyperparams[\'augmentation_scale\'] > 0:\n            augment_layer = AffineAugmentationLayer(\n                hyperparams[\'augmentation_scale\'], \'LINEAR\', \'ZERO\')\n            input_tensor = augment_layer(input_tensor)\n\n        ###################\n        ### Feedforward ###\n        ###################\n\n        # Initialize network components\n        dense_vnet = self.create_network()\n\n        # Store output feature maps from each component\n        feature_maps = []\n\n        # Downsample input to the network\n        downsample_layer = DownSampleLayer(func=\'AVG\', kernel_size=3, stride=2)\n        downsampled_tensor = downsample_layer(input_tensor)\n        bn_layer = BNLayer()\n        downsampled_tensor = bn_layer(\n            downsampled_tensor, is_training=is_training)\n        feature_maps.append(downsampled_tensor)\n\n        # All feature maps should match the downsampled tensor\'s shape\n        feature_map_shape = downsampled_tensor.shape.as_list()[1:-1]\n\n        # Prepare initial input to dense_vblocks\n        initial_features = dense_vnet.initial_conv(\n            input_tensor, is_training=is_training)\n        channel_dim = len(input_tensor.shape) - 1\n        down = tf.concat([downsampled_tensor, initial_features], channel_dim)\n\n        # Feed downsampled input through dense_vblocks\n        for dblock in dense_vnet.dense_vblocks:\n            # Get skip layer and activation output\n            skip, down = dblock(down,\n                                is_training=is_training,\n                                keep_prob=keep_prob)\n            # Resize skip layer to original shape and add to feature maps\n            skip = LinearResizeLayer(feature_map_shape)(skip)\n            feature_maps.append(skip)\n\n        # Merge feature maps\n        all_features = tf.concat(feature_maps, channel_dim)\n\n        # Perform final convolution to segment structures\n        output = dense_vnet.final_conv(all_features, is_training=is_training)\n\n        ######################\n        ### Postprocessing ###\n        ######################\n\n        # Get the number of spatial dimensions of input tensor\n        n_spatial_dims = input_tensor.shape.ndims - 2\n\n        # Refine segmentation with prior\n        if hyperparams[\'use_prior\']:\n            spatial_prior_shape = [hyperparams[\'prior_size\']] * n_spatial_dims\n            # Prior shape must be 4 or 5 dim to work with linear_resize layer\n            # ie to conform to shape=[batch, X, Y, Z, channels]\n            prior_shape = [1] + spatial_prior_shape + [1]\n            spatial_prior = SpatialPriorBlock(prior_shape, feature_map_shape)\n            output += spatial_prior()\n\n        # Invert augmentation\n        if is_training and hyperparams[\'augmentation_scale\'] > 0:\n            inverse_aug = augment_layer.inverse()\n            output = inverse_aug(output)\n\n        # Resize output to original size\n        input_tensor_spatial_size = input_tensor.shape.as_list()[1:-1]\n        output = LinearResizeLayer(input_tensor_spatial_size)(output)\n\n        # Segmentation summary\n        seg_argmax = tf.to_float(tf.expand_dims(tf.argmax(output, -1), -1))\n        seg_summary = seg_argmax * (255. / self.num_classes - 1)\n\n        # Image Summary\n        norm_axes = list(range(1, n_spatial_dims + 1))\n        mean, var = tf.nn.moments(input_tensor, axes=norm_axes, keep_dims=True)\n        timg = tf.to_float(input_tensor - mean) / (tf.sqrt(var) * 2.)\n        timg = (timg + 1.) * 127.\n        single_channel = tf.reduce_mean(timg, -1, True)\n        img_summary = tf.minimum(255., tf.maximum(0., single_channel))\n\n        if n_spatial_dims == 2:\n            tf.summary.image(\n                tf.get_default_graph().unique_name(\'imgseg\'),\n                tf.concat([img_summary, seg_summary], 1),\n                5, [tf.GraphKeys.SUMMARIES])\n        elif n_spatial_dims == 3:\n            image3_axial(\n                tf.get_default_graph().unique_name(\'imgseg\'),\n                tf.concat([img_summary, seg_summary], 1),\n                5, [tf.GraphKeys.SUMMARIES])\n        else:\n            raise NotImplementedError(\n                \'Image Summary only supports 2D and 3D images\')\n\n        return output\n\n\nclass SpatialPriorBlock(TrainableLayer):\n    def __init__(self,\n                 prior_shape,\n                 output_shape,\n                 name=\'spatial_prior_block\'):\n\n        """"""\n\n        :param prior_shape: shape of spatial prior\n        :param output_shape: target shape for resampling\n        :param name: layer name\n        """"""\n\n        super(SpatialPriorBlock, self).__init__(name=name)\n\n        self.prior_shape = prior_shape\n        self.output_shape = output_shape\n\n    def layer_op(self):\n        """"""\n\n        :return: spatial prior resampled to the target shape\n        """"""\n        # The internal representation is probabilities so\n        # that resampling makes sense\n        prior = tf.get_variable(\'prior\',\n                                shape=self.prior_shape,\n                                initializer=tf.constant_initializer(1))\n        return tf.log(LinearResizeLayer(self.output_shape)(prior))\n\n\nclass DenseFeatureStackBlock(TrainableLayer):\n    """"""\n    Dense Feature Stack Block\n\n    - Stack is initialized with the input from above layers.\n    - Iteratively the output of convolution layers is added to the feature stack.\n    - Each sequential convolution is performed over all the previous stacked\n      channels.\n\n    Diagram example:\n\n        feature_stack = [Input]\n        feature_stack = [feature_stack, conv(feature_stack)]\n        feature_stack = [feature_stack, conv(feature_stack)]\n        feature_stack = [feature_stack, conv(feature_stack)]\n        ...\n        Output = [feature_stack, conv(feature_stack)]\n\n    """"""\n\n    def __init__(self,\n                 n_dense_channels,\n                 kernel_size,\n                 dilation_rates,\n                 use_bdo,\n                 name=\'dense_feature_stack_block\',\n                 **kwargs):\n        """"""\n\n        :param n_dense_channels: int, number of dense channels in each block\n        :param kernel_size: kernel size for convolutional layers\n        :param dilation_rates: dilation rate of each layer in each vblock\n        :param use_bdo: boolean, set to True to use batch-wise drop-out\n        :param name: tensorflow scope name\n        :param kwargs:\n        """"""\n\n        super(DenseFeatureStackBlock, self).__init__(name=name)\n\n        self.n_dense_channels = n_dense_channels\n        self.kernel_size = kernel_size\n        self.dilation_rates = dilation_rates\n        self.use_bdo = use_bdo\n        self.kwargs = kwargs\n\n    def create_block(self):\n        """"""\n\n        :return:  dense feature stack block\n        """"""\n        dfs_block = []\n        for _ in self.dilation_rates:\n            if self.use_bdo:\n                conv = ChannelSparseConvolutionalLayer(\n                    self.n_dense_channels,\n                    kernel_size=self.kernel_size,\n                    **self.kwargs)\n            else:\n                conv = ConvolutionalLayer(\n                    self.n_dense_channels,\n                    kernel_size=self.kernel_size,\n                    **self.kwargs)\n\n            dfs_block.append(conv)\n\n        return dfs_block\n\n    def layer_op(self, input_tensor, is_training=True, keep_prob=None):\n        """"""\n\n        :param input_tensor: tf tensor, input to the DenseFeatureStackBlock\n        :param is_training: boolean, True if network is in training mode\n        :param keep_prob: double, percentage of nodes to keep for drop-out\n        :return: feature stack\n        """"""\n        # Create dense feature stack block\n        dfs_block = self.create_block()\n        # Initialize feature stack for block\n        feature_stack = [input_tensor]\n\n        # Create initial input mask for batch-wise dropout\n        n_channels = input_tensor.shape.as_list()[-1]\n        input_mask = tf.ones([n_channels]) > 0\n\n        # Stack convolution outputs\n        for i, conv in enumerate(dfs_block):\n            # No dropout on last layer of the stack\n            if i == len(dfs_block) - 1:\n                keep_prob = None\n\n            # Merge feature stack along channel dimension\n            channel_dim = len(input_tensor.shape) - 1\n            input_features = tf.concat(feature_stack, channel_dim)\n\n            if self.use_bdo:\n                output_features, new_input_mask = conv(input_features,\n                                                       input_mask=input_mask,\n                                                       is_training=is_training,\n                                                       keep_prob=keep_prob)\n                input_mask = tf.concat([input_mask, new_input_mask], 0)\n            else:\n                output_features = conv(input_features,\n                                       is_training=is_training,\n                                       keep_prob=keep_prob)\n\n            feature_stack.append(output_features)\n\n        # Unmask the convolution channels\n        if self.use_bdo:\n            # Modify the returning feature stack by:\n            # 1. Removing the input of the DFS from the feature stack\n            # 2. Unmasking the feature stack by filling in zeros\n            # see: https://github.com/NifTK/NiftyNet/pull/101\n\n            # Remove input of DFS from the feature stack\n            conv_channels = tf.concat(feature_stack[1:], axis=-1)\n\n            # Insert a channel with zeros to be placed\n            # where channels were not calculated\n            zero_channel = tf.zeros(conv_channels.shape[:-1])\n            zero_channel = tf.expand_dims(zero_channel, axis=-1)\n            conv_channels = tf.concat([zero_channel, conv_channels], axis=-1)\n\n            # Indices to keep\n            int_mask = tf.cast(input_mask[n_channels:], tf.int32)\n            indices = tf.cumsum(int_mask) * int_mask\n\n            # Rearrange stack with zeros where channels were not calculated\n            conv_channels = tf.gather(conv_channels, indices, axis=-1)\n            feature_stack = [conv_channels]\n\n        return feature_stack\n\n\nclass DenseFeatureStackBlockWithSkipAndDownsample(TrainableLayer):\n    """"""\n    Dense Feature Stack with Skip Layer and Downsampling\n\n    - Downsampling is done through strided convolution.\n\n    ---[ DenseFeatureStackBlock ]----------[ Conv ]------- Skip layer\n                                      |\n                                      -------------------- Downsampled Output\n\n    See DenseFeatureStackBlock for more info.\n    """"""\n\n    def __init__(self,\n                 n_dense_channels,\n                 kernel_size,\n                 dilation_rates,\n                 n_seg_channels,\n                 n_down_channels,\n                 use_bdo,\n                 name=\'dense_feature_stack_block\',\n                 **kwargs):\n        """"""\n\n        :param n_dense_channels: int, number of dense channels\n        :param kernel_size: kernel size for convolutional layers\n        :param dilation_rates: dilation rate of each layer in each vblock\n        :param n_seg_channels: int, number of segmentation channels\n        :param n_down_channels: int, number of output channels when downsampling\n        :param use_bdo: boolean, set to True to use batch-wise drop-out\n        :param name: layer name\n        :param kwargs:\n        """"""\n\n        super(DenseFeatureStackBlockWithSkipAndDownsample, self).__init__(\n            name=name)\n\n        self.n_dense_channels = n_dense_channels\n        self.kernel_size = kernel_size\n        self.dilation_rates = dilation_rates\n        self.n_seg_channels = n_seg_channels\n        self.n_down_channels = n_down_channels\n        self.use_bdo = use_bdo\n        self.kwargs = kwargs\n\n    def create_block(self):\n        """"""\n\n        :return: Dense Feature Stack with Skip Layer and Downsampling block\n        """"""\n        dfs_block = DenseFeatureStackBlock(self.n_dense_channels,\n                                           self.kernel_size,\n                                           self.dilation_rates,\n                                           self.use_bdo,\n                                           **self.kwargs)\n\n        skip_conv = ConvolutionalLayer(self.n_seg_channels,\n                                       kernel_size=self.kernel_size,\n                                       # name=\'skip_conv\',\n                                       **self.kwargs)\n\n        down_conv = None\n        if self.n_down_channels is not None:\n            down_conv = ConvolutionalLayer(self.n_down_channels,\n                                           kernel_size=self.kernel_size,\n                                           stride=2,\n                                           # \xc2\xa0name=\'down_conv\',\n                                           **self.kwargs)\n\n        dfssd_block = namedtuple(\'DenseSDBlock\',\n                                 [\'dfs_block\', \'skip_conv\', \'down_conv\'])\n\n        return dfssd_block(dfs_block=dfs_block,\n                           skip_conv=skip_conv,\n                           down_conv=down_conv)\n\n    def layer_op(self, input_tensor, is_training=True, keep_prob=None):\n        """"""\n\n        :param input_tensor: tf tensor, input to the DenseFeatureStackBlock\n        :param is_training: boolean, True if network is in training mode\n        :param keep_prob: double, percentage of nodes to keep for drop-out\n        :return: feature stack after skip convolution, feature stack after downsampling\n        """"""\n        # Create dense feature stack block with skip and downsample\n        dfssd_block = self.create_block()\n\n        # Feed input through the dense feature stack block\n        feature_stack = dfssd_block.dfs_block(input_tensor,\n                                              is_training=is_training,\n                                              keep_prob=keep_prob)\n\n        # Merge feature stack\n        merged_features = tf.concat(feature_stack, len(input_tensor.shape) - 1)\n\n        # Perform skip convolution\n        skip_conv = dfssd_block.skip_conv(merged_features,\n                                          is_training=is_training,\n                                          keep_prob=keep_prob)\n\n        # Downsample if needed\n        down_conv = None\n        if dfssd_block.down_conv is not None:\n            down_conv = dfssd_block.down_conv(merged_features,\n                                              is_training=is_training,\n                                              keep_prob=keep_prob)\n\n        return skip_conv, down_conv\n'"
niftynet/network/highres3dnet.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom six.moves import range\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.activation import ActiLayer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer\nfrom niftynet.layer.convolution import ConvLayer, ConvolutionalLayer\nfrom niftynet.layer.dilatedcontext import DilatedTensor\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.network.base_net import BaseNet\n\n\nclass HighRes3DNet(BaseNet):\n    """"""\n    implementation of HighRes3DNet:\n      Li et al., ""On the compactness, efficiency, and representation of 3D\n      convolutional networks: Brain parcellation as a pretext task"", IPMI \'17\n\n      ### Building blocks\n\n      {   }       -  Residual connections: see He et al. ""Deep residual learning for\n                                                          image recognition"", in CVPR \'16\n\n      [CONV]      -  Convolutional layer in form: Activation(Convolution(X))\n                     where X = input tensor or output of previous layer\n\n                     and Activation is a function which includes:\n\n                        a) Batch-Norm\n                        b) Activation Function (ReLu, PreLu, Sigmoid, Tanh etc.)\n                        c) Drop-out layer by sampling random variables from a Bernouilli distribution\n                           if p < 1\n\n      [CONV*]      - Convolutional layer with no activation function\n\n      (r)[D-CONV(d)] - Convolutional layer with dilated convolutions with blocks in\n                     pre-activation mode: D-Convolution(Activation(X))\n                     see He et al., ""Identity Mappings in Deep Residual Networks"", ECCV \'16\n\n                     dilation factor = d\n                     D-CONV(2) : dilated convolution with dilation factor 2\n\n                     repeat factor = r\n                     e.g.\n                     (2)[D-CONV(d)]     : 2 dilated convolutional layers in a row [D-CONV] -> [D-CONV]\n                     { (2)[D-CONV(d)] } : 2 dilated convolutional layers within residual block\n\n    ### Diagram\n\n    INPUT --> [CONV] --> { (3)[D-CONV(1)] } --> { (3)[D-CONV(2)] } --> { (3)[D-CONV(4)] } -> [CONV*] -> Loss\n\n    ### Constraints\n    - Input image size should be divisible by 8\n\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'prelu\',\n                 name=\'HighRes3DNet\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(HighRes3DNet, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.layers = [\n            {\'name\': \'conv_0\', \'n_features\': 16, \'kernel_size\': 3},\n            {\'name\': \'res_1\', \'n_features\': 16, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'res_2\', \'n_features\': 32, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'res_3\', \'n_features\': 64, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'conv_1\', \'n_features\': 80, \'kernel_size\': 1},\n            {\'name\': \'conv_2\', \'n_features\': num_classes, \'kernel_size\': 1}]\n\n    def layer_op(self, images, is_training=True, layer_id=-1, **unused_kwargs):\n        """"""\n\n        :param images: tensor to input to the network. Size has to be divisible by 8\n        :param is_training:  boolean, True if network is in training mode\n        :param layer_id: int, index of the layer to return as output\n        :param unused_kwargs:\n        :return: output of layer indicated by layer_id\n        """"""\n        assert layer_util.check_spatial_dims(\n            images, lambda x: x % 8 == 0)\n        # go through self.layers, create an instance of each layer\n        # and plugin data\n        layer_instances = []\n\n        ### first convolution layer\n        params = self.layers[0]\n        first_conv_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = first_conv_layer(images, is_training)\n        layer_instances.append((first_conv_layer, flow))\n\n        ### resblocks, all kernels dilated by 1 (normal convolution)\n        params = self.layers[1]\n        with DilatedTensor(flow, dilation_factor=1) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### resblocks, all kernels dilated by 2\n        params = self.layers[2]\n        with DilatedTensor(flow, dilation_factor=2) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### resblocks, all kernels dilated by 4\n        params = self.layers[3]\n        with DilatedTensor(flow, dilation_factor=4) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### 1x1x1 convolution layer\n        params = self.layers[4]\n        fc_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_layer(flow, is_training)\n        layer_instances.append((fc_layer, flow))\n\n        ### 1x1x1 convolution layer\n        params = self.layers[5]\n        fc_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=None,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_layer(flow, is_training)\n        layer_instances.append((fc_layer, flow))\n\n        # set training properties\n        if is_training:\n            self._print(layer_instances)\n            return layer_instances[-1][1]\n        return layer_instances[layer_id][1]\n\n    def _print(self, list_of_layers):\n        for (op, _) in list_of_layers:\n            print(op)\n\n\nclass HighResBlock(TrainableLayer):\n    """"""\n    This class defines a high-resolution block with residual connections\n    kernels\n\n        - specify kernel sizes of each convolutional layer\n        - e.g.: kernels=(5, 5, 5) indicate three conv layers of kernel_size 5\n\n    with_res\n\n        - whether to add residual connections to bypass the conv layers\n    """"""\n\n    def __init__(self,\n                 n_output_chns,\n                 kernels=(3, 3),\n                 acti_func=\'relu\',\n                 w_initializer=None,\n                 w_regularizer=None,\n                 with_res=True,\n                 name=\'HighResBlock\'):\n        """"""\n\n        :param n_output_chns: int, number of output channels\n        :param kernels: list of layer kernel sizes\n        :param acti_func: activation function to use\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param with_res: boolean, set to True if residual connection are to use\n        :param name: layer name\n        """"""\n\n        super(HighResBlock, self).__init__(name=name)\n\n        self.n_output_chns = n_output_chns\n        if hasattr(kernels, ""__iter__""):  # a list of layer kernel_sizes\n            self.kernels = kernels\n        else:  # is a single number (indicating single layer)\n            self.kernels = [kernels]\n        self.acti_func = acti_func\n        self.with_res = with_res\n\n        self.initializers = {\'w\': w_initializer}\n        self.regularizers = {\'w\': w_regularizer}\n\n    def layer_op(self, input_tensor, is_training):\n        """"""\n\n        :param input_tensor: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :return: tensor, output of the residual block\n        """"""\n        output_tensor = input_tensor\n        for (i, k) in enumerate(self.kernels):\n            # create parameterised layers\n            bn_op = BNLayer(regularizer=self.regularizers[\'w\'],\n                            name=\'bn_{}\'.format(i))\n            acti_op = ActiLayer(func=self.acti_func,\n                                regularizer=self.regularizers[\'w\'],\n                                name=\'acti_{}\'.format(i))\n            conv_op = ConvLayer(n_output_chns=self.n_output_chns,\n                                kernel_size=k,\n                                stride=1,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                name=\'conv_{}\'.format(i))\n            # connect layers\n            output_tensor = bn_op(output_tensor, is_training)\n            output_tensor = acti_op(output_tensor)\n            output_tensor = conv_op(output_tensor)\n        # make residual connections\n        if self.with_res:\n            output_tensor = ElementwiseLayer(\'SUM\')(output_tensor, input_tensor)\n        return output_tensor\n'"
niftynet/network/highres3dnet_large.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom six.moves import range\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.dilatedcontext import DilatedTensor\nfrom niftynet.network.base_net import BaseNet\nfrom niftynet.network.highres3dnet import HighResBlock\n\n\nclass HighRes3DNetLarge(BaseNet):\n    """"""\n    implementation of HighRes3DNet:\n\n        Li et al., ""On the compactness, efficiency, and representation of 3D\n        convolutional networks: Brain parcellation as a pretext task"", IPMI \'17\n\n    (This is a larger version with an additional 3x3x3 convolution)\n\n    ### Constraints\n    - Input image size should be divisible by 8\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'HighRes3DNet\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(HighRes3DNetLarge, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.layers = [\n            {\'name\': \'conv_0\', \'n_features\': 16, \'kernel_size\': 3},\n            {\'name\': \'res_1\', \'n_features\': 16, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'res_2\', \'n_features\': 32, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'res_3\', \'n_features\': 64, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'conv_1\', \'n_features\': 64, \'kernel_size\': 3},\n            {\'name\': \'conv_2\', \'n_features\': 64, \'kernel_size\': 1},\n            {\'name\': \'conv_3\', \'n_features\': num_classes, \'kernel_size\': 1}]\n\n    def layer_op(self,\n                 images,\n                 is_training=True,\n                 layer_id=-1,\n                 keep_prob=0.5,\n                 **unused_kwargs):\n        """"""\n\n        :param images: tensor to input to the network. Size has to be divisible by 8\n        :param is_training: boolean, True if network is in training mode\n        :param layer_id: int, index of the layer to return as output\n        :param keep_prob: double, percentage of nodes to keep for drop-out\n        :param unused_kwargs:\n        :return: output of layer indicated by layer_id\n        """"""\n        assert (layer_util.check_spatial_dims(\n            images, lambda x: x % 8 == 0))\n        # go through self.layers, create an instance of each layer\n        # and plugin data\n        layer_instances = []\n\n        ### first convolution layer\n        params = self.layers[0]\n        first_conv_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = first_conv_layer(images, is_training)\n        layer_instances.append((first_conv_layer, flow))\n\n        ### resblocks, all kernels dilated by 1 (normal convolution)\n        params = self.layers[1]\n        with DilatedTensor(flow, dilation_factor=1) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### resblocks, all kernels dilated by 2\n        params = self.layers[2]\n        with DilatedTensor(flow, dilation_factor=2) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### resblocks, all kernels dilated by 4\n        params = self.layers[3]\n        with DilatedTensor(flow, dilation_factor=4) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### 3x3x3 convolution layer\n        params = self.layers[4]\n        fc_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_layer(flow, is_training)\n        layer_instances.append((fc_layer, flow))\n\n        ### 1x1x1 convolution layer\n        params = self.layers[5]\n        fc_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_layer(flow, is_training, keep_prob)\n        layer_instances.append((fc_layer, flow))\n\n        ### 1x1x1 convolution layer\n        params = self.layers[6]\n        fc_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=None,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_layer(flow, is_training)\n        layer_instances.append((fc_layer, flow))\n\n        # set training properties\n        if is_training:\n            self._print(layer_instances)\n            return layer_instances[-1][1]\n        return layer_instances[layer_id][1]\n\n    def _print(self, list_of_layers):\n        for (op, _) in list_of_layers:\n            print(op)\n'"
niftynet/network/highres3dnet_small.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom six.moves import range\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer\nfrom niftynet.layer.dilatedcontext import DilatedTensor\nfrom niftynet.network.base_net import BaseNet\nfrom niftynet.network.highres3dnet import HighResBlock\n\n\nclass HighRes3DNetSmall(BaseNet):\n    """"""\n    implementation of HighRes3DNet:\n\n        Li et al., ""On the compactness, efficiency, and representation of 3D\n        convolutional networks: Brain parcellation as a pretext task"", IPMI \'17\n\n    (This is smaller model with an initial stride-2 convolution)\n\n    ### Constraints\n    - Input image size should be divisible by 8\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'HighRes3DNetSmall\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(HighRes3DNetSmall, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.layers = [\n            {\'name\': \'conv_0\', \'n_features\': 16, \'kernel_size\': 3},\n            {\'name\': \'res_1\', \'n_features\': 16, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'res_2\', \'n_features\': 32, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'res_3\', \'n_features\': 64, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'conv_1\', \'n_features\': 80, \'kernel_size\': 3},\n            {\'name\': \'conv_2\', \'n_features\': num_classes, \'kernel_size\': 1}]\n\n    def layer_op(self, images, is_training=True, layer_id=-1, **unused_kwargs):\n        """"""\n\n        :param images: tensor to input to the network. Size has to be divisible by 8\n        :param is_training: boolean, True if network is in training mode\n        :param layer_id: int, index of the layer to return as output\n        :param unused_kwargs:\n        :return: output of layer indicated by layer_id\n        """"""\n        assert (layer_util.check_spatial_dims(\n            images, lambda x: x % 8 == 0))\n        # go through self.layers, create an instance of each layer\n        # and plugin data\n        layer_instances = []\n\n        ### first convolution layer\n        params = self.layers[0]\n        first_conv_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            stride=2,\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = first_conv_layer(images, is_training)\n        layer_instances.append((first_conv_layer, flow))\n\n        ### resblocks, all kernels dilated by 1 (normal convolution)\n        params = self.layers[1]\n        with DilatedTensor(flow, dilation_factor=1) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### resblocks, all kernels dilated by 2\n        params = self.layers[2]\n        with DilatedTensor(flow, dilation_factor=2) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### resblocks, all kernels dilated by 4\n        params = self.layers[3]\n        with DilatedTensor(flow, dilation_factor=4) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        ### 1x1x1 convolution layer\n        params = self.layers[4]\n        fc_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_layer(flow, is_training)\n        layer_instances.append((fc_layer, flow))\n\n        ### 3x3x3 deconvolution layer\n        params = self.layers[4]\n        fc_layer = DeconvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=3,\n            stride=2,\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=\'deconv\')\n        flow = fc_layer(flow, is_training)\n        layer_instances.append((fc_layer, flow))\n\n        ### 1x1x1 convolution layer\n        params = self.layers[5]\n        fc_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=None,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_layer(flow, is_training)\n        layer_instances.append((fc_layer, flow))\n\n        # set training properties\n        if is_training:\n            self._print(layer_instances)\n            return layer_instances[-1][1]\n        return layer_instances[layer_id][1]\n\n    def _print(self, list_of_layers):\n        for (op, _) in list_of_layers:\n            print(op)\n'"
niftynet/network/holistic_net.py,13,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.dilatedcontext import DilatedTensor\nfrom niftynet.layer.downsample import DownSampleLayer\nfrom niftynet.layer.upsample import UpSampleLayer\nfrom niftynet.network.base_net import BaseNet\nfrom niftynet.network.highres3dnet import HighResBlock\n\n\nclass HolisticNet(BaseNet):\n    """"""\n    ### Description\n    Implementation of HolisticNet detailed in\n    Fidon, L. et. al. (2017) Generalised Wasserstein Dice Score for Imbalanced\n    Multi-class Segmentation using Holistic Convolutional Networks.\n    MICCAI 2017 (BrainLes)\n\n    ### Diagram Blocks\n    [CONV]          -   3x3x3 Convolutional layer in form: Activation(Convolution(X))\n                        where X = input tensor or output of previous layer\n\n                        and Activation is a function which includes:\n\n                            a) Batch-Norm\n                            b) Activation Function (Elu, ReLu, PreLu, Sigmoid, Tanh etc.)\n\n    [D-CONV(d)]     -   3x3x3 Convolutional layer with dilated convolutions with blocks in\n                         pre-activation mode: D-Convolution(Activation(X))\n                         see He et al., ""Identity Mappings in Deep Residual Networks"", ECCV \'16\n\n                         dilation factor = d\n                         D-CONV(2) : dilated convolution with dilation factor 2\n\n                         repeat factor = r\n                         e.g.\n                         (2)[D-CONV(d)]     : 2 dilated convolutional layers in a row [D-CONV] -> [D-CONV]\n                         { (2)[D-CONV(d)] } : 2 dilated convolutional layers within residual block\n\n    [SCORE]         -   Batch-Norm + 3x3x3 Convolutional layer  + Activation function + 1x1x1 Convolutional layer\n\n    [MERGE]         -   Channel-wise merging\n\n\n    ### Diagram\n\n    MULTIMODAL INPUT ----- [CONV]x3 -----[D-CONV(2)]x3 ----- MaxPooling ----- [CONV]x3 -----[D-CONV(2)]x3\n                                        |                   |                          |                  |\n                                      [SCORE]             [SCORE]                    [SCORE]            [SCORE]\n                                        |                   |                          |                  |\n                                        -------------------------------------------------------------------\n                                                                        |\n                                                                     [MERGE] --> OUTPUT\n\n    ### Constraints\n    - Input image size should be divisible by 8\n\n    ### Comments\n    - The network returns only the merged output, so the loss will be applied only to this\n    (different from the referenced paper)\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'elu\',\n                 name=\'HolisticNet\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n        super(HolisticNet, self).__init__(\n            num_classes=num_classes,\n            acti_func=acti_func,\n            name=name,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer)\n\n        self.num_res_blocks = [3, 3, 3, 3]\n        self.num_features = [70] * 4\n        self.num_fea_score_layers = [[70, 140]] * 4\n\n        # self.loss = LossFunction(num_classes, loss_type=\'Dice\', decay=0.0)\n\n    def layer_op(self,\n                 input_tensor,\n                 is_training=True,\n                 layer_id=-1,\n                 **unused_kwargs):\n        """"""\n\n        :param input_tensor: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :param layer_id: not in use\n        :param unused_kwargs:\n        :return: fused prediction from multiple scales\n        """"""\n        layer_instances = []\n        scores_instances = []\n        first_conv_layer = ConvolutionalLayer(\n            n_output_chns=self.num_features[0],\n            feature_normalization=\'batch\',\n            kernel_size=3,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            acti_func=self.acti_func,\n            name=\'conv_1_1\')\n        flow = first_conv_layer(input_tensor, is_training)\n        layer_instances.append((first_conv_layer, flow))\n\n        # SCALE 1\n        with DilatedTensor(flow, dilation_factor=1) as dilated:\n            for j in range(self.num_res_blocks[0]):\n                res_block = HighResBlock(\n                    self.num_features[0],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (\'res_1\', j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        score_layer_scale1 = ScoreLayer(\n            num_features=self.num_fea_score_layers[0],\n            num_classes=self.num_classes)\n        score_1 = score_layer_scale1(flow, is_training)\n        scores_instances.append(score_1)\n        # if is_training:\n        #     loss_s1 = WGDL(score_1, labels)\n        #     tf.add_to_collection(\'multiscale_loss\', loss_s1/num_scales)\n\n        # # SCALE 2\n        with DilatedTensor(flow, dilation_factor=2) as dilated:\n            for j in range(self.num_res_blocks[1]):\n                res_block = HighResBlock(\n                    self.num_features[1],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (\'res_2\', j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        score_layer_scale2 = ScoreLayer(\n            num_features=self.num_fea_score_layers[1],\n            num_classes=self.num_classes)\n        score_2 = score_layer_scale2(flow, is_training)\n\n        # score_2 = self.score_layer(flow, self.num_fea_score_layers[1])\n        up_score_2 = score_2\n        scores_instances.append(up_score_2)\n        # if is_training:\n        #     loss_s2 =  self.WGDL(score_2, labels)\n        #     # loss_s2 = self.new_dice_loss(score_2, labels)\n        #     tf.add_to_collection(\'multiscale_loss\', loss_s2/num_scales)\n\n\n        # SCALE 3\n        ## dowsampling factor = 2\n        downsample_scale3 = DownSampleLayer(\n            func=\'AVG\', kernel_size=2, stride=2)\n        flow = downsample_scale3(flow)\n        layer_instances.append((downsample_scale3, flow))\n        with DilatedTensor(flow, dilation_factor=1) as dilated:\n            for j in range(self.num_res_blocks[2]):\n                res_block = HighResBlock(\n                    self.num_features[2],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (\'res_3\', j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        score_layer_scale3 = ScoreLayer(\n            num_features=self.num_fea_score_layers[2],\n            num_classes=self.num_classes)\n        score_3 = score_layer_scale3(flow, is_training)\n\n        upsample_indep_scale3 = UpSampleLayer(\n            func=\'CHANNELWISE_DECONV\',\n            kernel_size=2,\n            stride=2,\n            w_initializer=tf.constant_initializer(1.0, dtype=tf.float32))\n        up_score_3 = upsample_indep_scale3(score_3)\n        scores_instances.append(up_score_3)\n\n        # up_score_3 = self.feature_indep_upsample_conv(score_3, factor=2)\n        # if is_training:\n        #     loss_s3 = self.WGDL(up_score_3, labels)\n        #     # loss_s3 = self.new_dice_loss(up_score_3, labels)\n        #     tf.add_to_collection(\'multiscale_loss\', loss_s3/num_scales)\n\n        # SCALE 4\n        with DilatedTensor(flow, dilation_factor=2) as dilated:\n            for j in range(self.num_res_blocks[3]):\n                res_block = HighResBlock(\n                    self.num_features[3],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (\'res_4\', j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n\n        score_layer_scale4 = ScoreLayer(\n            num_features=self.num_fea_score_layers[3],\n            num_classes=self.num_classes)\n        score_4 = score_layer_scale4(\n            flow,\n            self.num_fea_score_layers[3],\n            is_training)\n\n        upsample_indep_scale4 = UpSampleLayer(\n            func=\'CHANNELWISE_DECONV\',\n            kernel_size=1,\n            stride=2,\n            w_initializer=tf.constant_initializer(1.0, dtype=tf.float32))\n        up_score_4 = upsample_indep_scale4(score_4)\n        scores_instances.append(up_score_4)\n\n        # if is_training:\n        #     loss_s4 = self.WGDL(up_score_4, labels)\n        #     # loss_s4 = self.new_dice_loss(up_score_4, labels)\n        #     tf.add_to_collection(\'multiscale_loss\', loss_s4/num_scales)\n\n        # FUSED SCALES\n        merge_layer = MergeLayer(\'WEIGHTED_AVERAGE\')\n        soft_scores = []\n        for s in scores_instances:\n            soft_scores.append(tf.nn.softmax(s))\n        fused_score = merge_layer(soft_scores)\n        scores_instances.append(fused_score)\n        if is_training:\n            return scores_instances\n        return fused_score\n\n\nclass ScoreLayer(TrainableLayer):\n    def __init__(self,\n                 num_features=None,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 num_classes=1,\n                 acti_func=\'elu\',\n                 name=\'ScoreLayer\'):\n        """"""\n\n        :param num_features: int, number of features\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param num_classes: int, number of prediction channels\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n        super(ScoreLayer, self).__init__(name=name)\n        self.num_classes = num_classes\n        self.acti_func = acti_func\n        self.num_features = num_features\n        self.n_layers = len(self.num_features)\n        self.initializers = {\'w\': w_initializer}\n        self.regularizers = {\'w\': w_regularizer}\n\n    def layer_op(self, input_tensor, is_training, layer_id=-1):\n        """"""\n\n        :param input_tensor: tensor, input to the layer\n        :param is_training: boolean, True if network is in training mode\n        :param layer_id: not is use\n        :return: tensor with number of channels to num_classes\n        """"""\n        rank = input_tensor.shape.ndims\n        perm = [i for i in range(rank)]\n        perm[-2], perm[-1] = perm[-1], perm[-2]\n        output_tensor = input_tensor\n        n_layers = self.n_layers\n        # All layers except the last one consists in:\n        # BN + Conv_3x3x3 + Activation\n        # layer_instances = []\n\n        for layer in range(n_layers - 1):\n            layer_to_add = ConvolutionalLayer(\n                n_output_chns=self.num_features[layer + 1],\n                feature_normalization=\'batch\',\n                kernel_size=3,\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                acti_func=self.acti_func,\n                name=\'conv_fc_%d\' % layer)\n            output_tensor = layer_to_add(output_tensor, is_training)\n            # layer_instances.append((layer_to_add, output_tensor))\n        last_layer = ConvolutionalLayer(n_output_chns=self.num_classes,\n                                        kernel_size=1)\n        output_tensor = last_layer(output_tensor, is_training)\n        # layer_instances.append((last_layer, output_tensor))\n        return output_tensor\n\n\nSUPPORTED_OPS = set([\'AVERAGE\', \'WEIGHTED_AVERAGE\', \'MAXOUT\'])\n\n\nclass MergeLayer(TrainableLayer):\n    def __init__(self,\n                 func,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 acti_func=\'elu\',\n                 name=\'MergeLayer\'):\n        """"""\n\n        :param func: type of merging layer (SUPPORTED_OPS: AVERAGE, WEIGHTED_AVERAGE, MAXOUT)\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n        super(MergeLayer, self).__init__(name=name)\n        self.func = func\n        self.acti_func = acti_func\n        self.initializers = {\'w\': w_initializer}\n        self.regularizers = {\'w\': w_regularizer}\n\n    def layer_op(self, roots):\n        """"""\n        Performs channel-wise merging of input tensors\n        :param roots: tensors to be merged\n        :return: fused tensor\n        """"""\n        if self.func == \'MAXOUT\':\n            return tf.reduce_max(tf.stack(roots, axis=-1), axis=-1)\n        elif self.func == \'AVERAGE\':\n            return tf.reduce_mean(tf.stack(roots, axis=-1), axis=-1)\n        elif self.func == \'WEIGHTED_AVERAGE\':\n            input_tensor = tf.stack(roots, axis=-1)\n            rank = input_tensor.shape.ndims\n            perm = [i for i in range(rank)]\n            perm[-2], perm[-1] = perm[-1], perm[-2]\n\n            output_tensor = input_tensor\n            output_tensor = tf.transpose(output_tensor, perm=perm)\n            output_tensor = tf.unstack(output_tensor, axis=-1)\n            roots_merged = []\n            for f in range(len(output_tensor)):\n                conv_layer = ConvLayer(\n                    n_output_chns=1, kernel_size=1, stride=1)\n                roots_merged_f = conv_layer(output_tensor[f])\n                roots_merged.append(roots_merged_f)\n            return tf.concat(roots_merged, axis=-1)\n'"
niftynet/network/interventional_affine_net.py,5,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.engine.application_initializer import GlorotUniform\nfrom niftynet.layer.convolution import ConvolutionalLayer as Conv\nfrom niftynet.layer.downsample_res_block import DownBlock as DownRes\nfrom niftynet.layer.fully_connected import FullyConnectedLayer as FC\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer as Grid\nfrom niftynet.layer.layer_util import infer_spatial_rank\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\nfrom niftynet.network.base_net import BaseNet\n\n\nclass INetAffine(BaseNet):\n    """"""\n    ### Description\n    This network estimates affine transformations from\n        a pair of moving and fixed image:\n\n            Hu et al., Label-driven weakly-supervised learning for\n            multimodal deformable image registration, arXiv:1711.01666\n            https://arxiv.org/abs/1711.01666\n\n            Hu et al., Weakly-Supervised Convolutional Neural Networks for\n            Multimodal Image Registration, Medical Image Analysis (2018)\n            https://doi.org/10.1016/j.media.2018.07.002\n\n        see also:\n            https://github.com/YipengHu/label-reg\n\n    ### Building blocks\n    [DOWN CONV]         - Convolutional layer + Residual Unit + Max pooling\n    [CONV]              - Convolutional layer\n    [FC]                - Fully connected layer, outputs the affine matrix\n    [WARPER]            - Grid resampling with the obtained affine matrix\n\n    ### Diagram\n\n    INPUT PAIR --> [DOWN CONV]x4 --> [CONV] --> [FC] --> [WARPER] --> DISPLACEMENT FIELD\n\n    ### Constraints\n    - input spatial rank should be either 2 or 3 (2D or 3D images only)\n\n    """"""\n    def __init__(self,\n                 decay=1e-6,\n                 affine_w_initializer=None,\n                 affine_b_initializer=None,\n                 acti_func=\'relu\',\n                 name=\'inet-affine\'):\n        """"""\n\n        :param decay: float, regularisation decay\n        :param affine_w_initializer: weight initialisation for affine registration network\n        :param affine_b_initializer: bias initialisation for affine registration network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        BaseNet.__init__(self, name=name)\n\n        self.fea = [4, 8, 16, 32, 64]\n        self.k_conv = 3\n        self.affine_w_initializer = affine_w_initializer\n        self.affine_b_initializer = affine_b_initializer\n        self.res_param = {\n            \'w_initializer\': GlorotUniform.get_instance(\'\'),\n            \'w_regularizer\': regularizers.l2_regularizer(decay),\n            \'acti_func\': acti_func}\n        self.affine_param = {\n            \'w_regularizer\': regularizers.l2_regularizer(decay),\n            \'b_regularizer\': None}\n\n    def layer_op(self,\n                 fixed_image,\n                 moving_image,\n                 is_training=True,\n                 **unused_kwargs):\n        """"""\n\n        :param fixed_image: tensor, fixed image for registration (defines reference space)\n        :param moving_image: tensor, moving image to be registered to fixed\n        :param is_training: boolean, True if network is in training mode\n        :return: displacement fields transformed by estimating affine\n        """"""\n\n        spatial_rank = infer_spatial_rank(moving_image)\n        spatial_shape = fixed_image.get_shape().as_list()[1:-1]\n\n        # resize the moving image to match the fixed\n        moving_image = Resize(spatial_shape)(moving_image)\n        img = tf.concat([moving_image, fixed_image], axis=-1)\n        res_1 = DownRes(self.fea[0], kernel_size=7, **self.res_param)(img, is_training)[0]\n        res_2 = DownRes(self.fea[1], **self.res_param)(res_1, is_training)[0]\n        res_3 = DownRes(self.fea[2], **self.res_param)(res_2, is_training)[0]\n        res_4 = DownRes(self.fea[3], **self.res_param)(res_3, is_training)[0]\n\n        conv_5 = Conv(n_output_chns=self.fea[4],\n                      kernel_size=self.k_conv,\n                      with_bias=False, feature_normalization=\'batch\',\n                      **self.res_param)(res_4, is_training)\n\n        if spatial_rank == 2:\n            affine_size = 6\n        elif spatial_rank == 3:\n            affine_size = 12\n        else:\n            tf.logging.fatal(\'Not supported spatial rank\')\n            raise NotImplementedError\n\n        if self.affine_w_initializer is None:\n            self.affine_w_initializer = init_affine_w()\n        if self.affine_b_initializer is None:\n            self.affine_b_initializer = init_affine_b(spatial_rank)\n        affine = FC(n_output_chns=affine_size, feature_normalization=None,\n                    w_initializer=self.affine_w_initializer,\n                    b_initializer=self.affine_b_initializer,\n                    **self.affine_param)(conv_5)\n        grid_global = Grid(source_shape=spatial_shape,\n                           output_shape=spatial_shape)(affine)\n        return grid_global\n\n\ndef init_affine_w(std=1e-8):\n    """"""\n\n    :param std: float, standard deviation of normal distribution for weight initialisation\n    :return: random weight initialisation from normal distribution with zero mean\n    """"""\n    return tf.random_normal_initializer(0, std)\n\n\ndef init_affine_b(spatial_rank, initial_bias=0.0):\n    """"""\n\n    :param spatial_rank: int, rank of inputs (either 2D or 3D)\n    :param initial_bias: float, initial bias\n    :return: bias initialisation for the affine matrix\n    """"""\n    if spatial_rank == 2:\n        identity = np.array([[1., 0., 0.],\n                             [0., 1., 0.]]).flatten()\n    elif spatial_rank == 3:\n        identity = np.array([[1, 0, 0, 0],\n                             [0, 1, 0, 0],\n                             [0, 0, 1, 0]]).flatten()\n    else:\n        tf.logging.fatal(\'Not supported spatial rank\')\n        raise NotImplementedError\n    identity = identity.reshape([1, -1])\n    identity = np.tile(identity, [1, 1])\n    return tf.constant_initializer(identity + initial_bias)\n'"
niftynet/network/interventional_dense_net.py,19,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import regularizers\n\nfrom niftynet.engine.application_initializer import GlorotUniform\nfrom niftynet.layer.convolution import ConvolutionalLayer as Conv\nfrom niftynet.layer.downsample_res_block import DownBlock as DownRes\nfrom niftynet.layer.grid_warper import _create_affine_features\nfrom niftynet.layer.layer_util import infer_spatial_rank, check_spatial_dims\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\nfrom niftynet.layer.spatial_gradient import SpatialGradientLayer as ImgGrad\nfrom niftynet.layer.upsample_res_block import UpBlock as UpRes\nfrom niftynet.network.base_net import BaseNet\n\n\nclass INetDense(BaseNet):\n    """"""\n     ### Description\n        The network estimates dense displacement fields from a pair\n        of moving and fixed images:\n\n            Hu et al., Label-driven weakly-supervised learning for\n            multimodal deformable image registration, arXiv:1711.01666\n            https://arxiv.org/abs/1711.01666\n\n            Hu et al., Weakly-Supervised Convolutional Neural Networks for\n            Multimodal Image Registration, Medical Image Analysis (2018)\n            https://doi.org/10.1016/j.media.2018.07.002\n\n        see also:\n            https://github.com/YipengHu/label-reg\n\n     ### Building blocks\n     [DOWN CONV]         - Convolutional layer + Residual Unit + Downsampling (Max pooling)\n     [CONV]              - Convolutional layer\n     [UP CONV]           - Upsampling + Sum + Residual Unit\n     [FUSION]            - Multi-scale displacement fields fusion\n     [DISPtoDEF]         - (Smoothing if required) Conversion to deformation field (adding base grid)\n\n\n     ### Diagram\n     INPUT PAIR -->  [DOWN CONV]              [UP CONV] --> [CONV] --[FUSION] --> [DISPtoDEF] --> DENSE FIELD\n                         |                        |                     |\n                     [DOWN CONV]              [UP CONV] --> [CONV] -----|\n                         |                        |                     |\n                     [DOWN CONV]              [UP CONV] --> [CONV] -----|\n                         |                        |                     |\n                     [DOWN CONV]              [UP CONV] --> [CONV] -----|\n                          |                       |                     |\n                          -------- [CONV]------------------ [CONV]-------\n\n\n     ### Constraints\n        - input spatial rank should be either 2 or 3 (2D or 3D images only)\n        - fixed image size should be divisible by 16\n    """"""\n    def __init__(self,\n                 decay=0.0,\n                 smoothing=0,\n                 disp_w_initializer=None,\n                 disp_b_initializer=None,\n                 acti_func=\'relu\',\n                 multi_scale_fusion=True,\n                 name=\'inet-dense\'):\n        """"""\n\n        :param decay: float, regularisation decay\n        :param smoothing: float, smoothing factor for dense displacement field\n        :param disp_w_initializer: initialisation of the displacement fields\n        :param disp_b_initializer: initialisation of the displacement fields\n        :param acti_func: activation function to use\n        :param multi_scale_fusion: True/False indicating whether to use\n            multiscale feature fusion.\n        :param name: layer name\n        """"""\n        BaseNet.__init__(self, name=name)\n\n        # self.fea = [40, 80, 160, 320, 640]\n        # self.fea = [32, 64, 128, 256, 512]\n        self.fea = [30, 60, 120, 240, 480]\n        # self.fea = [16, 32, 64, 128, 256]\n        self.k_conv = 3\n        self.multi_scale_fusion = multi_scale_fusion\n\n        self.down_res_param = {\n            \'w_initializer\': GlorotUniform.get_instance(\'\'),\n            \'w_regularizer\': regularizers.l2_regularizer(decay),\n            \'acti_func\': acti_func}\n\n        self.up_res_param = {\n            \'acti_func\': acti_func,\n            \'w_initializer\': GlorotUniform.get_instance(\'\'),\n            \'w_regularizer\': regularizers.l2_regularizer(decay),\n            \'is_residual_upsampling\': True,\n            \'type_string\': \'bn_acti_conv\'}\n\n        # displacement initialiser & regulariser\n        if disp_w_initializer is None:\n            disp_b_initializer = tf.constant_initializer(0.0)\n            #disp_w_initializer = tf.random_normal_initializer(0, 1e-4)\n        if disp_b_initializer is None:\n            disp_b_initializer = tf.constant_initializer(0.0)\n            #disp_w_initializer = tf.random_normal_initializer(0, 0.0)\n        self.disp_param = {\n            \'w_initializer\': disp_w_initializer,\n            \'w_regularizer\': regularizers.l2_regularizer(decay),\n            \'b_initializer\': disp_b_initializer,\n            \'b_regularizer\': None}\n\n        if smoothing > 0:\n            self.smoothing_func = _smoothing_func(smoothing)\n        else:\n            self.smoothing_func = None\n\n    def layer_op(self,\n                 fixed_image,\n                 moving_image,\n                 base_grid=None,\n                 is_training=True,\n                 **unused_kwargs):\n        """"""\n\n        :param fixed_image: tensor, fixed image for registration (defines reference space)\n        :param moving_image: tensor, moving image to be registered to fixed\n        :param base_grid: initial identity or affine displacement field\n        :param is_training: boolean, True if network is in training mode\n        :return: estimated dense displacement fields\n        """"""\n\n        spatial_rank = infer_spatial_rank(fixed_image)\n        spatial_shape = fixed_image.get_shape().as_list()[1:-1]\n        check_spatial_dims(fixed_image, lambda x: x % 16 == 0)\n\n        # \xc2\xa0resize the moving image to match the fixed\n        moving_image = Resize(spatial_shape)(moving_image)\n        img = tf.concat([moving_image, fixed_image], axis=-1)\n        down_res_0, conv_0_0, _ = \\\n            DownRes(self.fea[0], kernel_size=7, **self.down_res_param)(img, is_training)\n        down_res_1, conv_0_1, _ = \\\n            DownRes(self.fea[1], **self.down_res_param)(down_res_0, is_training)\n        down_res_2, conv_0_2, _ = \\\n            DownRes(self.fea[2], **self.down_res_param)(down_res_1, is_training)\n        down_res_3, conv_0_3, _ = \\\n            DownRes(self.fea[3], **self.down_res_param)(down_res_2, is_training)\n\n        conv_4 = Conv(n_output_chns=self.fea[4],\n                      kernel_size=self.k_conv,\n                      **self.down_res_param)(down_res_3, is_training)\n\n        up_res_0 = UpRes(self.fea[3], **self.up_res_param)(\n            conv_4, conv_0_3, is_training)\n        up_res_1 = UpRes(self.fea[2], **self.up_res_param)(\n            up_res_0, conv_0_2, is_training)\n        up_res_2 = UpRes(self.fea[1], **self.up_res_param)(\n            up_res_1, conv_0_1, is_training)\n        up_res_3 = UpRes(self.fea[0], **self.up_res_param)(\n            up_res_2, conv_0_0, is_training)\n\n        if self.multi_scale_fusion:\n            output_list = [up_res_3, up_res_2, up_res_1, up_res_0, conv_4]\n        else:\n            output_list = [up_res_3]\n\n        # converting all output layers to displacement fields\xc2\xa0\n        dense_fields = []\n        for scale_out in output_list:\n            field = Conv(n_output_chns=spatial_rank,\n                         kernel_size=self.k_conv,\n                         with_bias=True,\n                         feature_normalization=None,\n                         acti_func=None,\n                         **self.disp_param)(scale_out)\n            resized_field = Resize(new_size=spatial_shape)(field)\n            dense_fields.append(resized_field)\n\n        if base_grid is None:\n            # adding a reference grid if it doesn\'t exist\n            in_spatial_size = [None] * spatial_rank\n            base_grid = _create_affine_features(output_shape=spatial_shape,\n                                                source_shape=in_spatial_size)\n            base_grid = np.asarray(base_grid[:-1])\n            base_grid = np.reshape(\n                base_grid.T, [-1] + spatial_shape + [spatial_rank])\n            base_grid = tf.constant(base_grid, dtype=resized_field.dtype)\n\n        if self.multi_scale_fusion and len(dense_fields) > 1:\n            dense_field = tf.reduce_sum(dense_fields, axis=0)\n        else:\n            dense_field = dense_fields[0]\n\n        # TODO filtering\n        if self.smoothing_func is not None:\n            dense_field = self.smoothing_func(dense_field, spatial_rank)\n\n        tf.add_to_collection(\'bending_energy\',\n                             _computing_bending_energy(dense_field))\n        tf.add_to_collection(\'gradient_norm\',\n                             _computing_gradient_norm(dense_field))\n\n        dense_field = dense_field + base_grid\n        return dense_field\n\n\ndef _get_smoothing_kernel(sigma, spatial_rank):\n    """"""\n\n    :param sigma: float, standard deviation for gaussian smoothing kernel\n    :param spatial_rank: int, rank of input\n    :return: smoothing kernel\n    """"""\n    # sigma defined in voxel not in freeform deformation grid\n    if sigma <= 0:\n        raise NotImplementedError\n    tail = int(sigma * 2)\n    if spatial_rank == 2:\n        x, y = np.mgrid[-tail:tail + 1, -tail:tail + 1]\n        g = np.exp(-0.5 * (x * x + y * y) / sigma * sigma)\n    elif spatial_rank == 3:\n        x, y, z = np.mgrid[-tail:tail + 1, -tail:tail + 1, -tail:tail + 1]\n        g = np.exp(-0.5 * (x * x + y * y + z * z) / sigma * sigma)\n    else:\n        raise NotImplementedError\n    return g / g.sum()\n\n\ndef _smoothing_func(sigma):\n    def smoothing(dense_field, spatial_rank):\n        """"""\n\n        :param dense_field: tensor, dense field to be smoothed\n        :param spatial_rank: int, rank of input images\n        :return: smoothed dense field\n        """"""\n        kernel = _get_smoothing_kernel(sigma, spatial_rank)\n        kernel = tf.constant(kernel, dtype=dense_field.dtype)\n        kernel = tf.expand_dims(kernel, axis=-1)\n        kernel = tf.expand_dims(kernel, axis=-1)\n        smoothed = [\n            tf.nn.convolution(tf.expand_dims(coord, axis=-1), kernel, \'SAME\')\n            for coord in tf.unstack(dense_field, axis=-1)]\n        return tf.concat(smoothed, axis=-1)\n\n    return smoothing\n\n\ndef _computing_bending_energy(displacement):\n    """"""\n\n    :param displacement: tensor, displacement field\n    :return: bending energy\n    """"""\n    spatial_rank = infer_spatial_rank(displacement)\n    if spatial_rank == 2:\n        return _computing_bending_energy_2d(displacement)\n    if spatial_rank == 3:\n        return _computing_bending_energy_3d(displacement)\n    raise NotImplementedError(\n        ""Not implmented: bending energy for {}-d input"".format(spatial_rank))\n\n\ndef _computing_bending_energy_2d(displacement):\n    """"""\n\n    :param displacement: 2D tensor, displacement field\n    :return: bending energy\n    """"""\n    dTdx = ImgGrad(spatial_axis=0)(displacement)\n    dTdy = ImgGrad(spatial_axis=1)(displacement)\n\n    dTdxx = ImgGrad(spatial_axis=0)(dTdx)\n    dTdyy = ImgGrad(spatial_axis=1)(dTdy)\n    dTdxy = ImgGrad(spatial_axis=1)(dTdx)\n\n    energy = tf.reduce_mean([dTdxx * dTdxx, dTdyy * dTdyy, 2 * dTdxy * dTdxy])\n    return energy\n\n\ndef _computing_bending_energy_3d(displacement):\n    """"""\n\n    :param displacement: 3D tensor, displacement field\n    :return: bending energy\n    """"""\n    dTdx = ImgGrad(spatial_axis=0)(displacement)\n    dTdy = ImgGrad(spatial_axis=1)(displacement)\n    dTdz = ImgGrad(spatial_axis=2)(displacement)\n\n    dTdxx = ImgGrad(spatial_axis=0)(dTdx)\n    dTdyy = ImgGrad(spatial_axis=1)(dTdy)\n    dTdzz = ImgGrad(spatial_axis=2)(dTdz)\n\n    dTdxy = ImgGrad(spatial_axis=1)(dTdx)\n    dTdyz = ImgGrad(spatial_axis=2)(dTdy)\n    dTdxz = ImgGrad(spatial_axis=2)(dTdx)\n\n    energy = tf.reduce_mean(\n        [dTdxx * dTdxx, dTdyy * dTdyy, dTdzz * dTdzz,\n         2 * dTdxy * dTdxy, 2 * dTdxz * dTdxz, 2 * dTdyz * dTdyz])\n    return energy\n\n\ndef _computing_gradient_norm(displacement, flag_L1=False):\n    """"""\n\n    :param displacement: tensor, displacement field\n    :param flag_L1: boolean, True if L1 norm shoudl be used\n    :return: L2 (or L1) norm of gradients\n    """"""\n    norms = []\n    for spatial_ind in range(infer_spatial_rank(displacement)):\n        dTdt = ImgGrad(spatial_axis=spatial_ind)(displacement)\n        if flag_L1:\n            norms.append(tf.abs(dTdt))\n        else:\n            norms.append(dTdt * dTdt)\n    return tf.reduce_mean(norms)\n'"
niftynet/network/interventional_hybrid_net.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer.resampler import ResamplerLayer as resampler\nfrom niftynet.network.base_net import BaseNet\nfrom niftynet.network.interventional_affine_net import INetAffine\nfrom niftynet.network.interventional_dense_net import INetDense\n\n\nclass INetHybridPreWarp(BaseNet):\n    """"""\n    ### Description\n    Re-implementation of the registration network proposed in:\n\n            Hu et al., Label-driven weakly-supervised learning for\n            multimodal deformable image registration, arXiv:1711.01666\n            https://arxiv.org/abs/1711.01666\n\n            Hu et al., Weakly-Supervised Convolutional Neural Networks for\n            Multimodal Image Registration, Medical Image Analysis (2018)\n            https://doi.org/10.1016/j.media.2018.07.002\n\n        see also:\n            https://github.com/YipengHu/label-reg\n\n    ### Building blocks\n    [GLOBAL]            - INetAffine from interventional_affine_net.py\n    [RESAMPLER]         - Layer to resample the moving image with estimated affine\n    [DENSE]             - INetDense from intervetional_dense_net.py\n\n    ### Diagram\n\n    INPUT PAIR --> [GLOBAL]  --> [RESAMPLER] --> [DENSE] --> DENSE FIELD, AFFINE FIELD\n\n    ### Constraints\n        - input spatial rank should be either 2 or 3 (2D or 3D images only)\n        - fixed image size should be divisible by 16\n    """"""\n    def __init__(self,\n                 decay,\n                 affine_w_initializer=None,\n                 affine_b_initializer=None,\n                 disp_w_initializer=None,\n                 disp_b_initializer=None,\n                 acti_func=\'relu\',\n                 interp=\'linear\',\n                 boundary=\'replicate\',\n                 name=\'inet-hybrid-pre-warp\'):\n        """"""\n\n        :param decay: float, regularisation decay\n        :param affine_w_initializer: weight initialisation for affine registration network\n        :param affine_b_initializer: bias initialisation for affine registration network\n        :param disp_w_initializer: weight initialisation for dense registration network\n        :param disp_b_initializer: bias initialisation for dense registration network\n        :param acti_func: activation function to use\n        :param interp: string, type of interpolation for the resampling [default:linear]\n        :param boundary: string, padding mode to deal with image boundary\n        :param name: layer name\n        """"""\n        BaseNet.__init__(self, name=name)\n        self.global_net = INetAffine(decay=decay,\n                                     affine_w_initializer=affine_w_initializer,\n                                     affine_b_initializer=affine_b_initializer,\n                                     acti_func=acti_func,\n                                     name=\'inet-global\')\n        self.local_net = INetDense(decay=decay,\n                                   disp_w_initializer=disp_w_initializer,\n                                   disp_b_initializer=disp_b_initializer,\n                                   acti_func=acti_func,\n                                   name=\'inet-local\')\n        self.interp = interp\n        self.boundary = boundary\n\n    def layer_op(self,\n                 fixed_image,\n                 moving_image,\n                 is_training=True,\n                 **unused_kwargs):\n        """"""\n\n        :param fixed_image: tensor, fixed image for registration (defines reference space)\n        :param moving_image: tensor, moving image to be registered to fixed\n        :param is_training: boolean, True if network is in training mode\n        :param unused_kwargs: not in use\n        :return: estimated final dense and affine displacement fields\n        """"""\n        affine_field = self.global_net(fixed_image, moving_image, is_training)\n        moving_image = resampler(\n            interpolation=self.interp,\n            boundary=self.boundary)(moving_image, affine_field)\n        dense_field = self.local_net(\n            fixed_image, moving_image, affine_field, is_training)\n        return dense_field, affine_field\n\n\nclass INetHybridTwoStream(BaseNet):\n    """"""\n    ### Description\n    Re-implementation of the registration network proposed in:\n\n            Hu et al., Label-driven weakly-supervised learning for\n            multimodal deformable image registration, arXiv:1711.01666\n            https://arxiv.org/abs/1711.01666\n\n            Hu et al., Weakly-Supervised Convolutional Neural Networks for\n            Multimodal Image Registration, Medical Image Analysis (2018)\n            https://doi.org/10.1016/j.media.2018.07.002\n\n        see also:\n            https://github.com/YipengHu/label-reg\n\n    ### Building blocks\n    [GLOBAL]            - INetAffine from interventional_affine_net.py\n    [DENSE]             - INetDense from intervetional_dense_net.py\n\n    ### Diagram\n\n    INPUT PAIR --> [GLOBAL] --> AFFINE FIELD --- DENSE + AFFINE FIELD\n         |                                       |\n          -------> [DENSE] --> DENSE FIELD ------\n\n    ### Constraints\n        - input spatial rank should be either 2 or 3 (2D or 3D images only)\n        - fixed image size should be divisible by 16\n    """"""\n    def __init__(self,\n                 decay,\n                 affine_w_initializer=None,\n                 affine_b_initializer=None,\n                 disp_w_initializer=None,\n                 disp_b_initializer=None,\n                 acti_func=\'relu\',\n                 interp=\'linear\',\n                 boundary=\'replicate\',\n                 name=\'inet-hybrid-two-stream\'):\n        """"""\n\n        :param decay: float, regularisation decay\n        :param affine_w_initializer: weight initialisation for affine registration network\n        :param affine_b_initializer: bias initialisation for affine registration network\n        :param disp_w_initializer: weight initialisation for dense registration network\n        :param disp_b_initializer: bias initialisation for dense registration network\n        :param acti_func: activation function to use\n        :param interp: string, type of interpolation for the resampling [default:linear] - not in use\n        :param boundary: string, padding mode to deal with image boundary [default: replicate] - not is use\n        :param name: layer name\n        """"""\n        BaseNet.__init__(self, name=name)\n        self.global_net = INetAffine(decay=decay,\n                                     affine_w_initializer=affine_w_initializer,\n                                     affine_b_initializer=affine_b_initializer,\n                                     acti_func=acti_func,\n                                     name=\'inet-global\')\n        self.local_net = INetDense(decay=decay,\n                                   disp_w_initializer=disp_w_initializer,\n                                   disp_b_initializer=disp_b_initializer,\n                                   acti_func=acti_func,\n                                   name=\'inet-local\')\n        self.interp = interp\n        self.boundary = boundary\n\n    def layer_op(self,\n                 fixed_image,\n                 moving_image,\n                 is_training=True,\n                 **unused_kwargs):\n        """"""\n\n        :param fixed_image: tensor, fixed image for registration (defines reference space)\n        :param moving_image: tensor, moving image to be registered to fixed\n        :param is_training: boolean, True if network is in training mode\n        :param unused_kwargs: not in use\n        :return: estimated total, dense and affine displacement fields\n        """"""\n        affine_field = self.global_net(fixed_image, moving_image, is_training)\n        dense_field = self.local_net(fixed_image, moving_image, is_training)\n        return dense_field + affine_field, dense_field, affine_field\n'"
niftynet/network/no_new_net.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer\nfrom niftynet.layer.downsample import DownSampleLayer\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.layer.crop import CropLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\n\nclass UNet3D(TrainableLayer):\n    """"""\n    Implementation of No New-Net\n      Isensee et al., ""No New-Net"", MICCAI BrainLesion Workshop 2018.\n\n      The major changes between this and our standard 3d U-Net:\n      * input size == output size: padded convs are used\n      * leaky relu as non-linearity\n      * reduced number of filters before upsampling\n      * instance normalization (not batch)\n      * fits 128x128x128 with batch size of 2 on one TitanX GPU for\n      training\n      * no learned upsampling: linear resizing. \n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'leakyrelu\',\n                 name=\'NoNewNet\'):\n        super(UNet3D, self).__init__(name=name)\n\n        self.n_features = [30, 60, 120, 240, 480]\n        self.acti_func = acti_func\n        self.num_classes = num_classes\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n        print(\'using {}\'.format(name))\n\n    def layer_op(self, thru_tensor, is_training=True, **unused_kwargs):\n        """"""\n\n        :param thru_tensor: the input is modified in-place as it goes through the network\n        :param is_training:\n        :param unused_kwargs:\n        :return:\n        """"""\n        # image_size  should be divisible by 16 because of max-pooling 4 times, 2x2x2\n        assert layer_util.check_spatial_dims(thru_tensor, lambda x: x % 16 == 0)\n        block_layer = UNetBlock(\'DOWNSAMPLE\',\n                                (self.n_features[0], self.n_features[0]),\n                                (3, 3), with_downsample_branch=True,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'L1\')\n        thru_tensor, conv_1 = block_layer(thru_tensor, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'DOWNSAMPLE\',\n                                (self.n_features[1], self.n_features[1]),\n                                (3, 3), with_downsample_branch=True,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'L2\')\n        thru_tensor, conv_2 = block_layer(thru_tensor, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'DOWNSAMPLE\',\n                                (self.n_features[2], self.n_features[2]),\n                                (3, 3), with_downsample_branch=True,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'L3\')\n        thru_tensor, conv_3 = block_layer(thru_tensor, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'DOWNSAMPLE\',\n                                (self.n_features[3], self.n_features[3]),\n                                (3, 3), with_downsample_branch=True,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'L4\')\n        thru_tensor, conv_4 = block_layer(thru_tensor, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'UPSAMPLE\',\n                                (self.n_features[4], self.n_features[3]),\n                                (3, 3), with_downsample_branch=False,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'bottom\')\n        thru_tensor, _ = block_layer(thru_tensor, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'UPSAMPLE\',\n                                (self.n_features[3], self.n_features[2]),\n                                (3, 3), with_downsample_branch=False,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'R4\')\n        concat_4 = ElementwiseLayer(\'CONCAT\')(conv_4, thru_tensor)\n        thru_tensor, _ = block_layer(concat_4, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'UPSAMPLE\',\n                                (self.n_features[2], self.n_features[1]),\n                                (3, 3), with_downsample_branch=False,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'R3\')\n        concat_3 = ElementwiseLayer(\'CONCAT\')(conv_3, thru_tensor)\n        thru_tensor, _ = block_layer(concat_3, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'UPSAMPLE\',\n                                (self.n_features[1], self.n_features[0]),\n                                (3, 3), with_downsample_branch=False,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'R2\')\n        concat_2 = ElementwiseLayer(\'CONCAT\')(conv_2, thru_tensor)\n        thru_tensor, _ = block_layer(concat_2, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'NONE\',\n                                (self.n_features[0], self.n_features[0], self.num_classes),\n                                (3, 3, 1),\n                                with_downsample_branch=False,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'R1_FC\')\n\n        concat_1 = ElementwiseLayer(\'CONCAT\')(conv_1, thru_tensor)\n        thru_tensor, _ = block_layer(concat_1, is_training)\n        print(block_layer)\n\n        return thru_tensor\n\n\nSUPPORTED_OP = {\'DOWNSAMPLE\', \'UPSAMPLE\', \'NONE\'}\n\n\nclass UNetBlock(TrainableLayer):\n    def __init__(self,\n                 func,\n                 n_chns,\n                 kernels,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 with_downsample_branch=False,\n                 acti_func=\'leakyrelu\',\n                 name=\'UNet_block\'):\n\n        super(UNetBlock, self).__init__(name=name)\n\n        self.func = look_up_operations(func.upper(), SUPPORTED_OP)\n\n        self.kernels = kernels\n        self.n_chns = n_chns\n        self.with_downsample_branch = with_downsample_branch\n        self.acti_func = acti_func\n\n        self.initializers = {\'w\': w_initializer}\n        self.regularizers = {\'w\': w_regularizer}\n\n    def layer_op(self, thru_tensor, is_training):\n        for (kernel_size, n_features) in zip(self.kernels, self.n_chns):\n            # no activation before final 1x1x1 conv layer \n            acti_func = self.acti_func if kernel_size > 1 else None\n            feature_normalization = \'instance\' if acti_func is not None else None\n\n            conv_op = ConvolutionalLayer(n_output_chns=n_features,\n                                         kernel_size=kernel_size,\n                                         w_initializer=self.initializers[\'w\'],\n                                         w_regularizer=self.regularizers[\'w\'],\n                                         acti_func=acti_func,\n                                         name=\'{}\'.format(n_features),\n                                         feature_normalization=feature_normalization)\n            thru_tensor = conv_op(thru_tensor, is_training)\n\n        if self.with_downsample_branch:\n            branch_output = thru_tensor\n        else:\n            branch_output = None\n\n        if self.func == \'DOWNSAMPLE\':\n            downsample_op = DownSampleLayer(\'MAX\', kernel_size=2, stride=2, name=\'down_2x2\')\n            thru_tensor = downsample_op(thru_tensor)\n        elif self.func == \'UPSAMPLE\':\n            up_shape = [2 * int(thru_tensor.shape[i]) for i in (1, 2, 3)]\n            upsample_op = LinearResizeLayer(up_shape)\n            thru_tensor = upsample_op(thru_tensor)\n\n        elif self.func == \'NONE\':\n            pass  # do nothing\n        return thru_tensor, branch_output\n'"
niftynet/network/resnet.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport functools\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.fully_connected import FCLayer\nfrom niftynet.network.base_net import BaseNet\n\nResNetDesc = namedtuple(\'ResNetDesc\', [\'bn\', \'fc\', \'conv1\', \'blocks\'])\n\n\nclass ResNet(BaseNet):\n    """"""\n    ### Description\n        implementation of Res-Net:\n          He et al., ""Identity Mappings in Deep Residual Networks"", arXiv:1603.05027v3\n\n    ### Building Blocks\n    [CONV]          - Convolutional layer, no activation, no batch norm\n    (s)[DOWNRES]    - Downsample residual block.\n                        Each block is composed of a first bottleneck block with stride s,\n                        followed by n_blocks_per_resolution bottleneck blocks with stride 1.\n    [FC]            - Fully connected layer with nr output channels == num_classes\n\n    ### Diagram\n\n    INPUT --> [CONV] -->(s=1)[DOWNRES] --> (s=2)[DOWNRES]x2 --> BN, ReLU, mean --> [FC] --> OUTPUT\n\n    ### Constraints\n\n    """"""\n    def __init__(self,\n                 num_classes,\n                 n_features=[16, 64, 128, 256],\n                 n_blocks_per_resolution=10,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'ResNet\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param n_features: array, number of features per ResNet block\n        :param n_blocks_per_resolution: int, number of BottleneckBlock per DownResBlock\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: ctivation function to use\n        :param name: layer name\n        """"""\n\n        super(ResNet, self).__init__(num_classes=num_classes,\n                                     w_initializer=w_initializer,\n                                     w_regularizer=w_regularizer,\n                                     b_initializer=b_initializer,\n                                     b_regularizer=b_regularizer,\n                                     acti_func=acti_func,\n                                     name=name)\n\n        self.n_features = n_features\n        self.n_blocks_per_resolution = n_blocks_per_resolution\n        self.Conv = functools.partial(ConvolutionalLayer,\n                                      w_initializer=w_initializer,\n                                      w_regularizer=w_regularizer,\n                                      b_initializer=b_initializer,\n                                      b_regularizer=b_regularizer,\n                                      preactivation=True,\n                                      acti_func=acti_func)\n\n    def create(self):\n        """"""\n\n        :return: tuple with batch norm layer, fully connected layer, first conv layer and all residual blocks\n        """"""\n        bn = BNLayer()\n        fc = FCLayer(self.num_classes)\n        conv1 = self.Conv(self.n_features[0],\n                          acti_func=None,\n                          feature_normalization=None)\n        blocks = []\n        blocks += [\n            DownResBlock(self.n_features[1], self.n_blocks_per_resolution, 1,\n                         self.Conv)\n        ]\n        for n in self.n_features[2:]:\n            blocks += [\n                DownResBlock(n, self.n_blocks_per_resolution, 2, self.Conv)\n            ]\n        return ResNetDesc(bn=bn, fc=fc, conv1=conv1, blocks=blocks)\n\n    def layer_op(self, images, is_training=True, **unused_kwargs):\n        """"""\n\n        :param images: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :param unused_kwargs: not in use\n        :return: tensor, output of the final fully connected layer\n        """"""\n        layers = self.create()\n        out = layers.conv1(images, is_training)\n        for block in layers.blocks:\n            out = block(out, is_training)\n\n        spatial_rank = layer_util.infer_spatial_rank(out)\n        axis_to_avg = [dim + 1 for dim in range(spatial_rank)]\n        out = tf.reduce_mean(tf.nn.relu(layers.bn(out, is_training)),\n                             axis=axis_to_avg)\n        return layers.fc(out)\n\n\nBottleneckBlockDesc1 = namedtuple(\'BottleneckBlockDesc1\', [\'conv\'])\nBottleneckBlockDesc2 = namedtuple(\'BottleneckBlockDesc2\',\n                                  [\'common_bn\', \'conv\', \'conv_shortcut\'])\n\n\nclass BottleneckBlock(TrainableLayer):\n    def __init__(self, n_output_chns, stride, Conv, name=\'bottleneck\'):\n        """"""\n\n        :param n_output_chns: int, number of output channels\n        :param stride: int, stride to use in the convolutional layers\n        :param Conv: layer, convolutional layer\n        :param name: layer name\n        """"""\n        self.n_output_chns = n_output_chns\n        self.stride = stride\n        self.bottle_neck_chns = n_output_chns // 4\n        self.Conv = Conv\n        super(BottleneckBlock, self).__init__(name=name)\n\n    def create(self, input_chns):\n        """"""\n\n        :param input_chns: int, number of input channel\n        :return: tuple, with series of convolutional layers\n        """"""\n        if self.n_output_chns == input_chns:\n            b1 = self.Conv(self.bottle_neck_chns,\n                           kernel_size=1,\n                           stride=self.stride)\n            b2 = self.Conv(self.bottle_neck_chns, kernel_size=3)\n            b3 = self.Conv(self.n_output_chns, 1)\n            return BottleneckBlockDesc1(conv=[b1, b2, b3])\n        else:\n            b1 = BNLayer()\n            b2 = self.Conv(self.bottle_neck_chns,\n                           kernel_size=1,\n                           stride=self.stride,\n                           acti_func=None,\n                           feature_normalization=None)\n            b3 = self.Conv(self.bottle_neck_chns, kernel_size=3)\n            b4 = self.Conv(self.n_output_chns, kernel_size=1)\n            b5 = self.Conv(self.n_output_chns,\n                           kernel_size=1,\n                           stride=self.stride,\n                           acti_func=None,\n                           feature_normalization=None)\n            return BottleneckBlockDesc2(common_bn=b1,\n                                        conv=[b2, b3, b4],\n                                        conv_shortcut=b5)\n\n    def layer_op(self, images, is_training=True):\n        """"""\n\n        :param images: tensor, input to the BottleNeck block\n        :param is_training: boolean, True if network is in training mode\n        :return: tensor, output of the BottleNeck block\n        """"""\n        layers = self.create(images.shape[-1])\n        if self.n_output_chns == images.shape[-1]:\n            out = layers.conv[0](images, is_training)\n            out = layers.conv[1](out, is_training)\n            out = layers.conv[2](out, is_training)\n            out = out + images\n        else:\n            tmp = tf.nn.relu(layers.common_bn(images, is_training))\n            out = layers.conv[0](tmp, is_training)\n            out = layers.conv[1](out, is_training)\n            out = layers.conv[2](out, is_training)\n            out = layers.conv_shortcut(tmp, is_training) + out\n        print(out.shape)\n        return out\n\n\nDownResBlockDesc = namedtuple(\'DownResBlockDesc\', [\'blocks\'])\n\n\nclass DownResBlock(TrainableLayer):\n    def __init__(self, n_output_chns, count, stride, Conv, name=\'downres\'):\n        """"""\n\n        :param n_output_chns: int, number of output channels\n        :param count: int, number of BottleneckBlocks to generate\n        :param stride: int, stride for convolutional layer\n        :param Conv: Layer, convolutional layer\n        :param name: layer name\n        """"""\n        self.count = count\n        self.stride = stride\n        self.n_output_chns = n_output_chns\n        self.Conv = Conv\n        super(DownResBlock, self).__init__(name=name)\n\n    def create(self):\n        """"""\n\n        :return: tuple, containing all the Bottleneck blocks composing the DownRes block\n        """"""\n        blocks = []\n        blocks += [BottleneckBlock(self.n_output_chns, self.stride, self.Conv)]\n        for it in range(1, self.count):\n            blocks += [BottleneckBlock(self.n_output_chns, 1, self.Conv)]\n        return DownResBlockDesc(blocks=blocks)\n\n    def layer_op(self, images, is_training):\n        """"""\n\n        :param images: tensor, input to the DownRes block\n        :param is_training: is_training: boolean, True if network is in training mode\n        :return: tensor, output of the DownRes block\n        """"""\n        layers = self.create()\n        out = images\n        for l in layers.blocks:\n            out = l(out, is_training)\n        return out\n'"
niftynet/network/scalenet.py,10,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.network.base_net import BaseNet\nfrom niftynet.network.highres3dnet import HighRes3DNet, HighResBlock\nfrom niftynet.utilities.util_common import look_up_operations\n\n\nclass ScaleNet(BaseNet):\n    """"""\n    implementation of ScaleNet:\n        Fidon et al., ""Scalable multimodal convolutional\n        networks for brain tumour segmentation"", MICCAI \'17\n\n    ### Diagram\n\n    INPUT --> [BACKEND] ----> [MERGING] ----> [FRONTEND] ---> OUTPUT\n\n    [BACKEND] and [MERGING] are provided by the ScaleBlock below\n    [FRONTEND]: it can be any NiftyNet network (default: HighRes3dnet)\n\n    ### Constraints:\n    - Input image size should be divisible by 8\n    - more than one modality should be used\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'prelu\',\n                 name=\'ScaleNet\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(ScaleNet, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.n_features = 16\n\n    def layer_op(self, images, is_training=True, layer_id=-1, **unused_kwargs):\n        """"""\n\n        :param images: tensor, concatenation of multiple input modalities\n        :param is_training: boolean, True if network is in training mode\n        :param layer_id: not in use\n        :param unused_kwargs:\n        :return: predicted tensor\n        """"""\n        n_modality = images.shape.as_list()[-1]\n        rank = images.shape.ndims\n        assert n_modality > 1\n        roots = tf.split(images, n_modality, axis=rank - 1)\n        for (idx, root) in enumerate(roots):\n            conv_layer = ConvolutionalLayer(\n                n_output_chns=self.n_features,\n                kernel_size=3,\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                acti_func=self.acti_func,\n                name=\'conv_{}\'.format(idx))\n            roots[idx] = conv_layer(root, is_training)\n        roots = tf.stack(roots, axis=-1)\n\n        back_end = ScaleBlock(\'AVERAGE\', n_layers=1)\n        output_tensor = back_end(roots, is_training)\n\n        front_end = HighRes3DNet(self.num_classes)\n        output_tensor = front_end(output_tensor, is_training)\n        return output_tensor\n\n\nSUPPORTED_OP = set([\'MAX\', \'AVERAGE\'])\n\n\nclass ScaleBlock(TrainableLayer):\n    """"""\n    Implementation of the ScaleBlock described in\n    Fidon et al., ""Scalable multimodal convolutional\n        networks for brain tumour segmentation"", MICCAI \'17\n\n    See Fig 2(a) for diagram details - SN BackEnd\n\n    """"""\n    def __init__(self,\n                 func,\n                 n_layers=1,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'scaleblock\'):\n        """"""\n        :param func: merging function (SUPPORTED_OP: MAX, AVERAGE)\n        :param n_layers: int, number of layers\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n        self.func = look_up_operations(func.upper(), SUPPORTED_OP)\n        super(ScaleBlock, self).__init__(name=name)\n        self.n_layers = n_layers\n        self.acti_func = acti_func\n\n        self.initializers = {\'w\': w_initializer}\n        self.regularizers = {\'w\': w_regularizer}\n\n    def layer_op(self, input_tensor, is_training):\n        """"""\n\n        :param input_tensor: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :return: merged tensor after backend layers\n        """"""\n        n_modality = input_tensor.shape.as_list()[-1]\n        n_chns = input_tensor.shape.as_list()[-2]\n        rank = input_tensor.shape.ndims\n        perm = [i for i in range(rank)]\n        perm[-2], perm[-1] = perm[-1], perm[-2]\n\n        output_tensor = input_tensor\n        for layer in range(self.n_layers):\n            # modalities => feature channels\n            output_tensor = tf.transpose(output_tensor, perm=perm)\n            output_tensor = tf.unstack(output_tensor, axis=-1)\n            for (idx, tensor) in enumerate(output_tensor):\n                block_name = \'M_F_{}_{}\'.format(layer, idx)\n                highresblock_op = HighResBlock(\n                    n_output_chns=n_modality,\n                    kernels=(3, 1),\n                    with_res=True,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    acti_func=self.acti_func,\n                    name=block_name)\n                output_tensor[idx] = highresblock_op(tensor, is_training)\n                print(highresblock_op)\n            output_tensor = tf.stack(output_tensor, axis=-1)\n\n            # feature channels => modalities\n            output_tensor = tf.transpose(output_tensor, perm=perm)\n            output_tensor = tf.unstack(output_tensor, axis=-1)\n            for (idx, tensor) in enumerate(output_tensor):\n                block_name = \'F_M_{}_{}\'.format(layer, idx)\n                highresblock_op = HighResBlock(\n                    n_output_chns=n_chns,\n                    kernels=(3, 1),\n                    with_res=True,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    acti_func=self.acti_func,\n                    name=block_name)\n                output_tensor[idx] = highresblock_op(tensor, is_training)\n                print(highresblock_op)\n            output_tensor = tf.stack(output_tensor, axis=-1)\n\n        if self.func == \'MAX\':\n            output_tensor = tf.reduce_max(output_tensor, axis=-1)\n        elif self.func == \'AVERAGE\':\n            output_tensor = tf.reduce_mean(output_tensor, axis=-1)\n        return output_tensor\n'"
niftynet/network/se_resnet.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport functools\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.fully_connected import FCLayer\nfrom niftynet.layer.squeeze_excitation import ChannelSELayer\nfrom niftynet.network.base_net import BaseNet\n\nSE_ResNetDesc = namedtuple(\'SE_ResNetDesc\', [\'bn\', \'fc\', \'conv1\', \'blocks\'])\n\n\nclass SE_ResNet(BaseNet):\n    """"""\n    ### Description\n        implementation of Res-Net:\n          He et al., ""Identity Mappings in Deep Residual Networks"", arXiv:1603.05027v3\n\n    ### Building Blocks\n    [CONV]          - Convolutional layer, no activation, no batch norm\n    (s)[DOWNRES]    - Downsample residual block.\n                        Each block is composed of a first bottleneck block with stride s,\n                        followed by n_blocks_per_resolution bottleneck blocks with stride 1.\n                        Bottleneck blocks include a squeeze-and-excitation block\n    [FC]            - Fully connected layer with nr output channels == num_classes\n\n    ### Diagram\n\n    INPUT --> [CONV] -->(s=1)[DOWNRES] --> (s=2)[DOWNRES] --> BN, ReLU, mean --> [FC] --> OUTPUT\n\n    ### Constraints\n\n    """"""\n    def __init__(self,\n                 num_classes,\n                 n_features=[16, 64, 128],\n                 n_blocks_per_resolution=1,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'SE_ResNet\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param n_features: array, number of features per ResNet block\n        :param n_blocks_per_resolution: int, number of BottleneckBlock per DownResBlock\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: ctivation function to use\n        :param name: layer name\n        """"""\n\n        super(SE_ResNet, self).__init__(num_classes=num_classes,\n                                        w_initializer=w_initializer,\n                                        w_regularizer=w_regularizer,\n                                        b_initializer=b_initializer,\n                                        b_regularizer=b_regularizer,\n                                        acti_func=acti_func,\n                                        name=name)\n\n        self.n_features = n_features\n        self.n_blocks_per_resolution = n_blocks_per_resolution\n        self.Conv = functools.partial(ConvolutionalLayer,\n                                      w_initializer=w_initializer,\n                                      w_regularizer=w_regularizer,\n                                      b_initializer=b_initializer,\n                                      b_regularizer=b_regularizer,\n                                      preactivation=True,\n                                      acti_func=acti_func)\n\n    def create(self):\n        """"""\n\n        :return: tuple with batch norm layer, fully connected layer, first conv layer and all residual blocks\n        """"""\n        bn = BNLayer()\n        fc = FCLayer(self.num_classes)\n        conv1 = self.Conv(self.n_features[0],\n                          acti_func=None,\n                          feature_normalization=None)\n        blocks = []\n        blocks += [\n            DownResBlock(self.n_features[1], self.n_blocks_per_resolution, 1,\n                         self.Conv)\n        ]\n        for n in self.n_features[2:]:\n            blocks += [\n                DownResBlock(n, self.n_blocks_per_resolution, 2, self.Conv)\n            ]\n        return SE_ResNetDesc(bn=bn, fc=fc, conv1=conv1, blocks=blocks)\n\n    def layer_op(self, images, is_training=True, **unused_kwargs):\n        """"""\n\n        :param images: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :param unused_kwargs: not in use\n        :return: tensor, output of the final fully connected layer\n        """"""\n        layers = self.create()\n        out = layers.conv1(images, is_training)\n        for block in layers.blocks:\n            out = block(out, is_training)\n\n        spatial_rank = layer_util.infer_spatial_rank(out)\n        axis_to_avg = [dim + 1 for dim in range(spatial_rank)]\n        out = tf.reduce_mean(tf.nn.relu(layers.bn(out, is_training)),\n                             axis=axis_to_avg)\n        return layers.fc(out)\n\n\nBottleneckBlockDesc1 = namedtuple(\'BottleneckBlockDesc1\', [\'conv\'])\nBottleneckBlockDesc2 = namedtuple(\'BottleneckBlockDesc2\',\n                                  [\'common_bn\', \'conv\', \'conv_shortcut\'])\n\n\nclass BottleneckBlock(TrainableLayer):\n    def __init__(self, n_output_chns, stride, Conv, name=\'bottleneck\'):\n        """"""\n\n        :param n_output_chns: int, number of output channels\n        :param stride: int, stride to use in the convolutional layers\n        :param Conv: layer, convolutional layer\n        :param name: layer name\n        """"""\n        self.n_output_chns = n_output_chns\n        self.stride = stride\n        self.bottle_neck_chns = n_output_chns // 4\n        self.Conv = Conv\n        super(BottleneckBlock, self).__init__(name=name)\n\n    def create(self, input_chns):\n        """"""\n\n        :param input_chns: int, number of input channel\n        :return: tuple, with series of convolutional layers\n        """"""\n\n        if self.n_output_chns == input_chns:\n            b1 = self.Conv(self.bottle_neck_chns,\n                           kernel_size=1,\n                           stride=self.stride)\n            b2 = self.Conv(self.bottle_neck_chns, kernel_size=3)\n            b3 = self.Conv(self.n_output_chns, 1)\n            return BottleneckBlockDesc1(conv=[b1, b2, b3])\n        else:\n            b1 = BNLayer()\n            b2 = self.Conv(self.bottle_neck_chns,\n                           kernel_size=1,\n                           stride=self.stride,\n                           acti_func=None,\n                           feature_normalization=None)\n            b3 = self.Conv(self.bottle_neck_chns, kernel_size=3)\n            b4 = self.Conv(self.n_output_chns, kernel_size=1)\n            b5 = self.Conv(self.n_output_chns,\n                           kernel_size=1,\n                           stride=self.stride,\n                           acti_func=None,\n                           feature_normalization=None)\n            return BottleneckBlockDesc2(common_bn=b1,\n                                        conv=[b2, b3, b4],\n                                        conv_shortcut=b5)\n\n    def layer_op(self, images, is_training=True):\n        """"""\n\n        :param images: tensor, input to the BottleNeck block\n        :param is_training: boolean, True if network is in training mode\n        :return: tensor, output of the BottleNeck block\n        """"""\n        layers = self.create(images.shape[-1])\n        se = ChannelSELayer()\n        if self.n_output_chns == images.shape[-1]:\n            out = layers.conv[0](images, is_training)\n            out = layers.conv[1](out, is_training)\n            out = layers.conv[2](out, is_training)\n            out = se(out)\n            out = out + images\n        else:\n            tmp = tf.nn.relu(layers.common_bn(images, is_training))\n            out = layers.conv[0](tmp, is_training)\n            out = layers.conv[1](out, is_training)\n            out = layers.conv[2](out, is_training)\n            out = se(out)\n            out = layers.conv_shortcut(tmp, is_training) + out\n        print(out.shape)\n        return out\n\n\nDownResBlockDesc = namedtuple(\'DownResBlockDesc\', [\'blocks\'])\n\n\nclass DownResBlock(TrainableLayer):\n    def __init__(self, n_output_chns, count, stride, Conv, name=\'downres\'):\n        """"""\n\n        :param n_output_chns: int, number of output channels\n        :param count: int, number of BottleneckBlocks to generate\n        :param stride: int, stride for convolutional layer\n        :param Conv: Layer, convolutional layer\n        :param name: layer name\n        """"""\n        self.count = count\n        self.stride = stride\n        self.n_output_chns = n_output_chns\n        self.Conv = Conv\n        super(DownResBlock, self).__init__(name=name)\n\n    def create(self):\n        """"""\n\n        :return: tuple, containing all the Bottleneck blocks composing the DownRes block\n        """"""\n        blocks = []\n        blocks += [BottleneckBlock(self.n_output_chns, self.stride, self.Conv)]\n        for it in range(1, self.count):\n            blocks += [BottleneckBlock(self.n_output_chns, 1, self.Conv)]\n        return DownResBlockDesc(blocks=blocks)\n\n    def layer_op(self, images, is_training):\n        """"""\n\n        :param images: tensor, input to the DownRes block\n        :param is_training: is_training: boolean, True if network is in training mode\n        :return: tensor, output of the DownRes block\n        """"""\n        layers = self.create()\n        out = images\n        for l in layers.blocks:\n            out = l(out, is_training)\n        return out\n'"
niftynet/network/simple_gan.py,38,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom niftynet.layer.base_layer import TrainableLayer\n\n\nclass GenericGAN(TrainableLayer):\n  """"""\n    ### Description\n    Generic Generative Adversarial Network\n\n    ### Diagram\n\n    RANDOM NOISE --> [GENERATOR] --> [DISCRIMINATOR] --> fake logits\n    TRAINING SET ------------------> [DISCRIMINATOR] --> real logits\n\n    ### Constraints\n\n  """"""\n  def __init__(self, generator, discriminator, name=\'generic_GAN\'):\n    self._generator=generator\n    self._discriminator=discriminator\n    super(GenericGAN, self).__init__(name=name)\n  def layer_op(self, random_source, population,is_training):\n    image=self._generator(random_source,population.shape.as_list()[1:],is_training)\n    fake_logits = self._discriminator(image,is_training)\n    real_logits = self._discriminator(population/2,is_training)\n    diff=image-population/2\n    #logging.image3_axial(\'fake\',(image+1)*127,1,[logging.LOG])\n    #logging.image3_axial(\'real\',tf.maximum(0.,tf.minimum(255.,(population/2+1)*127)),1,[logging.LOG])\n    return image, real_logits, fake_logits, diff\n\nclass SimpleGAN(GenericGAN):\n  """"""\n    ### Description\n    Specification of generator and discriminator for generic gan\n\n    ### Building blocks\n    [GENERATOR]       - See ImageGenerator class below\n    [DISCRIMINATOR]   - See ImageDiscriminator class below\n\n    ### Diagram\n\n    RANDOM NOISE --> [GENERATOR] --> [DISCRIMINATOR] --> fake logits\n    TRAINING SET ------------------> [DISCRIMINATOR] --> real logits\n\n    ### Constraints\n\n  """"""\n  def __init__(self, name=\'simple_GAN\'):\n    generator=ImageGenerator(hidden_layer_channels=[128,64,32,16],\n                             name=\'generator\')\n    discriminator = ImageDiscriminator(hidden_layer_channels=[16,32,64,128],name=\'discriminator\')\n    super(SimpleGAN, self).__init__(generator, discriminator, name)\n\nclass ImageGenerator(TrainableLayer):\n  """"""\n    ### Description\n\n    ### Diagram\n\n    ### Constraints\n  """"""\n  def __init__(self, hidden_layer_channels, name):\n    """"""\n\n    :param hidden_layer_channels:\n    :param name: layer name\n    """"""\n    super(ImageGenerator, self).__init__(name=name)\n    self._num_layers = len(hidden_layer_channels)\n    self._layer_channels = hidden_layer_channels\n    self.initializers = {\'w\':tf.contrib.layers.variance_scaling_initializer(),\'b\':tf.constant_initializer(0)}\n\n  def layer_op(self, random_source, image_size,is_training):\n    """"""\n\n    :param random_source: tensor, random noise to start generation\n    :param image_size: output image size\n    :param is_training: boolean, True if network is in training mode\n    :return: tensor, generated image\n    """"""\n    spatial_rank = len(image_size)-1\n    batch_size = random_source.shape.as_list()[0]\n    noise_size=random_source.shape.as_list()[1]\n    intermediate_sizes = [[]]*(self._num_layers)+[image_size]\n    for it in range(self._num_layers,0,-1):\n      intermediate_sizes[it-1]= [int(round(i/2)) for i in intermediate_sizes[it][:-1]]+[self._layer_channels[it-1]]\n\n    # Define first kernel noise->image\n    noise_to_image_kernel = tf.get_variable(""G_fcW1"", shape=[1,np.prod(intermediate_sizes[0]),noise_size], initializer=self.initializers[\'w\'])\n    noise_to_image_bias = tf.get_variable(""G_fcb1"", shape=[1,np.prod(intermediate_sizes[0]),1], initializer=self.initializers[\'b\'])\n    image = tf.reshape(tf.matmul(tf.tile(noise_to_image_kernel,[batch_size,1,1]),tf.expand_dims(random_source,2))+noise_to_image_bias,[batch_size]+intermediate_sizes[0])\n    # define components of upsampling units\n    acti_func=tf.nn.relu\n    dropout_func = lambda x: tf.nn.dropout(x,.5)\n    #dropout_func = tf.identity\n    norm_func = tf.contrib.layers.batch_norm\n    upscale=[\'resize\',\'conv_transpose\'][0]\n    if spatial_rank==2:\n      conv_func=tf.nn.conv2d\n      if upscale==\'conv_transpose\':\n        conv_t_func=tf.nn.conv2d_transpose\n      else:\n        conv_t_func=lambda x, Wt, sz, st, p: conv_func(tf.image.resize_images(x,sz[1:3]),tf.transpose(Wt,[0,1,3,2]),[1]*4,p)\n    elif spatial_rank in [3]:\n      conv_func = tf.nn.conv3d\n      if upscale==\'conv_transpose\':\n        conv_t_func = tf.nn.conv3d_transpose\n      else:\n        def resize3(x,sz):\n          r1=tf.image.resize_images(tf.reshape(x,x.shape.as_list()[:3]+[-1]),sz[1:3])\n          r2=tf.image.resize_images(tf.reshape(r1,[sz[0],sz[1]*sz[2],x.shape.as_list()[3],-1]),[sz[1]*sz[2],sz[3]])\n          return tf.reshape(r2,sz[:4]+[x.shape.as_list()[-1]])\n        conv_t_func=lambda x, Wt, sz, st, p: conv_func(resize3(x,sz),tf.transpose(Wt,[0,1,2,4,3]),[1]*5,p)\n    conv_t_unit = lambda x,Wt,sz,norm_func: acti_func(norm_func(conv_t_func(x, Wt, [batch_size]+sz, [1]+[2]*spatial_rank+[1], ""SAME"")))\n    conv_unit = lambda x,W,sz,norm_func: acti_func(norm_func(conv_func(x, W, [1]+[1]*spatial_rank+[1], ""SAME"")))\n    upsample_unit = lambda x,Wt,W,sz: conv_unit(conv_t_unit(x,Wt,sz,norm_func),W,sz,norm_func)\n    last_upsample_unit = lambda x,Wt,W,sz: conv_func(conv_t_unit(x,Wt,sz,norm_func),W,[1]+[1]*spatial_rank+[1], ""SAME"")\n    kernel_size=[3]*spatial_rank\n\n    for it in range(self._num_layers):\n      Wt=tf.get_variable(\'G_Wt{}\'.format(it),shape=kernel_size+[intermediate_sizes[it+1][-1],intermediate_sizes[it][-1]],initializer=self.initializers[\'w\'])\n      W =tf.get_variable( \'G_W{}\'.format(it),shape=kernel_size+[intermediate_sizes[it+1][-1],intermediate_sizes[it+1][-1]],initializer=self.initializers[\'w\'])\n      if it<self._num_layers-1:\n        image=upsample_unit(image,Wt,W,intermediate_sizes[it+1])\n      else:\n        image=last_upsample_unit(image,Wt,W,intermediate_sizes[it+1]) # NB. no batch_norm for true mean and scale\n    channel_scale = tf.get_variable(\'G_scale\',shape=[1]*(spatial_rank+1)+[intermediate_sizes[-1][-1]],initializer=tf.constant_initializer(.1))\n    #channel_shift = tf.get_variable(\'G_shift\',shape=[1]*(spatial_rank+1)+[intermediate_sizes[-1][-1]],initializer=tf.constant_initializer(0.))\n    channel_shift = tf.get_variable(\'G_shift\',shape=[1]+intermediate_sizes[-1],initializer=tf.constant_initializer(0.))\n\n    image = image*channel_scale+channel_shift\n    return tf.nn.tanh(image)\n\nclass ImageDiscriminator(TrainableLayer):\n  """"""\n    ### Description\n\n    ### Diagram\n\n    ### Constraints\n\n  """"""\n  def __init__(self, hidden_layer_channels,name):\n    """"""\n\n    :param hidden_layer_channels: array, number of output channels for each layer\n    :param name: layer name\n    """"""\n    super(ImageDiscriminator, self).__init__(name=name)\n    self._layer_channels = hidden_layer_channels\n    self.initializers = {\'w\':tf.contrib.layers.variance_scaling_initializer(),\'b\':tf.constant_initializer(0)}\n\n  def layer_op(self, image,is_training):\n    """"""\n\n    :param image: tensor, input image to distriminator\n    :param is_training: boolean, True if network is in training mode\n    :return: tensor, classification logits\n    """"""\n    batch_size=image.shape.as_list()[0]\n    spatial_rank=len(image.shape)-2\n    image_channels = image.shape.as_list()[-1]\n    acti_func=tf.nn.relu\n    dropout_func = lambda x: tf.nn.dropout(x,.5)\n    norm_func = tf.contrib.layers.batch_norm\n    downscale=[\'resize\',\'conv_stride\'][0]\n    if spatial_rank==2:\n      conv_func=tf.nn.conv2d\n      if downscale==\'conv_stride\':\n        conv_s_unit = lambda x,W: acti_func(norm_func(conv_func(x, W, [1]+[2]*spatial_rank+[1], ""SAME"")))\n      else:\n        conv_s_unit = lambda x,W: acti_func(norm_func(conv_func(tf.image.resize_images(x,[x.shape.as_list()[1]//2,x.shape.as_list()[2]//2]), W, [1]+[1]*spatial_rank+[1], ""SAME"")))\n    elif spatial_rank in [3]:\n      conv_func = tf.nn.conv3d\n      if downscale==\'conv_stride\':\n        conv_s_unit = lambda x,W: acti_func(norm_func(conv_func(x, W, [1]+[2]*spatial_rank+[1], ""SAME"")))\n      else:\n        def resize3(x,sz):\n          r1=tf.image.resize_images(tf.reshape(x,x.shape.as_list()[:3]+[-1]),sz[1:3])\n          r2=tf.image.resize_images(tf.reshape(r1,[sz[0],sz[1]*sz[2],x.shape.as_list()[3],-1]),[sz[1]*sz[2],sz[3]])\n          return tf.reshape(r2,sz[:4]+[x.shape.as_list()[-1]])\n        conv_s_unit = lambda x,W: acti_func(norm_func(conv_func(resize3(x,[a//b for a,b in zip(x.shape.as_list(),[1,2,2,2,1])]), W, [1]+[1]*spatial_rank+[1], ""SAME"")))\n    conv_unit = lambda x,W: acti_func(norm_func(conv_func(x, W, [1]+[1]*spatial_rank+[1], ""SAME"")))\n    down_sample_unit = lambda x,Ws,W: conv_s_unit(conv_unit(x,W),Ws)\n    kernel_size=[3]*spatial_rank\n    for it in range(len(self._layer_channels)-1):\n      if it==0:\n        W =tf.get_variable( \'D_W{}\'.format(it),shape=kernel_size+[image_channels,self._layer_channels[it]],initializer=self.initializers[\'w\'])\n      else:\n        W =tf.get_variable( \'D_W{}\'.format(it),shape=kernel_size+[self._layer_channels[it],self._layer_channels[it]],initializer=self.initializers[\'w\'])\n      Ws=tf.get_variable(\'D_Ws{}\'.format(it),shape=kernel_size+[self._layer_channels[it],self._layer_channels[it+1]],initializer=self.initializers[\'w\'])\n      image=down_sample_unit(image,Ws,W)\n    logits = tf.layers.dense(tf.reshape(image,[batch_size,-1]),1,activation=None,use_bias=True)\n    return logits\n\n'"
niftynet/network/simulator_gan.py,29,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.activation import ActiLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer\nfrom niftynet.layer.fully_connected import FullyConnectedLayer\nfrom niftynet.layer.gan_blocks import BaseDiscriminator\nfrom niftynet.layer.gan_blocks import BaseGenerator\nfrom niftynet.layer.gan_blocks import GANImageBlock\n\n\nclass SimulatorGAN(GANImageBlock):\n    """"""\n    ### Description\n        implementation of\n            Hu et al., ""Freehand Ultrasound Image Simulation with Spatially-Conditioned\n            Generative Adversarial Networks"", MICCAI RAMBO 2017\n            https://arxiv.org/abs/1707.05392\n\n    ### Building blocks\n    [GENERATOR]         - See ImageGenerator below\n    [DISCRIMINATOR]     - See ImageDiscriminator below\n    Note: See niftynet.layer.gan_blocks for layer_op\n\n    ### Diagram\n\n    RANDOM NOISE --> [GENERATOR] --> [DISCRIMINATOR] --> fake logits\n    TRAINING SET ------------------> [DISCRIMINATOR] --> real logits\n\n    ### Constraints\n    """"""\n\n    def __init__(self, name=\'simulator_GAN\'):\n        super(SimulatorGAN, self).__init__(\n            generator=ImageGenerator(name=\'generator\'),\n            discriminator=ImageDiscriminator(name=\'discriminator\'),\n            clip=None,\n            name=name)\n\n\nclass ImageGenerator(BaseGenerator):\n    """"""\n    ### Description\n        implementation of generator from\n            Hu et al., ""Freehand Ultrasound Image Simulation with Spatially-Conditioned\n            Generative Adversarial Networks"", MICCAI RAMBO 2017\n            https://arxiv.org/abs/1707.05392\n\n    ### Building blocks\n    [FC]            - Fully connected layer\n    [CONV]          - Convolutional layer, conditioning is concatenated to input before the convolution\n                        (if conditioning is used)\n                        kernel size = 3, activation = relu\n    [UPCONV]        - Upsampling block composed of upsampling deconvolution (stride 2, kernel size = 3,\n                        activation = relu) and concatenation of conditioning\n    [fCONV]         - Final convolutional layer, with no conditioning. Kernel size = 3, activation = tanh\n\n    ### Diagram\n\n    RANDOM NOISE --> [FC] --> [CONV] --> [UPCONV]x3 --> [fCONV] --> OUTPUT IMAGE\n\n    ### Constraints\n    """"""\n    def __init__(self, name):\n        """"""\n\n        :param name: layer name\n        """"""\n        super(ImageGenerator, self).__init__(name=name)\n        self.initializers = {\'w\': tf.random_normal_initializer(0, 0.02),\n                             \'b\': tf.constant_initializer(0.001)}\n        self.noise_channels_per_layer = 0\n        self.with_conditionings = [True, True, True, True, False]\n\n    def layer_op(self, random_source, image_size, conditioning, is_training):\n        """"""\n\n        :param random_source: tensor, random noise to start generation\n        :param image_size: output image size\n        :param conditioning: tensor, conditioning information (e.g. coordinates in physical space)\n        :param is_training: boolean, True if network is in training mode\n        :return: tensor, generated image\n        """"""\n        keep_prob_ph = 1  # not passed in as a placeholder\n        add_noise = self.noise_channels_per_layer\n        if conditioning is not None:\n            conditioning_channels = conditioning.shape.as_list()[-1]\n            conditioning_channels = conditioning_channels + add_noise\n        else:\n            conditioning_channels = add_noise\n\n        # feature channels design pattern\n        ch = [512]\n        sz = image_size[:-1]\n        for i in range(4):\n            # compute output n_feature_channels of i-th layer\n            new_ch = ch[-1] + conditioning_channels * self.with_conditionings[i]\n            new_ch = round(new_ch / 2)\n            ch.append(new_ch)\n            # compute output spatial size of i-th layer\n            sz = [int(round(spatial_len / 2)) for spatial_len in sz]\n        ch.append(1)  # last layer single channel image\n\n        # resizing utilities\n        spatial_rank = len(image_size) - 1\n        if spatial_rank == 3:\n            def resize_func(x, sz):\n                sz_x = x.shape.as_list()\n                r1 = tf.image.resize_images(\n                    tf.reshape(x, sz_x[:3] + [-1]), sz[0:2])\n                r2 = tf.image.resize_images(\n                    tf.reshape(r1, [sz_x[0], sz[0] * sz[1], sz_x[3], -1]),\n                    [sz[0] * sz[1], sz[2]])\n                resized_3d = tf.reshape(r2, [sz_x[0]] + sz + [sz_x[-1]])\n                return resized_3d\n        elif spatial_rank == 2:\n            resize_func = tf.image.resize_bilinear\n\n        def concat_cond(x, with_conditioning):\n            """"""\n\n            :param x: tensor, input\n            :param with_conditioning: boolean, True if conditioning is to be used\n            :return: tensor, concatenation of x and conditioning, with noise addition\n            """"""\n            noise = []\n            if add_noise:\n                feature_shape = x.shape.as_list()[0:-1]\n                noise_shape = feature_shape + [add_noise]\n                noise = [tf.random_normal(noise_shape, 0.0, 0.1)]\n\n            if with_conditioning and conditioning is not None:\n                with tf.name_scope(\'concat_conditioning\'):\n                    spatial_shape = x.shape.as_list()[1:-1]\n                    resized_cond = resize_func(conditioning, spatial_shape)\n                    return tf.concat([x, resized_cond] + noise, axis=-1)\n            return x\n\n        def conv(ch, x):\n            """"""\n            Generates and applies a convolutional layer with relu as activation, kernel size 3,\n            batch norm\n            :param ch: int, number of output channels for convolutional layer\n            :param x: tensor, input to convolutional layer\n            :return: tensor, output of convolutiona layer\n            """"""\n            with tf.name_scope(\'conv\'):\n                conv_layer = ConvolutionalLayer(\n                    n_output_chns=ch,\n                    kernel_size=3,\n                    feature_normalization=\'batch\',\n                    with_bias=False,\n                    acti_func=\'relu\',\n                    w_initializer=self.initializers[\'w\'])\n                return conv_layer(x, is_training=is_training)\n\n        def up(ch, x):\n            """"""\n            Performs deconvolution operation with kernel size 3, stride 2, batch norm, and relu\n            :param ch: int, number of output channels for deconvolutional layer\n            :param x: tensor, input to deconvolutional layer\n            :return: tensor, output of deconvolutiona layer\n            """"""\n            with tf.name_scope(\'up\'):\n                deconv_layer = DeconvolutionalLayer(\n                    n_output_chns=ch,\n                    kernel_size=3,\n                    stride=2,\n                    feature_normalization=\'batch\',\n                    with_bias=False,\n                    acti_func=\'relu\',\n                    w_initializer=self.initializers[\'w\'])\n                return deconv_layer(x, is_training=is_training)\n\n        def up_block(ch, x, with_conditioning):\n            """"""\n            Performs upsampling and concatenation with conditioning\n            :param ch: int, number of output channels for deconvolutional layer\n            :param x: tensor, input to deconvolutional layer\n            :param with_conditioning: boolean, True if conditioning is to be used\n            :return: tensor, output of upsampling and concatenation\n            """"""\n            with tf.name_scope(\'up_block\'):\n                u = up(ch, x)\n                cond = concat_cond(u, with_conditioning)\n                return conv(cond.shape.as_list()[-1], cond)\n\n        def noise_to_image(sz, ch, rand_tensor, with_conditioning):\n            """"""\n            Processes random noise with fully connected layer and then convolutional layer\n            :param sz: image size\n            :param ch: int, number of output channels\n            :param rand_tensor: tensor, input random noise to generate image\n            :param with_conditioning: boolean, True if conditioning is to be used\n            :return: tensor, output of convolutional layer\n            """"""\n            batch_size = rand_tensor.shape.as_list()[0]\n            output_shape = [batch_size] + sz + [ch]\n            with tf.name_scope(\'noise_to_image\'):\n                g_no_0 = np.prod(sz) * ch\n                fc_layer = FullyConnectedLayer(\n                    n_output_chns=g_no_0,\n                    feature_normalization=None,\n                    with_bias=True,\n                    w_initializer=self.initializers[\'w\'],\n                    b_initializer=self.initializers[\'b\'])\n                g_h1p = fc_layer(rand_tensor, keep_prob=keep_prob_ph)\n                g_h1p = tf.reshape(g_h1p, output_shape)\n                g_h1p = concat_cond(g_h1p, with_conditioning)\n                return conv(ch + conditioning_channels, g_h1p)\n\n        def final_image(n_chns, x):\n            """"""\n\n            :param n_chns: int, number of output channels\n            :param x: tensor, input tensor to layers\n            :return: tensor, generated image\n            """"""\n            with tf.name_scope(\'final_image\'):\n                if add_noise > 0:\n                    feature_shape = x.shape.as_list()[0:-1]\n                    noise_shape = feature_shape + [add_noise]\n                    noise = tf.random_normal(noise_shape, 0, .1)\n                    x = tf.concat([x, noise], axis=3)\n                conv_layer = ConvolutionalLayer(\n                    n_output_chns=n_chns,\n                    kernel_size=3,\n                    acti_func=\'tanh\',\n                    feature_normalization=None,\n                    with_bias=True,\n                    w_initializer=self.initializers[\'w\'],\n                    b_initializer=self.initializers[\'b\'])\n                x_sample = conv_layer(\n                    x, is_training=is_training, keep_prob=keep_prob_ph)\n                return tf.image.resize_images(x_sample, image_size[:-1])\n\n        # let the tensors flow...\n        flow = random_source\n        for (idx, chns) in enumerate(ch):\n            if idx == 0:  # first layer fully-connected\n                flow = noise_to_image(\n                    sz, chns, flow, self.with_conditionings[idx])\n            elif idx == len(ch) - 1:  # final conv without bn\n                return final_image(chns, flow)\n            else:  # upsampling block\n                flow = up_block(chns, flow, self.with_conditionings[idx])\n\n\nclass ImageDiscriminator(BaseDiscriminator):\n    """"""\n    ### Description\n        implementation of discrimator from\n            Hu et al., ""Freehand Ultrasound Image Simulation with Spatially-Conditioned\n            Generative Adversarial Networks"", MICCAI RAMBO 2017\n            https://arxiv.org/abs/1707.05392\n\n    ### Building blocks\n    [FEATURE BLOCK]     - Convolutional layer (kernel size = 5, activation = selu)\n                          + residual convolutiona layer (kernel size = 3, activation = selu, batch norm)\n                          + convolutional layer (kernel size = 3, activation = selu, batch norm)\n\n    [DOWN BLOCK]        - Downsampling block with residual connections:\n                          Downsampling convolutional layer (stride = 2, kernel size = 3,\n                          activation = selu, batch norm)\n                          + residual convolutiona layer (kernel size = 3, activation = selu, batch norm)\n                          + convolutional layer (kernel size = 3, activation = selu, batch norm)\n\n    [FC]                - Fully connected layer\n\n    If conditioning is used, it gets concatenated to the image at the discriminator input\n\n    ### Diagram\n\n    INPUT IMAGE --> [FEATURE BLOCK] --> [DOWN BLOCK]x5 --> [FC] --> OUTPUT LOGITS\n\n    ### Constraints\n    """"""\n    def __init__(self, name):\n        """"""\n\n        :param name: layer name\n        """"""\n        super(ImageDiscriminator, self).__init__(name=name)\n\n        w_init = tf.random_normal_initializer(0, 0.02)\n        b_init = tf.constant_initializer(0.001)\n        # w_init = tf.contrib.layers.variance_scaling_initializer()\n        # b_init = tf.constant_initializer(0)\n\n        self.initializers = {\'w\': w_init, \'b\': b_init}\n        self.chns = [32, 64, 128, 256, 512, 1024, 1]\n\n    def layer_op(self, image, conditioning, is_training):\n        """"""\n\n        :param image: tensor, input to the network\n        :param conditioning: tensor, conditioning information (e.g. coordinates in physical space)\n        :param is_training: boolean, True if network is in training mode\n        :return: tensor, classification logits\n        """"""\n\n        batch_size = image.shape.as_list()[0]\n\n        def down(ch, x):\n            """"""\n            Downsampling convolutional layer (stride 2, kernel size 3, activation selu, batch norm)\n            :param ch: int, number of output channels\n            :param x: tensor, input to the convolutional layer\n            :return: tensor, output of the convolutional layer\n            """"""\n            with tf.name_scope(\'downsample\'):\n                conv_layer = ConvolutionalLayer(\n                    n_output_chns=ch,\n                    kernel_size=3,\n                    stride=2,\n                    feature_normalization=\'batch\',\n                    acti_func=\'selu\',\n                    w_initializer=self.initializers[\'w\'])\n                return conv_layer(x, is_training=is_training)\n\n        def convr(ch, x):\n            """"""\n            Convolutional layer for residuals (stride 1, kernel size 3, activation selu, batch norm)\n            :param ch: int, number of output channels\n            :param x: tensor, input to the convolutional layer\n            :return: tensor, output of the convolutional layer\n            """"""\n            conv_layer = ConvolutionalLayer(\n                n_output_chns=ch,\n                kernel_size=3,\n                feature_normalization=\'batch\',\n                acti_func=\'selu\',\n                w_initializer=self.initializers[\'w\'])\n            return conv_layer(x, is_training=is_training)\n\n        def conv(ch, x, s):\n            """"""\n            Convolutional layer (stride 1, kernel size 3, batch norm)\n            combining two flows\n            :param ch: int, number of output channels\n            :param x: tensor, input to the convolutional layer\n            :param s: flow to be added after convolution\n            :return: tensor, output of selu activation layer (selu(conv(x) + s))\n            """"""\n            conv_layer = ConvolutionalLayer(\n                n_output_chns=ch,\n                kernel_size=3,\n                feature_normalization=\'batch\',\n                w_initializer=self.initializers[\'w\'])\n            acti_layer = ActiLayer(func=\'selu\')\n\n            # combining two flows\n            res_flow = conv_layer(x, is_training=is_training) + s\n            return acti_layer(res_flow)\n\n        def down_block(ch, x):\n            """"""\n            Downsampling block with residual connections\n            :param ch: int, number of output channels\n            :param x: tensor, input to the convolutional layer\n            :return: tensor, output of downsampling + conv + conv\n            """"""\n            with tf.name_scope(\'down_resnet\'):\n                s = down(ch, x)\n                r = convr(ch, s)\n                return conv(ch, r, s)\n\n        def feature_block(ch, image):\n            """"""\n            First discriminator processing block\n            :param ch: int, number of output channels\n            :param image: tensor, input image to discriminator\n            :return: tensor, output of conv(image) --> conv --> conv\n            """"""\n            with tf.name_scope(\'feature\'):\n                conv_layer = ConvolutionalLayer(\n                    n_output_chns=ch,\n                    kernel_size=5,\n                    with_bias=True,\n                    feature_normalization=None,\n                    acti_func=\'selu\',\n                    w_initializer=self.initializers[\'w\'],\n                    b_initializer=self.initializers[\'b\'])\n                d_h1s = conv_layer(image, is_training=is_training)\n                d_h1r = convr(ch, d_h1s)\n                return conv(ch, d_h1r, d_h1s)\n\n        def fully_connected(ch, features):\n            """"""\n            Final discriminator processing block\n            :param ch: int, number of output channels\n            :param features: tensor, input features for final classification\n            :return: tensor, output logits of discriminator classification\n            """"""\n            with tf.name_scope(\'fully_connected\'):\n                # with bn?\n                fc_layer = FullyConnectedLayer(\n                    n_output_chns=ch, feature_normalization=None, with_bias=True)\n                return fc_layer(features, is_training=is_training)\n\n        if conditioning is not None:\n            image = tf.concat([image, conditioning], axis=-1)\n\n        # let the tensors flow...\n        flow = image\n        for (idx, n_chns) in enumerate(self.chns):\n            if idx == 0:  # first layer\n                flow = feature_block(n_chns, flow)\n            elif idx == len(self.chns) - 1:  # last layer\n                return fully_connected(n_chns, flow)\n            else:\n                flow = down_block(n_chns, flow)\n'"
niftynet/network/toynet.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.network.base_net import BaseNet\n\n\nclass ToyNet(BaseNet):\n    """"""\n    ### Description\n        Toy net for testing\n\n    ### Diagram\n    INPUT --> CONV(kernel = 3, activation = relu) --> CONV(kernel = 1, activation = None) --> MULTICLASS OUTPUT\n\n    ### Constraints\n    None\n    """"""\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'prelu\',\n                 name=\'ToyNet\'):\n        """"""\n\n        :param num_classes: int, number of final output channels\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: ctivation function to use\n        :param name: layer name\n        """"""\n\n        super(ToyNet, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.hidden_features = 10\n\n    def layer_op(self, images, is_training=True, **unused_kwargs):\n        """"""\n\n        :param images: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :param unused_kwargs: other arguments, not in use\n        :return: tensor, network output\n        """"""\n        conv_1 = ConvolutionalLayer(self.hidden_features,\n                                    kernel_size=3,\n                                    w_initializer=self.initializers[\'w\'],\n                                    w_regularizer=self.regularizers[\'w\'],\n                                    b_initializer=self.initializers[\'b\'],\n                                    b_regularizer=self.regularizers[\'b\'],\n                                    acti_func=\'relu\',\n                                    name=\'conv_input\')\n\n        conv_2 = ConvolutionalLayer(self.num_classes,\n                                    kernel_size=1,\n                                    w_initializer=self.initializers[\'w\'],\n                                    w_regularizer=self.regularizers[\'w\'],\n                                    b_initializer=self.initializers[\'b\'],\n                                    b_regularizer=self.regularizers[\'b\'],\n                                    acti_func=None,\n                                    name=\'conv_output\')\n\n        flow = conv_1(images, is_training)\n        flow = conv_2(flow, is_training)\n        return flow\n'"
niftynet/network/unet.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer\nfrom niftynet.layer.downsample import DownSampleLayer\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.layer.crop import CropLayer\nfrom niftynet.utilities.util_common import look_up_operations\n\n\nclass UNet3D(TrainableLayer):\n    """"""\n    ### Description\n        reimplementation of 3D U-net\n          \xc3\x87i\xc3\xa7ek et al., ""3D U-Net: Learning dense Volumetric segmentation from\n          sparse annotation"", MICCAI \'16\n\n    ### Building blocks\n    [dBLOCK]        - Downsampling UNet Block\n    [uBLOCK]        - Upsampling UNet Block\n    [nBLOCK]        - UNet Block with no final operation\n    [CROP]          - Cropping layer\n\n    ### Diagram\n\n    INPUT  -->  [dBLOCK] - - - - - - - - - - - - - - - -  [nBLOCK] --> [CROP] --> OUTPUT\n                    |                                       |\n                    [dBLOCK] - - - - - - - - - - - - [uBLOCK]\n                        |                              |\n                        [dBLOCK]  - - - - - - - [uBLOCK]\n                            |                      |\n                            --------[uBLOCk] ------\n\n    ### Constraints\n     - Image size - 4 should be divisible by 8\n     - Label size should be more than 88\n     - border is 44\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'prelu\',\n                 name=\'UNet\'):\n        """"""\n\n        :param num_classes: int, number of final output channels\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n        super(UNet3D, self).__init__(name=name)\n\n        self.n_features = [32, 64, 128, 256, 512]\n        self.acti_func = acti_func\n        self.num_classes = num_classes\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n        print(\'using {}\'.format(name))\n\n    def layer_op(self, images, is_training=True, layer_id=-1, **unused_kwargs):\n        """"""\n\n        :param images: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :param layer_id: int, not in use\n        :param unused_kwargs: other arguments, not in use\n        :return: tensor, output of the network\n        """"""\n        # image_size  should be divisible by 8\n        assert layer_util.check_spatial_dims(images, lambda x: x % 8 == 0)\n        assert layer_util.check_spatial_dims(images, lambda x: x >= 89)\n        block_layer = UNetBlock(\'DOWNSAMPLE\',\n                                (self.n_features[0], self.n_features[1]),\n                                (3, 3), with_downsample_branch=True,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'L1\')\n        pool_1, conv_1 = block_layer(images, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'DOWNSAMPLE\',\n                                (self.n_features[1], self.n_features[2]),\n                                (3, 3), with_downsample_branch=True,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'L2\')\n        pool_2, conv_2 = block_layer(pool_1, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'DOWNSAMPLE\',\n                                (self.n_features[2], self.n_features[3]),\n                                (3, 3), with_downsample_branch=True,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'L3\')\n        pool_3, conv_3 = block_layer(pool_2, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'UPSAMPLE\',\n                                (self.n_features[3], self.n_features[4]),\n                                (3, 3), with_downsample_branch=False,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'L4\')\n        up_3, _ = block_layer(pool_3, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'UPSAMPLE\',\n                                (self.n_features[3], self.n_features[3]),\n                                (3, 3), with_downsample_branch=False,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'R3\')\n        concat_3 = ElementwiseLayer(\'CONCAT\')(conv_3, up_3)\n        up_2, _ = block_layer(concat_3, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'UPSAMPLE\',\n                                (self.n_features[2], self.n_features[2]),\n                                (3, 3), with_downsample_branch=False,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'R2\')\n        concat_2 = ElementwiseLayer(\'CONCAT\')(conv_2, up_2)\n        up_1, _ = block_layer(concat_2, is_training)\n        print(block_layer)\n\n        block_layer = UNetBlock(\'NONE\',\n                                (self.n_features[1],\n                                 self.n_features[1],\n                                 self.num_classes),\n                                (3, 3, 1),\n                                with_downsample_branch=True,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                acti_func=self.acti_func,\n                                name=\'R1_FC\')\n        concat_1 = ElementwiseLayer(\'CONCAT\')(conv_1, up_1)\n\n        # for the last layer, upsampling path is not used\n        _, output_tensor = block_layer(concat_1, is_training)\n\n        crop_layer = CropLayer(border=44, name=\'crop-88\')\n        output_tensor = crop_layer(output_tensor)\n        print(block_layer)\n        return output_tensor\n\n\nSUPPORTED_OP = set([\'DOWNSAMPLE\', \'UPSAMPLE\', \'NONE\'])\n\n\nclass UNetBlock(TrainableLayer):\n    def __init__(self,\n                 func,\n                 n_chns,\n                 kernels,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 with_downsample_branch=False,\n                 acti_func=\'relu\',\n                 name=\'UNet_block\'):\n        """"""\n\n        :param func: string, type of operation to perform after convolution (Downsampling, Upsampling, None)\n        :param n_chns: array, number of output channels for each convolutional layer of the block\n        :param kernels: array, kernel sizes for each convolutional layer of the block\n        :param w_initializer: weight initialisation of convolutional layers\n        :param w_regularizer: weight regularisation of convolutional layers\n        :param with_downsample_branch: boolean, returns also the tensor before func is applied\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(UNetBlock, self).__init__(name=name)\n\n        self.func = look_up_operations(func.upper(), SUPPORTED_OP)\n\n        self.kernels = kernels\n        self.n_chns = n_chns\n        self.with_downsample_branch = with_downsample_branch\n        self.acti_func = acti_func\n\n        self.initializers = {\'w\': w_initializer}\n        self.regularizers = {\'w\': w_regularizer}\n\n    def layer_op(self, input_tensor, is_training):\n        """"""\n\n        :param input_tensor: tensor, input to the UNet block\n        :param is_training: boolean, True if network is in training mode\n        :return: output tensor of the UNet block and branch before downsampling (if required)\n        """"""\n        output_tensor = input_tensor\n        for (kernel_size, n_features) in zip(self.kernels, self.n_chns):\n            conv_op = ConvolutionalLayer(n_output_chns=n_features,\n                                         kernel_size=kernel_size,\n                                         w_initializer=self.initializers[\'w\'],\n                                         w_regularizer=self.regularizers[\'w\'],\n                                         acti_func=self.acti_func,\n                                         name=\'{}\'.format(n_features))\n            output_tensor = conv_op(output_tensor, is_training)\n\n        if self.with_downsample_branch:\n            branch_output = output_tensor\n        else:\n            branch_output = None\n\n        if self.func == \'DOWNSAMPLE\':\n            downsample_op = DownSampleLayer(\'MAX\',\n                                            kernel_size=2,\n                                            stride=2,\n                                            name=\'down_2x2\')\n            output_tensor = downsample_op(output_tensor)\n        elif self.func == \'UPSAMPLE\':\n            upsample_op = DeconvolutionalLayer(n_output_chns=self.n_chns[-1],\n                                               kernel_size=2,\n                                               stride=2,\n                                               name=\'up_2x2\')\n            output_tensor = upsample_op(output_tensor, is_training)\n        elif self.func == \'NONE\':\n            pass  # do nothing\n        return output_tensor, branch_output\n'"
niftynet/network/unet_2d.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import TrainableLayer, Layer\nfrom niftynet.layer.convolution import ConvolutionalLayer as Conv\nfrom niftynet.layer.crop import CropLayer as Crop\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer as DeConv\nfrom niftynet.layer.downsample import DownSampleLayer as Pooling\nfrom niftynet.layer.elementwise import ElementwiseLayer as ElementWise\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\nfrom niftynet.network.base_net import BaseNet\n\n\nclass UNet2D(BaseNet):\n    """"""\n    ### Description\n        A reimplementation of 2D UNet:\n            Ronneberger et al., U-Net: Convolutional Networks for Biomedical\n            Image Segmentation, MICCAI \'15\n\n    ### Building blocks\n    [dBLOCK]        - Downsampling Block (conv 3x3, Relu + conv 3x3, Relu +  Max pooling)\n    [BLOCK]         - Two layer Block (conv 3x3, Relu + conv 3x3, Relu)\n    [uBLOCK]        - Upsampling Block (deconv 2x2 + crop and concat + conv 3x3, Relu + conv 3x3, Relu)\n    [CONV]          - Classification block (conv 1x1)\n\n    ### Diagram\n\n    INPUT  -->  [dBLOCK] - - - - - - - - - - - - - - - -  [BLOCK] --> [CONV] --> OUTPUT\n                    |                                       |\n                    [dBLOCK] - - - - - - - - - - - - [uBLOCK]\n                        |                              |\n                        [dBLOCK]  - - - - - - - [uBLOCK]\n                            |                      |\n                            [dBLOCK]  - - - [uBLOCK]\n                                |               |\n                                ----[BLOCk] ----\n\n    ### Constraints\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'UNet2D\'):\n        BaseNet.__init__(self,\n                         num_classes=num_classes,\n                         name=name)\n        self.n_fea = [64, 128, 256, 512, 1024]\n\n        net_params = {\'padding\': \'VALID\',\n                      \'with_bias\': True,\n                      \'feature_normalization\': \'batch\',\n                      \'group_size\': -1,\n                      \'acti_func\': acti_func,\n                      \'w_initializer\': w_initializer,\n                      \'b_initializer\': b_initializer,\n                      \'w_regularizer\': w_regularizer,\n                      \'b_regularizer\': b_regularizer}\n\n        self.conv_params = {\'kernel_size\': 3, \'stride\': 1}\n        self.deconv_params = {\'kernel_size\': 2, \'stride\': 2}\n        self.pooling_params = {\'kernel_size\': 2, \'stride\': 2}\n\n        self.conv_params.update(net_params)\n        self.deconv_params.update(net_params)\n\n    def layer_op(self, images, is_training=True, **unused_kwargs):\n        """"""\n\n        :param images: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :param unused_kwargs: other conditional arguments, not in use\n        :return: tensor, output of the network\n        """"""\n        # contracting path\n        output_1 = TwoLayerConv(self.n_fea[0], self.conv_params)(images, is_training=is_training)\n        down_1 = Pooling(func=\'MAX\', **self.pooling_params)(output_1)\n\n        output_2 = TwoLayerConv(self.n_fea[1], self.conv_params)(down_1, is_training=is_training)\n        down_2 = Pooling(func=\'MAX\', **self.pooling_params)(output_2)\n\n        output_3 = TwoLayerConv(self.n_fea[2], self.conv_params)(down_2, is_training=is_training)\n        down_3 = Pooling(func=\'MAX\', **self.pooling_params)(output_3)\n\n        output_4 = TwoLayerConv(self.n_fea[3], self.conv_params)(down_3, is_training=is_training)\n        down_4 = Pooling(func=\'MAX\', **self.pooling_params)(output_4)\n\n        output_5 = TwoLayerConv(self.n_fea[4], self.conv_params)(down_4, is_training=is_training)\n\n        # expansive path\n        up_4 = DeConv(self.n_fea[3], **self.deconv_params)(output_5, is_training=is_training)\n        output_4 = CropConcat()(output_4, up_4)\n        output_4 = TwoLayerConv(self.n_fea[3], self.conv_params)(output_4, is_training=is_training)\n\n        up_3 = DeConv(self.n_fea[2], **self.deconv_params)(output_4, is_training=is_training)\n        output_3 = CropConcat()(output_3, up_3)\n        output_3 = TwoLayerConv(self.n_fea[2], self.conv_params)(output_3, is_training=is_training)\n\n        up_2 = DeConv(self.n_fea[1], **self.deconv_params)(output_3, is_training=is_training)\n        output_2 = CropConcat()(output_2, up_2)\n        output_2 = TwoLayerConv(self.n_fea[1], self.conv_params)(output_2, is_training=is_training)\n\n        up_1 = DeConv(self.n_fea[0], **self.deconv_params)(output_2, is_training=is_training)\n        output_1 = CropConcat()(output_1, up_1)\n        output_1 = TwoLayerConv(self.n_fea[0], self.conv_params)(output_1, is_training=is_training)\n\n        # classification layer\n        classifier = Conv(n_output_chns=self.num_classes,\n                          kernel_size=1,\n                          with_bias=True,\n                          feature_normalization=None)\n        output_tensor = classifier(output_1)\n        tf.logging.info(\'output shape %s\', output_tensor.shape)\n        return output_tensor\n\n\nclass TwoLayerConv(TrainableLayer):\n    """"""\n    Two convolutional layers, number of output channels are ``n_chns`` for both\n    of them.\n\n    --conv--conv--\n    """"""\n\n    def __init__(self, n_chns, conv_params):\n        TrainableLayer.__init__(self, name=\'TwoConv\')\n        self.n_chns = n_chns\n        self.conv_params = conv_params\n\n    def layer_op(self, input_tensor, is_training=None):\n        """"""\n\n        :param input_tensor: tensor, input to the two layer convolution\n        :param is_training: flag for training\n        :return: tensor, output of --conv--conv\n        """"""\n        output_tensor = Conv(self.n_chns, **self.conv_params)(input_tensor, is_training=is_training)\n        output_tensor = Conv(self.n_chns, **self.conv_params)(output_tensor, is_training=is_training)\n\n        return output_tensor\n\n\nclass CropConcat(Layer):\n    """"""\n    This layer concatenates two input tensors,\n    the first one is cropped and resized to match the second one.\n\n    This layer assumes the same amount of differences\n    in every spatial dimension in between the two tensors.\n    """"""\n\n    def __init__(self, name=\'crop_concat\'):\n        Layer.__init__(self, name=name)\n\n    def layer_op(self, tensor_a, tensor_b):\n        """"""\n        match the spatial shape and concatenate the tensors\n        tensor_a will be cropped and resized to match tensor_b.\n\n        :param tensor_a: tensor, input\n        :param tensor_b: tensor, input\n        :return: concatenated tensor\n        """"""\n        crop_border = (tensor_a.shape[1] - tensor_b.shape[1]) // 2\n        tensor_a = Crop(border=crop_border)(tensor_a)\n        output_spatial_shape = tensor_b.shape[1:-1]\n        tensor_a = Resize(new_size=output_spatial_shape)(tensor_a)\n        return ElementWise(\'CONCAT\')(tensor_a, tensor_b)\n'"
niftynet/network/vae.py,28,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\n# import niftynet.engine.logging as logging\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer\nfrom niftynet.layer.fully_connected import FullyConnectedLayer\nfrom niftynet.layer.upsample import UpSampleLayer\n\n\nclass VAE(TrainableLayer):\n    """"""\n    ### Description\n        This is a denoising, convolutional, variational autoencoder (VAE),\n        composed of a sequence of {convolutions then downsampling} blocks,\n        followed by a sequence of fully-connected layers,\n        followed by a sequence of {transpose convolutions then upsampling} blocks.\n        See Auto-Encoding Variational Bayes, Kingma & Welling, 2014.\n        2DO: share the fully-connected parameters\n        between the mean and logvar decoders.\n\n    ### Building Blocks\n    [ENCODER]               - See ConvEncoder class below\n    [GAUSSIAN SAMPLER]      - See GaussianSampler class below\n    [DECODER]               - See ConvDecoder class below\n\n    ### Diagram\n\n    INPUT --> [ENCODER] --> [GAUSSIAN SAMPLER] --> [FCDEC] ---> [DECODER] for means --- OUTPUTS\n                                                        |                               |\n                                                        ------> [DECODER] for logvars ---\n\n    ### Constraints\n    """"""\n\n    def __init__(self,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 name=\'VAE\'):\n        """"""\n\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param name: layer name\n        """"""\n\n        super(VAE, self).__init__(name=name)\n\n        # The following options completely specify the model.\n        # Note that the network need not be symmetric!\n\n        # 1) Denoising\n        # NOTES: If (and only if) \'denoising_variance\' is\n        # greater than zero, Gaussian noise with zero mean\n        # and \'denoising_variance\' variance is added to the input.\n        self.denoising_variance = 0.001\n\n        # 2) The convolutional layers\n        # NOTES: the ith element of \'conv_pooling_factors\' is\n        # the amount of pooling in the ith layer\n        # (along all spatial dimensions).\n        # CONSTRAINTS: All four lists must be the same length.\n        self.conv_output_channels = [32, 64, 96]\n        self.conv_kernel_sizes = [3, 3, 3]\n        self.conv_pooling_factors = [2, 2, 2]\n        self.acti_func_conv = [\'selu\', \'selu\', \'selu\']\n\n        # 3) The fully-connected layers\n        # NOTES: If \'layer_sizes_decoder_shared\' is empty then\n        #  the data means and log variances are predicted\n        # (separately) directly from the approximate sample\n        # from the posterior. Otherwise, this sample is passed\n        # through fully-connected layers of size\n        # \'layer_sizes_decoder_shared\', with respective activation\n        # functions \'acti_func_decoder_shared\'.\n        # CONSTRAINTS:\n        #   \'layer_sizes_encoder\' and \'acti_func_encoder\'\n        #   must have equal length.\n        #   \'acti_func_decoder\' must be one element longer than\n        #   \'layer_sizes_decoder\', because the final\n        #   element of \'acti_func_decoder\' is the activation function\n        #   of the final fully-connected layer (which is added to the\n        #   network automatically, and whose dimensionality equals\n        #  that of the input to the fully-connected layers).\n\n        self.layer_sizes_encoder = [256, 128]\n        self.acti_func_encoder = [\'selu\', \'selu\']\n        self.number_of_latent_variables = 64\n        self.number_of_samples_from_posterior = 100\n        self.layer_sizes_decoder_shared = [128]\n        self.acti_func_decoder_shared = [\'selu\']\n        self.layer_sizes_decoder = self.layer_sizes_encoder[::-1]\n        self.acti_func_decoder = self.acti_func_encoder[::-1] + [\'selu\']\n\n        # 4) The transpose convolutional layers (for predicting means)\n        # NOTES: \'upsampling_mode\' determines how the feature maps\n        #  in the decoding layers are upsampled.\n        # The options are,\n        # 1. \'DECONV\' (recommended):\n        #     kernel shape is HxWxDxChannelsInxChannelsOut,\n        # 2. \'CHANNELWISE_DECONV\':\n        #     kernel shape is HxWxDx1x1,\n        # 3. \'REPLICATE\':\n        #     no parameters.\n        # CONSTRAINTS:\n        #     \'trans_conv_output_channels_means\' is one element\n        #     shorter than \'trans_conv_kernel_sizes_means\',\n        #     \'trans_conv_unpooling_factors_means\', and\n        #     \'trans_conv_unpooling_factors_means\'\n        #     because the final element of\n        #     \'trans_conv_output_channels_means\' must be\n        #     the number of channels in the input,\n        #     and this is added to the list automatically.\n        self.trans_conv_output_channels_means = \\\n            self.conv_output_channels[-2::-1]\n        self.trans_conv_kernel_sizes_means = self.conv_kernel_sizes[::-1]\n        self.trans_conv_unpooling_factors_means = \\\n            self.conv_pooling_factors[::-1]\n        self.acti_func_trans_conv_means = \\\n            self.acti_func_conv[-2::-1] + [\'sigmoid\']\n        self.upsampling_mode_means = \'DECONV\'\n\n        # 5) The transpose convolutional layers\n        #     (for predicting (log) variances)\n        # CONSTRAINTS:\n        #     same as for the mean-predicting layers above.\n        self.trans_conv_output_channels_logvars = \\\n            self.trans_conv_output_channels_means\n        self.trans_conv_kernel_sizes_logvars = \\\n            self.trans_conv_kernel_sizes_means\n        self.trans_conv_unpooling_factors_logvars = \\\n            self.trans_conv_unpooling_factors_means\n        self.acti_func_trans_conv_logvars = \\\n            self.acti_func_conv[-2::-1] + [None]\n        self.upsampling_mode_logvars = self.upsampling_mode_means\n\n        # 6) Clip logvars to avoid infs & nans\n        # NOTES: variance = exp(logvars),\n        # so we must keep logvars within reasonable limits.\n        self.logvars_upper_bound = 50\n        self.logvars_lower_bound = -self.logvars_upper_bound\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, images, is_training=True, **unused_kwargs):\n        """"""\n\n        :param images: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :param unused_kwargs: other conditional arguments, not in use\n        :return: posterior_means: means output by gaussian sampler for KL divergence\n                posterior_logvars: log of variances output by gaussian sampler for KL divergence\n                data_means: output of means decoder branch (predicted image)\n                data_logvars: output of variances decoder branch (capturing aleatoric uncertainty)\n                images: input\n                data_variances: exp of data_logvars\n                posterior_variances: exp of posterior_logvars\n                sample: random sample from latent space (from gaussian sampler)\n        """"""\n\n        def clip(x):\n            """"""\n            Clip input tensor using the lower and upper bounds\n            :param x: tensor, input\n            :return: clipped tensor\n            """"""\n            return tf.clip_by_value(x, self.logvars_lower_bound,\n                                    self.logvars_upper_bound)\n\n        def normalise(x):\n            """"""\n            Normalise input to [0, 255]\n            :param x: tensor, input to normalise\n            :return: normalised tensor in [0, 255]\n            """"""\n            min_val = tf.reduce_min(x)\n            max_val = tf.reduce_max(x)\n            return 255 * (x - min_val) / (max_val - min_val)\n\n        def infer_downsampled_shape(x, output_channels, pooling_factors):\n            """"""\n            Calculate the shape of the data as it emerges from\n            the convolutional part of the encoder\n            :param x: tensor, input\n            :param output_channels: int, number of output channels\n            :param pooling_factors: array, pooling factors\n            :return: array, shape of downsampled image\n            """"""\n            downsampled_shape = x.shape[1::].as_list()\n            downsampled_shape[-1] = output_channels[-1]\n            downsampled_shape[0:-1] = \\\n                downsampled_shape[0:-1] / np.prod(pooling_factors)\n            return [int(x) for x in downsampled_shape]\n\n        # Derive shape information from the input\n        input_shape = images.shape[1::].as_list()\n        number_of_input_channels = input_shape[-1]\n        downsampled_shape = infer_downsampled_shape(images,\n                                                    self.conv_output_channels,\n                                                    self.conv_pooling_factors)\n        serialised_shape = int(np.prod(downsampled_shape))\n\n        encoder = ConvEncoder(self.denoising_variance,\n                              self.conv_output_channels,\n                              self.conv_kernel_sizes,\n                              self.conv_pooling_factors,\n                              self.acti_func_conv,\n                              self.layer_sizes_encoder,\n                              self.acti_func_encoder,\n                              serialised_shape)\n\n        approximate_sampler = GaussianSampler(\n            self.number_of_latent_variables,\n            self.number_of_samples_from_posterior,\n            self.logvars_upper_bound,\n            self.logvars_lower_bound)\n\n        # Initialise the shared fully-connected layers,\n        # if and only if they have been specified\n        if len(self.layer_sizes_decoder_shared) > 0:\n            self.shared_decoder = FCDecoder(self.layer_sizes_decoder_shared,\n                                       self.acti_func_decoder_shared,\n                                       name=\'FCDecoder\')\n\n        self.decoder_means = ConvDecoder(\n            self.layer_sizes_decoder + [serialised_shape],\n            self.acti_func_decoder,\n            self.trans_conv_output_channels_means + [number_of_input_channels],\n            self.trans_conv_kernel_sizes_means,\n            self.trans_conv_unpooling_factors_means,\n            self.acti_func_trans_conv_means,\n            self.upsampling_mode_means,\n            downsampled_shape,\n            name=\'ConvDecoder_means\')\n\n        decoder_logvars = ConvDecoder(\n            self.layer_sizes_decoder + [serialised_shape],\n            self.acti_func_decoder,\n            self.trans_conv_output_channels_logvars + [\n                number_of_input_channels],\n            self.trans_conv_kernel_sizes_logvars,\n            self.trans_conv_unpooling_factors_logvars,\n            self.acti_func_trans_conv_logvars,\n            self.upsampling_mode_logvars,\n            downsampled_shape,\n            name=\'ConvDecoder_logvars\')\n\n        # Encode the input\n        encoding = encoder(images, is_training)\n\n        # Sample from the posterior distribution P(latent variables|input)\n        [sample, posterior_means, posterior_logvars] = approximate_sampler(\n            encoding, is_training)\n\n        if len(self.layer_sizes_decoder_shared) > 0:\n            partially_decoded_sample = self.shared_decoder(\n                sample, is_training)\n        else:\n            partially_decoded_sample = sample\n\n        [data_means, data_logvars] = [\n            self.decoder_means(partially_decoded_sample, is_training),\n            clip(decoder_logvars(sample, is_training))]\n\n        ## Monitor the KL divergence of\n        ## the (approximate) posterior from the prior\n        #KL_divergence = 1 + posterior_logvars \\\n        #                - tf.square(posterior_means) \\\n        #                - tf.exp(posterior_logvars)\n        #KL_divergence = -0.5 * tf.reduce_mean(\n        #    tf.reduce_sum(KL_divergence, axis=[1]))\n        ## tf.add_to_collection(\n        ##     logging.CONSOLE, tf.summary.scalar(\'KL_divergence\', KL_divergence))\n\n        ## Monitor the (negative log) likelihood of the parameters given the data\n        #log_likelihood = data_logvars + \\\n        #                 np.log(2 * np.pi) + \\\n        #                 tf.exp(-data_logvars) * tf.square(data_means - images)\n        #log_likelihood = -0.5 * tf.reduce_mean(tf.reduce_sum(\n        #    log_likelihood, axis=[1, 2, 3, 4]))\n        ## tf.add_to_collection(\n        ##     logging.CONSOLE,\n        ##     tf.summary.scalar(\'negative_log_likelihood\', -log_likelihood))\n\n        posterior_variances = tf.exp(posterior_logvars)\n        data_variances = tf.exp(data_logvars)\n\n        # Monitor reconstructions\n        # logging.image3_coronal(\'Originals\', normalise(images))\n        # logging.image3_coronal(\'Means\', normalise(data_means))\n        # logging.image3_coronal(\'Variances\', normalise(data_variances))\n\n        return [posterior_means,\n                posterior_logvars,\n                data_means,\n                data_logvars,\n                images,\n                data_variances,\n                posterior_variances,\n                sample]\n\n\nclass ConvEncoder(TrainableLayer):\n    """"""\n    ### Description\n        This is a generic encoder composed of\n        {convolutions then downsampling} blocks followed by\n        fully-connected layers.\n    """"""\n\n    def __init__(self,\n                 denoising_variance,\n                 conv_output_channels,\n                 conv_kernel_sizes,\n                 conv_pooling_factors,\n                 acti_func_conv,\n                 layer_sizes_encoder,\n                 acti_func_encoder,\n                 serialised_shape,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 name=\'ConvEncoder\'):\n        """"""\n\n        :param denoising_variance: variance of gaussian noise to add to the input\n        :param conv_output_channels: array, number of output channels for each conv layer\n        :param conv_kernel_sizes: array, kernel sizes for each conv layer\n        :param conv_pooling_factors: array, stride values for downsampling convolutions\n        :param acti_func_conv: array, activation functions of each layer\n        :param layer_sizes_encoder: array, number of output channels for each encoding FC layer\n        :param acti_func_encoder: array, activation functions for each encoding FC layer\n        :param serialised_shape: array, flatten shape to enter the FC layers\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param name: layer name\n        """"""\n\n        super(ConvEncoder, self).__init__(name=name)\n\n        self.denoising_variance = denoising_variance\n        self.conv_output_channels = conv_output_channels\n        self.conv_kernel_sizes = conv_kernel_sizes\n        self.conv_pooling_factors = conv_pooling_factors\n        self.acti_func_conv = acti_func_conv\n        self.layer_sizes_encoder = layer_sizes_encoder\n        self.acti_func_encoder = acti_func_encoder\n        self.serialised_shape = serialised_shape\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, images, is_training):\n        """"""\n\n        :param images: tensor, input to the network\n        :param is_training: boolean, True if network is in training mode\n        :return: tensor, output of the encoder branch\n        """"""\n\n        # Define the encoding convolutional layers\n        encoders_cnn = []\n        encoders_downsamplers = []\n        for i in range(0, len(self.conv_output_channels)):\n            encoders_cnn.append(ConvolutionalLayer(\n                n_output_chns=self.conv_output_channels[i],\n                kernel_size=self.conv_kernel_sizes[i],\n                padding=\'SAME\',\n                with_bias=True,\n                feature_normalization=\'batch\',\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=None,\n                acti_func=self.acti_func_conv[i],\n                name=\'encoder_conv_{}_{}\'.format(\n                    self.conv_kernel_sizes[i],\n                    self.conv_output_channels[i])))\n            print(encoders_cnn[-1])\n\n            encoders_downsamplers.append(ConvolutionalLayer(\n                n_output_chns=self.conv_output_channels[i],\n                kernel_size=self.conv_pooling_factors[i],\n                stride=self.conv_pooling_factors[i],\n                padding=\'SAME\',\n                with_bias=False,\n                feature_normalization=\'batch\',\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=None,\n                acti_func=self.acti_func_conv[i],\n                name=\'encoder_downsampler_{}_{}\'.format(\n                    self.conv_pooling_factors[i],\n                    self.conv_pooling_factors[i])))\n\n            print(encoders_downsamplers[-1])\n\n        # Define the encoding fully-connected layers\n        encoders_fc = []\n        for i in range(0, len(self.layer_sizes_encoder)):\n            encoders_fc.append(FullyConnectedLayer(\n                n_output_chns=self.layer_sizes_encoder[i],\n                with_bias=True,\n                feature_normalization=\'batch\',\n                acti_func=self.acti_func_encoder[i],\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                name=\'encoder_fc_{}\'.format(self.layer_sizes_encoder[i])))\n            print(encoders_fc[-1])\n\n        # Add Gaussian noise to the input\n        if self.denoising_variance > 0 and is_training:\n            flow = images + tf.random_normal(\n                    tf.shape(images), 0.0, self.denoising_variance)\n        else:\n            flow = images\n\n        # Convolutional encoder layers\n        for i in range(0, len(self.conv_output_channels)):\n            flow = encoders_downsamplers[i](\n                encoders_cnn[i](flow, is_training), is_training)\n\n        # Flatten the feature maps\n        flow = tf.reshape(flow, [-1, self.serialised_shape])\n\n        # Fully-connected encoder layers\n        for i in range(0, len(self.layer_sizes_encoder)):\n            flow = encoders_fc[i](flow, is_training)\n\n        return flow\n\n\nclass GaussianSampler(TrainableLayer):\n    """"""\n    ### Description\n        This predicts the mean and logvariance parameters,\n        then generates an approximate sample from the posterior.\n\n    ### Diagram\n\n    ### Constraints\n\n    """"""\n\n    def __init__(self,\n                 number_of_latent_variables,\n                 number_of_samples_from_posterior,\n                 logvars_upper_bound,\n                 logvars_lower_bound,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 name=\'gaussian_sampler\'):\n        """"""\n\n        :param number_of_latent_variables: int, number of output channels for FC layer\n        :param number_of_samples_from_posterior: int, number of samples to draw from standard gaussian\n        :param logvars_upper_bound: upper bound of log of variances for clipping\n        :param logvars_lower_bound: lower bound of log of variances for clipping\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param name: layer name\n        """"""\n\n        super(GaussianSampler, self).__init__(name=name)\n\n        self.number_of_latent_variables = number_of_latent_variables\n        self.number_of_samples = number_of_samples_from_posterior\n        self.logvars_upper_bound = logvars_upper_bound\n        self.logvars_lower_bound = logvars_lower_bound\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, codes, is_training):\n        """"""\n\n        :param codes: tensor, input latent space\n        :param is_training: boolean, True if network is in training mode\n        :return: samples from posterior distribution, means and log variances of the posterior distribution\n        """"""\n\n        def clip(input):\n            # This is for clipping logvars,\n            # so that variances = exp(logvars) behaves well\n            output = tf.maximum(input, self.logvars_lower_bound)\n            output = tf.minimum(output, self.logvars_upper_bound)\n            return output\n\n        encoder_means = FullyConnectedLayer(\n            n_output_chns=self.number_of_latent_variables,\n            feature_normalization=None,\n            acti_func=None,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=\'encoder_fc_means_{}\'.format(self.number_of_latent_variables))\n        print(encoder_means)\n\n        encoder_logvars = FullyConnectedLayer(\n            n_output_chns=self.number_of_latent_variables,\n            feature_normalization=None,\n            acti_func=None,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=\'encoder_fc_logvars_{}\'.format(\n                self.number_of_latent_variables))\n        print(encoder_logvars)\n\n        # Predict the posterior distribution\'s parameters\n        posterior_means = encoder_means(codes, is_training)\n        posterior_logvars = clip(encoder_logvars(codes, is_training))\n\n        if self.number_of_samples == 1:\n            noise_sample = tf.random_normal(tf.shape(posterior_means),\n                                            0.0,\n                                            1.0)\n        else:\n            sample_shape = tf.concat(\n                [tf.constant(self.number_of_samples, shape=[1, ]),\n                 tf.shape(posterior_means)], axis=0)\n            noise_sample = tf.reduce_mean(\n                tf.random_normal(sample_shape, 0.0, 1.0), axis=0)\n\n        return [\n            posterior_means + tf.exp(0.5 * posterior_logvars) * noise_sample,\n            posterior_means, posterior_logvars]\n\n\nclass ConvDecoder(TrainableLayer):\n    """"""\n    ### Description\n            This is a generic decoder composed of\n            fully-connected layers followed by\n            {upsampling then transpose convolution} blocks.\n            There is no batch normalisation on\n            the final transpose convolutional layer.\n    """"""\n\n    def __init__(self,\n                 layer_sizes_decoder,\n                 acti_func_decoder,\n                 trans_conv_output_channels,\n                 trans_conv_kernel_sizes,\n                 trans_conv_unpooling_factors,\n                 acti_func_trans_conv,\n                 upsampling_mode,\n                 downsampled_shape,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 name=\'ConvDecoder\'):\n        """"""\n\n        :param layer_sizes_decoder: array, number of output channels for each decoding FC layer\n        :param acti_func_decoder: array, activation functions for each decoding FC layer\n        :param trans_conv_output_channels: array, number of output channels for each transpose conv layer\n        :param trans_conv_kernel_sizes: array, kernel sizes for each transpose conv layer\n        :param trans_conv_unpooling_factors: array, stride values for upsampling transpose convolutions\n        :param acti_func_trans_conv: array, activation functions for each transpose conv layer\n        :param upsampling_mode: string, type of upsampling (Deconvolution, channelwise deconvolution, replicate)\n        :param downsampled_shape: array, final encoded shape before FC in encoder\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param name: layer name\n        """"""\n\n        super(ConvDecoder, self).__init__(name=name)\n\n        self.layer_sizes_decoder = layer_sizes_decoder\n        self.acti_func_decoder = acti_func_decoder\n        self.trans_conv_output_channels = trans_conv_output_channels\n        self.trans_conv_kernel_sizes = trans_conv_kernel_sizes\n        self.trans_conv_unpooling_factors = trans_conv_unpooling_factors\n        self.acti_func_trans_conv = acti_func_trans_conv\n        self.upsampling_mode = upsampling_mode\n        self.downsampled_shape = downsampled_shape\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, codes, is_training):\n        """"""\n\n        :param codes: tensor, input latent space after gaussian sampling\n        :param is_training: boolean, True if network is in training mode\n        :return: tensor, output of decoding branch\n        """"""\n\n        # Define the decoding fully-connected layers\n        decoders_fc = []\n        for i in range(0, len(self.layer_sizes_decoder)):\n            decoders_fc.append(FullyConnectedLayer(\n                n_output_chns=self.layer_sizes_decoder[i],\n                with_bias=True,\n                feature_normalization=\'batch\',\n                acti_func=self.acti_func_decoder[i],\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                name=\'decoder_fc_{}\'.format(self.layer_sizes_decoder[i])))\n            print(decoders_fc[-1])\n\n        # Define the decoding convolutional layers\n        decoders_cnn = []\n        decoders_upsamplers = []\n        for i in range(0, len(self.trans_conv_output_channels)):\n            if self.upsampling_mode == \'DECONV\':\n                decoders_upsamplers.append(DeconvolutionalLayer(\n                    n_output_chns=self.trans_conv_output_channels[i],\n                    kernel_size=self.trans_conv_unpooling_factors[i],\n                    stride=self.trans_conv_unpooling_factors[i],\n                    padding=\'SAME\',\n                    with_bias=True,\n                    feature_normalization=\'batch\',\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=None,\n                    acti_func=None,\n                    name=\'decoder_upsampler_{}_{}\'.format(\n                        self.trans_conv_unpooling_factors[i],\n                        self.trans_conv_unpooling_factors[i])))\n                print(decoders_upsamplers[-1])\n\n            decoders_cnn.append(DeconvolutionalLayer(\n                n_output_chns=self.trans_conv_output_channels[i],\n                kernel_size=self.trans_conv_kernel_sizes[i],\n                stride=1,\n                padding=\'SAME\',\n                with_bias=True,\n                feature_normalization=\'batch\',\n                #feature_normalization=not (i == len(self.trans_conv_output_channels) - 1),\n                # No BN on output\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=None,\n                acti_func=self.acti_func_trans_conv[i],\n                name=\'decoder_trans_conv_{}_{}\'.format(\n                    self.trans_conv_kernel_sizes[i],\n                    self.trans_conv_output_channels[i])))\n            print(decoders_cnn[-1])\n\n        # Fully-connected decoder layers\n        flow = codes\n        for i in range(0, len(self.layer_sizes_decoder)):\n            flow = decoders_fc[i](flow, is_training)\n\n        # Reconstitute the feature maps\n        flow = tf.reshape(flow, [-1] + self.downsampled_shape)\n\n        # Convolutional decoder layers\n        for i in range(0, len(self.trans_conv_output_channels)):\n            if self.upsampling_mode == \'DECONV\':\n                flow = decoders_upsamplers[i](flow, is_training)\n            elif self.upsampling_mode == \'CHANNELWISE_DECONV\':\n                flow = UpSampleLayer(\n                    \'CHANNELWISE_DECONV\',\n                    kernel_size=self.trans_conv_unpooling_factors[i],\n                    stride=self.trans_conv_unpooling_factors[i])(flow)\n            elif self.upsampling_mode == \'REPLICATE\':\n                flow = UpSampleLayer(\n                    \'REPLICATE\',\n                    kernel_size=self.trans_conv_unpooling_factors[i],\n                    stride=self.trans_conv_unpooling_factors[i])(flow)\n            flow = decoders_cnn[i](flow, is_training)\n\n        return flow\n\n\nclass FCDecoder(TrainableLayer):\n    """"""\n    ### Description\n        This is a generic fully-connected decoder.\n    """"""\n\n    def __init__(self,\n                 layer_sizes_decoder,\n                 acti_func_decoder,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 name=\'FCDecoder\'):\n        """"""\n\n        :param layer_sizes_decoder: array, number of output channels for each decoding FC layer\n        :param acti_func_decoder: array, activation functions for each decoding FC layer\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param name: layer name\n        """"""\n\n        super(FCDecoder, self).__init__(name=name)\n\n        self.layer_sizes_decoder = layer_sizes_decoder\n        self.acti_func_decoder = acti_func_decoder\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, codes, is_training):\n        """"""\n\n        :param codes: tensor, input latent codes\n        :param is_training: boolean, True if network is in training mode\n        :return: tensor, output of series of FC layers\n        """"""\n\n        # Define the decoding fully-connected layers\n        decoders_fc = []\n        for i in range(0, len(self.layer_sizes_decoder)):\n            decoders_fc.append(FullyConnectedLayer(\n                n_output_chns=self.layer_sizes_decoder[i],\n                with_bias=True,\n                feature_normalization=\'batch\',\n                acti_func=self.acti_func_decoder[i],\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                name=\'FCDecoder_fc_{}\'.format(self.layer_sizes_decoder[i])))\n            print(decoders_fc[-1])\n\n        # Fully-connected decoder layers\n        flow = codes\n        for i in range(0, len(self.layer_sizes_decoder)):\n            flow = decoders_fc[i](flow, is_training)\n\n        return flow\n'"
niftynet/network/vnet.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.activation import ActiLayer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.convolution import ConvLayer\nfrom niftynet.layer.deconvolution import DeconvLayer\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.network.base_net import BaseNet\nfrom niftynet.utilities.util_common import look_up_operations\n\n\nclass VNet(BaseNet):\n    """"""\n    ### Description\n        implementation of V-Net:\n          Milletari et al., ""V-Net: Fully convolutional neural networks for\n          volumetric medical image segmentation"", 3DV \'16\n\n    ### Building Blocks\n    (n)[dBLOCK]        - Downsampling VNet block with n conv layers (kernel size = 5,\n                            with residual connections, activation = relu as default)\n                            followed by downsampling conv layer (kernel size = 2,\n                            stride = 2) + final activation\n    (n)[uBLOCK]         - Upsampling VNet block with n conv layers (kernel size = 5,\n                            with residual connections, activation = relu as default)\n                            followed by deconv layer (kernel size = 2,\n                            stride = 2) + final activation\n    (n)[sBLOCK]         - VNet block with n conv layers (kernel size = 5,\n                            with residual connections, activation = relu as default)\n                            followed by 1x1x1 conv layer (kernel size = 1,\n                            stride = 1) + final activation\n\n    ### Diagram\n\n    INPUT  -->  (1)[dBLOCK] - - - - - - - - - - - - - - - - (1)[sBLOCK] --> OUTPUT\n                    |                                             |\n                   (2)[dBLOCK] - - - - - - - - - - - - (2)[uBLOCK]\n                        |                                   |\n                        (3)[dBLOCK]  - - - - - - - (3)[uBLOCK]\n                            |                           |\n                            (3)[dBLOCK]  - - - (3)[uBLOCK]\n                                |                   |\n                                ----(3)[uBLOCk] ----\n\n\n    ### Constraints\n     - Input size should be divisible by 8\n     - Input should be either 2D or 3D\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'VNet\'):\n        """"""\n\n        :param num_classes: int, number of channels of output\n        :param w_initializer: weight initialisation for network\n        :param w_regularizer: weight regularisation for network\n        :param b_initializer: bias initialisation for network\n        :param b_regularizer: bias regularisation for network\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(VNet, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.n_features = [16, 32, 64, 128, 256]\n\n    def layer_op(self, images, is_training=True, layer_id=-1, **unused_kwargs):\n        """"""\n\n        :param images: tensor to input to the network. Size has to be divisible by 8\n        :param is_training:  boolean, True if network is in training mode\n        :param layer_id: not in use\n        :param unused_kwargs: other conditional arguments, not in use\n        :return: tensor, network output\n        """"""\n        assert layer_util.check_spatial_dims(images, lambda x: x % 8 == 0)\n\n        if layer_util.infer_spatial_rank(images) == 2:\n            padded_images = tf.tile(images, [1, 1, 1, self.n_features[0]])\n        elif layer_util.infer_spatial_rank(images) == 3:\n            padded_images = tf.tile(images, [1, 1, 1, 1, self.n_features[0]])\n        else:\n            raise ValueError(\'not supported spatial rank of the input image\')\n        # downsampling  blocks\n        res_1, down_1 = VNetBlock(\'DOWNSAMPLE\', 1,\n                                  self.n_features[0],\n                                  self.n_features[1],\n                                  w_initializer=self.initializers[\'w\'],\n                                  w_regularizer=self.regularizers[\'w\'],\n                                  acti_func=self.acti_func,\n                                  name=\'L1\')(images, padded_images)\n        res_2, down_2 = VNetBlock(\'DOWNSAMPLE\', 2,\n                                  self.n_features[1],\n                                  self.n_features[2],\n                                  w_initializer=self.initializers[\'w\'],\n                                  w_regularizer=self.regularizers[\'w\'],\n                                  acti_func=self.acti_func,\n                                  name=\'L2\')(down_1, down_1)\n        res_3, down_3 = VNetBlock(\'DOWNSAMPLE\', 3,\n                                  self.n_features[2],\n                                  self.n_features[3],\n                                  w_initializer=self.initializers[\'w\'],\n                                  w_regularizer=self.regularizers[\'w\'],\n                                  acti_func=self.acti_func,\n                                  name=\'L3\')(down_2, down_2)\n        res_4, down_4 = VNetBlock(\'DOWNSAMPLE\', 3,\n                                  self.n_features[3],\n                                  self.n_features[4],\n                                  acti_func=self.acti_func,\n                                  name=\'L4\')(down_3, down_3)\n        # upsampling blocks\n        _, up_4 = VNetBlock(\'UPSAMPLE\', 3,\n                            self.n_features[4],\n                            self.n_features[4],\n                            w_initializer=self.initializers[\'w\'],\n                            w_regularizer=self.regularizers[\'w\'],\n                            acti_func=self.acti_func,\n                            name=\'V_\')(down_4, down_4)\n        concat_r4 = ElementwiseLayer(\'CONCAT\')(up_4, res_4)\n        _, up_3 = VNetBlock(\'UPSAMPLE\', 3,\n                            self.n_features[4],\n                            self.n_features[3],\n                            w_initializer=self.initializers[\'w\'],\n                            w_regularizer=self.regularizers[\'w\'],\n                            acti_func=self.acti_func,\n                            name=\'R4\')(concat_r4, up_4)\n        concat_r3 = ElementwiseLayer(\'CONCAT\')(up_3, res_3)\n        _, up_2 = VNetBlock(\'UPSAMPLE\', 3,\n                            self.n_features[3],\n                            self.n_features[2],\n                            w_initializer=self.initializers[\'w\'],\n                            w_regularizer=self.regularizers[\'w\'],\n                            acti_func=self.acti_func,\n                            name=\'R3\')(concat_r3, up_3)\n        concat_r2 = ElementwiseLayer(\'CONCAT\')(up_2, res_2)\n        _, up_1 = VNetBlock(\'UPSAMPLE\', 2,\n                            self.n_features[2],\n                            self.n_features[1],\n                            w_initializer=self.initializers[\'w\'],\n                            w_regularizer=self.regularizers[\'w\'],\n                            acti_func=self.acti_func,\n                            name=\'R2\')(concat_r2, up_2)\n        # final class score\n        concat_r1 = ElementwiseLayer(\'CONCAT\')(up_1, res_1)\n        _, output_tensor = VNetBlock(\'SAME\', 1,\n                                     self.n_features[1],\n                                     self.num_classes,\n                                     w_initializer=self.initializers[\'w\'],\n                                     w_regularizer=self.regularizers[\'w\'],\n                                     b_initializer=self.initializers[\'b\'],\n                                     b_regularizer=self.regularizers[\'b\'],\n                                     acti_func=self.acti_func,\n                                     name=\'R1\')(concat_r1, up_1)\n        return output_tensor\n\n\nSUPPORTED_OP = set([\'DOWNSAMPLE\', \'UPSAMPLE\', \'SAME\'])\n\n\nclass VNetBlock(TrainableLayer):\n    def __init__(self,\n                 func,\n                 n_conv,\n                 n_feature_chns,\n                 n_output_chns,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'relu\',\n                 name=\'vnet_block\'):\n        """"""\n\n        :param func: string, defines final block operation (Downsampling, upsampling, same)\n        :param n_conv: int, number of conv layers to apply\n        :param n_feature_chns: int, number of feature channels (output channels) for each conv layer\n        :param n_output_chns: int, number of output channels of the final block operation (func)\n        :param w_initializer: weight initialisation of convolutional layers\n        :param w_regularizer: weight regularisation of convolutional layers\n        :param b_initializer: bias initialisation of convolutional layers\n        :param b_regularizer: bias regularisation of convolutional layers\n        :param acti_func: activation function to use\n        :param name: layer name\n        """"""\n\n        super(VNetBlock, self).__init__(name=name)\n\n        self.func = look_up_operations(func.upper(), SUPPORTED_OP)\n        self.n_conv = n_conv\n        self.n_feature_chns = n_feature_chns\n        self.n_output_chns = n_output_chns\n        self.acti_func = acti_func\n\n        self.initializers = {\'w\': w_initializer, \'b\': b_initializer}\n        self.regularizers = {\'w\': w_regularizer, \'b\': b_regularizer}\n\n    def layer_op(self, main_flow, bypass_flow):\n        """"""\n\n        :param main_flow: tensor, input to the VNet block\n        :param bypass_flow: tensor, input from skip connection\n        :return: res_flow is tensor before final block operation (for residual connections),\n            main_flow is final output tensor\n        """"""\n        for i in range(self.n_conv):\n            main_flow = ConvLayer(name=\'conv_{}\'.format(i),\n                                  n_output_chns=self.n_feature_chns,\n                                  w_initializer=self.initializers[\'w\'],\n                                  w_regularizer=self.regularizers[\'w\'],\n                                  kernel_size=5)(main_flow)\n            if i < self.n_conv - 1:  # no activation for the last conv layer\n                main_flow = ActiLayer(\n                    func=self.acti_func,\n                    regularizer=self.regularizers[\'w\'])(main_flow)\n        res_flow = ElementwiseLayer(\'SUM\')(main_flow, bypass_flow)\n\n        if self.func == \'DOWNSAMPLE\':\n            main_flow = ConvLayer(\n                name=\'downsample\',\n                n_output_chns=self.n_output_chns,\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                kernel_size=2, stride=2, with_bias=True)(res_flow)\n        elif self.func == \'UPSAMPLE\':\n            main_flow = DeconvLayer(\n                name=\'upsample\',\n                n_output_chns=self.n_output_chns,\n                w_initializer=self.initializers[\'w\'],\n                w_regularizer=self.regularizers[\'w\'],\n                kernel_size=2, stride=2, with_bias=True)(res_flow)\n        elif self.func == \'SAME\':\n            main_flow = ConvLayer(name=\'conv_1x1x1\',\n                                  n_output_chns=self.n_output_chns,\n                                  w_initializer=self.initializers[\'w\'],\n                                  w_regularizer=self.regularizers[\'w\'],\n                                  b_initializer=self.initializers[\'b\'],\n                                  b_regularizer=self.regularizers[\'b\'],\n                                  kernel_size=1, with_bias=True)(res_flow)\n        main_flow = ActiLayer(self.acti_func)(main_flow)\n        print(self)\n        return res_flow, main_flow\n'"
niftynet/utilities/__init__.py,0,"b'""""""\n\n.. module:: niftynet.utilities\n   :synopsis: High-level re-usable utilities.\n\n""""""\n\n\nfrom .niftynet_launch_config import NiftyNetLaunchConfig\n'"
niftynet/utilities/decorators.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom functools import wraps\n\n\ndef singleton(cls):\n    """"""Decorate a class as singleton.\n\n    Inspired by: https://wiki.python.org/moin/PythonDecoratorLibrary#Singleton\n    """"""\n\n    cls.__new_original__ = cls.__new__\n\n    @wraps(cls.__new__)\n    def singleton_new(cls, *args, **kw):\n        it = cls.__dict__.get(\'__it__\')\n        if it is not None:\n            return it\n\n        cls.__it__ = it = cls.__new_original__(cls, *args, **kw)\n        it.__init_original__(*args, **kw)\n        return it\n\n    cls.__new__ = staticmethod(singleton_new)\n    cls.__init_original__ = cls.__init__\n    cls.__init__ = object.__init__\n\n    return cls\n'"
niftynet/utilities/download.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nDownloading model zoo specifications and model zoo contents.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport argparse\nimport math\nimport os\nimport re\nimport shutil\nimport tarfile\nimport tempfile\nfrom distutils.version import LooseVersion\n\n# pylint: disable=wrong-import-order\nfrom six.moves.urllib.parse import urlparse\nfrom six.moves.urllib.request import urlopen\nfrom six.moves.urllib.request import urlretrieve\n\n# Used with the min_download_api settings option to determine\n# if the downloaded configuration file is compatible with\n# this version of NiftyNet downloader code\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\nfrom niftynet.utilities import NiftyNetLaunchConfig\nfrom niftynet.utilities.util_common import print_progress_bar\nfrom niftynet.utilities.versioning import get_niftynet_version, \\\n    get_niftynet_version_string\n\nDOWNLOAD_API_VERSION = ""1.0""\nCONFIG_FILE_EXT = "".ini""\n\n\ndef download(example_ids,\n             download_if_already_existing=False,\n             verbose=True):\n    """"""\n    Downloads standard NiftyNet examples such as data, samples\n\n    :param example_ids:\n        A list of identifiers for the samples to download\n    :param download_if_already_existing:\n        If true, data will always be downloaded\n    :param verbose:\n        If true, download info will be printed\n    """"""\n\n    global_config = NiftyNetGlobalConfig()\n\n    config_store = ConfigStore(global_config, verbose=verbose)\n\n    # If a single id is specified, convert to a list\n    example_ids = [example_ids] \\\n        if not isinstance(example_ids, (tuple, list)) else example_ids\n\n    if not example_ids:\n        return False\n\n    # Check if the server is running by looking for a known file\n    remote_base_url_test = raw_file_url(\n        global_config.get_download_server_url())\n    server_ok = url_exists(remote_base_url_test)\n    if verbose:\n        print(""Accessing: {}"".format(global_config.get_download_server_url()))\n\n    any_error = False\n    for example_id in example_ids:\n        if not example_id:\n            any_error = True\n            continue\n        if config_store.exists(example_id):\n            update_ok = config_store.update_if_required(\n                example_id,\n                download_if_already_existing)\n            any_error = (not update_ok) or any_error\n        else:\n            any_error = True\n            if server_ok:\n                print(example_id + \': FAIL. \')\n                print(\'No NiftyNet example was found for \' + example_id + ""."")\n\n    # If errors occurred and the server is down report a message\n    if any_error and not server_ok:\n        print(""The NiftyNetExamples server is not running"")\n\n    return not any_error\n\n\ndef download_file(url, download_path):\n    """"""\n    Download a file from a resource URL to the given location\n\n    :param url: URL of the file to download\n    :param download_path: location where the file should be saved\n    """"""\n    # Extract the filename from the URL\n    filename = os.path.basename(download_path)\n\n    # Ensure the output directory exists\n    output_directory = os.path.dirname(download_path)\n    if not os.path.exists(output_directory):\n        os.makedirs(output_directory)\n\n    # Get a temporary file path for the compressed file download\n    temp_folder = tempfile.mkdtemp()\n    downloaded_file = os.path.join(temp_folder, filename)\n\n    # Download the file\n    urlretrieve(url, downloaded_file, reporthook=progress_bar_wrapper)\n\n    # Move the file to the destination folder\n    shutil.move(downloaded_file, download_path)\n    shutil.rmtree(temp_folder, ignore_errors=True)\n\n\ndef download_and_decompress(url, download_path, verbose=True):\n    """"""\n    Download an archive from a resource URL and\n    decompresses/unarchives to the given location\n\n    :param url: URL of the compressed file to download\n    :param download_path: location where the file should be extracted\n    """"""\n\n    # Extract the filename from the URL\n    print(\'Downloading from {}\'.format(url))\n    parsed = urlparse(url)\n    filename = os.path.basename(parsed.path)\n\n    # Ensure the output directory exists\n    if not os.path.exists(download_path):\n        os.makedirs(download_path)\n\n    # Get a temporary file path for the compressed file download\n    temp_folder = tempfile.mkdtemp()\n    downloaded_file = os.path.join(temp_folder, filename)\n\n    # Download the file\n    if verbose:\n        urlretrieve(url, downloaded_file, reporthook=progress_bar_wrapper)\n    else:\n        urlretrieve(url, downloaded_file)\n\n    # Decompress and extract all files to the specified local path\n    tar = tarfile.open(downloaded_file, ""r"")\n    tar.extractall(download_path)\n    tar.close()\n\n    # Remove the downloaded file\n    shutil.rmtree(temp_folder, ignore_errors=True)\n\n\nclass ConfigStore(object):\n    """"""\n    Manages a configuration file store based on a\n    remote repository with local caching\n    """"""\n\n    def __init__(self, global_config, verbose=True):\n        self._download_folder = global_config.get_niftynet_home_folder()\n        self._config_folder = global_config.get_niftynet_config_folder()\n        self._local = ConfigStoreCache(\n            os.path.join(self._config_folder, \'.downloads_local_config_cache\'))\n        self._remote = RemoteProxy(self._config_folder,\n                                   global_config.get_download_server_url())\n        self._verbose = verbose\n\n    def exists(self, example_id):\n        """"""\n        Returns True if a record exists for this example_id,\n        either locally or remotely\n        """"""\n\n        return self._local.exists(example_id) or self._remote.exists(example_id)\n\n    # pylint: disable=broad-except\n    def update_if_required(self,\n                           example_id,\n                           download_if_already_existing=False):\n        """"""\n        Downloads data using the configuration file\n        if it is not already up to date.\n        Returns True if no update was required and no errors occurred\n        """"""\n\n        try:\n            self._remote.update(example_id)\n            remote_update_failed = False\n        except Exception as fail_msg:\n            print(""Warning: updating the examples file ""\n                  ""from the server caused an error: {}"".format(fail_msg))\n            remote_update_failed = True\n\n        current_config, current_entries = \\\n            self._local.get_download_params(example_id)\n        remote_config, remote_entries = \\\n            self._remote.get_download_params(example_id)\n\n        if not remote_entries:\n            if remote_update_failed:\n                print(example_id + "": FAIL."")\n                print(""Cannot download the examples configuration file. ""\n                      ""Is the server down?"")\n            else:\n                print(example_id + "": FAIL. Nothing to download"")\n            return False\n\n        else:\n            # Always download if the local file is empty, or force by arguments\n            force_download = download_if_already_existing or \\\n                             (not current_config and not current_entries)\n            data_missing = self._are_data_missing(remote_entries, example_id)\n            if force_download or data_missing or self._is_update_required(\n                    current_config, remote_config):\n                self._check_minimum_versions(remote_config)\n                self._download(remote_entries, example_id)\n                self._replace_local_with_remote(example_id)\n            else:\n                print(example_id + "": OK. "")\n                print(""Already downloaded. ""\n                      ""Use the -r option to download again."")\n\n        return True\n\n    @staticmethod\n    def _check_minimum_versions(remote_config):\n        # Checks whether a minimum download API is specified\n        if \'min_download_api\' in remote_config:\n            try:\n                min_download_api = LooseVersion(\n                    remote_config[\'min_download_api\'])\n            except AttributeError:\n                min_download_api = LooseVersion(DOWNLOAD_API_VERSION)\n            current_download_api_version = LooseVersion(DOWNLOAD_API_VERSION)\n            if min_download_api > current_download_api_version:\n                raise ValueError(\n                    ""This example requires a newer version of NiftyNet."")\n\n        # Checks whether a minimum NiftyNet version is specified\n        if \'min_niftynet\' in remote_config:\n            try:\n                min_niftynet = LooseVersion(remote_config[\'min_niftynet\'])\n                current_version = LooseVersion(get_niftynet_version())\n            except AttributeError:\n                min_niftynet = LooseVersion(\'0\')\n                current_version = LooseVersion(\'0\')\n            if min_niftynet > current_version:\n                raise ValueError(""This example requires NiftyNet ""\n                                 ""version %s or later."", min_niftynet)\n\n    @staticmethod\n    def _is_update_required(current_config, remote_config):\n        """"""\n        If no version information locally,\n        then update only if version information is specified remotely\n        We are assuming that this is overridden\n        by the case of no local information at all\n        """"""\n\n        if \'version\' not in current_config:\n            return \'version\' in remote_config\n\n        return LooseVersion(current_config[\'version\']) < \\\n               LooseVersion(remote_config[\'version\'])\n\n    def _download(self, remote_config_sections, example_id):\n        for section_name, config_params in remote_config_sections.items():\n            if \'action\' in config_params:\n                action = config_params.get(\'action\').lower()\n                if action == \'expand\':\n                    if \'url\' not in config_params:\n                        raise ValueError(\'No URL was found in the download \'\n                                         \'configuration file\')\n                    local_download_path = self._get_local_download_path(\n                        config_params, example_id)\n                    download_and_decompress(\n                        url=config_params[\'url\'],\n                        download_path=local_download_path,\n                        verbose=self._verbose)\n                    print(\'{} -- {}: OK.\'.format(example_id, section_name))\n                    print(""Downloaded data to "" + local_download_path)\n                else:\n                    print(example_id + "": FAIL."")\n                    print(""I do not know the action "" + action +\n                          "". Perhaps you need to update NiftyNet?"")\n\n    def _get_local_download_path(self, remote_config, example_id):\n        destination = remote_config.get(\'destination\', \'examples\')\n        local_id = remote_config.get(\'local_id\', example_id)\n        return os.path.join(self._download_folder, destination, local_id)\n\n    def _replace_local_with_remote(self, example_id):\n        local_filename = self._local.get_local_path(example_id)\n        remote_filename = self._remote.get_local_path(example_id)\n        shutil.copyfile(remote_filename, local_filename)\n\n    def _are_data_missing(self, remote_config_sections, example_id):\n        for _, config_params in remote_config_sections.items():\n            if \'action\' in config_params:\n                action = config_params.get(\'action\').lower()\n                if action == \'expand\':\n                    local_download_path = self._get_local_download_path(\n                        config_params, example_id)\n                    if not os.path.isdir(local_download_path):\n                        return True\n\n                    non_system_files = [\n                        f for f in os.listdir(local_download_path)\n                        if not f.startswith(\'.\')]\n                    if not non_system_files:\n                        return True\n        return False\n\n\nclass ConfigStoreCache(object):\n    """"""\n    A local cache for configuration files\n    """"""\n\n    def __init__(self, cache_folder):\n        self._cache_folder = cache_folder\n        if not os.path.exists(self._cache_folder):\n            os.makedirs(self._cache_folder)\n\n    def exists(self, example_id):\n        """"""\n        Returns True if a record exists for this example_id,\n        either locally or remotely\n        """"""\n\n        return os.path.isfile(self.get_local_path(example_id))\n\n    def get_local_path(self, example_id):\n        """"""\n        Returns the full path to the locally cached configuration file\n        """"""\n\n        return os.path.join(self._cache_folder,\n                            example_id + \'_main\'+ CONFIG_FILE_EXT)\n        # return os.path.join(self._cache_folder, example_id + CONFIG_FILE_EXT)\n\n    def get_local_cache_folder(self):\n        """"""\n        Returns the folder in which the cached files are stored\n        """"""\n\n        return self._cache_folder\n\n    def get_download_params(self, example_id):\n        """"""\n        Returns the local configuration file for this example_id\n        """"""\n        config_filename = self.get_local_path(example_id)\n\n        parser = NiftyNetLaunchConfig()\n        parser.read(config_filename)\n        if parser.has_section(\'config\'):\n            config_section = dict(parser.items(\'config\'))\n        else:\n            config_section = {}\n\n        other_sections = {}\n        for section in parser.sections():\n            if section != \'config\' and section != \'DEFAULT\':\n                other_sections[section] = dict(parser.items(section))\n        return config_section, other_sections\n\n\nclass RemoteProxy(object):\n    """"""\n    A remote configuration file store with a local cache\n    """"""\n\n    def __init__(self, parent_store_folder, base_url):\n        self._cache = ConfigStoreCache(\n            os.path.join(parent_store_folder, \'.downloads_remote_config_cache\'))\n        self._remote = RemoteConfigStore(base_url)\n\n    def exists(self, example_id):\n        """"""\n        Returns True if a record exists locally or remotely\n        """"""\n\n        return self._cache.exists(example_id) or self._remote.exists(example_id)\n\n    def update(self, example_id):\n        """"""\n        Retrieves the latest record from the remote store and\n        puts locally into the remote cache\n        """"""\n\n        download_file(self._remote.get_url(example_id),\n                      self._cache.get_local_path(example_id))\n\n    def get_download_params(self, example_id):\n        """"""\n        Returns the local configuration file for this example_id\n        """"""\n\n        return self._cache.get_download_params(example_id)\n\n    def get_local_path(self, example_id):\n        """"""\n        Returns the full path to the locally cached configuration file\n        """"""\n\n        return self._cache.get_local_path(example_id)\n\n\nclass RemoteConfigStore(object):\n    """"""\n    A remote configuration file store\n    """"""\n\n    def __init__(self, base_url):\n        self._base_url = base_url\n\n    def exists(self, example_id):\n        """"""\n        Returns true if the record exists on the remote server\n        """"""\n\n        return url_exists(self.get_url(example_id))\n\n    def get_url(self, example_id):\n        """"""\n        Gets the URL for the record for this example_id\n        """"""\n        return raw_file_url(self._base_url, example_id)\n\n\ndef raw_file_url(base_url, example_id=None):\n    """"""\n    Returns the url for the raw file on a GitLab server\n    """"""\n    _branch_name = \'5-reorganising-with-lfs\'\n\n    if not example_id:\n        return \'{}/raw/{}/README.md\'.format(base_url, _branch_name)\n    example_id = re.sub(\'_model_zoo\', \'\', example_id, 1)\n    return \'{}/raw/{}/{}/main{}\'.format(\n        base_url, _branch_name, example_id, CONFIG_FILE_EXT)\n    # return base_url + \'/raw/new_dataset_api/\' + file_name\n    # return base_url + \'/raw/master/\' + file_name\n    # return base_url + \'/raw/revising-config/\' + file_name\n\n\n# pylint: disable=broad-except\ndef url_exists(url):\n    """"""\n    Returns true if the specified url exists, without any redirects\n    """"""\n\n    try:\n        connection = urlopen(url)\n        return connection.getcode() < 400\n    except Exception:\n        return False\n\n\ndef progress_bar_wrapper(count, block_size, total_size):\n    """"""\n    Uses the common progress bar in the urlretrieve hook format\n    """"""\n    if block_size * 5 >= total_size:\n        # no progress bar for tiny files\n        return\n    print_progress_bar(\n        iteration=count,\n        total=math.ceil(float(total_size) / float(block_size)),\n        prefix=""Downloading (total: %.2f M): "" % (total_size * 1.0 / 1e6))\n\n\ndef main():\n    """"""\n    Launch download process with user-specified options.\n\n    :return:\n    """"""\n    arg_parser = argparse.ArgumentParser(\n        description=""Download NiftyNet sample data"")\n    arg_parser.add_argument(\n        ""-r"", ""--retry"",\n        help=""Force data to be downloaded again"",\n        required=False,\n        action=\'store_true\')\n    arg_parser.add_argument(\n        \'sample_id\',\n        nargs=\'+\',\n        help=""Identifier string(s) for the example(s) to download"")\n    version_string = get_niftynet_version_string()\n    arg_parser.add_argument(\n        ""-v"", ""--version"",\n        action=\'version\',\n        version=version_string)\n    args = arg_parser.parse_args()\n\n    if not download(args.sample_id, args.retry):\n        return -1\n\n    return 0\n\n\nif __name__ == ""__main__"":\n    main()\n'"
niftynet/utilities/filename_matching.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nMatching file names by configuration options.\n""""""\nfrom __future__ import absolute_import, print_function\n\nimport os\nimport re\n\nimport six\n\nimport niftynet.io.misc_io as util\nimport tensorflow as tf\n\n\nclass KeywordsMatching(object):\n    """"""\n    This class is responsible for the search of the appropriate files to use\n    as input based on the constraints given in the config file.\n    """"""\n\n    def __init__(self,\n                 list_paths=(),\n                 list_contain=(),\n                 list_not_contain=(),\n                 regex_remove=()):\n        self.path_to_search = list_paths\n        self.filename_contains = list_contain\n        self.filename_not_contains = list_not_contain\n        self.filename_toremove_fromid = regex_remove\n\n    @classmethod\n    def from_dict(cls, input_dict, default_folder=None):\n        """"""\n        In the config file, constraints for a given search can be of three\n        types:\n        ``path_to_search``, ``filename_contains`` and\n        ``filename_not_contains``. Each associated value is a string.\n        Multiple constraints are delimited by a ``,``.\n        This function creates the corresponding matching object with the list\n        of constraints for each of these subtypes.\n\n        :param default_folder: relative paths are first tested against\n            the current folder, and then against this default folder.\n        :param input_dict: set of searching parameters.\n        :return:\n        """"""\n        path, contain, not_contain, regex = [], (), (), ()\n        for (name, value) in input_dict.items():\n            if not value:\n                continue\n            if name == ""path_to_search"":\n                try:\n                    # for a string of comma separated path.\n                    value = value.split(\',\')\n                except AttributeError:\n                    pass\n\n                for path_i in value:\n                    path_i = path_i.strip()\n                    path_orig = os.path.abspath(os.path.expanduser(path_i))\n                    if os.path.exists(path_orig):\n                        path.append(path_orig)\n                        continue\n\n                    if not default_folder:\n                        tf.logging.fatal(\n                            \'data input folder ""%s"" not found, did you maybe \'\n                            \'forget to download data?\', path_i)\n                        raise ValueError\n                    path_def = os.path.join(default_folder, path_i)\n                    path_def = os.path.abspath(path_def)\n                    if not os.path.exists(path_def):\n                        tf.logging.fatal(\n                            \'data input folder ""%s"" not found, did you maybe \'\n                            \'forget to download data?\', path_i)\n                        raise ValueError\n                    path.append(path_def)\n\n            elif name == ""filename_contains"":\n                contain = tuple(set(value)) \\\n                    if not isinstance(value, six.string_types) \\\n                    else tuple([value])\n            elif name == ""filename_not_contains"":\n                not_contain = tuple(set(value)) \\\n                    if not isinstance(value, six.string_types) \\\n                    else tuple([value])\n            elif name == ""filename_removefromid"":\n                regex = tuple(set(value)) \\\n                    if not isinstance(value, six.string_types) \\\n                    else tuple([value])\n        path = tuple(set(path))\n        new_matcher = cls(path, contain, not_contain, regex[0] if regex else """")\n        return new_matcher\n\n    def matching_subjects_and_filenames(self):\n        """"""\n        This function perform the search of the relevant files (stored in\n        filename_list) and extract\n        the corresponding possible list of subject names (stored in\n        subjectname_list).\n\n        :return: filename_list, subjectname_list\n        """"""\n        path_file = [\n            (p, filename) for p in self.path_to_search\n            for filename in sorted(os.listdir(p))]\n        matching_path_file = list(filter(self.__is_a_candidate, path_file))\n        filename_list = \\\n            [os.path.join(p, filename) for p, filename in matching_path_file]\n        subjectname_list = [self.__extract_subject_id_from(filename)\n            for p, filename in matching_path_file]\n        for sname, fname in zip(subjectname_list, filename_list):\n            if not sname:\n                subjectname_list.remove(sname)\n                filename_list.remove(fname)\n        self.__check_unique_names(filename_list, subjectname_list)\n        if not filename_list or not subjectname_list:\n            tf.logging.fatal(\'no file matched based on this matcher: %s\', self)\n            raise ValueError\n        return filename_list, subjectname_list\n\n    def __is_a_candidate(self, x):\n        all_pos_match = all(c in x[1] for c in self.filename_contains)\n        all_neg_match = not any(c in x[1] for c in self.filename_not_contains)\n        return all_pos_match and all_neg_match\n\n    def __extract_subject_id_from(self, fullname):\n        """"""\n        This function returns a list of potential subject names from a given\n        filename, knowing the imposed constraints. Constraints strings are\n        removed from the filename to provide the list of possible names. If\n        after reduction of the filename from the constraints the name is\n        empty the initial filename is returned.\n        if remove is not empty, will remove only the strings indicated in\n        remove. Otherwise, by default will remove all those in filename_contains\n\n        :param fullname:\n        :return name_pot: list of potential subject name given the constraint\n         list and the initial filename\n        """"""\n        _, name, _ = util.split_filename(fullname)\n        # split name into parts that might be the subject_id\n        potential_names = [name]\n        if not self.filename_toremove_fromid:\n            # \xc2\xa0regular expression not specified,\n            #   removing the matched file_contains keywords\n            #   use the rest of the string as subject id\n            noncapturing_regex_delimiters = \\\n                [\'(?:{})\'.format(re.escape(c)) for c in self.filename_contains]\n            if noncapturing_regex_delimiters:\n                potential_names = re.split(\n                    \'|\'.join(noncapturing_regex_delimiters), name)\n            # filter out non-alphanumeric characters and blank strings\n            potential_names = [\n                re.sub(r\'\\W+\', \'\', name) for name in potential_names]\n        else:\n            potential_names = [\n                re.sub(self.filename_toremove_fromid, """", name)]\n        potential_names = list(filter(bool, potential_names))\n        if len(potential_names) > 1:\n            potential_names.append(\'\'.join(potential_names))\n        return potential_names\n\n    def __check_unique_names(self, file_list, id_list):\n        uniq_dict = dict()\n        for idx, subject_id in enumerate(id_list):\n            if not subject_id:\n                continue\n            id_string = subject_id[0]\n            if id_string in uniq_dict:\n                tf.logging.fatal(\n                    \'extracted the same unique_id ""%s"" from \'\n                    \'filenames ""%s"" and ""%s"", using matcher: %s\',\n                    id_string, uniq_dict[id_string], file_list[idx], self)\n                raise ValueError\n            uniq_dict[id_string] = file_list[idx]\n\n    def __str__(self):\n        return self.to_string()\n\n    def to_string(self):\n        """"""\n        Formatting the class as an intuitive string for printing.\n\n        :return:\n        """"""\n        summary_str = \'\\n\\nMatch file names and extract subject_ids from: \\n\'\n        if self.path_to_search:\n            summary_str += \'-- path to search: {}\\n\'.format(self.path_to_search)\n        if self.filename_contains:\n            summary_str += \'-- filename contains: {}\\n\'.format(\n                self.filename_contains)\n        if self.filename_not_contains:\n            summary_str += \'-- filename not contains: {}\\n\'.format(\n                self.filename_not_contains)\n        if self.filename_toremove_fromid:\n            summary_str += \'-- filename to remove from id: {}\\n\'.format(\n                self.filename_toremove_fromid)\n        return summary_str\n'"
niftynet/utilities/histogram_standardisation.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nImplementation of\nNy\xc3\xbal L\xc3\xa1szl\xc3\xb3 G., Jayaram K. Udupa, and Xuan Zhang.\n""New variants of a method of MRI scale standardization.""\nIEEE transactions on medical imaging 19.2 (2000): 143-150.\n\nThis implementation only supports input images with floating point number,\n(not integers).\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport os\n\nimport numpy as np\nimport numpy.ma as ma\nimport tensorflow as tf\n\nfrom niftynet.io.misc_io import touch_folder\nfrom niftynet.utilities.util_common import \\\n    look_up_operations, print_progress_bar\n\nDEFAULT_CUTOFF = [0.01, 0.99]\nSUPPORTED_CUTPOINTS = set([\'percentile\', \'quartile\', \'median\'])\n\n\ndef __compute_percentiles(img, mask, cutoff):\n    """"""\n    Creates the list of percentile values to be used as landmarks for the\n    linear fitting.\n\n    :param img: Image on which to determine the percentiles\n    :param mask: Mask to use over the image to constraint to the relevant\n    information\n    :param cutoff: Values of the minimum and maximum percentiles to use for\n    the linear fitting\n    :return perc_results: list of percentiles value for the given image over\n    the mask\n    """"""\n    perc = [cutoff[0],\n            0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9,\n            cutoff[1]]\n    masked_img = ma.masked_array(img, np.logical_not(mask)).compressed()\n    perc_results = np.percentile(masked_img, 100 * np.array(perc))\n    # hist, bin = np.histogram(ma.compressed(masked_img), bins=50)\n    return perc_results\n\n\ndef __standardise_cutoff(cutoff, type_hist=\'quartile\'):\n    """"""\n    Standardises the cutoff values given in the configuration\n\n    :param cutoff:\n    :param type_hist: Type of landmark normalisation chosen (median,\n    quartile, percentile)\n    :return cutoff: cutoff with appropriate adapted values\n    """"""\n    cutoff = np.asarray(cutoff)\n    if cutoff is None:\n        return DEFAULT_CUTOFF\n    if len(cutoff) > 2:\n        cutoff = np.unique([np.min(cutoff), np.max(cutoff)])\n    if len(cutoff) < 2:\n        return DEFAULT_CUTOFF\n    if cutoff[0] > cutoff[1]:\n        cutoff[0], cutoff[1] = cutoff[1], cutoff[0]\n    cutoff[0] = max(0., cutoff[0])\n    cutoff[1] = min(1., cutoff[1])\n    if type_hist == \'quartile\':\n        cutoff[0] = np.min([cutoff[0], 0.24])\n        cutoff[1] = np.max([cutoff[1], 0.76])\n    else:\n        cutoff[0] = np.min([cutoff[0], 0.09])\n        cutoff[1] = np.max([cutoff[1], 0.91])\n    return cutoff\n\n\ndef create_mapping_from_multimod_arrayfiles(array_files,\n                                            field,\n                                            modalities,\n                                            mod_to_train,\n                                            cutoff,\n                                            masking_function):\n    """"""\n    Performs the mapping creation based on a list of files. For each of the\n    files (potentially multimodal), the landmarks are defined for each\n    modality and stored in a database. The average of these landmarks is\n    returned providing the landmarks to use for the linear mapping of any\n    new incoming data\n\n    :param array_files: List of image files to use\n    :param modalities: Name of the modalities used for the\n        standardisation and the corresponding order in the multimodal files\n    :param cutoff: Minimum and maximum landmarks percentile values to use for\n        the mapping\n    :param masking_function: Describes how the mask is defined for each image.\n    :return:\n    """"""\n    perc_database = {}\n    for (i, p) in enumerate(array_files):\n        print_progress_bar(i, len(array_files),\n                           prefix=\'normalisation histogram training\',\n                           decimals=1, length=10, fill=\'*\')\n        img_data = p[field].get_data()\n        assert img_data.shape[4] == len(modalities), \\\n            ""number of modalities are not consistent in the input image""\n        for mod_i, m in enumerate(modalities):\n            if m not in mod_to_train:\n                continue\n            if m not in perc_database.keys():\n                perc_database[m] = []\n            for t in range(img_data.shape[3]):\n                img_3d = img_data[..., t, mod_i]\n                if masking_function is not None:\n                    mask_3d = masking_function(img_3d)\n                else:\n                    mask_3d = np.ones_like(img_3d, dtype=np.bool)\n                perc = __compute_percentiles(img_3d, mask_3d, cutoff)\n                perc_database[m].append(perc)\n    mapping = {}\n    for m in list(perc_database):\n        perc_database[m] = np.vstack(perc_database[m])\n        s1, s2 = create_standard_range()\n        mapping[m] = tuple(__averaged_mapping(perc_database[m], s1, s2))\n    return mapping\n\n\ndef create_standard_range():\n    return 0., 100.\n\n\ndef __averaged_mapping(perc_database, s1, s2):\n    """"""\n    Map the landmarks of the database to the chosen range\n    :param perc_database: perc_database over which to perform the averaging\n    :param s1, s2: limits of the mapping range\n    :return final_map: the average mapping\n    """"""\n    # assuming shape: n_data_points = perc_database.shape[0]\n    #                 n_percentiles = perc_database.shape[1]\n    slope = (s2 - s1) / (perc_database[:, -1] - perc_database[:, 0])\n    slope = np.nan_to_num(slope)\n    final_map = slope.dot(perc_database) / perc_database.shape[0]\n    intercept = np.mean(s1 - slope * perc_database[:, 0])\n    final_map = final_map + intercept\n    return final_map\n\n\ndef transform_by_mapping(img, mask, mapping, cutoff, type_hist=\'quartile\'):\n    """"""\n    Performs the standardisation of a given image.\n\n    :param img: image to standardise\n    :param mask: mask over which to determine the landmarks\n    :param mapping: mapping landmarks to use for the piecewise linear\n        transformations\n    :param cutoff: cutoff points for the mapping\n    :param type_hist: Type of landmarks scheme to use: choice between\n        quartile percentile and median\n    :return new_img: the standardised image\n    """"""\n    image_shape = img.shape\n    img = img.reshape(-1)\n    mask = mask.reshape(-1)\n\n    type_hist = look_up_operations(type_hist.lower(), SUPPORTED_CUTPOINTS)\n    if type_hist == \'quartile\':\n        range_to_use = [0, 3, 6, 9, 12]\n    elif type_hist == \'percentile\':\n        range_to_use = [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12]\n    elif type_hist == \'median\':\n        range_to_use = [0, 6, 12]\n    else:\n        raise ValueError(\'unknown cutting points type_str\')\n    assert len(mapping) >= len(range_to_use), \\\n        ""wrong mapping format, please check the histogram reference file""\n    mapping = np.asarray(mapping)\n    cutoff = __standardise_cutoff(cutoff, type_hist)\n    perc = __compute_percentiles(img, mask, cutoff)\n    # Apply linear histogram standardisation\n    range_mapping = mapping[range_to_use]\n    range_perc = perc[range_to_use]\n    diff_mapping = range_mapping[1:] - range_mapping[:-1]\n    diff_perc = range_perc[1:] - range_perc[:-1]\n\n    # handling the case where two landmarks are the same\n    # for a given input image.  This usually happens when\n    # image background are not removed from the image.\n    diff_perc[diff_perc == 0] = np.inf\n\n    affine_map = np.zeros([2, len(range_to_use) - 1])\n    # compute slopes of the linear models\n    affine_map[0] = diff_mapping / diff_perc\n    # compute intercepts of the linear models\n    affine_map[1] = range_mapping[:-1] - affine_map[0] * range_perc[:-1]\n\n    bin_id = np.digitize(img, range_perc[1:-1], right=False)\n    lin_img = affine_map[0, bin_id]\n    aff_img = affine_map[1, bin_id]\n    # handling below cutoff[0] over cutoff[1]\n    # values are mapped linearly and then smoothed\n    new_img = lin_img * img + aff_img\n\n    # Apply smooth thresholding (exponential)\n    # below cutoff[0] and over cutoff[1]\n    # this might not guarantee one to one mapping\n\n    # lowest_values = img <= range_perc[0]\n    # highest_values = img >= range_perc[-1]\n    # new_img[lowest_values] = smooth_threshold(\n    #     new_img[lowest_values], mode=\'low\')\n    # new_img[highest_values] = smooth_threshold(\n    #     new_img[highest_values], mode=\'high\')\n\n    # Apply mask and set background to zero\n    # new_img[mask == False] = 0.\n    new_img = new_img.reshape(image_shape)\n    return new_img\n\n\ndef smooth_threshold(value, mode=\'high\'):\n    smoothness = 1.\n    if mode == \'high\':\n        affine = np.min(value)\n        smooth_value = (value - affine) / smoothness\n        smooth_value = (1. - np.exp((-1) * smooth_value)) + affine\n    elif mode == \'low\':\n        affine = np.max(value)\n        smooth_value = (value - affine) / smoothness\n        smooth_value = (np.exp(smooth_value) - 1.) + affine\n    else:\n        smooth_value = value\n    return smooth_value\n\n\ndef read_mapping_file(mapping_file):\n    """"""\n    Reads an existing mapping file with the given modalities.\n    :param mapping_file: file in which mapping is stored\n    :return mapping_dict: dictionary containing the mapping landmarks for\n    each modality stated in the mapping file\n    """"""\n    mapping_dict = {}\n    if not mapping_file:\n        return mapping_dict\n    if not os.path.isfile(mapping_file):\n        return mapping_dict\n\n    with open(mapping_file, ""r"") as f:\n        for line in f:\n            if len(line) <= 2:\n                continue\n            line = line.split()\n            if len(line) < 2:\n                continue\n            try:\n                map_name, map_value = line[0], np.float32(line[1:])\n                mapping_dict[map_name] = tuple(map_value)\n            except ValueError:\n                tf.logging.fatal(\n                    ""unknown input format: {}"".format(mapping_file))\n                raise\n    return mapping_dict\n\n\n# Function to modify the model file with the mapping if needed according\n# to existent mapping and modalities\ndef write_all_mod_mapping(hist_model_file, mapping):\n    # backup existing file first\n    if os.path.exists(hist_model_file):\n        backup_name = \'{}.backup\'.format(hist_model_file)\n        from shutil import copyfile\n        try:\n            copyfile(hist_model_file, backup_name)\n        except OSError:\n            tf.logging.warning(\'cannot backup file {}\'.format(hist_model_file))\n            raise\n        tf.logging.warning(\n            ""moved existing histogram reference file\\n""\n            "" from {} to {}"".format(hist_model_file, backup_name))\n\n    touch_folder(os.path.dirname(hist_model_file))\n    __force_writing_new_mapping(hist_model_file, mapping)\n\n\ndef __force_writing_new_mapping(filename, mapping_dict):\n    """"""\n    Writes a mapping dictionary to file\n\n    :param filename: name of the file in which to write the saved mapping\n    :param mapping_dict: mapping dictionary to save in the file\n    :return:\n    """"""\n    with open(filename, \'w+\') as f:\n        for mod in mapping_dict.keys():\n            mapping_string = \' \'.join(map(str, mapping_dict[mod]))\n            string_fin = \'{} {}\\n\'.format(mod, mapping_string)\n            f.write(string_fin)\n    return\n'"
niftynet/utilities/niftynet_global_config.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nNiftyNet user folder configuration\n""""""\n\nimport sys\nfrom random import choice\nfrom string import ascii_lowercase\nfrom time import strftime\n\nimport os\nfrom os.path import expanduser, join, split, isdir, isfile, splitext\n\n# pylint: disable=wrong-import-order\ntry:\n    from configparser import (ConfigParser, Error)\nexcept ImportError:\n    from ConfigParser import (ConfigParser, Error)\nfrom niftynet.utilities.decorators import singleton\n\nCONFIG_HOME_VAR = \'niftynet_config_home\'\nMODEL_ZOO_VAR = \'niftynet_model_zoo\'\n# SERVER_URL = \'https://cmiclab.cs.ucl.ac.uk/CMIC/NiftyNetExampleServer\'\nSERVER_URL = \'https://github.com/NifTK/NiftyNetModelZoo\'\n\n\n@singleton\nclass NiftyNetGlobalConfig(object):\n    """"""Global configuration settings""""""\n\n    global_section = \'global\'\n    home_key = \'home\'\n\n    niftynet_exts = {\'extensions\': [\'network\']}\n\n    def __init__(self):\n        self._download_server_url = SERVER_URL\n        self._config_home = join(expanduser(\'~\'), \'.niftynet\')\n\n        try:\n            if os.environ[MODEL_ZOO_VAR]:\n                self._download_server_url = os.environ[MODEL_ZOO_VAR]\n        except KeyError:\n            pass\n\n        try:\n            if os.environ[CONFIG_HOME_VAR]:\n                self._config_home = os.environ[CONFIG_HOME_VAR]\n        except KeyError:\n            pass\n\n        self._config_file = join(self._config_home, \'config.ini\')\n\n        self._niftynet_home = self._config_home\n\n        self.setup()\n\n    def setup(self):\n        """"""\n        Read variables from system environment and make directories.\n\n        :return:\n        """"""\n        config_opts = NiftyNetGlobalConfig.__load_or_create(self._config_file)\n        self._niftynet_home = expanduser(\n            config_opts[NiftyNetGlobalConfig.global_section][\n                NiftyNetGlobalConfig.home_key])\n\n        if not isdir(self._niftynet_home):\n            os.makedirs(self._niftynet_home)\n\n            # create folders for user-defined extensions such as new networks\n            for ext in list(NiftyNetGlobalConfig.niftynet_exts):\n                extension_subfolder = join(self._niftynet_home, ext)\n                NiftyNetGlobalConfig.__create_module(extension_subfolder)\n                for mod in NiftyNetGlobalConfig.niftynet_exts[ext]:\n                    extension_subsubfolder = join(self._niftynet_home, ext, mod)\n                    NiftyNetGlobalConfig.__create_module(extension_subsubfolder)\n\n        for ext in list(NiftyNetGlobalConfig.niftynet_exts):\n            extension_subfolder = join(self._niftynet_home, ext)\n            sys.path.insert(1, extension_subfolder)\n        sys.path.insert(1, self._niftynet_home)\n        return self\n\n    @staticmethod\n    def __create_module(path):\n        """"""Create the passed path, i.e. folder and place an empty\n        ``__init__.py`` file inside.\n\n        :param path: assumed not to exist\n        :type path: `os.path`\n        """"""\n        os.makedirs(path)\n        open(join(path, \'__init__.py\'), \'a\').close()\n\n    @staticmethod\n    def __load_or_create(config_file):\n        """"""Load passed configuration file, if it exists; create a default\n        otherwise. If this method finds an incorrect config file, it\n        backs the file up with a human-readable timestamp suffix and\n        creates a default one.\n\n        :param config_file: no sanity checks are performed, as this\n            method is for internal use only\n        :type config_file: `os.path`\n        :returns: a dictionary of parsed configuration options\n        :rtype: `dict`\n        """"""\n        required_sections = [NiftyNetGlobalConfig.global_section]\n        required_keys = {\n            required_sections[0]: [NiftyNetGlobalConfig.home_key]\n        }\n        default_values = {\n            required_sections[0]: {\n                NiftyNetGlobalConfig.home_key: \'~/niftynet\'\n            }\n        }\n\n        backup = False\n        if isfile(config_file):\n            try:\n                config = ConfigParser()\n                config.read(config_file)\n\n                # check all required sections and keys present\n                for required_section in required_sections:\n                    if required_section not in config:\n                        backup = True\n                        break\n\n                    for required_key in required_keys[required_section]:\n                        if required_key not in config[required_section]:\n                            backup = True\n                            break\n\n                    if backup:\n                        break\n\n            except Error:\n                backup = True\n\n            if not backup:  # loaded file contains all required\n                # config options: so return\n                return dict(config)\n\n        config_dir, config_filename = split(config_file)\n        if not isdir(config_dir):\n            os.makedirs(config_dir)\n\n        if backup:  # config file exists, but does not contain all required\n            # config opts: so backup not to override\n            timestamp = strftime(\'%Y-%m-%d-%H-%M-%S\')\n            random_str = \'\'.join(choice(ascii_lowercase) for _ in range(3))\n            backup_suffix = \'-\'.join([\'backup\', timestamp, random_str])\n\n            filename, extension = splitext(config_filename)\n            backup_filename = \'\'.join([filename, \'-\', backup_suffix, extension])\n            backup_file = join(config_dir, backup_filename)\n            os.rename(config_file, backup_file)\n\n        # create a new default global config file\n        config = ConfigParser(default_values)\n        for required_section in required_sections:\n            for required_key in required_keys[required_section]:\n                config.add_section(required_section)\n                config[required_section][required_key] = \\\n                    default_values[required_section][required_key]\n        with open(config_file, \'w\') as new_config_file:\n            config.write(new_config_file)\n        return dict(config)\n\n    def get_niftynet_home_folder(self):\n        """"""Return the folder containing NiftyNet models and data""""""\n        return self._niftynet_home\n\n    def get_niftynet_config_folder(self):\n        """"""Return the folder containing NiftyNet global configuration""""""\n        return self._config_home\n\n    def get_default_examples_folder(self):\n        """"""Return the default folder containing NiftyNet examples""""""\n        return join(self._niftynet_home, \'examples\')\n\n    def get_download_server_url(self):\n        """"""Return the URL to the NiftyNet examples server""""""\n        return self._download_server_url\n'"
niftynet/utilities/niftynet_launch_config.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nNiftyNet launch configuration\n""""""\n\n\ntry:\n    import ConfigParser as configparser\nexcept ImportError:\n    import configparser\n\n\nclass NiftyNetLaunchConfig(configparser.ConfigParser):\n    """"""Launch configuration settings""""""\n\n    pass\n'"
niftynet/utilities/restore_initializer.py,4,"b'# -*- coding: utf-8 -*-\r\n\r\n# Copyright 2018 The Sonnet Authors. All Rights Reserved.\r\n# Modifications copyright 2018 The NiftyNet Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#    http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ============================================================================\r\n\r\n""""""A checkpoint-restoring Tensorflow initializer.""""""\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import init_ops\r\nfrom tensorflow.python.ops import io_ops\r\n\r\n\r\n# Dependency imports\r\n\r\n\r\nclass _Restore(init_ops.Initializer):\r\n    """"""Initializer that restores tensors from a checkpoint.""""""\r\n\r\n    def __init__(self, filename, var_name, scope=None):\r\n        """"""Construct a new restoring initializer.\r\n        Will read from the checkpoint from the SSTables file `filename` using\r\n        the RestoreV2 Tensorflow op.\r\n        The actual variable read from the checkpoint will be\r\n        `scope_name` + \'/\' + `var_name` (or just `var_name` if `scope_name` is\r\n        empty), where `scope_name` is given by one of\r\n        (1) The current scope\'s name at the point where the initializer gets called,\r\n            if the `scope` argument to this constructor is None,\r\n        (2) If `scope` is callable, the result of applying it to the current scope\'s\r\n            name,\r\n        (3) Otherwise, the `scope` argument to this constructor itself.\r\n        Args:\r\n          filename: Name of an SSTables entry where the checkpoint is hosted.\r\n          var_name: Name of the variable to restore.\r\n          scope: The variable scope\'s name of the variable to restore, see above.\r\n        """"""\r\n        self._filename = filename\r\n        self._var_name = var_name\r\n        self._scope = scope\r\n\r\n    def _partition_spec(self, shape, partition_info):\r\n        """"""Build magic (and sparsely documented) shapes_and_slices spec string.""""""\r\n        if partition_info is None:\r\n            return \'\'  # Empty string indicates a non-partitioned tensor.\r\n        ssi = tf.Variable.SaveSliceInfo(\r\n            full_name=self._var_name,\r\n            full_shape=partition_info.full_shape,\r\n            var_offset=partition_info.var_offset,\r\n            var_shape=shape)\r\n        return ssi.spec\r\n\r\n    def __call__(self, shape, dtype=None, partition_info=None):\r\n        # Creating different RestoreV2 ops when a single one could\r\n        # output several tensors seems inefficient, but that\'s actually\r\n        # what tf.Saver.restore_op (via tf.BaseSaverBuilder) does too.\r\n        if self._scope is None:\r\n            scope_name = tf.get_variable_scope().name\r\n        elif callable(self._scope):\r\n            scope_name = self._scope(tf.get_variable_scope().name)\r\n        else:\r\n            scope_name = self._scope\r\n        tensor_name = self._var_name\r\n        if scope_name:\r\n            tensor_name = \'{}/{}\'.format(scope_name, tensor_name)\r\n        tensor = io_ops.restore_v2(\r\n            self._filename,\r\n            [tensor_name],\r\n            [self._partition_spec(shape, partition_info)],\r\n            [dtype])[0]\r\n        tensor.set_shape(shape)\r\n        return tensor\r\n\r\n\r\n# pylint: disable=invalid-name\r\nrestore_initializer = _Restore\r\n# pylint: enable=invalid-name\r\n'"
niftynet/utilities/user_parameters_custom.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines task specific parameters\n""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nfrom niftynet.utilities.user_parameters_helper import (add_input_name_args,\n                                                       int_array, str2boolean)\n\n\n#######################################################################\n# To support a CUSTOM_SECTION in config file:\n# (e.g., MYTASK; in parallel with SEGMENTATION, REGRESSION etc.)\n#\n# 1) update niftynet.utilities.user_parameters_custom.SUPPORTED_ARG_SECTIONS\n# with a key-value pair:\n# where the key should be MYTASK, a standardised string --\n# Standardised string is defined in\n# niftynet.utilities.user_parameters_helper.standardise_string\n# the section name will be filtered with,\n# re.sub(\'[^0-9a-zA-Z_\\- ]+\', \'\', input_string.strip())\n#\n# the value should be __add_mytask_args()\n#\n# 2) create a function __add_mytask_args() with task specific arguments\n# this function should return an argparse obj\n#\n# 3) in the application file, specify:\n# `REQUIRED_CONFIG_SECTION = ""MYTASK""`\n# so that the application will have access to the task specific arguments\n#########################################################################\n\n\ndef add_customised_args(parser, task_name):\n    """"""\n    loading keywords arguments to parser by task name\n    :param parser:\n    :param task_name: supported choices are listed in `SUPPORTED_ARG_SECTIONS`\n    :return: parser with updated actions\n    """"""\n    task_name = task_name.upper()\n    if task_name in SUPPORTED_ARG_SECTIONS:\n        return SUPPORTED_ARG_SECTIONS[task_name](parser)\n    raise NotImplementedError\n\n\ndef __add_regression_args(parser):\n    """"""\n    keywords defined for regression tasks\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--loss_border"",\n        metavar=\'\',\n        help=""Set the border size for the loss function to ignore"",\n        type=int,\n        default=0)\n\n    parser.add_argument(\n        ""--error_map"",\n        metavar=\'\',\n        help=""Set whether to output the regression error maps (the maps ""\n             ""will be stored in $model_dir/error_maps; the error maps ""\n             ""can be used for window sampling)."",\n        type=str2boolean,\n        default=False)\n\n    from niftynet.application.regression_application import SUPPORTED_INPUT\n    parser = add_input_name_args(parser, SUPPORTED_INPUT)\n    return parser\n\n\ndef __add_segmentation_args(parser):\n    """"""\n    keywords defined for segmentation tasks\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--num_classes"",\n        metavar=\'\',\n        help=""Set number of classes"",\n        type=int,\n        default=-1)\n\n    parser.add_argument(\n        ""--output_prob"",\n        metavar=\'\',\n        help=""[Inference only] whether to output multi-class probabilities"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--label_normalisation"",\n        metavar=\'\',\n        help=""whether to map unique labels in the training set to ""\n             ""consecutive integers (the smallest label will be  mapped to 0)"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--softmax"",\n        metavar=\'\',\n        help=""[Training only] whether to append a softmax layer to network ""\n             ""output before feeding it into loss function"",\n        type=str2boolean,\n        default=True)\n\n    # for selective sampling only\n    parser.add_argument(\n        ""--min_sampling_ratio"",\n        help=""[Training only] Minimum ratio of samples in a window for ""\n             ""selective sampler"",\n        metavar=\'\',\n        type=float,\n        default=0)\n\n    # for selective sampling only\n    parser.add_argument(\n        ""--compulsory_labels"",\n        help=""[Training only] List of labels to have in the window for ""\n             ""selective sampling"",\n        metavar=\'\',\n        type=int_array,\n        default=(0, 1))\n\n    # for selective sampling only\n    parser.add_argument(\n        ""--rand_samples"",\n        help=""[Training only] Number of completely random samples per image ""\n             ""when using selective sampler"",\n        metavar=\'\',\n        type=int,\n        default=0)\n\n    # for selective sampling only\n    parser.add_argument(\n        ""--min_numb_labels"",\n        help=""[Training only] Number of labels to have in the window for ""\n             ""selective sampler"",\n        metavar=\'\',\n        type=int,\n        default=1)\n\n    # for selective sampling only\n    parser.add_argument(\n        ""--proba_connect"",\n        help=""[Training only] Number of labels to have in the window for ""\n             ""selective sampler"",\n        metavar=\'\',\n        type=str2boolean,\n        default=True)\n\n    parser.add_argument(\n        ""--evaluation_units"",\n        help=""Compute per-component metrics for per label or per connected ""\n             ""component. [foreground, label, or cc]"",\n        choices=[\'foreground\', \'label\', \'cc\'],\n        default=\'foreground\')\n\n    #  for mixup augmentation\n    parser.add_argument(\n        ""--do_mixup"",\n        help=""Use the \'mixup\' option."",\n        type=str2boolean,\n        default=False)\n\n    #  for mixup augmentation\n    parser.add_argument(\n        ""--mixup_alpha"",\n        help=""The alpha value to parametrise the beta distribution ""\n             ""(alpha, alpha). Default: 0.2."",\n        type=float,\n        default=0.2)\n\n    #  for mixup augmentation\n    parser.add_argument(\n        ""--mix_match"",\n        help=""If true, matches bigger segmentations with ""\n             ""smaller segmentations."",\n        type=str2boolean,\n        default=False)\n\n    from niftynet.application.segmentation_application import SUPPORTED_INPUT\n    parser = add_input_name_args(parser, SUPPORTED_INPUT)\n    return parser\n\n\ndef __add_gan_args(parser):\n    """"""\n    keywords defined for GAN\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--noise_size"",\n        metavar=\'\',\n        help=""length of the noise vector"",\n        type=int,\n        default=-1)\n\n    parser.add_argument(\n        ""--n_interpolations"",\n        metavar=\'\',\n        help=""the method of generating window from image"",\n        type=int,\n        default=10)\n\n    from niftynet.application.gan_application import SUPPORTED_INPUT\n    parser = add_input_name_args(parser, SUPPORTED_INPUT)\n    return parser\n\n\ndef __add_classification_args(parser):\n    """"""\n    keywords defined for classification\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--num_classes"",\n        metavar=\'\',\n        help=""Set number of classes"",\n        type=int,\n        default=-1)\n\n    parser.add_argument(\n        ""--output_prob"",\n        metavar=\'\',\n        help=""[Inference only] whether to output multi-class probabilities"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--label_normalisation"",\n        metavar=\'\',\n        help=""whether to map unique labels in the training set to ""\n             ""consecutive integers (the smallest label will be  mapped to 0)"",\n        type=str2boolean,\n        default=False)\n\n    from niftynet.application.classification_application import SUPPORTED_INPUT\n    parser = add_input_name_args(parser, SUPPORTED_INPUT)\n    return parser\n\n\ndef __add_autoencoder_args(parser):\n    """"""\n    keywords defined for autoencoder\n\n    :param parser:\n    :return:\n    """"""\n    from niftynet.application.autoencoder_application import SUPPORTED_INFERENCE\n    parser.add_argument(\n        ""--inference_type"",\n        metavar=\'\',\n        help=""choose an inference type_str for the trained autoencoder"",\n        choices=list(SUPPORTED_INFERENCE))\n\n    parser.add_argument(\n        ""--noise_stddev"",\n        metavar=\'\',\n        help=""standard deviation of noise when inference type_str is sample"",\n        type=float)\n\n    parser.add_argument(\n        ""--n_interpolations"",\n        metavar=\'\',\n        help=""the method of generating window from image"",\n        type=int,\n        default=10)\n\n    from niftynet.application.autoencoder_application import SUPPORTED_INPUT\n    parser = add_input_name_args(parser, SUPPORTED_INPUT)\n    return parser\n\n\ndef __add_registration_args(parser):\n    """"""\n    keywords defined for image registration\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--label_normalisation"",\n        metavar=\'\',\n        help=""whether to map unique labels in the training set to ""\n             ""consecutive integers (the smallest label will be  mapped to 0)"",\n        type=str2boolean,\n        default=False)\n\n    from niftynet.application.label_driven_registration import SUPPORTED_INPUT\n    parser = add_input_name_args(parser, SUPPORTED_INPUT)\n    return parser\n\n\nSUPPORTED_ARG_SECTIONS = {\n    \'REGRESSION\': __add_regression_args,\n    \'SEGMENTATION\': __add_segmentation_args,\n    \'CLASSIFICATION\': __add_classification_args,\n    \'AUTOENCODER\': __add_autoencoder_args,\n    \'GAN\': __add_gan_args,\n    \'REGISTRATION\': __add_registration_args\n}\n'"
niftynet/utilities/user_parameters_default.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines niftynet parameters and their defaults.\n""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport os\n\nfrom niftynet.engine.image_window_dataset import SMALLER_FINAL_BATCH_MODE\nfrom niftynet.io.image_loader import SUPPORTED_LOADERS\nfrom niftynet.io.image_sets_partitioner import SUPPORTED_PHASES\nfrom niftynet.utilities.user_parameters_helper import (\n    float_array, int_array, spatial_atleast3d, spatialnumarray, str2boolean,\n    str_array)\nfrom niftynet.utilities.util_import import require_module\n\nDEFAULT_INFERENCE_OUTPUT = os.path.join(\'.\', \'output\')\nDEFAULT_EVALUATION_OUTPUT = os.path.join(\'.\', \'evaluation\')\nDEFAULT_DATASET_SPLIT_FILE = os.path.join(\'.\', \'dataset_split.csv\')\nDEFAULT_HISTOGRAM_REF_FILE = os.path.join(\'.\', \'histogram_ref_file.txt\')\nDEFAULT_MODEL_DIR = None\nDEFAULT_EVENT_HANDLERS = (\'model_saver\', \'model_restorer\', \'sampler_threading\',\n                          \'apply_gradients\', \'output_interpreter\',\n                          \'console_logger\', \'tensorboard_logger\',\n                          \'performance_logger\')\n\nDEFAULT_ITERATION_GENERATOR = \'iteration_generator\'\n\n\ndef add_application_args(parser):\n    """"""\n    Common keywords for all applications\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--cuda_devices"",\n        metavar=\'\',\n        help=""Set CUDA_VISIBLE_DEVICES variable, e.g. \'0,1,2,3\'; ""\n             ""leave blank to use the system default value"",\n        type=str,\n        default=\'\')\n\n    parser.add_argument(\n        ""--num_threads"",\n        help=""Set number of preprocessing threads"",\n        metavar=\'\',\n        type=int,\n        default=2)\n\n    parser.add_argument(\n        ""--num_gpus"",\n        help=""Set number of training GPUs"",\n        metavar=\'\',\n        type=int,\n        default=1)\n\n    parser.add_argument(\n        ""--model_dir"",\n        metavar=\'\',\n        help=""Directory to save/load intermediate training models and logs"",\n        default=DEFAULT_MODEL_DIR)\n\n    parser.add_argument(\n        ""--dataset_split_file"",\n        metavar=\'\',\n        help=""File assigning subjects to training/validation/inference subsets"",\n        default=DEFAULT_DATASET_SPLIT_FILE)\n\n    parser.add_argument(\n        ""--event_handler"",\n        metavar=\'\',\n        help=""String(s) representing event handler module(s)"",\n        type=str_array,\n        default=DEFAULT_EVENT_HANDLERS)\n\n    parser.add_argument(\n        ""--iteration_generator"",\n        metavar=\'\',\n        help=\'String representing an iteration generator class\',\n        type=str,\n        default=DEFAULT_ITERATION_GENERATOR)\n\n    return parser\n\n\ndef add_inference_args(parser):\n    """"""\n    keywords defined for inference action\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--spatial_window_size"",\n        type=spatial_atleast3d,\n        help=""Specify the spatial size of the input data (ndims <= 3)"",\n        default=())\n\n    parser.add_argument(\n        ""--inference_iter"",\n        metavar=\'\',\n        help=""[Inference only] Use the checkpoint at this iteration for ""\n             ""inference"",\n        type=int,\n        default=-1)\n\n    parser.add_argument(\n        ""--dataset_to_infer"",\n        metavar=\'\',\n        help=""[Inference only] which data set to compute inference for"",\n        choices=list(SUPPORTED_PHASES) + [\'\'],\n        default=\'\')\n\n    parser.add_argument(\n        ""--save_seg_dir"",\n        metavar=\'\',\n        help=""[Inference only] Prediction directory name"",  # without \'/\'\n        default=DEFAULT_INFERENCE_OUTPUT)\n\n    parser.add_argument(\n        ""--output_postfix"",\n        metavar=\'\',\n        help=""[Inference only] Prediction filename postfix"",\n        default=""_niftynet_out"")\n\n    parser.add_argument(\n        ""--output_interp_order"",\n        metavar=\'\',\n        help=""[Inference only] interpolation order of the network output"",\n        type=int,\n        default=0)\n\n    parser.add_argument(\n        ""--border"",\n        metavar=\'\',\n        help=""[Inference only] Width of borders to crop for segmented patch"",\n        type=spatialnumarray,\n        default=(0, 0, 0))\n\n    parser.add_argument(\n        ""--fill_constant"",\n        help=""[Inference only] Output fill value ""\n             ""used fill borders of output images."",\n        type=float,\n        default=0.0)\n\n    return parser\n\n\ndef add_evaluation_args(parser):\n    """"""\n    keywords defined for evaluation action\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--evaluations"",\n        metavar=\'\',\n        help=""[Evaluation only] List of evaluations to generate"",\n        default=\'\')\n\n    parser.add_argument(\n        ""--save_csv_dir"",\n        metavar=\'\',\n        help=""[Evaluation only] Directory to save evaluation metrics"",\n        default=DEFAULT_EVALUATION_OUTPUT)\n\n    return parser\n\n\ndef add_input_data_args(parser):\n    """"""\n    keywords defined for input data specification section\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--csv_file"",\n        metavar=\'\',\n        type=str,\n        help=""Input list of subjects in csv files"",\n        default=\'\')\n\n    parser.add_argument(\n        ""--csv_data_file"",\n        metavar=\'\',\n        type=str,\n        help=""Path to a csv with data; labels, features or coordinates for""\n             ""the patch based sampler"",\n        default=\'\')\n\n    parser.add_argument(\n        ""--to_ohe"",\n        help=""Indicates if the data provided in the csv should be ""\n             ""one-hot-encoded.""\n             ""This is only valid when the csv_data_file has 2 columns"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--path_to_search"",\n        metavar=\'\',\n        type=str,\n        help=""Input data folder to find a list of input image files"",\n        default=\'\')\n\n    parser.add_argument(\n        ""--filename_contains"",\n        metavar=\'\',\n        type=str_array,\n        help=""keywords in input file names, matched filenames will be used."")\n\n    parser.add_argument(\n        ""--filename_not_contains"",\n        metavar=\'\',\n        type=str_array,\n        help=""keywords in input file names, negatively matches filenames"",\n        default=\'\')\n    parser.add_argument(\n        ""--filename_removefromid"",\n        metavar=\'\',\n        type=str,\n        help=""Regular expression for extracting subject id from filename, ""\n             ""matched pattern will be removed from the file names ""\n             ""to form the subject id"",\n        default=\'\')\n\n    parser.add_argument(\n        ""--interp_order"",\n        type=int,\n        choices=[0, 1, 2, 3],\n        default=1,\n        help=""interpolation order of the input images"")\n\n    parser.add_argument(\n        ""--loader"",\n        type=str,\n        choices=list(SUPPORTED_LOADERS),\n        default=None,\n        help=""Image loader to use from {}. ""\n             ""Leave blank to try all loaders."".format(list(SUPPORTED_LOADERS)))\n\n    parser.add_argument(\n        ""--pixdim"",\n        type=float_array,\n        default=(),\n        help=""voxel width along each dimension"")\n\n    parser.add_argument(\n        ""--axcodes"",\n        type=str_array,\n        default=(),\n        help=""labels for positive end of voxel axes, possible labels are""\n             "" (\'L\',\'R\'),(\'P\',\'A\'),(\'I\',\'S\')""\n             "" *see also nibabel.orientations.ornt2axcodes"")\n\n    parser.add_argument(\n        ""--spatial_window_size"",\n        type=spatial_atleast3d,\n        help=""specify the spatial size of the input data (ndims <= 3)"",\n        default=())\n    return parser\n\n\ndef add_network_args(parser):\n    """"""\n    keywords defined for network specification\n\n    :param parser:\n    :return:\n    """"""\n    import niftynet.layer.binary_masking\n    import niftynet.layer.activation\n    import niftynet.utilities.histogram_standardisation as hist_std_module\n\n    parser.add_argument(\n        ""--name"",\n        help=""Choose a net from NiftyNet/niftynet/network/ or from ""\n             ""user specified module string"",\n        metavar=\'\')\n\n    parser.add_argument(\n        ""--activation_function"",\n        help=""Specify activation function types"",\n        choices=list(niftynet.layer.activation.SUPPORTED_OP),\n        metavar=\'TYPE_STR\',\n        default=\'relu\')\n\n    parser.add_argument(\n        ""--batch_size"",\n        metavar=\'\',\n        help=""Set batch size of the net"",\n        type=int,\n        default=2)\n\n    parser.add_argument(\n        ""--smaller_final_batch_mode"",\n        metavar=\'TYPE_STR\',\n        help=""If True, allow the final batch to be smaller ""\n             ""if there are insufficient items left in the queue, ""\n             ""and the batch size will be undetermined during ""\n             ""graph construction."",\n        choices=list(SMALLER_FINAL_BATCH_MODE),\n        default=\'pad\')\n\n    parser.add_argument(\n        ""--decay"",\n        help=""[Training only] Set weight decay"",\n        type=float,\n        default=0.0)\n\n    parser.add_argument(\n        ""--reg_type"",\n        metavar=\'TYPE_STR\',\n        help=""[Training only] Specify regulariser type_str"",\n        type=str,\n        default=\'L2\')\n\n    parser.add_argument(\n        ""--volume_padding_size"",\n        metavar=\'\',\n        help=""Set padding size of each volume (in all dimensions)"",\n        type=spatialnumarray,\n        default=(0, 0, 0))\n\n    parser.add_argument(\n        ""--volume_padding_mode"",\n        metavar=\'\',\n        help=""Set which type of numpy padding to do, see ""\n             ""https://docs.scipy.org/doc/numpy-1.14.0/""\n             ""reference/generated/numpy.pad.html ""\n             ""for details"",\n        type=str,\n        default=\'minimum\')\n\n    parser.add_argument(\n        ""--volume_padding_to_size"",\n        help=""Choose size to pad all input volumes to. Any dimensions ""\n             ""that exceed the desired size will be kept the same. Default: ""\n             ""(0, ) which indicates not to use this mode. "",\n        type=spatialnumarray,\n        default=(0,)\n    )\n\n    parser.add_argument(\n        ""--window_sampling"",\n        metavar=\'TYPE_STR\',\n        help=""How to sample patches from each loaded image:""\n             "" \'uniform\': fixed size uniformly distributed,""\n             "" \'resize\': resize image to the patch size."",\n        choices=[\'uniform\', \'resize\', \'balanced\', \'weighted\', \'patch\'],\n        default=\'uniform\')\n\n    parser.add_argument(\n        ""--force_output_identity_resizing"",\n        metavar=str2boolean,\n        help=""Forces the shape of the inferred output to match the ""\n        ""input label shape rather than be resized to input image shape."",\n        default=False)\n\n    parser.add_argument(\n        ""--queue_length"",\n        help=""Set size of preprocessing buffer queue"",\n        metavar=\'\',\n        type=int,\n        default=5)\n\n    parser.add_argument(\n        ""--multimod_foreground_type"",\n        choices=list(\n            niftynet.layer.binary_masking.SUPPORTED_MULTIMOD_MASK_TYPES),\n        help=""Way of combining the foreground masks from different ""\n             ""modalities. \'and\' is the intersection, \'or\' is the union ""\n             ""and \'multi\' permits each modality to use its own mask."",\n        default=\'and\')\n\n    parser.add_argument(\n        ""--histogram_ref_file"",\n        metavar=\'\',\n        type=str,\n        help=""A reference file of histogram for intensity normalisation"",\n        default=DEFAULT_HISTOGRAM_REF_FILE)\n\n    parser.add_argument(\n        ""--norm_type"",\n        help=""Type of normalisation to perform"",\n        type=str,\n        default=\'percentile\',\n        choices=list(hist_std_module.SUPPORTED_CUTPOINTS))\n\n    parser.add_argument(\n        ""--cutoff"",\n        help=""Cutoff values for the normalisation process"",\n        type=float_array,\n        default=(0.01, 0.99))\n\n    parser.add_argument(\n        ""--foreground_type"",\n        choices=list(niftynet.layer.binary_masking.SUPPORTED_MASK_TYPES),\n        help=""type_str of foreground masking strategy used"",\n        default=\'otsu_plus\')\n\n    parser.add_argument(\n        ""--normalisation"",\n        help=""Indicates if the normalisation must be performed"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--rgb_normalisation"",\n        help=""Indicates if RGB histogram equilisation should be performed"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--whitening"",\n        help=""Indicates if the whitening of the data should be applied"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--normalise_foreground_only"",\n        help=""Indicates whether a foreground mask should be applied when""\n             "" normalising volumes"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--weight_initializer"",\n        help=""Set the initializer for the weight parameters"",\n        type=str,\n        default=\'he_normal\')\n\n    parser.add_argument(\n        ""--bias_initializer"",\n        help=""Set the initializer for the bias parameters"",\n        type=str,\n        default=\'zeros\')\n\n    parser.add_argument(\n        ""--keep_prob"",\n        help=""Probability that each element is kept ""\n             ""if dropout is supported by the network"",\n        type=float,\n        default=1.0)\n\n    yaml = require_module(\'yaml\', mandatory=False)\n    if yaml:\n        parser.add_argument(\n            ""--weight_initializer_args"",\n            help=""Pass arguments to the initializer for the weight parameters"",\n            type=yaml.load,\n            default={})\n        parser.add_argument(\n            ""--bias_initializer_args"",\n            help=""Pass arguments to the initializer for the bias parameters"",\n            type=yaml.load,\n            default={})\n\n    return parser\n\n\ndef add_training_args(parser):\n    """"""\n    keywords defined for the training action\n\n    :param parser:\n    :return:\n    """"""\n    parser.add_argument(\n        ""--optimiser"",\n        help=""Choose an optimiser for computing graph gradients and applying"",\n        type=str,\n        default=\'adam\')\n\n    parser.add_argument(\n        ""--sample_per_volume"",\n        help=""[Training only] Set number of samples to take from ""\n             ""each image that was loaded in a given training epoch"",\n        metavar=\'\',\n        type=int,\n        default=1)\n\n    parser.add_argument(\n        ""--rotation_angle"",\n        help=""The min/max angles of rotation when rotation ""\n             ""augmentation is enabled"",\n        type=float_array,\n        default=())\n\n    parser.add_argument(\n        ""--rotation_angle_x"",\n        help=""The min/max angles of the x rotation when rotation ""\n             ""augmentation is enabled"",\n        type=float_array,\n        default=())\n\n    parser.add_argument(\n        ""--rotation_angle_y"",\n        help=""The min/max angles of the y rotation when rotation ""\n             ""augmentation is enabled"",\n        type=float_array,\n        default=())\n\n    parser.add_argument(\n        ""--rotation_angle_z"",\n        help=""The min/max angles of the z rotation when rotation ""\n             ""augmentation is enabled"",\n        type=float_array,\n        default=())\n\n    parser.add_argument(\n        ""--scaling_percentage"",\n        help=""The spatial scaling factor in [min_percentage, max_percentage]"",\n        type=float_array,\n        default=())\n\n    parser.add_argument(\n        ""--isotropic_scaling"",\n        help=""Indicates if the same random scaling factor should be applied ""\n             ""to each dimension"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--antialiasing"",\n        help=""Indicates if antialiasing must be performed ""\n             ""when randomly scaling the input images"",\n        type=str2boolean,\n        default=True)\n\n    parser.add_argument(\n        ""--bias_field_range"",\n        help=""[Training only] The range of bias field coeffs in [min_coeff, ""\n             ""max_coeff]"",\n        type=float_array,\n        default=())\n\n    parser.add_argument(\n        ""--bf_order"",\n        help=""[Training only] maximal polynomial order to use for the ""\n             ""creation of the bias field augmentation"",\n        metavar=\'\',\n        type=int,\n        default=3)\n\n    parser.add_argument(\n        ""--random_flipping_axes"",\n        help=""The axes which can be flipped to augment the data. Supply as ""\n             ""comma-separated values within single quotes, e.g. \'0,1\'. Note ""\n             ""that these are 0-indexed, so choose some combination of 0, 1."",\n        type=int_array,\n        default=-1)\n\n    # elastic deformation\n    parser.add_argument(\n        ""--do_elastic_deformation"",\n        help=""Enables elastic deformation"",\n        type=str2boolean,\n        default=False)\n\n    parser.add_argument(\n        ""--num_ctrl_points"",\n        help=""Number of control points for the elastic deformation"",\n        type=int,\n        default=4)\n\n    parser.add_argument(\n        ""--deformation_sigma"",\n        help=""The standard deviation for elastic deformation."",\n        type=float,\n        default=15)\n\n    parser.add_argument(\n        ""--proportion_to_deform"",\n        help=""What fraction of samples to deform elastically."",\n        type=float,\n        default=0.5)\n\n    parser.add_argument(\n        ""--lr"",\n        help=""[Training only] Set learning rate"",\n        type=float,\n        default=0.01)\n\n    parser.add_argument(\n        ""--loss_type"",\n        metavar=\'TYPE_STR\',\n        help=""[Training only] Specify loss type_str"",\n        default=\'Dice\')\n\n    parser.add_argument(\n        ""--starting_iter"",\n        metavar=\'\',\n        help=""[Training only] Resume from iteration n"",\n        type=int,\n        default=0)\n\n    parser.add_argument(\n        ""--save_every_n"",\n        metavar=\'\',\n        help=""[Training only] Model saving frequency"",\n        type=int,\n        default=500)\n\n    parser.add_argument(\n        ""--tensorboard_every_n"",\n        metavar=\'\',\n        help=""[Training only] Tensorboard summary frequency"",\n        type=int,\n        default=20)\n\n    parser.add_argument(\n        ""--max_iter"",\n        metavar=\'\',\n        help=""[Training only] Total number of iterations"",\n        type=int,\n        default=10000)\n\n    parser.add_argument(\n        ""--max_checkpoints"",\n        help=""Maximum number of model checkpoints that will be saved"",\n        type=int,\n        default=100)\n\n    parser.add_argument(\n        ""--validation_every_n"",\n        help=""Validate every n iterations"",\n        type=int,\n        default=-1)\n\n    parser.add_argument(\n        ""--validation_max_iter"",\n        help=""Number of validation batches to run"",\n        type=int,\n        default=1)\n\n    parser.add_argument(\n        ""--exclude_fraction_for_validation"",\n        help=""Fraction of dataset to use for validation"",\n        type=float,\n        default=0.)\n\n    parser.add_argument(\n        ""--exclude_fraction_for_inference"",\n        help=""Fraction of dataset to use for inference"",\n        type=float,\n        default=0.)\n\n    parser.add_argument(\n        ""--vars_to_restore"",\n        help=""regex strings matching variable names to restore"",\n        type=str,\n        default=\'\')\n\n    parser.add_argument(\n        ""--vars_to_freeze"",\n        help=""regex strings matching variable to be fixed during training"",\n        type=str,\n        default=\'\')\n\n    parser.add_argument(\n        ""--patience"",\n        metavar=\'\',\n        help=\'Number of iterations to wait before starting \'\n             \'performance monitoring\',\n        type=int,\n        default=100)\n\n    parser.add_argument(\n        ""--early_stopping_mode"",\n        metavar=\'\',\n        help=""Choose between {\'mean\', \'robust_mean\', \'median\', ""\n             ""\'generalisation_loss\', \'median_smoothing\', \'validation_up\'}"",\n        type=str,\n        default=\'mean\')\n\n    return parser\n\n\nSUPPORTED_DEFAULT_SECTIONS = {\n    \'SYSTEM\': add_application_args,\n    \'NETWORK\': add_network_args,\n    \'TRAINING\': add_training_args,\n    \'INFERENCE\': add_inference_args,\n    \'EVALUATION\': add_evaluation_args,\n}\n'"
niftynet/utilities/user_parameters_helper.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module mainly defines types and casting methods for input parameters\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport re\n\nfrom six import string_types\n\nfrom niftynet.utilities.user_parameters_regex import match_array\n\nTRUE_VALUE = {\'yes\', \'true\', \'t\', \'y\', \'1\'}\nFALSE_VALUE = {\'no\', \'false\', \'f\', \'n\', \'0\'}\n\n\ndef str2boolean(string_input):\n    """"""\n    convert user input config string to boolean\n\n    :param string_input: any string in TRUE_VALUE or FALSE_VALUE\n    :return: True or False\n    """"""\n    if string_input.lower() in TRUE_VALUE:\n        return True\n    if string_input.lower() in FALSE_VALUE:\n        return False\n    raise argparse.ArgumentTypeError(\n        \'Boolean value expected, received {}\'.format(string_input))\n\n\ndef int_array(string_input):\n    """"""\n    convert input into a tuple of int values\n\n    :param string_input:\n    :return:\n    """"""\n    try:\n        output_tuple = match_array(string_input, \'int\')\n    except ValueError:\n        raise argparse.ArgumentTypeError(\n            \'array of int expected, received {}\'.format(string_input))\n    return output_tuple\n\n\ndef float_array(string_input):\n    """"""\n    convert input into a tuple of float values\n\n    :param string_input:\n    :return:\n    """"""\n    try:\n        output_tuple = match_array(string_input, \'float\')\n    except ValueError:\n        raise argparse.ArgumentTypeError(\n            \'array of float expected, received {}\'.format(string_input))\n    return output_tuple\n\n\ndef str_array(string_input):\n    """"""\n    convert input into a tuple of strings\n\n    :param string_input:\n    :return:\n    """"""\n    try:\n        output_tuple = match_array(string_input, \'str\')\n    except ValueError:\n        raise argparse.ArgumentTypeError(\n            ""list of strings expected, for each list element the allowed ""\n            ""characters: [ a-zA-Z0-9_-.], but received {}"".format(string_input))\n    return output_tuple\n\n\ndef make_input_tuple(input_str, element_type=string_types):\n    """"""\n    converting input into a tuple of input\n\n    :param input_str:\n    :param element_type:\n    :return:\n    """"""\n    assert input_str, \\\n        ""invalid input {} for type {}"".format(input_str, element_type)\n    if isinstance(input_str, element_type):\n        new_tuple = (input_str,)\n    else:\n        try:\n            new_tuple = tuple(input_str)\n        except TypeError:\n            raise ValueError(""can\'t cast to tuple of {}"".format(element_type))\n    assert all([isinstance(item, element_type) for item in new_tuple]), \\\n        ""the input should be a tuple of {}"".format(element_type)\n    return new_tuple\n\n\ndef standardise_section_name(configparser, old_name):\n    """"""\n    rename configparser section\n    This helper is useful when user specifies complex section names\n    """"""\n    new_name = standardise_string(old_name)\n    if old_name == new_name:\n        return old_name\n    items = configparser.items(old_name)\n    configparser.add_section(new_name)\n    for (name, value) in items:\n        configparser.set(new_name, name, value)\n    configparser.remove_section(old_name)\n    return new_name\n\n\ndef standardise_string(input_string):\n    """"""\n    to make the user\'s input consistent\n    replace any characters not in set [0-9a-zA-Z] with underscore _\n\n    :param input_string: to be standardised\n    :return: capitalised string\n    """"""\n    if not isinstance(input_string, string_types):\n        return input_string\n    new_name = re.sub(r\'[^0-9a-zA-Z_\\- ]+\', \'\', input_string.strip())\n    return new_name\n\n\ndef has_section_in_config(config, required_custom_section):\n    """"""\n    check if section name exists in a config file\n    raises value error if it doesn\'t exist\n\n    :param config:\n    :param required_custom_section:\n    :return:\n    """"""\n    required_custom_section = standardise_string(required_custom_section)\n    if required_custom_section is not None:\n        user_sections = [\n            standardise_string(section_name)\n            for section_name in config.sections()]\n        if required_custom_section not in user_sections:\n            raise ValueError\n\n\ndef add_input_name_args(parser, supported_input):\n    """"""\n    adding keywords that defines grouping of the input sections\n    (mainly used for multi-modal input specifications)\n\n    :param parser:\n    :param supported_input:\n    :return:\n    """"""\n    for input_name in list(supported_input):\n        parser.add_argument(\n            ""--{}"".format(input_name),\n            metavar=\'\',\n            help=""names of grouping the input sections {}"".format(input_name),\n            type=str_array,\n            default=())\n    return parser\n\n\ndef spatialnumarray(string_input):\n    """"""\n    This function parses a 3-element tuple from a string input\n    if the input has less than 3 elements,\n    the last element is repeated as padding.\n    """"""\n    int_tuple = int_array(string_input) or (0,)\n    while len(int_tuple) < 3:\n        int_tuple = int_tuple + (int_tuple[-1],)\n    int_tuple = int_tuple[:3]\n    return int_tuple\n\n\ndef spatial_atleast3d(string_input):\n    """"""\n    This function parses a 3-element tuple from a string input.\n    The input will be padded with ones, if the length is less than 3.\n    """"""\n    output_tuple = int_array(string_input) or (0,)\n    if len(output_tuple) < 3:\n        # will pad the array with single dimensions\n        output_tuple = output_tuple + (1,) * (3 - len(output_tuple))\n    return output_tuple\n'"
niftynet/utilities/user_parameters_parser.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nParse user configuration file\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport textwrap\n\nfrom niftynet.engine.signal import TRAIN, INFER, EVAL\nfrom niftynet.utilities.util_common import look_up_operations\nfrom niftynet.engine.application_factory import ApplicationFactory\nfrom niftynet.engine.application_factory import SUPPORTED_APP\nfrom niftynet.io.misc_io import resolve_file_name\nfrom niftynet.utilities.niftynet_global_config import NiftyNetGlobalConfig\nfrom niftynet.utilities import NiftyNetLaunchConfig\nfrom niftynet.utilities.user_parameters_custom import SUPPORTED_ARG_SECTIONS\nfrom niftynet.utilities.user_parameters_custom import add_customised_args\nfrom niftynet.utilities.user_parameters_default import \\\n    SUPPORTED_DEFAULT_SECTIONS\nfrom niftynet.utilities.user_parameters_default import add_input_data_args\nfrom niftynet.utilities.user_parameters_helper import has_section_in_config\nfrom niftynet.utilities.user_parameters_helper import standardise_section_name\nfrom niftynet.utilities.util_common import \\\n    damerau_levenshtein_distance as edit_distance\nfrom niftynet.utilities.versioning import get_niftynet_version_string\n\nSYSTEM_SECTIONS = {\'SYSTEM\', \'NETWORK\', \'TRAINING\', \'INFERENCE\', \'EVALUATION\'}\nACTIONS = {\'train\': TRAIN, \'inference\': INFER, \'evaluation\': EVAL}\nEPILOG_STRING = \\\n    \'\\n\\n======\\nFor more information please visit:\\n\' \\\n    \'http://niftynet.readthedocs.io/en/dev/config_spec.html\\n\' \\\n    \'======\\n\\n\'\n\n\n# pylint: disable=protected-access\ndef available_keywords():\n    """"""\n    returns a list of all possible keywords defined in the parsers\n    (duplicates from sections are removed.)\n    """"""\n    all_key_parser = argparse.ArgumentParser(\n        parents=[],\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        conflict_handler=\'resolve\')\n\n    for _, add_args_func in SUPPORTED_DEFAULT_SECTIONS.items():\n        all_key_parser = add_args_func(all_key_parser)\n\n    all_key_parser = add_input_data_args(all_key_parser)\n\n    # add keys from custom sections\n    for _, add_args_func in SUPPORTED_ARG_SECTIONS.items():\n        all_key_parser = add_args_func(all_key_parser)\n\n    default_keys = []\n    for action in all_key_parser._actions:\n        try:\n            default_keys.append(action.option_strings[0][2:])\n        except (IndexError, AttributeError, ValueError):\n            pass\n    # remove duplicates\n    default_keys = list(set(default_keys))\n    # remove bad names\n    default_keys = [keyword for keyword in default_keys if keyword]\n    return default_keys\n\n\nKEYWORDS = available_keywords()\nNIFTYNET_HOME = NiftyNetGlobalConfig().get_niftynet_home_folder()\n\n\n# pylint: disable=too-many-branches\ndef run():\n    """"""\n    meta_parser is first used to find out location\n    of the configuration file. Based on the application_name\n    or meta_parser.prog name, the section parsers are organised\n    to find system parameters and application specific\n    parameters.\n\n    :return: system parameters is a group of parameters including\n        SYSTEM_SECTIONS and app_module.REQUIRED_CONFIG_SECTION\n        input_data_args is a group of input data sources to be\n        used by niftynet.io.ImageReader\n    """"""\n    meta_parser = argparse.ArgumentParser(\n        description=""Launch a NiftyNet application."",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=textwrap.dedent(EPILOG_STRING))\n    version_string = get_niftynet_version_string()\n    meta_parser.add_argument(""action"",\n                             help=""train networks, run inferences ""\n                                  ""or evaluate inferences"",\n                             metavar=\'ACTION\',\n                             choices=list(ACTIONS))\n    meta_parser.add_argument(""-v"", ""--version"",\n                             action=\'version\',\n                             version=version_string)\n    meta_parser.add_argument(""-c"", ""--conf"",\n                             help=""specify configurations from a file"",\n                             metavar=""CONFIG_FILE"")\n    meta_parser.add_argument(""-a"", ""--application_name"",\n                             help=""specify an application module name"",\n                             metavar=\'APPLICATION_NAME\',\n                             default="""")\n\n    meta_args, args_from_cmdline = meta_parser.parse_known_args()\n    print(version_string)\n\n    # read configurations, to be parsed by sections\n    config_file_name = __resolve_config_file_path(meta_args.conf)\n    config = NiftyNetLaunchConfig()\n    config.read([config_file_name])\n\n    # infer application name from command\n    app_name = None\n    try:\n        parser_prog = meta_parser.prog.replace(\'.py\', \'\')\n        app_name = parser_prog if parser_prog in SUPPORTED_APP \\\n            else meta_args.application_name\n        assert app_name\n    except (AttributeError, AssertionError):\n        raise ValueError(\n            ""\\nUnknown application {}, or did you forget \'-a\' ""\n            ""command argument?{}"".format(app_name, EPILOG_STRING))\n\n    # load application by name\n    app_module = ApplicationFactory.create(app_name)\n    try:\n        assert app_module.REQUIRED_CONFIG_SECTION, \\\n            ""\\nREQUIRED_CONFIG_SECTION should be static variable "" \\\n            ""in {}"".format(app_module)\n        has_section_in_config(config, app_module.REQUIRED_CONFIG_SECTION)\n    except AttributeError:\n        raise AttributeError(\n            ""Application code doesn\'t have REQUIRED_CONFIG_SECTION property. ""\n            ""{} should be an instance of ""\n            ""niftynet.application.base_application"".format(app_module))\n    except ValueError:\n        raise ValueError(\n            ""\\n{} requires [{}] section in the config file.{}"".format(\n                app_name, app_module.REQUIRED_CONFIG_SECTION, EPILOG_STRING))\n\n    # check keywords in configuration file\n    _check_config_file_keywords(config)\n\n    # using configuration as default, and parsing all command line arguments\n    # command line args override the configure file options\n    all_args = {}\n    for section in config.sections():\n        # try to rename user-specified sections for consistency\n        section = standardise_section_name(config, section)\n        section_defaults = dict(config.items(section))\n        section_args, args_from_cmdline = \\\n            _parse_arguments_by_section([],\n                                        section,\n                                        section_defaults,\n                                        args_from_cmdline,\n                                        app_module.REQUIRED_CONFIG_SECTION)\n        all_args[section] = section_args\n\n    # check if any args from command line not recognised\n    _check_cmd_remaining_keywords(list(args_from_cmdline))\n\n    # split parsed results in all_args\n    # into dictionaries of system_args and input_data_args\n    system_args, input_data_args = {}, {}\n    for section in all_args:\n\n        # copy system default sections to ``system_args``\n        if section in SYSTEM_SECTIONS:\n            system_args[section] = all_args[section]\n            continue\n\n        # copy application specific sections to ``system_args``\n        if section == app_module.REQUIRED_CONFIG_SECTION:\n            system_args[\'CUSTOM\'] = all_args[section]\n            vars(system_args[\'CUSTOM\'])[\'name\'] = app_name\n            continue\n\n        # copy non-default sections to ``input_data_args``\n        input_data_args[section] = all_args[section]\n\n        # set the output path of csv list if not exists\n        try:\n            csv_path = resolve_file_name(\n                input_data_args[section].csv_file,\n                (os.path.dirname(config_file_name), NIFTYNET_HOME))\n            input_data_args[section].csv_file = csv_path\n            # don\'t search files if csv specified in config\n            try:\n                delattr(input_data_args[section], \'path_to_search\')\n            except AttributeError:\n                pass\n        except (IOError, TypeError):\n            input_data_args[section].csv_file = \'\'\n\n    # preserve ``config_file`` and ``action parameter`` from the meta_args\n    system_args[\'CONFIG_FILE\'] = argparse.Namespace(path=config_file_name)\n    # mapping the captured action argument to a string in ACTIONS\n    system_args[\'SYSTEM\'].action = \\\n        look_up_operations(meta_args.action, ACTIONS)\n    if not system_args[\'SYSTEM\'].model_dir:\n        system_args[\'SYSTEM\'].model_dir = os.path.join(\n            os.path.dirname(config_file_name), \'model\')\n    return system_args, input_data_args\n\n\ndef _parse_arguments_by_section(parents,\n                                section,\n                                args_from_config_file,\n                                args_from_cmd,\n                                required_section):\n    """"""\n    This function first adds parameter names to a parser,\n    according to the section name.\n    Then it loads values from configuration files as tentative params.\n    Finally it overrides existing pairs of \'name, value\' with commandline\n    inputs.\n\n    Commandline inputs only override system/custom parameters.\n    input data related parameters needs to be defined in config file.\n\n    :param parents: a list, parsers will be created as\n        subparsers of parents\n    :param section: section name to be parsed\n    :param args_from_config_file: loaded parameters from config file\n    :param args_from_cmd: dictionary commandline parameters\n    :return: parsed parameters of the section and unknown\n        commandline params.\n    """"""\n    section_parser = argparse.ArgumentParser(\n        parents=parents,\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    try:\n        add_args_func = SUPPORTED_DEFAULT_SECTIONS[section]\n    except KeyError:\n        if section == required_section:\n            def add_args_func(parser):\n                """"""\n                wrapper around add_customised_args\n                """"""\n                return add_customised_args(parser, section.upper())\n        else:\n            # all remaining sections are defaulting to input section\n            add_args_func = add_input_data_args\n\n    section_parser = add_args_func(section_parser)\n\n    # loading all parameters a config file first\n    if args_from_config_file is not None:\n        section_parser.set_defaults(**args_from_config_file)\n    # input command line input overrides config file\n    if (section in SYSTEM_SECTIONS) or (section == required_section):\n        section_args, unknown = section_parser.parse_known_args(args_from_cmd)\n        return section_args, unknown\n    # don\'t parse user cmd for input source sections\n    section_args, _ = section_parser.parse_known_args([])\n    return section_args, args_from_cmd\n\n\ndef _check_config_file_keywords(config):\n    """"""\n    check config files, validate keywords provided against\n    parsers\' argument list\n    """"""\n\n    # collecting all keywords from the config\n    config_keywords = []\n    for section in config.sections():\n        if config.items(section):\n            config_keywords.extend(list(dict(config.items(section))))\n    _raises_bad_keys(config_keywords, error_info=\'config file\')\n\n\ndef _check_cmd_remaining_keywords(args_from_cmdline):\n    """"""\n    check list of remaining arguments from the command line input.\n    Normally `args_from_cmd` should be empty; non-empty list\n    means unrecognised parameters.\n    """"""\n\n    args_from_cmdline = [\n        arg_item.replace(\'-\', \'\') for arg_item in args_from_cmdline]\n    _raises_bad_keys(args_from_cmdline, error_info=\'command line\')\n\n    # command line parameters should be valid\n    # assertion will be triggered when keywords matched ones in custom\n    # sections that are not used in the current application.\n    assert not args_from_cmdline, \\\n        \'\\nUnknown parameter: {}{}\'.format(args_from_cmdline, EPILOG_STRING)\n\n\ndef _raises_bad_keys(keys, error_info=\'config file\'):\n    """"""\n    raises value error if keys is not in the system key set.\n    `error_info` is used to customise the error message.\n    """"""\n    for key in list(keys):\n        if key in KEYWORDS:\n            continue\n        dists = {k: edit_distance(k, key) for k in KEYWORDS}\n        closest = min(dists, key=dists.get)\n        raise ValueError(\n            \'Unknown keywords in {3}: By ""{0}"" \'\n            \'did you mean ""{1}""?\\n ""{0}"" is \'\n            \'not a valid option.{2}\'.format(\n                key, closest, EPILOG_STRING, error_info))\n\n\ndef __resolve_config_file_path(cmdline_arg):\n    """"""\n    Search for the absolute file name of the configuration file.\n    starting from `-c` value provided by the user.\n\n    :param cmdline_arg:\n    :return:\n    """"""\n    if not cmdline_arg:\n        raise IOError(""\\nNo configuration file has been provided, did you ""\n                      ""forget \'-c\' command argument?{}"".format(EPILOG_STRING))\n    # Resolve relative configuration file location\n    config_file_path = os.path.expanduser(cmdline_arg)\n    try:\n        config_file_path = resolve_file_name(\n            config_file_path, (\'.\', NIFTYNET_HOME))\n        if os.path.isfile(config_file_path):\n            return config_file_path\n    except (IOError, TypeError):\n        config_file_path = os.path.expanduser(cmdline_arg)\n\n    config_file_path = os.path.join(\n        NiftyNetGlobalConfig().get_default_examples_folder(),\n        config_file_path, config_file_path + ""_config.ini"")\n    if os.path.isfile(config_file_path):\n        return config_file_path\n\n    # could not proceed without a configuration file\n    raise IOError(""\\nConfiguration file not found: {}.{}"".format(\n        os.path.expanduser(cmdline_arg), EPILOG_STRING))\n'"
niftynet/utilities/user_parameters_regex.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nregular expressions to match tuples from user inputs\nkindly provided by\nLuis Carlos Garcia Peraza Herrera <luis.herrera.14@ucl.ac.uk>\n""""""\nfrom __future__ import unicode_literals\n\nimport re\n\nINT = r\'(?:[-+]?\\d+)\'\nFLOAT = r\'(?:[-+]?\\d*\\.\\d+|\' + INT + r\')\'\nLITERAL = r\'(?:[-_a-zA-Z0-9 \\\\:\\.]+)\'\nCOMMA = r\'(?:[,])\'\nLEFT_PARENTHESIS = r\'(?:\\()\'\nRIGHT_PARENTHESIS = r\'(?:\\))\'\nLEFT_BRACKET = r\'(?:[{])\'\nRIGHT_BRACKET = r\'(?:[}])\'\nOPTIONAL_BLANK = r\'(?:[ \\t\\r\\n]?)\'\nOPTIONAL_BLANKS = r\'(?:\' + OPTIONAL_BLANK + r\'+)\'\nOR = r\'|\'\n\nTUPLE = \\\n    r\'(?:\' \\\n    + r\'(?:\' \\\n    + r\'(?:\' \\\n    + OPTIONAL_BLANKS + FLOAT + OPTIONAL_BLANKS + COMMA \\\n    + r\')*\' \\\n    + OPTIONAL_BLANKS + FLOAT + OPTIONAL_BLANKS \\\n    + r\')\' \\\n    + OR \\\n    + r\'(?:\' \\\n    + r\'(?:\' \\\n    + OPTIONAL_BLANKS + LITERAL + OPTIONAL_BLANKS + COMMA \\\n    + r\')*\' \\\n    + OPTIONAL_BLANKS + LITERAL + OPTIONAL_BLANKS \\\n    + r\')\' \\\n    + r\')\'\n\nSTATEMENT = \\\n    r\'^\' \\\n    + LEFT_PARENTHESIS + r\'(\' + TUPLE + r\')?\' + RIGHT_PARENTHESIS + r\'$\' \\\n    + OR \\\n    + r\'^\' + LEFT_BRACKET + r\'(\' + TUPLE + r\')?\' + RIGHT_BRACKET + r\'$\' \\\n    + OR \\\n    + r\'^(\' + TUPLE + r\')?$\'\n\n\ndef match_array(string_input, type_str):\n    """"""\n    matching input string to a tuple of elements in `type_str` type\n\n    :param string_input:\n    :param type_str:\n    :return:\n    """"""\n    regex = re.compile(STATEMENT)\n    matched_str = regex.match(string_input)\n    if matched_str:\n        filtered_groups = [matched for matched in matched_str.groups()\n                           if matched is not None]\n        if not filtered_groups:\n            return ()\n        try:\n            values = [v.strip() for v in filtered_groups[0].split(\',\')]\n        except IndexError:\n            raise ValueError(\n                ""unrecognised input string {}"".format(string_input))\n        if type_str == \'int\':\n            return tuple(int(val) for val in values)\n        if type_str == \'float\':\n            return tuple(float(val) for val in values)\n        if type_str == \'str\':\n            return tuple(values)\n        raise ValueError(""unknown array type_str {}"".format(string_input))\n    raise ValueError(""invalid parameter {}"".format(string_input))\n'"
niftynet/utilities/util_common.py,7,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport os\nimport re\nfrom functools import partial\n\nimport numpy as np\nimport tensorflow as tf\nfrom scipy import ndimage\nfrom six import string_types\n\n\ndef traverse_nested(input_lists, types=(list, tuple)):\n    """"""\n    Flatten a nested list or tuple\n    """"""\n\n    if isinstance(input_lists, types):\n        for input_list in input_lists:\n            for sub_list in traverse_nested(input_list, types=types):\n                yield sub_list\n    else:\n        yield input_lists\n\n\ndef list_depth_count(input_list):\n    """"""\n    This function count the maximum depth of a nested list (recursively)\n    This is used to check compatibility of users\' input and system API\n    only to be used for list or tuple\n    """"""\n    if not isinstance(input_list, (list, tuple)):\n        return 0\n    if len(input_list) == 0:\n        return 1\n    return 1 + max(map(list_depth_count, input_list))\n\n\ndef average_multi_opt_gradients(multi_device_gradients):\n    """"""\n    This function averages the gradients generated by each optimiser\n    """"""\n    if not multi_device_gradients:\n        # nothing to average\n        return multi_device_gradients\n\n    if isinstance(multi_device_gradients[0], dict):\n        # multi_device_gradients is a list of N dictionaries, for N devices\n        # each dictionary is a pair of optimiser_name: device_gradient\n        optimiser_names = sorted(multi_device_gradients[0])\n        ave_gradients = dict()\n        for opt_name in optimiser_names:\n            multi_device_grad = [device_gradient.get(opt_name)\n                                 for device_gradient in multi_device_gradients]\n            ave_gradients[opt_name] = average_gradients(multi_device_grad)\n        return ave_gradients\n    # multi_device_gradients is a list of N device_gradients, for N devices\n    return average_gradients(multi_device_gradients)\n\n\ndef average_gradients(multi_device_gradients):\n    """"""\n    the input gradients are grouped by device,\n    this function average the gradients of multiple devices\n\n    :param multi_device_gradients: list of N gradients for N devices\n    :return:\n    """"""\n    # print(len(multi_device_gradients),\n    #   len(multi_device_gradients[0]),\n    #   len(multi_device_gradients[0][0]),\n    #   len(multi_device_gradients[0][0][0]))\n\n    if len(multi_device_gradients) == 1:\n        # only one device, so we get rid of the first level list\n        # that loops over devices\n        return multi_device_gradients[0]\n\n    nested_grads_depth = list_depth_count(multi_device_gradients)\n    if nested_grads_depth == 4:\n        gradients = zip(*multi_device_gradients)\n        averaged_grads = [__average_grads(g) for g in gradients]\n    elif nested_grads_depth == 3:\n        averaged_grads = __average_grads(multi_device_gradients)\n    else:\n        tf.logging.fatal(\n            ""The list of gradients are nested in an unusual way.""\n            ""application\'s gradient is not compatible with app driver.""\n            ""Please check the return value of gradients_collector ""\n            ""in connect_data_and_network() of the application"")\n        raise RuntimeError\n    return averaged_grads\n\n\ndef __average_grads(tower_grads):\n    """"""\n    Performs and return the average of the gradients\n    :param tower_grads: in form of [[tower_1_grad], [tower_2_grad], ...]\n    :return ave_grads: in form of [ave_grad]\n    """"""\n    # average gradients computed from multiple GPUs\n    ave_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        grads = [tf.expand_dims(g, 0)\n                 for g, _ in grad_and_vars if g is not None]\n        if not grads:\n            continue\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0, name=\'AveOverDevices\')\n\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        ave_grads.append(grad_and_var)\n    return ave_grads\n\n\ndef has_bad_inputs(input_args):\n    """"""\n    Check if all input params have been properly set in the configuration file.\n    :param input_args:\n    :return:\n    """"""\n    is_bad = False\n    for section in input_args:\n        section_args = input_args[section]\n        for input_arg in vars(section_args):\n            user_value = getattr(section_args, input_arg)\n            if user_value is None:\n                print(\'{} not set in section [{}] the config file\'.format(\n                    input_arg, section))\n                is_bad = True\n\n    return is_bad\n\n\ndef __print_argparse_section(args, section):\n    output_string = []\n    header_str = \'[{}]\'.format(section.upper())\n    print(header_str)\n    output_string.append(header_str)\n    section_args = args[section]\n    for arg in vars(section_args):\n        out_str = ""-- {}: {}"".format(arg, getattr(section_args, arg))\n        print(out_str)\n        output_string.append(out_str)\n    return output_string\n\n\ndef print_save_input_parameters(args, txt_file=None):\n    import niftynet.utilities.user_parameters_parser as param_parser\n    output_config = [\'Input params at \' + str(datetime.datetime.now())[:-6]]\n    for section in args:\n        if section not in param_parser.SYSTEM_SECTIONS:\n            output_config.extend(__print_argparse_section(args, section))\n    for section in args:\n        if section in param_parser.SYSTEM_SECTIONS:\n            output_config.extend(__print_argparse_section(args, section))\n\n    if txt_file is not None:\n        with open(txt_file, \'w\') as f:\n            [f.write(s + \'\\n\') for s in output_config]\n\n\nclass MorphologyOps(object):\n    """"""\n    Class that performs the morphological operations needed to get notably\n    connected component. To be used in the evaluation\n    """"""\n\n    def __init__(self, binary_img, neigh):\n        assert len(binary_img.shape) == 3, \'currently supports 3d inputs only\'\n        self.binary_map = np.asarray(binary_img, dtype=np.int8)\n        self.neigh = neigh\n\n    def border_map(self):\n        """"""\n        Creates the border for a 3D image\n        :return:\n        """"""\n        west = ndimage.shift(self.binary_map, [-1, 0, 0], order=0)\n        east = ndimage.shift(self.binary_map, [1, 0, 0], order=0)\n        north = ndimage.shift(self.binary_map, [0, 1, 0], order=0)\n        south = ndimage.shift(self.binary_map, [0, -1, 0], order=0)\n        top = ndimage.shift(self.binary_map, [0, 0, 1], order=0)\n        bottom = ndimage.shift(self.binary_map, [0, 0, -1], order=0)\n        cumulative = west + east + north + south + top + bottom\n        border = ((cumulative < 6) * self.binary_map) == 1\n        return border\n\n    def foreground_component(self):\n        return ndimage.label(self.binary_map)\n\n\ncache = {}\n\n\ndef CachedFunction(func):\n    def decorated(*args, **kwargs):\n        key = (func, args, frozenset(kwargs.items()))\n        if key not in cache:\n            cache[key] = func(*args, **kwargs)\n        return cache[key]\n    return decorated\n\n\ndef CachedFunctionByID(func):\n    def decorated(*args, **kwargs):\n        id_args = tuple(id(a) for a in args)\n        id_kwargs = ((k, id(kwargs[k])) for k in sorted(kwargs.keys()))\n        key = (func, id_args, id_kwargs)\n        if key not in cache:\n            cache[key] = func(*args, **kwargs)\n        return cache[key]\n    return decorated\n\n\nclass CacheFunctionOutput(object):\n    """"""\n    this provides a decorator to cache function outputs\n    to avoid repeating some heavy function computations\n    """"""\n\n    def __init__(self, func):\n        self.func = func\n\n    def __get__(self, obj, _=None):\n        if obj is None:\n            return self\n        return partial(self, obj)  # to remember func as self.func\n\n    def __call__(self, *args, **kw):\n        obj = args[0]\n        try:\n            cache = obj.__cache\n        except AttributeError:\n            cache = obj.__cache = {}\n        key = (self.func, args[1:], frozenset(kw.items()))\n        try:\n            value = cache[key]\n        except KeyError:\n            value = cache[key] = self.func(*args, **kw)\n        return value\n\n\ndef look_up_operations(type_str, supported):\n    """"""\n    This function validates the ``type_str`` against the supported set.\n\n    if ``supported`` is a ``set``, returns ``type_str``\n    if ``supported`` is a ``dict``, return ``supported[type_str]``\n    else:\n    raise an error possibly with a guess of the closest match.\n\n    :param type_str:\n    :param supported:\n    :return:\n    """"""\n    assert isinstance(type_str, string_types), \'unrecognised type string\'\n    if isinstance(supported, dict) and type_str in supported:\n        return supported[type_str]\n\n    if isinstance(supported, set) and type_str in supported:\n        return type_str\n\n    try:\n        set_to_check = set(supported)\n    except TypeError:\n        set_to_check = set()\n\n    edit_distances = {}\n    for supported_key in set_to_check:\n        edit_distance = damerau_levenshtein_distance(supported_key,\n                                                     type_str)\n        if edit_distance <= 3:\n            edit_distances[supported_key] = edit_distance\n    if edit_distances:\n        guess_at_correct_spelling = min(edit_distances,\n                                        key=edit_distances.get)\n        raise ValueError(\'By ""{0}"", did you mean ""{1}""?\\n\'\n                         \'""{0}"" is not a valid option.\\n\'\n                         \'Available options are {2}\\n\'.format(\n                             type_str, guess_at_correct_spelling, supported))\n    else:\n        raise ValueError(""No supported option \\""{}\\"" ""\n                         ""is not found.\\nAvailable options are {}\\n"".format(\n                             type_str, supported))\n\n\ndef damerau_levenshtein_distance(s1, s2):\n    """"""\n    Calculates an edit distance, for typo detection. Code based on :\n    https://en.wikipedia.org/wiki/Damerau\xe2\x80\x93Levenshtein_distance\n    """"""\n    d = {}\n    string_1_length = len(s1)\n    string_2_length = len(s2)\n    for i in range(-1, string_1_length + 1):\n        d[(i, -1)] = i + 1\n    for j in range(-1, string_2_length + 1):\n        d[(-1, j)] = j + 1\n\n    for i in range(string_1_length):\n        for j in range(string_2_length):\n            if s1[i] == s2[j]:\n                cost = 0\n            else:\n                cost = 1\n            d[(i, j)] = min(\n                d[(i - 1, j)] + 1,  # deletion\n                d[(i, j - 1)] + 1,  # insertion\n                d[(i - 1, j - 1)] + cost,  # substitution\n            )\n            if i and j and s1[i] == s2[j - 1] and s1[i - 1] == s2[j]:\n                d[(i, j)] = min(d[(i, j)],\n                                d[i - 2, j - 2] + cost)  # transposition\n\n    return d[string_1_length - 1, string_2_length - 1]\n\n\ndef otsu_threshold(img, nbins=256):\n    """"""\n    Implementation of otsu thresholding\n\n    :param img:\n    :param nbins:\n    :return:\n    """"""\n    hist, bin_edges = np.histogram(img.ravel(), bins=nbins)\n    hist = hist.astype(float)\n    half_bin_size = (bin_edges[1] - bin_edges[0]) * 0.5\n    bin_centers = bin_edges[:-1] + half_bin_size\n\n    weight_1 = np.copy(hist)\n    mean_1 = np.copy(hist)\n    weight_2 = np.copy(hist)\n    mean_2 = np.copy(hist)\n    for i in range(1, hist.shape[0]):\n        weight_1[i] = weight_1[i - 1] + hist[i]\n        mean_1[i] = mean_1[i - 1] + hist[i] * bin_centers[i]\n\n        weight_2[-i - 1] = weight_2[-i] + hist[-i - 1]\n        mean_2[-i - 1] = mean_2[-i] + hist[-i - 1] * bin_centers[-i - 1]\n\n    target_max = 0\n    threshold = bin_centers[0]\n    for i in range(0, hist.shape[0] - 1):\n        ratio_1 = mean_1[i] / weight_1[i]\n        ratio_2 = mean_2[i + 1] / weight_2[i + 1]\n        target = weight_1[i] * weight_2[i + 1] * (ratio_1 - ratio_2) ** 2\n        if target > target_max:\n            target_max, threshold = target, bin_centers[i]\n    return threshold\n\n\n# def otsu_threshold(img, nbins=256):\n#     """""" Implementation of otsu thresholding """"""\n#     hist, bin_edges = np.histogram(img.ravel(), bins=nbins, density=True)\n#     hist = hist.astype(float) * (bin_edges[1] - bin_edges[0])\n#     centre_bins = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n#\n#     hist_mul_val = hist * centre_bins\n#     sum_tot = np.sum(hist_mul_val)\n#\n#     threshold, target_max = centre_bins[0], 0\n#     sum_im, mean_im = 0, 0\n#     for i in range(0, hist.shape[0]-1):\n#         mean_im = mean_im + hist_mul_val[i]\n#         mean_ip = sum_tot - mean_im\n#\n#         sum_im = sum_im + hist[i]\n#         sum_ip = 1 - sum_im\n#\n#         target = sum_ip * sum_im * np.square(mean_ip/sum_ip - mean_im/sum_im)\n#         if target > target_max:\n#             threshold, target_max = centre_bins[i], target\n#     return threshold\n\n\n# Print iterations progress\ndef print_progress_bar(iteration, total,\n                       prefix=\'\', suffix=\'\', decimals=1, length=10, fill=\'=\'):\n    """"""\n    Call in a loop to create terminal progress bar\n\n    :param iteration: current iteration (Int)\n    :param total: total iterations (Int)\n    :param prefix: prefix string (Str)\n    :param suffix: suffix string (Str)\n    :param decimals: number of decimals in percent complete (Int)\n    :param length: character length of bar (Int)\n    :param fill: bar fill character (Str)\n    """"""\n    percent = (""{0:."" + str(decimals) + ""f}"").format(\n        100 * (iteration / float(total)))\n    filledLength = int(length * iteration // total)\n    bars = fill * filledLength + \'-\' * (length - filledLength)\n    print(\'\\r%s |%s| %s%% %s\' % (prefix, bars, percent, suffix), end=\'\\r\')\n    # Print New Line on Complete\n    if iteration == total:\n        print(\'\\n\')\n\n\ndef set_cuda_device(cuda_devices):\n    if re.findall(""\\\\d"", cuda_devices):\n        os.environ[""CUDA_VISIBLE_DEVICES""] = cuda_devices\n        tf.logging.info(\n            ""set CUDA_VISIBLE_DEVICES to {}"".format(cuda_devices))\n    else:\n        # using Tensorflow default choice\n        pass\n\n\nclass ParserNamespace(object):\n    """"""\n    Parser namespace for representing parsed parameters from config file\n\n    e.g.::\n\n        system_params = ParserNamespace(action=\'train\')\n        action_str = system_params.action\n\n    """"""\n\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\n    def update(self, **kwargs):\n        self.__dict__.update(kwargs)\n\n\ndef device_string(n_devices=0, device_id=0, is_worker=True, is_training=True):\n    """"""\n    assigning CPU/GPU based on user specifications\n    """"""\n    # pylint: disable=no-name-in-module\n    from tensorflow.python.client import device_lib\n    devices = device_lib.list_local_devices()\n    n_local_gpus = sum([x.device_type == \'GPU\' for x in devices])\n    if n_devices <= 0:  # user specified no gpu at all\n        return \'/cpu:{}\'.format(device_id)\n    if is_training:\n        # in training: use gpu only for workers whenever n_local_gpus\n        device = \'gpu\' if (is_worker and n_local_gpus > 0) else \'cpu\'\n        if device == \'gpu\' and device_id >= n_local_gpus:\n            tf.logging.warning(\n                \'trying to use gpu id %s, but only has %s GPU(s), \'\n                \'please set num_gpus to %s at most\',\n                device_id, n_local_gpus, n_local_gpus)\n            # raise ValueError\n        return \'/{}:{}\'.format(device, device_id)\n    # in inference: use gpu for everything whenever n_local_gpus\n    return \'/gpu:0\' if n_local_gpus > 0 else \'/cpu:0\'\n\n\ndef tf_config():\n    """"""\n    tensorflow system configurations\n    """"""\n    config = tf.ConfigProto()\n    config.log_device_placement = False\n    config.allow_soft_placement = True\n    return config\n\n\n'"
niftynet/utilities/util_csv.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport csv\nimport os\nimport sys\nfrom difflib import SequenceMatcher\n\nimport numpy as np\n\nfrom niftynet.io.misc_io import touch_folder\nfrom niftynet.utilities.filename_matching import KeywordsMatching\n\n\ndef match_first_degree(name_list1, name_list2):\n    """"""\n    First immediate matching between two possible name lists (exact equality\n    between one item of list1 and of list2\n    :param name_list1: First list of names to match\n    :param name_list2: Second list of names where to find a match\n    :return init_match1:\n    :return init_match2:\n    :return ind_match1: Indices of second list that correspond to each given\n    item of list 1 if exists (-1 otherwise)\n    :return ind_match2: Indices of first list that correspond to each given\n    item of list 2 if exists (-1) otherwise\n    """"""\n    if name_list1 is None or name_list2 is None:\n        return None, None, None, None\n    init_match1 = [\'\'] * len(name_list1)\n    init_match2 = [\'\'] * len(name_list2)\n    ind_match1 = [-1] * len(name_list1)\n    ind_match2 = [-1] * len(name_list2)\n    flatten_list1 = [item for sublist in name_list1 for item in sublist]\n    flatten_list2 = [item for sublist in name_list2 for item in sublist]\n    indflat_1 = [i for i in range(0, len(init_match1)) for item in\n                 name_list1[i] if init_match1[i] == \'\']\n    indflat_2 = [i for i in range(0, len(init_match2)) for item in\n                 name_list2[i] if init_match2[i] == \'\']\n    for i in range(0, len(name_list1)):\n        for name in name_list1[i]:\n            if name in flatten_list2:\n                init_match1[i] = name\n                ind_match1[i] = indflat_2[flatten_list2.index(name)]\n                break\n    for i in range(0, len(name_list2)):\n        for name in name_list2[i]:\n            if name in flatten_list1:\n                init_match2[i] = name\n                ind_match2[i] = indflat_1[flatten_list1.index(name)]\n                break\n    return init_match1, init_match2, ind_match1, ind_match2\n\n\ndef __find_max_overlap_in_list(name, list_names):\n    """"""\n    Given a name and list of names to match it to, find the maximum overlap\n    existing\n\n    :param name: string to match to any of list_names\n    :param list_names: list of candidate strings\n    :return match_seq: matched substring\n    :return index: index of element in list_names to which the match is\n    associated. Returns -1 if there is no found match\n    """"""\n    match_max = 0\n    match_seq = \'\'\n    match_orig = \'\'\n    match_ratio = 0\n    if not list_names:\n        return \'\', -1\n    for test in list_names:\n        if test:\n            match = SequenceMatcher(None, name, test).find_longest_match(\n                0, len(name), 0, len(test))\n            if match.size >= match_max \\\n                    and match.size / len(test) >= match_ratio:\n                match_max = match.size\n                match_seq = test[match.b:(match.b + match.size)]\n                match_ratio = match.size / len(test)\n                match_orig = test\n    if match_max == 0:\n        return \'\', -1\n    other_list = [name for name in list_names\n                  if match_seq in name and match_max / len(name) == match_ratio]\n    if len(other_list) > 1:\n        return \'\', -1\n    return match_seq, list_names.index(match_orig)\n\n\ndef match_second_degree(name_list1, name_list2):\n    """"""\n    Perform the double matching between two lists of\n    possible names.\n    First find the direct matches, remove them from\n    the ones still to match and\n    match the remaining ones using the maximum overlap.\n    Returns the name\n    match for each list, and the index correspondences.\n\n    More subtle matching with first direct matching and then secondary\n    overlap matching between list of list of potential names\n    :param name_list1:\n    :param name_list2:\n    :return init_match1:\n    :return ind_match1: Index of corresponding match in name_list2\n    :return init_match2: Matching string in list2\n    :return ind_match2: Index of corresponding match in name_list1\n    """"""\n    if name_list1 is None or name_list2 is None:\n        return None, None, None, None\n    init_match1, init_match2, ind_match1, ind_match2 = match_first_degree(\n        name_list1, name_list2)\n    reduced_list1 = [names for names in name_list1\n                     if init_match1[name_list1.index(names)] == \'\']\n    reduced_list2 = [names for names in name_list2\n                     if init_match2[name_list2.index(names)] == \'\']\n    redflat_1 = [item for sublist in reduced_list1 for item in sublist]\n    indflat_1 = [i for i in range(0, len(init_match1)) for item in\n                 name_list1[i] if init_match1[i] == \'\']\n    redflat_2 = [item for sublist in reduced_list2 for item in sublist]\n    indflat_2 = [i for i in range(0, len(init_match2)) for item in\n                 name_list2[i] if init_match2[i] == \'\']\n    for i in range(0, len(name_list1)):\n        if init_match1[i] == \'\':\n            for n in name_list1[i]:\n                init_match1[i], index = __find_max_overlap_in_list(n, redflat_2)\n                if index >= 0:\n                    ind_match1[i] = indflat_2[index]\n    for i in range(0, len(name_list2)):\n        if init_match2[i] == \'\':\n            for n in name_list2[i]:\n                init_match2[i], index = __find_max_overlap_in_list(n, redflat_1)\n                if index >= 0:\n                    ind_match2[i] = indflat_1[index]\n    return init_match1, ind_match1\n\n\n# From a list of list of names and a list of list of files that are\n# associated, find the name correspondence and therefore the files associations\ndef join_subject_id_and_filename_list(name_list, list_files):\n    """"""\n    From the list of list of names and the list of list of files\n    corresponding to each constraint find the association between a single\n    name id and the different file lists\n    :param name_list: list of list of names\n    :param list_files: list of list of files (one list per constraint)\n    :return list_combined: List per subject of name and list of files given\n    by the constraints\n    """"""\n    ind_max = np.argmax([len(names) for names in name_list])\n    name_max = name_list[ind_max]\n    name_tot = []\n    ind_tot = []\n    name_max_to_use = []\n    for c in range(0, len(list_files)):\n        name_match, ind_match = match_second_degree(name_max, name_list[c])\n        if c == ind_max:\n            name_max_to_use = name_match\n        name_tot.append(name_match)\n        ind_tot.append(ind_match)\n\n    list_combined = []\n    for (i, name) in enumerate(name_max_to_use):\n        list_temp = [name]\n        # To do : Taking care of the case when the list of a constraint is\n        # completely empty\n        for c in range(0, len(list_files)):\n            output = list_files[c][ind_tot[c][i]] if ind_tot[c][i] > -1 else \'\'\n            list_temp.append(output)\n        list_combined.append(list_temp)\n    return list_combined\n\n\ndef remove_duplicated_names(name_list):\n    """"""\n    From a list of list of names remove the items that are duplicated\n    :param name_list: list of list of names to investigate\n    :return duplicates_removed: list of list of names freed of duplicates\n    """"""\n    flattened_list = [item for sublist in name_list for item in sublist]\n    list_duplicated = [item for item in flattened_list\n                       if flattened_list.count(item) > 1]\n    duplicates_removed = []\n    for names in name_list:\n        duplicates_removed.append([name for name in names\n                                   if name not in list_duplicated])\n    return duplicates_removed\n\n\ndef write_csv(csv_file, list_combined):\n    # csv writer has different behaviour in python 2/3\n    if sys.version_info[0] >= 3:\n        with open(csv_file, \'w\', newline=\'\', encoding=\'utf8\') as csvfile:\n            file_writer = csv.writer(csvfile)\n            for list_temp in list_combined:\n                file_writer.writerow(list_temp)\n    else:\n        with open(csv_file, \'wb\') as csvfile:\n            file_writer = csv.writer(csvfile, delimiter=\',\')\n            for list_temp in list_combined:\n                file_writer.writerow(list_temp)\n    return\n\n\ndef match_and_write_filenames_to_csv(list_constraints, csv_file):\n    """"""\n    Combine all elements of file searching until finally writing the names\n    :param list_constraints: list of constraints (defined by list of paths to\n    search, list of elements the filename should contain and of those that\n    are forbidden\n    :param csv_file: file on which to write the final list of files.\n    :return:\n    """"""\n    name_tot = []\n    list_tot = []\n    if list_constraints is None or len(list_constraints) == 0:\n        return\n    for c in list_constraints:\n        list_files, name_list = \\\n            KeywordsMatching.matching_subjects_and_filenames(c)\n        name_list = remove_duplicated_names(name_list)\n        name_tot.append(name_list)\n        list_tot.append(list_files)\n    list_combined = join_subject_id_and_filename_list(name_tot, list_tot)\n    list_combined = filter(lambda names: \'\' not in names, list_combined)\n    list_combined = list(list_combined)\n    if not list_combined:\n        raise IOError(\'Nothing to write to {}\'.format(csv_file))\n    touch_folder(os.path.dirname(csv_file))\n    write_csv(csv_file, list_combined)\n\n    return list_combined\n'"
niftynet/utilities/util_import.py,1,"b'# -*- coding: utf-8 -*-\n"""""" check module to be imported""""""\nimport importlib\n\nimport tensorflow as tf\n\n\ndef require_module(name,\n                   min_version=None,\n                   descriptor=\'Optional\',\n                   mandatory=False):\n    """"""\n    Check if the module exists, and\n    satisfies the minimum version requirement.\n\n    Returns the imported module if it satisfies requirements.\n\n    Raises ImportError and AssertionError.\n\n    :param name:\n    :param min_version:\n    :param descriptor:\n    :param mandatory:\n    :return: the imported module\n    """"""\n\n    name = \'{}\'.format(name)\n    log_level = tf.logging.fatal if mandatory else tf.logging.info\n\n    try:\n        the_module = importlib.import_module(name)\n    except ImportError:\n        log_level(\n            descriptor + \' Python module %s not found, \'\n            \'please install %s and retry if the application fails.\',\n            name, name)\n        if mandatory:\n            raise\n\n    try:\n        if min_version is not None:\n            if isinstance(min_version, tuple):\n                version_number = the_module.__version__.split(\'.\')\n                min_version = tuple(int(v) for v in min_version)\n                mod_version = tuple(int(v) for v in version_number)\n            else:\n                mod_version = the_module.__version__\n                min_version = \'{}\'.format(min_version)\n\n            assert mod_version >= min_version\n        return the_module\n    except AttributeError:\n        pass\n    except (AssertionError, NameError):\n        log_level(\n            descriptor + \' Python module %s version %s not found, \'\n            \'please install %s-%s and retry if the application fails.\',\n            name, min_version, name, min_version)\n        if mandatory:\n            raise\n'"
niftynet/utilities/versioneer_version.py,0,"b'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""$Format:%d$""\n    git_full = ""$Format:%H$""\n    git_date = ""$Format:%ci$""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""pep440""\n    cfg.tag_prefix = ""v""\n    cfg.parentdir_prefix = ""None""\n    cfg.versionfile_source = ""niftynet/utilities/versioneer_version.py""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%s*"" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n'"
niftynet/utilities/versioning.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\nimport re\nfrom packaging import version\n\n\ndef get_niftynet_version_string():\n    """"""\n    Return a user-visible string describing the name and product version\n\n    This is a safe function that will never throw an exception\n    """"""\n\n    version_string = get_niftynet_version()\n    if not version_string:\n        version_string = ""unknown""\n\n    return ""NiftyNet version "" + version_string\n\n\ndef get_niftynet_version():\n    """"""\n    Return a user-visible string describing the product version.\n\n    This is a safe function that will never throw an exception.\n\n    :return: a PEP440-compliant version string on success, ``None`` otherwise\n    """"""\n\n    # Default: to be set only if conditions in the branches below are fulfilled\n    version_string = None\n\n    # Attempt to get the version string from the git repository\n    try:\n        from .versioneer_version import get_versions\n        version_info = get_versions()\n        if version_info[\'error\'] is None:\n            version_string = version_info[\'version\']\n        elif \'full-revisionid\' in version_info:\n            if version_info[\'full-revisionid\']:\n                version_string = \'{} ({})\'.format(\n                    version_info[\'full-revisionid\'], version_info[\'error\']\n                )\n    except:\n        pass  # version_string is None by default\n\n    # If we cannot get a git version, attempt to get a package version\n    if not version_string:\n        try:\n            import pkg_resources\n            version_string = pkg_resources.get_distribution(""niftynet"").version\n        except:\n            pass  # version_string is None by default\n\n    return version_string\n\ndef check_pep_440():\n    niftynet_version = get_niftynet_version()\n    # Regex for checking PEP 440 conformity\n    # https://www.python.org/dev/peps/pep-0440/#id79\n    pep440_regex = re.compile(\n        r""^\\s*"" + version.VERSION_PATTERN + r""\\s*$"",\n        re.VERBOSE | re.IGNORECASE,\n    )\n\n    # Check PEP 440 conformity\n    if niftynet_version is not None and \\\n            pep440_regex.match(niftynet_version) is None:\n        raise ValueError(\'The version string {} does not conform to\'\n                         \' PEP 440\'.format(niftynet_version))\n\n'"
demos/Learning_Rate_Decay/Demo_applications/decay_lr_comparison_application.py,7,"b'import tensorflow as tf\n\nfrom niftynet.application.segmentation_application import \\\n    SegmentationApplication\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.application_variables import TF_SUMMARIES\nfrom niftynet.layer.loss_segmentation import LossFunction\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'weight\'])\n\n\nclass DecayLearningRateApplication(SegmentationApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, is_training):\n        SegmentationApplication.__init__(\n            self, net_param, action_param, is_training)\n        tf.logging.info(\'starting decay learning segmentation application\')\n        self.learning_rate = None\n        self.current_lr = action_param.lr\n        if self.action_param.validation_every_n > 0:\n            raise NotImplementedError(""validation process is not implemented ""\n                                      ""in this demo."")\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        data_dict = self.get_sampler()[0][0].pop_batch_op()\n        image = tf.cast(data_dict[\'image\'], tf.float32)\n        net_out = self.net(image, self.is_training)\n\n        if self.is_training:\n            with tf.name_scope(\'Optimiser\'):\n                self.learning_rate = tf.placeholder(tf.float32, shape=[])\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.learning_rate)\n            loss_func = LossFunction(\n                n_class=self.segmentation_param.num_classes,\n                loss_type=self.action_param.loss_type)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None),\n                weight_map=data_dict.get(\'weight\', None))\n\n            loss = data_loss\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            grads = self.optimiser.compute_gradients(loss)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.learning_rate, name=\'lr\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n        else:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            SegmentationApplication.connect_data_and_network(\n                self, outputs_collector, gradients_collector)\n\n    def set_iteration_update(self, iteration_message):\n        """"""\n        This function will be called by the application engine at each\n        iteration.\n        """"""\n        current_iter = iteration_message.current_iter\n        if iteration_message.is_training:\n            if current_iter > 0 and current_iter % 5 == 0:\n                self.current_lr = self.current_lr / 1.02\n            iteration_message.data_feed_dict[self.is_validation] = False\n        elif iteration_message.is_validation:\n            iteration_message.data_feed_dict[self.is_validation] = True\n        iteration_message.data_feed_dict[self.learning_rate] = self.current_lr\n'"
demos/Learning_Rate_Decay/Demo_applications/no_decay_lr_comparison_application.py,7,"b'import tensorflow as tf\n\nfrom niftynet.application.segmentation_application import \\\n    SegmentationApplication\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.application_variables import TF_SUMMARIES\nfrom niftynet.layer.loss_segmentation import LossFunction\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'weight\'])\n\n\nclass DecayLearningRateApplication(SegmentationApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, is_training):\n        SegmentationApplication.__init__(\n            self, net_param, action_param, is_training)\n        tf.logging.info(\'starting decay learning segmentation application\')\n        self.learning_rate = None\n        self.current_lr = action_param.lr\n        if self.action_param.validation_every_n > 0:\n            raise NotImplementedError(""validation process is not implemented ""\n                                      ""in this demo."")\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        data_dict = self.get_sampler()[0][0].pop_batch_op()\n        image = tf.cast(data_dict[\'image\'], tf.float32)\n        net_out = self.net(image, self.is_training)\n\n        if self.is_training:\n            with tf.name_scope(\'Optimiser\'):\n                self.learning_rate = tf.placeholder(tf.float32, shape=[])\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.learning_rate)\n            loss_func = LossFunction(\n                n_class=self.segmentation_param.num_classes,\n                loss_type=self.action_param.loss_type)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None),\n                weight_map=data_dict.get(\'weight\', None))\n\n            loss = data_loss\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            grads = self.optimiser.compute_gradients(loss)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'dice_loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.learning_rate, name=\'lr\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'dice_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n        else:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            SegmentationApplication.connect_data_and_network(\n                self, outputs_collector, gradients_collector)\n\n    def set_iteration_update(self, iteration_message):\n        """"""\n        This function will be called by the application engine at each\n        iteration.\n        """"""\n        current_iter = iteration_message.current_iter\n        if iteration_message.is_training:\n            iteration_message.data_feed_dict[self.is_validation] = False\n        elif iteration_message.is_validation:\n            iteration_message.data_feed_dict[self.is_validation] = True\n        iteration_message.data_feed_dict[self.learning_rate] = self.current_lr\n'"
demos/PyTorchNiftyNet/libs/__init__.py,0,b''
demos/PyTorchNiftyNet/libs/dataset_niftynet.py,0,"b'import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass DatasetNiftySampler(Dataset):\n    """"""\n    A simple adapter\n    converting NiftyNet sampler\'s output into PyTorch Dataset properties\n    """"""\n    def __init__(self, sampler):\n        super(DatasetNiftySampler, self).__init__()\n        self.sampler = sampler\n\n    def __getitem__(self, index):\n        data = self.sampler(idx=index)\n\n        # Transpose to PyTorch format\n        image = np.transpose(data[\'image\'], (0, 5, 1, 2, 3, 4))\n        label = np.transpose(data[\'label\'], (0, 5, 1, 2, 3, 4))\n\n        image = torch.from_numpy(image).float()\n        label = torch.from_numpy(label).float()\n\n        return image, label\n\n    def __len__(self):\n        return len(self.sampler.reader.output_list)\n'"
demos/PyTorchNiftyNet/libs/loss.py,0,"b'import torch\nimport torch.nn as nn\n\n\nclass SoftDiceLoss(nn.Module):\n\n    def __init__(self):\n        super(SoftDiceLoss, self).__init__()\n\n    def forward(self, output, label):\n        probs = output.view(-1)\n        mask = label.view(-1)\n        smooth = 1\n        intersection = torch.sum(probs * mask)\n        den1 = torch.sum(probs)\n        den2 = torch.sum(mask)\n        soft_dice = (2 * intersection + smooth) / (den1 + den2 + smooth)\n        return -soft_dice\n\n\ndef dice(input, target):\n    epsilon = 1e-8\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    return 2 * intersection / (iflat.sum() + tflat.sum() + epsilon)\n'"
demos/PyTorchNiftyNet/libs/model.py,0,"b'""""""\nCode from the following repository:\nhttps://github.com/pykao/Modified-3D-UNet-Pytorch\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Modified3DUNet(nn.Module):\n    def __init__(self, in_channels, n_classes, base_n_filter=8):\n        super(Modified3DUNet, self).__init__()\n        self.in_channels = in_channels\n        self.n_classes = n_classes\n        self.base_n_filter = base_n_filter\n\n        self.lrelu = nn.LeakyReLU()\n        self.dropout3d = nn.Dropout3d(p=0.6)\n        self.upsacle = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.softmax = nn.Softmax(dim=1)\n\n        # Level 1 context pathway\n        self.conv3d_c1_1 = nn.Conv3d(\n            self.in_channels, self.base_n_filter,\n            kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.conv3d_c1_2 = nn.Conv3d(\n            self.base_n_filter, self.base_n_filter,\n            kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.lrelu_conv_c1 = self.lrelu_conv(\n            self.base_n_filter, self.base_n_filter)\n        self.inorm3d_c1 = nn.InstanceNorm3d(self.base_n_filter)\n\n        # Level 2 context pathway\n        self.conv3d_c2 = nn.Conv3d(\n            self.base_n_filter, self.base_n_filter * 2,\n            kernel_size=3, stride=2, padding=1, bias=False\n        )\n        self.norm_lrelu_conv_c2 = self.norm_lrelu_conv(\n            self.base_n_filter * 2, self.base_n_filter * 2)\n        self.inorm3d_c2 = nn.InstanceNorm3d(self.base_n_filter * 2)\n\n        # Level 3 context pathway\n        self.conv3d_c3 = nn.Conv3d(\n            self.base_n_filter * 2, self.base_n_filter * 4,\n            kernel_size=3, stride=2, padding=1, bias=False\n        )\n        self.norm_lrelu_conv_c3 = self.norm_lrelu_conv(\n            self.base_n_filter * 4, self.base_n_filter * 4)\n        self.inorm3d_c3 = nn.InstanceNorm3d(self.base_n_filter * 4)\n\n        # Level 4 context pathway\n        self.conv3d_c4 = nn.Conv3d(\n            self.base_n_filter * 4, self.base_n_filter * 8,\n            kernel_size=3, stride=2, padding=1, bias=False\n        )\n        self.norm_lrelu_conv_c4 = self.norm_lrelu_conv(\n            self.base_n_filter * 8, self.base_n_filter * 8)\n        self.inorm3d_c4 = nn.InstanceNorm3d(self.base_n_filter * 8)\n\n        # Level 5 context pathway, level 0 localization pathway\n        self.conv3d_c5 = nn.Conv3d(\n            self.base_n_filter * 8, self.base_n_filter * 16,\n            kernel_size=3, stride=2, padding=1, bias=False\n        )\n        self.norm_lrelu_conv_c5 = self.norm_lrelu_conv(\n            self.base_n_filter * 16, self.base_n_filter * 16)\n        self.norm_lrelu_upscale_conv_norm_lrelu_l0 = \\\n            self.norm_lrelu_upscale_conv_norm_lrelu(\n                self.base_n_filter * 16, self.base_n_filter * 8)\n\n        self.conv3d_l0 = nn.Conv3d(\n            self.base_n_filter * 8, self.base_n_filter * 8,\n            kernel_size=1, stride=1, padding=0, bias=False\n        )\n        self.inorm3d_l0 = nn.InstanceNorm3d(self.base_n_filter * 8)\n\n        # Level 1 localization pathway\n        self.conv_norm_lrelu_l1 = self.conv_norm_lrelu(\n            self.base_n_filter * 16, self.base_n_filter * 16)\n        self.conv3d_l1 = nn.Conv3d(\n            self.base_n_filter * 16, self.base_n_filter * 8,\n            kernel_size=1, stride=1, padding=0, bias=False\n        )\n        self.norm_lrelu_upscale_conv_norm_lrelu_l1 = \\\n            self.norm_lrelu_upscale_conv_norm_lrelu(\n                self.base_n_filter * 8, self.base_n_filter * 4)\n\n        # Level 2 localization pathway\n        self.conv_norm_lrelu_l2 = self.conv_norm_lrelu(\n            self.base_n_filter * 8, self.base_n_filter * 8)\n        self.conv3d_l2 = nn.Conv3d(\n            self.base_n_filter * 8, self.base_n_filter * 4,\n            kernel_size=1, stride=1, padding=0, bias=False\n        )\n        self.norm_lrelu_upscale_conv_norm_lrelu_l2 = \\\n            self.norm_lrelu_upscale_conv_norm_lrelu(\n                self.base_n_filter * 4, self.base_n_filter * 2)\n\n        # Level 3 localization pathway\n        self.conv_norm_lrelu_l3 = self.conv_norm_lrelu(\n            self.base_n_filter * 4, self.base_n_filter * 4)\n        self.conv3d_l3 = nn.Conv3d(\n            self.base_n_filter * 4, self.base_n_filter * 2,\n            kernel_size=1, stride=1, padding=0, bias=False\n        )\n        self.norm_lrelu_upscale_conv_norm_lrelu_l3 = \\\n            self.norm_lrelu_upscale_conv_norm_lrelu(\n                self.base_n_filter * 2, self.base_n_filter)\n\n        # Level 4 localization pathway\n        self.conv_norm_lrelu_l4 = self.conv_norm_lrelu(\n            self.base_n_filter * 2, self.base_n_filter * 2)\n        self.conv3d_l4 = nn.Conv3d(\n            self.base_n_filter * 2, self.n_classes,\n            kernel_size=1, stride=1, padding=0, bias=False\n        )\n\n        self.ds2_1x1_conv3d = nn.Conv3d(\n            self.base_n_filter * 8, self.n_classes,\n            kernel_size=1, stride=1, padding=0, bias=False\n        )\n        self.ds3_1x1_conv3d = nn.Conv3d(\n            self.base_n_filter * 4, self.n_classes,\n            kernel_size=1, stride=1, padding=0, bias=False\n        )\n\n    def conv_norm_lrelu(self, feat_in, feat_out):\n        return nn.Sequential(\n            nn.Conv3d(feat_in, feat_out, kernel_size=3,\n                      stride=1, padding=1, bias=False),\n            nn.InstanceNorm3d(feat_out),\n            nn.LeakyReLU())\n\n    def norm_lrelu_conv(self, feat_in, feat_out):\n        return nn.Sequential(\n            nn.InstanceNorm3d(feat_in),\n            nn.LeakyReLU(),\n            nn.Conv3d(feat_in, feat_out,\n                      kernel_size=3, stride=1, padding=1, bias=False))\n\n    def lrelu_conv(self, feat_in, feat_out):\n        return nn.Sequential(\n            nn.LeakyReLU(),\n            nn.Conv3d(feat_in, feat_out,\n                      kernel_size=3, stride=1, padding=1, bias=False))\n\n    def norm_lrelu_upscale_conv_norm_lrelu(self, feat_in, feat_out):\n        return nn.Sequential(\n            nn.InstanceNorm3d(feat_in),\n            nn.LeakyReLU(),\n            nn.Upsample(scale_factor=2, mode=\'nearest\'),\n            # should be feat_in*2 or feat_in\n            nn.Conv3d(feat_in, feat_out, kernel_size=3,\n                      stride=1, padding=1, bias=False),\n            nn.InstanceNorm3d(feat_out),\n            nn.LeakyReLU())\n\n    def forward(self, x):\n        #  Level 1 context pathway\n        out = self.conv3d_c1_1(x)\n        residual_1 = out\n        out = self.lrelu(out)\n        out = self.conv3d_c1_2(out)\n        out = self.dropout3d(out)\n        out = self.lrelu_conv_c1(out)\n        # Element Wise Summation\n        out += residual_1\n        context_1 = self.lrelu(out)\n        out = self.inorm3d_c1(out)\n        out = self.lrelu(out)\n\n        # Level 2 context pathway\n        out = self.conv3d_c2(out)\n        residual_2 = out\n        out = self.norm_lrelu_conv_c2(out)\n        out = self.dropout3d(out)\n        out = self.norm_lrelu_conv_c2(out)\n        out += residual_2\n        out = self.inorm3d_c2(out)\n        out = self.lrelu(out)\n        context_2 = out\n\n        # Level 3 context pathway\n        out = self.conv3d_c3(out)\n        residual_3 = out\n        out = self.norm_lrelu_conv_c3(out)\n        out = self.dropout3d(out)\n        out = self.norm_lrelu_conv_c3(out)\n        out += residual_3\n        out = self.inorm3d_c3(out)\n        out = self.lrelu(out)\n        context_3 = out\n\n        # Level 4 context pathway\n        out = self.conv3d_c4(out)\n        residual_4 = out\n        out = self.norm_lrelu_conv_c4(out)\n        out = self.dropout3d(out)\n        out = self.norm_lrelu_conv_c4(out)\n        out += residual_4\n        out = self.inorm3d_c4(out)\n        out = self.lrelu(out)\n        context_4 = out\n\n        # Level 5\n        out = self.conv3d_c5(out)\n        residual_5 = out\n        out = self.norm_lrelu_conv_c5(out)\n        out = self.dropout3d(out)\n        out = self.norm_lrelu_conv_c5(out)\n        out += residual_5\n        out = self.norm_lrelu_upscale_conv_norm_lrelu_l0(out)\n\n        out = self.conv3d_l0(out)\n        out = self.inorm3d_l0(out)\n        out = self.lrelu(out)\n\n        # Level 1 localization pathway\n        out = torch.cat([out, context_4], dim=1)\n        out = self.conv_norm_lrelu_l1(out)\n        out = self.conv3d_l1(out)\n        out = self.norm_lrelu_upscale_conv_norm_lrelu_l1(out)\n\n        # Level 2 localization pathway\n        out = torch.cat([out, context_3], dim=1)\n        out = self.conv_norm_lrelu_l2(out)\n        ds2 = out\n        out = self.conv3d_l2(out)\n        out = self.norm_lrelu_upscale_conv_norm_lrelu_l2(out)\n\n        # Level 3 localization pathway\n        out = torch.cat([out, context_2], dim=1)\n        out = self.conv_norm_lrelu_l3(out)\n        ds3 = out\n        out = self.conv3d_l3(out)\n        out = self.norm_lrelu_upscale_conv_norm_lrelu_l3(out)\n\n        # Level 4 localization pathway\n        out = torch.cat([out, context_1], dim=1)\n        out = self.conv_norm_lrelu_l4(out)\n        out_pred = self.conv3d_l4(out)\n\n        ds2_1x1_conv = self.ds2_1x1_conv3d(ds2)\n        ds1_ds2_sum_upscale = self.upsacle(ds2_1x1_conv)\n        ds3_1x1_conv = self.ds3_1x1_conv3d(ds3)\n        ds1_ds2_sum_upscale_ds3_sum = ds1_ds2_sum_upscale + ds3_1x1_conv\n        ds1_ds2_sum_upscale_ds3_sum_upscale = self.upsacle(\n            ds1_ds2_sum_upscale_ds3_sum)\n\n        out = out_pred + ds1_ds2_sum_upscale_ds3_sum_upscale\n        seg_layer = out\n        out = out.permute(0, 2, 3, 4, 1).contiguous().view(-1, self.n_classes)\n        out = self.softmax(out)\n        return F.sigmoid(seg_layer)\n'"
niftynet/contrib/checkpoint_tools/__init__.py,0,b''
niftynet/contrib/checkpoint_tools/rename_checkpoint_to_partial.py,7,"b'# -*- coding: utf-8 -*-\nimport sys,glob,csv\n\nimport tensorflow as tf\n\ndef rename_checkpoint_to_partial(source,target,transform):\n  vars=tf.contrib.framework.list_variables(source)\n  var_names = [v.split(\'/\') for v,s in vars]\n  transform_pairs = []\n  for s_name,t_name in transform:\n    print(vars,s_name,t_name)\n    if s_name[-1]==\'/\' and t_name[-1]==\'/\': # scope\n      s_names=s_name.split(\'/\')[:-1]\n      t_names=t_name.split(\'/\')[:-1]\n      transform_pairs += [(\'/\'.join(v),\'/\'.join(t_names+v[len(s_names):])) for v in var_names if v[:len(s_names)]==s_names]\n    elif s_name[-1]!=\'/\' and t_name[-1]!=\'/\': # variable\n      if s_name in [v for v,s in vars]:\n        transform_pairs.append((s_name,t_name))\n    else:\n      raise ValueError(\'Cannot rename a variable to a scope or vice versa: %s->%s\' %(s_name,t_name))\n  print(transform_pairs)\n  g = tf.Graph()\n  with g.as_default():\n    with tf.Session() as sess:\n      for s_name,t_name in transform_pairs:\n        var = tf.contrib.framework.load_variable(source, s_name)\n        var = tf.Variable(var, name=t_name)\n      saver = tf.train.Saver()\n      sess.run(tf.global_variables_initializer())\n      saver.save(sess, target)\nusage = \\\n""""""%s source_checkpoint destination_checkpoint rename_file\nrename_file has the format:\n\nsource_scope1,renamed_scope1\nsource_scope2/variable1,renamed_scope2/renamed_variable1\n\nwhich will rename source_scope1/* to renamed_scope1/* and source_scope2/variable1 to renamed_scope2/renamed_variable1\n"""""" %sys.argv[0]\n\ndef main(argv):\n  if len(argv)<3:\n    print(usage)\n    return 2\n  if not glob.glob(argv[0]+\'.index\'):\n    print(\'Checkpoint %s does not exist\' % argv[0])\n    return 2\n  if not glob.glob(argv[2]):\n    print(\'Transform file %s does not exist\' % argv[2])\n    return 2\n  with open(argv[2],\'rb\') as csvfile:\n    r=csv.reader(csvfile)\n    rows=[row for row in r]\n  if any(len(row)!=2 for row in rows):\n    print(\'Error %s: each line must have a source and target variable name\' %(argv[2]))\n    return 2\n  rename_checkpoint_to_partial(argv[0],argv[1],argv[2])\n  return 0\n\nif __name__ == \'__main__\':\n    sys.exit(main(sys.argv[1:]))\n'"
niftynet/contrib/csv_reader/__init__.py,0,b''
niftynet/contrib/csv_reader/class_seg_finnet.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom six.moves import range\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.activation import ActiLayer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer\nfrom niftynet.layer.convolution import ConvLayer, ConvolutionalLayer\nfrom niftynet.layer.dilatedcontext import DilatedTensor\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.network.base_net import BaseNet\nfrom niftynet.layer.layer_util import infer_spatial_rank\nfrom niftynet.layer.fully_connected import FullyConnectedLayer\n# from niftynet.layer.pool_full import PoolingLayer\nimport tensorflow as tf\n\n\nclass ClassSegFinnet(BaseNet):\n    """"""\n    implementation of HighRes3DNet:\n      Li et al., ""On the compactness, efficiency, and representation of 3D\n      convolutional networks: Brain parcellation as a pretext task"", IPMI \'17\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'prelu\',\n                 name=\'FinalClassSeg\'):\n\n        super(ClassSegFinnet, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n        self.num_classes = num_classes\n        self.layers = [\n            {\'name\': \'fc_seg\', \'n_features\': 10,\n             \'kernel_size\': 1},\n            {\'name\': \'pool\', \'n_features\': 10,\n             \'stride\': 1, \'func\': \'AVG\'},\n            {\'name\': \'fc_seg\', \'n_features\': num_classes,\n             \'kernel_size\': 1},\n            {\'name\': \'fc_class\', \'n_features\': 2, \'kernel_size\': 1}]\n\n    def layer_op(self, images, is_training, layer_id=-1):\n        # go through self.layers, create an instance of each layer\n        # and plugin data\n        layer_instances = []\n\n        # class convolution layer\n        params = self.layers[0]\n        fc_seg = ConvolutionalLayer(\n            with_bn=False,\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            padding=\'VALID\',\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_seg(images, is_training)\n        layer_instances.append((fc_seg, flow))\n\n        # pooling layer\n        params = self.layers[1]\n        # pool_layer = PoolingLayer(\n        #     func=params[\'func\'],\n        #     name=params[\'name\'])\n        # flow_pool = pool_layer(flow)\n        flow_pool = flow\n        flow_pool = tf.reshape(flow_pool, [tf.shape(images)[0], 1, 1, 1,\n                                           self.layers[1][\n            \'n_features\']])\n        print(""check flow pooling"", flow_pool.shape)\n        layer_instances.append((pool_layer, flow_pool))\n\n        # seg convolution layer\n        params = self.layers[2]\n        seg_conv_layer = ConvolutionalLayer(\n            with_bn=False,\n            n_output_chns=params[\'n_features\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            kernel_size=1,\n            name=params[\'name\'])\n        seg_flow = seg_conv_layer(flow, is_training)\n        layer_instances.append((seg_conv_layer, seg_flow))\n\n        # class convolution layer\n        params = self.layers[3]\n        class_conv_layer = ConvolutionalLayer(\n            with_bn=False,\n            n_output_chns=params[\'n_features\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            kernel_size=1,\n            name=params[\'name\'])\n        class_flow = class_conv_layer(flow_pool, is_training)\n        layer_instances.append((class_conv_layer, class_flow))\n\n        # set training properties\n        if is_training:\n            self._print(layer_instances)\n            return layer_instances[-2][1], layer_instances[-1][1]\n        return layer_instances[-2][1], layer_instances[layer_id][1]\n\n    def _print(self, list_of_layers):\n        for (op, _) in list_of_layers:\n            print(op)\n'"
niftynet/contrib/csv_reader/classification_application.py,17,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines an image-level classification application\nthat maps from images to scalar, multi-class labels.\n\nThis class is instantiated and initalized by the application_driver.\n""""""\n\nimport os\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.contrib.csv_reader.sampler_resize_v2_csv import ResizeSamplerCSV as ResizeSampler\n# from niftynet.engine.windows_aggregator_classifier import \\\n#     ClassifierSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.contrib.csv_reader.csv_reader import CSVReader\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.loss_classification import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.evaluation.classification_evaluator import ClassificationEvaluator\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'sampler\', \'inferred\'])\n\n\nclass ClassificationApplication(BaseApplication):\n    """"""This class defines an application for image-level classification\n    problems mapping from images to scalar labels.\n\n    This is the application class to be instantiated by the driver\n    and referred to in configuration files.\n\n    Although structurally similar to segmentation, this application\n    supports different samplers/aggregators (because patch-based\n    processing is not appropriate), and monitoring metrics.""""""\n\n    REQUIRED_CONFIG_SECTION = ""CLASSIFICATION""\n\n    def __init__(self, net_param, action_param, action):\n        super(ClassificationApplication, self).__init__()\n        tf.logging.info(\'starting classification application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.classification_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.classification_param = task_param\n\n        if self.is_training:\n            image_reader_names = (\'image\', \'sampler\')\n            csv_reader_names = (\'label\',)\n        elif self.is_inference:\n            image_reader_names = (\'image\',)\n        elif self.is_evaluation:\n            image_reader_names = (\'image\', \'inferred\')\n            csv_reader_names = (\'label\',)\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(image_reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n        self.csv_readers = [\n            CSVReader(csv_reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=self.net_param.foreground_type,\n            multimod_fusion=self.net_param.multimod_foreground_type,\n            threshold=0.0) \\\n            if self.net_param.normalise_foreground_only else None\n\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer) \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            binary_masking_func=foreground_masking_layer,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n\n        label_normaliser = DiscreteLabelNormalisationLayer(\n            image_name=\'label\',\n            modalities=vars(task_param).get(\'label\'),\n            model_filename=self.net_param.histogram_ref_file) \\\n            if (self.net_param.histogram_ref_file and\n                task_param.label_normalisation) else None\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n        if label_normaliser is not None:\n            normalisation_layers.append(label_normaliser)\n\n        augmentation_layers = []\n        if self.is_training:\n            train_param = self.action_param\n            if train_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=train_param.random_flipping_axes))\n            if train_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=train_param.scaling_percentage[0],\n                    max_percentage=train_param.scaling_percentage[1]))\n            if train_param.rotation_angle or \\\n                    self.action_param.rotation_angle_x or \\\n                    self.action_param.rotation_angle_y or \\\n                    self.action_param.rotation_angle_z:\n                rotation_layer = RandomRotationLayer()\n                if train_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        train_param.rotation_angle)\n                else:\n                    rotation_layer.init_non_uniform_angle(\n                        self.action_param.rotation_angle_x,\n                        self.action_param.rotation_angle_y,\n                        self.action_param.rotation_angle_z)\n                augmentation_layers.append(rotation_layer)\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n             normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(normalisation_layers)\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=image_reader,\n            csv_reader=csv_reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            queue_length=self.net_param.queue_length) for image_reader, csv_reader in\n            zip(self.readers, self.csv_readers)]]\n\n    def initialise_aggregator(self):\n        self.output_decoder = ClassifierSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        else:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=self.classification_param.num_classes,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def add_confusion_matrix_summaries_(self,\n                                        outputs_collector,\n                                        net_out,\n                                        data_dict):\n        """""" This method defines several monitoring metrics that\n        are derived from the confusion matrix """"""\n        labels = tf.reshape(tf.cast(data_dict[\'label\'], tf.int64), [-1])\n        prediction = tf.reshape(tf.argmax(net_out, -1), [-1])\n        num_classes = self.classification_param.num_classes\n        conf_mat = tf.confusion_matrix(labels, prediction, num_classes)\n        conf_mat = tf.to_float(conf_mat)\n        if self.classification_param.num_classes == 2:\n            outputs_collector.add_to_collection(\n                var=conf_mat[1][1], name=\'true_positives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[1][0], name=\'false_negatives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[0][1], name=\'false_positives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[0][0], name=\'true_negatives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n        else:\n            outputs_collector.add_to_collection(\n                var=conf_mat[tf.newaxis, :, :, tf.newaxis],\n                name=\'confusion_matrix\',\n                average_over_devices=True, summary_type=\'image\',\n                collection=TF_SUMMARIES)\n\n        outputs_collector.add_to_collection(\n            var=tf.trace(conf_mat), name=\'accuracy\',\n            average_over_devices=True, summary_type=\'scalar\',\n            collection=TF_SUMMARIES)\n\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(\n                n_class=self.classification_param.num_classes,\n                loss_type=self.action_param.loss_type)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None))\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n            grads = self.optimiser.compute_gradients(\n                loss, colocate_gradients_with_ops=True)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'data_loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'data_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            self.add_confusion_matrix_summaries_(outputs_collector,\n                                                 net_out,\n                                                 data_dict)\n        else:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n            tf.logging.info(\n                \'net_out.shape may need to be resized: %s\', net_out.shape)\n            output_prob = self.classification_param.output_prob\n            num_classes = self.classification_param.num_classes\n            if output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'SOFTMAX\', num_classes=num_classes)\n            elif not output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'ARGMAX\', num_classes=num_classes)\n            else:\n                post_process_layer = PostProcessingLayer(\n                    \'IDENTITY\', num_classes=num_classes)\n            net_out = post_process_layer(net_out)\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        if not self.is_training:\n            return self.output_decoder.decode_batch(\n                batch_output[\'window\'], batch_output[\'location\'])\n        return True\n\n    def initialise_evaluator(self, eval_param):\n        self.eval_param = eval_param\n        self.evaluator = ClassificationEvaluator(self.readers[0],\n                                                 self.classification_param,\n                                                 eval_param)\n\n    def add_inferred_output(self, data_param, task_param):\n        return self.add_inferred_output_like(data_param, task_param, \'label\')\n'"
niftynet/contrib/csv_reader/csv_reader.py,8,"b'\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.python.data.util import nest\n\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.io.image_reader import param_to_dict\n\n\nclass CSVReader(Layer):\n    \'\'\'\n    Class that performs the reading of the csv_input and select the lines\n    according to the subject id if available\n    \'\'\'\n    def __init__(self, names=None):\n        self.names = names\n        self._paths = None\n        self._labels = None\n        self._df = None\n        self.label_names = None\n        self.n_samples_per_id = None\n        self.dims = None\n        self.data_param = None\n        self._dims = None\n        self._indexable_output = {}\n        self.file_list = None\n        self.subject_ids = None\n        self._shapes = {}\n        self._input_sources = None\n        self.df_by_task = {}\n        self.valid_by_task = {}\n        self.pad_by_task = {}\n        self.dims_by_task = {}\n        self.type_by_task = {}\n        self._dtypes = {}\n        self.task_param = None\n        super(CSVReader, self).__init__(name=\'csv_reader\')\n    \n    def initialise(self, data_param, task_param=None, file_list=None,\n                   sample_per_volume=1):\n        """"""\n        this function takes in a data_param specifying the name of the source and the location of\n        the csv data. Three input modes are supported:\n        - \'label\' - expects a csv with header subject_id,label.\n        - \'features\' - expects a csv with header subject_id,<name of feature 1>,<name of feature 2>\n        e.g.::\n\n             data_param = {\'label\': {\'csv_data_file\': \'path/to/some_data.csv\', \'to_ohe\': False}}\n             \n        :param data_param: dictionary of input sections\n        :param task_param: Namespace object\n        :param file_list: a dataframe generated by ImagePartitioner\n        :param sample_per_volume: number of samples taken per volume (useful\n        to know how much to tile the csv output\n            for cross validation, so\n            that the reader only loads files in training/inference phases.\n        """"""\n        assert self.names is not None\n        data_param = param_to_dict(data_param)\n        self.n_samples_per_id = sample_per_volume\n        print(data_param)\n        if not task_param:\n            task_param = {mod: (mod,) for mod in list(data_param)}\n        try:\n            if not isinstance(task_param, dict):\n                task_param = vars(task_param)\n        except ValueError:\n            tf.logging.fatal(\n                ""To concatenate multiple input data arrays,\\n""\n                ""task_param should be a dictionary in the form:\\n""\n                ""{\'new_modality_name\': [\'modality_1\', \'modality_2\',...]}."")\n            raise\n        self.task_param = task_param\n        valid_names = [name for name in self.names if self.task_param.get(\n            name, None)]\n        if not valid_names:\n            tf.logging.fatal(""CSVReader requires task input keywords %s, but ""\n                             ""not exist in the config file.\\n""\n                             ""Available task keywords: %s"",\n                             self.names, list(self.task_param))\n            raise ValueError\n        self.names = valid_names\n        self.data_param = data_param\n        self._dims = None\n        self._indexable_output = {}\n        self.file_list = file_list\n        self.subject_ids = self.file_list[\'subject_id\'].values\n\n        self._input_sources = dict((name, self.task_param.get(name))\n                                   for name in self.names)\n        self.df_by_task = {}\n        self.valid_by_task = {}\n        self.pad_by_task = {}\n        self.dims_by_task = {}\n        self.type_by_task = {}\n\n        for name in valid_names:\n            df_fin, _indexable_output, _dims = self._parse_csv(\n                path_to_csv=data_param[name].get(\'csv_data_file\', None),\n                to_ohe=data_param[name].get(\'to_ohe\', False)\n            )\n            self.df_by_task[name] = df_fin\n            self.dims_by_task[name] = _dims\n            self._indexable_output[name] = _indexable_output\n            self.valid_by_task[name] = -1 * np.ones( [self.df_by_task[name].shape[0]])  # -1 means they have not been checked\n\n            self.pad_by_task[name] = np.zeros(\n                [self.df_by_task[name].shape[0], 2*_dims])\n            if df_fin.shape[0] > len(set(self.subject_ids)):\n                self.type_by_task[name] = \'multi\'\n            else:\n                self.type_by_task[name] = \'mono\'\n        # Converts Dictionary of Lists to List of Dictionaries\n        # self._indexable_output = pd.DataFrame(\n        # self._indexable_output).to_dict(\'records\')\n        assert file_list is not None\n        return self\n    \n    def _parse_csv(self, path_to_csv, to_ohe):\n        tf.logging.warning(\'This method will read your entire csv into memory\')\n        df_init = pd.read_csv(path_to_csv, index_col=0, header=None)\n\n        df_init.index = df_init.index.map(str)\n\n        if set(df_init.index) != set(self.subject_ids):\n            print(""probably different because of split file - drop not ""\n                  ""relevant ones"")\n            df_fin = df_init.drop(index=[s for s in set(df_init.index) if s\n                                         not in set(self.subject_ids)])\n            # df.reset_index(drop=True, inplace=True)\n\n            if set(df_fin.index) != set(self.subject_ids):\n                print(set(self.subject_ids) - set(df_fin.index))\n                tf.logging.fatal(\'csv file provided at: {} does not have \'\n                                 \'all the subject_ids\'.format(path_to_csv))\n                raise Exception\n        else:\n            df_fin = df_init.copy()\n        if to_ohe and len(df_fin.columns) == 1:\n            _dims = len(list(df_fin[1].unique()))\n            _indexable_output = self.to_ohe(df_fin[1].values, _dims)\n            return df_fin, _indexable_output, _dims\n        elif not to_ohe and len(df_fin.columns) == 1:\n            _dims = 1\n            _indexable_output = self.to_categorical(df_fin[1].values,\n                                                    np.sort(df_fin[1].unique()))\n            return df_fin, _indexable_output, _dims\n        elif not to_ohe:\n            _dims = len(df_fin.columns)\n            _indexable_output = list(df_fin.values)\n            return df_fin, _indexable_output, _dims\n        tf.logging.fatal(\'Unrecognised input format for {}\'.format(path_to_csv))\n        raise Exception(\'Unrecognised input format for {}\'.format(path_to_csv))\n\n    @staticmethod\n    def to_ohe(labels, _dims):\n        \'\'\'\n        Transform the labeling to one hot encoding\n        :param labels: labels to encode\n        :param _dims:\n        :return:\n        \'\'\'\n        label_names = list(set(labels))\n        ohe = [np.eye(_dims)[label_names.index(label)].astype(np.float32)\n               for label in labels]\n        return ohe\n\n    @staticmethod\n    def to_categorical(labels, label_names):\n        \'\'\'\n        Transformation of labels to categorical\n        :param labels: labels to change\n        :param label_names:\n        :return:\n        \'\'\'\n        return [np.array(list(label_names).index(label)).astype(np.float32)\n                for label in labels]\n\n    def layer_op(self, idx=None, subject_id=None, mode=\'single\', reject=True):\n        \'\'\'\n        Perform the csv_reading and assignment to dictionary\n        :param idx: index of the image\n        :param subject_id: subject id\n        :param mode: chosen mode (multi or single)\n        :param reject: if some elements should be rejected\n        :return:\n        \'\'\'\n        if idx is None and subject_id is not None:\n            print(""Need to decide upon idx from subject %s"" % subject_id)\n            idx_dict = {}\n            if mode == \'single\':\n                print(""Taking only one index among other valid"")\n                #  Take the list of idx corresponding to subject id and randomly\n                # sample from there\n                for name in self.names:\n\n                    relevant_indices = self.df_by_task[name].reset_index()[\n                        self.df_by_task[name].reset_index()[0] == subject_id].index.values\n                    if reject:\n                        relevant_valid = np.asarray(np.where(np.abs(\n                            self.valid_by_task[name][relevant_indices]) > 0)[0])\n                        if relevant_valid is None:\n                            relevant_valid = []\n                        print(relevant_valid, reject)\n                    else:\n                        relevant_valid = np.arange(len(relevant_indices))\n                        print(relevant_valid, "" is list of indices to sample ""\n                                              ""from"")\n                    print(np.asarray(relevant_valid).shape[0], ""is shape of ""\n                          ""relevant_valid"")\n                    relevant_final = [relevant_indices[v] for v in\n                                      relevant_valid] if \\\n                        np.asarray(relevant_valid).shape[0] > 0 else []\n                    print(relevant_final, ""is relevant final"")\n                    idx_dict[name] = random.choice(relevant_final) if \\\n                        list(relevant_final) else []\n\n            else: #self.df_by_task[self.df_by_task[name] == subject_id]\n                # mode full i.e. output all the lines corresponding to\n                # subject_id\n                print("" Taking all valid indices"")\n                for name in self.names:\n\n                    relevant_indices = self.df_by_task[name].reset_index()[\n                        self.df_by_task[name].reset_index()[0] == subject_id].index.values\n                    if reject:\n                        relevant_valid = np.asarray(np.where(np.abs(\n                            self.valid_by_task[name][relevant_indices]) > 0)[0])\n                        if relevant_valid is None:\n                            relevant_valid = []\n                    else:\n                        relevant_valid = np.arange(len(relevant_indices))\n                    relevant_final = [relevant_indices[v] for v in\n                                      relevant_valid] if \\\n                        np.asarray(relevant_valid).shape[0] > 0 else []\n                    idx_dict[name] = relevant_final\n\n        elif idx is None and subject_id is None:\n            idx_dict = {}\n            print(""Need to also choose subject id"")\n            for name in self.names:\n                if subject_id is None:\n                    idx_dict[name] = np.random.randint(\n                        self.df_by_task[name].shape[0])\n                    subject_id = self.df_by_task[name].iloc[idx_dict[name]].name\n                    print(""new subject id is "", subject_id)\n                if mode == \'single\':\n                    #  Take the list of idx corresponding to subject id\n                    # and randomly sample from there\n                    print(""Need to find index in single mode"")\n\n                    relevant_indices = np.asarray(np.where(self.df_by_task[\n                        name].index.get_loc(subject_id))[0])\n                    # print(""Found initial relevant"", relevant_indices,\n                    #       set(self.df_by_task[name].index), name,\n                    #       self.df_by_task[name].index.get_loc(subject_id).shape)\n                    if reject:\n                        relevant_valid = np.asarray(np.where(np.abs(\n                            self.valid_by_task[name][relevant_indices]) > 0)[0])\n                    else:\n                        relevant_valid = np.arange(len(relevant_indices))\n                    # print(""Found corresponding valid"", relevant_valid,\n                    #       np.max(relevant_valid), relevant_indices.shape)\n                    relevant_final = [relevant_indices[v] for v in\n                                      relevant_valid]\n                    # print(relevant_indices, subject_id)\n                    # relevant_indices = self._df.loc[subject_id]\n                    assert list(relevant_final), \'no valid index for subject \' \\\n                        \'%s and field %s\' % (subject_id, name)\n                    idx_dict[name] = random.choice(relevant_final)\n                else:  # mode full i.e. output all the lines corresponding to\n                    # subject_id\n                    relevant_indices = np.asarray(np.where(self.df_by_task[\n                        name].index.get_loc(subject_id))[0])\n                    # print(""Found initial relevant"", relevant_indices,\n                    #       set(self.df_by_task[name].index), name,\n                    #       self.df_by_task[name].index.get_loc(subject_id).shape)\n                    if reject:\n                        relevant_valid = np.asarray(np.where(np.abs(\n                            self.valid_by_task[name][relevant_indices]) > 0)[0])\n                    else:\n                        relevant_valid = np.ones_like(relevant_indices)\n                    # print(""Found corresponding valid"", relevant_valid,\n                    #       np.max(relevant_valid), relevant_indices.shape)\n                    relevant_final = [relevant_indices[v] for v in\n                                      relevant_valid]\n                    assert list(relevant_final), \'no valid index for subject \' \\\n                        \'%s and field %s\' % (subject_id, name)\n                    idx_dict[name] = relevant_final\n        elif not isinstance(idx, dict):\n            idx_dict = {}\n            for name in self.names:\n                idx_dict[name] = idx\n                if subject_id is None:\n                    subject_id = self.df_by_task[name].iloc[idx_dict[name]].name\n        else:\n            idx_dict = {}\n            for name in self.names:\n                idx_dict[name] = idx[name]\n                assert list(idx[name]), \'no valid index for %s\' % name\n                if subject_id is None:\n                    subject_id = self.df_by_task[name].iloc[idx_dict[name]].name\n\n        if self._indexable_output is not None:\n            output_dict = {k: self.apply_niftynet_format_to_data(\n                self.tile_nsamples(np.asarray(\n                    self._indexable_output[k])[idx_dict[k]])) for k in\n                           idx_dict.keys()}\n            # print(idx_dict, self._indexable_output[\'modality_label\'][\n            #     idx_dict[\'modality_label\']])\n            return idx_dict, output_dict, subject_id\n        raise Exception(\'Invalid mode\')\n\n\n    def tile_nsamples(self, data):\n        \'\'\'\n        Tile the csv_read to have the same value applied to the nsamples\n        extracted from the volume\n        :param data: csv data to tile for all samples extracted in the volume\n        :return: tiled data\n        \'\'\'\n        if self.n_samples_per_id > 1:\n            print(""preparing tiling"")\n            data = np.expand_dims(data, 1)\n            data = np.tile(data, np.asarray(\n                np.concatenate(\n                    ([self.n_samples_per_id],\n                     [1, ]*(len(np.asarray(data.shape))))),\n                dtype=np.int))\n            print(""tiling done"", data.shape)\n            return data\n\n        else:\n            return data\n    \n    @property\n    def shapes(self):\n        """"""\n        :return: dict of label shape and label location shape\n        """"""\n        for name in self.names:\n            if self.n_samples_per_id == 1:\n                self._shapes.update({name: (1, self.dims_by_task[name],\n                                            1, 1, 1, 1),\n                                     name + \'_location\': (1, 7)})\n            else:\n                self._shapes.update(\n                    {name: (self.n_samples_per_id,\n                            self.dims_by_task[name],\n                            1, 1, 1, 1),\n                     name + \'_location\': (self.n_samples_per_id, 7)})\n        return self._shapes\n\n    @property\n    def tf_dtypes(self):\n        """"""\n        Infer input data dtypes in TF\n        """"""\n        for name in self.names:\n            self._dtypes.update({name: tf.float32,\n                                 name + \'_location\': tf.int32})\n        return self._dtypes\n    \n    @property\n    def tf_shapes(self):\n        """"""\n        :return: a dictionary of sampler output tensor shapes\n        """"""\n        output_shapes = nest.map_structure_up_to(\n            self.tf_dtypes, tf.TensorShape, self.shapes)\n        return output_shapes\n\n    \n    @staticmethod\n    def apply_niftynet_format_to_data(data):\n        \'\'\'\n<<<<<<< HEAD\n        Transform the dtaa to be of dimension 5d\n=======\n        Transform the data to be of dimension 5d\n>>>>>>> 7a0386e78f01c88b707e08f759f910abba9b71b1\n        :param data: data to expand\n        :return: expanded data\n        \'\'\'\n        if len(data.shape) == 1:\n            data = np.expand_dims(data, 0)\n        while len(data.shape) < 6:\n            data = np.expand_dims(data, -1)\n        return data\n\n'"
niftynet/contrib/csv_reader/highres3dnet_features.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom six.moves import range\n\nfrom niftynet.layer import layer_util\nfrom niftynet.layer.activation import ActiLayer\nfrom niftynet.layer.base_layer import TrainableLayer\nfrom niftynet.layer.bn import BNLayer\nfrom niftynet.layer.convolution import ConvLayer, ConvolutionalLayer\nfrom niftynet.layer.dilatedcontext import DilatedTensor\nfrom niftynet.layer.elementwise import ElementwiseLayer\nfrom niftynet.network.base_net import BaseNet\nimport tensorflow as tf\n\n\nclass HighRes3DNetFeatures(BaseNet):\n    """"""\n    implementation of HighRes3DNet:\n      Li et al., ""On the compactness, efficiency, and representation of 3D\n      convolutional networks: Brain parcellation as a pretext task"", IPMI \'17\n    """"""\n\n    def __init__(self,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func=\'prelu\',\n                 name=\'HighRes3DNet\'):\n\n        super(HighRes3DNetFeatures, self).__init__(\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.layers = [\n            {\'name\': \'conv_0\', \'n_features\': 16, \'kernel_size\': 3},\n            {\'name\': \'res_1\', \'n_features\': 16, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'res_2\', \'n_features\': 32, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'res_3\', \'n_features\': 64, \'kernels\': (3, 3), \'repeat\': 3},\n            {\'name\': \'conv_1\', \'n_features\': 80, \'kernel_size\': 1}]\n\n    def layer_op(self, images, is_training, layer_id=-1):\n        assert layer_util.check_spatial_dims(\n            images, lambda x: x % 8 == 0)\n        # go through self.layers, create an instance of each layer\n        # and plugin data\n        layer_instances = []\n\n        # first convolution layer\n        params = self.layers[0]\n        first_conv_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = first_conv_layer(images, is_training)\n        layer_instances.append((first_conv_layer, flow))\n\n        # resblocks, all kernels dilated by 1 (normal convolution)\n        params = self.layers[1]\n        with DilatedTensor(flow, dilation_factor=1) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n        # tf.add_to_collection(\'checkpoints\', flow)\n\n        # resblocks, all kernels dilated by 2\n        params = self.layers[2]\n        with DilatedTensor(flow, dilation_factor=2) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n        # tf.add_to_collection(\'checkpoints\', flow)\n\n        # resblocks, all kernels dilated by 4\n        params = self.layers[3]\n        with DilatedTensor(flow, dilation_factor=4) as dilated:\n            for j in range(params[\'repeat\']):\n                res_block = HighResBlock(\n                    params[\'n_features\'],\n                    params[\'kernels\'],\n                    acti_func=self.acti_func,\n                    w_initializer=self.initializers[\'w\'],\n                    w_regularizer=self.regularizers[\'w\'],\n                    name=\'%s_%d\' % (params[\'name\'], j))\n                dilated.tensor = res_block(dilated.tensor, is_training)\n                layer_instances.append((res_block, dilated.tensor))\n        flow = dilated.tensor\n        # tf.add_to_collection(\'checkpoints\', flow)\n\n        # 1x1x1 convolution layer\n        params = self.layers[4]\n        fc_layer = ConvolutionalLayer(\n            n_output_chns=params[\'n_features\'],\n            kernel_size=params[\'kernel_size\'],\n            acti_func=self.acti_func,\n            w_initializer=self.initializers[\'w\'],\n            w_regularizer=self.regularizers[\'w\'],\n            name=params[\'name\'])\n        flow = fc_layer(flow, is_training)\n        layer_instances.append((fc_layer, flow))\n\n        # set training properties\n        if is_training:\n            self._print(layer_instances)\n            return layer_instances[-1][1]\n        return layer_instances[layer_id][1]\n\n    def _print(self, list_of_layers):\n        for (op, _) in list_of_layers:\n            print(op)\n\n\nclass HighResBlock(TrainableLayer):\n    """"""\n    This class define a high-resolution block with residual connections\n    kernels\n\n        - specify kernel sizes of each convolutional layer\n        - e.g.: kernels=(5, 5, 5) indicate three conv layers of kernel_size 5\n\n    with_res\n\n        - whether to add residual connections to bypass the conv layers\n    """"""\n\n    def __init__(self,\n                 n_output_chns,\n                 kernels=(3, 3),\n                 acti_func=\'relu\',\n                 w_initializer=None,\n                 w_regularizer=None,\n                 with_res=True,\n                 name=\'HighResBlock\'):\n\n        super(HighResBlock, self).__init__(name=name)\n\n        self.n_output_chns = n_output_chns\n        if hasattr(kernels, ""__iter__""):  # a list of layer kernel_sizes\n            self.kernels = kernels\n        else:  # is a single number (indicating single layer)\n            self.kernels = [kernels]\n        self.acti_func = acti_func\n        self.with_res = with_res\n\n        self.initializers = {\'w\': w_initializer}\n        self.regularizers = {\'w\': w_regularizer}\n\n    def layer_op(self, input_tensor, is_training):\n        output_tensor = input_tensor\n        for (i, k) in enumerate(self.kernels):\n            # create parameterised layers\n            bn_op = BNLayer(regularizer=self.regularizers[\'w\'],\n                            name=\'bn_{}\'.format(i))\n            acti_op = ActiLayer(func=self.acti_func,\n                                regularizer=self.regularizers[\'w\'],\n                                name=\'acti_{}\'.format(i))\n            conv_op = ConvLayer(n_output_chns=self.n_output_chns,\n                                kernel_size=k,\n                                stride=1,\n                                w_initializer=self.initializers[\'w\'],\n                                w_regularizer=self.regularizers[\'w\'],\n                                name=\'conv_{}\'.format(i))\n            # connect layers\n            output_tensor = bn_op(output_tensor, is_training)\n            output_tensor = acti_op(output_tensor)\n            output_tensor = conv_op(output_tensor)\n        # make residual connections\n        if self.with_res:\n            output_tensor = ElementwiseLayer(\'SUM\')(output_tensor, input_tensor)\n        return output_tensor\n'"
niftynet/contrib/csv_reader/multitask_classifseg_application.py,20,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines an image-level classification application\nthat maps from images to scalar, multi-class labels.\n\nThis class is instantiated and initalized by the application_driver.\n""""""\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.contrib.csv_reader.sampler_resize_v2_csv import ResizeSamplerCSV \\\n    as ResizeSampler\nfrom niftynet.contrib.csv_reader.sampler_uniform_v2_csv import \\\n    UniformSamplerCSV as UniformSampler\nfrom niftynet.contrib.csv_reader.sampler_weighted_v2_csv import \\\n    WeightedSamplerCSV as WeightedSampler\nfrom niftynet.contrib.csv_reader.sampler_balanced_v2_csv import \\\n    BalancedSamplerCSV as BalancedSampler\nfrom niftynet.contrib.csv_reader.sampler_grid_v2_csv import GridSamplerCSV as\\\n    GridSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.contrib.csv_reader.csv_reader import CSVReader\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.loss_classification import LossFunction as \\\n    LossFunctionClassification\nfrom niftynet.layer.loss_segmentation import \\\n    LossFunction as LossFunctionSegmentation\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.evaluation.classification_evaluator import ClassificationEvaluator\n\nSUPPORTED_INPUT = set([\'image\', \'value\', \'label\', \'sampler\', \'inferred\'])\n\n\nclass MultiClassifSegApplication(BaseApplication):\n    """"""This class defines an application for image-level classification\n    problems mapping from images to scalar labels.\n\n    This is the application class to be instantiated by the driver\n    and referred to in configuration files.\n\n    Although structurally similar to segmentation, this application\n    supports different samplers/aggregators (because patch-based\n    processing is not appropriate), and monitoring metrics.""""""\n\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, action):\n        super(MultiClassifSegApplication, self).__init__()\n        tf.logging.info(\'starting classification application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.eval_param = None\n        self.evaluator = None\n        self.action_param = action_param\n        self.net_multi = None\n        self.data_param = None\n        self.segmentation_param = None\n        self.csv_readers = None\n        self.SUPPORTED_SAMPLING = {\n            \'uniform\': (self.initialise_uniform_sampler,\n                        self.initialise_grid_sampler,\n                        self.initialise_grid_aggregator),\n            \'weighted\': (self.initialise_weighted_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler,\n                       self.initialise_resize_aggregator),\n            \'balanced\': (self.initialise_balanced_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        \'\'\'\n        Initialise the data loader both csv readers and image readers and\n        specify preprocessing layers\n        :param data_param:\n        :param task_param:\n        :param data_partitioner:\n        :return:\n        \'\'\'\n\n        self.data_param = data_param\n        self.segmentation_param = task_param\n\n        if self.is_training:\n            image_reader_names = (\'image\', \'sampler\', \'label\')\n            csv_reader_names = (\'value\',)\n        elif self.is_inference:\n            image_reader_names = (\'image\',)\n            csv_reader_names = ()\n        elif self.is_evaluation:\n            image_reader_names = (\'image\', \'inferred\', \'label\')\n            csv_reader_names = (\'value\',)\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(image_reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n        if self.is_inference:\n            self.action_param.sample_per_volume = 1\n        if csv_reader_names is not None and list(csv_reader_names):\n            self.csv_readers = [\n                CSVReader(csv_reader_names).initialise(\n                    data_param, task_param, file_list,\n                    sample_per_volume=self.action_param.sample_per_volume)\n                for file_list in file_lists]\n        else:\n            self.csv_readers = [None for file_list in file_lists]\n\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=self.net_param.foreground_type,\n            multimod_fusion=self.net_param.multimod_foreground_type,\n            threshold=0.0) \\\n            if self.net_param.normalise_foreground_only else None\n\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer) \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            binary_masking_func=foreground_masking_layer,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n\n        label_normaliser = DiscreteLabelNormalisationLayer(\n            image_name=\'label\',\n            modalities=vars(task_param).get(\'label\'),\n            model_filename=self.net_param.histogram_ref_file) \\\n            if (self.net_param.histogram_ref_file and\n                task_param.label_normalisation) else None\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n        if label_normaliser is not None:\n            normalisation_layers.append(label_normaliser)\n\n        augmentation_layers = []\n        if self.is_training:\n            train_param = self.action_param\n            if train_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=train_param.random_flipping_axes))\n            if train_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=train_param.scaling_percentage[0],\n                    max_percentage=train_param.scaling_percentage[1]))\n            if train_param.rotation_angle or \\\n                    self.action_param.rotation_angle_x or \\\n                    self.action_param.rotation_angle_y or \\\n                    self.action_param.rotation_angle_z:\n                rotation_layer = RandomRotationLayer()\n                if train_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        train_param.rotation_angle)\n                else:\n                    rotation_layer.init_non_uniform_angle(\n                        self.action_param.rotation_angle_x,\n                        self.action_param.rotation_angle_y,\n                        self.action_param.rotation_angle_z)\n                augmentation_layers.append(rotation_layer)\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n            normalisation_layers + augmentation_layers)\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(normalisation_layers)\n\n    def initialise_uniform_sampler(self):\n        \'\'\'\n        Create the uniform sampler using information from readers\n        :return:\n        \'\'\'\n        self.sampler = [[UniformSampler(\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader, csv_reader in\n                         zip(self.readers, self.csv_readers)]]\n\n    def initialise_weighted_sampler(self):\n        \'\'\'\n        Create the weighted sampler using the info from the csv_readers and\n        image_readers and the configuration parameters\n        :return:\n        \'\'\'\n        self.sampler = [[WeightedSampler(\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader, csv_reader in\n                         zip(self.readers, self.csv_readers)]]\n\n    def initialise_resize_sampler(self):\n        \'\'\'\n        Define the resize sampler using the information from the\n        configuration parameters, csv_readers and image_readers\n        :return:\n        \'\'\'\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader, csv_reader in\n                         zip(self.readers, self.csv_readers)]]\n\n    def initialise_grid_sampler(self):\n        \'\'\'\n        Define the grid sampler based on the information from configuration\n        and the csv_readers and image_readers specifications\n        :return:\n        \'\'\'\n        self.sampler = [[GridSampler(\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader, csv_reader in\n                         zip(self.readers, self.csv_readers)]]\n\n    def initialise_balanced_sampler(self):\n        \'\'\'\n        Define the balanced sampler based on the information from configuration\n        and the csv_readers and image_readers specifications\n        :return:\n        \'\'\'\n        self.sampler = [[BalancedSampler(\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader, csv_reader in\n                         zip(self.readers, self.csv_readers)]]\n\n    def initialise_grid_aggregator(self):\n        \'\'\'\n        Define the grid aggregator used for decoding using configuration\n        parameters\n        :return:\n        \'\'\'\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix,\n            fill_constant=self.action_param.fill_constant)\n\n    def initialise_resize_aggregator(self):\n        \'\'\'\n        Define the resize aggregator used for decoding using the\n        configuration parameters\n        :return:\n        \'\'\'\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        \'\'\'\n        Specifies the sampler used among those previously defined based on\n        the sampling choice\n        :return:\n        \'\'\'\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        elif self.is_inference:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_aggregator(self):\n        \'\'\'\n        Specifies the aggregator used based on the sampling choice\n        :return:\n        \'\'\'\n        self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]()\n\n    def initialise_network(self):\n        \'\'\'\n        Initialise the network and specifies the ordering of elements\n        :return:\n        \'\'\'\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(\n            \'niftynet.contrib.csv_reader.toynet_features.ToyNetFeat\')(\n                num_classes=self.segmentation_param.num_classes,\n                w_initializer=InitializerFactory.get_initializer(\n                    name=self.net_param.weight_initializer),\n                b_initializer=InitializerFactory.get_initializer(\n                    name=self.net_param.bias_initializer),\n                w_regularizer=w_regularizer,\n                b_regularizer=b_regularizer,\n                acti_func=self.net_param.activation_function)\n        self.net_multi = ApplicationNetFactory.create(\n            \'niftynet.contrib.csv_reader.class_seg_finnet.ClassSegFinnet\')(\n                num_classes=self.segmentation_param.num_classes,\n                w_initializer=InitializerFactory.get_initializer(\n                    name=self.net_param.weight_initializer),\n                b_initializer=InitializerFactory.get_initializer(\n                    name=self.net_param.bias_initializer),\n                w_regularizer=w_regularizer,\n                b_regularizer=b_regularizer,\n                acti_func=self.net_param.activation_function)\n\n    def add_confusion_matrix_summaries_(self,\n                                        outputs_collector,\n                                        net_out,\n                                        data_dict):\n        """""" This method defines several monitoring metrics that\n        are derived from the confusion matrix """"""\n        labels = tf.reshape(tf.cast(data_dict[\'label\'], tf.int64), [-1])\n        prediction = tf.reshape(tf.argmax(net_out, -1), [-1])\n        num_classes = 2\n        conf_mat = tf.confusion_matrix(labels, prediction, num_classes)\n        conf_mat = tf.to_float(conf_mat)\n        if self.segmentation_param.num_classes == 2:\n            outputs_collector.add_to_collection(\n                var=conf_mat[1][1], name=\'true_positives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[1][0], name=\'false_negatives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[0][1], name=\'false_positives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=conf_mat[0][0], name=\'true_negatives\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n        else:\n            outputs_collector.add_to_collection(\n                var=conf_mat[tf.newaxis, :, :, tf.newaxis],\n                name=\'confusion_matrix\',\n                average_over_devices=True, summary_type=\'image\',\n                collection=TF_SUMMARIES)\n\n        outputs_collector.add_to_collection(\n            var=tf.trace(conf_mat), name=\'accuracy\',\n            average_over_devices=True, summary_type=\'scalar\',\n            collection=TF_SUMMARIES)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n            net_out_seg, net_out_class = self.net_multi(net_out,\n                                                        self.is_training)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func_class = LossFunctionClassification(\n                n_class=2,\n                loss_type=\'CrossEntropy\')\n            loss_func_seg = LossFunctionSegmentation(\n                n_class=self.segmentation_param.num_classes,\n                loss_type=self.action_param.loss_type)\n            data_loss_seg = loss_func_seg(\n                prediction=net_out_seg,\n                ground_truth=data_dict.get(\'label\', None))\n            data_loss_class = loss_func_class(\n                prediction=net_out_class,\n                ground_truth=data_dict.get(\'value\', None))\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss_seg + data_loss_class + reg_loss\n            else:\n                loss = data_loss_seg + data_loss_class\n            self.total_loss = loss\n            self.total_loss = tf.Print(tf.cast(self.total_loss, tf.float32),\n                                       [loss, tf.shape(net_out_seg),\n                                        tf.shape(net_out_class)],\n                                       message=\'test\')\n            grads = self.optimiser.compute_gradients(\n                loss, colocate_gradients_with_ops=True)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss_class, name=\'data_loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss_seg, name=\'data_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            # self.add_confusion_matrix_summaries_(outputs_collector,\n            #                                      net_out_class,\n            #                                      data_dict)\n        else:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n            net_out_seg, net_out_class = self.net_multi(net_out,\n                                                        self.is_training)\n            tf.logging.info(\n                \'net_out.shape may need to be resized: %s\', net_out.shape)\n            output_prob = self.segmentation_param.output_prob\n            num_classes = self.segmentation_param.num_classes\n            if output_prob and num_classes > 1:\n                post_process_layer_class = PostProcessingLayer(\n                    \'SOFTMAX\', num_classes=num_classes)\n                post_process_layer_seg = PostProcessingLayer(\'SOFTMAX\',\n                                                             num_classes=2)\n            elif not output_prob and num_classes > 1:\n                post_process_layer_class = PostProcessingLayer(\n                    \'ARGMAX\', num_classes=num_classes)\n                post_process_layer_seg = PostProcessingLayer(\'ARGMAX\',\n                                                             num_classes=2)\n            else:\n                post_process_layer_class = PostProcessingLayer(\n                    \'IDENTITY\', num_classes=num_classes)\n                post_process_layer_seg = PostProcessingLayer(\'IDENTITY\',\n                                                             num_classes=2)\n\n            net_out_class = post_process_layer_class(net_out_class)\n            net_out_seg = post_process_layer_seg(net_out_seg)\n\n            outputs_collector.add_to_collection(\n                var=net_out_seg, name=\'seg\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(var=net_out_class,\n                                                name=\'value\',\n                                                average_over_devices=False,\n                                                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        \'\'\'\n        Specifies how the output should be decoded\n        :param batch_output:\n        :return:\n        \'\'\'\n        if not self.is_training:\n            return self.output_decoder.decode_batch(\n                {\'window_seg\': batch_output[\'seg\'],\n                 \'csv_class\': batch_output[\'value\']},\n                batch_output[\'location\'])\n        return True\n\n    def initialise_evaluator(self, eval_param):\n        \'\'\'\n        Define the evaluator\n        :param eval_param:\n        :return:\n        \'\'\'\n        self.eval_param = eval_param\n        self.evaluator = ClassificationEvaluator(self.readers[0],\n                                                 self.segmentation_param,\n                                                 eval_param)\n\n    def add_inferred_output(self, data_param, task_param):\n        \'\'\'\n        Define how to treat added inferred output\n        :param data_param:\n        :param task_param:\n        :return:\n        \'\'\'\n        return self.add_inferred_output_like(data_param, task_param, \'label\')\n'"
niftynet/contrib/csv_reader/sampler_balanced_v2_csv.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nGenerate image windows from a balanced sampling map as if every label\nhad the same probability of occurrence.\n\nConsider a mask with three classes I, J, K with prevalence 0.1, 0.1, and\n0.8, respectively. If 100 samples are drawn from the balanced sampler, the\nclasses should be approximately 33 I, 33 J, and 33 K.\n\nThis can also be considered a ""balanced random cropping"" layer of the\ninput image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.contrib.csv_reader.sampler_uniform_v2_csv import UniformSamplerCSV\nfrom niftynet.engine.image_window import N_SPATIAL\nfrom niftynet.engine.sampler_weighted_v2 import crop_sampling_map\n\n\nclass BalancedSamplerCSV(UniformSampler):\n    """"""\n    This class generators samples from a user provided frequency map for each\n    input volume. The sampling likelihood of each voxel is proportional its\n    intra class frequency. That is, if a given voxel is of class `A` and there\n    are 20 voxels with class `A`, the probability of selecting this voxel is\n    5%. If there are 10 classes, the probability becomes 10% * 5% = 0.5%.\n\n    In general, the likelihood of sampling a voxel is given by:\n        p(v) = (1)/(# of unique labels * # of voxels with same class as v)\n\n    This is done for balanced sampling. In the case of unbalanced labels,\n    this sampler should produce a roughly equal probability of sampling each\n    class.\n\n    This layer can be considered as a ""balanced random cropping"" layer of the\n    input image.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 csv_reader,\n                 window_sizes,\n                 batch_size=1,\n                 windows_per_image=1,\n                 queue_length=10,\n                 name=\'balanced_sampler\'):\n        UniformSamplerCSV.__init__(self,\n                                   reader=reader,\n                                   csv_reader=csv_reader,\n                                   window_sizes=window_sizes,\n                                   batch_size=batch_size,\n                                   windows_per_image=windows_per_image,\n                                   queue_length=queue_length,\n                                   name=name)\n        tf.logging.info(\'Initialised balanced sampler window instance\')\n        self.window_centers_sampler = balanced_spatial_coordinates\n\n\ndef balanced_spatial_coordinates(\n        n_samples, img_spatial_size, win_spatial_size, sampler_map):\n    """"""\n    Perform balanced sampling.\n\n    Each label in the input tensor has an equal probability of\n    being sampled.\n\n    :param n_samples: number of random coordinates to generate\n    :param img_spatial_size: input image size\n    :param win_spatial_size: input window size\n    :param sampler_map: sampling prior map, it\'s spatial shape should be\n            consistent with `img_spatial_size`\n    :return: (n_samples, N_SPATIAL) coordinates representing sampling\n              window centres relative to img_spatial_size\n    """"""\n    assert sampler_map is not None, \\\n        \'sampling prior map is not specified, \' \\\n        \'please check `sampler=` option in the config.\'\n    assert np.all(img_spatial_size[:N_SPATIAL] ==\n                  sampler_map.shape[:N_SPATIAL]), \\\n        \'image and sampling map shapes do not match\'\n\n    # Find the number of unique labels\n    win_spatial_size = np.asarray(win_spatial_size, dtype=np.int32)\n    cropped_map = crop_sampling_map(sampler_map, win_spatial_size)\n\n    flatten_map = cropped_map.flatten()\n    unique_labels = np.unique(flatten_map)\n    if len(unique_labels) > 500:\n        tf.logging.warning(\n            ""unusual discrete volume: number of unique ""\n            ""labels: %s"", len(unique_labels))\n\n    # system parameter?\n    class_probs = [1.0 / len(unique_labels)] * len(unique_labels)\n    label_counts = np.random.multinomial(n_samples, class_probs)\n    # Look inside each label and sample `count`. Add the middle_coord of\n    # each sample to `middle_coords`\n    middle_coords = np.zeros((n_samples, N_SPATIAL), dtype=np.int32)\n    sample_count = 0\n    for label, count in zip(unique_labels, label_counts):\n        # Get indices where(cropped_map == label)\n        valid_locations = np.where(flatten_map == label)[0]\n\n        # Sample `count` from those indices. Need replace=True. Consider the\n        # case where all pixels are background except for one pixel which is\n        # foreground. We ask for 10 samples. We should get 5 samples from\n        # background and the foreground pixel sampled 5 times (give or take\n        # random fluctuation).\n        try:\n            samples = np.random.choice(\n                valid_locations,\n                size=count,\n                replace=True)\n        except ValueError:\n            tf.logging.fatal(""unable to choose sampling window based on ""\n                             ""the current frequency map."")\n            raise\n\n        assert count == samples.size, ""Unable to sample from the image""\n\n        # Place into `middle_coords`\n        for sample in samples:\n            middle_coords[sample_count, :N_SPATIAL] = \\\n                np.unravel_index(sample, cropped_map.shape)[:N_SPATIAL]\n            sample_count += 1\n\n    # re-shift coords due to the crop\n    half_win = np.floor(win_spatial_size / 2).astype(np.int32)\n    middle_coords[:, :N_SPATIAL] = \\\n        middle_coords[:, :N_SPATIAL] + half_win[:N_SPATIAL]\n    return middle_coords\n'"
niftynet/contrib/csv_reader/sampler_csv_rows.py,0,"b'import numpy as np\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\n\n\nclass ImageWindowDatasetCSV(ImageWindowDataset):\n    """"""\n    Extending the default sampler to include csv data\n    """"""\n\n    def __init__(self,\n                 reader,\n                 csv_reader=None,\n                 window_sizes=None,\n                 batch_size=10,\n                 windows_per_image=1,\n                 shuffle=True,\n                 queue_length=10,\n                 num_threads=4,\n                 epoch=-1,\n                 smaller_final_batch_mode=\'pad\',\n                 name=\'random_vector_sampler\'):\n        self.csv_reader = csv_reader\n        print(""assigned csv_reader"")\n        ImageWindowDataset.__init__(\n            self,\n            reader=reader,\n            window_sizes=window_sizes,\n            batch_size=batch_size,\n            windows_per_image=windows_per_image,\n            shuffle=shuffle,\n            queue_length=queue_length,\n            epoch=epoch,\n            smaller_final_batch_mode=smaller_final_batch_mode,\n            name=name)\n        print(""initialised IWD"")\n        self.set_num_threads(num_threads)\n\n    def layer_op(self, idx=None):\n        """"""\n        Generating each image as a window.\n        Overriding this function to create new image sampling strategies.\n\n        This function should either yield a dictionary\n        (for single window per image)::\n\n            yield a dictionary\n\n            {\n             \'image_name\': a numpy array,\n             \'image_name_location\': (image_id,\n                                     x_start, y_start, z_start,\n                                     x_end, y_end, z_end)\n            }\n\n        or return a dictionary (for multiple windows per image)::\n\n            return a dictionary:\n            {\n             \'image_name\': a numpy array,\n             \'image_name_location\': [n_samples, 7]\n            }\n\n        where the 7-element location vector encode the image_id,\n        starting and ending coordinates of the image window.\n\n        Following the same notation, the dictionary can be extended\n        to multiple modalities; the keys will be::\n\n            {\'image_name_1\', \'image_name_location_1\',\n             \'image_name_2\', \'image_name_location_2\', ...}\n\n        :param idx: image_id used to load the image at the i-th row of\n            the input\n        :return: a image data dictionary\n        """"""\n\n        # dataset: from a window generator\n        # assumes self.window.n_samples == 1\n        # the generator should yield one window at each iteration\n\n        if self.window.n_samples == 1:\n            assert self.window.n_samples == 1, \\\n                \'image_window_dataset.layer_op() requires: \' \\\n                \'windows_per_image should be 1.\'\n\n            image_id, image_data, _ = self.reader(idx=idx)\n            print(image_id, idx)\n            for mod in list(image_data):\n                spatial_shape = image_data[mod].shape[:N_SPATIAL]\n                coords = self.dummy_coordinates(image_id, spatial_shape, 1)\n                image_data[LOCATION_FORMAT.format(mod)] = coords\n                image_data[mod] = image_data[mod][np.newaxis, ...]\n            if self.csv_reader is not None:\n                _, label_dict, _ = self.csv_reader(subject_id=image_id)\n                print(label_dict, image_id, idx)\n                image_data.update(label_dict)\n                for name in self.csv_reader.names:\n                    image_data[name + \'_location\'] = \\\n                        image_data[\'image_location\']\n            return image_data\n        else:\n            print(""Warning, it may not be ready yet"")\n            image_id, image_data, _ = self.reader(idx=idx)\n            print(image_id, idx)\n            for mod in list(image_data):\n                spatial_shape = image_data[mod].shape[:N_SPATIAL]\n                coords = self.dummy_coordinates(image_id, spatial_shape, 1)\n                image_data[LOCATION_FORMAT.format(mod)] = coords\n                image_data[mod] = image_data[mod][np.newaxis, ...]\n            if self.csv_reader is not None:\n                _, label_dict, _ = self.csv_reader(subject_id=image_id)\n                print(label_dict, image_id, idx)\n                image_data.update(label_dict)\n                for name in self.csv_reader.names:\n                    image_data[name + \'_location\'] = image_data[\n                        \'image_location\']\n            return image_data\n\n    @property\n    def tf_shapes(self):\n        """"""\n        returns a dictionary of sampler output tensor shapes\n        """"""\n        assert self.window, \'Unknown output shapes: self.window not initialised\'\n        shape_dict = self.window.tf_shapes\n        if self.csv_reader is not None:\n            shape_dict.update(self.csv_reader.tf_shapes)\n        return shape_dict\n\n    @property\n    def tf_dtypes(self):\n        """"""\n        returns a dictionary of sampler output tensorflow dtypes\n        """"""\n        assert self.window, \'Unknown output shapes: self.window not initialised\'\n        shape_dict = self.window.tf_dtypes\n        if self.csv_reader is not None:\n            shape_dict.update(self.csv_reader.tf_dtypes)\n        return shape_dict\n'"
niftynet/contrib/csv_reader/sampler_csvpatch.py,16,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating uniformly distributed image window from input image\nThis can also be considered as a ""random cropping"" layer of the\ninput image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\nfrom niftynet.contrib.csv_reader.sampler_csv_rows import ImageWindowDatasetCSV\n# from niftynet.engine.image_window import LOCATION_FORMAT\n# from niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\nfrom niftynet.io.misc_io import do_reorientation_idx, do_resampling_idx\n\nSUPPORTED_MODES_CORRECTION=[\'pad\', \'remove\', \'random\']\n\n\nclass CSVPatchSampler(ImageWindowDatasetCSV):\n    """"""\n    This class generates samples using the coordinates of the centre as\n    extracted from a csv file\n\n    This layer can be considered as a ""guided cropping"" layer of the\n    input image based on preselected input.\n    """"""\n\n    def __init__(self,\n                 reader, csv_reader,\n                 window_sizes,\n                 batch_size=1,\n                 windows_per_image=1,\n                 queue_length=10,\n                 mode_correction=\'pad\',\n                 name=\'csv_patchsampler_v2\'):\n        ImageWindowDatasetCSV.__init__(\n            self,\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=window_sizes,\n            batch_size=batch_size,\n            windows_per_image=windows_per_image,\n            queue_length=queue_length,\n            shuffle=True,\n            epoch=-1,\n            smaller_final_batch_mode=\'drop\',\n            name=name)\n\n        tf.logging.info(""initialised csv patch sampler %s "", self.window.shapes)\n        self.mode_correction = mode_correction\n        self.window_centers_sampler = rand_spatial_coordinates\n        self.available_subjects = reader._file_list.subject_id\n\n    # pylint: disable=too-many-locals\n    def layer_op(self, idx=None):\n        """"""\n        This function generates sampling windows to the input buffer\n        image data are from ``self.reader()``\n\n        It first find the appropriate indices from the data frame in which\n        the centre samples are stored and extract information about the\n        windows to draw on the data.\n        The final dictionary is filled according to the appropriate samples.\n        Different modes on how to take care of unsuitable centres (too big\n        patch size for instance are implemented)\n\n        :return: output data dictionary\n            ``{image_modality: data_array, image_location: n_samples * 7}``\n        """"""\n\n        if self.window.n_samples > 1:\n            raise ValueError(""\\nThe number of windows per image has to be ""\n                             ""1 with a csv_reader"")\n        # flag_multi_row = False\n        print(""Trying to run csv patch sampler "")\n        if \'sampler\' not in self.csv_reader.names:\n            tf.logging.warning(\'Uniform sampling because no csv sampler \'\n                               \'provided\')\n\n        # if \'multi\' in self.csv_reader.type_by_task.values():\n        #     flag_multi_row = True\n        try:\n            _, _, subject_id = self.csv_reader(idx)\n        except ValueError:\n            tf.logging.fatal(""No available subject"")\n            raise\n\n        assert len(self.available_subjects) >0, ""No available subject from "" \\\n                                                ""check""\n\n        # assert len(self.available_subjects) > 0, ""No available subject from "" \\\n        #                                          ""check""\n\n\n        print(""subject id is "", subject_id)\n        if len(self.available_subjects) > 0:\n            idx_subject_id = np.where(\n            self.available_subjects == subject_id)[0][0]\n            image_id, data, _ = self.reader(idx=idx_subject_id, shuffle=True)\n            subj_indices, csv_data, _ = self.csv_reader(subject_id=subject_id)\n            image_shapes = dict(\n                (name, data[name].shape) for name in self.window.names)\n            static_window_shapes = self.window.match_image_shapes(image_shapes)\n\n            # Perform the checks relative to the sample choices and create the\n            # corresponding (if needed) padding information to be applied\n            num_idx, num_discard = self.check_csv_sampler_valid(subject_id,\n                                                                image_shapes,\n                                                                static_window_shapes\n                                                                )\n\n            print(num_idx, num_discard, ""available, discarded"")\n\n\n            if \'sampler\' not in self.csv_reader.names:\n                tf.logging.warning(\'Uniform sampling because no csv sampler \'\n                                  \'provided\')\n\n\n\n            # In the remove configuration, none of the unsuitable sample is used.\n            #  Thus if the chosen subject does not have any suitable sample,\n            # another one must be drawn. An error is raised if none of the\n            # subjects has suitable samples\n            if self.mode_correction == \'remove\':\n                if num_idx == num_discard:\n                    if subject_id in set(self.available_subjects):\n                        self.available_subjects.drop([idx_subject_id], inplace=True)\n\n                        print(\'self.available_subjects\', self.available_subjects, idx_subject_id)\n\n                        subject_id = None\n                    else:\n                        tf.logging.warning(\'%s may have already been dropped from list of available subjects\' %subject_id)\n                        subject_id = None\n                    while subject_id is None and len(self.available_subjects) > 0:\n                        _, _, subject_id = self.csv_reader(idx)\n                        print(\'list of available subjects is \',\n                              self.available_subjects, idx_subject_id)\n                        # print(""subject id is "", subject_id)\n                        # Find the index corresponding to the drawn subject id in\n                        #  the reader\n                        if subject_id in set(self.available_subjects):\n                            idx_subject_id = np.where(\n                                self.available_subjects == subject_id)[0][0]\n                            image_id, data, _ = self.reader(idx=idx_subject_id,\n                                                            shuffle=True)\n                            subj_indices, csv_data, _ = self.csv_reader(\n                                subject_id=subject_id)\n                            if \'sampler\' not in self.csv_reader.names:\n                                tf.logging.warning(\n                                    \'Uniform sampling because no csv sampler provided\')\n                            image_shapes = dict(\n                                (name, data[name].shape) for name in self.window.names)\n                            static_window_shapes = self.window.match_image_shapes(\n                                image_shapes)\n                            num_idx, num_discard = self.check_csv_sampler_valid(\n                                subject_id,\n                                image_shapes,\n                                static_window_shapes)\n                            if num_idx == num_discard:\n                                if subject_id in set(self.available_subjects):\n                                    self.available_subjects.drop(idx_subject_id)\n                                    subject_id = None\n                                else:\n                                    subject_id = None\n                        else:\n                            subject_id = None\n                if subject_id is None:\n                    tf.logging.fatal(""None of the subjects has any suitable ""\n                                     ""samples. Consider using a different ""\n                                     ""alternative to unsuitable samples or ""\n                                     ""reducing your patch size"")\n                    raise ValueError\n\n\n            # find csv coordinates and return coordinates (not corrected) and\n            # corresponding csv indices\n            try:\n                print(\'subject id to try is %s\' % subject_id)\n                coordinates, idx = self.csvcenter_spatial_coordinates(\n                    subject_id=subject_id,\n                    data=data,\n                    img_sizes=image_shapes,\n                    win_sizes=static_window_shapes,\n                    n_samples=self.window.n_samples,\n                    mode_correction=self.mode_correction\n                    )\n                reject = False\n                if self.mode_correction == \'remove\':\n                    reject = True\n                # print(idx, ""index selected"")\n                # initialise output dict, placeholders as dictionary keys\n                # this dictionary will be used in\n                # enqueue operation in the form of: `feed_dict=output_dict`\n                output_dict = {}\n                potential_pad = self.csv_reader.pad_by_task[\'sampler\'][idx][0]\n                potential_pad_corr_end = -1.0 * np.asarray(potential_pad[N_SPATIAL:])\n                potential_pad_corr = np.concatenate((potential_pad[:N_SPATIAL],\n                                                     potential_pad_corr_end), 0)\n\n                # fill output dict with data\n                for name in list(data):\n                    coordinates_key = LOCATION_FORMAT.format(name)\n                    image_data_key = name\n\n                    # fill the coordinates\n                    location_array = coordinates[name]\n                    output_dict[coordinates_key] = location_array\n\n                    # fill output window array\n                    image_array = []\n                    for window_id in range(self.window.n_samples):\n                        x_start, y_start, z_start, x_end, y_end, z_end = \\\n                            location_array[window_id, 1:].astype(np.int32) + \\\n                            potential_pad_corr.astype(np.int32)\n                        # print(location_array[window_id, 1:]+potential_pad_corr)\n                        try:\n                            image_window = data[name][\n                                           x_start:x_end, y_start:y_end,\n                                           z_start:z_end, ...]\n                            if np.sum(potential_pad) > 0:\n                                new_pad = np.reshape(potential_pad, [2, N_SPATIAL]).T\n                                add_pad = np.tile([0, 0], [len(np.shape(\n                                    image_window))-N_SPATIAL, 1])\n                                new_pad = np.concatenate((new_pad, add_pad),\n                                                         0).astype(np.int32)\n                                # print(new_pad, ""is padding"")\n                                new_img = np.pad(image_window, pad_width=new_pad,\n                                                 mode=\'constant\',\n                                                 constant_values=0)\n                                image_array.append(new_img[np.newaxis, ...])\n                            else:\n                                image_array.append(image_window[np.newaxis, ...])\n                        except ValueError:\n                            tf.logging.fatal(\n                                ""dimensionality miss match in input volumes, ""\n                                ""please specify spatial_window_size with a ""\n                                ""3D tuple and make sure each element is ""\n                                ""smaller than the image length in each dim. ""\n                                ""Current coords %s"", location_array[window_id])\n                            raise\n                    if len(image_array) > 1:\n                        output_dict[image_data_key] = \\\n                            np.concatenate(image_array, axis=0)\n                    else:\n                        output_dict[image_data_key] = image_array[0]\n                # fill output dict with csv_data\n                # print(""filling output dict"")\n                if self.csv_reader is not None:\n                    idx_dict = {}\n                    list_keys = self.csv_reader.df_by_task.keys()\n                    for k in list_keys:\n                        if self.csv_reader.type_by_task[k] == \'multi\':\n                            idx_dict[k] = idx\n                        else:\n                            for n in range(0, self.window.n_samples):\n                                idx_dict[k] = 0\n                    _, csv_data_dict, _ = self.csv_reader(idx=idx_dict,\n                                                          subject_id=subject_id,\n                                                          reject=reject)\n                    for name in csv_data_dict.keys():\n                        csv_data_array = []\n                        for n in range(0, self.window.n_samples):\n                            csv_data_array.append(csv_data_dict[name])\n                        if len(csv_data_array) == 1:\n                            output_dict[name] = np.asarray(csv_data_array[0],\n                                                           dtype=np.float32)\n                        else:\n                            output_dict[name] = np.concatenate(\n                                csv_data_array, 0).astype(dtype=np.float32)\n\n                    for name in csv_data_dict.keys():\n                        output_dict[name + \'_location\'] = output_dict[\'image_location\']\n                return output_dict\n                # the output image shape should be\n                # [enqueue_batch_size, x, y, z, time, modality]\n                # where enqueue_batch_size = windows_per_image\n            except ValueError:\n                tf.logging.fatal(""Cannot provide output for %s"" %subject_id)\n                raise\n        else:\n            tf.logging.fatal(""%s not in available list of subjects"" %subject_id)\n            raise ValueError\n\n    def csvcenter_spatial_coordinates(self,\n                                      subject_id,\n                                      data,\n                                      img_sizes,\n                                      win_sizes,\n                                      mode_correction=\'pad\',\n                                      n_samples=1):\n        """"""\n        Generate spatial coordinates for sampling.\n\n        Values in ``win_sizes`` could be different --\n        for example in a segmentation network ``win_sizes`` could be\n        ``{\'training_image_spatial_window\': (32, 32, 10),\n           \'Manual_label_spatial_window\': (16, 16, 10)}``\n        (the network reduces x-y plane spatial resolution).\n\n        This function handles this situation by first find the largest\n        window across these window definitions, and generate the coordinates.\n        These coordinates are then adjusted for each of the\n        smaller window sizes (the output windows are almost concentric)\n        This function provide the appropriate sampled coordinates modified\n        according to knowledge of the reader constraints on resolution and\n        orientation.\n        """"""\n\n        assert data is not None, ""No input from image reader. Please check"" \\\n                                 ""the configuration file.""\n\n        # infer the largest spatial window size and check image spatial shapes\n        img_spatial_size, win_spatial_size = \\\n            _infer_spatial_size(img_sizes, win_sizes)\n\n        window_centres = []\n        reject = False\n        if mode_correction == \'remove\':\n            reject = True\n\n        # try:\n        #     window_centres = csv_data.get(\'sampler\', None)\n        # except AttributeError:\n        #     pass\n\n        n_samples = max(n_samples, 1)\n        all_coordinates = {}\n\n# If there is no csv reader for the sampler, we fall back to a uniform sampling\n        if \'sampler\' not in self.csv_reader.task_param.keys():\n            window_centres = rand_spatial_coordinates(n_samples,\n                                                      img_spatial_size,\n                                                      win_spatial_size,\n                                                      None)\n            list_idx = np.arange(0, n_samples)\n\n        else:\n            window_centres_list = []\n            list_idx = []\n            _, _ = self.check_csv_sampler_valid(subject_id, img_sizes,\n                                                win_sizes)\n            idx_check, _, _ = self.csv_reader(\n                    subject_id=subject_id,  mode=\'multi\', reject=False)\n            idx_multi = idx_check[\'sampler\']\n            for mod in self.csv_reader.task_param:\n                all_coordinates[mod] = []\n            for n in range(0, n_samples):\n                # print(""reject value is "", reject)\n                idx, data_csv, _ = self.csv_reader(\n                    subject_id=subject_id,  mode=\'single\', reject=reject)\n                # print(data_csv[\'sampler\'].shape[0], \'data_sampler\')\n                if data_csv[\'sampler\'].shape[0] > 0:\n                    centre_transform = self.transform_centres(\n                        subject_id, img_sizes,\n                        np.expand_dims(np.squeeze(data_csv[\'sampler\']), 0))\n                    # centre_tmp = np.expand_dims(centre_transform,0)\n                    # for mod in idx.keys():\n                    #     all_coordinates[mod].append(np.expand_dims(\n                    #         np.squeeze(data_csv[mod]), 0))\n                    list_idx.append(idx[\'sampler\'])\n                    print(centre_transform.shape)\n                    window_centres_list.append(centre_transform)\n                    window_centres = np.concatenate(window_centres_list, 0)\n            # If nothing is valid and the mode of correction is rand, then we\n            #  default back to a uniform sampling\n            if np.sum(self.csv_reader.valid_by_task[\'sampler\'][idx_multi]) ==\\\n                    0 and np.asarray(window_centres).shape[0] == 0 and \\\n                    mode_correction == \'rand\':\n                tf.logging.warning(""Nothing is valid, taking random centres"")\n                window_centres = rand_spatial_coordinates(n_samples,\n                                                          img_spatial_size,\n                                                          win_spatial_size,\n                                                          None)\n                list_idx = np.arange(0, n_samples)\n        print(""all prepared and added "")\n\n        assert window_centres.shape == (n_samples, N_SPATIAL), \\\n            ""the coordinates generator should return "" \\\n            ""{} samples of rank {} locations"".format(n_samples, N_SPATIAL)\n\n        # adjust spatial coordinates based on each mod spatial window size\n\n        for mod in list(win_sizes):\n            win_size = np.asarray(win_sizes[mod][:N_SPATIAL])\n            half_win = np.floor(win_size / 2.0).astype(int)\n\n            # Make starting coordinates of the window\n            spatial_coords = np.zeros(\n                (1, N_SPATIAL * 2), dtype=np.int32)\n            if mode_correction != \'pad\':\n                spatial_coords[:, :N_SPATIAL] = np.maximum(\n                    window_centres[0, :N_SPATIAL] - half_win[:N_SPATIAL], 0)\n            else:\n                spatial_coords[:, :N_SPATIAL] = window_centres[0, :N_SPATIAL] \\\n                                                - half_win[:N_SPATIAL]\n\n            # Make the opposite corner of the window is\n            # just adding the mod specific window size\n            spatial_coords[:, N_SPATIAL:] = \\\n                spatial_coords[:, :N_SPATIAL] + win_size[:N_SPATIAL]\n\n            # assert np.all(spatial_coords[:, N_SPATIAL:] <= img_spatial_size),\n            #     \'spatial coords: out of bounds.\'\n\n            # include subject id as the 1st column of all_coordinates values\n            idx_subject_id = np.where(self.reader._file_list.subject_id ==\n                                      subject_id)[0][0]\n            idx_subject_id = np.ones((n_samples,),\n                                     dtype=np.int32) * idx_subject_id\n            spatial_coords = np.append(\n                idx_subject_id[:, None], spatial_coords, axis=1)\n            all_coordinates[mod] = spatial_coords\n\n        return all_coordinates, list_idx\n\n    def transform_centres(self, subject_id, img_sizes, windows_centres):\n        # For the moment assuming that same img size and orientations across\n        # modalities\n        list_mod = list(img_sizes.keys())\n\n        print(list_mod)\n        idx_subject_id = np.where(self.reader._file_list.subject_id ==\n                                  subject_id)[0][0]\n        input_shape = self.reader.output_list[idx_subject_id][list_mod[\n            0]].original_shape[:N_SPATIAL]\n        output_shape = self.reader.output_list[idx_subject_id][list_mod[\n            0]].shape[:N_SPATIAL]\n        init_axcodes = self.reader.output_list[idx_subject_id][list_mod[\n            0]].original_axcodes\n\n        fin_axcodes = self.reader.output_list[idx_subject_id][\n            list_mod[0]].output_axcodes\n        print(output_shape, init_axcodes[0], fin_axcodes[0])\n        transformed_centres, ornt_transf = do_reorientation_idx(\n            windows_centres, init_axcodes[0], fin_axcodes[0], input_shape)\n\n        transformed_centres = np.squeeze(transformed_centres.astype(np.int32))\n\n        # then taking care of change in pixdim\n        input_pixdim = self.reader.output_list[idx_subject_id][list_mod[\n            0]].original_pixdim[0]\n        output_pixdim = self.reader.output_list[idx_subject_id][list_mod[\n            0]].output_pixdim[0]\n        reorder_axes = np.squeeze(np.asarray(ornt_transf[:, 0]).astype(\n            np.int32))\n        print(""found pixdim to change"", input_pixdim, output_pixdim,\n              reorder_axes)\n        input_pixdim_no = [input_pixdim[r] for r in reorder_axes]\n        transformed_centres = do_resampling_idx(transformed_centres,\n                                                input_pixdim_no, output_pixdim)\n\n        if transformed_centres.ndim == 1:\n            transformed_centres = np.expand_dims(transformed_centres, 0)\n\n        padding = (np.asarray(img_sizes[list_mod[0]][:N_SPATIAL]) -\n                   np.asarray(output_shape)) / 2.0\n        padding = padding.astype(np.int32)\n        print(transformed_centres.shape, padding.shape)\n        transformed_centres += np.tile(np.expand_dims(padding, 0),\n                                       [len(windows_centres), 1])\n        return transformed_centres\n\n    def check_csv_sampler_valid(self, subject_id, img_sizes, win_sizes):\n        print(""Checking if csv_sampler valid is updated"")\n        reject = False\n        if self.mode_correction != \'pad\':\n            reject = True\n        idx_multi, csv_data, _ = self.csv_reader(subject_id=subject_id,\n                                                 idx=None, mode=\'multi\',\n                                                 reject=reject)\n\n        windows_centres = csv_data[\'sampler\']\n        # print(""Windows extracted"", windows_centres)\n        numb = windows_centres.shape[0]\n        if windows_centres.shape[0] > 0:\n            checked = self.csv_reader.valid_by_task[\'sampler\'][\n                idx_multi[\'sampler\']]\n            print(np.sum(checked), \'is sum of checked\')\n            min_checked = np.min(checked)\n            numb_valid = np.sum(checked)\n        else:\n            min_checked = 0\n            numb_valid = 0\n        if min_checked >= 0:\n            print(""Already checked, no need for further analysis"")\n            return numb, numb-numb_valid\n        else:\n            transformed_centres = self.transform_centres(subject_id, img_sizes,\n                                                         windows_centres)\n\n            img_spatial_size, win_spatial_size = _infer_spatial_size(\n                img_sizes, win_sizes)\n            tf.logging.warning(""Need to checked validity of samples for ""\n                               ""subject %s"" %subject_id)\n            checked = np.ones([numb])\n            pad = np.zeros([numb, 2*N_SPATIAL])\n            print(list(win_sizes))\n            for mod in list(win_sizes):\n                print(""mod is %s"" % mod)\n                print(img_spatial_size)\n                win_size = np.asarray(win_sizes[mod][:N_SPATIAL])\n                half_win = np.floor(win_size / 2.0).astype(int)\n\n                # Make starting coordinates of the window\n                spatial_coords = np.zeros(\n                    (numb, N_SPATIAL * 2), dtype=np.int32)\n                half_win_tiled = np.tile(half_win[:N_SPATIAL], [numb, 1])\n                reshaped_windows = np.reshape(transformed_centres[:,\n                                              :N_SPATIAL],\n                                              half_win_tiled.shape)\n                spatial_coords[:, :N_SPATIAL] = reshaped_windows - \\\n                                                half_win_tiled\n\n                min_spatial_coords = np.max(-1*spatial_coords, 1)\n                checked = np.asarray(np.where(min_spatial_coords > 0,\n                                              np.zeros_like(checked), checked))\n                pad = np.maximum(-1*spatial_coords, pad)\n                # Make the opposite corner of the window is\n                # just adding the mod specific window size\n                spatial_coords[:, N_SPATIAL:] = spatial_coords[:, :N_SPATIAL] +\\\n                                                np.tile(win_size[:N_SPATIAL],\n                                                        [numb, 1])\n\n                max_spatial_coords = np.max(spatial_coords[:, N_SPATIAL:] -\n                                            np.tile(img_spatial_size,\n                                                    [numb, 1]),\n                                            axis=1)\n                diff_spatial_size = spatial_coords[:, N_SPATIAL:] - np.tile(\n                    img_spatial_size, [numb, 1])\n                checked = np.asarray(np.where(max_spatial_coords > 0,\n                                              np.zeros_like(checked), checked))\n                pad[:, N_SPATIAL:] = np.maximum(diff_spatial_size,\n                                                pad[:, N_SPATIAL:])\n\n                tf.logging.warning(""to discard or pad is %d out of %d for mod ""\n                                   ""%s"" % (numb-np.sum(checked), numb, mod))\n\n            idx_discarded = []\n            for i in range(0, len(checked)):\n                self.csv_reader.valid_by_task[\'sampler\'][idx_multi[\n                    \'sampler\'][i]] = checked[i]\n                self.csv_reader.pad_by_task[\'sampler\'][idx_multi[\'sampler\'][\n                    i]] = pad[i]\n                if checked[i] == 0:\n                    idx_discarded.append(idx_multi[\'sampler\'][i])\n            # self.csv_reader.valid_by_task[\'sampler\'][np.asarray(idx_multi[\n            #     \'sampler\'])] = checked\n            print(\'Updated check\')\n            if np.sum(checked) < numb:\n                tf.logging.warning(""The following indices are not valid for ""\n                                   ""%s %s "" %(subject_id, \' \'.join(map(str,\n                                              idx_discarded))))\n            print(\n                ""updated valid part of csv_reader for subject %s"" % subject_id)\n            return numb, numb-np.sum(checked)\n\n\n# def correction_coordinates(coordinates, idx, pb_coord, img_sizes, win_sizes,\n#                            csv_sampler, mode=""remove""):\n#     # infer the largest spatial window size and check image spatial shapes\n#\n#     img_spatial_size, win_spatial_size = _infer_spatial_size(\n#         img_sizes, win_sizes)\n#     overall_pb = np.zeros([len(idx), 1])\n#     numb_wind = len(idx)\n#     for mod in list(win_sizes):\n#         overall_pb += np.sum(np.abs(pb_coord[mod]), 1)\n#     if np.sum(overall_pb) == 0:\n#         return coordinates, None\n#     else:\n#\n#         list_nopb = np.where(overall_pb == 0)\n#         list_pb = np.where(overall_pb > 0)\n#         idx_pb = idx[list_pb]\n#         if mode == ""remove"":\n#             for mod in list(win_sizes):\n#                 coordinates[mod]=coordinates[mod][list_nopb, :]\n#             return coordinates, idx_pb\n#         elif mode == ""replace"" :\n#             n_pb = np.sum(overall_pb)\n#             window_centres_replacement = rand_spatial_coordinates(\n#                 n_pb, img_spatial_size, win_spatial_size, None)\n#             spatial_coords_replacement = np.zeros(\n#                 (n_pb, N_SPATIAL * 2), dtype=np.int32)\n#\n#             for mod in list(win_sizes):\n#                 win_size = np.asarray(win_sizes[mod][:N_SPATIAL])\n#                 half_win = np.floor(win_size / 2.0).astype(int)\n#\n#                 # Make starting coordinates of the window\n#                 spatial_coords_replacement[:, :N_SPATIAL] = np.maximum(\n#                     window_centres_replacement[:, :N_SPATIAL] - np.tile(\n#                         half_win[:N_SPATIAL], [n_pb, 1]), 0)\n#                 spatial_coords_replacement[:, N_SPATIAL:] = \\\n#                     spatial_coords_replacement[:, :N_SPATIAL] + np.tile(\n#                         win_size[:N_SPATIAL], [n_pb, 1])\n#             n_replaced = 0\n#             for n in range(0, numb_wind):\n#                 if overall_pb[n]:\n#                     coordinates[n, :] = spatial_coords_replacement[n_replaced]\n#                     n_replaced += 1\n#             return coordinates, idx_pb\n\n\ndef rand_spatial_coordinates(\n        n_samples, img_spatial_size, win_spatial_size, sampler_map):\n    """"""\n    Generate spatial coordinates from a discrete uniform distribution.\n\n    :param n_samples: number of random coordinates to generate\n    :param img_spatial_size: input image size\n    :param win_spatial_size: input window size\n    :param sampler_map: sampling prior map (not in use)\n    :return: (n_samples, N_SPATIAL) coordinates representing sampling\n              window centres relative to img_spatial_size\n    """"""\n    tf.logging.debug(\'uniform sampler, prior %s ignored\', sampler_map)\n\n    # Sample coordinates at random\n    half_win = np.floor(np.asarray(win_spatial_size) / 2.0).astype(np.int32)\n    max_coords = np.zeros((n_samples, N_SPATIAL), dtype=np.int32)\n    for (idx, (img, win)) in enumerate(\n            zip(img_spatial_size[:N_SPATIAL], win_spatial_size[:N_SPATIAL])):\n        max_coords[:, idx] = np.random.randint(\n            0, max(img - win + 1, 1), n_samples)\n    max_coords[:, :N_SPATIAL] = \\\n        max_coords[:, :N_SPATIAL] + half_win[:N_SPATIAL]\n    return max_coords\n\n\ndef _infer_spatial_size(img_sizes, win_sizes):\n    """"""\n    Utility function to find the spatial size of image,\n    and the largest spatial window size across input sections.\n\n    Raises NotImplementedError if the images have\n    different spatial dimensions.\n\n    :param img_sizes: dictionary of {\'input_name\': (img_size_x, img_size,y,...)}\n    :param win_sizes: dictionary of {\'input_name\': (win_size_x, win_size_y,...)}\n    :return: (image_spatial_size, window_largest_spatial_size)\n    """"""\n    uniq_spatial_size = \\\n        set([img_size[:N_SPATIAL] for img_size in list(img_sizes.values())])\n    if len(uniq_spatial_size) != 1:\n        tf.logging.fatal(""Don\'t know how to generate sampling ""\n                         ""locations: Spatial dimensions of the ""\n                         ""grouped input sources are not ""\n                         ""consistent. %s"", uniq_spatial_size)\n        raise NotImplementedError\n    img_spatial_size = np.asarray(uniq_spatial_size.pop(), dtype=np.int32)\n\n    # find the largest spatial window across input sections\n    _win_spatial_sizes = \\\n        [win_size[:N_SPATIAL] for win_size in win_sizes.values()]\n    _win_spatial_sizes = np.asarray(_win_spatial_sizes, dtype=np.int32)\n    win_spatial_size = np.max(_win_spatial_sizes, axis=0)\n\n    assert all([img_spatial_size[i] >= win_spatial_size[i]\n                for i in range(N_SPATIAL)]), \\\n        ""window size {} is larger than image size {}"".format(\n            win_spatial_size, img_spatial_size)\n\n    return img_spatial_size, win_spatial_size\n'"
niftynet/contrib/csv_reader/sampler_grid_v2_csv.py,9,"b'# -*- coding: utf-8 -*-\n""""""\nSampling image by a sliding window.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.contrib.csv_reader.sampler_csv_rows import ImageWindowDatasetCSV\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\n\n\n# pylint: disable=too-many-locals\nclass GridSamplerCSV(ImageWindowDatasetCSV):\n    """"""\n    This class generators ND image samples with a sliding window.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 csv_reader,\n                 window_sizes,\n                 batch_size=1,\n                 spatial_window_size=None,\n                 window_border=None,\n                 queue_length=10,\n                 smaller_final_batch_mode=\'pad\',\n                 name=\'grid_sampler\'):\n\n        # override all spatial window defined in input\n        # modalities sections\n        # this is useful when do inference with a spatial window\n        # which is different from the training specifications\n        ImageWindowDatasetCSV.__init__(\n            self,\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=spatial_window_size or window_sizes,\n            batch_size=batch_size,\n            windows_per_image=1,\n            queue_length=queue_length,\n            shuffle=False,\n            epoch=1,\n            smaller_final_batch_mode=smaller_final_batch_mode,\n            name=name)\n        self.csv_reader = csv_reader\n        self.border_size = window_border or (0, 0, 0)\n        assert isinstance(self.border_size, (list, tuple)), \\\n            ""window_border should be a list or tuple""\n        while len(self.border_size) < N_SPATIAL:\n            self.border_size = tuple(self.border_size) + \\\n                               (self.border_size[-1],)\n        self.border_size = self.border_size[:N_SPATIAL]\n        tf.logging.info(\'initialised window instance\')\n        tf.logging.info(""initialised grid sampler %s"", self.window.shapes)\n\n    def layer_op(self, idx=None):\n        while True:\n            image_id, data, _ = self.reader(idx=None, shuffle=False)\n            if not data:\n                break\n            image_shapes = {name: data[name].shape\n                            for name in self.window.names}\n            static_window_shapes = self.window.match_image_shapes(image_shapes)\n            coordinates = grid_spatial_coordinates(\n                image_id, image_shapes, static_window_shapes, self.border_size)\n\n            # extend the number of sampling locations to be divisible\n            # by batch size\n            n_locations = list(coordinates.values())[0].shape[0]\n            extra_locations = 0\n            if (n_locations % self.batch_size) > 0:\n                extra_locations = \\\n                    self.batch_size - n_locations % self.batch_size\n            total_locations = n_locations + extra_locations\n\n            tf.logging.info(\n                \'grid sampling image sizes: %s\', image_shapes)\n            tf.logging.info(\n                \'grid sampling window sizes: %s\', static_window_shapes)\n            if extra_locations > 0:\n                tf.logging.info(\n                    ""yielding %s locations from image, ""\n                    ""extended to %s to be divisible by batch size %s"",\n                    n_locations, total_locations, self.batch_size)\n            else:\n                tf.logging.info(\n                    ""yielding %s locations from image"", n_locations)\n            for i in range(total_locations):\n                idx = i % n_locations\n                # \xc2\xa0initialise output dict\n                output_dict = {}\n                for name in list(data):\n                    assert coordinates[name].shape[0] == n_locations, \\\n                        ""different number of grid samples from the input"" \\\n                        ""images, don\'t know how to combine them in the queue""\n                    x_start, y_start, z_start, x_end, y_end, z_end = \\\n                        coordinates[name][idx, 1:]\n                    try:\n                        image_window = data[name][\n                            x_start:x_end, y_start:y_end, z_start:z_end, ...]\n                    except ValueError:\n                        tf.logging.fatal(\n                            ""dimensionality miss match in input volumes, ""\n                            ""please specify spatial_window_size with a ""\n                            ""3D tuple and make sure each element is ""\n                            ""smaller than the image length in each dim."")\n                        raise\n                    # fill output dict with data\n                    coord_key = LOCATION_FORMAT.format(name)\n                    image_key = name\n                    output_dict[coord_key] = coordinates[name][idx:idx+1, ...]\n                    output_dict[image_key] = image_window[np.newaxis, ...]\n                    if self.csv_reader is not None:\n                        _, label_dict, _ = self.csv_reader(idx=image_id)\n                        output_dict.update(label_dict)\n                        for name in self.csv_reader.names:\n                            output_dict[name + \'_location\'] = output_dict[\n                                \'image_location\']\n                yield output_dict\n\n        # this is needed because otherwise reading beyond the last element\n        # raises an out-of-range error, and the last grid sample\n        # will not be processed properly.\n        try:\n            for name in list(output_dict):\n                output_dict[name] = np.ones_like(output_dict[name]) * -1\n            if self.csv_reader is not None:\n                _, label_dict, _ = self.csv_reader(idx=image_id)\n                output_dict.update(label_dict)\n                for name in self.csv_reader.task_param.keys():\n                    output_dict[name + \'_location\'] = output_dict[\n                        \'image_location\']\n            yield output_dict\n        except (NameError, KeyError):\n            tf.logging.fatal(""No feasible samples from %s"", self)\n            raise\n\ndef grid_spatial_coordinates(subject_id, img_sizes, win_sizes, border_size):\n    """"""\n    This function generates all coordinates of feasible windows, with\n    step sizes specified in grid_size parameter.\n\n    The border size changes the sampling locations but not the\n    corresponding window sizes of the coordinates.\n\n    :param subject_id: integer value indicates the position of of this\n        image in ``image_reader.file_list``\n    :param img_sizes: a dictionary of image shapes, ``{input_name: shape}``\n    :param win_sizes: a dictionary of window shapes, ``{input_name: shape}``\n    :param border_size: size of padding on both sides of each dim\n    :return:\n    """"""\n    all_coordinates = {}\n    for name, image_shape in img_sizes.items():\n        window_shape = win_sizes[name]\n        grid_size = [max(win_size - 2 * border, 0)\n                     for (win_size, border) in zip(window_shape, border_size)]\n        assert len(image_shape) >= N_SPATIAL, \\\n            \'incompatible image shapes in grid_spatial_coordinates\'\n        assert len(window_shape) >= N_SPATIAL, \\\n            \'incompatible window shapes in grid_spatial_coordinates\'\n        assert len(grid_size) >= N_SPATIAL, \\\n            \'incompatible border sizes in grid_spatial_coordinates\'\n        steps_along_each_dim = [\n            _enumerate_step_points(starting=0,\n                                   ending=image_shape[i],\n                                   win_size=window_shape[i],\n                                   step_size=grid_size[i])\n            for i in range(N_SPATIAL)]\n        starting_coords = np.asanyarray(np.meshgrid(*steps_along_each_dim))\n        starting_coords = starting_coords.reshape((N_SPATIAL, -1)).T\n        n_locations = starting_coords.shape[0]\n        # prepare the output coordinates matrix\n        spatial_coords = np.zeros((n_locations, N_SPATIAL * 2), dtype=np.int32)\n        spatial_coords[:, :N_SPATIAL] = starting_coords\n        for idx in range(N_SPATIAL):\n            spatial_coords[:, N_SPATIAL + idx] = \\\n                starting_coords[:, idx] + window_shape[idx]\n        max_coordinates = np.max(spatial_coords, axis=0)[N_SPATIAL:]\n        assert np.all(max_coordinates <= image_shape[:N_SPATIAL]), \\\n            ""window size greater than the spatial coordinates {} : {}"".format(\n                max_coordinates, image_shape)\n        subject_list = np.ones((n_locations, 1), dtype=np.int32) * subject_id\n        spatial_coords = np.append(subject_list, spatial_coords, axis=1)\n        all_coordinates[name] = spatial_coords\n    return all_coordinates\n\n\ndef _enumerate_step_points(starting, ending, win_size, step_size):\n    """"""\n    generate all possible sampling size in between starting and ending.\n\n    :param starting: integer of starting value\n    :param ending: integer of ending value\n    :param win_size: integer of window length\n    :param step_size: integer of distance between two sampling points\n    :return: a set of unique sampling points\n    """"""\n    try:\n        starting = max(int(starting), 0)\n        ending = max(int(ending), 0)\n        win_size = max(int(win_size), 1)\n        step_size = max(int(step_size), 1)\n    except (TypeError, ValueError):\n        tf.logging.fatal(\n            \'step points should be specified by integers, received:\'\n            \'%s, %s, %s, %s\', starting, ending, win_size, step_size)\n        raise ValueError\n    if starting > ending:\n        starting, ending = ending, starting\n    sampling_point_set = []\n    while (starting + win_size) <= ending:\n        sampling_point_set.append(starting)\n        starting = starting + step_size\n    additional_last_point = ending - win_size\n    sampling_point_set.append(max(additional_last_point, 0))\n    sampling_point_set = np.unique(sampling_point_set).flatten()\n    if len(sampling_point_set) == 2:\n        # in case of too few samples, adding\n        # an additional sampling point to\n        # the middle between starting and ending\n        sampling_point_set = np.append(\n            sampling_point_set, np.round(np.mean(sampling_point_set)))\n    _, uniq_idx = np.unique(sampling_point_set, return_index=True)\n    return sampling_point_set[np.sort(uniq_idx)]\n'"
niftynet/contrib/csv_reader/sampler_linear_interpolate_v2_csv.py,1,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating samples by linearly combining two input images.\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.contrib.csv_reader.sampler_csv_rows import ImageWindowDatasetCSV\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\n\n\nclass LinearInterpolateSamplerCSV(ImageWindowDatasetCSV):\n    """"""\n    This class reads two feature vectors from files (often generated\n    by running feature extractors on images in advance)\n    and returns n linear combinations of the vectors.\n    The coefficients are generated by::\n\n        np.linspace(0, 1, n_interpolations)\n    """"""\n\n    def __init__(self,\n                 reader,\n                 csv_reader,\n                 window_sizes,\n                 batch_size=10,\n                 n_interpolations=10,\n                 queue_length=10,\n                 name=\'linear_interpolation_sampler\'):\n        ImageWindowDatasetCSV.__init__(\n            self,\n            reader,\n            csv_reader=csv_reader,\n            window_sizes=window_sizes,\n            batch_size=batch_size,\n            queue_length=queue_length,\n            shuffle=False,\n            epoch=1,\n            smaller_final_batch_mode=\'drop\',\n            name=name)\n        self.n_interpolations = n_interpolations\n        # only try to use the first spatial shape available\n        image_spatial_shape = list(self.reader.shapes.values())[0][:3]\n        self.window.set_spatial_shape(image_spatial_shape)\n        tf.logging.info(\n            ""initialised linear interpolation sampler %s "", self.window.shapes)\n        assert not self.window.has_dynamic_shapes, \\\n            ""dynamic shapes not supported, please specify "" \\\n            ""spatial_window_size = (1, 1, 1)""\n\n    def layer_op(self, *_unused_args, **_unused_kwargs):\n        """"""\n        This function first reads two vectors, and interpolates them\n        with self.n_interpolations mixing coefficients.\n\n        Location coordinates are set to ``np.ones`` for all the vectors.\n        """"""\n        output_dict = {}\n        image_id_x, data_x, _ = self.reader(idx=None, shuffle=False)\n        image_id_y, data_y, _ = self.reader(idx=None, shuffle=True)\n        if not data_x or not data_y:\n            return\n        if image_id_x == image_id_y:\n            while image_id_x == image_id_y:\n                image_id_x, data_x, _ = self.reader(idx=None, shuffle=False)\n                image_id_y, data_y, _ = self.reader(idx=None, shuffle=True)\n                if not data_x or not data_y:\n                    return\n        embedding_x = data_x[self.window.names[0]]\n        embedding_y = data_y[self.window.names[0]]\n\n        steps = np.linspace(0, 1, self.n_interpolations)\n        for (_, mixture) in enumerate(steps):\n            output_vector = \\\n                embedding_x * mixture + embedding_y * (1 - mixture)\n            coordinates = np.ones((1, N_SPATIAL * 2 + 1), dtype=np.int32)\n            coordinates[0, 0:2] = [image_id_x, image_id_y]\n            output_dict = {}\n            for name in self.window.names:\n                coordinates_key = LOCATION_FORMAT.format(name)\n                image_data_key = name\n                output_dict[coordinates_key] = coordinates\n                output_dict[image_data_key] = output_vector[np.newaxis, ...]\n        if self.csv_reader is not None:\n            _, label_dict_x, _ = self.csv_reader(idx=image_id_x)\n            _, label_dict_y, _ = self.csv_reader(idx=image_id_y)\n            output_dict.update(label_dict_x)\n            output_dict.update(label_dict_y)\n            for name in self.csv_reader.names():\n                    output_dict[name + \'_location\'] = output_dict[\n                            \'image_location\']\n        return output_dict\n'"
niftynet/contrib/csv_reader/sampler_random_vector_v2_csv.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating sample arrays from random distributions.\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.engine.image_window_dataset import ImageWindowDataset\nfrom niftynet.contrib.csv_reader.sampler_csv_rows import ImageWindowDatasetCSV\nfrom niftynet.engine.image_window import \\\n    N_SPATIAL, LOCATION_FORMAT, ImageWindow\n\n\nclass RandomVectorSampler(ImageWindowDataset):\n    """"""\n    This class generates two samples from the standard normal\n    distribution.  These two samples are mixed with n\n    mixing coefficients. The coefficients are generated\n    by ``np.linspace(0, 1, n_interpolations)``\n    """"""\n\n    def __init__(self,\n                 names=(\'vector\',),\n                 vector_size=(100,),\n                 batch_size=10,\n                 n_interpolations=10,\n                 mean=0.0,\n                 stddev=1.0,\n                 repeat=1,\n                 queue_length=10,\n                 name=\'random_vector_sampler\'):\n        # repeat=None for infinite loops\n        self.n_interpolations = max(n_interpolations, 1)\n        self.mean = mean\n        self.stddev = stddev\n        self.repeat = repeat\n        self.names = names\n\n        ImageWindowDatasetCSV.__init__(\n            self,\n            reader=None,\n            csv_reader=None,\n            window_sizes={names[0]: {\'spatial_window_size\': vector_size}},\n            batch_size=batch_size,\n            queue_length=queue_length,\n            shuffle=False,\n            epoch=1,\n            smaller_final_batch_mode=\'drop\',\n            name=name)\n        self.window = ImageWindow(shapes={names[0]: vector_size},\n                                  dtypes={names[0]: tf.float32})\n        tf.logging.info(""initialised sampler output %s "", self.window.shapes)\n\n    def layer_op(self):\n        """"""\n        This function first draws two samples, and interpolates them\n        with self.n_interpolations mixing coefficients.\n\n        Location coordinates are set to ``np.ones`` for all the vectors.\n        """"""\n        total_iter = self.repeat if self.repeat is not None else 1\n        while total_iter > 0:\n            total_iter = total_iter - 1 if self.repeat is not None else 1\n            embedding_x = np.random.normal(\n                self.mean,\n                self.stddev,\n                self.window.shapes[self.window.names[0]])\n            embedding_y = np.random.normal(\n                self.mean,\n                self.stddev,\n                self.window.shapes[self.window.names[0]])\n            steps = np.linspace(0, 1, self.n_interpolations)\n            for (_, mixture) in enumerate(steps):\n                output_vector = \\\n                    embedding_x * mixture + embedding_y * (1 - mixture)\n                coordinates = np.ones((1, N_SPATIAL * 2 + 1), dtype=np.int32)\n                output_dict = {}\n                for name in self.window.names:\n                    coordinates_key = LOCATION_FORMAT.format(name)\n                    image_data_key = name\n                    output_dict[coordinates_key] = coordinates\n                    output_dict[image_data_key] = output_vector\n                yield output_dict\n'"
niftynet/contrib/csv_reader/sampler_resize_v2_csv.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nResize input image as output window.\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport scipy.ndimage\nimport tensorflow as tf\n\nfrom niftynet.contrib.csv_reader.sampler_csv_rows import ImageWindowDatasetCSV\nfrom niftynet.engine.image_window import LOCATION_FORMAT\n\n\nclass ResizeSamplerCSV(ImageWindowDatasetCSV):\n    """"""\n    This class generates samples by rescaling\n    the whole image to the desired size\n    Assuming the reader\'s output is 5d:\n    ``Height x Width x Depth x time x Modality``\n    """"""\n\n    def __init__(self,\n                 reader,\n                 csv_reader=None,\n                 window_sizes=None,\n                 batch_size=1,\n                 spatial_window_size=None,\n                 windows_per_image=1,\n                 shuffle=True,\n                 queue_length=10,\n                 num_threads=4,\n                 smaller_final_batch_mode=\'pad\',\n                 name=\'resize_sampler_v2\'):\n        tf.logging.info(\'reading size of preprocessed images\')\n        self.csv_reader = csv_reader\n        ImageWindowDatasetCSV.__init__(\n            self,\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=window_sizes,\n            batch_size=batch_size,\n            windows_per_image=windows_per_image,\n            queue_length=queue_length,\n            num_threads=num_threads,\n            shuffle=shuffle,\n            epoch=-1 if shuffle else 1,\n            smaller_final_batch_mode=smaller_final_batch_mode,\n            name=name)\n        if spatial_window_size:\n            # override all spatial window defined in input\n            # modalities sections\n            # this is useful when do inference with a spatial window\n            # which is different from the training specifications\n            self.window.set_spatial_shape(spatial_window_size)\n        # tf.logging.info(""initialised resize sampler %s "", self.window.shapes)\n        # tf.logging.info(\'CSV reader is {}\'.format(self.csv_reader))\n\n    def layer_op(self, idx=None):\n        """"""\n        This function generates sampling windows to the input buffer\n        image data are from ``self.reader()``.\n\n        It first completes window shapes based on image data,\n        then resize each image as window and output\n        a dictionary (required by input buffer)\n\n        :return: output data dictionary ``{\'image_modality\': data_array}``\n        """"""\n        image_id, data, interp_orders = self.reader(idx=idx)\n        image_shapes = \\\n            dict((name, data[name].shape) for name in self.window.names)\n        # window shapes can be dynamic, here they\n        # are converted to static ones\n        # as now we know the image shapes\n        static_window_shapes = self.window.match_image_shapes(image_shapes)\n        # for resize sampler the coordinates are not used\n        # simply use the spatial dims of the input image\n        output_dict = {}\n        for name in list(data):\n            # prepare output dictionary keys\n            coordinates_key = LOCATION_FORMAT.format(name)\n            image_data_key = name\n\n            output_dict[coordinates_key] = self.dummy_coordinates(\n                image_id, static_window_shapes[name], self.window.n_samples)\n            image_array = []\n            for _ in range(self.window.n_samples):\n                # prepare image data\n                image_shape = image_shapes[name]\n                window_shape = static_window_shapes[name]\n\n                if image_shape == window_shape or interp_orders[name][0] < 0:\n                    # already in the same shape\n                    image_window = data[name]\n                else:\n                    zoom_ratio = [float(p) / float(d) for p, d in\n                                  zip(window_shape, image_shape)]\n                    image_window = zoom_3d(image=data[name],\n                                           ratio=zoom_ratio, interp_order=\n                                           interp_orders[name][0])\n                image_array.append(image_window[np.newaxis, ...])\n            if len(image_array) > 1:\n                output_dict[image_data_key] = \\\n                    np.concatenate(image_array, axis=0)\n            else:\n                output_dict[image_data_key] = image_array[0]\n        # the output image shape should be\n        # [enqueue_batch_size, x, y, z, time, modality]\n        # here enqueue_batch_size = 1 as we only have one sample\n        # per image\n        if self.csv_reader is not None:\n            _, label_dict, _ = self.csv_reader(idx=image_id)\n            output_dict.update(label_dict)\n            for name in self.csv_reader.names:\n                output_dict[name + \'_location\'] = output_dict[\'image_location\']\n        return output_dict\n\n\ndef zoom_3d(image, ratio, interp_order):\n    """"""\n    Taking 5D image as input, and zoom each 3D slice independently\n    """"""\n    assert image.ndim == 5, ""input images should be 5D array""\n    output = []\n    for time_pt in range(image.shape[3]):\n        output_mod = []\n        for mod in range(image.shape[4]):\n            zoomed = scipy.ndimage.zoom(\n                image[..., time_pt, mod], ratio[:3], order=interp_order)\n            output_mod.append(zoomed[..., np.newaxis, np.newaxis])\n        output.append(np.concatenate(output_mod, axis=-1))\n    return np.concatenate(output, axis=-2)\n'"
niftynet/contrib/csv_reader/sampler_uniform_v2_csv.py,4,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating uniformly distributed image window from input image\nThis can also be considered as a ""random cropping"" layer of the\ninput image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.contrib.csv_reader.sampler_csv_rows import ImageWindowDatasetCSV\nfrom niftynet.engine.image_window import N_SPATIAL, LOCATION_FORMAT\n\n\nclass UniformSamplerCSV(ImageWindowDatasetCSV):\n    """"""\n    This class generates samples by uniformly sampling each input volume\n    currently the coordinates are randomised for spatial dims only,\n    i.e., the first three dims of image.\n\n    This layer can be considered as a ""random cropping"" layer of the\n    input image.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 csv_reader,\n                 window_sizes,\n                 batch_size=1,\n                 windows_per_image=1,\n                 queue_length=10,\n                 num_threads=4,\n                 smaller_final_batch_mode=\'drop\',\n                 name=\'uniform_sampler_v2\'):\n        ImageWindowDatasetCSV.__init__(\n            self,\n            reader=reader,\n            csv_reader=csv_reader,\n            window_sizes=window_sizes,\n            batch_size=batch_size,\n            windows_per_image=windows_per_image,\n            queue_length=queue_length,\n            num_threads=num_threads,\n            shuffle=True,\n            epoch=-1,\n            smaller_final_batch_mode=smaller_final_batch_mode,\n            name=name)\n\n        tf.logging.info(""initialised uniform sampler %s "", self.window.shapes)\n        self.window_centers_sampler = rand_spatial_coordinates\n\n    # pylint: disable=too-many-locals\n    def layer_op(self, idx=None):\n        """"""\n        This function generates sampling windows to the input buffer\n        image data are from ``self.reader()``\n\n        It first completes window shapes based on image data,\n        then finds random coordinates based on the window shapes\n        finally extract window with the coordinates and output\n        a dictionary (required by input buffer).\n\n        :return: output data dictionary\n            ``{image_modality: data_array, image_location: n_samples * 7}``\n        """"""\n        image_id, data, _ = self.reader(idx=idx, shuffle=True)\n        image_shapes = dict(\n            (name, data[name].shape) for name in self.window.names)\n        static_window_shapes = self.window.match_image_shapes(image_shapes)\n        # find random coordinates based on window and image shapes\n        coordinates = self._spatial_coordinates_generator(\n            subject_id=image_id,\n            data=data,\n            img_sizes=image_shapes,\n            win_sizes=static_window_shapes,\n            n_samples=self.window.n_samples)\n\n        # initialise output dict, placeholders as dictionary keys\n        # this dictionary will be used in\n        # enqueue operation in the form of: `feed_dict=output_dict`\n        output_dict = {}\n        # fill output dict with data\n        for name in list(data):\n            coordinates_key = LOCATION_FORMAT.format(name)\n            image_data_key = name\n\n            # fill the coordinates\n            location_array = coordinates[name]\n            output_dict[coordinates_key] = location_array\n\n            # fill output window array\n            image_array = []\n            for window_id in range(self.window.n_samples):\n                x_start, y_start, z_start, x_end, y_end, z_end = \\\n                    location_array[window_id, 1:]\n                try:\n                    image_window = data[name][\n                        x_start:x_end, y_start:y_end, z_start:z_end, ...]\n                    image_array.append(image_window[np.newaxis, ...])\n                except ValueError:\n                    tf.logging.fatal(\n                        ""dimensionality miss match in input volumes, ""\n                        ""please specify spatial_window_size with a ""\n                        ""3D tuple and make sure each element is ""\n                        ""smaller than the image length in each dim. ""\n                        ""Current coords %s"", location_array[window_id])\n                    raise\n            if len(image_array) > 1:\n                output_dict[image_data_key] = \\\n                    np.concatenate(image_array, axis=0)\n            else:\n                output_dict[image_data_key] = image_array[0]\n        # the output image shape should be\n        # [enqueue_batch_size, x, y, z, time, modality]\n        # where enqueue_batch_size = windows_per_image\n        if self.csv_reader is not None:\n            _, label_dict, _ = self.csv_reader(idx=image_id)\n            output_dict.update(label_dict)\n            for name in self.csv_reader.names:\n                output_dict[name + \'_location\'] = output_dict[\'image_location\']\n        print(""output_dict gotten"", output_dict.keys(), len(output_dict.keys()))\n        return output_dict\n\n    def _spatial_coordinates_generator(self,\n                                       subject_id,\n                                       data,\n                                       img_sizes,\n                                       win_sizes,\n                                       n_samples=1):\n        """"""\n        Generate spatial coordinates for sampling.\n\n        Values in ``win_sizes`` could be different --\n        for example in a segmentation network ``win_sizes`` could be\n        ``{\'training_image_spatial_window\': (32, 32, 10),\n           \'Manual_label_spatial_window\': (16, 16, 10)}``\n        (the network reduces x-y plane spatial resolution).\n\n        This function handles this situation by first find the largest\n        window across these window definitions, and generate the coordinates.\n        These coordinates are then adjusted for each of the\n        smaller window sizes (the output windows are almost concentric).\n        """"""\n\n        assert data is not None, ""No input from image reader. Please check"" \\\n                                 ""the configuration file.""\n\n        # infer the largest spatial window size and check image spatial shapes\n        img_spatial_size, win_spatial_size = \\\n            _infer_spatial_size(img_sizes, win_sizes)\n\n        sampling_prior_map = None\n        try:\n            sampling_prior_map = data.get(\'sampler\', None)\n        except AttributeError:\n            pass\n\n        n_samples = max(n_samples, 1)\n        window_centres = self.window_centers_sampler(\n            n_samples, img_spatial_size, win_spatial_size, sampling_prior_map)\n        assert window_centres.shape == (n_samples, N_SPATIAL), \\\n            ""the coordinates generator should return "" \\\n            ""{} samples of rank {} locations"".format(n_samples, N_SPATIAL)\n\n        # adjust spatial coordinates based on each mod spatial window size\n        all_coordinates = {}\n        for mod in list(win_sizes):\n            win_size = np.asarray(win_sizes[mod][:N_SPATIAL])\n            half_win = np.floor(win_size / 2.0).astype(int)\n\n            # Make starting coordinates of the window\n            spatial_coords = np.zeros(\n                (n_samples, N_SPATIAL * 2), dtype=np.int32)\n            spatial_coords[:, :N_SPATIAL] = np.maximum(\n                window_centres[:, :N_SPATIAL] - half_win[:N_SPATIAL], 0)\n\n            # Make the opposite corner of the window is\n            # just adding the mod specific window size\n            spatial_coords[:, N_SPATIAL:] = \\\n                spatial_coords[:, :N_SPATIAL] + win_size[:N_SPATIAL]\n            assert np.all(spatial_coords[:, N_SPATIAL:] <= img_spatial_size), \\\n                \'spatial coords: out of bounds.\'\n\n            # include subject id as the 1st column of all_coordinates values\n            subject_id = np.ones((n_samples,), dtype=np.int32) * subject_id\n            spatial_coords = np.append(\n                subject_id[:, None], spatial_coords, axis=1)\n            all_coordinates[mod] = spatial_coords\n\n        return all_coordinates\n\n\ndef rand_spatial_coordinates(\n        n_samples, img_spatial_size, win_spatial_size, sampler_map):\n    """"""\n    Generate spatial coordinates from a discrete uniform distribution.\n\n    :param n_samples: number of random coordinates to generate\n    :param img_spatial_size: input image size\n    :param win_spatial_size: input window size\n    :param sampler_map: sampling prior map (not in use)\n    :return: (n_samples, N_SPATIAL) coordinates representing sampling\n              window centres relative to img_spatial_size\n    """"""\n    tf.logging.debug(\'uniform sampler, prior %s ignored\', sampler_map)\n\n    # Sample coordinates at random\n    half_win = np.floor(np.asarray(win_spatial_size) / 2.0).astype(np.int32)\n    max_coords = np.zeros((n_samples, N_SPATIAL), dtype=np.int32)\n    for (idx, (img, win)) in enumerate(\n            zip(img_spatial_size[:N_SPATIAL], win_spatial_size[:N_SPATIAL])):\n        max_coords[:, idx] = np.random.randint(\n            0, max(img - win + 1, 1), n_samples)\n    max_coords[:, :N_SPATIAL] = \\\n        max_coords[:, :N_SPATIAL] + half_win[:N_SPATIAL]\n    return max_coords\n\n\ndef _infer_spatial_size(img_sizes, win_sizes):\n    """"""\n    Utility function to find the spatial size of image,\n    and the largest spatial window size across input sections.\n\n    Raises NotImplementedError if the images have\n    different spatial dimensions.\n\n    :param img_sizes: dictionary of {\'input_name\': (img_size_x, img_size,y,...)}\n    :param win_sizes: dictionary of {\'input_name\': (win_size_x, win_size_y,...)}\n    :return: (image_spatial_size, window_largest_spatial_size)\n    """"""\n    uniq_spatial_size = \\\n        set([img_size[:N_SPATIAL] for img_size in list(img_sizes.values())])\n    if len(uniq_spatial_size) != 1:\n        tf.logging.fatal(""Don\'t know how to generate sampling ""\n                         ""locations: Spatial dimensions of the ""\n                         ""grouped input sources are not ""\n                         ""consistent. %s"", uniq_spatial_size)\n        raise NotImplementedError\n    img_spatial_size = np.asarray(uniq_spatial_size.pop(), dtype=np.int32)\n\n    # find the largest spatial window across input sections\n    _win_spatial_sizes = \\\n        [win_size[:N_SPATIAL] for win_size in win_sizes.values()]\n    _win_spatial_sizes = np.asarray(_win_spatial_sizes, dtype=np.int32)\n    win_spatial_size = np.max(_win_spatial_sizes, axis=0)\n\n    assert all([img_spatial_size[i] >= win_spatial_size[i]\n                for i in range(N_SPATIAL)]), \\\n        ""window size {} is larger than image size {}"".format(\n            win_spatial_size, img_spatial_size)\n\n    return img_spatial_size, win_spatial_size\n'"
niftynet/contrib/csv_reader/sampler_weighted_v2_csv.py,3,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating image window by weighted sampling map from input image\nThis can also be considered as a ""weighted random cropping"" layer of the\ninput image.\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.contrib.csv_reader.sampler_uniform_v2_csv import UniformSamplerCSV\nfrom niftynet.engine.image_window import N_SPATIAL\n\n\nclass WeightedSamplerCSV(UniformSamplerCSV):\n    """"""\n    This class generators samples from a user provided\n    frequency map for each input volume\n    The sampling likelihood of each voxel (and window around)\n    is proportional to its frequency\n\n    This is implemented in a closed form using cumulative histograms\n    for efficiency purposes i.e., the first three dims of image.\n\n    This layer can be considered as a ""weighted random cropping"" layer of the\n    input image.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 csv_reader,\n                 window_sizes,\n                 batch_size=1,\n                 windows_per_image=1,\n                 queue_length=10,\n                 name=\'weighted_sampler\'):\n        UniformSamplerCSV.__init__(self,\n                                   reader=reader,\n                                   csv_reader=csv_reader,\n                                   window_sizes=window_sizes,\n                                   batch_size=batch_size,\n                                   windows_per_image=windows_per_image,\n                                   queue_length=queue_length,\n                                   name=name)\n        tf.logging.info(\'Initialised weighted sampler window instance\')\n        self.window_centers_sampler = weighted_spatial_coordinates\n\n\ndef weighted_spatial_coordinates(\n        n_samples, img_spatial_size, win_spatial_size, sampler_map):\n    """"""\n    Weighted sampling from a map.\n    This function uses a cumulative histogram for fast sampling.\n\n    see also `sampler_uniform.rand_spatial_coordinates`\n\n    :param n_samples: number of random coordinates to generate\n    :param img_spatial_size: input image size\n    :param win_spatial_size: input window size\n    :param sampler_map: sampling prior map, it\'s spatial shape should be\n            consistent with `img_spatial_size`\n    :return: (n_samples, N_SPATIAL) coordinates representing sampling\n              window centres relative to img_spatial_size\n    """"""\n    assert sampler_map is not None, \\\n        \'sampling prior map is not specified, \' \\\n        \'please check `sampler=` option in the config.\'\n    # Get the cumulative sum of the normalised sorted intensities\n    # i.e. first sort the sampling frequencies, normalise them\n    # to sum to one, and then accumulate them in order\n    assert np.all(img_spatial_size[:N_SPATIAL] ==\n                  sampler_map.shape[:N_SPATIAL]), \\\n        \'image and sampling map shapes do not match\'\n    win_spatial_size = np.asarray(win_spatial_size, dtype=np.int32)\n    cropped_map = crop_sampling_map(sampler_map, win_spatial_size)\n    flatten_map = cropped_map.flatten()\n    flatten_map = flatten_map - np.min(flatten_map)\n    normaliser = flatten_map.sum()\n    # get the sorting indexes to that we can invert the sorting later on.\n    sorted_indexes = np.argsort(flatten_map)\n    sorted_data = np.cumsum(\n        np.true_divide(flatten_map[sorted_indexes], normaliser))\n\n    middle_coords = np.zeros((n_samples, N_SPATIAL), dtype=np.int32)\n    for sample in range(0, n_samples):\n        # get n_sample from the cumulative histogram, spaced by 1/n_samples,\n        # plus a random perturbation to give us a stochastic sampler\n        sample_ratio = 1 - (np.random.random() + sample) / (n_samples + 1)\n        # find the index where the cumulative it above the sample threshold\n        try:\n            if normaliser == 0:\n                # constant map? reducing to a uniform sampling\n                sample_index = np.random.randint(len(sorted_data))\n            else:\n                sample_index = np.argmax(sorted_data >= sample_ratio)\n        except ValueError:\n            tf.logging.fatal(""unable to choose sampling window based on ""\n                             ""the current frequency map."")\n            raise\n        # invert the sample index to the pre-sorted index\n        inverted_sample_index = sorted_indexes[sample_index]\n        # get the x,y,z coordinates on the cropped_map\n        middle_coords[sample, :N_SPATIAL] = np.unravel_index(\n            inverted_sample_index, cropped_map.shape)[:N_SPATIAL]\n\n    # re-shift coords due to the crop\n    half_win = np.floor(win_spatial_size / 2).astype(np.int32)\n    middle_coords[:, :N_SPATIAL] = \\\n        middle_coords[:, :N_SPATIAL] + half_win[:N_SPATIAL]\n    return middle_coords\n\n\ndef crop_sampling_map(input_map, win_spatial_size):\n    """"""\n    Utility function for generating a cropped version of the\n    input sampling prior map (the input weight map where the centre of\n    the window might be). If the centre of the window was outside of\n    this crop area, the patch would be outside of the field of view\n\n    :param input_map: the input weight map where the centre of\n                      the window might be\n    :param win_spatial_size: size of the borders to be cropped\n    :return: cropped sampling map\n    """"""\n\n    # prepare cropping indices\n    _start, _end = [], []\n    for win_size, img_size in \\\n            zip(win_spatial_size[:N_SPATIAL], input_map.shape[:N_SPATIAL]):\n        # cropping floor of the half window\n        d_start = int(win_size / 2.0)\n        # using ceil of half window\n        d_end = img_size - win_size + int(win_size / 2.0 + 0.6)\n\n        _start.append(d_start)\n        _end.append(d_end + 1 if d_start == d_end else d_end)\n\n    try:\n        assert len(_start) == 3\n        cropped_map = input_map[\n            _start[0]:_end[0], _start[1]:_end[1], _start[2]:_end[2], 0, 0]\n        assert np.all(cropped_map.shape) > 0\n    except (IndexError, KeyError, TypeError, AssertionError):\n        tf.logging.fatal(\n            ""incompatible map: %s and window size: %s\\n""\n            ""try smaller (fully-specified) spatial window sizes?"",\n            input_map.shape, win_spatial_size)\n        raise\n    return cropped_map\n'"
niftynet/contrib/csv_reader/segmentation_application_patchsampler.py,11,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_weighted_v2 import WeightedSampler\nfrom niftynet.engine.sampler_balanced_v2 import BalancedSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_segmentation import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.evaluation.segmentation_evaluator import SegmentationEvaluator\nfrom niftynet.layer.rand_elastic_deform import RandomElasticDeformationLayer\nfrom niftynet.contrib.csv_reader.csv_reader import CSVReader\nfrom niftynet.contrib.csv_reader.sampler_csvpatch import CSVPatchSampler\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'weight\', \'sampler\', \'inferred\'])\n\n\nclass SegmentationApplicationPatchSampler(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, action):\n        super(SegmentationApplicationPatchSampler, self).__init__()\n        tf.logging.info(\'starting segmentation application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.segmentation_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'patch\': (self.initialise_csvsampler,\n                      self.initialise_grid_sampler,\n                      self.initialise_grid_aggregator),\n            \'uniform\': (self.initialise_uniform_sampler,\n                        self.initialise_grid_sampler,\n                        self.initialise_grid_aggregator),\n            \'weighted\': (self.initialise_weighted_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler,\n                       self.initialise_resize_aggregator),\n            \'balanced\': (self.initialise_balanced_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.segmentation_param = task_param\n\n        # initialise input image readers\n        if self.is_training:\n            reader_names = (\'image\', \'label\', \'weight\')\n            csv_reader_names = (\'sampler\',)\n        elif self.is_inference:\n            # in the inference process use `image` input only\n            reader_names = (\'image\',)\n        elif self.is_evaluation:\n            reader_names = (\'image\', \'label\', \'inferred\')\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n        self.csv_readers = [\n            CSVReader(csv_reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        # initialise input preprocessing layers\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=self.net_param.foreground_type,\n            multimod_fusion=self.net_param.multimod_foreground_type,\n            threshold=0.0) \\\n            if self.net_param.normalise_foreground_only else None\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer) \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            binary_masking_func=foreground_masking_layer,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n        label_normalisers = None\n        if self.net_param.histogram_ref_file and \\\n                task_param.label_normalisation:\n            label_normalisers = [DiscreteLabelNormalisationLayer(\n                image_name=\'label\',\n                modalities=vars(task_param).get(\'label\'),\n                model_filename=self.net_param.histogram_ref_file)]\n            if self.is_evaluation:\n                label_normalisers.append(\n                    DiscreteLabelNormalisationLayer(\n                        image_name=\'inferred\',\n                        modalities=vars(task_param).get(\'inferred\'),\n                        model_filename=self.net_param.histogram_ref_file))\n                label_normalisers[-1].key = label_normalisers[0].key\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n        if task_param.label_normalisation and \\\n                (self.is_training or not task_param.output_prob):\n            normalisation_layers.extend(label_normalisers)\n\n        volume_padding_layer = []\n        if self.net_param.volume_padding_size:\n            volume_padding_layer.append(PadLayer(\n                image_name=SUPPORTED_INPUT,\n                border=self.net_param.volume_padding_size,\n                mode=self.net_param.volume_padding_mode))\n\n        # initialise training data augmentation layers\n        augmentation_layers = []\n        if self.is_training:\n            train_param = self.action_param\n            if train_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=train_param.random_flipping_axes))\n            if train_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=train_param.scaling_percentage[0],\n                    max_percentage=train_param.scaling_percentage[1]))\n            if train_param.rotation_angle or \\\n                    train_param.rotation_angle_x or \\\n                    train_param.rotation_angle_y or \\\n                    train_param.rotation_angle_z:\n                rotation_layer = RandomRotationLayer()\n                if train_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        train_param.rotation_angle)\n                else:\n                    rotation_layer.init_non_uniform_angle(\n                        train_param.rotation_angle_x,\n                        train_param.rotation_angle_y,\n                        train_param.rotation_angle_z)\n                augmentation_layers.append(rotation_layer)\n            if train_param.do_elastic_deformation:\n                spatial_rank = list(self.readers[0].spatial_ranks.values())[0]\n                augmentation_layers.append(RandomElasticDeformationLayer(\n                    spatial_rank=spatial_rank,\n                    num_controlpoints=train_param.num_ctrl_points,\n                    std_deformation_sigma=train_param.deformation_sigma,\n                    proportion_to_augment=train_param.proportion_to_deform))\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n            volume_padding_layer + normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(\n                volume_padding_layer + normalisation_layers)\n\n    def initialise_csvsampler(self):\n        self.sampler = [[CSVPatchSampler(reader=image_reader,\n                                    csv_reader=csv_reader,\n                                    window_sizes=self.data_param,\n                                    batch_size=self.net_param.batch_size,\n                                    windows_per_image=self.action_param.sample_per_volume,\n                                    queue_length=self.net_param.queue_length\n                                    )\n                         for image_reader, csv_reader in\n            zip(self.readers, self.csv_readers)]]\n\n    def initialise_uniform_sampler(self):\n        self.sampler = [[UniformSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_weighted_sampler(self):\n        self.sampler = [[WeightedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_sampler(self):\n        self.sampler = [[GridSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_balanced_sampler(self):\n        self.sampler = [[BalancedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_aggregator(self):\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_resize_aggregator(self):\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        elif self.is_inference:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_aggregator(self):\n        self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=self.segmentation_param.num_classes,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(\n                n_class=self.segmentation_param.num_classes,\n                loss_type=self.action_param.loss_type,\n                softmax=self.segmentation_param.softmax)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None),\n                weight_map=data_dict.get(\'weight\', None))\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n            grads = self.optimiser.compute_gradients(\n                loss, colocate_gradients_with_ops=True)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            # outputs_collector.add_to_collection(\n            #    var=image*180.0, name=\'image\',\n            #    average_over_devices=False, summary_type=\'image3_sagittal\',\n            #    collection=TF_SUMMARIES)\n\n            # outputs_collector.add_to_collection(\n            #    var=image, name=\'image\',\n            #    average_over_devices=False,\n            #    collection=NETWORK_OUTPUT)\n\n            # outputs_collector.add_to_collection(\n            #    var=tf.reduce_mean(image), name=\'mean_image\',\n            #    average_over_devices=False, summary_type=\'scalar\',\n            #    collection=CONSOLE)\n        elif self.is_inference:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            output_prob = self.segmentation_param.output_prob\n            num_classes = self.segmentation_param.num_classes\n            if output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'SOFTMAX\', num_classes=num_classes)\n            elif not output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'ARGMAX\', num_classes=num_classes)\n            else:\n                post_process_layer = PostProcessingLayer(\n                    \'IDENTITY\', num_classes=num_classes)\n            net_out = post_process_layer(net_out)\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        if self.is_inference:\n            return self.output_decoder.decode_batch(\n                batch_output[\'window\'], batch_output[\'location\'])\n        return True\n\n    def initialise_evaluator(self, eval_param):\n        self.eval_param = eval_param\n        self.evaluator = SegmentationEvaluator(self.readers[0],\n                                               self.segmentation_param,\n                                               eval_param)\n\n    def add_inferred_output(self, data_param, task_param):\n        return self.add_inferred_output_like(data_param, task_param, \'label\')\n'"
niftynet/contrib/csv_reader/toynet_features.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.network.base_net import BaseNet\n\n\nclass ToyNetFeat(BaseNet):\n    def __init__(self,\n                 num_classes,\n                 w_initializer=None,\n                 w_regularizer=None,\n                 b_initializer=None,\n                 b_regularizer=None,\n                 acti_func='prelu',\n                 name='ToyNet'):\n\n        super(ToyNetFeat, self).__init__(\n            num_classes=num_classes,\n            w_initializer=w_initializer,\n            w_regularizer=w_regularizer,\n            b_initializer=b_initializer,\n            b_regularizer=b_regularizer,\n            acti_func=acti_func,\n            name=name)\n\n        self.hidden_features = 10\n\n    def layer_op(self, images, is_training=True, **unused_kwargs):\n        conv_1 = ConvolutionalLayer(self.hidden_features,\n                                    kernel_size=3,\n                                    w_initializer=self.initializers['w'],\n                                    w_regularizer=self.regularizers['w'],\n                                    b_initializer=self.initializers['b'],\n                                    b_regularizer=self.regularizers['b'],\n                                    acti_func='relu',\n                                    name='conv_input')\n\n        # conv_2 = ConvolutionalLayer(self.num_classes,\n        #                             kernel_size=1,\n        #                             w_initializer=self.initializers['w'],\n        #                             w_regularizer=self.regularizers['w'],\n        #                             b_initializer=self.initializers['b'],\n        #                             b_regularizer=self.regularizers['b'],\n        #                             acti_func=None,\n        #                             name='conv_output')\n\n        flow = conv_1(images, is_training)\n        # flow = conv_2(flow, is_training)\n        return flow\n"""
niftynet/contrib/deep_boosted_regression/__init__.py,0,b''
niftynet/contrib/deep_boosted_regression/regression_rec_application.py,22,"b'import tensorflow as tf\nimport copy\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import ApplicationNetFactory\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.application_variables import NETWORK_OUTPUT\nfrom niftynet.engine.application_variables import TF_SUMMARIES\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_weighted_v2 import WeightedSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.crop import CropLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_regression import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\n\nSUPPORTED_INPUT = set([\'image\', \'output\', \'weight\', \'sampler\'])\n\n\nclass RegressionRecApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""REGRESSION""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting recursive regression application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.net2_param = copy.deepcopy(net_param)\n        self.action_param = action_param\n        self.regression_param = None\n\n        self.data_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'uniform\': (self.initialise_uniform_sampler,\n                        self.initialise_grid_sampler,\n                        self.initialise_grid_aggregator),\n            \'weighted\': (self.initialise_weighted_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler,\n                       self.initialise_resize_aggregator),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        self.data_param = data_param\n        self.regression_param = task_param\n\n        # read each line of csv files into an instance of Subject\n        if self.is_training:\n            file_lists = []\n            if self.action_param.validation_every_n > 0:\n                file_lists.append(data_partitioner.train_files)\n                file_lists.append(data_partitioner.validation_files)\n            else:\n                file_lists.append(data_partitioner.train_files)\n\n            self.readers = []\n            for file_list in file_lists:\n                reader = ImageReader(SUPPORTED_INPUT)\n                reader.initialise(data_param, task_param, file_list)\n                self.readers.append(reader)\n        else:\n            inference_reader = ImageReader([\'image\'])\n            file_list = data_partitioner.inference_files\n            inference_reader.initialise(data_param, task_param, file_list)\n            self.readers = [inference_reader]\n\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\')\n        histogram_normaliser = None\n        if self.net_param.histogram_ref_file:\n            histogram_normaliser = HistogramNormalisationLayer(\n                image_name=\'image\',\n                modalities=vars(task_param).get(\'image\'),\n                model_filename=self.net_param.histogram_ref_file,\n                norm_type=self.net_param.norm_type,\n                cutoff=self.net_param.cutoff,\n                name=\'hist_norm_layer\')\n\n        normalisation_layers = []\n        if self.net_param.normalisation:\n            normalisation_layers.append(histogram_normaliser)\n        if self.net_param.whitening:\n            normalisation_layers.append(mean_var_normaliser)\n\n        augmentation_layers = []\n        if self.is_training:\n            if self.action_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=self.action_param.random_flipping_axes))\n            if self.action_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=self.action_param.scaling_percentage[0],\n                    max_percentage=self.action_param.scaling_percentage[1]))\n            if self.action_param.rotation_angle:\n                augmentation_layers.append(RandomRotationLayer())\n                augmentation_layers[-1].init_uniform_angle(\n                    self.action_param.rotation_angle)\n\n        volume_padding_layer = []\n        if self.net_param.volume_padding_size:\n            volume_padding_layer.append(PadLayer(\n                image_name=SUPPORTED_INPUT,\n                border=self.net_param.volume_padding_size))\n        for reader in self.readers:\n            reader.add_preprocessing_layers(volume_padding_layer +\n                                            normalisation_layers +\n                                            augmentation_layers)\n\n    def initialise_uniform_sampler(self):\n        self.sampler = [[UniformSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_weighted_sampler(self):\n        self.sampler = [[WeightedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_sampler(self):\n        self.sampler = [[GridSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_balanced_sampler(self):\n        self.sampler = [[BalancedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_aggregator(self):\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order)\n\n    def initialise_resize_aggregator(self):\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        else:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=1,\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n        self.net2 = ApplicationNetFactory.create(self.net2_param.name)(\n            num_classes=1,\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net2_param.activation_function)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(True),\n                                    lambda: switch_sampler(False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            pct1_out = self.net(image, self.is_training)\n            res2_out = self.net2(tf.concat([image, pct1_out],4), self.is_training)\n            pct2_out = tf.add(pct1_out,res2_out)\n            res3_out = self.net2(tf.concat([image, pct2_out],4), self.is_training)\n            pct3_out = tf.add(pct2_out,res3_out)\n            #res4_out = self.net2(tf.concat([image, pct3_out],4), self.is_training)\n            #pct4_out = tf.add(pct3_out,res4_out)\n\t    #net_out = self.net(image, is_training=self.is_training)\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(\n                loss_type=self.action_param.loss_type)\n\n            crop_layer = CropLayer(\n                border=self.regression_param.loss_border, name=\'crop-88\')\n\n            data_loss1 = loss_func(\n                prediction=crop_layer(pct1_out),\n                ground_truth=crop_layer(data_dict.get(\'output\', None)),\n                weight_map=None if data_dict.get(\'weight\', None) is None else crop_layer(data_dict.get(\'weight\', None)))\n            data_loss2 = loss_func(\n                prediction=crop_layer(pct2_out),\n                ground_truth=crop_layer(data_dict.get(\'output\', None)),\n                weight_map=None if data_dict.get(\'weight\', None) is None else crop_layer(data_dict.get(\'weight\', None)))\n            data_loss3 = loss_func(\n                prediction=crop_layer(pct3_out),\n                ground_truth=crop_layer(data_dict.get(\'output\', None)),\n                weight_map=None if data_dict.get(\'weight\', None) is None else crop_layer(data_dict.get(\'weight\', None)))\n\n            #prediction = crop_layer(net_out)\n            #ground_truth = crop_layer(data_dict.get(\'output\', None))\n            #weight_map = None if data_dict.get(\'weight\', None) is None \\\n                #else crop_layer(data_dict.get(\'weight\', None))\n            #data_loss = loss_func(prediction=prediction,\n                                  #ground_truth=ground_truth,\n                                  #weight_map=weight_map)\n\n            reg_losses = tf.get_collection(\n                tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = reg_loss + data_loss1 + data_loss2 + data_loss3\n            else:\n                loss = data_loss1 + data_loss2 + data_loss3\n            grads = self.optimiser.compute_gradients(loss)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=loss, name=\'Loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss1, name=\'data_loss1\',\n                average_over_devices=True,\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss2, name=\'data_loss2\',\n                average_over_devices=True,\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss3, name=\'data_loss3\',\n                average_over_devices=True,\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss1, name=\'data_loss1\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=data_loss2, name=\'data_loss2\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=data_loss3, name=\'data_loss3\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=loss, name=\'LossSum\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n#            outputs_collector.add_to_collection(\n#                var=pct3_out, name=""pct3_out"",\n#                average_over_devices=True, summary_type=""image3_axial"",\n#                collection=TF_SUMMARIES)\n\n        else:\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            #net_out = self.net(image, is_training=self.is_training)\n            pct1_out = self.net(image, self.is_training)\n            res2_out = self.net2(tf.concat([image, pct1_out],4), self.is_training)\n            pct2_out = tf.add(pct1_out,res2_out)\n            res3_out = self.net2(tf.concat([image, pct2_out],4), self.is_training)\n            pct3_out = tf.add(pct2_out,res3_out)\n            res4_out = self.net2(tf.concat([image, pct3_out],4), self.is_training)\n            pct4_out = tf.add(pct3_out,res4_out)\n            crop_layer = CropLayer(border=0, name=\'crop-88\')\n            post_process_layer = PostProcessingLayer(\'IDENTITY\')\n            net_out = post_process_layer(crop_layer(pct4_out))\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            init_aggregator = \\\n                self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]\n            init_aggregator()\n\n    def interpret_output(self, batch_output):\n        if not self.is_training:\n            return self.output_decoder.decode_batch(\n                {\'window_image\': batch_output[\'window\']},\n                batch_output[\'location\'])\n        else:\n            return True\n'"
niftynet/contrib/evaluation/__init__.py,0,b''
niftynet/contrib/evaluation/classification_evaluations.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nThis module defines built-in evaluation functions for classification \napplications\n\nMany classification metrics only make sense computed over all subjects,\nso aggregation is used.\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport pandas as pd\n\nfrom niftynet.evaluation.base_evaluations import BaseEvaluation\nfrom niftynet.evaluation.base_evaluator import ScalarAggregator,\\\n                                               DataFrameAggregator\n\n# These implement ROC based on preselected threshold levels, rather than actual \n# data. They are left here as an illustration for now \nclass roc(BaseEvaluation):\n    def layer_op(self, subject_id, data):\n        if not self.app_param.output_prob or\\\n           self.app_param.num_classes>2:\n           return []\n        pdf = pd.DataFrame.from_records([{\'subject_id\':subject_id,\n                            \'roc_i\':data[\'inferred\'][0,0,0,0,1],\n                            \'roc_l\':data[\'label\'][0,0,0,0,0]}],(\'subject_id\',))\n        return [pdf]\n\n    @classmethod\n    def aggregate(cls, df):\n        thresholds = np.linspace(0,1,10)\n        df_out = pd.DataFrame(index=range(len(thresholds)),columns=(\'thresholds\',\'tp\',\'fp\',\'tn\',\'fn\',\'acc\',\'sens\',\'spec\'))\n        df_out.thresholds = thresholds\n        for it in range(len(thresholds)):\n            df_out.loc[it,\'tp\'] = (df[df.roc_l==1].roc_i>thresholds[it]).sum()\n            df_out.loc[it,\'fp\'] = (df[df.roc_l==0].roc_i>thresholds[it]).sum()\n            df_out.loc[it,\'tn\'] = (df[df.roc_l==0].roc_i<=thresholds[it]).sum()\n            df_out.loc[it,\'fn\'] = (df[df.roc_l==1].roc_i<=thresholds[it]).sum()\n        df_out.acc = (df_out.tp+df_out.tn)/(\n            df_out.tp+df_out.tn+df_out.fp+df_out.fn)\n        denom = df_out.tp+df_out.fn\n        df_out.loc[denom>0,\'sens\'] = df_out.loc[denom>0,\'tp\']/denom[denom>0]\n        denom = df_out.tn+df_out.fp\n        df_out.loc[denom>0,\'spec\'] = df_out.loc[denom>0,\'tn\']/denom[denom>0]\n        df_out=df_out.set_index(\'thresholds\')\n        return [df_out]\n\n    def get_aggregations(self):\n        if not self.app_param.output_prob or\\\n           self.app_param.num_classes>2:\n           return []\n        return [DataFrameAggregator((\'subject_id\',), self.aggregate)]\n\nclass roc_auc(BaseEvaluation):\n    def layer_op(self, subject_id, data):\n        if not self.app_param.output_prob or\\\n           self.app_param.num_classes>2:\n           return []\n        pdf = pd.DataFrame.from_records([{\'subject_id\':subject_id,\n                            \'roc_auc_i\':data[\'inferred\'][0,0,0,0,1],\n                            \'roc_auc_l\':data[\'label\'][0,0,0,0,0]}],(\'subject_id\',))\n        return [pdf]\n\n    @classmethod\n    def aggregate(cls, df):\n        thresholds = np.linspace(1,0,10)\n        df_out = pd.DataFrame(index=range(len(thresholds)),columns=(\'thresholds\',\'tp\',\'fp\',\'tn\',\'fn\',\'acc\',\'sens\',\'spec\'))\n        df_out.thresholds = thresholds\n        for it in range(len(thresholds)):\n            df_out.loc[it,\'tp\'] = (df[df.roc_auc_l==1].roc_auc_i>thresholds[it]).sum()\n            df_out.loc[it,\'fp\'] = (df[df.roc_auc_l==0].roc_auc_i>thresholds[it]).sum()\n            df_out.loc[it,\'tn\'] = (df[df.roc_auc_l==0].roc_auc_i<=thresholds[it]).sum()\n            df_out.loc[it,\'fn\'] = (df[df.roc_auc_l==1].roc_auc_i<=thresholds[it]).sum()\n        df_out.acc = (df_out.tp+df_out.tn)/(\n            df_out.tp+df_out.tn+df_out.fp+df_out.fn)\n        denom = df_out.tp+df_out.fn\n        df_out.loc[denom>0,\'sens\'] = df_out.loc[denom>0,\'tp\']/denom[denom>0]\n        denom = df_out.tn+df_out.fp\n        df_out.loc[denom>0,\'spec\'] = df_out.loc[denom>0,\'tn\']/denom[denom>0]\n        by_threshold=df_out.set_index(\'thresholds\')\n\n        tpr = np.array(list(by_threshold.sens))\n        fpr = np.array(1-by_threshold.spec)\n        roc_auc = np.sum((tpr[:-1] + tpr[1:]) * (fpr[1:] - fpr[:-1]) / 2)\n        return [pd.DataFrame.from_records([{\'roc_auc\':roc_auc}])]\n\n\n    def get_aggregations(self):\n        if not self.app_param.output_prob or\\\n           self.app_param.num_classes>2:\n           return []\n        return [DataFrameAggregator((\'subject_id\',), self.aggregate)]\n'"
niftynet/contrib/evaluation/regression_evaluations.py,0,b''
niftynet/contrib/evaluation/segmentation_evaluations.py,0,"b'""""""\r\nThis module holds built-in segmentation evaluations without tests\r\n""""""\r\n\r\nimport os\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom scipy import ndimage\r\n\r\nfrom niftynet.evaluation.base_evaluations import BaseEvaluation\r\nfrom niftynet.evaluation.segmentation_evaluations import \\\r\n    PerComponentEvaluation, connected_components, cached_label_binarizer, \\\r\n    union_of_seg_for_each_ref_cc\r\nfrom niftynet.io.misc_io import save_data_array\r\n\r\n\r\nclass com_ref(PerComponentEvaluation):\r\n    """"""\r\n    Computes the centers of mass of each component in the reference standard\r\n    """"""\r\n\r\n    def metric_from_binarized(self, seg, ref):\r\n        """"""\r\n        :param seg: numpy array with binary mask from inferred segmentation\r\n        :param ref: numpy array with binary mask from reference segmentation\r\n        :return: dict of centers of mass in each axis\r\n        """"""\r\n        return {d: \'com_ref_\' + x\r\n                for d, x in zip(\'XYZ\', ndimage.center_of_mass(ref))}\r\n\r\n\r\nclass ErrorMapsCC(BaseEvaluation):\r\n    """"""\r\n    Create 3 maps of connected component detection:\r\n    tpc_map shows each detected ref cc (having at least on seg cc that\r\n    overlaps) and the union of all overlapping seg ccs\r\n    fnc_map shows all ref ccs that were not detected\r\n    fpc_map shows all seg ccs that did not overlap any ref ccs\r\n    Note we currently arbitrarily limit image generation to binary problems\r\n    """"""\r\n\r\n    def layer_op(self, subject_id, data):\r\n        analyses = self.app_param.evaluation_units.split(\',\')\r\n        if \'label\' not in analyses and \'foreground\' not in analyses:\r\n            raise ValueError(\'ErrorMaps work with label or foreground \'\r\n                             \'analyses only\')\r\n        if self.app_param.num_classes > 2:\r\n            raise ValueError(\'ErrorMaps work with binary segmentations only\')\r\n\r\n        binarizer = cached_label_binarizer(1, self.app_param.output_prob)\r\n        seg, ref = binarizer(data)\r\n        cc_func = connected_components\r\n        cc_seg, cc_ref = cc_func(seg, ref, self.app_param.output_prob)\r\n\r\n        cc_aggregator = union_of_seg_for_each_ref_cc\r\n        ccs = cc_aggregator(cc_ref, cc_seg)\r\n\r\n        tp_seg_labels = set(s for seg_l, ref_l in ccs for s in seg_l)\r\n        tp_ref_labels = set(r for seg_l, ref_l in ccs for r in ref_l if len(\r\n            seg_l))\r\n        fn_ref_labels = set(range(1, cc_ref[1])) - tp_ref_labels\r\n        fp_seg_labels = set(range(1, cc_seg[1])) - tp_seg_labels\r\n        maps = {}\r\n        maps[\'tpc_map\'] = np.logical_or(cc_seg in tp_seg_labels,\r\n                                        cc_ref in tp_ref_labels)\r\n        maps[\'fnc_map\'] = cc_ref in fn_ref_labels\r\n        maps[\'fpc_map\'] = cc_seg in fp_seg_labels\r\n\r\n        image_idx = self.reader.get_image_index(subject_id)\r\n        file_path = os.path.join(self.eval_param.save_csv_dir, \'images\')\r\n        out = {\'subject_id\': subject_id}\r\n        for key in maps:\r\n            out[key] = os.path.join(file_path, subject_id + \'_\' + key + \'.nii\')\r\n            save_data_array(file_path,\r\n                            subject_id + \'_\' + key + \'.nii\',\r\n                            maps[key],\r\n                            self.reader.output_list[image_idx][\'label\'], 0)\r\n        pdf = pd.DataFrame.from_records([out], (\'subject_id\',))\r\n        return [pdf]\r\n'"
niftynet/contrib/layer/__init__.py,0,b''
niftynet/contrib/layer/resampler_optional_niftyreg.py,1,"b""# Flag stating whether C++/CUDA image resampling is available\nHAS_NIFTYREG_RESAMPLING = False\n\ntry:\n    from niftyreg_image_resampling import NiftyregImageResamplingLayer\n    import niftyreg_image_resampling as resampler_module\n\n    ResamplerOptionalNiftyRegLayer = NiftyregImageResamplingLayer\n\n    HAS_NIFTYREG_RESAMPLING = True\nexcept ImportError:\n    import tensorflow as tf\n\n    tf.logging.warning('''\n    niftyreg_image_resampling is not installed; falling back onto\n    niftynet.layer.resampler.ResamplerLayer. To install\n    niftyreg_image_resampling please see\n    niftynet/contrib/niftyreg_image_resampling/README.md\n    ''')\n\n    from niftynet.layer.resampler import ResamplerLayer\n    import niftynet.layer.resampler as resampler_module\n\n    ResamplerOptionalNiftyRegLayer = ResamplerLayer\n\n\n# Passthrough of supported boundary types\nSUPPORTED_BOUNDARY = resampler_module.SUPPORTED_BOUNDARY\n\n\n# Passthrough of supported interpolation types\nSUPPORTED_INTERPOLATION = resampler_module.SUPPORTED_INTERPOLATION\n"""
niftynet/contrib/learning_rate_schedule/__init__.py,0,b''
niftynet/contrib/learning_rate_schedule/decay_lr_application.py,7,"b'import tensorflow as tf\r\n\r\nfrom niftynet.application.segmentation_application import \\\r\n    SegmentationApplication\r\nfrom niftynet.engine.application_factory import OptimiserFactory\r\nfrom niftynet.engine.application_variables import CONSOLE\r\nfrom niftynet.engine.application_variables import TF_SUMMARIES\r\nfrom niftynet.layer.loss_segmentation import LossFunction\r\n\r\nSUPPORTED_INPUT = set([\'image\', \'label\', \'weight\'])\r\n\r\n\r\nclass DecayLearningRateApplication(SegmentationApplication):\r\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\r\n\r\n    def __init__(self, net_param, action_param, is_training):\r\n        SegmentationApplication.__init__(\r\n            self, net_param, action_param, is_training)\r\n        tf.logging.info(\'starting decay learning segmentation application\')\r\n        self.learning_rate = None\r\n        self.current_lr = action_param.lr\r\n        if self.action_param.validation_every_n > 0:\r\n            raise NotImplementedError(""validation process is not implemented ""\r\n                                      ""in this demo."")\r\n\r\n    def connect_data_and_network(self,\r\n                                 outputs_collector=None,\r\n                                 gradients_collector=None):\r\n        data_dict = self.get_sampler()[0][0].pop_batch_op()\r\n        image = tf.cast(data_dict[\'image\'], tf.float32)\r\n        net_out = self.net(image, self.is_training)\r\n\r\n        if self.is_training:\r\n            with tf.name_scope(\'Optimiser\'):\r\n                self.learning_rate = tf.placeholder(tf.float32, shape=[])\r\n                optimiser_class = OptimiserFactory.create(\r\n                    name=self.action_param.optimiser)\r\n                self.optimiser = optimiser_class.get_instance(\r\n                    learning_rate=self.learning_rate)\r\n            loss_func = LossFunction(\r\n                n_class=self.segmentation_param.num_classes,\r\n                loss_type=self.action_param.loss_type)\r\n            data_loss = loss_func(\r\n                prediction=net_out,\r\n                ground_truth=data_dict.get(\'label\', None),\r\n                weight_map=data_dict.get(\'weight\', None))\r\n\r\n            loss = data_loss\r\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\n\r\n            if self.net_param.decay > 0.0 and reg_losses:\r\n                reg_loss = tf.reduce_mean(\r\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\r\n                loss = data_loss + reg_loss\r\n            grads = self.optimiser.compute_gradients(loss)\r\n            # collecting gradients variables\r\n            gradients_collector.add_to_collection([grads])\r\n            # collecting output variables\r\n            outputs_collector.add_to_collection(\r\n                var=data_loss, name=\'dice_loss\',\r\n                average_over_devices=False, collection=CONSOLE)\r\n            outputs_collector.add_to_collection(\r\n                var=self.learning_rate, name=\'lr\',\r\n                average_over_devices=False, collection=CONSOLE)\r\n            outputs_collector.add_to_collection(\r\n                var=data_loss, name=\'dice_loss\',\r\n                average_over_devices=True, summary_type=\'scalar\',\r\n                collection=TF_SUMMARIES)\r\n        else:\r\n            # converting logits into final output for\r\n            # classification probabilities or argmax classification labels\r\n            SegmentationApplication.connect_data_and_network(\r\n                self, outputs_collector, gradients_collector)\r\n\r\n    def set_iteration_update(self, iteration_message):\r\n        """"""\r\n        This function will be called by the application engine at each\r\n        iteration.\r\n        """"""\r\n        current_iter = iteration_message.current_iter\r\n        if iteration_message.is_training:\r\n            if current_iter > 0 and current_iter % 3 == 0:\r\n                self.current_lr = self.current_lr / 2.0\r\n            iteration_message.data_feed_dict[self.is_validation] = False\r\n        elif iteration_message.is_validation:\r\n            iteration_message.data_feed_dict[self.is_validation] = True\r\n        iteration_message.data_feed_dict[self.learning_rate] = self.current_lr\r\n'"
niftynet/contrib/multi_output/__init__.py,0,b''
niftynet/contrib/multi_output/multi_output_test.py,17,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_weighted_v2 import WeightedSampler\nfrom niftynet.engine.sampler_balanced_v2 import BalancedSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.engine.windows_aggregator_identity import WindowAsImageAggregator\n# from niftynet.engine.windows_aggregator_classifier import ClassifierSamplesAggregator\nfrom niftynet.layer.loss_segmentation import LossFunction\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.engine.application_variables import CONSOLE, TF_SUMMARIES, NETWORK_OUTPUT\n\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'weight\', \'sampler\', \'inferred\'])\n\n\nclass MultiOutputApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting multioutput test\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.multioutput_param = None\n\n        self.SUPPORTED_SAMPLING = {\n            \'uniform\': (self.initialise_uniform_sampler,\n                        self.initialise_grid_sampler,\n                        self.initialise_grid_aggregator),\n            \'weighted\': (self.initialise_weighted_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler,\n                       self.initialise_resize_aggregator),\n            \'classifier\': (self.initialise_resize_sampler,\n                           self.initialise_resize_sampler,\n                           self.initialise_classifier_aggregator),\n            \'identity\': (self.initialise_uniform_sampler,\n                         self.initialise_resize_sampler,\n                         self.initialise_identity_aggregator)\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.multioutput_param = task_param\n\n        # initialise input image readers\n        if self.is_training:\n            reader_names = (\'image\', \'label\', \'weight\', \'sampler\')\n        elif self.is_inference:\n            # in the inference process use `image` input only\n            reader_names = (\'image\',)\n        elif self.is_evaluation:\n            reader_names = (\'image\', \'label\', \'inferred\')\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n    def initialise_uniform_sampler(self):\n        self.sampler = [[UniformSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_weighted_sampler(self):\n        self.sampler = [[WeightedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_sampler(self):\n        self.sampler = [[GridSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_balanced_sampler(self):\n        self.sampler = [[BalancedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_aggregator(self):\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix,\n            fill_constant=self.action_param.fill_constant)\n\n    def initialise_resize_aggregator(self):\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_identity_aggregator(self):\n        self.output_decoder = WindowAsImageAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_classifier_aggregator(self):\n        pass\n        # self.output_decoder = ClassifierSamplesAggregator(\n        #     image_reader=self.readers[0],\n        #     output_path=self.action_param.save_seg_dir,\n        #     postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        elif self.is_inference:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_aggregator(self):\n        self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(\'toynet\')(\n            num_classes=self.multioutput_param.num_classes,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            # extract data\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n\n            loss_func = LossFunction(\n                n_class=self.multioutput_param.num_classes,\n                loss_type=self.action_param.loss_type,\n                softmax=self.multioutput_param.softmax)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None),\n                weight_map=data_dict.get(\'weight\', None))\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n\n            # set the optimiser and the gradient\n            to_optimise = tf.trainable_variables()\n            vars_to_freeze = \\\n                self.action_param.vars_to_freeze or \\\n                self.action_param.vars_to_restore\n            if vars_to_freeze:\n                import re\n                var_regex = re.compile(vars_to_freeze)\n                # Only optimise vars that are not frozen\n                to_optimise = \\\n                    [v for v in to_optimise if not var_regex.search(v.name)]\n                tf.logging.info(\n                    ""Optimizing %d out of %d trainable variables, ""\n                    ""the other variables fixed (--vars_to_freeze %s)"",\n                    len(to_optimise),\n                    len(tf.trainable_variables()),\n                    vars_to_freeze)\n\n            grads = self.optimiser.compute_gradients(\n                loss, var_list=to_optimise, colocate_gradients_with_ops=True)\n\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n        elif self.is_inference:\n\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            num_classes = self.multioutput_param.num_classes\n            argmax_layer =  PostProcessingLayer(\n                    \'ARGMAX\', num_classes=num_classes)\n            softmax_layer = PostProcessingLayer(\n                    \'SOFTMAX\', num_classes=num_classes)\n\n            arg_max_out = argmax_layer(net_out)\n            soft_max_out = softmax_layer(net_out)\n            # sum_prob_out = tf.reshape(tf.reduce_sum(soft_max_out),[1,1])\n            # min_prob_out = tf.reshape(tf.reduce_min(soft_max_out),[1,1])\n            sum_prob_out = tf.reduce_sum(soft_max_out)\n            min_prob_out = tf.reduce_min(soft_max_out)\n\n            outputs_collector.add_to_collection(\n                var=arg_max_out, name=\'window_argmax\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=soft_max_out, name=\'window_softmax\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=sum_prob_out, name=\'csv_sum\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=min_prob_out, name=\'csv_min\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        if self.is_inference:\n            return self.output_decoder.decode_batch(\n                {\'window_argmax\': batch_output[\'window_argmax\'],\n                 \'window_softmax\': batch_output[\'window_softmax\'],\n                 \'csv_sum\': batch_output[\'csv_sum\'],\n                 \'csv_min\': batch_output[\'csv_min\']},\n                batch_output[\'location\'])\n        return True\n\n\n'"
niftynet/contrib/niftyreg_image_resampling/__init__.py,0,b''
niftynet/contrib/niftyreg_image_resampling/niftyreg_image_resampling.py,10,"b'from __future__ import print_function, division\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.layer_util import infer_spatial_rank\n\nfrom niftyreg_module_loader import get_niftyreg_module\n\n\n# NiftyNet boundary types to NiftyReg code mapping\n__BOUNDARY_CODES__ = {\n    \'ZERO\': 0,\n    \'NAN\': 1,\n    \'REPLICATE\': 2,\n    \'SYMMETRIC\': 3\n}\n\n\n# Exposure of supported boundary types for compat. w/ ResamplerLayer\nSUPPORTED_BOUNDARY = {k for k in __BOUNDARY_CODES__}\n\n\n# NiftyNet interpolation types to NiftyReg code mapping\n__INTERPOLATION_CODES__ = {\'NEAREST\': 0,\n                           \'LINEAR\': 1,\n                           \'BSPLINE\': 3}\n\n\n# Exposure of supported interpolation types for compat. w/ ResamplerLayer\nSUPPORTED_INTERPOLATION = {k for k in __INTERPOLATION_CODES__}\n\n\n# NiftyReg expects displacement components to be\n# indexed w/ slowest index\ndef _transpose(data):\n    nof_dims = len(data.shape) - 1\n    perm = [0] + list(range(nof_dims, 0, -1))\n    perm += list(range(nof_dims + 1, len(data.shape)))\n    assert len(perm) == len(data.shape)\n\n    return tf.transpose(data, perm)\n\n\n@ops.RegisterGradient(""NiftyregImageResampling"")\ndef _niftyreg_resampling_grad(op, grad):\n    grad_op = get_niftyreg_module().niftyreg_image_resampling_gradient(\n        op.inputs[0],\n        op.inputs[1],\n        interpolation=op.get_attr(\'interpolation\'),\n        boundary=op.get_attr(\'boundary\'))\n\n    chained_grad = None\n\n    nof_modalities = op.inputs[0].shape.as_list()[1]\n    if not nof_modalities is None and nof_modalities != 1:\n        nof_dims = op.inputs[1].shape.as_list()[1]\n\n        assert grad_op.shape.as_list()[1] == nof_modalities*nof_dims\n\n        chained_grads = []\n        for m in range(nof_modalities):\n            mod_grad = grad_op[:,(nof_dims*m):((m+1)*nof_dims),...]\n            out_mod_grad = tf.expand_dims(grad[:,m,...], axis=1)\n\n            out_grad = tf.tile(out_mod_grad, [1] + [nof_dims]\n                               + [1]*(len(grad_op.shape) - 2))\n\n            chained_grads.append(tf.multiply(mod_grad, out_grad))\n\n        chained_grad = tf.reduce_sum(tf.stack(chained_grads, axis=0), axis=0)\n\n    else:\n        grad_rep = tf.tile(grad, [1] + [grad_op.shape[1]]\n                           + [1]*(len(grad_op.shape) - 2))\n        chained_grad = tf.multiply(grad_rep, grad_op)\n\n    image_grad_op \\\n        = get_niftyreg_module().niftyreg_image_resampling_image_gradient(\n            op.inputs[0],\n            op.inputs[1],\n            grad,\n            interpolation=op.get_attr(\'interpolation\'),\n            boundary=op.get_attr(\'boundary\'))\n\n    return [image_grad_op, chained_grad]\n\n\nclass NiftyregImageResamplingLayer(Layer):\n    def __init__(self, interpolation, boundary=\'ZERO\', **kwargs):\n        super(NiftyregImageResamplingLayer, self).__init__(**kwargs)\n\n        self._interpolation = __INTERPOLATION_CODES__[interpolation.upper()]\n        self._boundary = boundary.upper()\n\n    def layer_op(self, inputs, deformation, **kwargs):\n        nof_dims = infer_spatial_rank(inputs)\n        nof_output_dims = infer_spatial_rank(deformation)\n\n        batch_size = inputs.shape.as_list()[0]\n        if deformation.shape.as_list()[0] != batch_size:\n            deformation = tf.tile(deformation,\n                                  [batch_size] + [1]*(nof_output_dims + 1))\n\n        output_spatial_dims = deformation.shape.as_list()[1:-1]\n        input_dims = [d if d else -1 for d in inputs.shape.as_list()]\n        if len(output_spatial_dims) != nof_dims:\n            resample_def = deformation\n            while len(resample_def.shape) < len(inputs.shape):\n                resample_def = tf.expand_dims(resample_def,\n                                              axis=len(resample_def.shape) - 2)\n        else:\n            resample_def = deformation\n        assert infer_spatial_rank(resample_def) == nof_dims\n\n        resampled = get_niftyreg_module().niftyreg_image_resampling(\n            _transpose(inputs),\n            _transpose(resample_def),\n            interpolation=self._interpolation,\n            boundary=__BOUNDARY_CODES__[self._boundary])\n\n        return tf.reshape(\n            _transpose(resampled),\n            [batch_size] + output_spatial_dims + [input_dims[-1]])\n\n'"
niftynet/contrib/niftyreg_image_resampling/setup.py,0,"b""from __future__ import print_function\n\nimport os\nimport os.path as osp\nimport platform\nfrom setuptools import setup, Extension, Command\nfrom setuptools.command.build_ext import build_ext\nfrom shutil import which\nimport subprocess as sp\nimport sys\n\n__CMAKE_OVERRIDE_FLAGS__ = {}\n\n\nclass CMakeExtension(Extension):\n    def __init__(self, name):\n        super(CMakeExtension, self).__init__(name, sources=[])\n\n\nclass CMakeOverride(Command):\n    description = 'Overrides CMake variables for build'\n\n    user_options = [('settings=', 's',\n                     'CMake variable override: <KEY>:<VALUE>:<KEY>:<VALUE>...')]\n\n    def initialize_options(self):\n        self.settings = ''\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        global __CMAKE_OVERRIDE_FLAGS__\n\n        overrides = self.settings.split(':')\n        for i in range(0, len(overrides), 2):\n            print('Overriding %s with %s' % (overrides[i], overrides[i+1]))\n            __CMAKE_OVERRIDE_FLAGS__[overrides[i]] = overrides[i+1]\n\n\nclass CMakeBuildExt(build_ext):\n    def run(self):\n        for ext in self.extensions:\n            self.build_extension(ext)\n\n    def build_extension(self, ext):\n        print('Building ' + ext.name)\n\n        outdir = osp.abspath(osp.dirname(self.get_ext_fullpath(ext.name)))\n        args = ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=' + outdir]\n        if not osp.isdir(outdir):\n            os.makedirs(outdir)\n        args += ['-DGPU_RESAMPLING_CONFIGFILE_DIR=' + outdir]\n        args += ['-DCMAKE_BUILD_TYPE=' + ('Debug' if self.debug else 'Release')]\n\n        if platform.system() == 'Linux' \\\n           and any(dist in platform.dist() for dist in ('Debian', 'Ubuntu')):\n            # Need to find compilers that play nice with nvcc;\n            # this assumes compatible versions have been linked to\n            # /PATH/TO/cuda/bin/cc and /PATH/TO/cuda/bin/c++, and\n            # that they appear first on the search path.\n            if not 'CMAKE_C_COMPILER' in __CMAKE_OVERRIDE_FLAGS__:\n                args += ['-DCMAKE_C_COMPILER=' + which('cc')]\n            if not 'CMAKE_CXX_COMPILER' in __CMAKE_OVERRIDE_FLAGS__:\n                args += ['-DCMAKE_CXX_COMPILER=' + which('c++')]\n\n        for key, val in __CMAKE_OVERRIDE_FLAGS__.items():\n            args += ['-D' + key + '=' + val]\n\n        args += [osp.join(osp.dirname(osp.abspath(__file__)),\n                          'niftyreg_gpu_resampler')]\n\n        if not osp.isdir(self.build_temp):\n            os.makedirs(self.build_temp)\n\n        print('Building in ' + str(self.build_temp)\n              + ': cmake ' + ' '.join(args))\n        sp.call(['cmake'] + args, cwd=self.build_temp)\n        sp.call(['cmake'] + args, cwd=self.build_temp)\n        sp.call(['cmake', '--build', self.build_temp])\n\n\nsetup(\n    name='niftyreg_gpu_resampler',\n    description='A NiftyNet image resampling sub-module powered by NiftyReg '\n    'GPU code.',\n    packages=['.'],\n    ext_modules=[CMakeExtension('niftyreg_gpu_resampler')],\n    cmdclass={'override': CMakeOverride,\n              'build_ext': CMakeBuildExt},\n    zip_safe=False,\n)\n"""
niftynet/contrib/preprocessors/__init__.py,0,b''
niftynet/contrib/preprocessors/preprocessing.py,0,"b'from niftynet.layer.mean_variance_normalisation import MeanVarNormalisationLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.histogram_normalisation import HistogramNormalisationLayer\n\n\nclass Preprocessing:\n    """"""\n    This class returns normalisation and augmentation layers for use with reader objects.\n    """"""\n    def __init__(self, net_param, action_param, task_param):\n        self.net_param = net_param\n        self.action_param = action_param\n        self.task_param = task_param\n\n    def prepare_normalisation_layers(self):\n        """"""\n        returns list of normalisation layers\n        """"""\n        foreground_masking_layer = None\n        if self.net_param.normalise_foreground_only:\n            foreground_masking_layer = BinaryMaskingLayer(\n                type_str=self.net_param.foreground_type,\n                multimod_fusion=self.net_param.multimod_foreground_type,\n                threshold=0.0)\n\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\',\n            binary_masking_func=foreground_masking_layer)\n        histogram_normaliser = None\n        if self.net_param.histogram_ref_file:\n            histogram_normaliser = HistogramNormalisationLayer(\n                image_name=\'image\',\n                modalities=vars(self.task_param).get(\'image\'),\n                model_filename=self.net_param.histogram_ref_file,\n                binary_masking_func=foreground_masking_layer,\n                norm_type=self.net_param.norm_type,\n                cutoff=self.net_param.cutoff,\n                name=\'hist_norm_layer\')\n        normalisation_layers = []\n        if self.net_param.normalisation:\n            normalisation_layers.append(histogram_normaliser)\n        if self.net_param.whitening:\n            normalisation_layers.append(mean_var_normaliser)\n        return normalisation_layers\n\n    def prepare_augmentation_layers(self):\n        """"""\n        returns list of augmentation layers\n        """"""\n        augmentation_layers = []\n        if self.action_param.random_flipping_axes != -1:\n            augmentation_layers.append(RandomFlipLayer(\n                flip_axes=self.action_param.random_flipping_axes))\n        if self.action_param.scaling_percentage:\n            augmentation_layers.append(RandomSpatialScalingLayer(\n                min_percentage=self.action_param.scaling_percentage[0],\n                max_percentage=self.action_param.scaling_percentage[1],\n                antialiasing=self.action_param.antialiasing,\n                isotropic=self.action_param.isotropic_scaling))\n        if self.action_param.rotation_angle or \\\n                self.action_param.rotation_angle_x or \\\n                self.action_param.rotation_angle_y or \\\n                self.action_param.rotation_angle_z:\n            rotation_layer = RandomRotationLayer()\n            if self.action_param.rotation_angle:\n                rotation_layer.init_uniform_angle(\n                    self.action_param.rotation_angle)\n            else:\n                rotation_layer.init_non_uniform_angle(\n                    self.action_param.rotation_angle_x,\n                    self.action_param.rotation_angle_y,\n                    self.action_param.rotation_angle_z)\n            augmentation_layers.append(rotation_layer)\n        return augmentation_layers\n'"
niftynet/contrib/regression_weighted_sampler/__init__.py,0,b''
niftynet/contrib/regression_weighted_sampler/isample_regression.py,4,"b""import os\n\nimport tensorflow as tf\n\nfrom niftynet.application.regression_application import \\\n    RegressionApplication, SUPPORTED_INPUT\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_weighted_v2 import WeightedSampler\nfrom niftynet.engine.application_variables import NETWORK_OUTPUT\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\n\n\nclass ISampleRegression(RegressionApplication):\n\n    #def initialise_weighted_sampler(self):\n    #    if len(self.readers) == 2:\n    #        training_sampler = WeightedSampler(\n    #            reader=self.readers[0],\n    #            window_sizes=self.data_param,\n    #            batch_size=self.net_param.batch_size,\n    #            windows_per_image=self.action_param.sample_per_volume,\n    #            queue_length=self.net_param.queue_length)\n    #        validation_sampler = UniformSampler(\n    #            reader=self.readers[1],\n    #            window_sizes=self.data_param,\n    #            batch_size=self.net_param.batch_size,\n    #            windows_per_image=self.action_param.sample_per_volume,\n    #            queue_length=self.net_param.queue_length)\n    #        self.sampler = [[training_sampler, validation_sampler]]\n    #    else:\n    #        RegressionApplication.initialise_weighted_sampler()\n\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        RegressionApplication.initialise_dataset_loader(\n            self, data_param, task_param, data_partitioner)\n        if self.is_training:\n            return\n        if not task_param.error_map:\n            # use the regression application implementation\n            return\n\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        # modifying the original readers in regression application\n        # as we need ground truth labels to generate error maps\n        self.readers = [\n            ImageReader(['image', 'output']).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        mean_var_normaliser = MeanVarNormalisationLayer(image_name='image')\n        histogram_normaliser = None\n        if self.net_param.histogram_ref_file:\n            histogram_normaliser = HistogramNormalisationLayer(\n                image_name='image',\n                modalities=vars(task_param).get('image'),\n                model_filename=self.net_param.histogram_ref_file,\n                norm_type=self.net_param.norm_type,\n                cutoff=self.net_param.cutoff,\n                name='hist_norm_layer')\n\n        preprocessors = []\n        if self.net_param.normalisation:\n            preprocessors.append(histogram_normaliser)\n        if self.net_param.whitening:\n            preprocessors.append(mean_var_normaliser)\n        if self.net_param.volume_padding_size:\n            preprocessors.append(PadLayer(\n                image_name=SUPPORTED_INPUT,\n                border=self.net_param.volume_padding_size))\n        self.readers[0].add_preprocessing_layers(preprocessors)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        if self.is_training:\n            # using the original training pipeline\n            RegressionApplication.connect_data_and_network(\n                self, outputs_collector, gradients_collector)\n        else:\n            init_aggregator = \\\n                self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]\n            init_aggregator()\n\n            # modifying the original pipeline so that\n            # the error maps are computed instead of the regression output\n            with tf.name_scope('validation'):\n                data_dict = self.get_sampler()[0][-1].pop_batch_op()\n            image = tf.cast(data_dict['image'], tf.float32)\n            net_out = self.net(image, is_training=self.is_training)\n\n            if self.regression_param.error_map:\n                # writing error maps to folder without prefix\n                error_map_folder = os.path.join(\n                    os.path.dirname(self.action_param.save_seg_dir),\n                    'error_maps')\n                self.output_decoder.output_path = error_map_folder\n                self.output_decoder.prefix = ''\n\n                # computes absolute error\n                target = tf.cast(data_dict['output'], tf.float32)\n                net_out = tf.squared_difference(target, net_out)\n\n            # window output and locations for aggregating volume results\n            outputs_collector.add_to_collection(\n                var=net_out, name='window',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict['image_location'], name='location',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n"""
niftynet/contrib/sampler_pairwise/__init__.py,0,b''
niftynet/contrib/sampler_pairwise/sampler_pairwise_resize.py,18,"b'from __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n#from tensorflow.contrib.data.python.ops.dataset_ops import Dataset\n\nfrom niftynet.engine.image_window import ImageWindow\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\n\n\nclass PairwiseResizeSampler(Layer):\n    def __init__(self,\n                 reader_0,\n                 reader_1,\n                 data_param,\n                 batch_size=1):\n        Layer.__init__(self, name=\'pairwise_sampler_resize\')\n        # reader for the fixed images\n        self.reader_0 = reader_0\n        # reader for the moving images\n        self.reader_1 = reader_1\n\n        # TODO:\n        # 0) check the readers should have the same length file list\n        # 1) detect window shape mismatches or defaulting\n        #    windows to the fixed image reader properties\n        # 2) reshape images to (supporting multi-modal data)\n        #    [batch, x, y, channel] or [batch, x, y, z, channels]\n        # 3) infer spatial rank\n        # 4) make ``label`` optional\n        self.batch_size = int(batch_size)\n        assert self.batch_size > 0, ""batch size must be greater than 0""\n        self.spatial_rank = 3\n        self.window = ImageWindow.from_data_reader_properties(\n            self.reader_0.input_sources,\n            self.reader_0.shapes,\n            self.reader_0.tf_dtypes,\n            data_param)\n        if self.window.has_dynamic_shapes:\n            tf.logging.fatal(\'Dynamic shapes not supported.\\nPlease specify \'\n                             \'all spatial dims of the input data, for the \'\n                             \'spatial_window_size parameter.\')\n            raise NotImplementedError\n        # TODO: check spatial dims the same across input modalities\n        self.image_shape = \\\n            self.reader_0.shapes[\'fixed_image\'][:self.spatial_rank]\n        self.moving_image_shape = \\\n            self.reader_1.shapes[\'moving_image\'][:self.spatial_rank]\n        self.window_size = self.window.shapes[\'fixed_image\'][1:]\n\n        # initialise a dataset prefetching pairs of image and label volumes\n        n_subjects = len(self.reader_0.output_list)\n        int_seq = list(range(n_subjects))\n        # make the list of sequence divisible by batch size\n        while len(int_seq) > 0 and len(int_seq) % self.batch_size != 0:\n            int_seq.append(int_seq[-1])\n\n        image_dataset = tf.data.Dataset.from_tensor_slices(int_seq)\n        # mapping random integer id to 4 volumes moving/fixed x image/label\n        # tf.py_func wrapper of ``get_pairwise_inputs``\n        image_dataset = image_dataset.map(\n            lambda image_id: tuple(tf.py_func(\n                self.get_pairwise_inputs, [image_id],\n                [tf.int32, tf.float32, tf.float32, tf.int32, tf.int32])),\n            num_parallel_calls=4)  # supported by tf 1.4?\n        # todo: sequential and no repeatition\n        image_dataset = image_dataset.batch(self.batch_size)\n        self.iterator = image_dataset.make_initializable_iterator()\n\n    def get_pairwise_inputs(self, image_id):\n        # fetch fixed image\n        fixed_inputs = []\n        fixed_inputs.append(self._get_image(\'fixed_image\', image_id)[0])\n        fixed_inputs.append(self._get_image(\'fixed_label\', image_id)[0])\n        fixed_inputs = np.concatenate(fixed_inputs, axis=-1)\n        fixed_shape = np.asarray(fixed_inputs.shape).T.astype(np.int32)\n\n        # fetch moving image\n        moving_inputs = []\n        moving_inputs.append(self._get_image(\'moving_image\', image_id)[0])\n        moving_inputs.append(self._get_image(\'moving_label\', image_id)[0])\n        moving_inputs = np.concatenate(moving_inputs, axis=-1)\n        moving_shape = np.asarray(moving_inputs.shape).T.astype(np.int32)\n\n        return image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape\n\n    def _get_image(self, image_source_type, image_id):\n        # returns a random image from either the list of fixed images\n        # or the list of moving images\n        try:\n            image_source_type = image_source_type.decode()\n        except AttributeError:\n            pass\n        if image_source_type.startswith(\'fixed\'):\n            _, data, _ = self.reader_0(idx=image_id)\n        else:  # image_source_type.startswith(\'moving\'):\n            _, data, _ = self.reader_1(idx=image_id)\n        image = np.asarray(data[image_source_type]).astype(np.float32)\n        image_shape = list(image.shape)\n        image = np.reshape(image, image_shape[:self.spatial_rank] + [-1])\n        image_shape = np.asarray(image.shape).astype(np.int32)\n        return image, image_shape\n\n    def layer_op(self):\n        image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape = \\\n            self.iterator.get_next()\n        # TODO preprocessing layer modifying\n        #      image shapes will not be supported\n        # assuming the same shape across modalities, using the first\n        image_id.set_shape((self.batch_size,))\n        image_id = tf.to_float(image_id)\n\n        fixed_inputs.set_shape(\n            (self.batch_size,) + (None,) * self.spatial_rank + (2,))\n        # last dim is 1 image + 1 label\n        moving_inputs.set_shape(\n            (self.batch_size,) + self.moving_image_shape + (2,))\n        fixed_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n        moving_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n\n        # resizing the moving_inputs to match the target\n        # assumes the same shape across the batch\n        target_spatial_shape = \\\n            tf.unstack(fixed_shape[0], axis=0)[:self.spatial_rank]\n        moving_inputs = Resize(new_size=target_spatial_shape)(moving_inputs)\n        combined_volume = tf.concat([fixed_inputs, moving_inputs], axis=-1)\n\n        # TODO affine data augmentation here\n        if self.spatial_rank == 3:\n\n            window_channels = np.prod(self.window_size[self.spatial_rank:]) * 4\n            # TODO if no affine augmentation:\n            img_spatial_shape = target_spatial_shape\n            win_spatial_shape = [tf.constant(dim) for dim in\n                                 self.window_size[:self.spatial_rank]]\n\n            # scale the image to new space\n            batch_scale = [\n                tf.reshape(tf.to_float(img) / tf.to_float(win), (1,1))\n                for (win, img) in zip(win_spatial_shape, img_spatial_shape)]\n            batch_scale = tf.concat(batch_scale, axis=1)\n            affine_constraints = ((None, 0.0, 0.0, 0.0),\n                                  (0.0, None, 0.0, 0.0),\n                                  (0.0, 0.0, None, 0.0))\n            computed_grid = AffineGridWarperLayer(\n                source_shape=(None, None, None),\n                output_shape=self.window_size[:self.spatial_rank],\n                constraints=affine_constraints)(batch_scale)\n            computed_grid.set_shape((1,) +\n                                    self.window_size[:self.spatial_rank] +\n                                    (self.spatial_rank,))\n            resampler = ResamplerLayer(\n                interpolation=\'linear\', boundary=\'replicate\')\n            windows = resampler(combined_volume, computed_grid)\n            out_shape = [self.batch_size] + \\\n                        list(self.window_size[:self.spatial_rank]) + \\\n                        [window_channels]\n            windows.set_shape(out_shape)\n\n            image_id = tf.reshape(image_id, (self.batch_size, 1))\n            start_location = tf.zeros((self.batch_size, self.spatial_rank))\n            end_location = tf.constant(self.window_size[:self.spatial_rank])\n            end_location = tf.reshape(end_location, (1, self.spatial_rank))\n            end_location = tf.to_float(tf.tile(\n                end_location, [self.batch_size, 1]))\n            locations = tf.concat([\n                image_id, start_location, end_location], axis=1)\n        return windows, locations\n        #return windows, [tf.reduce_max(computed_grid), batch_scale]\n\n    # overriding input buffers\n    def run_threads(self, session, *args, **argvs):\n        """"""\n        To be called at the beginning of running graph variables\n        """"""\n        session.run(self.iterator.initializer)\n        return\n\n    def close_all(self):\n        # do nothing\n        pass\n'"
niftynet/contrib/sampler_pairwise/sampler_pairwise_resize_csv.py,18,"b'from __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n#from tensorflow.contrib.data.python.ops.dataset_ops import Dataset\n\nfrom niftynet.engine.image_window import ImageWindow\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\n\n\nclass PairwiseResizeSampler(Layer):\n    def __init__(self,\n                 reader_0,\n                 reader_1,\n                 data_param,\n                 batch_size=1):\n        Layer.__init__(self, name=\'pairwise_sampler_resize\')\n        # reader for the fixed images\n        self.reader_0 = reader_0\n        # reader for the moving images\n        self.reader_1 = reader_1\n\n        # TODO:\n        # 0) check the readers should have the same length file list\n        # 1) detect window shape mismatches or defaulting\n        #    windows to the fixed image reader properties\n        # 2) reshape images to (supporting multi-modal data)\n        #    [batch, x, y, channel] or [batch, x, y, z, channels]\n        # 3) infer spatial rank\n        # 4) make ``label`` optional\n        self.batch_size = int(batch_size)\n        assert self.batch_size > 0, ""batch size must be greater than 0""\n        self.spatial_rank = 3\n        self.window = ImageWindow.from_data_reader_properties(\n            self.reader_0.input_sources,\n            self.reader_0.shapes,\n            self.reader_0.tf_dtypes,\n            data_param)\n        if self.window.has_dynamic_shapes:\n            tf.logging.fatal(\'Dynamic shapes not supported.\\nPlease specify \'\n                             \'all spatial dims of the input data, for the \'\n                             \'spatial_window_size parameter.\')\n            raise NotImplementedError\n        # TODO: check spatial dims the same across input modalities\n        self.image_shape = \\\n            self.reader_0.shapes[\'fixed_image\'][:self.spatial_rank]\n        self.moving_image_shape = \\\n            self.reader_1.shapes[\'moving_image\'][:self.spatial_rank]\n        self.window_size = self.window.shapes[\'fixed_image\'][1:]\n\n        # initialise a dataset prefetching pairs of image and label volumes\n        n_subjects = len(self.reader_0.output_list)\n        int_seq = list(range(n_subjects))\n        # make the list of sequence divisible by batch size\n        while len(int_seq) > 0 and len(int_seq) % self.batch_size != 0:\n            int_seq.append(int_seq[-1])\n\n        image_dataset = tf.data.Dataset.from_tensor_slices(int_seq)\n        # mapping random integer id to 4 volumes moving/fixed x image/label\n        # tf.py_func wrapper of ``get_pairwise_inputs``\n        image_dataset = image_dataset.map(\n            lambda image_id: tuple(tf.py_func(\n                self.get_pairwise_inputs, [image_id],\n                [tf.int32, tf.float32, tf.float32, tf.int32, tf.int32])),\n            num_parallel_calls=4)  # supported by tf 1.4?\n        # todo: sequential and no repeatition\n        image_dataset = image_dataset.batch(self.batch_size)\n        self.iterator = image_dataset.make_initializable_iterator()\n\n    def get_pairwise_inputs(self, image_id):\n        # fetch fixed image\n        fixed_inputs = []\n        fixed_inputs.append(self._get_image(\'fixed_image\', image_id)[0])\n        fixed_inputs.append(self._get_image(\'fixed_label\', image_id)[0])\n        fixed_inputs = np.concatenate(fixed_inputs, axis=-1)\n        fixed_shape = np.asarray(fixed_inputs.shape).T.astype(np.int32)\n\n        # fetch moving image\n        moving_inputs = []\n        moving_inputs.append(self._get_image(\'moving_image\', image_id)[0])\n        moving_inputs.append(self._get_image(\'moving_label\', image_id)[0])\n        moving_inputs = np.concatenate(moving_inputs, axis=-1)\n        moving_shape = np.asarray(moving_inputs.shape).T.astype(np.int32)\n\n        return image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape\n\n    def _get_image(self, image_source_type, image_id):\n        # returns a random image from either the list of fixed images\n        # or the list of moving images\n        try:\n            image_source_type = image_source_type.decode()\n        except AttributeError:\n            pass\n        if image_source_type.startswith(\'fixed\'):\n            _, data, _ = self.reader_0(idx=image_id)\n        else:  # image_source_type.startswith(\'moving\'):\n            _, data, _ = self.reader_1(idx=image_id)\n        image = np.asarray(data[image_source_type]).astype(np.float32)\n        image_shape = list(image.shape)\n        image = np.reshape(image, image_shape[:self.spatial_rank] + [-1])\n        image_shape = np.asarray(image.shape).astype(np.int32)\n        return image, image_shape\n\n    def layer_op(self):\n        image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape = \\\n            self.iterator.get_next()\n        # TODO preprocessing layer modifying\n        #      image shapes will not be supported\n        # assuming the same shape across modalities, using the first\n        image_id.set_shape((self.batch_size,))\n        image_id = tf.to_float(image_id)\n\n        fixed_inputs.set_shape(\n            (self.batch_size,) + (None,) * self.spatial_rank + (2,))\n        # last dim is 1 image + 1 label\n        moving_inputs.set_shape(\n            (self.batch_size,) + self.moving_image_shape + (2,))\n        fixed_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n        moving_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n\n        # resizing the moving_inputs to match the target\n        # assumes the same shape across the batch\n        target_spatial_shape = \\\n            tf.unstack(fixed_shape[0], axis=0)[:self.spatial_rank]\n        moving_inputs = Resize(new_size=target_spatial_shape)(moving_inputs)\n        combined_volume = tf.concat([fixed_inputs, moving_inputs], axis=-1)\n\n        # TODO affine data augmentation here\n        if self.spatial_rank == 3:\n\n            window_channels = np.prod(self.window_size[self.spatial_rank:]) * 4\n            # TODO if no affine augmentation:\n            img_spatial_shape = target_spatial_shape\n            win_spatial_shape = [tf.constant(dim) for dim in\n                                 self.window_size[:self.spatial_rank]]\n\n            # scale the image to new space\n            batch_scale = [\n                tf.reshape(tf.to_float(img) / tf.to_float(win), (1,1))\n                for (win, img) in zip(win_spatial_shape, img_spatial_shape)]\n            batch_scale = tf.concat(batch_scale, axis=1)\n            affine_constraints = ((None, 0.0, 0.0, 0.0),\n                                  (0.0, None, 0.0, 0.0),\n                                  (0.0, 0.0, None, 0.0))\n            computed_grid = AffineGridWarperLayer(\n                source_shape=(None, None, None),\n                output_shape=self.window_size[:self.spatial_rank],\n                constraints=affine_constraints)(batch_scale)\n            computed_grid.set_shape((1,) +\n                                    self.window_size[:self.spatial_rank] +\n                                    (self.spatial_rank,))\n            resampler = ResamplerLayer(\n                interpolation=\'linear\', boundary=\'replicate\')\n            windows = resampler(combined_volume, computed_grid)\n            out_shape = [self.batch_size] + \\\n                        list(self.window_size[:self.spatial_rank]) + \\\n                        [window_channels]\n            windows.set_shape(out_shape)\n\n            image_id = tf.reshape(image_id, (self.batch_size, 1))\n            start_location = tf.zeros((self.batch_size, self.spatial_rank))\n            end_location = tf.constant(self.window_size[:self.spatial_rank])\n            end_location = tf.reshape(end_location, (1, self.spatial_rank))\n            end_location = tf.to_float(tf.tile(\n                end_location, [self.batch_size, 1]))\n            locations = tf.concat([\n                image_id, start_location, end_location], axis=1)\n        return windows, locations\n        #return windows, [tf.reduce_max(computed_grid), batch_scale]\n\n    # overriding input buffers\n    def run_threads(self, session, *args, **argvs):\n        """"""\n        To be called at the beginning of running graph variables\n        """"""\n        session.run(self.iterator.initializer)\n        return\n\n    def close_all(self):\n        # do nothing\n        pass\n'"
niftynet/contrib/sampler_pairwise/sampler_pairwise_uniform.py,22,"b'from __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n#from tensorflow.contrib.data.python.ops.dataset_ops import Dataset\n\nfrom niftynet.engine.image_window import ImageWindow\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\n#from niftynet.layer.approximated_smoothing import SmoothingLayer as Smooth\n\n\nclass PairwiseUniformSampler(Layer):\n    def __init__(self,\n                 reader_0,\n                 reader_1,\n                 data_param,\n                 batch_size=1):\n        Layer.__init__(self, name=\'pairwise_sampler_uniform\')\n        # reader for the fixed images\n        self.reader_0 = reader_0\n        # reader for the moving images\n        self.reader_1 = reader_1\n\n        # TODO:\n        # 0) check the readers should have the same length file list\n        # 1) detect window shape mismatches or defaulting\n        #    windows to the fixed image reader properties\n        # 2) reshape images to (supporting multi-modal data)\n        #    [batch, x, y, channel] or [batch, x, y, z, channels]\n        # 3) infer spatial rank\n        # 4) make ``label`` optional\n        self.batch_size = batch_size\n        self.spatial_rank = 3\n        self.window = ImageWindow.from_data_reader_properties(\n            self.reader_0.input_sources,\n            self.reader_0.shapes,\n            self.reader_0.tf_dtypes,\n            data_param)\n        if self.window.has_dynamic_shapes:\n            tf.logging.fatal(\'Dynamic shapes not supported.\\nPlease specify \'\n                             \'all spatial dims of the input data, for the \'\n                             \'spatial_window_size parameter.\')\n            raise NotImplementedError\n        # TODO: check spatial dims the same across input modalities\n        self.image_shape = \\\n            self.reader_0.shapes[\'fixed_image\'][:self.spatial_rank]\n        self.moving_image_shape = \\\n            self.reader_1.shapes[\'moving_image\'][:self.spatial_rank]\n        self.window_size = self.window.shapes[\'fixed_image\'][1:]\n\n        # initialise a dataset prefetching pairs of image and label volumes\n        n_subjects = len(self.reader_0.output_list)\n        rand_ints = np.random.randint(n_subjects, size=[n_subjects])\n        image_dataset = tf.data.Dataset.from_tensor_slices(rand_ints)\n        # mapping random integer id to 4 volumes moving/fixed x image/label\n        # tf.py_func wrapper of ``get_pairwise_inputs``\n        image_dataset = image_dataset.map(\n            lambda image_id: tuple(tf.py_func(\n                self.get_pairwise_inputs, [image_id],\n                [tf.int64, tf.float32, tf.float32, tf.int32, tf.int32])),\n            num_parallel_calls=4)  # supported by tf 1.4?\n        image_dataset = image_dataset.repeat()  # num_epochs can be param\n        image_dataset = image_dataset.shuffle(\n            buffer_size=self.batch_size * 20)\n        image_dataset = image_dataset.batch(self.batch_size)\n        self.iterator = image_dataset.make_initializable_iterator()\n\n    def get_pairwise_inputs(self, image_id):\n        # fetch fixed image\n        fixed_inputs = []\n        fixed_inputs.append(self._get_image(\'fixed_image\', image_id)[0])\n        fixed_inputs.append(self._get_image(\'fixed_label\', image_id)[0])\n        fixed_inputs = np.concatenate(fixed_inputs, axis=-1)\n        fixed_shape = np.asarray(fixed_inputs.shape).T.astype(np.int32)\n\n        # fetch moving image\n        moving_inputs = []\n        moving_inputs.append(self._get_image(\'moving_image\', image_id)[0])\n        moving_inputs.append(self._get_image(\'moving_label\', image_id)[0])\n        moving_inputs = np.concatenate(moving_inputs, axis=-1)\n        moving_shape = np.asarray(moving_inputs.shape).T.astype(np.int32)\n\n        return image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape\n\n    def _get_image(self, image_source_type, image_id):\n        # returns a random image from either the list of fixed images\n        # or the list of moving images\n        try:\n            image_source_type = image_source_type.decode()\n        except AttributeError:\n            pass\n        if image_source_type.startswith(\'fixed\'):\n            _, data, _ = self.reader_0(idx=image_id)\n        else:  # image_source_type.startswith(\'moving\'):\n            _, data, _ = self.reader_1(idx=image_id)\n        image = np.asarray(data[image_source_type]).astype(np.float32)\n        image_shape = list(image.shape)\n        image = np.reshape(image, image_shape[:self.spatial_rank] + [-1])\n        image_shape = np.asarray(image.shape).astype(np.int32)\n        return image, image_shape\n\n    def layer_op(self):\n        """"""\n        This function concatenate image and label volumes at the last dim\n        and randomly cropping the volumes (also the cropping margins)\n        """"""\n        image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape = \\\n            self.iterator.get_next()\n        # TODO preprocessing layer modifying\n        #      image shapes will not be supported\n        # assuming the same shape across modalities, using the first\n        image_id.set_shape((self.batch_size,))\n        image_id = tf.to_float(image_id)\n\n        fixed_inputs.set_shape(\n            (self.batch_size,) + (None,) * self.spatial_rank + (2,))\n        # last dim is 1 image + 1 label\n        moving_inputs.set_shape(\n            (self.batch_size,) + self.moving_image_shape + (2,))\n        fixed_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n        moving_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n\n        # resizing the moving_inputs to match the target\n        # assumes the same shape across the batch\n        target_spatial_shape = \\\n            tf.unstack(fixed_shape[0], axis=0)[:self.spatial_rank]\n        moving_inputs = Resize(new_size=target_spatial_shape)(moving_inputs)\n        combined_volume = tf.concat([fixed_inputs, moving_inputs], axis=-1)\n\n        # smoothing_layer = Smoothing(\n        #     sigma=1, truncate=3.0, type_str=\'gaussian\')\n        # combined_volume = tf.unstack(combined_volume, axis=-1)\n        # combined_volume[0] = tf.expand_dims(combined_volume[0], axis=-1)\n        # combined_volume[1] = smoothing_layer(\n        #     tf.expand_dims(combined_volume[1]), axis=-1)\n        # combined_volume[2] = tf.expand_dims(combined_volume[2], axis=-1)\n        # combined_volume[3] = smoothing_layer(\n        #     tf.expand_dims(combined_volume[3]), axis=-1)\n        # combined_volume = tf.stack(combined_volume, axis=-1)\n\n        # TODO affine data augmentation here\n        if self.spatial_rank == 3:\n\n            window_channels = np.prod(self.window_size[self.spatial_rank:]) * 4\n            # TODO if no affine augmentation:\n            img_spatial_shape = target_spatial_shape\n            win_spatial_shape = [tf.constant(dim) for dim in\n                                 self.window_size[:self.spatial_rank]]\n            # when img==win make sure shift => 0.0\n            # otherwise interpolation is out of bound\n            batch_shift = [\n                tf.random_uniform(\n                    shape=(self.batch_size, 1),\n                    minval=0,\n                    maxval=tf.maximum(tf.to_float(img - win - 1), 0.01))\n                for (win, img) in zip(win_spatial_shape, img_spatial_shape)]\n            batch_shift = tf.concat(batch_shift, axis=1)\n            affine_constraints = ((1.0, 0.0, 0.0, None),\n                                  (0.0, 1.0, 0.0, None),\n                                  (0.0, 0.0, 1.0, None))\n            computed_grid = AffineGridWarperLayer(\n                source_shape=(None, None, None),\n                output_shape=self.window_size[:self.spatial_rank],\n                constraints=affine_constraints)(batch_shift)\n            computed_grid.set_shape((self.batch_size,) +\n                                    self.window_size[:self.spatial_rank] +\n                                    (self.spatial_rank,))\n            resampler = ResamplerLayer(\n                interpolation=\'linear\', boundary=\'replicate\')\n            windows = resampler(combined_volume, computed_grid)\n            out_shape = [self.batch_size] + \\\n                        list(self.window_size[:self.spatial_rank]) + \\\n                        [window_channels]\n            windows.set_shape(out_shape)\n\n            image_id = tf.reshape(image_id, (self.batch_size, 1))\n            start_location = tf.zeros((self.batch_size, self.spatial_rank))\n            locations = tf.concat([\n                image_id, start_location, batch_shift], axis=1)\n        return windows, locations\n        # return windows, [tf.reduce_max(computed_grid), batch_shift]\n\n    # overriding input buffers\n    def run_threads(self, session, *args, **argvs):\n        """"""\n        To be called at the beginning of running graph variables\n        """"""\n        session.run(self.iterator.initializer)\n        return\n\n    def close_all(self):\n        # do nothing\n        pass\n'"
niftynet/contrib/sampler_pairwise/sampler_pairwise_uniform_csv.py,22,"b'from __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n#from tensorflow.contrib.data.python.ops.dataset_ops import Dataset\n\nfrom niftynet.engine.image_window import ImageWindow\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\n#from niftynet.layer.approximated_smoothing import SmoothingLayer as Smooth\n\n\nclass PairwiseUniformSampler(Layer):\n    def __init__(self,\n                 reader_0,\n                 reader_1,\n                 data_param,\n                 batch_size=1):\n        Layer.__init__(self, name=\'pairwise_sampler_uniform\')\n        # reader for the fixed images\n        self.reader_0 = reader_0\n        # reader for the moving images\n        self.reader_1 = reader_1\n\n        # TODO:\n        # 0) check the readers should have the same length file list\n        # 1) detect window shape mismatches or defaulting\n        #    windows to the fixed image reader properties\n        # 2) reshape images to (supporting multi-modal data)\n        #    [batch, x, y, channel] or [batch, x, y, z, channels]\n        # 3) infer spatial rank\n        # 4) make ``label`` optional\n        self.batch_size = batch_size\n        self.spatial_rank = 3\n        self.window = ImageWindow.from_data_reader_properties(\n            self.reader_0.input_sources,\n            self.reader_0.shapes,\n            self.reader_0.tf_dtypes,\n            data_param)\n        if self.window.has_dynamic_shapes:\n            tf.logging.fatal(\'Dynamic shapes not supported.\\nPlease specify \'\n                             \'all spatial dims of the input data, for the \'\n                             \'spatial_window_size parameter.\')\n            raise NotImplementedError\n        # TODO: check spatial dims the same across input modalities\n        self.image_shape = \\\n            self.reader_0.shapes[\'fixed_image\'][:self.spatial_rank]\n        self.moving_image_shape = \\\n            self.reader_1.shapes[\'moving_image\'][:self.spatial_rank]\n        self.window_size = self.window.shapes[\'fixed_image\'][1:]\n\n        # initialise a dataset prefetching pairs of image and label volumes\n        n_subjects = len(self.reader_0.output_list)\n        rand_ints = np.random.randint(n_subjects, size=[n_subjects])\n        image_dataset = tf.data.Dataset.from_tensor_slices(rand_ints)\n        # mapping random integer id to 4 volumes moving/fixed x image/label\n        # tf.py_func wrapper of ``get_pairwise_inputs``\n        image_dataset = image_dataset.map(\n            lambda image_id: tuple(tf.py_func(\n                self.get_pairwise_inputs, [image_id],\n                [tf.int64, tf.float32, tf.float32, tf.int32, tf.int32])),\n            num_parallel_calls=4)  # supported by tf 1.4?\n        image_dataset = image_dataset.repeat()  # num_epochs can be param\n        image_dataset = image_dataset.shuffle(\n            buffer_size=self.batch_size * 20)\n        image_dataset = image_dataset.batch(self.batch_size)\n        self.iterator = image_dataset.make_initializable_iterator()\n\n    def get_pairwise_inputs(self, image_id):\n        # fetch fixed image\n        fixed_inputs = []\n        fixed_inputs.append(self._get_image(\'fixed_image\', image_id)[0])\n        fixed_inputs.append(self._get_image(\'fixed_label\', image_id)[0])\n        fixed_inputs = np.concatenate(fixed_inputs, axis=-1)\n        fixed_shape = np.asarray(fixed_inputs.shape).T.astype(np.int32)\n\n        # fetch moving image\n        moving_inputs = []\n        moving_inputs.append(self._get_image(\'moving_image\', image_id)[0])\n        moving_inputs.append(self._get_image(\'moving_label\', image_id)[0])\n        moving_inputs = np.concatenate(moving_inputs, axis=-1)\n        moving_shape = np.asarray(moving_inputs.shape).T.astype(np.int32)\n\n        return image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape\n\n    def _get_image(self, image_source_type, image_id):\n        # returns a random image from either the list of fixed images\n        # or the list of moving images\n        try:\n            image_source_type = image_source_type.decode()\n        except AttributeError:\n            pass\n        if image_source_type.startswith(\'fixed\'):\n            _, data, _ = self.reader_0(idx=image_id)\n        else:  # image_source_type.startswith(\'moving\'):\n            _, data, _ = self.reader_1(idx=image_id)\n        image = np.asarray(data[image_source_type]).astype(np.float32)\n        image_shape = list(image.shape)\n        image = np.reshape(image, image_shape[:self.spatial_rank] + [-1])\n        image_shape = np.asarray(image.shape).astype(np.int32)\n        return image, image_shape\n\n    def layer_op(self):\n        """"""\n        This function concatenate image and label volumes at the last dim\n        and randomly cropping the volumes (also the cropping margins)\n        """"""\n        image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape = \\\n            self.iterator.get_next()\n        # TODO preprocessing layer modifying\n        #      image shapes will not be supported\n        # assuming the same shape across modalities, using the first\n        image_id.set_shape((self.batch_size,))\n        image_id = tf.to_float(image_id)\n\n        fixed_inputs.set_shape(\n            (self.batch_size,) + (None,) * self.spatial_rank + (2,))\n        # last dim is 1 image + 1 label\n        moving_inputs.set_shape(\n            (self.batch_size,) + self.moving_image_shape + (2,))\n        fixed_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n        moving_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n\n        # resizing the moving_inputs to match the target\n        # assumes the same shape across the batch\n        target_spatial_shape = \\\n            tf.unstack(fixed_shape[0], axis=0)[:self.spatial_rank]\n        moving_inputs = Resize(new_size=target_spatial_shape)(moving_inputs)\n        combined_volume = tf.concat([fixed_inputs, moving_inputs], axis=-1)\n\n        # smoothing_layer = Smoothing(\n        #     sigma=1, truncate=3.0, type_str=\'gaussian\')\n        # combined_volume = tf.unstack(combined_volume, axis=-1)\n        # combined_volume[0] = tf.expand_dims(combined_volume[0], axis=-1)\n        # combined_volume[1] = smoothing_layer(\n        #     tf.expand_dims(combined_volume[1]), axis=-1)\n        # combined_volume[2] = tf.expand_dims(combined_volume[2], axis=-1)\n        # combined_volume[3] = smoothing_layer(\n        #     tf.expand_dims(combined_volume[3]), axis=-1)\n        # combined_volume = tf.stack(combined_volume, axis=-1)\n\n        # TODO affine data augmentation here\n        if self.spatial_rank == 3:\n\n            window_channels = np.prod(self.window_size[self.spatial_rank:]) * 4\n            # TODO if no affine augmentation:\n            img_spatial_shape = target_spatial_shape\n            win_spatial_shape = [tf.constant(dim) for dim in\n                                 self.window_size[:self.spatial_rank]]\n            # when img==win make sure shift => 0.0\n            # otherwise interpolation is out of bound\n            batch_shift = [\n                tf.random_uniform(\n                    shape=(self.batch_size, 1),\n                    minval=0,\n                    maxval=tf.maximum(tf.to_float(img - win - 1), 0.01))\n                for (win, img) in zip(win_spatial_shape, img_spatial_shape)]\n            batch_shift = tf.concat(batch_shift, axis=1)\n            affine_constraints = ((1.0, 0.0, 0.0, None),\n                                  (0.0, 1.0, 0.0, None),\n                                  (0.0, 0.0, 1.0, None))\n            computed_grid = AffineGridWarperLayer(\n                source_shape=(None, None, None),\n                output_shape=self.window_size[:self.spatial_rank],\n                constraints=affine_constraints)(batch_shift)\n            computed_grid.set_shape((self.batch_size,) +\n                                    self.window_size[:self.spatial_rank] +\n                                    (self.spatial_rank,))\n            resampler = ResamplerLayer(\n                interpolation=\'linear\', boundary=\'replicate\')\n            windows = resampler(combined_volume, computed_grid)\n            out_shape = [self.batch_size] + \\\n                        list(self.window_size[:self.spatial_rank]) + \\\n                        [window_channels]\n            windows.set_shape(out_shape)\n\n            image_id = tf.reshape(image_id, (self.batch_size, 1))\n            start_location = tf.zeros((self.batch_size, self.spatial_rank))\n            locations = tf.concat([\n                image_id, start_location, batch_shift], axis=1)\n        return windows, locations\n        # return windows, [tf.reduce_max(computed_grid), batch_shift]\n\n    # overriding input buffers\n    def run_threads(self, session, *args, **argvs):\n        """"""\n        To be called at the beginning of running graph variables\n        """"""\n        session.run(self.iterator.initializer)\n        return\n\n    def close_all(self):\n        # do nothing\n        pass\n'"
niftynet/contrib/segmentation_bf_aug/__init__.py,0,b''
niftynet/contrib/segmentation_bf_aug/segmentation_application_bfaug.py,2,"b'import tensorflow as tf\n\nfrom niftynet.application.segmentation_application import \\\n    SegmentationApplication, SUPPORTED_INPUT\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.rand_bias_field import RandomBiasFieldLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\n\n\nclass SegmentationApplicationBFAug(SegmentationApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, is_training):\n        SegmentationApplication.__init__(\n            self, net_param, action_param, is_training)\n        tf.logging.info(\'starting segmentation application\')\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.segmentation_param = task_param\n\n        # initialise input image readers\n        if self.is_training:\n            reader_names = (\'image\', \'label\', \'weight\', \'sampler\')\n        elif self.is_inference:\n            # in the inference process use `image` input only\n            reader_names = (\'image\',)\n        elif self.is_evaluation:\n            reader_names = (\'image\', \'label\', \'inferred\')\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        foreground_masking_layer = None\n        if self.net_param.normalise_foreground_only:\n            foreground_masking_layer = BinaryMaskingLayer(\n                type_str=self.net_param.foreground_type,\n                multimod_fusion=self.net_param.multimod_foreground_type,\n                threshold=0.0)\n\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer)\n        histogram_normaliser = None\n        if self.net_param.histogram_ref_file:\n            histogram_normaliser = HistogramNormalisationLayer(\n                image_name=\'image\',\n                modalities=vars(task_param).get(\'image\'),\n                model_filename=self.net_param.histogram_ref_file,\n                binary_masking_func=foreground_masking_layer,\n                norm_type=self.net_param.norm_type,\n                cutoff=self.net_param.cutoff,\n                name=\'hist_norm_layer\')\n\n        label_normaliser = None\n        if self.net_param.histogram_ref_file:\n            label_normaliser = DiscreteLabelNormalisationLayer(\n                image_name=\'label\',\n                modalities=vars(task_param).get(\'label\'),\n                model_filename=self.net_param.histogram_ref_file)\n\n        normalisation_layers = []\n        if self.net_param.normalisation:\n            normalisation_layers.append(histogram_normaliser)\n        if self.net_param.whitening:\n            normalisation_layers.append(mean_var_normaliser)\n        if task_param.label_normalisation and \\\n                (self.is_training or not task_param.output_prob):\n            normalisation_layers.append(label_normaliser)\n\n        augmentation_layers = []\n        if self.is_training:\n            if self.action_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=self.action_param.random_flipping_axes))\n            if self.action_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=self.action_param.scaling_percentage[0],\n                    max_percentage=self.action_param.scaling_percentage[1]))\n            if self.action_param.rotation_angle or \\\n                    self.action_param.rotation_angle_x or \\\n                    self.action_param.rotation_angle_y or \\\n                    self.action_param.rotation_angle_z:\n                rotation_layer = RandomRotationLayer()\n                if self.action_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        self.action_param.rotation_angle)\n                else:\n                    rotation_layer.init_non_uniform_angle(\n                        self.action_param.rotation_angle_x,\n                        self.action_param.rotation_angle_y,\n                        self.action_param.rotation_angle_z)\n                augmentation_layers.append(rotation_layer)\n            if self.action_param.bias_field_range:\n                bias_field_layer = RandomBiasFieldLayer()\n                bias_field_layer.init_order(self.action_param.bf_order)\n                bias_field_layer.init_uniform_coeff(\n                    self.action_param.bias_field_range)\n                augmentation_layers.append(bias_field_layer)\n\n        volume_padding_layer = [PadLayer(\n            image_name=SUPPORTED_INPUT,\n            border=self.net_param.volume_padding_size,\n            mode=self.net_param.volume_padding_mode,\n            pad_to=self.net_param.volume_padding_to_size)\n        ]\n\n        self.readers[0].add_preprocessing_layers(\n            volume_padding_layer + normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(\n                volume_padding_layer + normalisation_layers)\n'"
niftynet/contrib/segmentation_selective_sampler/__init__.py,0,b''
niftynet/contrib/segmentation_selective_sampler/sampler_selective.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nGenerating sample arrays from random distributions\n""""""\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\nimport tensorflow as tf\nfrom scipy import ndimage\nfrom scipy.signal import fftconvolve\n\nfrom niftynet.engine.image_window import N_SPATIAL\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\n\n\n# pylint: disable=too-many-arguments\nclass SelectiveSampler(UniformSampler):\n    """"""\n    This class generators samples by uniformly sampling each input volume\n    currently the coordinates are randomised for spatial dims only,\n    i.e., the first three dims of image.\n\n    This layer can be considered as a `random cropping` layer of the\n    input image.\n    """"""\n\n    def __init__(self,\n                 reader,\n                 data_param,\n                 batch_size,\n                 windows_per_image,\n                 constraint,\n                 random_windows_per_image=0,\n                 queue_length=10):\n        UniformSampler.__init__(self,\n                                reader=reader,\n                                data_param=data_param,\n                                batch_size=batch_size,\n                                windows_per_image=windows_per_image,\n                                queue_length=queue_length)\n        self.constraint = constraint\n        self.n_samples_rand = random_windows_per_image\n        self.spatial_coordinates_generator = \\\n            self.selective_spatial_coordinates()\n        tf.logging.info(\'initialised selective sampling\')\n\n    def selective_spatial_coordinates(self):\n        """"""\n        this function generates a function, the function will be used\n        to replace the random coordinate generated in UniformSampler\n\n        This wrapper is created in order to use self.properties\n        directly in the generator.\n\n        :return: a spatial coordinates generator function\n        """"""\n        print(\'self rand samples is \', self.n_samples_rand)\n\n        def spatial_coordinates_function(\n                subject_id,\n                data,\n                img_sizes,\n                win_sizes,\n                n_samples=1,\n                n_samples_rand=0):\n            """"""\n            this function first find a set of feasible locations,\n            based on discrete labels and randomly choose n_samples\n            from the feasible locations.\n\n            :param subject_id:\n            :param data:\n            :param img_sizes:\n            :param win_sizes:\n            :param n_samples: total number of samples\n            :param n_samples_rand: number of totally random samples apart\n            from selected ones\n            :return:\n            """"""\n            candidates, proba_cand = candidate_indices(\n                win_sizes[\'label\'], data[\'label\'], self.constraint)\n            print(\'feasible locations found\')\n            if np.sum(candidates) < self.window.n_samples:\n                print(\'Constraints not fulfilled for this case\')\n                # return something here...\n\n            # find random coordinates based on window, potential\n            # candidates and image shapes\n\n            coordinates = rand_choice_coordinates(subject_id,\n                                                  img_sizes,\n                                                  win_sizes,\n                                                  candidates,\n                                                  proba_cand,\n                                                  n_samples,\n                                                  n_samples_rand)\n            return coordinates\n\n        return spatial_coordinates_function\n\n\ndef create_label_size_map(data):\n    """"""\n    This function creates the maps of label size. For each connected\n    component of a label with value :value:, the binary segmentation is\n    replaced by the size of the considered element\n    :param data: segmentation\n    :param value: value of the label to consider\n    :return: count_data\n    """"""\n    labelled_data, _ = ndimage.label(data)\n    components, count = np.unique(labelled_data, return_counts=True)\n    count_data = np.copy(labelled_data)\n    for label, size in zip(components, count):\n        if label == 0:\n            continue\n        count_data[labelled_data == label] = size\n    return count_data\n\n\ndef candidate_indices(win_sizes, data, constraint):\n    """"""\n    This functions creates a binary map of potential candidate indices given\n    the specified constraints and the recalculated probability to select each of\n     these candidates so as to uniformise the sampling according to the size of\n     connected elements\n    :param win_sizes:\n    :param data: segmentation\n    :param constraint: sampling constraint\n    :return: candidates: binary map of potential indices, proba_fin:\n    corresponding maps of associated sampling probability\n    """"""\n    unique = np.unique(np.round(data))\n    list_labels = []\n    data = np.round(data)\n    if constraint.list_labels:\n        list_labels = constraint.list_labels\n\n        print(\'list labels is \', list_labels)\n        for label in list_labels:\n            if label not in unique:\n                print(\'Label %d is not there\' % label)\n                return np.zeros_like(data), None\n    num_labels_add = max(constraint.num_labels - len(list_labels), 0) \\\n        if constraint.num_labels > 0 else 0\n    if len(unique) < constraint.num_labels:\n        print(\'Missing labels\')\n        return np.zeros_like(data), None\n    if constraint.min_ratio > 0:\n        num_min = constraint.min_number_from_ratio(win_sizes)\n        spatial_win_sizes = np.asarray(win_sizes[:N_SPATIAL], dtype=np.int32)\n        max_spatial_win = spatial_win_sizes[0]\n        # Create segmentation for this label\n        list_counts = []\n        shape_ones = np.asarray(data.shape)\n        # print(shape_ones, max_spatial_win)\n        half_max_size = np.floor(max_spatial_win / 2)\n        padding = []\n        for i in range(0, len(win_sizes)):\n            if i < N_SPATIAL:\n                shape_ones[i] -= 2 * half_max_size\n                padding = padding + [[half_max_size, half_max_size], ]\n            else:\n                padding = padding + [[0, 0], ]\n\n        final = np.pad(np.ones(shape_ones),\n                       np.asarray(padding, dtype=np.int32),\n                       \'constant\')\n        # print(shape_ones, padding, data.shape, np.sum(np.ones(data.shape)),\n        #       np.sum(final))\n        window_ones = np.ones(win_sizes, dtype=np.int32)\n        mean_counts_size = []\n        # print(unique)\n        for value in unique:\n            # print(np.sum(data), \'sum in data\', np.prod(data.shape),\n            #       \' elements in data\')\n            seg_label = (data == value).astype(data.dtype)\n            # print(np.sum(seg_label), "" num values in seg_label "", value)\n            label_size = create_label_size_map(seg_label)\n            # print(value, np.sum(seg_label), seg_label.shape,\n            #       window_ones.shape, num_min)\n            # print(\'Begin fft convolve\')\n            counts_window = fftconvolve(seg_label, window_ones, \'same\')\n            # print(\'finished fft convolve\')\n            valid_places = \\\n                (counts_window > np.max([num_min, 1])).astype(data.dtype)\n            counts_size = fftconvolve(\n                label_size * valid_places, window_ones, \'same\')\n            mean_counts_size_temp = np.nan_to_num(\n                counts_size * 1.0 / counts_window)\n            mean_counts_size_temp[counts_window == 0] = 0\n            # print(np.max(counts_size), "" max size"")\n            # print(np.sum(valid_places), value)\n            if value in list_labels:\n                # print(value, \'in list_labels\')\n                mean_counts_size.append(mean_counts_size_temp)\n                # print(np.sum(valid_places))\n                valid_places = np.where(final == 0, np.zeros_like(final),\n                                        valid_places)\n                final = np.copy(valid_places)\n                # print(np.sum(valid_places))\n                print(\'final calculated for value in list_labels\')\n            else:\n                list_counts.append(valid_places)\n        list_counts.append(final)\n        # print(len(list_counts))\n        print(\'final characterisation\')\n        # for i in range(0, len(list_counts)):\n        #     # print(final.shape, list_counts[i].shape, np.max(final), np.max(\n        #     #     list_counts[i]))\n        #     final += list_counts[i]\n        final = np.sum(np.asarray(list_counts), axis=0)\n        # print(final.shape, \'shape of final\', len(list_counts))\n        print(\'initialising candidates\', num_labels_add)\n        candidates = np.where(final >= num_labels_add + 1, np.ones_like(final),\n                              np.zeros_like(final))\n        # candidates[final >= num_labels_add + 1] = 1\n        print(np.sum(candidates), \'number of candidates\')\n        proba_fin = None\n        if constraint.proba_connected:\n            proba_fin = create_probability_weights(candidates, mean_counts_size)\n        return candidates, proba_fin\n\n\ndef create_probability_weights(candidates, mean_counts_size):\n    """"""\n    This functions creates the probability weighting given the valid\n    candidates and the size of connected components associated to this candidate\n    :param candidates: binary map of the valid candidates\n    :param mean_counts_size: counts attributed to each candidate\n    :return: probability map for the selection of any voxel as candidate\n    """"""\n    proba_weight = np.ones_like(candidates)\n    for i in range(0, len(mean_counts_size)):\n        # print(candidates.shape, mean_counts_size[i].shape, np.max(candidates),\n        #       np.max(mean_counts_size[i]))\n\n        possible_mean_count = np.nan_to_num(candidates * mean_counts_size[i])\n        max_count = np.ceil(np.max(possible_mean_count))\n        print(max_count , \'is max_count\')\n        unique, counts = np.unique(np.round(possible_mean_count),\n                                   return_counts=True)\n        reciprocal_hist = np.sum(counts[1:]) * np.reciprocal(\n            np.asarray(counts[1:], dtype=np.float32))\n        sum_rec = np.sum(reciprocal_hist)\n        proba_hist = np.divide(reciprocal_hist, sum_rec)\n        print(unique, counts, sum_rec, len(proba_hist))\n        e_start = unique[1:]\n        e_end = unique[2:]\n        e_end.tolist().append(max_count + 1)\n        # print(e_start.shape, e_end.shape, e_start[-1], e_end[-1], len(\n        #     proba_hist))\n        candidates_proba = np.zeros_like(candidates)\n        if len(unique) == 2:\n            proba_weight = np.ones_like(candidates, dtype=np.float32) * \\\n                           1.0/np.sum(candidates)\n            proba_weight = np.multiply(proba_weight, candidates)\n        else:\n            for (e_s, e_e, size) in zip(e_start, e_end, proba_hist):\n                prob_selector = \\\n                    (possible_mean_count >= e_s) & (possible_mean_count < e_e)\n                candidates_proba[prob_selector.astype(np.bool)] = size\n            proba_weight = np.multiply(proba_weight, candidates_proba)\n        print(""Finished probability calculation"")\n    return np.divide(proba_weight, np.sum(proba_weight))\n\n\ndef rand_choice_coordinates(subject_id,\n                            img_sizes,\n                            win_sizes,\n                            candidates,\n                            mean_counts_size=None,\n                            n_samples=1, n_samples_rand=0):\n    """"""\n    win_sizes could be different, for example in segmentation network\n    input image window size is 32x32x10,\n    training label window is 16x16x10, the network reduces x-y plane\n    spatial resolution.\n    This function handles this situation by first find the largest\n    window across these window definitions, and generate the coordinates.\n    These coordinates are then adjusted for each of the\n    smaller window sizes (the output windows are concentric).\n    """"""\n    print(n_samples)\n    n_samples_rand = np.min([n_samples_rand, n_samples-1])\n    n_samples_cand = n_samples - n_samples_rand\n    uniq_spatial_size = set([img_size[:N_SPATIAL]\n                             for img_size in list(img_sizes.values())])\n    if len(uniq_spatial_size) > 1:\n        tf.logging.fatal(""Don\'t know how to generate sampling ""\n                         ""locations: Spatial dimensions of the ""\n                         ""grouped input sources are not ""\n                         ""consistent. %s"", uniq_spatial_size)\n        raise NotImplementedError\n    uniq_spatial_size = uniq_spatial_size.pop()\n\n    # find spatial window location based on the largest spatial window\n    spatial_win_sizes = [win_size[:N_SPATIAL]\n                         for win_size in win_sizes.values()]\n    spatial_win_sizes = np.asarray(spatial_win_sizes, dtype=np.int32)\n    max_spatial_win = np.max(spatial_win_sizes, axis=0)\n    for i in range(0, N_SPATIAL):\n        assert uniq_spatial_size[i] >= max_spatial_win[i], \\\n            ""window size {} is larger than image size {}"".format(\n                max_spatial_win[i], uniq_spatial_size[i])\n\n    # randomly choose n_samples from candidate locations\n    candidates_indices = np.vstack(np.where(candidates == 1)).T\n    # print(np.min(candidates_indices), np.max(candidates_indices),\n    # len(candidates_indices))\n    list_indices_fin = np.arange(len(candidates_indices))\n    if mean_counts_size is not None:\n        # Probability weighting considered\n        print(np.sum(mean_counts_size), \'proba weighting considered\')\n        proba = [p for (c, p)\n                 in zip(candidates.flatten(), mean_counts_size.flatten())\n                 if c >= 1]\n        list_indices_fin = np.random.choice(\n            list_indices_fin, n_samples_cand, replace=False, p=proba)\n    else:\n        np.random.shuffle(list_indices_fin)\n        list_indices_fin = list_indices_fin[:n_samples_cand]\n    if n_samples_rand > 0:\n        spatial_win_sizes = np.asarray(win_sizes[:N_SPATIAL], dtype=np.int32)\n        max_spatial_win = spatial_win_sizes[0]\n        # Create segmentation for this label\n        # list_counts = []\n        shape_ones = np.asarray(candidates.shape)\n        # print(shape_ones, max_spatial_win)\n        half_max_size = np.floor(max_spatial_win / 2)\n        padding = []\n        for i in range(0, len(shape_ones)):\n            if i < N_SPATIAL:\n                shape_ones[i] -= 2 * half_max_size\n                padding = padding + [[half_max_size, half_max_size], ]\n            else:\n                padding = padding + [[0, 0], ]\n        # print(shape_ones, padding)\n        rand_one = np.pad(np.ones(shape_ones),\n                          np.asarray(padding, dtype=np.int32),\n                          \'constant\')\n        # rand_one = np.ones_like(candidates)\n        list_possible_rand = np.vstack(np.where(rand_one == 1)).T\n        list_indices_rand = np.random.choice(list_possible_rand, n_samples_rand,\n                                             replace=False)\n        list_indices_fin = np.concatenate((list_indices_fin, list_indices_rand))\n\n    max_coords = np.zeros((n_samples, N_SPATIAL), dtype=np.int32)\n    half_win = np.floor(np.asarray(win_sizes[\'image\']) / 2).astype(np.int)\n    for (i_sample, ind) in enumerate(list_indices_fin):\n        max_coords[i_sample, :N_SPATIAL] = \\\n            candidates_indices[ind][:N_SPATIAL] - half_win[:N_SPATIAL]\n    # print(np.min(max_coords), np.max(max_coords))\n    # adjust max spatial coordinates based on each spatial window size\n    all_coordinates = {}\n    for mod in list(win_sizes):\n        win_size = win_sizes[mod][:N_SPATIAL]\n        half_win_diff = np.floor((max_spatial_win - win_size) / 2.0)\n        # print(win_size, half_win_diff, \'win and half_Win_diff\')\n        # shift starting coords of the window\n        # so that smaller windows are centred within the large windows\n        spatial_coords = np.zeros((n_samples, N_SPATIAL * 2), dtype=np.int32)\n        spatial_coords[:, :N_SPATIAL] = \\\n            max_coords[:, :N_SPATIAL] + half_win_diff[:N_SPATIAL]\n        spatial_coords[:, N_SPATIAL:] = \\\n            spatial_coords[:, :N_SPATIAL] + win_size[:N_SPATIAL]\n        # include the subject id\n        subject_id = np.ones((n_samples,), dtype=np.int32) * subject_id\n        spatial_coords = np.append(\n            subject_id[:, None], spatial_coords, axis=1)\n        all_coordinates[mod] = spatial_coords\n    # print(all_coordinates)\n    return all_coordinates\n\n\nclass Constraint(object):\n    """"""\n    group of user specified constraints for choosing window samples\n    """"""\n\n    def __init__(self,\n                 compulsory_labels=(0, 1),\n                 min_ratio=0.000001,\n                 min_num_labels=2,\n                 flag_proba_uniform=False):\n        self.list_labels = compulsory_labels\n        self.min_ratio = min_ratio\n        self.num_labels = min_num_labels\n        self.proba_connected = flag_proba_uniform\n\n    def min_number_from_ratio(self, win_size):\n        """"""\n        number of voxels from ratio\n        :param win_size:\n        :return:\n        """"""\n        num_elements = np.prod(win_size)\n        print(num_elements, self.min_ratio)\n        min_num = np.ceil(self.min_ratio * num_elements)\n        return min_num\n'"
niftynet/contrib/segmentation_selective_sampler/ss_app.py,11,"b'import tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.contrib.segmentation_selective_sampler.sampler_selective import \\\n    SelectiveSampler, Constraint\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_segmentation import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'weight\', \'sampler\'])\n\n\nclass SelectiveSampling(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, is_training):\n        super(SelectiveSampling, self).__init__()\n        tf.logging.info(\'starting segmentation application\')\n        self.is_training = is_training\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.segmentation_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'selective\': (self.initialise_selective_sampler,\n                          self.initialise_grid_sampler,\n                          self.initialise_grid_aggregator)\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.segmentation_param = task_param\n\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        # read each line of csv files into an instance of Subject\n        if self.is_training:\n            self.readers = []\n            for file_list in file_lists:\n                reader = ImageReader(SUPPORTED_INPUT)\n                reader.initialise(data_param, task_param, file_list)\n                self.readers.append(reader)\n\n        else:  # in the inference process use image input only\n            inference_reader = ImageReader([\'image\'])\n            inference_reader.initialise(data_param, task_param, file_lists[0])\n            self.readers = [inference_reader]\n\n        foreground_masking_layer = None\n        if self.net_param.normalise_foreground_only:\n            foreground_masking_layer = BinaryMaskingLayer(\n                type_str=self.net_param.foreground_type,\n                multimod_fusion=self.net_param.multimod_foreground_type,\n                threshold=0.0)\n\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer)\n        histogram_normaliser = None\n        if self.net_param.histogram_ref_file:\n            histogram_normaliser = HistogramNormalisationLayer(\n                image_name=\'image\',\n                modalities=vars(task_param).get(\'image\'),\n                model_filename=self.net_param.histogram_ref_file,\n                binary_masking_func=foreground_masking_layer,\n                norm_type=self.net_param.norm_type,\n                cutoff=self.net_param.cutoff,\n                name=\'hist_norm_layer\')\n\n        label_normaliser = None\n        if self.net_param.histogram_ref_file:\n            label_normaliser = DiscreteLabelNormalisationLayer(\n                image_name=\'label\',\n                modalities=vars(task_param).get(\'label\'),\n                model_filename=self.net_param.histogram_ref_file)\n\n        normalisation_layers = []\n        if self.net_param.normalisation:\n            normalisation_layers.append(histogram_normaliser)\n        if self.net_param.whitening:\n            normalisation_layers.append(mean_var_normaliser)\n        if task_param.label_normalisation:\n            normalisation_layers.append(label_normaliser)\n\n        augmentation_layers = []\n        if self.is_training:\n            if self.action_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=self.action_param.random_flipping_axes))\n            if self.action_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=self.action_param.scaling_percentage[0],\n                    max_percentage=self.action_param.scaling_percentage[1]))\n            if self.action_param.rotation_angle or \\\n                    self.action_param.rotation_angle_x or \\\n                    self.action_param.rotation_angle_y or \\\n                    self.action_param.rotation_angle_z:\n                rotation_layer = RandomRotationLayer()\n                if self.action_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        self.action_param.rotation_angle)\n                else:\n                    rotation_layer.init_non_uniform_angle(\n                        self.action_param.rotation_angle_x,\n                        self.action_param.rotation_angle_y,\n                        self.action_param.rotation_angle_z)\n                augmentation_layers.append(rotation_layer)\n\n        volume_padding_layer = [PadLayer(\n            image_name=SUPPORTED_INPUT,\n            border=self.net_param.volume_padding_size,\n            mode=self.net_param.volume_padding_mode,\n            pad_to=self.net_param.volume_padding_to_size)\n        ]\n\n        for reader in self.readers:\n            reader.add_preprocessing_layers(\n                volume_padding_layer +\n                normalisation_layers +\n                augmentation_layers)\n\n    def initialise_selective_sampler(self):\n        # print(""Initialisation "",\n        #       self.segmentation_param.compulsory_labels,\n        #       self.segmentation_param.proba_connect)\n        # print(self.segmentation_param.num_min_labels,\n        #       self.segmentation_param.proba_connect)\n        selective_constraints = Constraint(\n            self.segmentation_param.compulsory_labels,\n            self.segmentation_param.min_sampling_ratio,\n            self.segmentation_param.min_numb_labels,\n            self.segmentation_param.proba_connect)\n        self.sampler = [[\n            SelectiveSampler(\n                reader=reader,\n                data_param=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=self.action_param.sample_per_volume,\n                constraint=selective_constraints,\n                random_windows_per_image=self.segmentation_param.rand_samples,\n                queue_length=self.net_param.queue_length)\n            for reader in self.readers]]\n\n    def initialise_grid_sampler(self):\n        self.sampler = [[GridSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_aggregator(self):\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[\'selective\'][0]()\n        else:\n            self.SUPPORTED_SAMPLING[\'selective\'][1]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=self.segmentation_param.num_classes,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_out = self.net(image, is_training=self.is_training)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(\n                n_class=self.segmentation_param.num_classes,\n                loss_type=self.action_param.loss_type)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None),\n                weight_map=data_dict.get(\'weight\', None))\n            reg_losses = tf.get_collection(\n                tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n            grads = self.optimiser.compute_gradients(loss)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'dice_loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'dice_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            # outputs_collector.add_to_collection(\n            #    var=image*180.0, name=\'image\',\n            #    average_over_devices=False, summary_type=\'image3_sagittal\',\n            #    collection=TF_SUMMARIES)\n\n            # outputs_collector.add_to_collection(\n            #    var=image, name=\'image\',\n            #    average_over_devices=False,\n            #    collection=NETWORK_OUTPUT)\n\n            # outputs_collector.add_to_collection(\n            #    var=tf.reduce_mean(image), name=\'mean_image\',\n            #    average_over_devices=False, summary_type=\'scalar\',\n            #    collection=CONSOLE)\n        else:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_out = self.net(image, is_training=self.is_training)\n\n            output_prob = self.segmentation_param.output_prob\n            num_classes = self.segmentation_param.num_classes\n            if output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'SOFTMAX\', num_classes=num_classes)\n            elif not output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'ARGMAX\', num_classes=num_classes)\n            else:\n                post_process_layer = PostProcessingLayer(\n                    \'IDENTITY\', num_classes=num_classes)\n            net_out = post_process_layer(net_out)\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            init_aggregator = \\\n                self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]\n            init_aggregator()\n\n    def interpret_output(self, batch_output):\n        if not self.is_training:\n            return self.output_decoder.decode_batch(\n                {\'window_image\': batch_output[\'window\']},\n                batch_output[\'location\'])\n        return True\n'"
niftynet/contrib/segmentation_selective_sampler/test_sampler_selective.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.contrib.segmentation_selective_sampler.sampler_selective import \\\n    SelectiveSampler, Constraint\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\n\n### utility function for testing purposes\ndef check_constraint(data, constraint):\n    unique, count = np.unique(np.round(data), return_counts=True)\n    list_labels = []\n    data = np.round(data)\n    if constraint.list_labels is not None:\n        list_labels = constraint.list_labels\n        for label in constraint.list_labels:\n            if label not in unique:\n                print(\'Label %d is not there\' % label)\n                return False\n    num_labels_add = 0\n    if constraint.num_labels > 0:\n        num_labels_add = constraint.num_labels - len(list_labels)\n        if num_labels_add <= 0:\n            num_labels_add = 0\n        if len(unique) < constraint.num_labels:\n            print(\'Missing labels\')\n            return False\n    to_add = num_labels_add\n    if constraint.min_ratio > 0:\n        num_min = constraint.min_number_from_ratio(data.shape)\n        print(\'unique in test is \', unique)\n        for value, c in zip(unique, count):\n            if value in list_labels:\n                if c < num_min:\n                    print(\'Not enough in label %d\', value)\n                    return False\n            else:\n                if c > num_min:\n                    to_add -= 1\n        if to_add > 0:\n            print(\'to add initial is \', num_labels_add)\n            print(\'Not enough in additional labels\')\n            return False\n    return True\n\n\nMULTI_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    ),\n    \'Label\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'lesion.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'LesionFin_\',),\n        filename_not_constains=(\'FLAIR_\',),\n        interp_order=1,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    )\n}\nLABEL_TASK = {\n    \'Lesion\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'lesion.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'LesionFin_\'),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(7, 10, 2),\n        loader=None\n    )\n}\nMULTI_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'), label=(\'Label\',))\n\nMOD_2D_DATA = {\n    \'ultrasound\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler2d.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'2d_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(10, 9, 1),\n        loader=None\n    ),\n}\nMOD_2D_TASK = ParserNamespace(image=(\'ultrasound\',))\n\nDYNAMIC_MOD_DATA = {\n    \'T1\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'T1sampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'_o_T1_time\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    ),\n    \'FLAIR\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'FLAIRsampler.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'FLAIR_\',),\n        filename_not_contains=(\'Parcellation\',),\n        interp_order=3,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    ),\n    \'Label\': ParserNamespace(\n        csv_file=os.path.join(\'testing_data\', \'labels.csv\'),\n        path_to_search=\'testing_data\',\n        filename_contains=(\'T1_\', \'_NeuroMorph_Parcellation\',),\n        filename_not_constains=(\'FLAIR_\',),\n        interp_order=1,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(8, 2),\n        loader=None\n    )\n}\n\n# LABEL_TASK = {\n#     \'Parcellation\': ParserNamespace(\n#         csv_file=os.path.join(\'testing_data\', \'labels.csv\'),\n#         path_to_search=\'testing_data\',\n#         filename_contains=(\'Parcellation\',),\n#         filename_not_constains=(\'FLAIR_\',),\n#         interp_order=1,\n#         pixdim=None,\n#         axcodes=None,\n#         spatial_window_size=(8,2)\n#     )\n# }\n\nDYNAMIC_MOD_TASK = ParserNamespace(image=(\'T1\', \'FLAIR\'), label=(\'Label\',))\n\ndata_partitioner = ImageSetsPartitioner()\n\n\ndef get_3d_reader():\n    multi_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\n    print(MULTI_MOD_DATA, MULTI_MOD_TASK)\n    reader = ImageReader([\'image\', \'label\'])\n    reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n    return reader\n\n\ndef get_2d_reader():\n    mod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\n    reader = ImageReader([\'image\'])\n    reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n    return reader\n\n\ndef get_dynamic_window_reader():\n    dynamic_list = data_partitioner.initialise(DYNAMIC_MOD_DATA).get_file_list()\n    reader = ImageReader([\'image\', \'label\'])\n    reader.initialise(DYNAMIC_MOD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return reader\n\n\nclass SelectiveSamplerTest(NiftyNetTestCase):\n    def test_3d_init(self):\n        constraint_built = Constraint(compulsory_labels=[1],\n                                      min_ratio=0.000001,\n                                      min_num_labels=2)\n        sampler = SelectiveSampler(reader=get_3d_reader(),\n                                   data_param=MULTI_MOD_DATA,\n                                   batch_size=2,\n                                   constraint=constraint_built,\n                                   windows_per_image=2,\n                                   queue_length=10)\n        with self.cached_session() as sess:\n            sampler.run_threads(sess, num_threads=1)\n            out = sess.run(sampler.pop_batch_op())\n            self.assertTrue(check_constraint(out[\'label\'], constraint_built))\n            self.assertAllClose(out[\'image\'].shape, (2, 7, 10, 2, 2))\n            self.assertAllClose(out[\'label\'].shape, (2, 7, 10, 2, 1))\n            print(""Test should finish here"")\n        sampler.close_all()\n\n    # def test_2d_init(self):\n    #     sampler = UniformSampler(reader=get_2d_reader(),\n    #                              data_param=MOD_2D_DATA,\n    #                              batch_size=2,\n    #                              windows_per_image=10,\n    #                              queue_length=10)\n    #     with self.cached_session() as sess:\n    #         sampler.run_threads(sess, num_threads=2)\n    #         out = sess.run(sampler.pop_batch_op())\n    #         self.assertAllClose(out[\'image\'].shape, (2, 10, 9, 1))\n    #     sampler.close_all()\n\n    # def test_dynamic_init(self):\n    #     sampler = SelectiveSampler(reader=get_dynamic_window_reader(),\n    #                                data_param=DYNAMIC_MOD_DATA,\n    #                                batch_size=2,\n    #                                constraint=Constraint(\n    #                                    compulsory_labels=[1],\n    #                                    min_ratio=0.000001,\n    #                                    min_num_labels=2),\n    #                                windows_per_image=2,\n    #                                queue_length=2)\n    #     with self.cached_session() as sess:\n    #         sampler.run_threads(sess, num_threads=2)\n    #         out = sess.run(sampler.pop_batch_op())\n    #         test = np.zeros_like(out[\'label\'])\n    #         test[out[\'label\'] == 1] = 1\n    #         print(\'Number label 52 is \', np.sum(test))\n    #         print(out[\'image_location\'], out[\'label\'].shape)\n    #         self.assertAllClose(out[\'image\'].shape, (1, 8, 2, 256, 2))\n    #         self.assertAllClose(out[\'label\'].shape, (1, 8, 2, 256, 1))\n    #     sampler.close_all()\n\n    # def test_ill_init(self):\n    #    with self.assertRaisesRegexp(KeyError, """"):\n    #        sampler = SelectiveSampler(reader=get_3d_reader(),\n    #                                   data_param=MOD_2D_DATA,\n    #                                   batch_size=2,\n    #                                   windows_per_image=10,\n    #                                   queue_length=10)\n\n    # def test_close_early(self):\n    #    sampler = SelectiveSampler(reader=get_3d_reader(),\n    #                               data_param=DYNAMIC_MOD_DATA,\n    #                               batch_size=2,\n    #                               windows_per_image=10,\n    #                               queue_length=10)\n    #    sampler.close_all()\n\n\n# class RandomCoordinatesTest(NiftyNetTestCase):\n#     def test_coordinates(self):\n#         coords = rand_choice_coordinates(\n#             subject_id=1,\n#             img_sizes={\'image\': (42, 42, 42, 1, 2),\n#                        \'label\': (42, 42, 42, 1, 1)},\n#             win_sizes={\'image\': (23, 23, 40),\n#                        \'label\': (40, 32, 33)},\n#             candidates=np.round(np.random.random((256,256,168,1,1))),\n#             n_samples=10,\n#             mean_counts_size=None)\n#         self.assertEquals(np.all(coords[\'image\'][:0] == 1), True)\n#         self.assertEquals(coords[\'image\'].shape, (10, 7))\n#         self.assertEquals(coords[\'label\'].shape, (10, 7))\n#         self.assertAllClose(\n#             (coords[\'image\'][:, 4] + coords[\'image\'][:, 1]),\n#             (coords[\'label\'][:, 4] + coords[\'label\'][:, 1]), atol=1.0)\n#         self.assertAllClose(\n#             (coords[\'image\'][:, 5] + coords[\'image\'][:, 2]),\n#             (coords[\'label\'][:, 5] + coords[\'label\'][:, 2]), atol=1.0)\n#         self.assertAllClose(\n#             (coords[\'image\'][:, 6] + coords[\'image\'][:, 3]),\n#             (coords[\'label\'][:, 6] + coords[\'label\'][:, 3]), atol=1.0)\n#\n#\n#     def test_ill_coordinates(self):\n#         with self.assertRaisesRegexp(IndexError, """"):\n#             coords = rand_choice_coordinates(\n#                 subject_id=1,\n#                 img_sizes={\'image\': (42, 42, 1, 1, 1),\n#                            \'label\': (42, 42, 1, 1, 1)},\n#                 win_sizes={\'image\': (23, 23),\n#                            \'label\': (40, 32)},\n#                 candidates=np.round(np.random.random((256,256,168,1,1))),\n#                 n_samples=10,\n#                 mean_counts_size=None)\n#\n#         with self.assertRaisesRegexp(TypeError, """"):\n#             coords = rand_choice_coordinates(\n#                 subject_id=1,\n#                 img_sizes={\'image\': (42, 42, 1, 1, 1),\n#                            \'label\': (42, 42, 1, 1, 1)},\n#                 win_sizes={\'image\': (23, 23, 1),\n#                            \'label\': (40, 32, 1)},\n#                 candidates=np.round(np.random.random((256, 256, 168, 1, 1))),\n#                 n_samples=\'test\',\n#                 mean_counts_size=None)\n#\n#         with self.assertRaisesRegexp(AssertionError, """"):\n#             coords = rand_choice_coordinates(\n#                 subject_id=1,\n#                 img_sizes={\'label\': (42, 1, 1, 1)},\n#                 win_sizes={\'image\': (23, 23, 1)},\n#                 candidates=np.round(np.random.random((256, 256, 168, 1, 1))),\n#                 n_samples=0,\n#                 mean_counts_size=None)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
niftynet/contrib/ultrasound_simulator_gan/__init__.py,0,b''
niftynet/contrib/ultrasound_simulator_gan/ultrasound_simulator_gan.py,44,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom niftynet.layer.convolution import ConvolutionalLayer\nfrom niftynet.layer.deconvolution import DeconvolutionalLayer\nfrom niftynet.layer.gan_blocks import GANImageBlock, BaseGenerator, BaseDiscriminator\n\n\nclass SimulatorGAN(GANImageBlock):\n    def __init__(self, load_from_checkpoint=True, name=\'simulator_GAN\'):\n        generator = ImageGenerator(load_from_checkpoint=load_from_checkpoint, name=\'generator\')\n        discriminator = ImageDiscriminator(load_from_checkpoint=load_from_checkpoint, name=\'discriminator\')\n        super(SimulatorGAN, self).__init__(generator, discriminator, clip=None, name=name)\n\n\nclass ImageGenerator(BaseGenerator):\n    def __init__(self, load_from_checkpoint=True, name=\'generator\'):\n        self.load_from_checkpoint = load_from_checkpoint\n        super(ImageGenerator, self).__init__(name=name)\n        self.initializers = {\'w\': tf.contrib.layers.variance_scaling_initializer(), \'b\': tf.constant_initializer(0)}\n        self.noise_channels_per_layer = 0\n        self.generator_shortcuts = [True, True, True, True, False]\n\n    def layer_op(self, random_source, image_size, conditioning, is_training):\n        spatial_rank = len(image_size) - 1\n        add_noise = self.noise_channels_per_layer\n        conditioning_channels = conditioning.shape.as_list()[\n                                    -1] + add_noise if not conditioning is None else add_noise\n        batch_size = random_source.shape.as_list()[0]\n        noise_size = random_source.shape.as_list()[1]\n\n        w_init = tf.random_normal_initializer(0, 0.02)\n        b_init = tf.constant_initializer(0.001)\n        ch = [512]\n        sz = [[160, 120]]\n\n        keep_prob_ph = 1  # not passed in as a placeholder\n        for i in range(4):\n            ch.append(round((ch[-1] + conditioning_channels * self.generator_shortcuts[i]) / 2))\n            sz = [[round(i / 2) for i in sz[0]]] + sz\n        if spatial_rank == 3:\n            def resize_func(x, sz):\n                sz_x = x.shape.as_list()\n                r1 = tf.image.resize_images(tf.reshape(x, sz_x[:3] + [-1]), sz[0:2])\n                r2 = tf.image.resize_images(tf.reshape(r1, [sz_x[0], sz[0] * sz[1], sz_x[3], -1]),\n                                            [sz[0] * sz[1], sz[2]])\n                return tf.reshape(r2, [sz_x[0]] + sz + [sz_x[-1]])\n        elif spatial_rank == 2:\n            resize_func = tf.image.resize_bilinear\n\n        def concat_cond(x, i):\n            if add_noise:\n                noise = [tf.random_normal(x.shape.as_list()[0:-1] + [add_noise], 0, .1)]\n            else:\n                noise = []\n            if not conditioning is None and self.generator_shortcuts[i]:\n                with tf.name_scope(\'concat_conditioning\'):\n                    return tf.concat([x, resize_func(conditioning, x.shape.as_list()[1:-1])] + noise, axis=3)\n            else:\n                return x\n\n        def conv(ch, x):\n            with tf.name_scope(\'conv\'):\n                conv_layer = ConvolutionalLayer(ch, 3, feature_normalization=None, w_initializer=w_init)\n                c = conv_layer(x, is_training=is_training)\n                return tf.nn.relu(tf.contrib.layers.batch_norm(c))\n\n        def up(ch, x, hack=False):\n            with tf.name_scope(\'up\'):\n                deconv = DeconvolutionalLayer(ch, 3, feature_normalization=None, stride=2, w_initializer=w_init)(x, is_training=is_training)\n                if hack:\n                    deconv = deconv[:, :, 1:, :]  # hack to match Yipeng\'s image size\n                return tf.nn.relu(tf.contrib.layers.batch_norm(deconv))\n\n        def up_block(ch, x, i, hack=False):\n            with tf.name_scope(\'up_block\'):\n                cond = concat_cond(up(ch, x, hack), i)\n                return conv(cond.shape.as_list()[-1], cond)\n\n        with tf.name_scope(\'noise_to_image\'):\n            g_no_0 = np.prod(sz[0]) * ch[0]\n            w1p = tf.get_variable(""G_W1p"", shape=[noise_size, g_no_0], initializer=w_init)\n            b1p = tf.get_variable(\'G_b1p\', shape=[g_no_0], initializer=b_init)\n            g_h1p = tf.nn.dropout(tf.nn.relu(tf.matmul(random_source, w1p) + b1p), keep_prob_ph)\n            g_h1p = tf.reshape(g_h1p, [batch_size] + sz[0] + [ch[0]])\n            g_h1p = concat_cond(g_h1p, 0)\n            g_h1 = conv(ch[0] + conditioning_channels, g_h1p)\n        g_h2 = up_block(ch[1], g_h1, 1, hack=True)\n        g_h3 = up_block(ch[2], g_h2, 2)\n        g_h4 = up_block(ch[3], g_h3, 3)\n        g_h5 = up_block(ch[4], g_h4, 4)\n        with tf.name_scope(\'final_image\'):\n            if add_noise:\n                noise = tf.random_normal(g_h5.shape.as_list()[0:-1] + [add_noise], 0, .1)\n                g_h5 = tf.concat([g_h5, noise],axis=3)\n            x_sample = ConvolutionalLayer(1, 3, feature_normalization=None, with_bias=True,\n                                          w_initializer=w_init,\n                                          b_initializer=b_init)(g_h5, is_training=is_training)\n            x_sample = tf.nn.dropout(tf.nn.tanh(x_sample), keep_prob_ph)\n        if self.load_from_checkpoint:\n            checkpoint_name = \'/home/egibson/deeplearning/TensorFlow/NiftyNet/NiftyNet/\' + \\\n                              \'contrib/ultrasound_simulator_gan/ultrasound_simulator_gan\'\n            restores = [\n                [\'G_Wo\', \'generator/conv_5/conv_/w\'],\n                [\'G_W5t\', \'generator/deconv_3/deconv_/w\'],\n                [\'G_W5\', \'generator/conv_4/conv_/w\'],\n                [\'G_W4t\', \'generator/deconv_2/deconv_/w\'],\n                [\'G_W4\', \'generator/conv_3/conv_/w\'],\n                [\'G_W3t\', \'generator/deconv_1/deconv_/w\'],\n                [\'G_W3\', \'generator/conv_2/conv_/w\'],\n                [\'G_W2t\', \'generator/deconv/deconv_/w\'],\n                [\'G_W2\', \'generator/conv_1/conv_/w\'],\n                [\'G_W1p\', \'generator/G_W1p\'],\n                [\'G_W1\', \'generator/conv/conv_/w\'],\n                [\'G_b1p\', \'generator/G_b1p\']]\n            [tf.add_to_collection(\'NiftyNetObjectsToRestore\', (r[1], checkpoint_name, r[0]))\n             for r in restores]\n        return x_sample\n\n\nclass ImageDiscriminator(BaseDiscriminator):\n    def __init__(self, load_from_checkpoint=True, name=\'discriminator\'):\n        self.load_from_checkpoint=load_from_checkpoint\n        super(ImageDiscriminator, self).__init__(name=name)\n        self.initializers = {\'w\': tf.contrib.layers.variance_scaling_initializer(),\n                             \'b\': tf.constant_initializer(0)}\n\n    def layer_op(self, image, conditioning, is_training):\n        conditioning = tf.image.resize_images(conditioning, [160, 120])\n\n        batch_size = image.shape.as_list()[0]\n\n        w_init = tf.random_normal_initializer(0, 0.02)\n        b_init = tf.constant_initializer(0.001)\n\n        def leaky_relu(x, alpha=0.2):\n            with tf.name_scope(\'leaky_relu\'):\n                return 0.5 * (1 + alpha) * x + 0.5 * (1 - alpha) * abs(x)\n\n        ch = [32, 64, 128, 256, 512, 1024]\n\n        def down(ch, x):\n            with tf.name_scope(\'downsample\'):\n                c = ConvolutionalLayer(ch, 3, stride=2, feature_normalization=None,\n                                       w_initializer=w_init)(x, is_training=is_training)\n                c=tf.contrib.layers.batch_norm(c)\n                c = leaky_relu(c)\n                return c\n\n        def convr(ch, x):\n            c= ConvolutionalLayer(ch, 3, feature_normalization=None,\n                                  w_initializer=w_init)(x, is_training=is_training)\n            return leaky_relu(tf.contrib.layers.batch_norm(c))\n        def conv(ch, x, s):\n            c = (ConvolutionalLayer(ch, 3, feature_normalization=None,\n                                      w_initializer=w_init)(x, is_training=is_training))\n            return leaky_relu(tf.contrib.layers.batch_norm(c) + s)\n\n        def down_block(ch, x):\n            with tf.name_scope(\'down_resnet\'):\n                s = down(ch, x)\n                r = convr(ch, s)\n                return conv(ch, r, s)\n\n        if not conditioning is None:\n            image = tf.concat([image, conditioning], axis=-1)\n        with tf.name_scope(\'feature\'):\n            d_h1s = ConvolutionalLayer(ch[0], 5, with_bias=True,\n                                                  feature_normalization=None,\n                                                  w_initializer=w_init,\n                                                  b_initializer=b_init)(image,\n                                                                        is_training=is_training)\n\n            d_h1s = leaky_relu(d_h1s)\n            d_h1r = convr(ch[0], d_h1s)\n            d_h1 = conv(ch[0], d_h1r, d_h1s)\n        d_h2 = down_block(ch[1], d_h1)\n        d_h3 = down_block(ch[2], d_h2)\n        d_h4 = down_block(ch[3], d_h3)\n        d_h5 = down_block(ch[4], d_h4)\n        d_h6 = down_block(ch[5], d_h5)\n        with tf.name_scope(\'fc\'):\n            d_hf = tf.reshape(d_h6, [batch_size, -1])\n            d_nf_o = np.prod(d_hf.shape.as_list()[1:])\n            D_Wo = tf.get_variable(""D_Wo"", shape=[d_nf_o, 1], initializer=w_init)\n            D_bo = tf.get_variable(\'D_bo\', shape=[1], initializer=b_init)\n\n            d_logit = tf.matmul(d_hf, D_Wo) + D_bo\n        if self.load_from_checkpoint:\n            checkpoint_name = \'/home/egibson/deeplearning/TensorFlow/NiftyNet/NiftyNet/\' + \\\n                              \'contrib/ultrasound_simulator_gan/ultrasound_simulator_gan\'\n            restores = [\n                [\'D_b1s\', \'discriminator/conv/conv_/b\'],\n                [\'D_W1s\', \'discriminator/conv/conv_/w\'],\n                [\'D_W1r1\', \'discriminator/conv_1/conv_/w\'],\n                [\'D_W1r2\', \'discriminator/conv_2/conv_/w\'],\n                [\'D_W2s\', \'discriminator/conv_3/conv_/w\'],\n                [\'D_W2r1\', \'discriminator/conv_4/conv_/w\'],\n                [\'D_W2r2\', \'discriminator/conv_5/conv_/w\'],\n                [\'D_W3s\', \'discriminator/conv_6/conv_/w\'],\n                [\'D_W3r1\', \'discriminator/conv_7/conv_/w\'],\n                [\'D_W3r2\', \'discriminator/conv_8/conv_/w\'],\n                [\'D_W4s\', \'discriminator/conv_9/conv_/w\'],\n                [\'D_W4r1\', \'discriminator/conv_10/conv_/w\'],\n                [\'D_W4r2\', \'discriminator/conv_11/conv_/w\'],\n                [\'D_W5s\', \'discriminator/conv_12/conv_/w\'],\n                [\'D_W5r1\', \'discriminator/conv_13/conv_/w\'],\n                [\'D_W5r2\', \'discriminator/conv_14/conv_/w\'],\n                [\'D_W6s\', \'discriminator/conv_15/conv_/w\'],\n                [\'D_W6r1\', \'discriminator/conv_16/conv_/w\'],\n                [\'D_W6r2\', \'discriminator/conv_17/conv_/w\'],\n                [\'D_Wo\', \'discriminator/D_Wo\'],\n                [\'D_bo\', \'discriminator/D_bo\']]\n            [tf.add_to_collection(\'NiftyNetObjectsToRestore\', (r[1], checkpoint_name, r[0]))\n             for r in restores]\n        return d_logit\n'"
niftynet/contrib/csv_reader/applications_maybe/__init__.py,0,b''
niftynet/contrib/csv_reader/applications_maybe/autoencoder_application.py,15,"b'import tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import ApplicationNetFactory\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import CONSOLE\nfrom niftynet.engine.application_variables import NETWORK_OUTPUT\nfrom niftynet.engine.application_variables import TF_SUMMARIES\nfrom niftynet.engine.sampler_linear_interpolate_v2 import LinearInterpolateSampler\nfrom niftynet.contrib.csv_reader.sampler_linear_interpolate_v2_csv import \\\n    LinearInterpolateSamplerCSV as LinearInterpolateSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.contrib.csv_reader.sampler_resize_v2_csv import \\\n    ResizeSamplerCSV as ResizeSampler\nfrom niftynet.contrib.csv_reader.csv_reader import CSVReader\nfrom niftynet.engine.windows_aggregator_identity import WindowAsImageAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.loss_autoencoder import LossFunction\nfrom niftynet.utilities.util_common import look_up_operations\n\nSUPPORTED_INPUT = set([\'image\', \'feature\'])\nSUPPORTED_INFERENCE = \\\n    set([\'encode\', \'encode-decode\', \'sample\', \'linear_interpolation\'])\n\n\nclass AutoencoderApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""AUTOENCODER""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting autoencoder application\')\n\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.autoencoder_param = None\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        self.data_param = data_param\n        self.autoencoder_param = task_param\n\n        if not self.is_training:\n            self._infer_type = look_up_operations(\n                self.autoencoder_param.inference_type, SUPPORTED_INFERENCE)\n        else:\n            self._infer_type = None\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        # read each line of csv files into an instance of Subject\n        if self.is_evaluation:\n            NotImplementedError(\'Evaluation is not yet \'\n                                \'supported in this application.\')\n        if self.is_training:\n            self.readers = []\n            self.csv_reader = []\n            for file_list in file_lists:\n                reader = ImageReader([\'image\'])\n                reader.initialise(data_param, task_param, file_list)\n                self.readers.append(reader)\n        if self._infer_type in (\'encode\', \'encode-decode\'):\n            self.readers = [ImageReader([\'image\'])]\n            self.readers[0].initialise(data_param, task_param, file_lists[0])\n        elif self._infer_type == \'sample\':\n            self.readers = []\n            self.csv_reader = []\n        elif self._infer_type == \'linear_interpolation\':\n            self.csv_reader = []\n            self.readers = [ImageReader([\'feature\'])]\n            self.readers[0].initialise(data_param, task_param, file_lists[0])\n        # if self.is_training or self._infer_type in (\'encode\', \'encode-decode\'):\n        #    mean_var_normaliser = MeanVarNormalisationLayer(image_name=\'image\')\n        #    self.reader.add_preprocessing_layers([mean_var_normaliser])\n\n    def initialise_sampler(self):\n        self.sampler = []\n        if self.is_training:\n            self.sampler.append([ResizeSampler(\n                reader=reader,\n                csv_reader=None,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=1,\n                shuffle=True,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n            return\n        if self._infer_type in (\'encode\', \'encode-decode\'):\n            self.sampler.append([ResizeSampler(\n                reader=reader,\n                csv_reader=None,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=1,\n                shuffle=False,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n            return\n        if self._infer_type == \'linear_interpolation\':\n            self.sampler.append([LinearInterpolateSampler(\n                reader=reader,\n                csv_reader=None,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                n_interpolations=self.autoencoder_param.n_interpolations,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n            return\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            self.patience = self.action_param.patience\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(True),\n                                    lambda: switch_sampler(False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_output = self.net(image, is_training=self.is_training)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n\n            loss_func = LossFunction(loss_type=self.action_param.loss_type)\n            data_loss = loss_func(net_output)\n            loss = data_loss\n            if self.net_param.decay > 0.0:\n                reg_losses = tf.get_collection(\n                    tf.GraphKeys.REGULARIZATION_LOSSES)\n                if reg_losses:\n                    reg_loss = tf.reduce_mean(\n                        [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                    loss = loss + reg_loss\n\n            self.total_loss = loss\n            grads = self.optimiser.compute_gradients(\n                loss, colocate_gradients_with_ops=True)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'variational_lower_bound\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'variational_lower_bound\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            outputs_collector.add_to_collection(\n                var=net_output[4], name=\'Originals\',\n                average_over_devices=False, summary_type=\'image3_coronal\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=net_output[2], name=\'Means\',\n                average_over_devices=False, summary_type=\'image3_coronal\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=net_output[5], name=\'Variances\',\n                average_over_devices=False, summary_type=\'image3_coronal\',\n                collection=TF_SUMMARIES)\n        else:\n            if self._infer_type in (\'encode\', \'encode-decode\'):\n                data_dict = self.get_sampler()[0][0].pop_batch_op()\n                image = tf.cast(data_dict[\'image\'], dtype=tf.float32)\n                net_output = self.net(image, is_training=False)\n\n                outputs_collector.add_to_collection(\n                    var=data_dict[\'image_location\'], name=\'location\',\n                    average_over_devices=True, collection=NETWORK_OUTPUT)\n\n                if self._infer_type == \'encode-decode\':\n                    outputs_collector.add_to_collection(\n                        var=net_output[2], name=\'generated_image\',\n                        average_over_devices=True, collection=NETWORK_OUTPUT)\n                if self._infer_type == \'encode\':\n                    outputs_collector.add_to_collection(\n                        var=net_output[7], name=\'embedded\',\n                        average_over_devices=True, collection=NETWORK_OUTPUT)\n\n                self.output_decoder = WindowAsImageAggregator(\n                    image_reader=self.readers[0],\n                    output_path=self.action_param.save_seg_dir)\n                return\n            elif self._infer_type == \'sample\':\n                image_size = (self.net_param.batch_size,) + \\\n                             self.action_param.spatial_window_size + (1,)\n                dummy_image = tf.zeros(image_size)\n                net_output = self.net(dummy_image, is_training=False)\n                noise_shape = net_output[-1].shape.as_list()\n                stddev = self.autoencoder_param.noise_stddev\n                noise = tf.random_normal(shape=noise_shape,\n                                         mean=0.0,\n                                         stddev=stddev,\n                                         dtype=tf.float32)\n                partially_decoded_sample = self.net.shared_decoder(\n                    noise, is_training=False)\n                decoder_output = self.net.decoder_means(\n                    partially_decoded_sample, is_training=False)\n\n                outputs_collector.add_to_collection(\n                    var=decoder_output, name=\'generated_image\',\n                    average_over_devices=True, collection=NETWORK_OUTPUT)\n                self.output_decoder = WindowAsImageAggregator(\n                    image_reader=None,\n                    output_path=self.action_param.save_seg_dir)\n                return\n            elif self._infer_type == \'linear_interpolation\':\n                # construct the entire network\n                image_size = (self.net_param.batch_size,) + \\\n                             self.action_param.spatial_window_size + (1,)\n                dummy_image = tf.zeros(image_size)\n                net_output = self.net(dummy_image, is_training=False)\n                data_dict = self.get_sampler()[0][0].pop_batch_op()\n                real_code = data_dict[\'feature\']\n                real_code = tf.reshape(real_code, net_output[-1].get_shape())\n                partially_decoded_sample = self.net.shared_decoder(\n                    real_code, is_training=False)\n                decoder_output = self.net.decoder_means(\n                    partially_decoded_sample, is_training=False)\n\n                outputs_collector.add_to_collection(\n                    var=decoder_output, name=\'generated_image\',\n                    average_over_devices=True, collection=NETWORK_OUTPUT)\n                outputs_collector.add_to_collection(\n                    var=data_dict[\'feature_location\'], name=\'location\',\n                    average_over_devices=True, collection=NETWORK_OUTPUT)\n                self.output_decoder = WindowAsImageAggregator(\n                    image_reader=self.readers[0],\n                    output_path=self.action_param.save_seg_dir)\n            else:\n                raise NotImplementedError\n\n    def interpret_output(self, batch_output):\n        if self.is_training:\n            return True\n        else:\n            infer_type = look_up_operations(\n                self.autoencoder_param.inference_type,\n                SUPPORTED_INFERENCE)\n            if infer_type == \'encode\':\n                return self.output_decoder.decode_batch(\n                    {\'window_embedded\':batch_output[\'embedded\']},\n                    batch_output[\'location\'][:, 0:1])\n            if infer_type == \'encode-decode\':\n                return self.output_decoder.decode_batch(\n                    {\'window_generated_image\':batch_output[\'generated_image\']},\n                    batch_output[\'location\'][:, 0:1])\n            if infer_type == \'sample\':\n                return self.output_decoder.decode_batch(\n                    {\'generated_image\':batch_output[\'generated_image\']},\n                    None)\n            if infer_type == \'linear_interpolation\':\n                return self.output_decoder.decode_batch(\n                    {\'generated_image\':batch_output[\'generated_image\']},\n                    batch_output[\'location\'][:, :2])\n'"
niftynet/contrib/csv_reader/applications_maybe/gan_application.py,19,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import ApplicationNetFactory\nfrom niftynet.engine.application_factory import OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_random_vector_v2 import RandomVectorSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.contrib.csv_reader.sampler_resize_v2_csv import \\\n    ResizeSamplerCSV as ResizeSampler\nfrom niftynet.engine.windows_aggregator_identity import WindowAsImageAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_gan import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\n\nSUPPORTED_INPUT = set([\'image\', \'conditioning\'])\n\n\nclass GANApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""GAN""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting GAN application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.gan_param = None\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        self.data_param = data_param\n        self.gan_param = task_param\n\n        if self.is_training:\n            reader_names = (\'image\', \'conditioning\')\n        elif self.is_inference:\n            # in the inference process use `conditioning` input only\n            reader_names = (\'conditioning\',)\n        elif self.is_evaluation:\n            tf.logging.fatal(\n                \'Evaluation is not yet supported in this application.\')\n            raise NotImplementedError\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        # initialise input preprocessing layers\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=self.net_param.foreground_type,\n            multimod_fusion=self.net_param.multimod_foreground_type,\n            threshold=0.0) \\\n            if self.net_param.normalise_foreground_only else None\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer) \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            binary_masking_func=foreground_masking_layer,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n\n        # initialise training data augmentation layers\n        augmentation_layers = []\n        if self.is_training:\n            if self.action_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=self.action_param.random_flipping_axes))\n            if self.action_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=self.action_param.scaling_percentage[0],\n                    max_percentage=self.action_param.scaling_percentage[1],\n                    antialiasing=self.action_param.antialiasing,\n                    isotropic=self.action_param.isotropic_scaling))\n            if self.action_param.rotation_angle:\n                augmentation_layers.append(RandomRotationLayer())\n                augmentation_layers[-1].init_uniform_angle(\n                    self.action_param.rotation_angle)\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n            normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(normalisation_layers)\n\n    def initialise_sampler(self):\n        self.sampler = []\n        if self.is_training:\n            self.sampler.append([ResizeSampler(\n                reader=reader,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=1,\n                shuffle=True,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n        else:\n            self.sampler.append([RandomVectorSampler(\n                names=(\'vector\',),\n                vector_size=(self.gan_param.noise_size,),\n                batch_size=self.net_param.batch_size,\n                n_interpolations=self.gan_param.n_interpolations,\n                repeat=None,\n                queue_length=self.net_param.queue_length) for _ in\n                self.readers])\n            # repeat each resized image n times, so that each\n            # image matches one random vector,\n            # (n = self.gan_param.n_interpolations)\n            self.sampler.append([ResizeSampler(\n                reader=reader,\n                window_sizes=self.data_param,\n                batch_size=self.net_param.batch_size,\n                windows_per_image=self.gan_param.n_interpolations,\n                shuffle=False,\n                queue_length=self.net_param.queue_length) for reader in\n                self.readers])\n\n    def initialise_network(self):\n        self.net = ApplicationNetFactory.create(self.net_param.name)()\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n        if self.is_training:\n            self.patience = self.action_param.patience\n\n            def switch_sampler(for_training):\n                with tf.name_scope(\'train\' if for_training else \'validation\'):\n                    sampler = self.get_sampler()[0][0 if for_training else -1]\n                    return sampler.pop_batch_op()\n\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            images = tf.cast(data_dict[\'image\'], tf.float32)\n            noise_shape = [self.net_param.batch_size,\n                           self.gan_param.noise_size]\n            noise = tf.random_normal(shape=noise_shape,\n                                     mean=0.0,\n                                     stddev=1.0,\n                                     dtype=tf.float32)\n            conditioning = data_dict[\'conditioning\']\n            net_output = self.net(\n                noise, images, conditioning, self.is_training)\n\n            loss_func = LossFunction(\n                loss_type=self.action_param.loss_type)\n            real_logits = net_output[1]\n            fake_logits = net_output[2]\n            lossG, lossD = loss_func(real_logits, fake_logits)\n            if self.net_param.decay > 0:\n                reg_losses = tf.get_collection(\n                    tf.GraphKeys.REGULARIZATION_LOSSES)\n                if reg_losses:\n                    reg_loss = tf.reduce_mean(\n                        [tf.reduce_mean(l_reg) for l_reg in reg_losses])\n                    lossD = lossD + reg_loss\n                    lossG = lossG + reg_loss\n\n            self.total_loss = lossD + lossG\n\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            # variables to display in STDOUT\n            outputs_collector.add_to_collection(\n                var=lossD, name=\'lossD\', average_over_devices=True,\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=lossG, name=\'lossG\', average_over_devices=False,\n                collection=CONSOLE)\n            # variables to display in tensorboard\n            outputs_collector.add_to_collection(\n                var=lossG, name=\'lossG\', average_over_devices=False,\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=lossG, name=\'lossD\', average_over_devices=True,\n                collection=TF_SUMMARIES)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n\n            with tf.name_scope(\'ComputeGradients\'):\n                # gradients of generator\n                generator_variables = tf.get_collection(\n                    tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'generator\')\n                generator_grads = self.optimiser.compute_gradients(\n                    lossG,\n                    var_list=generator_variables,\n                    colocate_gradients_with_ops=True)\n\n                # gradients of discriminator\n                discriminator_variables = tf.get_collection(\n                    tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'discriminator\')\n                discriminator_grads = self.optimiser.compute_gradients(\n                    lossD,\n                    var_list=discriminator_variables,\n                    colocate_gradients_with_ops=True)\n                grads = [generator_grads, discriminator_grads]\n\n                # add the grads back to application_driver\'s training_grads\n                gradients_collector.add_to_collection(grads)\n        else:\n            data_dict = self.get_sampler()[0][0].pop_batch_op()\n            conditioning_dict = self.get_sampler()[1][0].pop_batch_op()\n            conditioning = conditioning_dict[\'conditioning\']\n            image_size = conditioning.shape.as_list()[:-1]\n            dummy_image = tf.zeros(image_size + [1])\n            net_output = self.net(data_dict[\'vector\'],\n                                  dummy_image,\n                                  conditioning,\n                                  self.is_training)\n            outputs_collector.add_to_collection(\n                var=net_output[0],\n                name=\'image\',\n                average_over_devices=False,\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=conditioning_dict[\'conditioning_location\'],\n                name=\'location\',\n                average_over_devices=False,\n                collection=NETWORK_OUTPUT)\n\n            self.output_decoder = WindowAsImageAggregator(\n                image_reader=self.readers[0],\n                output_path=self.action_param.save_seg_dir)\n\n    def interpret_output(self, batch_output):\n        if self.is_training:\n            return True\n        return self.output_decoder.decode_batch(\n            {\'window_image\': batch_output[\'image\']},\n            batch_output[\'location\'])\n'"
niftynet/contrib/csv_reader/applications_maybe/label_driven_registration.py,13,"b'""""""\nA preliminary re-implementation of:\n    Hu et al., Weakly-Supervised Convolutional Neural Networks for\n    Multimodal Image Registration, Medical Image Analysis (2018)\n    https://doi.org/10.1016/j.media.2018.07.002\n\nThe original implementation and tutorial is available at:\n    https://github.com/YipengHu/label-reg\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.contrib.sampler_pairwise.sampler_pairwise_uniform import \\\n    PairwiseUniformSampler\nfrom niftynet.contrib.sampler_pairwise.sampler_pairwise_resize import \\\n    PairwiseResizeSampler\nfrom niftynet.contrib.csv_reader.csv_reader import CSVReader\nfrom niftynet.engine.application_factory import \\\n    OptimiserFactory, ApplicationNetFactory\nfrom niftynet.engine.application_variables import \\\n    NETWORK_OUTPUT, CONSOLE, TF_SUMMARIES\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\n\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.loss_segmentation import LossFunction\n\n\nSUPPORTED_INPUT = {\'moving_image\', \'moving_label\',\n                   \'fixed_image\', \'fixed_label\'}\n\n\nclass RegApp(BaseApplication):\n\n    REQUIRED_CONFIG_SECTION = ""REGISTRATION""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting label-driven registration\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.registration_param = None\n        self.data_param = None\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n        self.data_param = data_param\n        self.registration_param = task_param\n\n        if self.is_evaluation:\n            NotImplementedError(\'Evaluation is not yet \'\n                                \'supported in this application.\')\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n\n        self.readers = []\n        for file_list in file_lists:\n            fixed_reader = ImageReader({\'fixed_image\', \'fixed_label\'})\n            fixed_reader.initialise(data_param, task_param, file_list)\n            self.readers.append(fixed_reader)\n\n            moving_reader = ImageReader({\'moving_image\', \'moving_label\'})\n            moving_reader.initialise(data_param, task_param, file_list)\n            self.readers.append(moving_reader)\n\n        # pad the fixed target only\n        # moving image will be resampled to match the targets\n        #volume_padding_layer = []\n        #if self.net_param.volume_padding_size:\n        #    volume_padding_layer.append(PadLayer(\n        #        image_name=(\'fixed_image\', \'fixed_label\'),\n        #        border=self.net_param.volume_padding_size))\n\n        #for reader in self.readers:\n        #    reader.add_preprocessing_layers(volume_padding_layer)\n\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.sampler = []\n            assert len(self.readers) >= 2, \'at least two readers are required\'\n            training_sampler = PairwiseUniformSampler(\n                reader_0=self.readers[0],\n                reader_1=self.readers[1],\n                data_param=self.data_param,\n                batch_size=self.net_param.batch_size)\n            self.sampler.append(training_sampler)\n            # adding validation readers if possible\n            if len(self.readers) >= 4:\n                validation_sampler = PairwiseUniformSampler(\n                    reader_0=self.readers[2],\n                    reader_1=self.readers[3],\n                    data_param=self.data_param,\n                    batch_size=self.net_param.batch_size)\n                self.sampler.append(validation_sampler)\n        else:\n            self.sampler = PairwiseResizeSampler(\n                reader_0=self.readers[0],\n                reader_1=self.readers[1],\n                data_param=self.data_param,\n                batch_size=self.net_param.batch_size)\n\n    def initialise_network(self):\n        decay = self.net_param.decay\n        self.net = ApplicationNetFactory.create(self.net_param.name)(decay)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_samplers(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0 if for_training else -1]\n                return sampler()  # returns image only\n\n        if self.is_training:\n            self.patience = self.action_param.patience\n            if self.action_param.validation_every_n > 0:\n                sampler_window = \\\n                    tf.cond(tf.logical_not(self.is_validation),\n                            lambda: switch_samplers(True),\n                            lambda: switch_samplers(False))\n            else:\n                sampler_window = switch_samplers(True)\n\n            image_windows, _ = sampler_window\n            # image_windows, locations = sampler_window\n\n            # decode channels for moving and fixed images\n            image_windows_list = [\n                tf.expand_dims(img, axis=-1)\n                for img in tf.unstack(image_windows, axis=-1)]\n            fixed_image, fixed_label, moving_image, moving_label = \\\n                image_windows_list\n\n            # estimate ddf\n            dense_field = self.net(fixed_image, moving_image)\n            if isinstance(dense_field, tuple):\n                dense_field = dense_field[0]\n\n            # transform the moving labels\n            resampler = ResamplerLayer(\n                interpolation=\'linear\', boundary=\'replicate\')\n            resampled_moving_label = resampler(moving_label, dense_field)\n\n            # compute label loss (foreground only)\n            loss_func = LossFunction(\n                n_class=1,\n                loss_type=self.action_param.loss_type,\n                softmax=False)\n            label_loss = loss_func(prediction=resampled_moving_label,\n                                   ground_truth=fixed_label)\n\n            dice_fg = 1.0 - label_loss\n            # appending regularisation loss\n            total_loss = label_loss\n            reg_loss = tf.get_collection(\'bending_energy\')\n            if reg_loss:\n                total_loss = total_loss + \\\n                    self.net_param.decay * tf.reduce_mean(reg_loss)\n\n            self.total_loss = total_loss\n\n            # compute training gradients\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            grads = self.optimiser.compute_gradients(\n                total_loss, colocate_gradients_with_ops=True)\n            gradients_collector.add_to_collection(grads)\n\n            metrics_dice = loss_func(\n                prediction=tf.to_float(resampled_moving_label >= 0.5),\n                ground_truth=tf.to_float(fixed_label >= 0.5))\n            metrics_dice = 1.0 - metrics_dice\n\n            # command line output\n            outputs_collector.add_to_collection(\n                var=dice_fg, name=\'one_minus_data_loss\',\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=tf.reduce_mean(reg_loss), name=\'bending_energy\',\n                collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=total_loss, name=\'total_loss\', collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=metrics_dice, name=\'ave_fg_dice\', collection=CONSOLE)\n\n            # for tensorboard\n            outputs_collector.add_to_collection(\n                var=dice_fg,\n                name=\'data_loss\',\n                average_over_devices=True,\n                summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=total_loss,\n                name=\'total_loss\',\n                average_over_devices=True,\n                summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=metrics_dice,\n                name=\'averaged_foreground_Dice\',\n                average_over_devices=True,\n                summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            # for visualisation debugging\n            # resampled_moving_image = resampler(moving_image, dense_field)\n            # outputs_collector.add_to_collection(\n            #     var=fixed_image, name=\'fixed_image\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=fixed_label, name=\'fixed_label\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=moving_image, name=\'moving_image\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=moving_label, name=\'moving_label\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=resampled_moving_image, name=\'resampled_image\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=resampled_moving_label, name=\'resampled_label\',\n            #     collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=dense_field, name=\'ddf\', collection=NETWORK_OUTPUT)\n            # outputs_collector.add_to_collection(\n            #     var=locations, name=\'locations\', collection=NETWORK_OUTPUT)\n\n            # outputs_collector.add_to_collection(\n            #     var=shift[0], name=\'a\', collection=CONSOLE)\n            # outputs_collector.add_to_collection(\n            #     var=shift[1], name=\'b\', collection=CONSOLE)\n        else:\n            image_windows, locations = self.sampler()\n            image_windows_list = [\n                tf.expand_dims(img, axis=-1)\n                for img in tf.unstack(image_windows, axis=-1)]\n            fixed_image, fixed_label, moving_image, moving_label = \\\n                image_windows_list\n\n            dense_field = self.net(fixed_image, moving_image)\n            if isinstance(dense_field, tuple):\n                dense_field = dense_field[0]\n\n            # transform the moving labels\n            resampler = ResamplerLayer(\n                interpolation=\'linear\', boundary=\'replicate\')\n            resampled_moving_image = resampler(moving_image, dense_field)\n            resampled_moving_label = resampler(moving_label, dense_field)\n\n            outputs_collector.add_to_collection(\n                var=fixed_image, name=\'fixed_image\',\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=moving_image, name=\'moving_image\',\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=resampled_moving_image,\n                name=\'resampled_moving_image\',\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=resampled_moving_label,\n                name=\'resampled_moving_label\',\n                collection=NETWORK_OUTPUT)\n\n            outputs_collector.add_to_collection(\n                var=fixed_label, name=\'fixed_label\',\n                collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=moving_label, name=\'moving_label\',\n                collection=NETWORK_OUTPUT)\n            #outputs_collector.add_to_collection(\n            #    var=dense_field, name=\'field\',\n            #    collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=locations, name=\'locations\',\n                collection=NETWORK_OUTPUT)\n\n            self.output_decoder = ResizeSamplesAggregator(\n                image_reader=self.readers[0], # fixed image reader\n                name=\'fixed_image\',\n                output_path=self.action_param.save_seg_dir,\n                interp_order=self.action_param.output_interp_order)\n\n    def interpret_output(self, batch_output):\n        if self.is_training:\n            return True\n        return self.output_decoder.decode_batch(\n            {\'window_resampled\':batch_output[\'resampled_moving_image\']},\n            batch_output[\'locations\'])\n\n'"
niftynet/contrib/csv_reader/applications_maybe/regression_application.py,13,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_weighted_v2 import WeightedSampler\nfrom niftynet.engine.sampler_balanced_v2 import BalancedSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.crop import CropLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_regression import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.layer.rgb_histogram_equilisation import \\\n    RGBHistogramEquilisationLayer\nfrom niftynet.evaluation.regression_evaluator import RegressionEvaluator\nfrom niftynet.layer.rand_elastic_deform import RandomElasticDeformationLayer\nfrom niftynet.contrib.csv_reader.csv_reader import CSVReader\nfrom niftynet.contrib.csv_reader.sampler_resize_v2_csv import ResizeSamplerCSV as ResizeSampler\nfrom niftynet.contrib.csv_reader.sampler_grid_v2_csv import GridSamplerCSV as \\\n    GridSampler\nfrom niftynet.contrib.csv_reader.sampler_uniform_v2_csv import \\\n    UniformSamplerCSV \\\n    as UniformSampler\nfrom niftynet.contrib.csv_reader.sampler_weighted_v2_csv import \\\n    WeightedSamplerCSV as WeightedSampler\nfrom niftynet.contrib.csv_reader.sampler_balanced_v2_csv import \\\n    BalancedSamplerCSV as BalanceSampler\n\nSUPPORTED_INPUT = set([\'image\', \'output\', \'weight\', \'sampler\', \'inferred\'])\n\n\nclass RegressionApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""REGRESSION""\n\n    def __init__(self, net_param, action_param, action):\n        BaseApplication.__init__(self)\n        tf.logging.info(\'starting regression application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.regression_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'uniform\': (self.initialise_uniform_sampler,\n                        self.initialise_grid_sampler,\n                        self.initialise_grid_aggregator),\n            \'weighted\': (self.initialise_weighted_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler,\n                       self.initialise_resize_aggregator),\n            \'balanced\': (self.initialise_balanced_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.regression_param = task_param\n\n        # initialise input image readers\n        if self.is_training:\n            reader_names = (\'image\', \'output\', \'weight\', \'sampler\')\n            csv_reader_names = ()\n        elif self.is_inference:\n            # in the inference process use `image` input only\n            reader_names = (\'image\',)\n            csv_reader_names = ()\n        elif self.is_evaluation:\n            reader_names = (\'image\', \'output\', \'inferred\')\n            csv_reader_names =()\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n        self.csv_readers = [CSVReader(csv_reader_names).initialise(\n            data_param, task_param, file_list) for file_list in file_lists]\n\n        # initialise input preprocessing layers\n        mean_var_normaliser = MeanVarNormalisationLayer(image_name=\'image\') \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n        rgb_normaliser = RGBHistogramEquilisationLayer(\n            image_name=\'image\',\n            name=\'rbg_norm_layer\') if self.net_param.rgb_normalisation else None\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n        if rgb_normaliser is not None:\n            normalisation_layers.append(rgb_normaliser)\n\n        volume_padding_layer = [PadLayer(\n            image_name=SUPPORTED_INPUT,\n            border=self.net_param.volume_padding_size,\n            mode=self.net_param.volume_padding_mode,\n            pad_to=self.net_param.volume_padding_to_size)\n        ]\n\n        # initialise training data augmentation layers\n        augmentation_layers = []\n        if self.is_training:\n            train_param = self.action_param\n            if train_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=train_param.random_flipping_axes))\n            if train_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=train_param.scaling_percentage[0],\n                    max_percentage=train_param.scaling_percentage[1],\n                    antialiasing=train_param.antialiasing,\n                    isotropic=train_param.isotropic_scaling))\n            if train_param.rotation_angle:\n                rotation_layer = RandomRotationLayer()\n                if train_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        train_param.rotation_angle)\n                augmentation_layers.append(rotation_layer)\n            if train_param.do_elastic_deformation:\n                spatial_rank = list(self.readers[0].spatial_ranks.values())[0]\n                augmentation_layers.append(RandomElasticDeformationLayer(\n                    spatial_rank=spatial_rank,\n                    num_controlpoints=train_param.num_ctrl_points,\n                    std_deformation_sigma=train_param.deformation_sigma,\n                    proportion_to_augment=train_param.proportion_to_deform))\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n            volume_padding_layer + normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(\n                volume_padding_layer + normalisation_layers)\n\n    def initialise_uniform_sampler(self):\n        self.sampler = [[UniformSampler(\n            reader=reader,\n            csv_reader=None,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_weighted_sampler(self):\n        self.sampler = [[WeightedSampler(\n            reader=reader,\n            csv_reader=None,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            csv_reader=None,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_sampler(self):\n        self.sampler = [[GridSampler(\n            reader=reader,\n            csv_reader=None,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_balanced_sampler(self):\n        self.sampler = [[BalancedSampler(\n            reader=reader,\n            csv_reader=None,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_aggregator(self):\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix,\n            fill_constant=self.action_param.fill_constant)\n\n    def initialise_resize_aggregator(self):\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        elif self.is_inference:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_aggregator(self):\n        self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=1,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            self.patience = self.action_param.patience\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(loss_type=self.action_param.loss_type)\n\n            crop_layer = CropLayer(border=self.regression_param.loss_border)\n            weight_map = data_dict.get(\'weight\', None)\n            weight_map = None if weight_map is None else crop_layer(weight_map)\n            data_loss = loss_func(\n                prediction=crop_layer(net_out),\n                ground_truth=crop_layer(data_dict[\'output\']),\n                weight_map=weight_map)\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n\n            # Get all vars\n            to_optimise = tf.trainable_variables()\n            vars_to_freeze = \\\n                self.action_param.vars_to_freeze or \\\n                self.action_param.vars_to_restore\n            if vars_to_freeze:\n                import re\n                var_regex = re.compile(vars_to_freeze)\n                # Only optimise vars that are not frozen\n                to_optimise = \\\n                    [v for v in to_optimise if not var_regex.search(v.name)]\n                tf.logging.info(\n                    ""Optimizing %d out of %d trainable variables, ""\n                    ""the other variables are fixed (--vars_to_freeze %s)"",\n                    len(to_optimise),\n                    len(tf.trainable_variables()),\n                    vars_to_freeze)\n\n            self.total_loss = loss\n\n            grads = self.optimiser.compute_gradients(\n                loss, var_list=to_optimise, colocate_gradients_with_ops=True)\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n\n        elif self.is_inference:\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n            net_out = PostProcessingLayer(\'IDENTITY\')(net_out)\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        if self.is_inference:\n            return self.output_decoder.decode_batch(\n                {\'window_reg\':batch_output[\'window\']}, batch_output[\'location\'])\n        return True\n\n    def initialise_evaluator(self, eval_param):\n        self.eval_param = eval_param\n        self.evaluator = RegressionEvaluator(self.readers[0],\n                                             self.regression_param,\n                                             eval_param)\n\n    def add_inferred_output(self, data_param, task_param):\n        return self.add_inferred_output_like(data_param, task_param, \'output\')\n'"
niftynet/contrib/csv_reader/applications_maybe/segmentation_application.py,14,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nfrom niftynet.application.base_application import BaseApplication\nfrom niftynet.engine.application_factory import \\\n    ApplicationNetFactory, InitializerFactory, OptimiserFactory\nfrom niftynet.engine.application_variables import \\\n    CONSOLE, NETWORK_OUTPUT, TF_SUMMARIES\nfrom niftynet.engine.sampler_grid_v2 import GridSampler\nfrom niftynet.engine.sampler_resize_v2 import ResizeSampler\nfrom niftynet.engine.sampler_uniform_v2 import UniformSampler\nfrom niftynet.engine.sampler_weighted_v2 import WeightedSampler\nfrom niftynet.engine.sampler_balanced_v2 import BalancedSampler\nfrom niftynet.engine.windows_aggregator_grid import GridSamplesAggregator\nfrom niftynet.engine.windows_aggregator_resize import ResizeSamplesAggregator\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.layer.binary_masking import BinaryMaskingLayer\nfrom niftynet.layer.discrete_label_normalisation import \\\n    DiscreteLabelNormalisationLayer\nfrom niftynet.layer.histogram_normalisation import \\\n    HistogramNormalisationLayer\nfrom niftynet.layer.loss_segmentation import LossFunction\nfrom niftynet.layer.mean_variance_normalisation import \\\n    MeanVarNormalisationLayer\nfrom niftynet.layer.pad import PadLayer\nfrom niftynet.layer.post_processing import PostProcessingLayer\nfrom niftynet.layer.rand_flip import RandomFlipLayer\nfrom niftynet.layer.rand_rotation import RandomRotationLayer\nfrom niftynet.layer.rand_spatial_scaling import RandomSpatialScalingLayer\nfrom niftynet.layer.rgb_histogram_equilisation import \\\n    RGBHistogramEquilisationLayer\nfrom niftynet.evaluation.segmentation_evaluator import SegmentationEvaluator\nfrom niftynet.layer.rand_elastic_deform import RandomElasticDeformationLayer\n\nSUPPORTED_INPUT = set([\'image\', \'label\', \'weight\', \'sampler\', \'inferred\'])\n\n\nclass SegmentationApplication(BaseApplication):\n    REQUIRED_CONFIG_SECTION = ""SEGMENTATION""\n\n    def __init__(self, net_param, action_param, action):\n        super(SegmentationApplication, self).__init__()\n        tf.logging.info(\'starting segmentation application\')\n        self.action = action\n\n        self.net_param = net_param\n        self.action_param = action_param\n\n        self.data_param = None\n        self.segmentation_param = None\n        self.SUPPORTED_SAMPLING = {\n            \'uniform\': (self.initialise_uniform_sampler,\n                        self.initialise_grid_sampler,\n                        self.initialise_grid_aggregator),\n            \'weighted\': (self.initialise_weighted_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n            \'resize\': (self.initialise_resize_sampler,\n                       self.initialise_resize_sampler,\n                       self.initialise_resize_aggregator),\n            \'balanced\': (self.initialise_balanced_sampler,\n                         self.initialise_grid_sampler,\n                         self.initialise_grid_aggregator),\n        }\n\n    def initialise_dataset_loader(\n            self, data_param=None, task_param=None, data_partitioner=None):\n\n        self.data_param = data_param\n        self.segmentation_param = task_param\n\n        # initialise input image readers\n        if self.is_training:\n            reader_names = (\'image\', \'label\', \'weight\', \'sampler\')\n        elif self.is_inference:\n            # in the inference process use `image` input only\n            reader_names = (\'image\',)\n        elif self.is_evaluation:\n            reader_names = (\'image\', \'label\', \'inferred\')\n        else:\n            tf.logging.fatal(\n                \'Action `%s` not supported. Expected one of %s\',\n                self.action, self.SUPPORTED_PHASES)\n            raise ValueError\n        try:\n            reader_phase = self.action_param.dataset_to_infer\n        except AttributeError:\n            reader_phase = None\n        file_lists = data_partitioner.get_file_lists_by(\n            phase=reader_phase, action=self.action)\n        self.readers = [\n            ImageReader(reader_names).initialise(\n                data_param, task_param, file_list) for file_list in file_lists]\n\n        # initialise input preprocessing layers\n        foreground_masking_layer = BinaryMaskingLayer(\n            type_str=self.net_param.foreground_type,\n            multimod_fusion=self.net_param.multimod_foreground_type,\n            threshold=0.0) \\\n            if self.net_param.normalise_foreground_only else None\n        mean_var_normaliser = MeanVarNormalisationLayer(\n            image_name=\'image\', binary_masking_func=foreground_masking_layer) \\\n            if self.net_param.whitening else None\n        histogram_normaliser = HistogramNormalisationLayer(\n            image_name=\'image\',\n            modalities=vars(task_param).get(\'image\'),\n            model_filename=self.net_param.histogram_ref_file,\n            binary_masking_func=foreground_masking_layer,\n            norm_type=self.net_param.norm_type,\n            cutoff=self.net_param.cutoff,\n            name=\'hist_norm_layer\') \\\n            if (self.net_param.histogram_ref_file and\n                self.net_param.normalisation) else None\n        rgb_normaliser = RGBHistogramEquilisationLayer(\n            image_name=\'image\',\n            name=\'rbg_norm_layer\') if self.net_param.rgb_normalisation else None\n        label_normalisers = None\n        if self.net_param.histogram_ref_file and \\\n                task_param.label_normalisation:\n            label_normalisers = [DiscreteLabelNormalisationLayer(\n                image_name=\'label\',\n                modalities=vars(task_param).get(\'label\'),\n                model_filename=self.net_param.histogram_ref_file)]\n            if self.is_evaluation:\n                label_normalisers.append(\n                    DiscreteLabelNormalisationLayer(\n                        image_name=\'inferred\',\n                        modalities=vars(task_param).get(\'inferred\'),\n                        model_filename=self.net_param.histogram_ref_file))\n                label_normalisers[-1].key = label_normalisers[0].key\n\n        normalisation_layers = []\n        if histogram_normaliser is not None:\n            normalisation_layers.append(histogram_normaliser)\n        if rgb_normaliser is not None:\n            normalisation_layers.append(rgb_normaliser)\n        if mean_var_normaliser is not None:\n            normalisation_layers.append(mean_var_normaliser)\n        if task_param.label_normalisation and \\\n                (self.is_training or not task_param.output_prob):\n            normalisation_layers.extend(label_normalisers)\n\n        volume_padding_layer = [PadLayer(\n            image_name=SUPPORTED_INPUT,\n            border=self.net_param.volume_padding_size,\n            mode=self.net_param.volume_padding_mode,\n            pad_to=self.net_param.volume_padding_to_size)\n        ]\n        # initialise training data augmentation layers\n        augmentation_layers = []\n        if self.is_training:\n            train_param = self.action_param\n            self.patience = train_param.patience\n            if train_param.random_flipping_axes != -1:\n                augmentation_layers.append(RandomFlipLayer(\n                    flip_axes=train_param.random_flipping_axes))\n            if train_param.scaling_percentage:\n                augmentation_layers.append(RandomSpatialScalingLayer(\n                    min_percentage=train_param.scaling_percentage[0],\n                    max_percentage=train_param.scaling_percentage[1],\n                    antialiasing=train_param.antialiasing,\n                    isotropic=train_param.isotropic_scaling))\n            if train_param.rotation_angle or \\\n                    train_param.rotation_angle_x or \\\n                    train_param.rotation_angle_y or \\\n                    train_param.rotation_angle_z:\n                rotation_layer = RandomRotationLayer()\n                if train_param.rotation_angle:\n                    rotation_layer.init_uniform_angle(\n                        train_param.rotation_angle)\n                else:\n                    rotation_layer.init_non_uniform_angle(\n                        train_param.rotation_angle_x,\n                        train_param.rotation_angle_y,\n                        train_param.rotation_angle_z)\n                augmentation_layers.append(rotation_layer)\n            if train_param.do_elastic_deformation:\n                spatial_rank = list(self.readers[0].spatial_ranks.values())[0]\n                augmentation_layers.append(RandomElasticDeformationLayer(\n                    spatial_rank=spatial_rank,\n                    num_controlpoints=train_param.num_ctrl_points,\n                    std_deformation_sigma=train_param.deformation_sigma,\n                    proportion_to_augment=train_param.proportion_to_deform))\n\n        # only add augmentation to first reader (not validation reader)\n        self.readers[0].add_preprocessing_layers(\n            volume_padding_layer + normalisation_layers + augmentation_layers)\n\n        for reader in self.readers[1:]:\n            reader.add_preprocessing_layers(\n                volume_padding_layer + normalisation_layers)\n\n    def initialise_uniform_sampler(self):\n        self.sampler = [[UniformSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_weighted_sampler(self):\n        self.sampler = [[WeightedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_resize_sampler(self):\n        self.sampler = [[ResizeSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            shuffle=self.is_training,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_sampler(self):\n        self.sampler = [[GridSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            spatial_window_size=self.action_param.spatial_window_size,\n            window_border=self.action_param.border,\n            smaller_final_batch_mode=self.net_param.smaller_final_batch_mode,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_balanced_sampler(self):\n        self.sampler = [[BalancedSampler(\n            reader=reader,\n            window_sizes=self.data_param,\n            batch_size=self.net_param.batch_size,\n            windows_per_image=self.action_param.sample_per_volume,\n            queue_length=self.net_param.queue_length) for reader in\n            self.readers]]\n\n    def initialise_grid_aggregator(self):\n        self.output_decoder = GridSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix,\n            fill_constant=self.action_param.fill_constant)\n\n    def initialise_resize_aggregator(self):\n        self.output_decoder = ResizeSamplesAggregator(\n            image_reader=self.readers[0],\n            output_path=self.action_param.save_seg_dir,\n            window_border=self.action_param.border,\n            interp_order=self.action_param.output_interp_order,\n            postfix=self.action_param.output_postfix)\n\n    def initialise_sampler(self):\n        if self.is_training:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][0]()\n        elif self.is_inference:\n            self.SUPPORTED_SAMPLING[self.net_param.window_sampling][1]()\n\n    def initialise_aggregator(self):\n        self.SUPPORTED_SAMPLING[self.net_param.window_sampling][2]()\n\n    def initialise_network(self):\n        w_regularizer = None\n        b_regularizer = None\n        reg_type = self.net_param.reg_type.lower()\n        decay = self.net_param.decay\n        if reg_type == \'l2\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l2_regularizer(decay)\n            b_regularizer = regularizers.l2_regularizer(decay)\n        elif reg_type == \'l1\' and decay > 0:\n            from tensorflow.contrib.layers.python.layers import regularizers\n            w_regularizer = regularizers.l1_regularizer(decay)\n            b_regularizer = regularizers.l1_regularizer(decay)\n\n        self.net = ApplicationNetFactory.create(self.net_param.name)(\n            num_classes=self.segmentation_param.num_classes,\n            w_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.weight_initializer),\n            b_initializer=InitializerFactory.get_initializer(\n                name=self.net_param.bias_initializer),\n            w_regularizer=w_regularizer,\n            b_regularizer=b_regularizer,\n            acti_func=self.net_param.activation_function)\n\n    def connect_data_and_network(self,\n                                 outputs_collector=None,\n                                 gradients_collector=None):\n\n        def switch_sampler(for_training):\n            with tf.name_scope(\'train\' if for_training else \'validation\'):\n                sampler = self.get_sampler()[0][0 if for_training else -1]\n                return sampler.pop_batch_op()\n\n        if self.is_training:\n            if self.action_param.validation_every_n > 0:\n                data_dict = tf.cond(tf.logical_not(self.is_validation),\n                                    lambda: switch_sampler(for_training=True),\n                                    lambda: switch_sampler(for_training=False))\n            else:\n                data_dict = switch_sampler(for_training=True)\n\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            with tf.name_scope(\'Optimiser\'):\n                optimiser_class = OptimiserFactory.create(\n                    name=self.action_param.optimiser)\n                self.optimiser = optimiser_class.get_instance(\n                    learning_rate=self.action_param.lr)\n            loss_func = LossFunction(\n                n_class=self.segmentation_param.num_classes,\n                loss_type=self.action_param.loss_type,\n                softmax=self.segmentation_param.softmax)\n            data_loss = loss_func(\n                prediction=net_out,\n                ground_truth=data_dict.get(\'label\', None),\n                weight_map=data_dict.get(\'weight\', None))\n            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            if self.net_param.decay > 0.0 and reg_losses:\n                reg_loss = tf.reduce_mean(\n                    [tf.reduce_mean(reg_loss) for reg_loss in reg_losses])\n                loss = data_loss + reg_loss\n            else:\n                loss = data_loss\n\n            # Get all vars\n            to_optimise = tf.trainable_variables()\n            vars_to_freeze = \\\n                self.action_param.vars_to_freeze or \\\n                self.action_param.vars_to_restore\n            if vars_to_freeze:\n                import re\n                var_regex = re.compile(vars_to_freeze)\n                # Only optimise vars that are not frozen\n                to_optimise = \\\n                    [v for v in to_optimise if not var_regex.search(v.name)]\n                tf.logging.info(\n                    ""Optimizing %d out of %d trainable variables, ""\n                    ""the other variables fixed (--vars_to_freeze %s)"",\n                    len(to_optimise),\n                    len(tf.trainable_variables()),\n                    vars_to_freeze)\n\n            grads = self.optimiser.compute_gradients(\n                loss, var_list=to_optimise, colocate_gradients_with_ops=True)\n\n            self.total_loss = loss\n\n            # collecting gradients variables\n            gradients_collector.add_to_collection([grads])\n\n            # collecting output variables\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=self.total_loss, name=\'total_loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=False, collection=CONSOLE)\n            outputs_collector.add_to_collection(\n                var=data_loss, name=\'loss\',\n                average_over_devices=True, summary_type=\'scalar\',\n                collection=TF_SUMMARIES)\n\n            # outputs_collector.add_to_collection(\n            #    var=image*180.0, name=\'image\',\n            #    average_over_devices=False, summary_type=\'image3_sagittal\',\n            #    collection=TF_SUMMARIES)\n\n            # outputs_collector.add_to_collection(\n            #    var=image, name=\'image\',\n            #    average_over_devices=False,\n            #    collection=NETWORK_OUTPUT)\n\n            # outputs_collector.add_to_collection(\n            #    var=tf.reduce_mean(image), name=\'mean_image\',\n            #    average_over_devices=False, summary_type=\'scalar\',\n            #    collection=CONSOLE)\n        elif self.is_inference:\n            # converting logits into final output for\n            # classification probabilities or argmax classification labels\n            data_dict = switch_sampler(for_training=False)\n            image = tf.cast(data_dict[\'image\'], tf.float32)\n            net_args = {\'is_training\': self.is_training,\n                        \'keep_prob\': self.net_param.keep_prob}\n            net_out = self.net(image, **net_args)\n\n            output_prob = self.segmentation_param.output_prob\n            num_classes = self.segmentation_param.num_classes\n            if output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'SOFTMAX\', num_classes=num_classes)\n            elif not output_prob and num_classes > 1:\n                post_process_layer = PostProcessingLayer(\n                    \'ARGMAX\', num_classes=num_classes)\n            else:\n                post_process_layer = PostProcessingLayer(\n                    \'IDENTITY\', num_classes=num_classes)\n            net_out = post_process_layer(net_out)\n\n            outputs_collector.add_to_collection(\n                var=net_out, name=\'window\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            outputs_collector.add_to_collection(\n                var=data_dict[\'image_location\'], name=\'location\',\n                average_over_devices=False, collection=NETWORK_OUTPUT)\n            self.initialise_aggregator()\n\n    def interpret_output(self, batch_output):\n        if self.is_inference:\n            return self.output_decoder.decode_batch(\n                {\'window_seg\':batch_output[\'window\']}, batch_output[\'location\'])\n        return True\n\n    def initialise_evaluator(self, eval_param):\n        self.eval_param = eval_param\n        self.evaluator = SegmentationEvaluator(self.readers[0],\n                                               self.segmentation_param,\n                                               eval_param)\n\n    def add_inferred_output(self, data_param, task_param):\n        return self.add_inferred_output_like(data_param, task_param, \'label\')\n'"
niftynet/contrib/niftyreg_image_resampling/tests/__init__.py,0,b''
niftynet/contrib/niftyreg_image_resampling/tests/test_python_wrapper.py,14,"b'from niftynet.contrib.niftyreg_image_resampling.niftyreg_image_resampling import NiftyregImageResamplingLayer\nfrom niftynet.contrib.niftyreg_image_resampling.tests.test_resampler import ResamplerTest\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.test as tft\n\nclass WrapperResamplerTest(ResamplerTest):\n    """"""\n    Unit test for NiftyregImageResamplingLayer\n    """"""\n\n    INTERPOLATIONS = ((0, \'NEAREST\'),\n                      (1, \'LINEAR\'),\n                      (3, \'BSPLINE\'))\n\n    def _test_differential(self, use_gpu, interpolations):\n        for floating in self._get_images(True):\n            floating_data = floating.get_data()\n            nof_dims = len(floating_data.shape)\n            nof_mods = 2\n\n            if nof_dims == 3:\n                # 3D test takes forever (at least with debug builds)\n                continue\n\n            floating_shape = list(floating_data.shape)\n            image_batch_shape = [1] + floating_shape + [nof_mods]\n            disp_batch_shape = [1] + floating_shape + [nof_dims]\n\n            disp_template \\\n                = self._make_constant_displacement_image(0, 0,\n                                                         floating_data)\n            u = 2.5001\n\n            floating_data = np.stack([floating_data]*nof_mods, axis=-1)\n            for m in range(nof_mods):\n                floating_data[...,m] = (1 + m)*floating_data[...,m]\n\n            for iname in interpolations:\n                d = 1\n                with self.session(use_gpu=use_gpu) as sess:\n                    img = tf.constant(\n                        floating_data.reshape(image_batch_shape),\n                        dtype=tf.float32)\n                    disp = tf.constant(u,\n                                       dtype=tf.float32)\n\n                    base_field = tf.constant(disp_template\\\n                                             .reshape(disp_batch_shape),\n                                             dtype=tf.float32)\n\n                    disp_field = []\n                    for i in range(nof_dims):\n                        if i == d:\n                            disp_field.append(base_field[...,i] + disp)\n                        else:\n                            disp_field.append(base_field[...,i])\n                    disp_field = tf.stack(disp_field, axis=-1)\n\n                    warped = NiftyregImageResamplingLayer(interpolation=iname,\n                                                          boundary=\'ZERO\')\n                    warped = warped(img, disp_field)\n\n                    tgrad, refgrad = tft.compute_gradient(\n                        disp,\n                        (),\n                        warped,\n                        tuple(image_batch_shape))\n\n                    error = np.power(tgrad - refgrad, 2).sum()\n                    refmag = np.power(refgrad, 2).sum()\n\n                    self.assertLessEqual(error, 1e-2*refmag)\n\n    def test_cpu_differential(self):\n        self._test_differential(False, (\'LINEAR\', \'BSPLINE\'))\n\n    def test_gpu_differential(self):\n        if tft.is_gpu_available(cuda_only=True) and tft.is_built_with_cuda():\n            self._test_differential(True, (\'LINEAR\', \'BSPLINE\'))\n        else:\n            self.skipTest(\'No CUDA support available\')\n\n    def test_image_gradient(self):\n        for img in self._get_images(True):\n            basedata = img.get_data()\n\n            imgshape = list(basedata.shape)\n            nof_dims = len(imgshape)\n            for nof_mods in range(1, 3):\n                image_batch_shape = [1] + imgshape + [nof_mods]\n                disp_batch_shape = [1] + imgshape + [nof_dims]\n\n                multimod_data = []\n                for j in range(nof_mods):\n                    multimod_data.append((j + 1)*basedata)\n                imgdata = np.stack(multimod_data, axis=-1)\n\n                for _, inter in self.INTERPOLATIONS:\n                    for bdy in (\'REPLICATE\', \'ZERO\', \'SYMMETRIC\'):\n                        u = 3.5001\n                        d = 1\n                        disp = self._make_constant_displacement_image(\n                            u, d, imgdata[...,0].reshape(imgshape))\n\n                        with self.session() as sess:\n                            tfimg = tf.constant(\n                                imgdata.reshape(image_batch_shape),\n                                dtype=tf.float32)\n                            tfdisp = tf.constant(\n                                disp.reshape(disp_batch_shape),\n                                dtype=tf.float32)\n\n                            warped = NiftyregImageResamplingLayer(interpolation=inter,\n                                                                  boundary=bdy)\n                            warped = warped(tfimg, tfdisp)\n                            dummy_cost = tf.reduce_mean(tf.pow(warped, 2))\n\n                            tgrad, refgrad = tft.compute_gradient(\n                                tfimg,\n                                image_batch_shape,\n                                dummy_cost,\n                                ())\n\n                            error = np.power(tgrad - refgrad, 2).sum()\n                            refmag = np.power(refgrad, 2).sum()\n\n                            self.assertLessEqual(error, 5e-2*refmag)\n\n    def _test_resampling(self, use_gpu):\n        for floating in self._get_images():\n            floating_data = floating.get_data()\n            nof_dims = len(floating_data.shape)\n\n            floating_shape = list(floating_data.shape)\n            if floating_shape[-1] == 1:\n                floating_shape = floating_shape[:-1]\n            image_batch_shape = [1] + floating_shape + [1]\n            disp_batch_shape = [1] + floating_shape + [nof_dims]\n\n            for code, inter in self.INTERPOLATIONS:\n                with self.session(use_gpu=use_gpu) as sess:\n                    img = tf.placeholder(tf.float32,\n                                         shape=image_batch_shape)\n                    disp = tf.placeholder(tf.float32,\n                                          shape=disp_batch_shape)\n\n                    warped = NiftyregImageResamplingLayer(interpolation=inter,\n                                                          boundary=\'NAN\')\n                    warped = warped(img, disp)\n\n                    for u in (0.5001, 3.50001):\n                        for d in range(nof_dims):\n                            displacement_data \\\n                                = self._make_constant_displacement_image(\n                                    u, d, floating_data)\n\n                            resampled_data = sess.run(\n                                warped,\n                                feed_dict={\n                                    img: floating_data\\\n                                    .reshape(image_batch_shape),\n                                    disp: displacement_data\\\n                                    .reshape(disp_batch_shape),\n                                })\n\n                            resampled_data = resampled_data.reshape(floating_shape)\n\n                            self._test_resampled(resampled_data, floating_data,\n                                                 u, code, d)\n\n\n    def test_cpu_resampling(self):\n        self._test_resampling(False)\n\n    def test_gpu_resampling(self):\n        if tft.is_gpu_available(cuda_only=True) and tft.is_built_with_cuda():\n            self._test_resampling(True)\n        else:\n            self.skipTest(\'No CUDA support available\')\n\n\nif __name__ == \'__main__\':\n    tft.main()\n'"
niftynet/contrib/niftyreg_image_resampling/tests/test_resampler.py,2,"b'from glob import glob\nimport math\nimport os.path as osp\nimport nibabel as nib\nimport numpy as np\nimport copy\n\nimport tensorflow as tf\nimport tensorflow.test as tft\n\nfrom niftynet.contrib.niftyreg_image_resampling.niftyreg_module_loader import get_niftyreg_module\n\n\nres = get_niftyreg_module()\n\n\nclass ResamplerTest(tft.TestCase):\n    """"""\n    Unit test for GPUImageResampling defined in python_wrapper.cpp\n    """"""\n\n    def _get_images(self, do_small=False):\n        data_dir = osp.join(osp.dirname(__file__), \'data\')\n        filename_temp = \'test_image_\' + (\'vsmall\' if do_small else \'large\') + \'_*.nii\'\n\n        for test_img_path in glob(osp.join(data_dir, filename_temp)):\n            yield nib.load(test_img_path)\n\n    @staticmethod\n    def _dump_image(dst_path, img):\n        nib.save(nib.Nifti1Image(img, np.eye(4)),\n                 str(dst_path))\n\n    def _test_resampled(self, resampled, floating, d, inter, axis):\n        use_nearest = inter == 0\n        max_idx_disp = int(round(d)) if use_nearest else int(math.ceil(d))\n\n        self.assertAllEqual(resampled.shape, floating.shape)\n\n        ref = float(\'nan\')*np.zeros_like(resampled)\n        size = ref.shape[axis]\n\n        ref_slice = slice(0, size - max_idx_disp)\n        flo_slice = slice(max_idx_disp, size)\n\n        base_block = [slice(0, d) for d in floating.shape]\n        flo_block = copy.copy(base_block)\n        ref_block = copy.copy(base_block)\n        flo_block[axis] = flo_slice\n        ref_block[axis] = ref_slice\n\n        ref[tuple(ref_block)] = floating[tuple(flo_block)]\n\n        if not use_nearest:\n            flo_slice = slice(max_idx_disp - 1, size - 1)\n            flo_block = copy.copy(base_block)\n            flo_block[axis] = flo_slice\n            ref[tuple(ref_block)] += floating[tuple(flo_block)]\n            ref /= 2\n\n        mask = np.isfinite(ref + resampled)\n        max_err = abs(ref[mask] - resampled[mask]).max()\n        self.assertLessEqual(max_err, 1e-2)\n\n        if len(resampled.shape) == 2:\n            bdy_size = 1 + max(max_idx_disp, 1)*(inter + 1)\\\n                *(resampled.shape[0] + resampled.shape[1])\n        else:\n            bdy_size = 7 + (max(max_idx_disp, 1) + 2)*(inter + 1)\\\n                *max(resampled.shape)**2\n\n        nof_nans = resampled.size - mask.sum()\n        self.assertLess(nof_nans, bdy_size)\n\n    def _make_constant_displacement_image(self, u, axis, image_data):\n        nof_dims = len(image_data.shape)\n        displacement_shape = list(image_data.shape) \\\n            + [1]*(3 - nof_dims) + [nof_dims]\n\n        displacement_data = np.zeros(displacement_shape)\n        displacement_data[...,axis] = u\n        for dd in range(nof_dims):\n            idcs = np.arange(displacement_data.shape[dd])\n            idcs = idcs.reshape(\n                [1]*dd + [displacement_shape[dd]] \\\n                + [1]*(nof_dims - dd - 1))\n\n            tile_dim = list(displacement_shape[:dd]) + [1] \\\n                + list(displacement_shape[dd+1:nof_dims])\n\n            idcs = np.tile(idcs, tile_dim)\n            if len(idcs.shape) < 3:\n                idcs = idcs.reshape(list(idcs.shape) + [1])\n\n            displacement_data[...,dd] += idcs\n\n        return displacement_data\n\n    def _test_resampling(self, use_gpu):\n        for floating in self._get_images():\n            floating_data = floating.get_data()\n            nof_dims = len(floating_data.shape)\n\n            transposed_shape = list(floating_data.shape)\n            transposed_shape.reverse()\n            image_batch_shape = [1]*2 + transposed_shape\n            disp_batch_shape = [1] + [nof_dims] + transposed_shape\n\n            for inter in (0, 1, 3):\n                with self.session(use_gpu=use_gpu) as sess:\n                    img = tf.placeholder(tf.float32,\n                                         shape=image_batch_shape)\n                    disp = tf.placeholder(tf.float32,\n                                          shape=disp_batch_shape)\n\n                    warped = res.niftyreg_image_resampling(img, disp,\n                                                           interpolation=inter)\n\n                    for u in (0.5001, 3.50001):\n                        for d in range(nof_dims):\n                            displacement_data \\\n                                = self._make_constant_displacement_image(\n                                    u, d, floating_data)\n\n                            # NiftyReg expects displacement components to be\n                            # indexed w/ slowest index\n                            def _transpose(data):\n                                return np.transpose(\n                                    data, range(len(data.shape) - 1, -1, -1))\n\n                            resampled_data = sess.run(\n                                warped,\n                                feed_dict={\n                                    img: _transpose(floating_data)\\\n                                    .reshape(image_batch_shape),\n                                    disp: _transpose(displacement_data)\\\n                                    .reshape(disp_batch_shape),\n                                })\n\n                            resampled_data = _transpose(resampled_data).reshape(floating_data.shape)\n\n                            self._test_resampled(resampled_data, floating_data,\n                                                 u, inter, d)\n\n\n    def test_cpu_resampling(self):\n        self._test_resampling(False)\n\n    def test_gpu_resampling(self):\n        if tft.is_gpu_available(cuda_only=True) and tft.is_built_with_cuda():\n            self._test_resampling(True)\n        else:\n            self.skipTest(\'No CUDA support available\')\n\n\nif __name__ == \'__main__\':\n    tft.main()\n'"
